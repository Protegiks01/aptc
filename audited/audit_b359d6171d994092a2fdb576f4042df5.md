# Audit Report

## Title
Unbounded window_size Configuration Allows Memory Exhaustion During Fast-Forward Sync Operations

## Summary
The `window_size` parameter in `OnChainConsensusConfig` lacks validation, allowing governance to set arbitrarily large values. During fast-forward sync operations (epoch changes or node recovery), this can cause nodes to attempt allocating memory for millions of blocks, leading to memory exhaustion and node crashes.

## Finding Description

The `window_size` parameter controls the execution pool block window size and can be set via on-chain governance. However, there is **no validation or upper bound** on this value. [1](#0-0) 

The configuration deserialization performs no validation: [2](#0-1) 

During fast-forward sync operations triggered by epoch changes or node recovery, the `generate_target_block_retrieval_payload_and_num_blocks` function calculates the number of blocks to retrieve based on `window_size`: [3](#0-2) 

With a large `window_size` value (e.g., 1,000,000) and a blockchain at round 1,000,000:
- `target_round = (1,000,000 + 1).saturating_sub(1,000,000) = 1`
- `num_blocks = 1,000,000 - 1 + 1 = 1,000,000`

The only validation is a simple assertion that doesn't prevent excessive values: [4](#0-3) 

The retrieval process allocates memory for all blocks in a growing vector: [5](#0-4) 

**Attack Scenario:**
1. During a governance upgrade, `window_size` is set to an excessively large value (intentionally or accidentally)
2. An epoch change occurs or a validator node restarts
3. The node initiates fast-forward sync
4. The sync process attempts to retrieve and store millions of blocks
5. Memory allocation fails, causing the node to crash with OOM

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under Aptos bug bounty criteria:
- **State inconsistencies requiring intervention**: Nodes crash during epoch transitions, requiring manual intervention
- **Validator node slowdowns**: Even partial allocation causes severe performance degradation

The impact is limited because:
- Block retrieval occurs in batches, and peers may not have all historical blocks
- Sync may fail with "NotEnoughBlocks" before complete exhaustion
- However, partial memory allocation alone can destabilize nodes

## Likelihood Explanation

**Likelihood: Medium-Low**

While this requires governance action, the scenario is realistic:
- Legitimate governance proposals could misconfigure `window_size`
- No validation warnings exist during configuration
- Testing might not catch extreme values
- During emergency upgrades, misconfigurations are more likely

The issue becomes likely during:
- Epoch transitions when all nodes sync simultaneously
- Node recovery after crashes or restarts
- Network upgrades changing execution pool configuration

## Recommendation

Implement strict validation on `window_size`:

```rust
// In types/src/on_chain_config/consensus_config.rs
const MAX_WINDOW_SIZE: u64 = 1000; // Reasonable upper bound

impl OnChainConsensusConfig {
    pub fn window_size(&self) -> Option<u64> {
        match self {
            OnChainConsensusConfig::V1(_)
            | OnChainConsensusConfig::V2(_)
            | OnChainConsensusConfig::V3 { .. } => None,
            OnChainConsensusConfig::V4 { window_size, .. }
            | OnChainConsensusConfig::V5 { window_size, .. } => {
                // Validate window_size
                if let Some(size) = window_size {
                    if *size > MAX_WINDOW_SIZE {
                        warn!("window_size {} exceeds maximum {}, capping", size, MAX_WINDOW_SIZE);
                        return Some(MAX_WINDOW_SIZE);
                    }
                }
                *window_size
            }
        }
    }
}
```

Additionally, add validation in `fast_forward_sync`:

```rust
// In consensus/src/block_storage/sync_manager.rs
const MAX_BLOCKS_TO_RETRIEVE: u64 = 10_000;

// After line 388
ensure!(
    num_blocks <= MAX_BLOCKS_TO_RETRIEVE,
    "Refusing to retrieve {} blocks (max: {}). window_size may be misconfigured.",
    num_blocks,
    MAX_BLOCKS_TO_RETRIEVE
);
```

## Proof of Concept

```rust
#[test]
fn test_unbounded_window_size_memory_exhaustion() {
    // Setup: Create consensus config with excessive window_size
    let mut config = OnChainConsensusConfig::V5 {
        alg: ConsensusAlgorithmConfig::default_for_genesis(),
        vtxn: ValidatorTxnConfig::default_for_genesis(),
        window_size: Some(10_000_000), // Excessive value
        rand_check_enabled: true,
    };
    
    // Serialize and deserialize (no validation occurs)
    let serialized = bcs::to_bytes(&config).unwrap();
    let deserialized: OnChainConsensusConfig = bcs::from_bytes(&serialized).unwrap();
    
    // Verify excessive window_size is preserved
    assert_eq!(deserialized.window_size(), Some(10_000_000));
    
    // Simulate fast_forward_sync scenario
    let current_round = 1_000_000u64;
    let window_size = 10_000_000u64;
    
    // Calculate target_round as the code does
    let target_round = ((current_round + 1).saturating_sub(window_size)).max(1);
    let num_blocks = current_round - target_round + 1;
    
    // This would attempt to allocate for 1,000,000 blocks
    assert_eq!(num_blocks, 1_000_000);
    println!("Would attempt to allocate memory for {} blocks", num_blocks);
    
    // In production, this leads to memory exhaustion
    // let mut blocks: Vec<Block> = Vec::with_capacity(num_blocks as usize); // OOM
}
```

**Notes:**
- The vulnerability stems from missing input validation on governance-controlled parameters
- While governance is a trusted role, configuration errors during upgrades are realistic
- The issue violates the **Resource Limits** invariant requiring operations to respect memory constraints
- Impact is mitigated by batch retrieval and peer block availability limitations
- Nevertheless, the lack of bounds checking represents a significant robustness issue during critical operations like epoch transitions

### Citations

**File:** types/src/on_chain_config/consensus_config.rs (L199-213)
```rust
    V4 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
        // Execution pool block window
        window_size: Option<u64>,
    },
    V5 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
        // Execution pool block window
        window_size: Option<u64>,
        // Whether to check if we can skip generating randomness for blocks
        rand_check_enabled: bool,
    },
}
```

**File:** types/src/on_chain_config/consensus_config.rs (L464-468)
```rust
    fn deserialize_into_config(bytes: &[u8]) -> Result<Self> {
        let raw_bytes: Vec<u8> = bcs::from_bytes(bytes)?;
        bcs::from_bytes(&raw_bytes)
            .map_err(|e| format_err!("[on-chain config] Failed to deserialize into config: {}", e))
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L349-362)
```rust
            Some(window_size) => {
                let target_round = calculate_window_start_round(
                    highest_commit_cert.ledger_info().ledger_info().round(),
                    window_size,
                )
                .max(1); // Never retrieve genesis block
                let num_blocks = highest_quorum_cert.certified_block().round() - target_round + 1;
                info!(
                    "[FastForwardSync] with window_size: {}, target_round: {}, num_blocks: {}",
                    window_size, target_round, num_blocks
                );
                (TargetBlockRetrieval::TargetRound(target_round), num_blocks)
            },
        }
```

**File:** consensus/src/block_storage/sync_manager.rs (L390-391)
```rust
        // although unlikely, we might wrap num_blocks around on a 32-bit machine
        assert!(num_blocks < usize::MAX as u64);
```

**File:** consensus/src/block_storage/sync_manager.rs (L812-842)
```rust
        let mut progress = 0;
        let mut last_block_id = block_id;
        let mut result_blocks: Vec<Block> = vec![];
        let mut retrieve_batch_size = self.max_blocks_to_request;
        if peers.is_empty() {
            bail!("Failed to fetch block {}: no peers available", block_id);
        }
        while progress < num_blocks {
            // in case this is the last retrieval
            retrieve_batch_size = min(retrieve_batch_size, num_blocks - progress);

            info!(
                "Retrieving chunk: {} blocks starting from {}, original start {}",
                retrieve_batch_size, last_block_id, block_id
            );

            let response = self
                .retrieve_block_chunk(
                    last_block_id,
                    target_block_retrieval_payload,
                    retrieve_batch_size,
                    peers.clone(),
                )
                .await;
            match response {
                Ok(result) if matches!(result.status(), BlockRetrievalStatus::Succeeded) => {
                    // extend the result blocks
                    let batch = result.blocks().clone();
                    progress += batch.len() as u64;
                    last_block_id = batch.last().expect("Batch should not be empty").parent_id();
                    result_blocks.extend(batch);
```
