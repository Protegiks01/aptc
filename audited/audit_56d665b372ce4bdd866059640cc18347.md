# Audit Report

## Title
State Restore Completes Successfully with Empty State Despite Non-Empty Expected Root Hash

## Summary
When state restoration completes with zero chunks successfully added, the system writes an empty state tree (`Node::Null` with `SPARSE_MERKLE_PLACEHOLDER_HASH`) without validating it matches the `expected_root_hash`. This allows a node to finalize an incorrect empty state while the ledger metadata records a non-empty state root hash, causing consensus divergence and state inconsistency.

## Finding Description

The vulnerability exists in two related components:

**1. StateValueRestore::finish() - No validation on empty completion:** [1](#0-0) 

When `get_progress()` returns `None` (no chunks were successfully added), the function defaults to `StateStorageUsage::zero()` and completes successfully without any validation that this is the expected outcome.

**2. JellyfishMerkleRestore::finish_impl() - Writes Node::Null without root hash validation:** [2](#0-1) 

When no children exist (`num_children == 0`), the function writes `Node::Null` to storage and returns successfully. Critically, it **never validates** that the `expected_root_hash` matches `SPARSE_MERKLE_PLACEHOLDER_HASH` (the hash of an empty tree).

**The Attack Path:**

1. State sync initializes a restore to version V with `expected_root_hash = H` (a non-empty state root) [3](#0-2) 

2. Due to network failures, malicious peers, or bugs, zero state chunks are successfully added (or all chunks are skipped as overlapping with non-existent previous progress)

3. The state sync driver receives `is_last_chunk() == true` and calls `finalize_storage_and_send_commit` [4](#0-3) 

4. `StateValueRestore::finish()` completes with zero usage (no validation)

5. `JellyfishMerkleRestore::finish_impl()` writes `Node::Null` with hash `SPARSE_MERKLE_PLACEHOLDER_HASH` (no validation that this matches expected_root_hash = H)

6. `finalize_state_snapshot()` writes the transaction info containing `state_checkpoint_hash = H` to the ledger [5](#0-4) 

7. The node believes it has successfully restored state to version V with root hash H

8. **But the actual state tree root is `SPARSE_MERKLE_PLACEHOLDER_HASH` â‰  H**

**Broken Invariants:**
- **State Consistency**: The ledger metadata claims state_checkpoint_hash = H, but the actual state merkle tree root is `SPARSE_MERKLE_PLACEHOLDER_HASH`
- **Deterministic Execution**: Nodes with correctly restored state will have different execution results than nodes with empty state
- **Consensus Safety**: Different nodes will produce different state roots when executing the same transactions

## Impact Explanation

**Critical Severity** - This vulnerability causes consensus divergence and state inconsistency:

1. **Consensus Safety Violation**: Nodes with empty state vs. nodes with correct state will produce different state roots for identical blocks, violating the fundamental safety property of AptosBFT consensus

2. **State Inconsistency**: The system has divergent views of state - the ledger claims one root hash, storage contains another

3. **Transaction Execution Failures**: Any transaction attempting to read state will either fail or produce incorrect results on affected nodes

4. **Network Partition**: Affected nodes cannot participate in consensus as they will reject blocks from honest nodes (or vice versa)

5. **Requires Manual Intervention**: Recovery requires wiping state and re-syncing from genesis or a known-good snapshot

This meets the **Critical Severity** criteria: "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)".

## Likelihood Explanation

**Medium-High Likelihood:**

This vulnerability can be triggered through several realistic scenarios:

1. **Network/Peer Failures**: During state sync, if the peer connection drops after sending the "last chunk" signal but before all chunks are received, the restore completes with empty state

2. **Malicious Peer**: A Byzantine peer could send a state chunk marked as `is_last_chunk()` with zero or invalid data that gets silently skipped

3. **Race Conditions**: In concurrent state sync scenarios, chunks might be marked as "already processed" due to timing issues, resulting in zero actual additions

4. **Proof Validation Failures**: If chunk proofs fail validation but errors are not properly propagated, chunks get skipped silently

The vulnerability requires no special privileges - any peer participating in state sync can potentially trigger this condition. The lack of validation in the finish path makes exploitation straightforward once the condition is achieved.

## Recommendation

Add explicit validation in both `StateValueRestore::finish()` and `JellyfishMerkleRestore::finish_impl()` to ensure the restored state matches expectations:

**Fix for StateValueRestore::finish():**
```rust
pub fn finish(self) -> Result<()> {
    let progress = self.db.get_progress(self.version)?;
    let usage = progress.map_or(StateStorageUsage::zero(), |p| p.usage);
    
    // Validate that if usage is zero, we expected an empty state
    // This should be coordinated with the tree restore validation
    
    self.db.kv_finish(self.version, usage)
}
```

**Fix for JellyfishMerkleRestore::finish_impl():**
```rust
pub fn finish_impl(mut self) -> Result<()> {
    self.wait_for_async_commit()?;
    
    if self.partial_nodes.len() == 1 {
        let mut num_children = 0;
        let mut leaf = None;
        for i in 0..16 {
            if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                num_children += 1;
                if let ChildInfo::Leaf(node) = child_info {
                    leaf = Some(node.clone());
                }
            }
        }

        match num_children {
            0 => {
                // CRITICAL FIX: Validate that expected_root_hash matches empty tree
                ensure!(
                    self.expected_root_hash == *SPARSE_MERKLE_PLACEHOLDER_HASH,
                    "Expected non-empty state (root hash: {}), but no data was added. \
                     This indicates a failed restore.",
                    self.expected_root_hash
                );
                
                let node_key = NodeKey::new_empty_path(self.version);
                assert!(self.frozen_nodes.is_empty());
                self.frozen_nodes.insert(node_key, Node::Null);
                self.store.write_node_batch(&self.frozen_nodes)?;
                return Ok(());
            },
            // ... rest of the function
        }
    }
    
    self.freeze(0);
    self.store.write_node_batch(&self.frozen_nodes)?;
    Ok(())
}
```

## Proof of Concept

```rust
#[test]
fn test_empty_restore_with_non_empty_expected_root() {
    use aptos_crypto::{hash::CryptoHash, HashValue};
    use aptos_jellyfish_merkle::JellyfishMerkleTree;
    
    // Setup: Create a non-empty tree to get a valid root hash
    let source_db = Arc::new(MockSnapshotStore::default());
    let mut source_kvs = BTreeMap::new();
    source_kvs.insert(
        HashValue::random(),
        (ValueBlob(vec![1, 2, 3]), ValueBlob(vec![4, 5, 6]))
    );
    
    // Initialize source with some data
    for (i, (k, v)) in source_kvs.iter().enumerate() {
        source_db.kv_store.write().insert(
            (k.clone(), i as Version),
            v.clone()
        );
    }
    
    let source_tree = JellyfishMerkleTree::new(&source_db);
    let expected_root_hash = source_tree.get_root_hash(0).unwrap();
    
    // This is a NON-EMPTY root hash
    assert_ne!(expected_root_hash, *SPARSE_MERKLE_PLACEHOLDER_HASH);
    
    // Now attempt a restore with this expected_root_hash but add ZERO chunks
    let restore_db = Arc::new(MockSnapshotStore::default());
    let version = 100;
    
    let restore = StateSnapshotRestore::new(
        &restore_db,
        &restore_db,
        version,
        expected_root_hash,  // Expecting non-empty state!
        false,
        StateSnapshotRestoreMode::Default,
    ).unwrap();
    
    // Add zero chunks (simulate network failure or malicious peer)
    // Just call finish() directly
    
    // BUG: This succeeds when it should fail!
    let result = restore.finish();
    assert!(result.is_ok(), "Restore succeeded with zero chunks!");
    
    // Verify the vulnerability: tree root is empty but ledger expects non-empty
    let actual_root = JellyfishMerkleTree::new(&restore_db)
        .get_root_hash(version)
        .unwrap();
    
    // VULNERABILITY DEMONSTRATED:
    assert_eq!(actual_root, *SPARSE_MERKLE_PLACEHOLDER_HASH);
    assert_ne!(actual_root, expected_root_hash);
    
    println!("VULNERABILITY: Expected root hash {} but got empty tree hash {}",
             expected_root_hash, actual_root);
}
```

This test demonstrates that a state restore can complete successfully even when the restored state (empty tree) doesn't match the expected state (non-empty tree), leading to state inconsistency and potential consensus divergence.

### Citations

**File:** storage/aptosdb/src/state_restore/mod.rs (L129-135)
```rust
    pub fn finish(self) -> Result<()> {
        let progress = self.db.get_progress(self.version)?;
        self.db.kv_finish(
            self.version,
            progress.map_or(StateStorageUsage::zero(), |p| p.usage),
        )
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L845-860)
```rust
        // Get the target version and expected root hash
        let version = target_ledger_info.ledger_info().version();
        let expected_root_hash = target_output_with_proof
            .get_output_list_with_proof()
            .proof
            .transaction_infos
            .first()
            .expect("Target transaction info should exist!")
            .ensure_state_checkpoint_hash()
            .expect("Must be at state checkpoint.");

        // Create the snapshot receiver
        let mut state_snapshot_receiver = storage
            .writer
            .get_state_snapshot_receiver(version, expected_root_hash)
            .expect("Failed to initialize the state snapshot receiver!");
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L874-954)
```rust
                    let all_states_synced = states_with_proof.is_last_chunk();
                    let last_committed_state_index = states_with_proof.last_index;
                    let num_state_values = states_with_proof.raw_values.len();

                    let result = state_snapshot_receiver.add_chunk(
                        states_with_proof.raw_values,
                        states_with_proof.proof.clone(),
                    );

                    // Handle the commit result
                    match result {
                        Ok(()) => {
                            // Update the logs and metrics
                            info!(
                                LogSchema::new(LogEntry::StorageSynchronizer).message(&format!(
                                    "Committed a new state value chunk! Chunk size: {:?}, last persisted index: {:?}",
                                    num_state_values,
                                    last_committed_state_index
                                ))
                            );

                            // Update the chunk metrics
                            let operation_label =
                                metrics::StorageSynchronizerOperations::SyncedStates.get_label();
                            metrics::set_gauge(
                                &metrics::STORAGE_SYNCHRONIZER_OPERATIONS,
                                operation_label,
                                last_committed_state_index,
                            );
                            metrics::observe_value(
                                &metrics::STORAGE_SYNCHRONIZER_CHUNK_SIZES,
                                operation_label,
                                num_state_values as u64,
                            );

                            if !all_states_synced {
                                // Update the metadata storage with the last committed state index
                                if let Err(error) = metadata_storage
                                    .clone()
                                    .update_last_persisted_state_value_index(
                                        &target_ledger_info,
                                        last_committed_state_index,
                                        all_states_synced,
                                    )
                                {
                                    let error = format!("Failed to update the last persisted state index at version: {:?}! Error: {:?}", version, error);
                                    send_storage_synchronizer_error(
                                        error_notification_sender.clone(),
                                        notification_id,
                                        error,
                                    )
                                    .await;
                                }
                                decrement_pending_data_chunks(pending_data_chunks.clone());
                                continue; // Wait for the next chunk
                            }

                            // Finalize storage and send a commit notification
                            if let Err(error) = finalize_storage_and_send_commit(
                                chunk_executor,
                                &mut commit_notification_sender,
                                metadata_storage,
                                state_snapshot_receiver,
                                storage,
                                &epoch_change_proofs,
                                target_output_with_proof,
                                version,
                                &target_ledger_info,
                                last_committed_state_index,
                            )
                            .await
                            {
                                send_storage_synchronizer_error(
                                    error_notification_sender.clone(),
                                    notification_id,
                                    error,
                                )
                                .await;
                            }
                            decrement_pending_data_chunks(pending_data_chunks.clone());
                            return; // There's nothing left to do!
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-241)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
            restore_utils::save_ledger_infos(
                self.ledger_db.metadata_db(),
                ledger_infos,
                Some(&mut ledger_db_batch.ledger_metadata_db_batches),
            )?;

            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::LedgerCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::OverallCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;

            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;

            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;

            restore_utils::update_latest_ledger_info(self.ledger_db.metadata_db(), ledger_infos)?;
            self.state_store.reset();

            Ok(())
        })
    }
```
