# Audit Report

## Title
Blockchain Reorganization Causes Permanent Stale Data in Delegated Staking Activities Table

## Summary
The indexer's `delegated_staking_activities` table uses `on_conflict((transaction_version, event_index)).do_nothing()` when inserting staking events. After a blockchain reorganization (reorg), if a transaction version is reprocessed with different events, the old stale events remain in the database while the new correct events are silently discarded, causing permanent data inconsistency in critical staking activity records. [1](#0-0) 

## Finding Description

The vulnerability occurs in the `insert_delegator_activities` function where delegated staking events are persisted to PostgreSQL. The function uses Diesel's conflict resolution strategy with `do_nothing()`: [2](#0-1) 

The `delegated_staking_activities` table has a compound primary key of `(transaction_version, event_index)`: [3](#0-2) [4](#0-3) 

**Attack Scenario:**

1. **Initial State**: Transaction at version 1000 contains staking events:
   - Event 0: AddStakeEvent (Alice, 100 APT, Pool A)
   - Event 1: UnlockStakeEvent (Bob, 50 APT, Pool B)

2. **Indexer Processing**: These events are inserted into `delegated_staking_activities` with keys (1000, 0) and (1000, 1).

3. **Blockchain Reorg Occurs**: Due to consensus operations, the chain reorganizes. Transaction version 1000 now contains completely different events:
   - Event 0: WithdrawStakeEvent (Charlie, 75 APT, Pool C)
   - Event 1: ReactivateStakeEvent (Dave, 25 APT, Pool D)

4. **Reprocessing**: The indexer detects a gap or restarts from an earlier version and reprocesses version 1000.

5. **Conflict Resolution Failure**: When inserting the new events:
   - Try to insert WithdrawStakeEvent at (1000, 0) → Conflict with existing AddStakeEvent → `do_nothing()` → **Old data retained**
   - Try to insert ReactivateStakeEvent at (1000, 1) → Conflict with existing UnlockStakeEvent → `do_nothing()` → **Old data retained**

6. **Result**: The database permanently contains incorrect staking activity records that don't match the actual blockchain state.

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable." The indexed data no longer reflects the true blockchain state and cannot be corrected without manual database intervention. [5](#0-4) 

The indexer has no mechanism to detect or correct this inconsistency. The `check_or_update_chain_id()` function only prevents indexing a completely different chain, not same-chain reorgs: [6](#0-5) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Significant Protocol Violations**: The indexer provides critical infrastructure for tracking staking activities. Incorrect data undermines trust in the system and breaks data integrity guarantees.

2. **State Inconsistencies Requiring Intervention**: The stale data is permanent and cannot be automatically corrected. It requires manual database cleanup and reindexing from a clean state.

3. **Affects Critical Staking Data**: The `delegated_staking_activities` table records:
   - AddStakeEvent: When users stake coins
   - UnlockStakeEvent: When users begin unlock process
   - WithdrawStakeEvent: When users withdraw staked coins
   - ReactivateStakeEvent: When users reactivate unlocking stakes
   - DistributeRewardsEvent: When rewards are distributed

   Incorrect records could lead to:
   - Users seeing wrong staking history in explorers/UIs
   - Accounting errors in applications built on this data
   - Compliance and auditing issues for validators

4. **No Automatic Recovery**: Unlike other tables that use `do_update()` with version checking (like `current_staking_pool_voter`), these activity records have no correction mechanism: [7](#0-6) 

While this doesn't directly cause "Loss of Funds" (Critical severity), it represents a significant state inconsistency that requires manual intervention and could lead to incorrect financial reporting or user confusion.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability will occur whenever:

1. **Blockchain Reorg Happens**: While Aptos uses AptosBFT which has strong finality guarantees, reorgs can still occur:
   - During network partitions or synchronization issues
   - In the brief window before finality (though this is typically very short)
   - During testing or development environments
   - During chain upgrades or maintenance

2. **Indexer Reprocesses Transactions**: The indexer may reprocess versions when:
   - Restarting after a crash and detecting gaps
   - Manually configured with a `starting_version` to reindex historical data
   - Catching up after being offline [8](#0-7) 

3. **No Detection Mechanism**: The system has no way to detect when reprocessed data differs from existing data. The `do_nothing()` strategy silently discards new data without logging or alerting.

The bug is **deterministic** - it will definitely occur if the conditions are met. Given that blockchain reorgs, while rare in production Aptos, are not impossible, and that indexer restarts are common operational events, this vulnerability has a meaningful likelihood of occurring.

## Recommendation

Replace `do_nothing()` with `do_update()` and add version checking or use a strategy that allows re-insertion of historical data:

**Option 1: Use do_update() with SET to always use latest data**
```rust
fn insert_delegator_activities(
    conn: &mut PgConnection,
    item_to_insert: &[DelegatedStakingActivity],
) -> Result<(), diesel::result::Error> {
    use schema::delegated_staking_activities::dsl::*;

    let chunks = get_chunks(
        item_to_insert.len(),
        DelegatedStakingActivity::field_count(),
    );
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::delegated_staking_activities::table)
                .values(&item_to_insert[start_ind..end_ind])
                .on_conflict((transaction_version, event_index))
                .do_update()
                .set((
                    delegator_address.eq(excluded(delegator_address)),
                    pool_address.eq(excluded(pool_address)),
                    event_type.eq(excluded(event_type)),
                    amount.eq(excluded(amount)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            None,
        )?;
    }
    Ok(())
}
```

**Option 2: Delete and reinsert (more explicit)**
Before reprocessing a version range, explicitly delete existing records:
```sql
DELETE FROM delegated_staking_activities 
WHERE transaction_version >= ? AND transaction_version <= ?
```

**Option 3: Add a processing timestamp/hash to detect stale data**
Modify the schema to include a `chain_state_hash` or `reorg_indicator` field that would help detect when data needs updating.

The same issue exists in other tables using `do_nothing()`:
- `proposal_votes` (line 174-175)
- `delegated_staking_pool_balances` (line 271-272)

These should be reviewed and fixed similarly. [9](#0-8) [10](#0-9) 

## Proof of Concept

**Reproduction Steps:**

1. **Setup**: Run an Aptos indexer connected to a fullnode with PostgreSQL backend.

2. **Initial Indexing**: Process a transaction containing staking events:
```sql
    -- Verify initial state
SELECT * FROM delegated_staking_activities WHERE transaction_version = 1000;
    -- Should show: AddStakeEvent, event_index=0, amount=100
```

3. **Simulate Reorg**: 
   - In a test environment, create a blockchain state where version 1000 has different events
   - This can be simulated by:
     a. Stopping the indexer
     b. Manually inserting different event data for version 1000 in the source node
     c. Restarting the indexer with a `starting_version` before 1000

4. **Reprocess**: The indexer attempts to reindex version 1000 with new events.

5. **Verify Bug**:
```sql
    -- Check database - old events remain despite reprocessing
SELECT transaction_version, event_index, event_type, amount, delegator_address
FROM delegated_staking_activities 
WHERE transaction_version = 1000;
    -- Will still show old AddStakeEvent instead of new WithdrawStakeEvent
```

6. **Compare with Blockchain State**: Query the actual blockchain via API:
```bash
curl http://fullnode/v1/transactions/by_version/1000
```
The events in the API response will differ from the database, confirming the inconsistency.

**SQL Test Case:**
```sql
-- Initial insert
INSERT INTO delegated_staking_activities 
(transaction_version, event_index, delegator_address, pool_address, event_type, amount)
VALUES (1000, 0, '0xALICE', '0xPOOLA', 'AddStakeEvent', 100);

-- Simulate reprocessing with different data
INSERT INTO delegated_staking_activities 
(transaction_version, event_index, delegator_address, pool_address, event_type, amount)
VALUES (1000, 0, '0xCHARLIE', '0xPOOLC', 'WithdrawStakeEvent', 75)
ON CONFLICT (transaction_version, event_index) DO NOTHING;

-- Verify bug: old data remains
SELECT * FROM delegated_staking_activities WHERE transaction_version = 1000;
-- Result: Still shows AddStakeEvent with Alice and 100, not WithdrawStakeEvent with Charlie and 75
```

This demonstrates that the `do_nothing()` strategy prevents correction of stale data after blockchain reorganizations.

### Citations

**File:** crates/indexer/src/processors/stake_processor.rs (L133-160)
```rust
fn insert_current_stake_pool_voter(
    conn: &mut PgConnection,
    item_to_insert: &[CurrentStakingPoolVoter],
) -> Result<(), diesel::result::Error> {
    use schema::current_staking_pool_voter::dsl::*;

    let chunks = get_chunks(item_to_insert.len(), CurrentStakingPoolVoter::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_staking_pool_voter::table)
                .values(&item_to_insert[start_ind..end_ind])
                .on_conflict(staking_pool_address)
                .do_update()
                .set((
                    staking_pool_address.eq(excluded(staking_pool_address)),
                    voter_address.eq(excluded(voter_address)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    inserted_at.eq(excluded(inserted_at)),
                    operator_address.eq(excluded(operator_address)),
                )),
            Some(
                " WHERE current_staking_pool_voter.last_transaction_version <= EXCLUDED.last_transaction_version ",
            ),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/stake_processor.rs (L162-180)
```rust
fn insert_proposal_votes(
    conn: &mut PgConnection,
    item_to_insert: &[ProposalVote],
) -> Result<(), diesel::result::Error> {
    use schema::proposal_votes::dsl::*;

    let chunks = get_chunks(item_to_insert.len(), ProposalVote::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::proposal_votes::table)
                .values(&item_to_insert[start_ind..end_ind])
                .on_conflict((transaction_version, proposal_id, voter_address))
                .do_nothing(),
            None,
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/stake_processor.rs (L182-203)
```rust
fn insert_delegator_activities(
    conn: &mut PgConnection,
    item_to_insert: &[DelegatedStakingActivity],
) -> Result<(), diesel::result::Error> {
    use schema::delegated_staking_activities::dsl::*;

    let chunks = get_chunks(
        item_to_insert.len(),
        DelegatedStakingActivity::field_count(),
    );
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::delegated_staking_activities::table)
                .values(&item_to_insert[start_ind..end_ind])
                .on_conflict((transaction_version, event_index))
                .do_nothing(),
            None,
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/stake_processor.rs (L259-277)
```rust
fn insert_delegator_pool_balances(
    conn: &mut PgConnection,
    item_to_insert: &[DelegatorPoolBalance],
) -> Result<(), diesel::result::Error> {
    use schema::delegated_staking_pool_balances::dsl::*;

    let chunks = get_chunks(item_to_insert.len(), DelegatorPoolBalance::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::delegated_staking_pool_balances::table)
                .values(&item_to_insert[start_ind..end_ind])
                .on_conflict((transaction_version, staking_pool_address))
                .do_nothing(),
            None,
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/migrations/2023-02-15-070116_stake_delegation/up.sql (L3-13)
```sql
CREATE TABLE delegated_staking_activities (
  transaction_version BIGINT NOT NULL,
  event_index BIGINT NOT NULL,
  delegator_address VARCHAR(66) NOT NULL,
  pool_address VARCHAR(66) NOT NULL,
  event_type text NOT NULL,
  amount NUMERIC NOT NULL,
  inserted_at TIMESTAMP NOT NULL DEFAULT NOW(),
  -- Constraints
  PRIMARY KEY (transaction_version, event_index)
);
```

**File:** crates/indexer/src/models/stake_models/delegator_activities.rs (L17-27)
```rust
#[derive(Debug, Deserialize, FieldCount, Identifiable, Insertable, Serialize)]
#[diesel(primary_key(transaction_version, event_index))]
#[diesel(table_name = delegated_staking_activities)]
pub struct DelegatedStakingActivity {
    pub transaction_version: i64,
    pub event_index: i64,
    pub delegator_address: String,
    pub pool_address: String,
    pub event_type: String,
    pub amount: BigDecimal,
}
```

**File:** crates/indexer/src/models/stake_models/delegator_activities.rs (L29-93)
```rust
impl DelegatedStakingActivity {
    /// Pretty straightforward parsing from known delegated staking events
    pub fn from_transaction(transaction: &APITransaction) -> anyhow::Result<Vec<Self>> {
        let mut delegator_activities = vec![];
        let (txn_version, events) = match transaction {
            APITransaction::UserTransaction(txn) => (txn.info.version.0 as i64, &txn.events),
            APITransaction::BlockMetadataTransaction(txn) => {
                (txn.info.version.0 as i64, &txn.events)
            },
            _ => return Ok(delegator_activities),
        };
        for (index, event) in events.iter().enumerate() {
            let event_type = event.typ.to_string();
            let event_index = index as i64;
            if let Some(staking_event) =
                StakeEvent::from_event(event_type.as_str(), &event.data, txn_version)?
            {
                let activity = match staking_event {
                    StakeEvent::AddStakeEvent(inner) => DelegatedStakingActivity {
                        transaction_version: txn_version,
                        event_index,
                        delegator_address: standardize_address(&inner.delegator_address),
                        pool_address: standardize_address(&inner.pool_address),
                        event_type: event_type.clone(),
                        amount: u64_to_bigdecimal(inner.amount_added),
                    },
                    StakeEvent::UnlockStakeEvent(inner) => DelegatedStakingActivity {
                        transaction_version: txn_version,
                        event_index,
                        delegator_address: standardize_address(&inner.delegator_address),
                        pool_address: standardize_address(&inner.pool_address),
                        event_type: event_type.clone(),
                        amount: u64_to_bigdecimal(inner.amount_unlocked),
                    },
                    StakeEvent::WithdrawStakeEvent(inner) => DelegatedStakingActivity {
                        transaction_version: txn_version,
                        event_index,
                        delegator_address: standardize_address(&inner.delegator_address),
                        pool_address: standardize_address(&inner.pool_address),
                        event_type: event_type.clone(),
                        amount: u64_to_bigdecimal(inner.amount_withdrawn),
                    },
                    StakeEvent::ReactivateStakeEvent(inner) => DelegatedStakingActivity {
                        transaction_version: txn_version,
                        event_index,
                        delegator_address: standardize_address(&inner.delegator_address),
                        pool_address: standardize_address(&inner.pool_address),
                        event_type: event_type.clone(),
                        amount: u64_to_bigdecimal(inner.amount_reactivated),
                    },
                    StakeEvent::DistributeRewardsEvent(inner) => DelegatedStakingActivity {
                        transaction_version: txn_version,
                        event_index,
                        delegator_address: "".to_string(),
                        pool_address: standardize_address(&inner.pool_address),
                        event_type: event_type.clone(),
                        amount: u64_to_bigdecimal(inner.rewards_amount),
                    },
                    _ => continue,
                };
                delegator_activities.push(activity);
            }
        }
        Ok(delegator_activities)
    }
```

**File:** crates/indexer/src/indexer/tailer.rs (L65-109)
```rust
    /// If chain id doesn't exist, save it. Otherwise, make sure that we're indexing the same chain
    pub async fn check_or_update_chain_id(&self) -> Result<u64> {
        info!(
            processor_name = self.processor.name(),
            "Checking if chain id is correct"
        );
        let mut conn = self.connection_pool.get()?;

        let maybe_existing_chain_id = LedgerInfo::get(&mut conn)?.map(|li| li.chain_id);

        let new_chain_id = self
            .transaction_fetcher
            .lock()
            .await
            .fetch_ledger_info()
            .chain_id as i64;

        match maybe_existing_chain_id {
            Some(chain_id) => {
                ensure!(chain_id == new_chain_id, "Wrong chain detected! Trying to index chain {} now but existing data is for chain {}", new_chain_id, chain_id);
                info!(
                    processor_name = self.processor.name(),
                    chain_id = chain_id,
                    "Chain id matches! Continue to index...",
                );
                Ok(chain_id as u64)
            },
            None => {
                info!(
                    processor_name = self.processor.name(),
                    chain_id = new_chain_id,
                    "Adding chain id to db, continue to index.."
                );
                execute_with_better_error(
                    &mut conn,
                    diesel::insert_into(ledger_infos::table).values(LedgerInfo {
                        chain_id: new_chain_id,
                    }),
                    None,
                )
                .context(r#"Error updating chain_id!"#)
                .map(|_| new_chain_id as u64)
            },
        }
    }
```

**File:** crates/indexer/src/runtime.rs (L173-185)
```rust
    let start_version = match config.starting_version {
        None => starting_version_from_db_short,
        Some(version) => version,
    };

    info!(
        processor_name = processor_name,
        final_start_version = start_version,
        start_version_from_config = config.starting_version,
        starting_version_from_db = starting_version_from_db_short,
        "Setting starting version..."
    );
    tailer.set_fetcher_version(start_version).await;
```
