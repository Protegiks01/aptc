# Audit Report

## Title
Asynchronous Aggregation Error Swallowing Causes State Inconsistency and Potential Consensus Halt

## Summary
The randomness generation system contains a state machine race condition where `RandItem` transitions to `Decided` state before cryptographic aggregation completes in a background task. If aggregation or channel communication fails, no randomness is produced but the state remains `Decided`, causing blocks to permanently stall in the queue and requiring manual intervention to recover, as documented in the codebase's own recovery procedures.

## Finding Description
The vulnerability exists in the asynchronous aggregation flow within the randomness generation consensus module.

**State Machine Race Condition:**

When the threshold weight is met, `ShareAggregator::try_aggregate()` spawns a background task for cryptographic aggregation but immediately returns `Either::Right(self_share)` before the task completes. [1](#0-0) 

This immediate return causes `RandItem::try_aggregate()` to synchronously transition the item from `PendingDecision` to `Decided` state before the background aggregation work finishes. [2](#0-1) 

**Error Swallowing in Background Task:**

The background task performs the actual cryptographic aggregation by calling `S::aggregate()`. If this fails, the error is only logged with `warn!` and never propagated to any recovery mechanism. [3](#0-2) 

Additionally, successful aggregation results are sent via `decision_tx.unbounded_send()`, but the send error is silently ignored with `let _ =`, meaning channel disconnection causes silent randomness loss. [4](#0-3) 

**Permanent Stuck State:**

Once in `Decided` state, the `RandItem` silently ignores all future incoming shares for that round. [5](#0-4) 

The block queue only releases blocks when all blocks have received randomness (`num_undecided() == 0`). [6](#0-5) 

If randomness is never produced due to aggregation failure or channel error, blocks remain stuck indefinitely, blocking all subsequent blocks in the ordered queue.

**Manual Recovery Required:**

The codebase includes explicit recovery mechanisms for randomness stall scenarios. The recovery procedure requires all validators to restart with modified configuration files setting `randomness_override_seq_num`, followed by a governance proposal to re-enable randomness. [7](#0-6) 

A smoke test explicitly validates that randomness stall causes chain halt and requires this manual recovery procedure. [8](#0-7) 

## Impact Explanation
This constitutes **Medium Severity** per Aptos bug bounty criteria: "State inconsistencies requiring manual intervention."

The vulnerability breaks the State Consistency invariant by allowing state transitions to commit before the associated work completes. When triggered, the network enters an unrecoverable state where:
1. Blocks are permanently stuck waiting for randomness that will never arrive
2. Consensus cannot progress past the stuck blocks
3. Manual intervention is required (validator restart + governance proposal)

The existence of dedicated recovery infrastructure (`randomness_config_seqnum` module and recovery test) confirms that Aptos developers recognize randomness stall as a real failure mode requiring manual intervention.

## Likelihood Explanation
**Low to Moderate Likelihood** - While cryptographic operations with valid shares are reliable, the architectural flaw creates multiple failure modes:

1. **Channel Closure/Disconnection**: The ignored `unbounded_send` error means any channel communication failure results in permanent randomness loss, even if aggregation succeeds.

2. **Background Task Panics**: The spawned blocking task has no panic handler. Any panic in the cryptographic operations results in no randomness being sent.

3. **BCS Serialization Failures**: The aggregation code serializes metadata and evaluation results. While unlikely with valid data, these operations can fail. [9](#0-8) 

The likelihood is elevated by the fact that this is a logic vulnerability in the state machine design itself, not dependent on specific malicious inputs. Any transient failure in the background task execution path becomes permanent due to the premature state transition.

## Recommendation
Implement atomic state transitions that only commit after background work completes:

1. **Defer State Transition**: Keep `RandItem` in `PendingDecision` state until aggregation actually completes and randomness is confirmed sent on the channel.

2. **Error Propagation**: Propagate aggregation errors back to the state machine to enable retry logic or explicit failure handling.

3. **Channel Send Verification**: Check the result of `decision_tx.unbounded_send()` and handle disconnection errors appropriately.

4. **Timeout Mechanism**: Add a timeout-based retry or recovery mechanism that doesn't require manual validator restart.

5. **Task Panic Handling**: Wrap the blocking task in proper error handling to catch panics and trigger recovery.

## Proof of Concept
The existing smoke test demonstrates the vulnerability impact. The test shows that when randomness generation stalls, the chain halts and requires the documented manual recovery procedure involving validator restart and governance action. [10](#0-9) 

The test validates:
- Chain halts when randomness stalls (liveness check fails)
- Recovery requires restarting all validators with `randomness_override_seq_num = 1`
- Governance proposal needed to set `RandomnessConfigSeqNum` to `2` to re-enable randomness
- This matches exactly the documented recovery procedure in the framework

## Notes
This vulnerability represents a fundamental architectural issue in the randomness generation state machine where commitment happens before work completion. The presence of dedicated recovery infrastructure in the codebase confirms this is a recognized failure mode. While the cryptographic operations themselves are secure, the asynchronous error handling design creates a gap where any failure in the background task execution results in permanent consensus stall requiring coordinated manual intervention across all validators.

### Citations

**File:** consensus/src/rand/rand_gen/rand_store.rs (L41-89)
```rust
    pub fn try_aggregate(
        self,
        rand_config: &RandConfig,
        rand_metadata: FullRandMetadata,
        decision_tx: Sender<Randomness>,
    ) -> Either<Self, RandShare<S>> {
        if self.total_weight < rand_config.threshold() {
            return Either::Left(self);
        }
        match self.path_type {
            PathType::Fast => {
                observe_block(
                    rand_metadata.timestamp,
                    BlockStage::RAND_ADD_ENOUGH_SHARE_FAST,
                );
            },
            PathType::Slow => {
                observe_block(
                    rand_metadata.timestamp,
                    BlockStage::RAND_ADD_ENOUGH_SHARE_SLOW,
                );
            },
        }

        let rand_config = rand_config.clone();
        let self_share = self
            .get_self_share()
            .expect("Aggregated item should have self share");
        tokio::task::spawn_blocking(move || {
            let maybe_randomness = S::aggregate(
                self.shares.values(),
                &rand_config,
                rand_metadata.metadata.clone(),
            );
            match maybe_randomness {
                Ok(randomness) => {
                    let _ = decision_tx.unbounded_send(randomness);
                },
                Err(e) => {
                    warn!(
                        epoch = rand_metadata.metadata.epoch,
                        round = rand_metadata.metadata.round,
                        "Aggregation error: {e}"
                    );
                },
            }
        });
        Either::Right(self_share)
    }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L158-158)
```rust
            RandItem::Decided { .. } => Ok(()),
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L162-178)
```rust
    fn try_aggregate(&mut self, rand_config: &RandConfig, decision_tx: Sender<Randomness>) {
        let item = std::mem::replace(self, Self::new(Author::ONE, PathType::Slow));
        let new_item = match item {
            RandItem::PendingDecision {
                share_aggregator,
                metadata,
            } => match share_aggregator.try_aggregate(rand_config, metadata.clone(), decision_tx) {
                Either::Left(share_aggregator) => Self::PendingDecision {
                    metadata,
                    share_aggregator,
                },
                Either::Right(self_share) => Self::Decided { self_share },
            },
            item @ (RandItem::Decided { .. } | RandItem::PendingMetadata(_)) => item,
        };
        let _ = std::mem::replace(self, new_item);
    }
```

**File:** consensus/src/rand/rand_gen/block_queue.rs (L118-136)
```rust
    pub fn dequeue_rand_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut rand_ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.num_undecided() == 0 {
                let (_, item) = self.queue.pop_first().unwrap();
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::RAND_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                debug_assert!(ordered_blocks
                    .ordered_blocks
                    .iter()
                    .all(|block| block.has_randomness()));
                rand_ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        rand_ready_prefix
```

**File:** aptos-move/framework/aptos-framework/sources/configs/randomness_config_seqnum.move (L1-9)
```text
/// Randomness stall recovery utils.
///
/// When randomness generation is stuck due to a bug, the chain is also stuck. Below is the recovery procedure.
/// 1. Ensure more than 2/3 stakes are stuck at the same version.
/// 1. Every validator restarts with `randomness_override_seq_num` set to `X+1` in the node config file,
///    where `X` is the current `RandomnessConfigSeqNum` on chain.
/// 1. The chain should then be unblocked.
/// 1. Once the bug is fixed and the binary + framework have been patched,
///    a governance proposal is needed to set `RandomnessConfigSeqNum` to be `X+2`.
```

**File:** testsuite/smoke-test/src/randomness/randomness_stall_recovery.rs (L19-165)
```rust
/// Chain recovery using a local config from randomness stall should work.
/// See `randomness_config_seqnum.move` for more details.
#[tokio::test]
async fn randomness_stall_recovery() {
    let epoch_duration_secs = 20;

    let (mut swarm, mut cli, _faucet) = SwarmBuilder::new_local(4)
        .with_num_fullnodes(0) //TODO: revert back to 1 after invalid version bug is fixed
        .with_aptos()
        .with_init_config(Arc::new(|_, conf, _| {
            conf.api.failpoints_enabled = true;
        }))
        .with_init_genesis_config(Arc::new(move |conf| {
            conf.epoch_duration_secs = epoch_duration_secs;

            // Ensure randomness is enabled.
            conf.consensus_config.enable_validator_txns();
            conf.randomness_config_override = Some(OnChainRandomnessConfig::default_enabled());
        }))
        .build_with_cli(0)
        .await;

    let root_addr = swarm.chain_info().root_account().address();
    let root_idx = cli.add_account_with_address_to_cli(swarm.root_key(), root_addr);

    let rest_client = swarm.validators().next().unwrap().rest_client();

    info!("Wait for epoch 2.");
    swarm
        .wait_for_all_nodes_to_catchup_to_epoch(2, Duration::from_secs(epoch_duration_secs * 2))
        .await
        .expect("Epoch 2 taking too long to arrive!");

    info!("Halting the chain by putting every validator into sync_only mode.");
    for validator in swarm.validators_mut() {
        enable_sync_only_mode(4, validator).await;
    }

    info!("Chain should have halted.");
    let liveness_check_result = swarm
        .liveness_check(Instant::now().add(Duration::from_secs(20)))
        .await;
    info!("liveness_check_result={:?}", liveness_check_result);
    assert!(liveness_check_result.is_err());

    info!("Hot-fixing all validators.");
    for (idx, validator) in swarm.validators_mut().enumerate() {
        info!("Stopping validator {}.", idx);
        validator.stop();
        let config_path = validator.config_path();
        let mut validator_override_config =
            OverrideNodeConfig::load_config(config_path.clone()).unwrap();
        validator_override_config
            .override_config_mut()
            .randomness_override_seq_num = 1;
        validator_override_config
            .override_config_mut()
            .consensus
            .sync_only = false;
        info!("Updating validator {} config.", idx);
        validator_override_config.save_config(config_path).unwrap();
        info!("Restarting validator {}.", idx);
        validator.start().unwrap();
        info!("Let validator {} bake for 5 secs.", idx);
        tokio::time::sleep(Duration::from_secs(5)).await;
    }

    info!("Hot-fixing the VFNs.");
    for (idx, vfn) in swarm.fullnodes_mut().enumerate() {
        info!("Stopping VFN {}.", idx);
        vfn.stop();
        let config_path = vfn.config_path();
        let mut vfn_override_config = OverrideNodeConfig::load_config(config_path.clone()).unwrap();
        vfn_override_config
            .override_config_mut()
            .randomness_override_seq_num = 1;
        info!("Updating VFN {} config.", idx);
        vfn_override_config.save_config(config_path).unwrap();
        info!("Restarting VFN {}.", idx);
        vfn.start().unwrap();
        info!("Let VFN {} bake for 5 secs.", idx);
        tokio::time::sleep(Duration::from_secs(5)).await;
    }

    let liveness_check_result = swarm
        .liveness_check(Instant::now().add(Duration::from_secs(30)))
        .await;
    assert!(liveness_check_result.is_ok());

    info!("There should be no randomness at the moment.");
    let block_randomness_seed = get_on_chain_resource::<PerBlockRandomness>(&rest_client).await;
    assert!(block_randomness_seed.seed.is_none());

    info!("Bump on-chain conig seqnum to re-enable randomness.");
    let script = r#"
script {
    use aptos_framework::aptos_governance;
    use aptos_framework::randomness_config_seqnum;

    fun main(core_resources: &signer) {
        let framework_signer = aptos_governance::get_signer_testnet_only(core_resources, @0x1);
        randomness_config_seqnum::set_for_next_epoch(&framework_signer, 2);
        aptos_governance::force_end_epoch(&framework_signer); // reconfigure() won't work at the moment.
    }
}
    "#;
    let gas_options = GasOptions {
        gas_unit_price: Some(1),
        max_gas: Some(2000000),
        expiration_secs: 60,
    };
    let txn_summary = cli
        .run_script_with_gas_options(root_idx, script, Some(gas_options))
        .await
        .expect("Txn execution error.");
    debug!("txn_summary={:?}", txn_summary);

    tokio::time::sleep(Duration::from_secs(10)).await;

    let epoch = rest_client
        .get_ledger_information()
        .await
        .unwrap()
        .into_inner()
        .epoch;
    info!(
        "Current epoch is {}. Wait until epoch {}, and randomness should be back.",
        epoch,
        epoch + 1
    );

    swarm
        .wait_for_all_nodes_to_catchup_to_epoch(
            epoch + 1,
            Duration::from_secs(epoch_duration_secs * 2),
        )
        .await
        .unwrap_or_else(|_| panic!("Epoch {} taking too long to arrive!", epoch + 1));

    let PerBlockRandomness {
        epoch: actual_epoch,
        ..
    } = get_on_chain_resource::<PerBlockRandomness>(&rest_client).await;
    // seed is not necessarily generated because of the rand check optimization.
    // but epoch and round should be updated.
    assert_eq!(epoch + 1, actual_epoch);
}
```

**File:** consensus/src/rand/rand_gen/types.rs (L131-145)
```rust
        let metadata_serialized = bcs::to_bytes(&rand_metadata).map_err(|e| {
            anyhow!("Share::aggregate failed with metadata serialization error: {e}")
        })?;
        let eval = WVUF::derive_eval(
            &rand_config.wconfig,
            &rand_config.vuf_pp,
            metadata_serialized.as_slice(),
            &rand_config.get_all_certified_apk(),
            &proof,
            THREAD_MANAGER.get_exe_cpu_pool(),
        )
        .map_err(|e| anyhow!("Share::aggregate failed with WVUF derive_eval error: {e}"))?;
        debug!("WVUF derivation time: {} ms", timer.elapsed().as_millis());
        let eval_bytes = bcs::to_bytes(&eval)
            .map_err(|e| anyhow!("Share::aggregate failed with eval serialization error: {e}"))?;
```
