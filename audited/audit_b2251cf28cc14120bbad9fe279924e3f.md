# Audit Report

## Title
Priority Inversion in Storage Service Network Request Processing Allows Low-Priority Peers to Starve High-Priority State Sync Operations

## Summary
The storage service server uses `select_all()` to merge network event streams from different NetworkIds (Validator, VFN, Public) without any priority-based scheduling. This creates a priority inversion vulnerability where valid requests from low-priority peers (e.g., Public network PFNs) can starve critical state sync operations from high-priority peers (e.g., Validator network), degrading consensus performance and validator liveness.

## Finding Description
The Aptos state sync system implements a comprehensive peer priority system that categorizes peers into HighPriority, MediumPriority, and LowPriority based on their NetworkId and node role. [1](#0-0) 

For validators, the priority mapping is:
- Validator network peers: HighPriority
- VFN network peers: MediumPriority  
- Public network peers: LowPriority [2](#0-1) 

However, the storage service server completely ignores this priority system when processing incoming network requests. In the network event stream initialization, all network streams are combined using `select_all()` without any priority handling: [3](#0-2) 

The `select_all()` function from the futures crate uses round-robin or first-available polling semantics, treating all input streams equally. This means requests from low-priority Public network peers receive the same processing attention as critical requests from high-priority Validator network peers.

The server then processes these requests in first-come-first-served order: [4](#0-3) 

**Attack Scenario:**

1. An attacker connects multiple malicious nodes to a validator/VFN on the Public network (low priority)
2. The attacker floods the storage service with valid but resource-intensive requests (e.g., large transaction chunk requests, state value requests with large ranges)
3. Due to `select_all()`'s round-robin behavior, these low-priority requests are interleaved equally with high-priority validator requests
4. Critical state sync operations from validators experience significant delays
5. Validators fall behind in state synchronization, degrading consensus performance

The RequestModerator only validates request correctness and blocks peers sending invalid requests - it provides no rate limiting or priority-based scheduling for valid requests: [5](#0-4) 

## Impact Explanation
This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Node Slowdowns**: High-priority validators experience delays in state synchronization when low-priority peers flood the system with valid requests. This directly impacts validator performance.

2. **Significant Protocol Violations**: The implementation violates the intended priority system design. The state sync system explicitly defines peer priorities, but the storage service server ignores them completely.

3. **Consensus Performance Degradation**: Validators that cannot sync state quickly may lag behind, impacting consensus efficiency and potentially causing timeout-related issues in the consensus protocol.

4. **Availability Impact**: In extreme cases where multiple validators are affected simultaneously, the network could experience degraded liveness if validators fall too far behind.

The vulnerability breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits" - in this case, the system fails to enforce proper resource allocation based on peer priority.

## Likelihood Explanation
The likelihood of this vulnerability being exploited is **HIGH** for the following reasons:

1. **Low Barrier to Entry**: Any actor can connect nodes to the Public network and send valid storage service requests. No special privileges or validator access is required.

2. **Simple Attack Vector**: The attack requires only sending valid but resource-intensive requests at high volume. No complex protocol manipulation or cryptographic operations are needed.

3. **No Effective Mitigation**: The RequestModerator only blocks peers after they send invalid requests. Valid requests from low-priority peers face no rate limiting or deprioritization.

4. **High Impact on Critical Operations**: State sync is a fundamental operation for validator consensus participation. Delays directly impact validator effectiveness.

5. **Scalable Attack**: An attacker can amplify the attack by connecting multiple malicious nodes, increasing the volume of low-priority requests.

## Recommendation
Implement priority-based scheduling for network request processing in the storage service server. Instead of using `select_all()` which treats all streams equally, implement a priority queue or weighted stream selection that respects the peer priority system.

**Recommended Fix Approach:**

1. **Priority-Based Stream Polling**: Replace `select_all()` with a custom stream combinator that polls high-priority streams (Validator network) more frequently than low-priority streams (Public network).

2. **Weighted Request Processing**: Implement a weighted round-robin or priority queue system where high-priority requests are guaranteed a larger share of processing capacity.

3. **Per-Priority-Class Rate Limiting**: Implement separate rate limits for each priority class (High/Medium/Low) to prevent low-priority requests from consuming all available capacity.

**Example Implementation Pattern:**

```rust
// Instead of select_all(), use a custom priority-aware stream combinator
pub fn new(network_service_events: NetworkServiceEvents<StorageServiceMessage>) -> Self {
    let network_events_by_priority: HashMap<PeerPriority, Vec<_>> = 
        network_service_events
            .into_network_and_events()
            .into_iter()
            .map(|(network_id, events)| {
                let priority = map_network_id_to_priority(network_id);
                (priority, events.map(move |event| (network_id, event)))
            })
            .into_group_map();
    
    // Create a priority-aware stream that polls high-priority streams
    // more frequently than low-priority streams
    let network_request_stream = PriorityStream::new(network_events_by_priority)
        .filter_map(|(network_id, event)| {
            future::ready(Self::event_to_request(network_id, event))
        })
        .boxed();
    
    Self { network_request_stream }
}
```

4. **Configuration Parameters**: Add configuration options to control the priority ratios (e.g., "process 10 high-priority requests for every 1 low-priority request").

## Proof of Concept

```rust
#[tokio::test]
async fn test_priority_inversion_vulnerability() {
    use aptos_config::network_id::{NetworkId, PeerNetworkId};
    use aptos_types::PeerId;
    use aptos_storage_service_types::requests::StorageServiceRequest;
    use std::time::{Duration, Instant};
    
    // Initialize mock storage service with all three networks
    let (mut mock_client, storage_server, _, _, _) = 
        MockClient::new(None, None);
    
    // Start the storage service server
    tokio::spawn(storage_server.start());
    
    // Create peers on different networks
    let validator_peer = PeerNetworkId::new(NetworkId::Validator, PeerId::random());
    let public_peer_1 = PeerNetworkId::new(NetworkId::Public, PeerId::random());
    let public_peer_2 = PeerNetworkId::new(NetworkId::Public, PeerId::random());
    
    // Flood with low-priority requests from Public network
    for _ in 0..100 {
        let request = StorageServiceRequest::GetTransactionsWithProof(...);
        mock_client.send_request_from_peer(public_peer_1, request);
        mock_client.send_request_from_peer(public_peer_2, request);
    }
    
    // Now send a high-priority request from Validator network
    let start = Instant::now();
    let request = StorageServiceRequest::GetServerProtocolVersion;
    let response = mock_client.send_request_from_peer(validator_peer, request).await;
    let duration = start.elapsed();
    
    // The high-priority validator request should be fast, but due to
    // priority inversion, it gets delayed by low-priority requests
    assert!(duration > Duration::from_millis(100), 
        "High-priority request was delayed by low-priority flood");
    
    // In a properly implemented priority system, the validator request
    // should be processed quickly despite the low-priority flood
}
```

**Notes:**

The vulnerability is confirmed in the production codebase. The `select_all()` combinator provides no priority guarantees, treating all network streams equally regardless of their designated priority level. This allows malicious or misbehaving low-priority peers to effectively DoS high-priority state sync operations by consuming processing capacity with valid but resource-intensive requests.

The fix requires implementing priority-aware stream processing to ensure high-priority validators receive preferential treatment over low-priority public peers, consistent with the priority system designed for the state sync architecture.

### Citations

**File:** state-sync/aptos-data-client/src/priority.rs (L18-22)
```rust
pub enum PeerPriority {
    HighPriority,   // Peers to highly prioritize when requesting data
    MediumPriority, // Peers to prioritize iff high priority peers are unavailable
    LowPriority, // Peers to use iff no other peers are available (these are generally unreliable)
}
```

**File:** state-sync/aptos-data-client/src/priority.rs (L58-73)
```rust
    // Handle the case that this node is a validator
    let peer_network_id = peer.network_id();
    if base_config.role.is_validator() {
        // Validators should highly prioritize other validators
        if peer_network_id.is_validator_network() {
            return PeerPriority::HighPriority;
        }

        // VFNs should be prioritized over PFNs. Note: having PFNs
        // connected to a validator is a rare (but possible) scenario.
        return if peer_network_id.is_vfn_network() {
            PeerPriority::MediumPriority
        } else {
            PeerPriority::LowPriority
        };
    }
```

**File:** state-sync/storage-service/server/src/network.rs (L40-54)
```rust
    pub fn new(network_service_events: NetworkServiceEvents<StorageServiceMessage>) -> Self {
        // Transform the event streams to also include the network ID
        let network_events: Vec<_> = network_service_events
            .into_network_and_events()
            .into_iter()
            .map(|(network_id, events)| events.map(move |event| (network_id, event)))
            .collect();
        let network_events = select_all(network_events).fuse();

        // Transform each event to a network request
        let network_request_stream = network_events
            .filter_map(|(network_id, event)| {
                future::ready(Self::event_to_request(network_id, event))
            })
            .boxed();
```

**File:** state-sync/storage-service/server/src/lib.rs (L389-419)
```rust
        while let Some(network_request) = self.network_requests.next().await {
            // All handler methods are currently CPU-bound and synchronous
            // I/O-bound, so we want to spawn on the blocking thread pool to
            // avoid starving other async tasks on the same runtime.
            let storage = self.storage.clone();
            let config = self.storage_service_config;
            let cached_storage_server_summary = self.cached_storage_server_summary.clone();
            let optimistic_fetches = self.optimistic_fetches.clone();
            let subscriptions = self.subscriptions.clone();
            let lru_response_cache = self.lru_response_cache.clone();
            let request_moderator = self.request_moderator.clone();
            let time_service = self.time_service.clone();
            self.runtime.spawn_blocking(move || {
                Handler::new(
                    cached_storage_server_summary,
                    optimistic_fetches,
                    lru_response_cache,
                    request_moderator,
                    storage,
                    subscriptions,
                    time_service,
                )
                .process_request_and_respond(
                    config,
                    network_request.peer_network_id,
                    network_request.protocol_id,
                    network_request.storage_service_request,
                    network_request.response_sender,
                );
            });
        }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L132-196)
```rust
    /// Validates the given request and verifies that the peer is behaving
    /// correctly. If the request fails validation, an error is returned.
    pub fn validate_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> Result<(), Error> {
        // Validate the request and time the operation
        let validate_request = || {
            // If the peer is being ignored, return an error
            if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
                if peer_state.is_ignored() {
                    return Err(Error::TooManyInvalidRequests(format!(
                        "Peer is temporarily ignored. Unable to handle request: {:?}",
                        request
                    )));
                }
            }

            // Get the latest storage server summary
            let storage_server_summary = self.cached_storage_server_summary.load();

            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }

            Ok(()) // The request is valid
        };
        utils::execute_and_time_duration(
            &metrics::STORAGE_REQUEST_VALIDATION_LATENCY,
            Some((peer_network_id, request)),
            None,
            validate_request,
            None,
        )
    }
```
