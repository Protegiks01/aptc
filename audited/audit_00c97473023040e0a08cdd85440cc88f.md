# Audit Report

## Title
Pending Block Persistence After Partial Subscription Termination Allows Processing of Blocks from Untrusted Peers

## Summary
When consensus observer subscription changes occur with partial peer termination (not all subscriptions terminated), `clear_pending_block_state()` is not invoked, leaving blocks from terminated peers in the pending block store. These blocks can subsequently be processed when their payloads arrive, even after the sending peer has been deemed unhealthy and removed from active subscriptions.

## Finding Description
The consensus observer maintains subscriptions to validator peers and processes blocks received from them. The subscription health check mechanism in `check_and_manage_subscriptions()` can terminate unhealthy subscriptions and create new ones. However, a critical gap exists in the state clearing logic. [1](#0-0) 

The vulnerability manifests in this flow:

1. Observer has active subscriptions to peers [A, B, C]
2. Peer A sends ordered blocks for epoch E, rounds R1-R3 that are missing payloads
3. These blocks pass initial verification and are stored in the pending block store [2](#0-1) 

4. Subscription health check determines peer A is unhealthy (timeout, connection loss, etc.) [3](#0-2) 

5. Peer A is terminated and removed from `active_observer_subscriptions`
6. Since peers B and C remain active, `check_and_manage_subscriptions()` returns `Ok()` [4](#0-3) 

7. **Critical**: `clear_pending_block_state()` is NOT called because the error condition is not met
8. Peer A's blocks remain in the pending block store with their associated `PeerNetworkId`
9. When block payloads arrive (from any source), peer A's blocks are processed [5](#0-4) 

This creates a Time-Of-Check-Time-Of-Use (TOCTOU) vulnerability where:
- **Check**: `verify_message_for_subscription()` validates the peer is subscribed when the block message arrives [6](#0-5) 

- **Use**: Block processing occurs potentially much later, after the peer has been deemed unhealthy and unsubscribed

## Impact Explanation
This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program for the following reasons:

**State Inconsistencies**: The observer node processes blocks from peers that have been explicitly deemed unhealthy and removed from trusted subscriptions. This violates the subscription model's trust assumptions and can lead to:

1. **Conflicting Block States**: If the terminated peer A sent blocks that differ from blocks subsequently received from healthy peer D for the same epoch/round, the first-received blocks (from untrusted A) take precedence due to the `existing_pending_block` check [7](#0-6) 

2. **Resource Consumption from Untrusted Sources**: Blocks from terminated peers consume processing resources and memory, potentially degrading performance when the observer should have discarded this data

3. **Validation Bypass Intent**: While blocks must still pass cryptographic verification, the subscription health mechanism's intent is to filter out unreliable or potentially malicious peers. Processing their blocks after termination undermines this security layer.

The impact is not Critical because blocks must still pass cryptographic verification against the epoch state's validator set. However, it represents a significant protocol violation that could be exploited to degrade observer node performance or introduce subtle state inconsistencies.

## Likelihood Explanation
**Likelihood: Medium-High**

This vulnerability is triggered by normal operational conditions:

1. **Common Occurrence**: Network issues, peer timeouts, and subscription rebalancing happen regularly in production networks
2. **Multiple Subscriptions**: The observer typically maintains multiple concurrent subscriptions (`max_concurrent_subscriptions` > 1), making partial termination the common case rather than full termination
3. **Quorum Store Usage**: When quorum store is enabled, blocks frequently arrive without payloads, requiring them to be stored as pending
4. **No Special Access Required**: An attacker doesn't need special privileges beyond being initially selected as a subscription peer (possible for any validator node)

The vulnerability requires:
- Observer has multiple active subscriptions (typical)
- Attacker peer gets selected for subscription (validator nodes)
- Attacker sends blocks missing payloads (normal for quorum store)
- Attacker peer later becomes unhealthy/terminated (easily triggered via timeout or disconnect)
- Other subscriptions remain active (common)

## Recommendation

**Solution 1: Always Clear State on Subscription Changes (Recommended)**

Modify `check_progress()` to clear pending block state whenever any subscription change occurs, not just when all subscriptions are terminated:

```rust
async fn check_progress(&mut self) {
    // ... existing code ...
    
    // Track if any subscriptions changed
    let initial_subscription_peers = self.subscription_manager.get_active_subscription_peers();
    
    // Check the health of the active subscriptions
    let subscription_result = self
        .subscription_manager
        .check_and_manage_subscriptions()
        .await;
    
    // Check if subscriptions changed
    let final_subscription_peers = self.subscription_manager.get_active_subscription_peers();
    let subscriptions_changed = initial_subscription_peers != final_subscription_peers;
    
    // Clear state if subscriptions changed OR all were terminated
    if subscriptions_changed || subscription_result.is_err() {
        warn!(LogSchema::new(LogEntry::ConsensusObserver)
            .message(&format!("Subscriptions changed or failed! Clearing pending state.")));
        self.clear_pending_block_state().await;
    }
}
```

**Solution 2: Track Peer Association in Pending Blocks**

Enhance the pending block validation to reject blocks whose sending peer is no longer subscribed:

```rust
// In process_ordered_block_message, after checking existing_pending_block:
if let Some(pending_block) = self.observer_block_data.lock().get_pending_block(...) {
    let sender_peer = pending_block.peer_network_id();
    if !self.subscription_manager.is_peer_subscribed(sender_peer) {
        warn!("Rejecting pending block from unsubscribed peer");
        return;
    }
}
```

**Recommended Approach**: Solution 1 is simpler and more robust, ensuring clean state on any subscription topology change. Solution 2 provides defense-in-depth but adds complexity.

## Proof of Concept

```rust
#[tokio::test]
async fn test_pending_blocks_survive_partial_subscription_termination() {
    // Setup consensus observer with 3 peer subscriptions
    let mut observer = setup_test_consensus_observer(3).await;
    let peers = vec![peer_a, peer_b, peer_c];
    
    // Peer A sends ordered blocks missing payloads (epoch 1, rounds 10-12)
    let ordered_block = create_test_ordered_block(1, 10, None); // no payload
    observer.process_ordered_block_message(peer_a, Instant::now(), ordered_block).await;
    
    // Verify block is in pending store
    assert!(observer.observer_block_data.lock().existing_pending_block(&ordered_block));
    
    // Simulate peer A becoming unhealthy (timeout)
    advance_time_to_trigger_timeout();
    
    // Run subscription health check - peer A terminated, B and C remain
    let result = observer.subscription_manager.check_and_manage_subscriptions().await;
    assert!(result.is_ok()); // Returns Ok because not all subscriptions terminated
    
    // Verify peer A is no longer subscribed
    assert!(!observer.subscription_manager.is_peer_subscribed(peer_a));
    
    // VULNERABILITY: Peer A's block is still in pending store
    assert!(observer.observer_block_data.lock().existing_pending_block(&ordered_block));
    
    // When payload arrives, peer A's block is processed despite A being untrusted
    let payload = create_test_block_payload(1, 10);
    observer.process_block_payload_message(peer_b, Instant::now(), payload).await;
    
    // Block from untrusted peer A gets processed
    assert!(observer.observer_block_data.lock().get_ordered_block(1, 10).is_some());
    
    println!("VULNERABILITY: Block from terminated peer A was processed!");
}
```

This test demonstrates that blocks from a terminated peer remain in the pending store and get processed when payloads arrive, violating the subscription trust model.

---

**Notes:**

While blocks must still pass cryptographic verification (preventing completely malicious data), this vulnerability allows blocks from peers explicitly deemed unhealthy/untrusted to influence observer state. This undermines the subscription health mechanism's purpose and could be exploited by a malicious validator to inject blocks after being detected as suspicious, potentially causing performance degradation or subtle state inconsistencies across observer nodes.

### Citations

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L203-214)
```rust
        // Otherwise, check the health of the active subscriptions
        if let Err(error) = self
            .subscription_manager
            .check_and_manage_subscriptions()
            .await
        {
            // Log the failure and clear the pending block state
            warn!(LogSchema::new(LogEntry::ConsensusObserver)
                .message(&format!("Subscription checks failed! Error: {:?}", error)));
            self.clear_pending_block_state().await;
        }
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L432-438)
```rust
        // Check if there are blocks that were missing payloads but are
        // now ready because of the new payload. Note: this should only
        // be done if the payload has been verified correctly.
        if verified_payload {
            self.order_ready_pending_block(block_epoch, block_round)
                .await;
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L578-594)
```rust
        // Verify the message is from the peers we've subscribed to
        if let Err(error) = self
            .subscription_manager
            .verify_message_for_subscription(peer_network_id)
        {
            // Update the rejected message counter
            increment_rejected_message_counter(&peer_network_id, &message);

            // Log the error and return
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Received message that was not from an active subscription! Error: {:?}",
                    error,
                ))
            );
            return;
        }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L90-99)
```rust
    /// Returns true iff the store contains an entry for the given ordered block
    pub fn existing_pending_block(&self, ordered_block: &OrderedBlock) -> bool {
        // Get the epoch and round of the first block
        let first_block = ordered_block.first_block();
        let first_block_epoch_round = (first_block.epoch(), first_block.round());

        // Check if the block is already in the store by epoch and round
        self.blocks_without_payloads
            .contains_key(&first_block_epoch_round)
    }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L111-132)
```rust
    /// Inserts a pending block (without payloads) into the store
    pub fn insert_pending_block(&mut self, pending_block: Arc<PendingBlockWithMetadata>) {
        // Get the first block in the ordered blocks
        let first_block = pending_block.ordered_block().first_block();

        // Insert the block into the store using the epoch round of the first block
        let first_block_epoch_round = (first_block.epoch(), first_block.round());
        match self.blocks_without_payloads.entry(first_block_epoch_round) {
            Entry::Occupied(_) => {
                // The block is already in the store
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "A pending block was already found for the given epoch and round: {:?}",
                        first_block_epoch_round
                    ))
                );
            },
            Entry::Vacant(entry) => {
                // Insert the block into the store
                entry.insert(pending_block.clone());
            },
        }
```

**File:** consensus/src/consensus_observer/observer/subscription_manager.rs (L142-149)
```rust
        if all_subscriptions_terminated {
            Err(Error::SubscriptionsReset(format!(
                "All {:?} subscriptions were unhealthy and terminated!",
                num_terminated_subscriptions,
            )))
        } else {
            Ok(())
        }
```

**File:** consensus/src/consensus_observer/observer/subscription_manager.rs (L270-305)
```rust
    /// Terminates any unhealthy subscriptions and returns the list of terminated subscriptions
    fn terminate_unhealthy_subscriptions(
        &mut self,
        connected_peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
    ) -> Vec<(PeerNetworkId, Error)> {
        // Go through all active subscriptions and terminate any unhealthy ones
        let mut terminated_subscriptions = vec![];
        for subscription_peer in self.get_active_subscription_peers() {
            // To avoid terminating too many subscriptions at once, we should skip
            // the peer optimality check if we've already terminated a subscription.
            let skip_peer_optimality_check = !terminated_subscriptions.is_empty();

            // Check the health of the subscription and terminate it if needed
            if let Err(error) = self.check_subscription_health(
                connected_peers_and_metadata,
                subscription_peer,
                skip_peer_optimality_check,
            ) {
                // Log the subscription termination error
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Terminating subscription to peer: {:?}! Termination reason: {:?}",
                        subscription_peer, error
                    ))
                );

                // Unsubscribe from the peer and remove the subscription
                self.unsubscribe_from_peer(subscription_peer);

                // Add the peer to the list of terminated subscriptions
                terminated_subscriptions.push((subscription_peer, error));
            }
        }

        terminated_subscriptions
    }
```
