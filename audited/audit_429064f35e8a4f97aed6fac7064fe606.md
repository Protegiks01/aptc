# Audit Report

## Title
Unbounded Channel Buffer Overflow in DBCommitter Causing Memory Exhaustion and Node Crashes

## Summary
The `DBIndexer` uses an unbounded MPSC channel to send indexing batches to the `DBCommitter` thread. During catch-up scenarios after node downtime, rapid batch submissions can accumulate unbounded memory in the channel, leading to out-of-memory (OOM) conditions and node crashes with temporary loss of uncommitted state key indexing data.

## Finding Description

The vulnerability exists in the channel architecture used for asynchronous database commits in the internal indexer system.

**Architecture:**

The `DBIndexer` creates an unbounded channel for sending `SchemaBatch` objects to a separate `DBCommitter` thread [1](#0-0) . This channel has no buffer size limit, allowing unlimited message accumulation in memory.

**Vulnerable Flow:**

1. When the indexer falls behind (e.g., after node downtime), `InternalIndexerDBService::run()` calls `process()` with a large version gap [2](#0-1) .

2. The `process()` method enters a tight loop calling `process_a_batch()` repeatedly [3](#0-2) .

3. Each `process_a_batch()` creates a `SchemaBatch` containing up to 10,000 transactions worth of indexed data (state keys, events, transaction indices) [4](#0-3)  and immediately sends it via the unbounded channel [5](#0-4) .

4. The send operation **never blocks** because the channel is unbounded - batches accumulate in memory if the `DBCommitter` cannot keep up with disk writes [6](#0-5) .

5. The `DBCommitter` performs **synchronous** RocksDB writes which can take 10-100ms per batch depending on disk I/O characteristics.

**Memory Consumption:**

Each `SchemaBatch` stores `WriteOp` entries in a HashMap [7](#0-6) . For state key indexing enabled, each transaction's write operations are added to the batch [8](#0-7) . With typical transaction complexity:
- 10,000 transactions per batch
- Average 10 state keys per transaction = 100,000 keys
- ~50 bytes per state key entry
- Plus events, transaction indices, metadata
- **Estimated: 5-50 MB per batch**

**Attack Scenario:**

If a validator experiences a version gap of 1,000,000 transactions (realistic after several hours downtime):
- 100 batches sent rapidly to the channel
- 100 batches × 20 MB average = **2 GB memory consumption**
- Larger gaps or more complex transactions scale proportionally
- On memory-constrained systems, this triggers OOM killer

**Contrast with Safe Implementation:**

The codebase uses bounded channels elsewhere for backpressure control. For example, `BufferedState` uses `sync_channel` with buffer size 1 [9](#0-8)  and [10](#0-9) , ensuring the sender blocks when the receiver cannot keep up. The DBIndexer lacks this protection.

**State Key Loss:**

When OOM kills the process, batches queued in the channel but not yet written to disk are lost. The indexer metadata shows the last committed version [11](#0-10) , so upon restart, the indexer re-processes from that point. While eventual consistency is maintained, there is temporary data loss and service disruption.

## Impact Explanation

This qualifies as **High Severity** under Aptos bug bounty criteria:

1. **"Validator node slowdowns"** - Memory exhaustion causes severe performance degradation before crash
2. **"API crashes"** - The internal indexer provides query APIs for events, transactions, and state keys that become unavailable
3. **Availability Impact** - Node requires restart and re-indexing, causing downtime

While not consensus-critical (the main ledger DB is separate), the internal indexer is part of validator node infrastructure. Crashes affect node operators and API availability.

The issue does NOT qualify as Critical because:
- No funds loss or consensus safety violation
- Indexer is not consensus-critical
- Recovery is automatic (eventual consistency)
- No permanent data loss

## Likelihood Explanation

**High Likelihood** - This occurs naturally under common operational scenarios:

1. **Node Maintenance**: Operators routinely restart nodes for upgrades, causing indexer lag
2. **Network Partitions**: Brief disconnections create version gaps
3. **Resource Constraints**: Validators may run on moderate hardware where 2-5 GB channel buffer exceeds available memory
4. **Fast Sync Mode**: The code specifically handles fast sync scenarios where large gaps are expected [12](#0-11) 

No attacker interaction required - the vulnerability triggers during normal catch-up operations when the indexer falls behind the ledger.

## Recommendation

Replace the unbounded channel with a bounded synchronous channel to provide backpressure:

```rust
// In DBIndexer::new() at line 328
// Replace:
let (sender, receiver) = mpsc::channel();

// With:
const DB_COMMITTER_CHANNEL_BUFFER_SIZE: usize = 3;
let (sender, receiver) = mpsc::sync_channel(DB_COMMITTER_CHANNEL_BUFFER_SIZE);
```

A buffer size of 1-3 is recommended (similar to other committer patterns in the codebase). This ensures:
- The sender blocks when the channel is full
- Backpressure prevents unbounded memory growth
- The batch producer naturally throttles to the committer's write speed
- Memory consumption is bounded to buffer_size × batch_size

The sending code already handles errors properly [5](#0-4) , so no additional changes are needed for error propagation.

## Proof of Concept

```rust
// Test case demonstrating the vulnerability
// File: storage/indexer/src/db_indexer_test.rs

#[test]
fn test_channel_buffer_overflow_simulation() {
    use std::sync::{Arc, mpsc};
    use std::thread;
    use std::time::Duration;
    
    // Simulate unbounded channel like DBIndexer
    let (sender, receiver) = mpsc::channel();
    
    // Simulate slow DBCommitter (10ms per batch)
    let receiver_handle = thread::spawn(move || {
        let mut count = 0;
        while let Ok(_batch) = receiver.recv() {
            thread::sleep(Duration::from_millis(10)); // Simulate disk write
            count += 1;
        }
        count
    });
    
    // Simulate rapid batch producer like process() loop
    let sender_handle = thread::spawn(move || {
        let batch_data = vec![0u8; 20_000_000]; // 20 MB per batch
        for i in 0..100 {
            // Send 100 batches rapidly (2 GB total)
            sender.send(batch_data.clone()).unwrap();
            println!("Sent batch {}, estimated memory: {} MB", i, (i + 1) * 20);
        }
        drop(sender);
    });
    
    sender_handle.join().unwrap();
    let processed = receiver_handle.join().unwrap();
    
    println!("Processed {} batches", processed);
    // On memory-constrained systems, this would OOM before completing
}

// Integration test with actual DBIndexer (requires test setup)
#[tokio::test]
async fn test_indexer_catch_up_memory_pressure() {
    // 1. Setup test DB with large version gap (e.g., 1M transactions)
    // 2. Enable state keys indexing
    // 3. Start internal indexer service
    // 4. Monitor memory consumption during catch-up
    // 5. Verify memory grows unbounded as batches queue
    // 6. Expected: Memory exhaustion or very high memory usage
    
    // This test would need full environment setup with:
    // - Test database with historical transactions
    // - InternalIndexerDB configuration
    // - Memory monitoring instrumentation
}
```

**Notes:**

- The internal indexer is designed for query API support, not consensus operations, making this a High rather than Critical severity issue
- The vulnerability manifests during operational scenarios (catch-up after downtime) rather than requiring attacker interaction
- Fix is straightforward: replace unbounded channel with bounded channel for backpressure control
- Similar patterns exist elsewhere in the codebase (BufferedState, StateSnapshotCommitter) that correctly use bounded channels
- Memory consumption scales with both version gap size and transaction complexity (state key count per transaction)

### Citations

**File:** storage/indexer/src/db_indexer.rs (L64-71)
```rust
            let batch_opt = self
                .receiver
                .recv()
                .expect("Failed to receive batch from DB Indexer");
            if let Some(batch) = batch_opt {
                self.db
                    .write_schemas(batch)
                    .expect("Failed to write batch to indexer db");
```

**File:** storage/indexer/src/db_indexer.rs (L328-328)
```rust
        let (sender, reciver) = mpsc::channel();
```

**File:** storage/indexer/src/db_indexer.rs (L399-400)
```rust
        while version < end_version {
            let next_version = self.process_a_batch(version, end_version)?;
```

**File:** storage/indexer/src/db_indexer.rs (L489-496)
```rust
            if self.indexer_db.statekeys_enabled() {
                writeset.write_op_iter().for_each(|(state_key, write_op)| {
                    if write_op.is_creation() || write_op.is_modification() {
                        batch
                            .put::<StateKeysSchema>(state_key, &())
                            .expect("Failed to put state keys to a batch");
                    }
                });
```

**File:** storage/indexer/src/db_indexer.rs (L542-545)
```rust
        batch.put::<InternalIndexerMetadataSchema>(
            &MetadataKey::LatestVersion,
            &MetadataValue::Version(version - 1),
        )?;
```

**File:** storage/indexer/src/db_indexer.rs (L546-548)
```rust
        self.sender
            .send(Some(batch))
            .map_err(|e| AptosDbError::Other(e.to_string()))?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L88-100)
```rust
    pub async fn get_start_version(&self, node_config: &NodeConfig) -> Result<Version> {
        let fast_sync_enabled = node_config
            .state_sync
            .state_sync_driver
            .bootstrapping_mode
            .is_fast_sync();
        let mut main_db_synced_version = self.db_indexer.main_db_reader.ensure_synced_version()?;

        // Wait till fast sync is done
        while fast_sync_enabled && main_db_synced_version == 0 {
            tokio::time::sleep(std::time::Duration::from_secs(1)).await;
            main_db_synced_version = self.db_indexer.main_db_reader.ensure_synced_version()?;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L183-183)
```rust
            let next_version = self.db_indexer.process(start_version, target_version)?;
```

**File:** config/src/config/internal_indexer_db_config.rs (L77-77)
```rust
            batch_size: 10_000,
```

**File:** storage/schemadb/src/batch.rs (L130-132)
```rust
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
```

**File:** storage/aptosdb/src/state_store/buffered_state.rs (L28-28)
```rust
pub(crate) const ASYNC_COMMIT_CHANNEL_BUFFER_SIZE: u64 = 1;
```

**File:** storage/aptosdb/src/state_store/buffered_state.rs (L62-63)
```rust
        let (state_commit_sender, state_commit_receiver) =
            mpsc::sync_channel(ASYNC_COMMIT_CHANNEL_BUFFER_SIZE as usize);
```
