# Audit Report

## Title
DKG Manager Index Validation Missing - Validator Node Panic on Epoch State Mismatch

## Summary
The `DKGManager::new()` function accepts `my_index` without validating it against the validator set size, and later uses this index for unchecked array access in `RealDKG::generate_transcript()`, causing a panic when the index is out of bounds. While the normal production flow provides valid indices, state inconsistencies during epoch transitions could trigger this vulnerability.

## Finding Description

The vulnerability exists in a validation gap between the DKG manager initialization and transcript generation: [1](#0-0) 

The `my_index` parameter is stored without bounds checking against the epoch state's validator count. This stored index is later used at: [2](#0-1) 

The `my_index` is passed to `DKG::generate_transcript()`, which performs an unchecked array access: [3](#0-2) 

At line 250, `dealer_validator_set[my_index]` is accessed without verifying `my_index < dealer_validator_set.len()`.

The only validation in the event processing path checks epoch numbers but not validator set compatibility: [4](#0-3) 

**Attack Scenario:**
1. A validator node initializes with `epoch_state` containing N validators, where the node's index is X (0 ≤ X < N)
2. Due to epoch transition timing or state inconsistency, a `DKGStartEvent` arrives with `dealer_validator_set` containing M validators where M ≤ X
3. The epoch number check at line 442 passes (same epoch)
4. `setup_deal_broadcast()` is called with this metadata
5. At line 250 of `real_dkg/mod.rs`, the access `dealer_validator_set[X]` panics because X ≥ M
6. The DKG manager task crashes, preventing DKG completion for that validator

## Impact Explanation

**Severity: High** (per Aptos Bug Bounty criteria: "Validator node slowdowns" / "API crashes" / "Significant protocol violations")

When triggered, this vulnerability causes:
- **Validator node crash**: The DKG manager tokio task panics and terminates
- **DKG failure**: The affected validator cannot participate in DKG, potentially preventing the randomness beacon from being established
- **Network liveness risk**: If multiple validators encounter this condition simultaneously, DKG cannot complete, blocking epoch transitions that depend on randomness

This breaks the **Consensus Safety** invariant requirement that validator nodes must operate reliably during protocol-critical operations like DKG.

## Likelihood Explanation

**Likelihood: Low to Medium**

While the production code path in `epoch_manager.rs` normally provides valid indices, this vulnerability can manifest under:

1. **Epoch transition edge cases**: State replication delays could cause temporary mismatches between local epoch state and on-chain DKG metadata
2. **State synchronization issues**: Validators catching up from snapshots might have validator set inconsistencies
3. **Governance-driven validator set changes**: Rapid validator additions/removals during epoch boundaries

The vulnerability does NOT require:
- Malicious on-chain state (validator sets come from legitimate stake module)
- Direct code access or parameter injection
- Validator collusion

However, it requires state timing conditions that are rare but not impossible in a distributed system with multiple data sources (local state vs. on-chain events).

## Recommendation

Add validation in `DKGManager::new()` to ensure `my_index` is within bounds:

```rust
pub fn new(
    dealer_sk: Arc<DKG::DealerPrivateKey>,
    dealer_pk: Arc<DKG::DealerPublicKey>,
    my_index: usize,
    my_addr: AccountAddress,
    epoch_state: Arc<EpochState>,
    agg_trx_producer: Arc<dyn TAggTranscriptProducer<DKG>>,
    vtxn_pool: VTxnPoolState,
) -> Result<Self> {
    // Validate my_index against epoch_state validator count
    ensure!(
        my_index < epoch_state.verifier.len(),
        "my_index {} out of bounds for validator set of size {}",
        my_index,
        epoch_state.verifier.len()
    );
    
    let (pull_notification_tx, pull_notification_rx) =
        aptos_channel::new(QueueStyle::KLAST, 1, None);
    Ok(Self {
        dealer_sk,
        dealer_pk,
        my_addr,
        my_index,
        epoch_state,
        vtxn_pool,
        agg_trx_tx: None,
        pull_notification_tx,
        pull_notification_rx,
        agg_trx_producer,
        stopped: false,
        state: InnerState::NotStarted,
    })
}
```

Additionally, add validation in `process_dkg_start_event()` to verify validator set compatibility:

```rust
async fn process_dkg_start_event(&mut self, event: DKGStartEvent) -> Result<()> {
    // ... existing code ...
    
    // Validate my_index is within dealer_validator_set bounds
    ensure!(
        self.my_index < session_metadata.dealer_validator_set.len(),
        "[DKG] my_index {} out of bounds for dealer_validator_set of size {}",
        self.my_index,
        session_metadata.dealer_validator_set.len()
    );
    
    self.setup_deal_broadcast(start_time_us, &session_metadata).await
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test_index_bounds {
    use super::*;
    use aptos_types::{
        validator_verifier::ValidatorVerifier,
        validator_consensus_info::ValidatorConsensusInfo,
    };
    
    #[tokio::test]
    #[should_panic(expected = "index out of bounds")]
    async fn test_out_of_bounds_my_index_causes_panic() {
        // Create epoch_state with 3 validators
        let validators = vec![
            create_test_validator(0),
            create_test_validator(1), 
            create_test_validator(2),
        ];
        let verifier = ValidatorVerifier::new(validators.clone());
        let epoch_state = Arc::new(EpochState::new(1, verifier.into()));
        
        // Create DKG manager with my_index=5 (out of bounds for 3 validators)
        let dkg_manager = DKGManager::<DefaultDKG>::new(
            Arc::new(test_dealer_sk()),
            Arc::new(test_dealer_pk()),
            5, // OUT OF BOUNDS
            test_address(),
            epoch_state,
            Arc::new(test_agg_producer()),
            test_vtxn_pool(),
        );
        
        // Create DKGStartEvent with dealer_validator_set containing only 2 validators
        let metadata = DKGSessionMetadata {
            dealer_epoch: 1,
            randomness_config: default_config(),
            dealer_validator_set: vec![
                validators[0].clone(),
                validators[1].clone(),
            ], // Only 2 validators, my_index=5 is out of bounds
            target_validator_set: validators,
        };
        
        let event = DKGStartEvent {
            session_metadata: metadata,
            start_time_us: 0,
        };
        
        // This will panic when generate_transcript tries dealer_validator_set[5]
        dkg_manager.process_dkg_start_event(event).await.unwrap();
    }
}
```

**Notes:**
- The vulnerability requires state inconsistency between local validator set and DKG session metadata, which can occur during epoch transitions
- The missing validation violates defensive programming principles and Rust's safety-first philosophy
- While not directly exploitable by external attackers, it represents a reliability gap that could cause node crashes during edge-case network conditions
- The fix is straightforward: add bounds checking before storing and using `my_index`

### Citations

**File:** dkg/src/dkg_manager/mod.rs (L93-118)
```rust
    pub fn new(
        dealer_sk: Arc<DKG::DealerPrivateKey>,
        dealer_pk: Arc<DKG::DealerPublicKey>,
        my_index: usize,
        my_addr: AccountAddress,
        epoch_state: Arc<EpochState>,
        agg_trx_producer: Arc<dyn TAggTranscriptProducer<DKG>>,
        vtxn_pool: VTxnPoolState,
    ) -> Self {
        let (pull_notification_tx, pull_notification_rx) =
            aptos_channel::new(QueueStyle::KLAST, 1, None);
        Self {
            dealer_sk,
            dealer_pk,
            my_addr,
            my_index,
            epoch_state,
            vtxn_pool,
            agg_trx_tx: None,
            pull_notification_tx,
            pull_notification_rx,
            agg_trx_producer,
            stopped: false,
            state: InnerState::NotStarted,
        }
    }
```

**File:** dkg/src/dkg_manager/mod.rs (L332-339)
```rust
        let trx = DKG::generate_transcript(
            &mut rng,
            &public_params,
            &input_secret,
            self.my_index as u64,
            &self.dealer_sk,
            &self.dealer_pk,
        );
```

**File:** dkg/src/dkg_manager/mod.rs (L427-451)
```rust
    async fn process_dkg_start_event(&mut self, event: DKGStartEvent) -> Result<()> {
        info!(
            epoch = self.epoch_state.epoch,
            my_addr = self.my_addr,
            "[DKG] Processing DKGStart event."
        );
        fail_point!("dkg::process_dkg_start_event");
        let DKGStartEvent {
            session_metadata,
            start_time_us,
        } = event;
        ensure!(
            matches!(&self.state, InnerState::NotStarted),
            "[DKG] dkg already started"
        );
        if self.epoch_state.epoch != session_metadata.dealer_epoch {
            warn!(
                "[DKG] event (from epoch {}) not for current epoch ({}), ignoring",
                session_metadata.dealer_epoch, self.epoch_state.epoch
            );
            return Ok(());
        }
        self.setup_deal_broadcast(start_time_us, &session_metadata)
            .await
    }
```

**File:** types/src/dkg/real_dkg/mod.rs (L241-251)
```rust
    fn generate_transcript<R: CryptoRng + RngCore>(
        rng: &mut R,
        pub_params: &Self::PublicParams,
        input_secret: &Self::InputSecret,
        my_index: u64,
        sk: &Self::DealerPrivateKey,
        pk: &Self::DealerPublicKey,
    ) -> Self::Transcript {
        let my_index = my_index as usize;
        let my_addr = pub_params.session_metadata.dealer_validator_set[my_index].addr;
        let aux = (pub_params.session_metadata.dealer_epoch, my_addr);
```
