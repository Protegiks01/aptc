# Audit Report

## Title
Chunked Package Deployment State Corruption via Transaction Failure Mid-Sequence

## Summary
The chunked package publishing mechanism in Aptos can leave the `StagingArea` resource in a corrupted state when intermediate staging transactions fail during execution. While transaction sequence numbers prevent true out-of-order execution, transaction failures mid-sequence cause incomplete chunk accumulation, leading to publish failures, gas waste, and requiring manual state cleanup. The TODO comment indicates future orderless transaction support, which would significantly worsen this vulnerability.

## Finding Description

The `generate_transactions()` function creates multiple signed transactions for publishing large packages via the chunked publishing mechanism. [1](#0-0) 

These transactions call `stage_code_chunk()` to accumulate package chunks in a `StagingArea` resource. [2](#0-1) 

The chunking logic assigns all chunks of the same module the same index, and chunks are appended to existing data at that index. [3](#0-2) 

The critical vulnerability occurs when:
1. Transaction N successfully stages chunks for module 0 (part 1)
2. Transaction N+1 FAILS during execution (e.g., out of gas) before staging module 0 (part 2)
3. Transaction N+2 successfully stages module 0 (part 3)

This results in the `StagingArea` containing: `code[0] = [part1, part3]` - missing part 2. [4](#0-3) 

When the final publish transaction executes, `assemble_module_code()` iterates through all indices and borrows from the SmartTable. If an index is missing entirely, it aborts. If chunks within a module are missing, corrupted bytecode is assembled. [5](#0-4) 

The CLI implementation acknowledges this risk with warning messages about incomplete data in the `StagingArea`. [6](#0-5) 

A TODO comment indicates plans to migrate to orderless transactions (payload v2 format), which would eliminate sequence number ordering guarantees and make true out-of-order execution possible. [7](#0-6) 

## Impact Explanation

**Current Impact: Medium Severity**
- State inconsistency in `StagingArea` resource requiring manual intervention
- Gas waste from failed publish attempts
- Denial of service for affected users unable to publish until cleanup

**Future Impact (with orderless transactions): High to Critical Severity**
- True out-of-order execution possible
- Corrupted bytecode could potentially pass verification
- Consensus divergence if different nodes assemble modules differently
- Violates the State Consistency invariant: "State transitions must be atomic and verifiable via Merkle proofs"

## Likelihood Explanation

**Current Likelihood: Medium**
- Transaction failures mid-sequence can occur due to:
  - Incorrect gas estimation
  - Runtime errors in stage_code_chunk execution
  - Storage quota limits
  - Network issues causing transaction to expire
- The transaction-generator-lib lacks the error handling present in the CLI tool
- Affects any user publishing packages larger than 55KB

**Future Likelihood (with orderless transactions): High**
- Without sequence number enforcement, network delays or mempool reordering could cause out-of-order execution
- Malicious actors could intentionally submit transactions out of order

## Recommendation

**Immediate fixes:**

1. Add transaction validation to ensure all required chunks are present before publishing:

```move
// In stage_code_chunk_internal, track which chunks have been received
struct ChunkTracker has store {
    expected_chunks: u64,
    received_chunks: vector<bool>
}

// In final publish functions, verify completeness:
inline fun verify_all_chunks_present(staging_area: &StagingArea) {
    let i = 0;
    while (i <= staging_area.last_module_idx) {
        assert!(smart_table::contains(&staging_area.code, i), error::invalid_state(EINCOMPLETE_CHUNKS));
        i = i + 1;
    };
}
```

2. Implement automatic `StagingArea` cleanup on transaction failure using transaction epilogue hooks.

3. Add chunk sequence numbers or checksums to detect missing/corrupted data before publish.

**For orderless transaction support:**

1. Include a transaction dependency field linking each chunk transaction to its predecessor
2. Use a merkle tree commitment of all expected chunks in the first transaction
3. Verify merkle proof completeness before final publish
4. Consider atomic batch transactions that either all succeed or all fail

## Proof of Concept

```move
#[test(publisher = @0x123)]
fun test_partial_deployment_corruption(publisher: &signer) {
    use aptos_experimental::large_packages;
    
    // Stage first chunk successfully
    large_packages::stage_code_chunk(
        publisher,
        vector[0x01, 0x02],  // metadata chunk
        vector[0u16],         // module 0
        vector[vector[0xAA, 0xBB]]  // first chunk of module 0
    );
    
    // Simulate second transaction FAILING (not staged)
    // In reality, this would fail due to gas, but we simulate by skipping
    
    // Stage third chunk successfully  
    large_packages::stage_code_chunk(
        publisher,
        vector[],
        vector[0u16],         // module 0 again
        vector[vector[0xDD, 0xEE]]  // third chunk - creates corrupted module!
    );
    
    // Attempt to publish - will fail with corrupted bytecode
    large_packages::stage_code_chunk_and_publish_to_account(
        publisher,
        vector[],
        vector[],
        vector[]
    );
    // This will either abort due to missing chunks or assemble corrupted bytecode
}
```

**Notes**

The vulnerability exists in the interaction between the transaction generation layer (Rust) and the on-chain staging logic (Move). The key issue is the lack of atomicity guarantees across the multi-transaction sequence. Current sequence number enforcement provides partial mitigation, but planned orderless transaction support will require significant hardening of the chunked publishing protocol. The system would benefit from treating chunked publishing as a distributed transaction requiring proper two-phase commit semantics.

### Citations

**File:** crates/transaction-generator-lib/src/publish_modules.rs (L35-64)
```rust
    fn generate_transactions(
        &mut self,
        account: &LocalAccount,
        num_to_create: usize,
    ) -> Vec<SignedTransaction> {
        let mut requests = Vec::with_capacity(num_to_create);

        // First publish the module and then use it
        let package = self
            .package_handler
            .write()
            .pick_package(&mut self.rng, account.address());

        for payload in package.publish_transaction_payload(&self.txn_factory.get_chain_id()) {
            let txn = account.sign_with_transaction_builder(self.txn_factory.payload(payload));
            requests.push(txn);
        }
        // for _ in 1..num_to_create {
        //     let request = package.use_random_transaction(&mut self.rng, account, &self.txn_factory);
        //     requests.push(request);
        // }
        // republish
        // let package = self
        //     .package_handler
        //     .write()
        //     .pick_package(&mut self.rng, account.address());
        // let txn = package.publish_transaction(account, &self.txn_factory);
        // requests.push(txn);
        requests
    }
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L132-181)
```text
    inline fun stage_code_chunk_internal(
        owner: &signer,
        metadata_chunk: vector<u8>,
        code_indices: vector<u16>,
        code_chunks: vector<vector<u8>>
    ): &mut StagingArea {
        assert!(
            vector::length(&code_indices) == vector::length(&code_chunks),
            error::invalid_argument(ECODE_MISMATCH)
        );

        let owner_address = signer::address_of(owner);

        if (!exists<StagingArea>(owner_address)) {
            move_to(
                owner,
                StagingArea {
                    metadata_serialized: vector[],
                    code: smart_table::new(),
                    last_module_idx: 0
                }
            );
        };

        let staging_area = borrow_global_mut<StagingArea>(owner_address);

        if (!vector::is_empty(&metadata_chunk)) {
            vector::append(&mut staging_area.metadata_serialized, metadata_chunk);
        };

        let i = 0;
        while (i < vector::length(&code_chunks)) {
            let inner_code = *vector::borrow(&code_chunks, i);
            let idx = (*vector::borrow(&code_indices, i) as u64);

            if (smart_table::contains(&staging_area.code, idx)) {
                vector::append(
                    smart_table::borrow_mut(&mut staging_area.code, idx), inner_code
                );
            } else {
                smart_table::add(&mut staging_area.code, idx, inner_code);
                if (idx > staging_area.last_module_idx) {
                    staging_area.last_module_idx = idx;
                }
            };
            i = i + 1;
        };

        staging_area
    }
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L213-225)
```text
    inline fun assemble_module_code(staging_area: &mut StagingArea): vector<vector<u8>> {
        let last_module_idx = staging_area.last_module_idx;
        let code = vector[];
        let i = 0;
        while (i <= last_module_idx) {
            vector::push_back(
                &mut code,
                *smart_table::borrow(&staging_area.code, i)
            );
            i = i + 1;
        };
        code
    }
```

**File:** aptos-move/framework/src/chunked_publish.rs (L60-83)
```rust
    for (idx, module_code) in package_code.into_iter().enumerate() {
        let chunked_module = create_chunks(module_code, chunk_size);
        for chunk in chunked_module {
            if taken_size + chunk.len() > chunk_size {
                // Create a payload and reset accumulators
                let payload = large_packages_stage_code_chunk(
                    metadata_chunk,
                    code_indices.clone(),
                    code_chunks.clone(),
                    large_packages_module_address,
                );
                payloads.push(payload);

                metadata_chunk = vec![];
                code_indices.clear();
                code_chunks.clear();
                taken_size = 0;
            }

            code_indices.push(idx as u16);
            taken_size += chunk.len();
            code_chunks.push(chunk);
        }
    }
```

**File:** aptos-move/framework/src/chunked_publish.rs (L148-148)
```rust
    // TODO[Orderless]: Change this to payload v2 format.
```

**File:** crates/aptos/src/move_tool/mod.rs (L1735-1741)
```rust
            Err(e) => {
                println!("{}", "Caution: An error occurred while submitting chunked publish transactions. \
                \nDue to this error, there may be incomplete data left in the `StagingArea` resource. \
                \nThis could cause further errors if you attempt to run the chunked publish command again. \
                \nTo avoid this, use the `aptos move clear-staging-area` command to clean up the `StagingArea` resource under your account before retrying.".bold());
                return Err(e);
            },
```
