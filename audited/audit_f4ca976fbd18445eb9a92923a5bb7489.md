# Audit Report

## Title
Cache Corruption via Overlapping Batch Ranges from Malicious Fullnode

## Summary
The indexer-grpc cache worker lacks validation of individual transaction version numbers in received data chunks, allowing a malicious fullnode to send overlapping batch ranges that overwrite cache entries and corrupt the cache state. The worker only validates transaction counts, not the actual version fields within transactions.

## Finding Description

The cache worker's `process_streaming_response()` function processes streaming transaction data from fullnodes without validating that the version field of each received transaction matches the expected sequential value. [1](#0-0) 

When processing `ChunkDataOk` responses, the worker extracts transaction versions only for logging purposes but never validates them: [2](#0-1) 

The worker simply increments `current_version` by the transaction count: [3](#0-2) 

At `BatchEnd`, the only validation checks if the count matches, not the actual versions: [4](#0-3) 

When storing transactions in Redis, each transaction is written using its own version field as the cache key without validation: [5](#0-4) 

**Attack Scenario:**

1. **Batch 1**: Malicious fullnode sends transactions with versions 0-100 (101 transactions)
   - Worker writes to Redis keys: "l4:0" through "l4:100"
   - Increments `current_version`: 0 → 101
   - Sends `BatchEnd(start=0, num=101)`
   - Validation passes: `101 == 0 + 101` ✓

2. **Batch 2**: Malicious fullnode sends transactions with versions **50-150** (overlapping!)
   - Worker writes to Redis, **overwriting** keys "l4:50" through "l4:100"
   - Increments `current_version`: 101 → 202
   - Sends `BatchEnd(start=101, num=101)` ← **LIES about start_version**
   - Validation passes: `202 == 101 + 101` ✓

**Result**: Redis cache now contains corrupted data with transactions 50-100 overwritten, and the latest_version (202) is inconsistent with actual cached data (only up to 150).

**Contrast with File Store Backfiller**: The backfiller correctly validates each transaction's version: [6](#0-5) 

This validation is **absent** in the cache worker.

## Impact Explanation

**Severity: Critical**

This vulnerability qualifies as **Critical** severity under Aptos bug bounty criteria due to:

1. **State Consistency Violation**: Breaks the "State transitions must be atomic and verifiable" invariant by allowing cache corruption
2. **Data Integrity Compromise**: Indexers relying on the corrupted cache will process incorrect transaction data, leading to:
   - Incorrect balance calculations
   - Missed or duplicated transaction processing
   - Invalid state representations
   - Potential loss of funds if financial applications depend on this data
3. **System-Wide Impact**: The Redis cache is shared infrastructure serving multiple indexers, meaning one malicious fullnode can corrupt data for all downstream consumers
4. **Non-Recoverable Without Intervention**: Cache corruption requires manual intervention to detect and repair

## Likelihood Explanation

**Likelihood: High**

1. **Low Attack Complexity**: The attack only requires running a malicious fullnode that the cache worker connects to - no special cryptographic capabilities or validator access needed
2. **No Authentication**: The cache worker trusts fullnode data without cryptographic verification
3. **Automatic Propagation**: Once the cache is corrupted, the incorrect data automatically propagates to all indexers reading from it
4. **Difficult to Detect**: The validation check passes, and logs show expected behavior, making the corruption subtle and hard to notice

## Recommendation

Add per-transaction version validation in `process_transactions_from_node_response()` to ensure each transaction's version field matches the expected sequential value:

```rust
// In process_transactions_from_node_response, after extracting transactions:
Response::Data(data) => {
    let transactions = data.transactions;
    let transaction_len = transactions.len();
    
    // NEW VALIDATION: Verify each transaction version is sequential
    let first_transaction = transactions.first()
        .context("There were unexpectedly no transactions in the response")?;
    let expected_start_version = first_transaction.version;
    
    for (idx, transaction) in transactions.iter().enumerate() {
        ensure!(
            transaction.version == expected_start_version + idx as u64,
            "Transaction version mismatch: expected {}, got {} at index {}",
            expected_start_version + idx as u64,
            transaction.version,
            idx
        );
    }
    
    // Continue with existing logic...
}
```

Additionally, validate that the first transaction's version matches the expected `current_version` in the main loop before processing the chunk.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_protos::transaction::v1::Transaction;
    use futures::stream;
    
    #[tokio::test]
    async fn test_overlapping_batch_attack() {
        // Setup mock Redis connection and file store
        let mock_conn = create_mock_redis_conn();
        let file_metadata = FileStoreMetadata::new(1, 0, StorageFormat::Lz4CompressedProto);
        
        // Create malicious stream with overlapping batches
        let responses = vec![
            // Init signal
            create_init_response(0),
            
            // Batch 1: transactions 0-100
            create_data_response(create_transactions(0, 101)),
            create_batch_end_response(0, 100),
            
            // Batch 2: transactions 50-150 (OVERLAPPING!)
            create_data_response(create_transactions(50, 101)),
            create_batch_end_response(101, 201), // Malicious: lies about start
        ];
        
        let stream = stream::iter(responses.into_iter().map(Ok));
        
        // This should fail with proper validation but currently succeeds
        let result = process_streaming_response(
            mock_conn,
            StorageFormat::Lz4CompressedProto,
            file_metadata,
            stream,
        ).await;
        
        // Verify cache corruption occurred
        // Transactions 50-100 were overwritten
        // latest_version is 202 but cache only has up to 150
    }
    
    fn create_transactions(start: u64, count: u64) -> Vec<Transaction> {
        (start..start + count)
            .map(|v| Transaction { version: v, ..Default::default() })
            .collect()
    }
}
```

The vulnerability allows a malicious fullnode to corrupt the shared Redis cache infrastructure, affecting all indexers that depend on it. The fix requires validating individual transaction versions against expected sequential values, similar to how the file-store-backfiller already implements this check.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L214-224)
```rust
                let first_transaction = data
                    .transactions
                    .first()
                    .context("There were unexpectedly no transactions in the response")?;
                let first_transaction_version = first_transaction.version;
                let last_transaction = data
                    .transactions
                    .last()
                    .context("There were unexpectedly no transactions in the response")?;
                let last_transaction_version = last_transaction.version;
                let start_version = first_transaction.version;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L328-505)
```rust
async fn process_streaming_response(
    conn: redis::aio::ConnectionManager,
    cache_storage_format: StorageFormat,
    file_store_metadata: FileStoreMetadata,
    mut resp_stream: impl futures_core::Stream<Item = Result<TransactionsFromNodeResponse, tonic::Status>>
        + std::marker::Unpin,
) -> Result<()> {
    let mut tps_calculator = MovingAverage::new(10_000);
    let mut transaction_count = 0;
    // 3. Set up the cache operator with init signal.
    let init_signal = match resp_stream.next().await {
        Some(Ok(r)) => r,
        _ => {
            bail!("[Indexer Cache] Streaming error: no response.");
        },
    };
    let mut cache_operator = CacheOperator::new(conn, cache_storage_format);

    let (fullnode_chain_id, starting_version) =
        verify_fullnode_init_signal(&mut cache_operator, init_signal, file_store_metadata)
            .await
            .context("[Indexer Cache] Failed to verify init signal")?;

    let mut current_version = starting_version;
    let mut batch_start_time = std::time::Instant::now();

    let mut tasks_to_run = vec![];
    // 4. Process the streaming response.
    loop {
        let download_start_time = std::time::Instant::now();
        let received = match resp_stream.next().await {
            Some(r) => r,
            _ => {
                error!(
                    service_type = SERVICE_TYPE,
                    "[Indexer Cache] Streaming error: no response."
                );
                ERROR_COUNT.with_label_values(&["streaming_error"]).inc();
                break;
            },
        };
        // 10 batches doewnload + slowest processing& uploading task
        let received: TransactionsFromNodeResponse = match received {
            Ok(r) => r,
            Err(err) => {
                error!(
                    service_type = SERVICE_TYPE,
                    "[Indexer Cache] Streaming error: {}", err
                );
                ERROR_COUNT.with_label_values(&["streaming_error"]).inc();
                break;
            },
        };

        if received.chain_id as u64 != fullnode_chain_id as u64 {
            panic!("[Indexer Cache] Chain id mismatch happens during data streaming.");
        }

        let size_in_bytes = received.encoded_len();
        match process_transactions_from_node_response(
            received,
            &mut cache_operator,
            download_start_time,
        )
        .await
        {
            Ok(status) => match status {
                GrpcDataStatus::ChunkDataOk {
                    num_of_transactions,
                    task,
                } => {
                    current_version += num_of_transactions;
                    transaction_count += num_of_transactions;
                    tps_calculator.tick_now(num_of_transactions);

                    tasks_to_run.push(task);
                },
                GrpcDataStatus::StreamInit(new_version) => {
                    error!(
                        current_version = new_version,
                        "[Indexer Cache] Init signal received twice."
                    );
                    ERROR_COUNT.with_label_values(&["data_init_twice"]).inc();
                    break;
                },
                GrpcDataStatus::BatchEnd {
                    start_version,
                    num_of_transactions,
                } => {
                    // Handle the data multithreading.
                    let result = join_all(tasks_to_run).await;
                    if result
                        .iter()
                        .any(|r| r.is_err() || r.as_ref().unwrap().is_err())
                    {
                        error!(
                            start_version = start_version,
                            num_of_transactions = num_of_transactions,
                            "[Indexer Cache] Process transactions from fullnode failed."
                        );
                        ERROR_COUNT.with_label_values(&["response_error"]).inc();
                        panic!("Error happens when processing transactions from fullnode.");
                    }
                    // Cleanup.
                    tasks_to_run = vec![];
                    if current_version != start_version + num_of_transactions {
                        error!(
                            current_version = current_version,
                            actual_current_version = start_version + num_of_transactions,
                            "[Indexer Cache] End signal received with wrong version."
                        );
                        ERROR_COUNT
                            .with_label_values(&["data_end_wrong_version"])
                            .inc();
                        break;
                    }
                    cache_operator
                        .update_cache_latest_version(transaction_count, current_version)
                        .await
                        .context("Failed to update the latest version in the cache")?;
                    transaction_count = 0;

                    log_grpc_step(
                        SERVICE_TYPE,
                        IndexerGrpcStep::CacheWorkerBatchProcessed,
                        Some(start_version as i64),
                        Some((start_version + num_of_transactions - 1) as i64),
                        None,
                        None,
                        Some(batch_start_time.elapsed().as_secs_f64()),
                        Some(size_in_bytes),
                        Some(num_of_transactions as i64),
                        None,
                    );
                    batch_start_time = std::time::Instant::now();
                },
            },
            Err(e) => {
                error!(
                    start_version = current_version,
                    chain_id = fullnode_chain_id,
                    service_type = SERVICE_TYPE,
                    "[Indexer Cache] Process transactions from fullnode failed: {}",
                    e
                );
                ERROR_COUNT.with_label_values(&["response_error"]).inc();
                break;
            },
        }

        // Check if the file store isn't too far away
        loop {
            let file_store_version = cache_operator
                .get_file_store_latest_version()
                .await?
                .unwrap();
            if file_store_version + FILE_STORE_VERSIONS_RESERVED < current_version {
                tokio::time::sleep(std::time::Duration::from_millis(
                    CACHE_WORKER_WAIT_FOR_FILE_STORE_MS,
                ))
                .await;
                tracing::warn!(
                    current_version = current_version,
                    file_store_version = file_store_version,
                    "[Indexer Cache] File store version is behind current version too much."
                );
                WAIT_FOR_FILE_STORE_COUNTER.inc();
            } else {
                // File store is up to date, continue cache update.
                break;
            }
        }
    }

    // It is expected that we get to this point, the upstream server disconnects
    // clients after 5 minutes.
    Ok(())
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L264-278)
```rust
        for transaction in transactions {
            let version = transaction.version;
            let cache_key = CacheEntry::build_key(version, self.storage_format).to_string();
            let timestamp_in_seconds = transaction.timestamp.map_or(0, |t| t.seconds as u64);
            let cache_entry: CacheEntry =
                CacheEntry::from_transaction(transaction, self.storage_format);
            let bytes = cache_entry.into_inner();
            size_in_bytes += bytes.len();
            redis_pipeline
                .cmd("SET")
                .arg(cache_key)
                .arg(bytes)
                .arg("EX")
                .arg(get_ttl_in_seconds(timestamp_in_seconds))
                .ignore();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L194-199)
```rust
                        for (ide, t) in transactions.iter().enumerate() {
                            ensure!(
                                t.version == transactions[0].version + ide as u64,
                                "Unexpected version"
                            );
                        }
```
