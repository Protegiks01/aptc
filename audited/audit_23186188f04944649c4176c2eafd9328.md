# Audit Report

## Title
Consensus Split Vulnerability During Proposer Election Algorithm Upgrades

## Summary
Validators running different code versions compute different valid proposers for the same round when the proposer election algorithm is upgraded, causing an irrecoverable consensus split that halts the network and requires a hard fork to resolve.

## Finding Description

The Aptos consensus layer determines the valid proposer for each round using the `ProposerElection` trait. The algorithm used is specified in the on-chain `OnChainConsensusConfig` resource, which validators read during epoch transitions.

The critical vulnerability occurs when:

1. **Governance introduces a new algorithm**: A new `ProposerElectionType` enum variant is added (e.g., a V3 algorithm) or an existing algorithm's implementation is modified.

2. **On-chain config is updated**: Governance updates the consensus config to use the new algorithm variant by calling `set_for_next_epoch()` which only validates that the config bytes are non-empty. [1](#0-0) 

3. **Validators upgrade asynchronously**: During the upgrade window, some validators run new code while others remain on old versions.

4. **Deserialization divergence**: When validators process the epoch transition, old validators fail to deserialize the unknown enum variant and fall back to the default configuration with only a warning log. [2](#0-1) 

5. **Different proposer calculations**: The default configuration uses `LeaderReputation(ProposerAndVoterV2)` with hardcoded parameters. [3](#0-2)  Old validators use this default while new validators use the upgraded algorithm, causing them to compute DIFFERENT valid proposers for the same round.

6. **Proposal rejection cascade**: The system validates that proposals come from the expected proposer by checking if the author matches `get_valid_proposer(round)`. [4](#0-3)  Proposals from the "wrong" proposer are rejected with a security warning. [5](#0-4)  This validation occurs in `process_proposal()`. [6](#0-5) 

7. **Network partition**: The network splits into incompatible groups where neither can form a 2f+1 quorum because each group rejects the other's proposals as coming from an invalid proposer.

**This vulnerability also applies to implementation changes**: If the formula in `RotatingProposer::get_valid_proposer()` is modified, [7](#0-6)  or if the weighted selection logic in `LeaderReputation::get_valid_proposer_and_voting_power_participation_ratio()` changes, [8](#0-7)  validators running different code versions will disagree on the valid proposer even without new enum variants.

## Impact Explanation

This qualifies as **Critical Severity** under the Aptos Bug Bounty program:

1. **Non-recoverable network partition**: Once validators split into groups computing different valid proposers, the network cannot self-heal. Each group rejects the other's proposals, preventing consensus convergence. Recovery requires coordinating all validators to upgrade to compatible versions and potentially rolling back to a common ancestor blockâ€”constituting a hard fork.

2. **Total loss of liveness**: During the partition, no new blocks can be committed as neither group can achieve quorum with the full validator set. This freezes all on-chain activity including transactions, governance, staking operations, and asset transfers.

3. **Breaks consensus safety invariant**: The fundamental guarantee that all honest validators agree on the canonical chain is violated when different validators use incompatible proposer election algorithms.

## Likelihood Explanation

**Likelihood: HIGH**

1. **Normal upgrade process**: Algorithm upgrades are expected as evidenced by the existing transition from `ProposerAndVoter` to `ProposerAndVoterV2`. [9](#0-8) 

2. **No version checks**: The codebase lacks any compatibility verification between the on-chain config and validator code versions. Governance can set any byte sequence as the config. [1](#0-0) 

3. **Silent failure mode**: Old validators silently fall back to defaults with only a warning log, making the incompatibility invisible until consensus fails.

4. **Asynchronous upgrades**: Validators upgrade at different times based on operator schedules, creating an inevitable window where different versions coexist.

5. **Multiple trigger paths**: The vulnerability can be triggered by adding new enum variants OR by modifying existing algorithm implementations, making it even more likely to occur during routine development.

## Recommendation

Implement version compatibility checking during epoch transitions:

1. **Add version field to OnChainConsensusConfig**: Include a minimum required validator code version in the consensus config struct.

2. **Enforce version checks**: Before using the deserialized config, validate that the current validator version supports all features in the config. If not, halt the validator with a clear error message instead of silently falling back to defaults.

3. **Governance validation**: Add a native function that validates new consensus configs can be deserialized by the current validator set before allowing governance to set them on-chain.

4. **Graceful degradation**: For algorithm implementation changes, use feature flags tied to epoch numbers to ensure all validators switch to the new implementation simultaneously at a predetermined epoch boundary.

## Proof of Concept

While a complete PoC would require deploying test validators, the vulnerability mechanism is clearly demonstrated in the code paths:

1. Governance sets a config with an unknown `ProposerElectionType` variant
2. Old validators fail deserialization at [10](#0-9)  and fall back to default at [11](#0-10) 
3. New validators successfully create proposer election via [12](#0-11) 
4. For any round, the two groups compute different proposers, causing mutual proposal rejection at [6](#0-5) 

## Notes

This vulnerability represents a critical gap in the protocol's upgrade coordination mechanism. The existence of versioned algorithms (`ProposerAndVoterV2`) confirms that such transitions are part of the protocol's evolution, making this not a theoretical concern but an imminent risk during the next algorithm upgrade. The lack of any safeguards against incompatible configurations being applied during rolling upgrades is a fundamental protocol safety issue.

### Citations

**File:** aptos-move/framework/aptos-framework/sources/configs/consensus_config.move (L52-55)
```text
    public fun set_for_next_epoch(account: &signer, config: vector<u8>) {
        system_addresses::assert_aptos_framework(account);
        assert!(vector::length(&config) > 0, error::invalid_argument(EINVALID_CONFIG));
        std::config_buffer::upsert<ConsensusConfig>(ConsensusConfig {config});
```

**File:** consensus/src/epoch_manager.rs (L287-296)
```rust
    fn create_proposer_election(
        &self,
        epoch_state: &EpochState,
        onchain_config: &OnChainConsensusConfig,
    ) -> Arc<dyn ProposerElection + Send + Sync> {
        let proposers = epoch_state
            .verifier
            .get_ordered_account_addresses_iter()
            .collect::<Vec<_>>();
        match &onchain_config.proposer_election_type() {
```

**File:** consensus/src/epoch_manager.rs (L1178-1201)
```rust
        let onchain_consensus_config: anyhow::Result<OnChainConsensusConfig> = payload.get();
        let onchain_execution_config: anyhow::Result<OnChainExecutionConfig> = payload.get();
        let onchain_randomness_config_seq_num: anyhow::Result<RandomnessConfigSeqNum> =
            payload.get();
        let randomness_config_move_struct: anyhow::Result<RandomnessConfigMoveStruct> =
            payload.get();
        let onchain_jwk_consensus_config: anyhow::Result<OnChainJWKConsensusConfig> = payload.get();
        let dkg_state = payload.get::<DKGState>();

        if let Err(error) = &onchain_consensus_config {
            warn!("Failed to read on-chain consensus config {}", error);
        }

        if let Err(error) = &onchain_execution_config {
            warn!("Failed to read on-chain execution config {}", error);
        }

        if let Err(error) = &randomness_config_move_struct {
            warn!("Failed to read on-chain randomness config {}", error);
        }

        self.epoch_state = Some(epoch_state.clone());

        let consensus_config = onchain_consensus_config.unwrap_or_default();
```

**File:** types/src/on_chain_config/consensus_config.rs (L481-505)
```rust
impl Default for ConsensusConfigV1 {
    fn default() -> Self {
        Self {
            decoupled_execution: true,
            back_pressure_limit: 10,
            exclude_round: 40,
            max_failed_authors_to_store: 10,
            proposer_election_type: ProposerElectionType::LeaderReputation(
                LeaderReputationType::ProposerAndVoterV2(ProposerAndVoterConfig {
                    active_weight: 1000,
                    inactive_weight: 10,
                    failed_weight: 1,
                    failure_threshold_percent: 10, // = 10%
                    // In each round we get stastics for the single proposer
                    // and large number of validators. So the window for
                    // the proposers needs to be significantly larger
                    // to have enough useful statistics.
                    proposer_window_num_validators_multiplier: 10,
                    voter_window_num_validators_multiplier: 1,
                    weight_by_voting_power: true,
                    use_history_from_previous_epoch_max_count: 5,
                }),
            ),
        }
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L527-538)
```rust
pub enum LeaderReputationType {
    // Proposer election based on whether nodes succeeded or failed
    // their proposer election rounds, and whether they voted.
    // Version 1:
    // * use reputation window from stale end
    // * simple (predictable) seed
    ProposerAndVoter(ProposerAndVoterConfig),
    // Version 2:
    // * use reputation window from recent end
    // * unpredictable seed, based on root hash
    ProposerAndVoterV2(ProposerAndVoterConfig),
}
```

**File:** consensus/src/liveness/proposer_election.rs (L14-15)
```rust
    fn is_valid_proposer(&self, author: Author, round: Round) -> bool {
        self.get_valid_proposer(round) == author
```

**File:** consensus/src/liveness/unequivocal_proposer_election.rs (L46-60)
```rust
    pub fn is_valid_proposal(&self, block: &Block) -> bool {
        block.author().is_some_and(|author| {
            let valid_author = self.is_valid_proposer(author, block.round());
            if !valid_author {
                warn!(
                    SecurityEvent::InvalidConsensusProposal,
                    "Proposal is not from valid author {}, expected {} for round {} and id {}",
                    author,
                    self.get_valid_proposer(block.round()),
                    block.round(),
                    block.id()
                );

                return false;
            }
```

**File:** consensus/src/round_manager.rs (L1195-1200)
```rust
        ensure!(
            self.proposer_election.is_valid_proposal(&proposal),
            "[RoundManager] Proposer {} for block {} is not a valid proposer for this round or created duplicate proposal",
            author,
            proposal,
        );
```

**File:** consensus/src/liveness/rotating_proposer_election.rs (L36-38)
```rust
    fn get_valid_proposer(&self, round: Round) -> Author {
        self.proposers
            [((round / u64::from(self.contiguous_rounds)) % self.proposers.len() as u64) as usize]
```

**File:** consensus/src/liveness/leader_reputation.rs (L696-739)
```rust
    fn get_valid_proposer_and_voting_power_participation_ratio(
        &self,
        round: Round,
    ) -> (Author, VotingPowerRatio) {
        let target_round = round.saturating_sub(self.exclude_round);
        let (sliding_window, root_hash) = self.backend.get_block_metadata(self.epoch, target_round);
        let voting_power_participation_ratio =
            self.compute_chain_health_and_add_metrics(&sliding_window, round);
        let mut weights =
            self.heuristic
                .get_weights(self.epoch, &self.epoch_to_proposers, &sliding_window);
        let proposers = &self.epoch_to_proposers[&self.epoch];
        assert_eq!(weights.len(), proposers.len());

        // Multiply weights by voting power:
        let stake_weights: Vec<u128> = weights
            .iter_mut()
            .enumerate()
            .map(|(i, w)| *w as u128 * self.voting_powers[i] as u128)
            .collect();

        let state = if self.use_root_hash {
            [
                root_hash.to_vec(),
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        } else {
            [
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        };

        let chosen_index = choose_index(stake_weights, state);
        (proposers[chosen_index], voting_power_participation_ratio)
    }

    fn get_valid_proposer(&self, round: Round) -> Author {
        self.get_valid_proposer_and_voting_power_participation_ratio(round)
            .0
    }
```
