# Audit Report

## Title
Memory Ordering Relaxation in Ledger Pruner Allows Stale Reads Leading to Inconsistent Data Availability Across Nodes

## Summary
Relaxing the memory ordering in `get_min_readable_version()` from `SeqCst` to `Acquire` breaks sequential consistency guarantees, allowing different reader threads to observe different values of `min_readable_version` simultaneously. This causes API nodes to give inconsistent responses about historical data availability and can lead to read failures when clients attempt to access already-pruned data that stale checks incorrectly indicated as available.

## Finding Description

The vulnerability exists in the interaction between the pruner target-setting mechanism and reader threads checking data availability. The current implementation uses `Ordering::SeqCst` for both storing and loading `min_readable_version`, which provides total sequential consistency across all threads. [1](#0-0) [2](#0-1) 

The critical issue arises because:

1. **Optimistic Update**: `set_pruner_target_db_version()` updates `min_readable_version` with `SeqCst` BEFORE the pruner worker actually deletes the data. This is an optimistic update indicating the target state, not the completed state. [3](#0-2) 

2. **Concurrent Pruning**: The pruner worker runs asynchronously in a separate thread and physically deletes database records. [4](#0-3) 

3. **Data Availability Checks**: Readers check `min_readable_version` to determine if data is available before attempting database access. [5](#0-4) [6](#0-5) 

**The Race Condition with Acquire Ordering:**

With `SeqCst`, all threads observe a total sequential order of all `SeqCst` operations. If Reader Thread A sees `min_readable_version = 9000`, then Reader Thread B's subsequent load MUST see at least 9000 or later.

With `Acquire` on the load, this total ordering guarantee is lost. Thread synchronization becomes pairwise rather than global. This enables the following race:

```
Initial: min_readable_version = 0, DB contains versions 0-9999

Timeline:
T1: Pruner setter stores min_readable_version = 9000 (SeqCst)
T2: Pruner worker begins deleting versions < 9000
T3: Reader A loads min_readable_version (SeqCst) → sees 9000
T4: Pruner worker deletes version 5000 from database
T5: Reader B loads min_readable_version (Acquire) → sees 0 (stale!)
T6: Reader A checks version 5000: 5000 < 9000 → Returns "Data Not Available"
T7: Reader B checks version 5000: 5000 >= 0 → Returns "Data Available"
T8: Client queries Reader B for version 5000
T9: Reader B attempts database read → NotFound (already pruned at T4)
T10: Reader B crashes or returns error to client
```

The `Acquire` ordering only guarantees synchronization with the specific store it observes, not a total order across all threads. Different API nodes can see different values of `min_readable_version` at the same logical time, causing:

- **Inconsistent API responses**: Different nodes report different data availability
- **Read failures**: Stale checks pass but actual data is pruned, causing `AptosDbError::NotFound`
- **Node crashes**: Unexpected database errors may cause panic in error handling paths

This violates the **State Consistency** invariant: "State transitions must be atomic and verifiable" - different validators/API nodes must have consistent views of data availability.

## Impact Explanation

**HIGH SEVERITY** per Aptos bug bounty criteria:

1. **API Crashes**: When stale `min_readable_version` checks pass but the actual database read fails, this causes unexpected `NotFound` errors that can crash API nodes or validators attempting to serve historical queries.

2. **Significant Protocol Violations**: The storage layer's consistency guarantees are broken. Different nodes give contradictory responses about the same version's availability, violating the deterministic execution invariant where all nodes should observe the same state.

3. **State Sync Failures**: During state synchronization, if a syncing node queries historical transactions based on a stale `min_readable_version`, the sync process will fail when the actual read returns `NotFound`, potentially preventing nodes from joining the network.

The issue does not reach Critical severity because:
- It doesn't directly cause consensus safety violations
- It doesn't enable fund theft or permanent data loss
- The network can recover by retrying with properly synchronized nodes

However, it significantly degrades system reliability and can cause cascading failures across the API layer.

## Likelihood Explanation

**HIGH LIKELIHOOD** in production environments:

1. **Continuous Pruning**: Mainnet continuously prunes historical data, triggering frequent updates to `min_readable_version`

2. **Multi-threaded Access**: API nodes handle concurrent client requests across multiple threads, each potentially loading `min_readable_version` independently

3. **High Query Load**: Popular API endpoints query historical transactions frequently (e.g., block explorers, indexers), creating numerous opportunities for the race condition

4. **No External Prerequisites**: The vulnerability requires no attacker action - it occurs naturally under normal operation if the ordering is relaxed

5. **Timing Window**: The window between optimistic `min_readable_version` update and actual data deletion is significant (batch pruning of thousands of versions), making races highly probable

## Recommendation

**DO NOT relax the memory ordering**. The `SeqCst` ordering is necessary for correctness.

The current implementation is correct:

```rust
fn get_min_readable_version(&self) -> Version {
    self.min_readable_version.load(Ordering::SeqCst)
}
```

**Why SeqCst is Required:**

1. **Total Ordering**: `SeqCst` provides a total sequential order across all threads, ensuring all readers observe the same value at any logical point
2. **Cross-Thread Consistency**: Multiple API nodes/threads must give consistent answers about data availability
3. **Synchronization with Pruner**: The pruner updates `min_readable_version` optimistically before deletion; readers must see these updates in a globally consistent order

**Alternative Improvements** (if performance is a concern):

1. **Deferred Update**: Only update `min_readable_version` AFTER pruning completes, not before:
```rust
// In pruner completion callback
fn on_prune_complete(&self, completed_version: Version) {
    self.min_readable_version.store(completed_version, Ordering::SeqCst);
}
```

2. **Read-Validate-Read Pattern**: Add a second check after database read attempt to detect races:
```rust
fn read_with_validation(&self, version: Version) -> Result<Data> {
    let min_v1 = self.get_min_readable_version();
    ensure!(version >= min_v1, "Version pruned");
    let data = self.db.read(version)?;
    let min_v2 = self.get_min_readable_version();
    ensure!(version >= min_v2, "Version pruned during read");
    Ok(data)
}
```

However, these alternatives add complexity. The current `SeqCst` approach is simple and correct - **it should be preserved**.

## Proof of Concept

```rust
// Proof of concept demonstrating the race condition
// This test would be added to storage/aptosdb/src/pruner/ledger_pruner/test.rs

#[test]
fn test_memory_ordering_race_with_acquire() {
    use std::sync::Arc;
    use std::thread;
    use std::sync::atomic::{AtomicU64, Ordering};
    
    // Simulate the min_readable_version atomic
    let min_readable = Arc::new(AtomicU64::new(0));
    
    // Simulate pruned data set (versions that have been deleted)
    let pruned_data = Arc::new(std::sync::Mutex::new(std::collections::HashSet::new()));
    
    let min_readable_writer = Arc::clone(&min_readable);
    let pruned_data_writer = Arc::clone(&pruned_data);
    
    // Thread 1: Pruner setter and worker
    let pruner = thread::spawn(move || {
        // Set target optimistically
        min_readable_writer.store(9000, Ordering::SeqCst);
        thread::sleep(std::time::Duration::from_micros(10));
        
        // Simulate actual pruning (deletes data)
        for v in 0..9000 {
            pruned_data_writer.lock().unwrap().insert(v);
        }
    });
    
    // Thread 2: Reader with SeqCst (correct behavior)
    let min_readable_reader1 = Arc::clone(&min_readable);
    let reader1 = thread::spawn(move || {
        thread::sleep(std::time::Duration::from_micros(15));
        let min_v = min_readable_reader1.load(Ordering::SeqCst);
        assert!(min_v == 9000, "SeqCst sees updated value");
    });
    
    // Thread 3: Reader with Acquire (potential stale read)
    let min_readable_reader2 = Arc::clone(&min_readable);
    let pruned_data_reader = Arc::clone(&pruned_data);
    let reader2 = thread::spawn(move || {
        thread::sleep(std::time::Duration::from_micros(15));
        
        // With Acquire instead of SeqCst, this could see stale value
        // (In reality, this test may pass due to hardware ordering,
        //  but the specification allows stale reads with Acquire)
        let min_v = min_readable_reader2.load(Ordering::Acquire);
        
        let test_version = 5000;
        if test_version >= min_v {
            // Check passes, but data might be pruned
            let is_pruned = pruned_data_reader.lock().unwrap().contains(&test_version);
            
            if is_pruned {
                // VULNERABILITY: Check passed but data is pruned!
                panic!("Stale min_readable_version allowed read of pruned data");
            }
        }
    });
    
    pruner.join().unwrap();
    reader1.join().unwrap();
    
    // Reader2 may or may not panic depending on timing and hardware
    // On architectures with weak memory ordering, this demonstrates the race
    let _ = reader2.join();
}
```

**Notes:**
- The PoC demonstrates that `Acquire` ordering can allow stale reads in a multi-threaded scenario
- On x86 architectures with strong memory ordering, the race may be harder to reproduce, but the C++/Rust memory model specification explicitly allows it
- The production impact is more severe because database operations add additional latency, widening the race window
- The current `SeqCst` implementation is correct and should NOT be changed to `Acquire`

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L48-50)
```rust
    fn get_min_readable_version(&self) -> Version {
        self.min_readable_version.load(Ordering::SeqCst)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L162-176)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L261-271)
```rust
    pub(super) fn error_if_ledger_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.ledger_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L1074-1074)
```rust
        self.error_if_ledger_pruned("Transaction", version)?;
```
