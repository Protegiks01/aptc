# Audit Report

## Title
Unbounded Memory Allocation in Admin Service BCS Block Dump Leading to Node Crashes

## Summary
The `dump_blocks_bcs()` function in the admin service performs unbounded memory allocation when serializing all blocks and transactions to BCS format, allowing an unauthenticated attacker on testnet/devnet (or authenticated attacker on mainnet) to crash validator nodes through memory exhaustion.

## Finding Description

The vulnerability exists in the `dump_blocks_bcs()` function which is exposed via the HTTP endpoint `/debug/consensus/block?bcs=true`. [1](#0-0) 

When invoked without a `block_id` parameter, the function:

1. **Retrieves ALL blocks from ConsensusDB** - The function calls `consensus_db.consensus_db().get_data()` which returns every block currently stored in the database via `get_all::<BlockSchema>()`. [2](#0-1) 

2. **Creates an unbounded vector** - An unbounded `Vec::new()` is allocated to store all transactions from all blocks with no size limits or pagination.

3. **Clones all transactions** - For each block, the function extracts transactions and uses `.cloned()` to duplicate every transaction into the vector, doubling memory consumption.

4. **Serializes entire dataset at once** - The complete vector is serialized to BCS format in a single allocation via `bcs::to_bytes(&all_txns)`.

**Attack Path:**
On testnet/devnet networks, the admin service is **enabled by default** without authentication. [3](#0-2) 

An attacker only needs to send: `GET http://[validator-ip]:9102/debug/consensus/block?bcs=true`

The server processes this request in a blocking thread [4](#0-3)  and attempts to load all blocks into memory.

**Memory Consumption Calculation:**
- Consensus DB can contain 100+ unpruned blocks during normal operation
- Each block can contain up to 10,000 transactions (per receiving limits) [5](#0-4) 
- Average transaction size: ~500 bytes (conservative estimate)
- **Total memory**: 100 blocks × 10,000 txns × 500 bytes × 2 (cloning + serialization) = **1 GB minimum**
- During periods of delayed commits or network issues, far more blocks accumulate, exponentially increasing memory consumption

This breaks the critical invariant: **"Resource Limits: All operations must respect gas, storage, and computational limits"**

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

1. **Validator Node Crashes** - Memory exhaustion causes OOM (Out-Of-Memory) kills, crashing validator processes
2. **API Unavailability** - The admin service and potentially other node services become unresponsive
3. **Consensus Disruption** - If multiple validators are targeted simultaneously, consensus performance degrades

The vulnerability qualifies as "Validator node slowdowns" and "API crashes" under High Severity. While it doesn't cause permanent network partition or consensus safety violations, it enables targeted DoS attacks against validator infrastructure.

On production mainnet, while authentication is required [6](#0-5) , if authentication credentials are compromised (via phishing, insider threat, or leaked passcodes), the same attack vector applies.

## Likelihood Explanation

**High Likelihood** on testnet/devnet:
- Admin service enabled by default with no authentication [7](#0-6) 
- Simple HTTP GET request requires no special privileges
- Endpoint is well-documented and discoverable
- No rate limiting or request throttling observed in the code

**Medium Likelihood** on mainnet:
- Requires compromised authentication (passcode SHA256)
- Admin service typically disabled but can be enabled
- Still exploitable if authentication is weak or leaked

The attack requires:
- Network access to validator's admin port (9102)
- Knowledge of the endpoint (publicly documented)
- For mainnet: compromised authentication credentials

No special skills, tools, or insider access required beyond basic HTTP requests.

## Recommendation

Implement multiple layers of protection:

1. **Add Pagination Support:**
```rust
fn dump_blocks_bcs(
    consensus_db: &dyn PersistentLivenessStorage,
    quorum_store_db: &dyn QuorumStoreStorage,
    block_id: Option<HashValue>,
    max_blocks: Option<usize>, // Add limit parameter
) -> anyhow::Result<Vec<u8>> {
    let all_batches = quorum_store_db.get_all_batches()?;
    let (_, _, blocks, _) = consensus_db.consensus_db().get_data()?;
    
    let mut all_txns = Vec::new();
    let mut block_count = 0;
    let max_blocks = max_blocks.unwrap_or(10); // Default limit
    
    for block in blocks {
        if block_count >= max_blocks {
            break; // Enforce hard limit
        }
        
        let id = block.id();
        if block_id.is_none() || id == block_id.unwrap() {
            // Add size check before extending
            if all_txns.len() > MAX_TRANSACTIONS_PER_DUMP {
                bail!("Transaction limit exceeded");
            }
            
            match extract_txns_from_block(&block, &all_batches) {
                Ok(txns) => {
                    all_txns.extend(txns.into_iter().cloned().map(Transaction::UserTransaction));
                },
                Err(e) => bail!("Failed to extract txns from block ({id:?}): {e:?}."),
            };
            block_count += 1;
        }
    }
    
    bcs::to_bytes(&all_txns).map_err(Error::msg)
}
```

2. **Require `block_id` Parameter:**
Modify the endpoint to reject requests without a specific `block_id`, preventing bulk dumps entirely.

3. **Enforce Authentication on All Networks:**
Remove the exception for testnet/devnet and require authentication universally.

4. **Add Memory Budget Checks:**
Track allocated memory and abort if exceeding a predefined threshold (e.g., 100 MB).

5. **Stream Response:**
Instead of buffering everything in memory, stream blocks incrementally to the HTTP response.

## Proof of Concept

**Step 1: Set up testnet validator with default admin service config**

**Step 2: Send malicious request:**
```bash
# From attacker machine
curl -v "http://[testnet-validator-ip]:9102/debug/consensus/block?bcs=true"

# Monitor validator node memory usage - will spike and potentially crash
# Check validator logs for OOM errors
```

**Step 3: Reproduce in Rust integration test:**
```rust
#[tokio::test]
async fn test_unbounded_bcs_dump_dos() {
    // Setup: Create consensus DB with many blocks
    let temp_dir = TempPath::new();
    let consensus_db = create_consensus_db_with_blocks(&temp_dir, 100); // 100 blocks
    let quorum_store_db = create_quorum_store_db(&temp_dir);
    
    // Populate each block with maximum transactions
    for i in 0..100 {
        let block = create_test_block_with_max_txns(i);
        consensus_db.save_blocks_and_quorum_certificates(vec![block], vec![]).unwrap();
    }
    
    // Attack: Call dump_blocks_bcs without block_id
    let result = dump_blocks_bcs(
        consensus_db.as_ref(),
        quorum_store_db.as_ref(),
        None, // No block_id = dump ALL blocks
    );
    
    // Expected: Memory exhaustion or very large allocation
    // Actual: System attempts to allocate gigabytes of memory
    match result {
        Ok(data) => {
            println!("BCS dump size: {} MB", data.len() / 1_000_000);
            assert!(data.len() > 500_000_000); // > 500 MB = DoS vector
        },
        Err(e) => {
            println!("OOM error: {}", e); // Memory allocation failed
        }
    }
}
```

**Expected Result:** The validator process consumes excessive memory (1+ GB), causing system slowdown or OOM crash. On resource-constrained validators, this leads to immediate process termination.

**Notes**

This vulnerability specifically affects the admin service debug endpoints, which are primarily used for operational debugging and monitoring. However, because these endpoints are enabled by default on testnet/devnet without authentication, they present a realistic attack surface for malicious actors targeting Aptos network infrastructure.

The issue is exacerbated during periods of consensus delays or network partitioning, when more blocks accumulate in the ConsensusDB before pruning occurs. An attacker can strategically time requests during such periods to maximize memory consumption.

While the admin service is disabled on mainnet by default, operators who enable it for debugging purposes with weak authentication remain vulnerable. The combination of unbounded allocation, lack of pagination, and default unauthenticated access creates a critical DoS attack vector against Aptos validator nodes.

### Citations

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L106-113)
```rust
    match spawn_blocking(move || {
        if bcs {
            dump_blocks_bcs(consensus_db.as_ref(), quorum_store_db.as_ref(), block_id)
                .map(Into::<Body>::into)
        } else {
            dump_blocks(consensus_db.as_ref(), quorum_store_db.as_ref(), block_id).map(Into::into)
        }
    })
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L217-240)
```rust
fn dump_blocks_bcs(
    consensus_db: &dyn PersistentLivenessStorage,
    quorum_store_db: &dyn QuorumStoreStorage,
    block_id: Option<HashValue>,
) -> anyhow::Result<Vec<u8>> {
    let all_batches = quorum_store_db.get_all_batches()?;

    let (_, _, blocks, _) = consensus_db.consensus_db().get_data()?;

    let mut all_txns = Vec::new();
    for block in blocks {
        let id = block.id();
        if block_id.is_none() || id == block_id.unwrap() {
            match extract_txns_from_block(&block, &all_batches) {
                Ok(txns) => {
                    all_txns.extend(txns.into_iter().cloned().map(Transaction::UserTransaction));
                },
                Err(e) => bail!("Failed to extract txns from block ({id:?}): {e:?}."),
            };
        }
    }

    bcs::to_bytes(&all_txns).map_err(Error::msg)
}
```

**File:** consensus/src/consensusdb/mod.rs (L80-106)
```rust
    pub fn get_data(
        &self,
    ) -> Result<(
        Option<Vec<u8>>,
        Option<Vec<u8>>,
        Vec<Block>,
        Vec<QuorumCert>,
    )> {
        let last_vote = self.get_last_vote()?;
        let highest_2chain_timeout_certificate = self.get_highest_2chain_timeout_certificate()?;
        let consensus_blocks = self
            .get_all::<BlockSchema>()?
            .into_iter()
            .map(|(_, block)| block)
            .collect();
        let consensus_qcs = self
            .get_all::<QCSchema>()?
            .into_iter()
            .map(|(_, qc)| qc)
            .collect();
        Ok((
            last_vote,
            highest_2chain_timeout_certificate,
            consensus_blocks,
            consensus_qcs,
        ))
    }
```

**File:** config/src/config/admin_service_config.rs (L41-50)
```rust
impl Default for AdminServiceConfig {
    fn default() -> Self {
        Self {
            enabled: None,
            address: "0.0.0.0".to_string(),
            port: 9102,
            authentication_configs: vec![],
            malloc_stats_max_len: 2 * 1024 * 1024,
        }
    }
```

**File:** config/src/config/admin_service_config.rs (L59-82)
```rust
impl ConfigSanitizer for AdminServiceConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        if node_config.admin_service.enabled == Some(true) {
            if let Some(chain_id) = chain_id {
                if chain_id.is_mainnet()
                    && node_config.admin_service.authentication_configs.is_empty()
                {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "Must enable authentication for AdminService on mainnet.".into(),
                    ));
                }
            }
        }

        Ok(())
    }
}
```

**File:** config/src/config/admin_service_config.rs (L84-106)
```rust
impl ConfigOptimizer for AdminServiceConfig {
    fn optimize(
        node_config: &mut NodeConfig,
        _local_config_yaml: &Value,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<bool, Error> {
        let mut modified_config = false;

        if node_config.admin_service.enabled.is_none() {
            // Only enable the admin service if the chain is not mainnet
            let admin_service_enabled = if let Some(chain_id) = chain_id {
                !chain_id.is_mainnet()
            } else {
                false // We cannot determine the chain ID, so we disable the admin service
            };
            node_config.admin_service.enabled = Some(admin_service_enabled);

            modified_config = true; // The config was modified
        }

        Ok(modified_config)
    }
```

**File:** config/src/config/consensus_config.rs (L1-100)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

#![allow(unexpected_cfgs)]

use super::DEFEAULT_MAX_BATCH_TXNS;
use crate::config::{
    config_optimizer::ConfigOptimizer, config_sanitizer::ConfigSanitizer,
    node_config_loader::NodeType, Error, NodeConfig, QuorumStoreConfig, ReliableBroadcastConfig,
    SafetyRulesConfig, BATCH_PADDING_BYTES,
};
use aptos_crypto::_once_cell::sync::Lazy;
use aptos_types::chain_id::ChainId;
use cfg_if::cfg_if;
use serde::{Deserialize, Serialize};
use serde_yaml::Value;
use std::path::PathBuf;

// NOTE: when changing, make sure to update QuorumStoreBackPressureConfig::backlog_txn_limit_count as well.
const MAX_SENDING_BLOCK_TXNS_AFTER_FILTERING: u64 = 1800;
const MAX_SENDING_OPT_BLOCK_TXNS_AFTER_FILTERING: u64 = 1000;
const MAX_SENDING_BLOCK_TXNS: u64 = 5000;
pub(crate) static MAX_RECEIVING_BLOCK_TXNS: Lazy<u64> =
    Lazy::new(|| 10000.max(2 * MAX_SENDING_BLOCK_TXNS));
// stop reducing size at this point, so 1MB transactions can still go through
const MIN_BLOCK_BYTES_OVERRIDE: u64 = 1024 * 1024 + BATCH_PADDING_BYTES as u64;
// We should reduce block size only until two QS batch sizes.
const MIN_BLOCK_TXNS_AFTER_FILTERING: u64 = DEFEAULT_MAX_BATCH_TXNS as u64 * 2;

#[derive(Clone, Debug, Deserialize, PartialEq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct ConsensusConfig {
    // length of inbound queue of messages
    pub max_network_channel_size: usize,
    pub max_sending_block_txns: u64,
    pub max_sending_block_txns_after_filtering: u64,
    pub max_sending_opt_block_txns_after_filtering: u64,
    pub max_sending_block_bytes: u64,
    pub max_sending_inline_txns: u64,
    pub max_sending_inline_bytes: u64,
    pub max_receiving_block_txns: u64,
    pub max_receiving_block_bytes: u64,
    pub max_pruned_blocks_in_mem: usize,
    // Timeout for consensus to get an ack from mempool for executed transactions (in milliseconds)
    pub mempool_executed_txn_timeout_ms: u64,
    // Timeout for consensus to pull transactions from mempool and get a response (in milliseconds)
    pub mempool_txn_pull_timeout_ms: u64,
    pub round_initial_timeout_ms: u64,
    pub round_timeout_backoff_exponent_base: f64,
    pub round_timeout_backoff_max_exponent: usize,
    pub safety_rules: SafetyRulesConfig,
    // Only sync committed transactions but not vote for any pending blocks. This is useful when
    // validators coordinate on the latest version to apply a manual transaction.
    pub sync_only: bool,
    // The size of the round/recovery manager and proposal buffer channels.
    pub internal_per_key_channel_size: usize,
    pub quorum_store_pull_timeout_ms: u64,
    // Decides how long the leader waits before proposing empty block if there's no txns in mempool
    pub quorum_store_poll_time_ms: u64,
    // Whether to create partial blocks when few transactions exist, or empty blocks when there is
    // pending ordering, or to wait for quorum_store_poll_count * 30ms to collect transactions for a block
    //
    // It is more efficient to execute larger blocks, as it creates less overhead. On the other hand
    // waiting increases latency (unless we are under high load that added waiting latency
    // is compensated by faster execution time). So we want to balance the two, by waiting only
    // when we are saturating the execution pipeline:
    // - if there are more pending blocks then usual in the execution pipeline,
    //   block is going to wait there anyways, so we can wait to create a bigger/more efificent block
    // - in case our node is faster than others, and we don't have many pending blocks,
    //   but we still see very large recent (pending) blocks, we know that there is demand
    //   and others are creating large blocks, so we can wait as well.
    pub wait_for_full_blocks_above_pending_blocks: usize,
    pub wait_for_full_blocks_above_recent_fill_threshold: f32,
    pub intra_consensus_channel_buffer_size: usize,
    pub quorum_store: QuorumStoreConfig,
    pub vote_back_pressure_limit: u64,
    /// If backpressure target block size is below it, update `max_txns_to_execute` instead.
    /// Applied to execution, pipeline and chain health backpressure.
    /// Needed as we cannot subsplit QS batches.
    pub min_max_txns_in_block_after_filtering_from_backpressure: u64,
    pub execution_backpressure: Option<ExecutionBackpressureConfig>,
    pub pipeline_backpressure: Vec<PipelineBackpressureValues>,
    // Used to decide if backoff is needed.
    // must match one of the CHAIN_HEALTH_WINDOW_SIZES values.
    pub window_for_chain_health: usize,
    pub chain_health_backoff: Vec<ChainHealthBackoffValues>,
    // Deprecated
    pub qc_aggregator_type: QcAggregatorType,
    // Max blocks allowed for block retrieval requests
    pub max_blocks_per_sending_request: u64,
    pub max_blocks_per_sending_request_quorum_store_override: u64,
    pub max_blocks_per_receiving_request: u64,
    pub max_blocks_per_receiving_request_quorum_store_override: u64,
    pub broadcast_vote: bool,
    pub proof_cache_capacity: u64,
    pub rand_rb_config: ReliableBroadcastConfig,
    pub num_bounded_executor_tasks: u64,
    pub enable_pre_commit: bool,
    pub max_pending_rounds_in_commit_vote_cache: u64,
    pub optimistic_sig_verification: bool,
```
