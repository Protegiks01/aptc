# Audit Report

## Title
Decompression Bomb Memory Exhaustion in Backup Restore via Unbounded Record Size Allocation

## Summary
The `read_record_bytes()` function in the backup-cli component allocates memory based on an untrusted 4-byte size header without any upper bound validation. When combined with compressed backup storage (via gzip in CommandAdapter), this enables decompression bomb attacks where small malicious backup files can expand to gigabytes during restore, exhausting validator memory and causing node crashes.

## Finding Description

The vulnerability exists in the backup restore process where compressed backup data flows through an unsafe deserialization path: [1](#0-0) 

The `read_record_bytes()` function reads a 4-byte size header and directly allocates a buffer of that size without validation. Since the size is a u32, this allows allocation of up to 4GB per record.

During backup restore operations with compressed storage, the attack flow is:

1. **Backup Storage Configuration**: The CommandAdapter uses shell commands for storage operations. Sample configurations show gzip compression: [2](#0-1) 

2. **Decompression During Restore**: When opening files for read, the `gzip -cd` command decompresses data before it reaches `read_record_bytes()`.

3. **Unsafe Allocation**: The restore process uses this function to read transaction records: [3](#0-2) 

And state snapshot records: [4](#0-3) 

**Attack Scenario**: An attacker with write access to backup storage (or who can provide malicious backups) creates a file containing:
- Compressed size: ~10KB (highly compressible data)
- Decompressed content: Malicious 4-byte size headers claiming 4GB per record
- When `gzip -cd` decompresses and `read_record_bytes()` processes it, each record triggers 4GB allocation
- Multiple such records exhaust validator memory

This breaks **Invariant #9 (Resource Limits)**: "All operations must respect gas, storage, and computational limits." While legitimate state values are bounded by protocol limits (1MB per state item), the backup restore process does not enforce these bounds. [5](#0-4) 

## Impact Explanation

**Severity: HIGH** (up to $50,000 per Aptos Bug Bounty)

This qualifies as "Validator node slowdowns" and "API crashes" under High severity criteria:

- **Validator Availability Impact**: During critical restore operations (disaster recovery, new validator onboarding), malicious backups cause memory exhaustion and node crashes
- **No Recovery Without Fix**: Validators cannot restore from compromised backups until files are sanitized
- **Infrastructure Attack Surface**: Misconfigured cloud storage buckets or compromised backup infrastructure enables exploitation
- **Cascading Failures**: If multiple validators use the same compromised backup source during an emergency, the network could experience coordinated availability loss

## Likelihood Explanation

**Likelihood: MEDIUM**

**Attack Prerequisites**:
- Write access to backup storage (S3/GCS/Azure) OR ability to provide malicious backup files to validators
- Validators must attempt restore operation from compromised source

**Realistic Scenarios**:
1. **Misconfigured Cloud Storage**: Public/overly permissive S3/GCS buckets (common misconfiguration)
2. **Compromised Backup Infrastructure**: Attacker gains access to backup storage via credential theft
3. **Supply Chain Attack**: Malicious backup files distributed during emergency recovery
4. **Insider Threat**: Backup storage administrator with malicious intent

**Mitigating Factors**:
- Requires access beyond typical network-level attacks
- Validators should use trusted backup sources
- May have monitoring/alerts on abnormal memory usage

However, backup restore is a critical operation performed during emergencies when normal security procedures may be relaxed, making this a realistic attack vector.

## Recommendation

Add maximum record size validation to `read_record_bytes()`. Since legitimate records are bounded by protocol limits (1MB for state items, 1MB for governance transactions, 64KB for regular transactions), enforce a reasonable upper bound:

```rust
async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
    let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
    
    // Maximum reasonable record size: 10MB (generous upper bound for BCS-serialized data)
    const MAX_RECORD_SIZE: usize = 10 * 1024 * 1024;
    
    // read record size
    let mut size_buf = BytesMut::with_capacity(4);
    self.read_full_buf_or_none(&mut size_buf).await?;
    if size_buf.is_empty() {
        return Ok(None);
    }

    // empty record
    let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
    if record_size == 0 {
        return Ok(Some(Bytes::new()));
    }
    
    // Validate record size before allocation
    if record_size > MAX_RECORD_SIZE {
        bail!(
            "Record size {} exceeds maximum allowed size of {} bytes. \
            Possible decompression bomb or corrupted backup data.",
            record_size,
            MAX_RECORD_SIZE
        );
    }

    // read record
    let mut record_buf = BytesMut::with_capacity(record_size);
    self.read_full_buf_or_none(&mut record_buf).await?;
    if record_buf.is_empty() {
        bail!("Hit EOF when reading record.")
    }

    Ok(Some(record_buf.freeze()))
}
```

**Additional Recommendations**:
1. Add metrics for record sizes to detect anomalies
2. Implement cryptographic signatures on backup manifests to detect tampering
3. Document secure backup storage configuration practices
4. Add backup verification tools that check for malformed records before restore

## Proof of Concept

```rust
// test_decompression_bomb.rs
use bytes::BytesMut;
use tokio::io::AsyncReadExt;

#[tokio::test]
async fn test_decompression_bomb_via_malicious_size_header() {
    // Simulate a malicious backup record with huge size header
    let malicious_size = u32::MAX; // 4GB
    let mut malicious_data = malicious_size.to_be_bytes().to_vec();
    
    // In real attack, this would be compressed to ~10KB via gzip
    // but claim to decompress to 4GB via the size header
    // For test purposes, we just show the allocation attempt
    
    let mut reader = malicious_data.as_slice();
    
    // Simulate read_record_bytes behavior
    let mut size_buf = BytesMut::with_capacity(4);
    reader.read_buf(&mut size_buf).await.unwrap();
    
    let record_size = u32::from_be_bytes(size_buf.as_ref().try_into().unwrap()) as usize;
    
    println!("Attempting to allocate {} bytes ({} GB)", record_size, record_size / (1024*1024*1024));
    
    // This allocation would fail or exhaust memory on validator nodes
    // In the actual vulnerable code, this happens without any size check:
    // let mut record_buf = BytesMut::with_capacity(record_size);
    
    assert!(record_size == u32::MAX as usize, "Size header can specify up to 4GB");
    assert!(record_size > 1024 * 1024, "Exceeds legitimate state item limit of 1MB");
}

// To create actual malicious compressed backup:
// 1. Create file with 4GB size header + minimal payload
// 2. Compress with: gzip -9 malicious_backup.bin
// 3. Result: ~10KB compressed file that claims 4GB when decompressed
// 4. Place in backup storage
// 5. Validator restore triggers memory exhaustion
```

**Attack Demonstration Steps**:
1. Create malicious backup file: `python3 -c "import struct; open('bad.bin','wb').write(struct.pack('>I', 4294967295) + b'\x00'*100)" `
2. Compress it: `gzip -9 bad.bin` (results in ~200 byte file)
3. Upload to backup storage replacing legitimate backup chunk
4. Trigger validator restore operation
5. Observe memory allocation of 4GB per malicious record during `read_record_bytes()`
6. Validator OOM crash or severe memory pressure

**Notes**

This vulnerability demonstrates a classic input validation failure in critical infrastructure code. While the attack requires compromise of backup storage infrastructure, this is a realistic threat model for cloud-based deployments where storage buckets may be misconfigured or credentials compromised. The fix is straightforward: validate all externally-provided size values against reasonable upper bounds before allocation.

### Citations

**File:** storage/backup/backup-cli/src/utils/read_record_bytes.rs (L44-67)
```rust
    async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
        let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
        // read record size
        let mut size_buf = BytesMut::with_capacity(4);
        self.read_full_buf_or_none(&mut size_buf).await?;
        if size_buf.is_empty() {
            return Ok(None);
        }

        // empty record
        let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
        if record_size == 0 {
            return Ok(Some(Bytes::new()));
        }

        // read record
        let mut record_buf = BytesMut::with_capacity(record_size);
        self.read_full_buf_or_none(&mut record_buf).await?;
        if record_buf.is_empty() {
            bail!("Hit EOF when reading record.")
        }

        Ok(Some(record_buf.freeze()))
    }
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/sample_configs/gcp.sample.yaml (L18-21)
```yaml
    gzip -c | gsutil -q cp - "gs://$BUCKET/$SUB_DIR/$FILE_HANDLE" > /dev/null
  open_for_read: |
    # route file handle content to stdout
    gsutil -q cp "gs://$BUCKET/$SUB_DIR/$FILE_HANDLE" - | gzip -cd
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L112-137)
```rust
        while let Some(record_bytes) = file.read_record_bytes().await? {
            let (txn, aux_info, txn_info, events, write_set): (
                _,
                PersistedAuxiliaryInfo,
                _,
                _,
                WriteSet,
            ) = match manifest.format {
                TransactionChunkFormat::V0 => {
                    let (txn, txn_info, events, write_set) = bcs::from_bytes(&record_bytes)?;
                    (
                        txn,
                        PersistedAuxiliaryInfo::None,
                        txn_info,
                        events,
                        write_set,
                    )
                },
                TransactionChunkFormat::V1 => bcs::from_bytes(&record_bytes)?,
            };
            txns.push(txn);
            persisted_aux_info.push(aux_info);
            txn_infos.push(txn_info);
            event_vecs.push(events);
            write_sets.push(write_set);
        }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L253-266)
```rust
    async fn read_state_value(
        storage: &Arc<dyn BackupStorage>,
        file_handle: FileHandle,
    ) -> Result<Vec<(StateKey, StateValue)>> {
        let mut file = storage.open_for_read(&file_handle).await?;

        let mut chunk = vec![];

        while let Some(record_bytes) = file.read_record_bytes().await? {
            chunk.push(bcs::from_bytes(&record_bytes)?);
        }

        Ok(chunk)
    }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L154-156)
```rust
            max_bytes_per_write_op: NumBytes,
            { 5.. => "max_bytes_per_write_op" },
            1 << 20, // a single state item is 1MB max
```
