# Audit Report

## Title
Cross-Shard Channel Deadlock During Shutdown Causes Indefinite Node Hang

## Summary
The `shutdown()` method in `LocalExecutorClient` is empty and fails to properly signal cross-shard communication channels during shutdown. When shutdown occurs while shards are executing blocks with cross-shard dependencies, `CrossShardCommitReceiver` threads can remain blocked indefinitely waiting for messages that will never arrive, causing validator nodes to hang. [1](#0-0) 

## Finding Description
The sharded block executor uses cross-shard channels for inter-shard communication during parallel transaction execution. Each shard spawns a `CrossShardCommitReceiver` thread that blocks on `receive_cross_shard_msg()` waiting for cross-shard state updates. [2](#0-1) 

The `receive_cross_shard_msg()` implementation uses a blocking `recv().unwrap()` call without timeout: [3](#0-2) 

During normal execution, each shard sends a `StopMsg` to its own receiver after completing block execution: [4](#0-3) 

However, the shutdown logic has a critical flaw. The `Drop` implementation sends `ExecutorShardCommand::Stop` only to command channels: [5](#0-4) 

**Deadlock Scenario:**
1. Shards are executing a block with cross-shard dependencies
2. Shard 0's `CrossShardCommitReceiver` is blocked waiting for a cross-shard message from Shard 1
3. Node shutdown is initiated (e.g., for maintenance or upgrade)
4. `Drop` sends `Stop` commands to command channels and waits for join handles
5. However, Shard 0's receiver thread remains blocked at `recv()` because:
   - No `StopMsg` is sent to cross-shard channels during shutdown
   - The channels are not closed (senders remain alive)
   - No timeout mechanism exists
6. The `scope()` in `execute_transactions_with_dependencies` never completes because it waits for all spawned threads, including the blocked receiver: [6](#0-5) 

7. The `join()` call in `Drop` hangs indefinitely waiting for Shard 0's thread
8. Validator node becomes unresponsive, requiring forceful termination

This breaks the **liveness invariant** - validator nodes must remain available and responsive to maintain network consensus.

## Impact Explanation
**High Severity** per Aptos bug bounty criteria:
- **Validator node slowdowns/freezes**: Nodes hang indefinitely during shutdown, requiring manual intervention or forceful process termination
- **Significant protocol violation**: Affects validator availability during critical operations like epoch transitions or coordinated upgrades
- **Operational impact**: If multiple validators experience this during synchronized maintenance windows, it can severely impact network liveness

While this doesn't directly cause consensus safety violations or fund loss, it represents a severe availability issue that can:
- Delay network upgrades requiring validator restarts
- Reduce validator set availability during maintenance windows
- Potentially cause temporary consensus stalls if enough validators hang simultaneously during coordinated operations

## Likelihood Explanation
**Medium to High likelihood:**
- Occurs whenever shutdown is initiated while shards are actively processing blocks with cross-shard dependencies
- More likely during high transaction throughput when block execution spans longer durations
- Guaranteed to occur if shutdown timing coincides with in-flight cross-shard message exchanges
- Affects all validators using sharded block execution during normal operational procedures (upgrades, restarts, maintenance)

The issue is deterministic once the race condition occurs - there is no recovery mechanism, timeout, or fallback path.

## Recommendation
Implement proper shutdown signaling for cross-shard channels:

1. **Move shutdown logic from `Drop` to `shutdown()` method:**
   - Send `ExecutorShardCommand::Stop` to command channels
   - Send `CrossShardMsg::StopMsg` to all cross-shard channels to unblock waiting receivers
   - Optionally implement timeout-based `recv_timeout()` as a fallback

2. **Add graceful shutdown coordination:**
   - Wait for in-flight executions to complete or implement forced termination after timeout
   - Close cross-shard channels explicitly to trigger `recv()` errors

3. **Implement timeout mechanism in `receive_cross_shard_msg()`:**
   - Replace `recv().unwrap()` with `recv_timeout(Duration)` to prevent indefinite blocking
   - Handle timeout by checking if shutdown was requested

**Example fix for `shutdown()` method:**
```rust
fn shutdown(&mut self) {
    // Send Stop to command channels
    for command_tx in self.command_txs.iter() {
        let _ = command_tx.send(ExecutorShardCommand::Stop);
    }
    
    // Send StopMsg to all cross-shard channels to unblock receivers
    // This requires exposing cross-shard channel senders or adding shutdown signaling
    // Details depend on architecture decisions for cross-shard cleanup
    
    // Wait for joins with timeout
    for executor_service in self.executor_services.iter_mut() {
        let _ = executor_service.join_handle.take().unwrap().join();
    }
}
```

## Proof of Concept
```rust
// Rust test demonstrating the deadlock
// This test creates a sharded executor, starts execution with cross-shard dependencies,
// then triggers shutdown while receivers are waiting for messages

#[test]
#[ignore] // Ignore by default as this will hang
fn test_shutdown_deadlock() {
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    
    // Setup: Create LocalExecutorClient with 2 shards
    let num_shards = 2;
    let executor_client = LocalExecutorService::<MockStateView>::setup_local_executor_shards(
        num_shards,
        Some(4),
    );
    
    // Create transactions with cross-shard dependencies
    let state_view = Arc::new(MockStateView::new());
    let transactions = create_partitioned_transactions_with_cross_shard_deps(num_shards);
    
    // Start execution in background thread
    let executor_handle = thread::spawn(move || {
        executor_client.execute_block(
            state_view,
            transactions,
            4,
            BlockExecutorConfigFromOnchain::default(),
        )
    });
    
    // Wait briefly for execution to start and receivers to block
    thread::sleep(Duration::from_millis(100));
    
    // Trigger shutdown by dropping executor_client
    // This should complete quickly, but will hang due to blocked receivers
    drop(executor_client);
    
    // This join should complete, but will hang indefinitely
    // demonstrating the deadlock
    executor_handle.join().expect("Execution should complete");
    
    // If we reach here, shutdown completed successfully (test passes)
    // If we hang, the deadlock vulnerability is confirmed
}
```

**Notes:**
- The empty `shutdown()` implementation provides no cleanup mechanism
- Cross-shard channels remain open during shutdown with no signaling to unblock receivers
- The blocking `recv()` calls have no timeout or interruption mechanism
- This affects validator availability during routine operational procedures

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L225-225)
```rust
    fn shutdown(&mut self) {}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L228-239)
```rust
impl<S: StateView + Sync + Send + 'static> Drop for LocalExecutorClient<S> {
    fn drop(&mut self) {
        for command_tx in self.command_txs.iter() {
            let _ = command_tx.send(ExecutorShardCommand::Stop);
        }

        // wait for join handles to finish
        for executor_service in self.executor_services.iter_mut() {
            let _ = executor_service.join_handle.take().unwrap().join();
        }
    }
}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L335-337)
```rust
    fn receive_cross_shard_msg(&self, current_round: RoundId) -> CrossShardMsg {
        self.message_rxs[current_round].recv().unwrap()
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L26-45)
```rust
    pub fn start<S: StateView + Sync + Send>(
        cross_shard_state_view: Arc<CrossShardStateView<S>>,
        cross_shard_client: Arc<dyn CrossShardClient>,
        round: RoundId,
    ) {
        loop {
            let msg = cross_shard_client.receive_cross_shard_msg(round);
            match msg {
                RemoteTxnWriteMsg(txn_commit_msg) => {
                    let (state_key, write_op) = txn_commit_msg.take();
                    cross_shard_state_view
                        .set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
                },
                CrossShardMsg::StopMsg => {
                    trace!("Cross shard commit receiver stopped for round {}", round);
                    break;
                },
            }
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L134-180)
```rust
        executor_thread_pool.clone().scope(|s| {
            s.spawn(move |_| {
                CrossShardCommitReceiver::start(
                    cross_shard_state_view_clone,
                    cross_shard_client,
                    round,
                );
            });
            s.spawn(move |_| {
                let txn_provider =
                    DefaultTxnProvider::new_without_info(signature_verified_transactions);
                let ret = AptosVMBlockExecutorWrapper::execute_block_on_thread_pool(
                    executor_thread_pool,
                    &txn_provider,
                    aggr_overridden_state_view.as_ref(),
                    // Since we execute blocks in parallel, we cannot share module caches, so each
                    // thread has its own caches.
                    &AptosModuleCacheManager::new(),
                    config,
                    TransactionSliceMetadata::unknown(),
                    cross_shard_commit_sender,
                )
                .map(BlockOutput::into_transaction_outputs_forced);
                if let Some(shard_id) = shard_id {
                    trace!(
                        "executed sub block for shard {} and round {}",
                        shard_id,
                        round
                    );
                    // Send a self message to stop the cross-shard commit receiver.
                    cross_shard_client_clone.send_cross_shard_msg(
                        shard_id,
                        round,
                        CrossShardMsg::StopMsg,
                    );
                } else {
                    trace!("executed block for global shard and round {}", round);
                    // Send a self message to stop the cross-shard commit receiver.
                    cross_shard_client_clone.send_global_msg(CrossShardMsg::StopMsg);
                }
                callback.send(ret).unwrap();
                executor_thread_pool_clone.spawn(move || {
                    // Explicit async drop
                    drop(txn_provider);
                });
            });
        });
```
