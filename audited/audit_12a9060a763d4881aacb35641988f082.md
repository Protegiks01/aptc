# Audit Report

## Title
Race Condition in IndexerAsyncV2::create_checkpoint() Allows Checkpoint Corruption

## Summary
The `create_checkpoint()` method in `IndexerAsyncV2` lacks synchronization mechanisms, allowing concurrent calls on the same path to cause file corruption, incomplete checkpoints, and TOCTOU (Time-of-Check-Time-of-Use) race conditions between directory removal and checkpoint creation.

## Finding Description

The `create_checkpoint()` function performs two sequential operations without any synchronization: [1](#0-0) 

**The vulnerability occurs through the following race condition:**

1. Thread A calls `create_checkpoint(&path)`
2. Thread A executes `fs::remove_dir_all(path)` - removes directory
3. Thread A begins `self.db.create_checkpoint(path)` - RocksDB starts writing files
4. Thread B calls `create_checkpoint(&path)` concurrently
5. Thread B executes `fs::remove_dir_all(path)` - **deletes Thread A's partially written checkpoint**
6. Thread B begins `self.db.create_checkpoint(path)` - starts writing to same location
7. Both threads write to the same checkpoint directory concurrently

**Evidence of shared concurrent access:**

The `IndexerAsyncV2` struct is designed for concurrent access, using thread-safe primitives: [2](#0-1) 

The instance is wrapped in `Arc` for shared ownership across async tasks: [3](#0-2) 

**Parent service explicitly marked as not thread-safe:** [4](#0-3) 

This warning indicates the developers recognize threading issues but have not implemented proper synchronization at the checkpoint creation level.

**The method signature allows concurrent calls:**

The method takes `&self` (shared immutable reference), not `&mut self`, permitting multiple simultaneous invocations from different threads or async tasks.

## Impact Explanation

This vulnerability qualifies as **Medium severity** under Aptos bug bounty criteria due to potential **state inconsistencies requiring intervention**:

1. **Checkpoint Corruption**: Concurrent writes to the same files produce corrupted, non-recoverable checkpoint data
2. **Data Loss Risk**: If a corrupted checkpoint is used for recovery or backup, the indexer state cannot be properly restored
3. **Operational Impact**: Failed checkpoint restoration requires manual intervention to rebuild indexer state from scratch
4. **State Inconsistency**: The indexer may end up with incomplete or invalid table information mappings

While this doesn't directly affect consensus or validator operations (indexer is not on critical path), it violates the **State Consistency** invariant requiring atomic and verifiable state transitions. Checkpoint operations must be atomic - either fully complete or not performed at all.

## Likelihood Explanation

**Moderate likelihood** due to multiple realistic trigger scenarios:

1. **External API Exposure**: If `create_checkpoint()` is exposed via any RPC/API endpoint (for administrative backup operations), concurrent requests could trigger the race
2. **Automated Backup Systems**: Multiple backup jobs or schedulers attempting checkpoint creation simultaneously
3. **Manual Operator Intervention**: Administrator manually triggering checkpoint while automated epoch-boundary snapshot is in progress
4. **Service Restart Race**: During node restart or failover, race conditions in initialization code could cause duplicate checkpoint attempts
5. **Bug in Epoch Logic**: Future code changes introducing bugs in epoch transition handling could cause the same epoch to trigger multiple snapshots

The vulnerability requires:
- No special privileges (the method is public)
- Same checkpoint path used concurrently (could occur through bugs or misconfiguration)
- Timing alignment (but with high-frequency operations, becomes probable)

## Recommendation

**Implement mutex-based synchronization to ensure checkpoint atomicity:**

```rust
use std::sync::Mutex;

pub struct IndexerAsyncV2 {
    pub db: DB,
    next_version: AtomicU64,
    pending_on: DashMap<TableHandle, DashSet<Bytes>>,
    // Add mutex to protect checkpoint operations
    checkpoint_mutex: Mutex<()>,
}

impl IndexerAsyncV2 {
    pub fn new(db: DB) -> Result<Self> {
        let next_version = db
            .get::<IndexerMetadataSchema>(&MetadataKey::LatestVersion)?
            .map_or(0, |v| v.expect_version());

        Ok(Self {
            db,
            next_version: AtomicU64::new(next_version),
            pending_on: DashMap::new(),
            checkpoint_mutex: Mutex::new(()), // Initialize mutex
        })
    }

    pub fn create_checkpoint(&self, path: &PathBuf) -> Result<()> {
        // Acquire lock before checkpoint operation
        let _guard = self.checkpoint_mutex.lock().unwrap();
        
        fs::remove_dir_all(path).unwrap_or(());
        self.db.create_checkpoint(path)
        
        // Lock automatically released when _guard goes out of scope
    }
}
```

**Alternative: Path-based locking if different paths should not block each other:**

```rust
use std::sync::Mutex;
use std::collections::HashMap;

pub struct IndexerAsyncV2 {
    pub db: DB,
    next_version: AtomicU64,
    pending_on: DashMap<TableHandle, DashSet<Bytes>>,
    // Path-specific locks
    checkpoint_locks: DashMap<PathBuf, Arc<Mutex<()>>>,
}

impl IndexerAsyncV2 {
    pub fn create_checkpoint(&self, path: &PathBuf) -> Result<()> {
        // Get or create lock for this specific path
        let lock = self.checkpoint_locks
            .entry(path.clone())
            .or_insert_with(|| Arc::new(Mutex::new(())))
            .clone();
        
        let _guard = lock.lock().unwrap();
        
        fs::remove_dir_all(path).unwrap_or(());
        self.db.create_checkpoint(path)?;
        
        // Clean up lock entry if no longer needed
        self.checkpoint_locks.remove(path);
        
        Ok(())
    }
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::thread;
    use std::sync::Arc;
    use tempfile::TempDir;

    #[test]
    fn test_concurrent_checkpoint_race_condition() {
        // Setup: Create IndexerAsyncV2 instance
        let tmpdir = TempDir::new().unwrap();
        let db_path = tmpdir.path().join("test_db");
        let rocksdb_config = RocksdbConfig::default();
        let db = open_db(db_path, &rocksdb_config, false).unwrap();
        let indexer = Arc::new(IndexerAsyncV2::new(db).unwrap());
        
        // Write some data to create checkpoint from
        let mut batch = SchemaBatch::new();
        batch.put::<TableInfoSchema>(
            &TableHandle(AccountAddress::random()),
            &TableInfo {
                key_type: TypeTag::Bool,
                value_type: TypeTag::U64,
            }
        ).unwrap();
        indexer.db.write_schemas(batch).unwrap();

        // Checkpoint path that both threads will use
        let checkpoint_path = Arc::new(tmpdir.path().join("checkpoint"));
        
        // Spawn two threads that create checkpoints concurrently to same path
        let indexer1 = Arc::clone(&indexer);
        let path1 = Arc::clone(&checkpoint_path);
        let handle1 = thread::spawn(move || {
            for _ in 0..10 {
                let _ = indexer1.create_checkpoint(&path1);
                thread::sleep(std::time::Duration::from_millis(1));
            }
        });

        let indexer2 = Arc::clone(&indexer);
        let path2 = Arc::clone(&checkpoint_path);
        let handle2 = thread::spawn(move || {
            for _ in 0..10 {
                let _ = indexer2.create_checkpoint(&path2);
                thread::sleep(std::time::Duration::from_millis(1));
            }
        });

        handle1.join().unwrap();
        handle2.join().unwrap();

        // Verify checkpoint integrity
        // A corrupted checkpoint will fail to open or have inconsistent data
        let checkpoint_db = DB::open_cf_readonly(
            &rocksdb::Options::default(),
            &checkpoint_path,
            "test",
            vec![]
        );
        
        // This assertion may fail due to checkpoint corruption
        assert!(
            checkpoint_db.is_ok(),
            "Checkpoint corrupted due to concurrent access: {:?}",
            checkpoint_db.err()
        );
    }
}
```

**Expected behavior with the vulnerability:**
- Random failures when opening checkpoint
- Incomplete or corrupted checkpoint files
- Non-deterministic test failures depending on thread scheduling

**Expected behavior after fix:**
- Checkpoint operations are serialized
- All checkpoints complete successfully
- Test passes consistently

## Notes

While the current system design (single `TableInfoService` instance, epoch-based unique paths) reduces the likelihood of this race condition occurring in normal operation, the lack of synchronization represents a latent vulnerability that could be triggered by:
- Future code changes introducing concurrent checkpoint creation
- External tooling or APIs that expose checkpoint functionality
- Operational procedures or scripts that don't account for concurrent access

The fix should be implemented defensively to prevent future exploitation as the codebase evolves.

### Citations

**File:** storage/indexer/src/db_v2.rs (L46-58)
```rust
pub struct IndexerAsyncV2 {
    pub db: DB,
    // Next version to be processed
    next_version: AtomicU64,
    // It is used in the context of processing write ops and extracting table information.
    // As the code iterates through the write ops, it checks if the state key corresponds to a table item.
    // If it does, the associated bytes are added to the pending_on map under the corresponding table handle.
    // Later, when the table information becomes available, the pending items can be retrieved and processed accordingly.
    // One example could be a nested table item, parent table contains child table, so when parent table is first met and parsed,
    // is obscure and will be stored as bytes with parent table's handle, once parent table's parsed with instructions,
    // child table handle will be parsed accordingly.
    pending_on: DashMap<TableHandle, DashSet<Bytes>>,
}
```

**File:** storage/indexer/src/db_v2.rs (L193-196)
```rust
    pub fn create_checkpoint(&self, path: &PathBuf) -> Result<()> {
        fs::remove_dir_all(path).unwrap_or(());
        self.db.create_checkpoint(path)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/runtime.rs (L78-79)
```rust
    let indexer_async_v2 =
        Arc::new(IndexerAsyncV2::new(db).expect("Failed to initialize indexer async v2"));
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L31-33)
```rust
/// TableInfoService is responsible for parsing table info from transactions and writing them to rocksdb.
/// Not thread safe.
pub struct TableInfoService {
```
