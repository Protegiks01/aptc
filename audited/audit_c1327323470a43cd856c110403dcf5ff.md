# Audit Report

## Title
Indexer gRPC Service Starts Without Database Synchronization Validation, Potentially Serving Stale Data

## Summary
The `bootstrap()` function in the indexer gRPC fullnode service starts accepting client connections immediately without validating that the database is synchronized with the blockchain network. This allows indexers to connect and receive incomplete or stale transaction data during the node's initial sync period, causing potential state inconsistencies in downstream systems.

## Finding Description

The indexer gRPC service is initialized in the `bootstrap()` function which accepts a database reader parameter and immediately starts a gRPC server to serve transaction data to indexer clients. [1](#0-0) 

The database parameter is passed directly to create a `Context` without any validation of its synchronization state. The gRPC server immediately begins accepting connections: [2](#0-1) 

The node initialization sequence reveals the critical timing issue. The indexer gRPC service is bootstrapped before the node waits for state synchronization to complete: [3](#0-2) 

State sync is started at line 762-769, the indexer gRPC service begins accepting connections at line 787, and only then does the node wait for state sync initialization at line 824-826. This ordering allows a window where the service accepts client connections while the database contains only genesis data or is partially synchronized.

When clients request transaction data, the `IndexerStreamCoordinator` does check `highest_known_version` and will wait if needed: [4](#0-3) 

However, this validation occurs AFTER the client has already established a connection and made a request. A client connecting to a newly started node will:
1. Successfully establish a gRPC connection
2. Request historical transaction data
3. Receive whatever data is currently available in the database (potentially only genesis data)
4. Have no automatic indication that the node is not fully synchronized with the network

The `ping` endpoint does expose sync status information, but it doesn't prevent the service from accepting connections or proactively rejecting requests when the database is not synchronized: [5](#0-4) 

This breaks the **State Consistency** invariant (#4) which requires that state transitions be atomic and verifiable. Indexers connecting to an unsynchronized node will process incomplete data and may commit this to their databases, creating inconsistencies that require manual intervention to resolve.

## Impact Explanation

This vulnerability qualifies as **Medium severity** per the Aptos bug bounty criteria:
- **State inconsistencies requiring intervention**: Indexers processing incomplete data from unsynchronized nodes will have incorrect state in their databases, requiring manual reconciliation or rebuilding of indexer state
- The issue does not directly cause loss of funds or consensus violations (which would be Critical/High severity)
- Multiple downstream systems depending on indexer data could be affected simultaneously if they connect during a node's sync period

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of occurrence:

1. **Common operational scenarios**: Nodes frequently restart due to upgrades, crashes, or configuration changes. Each restart creates a window where the indexer gRPC service is available but the database is not fully synchronized.

2. **Network expansion**: New nodes joining the Aptos network will serve incomplete data to any indexers that connect before full synchronization completes.

3. **No client-side protection**: Indexer clients have no way to automatically detect that they're receiving incomplete data unless they explicitly call the `ping` endpoint and verify sync status themselves.

4. **Time window**: Depending on the blockchain state size, synchronization can take significant time (hours to days), creating a large exposure window.

## Recommendation

Add synchronization validation before starting the gRPC service. The fix should:

1. Check that the database is synchronized with the network before accepting connections
2. Wait for a minimum sync threshold to be reached
3. Optionally add a configuration flag to control this behavior for specific deployment scenarios

**Proposed fix for `bootstrap()` function:**

```rust
pub fn bootstrap(
    config: &NodeConfig,
    chain_id: ChainId,
    db: Arc<dyn DbReader>,
    mp_sender: MempoolClientSender,
    indexer_reader: Option<Arc<dyn IndexerReader>>,
    port_tx: Option<oneshot::Sender<u16>>,
) -> Option<Runtime> {
    if !config.indexer_grpc.enabled {
        return None;
    }

    let runtime = aptos_runtimes::spawn_named_runtime("indexer-grpc".to_string(), None);
    let node_config = config.clone();
    // ... [other config extraction] ...

    runtime.spawn(async move {
        // NEW: Wait for database to be synchronized before starting service
        if node_config.indexer_grpc.require_sync_before_start {
            info!("[indexer-grpc] Waiting for database synchronization before starting service");
            loop {
                match db.get_synced_version() {
                    Ok(Some(version)) if version > 0 => {
                        info!("[indexer-grpc] Database synchronized at version {}, starting service", version);
                        break;
                    }
                    Ok(_) => {
                        tokio::time::sleep(Duration::from_secs(5)).await;
                    }
                    Err(e) => {
                        error!("[indexer-grpc] Error checking sync status: {}", e);
                        tokio::time::sleep(Duration::from_secs(5)).await;
                    }
                }
            }
        }

        let context = Arc::new(Context::new(
            chain_id,
            db,
            mp_sender,
            node_config.clone(),
            indexer_reader,
        ));
        // ... [rest of service initialization] ...
    });
    Some(runtime)
}
```

Additionally, add a readiness indicator in the `ping` response to explicitly communicate sync status to clients.

## Proof of Concept

**Scenario**: Demonstrate that the indexer gRPC service accepts connections and serves data before database synchronization completes.

**Steps to reproduce**:

1. Start a fresh Aptos node with indexer gRPC enabled:
```bash
# Fresh node with no synced data
aptos-node --config node.yaml
```

2. Immediately connect an indexer client before sync completes:
```rust
use tonic::Request;
use aptos_protos::internal::fullnode::v1::{
    fullnode_data_client::FullnodeDataClient,
    GetTransactionsFromNodeRequest,
};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Connect immediately after node startup
    let mut client = FullnodeDataClient::connect("http://127.0.0.1:50051").await?;
    
    // Request transactions starting from version 0
    let request = Request::new(GetTransactionsFromNodeRequest {
        starting_version: Some(0),
        transactions_count: Some(1000),
    });
    
    let mut stream = client.get_transactions_from_node(request).await?.into_inner();
    
    // Client successfully receives a stream even though database may not be synced
    while let Some(response) = stream.message().await? {
        println!("Received response: {:?}", response);
        // If database has only genesis data, client will only receive genesis transactions
        // with no indication that more data exists but hasn't synced yet
    }
    
    Ok(())
}
```

**Expected vulnerable behavior**: The client successfully establishes a connection and receives whatever data is currently in the database (potentially only genesis block), with no automatic indication that the node is still synchronizing and more data should exist.

**Expected secure behavior**: The service should either:
- Refuse connections until synchronized, OR
- Clearly indicate in responses that the node is still syncing

## Notes

This vulnerability affects the data integrity guarantees of the Aptos indexer ecosystem. While the indexer gRPC service implements proper waiting mechanisms during data streaming (via `ensure_highest_known_version()`), the lack of upfront synchronization validation creates a window for downstream state inconsistencies. The issue is particularly impactful because indexer infrastructure is critical for dApp functionality, block explorers, and analytics platforms that depend on complete and accurate historical data.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/runtime.rs (L36-67)
```rust
pub fn bootstrap(
    config: &NodeConfig,
    chain_id: ChainId,
    db: Arc<dyn DbReader>,
    mp_sender: MempoolClientSender,
    indexer_reader: Option<Arc<dyn IndexerReader>>,
    port_tx: Option<oneshot::Sender<u16>>,
) -> Option<Runtime> {
    if !config.indexer_grpc.enabled {
        return None;
    }

    let runtime = aptos_runtimes::spawn_named_runtime("indexer-grpc".to_string(), None);

    let node_config = config.clone();

    let address = node_config.indexer_grpc.address;
    let use_data_service_interface = node_config.indexer_grpc.use_data_service_interface;
    let processor_task_count = node_config
        .indexer_grpc
        .processor_task_count
        .unwrap_or_else(|| get_default_processor_task_count(use_data_service_interface));
    let processor_batch_size = node_config.indexer_grpc.processor_batch_size;
    let output_batch_size = node_config.indexer_grpc.output_batch_size;
    let transaction_channel_size = node_config.indexer_grpc.transaction_channel_size;
    let max_transaction_filter_size_bytes =
        node_config.indexer_grpc.max_transaction_filter_size_bytes;

    runtime.spawn(async move {
        let context = Arc::new(Context::new(
            chain_id,
            db,
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/runtime.rs (L123-130)
```rust
        let listener = TcpListener::bind(address).await.unwrap();
        if let Some(port_tx) = port_tx {
            port_tx.send(listener.local_addr().unwrap().port()).unwrap();
        }
        let incoming = TcpIncoming::from_listener(listener, false, None).unwrap();

        // Make port into a config
        router.serve_with_incoming(incoming).await.unwrap();
```

**File:** aptos-node/src/lib.rs (L778-827)
```rust
    // Bootstrap the API and indexer
    let (
        mempool_client_receiver,
        api_runtime,
        indexer_table_info_runtime,
        indexer_runtime,
        indexer_grpc_runtime,
        internal_indexer_db_runtime,
        mempool_client_sender,
    ) = services::bootstrap_api_and_indexer(
        &node_config,
        db_rw.clone(),
        chain_id,
        indexer_db_opt,
        update_receiver,
        api_port_tx,
        indexer_grpc_port_tx,
    )?;

    // Set mempool client sender in order to enable the Mempool API in the admin service
    admin_service.set_mempool_client_sender(mempool_client_sender);

    // Create mempool and get the consensus to mempool sender
    let (mempool_runtime, consensus_to_mempool_sender) =
        services::start_mempool_runtime_and_get_consensus_sender(
            &mut node_config,
            &db_rw,
            mempool_reconfig_subscription,
            mempool_network_interfaces,
            mempool_listener,
            mempool_client_receiver,
            peers_and_metadata,
        );

    // Create the DKG runtime and get the VTxn pool
    let (vtxn_pool, dkg_runtime) =
        consensus::create_dkg_runtime(&mut node_config, dkg_subscriptions, dkg_network_interfaces);

    // Create the JWK consensus runtime
    let jwk_consensus_runtime = consensus::create_jwk_consensus_runtime(
        &mut node_config,
        jwk_consensus_subscriptions,
        jwk_consensus_network_interfaces,
        &vtxn_pool,
    );

    // Wait until state sync has been initialized
    debug!("Waiting until state sync is initialized!");
    state_sync_runtimes.block_until_initialized();
    debug!("State sync initialization complete.");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L548-579)
```rust
    /// Will keep looping and checking the latest ledger info to see if there are new transactions
    /// If there are, it will set the highest known version
    async fn ensure_highest_known_version(&mut self) -> bool {
        let mut empty_loops = 0;
        while self.highest_known_version == 0 || self.current_version > self.highest_known_version {
            if let Some(abort_handle) = self.abort_handle.as_ref() {
                if abort_handle.load(Ordering::SeqCst) {
                    return false;
                }
            }
            if empty_loops > 0 {
                tokio::time::sleep(Duration::from_millis(RETRY_TIME_MILLIS)).await;
            }
            empty_loops += 1;
            if let Err(err) = self.set_highest_known_version() {
                error!(
                    error = format!("{:?}", err),
                    "[Indexer Fullnode] Failed to set highest known version"
                );
                continue;
            } else {
                sample!(
                    SampleRate::Frequency(10),
                    info!(
                        highest_known_version = self.highest_known_version,
                        "[Indexer Fullnode] Found new highest known version",
                    )
                );
            }
        }
        true
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L207-242)
```rust
    async fn ping(
        &self,
        _request: Request<PingFullnodeRequest>,
    ) -> Result<Response<PingFullnodeResponse>, Status> {
        let timestamp = timestamp_now_proto();
        let known_latest_version = self
            .service_context
            .context
            .db
            .get_synced_version()
            .map_err(|e| Status::internal(format!("{e}")))?;

        let table_info_version = self
            .service_context
            .context
            .indexer_reader
            .as_ref()
            .and_then(|r| r.get_latest_table_info_ledger_version().ok().flatten());

        if known_latest_version.is_some() && table_info_version.is_some() {
            let version = std::cmp::min(known_latest_version.unwrap(), table_info_version.unwrap());
            if let Ok(timestamp_us) = self.service_context.context.db.get_block_timestamp(version) {
                let latency = SystemTime::now().duration_since(UNIX_EPOCH).unwrap()
                    - Duration::from_micros(timestamp_us);
                LATENCY_MS.set(latency.as_millis() as i64);
            }
        }

        let info = FullnodeInfo {
            chain_id: self.service_context.context.chain_id().id() as u64,
            timestamp: Some(timestamp),
            known_latest_version,
        };
        let response = PingFullnodeResponse { info: Some(info) };
        Ok(Response::new(response))
    }
```
