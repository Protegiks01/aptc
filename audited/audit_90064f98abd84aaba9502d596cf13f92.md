# Audit Report

## Title
Unauthenticated Cross-Shard Message Injection Enables Consensus Safety Violations in Remote Executor Service

## Summary
The remote cross-shard communication system in the distributed executor service lacks authentication and validation of incoming messages. An attacker with network access can inject malicious `RemoteTxnWriteMsg` messages containing arbitrary `WriteOp` operations (deletions or modifications of any state including system resources), causing different validators to execute with different state values and violate consensus safety by producing different state roots.

## Finding Description

The vulnerability exists in the cross-shard message handling pipeline used by the remote executor service. When transactions are partitioned across multiple executor shards for parallel execution, shards communicate state dependencies via network messages. This communication channel has three critical security gaps:

**1. No Network Authentication**

The GRPC service accepts all incoming cross-shard messages without any authentication: [1](#0-0) 

Any attacker who can reach the network endpoint can send messages. There is no signature verification, peer authentication, or access control.

**2. No WriteOp Validation After Deserialization**

The `receive_cross_shard_msg` function deserializes messages using BCS without subsequent validation: [2](#0-1) 

The deserialized `CrossShardMsg` contains `RemoteTxnWrite` with a `WriteOp` that can specify any operation: [3](#0-2) 

**3. Direct Application Without Validation**

The received `WriteOp` is directly applied to the `CrossShardStateView` without verifying:
- Whether the WriteOp is legitimate (matches what the sending shard actually computed)
- Whether it's authorized (not deleting system resources)
- Whether the sender is authenticated [4](#0-3) 

The malicious state value is then used during transaction execution: [5](#0-4) 

**Attack Scenario:**

1. Transaction T in Shard A depends on StateKey K (e.g., a validator's stake) from Shard B
2. Attacker monitors network to identify the cross-shard dependency
3. Attacker sends spoofed `RemoteTxnWriteMsg` to Shard A before legitimate message arrives:
   - `state_key = K` (validator stake key)
   - `write_op = Deletion` (to make stake appear non-existent)
4. Shard A's `CrossShardStateView` stores `None` for this key
5. Transaction T executes seeing no stake for the validator
6. Different validators may receive different messages (selective injection), computing different state roots
7. **Consensus safety violated**: Validators cannot agree on state root, causing chain halt

This bypasses all Move VM access control because the malicious state is injected before VM execution, appearing as legitimate cross-shard data.

## Impact Explanation

**Critical Severity - Consensus/Safety Violation**

This vulnerability directly violates the most fundamental blockchain invariant:

**Broken Invariant #1**: "Deterministic Execution: All validators must produce identical state roots for identical blocks"

The attack enables:
- **Consensus splits**: Different validators compute different state roots for the same block
- **Chain halt**: AptosBFT cannot reach consensus when validators disagree on state
- **System resource manipulation**: Deletion or modification of critical state like validator stakes, governance voting power, or framework resources
- **Non-recoverable state**: Requires hard fork to recover if successful

This meets the **Critical Severity** category per Aptos bug bounty:
- "Consensus/Safety violations" 
- "Non-recoverable network partition (requires hardfork)"
- Potential for "Total loss of liveness/network availability"

The impact extends to ALL validators and the entire network, not just individual nodes or transactions.

## Likelihood Explanation

**Likelihood: Medium-High (depends on deployment)**

**Required Conditions:**
1. Remote executor service must be deployed (production code path exists)
2. Attacker must have network access to executor service endpoints
3. Attacker must identify cross-shard dependencies (possible via traffic analysis or transaction analysis)

**Attacker Capabilities Required:**
- Network-level access (MITM, compromised infrastructure, or insider)
- Ability to craft valid BCS-serialized messages
- Timing to send spoofed messages before legitimate ones

**Mitigating Factors:**
- Executor services may run on isolated private networks
- Network firewalls may block external access
- Deployment may use local executor instead of remote

**Aggravating Factors:**
- Zero authentication means ANY network attacker can exploit
- No rate limiting or anomaly detection visible in code
- Selective message injection allows targeted attacks on specific validators
- The remote executor service has a dedicated main.rs entry point, suggesting production use: [6](#0-5) 

The likelihood is MEDIUM-HIGH if remote executors are deployed, LOW if only local execution is used in production.

## Recommendation

**Immediate Mitigation: Add Cryptographic Authentication**

Implement message signing and verification for all cross-shard messages:

```rust
// In messages.rs, add signature field:
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct RemoteTxnWrite {
    state_key: StateKey,
    write_op: Option<WriteOp>,
    signature: Ed25519Signature,  // Sign over (state_key || write_op)
    signer_public_key: Ed25519PublicKey,
}

// In receive_cross_shard_msg:
fn receive_cross_shard_msg(&self, current_round: RoundId) -> CrossShardMsg {
    let rx = self.message_rxs[current_round].lock().unwrap();
    let message = rx.recv().unwrap();
    let msg: CrossShardMsg = bcs::from_bytes(&message.to_bytes()).unwrap();
    
    // NEW: Verify signature
    match &msg {
        CrossShardMsg::RemoteTxnWriteMsg(txn_write) => {
            verify_cross_shard_message_signature(txn_write)
                .expect("Invalid cross-shard message signature");
        },
        _ => {}
    }
    msg
}
```

**Additional Recommendations:**

1. **Network-level TLS**: Use mutual TLS for GRPC connections with certificate pinning
2. **Peer Allowlisting**: Only accept messages from known executor shard endpoints
3. **Message Ordering**: Add sequence numbers and replay protection
4. **Audit Logging**: Log all cross-shard messages for forensic analysis
5. **WriteOp Validation**: Verify WriteOps don't delete system resources before application [7](#0-6) 

The NetworkController should be enhanced with authentication middleware similar to the main Aptos network framework's Noise protocol implementation.

## Proof of Concept

```rust
// PoC demonstrating unauthenticated message injection
// This would be a Rust integration test (excluded from scope but shown for concept)

#[test]
fn test_cross_shard_message_injection_attack() {
    // Setup: Create two remote executor shards
    let shard_a_addr = get_available_addr();
    let shard_b_addr = get_available_addr();
    
    // Start legitimate Shard B
    let mut shard_b = start_executor_shard(1, vec![shard_a_addr]);
    
    // Start Shard A expecting messages from Shard B
    let mut shard_a = start_executor_shard(0, vec![shard_b_addr]);
    
    // Attacker: Create malicious GRPC client
    let mut attacker_client = NetworkMessageServiceClient::connect(
        format!("http://{}", shard_a_addr)
    ).await.unwrap();
    
    // Craft malicious message deleting a system resource
    let system_resource_key = StateKey::access_path(
        AccessPath::new(CORE_CODE_ADDRESS, b"ValidatorSet".to_vec())
    );
    let malicious_msg = CrossShardMsg::RemoteTxnWriteMsg(RemoteTxnWrite::new(
        system_resource_key,
        Some(WriteOp::legacy_deletion()), // DELETE system resource
    ));
    
    // Inject malicious message (no authentication required!)
    let network_msg = NetworkMessage {
        message: bcs::to_bytes(&malicious_msg).unwrap(),
        message_type: "cross_shard_0".to_string(),
    };
    
    // This succeeds because there's no authentication
    attacker_client.simple_msg_exchange(network_msg).await.unwrap();
    
    // Shard A now has corrupted state showing ValidatorSet as deleted
    // Transactions will execute incorrectly, breaking consensus
}
```

The PoC demonstrates that an attacker with network access can inject arbitrary cross-shard messages containing malicious `WriteOp` operations without any authentication barrier. The messages are accepted and applied, corrupting the execution state.

---

## Notes

This vulnerability is particularly severe because it undermines the foundational security assumption of distributed execution: that cross-shard communication occurs over a trusted channel. While network isolation may provide some protection in practice, the absence of cryptographic authentication violates defense-in-depth principles and creates risk from insider threats, misconfigured networks, or network compromise scenarios. The vulnerability is exploitable without requiring validator private keys or consensus-level Byzantine behavior.

### Citations

**File:** secure/net/src/grpc_network_service/mod.rs (L93-115)
```rust
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L61-66)
```rust
    fn receive_cross_shard_msg(&self, current_round: RoundId) -> CrossShardMsg {
        let rx = self.message_rxs[current_round].lock().unwrap();
        let message = rx.recv().unwrap();
        let msg: CrossShardMsg = bcs::from_bytes(&message.to_bytes()).unwrap();
        msg
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/messages.rs (L7-18)
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
pub enum CrossShardMsg {
    RemoteTxnWriteMsg(RemoteTxnWrite),
    StopMsg,
}

#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct RemoteTxnWrite {
    state_key: StateKey,
    // The write op is None if the transaction is aborted.
    write_op: Option<WriteOp>,
}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L31-44)
```rust
        loop {
            let msg = cross_shard_client.receive_cross_shard_msg(round);
            match msg {
                RemoteTxnWriteMsg(txn_commit_msg) => {
                    let (state_key, write_op) = txn_commit_msg.take();
                    cross_shard_state_view
                        .set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
                },
                CrossShardMsg::StopMsg => {
                    trace!("Cross shard commit receiver stopped for round {}", round);
                    break;
                },
            }
        }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_state_view.rs (L49-56)
```rust
    pub fn set_value(&self, state_key: &StateKey, state_value: Option<StateValue>) {
        self.cross_shard_data
            .get(state_key)
            .unwrap()
            .set_value(state_value);
        // uncomment the following line to debug waiting count
        // trace!("waiting count for shard id {} is {}", self.shard_id, self.waiting_count());
    }
```

**File:** execution/executor-service/src/main.rs (L27-48)
```rust
fn main() {
    let args = Args::parse();
    aptos_logger::Logger::new().init();

    let (tx, rx) = crossbeam_channel::unbounded();
    ctrlc::set_handler(move || {
        tx.send(()).unwrap();
    })
    .expect("Error setting Ctrl-C handler");

    let _exe_service = ProcessExecutorService::new(
        args.shard_id,
        args.num_shards,
        args.num_executor_threads,
        args.coordinator_address,
        args.remote_executor_addresses,
    );

    rx.recv()
        .expect("Could not receive Ctrl-C msg from channel.");
    info!("Process executor service shutdown successfully.");
}
```

**File:** secure/net/src/network_controller/mod.rs (L72-92)
```rust
/// NetworkController is the main entry point for sending and receiving messages over the network.
/// 1. If a node acts as both client and server, albeit in different contexts, GRPC needs separate
///    runtimes for client context and server context. Otherwise we a hang in GRPC. This seems to be
///    an internal bug in GRPC.
/// 2. We want to use tokio runtimes because it is best for async IO and tonic GRPC
///    implementation is async. However, we want the rest of the system (remote executor service)
///    to use rayon thread pools because it is best for CPU bound tasks.
/// 3. NetworkController, InboundHandler and OutboundHandler work as a bridge between the sync and
///    async worlds.
/// 4. We need to shutdown all the async tasks spawned by the NetworkController runtimes, otherwise
///    the program will hang, or have resource leaks.
#[allow(dead_code)]
pub struct NetworkController {
    inbound_handler: Arc<Mutex<InboundHandler>>,
    outbound_handler: OutboundHandler,
    inbound_rpc_runtime: Runtime,
    outbound_rpc_runtime: Runtime,
    inbound_server_shutdown_tx: Option<oneshot::Sender<()>>,
    outbound_task_shutdown_tx: Option<Sender<Message>>,
    listen_addr: SocketAddr,
}
```
