# Audit Report

## Title
Panic-Inducing Assertions in DKG Secret Reconstruction During Epoch Transitions

## Summary
The `reconstruct()` function in the BLSTRS scalar secret sharing implementation uses hard assertions (`assert_ge!`, `assert_le!`) that cause validator nodes to panic instead of gracefully handling reconstruction failures. During epoch transitions when validator sets change, insufficient or excessive shares can trigger these assertions, crashing validator nodes and potentially halting consensus.

## Finding Description

The vulnerability exists in the secret reconstruction logic used during Distributed Key Generation (DKG) for randomness generation across epoch boundaries. 

In `crates/aptos-crypto/src/blstrs/scalar_secret_key.rs`, the `reconstruct()` function enforces share count requirements via panic-inducing assertions: [1](#0-0) 

The identical pattern appears in the dealt secret key reconstruction: [2](#0-1) 

This contrasts sharply with the arkworks implementation which uses proper error handling: [3](#0-2) 

The DKG reconstruction is invoked during epoch transitions via the `DKGTrait::reconstruct_secret_from_shares()` method: [4](#0-3) 

Note that lines 483 and 498 call `.unwrap()` on the reconstruction result, which would propagate any panic from the underlying assertions.

**Attack Scenario During Epoch Transition:**

1. **Epoch N â†’ N+1 Transition Begins**: Validator set changes (e.g., validators join/leave, stake distribution shifts)
2. **DKG Share Collection**: Validators collect shares based on epoch N configuration
3. **Configuration Mismatch**: Due to network delays or validator crashes, the number of shares collected doesn't satisfy the new epoch's threshold requirements
4. **Reconstruction Attempted**: `reconstruct()` is called with shares count that violates assertions
5. **Validator Crash**: `assert_ge!` or `assert_le!` panics, crashing the validator process
6. **Consensus Impact**: If multiple validators crash simultaneously during epoch transition, consensus could stall

The validator set metadata tracking shows potential for mismatch: [5](#0-4) 

## Impact Explanation

**High Severity** - This qualifies as "Validator node slowdowns" and "Significant protocol violations" per the Aptos bug bounty criteria:

1. **Validator Availability**: Crashed validators cannot participate in consensus until manually restarted
2. **Epoch Transition Fragility**: Critical epoch transitions become vulnerable to timing issues
3. **Cascading Failures**: Multiple validators experiencing network delays could crash simultaneously
4. **Randomness System Disruption**: DKG failure impacts on-chain randomness generation for the next epoch

While not directly "halting consensus" (validators can restart), this creates operational instability during the critical epoch transition window and could contribute to liveness issues if enough validators crash.

## Likelihood Explanation

**Medium Likelihood**:

- **Triggering Conditions**: Requires validator set changes during epoch transitions AND network coordination issues
- **Not Externally Exploitable**: Cannot be directly triggered by malicious transactions
- **Natural Occurrence**: Can happen during normal network instability (Byzantine network conditions, slow validators, network partitions)
- **Epoch Frequency**: Occurs at every epoch boundary where DKG is active
- **Validator Diversity**: Heterogeneous network conditions increase probability

The issue is more likely a **reliability bug** than a deliberate attack vector, but the consequences during epoch transitions are significant.

## Recommendation

Replace hard assertions with graceful error handling matching the arkworks pattern:

```rust
impl Reconstructable<ThresholdConfigBlstrs> for Scalar {
    type ShareValue = Scalar;

    fn reconstruct(
        sc: &ThresholdConfigBlstrs,
        shares: &[ShamirShare<Self::ShareValue>],
    ) -> anyhow::Result<Self> {
        // Replace assertions with proper error returns
        if shares.len() < sc.get_threshold() {
            return Err(anyhow::anyhow!(
                "Insufficient shares for reconstruction: received {}, need at least {}",
                shares.len(),
                sc.get_threshold()
            ));
        }
        
        if shares.len() > sc.get_total_num_players() {
            return Err(anyhow::anyhow!(
                "Too many shares for reconstruction: received {}, maximum is {}",
                shares.len(),
                sc.get_total_num_players()
            ));
        }

        let ids = shares.iter().map(|(p, _)| p.id).collect::<Vec<usize>>();
        let lagr = lagrange_coefficients(
            sc.get_batch_evaluation_domain(),
            ids.as_slice(),
            &Scalar::ZERO,
        );
        let shares = shares
            .iter()
            .map(|(_, share)| *share)
            .collect::<Vec<Scalar>>();

        if lagr.len() != shares.len() {
            return Err(anyhow::anyhow!(
                "Lagrange coefficient count mismatch: {} coefficients for {} shares",
                lagr.len(),
                shares.len()
            ));
        }

        Ok(shares
            .iter()
            .zip(lagr.iter())
            .map(|(&share, &lagr)| share * lagr)
            .sum::<Scalar>())
    }
}
```

Apply the same pattern to `DealtSecretKey::reconstruct()` in `dealt_secret_key.rs`.

Additionally, update call sites to properly handle errors instead of using `.unwrap()`: [6](#0-5) 

## Proof of Concept

```rust
#[cfg(test)]
mod test_reconstruction_panic {
    use super::*;
    use aptos_crypto::blstrs::threshold_config::ThresholdConfigBlstrs;
    use aptos_crypto::arkworks::shamir::Reconstructable;
    use blstrs::Scalar;
    use ff::Field;
    
    #[test]
    #[should_panic(expected = "shares.len() >= sc.get_threshold()")]
    fn test_insufficient_shares_panic() {
        // Setup: 3-of-5 threshold config
        let threshold = 3;
        let total_players = 5;
        let tc = ThresholdConfigBlstrs::new(threshold, total_players).unwrap();
        
        // Simulate epoch transition scenario: only 2 shares collected (less than threshold)
        let shares = vec![
            (Player { id: 0 }, Scalar::ONE),
            (Player { id: 1 }, Scalar::ONE),
        ];
        
        // This will PANIC instead of returning an error
        let _ = Scalar::reconstruct(&tc, &shares);
    }
    
    #[test]
    #[should_panic(expected = "shares.len() <= sc.get_total_num_players()")]
    fn test_excessive_shares_panic() {
        // Setup: 3-of-5 threshold config
        let threshold = 3;
        let total_players = 5;
        let tc = ThresholdConfigBlstrs::new(threshold, total_players).unwrap();
        
        // Simulate misconfiguration: 6 shares for 5-player system
        let shares = vec![
            (Player { id: 0 }, Scalar::ONE),
            (Player { id: 1 }, Scalar::ONE),
            (Player { id: 2 }, Scalar::ONE),
            (Player { id: 3 }, Scalar::ONE),
            (Player { id: 4 }, Scalar::ONE),
            (Player { id: 5 }, Scalar::ONE), // Exceeds total_players
        ];
        
        // This will PANIC instead of returning an error
        let _ = Scalar::reconstruct(&tc, &shares);
    }
}
```

**Notes**

The vulnerability demonstrates inconsistent error handling patterns between the BLSTRS and arkworks cryptographic implementations. While the arkworks code gracefully returns `Result` errors, the BLSTRS implementation uses assertions that crash the process. During distributed protocols like DKG at epoch boundaries, such panics should be treated as recoverable errors rather than fatal programmer mistakes, allowing validators to remain operational even when coordination issues occur.

### Citations

**File:** crates/aptos-crypto/src/blstrs/scalar_secret_key.rs (L18-44)
```rust
    fn reconstruct(
        sc: &ThresholdConfigBlstrs,
        shares: &[ShamirShare<Self::ShareValue>],
    ) -> anyhow::Result<Self> {
        assert_ge!(shares.len(), sc.get_threshold());
        assert_le!(shares.len(), sc.get_total_num_players());

        let ids = shares.iter().map(|(p, _)| p.id).collect::<Vec<usize>>();
        let lagr = lagrange_coefficients(
            sc.get_batch_evaluation_domain(),
            ids.as_slice(),
            &Scalar::ZERO,
        );
        let shares = shares
            .iter()
            .map(|(_, share)| *share)
            .collect::<Vec<Scalar>>();

        // TODO should this return a
        assert_eq!(lagr.len(), shares.len());

        Ok(shares
            .iter()
            .zip(lagr.iter())
            .map(|(&share, &lagr)| share * lagr)
            .sum::<Scalar>())
    }
```

**File:** crates/aptos-dkg/src/pvss/dealt_secret_key.rs (L86-123)
```rust
        impl Reconstructable<ThresholdConfigBlstrs> for DealtSecretKey {
            type ShareValue = DealtSecretKeyShare;

            /// Reconstructs the `DealtSecretKey` given a sufficiently-large subset of shares from players.
            /// Mainly used for testing the PVSS transcript dealing and decryption.
            fn reconstruct(sc: &ThresholdConfigBlstrs, shares: &[ShamirShare<Self::ShareValue>]) -> anyhow::Result<Self> {
                assert_ge!(shares.len(), sc.get_threshold());
                assert_le!(shares.len(), sc.get_total_num_players());

                let ids = shares.iter().map(|(p, _)| p.id).collect::<Vec<usize>>();
                let lagr = lagrange_coefficients(
                    sc.get_batch_evaluation_domain(),
                    ids.as_slice(),
                    &Scalar::ZERO,
                );
                let bases = shares
                    .iter()
                    .map(|(_, share)| *share.as_group_element())
                    .collect::<Vec<$GTProjective>>();

                // println!();
                // println!("Lagrange IDs: {:?}", ids);
                // println!("Lagrange coeffs");
                // for l in lagr.iter() {
                // println!(" + {}", hex::encode(l.to_bytes_le()));
                // }
                // println!("Bases: ");
                // for b in bases.iter() {
                // println!(" + {}", hex::encode(b.to_bytes()));
                // }

                assert_eq!(lagr.len(), bases.len());

                Ok(DealtSecretKey {
                    h_hat: $gt_multi_exp(bases.as_slice(), lagr.as_slice()),
                })
            }
        }
```

**File:** crates/aptos-crypto/src/arkworks/shamir.rs (L305-331)
```rust
impl<T: WeightedSum> Reconstructable<ShamirThresholdConfig<T::Scalar>> for T {
    type ShareValue = T;

    // Can receive more than `sc.t` shares, but will only use the first `sc.t` shares for efficiency
    fn reconstruct(
        sc: &ShamirThresholdConfig<T::Scalar>,
        shares: &[ShamirShare<Self::ShareValue>],
    ) -> Result<Self> {
        if shares.len() < sc.t {
            Err(anyhow!(
                "Incorrect number of shares provided, received {} but expected at least {}",
                shares.len(),
                sc.t
            ))
        } else {
            let (roots_of_unity_indices, bases): (Vec<usize>, Vec<Self::ShareValue>) = shares
                [..sc.t]
                .iter()
                .map(|(p, g_y)| (p.get_id(), g_y))
                .collect();

            let lagrange_coeffs = sc.lagrange_for_subset(&roots_of_unity_indices);

            Ok(T::weighted_sum(&bases, &lagrange_coeffs))
        }
    }
}
```

**File:** types/src/dkg/real_dkg/mod.rs (L470-505)
```rust
    fn reconstruct_secret_from_shares(
        pub_params: &Self::PublicParams,
        input_player_share_pairs: Vec<(u64, Self::DealtSecretShare)>,
    ) -> anyhow::Result<Self::DealtSecret> {
        let player_share_pairs: Vec<_> = input_player_share_pairs
            .clone()
            .into_iter()
            .map(|(x, y)| (Player { id: x as usize }, y.main))
            .collect();
        let reconstructed_secret = <WTrx as Transcript>::DealtSecretKey::reconstruct(
            &pub_params.pvss_config.wconfig,
            &player_share_pairs,
        )
        .unwrap();
        if input_player_share_pairs
            .clone()
            .into_iter()
            .all(|(_, y)| y.fast.is_some())
            && pub_params.pvss_config.fast_wconfig.is_some()
        {
            let fast_player_share_pairs: Vec<_> = input_player_share_pairs
                .into_iter()
                .map(|(x, y)| (Player { id: x as usize }, y.fast.unwrap()))
                .collect();
            let fast_reconstructed_secret = <WTrx as Transcript>::DealtSecretKey::reconstruct(
                pub_params.pvss_config.fast_wconfig.as_ref().unwrap(),
                &fast_player_share_pairs,
            )
            .unwrap();
            ensure!(
                reconstructed_secret == fast_reconstructed_secret,
                "real_dkg::reconstruct_secret_from_shares failed with inconsistent dealt secrets."
            );
        }
        Ok(reconstructed_secret)
    }
```

**File:** aptos-move/framework/aptos-framework/sources/dkg.move (L1-50)
```text
/// DKG on-chain states and helper functions.
module aptos_framework::dkg {
    use std::error;
    use std::option;
    use std::option::Option;
    use aptos_framework::event::emit;
    use aptos_framework::randomness_config::RandomnessConfig;
    use aptos_framework::system_addresses;
    use aptos_framework::timestamp;
    use aptos_framework::validator_consensus_info::ValidatorConsensusInfo;
    friend aptos_framework::block;
    friend aptos_framework::reconfiguration_with_dkg;

    const EDKG_IN_PROGRESS: u64 = 1;
    const EDKG_NOT_IN_PROGRESS: u64 = 2;

    /// This can be considered as the public input of DKG.
    struct DKGSessionMetadata has copy, drop, store {
        dealer_epoch: u64,
        randomness_config: RandomnessConfig,
        dealer_validator_set: vector<ValidatorConsensusInfo>,
        target_validator_set: vector<ValidatorConsensusInfo>,
    }

    #[event]
    struct DKGStartEvent has drop, store {
        session_metadata: DKGSessionMetadata,
        start_time_us: u64,
    }

    /// The input and output of a DKG session.
    /// The validator set of epoch `x` works together for an DKG output for the target validator set of epoch `x+1`.
    struct DKGSessionState has copy, store, drop {
        metadata: DKGSessionMetadata,
        start_time_us: u64,
        transcript: vector<u8>,
    }

    /// The completed and in-progress DKG sessions.
    struct DKGState has key {
        last_completed: Option<DKGSessionState>,
        in_progress: Option<DKGSessionState>,
    }

    /// Called in genesis to initialize on-chain states.
    public fun initialize(aptos_framework: &signer) {
        system_addresses::assert_aptos_framework(aptos_framework);
        if (!exists<DKGState>(@aptos_framework)) {
            move_to<DKGState>(
                aptos_framework,
```
