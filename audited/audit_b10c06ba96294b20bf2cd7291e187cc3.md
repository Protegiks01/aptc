# Audit Report

## Title
Indefinite Validator Blocking via Unbounded State Sync Duration with Pending Storage Data

## Summary

The `sync_for_duration()` implementation can exceed the specified duration indefinitely when the storage synchronizer has pending data that never completes processing. This occurs because: (1) the consensus notification sender has no timeout on the callback response, (2) the state sync driver loops indefinitely waiting for pending storage data to drain, and (3) the consensus layer holds a critical mutex for the entire operation. If triggered by corrupted data from malicious peers or network issues, validators become permanently stuck and cannot return to consensus participation.

## Finding Description

The vulnerability exists across three interconnected components:

**1. Unbounded Callback Wait in ConsensusNotifier** [1](#0-0) 

The `sync_for_duration()` method performs an unbounded await on `callback_receiver.await` with no timeout enforcement. Unlike `notify_new_commit()` which applies a timeout, this method waits indefinitely for state sync to respond.

**2. Infinite Loop Waiting for Pending Data** [2](#0-1) 

When the sync duration has elapsed, the state sync driver enters an infinite loop waiting for `storage_synchronizer.pending_storage_data()` to return false. The loop only yields to avoid starvation but has no timeout or upper bound on iterations. If pending data never completes (due to corrupted chunks, network issues, or processing bugs), this loop never terminates.

**3. Mutex Held During Entire Operation** [3](#0-2) 

The consensus `ExecutionProxy.sync_for_duration()` acquires the `write_mutex` at the start and holds it until the function returns. Since state sync may never respond, this lock is never released, blocking all future calls to `sync_for_duration()` and `sync_to_target()`.

**Exploitation Path:**

1. Validator calls `sync_for_duration(Duration::from_secs(10))` during consensus observer fallback or recovery
2. Consensus acquires `write_mutex` and sends notification to state sync
3. State sync begins syncing data from peers
4. Malicious peer sends corrupted state chunk that causes chunk executor to hang or fail to complete
5. Storage synchronizer marks chunk as pending but processing never completes
6. `pending_storage_data()` returns true indefinitely
7. State sync loops forever at lines 556-564 of driver.rs, never calling `handle_satisfied_sync_request()`
8. State sync never sends callback response
9. Consensus's `callback_receiver.await` never completes
10. `write_mutex` is never released
11. Any subsequent sync operations block on the mutex
12. Validator cannot participate in consensus and is effectively dead

**Attack Vector:**

A malicious peer can send specially crafted state chunks that:
- Contain invalid proofs that cause verification to hang
- Reference non-existent state keys that cause lookups to stall
- Trigger edge cases in chunk processing logic

The trait definition acknowledges this possibility but provides no safeguards: [4](#0-3) 

## Impact Explanation

**Severity: Medium** (potentially High depending on frequency)

This vulnerability causes **validator liveness failure**, which maps to the Medium severity category: "State inconsistencies requiring intervention". The impact includes:

1. **Validator Unavailability**: Affected validator cannot participate in consensus, reducing network decentralization
2. **Consensus Impact**: If multiple validators are affected simultaneously, consensus may slow or halt (if > 1/3 become unavailable)
3. **Recovery Requires Restart**: The validator must be manually restarted to recover, as there is no automatic timeout
4. **Amplification Risk**: During network-wide sync events (epoch changes, upgrades), multiple validators could be affected simultaneously

While not causing direct fund loss or safety violations, this breaks the liveness invariant critical to blockchain operation. According to Aptos bug bounty criteria, "Validator node slowdowns" qualify as High severity, and this vulnerability can cause complete validator stoppage.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability is likely to occur in production because:

1. **Natural Triggers**: Network partitions, slow peers, or temporary storage issues during sync can cause pending data to accumulate
2. **Malicious Exploitation**: Attackers can deliberately send corrupted data to trigger the hang
3. **No Defensive Timeouts**: The code has no upper bounds or safety timeouts at any layer
4. **Common Code Path**: `sync_for_duration()` is called during consensus observer fallback mode, which is a regular operational scenario [5](#0-4) 

The consensus observer calls this method without any timeout wrapper, making validators vulnerable during fallback synchronization.

## Recommendation

Implement multi-layer timeout protection:

**1. Add Timeout to ConsensusNotifier.sync_for_duration():**

```rust
async fn sync_for_duration(
    &self,
    duration: Duration,
) -> Result<LedgerInfoWithSignatures, Error> {
    // ... send notification ...
    
    // Add timeout with buffer (e.g., 2x requested duration)
    let timeout_duration = duration.checked_mul(2).unwrap_or(duration);
    if let Ok(response) = timeout(timeout_duration, callback_receiver).await {
        // Process response...
    } else {
        Err(Error::TimeoutWaitingForStateSync)
    }
}
```

**2. Add Maximum Iteration Limit in check_sync_request_progress():**

```rust
// Add configurable max wait time
const MAX_PENDING_DATA_WAIT_SECS: u64 = 60;
let wait_start = self.time_service.now();

while self.storage_synchronizer.pending_storage_data() {
    // Check timeout
    if self.time_service.now().duration_since(wait_start) 
        > Duration::from_secs(MAX_PENDING_DATA_WAIT_SECS) {
        warn!("Timeout waiting for pending storage data to drain");
        return Err(Error::PendingDataTimeout);
    }
    
    sample!(
        SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
        info!("Waiting for the storage synchronizer to handle pending data!")
    );
    yield_now().await;
}
```

**3. Add Watchdog Timer in ExecutionProxy:**

Wrap the state sync call with a timeout that's longer than the requested duration:

```rust
async fn sync_for_duration(
    &self,
    duration: Duration,
) -> Result<LedgerInfoWithSignatures, StateSyncError> {
    let mut latest_logical_time = self.write_mutex.lock().await;
    self.executor.finish();
    
    // Add safety timeout (e.g., 3x requested duration)
    let safety_timeout = duration.checked_mul(3).unwrap_or(Duration::from_secs(300));
    let result = match timeout(safety_timeout, 
        self.state_sync_notifier.sync_for_duration(duration)
    ).await {
        Ok(result) => result,
        Err(_) => {
            error!("State sync exceeded safety timeout");
            Err(StateSyncError::from(anyhow::anyhow!("Timeout exceeded")))
        }
    };
    
    // ... rest of function ...
}
```

## Proof of Concept

**Rust Integration Test Demonstrating the Hang:**

```rust
#[tokio::test(flavor = "multi_thread")]
async fn test_sync_for_duration_indefinite_hang() {
    // Setup: Create mock storage synchronizer that simulates stuck pending data
    struct StuckStorageSynchronizer {
        pending: AtomicBool,
    }
    
    impl StorageSynchronizerInterface for StuckStorageSynchronizer {
        fn pending_storage_data(&self) -> bool {
            // Simulate permanently stuck pending data
            self.pending.load(Ordering::Relaxed)
        }
        // ... other trait methods ...
    }
    
    // Setup state sync driver with stuck synchronizer
    let stuck_sync = Arc::new(StuckStorageSynchronizer {
        pending: AtomicBool::new(true), // Never completes
    });
    
    let (consensus_notifier, consensus_listener) = 
        new_consensus_notifier_listener_pair(5000);
    
    // Initialize state sync driver
    let mut driver = StateSyncDriver::new(
        // ... dependencies ...
        stuck_sync,
        // ...
    );
    
    // Start driver in background
    tokio::spawn(async move {
        driver.start_driver().await;
    });
    
    // Validator calls sync_for_duration
    let sync_handle = tokio::spawn(async move {
        consensus_notifier.sync_for_duration(Duration::from_secs(1)).await
    });
    
    // Wait for reasonable time (should complete in ~1 second)
    let result = timeout(Duration::from_secs(10), sync_handle).await;
    
    // ASSERTION: This should timeout because state sync never responds
    assert!(result.is_err(), "sync_for_duration should have timed out but didn't!");
    
    // The validator is now stuck - write_mutex is held indefinitely
    // Any subsequent sync operations would block forever
}
```

**Simulation Steps:**

1. Configure a validator node with consensus observer enabled
2. Use network fault injection to simulate slow/corrupted peer data during fallback sync
3. Observe validator entering `sync_for_duration()` 
4. Inject corrupted state chunk that causes chunk executor to hang
5. Monitor that `pending_storage_data()` remains true
6. Verify validator never returns from sync and cannot participate in consensus
7. Confirm node restart is required to recover

## Notes

- This vulnerability is particularly dangerous during epoch transitions or network upgrades when multiple validators may sync simultaneously
- The issue is exacerbated by the lack of observability - the validator appears "stuck" with minimal logging
- The comment in the trait definition acknowledges syncing can exceed duration but provides no implementation safeguards
- Defense-in-depth requires timeouts at multiple layers: consensus, notification layer, and driver layer

### Citations

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L140-179)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, Error> {
        // Create a consensus sync duration notification
        let (notification, callback_receiver) = ConsensusSyncDurationNotification::new(duration);
        let sync_duration_notification = ConsensusNotification::SyncForDuration(notification);

        // Send the notification to state sync
        if let Err(error) = self
            .notification_sender
            .clone()
            .send(sync_duration_notification)
            .await
        {
            return Err(Error::NotificationError(format!(
                "Failed to notify state sync of sync duration! Error: {:?}",
                error
            )));
        }

        // Process the response
        match callback_receiver.await {
            Ok(response) => match response.get_result() {
                Ok(_) => response.get_latest_synced_ledger_info().ok_or_else(|| {
                    Error::UnexpectedErrorEncountered(
                        "Sync for duration returned an empty latest synced ledger info!".into(),
                    )
                }),
                Err(error) => Err(Error::UnexpectedErrorEncountered(format!(
                    "Sync for duration returned an error: {:?}",
                    error
                ))),
            },
            Err(error) => Err(Error::UnexpectedErrorEncountered(format!(
                "Sync for duration failure: {:?}",
                error
            ))),
        }
    }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L554-564)
```rust
        // The sync request has been satisfied. Wait for the storage synchronizer
        // to drain. This prevents notifying consensus prematurely.
        while self.storage_synchronizer.pending_storage_data() {
            sample!(
                SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                info!("Waiting for the storage synchronizer to handle pending data!")
            );

            // Yield to avoid starving the storage synchronizer threads.
            yield_now().await;
        }
```

**File:** consensus/src/state_computer.rs (L132-174)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        // Grab the logical time lock
        let mut latest_logical_time = self.write_mutex.lock().await;

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by the BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // Inject an error for fail point testing
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Invoke state sync to synchronize for the specified duration. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_for_duration",
            self.state_sync_notifier.sync_for_duration(duration).await
        );

        // Update the latest logical time
        if let Ok(latest_synced_ledger_info) = &result {
            let ledger_info = latest_synced_ledger_info.ledger_info();
            let synced_logical_time = LogicalTime::new(ledger_info.epoch(), ledger_info.round());
            *latest_logical_time = synced_logical_time;
        }

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```

**File:** consensus/src/state_replication.rs (L24-31)
```rust
    /// Best effort state synchronization for the specified duration.
    /// This function returns the latest synced ledger info after state syncing.
    /// Note: it is possible that state sync may run longer than the specified
    /// duration (e.g., if the node is very far behind).
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError>;
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L150-153)
```rust
                let latest_synced_ledger_info = match execution_client
                    .clone()
                    .sync_for_duration(fallback_duration)
                    .await
```
