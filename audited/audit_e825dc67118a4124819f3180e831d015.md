# Audit Report

## Title
V2 Batches Not Deleted from Database During Expiration Leading to Unbounded Database Growth and State Inconsistency

## Summary
The quorum store batch expiration mechanism fails to delete V2 batches from persistent storage during normal operation. While V2 batches are correctly removed from the in-memory cache, they remain in the database indefinitely, causing unbounded database growth and state inconsistency between cache and persistent storage.

## Finding Description

The Aptos consensus quorum store manages transaction batches using two separate storage schemas with distinct column families: `BatchSchema` (V1) uses the "batch" column family, while `BatchV2Schema` (V2) uses the "batch_v2" column family. [1](#0-0) 

The in-memory cache (`db_cache`) stores both V1 and V2 batches as `PersistedValue<BatchInfoExt>`. [2](#0-1) 

When batches are persisted to disk, the system correctly routes them based on version - V1 batches use `save_batch()` while V2 batches use `save_batch_v2()`. [3](#0-2) 

However, **two critical bugs exist in the expiration/garbage collection logic**:

**Bug #1**: In `update_certified_timestamp()`, expired batches are removed from cache by `clear_expired_payload()` (which handles both V1 and V2), but the subsequent database deletion only calls `delete_batches()` which operates on `BatchSchema` (V1) only. [4](#0-3) 

The `delete_batches()` method only deletes from the "batch" column family. [5](#0-4) 

The separate `delete_batches_v2()` method exists for V2 batches but is never called. [6](#0-5) 

**Bug #2**: In epoch-based garbage collection, `gc_previous_epoch_batches_from_db_v2()` reads V2 batches using `get_all_batches_v2()` but attempts to delete them using `delete_batches()` instead of `delete_batches_v2()`. [7](#0-6) 

**Trigger Condition**: This bug is triggered when `enable_batch_v2` configuration is set to true. [8](#0-7)  When enabled, the batch generator creates V2 batches. [9](#0-8) 

**Attack Propagation**:
1. Validator node with `enable_batch_v2: true` creates and persists V2 batches
2. Batches are stored in both cache and "batch_v2" database column family
3. As batches expire, `clear_expired_payload()` removes them from cache
4. `delete_batches()` is called, but only touches "batch" column family (V1)
5. V2 batches remain permanently in "batch_v2" column family
6. Database grows unbounded as more V2 batches accumulate
7. On node restart, `populate_cache_and_gc_expired_batches_v2()` correctly uses `delete_batches_v2()`, but expired batches accumulated during runtime persist

This breaks the **State Consistency** invariant: the cache and persistent storage maintain inconsistent views of what batches exist.

## Impact Explanation

**High Severity** - This vulnerability qualifies under Aptos bug bounty "Validator node slowdowns" criteria:

1. **Unbounded Database Growth**: V2 batches accumulate indefinitely in the database, never being deleted during normal operation.

2. **Resource Exhaustion**: As the database grows, disk I/O operations slow down, eventually exhausting available disk space.

3. **Validator Performance Degradation**: Increased database size impacts read/write performance, slowing down consensus operations and potentially causing validators to fall behind.

4. **State Inconsistency**: The cache (correctly cleared) and database (incorrectly retaining expired data) diverge, violating fundamental storage invariants. This could cause unexpected behavior in batch availability checks.

5. **Cumulative Impact**: Unlike transient bugs, this issue worsens continuously over time, eventually causing node failures.

## Likelihood Explanation

**Conditional High Likelihood**:

This is a **logic vulnerability** - the code objectively contains a programming error where read and write operations use mismatched schema methods. 

**When Triggered** (if `enable_batch_v2: true`):
1. **Automatic Activation**: No attacker action required - occurs during normal consensus operation
2. **Guaranteed Occurrence**: Every expired V2 batch triggers the bug
3. **Continuous Accumulation**: Impact compounds with every batch expiration
4. **No Recovery**: Runtime cleanup never occurs; only restart triggers correct V2 deletion

**Current Status**: The `enable_batch_v2` flag defaults to false, limiting immediate production impact. [10](#0-9)  However, the feature exists and can be enabled through configuration, making this a latent bug that becomes critical when activated.

## Recommendation

Fix both bugs by calling the correct deletion method for V2 batches:

**Fix #1** - In `update_certified_timestamp()`:
```rust
pub fn update_certified_timestamp(&self, certified_time: u64) {
    trace!("QS: batch reader updating time {:?}", certified_time);
    self.last_certified_time
        .fetch_max(certified_time, Ordering::SeqCst);

    let expired_keys = self.clear_expired_payload(certified_time);
    // Split expired keys by batch version and delete from correct schema
    let (v1_keys, v2_keys): (Vec<_>, Vec<_>) = expired_keys
        .into_iter()
        .partition(|k| {
            self.db_cache.get(k)
                .map(|v| !v.batch_info().is_v2())
                .unwrap_or(true)
        });
    
    if let Err(e) = self.db.delete_batches(v1_keys) {
        debug!("Error deleting V1 batches: {:?}", e)
    }
    if let Err(e) = self.db.delete_batches_v2(v2_keys) {
        debug!("Error deleting V2 batches: {:?}", e)
    }
}
```

**Fix #2** - In `gc_previous_epoch_batches_from_db_v2()`:
```rust
fn gc_previous_epoch_batches_from_db_v2(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
    let db_content = db
        .get_all_batches_v2()
        .expect("failed to read data from db");
    // ... existing logic ...
    db.delete_batches_v2(expired_keys)  // Changed from delete_batches
        .expect("Deletion of expired keys should not fail");
}
```

## Proof of Concept

The bug can be demonstrated by examining the code flow:

1. V2 batch creation occurs when `enable_batch_v2: true`
2. Persistence uses correct method: `save_batch_v2()`
3. Expiration uses wrong method: `delete_batches()` instead of `delete_batches_v2()`
4. V2 batches remain in "batch_v2" column family indefinitely

The schema separation confirms these are distinct storage locations that require matching operations for consistency.

## Notes

This is a **logic vulnerability** - a clear programming error where database operations use mismatched schema methods. While currently mitigated by the feature being disabled by default, it represents a serious bug that would cause immediate and escalating problems if V2 batches were enabled in production or testing environments. The vulnerability demonstrates a violation of the basic software engineering principle that paired operations (create/delete, open/close) must use matching methods when multiple versions exist.

### Citations

**File:** consensus/src/quorum_store/schema.rs (L14-56)
```rust
pub(crate) const BATCH_CF_NAME: ColumnFamilyName = "batch";
pub(crate) const BATCH_ID_CF_NAME: ColumnFamilyName = "batch_ID";
pub(crate) const BATCH_V2_CF_NAME: ColumnFamilyName = "batch_v2";

#[derive(Debug)]
pub(crate) struct BatchSchema;

impl Schema for BatchSchema {
    type Key = HashValue;
    type Value = PersistedValue<BatchInfo>;

    const COLUMN_FAMILY_NAME: aptos_schemadb::ColumnFamilyName = BATCH_CF_NAME;
}

impl KeyCodec<BatchSchema> for HashValue {
    fn encode_key(&self) -> Result<Vec<u8>> {
        Ok(self.to_vec())
    }

    fn decode_key(data: &[u8]) -> Result<Self> {
        Ok(HashValue::from_slice(data)?)
    }
}

impl ValueCodec<BatchSchema> for PersistedValue<BatchInfo> {
    fn encode_value(&self) -> Result<Vec<u8>> {
        Ok(bcs::to_bytes(&self)?)
    }

    fn decode_value(data: &[u8]) -> Result<Self> {
        Ok(bcs::from_bytes(data)?)
    }
}

#[derive(Debug)]
pub(crate) struct BatchV2Schema;

impl Schema for BatchV2Schema {
    type Key = HashValue;
    type Value = PersistedValue<BatchInfoExt>;

    const COLUMN_FAMILY_NAME: aptos_schemadb::ColumnFamilyName = BATCH_V2_CF_NAME;
}
```

**File:** consensus/src/quorum_store/batch_store.rs (L113-126)
```rust
pub struct BatchStore {
    epoch: OnceCell<u64>,
    last_certified_time: AtomicU64,
    db_cache: DashMap<HashValue, PersistedValue<BatchInfoExt>>,
    peer_quota: DashMap<PeerId, QuotaManager>,
    expirations: Mutex<TimeExpirations<HashValue>>,
    db: Arc<dyn QuorumStoreStorage>,
    memory_quota: usize,
    db_quota: usize,
    batch_quota: usize,
    validator_signer: ValidatorSigner,
    persist_subscribers: DashMap<HashValue, Vec<oneshot::Sender<PersistedValue<BatchInfoExt>>>>,
    expiration_buffer_usecs: u64,
}
```

**File:** consensus/src/quorum_store/batch_store.rs (L212-243)
```rust
    fn gc_previous_epoch_batches_from_db_v2(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L500-513)
```rust
                if needs_db {
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L93-101)
```rust
    fn delete_batches(&self, digests: Vec<HashValue>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        for digest in digests.iter() {
            trace!("QS: db delete digest {}", digest);
            batch.delete::<BatchSchema>(digest)?;
        }
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L123-131)
```rust
    fn delete_batches_v2(&self, digests: Vec<HashValue>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        for digest in digests.iter() {
            trace!("QS: db delete digest {}", digest);
            batch.delete::<BatchV2Schema>(digest)?;
        }
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** config/src/config/quorum_store_config.rs (L102-102)
```rust
    pub enable_batch_v2: bool,
```

**File:** config/src/config/quorum_store_config.rs (L144-144)
```rust
            enable_batch_v2: false,
```

**File:** consensus/src/quorum_store/batch_generator.rs (L190-210)
```rust
        if self.config.enable_batch_v2 {
            // TODO(ibalajiarun): Specify accurate batch kind
            let batch_kind = BatchKind::Normal;
            Batch::new_v2(
                batch_id,
                txns,
                self.epoch,
                expiry_time,
                self.my_peer_id,
                bucket_start,
                batch_kind,
            )
        } else {
            Batch::new_v1(
                batch_id,
                txns,
                self.epoch,
                expiry_time,
                self.my_peer_id,
                bucket_start,
            )
```
