# Audit Report

## Title
Validator Consensus Divergence Due to Non-Deterministic InternedModuleId Assignment Across Differential Cache Flush Points

## Summary
Different validators can permanently diverge in their execution results due to non-deterministic `InternedModuleId` and `StructNameIndex` assignments when their local cache configurations cause them to flush at different points. This breaks the fundamental consensus invariant requiring deterministic execution across all validators.

## Finding Description

The vulnerability exists in the interaction between several components:

1. **Local Configuration Variability**: The `BlockExecutorModuleCacheLocalConfig` contains `max_interned_module_ids` as a local, per-node setting that determines when to flush the module ID interning pool. [1](#0-0) 

2. **Flush Decision Logic**: The `check_ready()` function flushes the module ID pool, struct name index map, and module cache when the threshold is exceeded. [2](#0-1) 

3. **Sequential Index Assignment**: The interning mechanism assigns indices sequentially based on insertion order (`idx = vec.len() - 1`), not based on any deterministic property of the value itself. [3](#0-2) 

4. **Cross-Block Cache Persistence**: The `RuntimeEnvironment` uses `Arc`-wrapped pools that persist across blocks, meaning modules loaded in previous blocks retain their interned IDs unless flushed. [4](#0-3) 

5. **Equality Semantics**: Both `StructIdentifier` and `Type` derive `Eq` which includes `interned_module_id` and `StructNameIndex` fields in comparisons. [5](#0-4) [6](#0-5) 

**Attack Scenario:**

Consider two honest validators with different local configurations:
- Validator A: `max_interned_module_ids = 100,000` (default)
- Validator B: `max_interned_module_ids = 50,000` (custom)

After executing blocks 1-99:
- Validator A has 60,000 interned module IDs
- Validator B has 50,001 interned module IDs

Before executing block 100:
- Validator A: `60,000 < 100,000` → does NOT flush
- Validator B: `50,001 > 50,000` → FLUSHES all caches

During block 100 execution for module `0x1::coin`:
- **Validator A**: Module already loaded, has `InternedModuleId(15000)` → `StructIdentifier { module: 0x1::coin, interned_module_id: 15000, name: "Coin" }` → looks up in struct map, gets existing or new `StructNameIndex`
- **Validator B**: Module freshly loaded after flush, gets `InternedModuleId(0)` → `StructIdentifier { module: 0x1::coin, interned_module_id: 0, name: "Coin" }` → looks up in struct map (now empty), creates `StructNameIndex(0)`

The `StructNameIndexMap` uses `StructIdentifier` as the key in its `BTreeMap`: [7](#0-6) 

Since `StructIdentifier` equality includes `interned_module_id`, the same logical struct `0x1::coin::Coin` is treated as DIFFERENT on the two validators. This leads to different `StructNameIndex` assignments, which are then embedded in `Type::Struct` representations used throughout execution.

Type equality comparisons during bytecode execution will then diverge, causing different execution paths and ultimately different state roots, violating the core consensus invariant.

## Impact Explanation

**Critical Severity - Consensus/Safety Violation**

This vulnerability directly violates the first and most fundamental invariant: **"Deterministic Execution: All validators must produce identical state roots for identical blocks"**.

When validators diverge:
1. They produce different state roots for the same block
2. They cannot reach consensus on the block's validity
3. The blockchain forks, with different validators on different branches
4. This requires a hard fork to recover, as there's no automatic reconciliation mechanism

This falls squarely under the Critical Severity category: "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)".

## Likelihood Explanation

**HIGH Likelihood**

This vulnerability is HIGHLY likely to occur in production because:

1. **Legitimate Configuration Differences**: Validator operators have legitimate reasons to tune `max_interned_module_ids` based on their hardware (memory capacity, performance characteristics). The configuration is explicitly documented as "Local, per-node configurations".

2. **Gradual Divergence**: As the network processes transactions over time, different validators will reach their configured thresholds at different blocks, causing the divergence to manifest eventually.

3. **No Warning Mechanism**: There is no validation that all validators use the same local configuration values, and no runtime checks to detect when validators begin diverging.

4. **Persistent State**: The Arc-wrapped pools persist across blocks, so once divergence occurs, it compounds over subsequent blocks.

## Recommendation

**Fix: Make interning deterministic or sync across validators**

Option 1 (Preferred): Make index assignment deterministic by using a hash or ordered key:

```rust
// In ConcurrentBTreeInterner::intern_deferred()
// Instead of: let idx = inner.vec.len() - 1;
// Use deterministic assignment based on value hash or canonical ordering:

let idx = if let Some(&existing_idx) = inner.map.get(&val) {
    existing_idx
} else {
    // Assign index based on deterministic hash of the value
    let deterministic_idx = compute_deterministic_index(&val, &inner.vec);
    inner.vec.insert_at(deterministic_idx, r);
    inner.map.insert(r, deterministic_idx);
    deterministic_idx
};
```

Option 2: Move cache size limits to on-chain configuration:

```rust
// In types/src/block_executor/config.rs
// Move max_interned_module_ids from BlockExecutorModuleCacheLocalConfig 
// to BlockExecutorConfigFromOnchain

#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct BlockExecutorConfigFromOnchain {
    pub block_gas_limit_type: BlockGasLimitType,
    enable_per_block_gas_limit: bool,
    per_block_gas_limit: Option<u64>,
    gas_price_to_burn: Option<u64>,
    // ADD:
    pub max_interned_module_ids: usize,
}
```

Option 3: Flush synchronization - require all validators to flush at block boundaries based on on-chain triggers rather than local thresholds.

## Proof of Concept

```rust
// Reproduction test demonstrating the divergence
#[test]
fn test_interned_module_id_divergence() {
    use move_core_types::{account_address::AccountAddress, identifier::Identifier, language_storage::ModuleId};
    use move_vm_types::module_id_interner::InternedModuleIdPool;
    use move_vm_types::loaded_data::runtime_types::StructIdentifier;
    
    // Simulate two validators
    let pool_a = InternedModuleIdPool::new();
    let pool_b = InternedModuleIdPool::new();
    
    // Validator A loads modules 0-100
    let mut modules_a = vec![];
    for i in 0..101 {
        let module = ModuleId::new(AccountAddress::ONE, Identifier::new(format!("M{}", i)).unwrap());
        modules_a.push(pool_a.intern(module));
    }
    
    // Validator B loads modules 0-100 then FLUSHES (simulating threshold breach)
    for i in 0..101 {
        let module = ModuleId::new(AccountAddress::ONE, Identifier::new(format!("M{}", i)).unwrap());
        pool_b.intern(module);
    }
    pool_b.flush(); // FLUSH EVENT
    
    // Both validators now load the same module in block N
    let target_module = ModuleId::new(AccountAddress::ONE, Identifier::new("coin").unwrap());
    let id_a = pool_a.intern_by_ref(&target_module);
    let id_b = pool_b.intern_by_ref(&target_module);
    
    // Create StructIdentifiers
    let struct_a = StructIdentifier::new(&pool_a, target_module.clone(), Identifier::new("Coin").unwrap());
    let struct_b = StructIdentifier::new(&pool_b, target_module, Identifier::new("Coin").unwrap());
    
    // ASSERTION FAILS: Same struct has different representations!
    assert_ne!(struct_a, struct_b, "Validators diverged: same struct appears different!");
    assert_ne!(id_a, id_b, "Different InternedModuleId for same module!");
    
    println!("Validator A: {:?}", struct_a);
    println!("Validator B: {:?}", struct_b);
    println!("CONSENSUS BREAK: Validators will produce different state roots!");
}
```

## Notes

This vulnerability is particularly insidious because:
1. It only manifests after extended network operation when caches fill up
2. Different validators may diverge at different times based on their transaction load and configuration
3. The divergence is silent - there are no error messages or warnings
4. Once divergence occurs, it persists and compounds across subsequent blocks
5. The issue is rooted in a fundamental design assumption that local optimizations (interning) don't affect determinism

The fix requires either making the interning mechanism deterministic across all validators, or ensuring all validators use identical configurations for cache management by moving these settings to on-chain governance.

### Citations

**File:** types/src/block_executor/config.rs (L9-29)
```rust
/// Local, per-node configurations for module cache. While caches can be persisted across multiple
/// block executions, these configurations allow to specify cache sizes, etc.
#[derive(Clone, Debug)]
pub struct BlockExecutorModuleCacheLocalConfig {
    /// If true, when global caches are empty, Aptos framework is prefetched into module cache.
    pub prefetch_framework_code: bool,
    /// The maximum size of module cache (the sum of serialized sizes of all cached modules in
    /// bytes).
    pub max_module_cache_size_in_bytes: usize,
    /// The maximum size (in terms of entries) of struct name re-indexing map stored in the runtime
    /// environment.
    pub max_struct_name_index_map_num_entries: usize,
    /// The maximum number of types to intern.
    pub max_interned_tys: usize,
    /// The maximum number of type vectors to intern.
    pub max_interned_ty_vecs: usize,
    /// The maximum number of layout entries.
    pub max_layout_cache_size: usize,
    /// The maximum number of module IDs to intern.
    pub max_interned_module_ids: usize,
}
```

**File:** aptos-move/block-executor/src/code_cache_global_manager.rs (L162-166)
```rust
        if num_interned_module_ids > config.max_interned_module_ids {
            runtime_environment.module_id_pool().flush();
            runtime_environment.struct_name_index_map().flush();
            self.module_cache.flush();
        }
```

**File:** third_party/move/move-vm/types/src/interner.rs (L157-163)
```rust
        unsafe {
            let r = inner.alloc(val);
            inner.vec.push(r);
            let idx = inner.vec.len() - 1;
            inner.map.insert(r, idx);
            idx
        }
```

**File:** third_party/move/move-vm/runtime/src/storage/environment.rs (L69-74)
```rust
    /// Pool of interned type representations. Same lifetime as struct index map.
    interned_ty_pool: Arc<InternedTypePool>,

    /// Pool of interned module ids.
    interned_module_id_pool: Arc<InternedModuleIdPool>,
}
```

**File:** third_party/move/move-vm/types/src/loaded_data/runtime_types.rs (L262-267)
```rust
#[derive(Debug, Clone, Eq, Hash, Ord, PartialEq, PartialOrd)]
pub struct StructIdentifier {
    module: ModuleId,
    interned_module_id: InternedModuleId,
    name: Identifier,
}
```

**File:** third_party/move/move-vm/types/src/loaded_data/runtime_types.rs (L296-313)
```rust
#[derive(Debug, Clone, Eq, Hash, Ord, PartialEq, PartialOrd)]
pub enum Type {
    Bool,
    U8,
    U64,
    U128,
    Address,
    Signer,
    Vector(TriompheArc<Type>),
    Struct {
        idx: StructNameIndex,
        ability: AbilityInfo,
    },
    StructInstantiation {
        idx: StructNameIndex,
        ty_args: TriompheArc<Vec<Type>>,
        ability: AbilityInfo,
    },
```

**File:** third_party/move/move-vm/types/src/loaded_data/struct_name_indexing.rs (L70-99)
```rust
    pub fn struct_name_to_idx(
        &self,
        struct_name: &StructIdentifier,
    ) -> PartialVMResult<StructNameIndex> {
        {
            let index_map = self.0.read();
            if let Some(idx) = index_map.forward_map.get(struct_name) {
                return Ok(StructNameIndex(*idx));
            }
        }

        // Possibly need to insert, so make the copies outside of the lock.
        let forward_key = struct_name.clone();
        let backward_value = Arc::new(struct_name.clone());

        let idx = {
            let mut index_map = self.0.write();

            if let Some(idx) = index_map.forward_map.get(struct_name) {
                return Ok(StructNameIndex(*idx));
            }

            let idx = index_map.backward_map.len() as u32;
            index_map.backward_map.push(backward_value);
            index_map.forward_map.insert(forward_key, idx);
            idx
        };

        Ok(StructNameIndex(idx))
    }
```
