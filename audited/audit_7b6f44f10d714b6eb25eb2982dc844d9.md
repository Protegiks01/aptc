# Audit Report

## Title
Netbench Configuration Missing Bounds Validation Enabling Node Resource Exhaustion in Test Environments

## Summary
The `sanitize()` function in `NetbenchConfig` fails to validate numeric configuration fields (`direct_send_data_size`, `rpc_data_size`, `direct_send_per_second`, `rpc_per_second`, `max_network_channel_size`, `rpc_in_flight`), allowing extreme values that cause immediate memory exhaustion, division-by-zero panics, or resource exhaustion when netbench is enabled. [1](#0-0) 

## Finding Description

The `sanitize()` function only validates that netbench is not enabled on testnet/mainnet, but performs no bounds checking on numeric configuration values: [2](#0-1) 

**Critical Issue #1: Memory Exhaustion via `direct_send_data_size` and `rpc_data_size`**

When a peer connects and direct send testing is enabled, the `direct_sender` function allocates a vector with capacity equal to `direct_send_data_size` and fills it with random bytes: [3](#0-2) 

Setting `direct_send_data_size` to extremely large values (e.g., multiple gigabytes or `usize::MAX`) causes:
- Immediate allocation failure and OOM crash
- This occurs PER connected peer (line 310-318 spawns one sender per peer)
- The same issue exists for `rpc_data_size` at lines 423-428 [4](#0-3) 

**Critical Issue #2: Division by Zero via `direct_send_per_second` and `rpc_per_second`**

These values are used in division operations to calculate intervals: [5](#0-4) 

If set to 0, this causes a panic. If set to extremely large values (approaching `u64::MAX`), the interval becomes 0 nanoseconds, causing the loop to run continuously without yielding, effectively creating a busy-wait DoS.

**Issue #3: Memory Exhaustion via `max_network_channel_size`**

This value controls channel capacity and is used without upper bound validation: [6](#0-5) 

The underlying `aptos_channel` only validates non-zero: [7](#0-6) 

While less immediately exploitable (queues start with capacity 1), setting `max_network_channel_size` to `u64::MAX` allows unbounded memory growth as messages arrive from multiple peers. [8](#0-7) 

## Impact Explanation

**THIS ISSUE DOES NOT MEET APTOS BUG BOUNTY CRITERIA** because:

1. **Requires Privileged Access**: Exploitation requires filesystem access to modify node configuration files or the ability to inject malicious test configurations. This violates the requirement "Exploitable by unprivileged attacker (no validator insider access required)."

2. **Limited to Non-Production**: The sanitizer explicitly prevents netbench from running on testnet/mainnet, limiting the attack surface to development and testing environments only.

3. **Trust Model Violation**: According to the stated trust model, "validator operators" are trusted actors, and this attack requires their cooperation or compromise.

While the missing validation is a **configuration robustness issue** and violates defense-in-depth principles, it does not constitute an exploitable vulnerability under the Aptos bug bounty program's criteria. The potential impacts (node crashes, resource exhaustion) would be **High Severity** if the attack vector were valid, but the privileged access requirement disqualifies it.

## Likelihood Explanation

**Low to None** for production exploitation:
- Requires config file access or malicious test injection
- Explicitly disabled on testnet/mainnet
- Would only affect development/CI environments
- Validator operators are trusted actors per the security model

This is a **code quality and defensive programming issue**, not an exploitable security vulnerability.

## Recommendation

Add bounds validation in the `sanitize()` function:

```rust
// In netbench_config.rs sanitize() function, after line 63:

// Define reasonable upper bounds
const MAX_DATA_SIZE: usize = 100 * 1024 * 1024; // 100 MB
const MAX_CHANNEL_SIZE: u64 = 1_000_000; // 1 million messages
const MAX_PER_SECOND: u64 = 1_000_000; // 1 million per second
const MAX_IN_FLIGHT: usize = 10_000; // 10k concurrent RPCs
const MAX_SERVICE_THREADS: usize = 128; // 128 threads

// Validate direct send configuration
if cfg.direct_send_data_size > MAX_DATA_SIZE {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        format!("direct_send_data_size ({}) exceeds maximum ({})", 
            cfg.direct_send_data_size, MAX_DATA_SIZE)
    ));
}

if cfg.direct_send_per_second == 0 {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "direct_send_per_second cannot be zero (division by zero)".to_string()
    ));
}

if cfg.direct_send_per_second > MAX_PER_SECOND {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        format!("direct_send_per_second ({}) exceeds maximum ({})",
            cfg.direct_send_per_second, MAX_PER_SECOND)
    ));
}

// Similar validation for rpc_data_size, rpc_per_second, rpc_in_flight
// and max_network_channel_size, netbench_service_threads
```

## Proof of Concept

Since this requires configuration file access (privileged operation), a traditional PoC would involve:

1. Create a malicious `node.yaml`:
```yaml
netbench:
  enabled: true
  max_network_channel_size: 18446744073709551615  # u64::MAX
  direct_send_data_size: 10737418240  # 10 GB
  direct_send_per_second: 0  # Division by zero
```

2. Attempt to start a node with this configuration
3. Observe immediate crash or resource exhaustion

However, **this is not a valid security vulnerability** because it requires privileged access to modify node configuration, which violates the attack model requirement for unprivileged exploitation.

---

**CRITICAL NOTE**: While the missing validation is a legitimate code quality issue that should be addressed for defensive programming and robustness, it **does not qualify as a security vulnerability** under the Aptos bug bounty program criteria due to the privileged access requirement.

### Citations

**File:** config/src/config/netbench_config.rs (L12-25)
```rust
pub struct NetbenchConfig {
    pub enabled: bool,
    pub max_network_channel_size: u64, // Max num of pending network messages
    pub netbench_service_threads: Option<usize>, // Number of kernel threads for tokio runtime. None default for num-cores.

    pub enable_direct_send_testing: bool, // Whether or not to enable direct send test mode
    pub direct_send_data_size: usize,     // The amount of data to send in each request
    pub direct_send_per_second: u64,      // The interval (microseconds) between requests

    pub enable_rpc_testing: bool,
    pub rpc_data_size: usize,
    pub rpc_per_second: u64,
    pub rpc_in_flight: usize,
}
```

**File:** config/src/config/netbench_config.rs (L47-77)
```rust
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        // If no netbench config is specified, there's nothing to do
        if node_config.netbench.is_none() {
            return Ok(());
        }

        // If netbench is disabled, there's nothing to do
        let netbench_config = node_config.netbench.unwrap();
        if !netbench_config.enabled {
            return Ok(());
        }

        // Otherwise, verify that netbench is not enabled in testnet or mainnet
        if let Some(chain_id) = chain_id {
            if chain_id.is_testnet() || chain_id.is_mainnet() {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The netbench application should not be enabled in testnet or mainnet!"
                        .to_string(),
                ));
            }
        }

        Ok(())
    }
```

**File:** network/benchmark/src/lib.rs (L309-318)
```rust
                    if config.enable_direct_send_testing {
                        handle.spawn(direct_sender(
                            node_config.clone(),
                            network_client.clone(),
                            time_service.clone(),
                            network_id,
                            meta.remote_peer_id,
                            shared.clone(),
                        ));
                    }
```

**File:** network/benchmark/src/lib.rs (L352-353)
```rust
    let interval = Duration::from_nanos(1_000_000_000 / config.direct_send_per_second);
    let ticker = time_service.interval(interval);
```

**File:** network/benchmark/src/lib.rs (L355-362)
```rust
    let data_size = config.direct_send_data_size;
    let mut rng = OsRng;
    let mut blob = Vec::<u8>::with_capacity(data_size);

    // random payload filler
    for _ in 0..data_size {
        blob.push(rng.r#gen());
    }
```

**File:** aptos-node/src/network.rs (L203-209)
```rust
    let max_network_channel_size = cfg.max_network_channel_size as usize;
    let network_service_config = NetworkServiceConfig::new(
        direct_send_protocols,
        rpc_protocols,
        aptos_channel::Config::new(max_network_channel_size)
            .queue_style(QueueStyle::FIFO)
            .counters(&aptos_network_benchmark::PENDING_NETBENCH_NETWORK_EVENTS),
```

**File:** crates/channel/src/aptos_channel.rs (L240-241)
```rust
    let max_queue_size_per_key =
        NonZeroUsize!(max_queue_size_per_key, "aptos_channel cannot be of size 0");
```

**File:** crates/channel/src/message_queues.rs (L117-127)
```rust
        let key_message_queue = self
            .per_key_queue
            .entry(key.clone())
            // Only allocate a small initial queue for a new key. Previously, we
            // allocated a queue with all `max_queue_size_per_key` entries;
            // however, this breaks down when we have lots of transient peers.
            // For example, many of our queues have a max capacity of 1024. To
            // handle a single rpc from a transient peer, we would end up
            // allocating ~ 96 b * 1024 ~ 64 Kib per queue.
            .or_insert_with(|| VecDeque::with_capacity(1));

```
