# Audit Report

## Title
Missing Root Hash Verification in Jellyfish Merkle Tree Restoration Allows State Corruption

## Summary
The `finish_impl()` function in the Jellyfish Merkle tree restoration process does not verify that the final reconstructed tree's root hash matches the `expected_root_hash`. While intermediate chunks are verified during restoration, the final state commitment occurs without validation, allowing restoration to complete with an incorrect state tree.

## Finding Description

The Jellyfish Merkle tree restoration process in `JellyfishMerkleRestore` is designed to rebuild a state tree from chunks of key-value pairs. The restoration process maintains an `expected_root_hash` field that represents the correct root hash the restored tree should have. [1](#0-0) 

During chunk addition, the `verify()` function checks that the partial tree combined with proof siblings equals the expected root hash: [2](#0-1) 

However, when restoration completes via `finish_impl()`, the function freezes all remaining nodes and writes them to storage **without verifying** that the final root node's hash matches `expected_root_hash`: [3](#0-2) 

The only place where `expected_root_hash` is validated against an actual root hash is in the constructor `new()`, which checks if a **previous** restoration was completed correctly: [4](#0-3) 

This check only occurs when creating a new restore instance and detecting prior completion, not during the current restoration.

**Attack Scenario:**

1. A malicious backup coordinator or compromised state sync peer provides chunks with valid intermediate proofs
2. Due to implementation bugs, race conditions, or subtle manipulation, the final frozen tree has a different structure than expected
3. `finish_impl()` completes successfully, writing the incorrect tree to storage
4. Different nodes may restore different state trees from the same backup data
5. Nodes diverge on state root hashes, causing consensus splits

**Specific vulnerability points:**

- If restoration is interrupted and resumed with overlapping chunks, edge cases in node freezing could produce different tree structures
- Placeholder hash handling in sparse portions of the tree may differ from expectations
- Concurrent writes or async commit race conditions could corrupt the final tree
- Malicious proofs claiming non-existent right siblings could pass intermediate verification but produce wrong final structure

## Impact Explanation

**Critical Severity** - This vulnerability breaks the **Deterministic Execution** and **State Consistency** invariants:

1. **Consensus Safety Violation**: If different validators restore from the same backup with different bugs/timing, they may produce different state root hashes. This breaks the fundamental requirement that all validators must produce identical state roots for identical blocks.

2. **State Corruption**: A corrupted state tree with wrong root hash could be written to AptosDB. Once committed, this requires manual intervention or a hardfork to fix, as the entire chain's state validity depends on correct Merkle root hashes.

3. **Chain Split Risk**: Nodes with different state roots cannot reach consensus on subsequent blocks. This could cause a network partition requiring hardfork resolution.

4. **Silent Failure**: The restoration completes with `Ok(())`, providing no indication that the state is incorrect until consensus fails or explicit verification is performed.

This meets the Critical severity criteria: "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)".

## Likelihood Explanation

**High Likelihood** - This vulnerability will manifest in several realistic scenarios:

1. **Implementation Bugs**: Any subtle bug in the tree construction logic (freezing, placeholder hash handling, nibble path calculation) will go undetected, as there's no final verification gate.

2. **Concurrent State Sync**: During rapid state synchronization from multiple peers, race conditions in async commit could produce inconsistent final trees that pass intermediate checks but fail final validation.

3. **Malicious Backup Data**: An attacker providing tampered backup data could craft chunks that pass intermediate verification (via manipulated proofs) but produce a final tree with wrong root hash.

4. **Interrupted Restoration**: The resumption logic reconstructs partial nodes from storage. Edge cases in this recovery path could produce different tree structures on different nodes.

The vulnerability is deterministic - if triggered, it **will** cause state corruption. The only question is frequency of triggering conditions.

## Recommendation

Add final root hash verification in `finish_impl()` before writing to storage:

```rust
pub fn finish_impl(mut self) -> Result<()> {
    self.wait_for_async_commit()?;
    
    // ... existing special case handling for single leaf/null node ...
    
    self.freeze(0);
    
    // CRITICAL FIX: Verify final root hash before committing
    let root_node_key = NodeKey::new_empty_path(self.version);
    let root_node = self.frozen_nodes.get(&root_node_key)
        .ok_or_else(|| AptosDbError::Other(
            format!("Root node missing after freeze")
        ))?;
    
    let actual_root_hash = root_node.hash();
    ensure!(
        actual_root_hash == self.expected_root_hash,
        "State restoration failed: final root hash {} does not match expected {}. \
         This indicates corrupted backup data or a restoration bug.",
        actual_root_hash,
        self.expected_root_hash
    );
    
    self.store.write_node_batch(&self.frozen_nodes)?;
    Ok(())
}
```

Additionally, consider adding verification in the special case branches (null node and single leaf) to ensure they also match expected root hash.

## Proof of Concept

The vulnerability can be demonstrated by examining the test code, which performs manual verification that production code lacks: [5](#0-4) 

The test's `assert_success()` function manually retrieves and verifies the root hash **after** restoration completes, demonstrating that production code should perform this verification.

To reproduce the vulnerability:

1. Instrument `finish_impl()` to inject a deliberate corruption (e.g., flip one bit in a frozen node's hash)
2. Run restoration with valid chunks
3. Observe that restoration completes successfully with `Ok(())`
4. Query the tree's root hash via `get_root_hash(version)`
5. Observe it differs from `expected_root_hash`
6. Note that no error was raised during restoration

**Rust test demonstrating the issue:**

```rust
#[test]
fn test_missing_root_hash_verification() {
    let (db, version) = init_mock_store(&test_data);
    let tree = JellyfishMerkleTree::new(&db);
    let correct_root_hash = tree.get_root_hash(version).unwrap();
    
    let restore_db = Arc::new(MockSnapshotStore::default());
    let mut restore = StateSnapshotRestore::new(
        &restore_db, 
        &restore_db, 
        version,
        correct_root_hash,
        false,
        StateSnapshotRestoreMode::Default
    ).unwrap();
    
    // Add all chunks
    for (key, value) in test_data {
        let proof = tree.get_range_proof(key.hash(), version).unwrap();
        restore.add_chunk(vec![(key, value)], proof).unwrap();
    }
    
    // Finish restoration - this should verify root hash but doesn't
    restore.finish().unwrap();
    
    // If we deliberately corrupted the tree during restoration,
    // it would still succeed. Only manual verification catches it:
    let restored_tree = JellyfishMerkleTree::new(&restore_db);
    let actual_root = restored_tree.get_root_hash(version).unwrap();
    
    // This assertion would fail if corruption occurred, but finish() didn't catch it
    assert_eq!(actual_root, correct_root_hash);
}
```

**Notes**

The vulnerability exists because the restoration process assumes that if all intermediate verifications pass, the final tree must be correct. However, this assumption is false in the presence of:

- Implementation bugs in tree construction logic
- Race conditions in concurrent restoration
- Malicious or corrupted backup data
- Edge cases in partial node reconstruction

The fix is straightforward: add explicit final verification before committing the restored tree to storage, matching the validation pattern already used in the constructor for detecting previously completed restorations.

### Citations

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L175-176)
```rust
    /// When the restoration process finishes, we expect the tree to have this root hash.
    expected_root_hash: HashValue,
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L196-206)
```rust
        let (finished, partial_nodes, previous_leaf) = if let Some(root_node) =
            tree_reader.get_node_option(&NodeKey::new_empty_path(version), "restore")?
        {
            info!("Previous restore is complete, checking root hash.");
            ensure!(
                root_node.hash() == expected_root_hash,
                "Previous completed restore has root hash {}, expecting {}",
                root_node.hash(),
                expected_root_hash,
            );
            (true, vec![], None)
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L689-696)
```rust
        // Verify the proof now that we have all the siblings
        proof
            .verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(*previous_key, previous_leaf.value_hash()),
                left_siblings,
            )
            .map_err(Into::into)
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```

**File:** storage/aptosdb/src/state_restore/restore_test.rs (L251-252)
```rust
    let actual_root_hash = tree.get_root_hash(version).unwrap();
    assert_eq!(actual_root_hash, expected_root_hash);
```
