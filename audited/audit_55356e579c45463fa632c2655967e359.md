# Audit Report

## Title
Transaction Loss via Empty Sub-Blocks When max_partitioning_rounds=1

## Summary
When `max_partitioning_rounds` is set to 1 and `partition_last_round` is false, the block partitioner silently drops transactions from all shards except the last shard, violating the fundamental invariant that all transactions in a block must be executed.

## Finding Description

The vulnerability exists in the `add_edges()` function where `final_num_rounds` can become 0, causing empty sub-blocks and transaction loss. [1](#0-0) 

When `partition_last_round` is false, the code pops the last round from `sub_block_matrix` and extracts only the last shard's transactions into `global_txns`. All other shards' transactions in the popped round are dropped. [2](#0-1) 

If `num_rounds()` returns 1 (which happens when `max_partitioning_rounds=1`), after the pop operation, `final_num_rounds` becomes 0. The range `(0..0)` at line 75 creates empty vectors for all shards' sub_blocks.

The root cause is that `remove_cross_shard_dependencies()` always creates at least one round in `finalized_txn_matrix`: [3](#0-2) [4](#0-3) 

When `max_partitioning_rounds=1`, the loop range `0..(1-1)` is empty, so only the final push executes, resulting in exactly 1 round.

The `num_rounds()` method simply returns the length: [5](#0-4) 

During execution, empty sub-blocks are processed without error: [6](#0-5) 

The result aggregation also handles 0 rounds: [7](#0-6) 

**Invariant Violation:** This breaks the "Deterministic Execution" and "State Consistency" invariants - all validators must execute all transactions in a block, but some transactions are silently dropped.

## Impact Explanation

**Critical Severity** - This meets multiple critical severity criteria:
- **Loss of Funds**: Payment transactions from dropped shards are never executed, causing permanent fund loss
- **Consensus/Safety Violations**: Different nodes may have different transaction counts if misconfigured differently, breaking consensus
- **State Inconsistency**: State transitions are incomplete, violating atomicity guarantees

If triggered with 4 shards, 75% of transactions (shards 0-2) would be permanently lost, affecting:
- User balances (lost payments)
- Smart contract state (incomplete state transitions)
- Validator rewards (missed staking transactions)

## Likelihood Explanation

**Low Likelihood** - Requires specific misconfiguration:

The default configuration prevents this: [8](#0-7) 

However, the configuration can be overridden: [9](#0-8) [10](#0-9) 

This vulnerability requires:
1. Operator/developer access to set `max_partitioning_rounds=1`
2. `partition_last_round=false` (default when using global executor)
3. No validation prevents invalid configuration values

## Recommendation

Add validation to prevent invalid configurations:

```rust
// In PartitionerV2::new() or PartitionState::new()
assert!(
    num_rounds_limit >= 2,
    "max_partitioning_rounds must be at least 2 to prevent transaction loss"
);
```

Alternatively, fix the `add_edges()` logic to handle `final_num_rounds=0`:

```rust
let global_txns: Vec<TransactionWithDependencies<AnalyzedTransaction>> =
    if !state.partition_last_round && state.sub_block_matrix.len() > 1 {
        // Only pop if we have more than 1 round
        state.sub_block_matrix.pop()...
    } else {
        vec![]
    };
```

Or extract ALL shards when popping, not just the last one.

## Proof of Concept

```rust
#[test]
fn test_transaction_loss_with_single_round() {
    use crate::v2::{config::PartitionerV2Config, PartitionerV2};
    use crate::pre_partition::uniform_partitioner::UniformPartitionerConfig;
    
    let num_shards = 4;
    let num_txns = 100;
    
    // Create non-conflicting transactions
    let txns = create_non_conflicting_transactions(num_txns);
    
    // Configure partitioner with max_partitioning_rounds=1
    let config = PartitionerV2Config {
        num_threads: 4,
        max_partitioning_rounds: 1,  // Trigger condition
        cross_shard_dep_avoid_threshold: 0.9,
        dashmap_num_shards: 64,
        partition_last_round: false,  // Trigger condition
        pre_partitioner_config: Box::new(UniformPartitionerConfig),
    };
    
    let partitioner = config.build();
    let result = partitioner.partition(txns, num_shards);
    
    // BUG: Should have all 100 transactions, but only has ~25 from last shard
    let total_txns = result.num_sharded_txns() + result.global_txns.len();
    assert_eq!(total_txns, num_txns); // This will FAIL - txns are lost!
}
```

**Note:** This bug requires operator/configuration access and is not directly exploitable by unprivileged external attackers. However, it represents a critical defensive programming failure that should be fixed with proper input validation.

### Citations

**File:** execution/block-partitioner/src/v2/build_edge.rs (L56-72)
```rust
            if !state.partition_last_round {
                state
                    .sub_block_matrix
                    .pop()
                    .unwrap()
                    .last()
                    .unwrap()
                    .lock()
                    .unwrap()
                    .take()
                    .unwrap()
                    .into_transactions_with_deps()
            } else {
                vec![]
            };

        let final_num_rounds = state.sub_block_matrix.len();
```

**File:** execution/block-partitioner/src/v2/build_edge.rs (L73-86)
```rust
        let sharded_txns = (0..state.num_executor_shards)
            .map(|shard_id| {
                let sub_blocks: Vec<SubBlock<AnalyzedTransaction>> = (0..final_num_rounds)
                    .map(|round_id| {
                        state.sub_block_matrix[round_id][shard_id]
                            .lock()
                            .unwrap()
                            .take()
                            .unwrap()
                    })
                    .collect();
                SubBlocksForShard::new(shard_id, sub_blocks)
            })
            .collect();
```

**File:** execution/block-partitioner/src/v2/partition_to_matrix.rs (L37-48)
```rust
        for round_id in 0..(state.num_rounds_limit - 1) {
            let (accepted, discarded) = Self::discarding_round(state, round_id, remaining_txns);
            state.finalized_txn_matrix.push(accepted);
            remaining_txns = discarded;
            num_remaining_txns = remaining_txns.iter().map(|ts| ts.len()).sum();

            if num_remaining_txns
                < ((1.0 - state.cross_shard_dep_avoid_threshold) * state.num_txns() as f32) as usize
            {
                break;
            }
        }
```

**File:** execution/block-partitioner/src/v2/partition_to_matrix.rs (L70-71)
```rust
        state.finalized_txn_matrix.push(remaining_txns);
    }
```

**File:** execution/block-partitioner/src/v2/state.rs (L278-280)
```rust
    pub(crate) fn num_rounds(&self) -> usize {
        self.finalized_txn_matrix.len()
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L185-212)
```rust
    fn execute_block(
        &self,
        transactions: SubBlocksForShard<AnalyzedTransaction>,
        state_view: &S,
        config: BlockExecutorConfig,
    ) -> Result<Vec<Vec<TransactionOutput>>, VMStatus> {
        let mut result = vec![];
        for (round, sub_block) in transactions.into_sub_blocks().into_iter().enumerate() {
            let _timer = SHARDED_BLOCK_EXECUTION_BY_ROUNDS_SECONDS
                .timer_with(&[&self.shard_id.to_string(), &round.to_string()]);
            SHARDED_BLOCK_EXECUTOR_TXN_COUNT.observe_with(
                &[&self.shard_id.to_string(), &round.to_string()],
                sub_block.transactions.len() as f64,
            );
            info!(
                "executing sub block for shard {} and round {}, number of txns {}",
                self.shard_id,
                round,
                sub_block.transactions.len()
            );
            result.push(self.execute_sub_block(sub_block, round, state_view, config.clone())?);
            trace!(
                "Finished executing sub block for shard {} and round {}",
                self.shard_id,
                round
            );
        }
        Ok(result)
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L98-110)
```rust
        let num_rounds = sharded_output[0].len();
        let mut aggregated_results = vec![];
        let mut ordered_results = vec![vec![]; num_executor_shards * num_rounds];
        // Append the output from individual shards in the round order
        for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
            for (round, result) in results_from_shard.into_iter().enumerate() {
                ordered_results[round * num_executor_shards + shard_id] = result;
            }
        }

        for result in ordered_results.into_iter() {
            aggregated_results.extend(result);
        }
```

**File:** execution/block-partitioner/src/v2/config.rs (L54-65)
```rust
impl Default for PartitionerV2Config {
    fn default() -> Self {
        Self {
            num_threads: 8,
            max_partitioning_rounds: 4,
            cross_shard_dep_avoid_threshold: 0.9,
            dashmap_num_shards: 64,
            partition_last_round: false,
            pre_partitioner_config: Box::<ConnectedComponentPartitionerConfig>::default(),
        }
    }
}
```

**File:** execution/executor-benchmark/src/main.rs (L216-217)
```rust
    #[clap(long, default_value = "4")]
    max_partitioning_rounds: usize,
```

**File:** execution/executor-benchmark/src/main.rs (L244-252)
```rust
    fn partitioner_config(&self) -> PartitionerV2Config {
        match self.partitioner_version.as_deref() {
            Some("v2") => PartitionerV2Config {
                num_threads: self.partitioner_v2_num_threads,
                max_partitioning_rounds: self.max_partitioning_rounds,
                cross_shard_dep_avoid_threshold: self.partitioner_cross_shard_dep_avoid_threshold,
                dashmap_num_shards: self.partitioner_v2_dashmap_num_shards,
                partition_last_round: !self.use_global_executor,
                pre_partitioner_config: self.pre_partitioner_config(),
```
