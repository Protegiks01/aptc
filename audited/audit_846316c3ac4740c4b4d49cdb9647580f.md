# Audit Report

## Title
Unconditional Logical Time Update in sync_to_target Causes Incorrect Sync Target Rejection at Epoch Boundaries

## Summary
The `ExecutionProxy::sync_to_target` method unconditionally updates the logical time tracker even when state synchronization fails. This creates a state inconsistency where the node's tracked logical time (epoch + round) diverges from its actual synced state. At epoch boundaries, where rounds reset to 0 and sync failures are more likely due to network reconfiguration, this bug causes nodes to incorrectly reject valid sync targets, potentially preventing them from catching up to the network.

## Finding Description

The vulnerability exists in the `sync_to_target` implementation where logical time is updated regardless of sync success or failure. [1](#0-0) 

The logical time is unconditionally updated on line 222, but the sync result is only checked later when returned. If the `state_sync_notifier.sync_to_target(target)` call fails, the node's `latest_logical_time` is still updated to `target_logical_time`, creating a dangerous inconsistency.

Contrast this with the correct implementation in `sync_for_duration`: [2](#0-1) 

Here, the logical time update only occurs within an `if let Ok(...)` block, ensuring it only happens on successful sync.

**Attack Scenario at Epoch Boundary:**

1. Node is properly synced to epoch 1, round 1000
2. Epoch transition occurs, rounds reset to 0 for epoch 2
3. Node receives `sync_to_target` call for epoch 2, round 50
4. State sync fails (network partition, target unavailable, storage error)
5. Despite the failure, `latest_logical_time` is updated to `LogicalTime { epoch: 2, round: 50 }`
6. Node's actual state remains at epoch 1, round 1000

Now when the node receives another `sync_to_target` with epoch 2, round 30: [3](#0-2) 

The comparison evaluates `LogicalTime { epoch: 2, round: 50 } >= LogicalTime { epoch: 2, round: 30 }`, which is TRUE (same epoch, 50 >= 30), causing the sync to be incorrectly rejected. The node believes it's already past round 30 of epoch 2, when in reality it never left epoch 1.

**LogicalTime Comparison Mechanism:** [4](#0-3) 

The derived `Ord` trait compares lexicographically (epoch first, then round), which is correct. However, the bug makes this comparison operate on incorrect data.

This breaks the **State Consistency** invariant: the node's internal tracking of its position (logical time) must accurately reflect its actual synced state.

## Impact Explanation

**High Severity** per Aptos Bug Bounty criteria:

1. **Significant Protocol Violations**: Incorrect sync target validation violates the state synchronization protocol's correctness guarantees
2. **Validator Node Slowdowns**: Nodes that experience failed syncs during epoch transitions become stuck, unable to accept valid sync targets to catch up
3. **Liveness Degradation**: During epoch boundaries when network reconfiguration occurs, multiple nodes could become stuck simultaneously, reducing network liveness

The impact is amplified at epoch boundaries because:
- Round resets to 0 create many "low round" sync targets that could be rejected
- Network reconfiguration increases sync failure probability  
- Multiple validators experiencing this simultaneously could prevent epoch progression

This does not reach Critical severity because:
- No direct fund loss or theft
- Not a complete network halt (some nodes may continue)
- Recoverable through manual intervention or eventual successful sync

## Likelihood Explanation

**HIGH likelihood** of occurrence:

1. **Natural Triggers**: Network partitions, transient failures, and timeout errors during state sync are common operational conditions, not requiring any attacker action

2. **Epoch Boundary Amplification**: Aptos epochs change regularly (typically when validator set changes or governance actions occur). Each epoch transition:
   - Triggers validator set reconfiguration
   - Causes network churn
   - Increases probability of sync failures
   - Resets rounds to 0, maximizing rejection window

3. **No Attacker Required**: This is a pure implementation bug that manifests under normal adverse network conditions

4. **Observable in Production**: Any node experiencing transient network issues during an epoch transition will exhibit this bug

## Recommendation

Update `sync_to_target` to match the correct pattern used in `sync_for_duration` - only update logical time on successful sync:

```rust
async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
    let mut latest_logical_time = self.write_mutex.lock().await;
    let target_logical_time =
        LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

    self.executor.finish();

    if *latest_logical_time >= target_logical_time {
        warn!(
            "State sync target {:?} is lower than already committed logical time {:?}",
            target_logical_time, *latest_logical_time
        );
        return Ok(());
    }

    if let Some(inner) = self.state.read().as_ref() {
        let block_timestamp = target.commit_info().timestamp_usecs();
        inner
            .payload_manager
            .notify_commit(block_timestamp, Vec::new());
    }

    fail_point!("consensus::sync_to_target", |_| {
        Err(anyhow::anyhow!("Injected error in sync_to_target").into())
    });

    let result = monitor!(
        "sync_to_target",
        self.state_sync_notifier.sync_to_target(target).await
    );

    // FIX: Only update logical time on successful sync
    if result.is_ok() {
        *latest_logical_time = target_logical_time;
    }

    self.executor.reset()?;

    result.map_err(|error| {
        let anyhow_error: anyhow::Error = error.into();
        anyhow_error.into()
    })
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_sync_to_target_logical_time_inconsistency() {
    // Setup: Create ExecutionProxy with mocked components
    let (state_sync_notifier, mut state_sync_receiver) = 
        create_mock_state_sync_notifier();
    let executor = Arc::new(MockBlockExecutor::new());
    let txn_notifier = Arc::new(MockTxnNotifier::new());
    
    let execution_proxy = ExecutionProxy::new(
        executor,
        txn_notifier,
        state_sync_notifier,
        BlockTransactionFilterConfig::default(),
        false,
        None,
    );

    // Initial state: Synced to epoch 1, round 1000
    let initial_target = create_ledger_info_with_sigs(1, 1000);
    execution_proxy.sync_to_target(initial_target).await.unwrap();

    // Epoch boundary: Attempt sync to epoch 2, round 50
    let epoch_2_target = create_ledger_info_with_sigs(2, 50);
    
    // Mock state sync to fail
    tokio::spawn(async move {
        if let Some(request) = state_sync_receiver.recv().await {
            // Simulate sync failure
            request.respond(Err(anyhow::anyhow!("Network error during epoch transition")));
        }
    });

    // This should fail but will still update logical time
    let result = execution_proxy.sync_to_target(epoch_2_target).await;
    assert!(result.is_err(), "Sync should fail");

    // BUG: Node now believes it's at epoch 2, round 50 despite failed sync
    // Try to sync to epoch 2, round 30 (valid target that should be accepted)
    let valid_target = create_ledger_info_with_sigs(2, 30);
    let result = execution_proxy.sync_to_target(valid_target).await;
    
    // BUG MANIFESTATION: This sync is incorrectly rejected!
    // The node thinks it's already at epoch 2 round 50 > 30
    // But it's actually still at epoch 1 round 1000
    assert!(result.is_ok(), "Valid sync target incorrectly rejected due to logical time inconsistency");
}
```

## Notes

This vulnerability is particularly insidious because it silently causes liveness degradation rather than immediate failure. Nodes experiencing this bug will continue operating but will be unable to catch up to the network, especially if multiple sync attempts fail during an epoch transition. The round reset at epoch boundaries creates a particularly wide window where valid sync targets will be incorrectly rejected.

### Citations

**File:** consensus/src/state_computer.rs (L27-37)
```rust
#[derive(Clone, Copy, Debug, Eq, PartialEq, PartialOrd, Ord, Hash)]
struct LogicalTime {
    epoch: u64,
    round: Round,
}

impl LogicalTime {
    pub fn new(epoch: u64, round: Round) -> Self {
        Self { epoch, round }
    }
}
```

**File:** consensus/src/state_computer.rs (L153-163)
```rust
        let result = monitor!(
            "sync_for_duration",
            self.state_sync_notifier.sync_for_duration(duration).await
        );

        // Update the latest logical time
        if let Ok(latest_synced_ledger_info) = &result {
            let ledger_info = latest_synced_ledger_info.ledger_info();
            let synced_logical_time = LogicalTime::new(ledger_info.epoch(), ledger_info.round());
            *latest_logical_time = synced_logical_time;
        }
```

**File:** consensus/src/state_computer.rs (L188-194)
```rust
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }
```

**File:** consensus/src/state_computer.rs (L216-222)
```rust
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );

        // Update the latest logical time
        *latest_logical_time = target_logical_time;
```
