# Audit Report

## Title
Database Inconsistency Vulnerability Due to Lack of Initialization Validation in TransactionStore

## Summary
The `TransactionStore::new()` constructor accepts any `LedgerDb` instance without validating its internal consistency. When storage sharding is enabled (the default configuration), sub-databases can become inconsistent during crashes, and this corrupted state is accepted on restart without validation, leading to state corruption and potential consensus violations.

## Finding Description

The vulnerability exists at multiple layers:

1. **No validation in TransactionStore::new()**: The constructor simply wraps the provided `LedgerDb` without any consistency checks. [1](#0-0) 

2. **Acknowledged unhandled inconsistency in LedgerDb**: The `LedgerDb::new()` function explicitly acknowledges data inconsistency is not handled. [2](#0-1) 

3. **Storage sharding enabled by default**: With `enable_storage_sharding` set to `true` by default, each sub-database (transaction_db, event_db, write_set_db, etc.) is a separate RocksDB instance. [3](#0-2) 

4. **Non-atomic parallel writes**: During commits, the system spawns 7 parallel threads to write to different sub-databases, with an explicit TODO acknowledging the need to handle inconsistency. [4](#0-3) [5](#0-4) 

5. **Sequential writes to sub-databases**: The `write_schemas()` method writes to each sub-database sequentially, not atomically across all databases. [6](#0-5) 

**Attack Scenario:**
1. A validator node performs a commit operation via `calculate_and_commit_ledger_and_state_kv()`
2. Parallel threads begin writing to different sub-databases (transaction_db, event_db, etc.)
3. The process crashes or is killed (hardware failure, OOM, power loss, kill signal)
4. Some sub-databases successfully commit their data (e.g., transaction_db at version N+1)
5. Other sub-databases do not complete the commit (e.g., event_db still at version N)
6. On restart, `TransactionStore::new()` accepts this corrupted `LedgerDb` without validation
7. Queries return inconsistent data: transactions may appear at version N+1 but their events missing
8. Different validators may have different inconsistent states, breaking consensus

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs."

## Impact Explanation

This is a **Medium Severity** vulnerability per Aptos bug bounty criteria: "State inconsistencies requiring intervention."

**Impact:**
- **State Corruption**: Transaction data, events, write sets, and auxiliary data can become inconsistent across sub-databases
- **Consensus Violations**: If multiple validators experience crashes during commits, they may restart with different inconsistent states, breaking the deterministic execution invariant
- **Data Integrity Loss**: Queries to `TransactionStore` methods like `get_account_ordered_transaction_version()` or `get_account_transaction_summaries_iter()` may return incorrect or inconsistent results
- **Manual Intervention Required**: Recovery requires detecting and resolving inconsistencies, potentially requiring database restoration from checkpoints

The validation functionality exists in the codebase for debugging purposes but is not integrated into the normal startup flow. [7](#0-6) 

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability is triggered by common operational scenarios:
- Hardware failures during write operations
- Out-of-memory (OOM) kills
- Power failures in data centers
- Operator intervention (e.g., kill signals during maintenance)
- Kubernetes pod evictions during rolling updates

The parallel write architecture using `.unwrap()` for error handling means any thread failure will panic, but external process termination during the parallel write window can leave databases inconsistent. The window is small but non-zero, and with thousands of validator nodes operating globally, the probability of occurrence across the network is significant.

## Recommendation

Implement comprehensive initialization validation in `TransactionStore::new()` and `LedgerDb::new()`:

1. **Add version consistency checks**: Verify all sub-databases are at consistent versions on initialization
2. **Implement recovery mechanism**: On detecting inconsistency, either:
   - Roll back uncommitted sub-databases to the last consistent version (using OverallCommitProgress)
   - Or mark the database as corrupted and require operator intervention
3. **Add progress tracking per sub-database**: As acknowledged in the TODO, track commit progress for each database
4. **Consider atomic commit coordinator**: Implement a two-phase commit protocol or use RocksDB's transaction API across all sub-databases

**Proposed fix outline:**
```rust
impl TransactionStore {
    pub fn new(ledger_db: Arc<LedgerDb>) -> Result<Self> {
        // Validate database consistency
        ledger_db.validate_consistency()?;
        Ok(Self { ledger_db })
    }
}

impl LedgerDb {
    fn validate_consistency(&self) -> Result<()> {
        let overall_version = self.metadata_db().get_synced_version()?;
        
        // Check each sub-database is at expected version
        let tx_version = self.transaction_db().get_latest_version()?;
        let event_version = self.event_db().get_latest_version()?;
        // ... check other databases
        
        if tx_version != overall_version || event_version != overall_version {
            return Err(AptosDbError::Other(
                "Database inconsistency detected - recovery required".to_string()
            ));
        }
        Ok(())
    }
}
```

## Proof of Concept

**Simulation Steps:**

1. Start an Aptos validator node with default configuration (storage sharding enabled)
2. Begin processing transactions normally
3. During a commit operation in `calculate_and_commit_ledger_and_state_kv()`, send SIGKILL to the process:
   ```bash
   # Monitor for commit operations
   while true; do
     if grep -q "commit_ledger" /proc/$(pidof aptos-node)/stack; then
       kill -9 $(pidof aptos-node)
       break
     fi
     sleep 0.01
   done
   ```
4. Restart the node
5. Query transaction data using `TransactionStore` methods
6. Verify inconsistencies by checking:
   - Transaction exists in transaction_db but corresponding events missing from event_db
   - Or vice versa

**Expected Result:** The node restarts successfully but serves inconsistent data, with some transactions having events and others missing them, despite all being marked as committed.

**Verification:** Use the db_debugger validation tool to confirm inconsistencies: [8](#0-7) 

## Notes

This vulnerability is explicitly acknowledged by the development team through multiple TODO comments but remains unresolved. The validation infrastructure exists for debugging but is not integrated into production startup paths. The default configuration with storage sharding enabled increases the attack surface by creating multiple independent RocksDB instances that cannot be committed atomically.

### Citations

**File:** storage/aptosdb/src/transaction_store/mod.rs (L31-33)
```rust
    pub fn new(ledger_db: Arc<LedgerDb>) -> Self {
        Self { ledger_db }
    }
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L281-281)
```rust
        // TODO(grao): Handle data inconsistency.
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L531-548)
```rust
    pub fn write_schemas(&self, schemas: LedgerDbSchemaBatches) -> Result<()> {
        self.write_set_db
            .write_schemas(schemas.write_set_db_batches)?;
        self.transaction_info_db
            .write_schemas(schemas.transaction_info_db_batches)?;
        self.transaction_db
            .write_schemas(schemas.transaction_db_batches)?;
        self.persisted_auxiliary_info_db
            .write_schemas(schemas.persisted_auxiliary_info_db_batches)?;
        self.event_db.write_schemas(schemas.event_db_batches)?;
        self.transaction_accumulator_db
            .write_schemas(schemas.transaction_accumulator_db_batches)?;
        self.transaction_auxiliary_data_db
            .write_schemas(schemas.transaction_auxiliary_data_db_batches)?;
        // TODO: remove this after sharding migration
        self.ledger_metadata_db
            .write_schemas(schemas.ledger_metadata_db_batches)
    }
```

**File:** config/src/config/storage_config.rs (L233-233)
```rust
            enable_storage_sharding: true,
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L272-273)
```rust
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L276-319)
```rust
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });
```

**File:** storage/aptosdb/src/db_debugger/validation.rs (L57-112)
```rust
pub fn validate_db_data(
    db_root_path: &Path,
    internal_indexer_db_path: &Path,
    mut target_ledger_version: u64,
) -> Result<()> {
    let num_threads = 30;
    ThreadPoolBuilder::new()
        .num_threads(num_threads)
        .build_global()
        .unwrap();
    let internal_db =
        open_internal_indexer_db(internal_indexer_db_path, &RocksdbConfig::default())?;

    verify_state_kvs(db_root_path, &internal_db, target_ledger_version)?;

    let aptos_db = AptosDB::new_for_test_with_sharding(db_root_path, 1000000);
    let batch_size = 20_000;
    let start_version = aptos_db.get_first_txn_version()?.unwrap();
    target_ledger_version = std::cmp::min(
        aptos_db.get_synced_version()?.unwrap(),
        target_ledger_version,
    );
    assert!(
        start_version < target_ledger_version,
        "{}, {}",
        start_version,
        target_ledger_version
    );
    println!(
        "Validating events and transactions {}, {}",
        start_version, target_ledger_version
    );

    // Calculate ranges and split into chunks
    let ranges: Vec<(u64, u64)> = (start_version..target_ledger_version)
        .step_by(batch_size as usize)
        .map(|start| {
            let end = cmp::min(start + batch_size, target_ledger_version);
            (start, end)
        })
        .collect();

    // Process each chunk in parallel
    ranges.into_par_iter().for_each(|(start, end)| {
        let num_of_txns = end - start;
        println!("Validating transactions from {} to {}", start, end);
        let txns = aptos_db
            .get_transactions(start, num_of_txns, target_ledger_version, true)
            .unwrap();
        verify_batch_txn_events(&txns, &internal_db, start)
            .unwrap_or_else(|_| panic!("{}, {} failed to verify", start, end));
        assert_eq!(txns.get_num_transactions() as u64, num_of_txns);
    });

    Ok(())
}
```
