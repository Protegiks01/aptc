# Audit Report

## Title
Epoch Snapshot Gap Vulnerability in Table Info Backup Service Causes Irreversible Backup Coverage Holes

## Summary
The table info backup service fails to create snapshots for epochs that contain zero transactions, creating permanent gaps in the backup sequence. This breaks the restore operation's ability to recover from arbitrary epochs and violates backup continuity guarantees.

## Finding Description

The `TableInfoService::run()` function contains flawed logic for determining when to snapshot the database at epoch boundaries. The vulnerability occurs in the epoch transition detection code: [1](#0-0) 

This code only snapshots when `transactions_in_previous_epoch` is not empty. The else branch attempts to handle the empty case: [2](#0-1) 

However, this logic is **fundamentally broken** when an epoch has zero transactions and is completely skipped during batch processing.

**The Attack Scenario:**

Epochs in Aptos can have zero transactions, as confirmed by the reconfiguration mechanism: [3](#0-2) 

When an epoch N has zero transactions:

1. The indexer processes transactions up to epoch N-1, setting `current_epoch = Some(N-1)`
2. Epoch N occurs with 0 transactions (reconfiguration event only)
3. Epoch N+1 begins with transactions
4. Next batch fetch returns only epoch N+1 transactions
5. The `transactions_in_epochs()` function splits the batch: [4](#0-3) 

6. Since all transactions are from epoch N+1 and `current_epoch = Some(N-1)`, the function returns `(vec![], transactions, N+1)` - empty previous epoch
7. The run() function's else branch triggers (line 144)
8. It compares `current_epoch (N-1) != epoch (N+1)` â†’ true
9. It snapshots epoch N-1 **again** (already snapshotted)
10. **Epoch N is NEVER snapshotted**
11. `current_epoch` is updated to N+1, permanently skipping epoch N

The metadata in GCS will show epochs [N-1, N+1, N+2, ...] with epoch N missing entirely.

## Impact Explanation

**Severity: HIGH** - This qualifies as "State inconsistencies requiring intervention" under Medium severity criteria, but escalates to High due to:

1. **Permanent Backup Gaps**: Once an epoch is skipped, there's no mechanism to retroactively create its snapshot. The gap is permanent.

2. **Broken Restore Operations**: The restore logic expects continuous epoch snapshots. Missing epochs break restoration guarantees: [5](#0-4) 

The metadata tracks the "latest backed up epoch" but with gaps, the backup history is inconsistent. The TODO comment reveals this is a known incomplete area: [6](#0-5) 

3. **Compliance/Audit Failures**: Operators cannot prove continuous backup coverage, violating data retention policies.

4. **Production Occurrence**: Epoch 1 (post-genesis) is documented as having minimal/no transactions, making this immediately exploitable in production.

## Likelihood Explanation

**Likelihood: HIGH**

- Epochs with zero transactions are **documented to occur** in the Aptos blockchain (epoch 1 after genesis)
- The reconfiguration mechanism allows epochs to end via events without requiring transactions
- No minimum transaction requirement per epoch exists in the codebase
- This vulnerability triggers automatically during normal network operation - no attacker action required
- Confirmed by the indexer code itself checking for empty epoch scenarios, though incorrectly handling them

## Recommendation

The snapshot logic must track all epochs, not just those with transactions. Here's the fix:

```rust
// At lines 124-166, replace the entire if/else block with:
// Track epochs we've seen to ensure we snapshot all of them
let epochs_to_snapshot = if !transactions_in_previous_epoch.is_empty() {
    // Normal case: process previous epoch transactions and snapshot it
    self.process_transactions_in_parallel(
        self.indexer_async_v2.clone(),
        transactions_in_previous_epoch,
    )
    .await;
    vec![epoch - 1]
} else if let Some(curr_epoch) = current_epoch {
    // No transactions in previous epoch - check if we skipped epochs
    let mut epochs_to_snapshot = Vec::new();
    for skipped_epoch in (curr_epoch + 1)..epoch {
        epochs_to_snapshot.push(skipped_epoch);
    }
    epochs_to_snapshot
} else {
    vec![]
};

// Snapshot all missed epochs
if backup_is_enabled {
    for snapshot_epoch in epochs_to_snapshot {
        aptos_logger::info!(
            epoch = snapshot_epoch,
            "[Table Info] Snapshot taken for epoch"
        );
        Self::snapshot_indexer_async_v2(
            self.context.clone(),
            self.indexer_async_v2.clone(),
            snapshot_epoch,
        )
        .await
        .expect("Failed to snapshot indexer async v2");
    }
}
```

This ensures all epochs between `current_epoch` and `epoch` are snapshotted, even if they had zero transactions.

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[tokio::test]
async fn test_empty_epoch_snapshot_gap() {
    // Setup: Create indexer with backup enabled
    let context = create_test_context().await;
    let backup_operator = create_test_gcs_operator().await;
    let indexer = Arc::new(create_test_indexer_async_v2());
    
    let service = TableInfoService::new(
        context.clone(),
        0,
        10,
        100,
        Some(backup_operator.clone()),
        indexer.clone(),
    );
    
    // Simulate epoch 0 with transactions - gets snapshotted
    process_transactions_for_epoch(&service, 0, vec![tx1, tx2]).await;
    
    // Simulate epoch 1 with ZERO transactions (empty epoch)
    // No transactions to process
    
    // Simulate epoch 2 with transactions
    process_transactions_for_epoch(&service, 2, vec![tx3, tx4]).await;
    
    // Verify: Check GCS for snapshots
    let metadata = backup_operator.get_metadata().await.unwrap();
    assert_eq!(metadata.epoch, 2); // Latest is epoch 2
    
    // Check for epoch 1 snapshot
    let epoch_1_exists = backup_operator
        .snapshot_exists_for_epoch(1)
        .await;
    
    assert!(epoch_1_exists, "VULNERABILITY: Epoch 1 snapshot missing!");
    // This assertion will FAIL, demonstrating the vulnerability
}
```

## Notes

This vulnerability affects the **backup integrity** of the table info indexer service, a critical component for fullnode operators who need reliable disaster recovery. The impact is amplified because:

1. The vulnerability is **silent** - no errors are logged when epochs are skipped
2. Operators won't discover the gaps until restore operations fail
3. The production Aptos blockchain has at least one guaranteed empty epoch (epoch 1 post-genesis)
4. Future network conditions (low transaction volume, rapid reconfigurations) could create additional empty epochs

The incomplete restore logic (per the TODO comment) suggests this system is under active development, making it critical to fix this snapshot gap issue before restore functionality is fully implemented.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L82-82)
```rust
        // TODO: fix the restore logic.
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L124-143)
```rust
            if !transactions_in_previous_epoch.is_empty() {
                self.process_transactions_in_parallel(
                    self.indexer_async_v2.clone(),
                    transactions_in_previous_epoch,
                )
                .await;
                let previous_epoch = epoch - 1;
                if backup_is_enabled {
                    aptos_logger::info!(
                        epoch = previous_epoch,
                        "[Table Info] Snapshot taken at the end of the epoch"
                    );
                    Self::snapshot_indexer_async_v2(
                        self.context.clone(),
                        self.indexer_async_v2.clone(),
                        previous_epoch,
                    )
                    .await
                    .expect("Failed to snapshot indexer async v2");
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L144-166)
```rust
            } else {
                // If there are no transactions in the previous epoch, it means we have caught up to the latest epoch.
                // We still need to figure out if we're at the start of the epoch or in the middle of the epoch.
                if let Some(current_epoch) = current_epoch {
                    if current_epoch != epoch {
                        // We're at the start of the epoch.
                        // We need to snapshot the database.
                        if backup_is_enabled {
                            aptos_logger::info!(
                                epoch = current_epoch,
                                "[Table Info] Snapshot taken at the start of the epoch"
                            );
                            Self::snapshot_indexer_async_v2(
                                self.context.clone(),
                                self.indexer_async_v2.clone(),
                                current_epoch,
                            )
                            .await
                            .expect("Failed to snapshot indexer async v2");
                        }
                    }
                }
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L616-677)
```rust
fn transactions_in_epochs(
    context: &ApiContext,
    current_epoch: Option<u64>,
    mut transactions: Vec<TransactionOnChainData>,
) -> (
    Vec<TransactionOnChainData>,
    Vec<TransactionOnChainData>,
    u64,
) {
    let last_version = transactions
        .last()
        .map(|txn| txn.version)
        .unwrap_or_default();
    let first_version = transactions
        .first()
        .map(|txn| txn.version)
        .unwrap_or_default();
    // Get epoch information.
    let (epoch_first_version, _, block_epoch) = context
        .db
        .get_block_info_by_version(last_version)
        .unwrap_or_else(|_| panic!("Could not get block_info for last version {}", last_version));

    if current_epoch.is_none() {
        // Current epoch is not tracked yet, assume that all transactions are in the current epoch.
        return (vec![], transactions, block_epoch.epoch());
    }
    let current_epoch = current_epoch.unwrap();

    let split_off_index = match current_epoch.cmp(&block_epoch.epoch()) {
        CmpOrdering::Equal => {
            // All transactions are in the this epoch.
            // Previous epoch is empty, i.e., [0, 0), and this epoch is [first_version, last_version].
            0
        },
        CmpOrdering::Less => {
            // Try the best to split the transactions into two epochs.
            epoch_first_version - first_version
        },
        _ => unreachable!("Epochs are not sorted."),
    };

    // Log the split of the transactions.
    aptos_logger::info!(
        split_off_index = split_off_index,
        last_version = last_version,
        first_version = first_version,
        epoch_first_version = epoch_first_version,
        block_epoch = block_epoch.epoch(),
        current_epoch = current_epoch,
        "[Table Info] Split transactions into two epochs."
    );

    let transactions_in_this_epoch = transactions.split_off(split_off_index as usize);
    // The rest of the transactions are in the previous epoch.
    let transactions_in_previous_epoch = transactions;
    (
        transactions_in_previous_epoch,
        transactions_in_this_epoch,
        block_epoch.epoch(),
    )
}
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration.move (L171-189)
```text
    fun emit_genesis_reconfiguration_event() acquires Configuration {
        let config_ref = borrow_global_mut<Configuration>(@aptos_framework);
        assert!(config_ref.epoch == 0 && config_ref.last_reconfiguration_time == 0, error::invalid_state(ECONFIGURATION));
        config_ref.epoch = 1;

        if (std::features::module_event_migration_enabled()) {
            event::emit(
                NewEpoch {
                    epoch: config_ref.epoch,
                },
            );
        };
        event::emit_event<NewEpochEvent>(
            &mut config_ref.events,
            NewEpochEvent {
                epoch: config_ref.epoch,
            },
        );
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L264-314)
```rust
    pub async fn restore_db_snapshot(
        &self,
        chain_id: u64,
        metadata: BackupRestoreMetadata,
        db_path: PathBuf,
        base_path: PathBuf,
    ) -> anyhow::Result<()> {
        assert!(metadata.chain_id == chain_id, "Chain ID mismatch.");

        let epoch = metadata.epoch;
        let epoch_based_filename = generate_blob_name(chain_id, epoch);

        match self
            .gcs_client
            .download_streamed_object(
                &GetObjectRequest {
                    bucket: self.bucket_name.clone(),
                    object: epoch_based_filename.clone(),
                    ..Default::default()
                },
                &Range::default(),
            )
            .await
        {
            Ok(mut stream) => {
                // Create a temporary file and write the stream to it directly
                let temp_file_name = "snapshot.tar.gz";
                let temp_file_path = base_path.join(temp_file_name);
                let temp_file_path_clone = temp_file_path.clone();
                let mut temp_file = File::create(&temp_file_path_clone).await?;
                while let Some(chunk) = stream.next().await {
                    match chunk {
                        Ok(data) => temp_file.write_all(&data).await?,
                        Err(e) => return Err(anyhow::Error::new(e)),
                    }
                }
                temp_file.sync_all().await?;

                // Spawn blocking a thread to synchronously unpack gzipped tar file without blocking the async thread
                task::spawn_blocking(move || unpack_tar_gz(&temp_file_path_clone, &db_path))
                    .await?
                    .expect("Failed to unpack gzipped tar file");

                fs::remove_file(&temp_file_path)
                    .await
                    .context("Failed to remove temporary file after unpacking")?;
                Ok(())
            },
            Err(e) => Err(anyhow::Error::new(e)),
        }
    }
```
