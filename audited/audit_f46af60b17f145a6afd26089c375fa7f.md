# Audit Report

## Title
Resource Leak in Backup Operations: Orphaned Processes from CommandAdapter Shutdown Failures

## Summary
When `shutdown()` fails in backup operations using the CommandAdapter storage backend (for S3, GCS, Azure), the child processes spawned for data compression and upload are not properly terminated. This leads to accumulation of orphaned processes, open file descriptors, and network connections that persist across multiple backup operations, potentially degrading node operational health over time.

## Finding Description

The vulnerability exists in the backup operation flow when using CommandAdapter-based storage backends. When writing backup chunks, the code follows this pattern: [1](#0-0) 

The `chunk_file` is a `Box<dyn AsyncWrite>` that, for CommandAdapter backends, wraps a `ChildStdinAsDataSink` struct. This struct manages a spawned shell command pipeline (e.g., `gzip -c | aws s3 cp -`): [2](#0-1) 

When `shutdown()` is called, it attempts to:
1. Close the stdin pipe to the child process
2. Wait for the child process to complete [3](#0-2) 

**Critical Issue**: If `shutdown()` fails (returns an error), the function returns early via the `?` operator. The `ChildStdinAsDataSink` struct has **no Drop implementation**: [4](#0-3) 

When dropped, the wrapped `tokio::process::Child` is also dropped. However, Tokio's `Child` Drop implementation **does not kill or wait on the child process** - it only closes the stdio handles. This means:

1. The bash shell process continues running
2. The compression process (gzip) continues running  
3. The cloud upload process (aws s3 cp / gsutil / az) continues running
4. All file descriptors, network connections, and memory remain allocated
5. When these processes eventually exit, they become zombie processes (not reaped)

This pattern appears in multiple backup types:
- Epoch ending backups (2 occurrences)
- Transaction backups (3 occurrences)  
- State snapshot backups (4 occurrences) [5](#0-4) 

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty categories, specifically falling under operational reliability issues that can lead to validator node slowdowns and operational intervention requirements.

**Impact Scope**:
- Affects validators and fullnodes performing regular backups using CommandAdapter (production deployments to S3/GCS/Azure)
- Each failed shutdown leaks: bash process, gzip process, cloud CLI process, file descriptors, network connections, memory
- Accumulation over time from periodic backups can lead to:
  - Process limit exhaustion (RLIMIT_NPROC)
  - File descriptor limit exhaustion (RLIMIT_NOFILE)  
  - Memory exhaustion from accumulated processes
  - Degraded network performance from leaked connections
  
**Why Medium Severity**:
- Does not directly impact consensus, funds, or state consistency
- Requires operational failures (network issues, disk errors) to trigger
- Impact is gradual accumulation rather than immediate exploitation
- Can eventually cause node operational issues requiring manual intervention
- Affects backup reliability, which is critical for disaster recovery

## Likelihood Explanation

**Likelihood: Medium to High** in production environments:

**When shutdown() can fail**:
- Network interruptions during cloud uploads (S3/GCS/Azure API timeouts)
- Disk space exhaustion on compression operations
- Cloud provider rate limiting or authentication failures
- Process crashes in gzip or cloud CLI tools
- Pipe errors from broken pipelines

**Accumulation scenarios**:
- Validators run backups every epoch (every ~2 hours typically)
- Transaction backups run continuously
- State snapshots run periodically
- Each backup operation can create multiple chunks, each calling `write_chunk()`
- If even 1% of shutdown operations fail, resources accumulate significantly over days/weeks

**Real-world triggers**:
- Transient network issues are common in cloud environments
- S3/GCS throttling can occur under high load
- Disk space issues can occur on busy nodes
- These are operational realities, not theoretical edge cases

## Recommendation

Implement proper cleanup in the Drop implementation for `ChildStdinAsDataSink` to ensure child processes are terminated even when shutdown fails:

```rust
impl Drop for ChildStdinAsDataSink<'_> {
    fn drop(&mut self) {
        if let Some(mut child) = self.child.take() {
            // If we still have a child and join_fut wasn't started,
            // it means shutdown() didn't complete successfully
            if self.join_fut.is_none() {
                // Try to kill the child process to prevent resource leaks
                // Errors are intentionally ignored in Drop
                let _ = child.child.start_kill();
            }
        }
    }
}
```

Additionally, add explicit error handling and logging in `write_chunk()` to track shutdown failures:

```rust
async fn write_chunk(...) -> Result<EpochEndingChunk> {
    let (chunk_handle, mut chunk_file) = self
        .storage
        .create_for_write(backup_handle, &Self::chunk_name(first_epoch))
        .await?;
    chunk_file.write_all(chunk_bytes).await?;
    
    if let Err(e) = chunk_file.shutdown().await {
        error!(
            "Failed to shutdown backup chunk file for epochs {}-{}: {}",
            first_epoch, last_epoch, e
        );
        return Err(e.into());
    }
    
    Ok(EpochEndingChunk { ... })
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_shutdown_failure_resource_leak() {
    use std::process::Command as StdCommand;
    use tempfile::TempDir;
    
    let temp_dir = TempDir::new().unwrap();
    
    // Create a CommandAdapter config with a command that will fail on shutdown
    let config_content = r#"
env_vars: []
commands:
  create_backup: |
    echo "$BACKUP_NAME"
  create_for_write: |
    echo "$BACKUP_HANDLE/$FILE_NAME"
    exec 1>&-
    # This sleep command will become orphaned when shutdown fails
    sleep 300 > /dev/null
"#;
    
    let config_path = temp_dir.path().join("config.yaml");
    std::fs::write(&config_path, config_content).unwrap();
    
    // Count processes before
    let before_count = count_sleep_processes();
    
    // Attempt backup operation
    let opt = CommandAdapterOpt {
        config: config_path.clone(),
    };
    let storage = CommandAdapter::new_with_opt(opt).await.unwrap();
    
    let backup_handle = storage.create_backup(&"test_backup".parse().unwrap()).await.unwrap();
    let (_, mut file) = storage
        .create_for_write(&backup_handle, &"test.chunk".parse().unwrap())
        .await.unwrap();
    
    // Write some data
    file.write_all(b"test data").await.unwrap();
    
    // Shutdown will fail because the sleep command doesn't consume stdin properly
    // This should leak the sleep process
    let shutdown_result = file.shutdown().await;
    assert!(shutdown_result.is_err(), "Expected shutdown to fail");
    
    // Drop the file handle
    drop(file);
    
    // Give time for processes to spawn
    tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
    
    // Count processes after - should show leaked sleep process
    let after_count = count_sleep_processes();
    
    assert!(
        after_count > before_count,
        "Expected orphaned sleep process to be leaked. Before: {}, After: {}",
        before_count,
        after_count
    );
}

fn count_sleep_processes() -> usize {
    let output = StdCommand::new("pgrep")
        .arg("-c")
        .arg("sleep")
        .output()
        .ok();
    
    output
        .and_then(|o| String::from_utf8(o.stdout).ok())
        .and_then(|s| s.trim().parse::<usize>().ok())
        .unwrap_or(0)
}
```

## Notes

This vulnerability specifically affects CommandAdapter-based storage backends (S3, GCS, Azure) used in production environments. The LocalFs backend does not suffer from this issue as `tokio::fs::File` properly closes file descriptors in its Drop implementation.

The issue is classified as an operational reliability vulnerability rather than a direct security exploit, as it cannot be triggered by external attackers but occurs through normal operational failures (network issues, resource exhaustion). However, the accumulated resource leaks can degrade node performance and stability over time, potentially requiring manual intervention to restart affected nodes.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/epoch_ending/backup.rs (L151-169)
```rust
    async fn write_chunk(
        &self,
        backup_handle: &BackupHandleRef,
        chunk_bytes: &[u8],
        first_epoch: u64,
        last_epoch: u64,
    ) -> Result<EpochEndingChunk> {
        let (chunk_handle, mut chunk_file) = self
            .storage
            .create_for_write(backup_handle, &Self::chunk_name(first_epoch))
            .await?;
        chunk_file.write_all(chunk_bytes).await?;
        chunk_file.shutdown().await?;
        Ok(EpochEndingChunk {
            first_epoch,
            last_epoch,
            ledger_infos: chunk_handle,
        })
    }
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/sample_configs/s3.sample.yaml (L10-18)
```yaml
  create_for_write: |
    # file handle is the file name under the folder with the name of the backup handle
    FILE_HANDLE="$BACKUP_HANDLE/$FILE_NAME"
    # output file handle to stdout
    echo "$FILE_HANDLE"
    # close stdout
    exec 1>&-
    # route stdin to file handle
    gzip -c | aws s3 cp - "s3://$BUCKET/$SUB_DIR/$FILE_HANDLE"
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/command.rs (L167-179)
```rust
pub(super) struct ChildStdinAsDataSink<'a> {
    child: Option<SpawnedCommand>,
    join_fut: Option<BoxFuture<'a, Result<()>>>,
}

impl ChildStdinAsDataSink<'_> {
    fn new(child: SpawnedCommand) -> Self {
        Self {
            child: Some(child),
            join_fut: None,
        }
    }
}
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/command.rs (L205-222)
```rust
    fn poll_shutdown(
        mut self: Pin<&mut Self>,
        cx: &mut Context<'_>,
    ) -> Poll<Result<(), tokio::io::Error>> {
        if self.join_fut.is_none() {
            let res = Pin::new(self.child.as_mut().unwrap().stdin()).poll_shutdown(cx);
            if let Poll::Ready(Ok(_)) = res {
                // pipe shutdown successful
                self.join_fut = Some(self.child.take().unwrap().join().boxed())
            } else {
                return res;
            }
        }

        Pin::new(self.join_fut.as_mut().unwrap())
            .poll(cx)
            .map_err(tokio::io::Error::other)
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/backup.rs (L171-178)
```rust
        proof_file.shutdown().await?;

        let (chunk_handle, mut chunk_file) = self
            .storage
            .create_for_write(backup_handle, &Self::chunk_name(first_version))
            .await?;
        chunk_file.write_all(chunk_bytes).await?;
        chunk_file.shutdown().await?;
```
