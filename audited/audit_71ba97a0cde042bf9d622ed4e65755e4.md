# Audit Report

## Title
DoS via Incomplete Frame Transmission Blocking Network Message Reader

## Summary
An attacker can cause a Denial of Service by sending partial network message frames, blocking the message reader indefinitely until health check timeouts disconnect the peer (~90 seconds). This allows exhaustion of inbound connection slots, potentially impairing validator connectivity and consensus participation.

## Finding Description

The `poll_read_exact()` function in the Noise protocol stream implementation can block indefinitely waiting for data to complete a frame read. [1](#0-0) 

The function's documentation explicitly warns that it may never complete and requires timeout enforcement at the caller side. [2](#0-1) 

However, when messages are read from connected peers via `MultiplexMessageStream`, there is **no timeout wrapper** applied to the read operation. [3](#0-2) 

In contrast, the write path explicitly applies `TRANSPORT_TIMEOUT` for sending messages. [4](#0-3) 

The timeout constant is defined as 30 seconds. [5](#0-4) 

**Attack Scenario:**
1. Attacker establishes connection with validator (completing Noise handshake)
2. Attacker sends a `MultiplexMessage` length prefix (4 bytes) indicating a large message
3. Attacker sends only partial frame data and stops transmission
4. Validator's `poll_read_exact()` continuously returns `Poll::Pending` waiting for remaining data
5. The reader state machine is blocked and cannot process ANY incoming messages, including health check responses
6. Health checker attempts to send pings with 20-second timeout [6](#0-5) 
7. Since reader is blocked, pong responses cannot be processed, causing ping failures
8. After 3 consecutive failures (configured tolerance), connection is disconnected after ~90 seconds [7](#0-6) 

The default health check configuration uses 10-second intervals, 20-second timeouts, and tolerates 3 failures. [8](#0-7) 

With a maximum of 100 inbound connections, [9](#0-8)  an attacker can exhaust all connection slots by repeatedly establishing connections and sending incomplete frames.

## Impact Explanation

This vulnerability constitutes a **Medium Severity** DoS attack per the Aptos bug bounty criteria:

- **Connection Slot Exhaustion**: Attacker can occupy all 100 inbound connection slots for ~90 seconds each, preventing legitimate peers from connecting
- **Consensus Impairment**: If validator-to-validator connections are targeted, consensus participation could be degraded
- **State Inconsistency Risk**: Prolonged connectivity issues may require manual intervention to restore proper network state
- **Resource Drain**: Each blocked connection consumes memory and file descriptors

The impact is limited to network availability degradation rather than fund loss or consensus safety violations, making it Medium rather than High/Critical severity. However, the attack can be sustained by reconnecting after disconnection, allowing prolonged network disruption.

## Likelihood Explanation

**Likelihood: HIGH**

The attack is highly likely to succeed because:
- **Low Complexity**: Attacker only needs to establish a connection and send partial data
- **No Authentication Required**: Any peer can connect and attempt this attack
- **Deterministic Behavior**: The lack of timeout is a code-level issue, not a race condition
- **Minimal Resources**: Attacker needs minimal bandwidth (just initial handshake + small data)
- **Repeatable**: Attack can be repeated across multiple connections and after disconnections

The only mitigating factor is the eventual health check disconnection, but this provides 90 seconds of DoS per connection, which is substantial.

## Recommendation

Apply explicit timeouts to the message reading path, consistent with the write path protection. Modify the peer message reading loop to wrap the reader with a timeout:

**In `network/framework/src/peer/mod.rs`**, modify the message reading branch:

```rust
// Handle a new inbound MultiplexMessage that we've just read off
// the wire from the remote peer.
maybe_message = timeout(transport::TRANSPORT_TIMEOUT, reader.next()) => {
    match maybe_message {
        Ok(Some(message)) => {
            if let Err(err) = self.handle_inbound_message(message, &mut write_reqs_tx) {
                warn!(...);
            }
        },
        Ok(None) => self.shutdown(DisconnectReason::ConnectionClosed),
        Err(_timeout_elapsed) => {
            warn!("Timeout reading message from peer");
            self.shutdown(DisconnectReason::InputOutputError)
        }
    }
},
```

This ensures that if a message read operation (including frame completion) takes longer than 30 seconds, the connection is proactively closed rather than waiting for health check failures.

**Alternative**: Apply timeout at the `MultiplexMessageStream` level or within `poll_read_exact()` itself, though the peer-level timeout is simpler and more consistent with existing timeout patterns in the codebase.

## Proof of Concept

```rust
// PoC: Demonstrate incomplete frame transmission causing blocked reader
// This would be implemented as a network integration test

use aptos_network::noise::NoiseStream;
use futures::io::{AsyncWriteExt, AsyncReadExt};
use tokio::time::{sleep, timeout, Duration};

#[tokio::test]
async fn test_incomplete_frame_blocks_reader() {
    // Setup: Establish two connected peers via in-memory socket
    let (mut client_stream, mut server_stream) = establish_noise_connection().await;
    
    // Attacker: Send frame length indicating 1MB message
    let frame_length: u32 = 1_000_000;
    client_stream.write_all(&frame_length.to_be_bytes()).await.unwrap();
    client_stream.flush().await.unwrap();
    
    // Attacker: Send only 100 bytes then stop
    let partial_data = vec![0x42; 100];
    client_stream.write_all(&partial_data).await.unwrap();
    client_stream.flush().await.unwrap();
    
    // Validator: Try to read message with timeout
    let mut read_buffer = vec![0u8; 2_000_000];
    let read_result = timeout(
        Duration::from_secs(5),
        server_stream.read(&mut read_buffer)
    ).await;
    
    // Assert: Read operation times out (blocked waiting for remaining data)
    assert!(read_result.is_err(), "Read should timeout waiting for incomplete frame");
    
    // Demonstrate: Multiple connections can exhaust slots
    for _ in 0..100 {
        // Spawn connection, send incomplete frame, leave hanging
        // Each blocks a connection slot for ~90 seconds
    }
}
```

**Notes:**
- The actual PoC would require full network stack initialization with peer manager
- Real attack sends incomplete frames through the MultiplexMessage layer after protocol negotiation
- Each incomplete frame blocks one connection slot until health check timeout
- Attack is amplified by opening connections up to `MAX_INBOUND_CONNECTIONS` limit

### Citations

**File:** network/framework/src/noise/stream.rs (L488-491)
```rust
/// As data can be fragmented over multiple TCP packets, poll_read_exact
/// continuously calls poll_read on the socket until enough data is read.
/// It is possible that this function never completes,
/// so a timeout needs to be set on the caller side.
```

**File:** network/framework/src/noise/stream.rs (L492-514)
```rust
fn poll_read_exact<TSocket>(
    context: &mut Context,
    mut socket: Pin<&mut TSocket>,
    buf: &mut [u8],
    offset: &mut usize,
) -> Poll<io::Result<()>>
where
    TSocket: AsyncRead,
{
    assert!(*offset <= buf.len());
    loop {
        let n = ready!(socket.as_mut().poll_read(context, &mut buf[*offset..]))?;
        trace!("poll_read_exact: read {}/{} bytes", *offset + n, buf.len());
        if n == 0 {
            return Poll::Ready(Err(io::ErrorKind::UnexpectedEof.into()));
        }
        assert!(n <= buf.len() - *offset);
        *offset += n;
        if *offset == buf.len() {
            return Poll::Ready(Ok(()));
        }
    }
}
```

**File:** network/framework/src/peer/mod.rs (L240-269)
```rust
            futures::select! {
                // Handle a new outbound request from the PeerManager.
                maybe_request = self.peer_reqs_rx.next() => {
                    match maybe_request {
                        Some(request) => self.handle_outbound_request(request, &mut write_reqs_tx),
                        // The PeerManager is requesting this connection to close
                        // by dropping the corresponding peer_reqs_tx handle.
                        None => self.shutdown(DisconnectReason::RequestedByPeerManager),
                    }
                },
                // Handle a new inbound MultiplexMessage that we've just read off
                // the wire from the remote peer.
                maybe_message = reader.next() => {
                    match maybe_message {
                        Some(message) =>  {
                            if let Err(err) = self.handle_inbound_message(message, &mut write_reqs_tx) {
                                warn!(
                                    NetworkSchema::new(&self.network_context)
                                        .connection_metadata(&self.connection_metadata),
                                    error = %err,
                                    "{} Error in handling inbound message from peer: {}, error: {}",
                                    self.network_context,
                                    remote_peer_id.short_str(),
                                    err
                                );
                            }
                        },
                        // The socket was gracefully closed by the remote peer.
                        None => self.shutdown(DisconnectReason::ConnectionClosed),
                    }
```

**File:** network/framework/src/peer/mod.rs (L358-368)
```rust
                futures::select! {
                    message = stream.select_next_some() => {
                        if let Err(err) = timeout(transport::TRANSPORT_TIMEOUT,writer.send(&message)).await {
                            warn!(
                                log_context,
                                error = %err,
                                "{} Error in sending message to peer: {}",
                                network_context,
                                remote_peer_id.short_str(),
                            );
                        }
```

**File:** network/framework/src/transport/mod.rs (L40-41)
```rust
/// A timeout for the connection to open and complete all of the upgrade steps.
pub const TRANSPORT_TIMEOUT: Duration = Duration::from_secs(30);
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L356-392)
```rust
                // If the ping failures are now more than
                // `self.ping_failures_tolerated`, we disconnect from the node.
                // The HealthChecker only performs the disconnect. It relies on
                // ConnectivityManager or the remote peer to re-establish the connection.
                let failures = self
                    .network_interface
                    .get_peer_failures(peer_id)
                    .unwrap_or(0);
                if failures > self.ping_failures_tolerated {
                    info!(
                        NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                        "{} Disconnecting from peer: {}",
                        self.network_context,
                        peer_id.short_str()
                    );
                    let peer_network_id =
                        PeerNetworkId::new(self.network_context.network_id(), peer_id);
                    if let Err(err) = timeout(
                        Duration::from_millis(50),
                        self.network_interface.disconnect_peer(
                            peer_network_id,
                            DisconnectReason::NetworkHealthCheckFailure,
                        ),
                    )
                    .await
                    {
                        warn!(
                            NetworkSchema::new(&self.network_context)
                                .remote_peer(&peer_id),
                            error = ?err,
                            "{} Failed to disconnect from peer: {} with error: {:?}",
                            self.network_context,
                            peer_id.short_str(),
                            err
                        );
                    }
                }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L397-428)
```rust
    async fn ping_peer(
        network_context: NetworkContext,
        network_client: NetworkClient, // TODO: we shouldn't need to pass the client directly
        peer_id: PeerId,
        round: u64,
        nonce: u32,
        ping_timeout: Duration,
    ) -> (PeerId, u64, u32, Result<Pong, RpcError>) {
        trace!(
            NetworkSchema::new(&network_context).remote_peer(&peer_id),
            round = round,
            "{} Sending Ping request to peer: {} for round: {} nonce: {}",
            network_context,
            peer_id.short_str(),
            round,
            nonce
        );
        let peer_network_id = PeerNetworkId::new(network_context.network_id(), peer_id);
        let res_pong_msg = network_client
            .send_to_peer_rpc(
                HealthCheckerMsg::Ping(Ping(nonce)),
                ping_timeout,
                peer_network_id,
            )
            .await
            .map_err(|error| RpcError::Error(error.into()))
            .and_then(|msg| match msg {
                HealthCheckerMsg::Pong(res) => Ok(res),
                _ => Err(RpcError::InvalidRpcResponse),
            });
        (peer_id, round, nonce, res_pong_msg)
    }
```

**File:** config/src/config/network_config.rs (L38-40)
```rust
pub const PING_INTERVAL_MS: u64 = 10_000;
pub const PING_TIMEOUT_MS: u64 = 20_000;
pub const PING_FAILURES_TOLERATED: u64 = 3;
```

**File:** config/src/config/network_config.rs (L44-44)
```rust
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
```
