# Audit Report

## Title
State Synchronization Incomplete Due to Index Manipulation and Missing Final Root Hash Verification

## Summary
A malicious P2P peer can cause a victim node to accept incomplete state during synchronization by exploiting three critical weaknesses: (1) duplicate key filtering that bypasses proof verification, (2) progress tracking using attacker-controlled indices, and (3) missing final root hash verification. This results in the victim node having an incorrect state tree root hash, violating consensus safety guarantees.

## Finding Description

The vulnerability exists in the state synchronization process where a malicious peer can manipulate state sync chunks to cause incomplete state restoration. The attack exploits three architectural weaknesses:

**Weakness 1: Early Return Without Verification**

When a state value chunk contains only duplicate keys (already synced), the system filters them and returns success without verifying the cryptographic proof. [1](#0-0) 

The proof verification call at line 391 is never reached when all keys are filtered as duplicates, allowing an attacker to send any arbitrary proof without detection.

**Weakness 2: Index Tracking Decoupled from Actual Writes**

The system tracks synchronization progress using the attacker-controlled `last_index` field from the chunk metadata rather than counting actual state keys written to storage. [2](#0-1) 

The storage layer completely ignores these index fields during the actual write operation. [3](#0-2) 

**Weakness 3: Missing Final Root Hash Verification**

The `finish_impl()` method finalizes the state tree by freezing remaining nodes and writing them to storage, but never verifies that the computed root hash matches the expected root hash. [4](#0-3) 

Similarly, the `finalize_state_snapshot()` method persists transaction data without verifying the final state root. [5](#0-4) 

**Attack Execution:**

1. Victim node begins syncing 1000 state keys (K0-K999) at version V from a malicious peer
2. Malicious peer sends **Chunk 1** (legitimate): indices 0-499 with keys K0-K499 and valid proof
   - Victim successfully processes and writes K0-K499
   - Updates `next_state_index_to_process = 500`
3. Malicious peer sends **Chunk 2** (malicious): indices 500-999 with **duplicate** keys K0-K499 and crafted proof P_last
   - The proof P_last has all-placeholder right siblings, causing `is_last_chunk()` to return true [6](#0-5) 
   - Bootstrapper verifies chunk root hash matches expected (this only checks the claimed root_hash field) [7](#0-6) 
   - Storage layer filters all keys as duplicates and returns Ok() without verifying proof
   - System marks sync as complete: `all_states_synced = true` [8](#0-7) 
4. System calls finalization without root hash verification
5. **Result**: Only 500/1000 keys written, but system believes sync succeeded

This breaks the **State Consistency Invariant**: the victim node's state tree has a different root hash than honest nodes, causing consensus divergence when executing transactions.

## Impact Explanation

**Critical Severity** - This qualifies for the highest severity category per Aptos bug bounty criteria:

1. **Consensus/Safety Violation (Critical)**: The victim node will have incomplete state with an incorrect root hash. When attempting to execute new transactions or participate in consensus, it will compute different state roots than honest validators, breaking the deterministic execution requirement fundamental to blockchain consensus. This directly violates AptosBFT safety guarantees.

2. **State Inconsistency**: The victim cannot correctly verify or execute transactions touching the missing state keys (K500-K999 in the example), causing permanent divergence from the canonical chain state.

3. **Network Partition Risk**: Multiple nodes syncing from malicious peers could end up with different incomplete states, fragmenting the network into incompatible partitions requiring manual intervention or hardfork to resolve.

4. **Byzantine Fault Tolerance Compromise**: This attack succeeds with a single malicious peer controlling no validator stake, violating the assumption that state sync maintains Byzantine fault tolerance properties.

## Likelihood Explanation

**High Likelihood**:

1. **No Privileged Access Required**: Any peer participating in the P2P network can execute this attack against nodes syncing from them. Attackers only need to respond to state sync requests with crafted chunks.

2. **Simple Execution**: The attacker captures legitimate state chunks from honest nodes and rearranges them with manipulated indices and crafted proofs. No complex cryptographic attacks or precise timing required.

3. **Silent Failure**: The attack succeeds during initial bootstrap and remains undetected until the victim attempts to access missing state, making post-mortem analysis difficult.

4. **Common Attack Surface**: New nodes joining the network and nodes recovering from downtime regularly perform full state synchronization, providing frequent opportunities for exploitation.

5. **Eclipse Attack Synergy**: Combined with network-level peer selection manipulation, attackers can ensure victims connect to malicious peers for state sync.

## Recommendation

Implement three defense layers:

**Fix 1: Always Verify Proofs**
Modify `add_chunk_impl()` to verify the proof even when all keys are duplicates:
```rust
// In storage/jellyfish-merkle/src/restore/mod.rs add_chunk_impl()
// After filtering duplicates at line 368, add:
if chunk.is_empty() {
    // Still verify the proof even with no new keys
    self.verify(proof)?;
    return Ok(());
}
```

**Fix 2: Verify Final Root Hash**
Add root hash verification in `finish_impl()`:
```rust
// In storage/jellyfish-merkle/src/restore/mod.rs finish_impl()
// Before line 787 (write_node_batch), add:
self.freeze(0);
let computed_root = self.compute_root_hash(); // Implement this
ensure!(
    computed_root == self.expected_root_hash,
    "Final root hash mismatch: computed {:?}, expected {:?}",
    computed_root,
    self.expected_root_hash
);
self.store.write_node_batch(&self.frozen_nodes)?;
```

**Fix 3: Track Actual Keys Written**
Replace index-based progress tracking with actual key counting:
```rust
// In state-sync/state-sync-driver/src/bootstrapper.rs
// Track actual keys written instead of using last_index
let actual_keys_written = count_non_duplicate_keys(state_value_chunk);
self.state_value_syncer.total_keys_written += actual_keys_written;
```

## Proof of Concept

While a complete PoC would require setting up P2P network infrastructure, the vulnerability can be demonstrated by examining the code paths:

1. The filtering logic at `storage/jellyfish-merkle/src/restore/mod.rs:349-370` shows early return when duplicates are detected
2. The verification call at line 391 is unreachable when line 356 or 370 executes
3. The finish_impl at lines 750-789 contains no root hash comparison
4. An attacker crafting a chunk with duplicates and manipulated indices will bypass all verification

The attack succeeds because the three weaknesses combine: duplicate filtering prevents proof verification (W1), index manipulation marks sync complete (W2), and missing final check allows incorrect state (W3).

---

**Notes:**

This is a critical protocol-level vulnerability affecting state sync security. The issue lies in the architectural assumption that proof verification occurs for every chunk, when in reality duplicate filtering creates a bypass. The missing final root hash verification compounds this by removing the last line of defense. Exploitation requires only P2P network access, making this a high-priority fix for network security.

### Citations

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L349-370)
```rust
        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1007-1031)
```rust
        // Verify the chunk root hash matches the expected root hash
        let first_transaction_info = transaction_output_to_sync
            .get_output_list_with_proof()
            .proof
            .transaction_infos
            .first()
            .ok_or_else(|| {
                Error::UnexpectedError("Target transaction info does not exist!".into())
            })?;
        let expected_root_hash = first_transaction_info
            .ensure_state_checkpoint_hash()
            .map_err(|error| {
                Error::UnexpectedError(format!("State checkpoint must exist! Error: {:?}", error))
            })?;
        if state_value_chunk_with_proof.root_hash != expected_root_hash {
            self.reset_active_stream(Some(NotificationAndFeedback::new(
                notification_id,
                NotificationFeedback::InvalidPayloadData,
            )))
            .await?;
            return Err(Error::VerificationError(format!(
                "The states chunk with proof root hash: {:?} didn't match the expected hash: {:?}!",
                state_value_chunk_with_proof.root_hash, expected_root_hash,
            )));
        }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1051-1057)
```rust
        // Update the next state value index to process
        self.state_value_syncer.next_state_index_to_process =
            last_state_value_index.checked_add(1).ok_or_else(|| {
                Error::IntegerOverflow(
                    "The next state value index to process has overflown!".into(),
                )
            })?;
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L88-127)
```rust
    pub fn add_chunk(&mut self, mut chunk: Vec<(K, V)>) -> Result<()> {
        // load progress
        let progress_opt = self.db.get_progress(self.version)?;

        // skip overlaps
        if let Some(progress) = progress_opt {
            let idx = chunk
                .iter()
                .position(|(k, _v)| CryptoHash::hash(k) > progress.key_hash)
                .unwrap_or(chunk.len());
            chunk = chunk.split_off(idx);
        }

        // quit if all skipped
        if chunk.is_empty() {
            return Ok(());
        }

        // save
        let mut usage = progress_opt.map_or(StateStorageUsage::zero(), |p| p.usage);
        let (last_key, _last_value) = chunk.last().unwrap();
        let last_key_hash = CryptoHash::hash(last_key);

        // In case of TreeOnly Restore, we only restore the usage of KV without actually writing KV into DB
        for (k, v) in chunk.iter() {
            usage.add_item(k.key_size() + v.value_size());
        }

        // prepare the sharded kv batch
        let kv_batch: StateValueBatch<K, Option<V>> = chunk
            .into_iter()
            .map(|(k, v)| ((k, self.version), Some(v)))
            .collect();

        self.db.write_kv_batch(
            self.version,
            &kv_batch,
            StateSnapshotProgress::new(last_key_hash, usage),
        )
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-241)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
            restore_utils::save_ledger_infos(
                self.ledger_db.metadata_db(),
                ledger_infos,
                Some(&mut ledger_db_batch.ledger_metadata_db_batches),
            )?;

            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::LedgerCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::OverallCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;

            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;

            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;

            restore_utils::update_latest_ledger_info(self.ledger_db.metadata_db(), ledger_infos)?;
            self.state_store.reset();

            Ok(())
        })
    }
```

**File:** types/src/state_store/state_value.rs (L358-363)
```rust
    pub fn is_last_chunk(&self) -> bool {
        let right_siblings = self.proof.right_siblings();
        right_siblings
            .iter()
            .all(|sibling| *sibling == *SPARSE_MERKLE_PLACEHOLDER_HASH)
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L874-875)
```rust
                    let all_states_synced = states_with_proof.is_last_chunk();
                    let last_committed_state_index = states_with_proof.last_index;
```
