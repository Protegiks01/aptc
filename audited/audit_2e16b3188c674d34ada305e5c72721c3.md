# Audit Report

## Title
Panic Amplification in aptos-infallible Mutex Leading to Validator Block Execution Failure

## Summary
The `aptos-infallible::Mutex` wrapper uses `.expect()` on poisoned locks, causing panic amplification when combined with nested mutex locking patterns in the block executor scheduler. While this can disrupt parallel transaction execution, fallback mechanisms prevent total validator failure.

## Finding Description

The `aptos-infallible::Mutex::lock()` method panics unconditionally on poisoned locks: [1](#0-0) 

The block executor scheduler employs nested mutex acquisition where a thread holds a dependency mutex while attempting to acquire status mutexes: [2](#0-1) 

The `wait_for_dependency` function acquires the dependency mutex, then calls `is_executed()` and `suspend()` which acquire status mutexes: [3](#0-2) 

The `is_executed()` method acquires a read lock on the status mutex: [4](#0-3) 

The `suspend()` method acquires a write lock on the status mutex: [5](#0-4) 

**Attack Scenario:**
1. Thread A holds `dependency_mutex[X]`, attempts to acquire `status_mutex[Y]`
2. Thread B holds `status_mutex[Y]`, encounters a panic (bug, resource exhaustion, invariant violation)
3. Thread B's panic poisons `status_mutex[Y]` during stack unwinding
4. Thread A's `.lock().expect()` receives `PoisonError`, panics
5. Thread A's panic poisons `dependency_mutex[X]`
6. Other threads attempting to acquire either mutex also panic, cascading the failure
7. Parallel block execution fails, triggering fallback to sequential execution [6](#0-5) 

## Impact Explanation

**Severity: Medium (up to $10,000)**

This issue causes validator node slowdowns and requires intervention, but does NOT result in:
- Total validator failure (fallback to sequential execution exists)
- Consensus safety violations
- Loss of funds
- Permanent liveness loss

The parallel execution framework catches worker errors and has explicit fallback mechanisms. If panic amplification occurs, the parallel execution returns `Err(())`, triggering sequential re-execution of the block. [7](#0-6) 

However, this represents a defense-in-depth weakness that could amplify the impact of other bugs.

## Likelihood Explanation

**Likelihood: Low**

Exploitation requires:
1. Triggering an **actual panic** (not error return) in transaction execution code
2. The panic occurring **while holding a mutex**
3. Affecting multiple threads before error handling catches the failure

The codebase uses `Result<T, PanicError>` extensively with `?` operator for error propagation rather than panicking. Worker thread errors are caught: [8](#0-7) 

However, panics could still occur from:
- Unhandled edge cases causing `unwrap()`/`expect()` failures
- Resource exhaustion (OOM, stack overflow)
- Integer overflow in unchecked arithmetic
- Bugs in dependencies
- Array bounds violations

The mutex implementation explicitly acknowledges this limitation but provides no recovery: [9](#0-8) 

## Recommendation

**Short-term mitigation:**
Replace `.expect()` with explicit poison handling that logs the error and attempts recovery or controlled shutdown:

```rust
pub fn lock(&self) -> MutexGuard<'_, T> {
    match self.0.lock() {
        Ok(guard) => guard,
        Err(poisoned) => {
            alert!("Mutex poisoned, attempting recovery");
            // Log diagnostic information
            // Attempt to continue with poisoned data or trigger controlled halt
            poisoned.into_inner()
        }
    }
}
```

**Long-term solution:**
1. Implement poison-aware locking with explicit recovery strategies per use case
2. Add circuit breaker pattern to detect cascading failures
3. Consider replacing nested locking with lock-free data structures or RwLock hierarchies
4. Add comprehensive panic handling in all worker thread entry points

## Proof of Concept

```rust
#[cfg(test)]
mod panic_amplification_test {
    use super::*;
    use std::sync::Arc;
    use std::thread;

    #[test]
    #[should_panic(expected = "Cannot currently handle a poisoned lock")]
    fn test_panic_amplification() {
        let mutex_a = Arc::new(Mutex::new(0));
        let mutex_b = Arc::new(Mutex::new(0));
        
        let mutex_b_clone = mutex_b.clone();
        let thread_b = thread::spawn(move || {
            let _guard_b = mutex_b_clone.lock();
            // Simulate panic while holding lock
            panic!("Intentional panic in thread B");
        });
        
        // Wait for thread B to panic and poison mutex_b
        let _ = thread_b.join();
        
        // Thread A attempts nested locking
        let mutex_a_clone = mutex_a.clone();
        let mutex_b_clone = mutex_b.clone();
        let thread_a = thread::spawn(move || {
            let _guard_a = mutex_a_clone.lock();
            // This will panic due to poisoned mutex_b
            let _guard_b = mutex_b_clone.lock(); 
        });
        
        // Thread A panics, poisoning mutex_a
        thread_a.join().unwrap();
    }
}
```

## Notes

While this issue represents a real weakness in the mutex implementation's panic handling, it does **not** constitute a directly exploitable high-severity vulnerability because:

1. **Known Limitation**: The code explicitly documents it cannot handle poisoned locks
2. **Fallback Exists**: Block executor falls back to sequential execution on parallel failure  
3. **Requires Underlying Bug**: Exploitation requires triggering an actual panic, which the defensive error handling makes difficult
4. **Isolated Impact**: Affects single block execution, not consensus safety or validator liveness permanently

This is primarily a **code quality and robustness issue** that could amplify the severity of other bugs that cause panics. The recommendation is to improve poison handling as a defense-in-depth measure, but this alone does not meet the threshold for high-severity vulnerability classification per the Aptos bug bounty criteria.

### Citations

**File:** crates/aptos-infallible/src/mutex.rs (L7-10)
```rust
/// A simple wrapper around the lock() function of a std::sync::Mutex
/// The only difference is that you don't need to call unwrap() on it.
#[derive(Debug)]
pub struct Mutex<T>(StdMutex<T>);
```

**File:** crates/aptos-infallible/src/mutex.rs (L19-23)
```rust
    pub fn lock(&self) -> MutexGuard<'_, T> {
        self.0
            .lock()
            .expect("Cannot currently handle a poisoned lock")
    }
```

**File:** aptos-move/block-executor/src/scheduler.rs (L719-723)
```rust
        let mut stored_deps = self.txn_dependency[dep_txn_idx as usize].lock();

        // Note: is_executed & suspend calls acquire (a different, status) mutex, while holding
        // (dependency) mutex. This is the only place in scheduler where a thread may hold > 1
        // mutexes. Thus, acquisitions always happen in the same order (here), may not deadlock.
```

**File:** aptos-move/block-executor/src/scheduler.rs (L725-741)
```rust
        if self.is_executed(dep_txn_idx, true).is_some() {
            // Current status of dep_txn_idx is 'executed' (or even committed), so the dependency
            // got resolved. To avoid zombie dependency (and losing liveness), must return here
            // and not add a (stale) dependency.

            // Note: acquires (a different, status) mutex, while holding (dependency) mutex.
            // For status lock this only happens here, thus the order is always higher index to lower.
            return Ok(DependencyResult::Resolved);
        }

        // If the execution is already halted, suspend will return false.
        // The synchronization is guaranteed by the Mutex around txn_status.
        // If the execution is halted, the first finishing thread will first set the status of each txn
        // to be ExecutionHalted, then notify the conditional variable. So if a thread sees ExecutionHalted,
        // it knows the execution is halted and it can return; otherwise, the finishing thread will notify
        // the conditional variable later and awake the pending thread.
        if !self.suspend(txn_idx, dep_condvar.clone())? {
```

**File:** aptos-move/block-executor/src/scheduler.rs (L879-894)
```rust
    fn is_executed(&self, txn_idx: TxnIndex, include_committed: bool) -> Option<Incarnation> {
        let status = self.txn_status[txn_idx as usize].0.read();
        match *status {
            ExecutionStatus::Executed(incarnation) => Some(incarnation),
            ExecutionStatus::Committed(incarnation) => {
                if include_committed {
                    // Committed txns are also considered executed for dependency resolution purposes.
                    Some(incarnation)
                } else {
                    // Committed txns do not need to be scheduled for validation in try_validate_next_version.
                    None
                }
            },
            _ => None,
        }
    }
```

**File:** aptos-move/block-executor/src/scheduler.rs (L974-991)
```rust
    fn suspend(
        &self,
        txn_idx: TxnIndex,
        dep_condvar: DependencyCondvar,
    ) -> Result<bool, PanicError> {
        let mut status = self.txn_status[txn_idx as usize].0.write();
        match *status {
            ExecutionStatus::Executing(incarnation, _) => {
                *status = ExecutionStatus::Suspended(incarnation, dep_condvar);
                Ok(true)
            },
            ExecutionStatus::ExecutionHalted(_) => Ok(false),
            _ => Err(code_invariant_error(format!(
                "Unexpected status {:?} in suspend",
                &*status,
            ))),
        }
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L1778-1799)
```rust
                    if let Err(err) = self.worker_loop_v2(
                        &executor,
                        signature_verified_block,
                        environment,
                        *worker_id,
                        num_workers,
                        &scheduler,
                        &shared_sync_params,
                    ) {
                        // If there are multiple errors, they all get logged: FatalVMError is
                        // logged at construction, below we log CodeInvariantErrors.
                        if let PanicOr::CodeInvariantError(err_msg) = err {
                            alert!(
                                "[BlockSTMv2] worker loop: CodeInvariantError({:?})",
                                err_msg
                            );
                        }
                        shared_maybe_error.store(true, Ordering::SeqCst);

                        // Make sure to halt the scheduler if it hasn't already been halted.
                        scheduler.halt();
                    }
```

**File:** aptos-move/block-executor/src/executor.rs (L2576-2597)
```rust
            // If parallel gave us result, return it
            if let Ok(output) = parallel_result {
                return Ok(output);
            }

            if !self.config.local.allow_fallback {
                panic!("Parallel execution failed and fallback is not allowed");
            }

            // All logs from the parallel execution should be cleared and not reported.
            // Clear by re-initializing the speculative logs.
            init_speculative_logs(signature_verified_block.num_txns() + 1);

            // Flush all caches to re-run from the "clean" state.
            module_cache_manager_guard
                .environment()
                .runtime_environment()
                .flush_all_caches();
            module_cache_manager_guard.module_cache_mut().flush();

            info!("parallel execution requiring fallback");
        }
```

**File:** aptos-move/block-executor/src/executor.rs (L2599-2606)
```rust
        // If we didn't run parallel, or it didn't finish successfully - run sequential
        let sequential_result = self.execute_transactions_sequential(
            signature_verified_block,
            base_view,
            transaction_slice_metadata,
            module_cache_manager_guard,
            false,
        );
```
