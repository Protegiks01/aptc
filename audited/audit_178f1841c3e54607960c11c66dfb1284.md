# Audit Report

## Title
Table Info Restore Mode Fails to Validate Snapshots Leading to Resource Exhaustion from Genesis Re-indexing

## Summary
When a node is configured with `TableInfoServiceMode::Restore` but the GCS bucket contains no snapshots, the node does not fail safely. Instead, it silently ignores the Restore mode and begins indexing all table information from genesis (version 0), causing massive resource consumption and unexpected operational delays.

## Finding Description

The vulnerability exists in the table info service bootstrap logic. When an operator configures a fullnode with `table_info_service_mode: Restore: "bucket-name"` in `fullnode.yaml`, they expect the node to restore table info data from a GCS snapshot. However, the restore functionality is not implemented. [1](#0-0) 

The bootstrap code only creates a `GcsBackupRestoreOperator` for `Backup` mode, while `Restore` mode falls through to the `_ => None` case. This means no backup/restore operator is instantiated for Restore mode. [2](#0-1) 

The service acknowledges this with a TODO comment and proceeds to run without restoration. When `backup_restore_operator` is `None`, the service simply disables backup functionality and starts its normal indexing loop. [3](#0-2) 

With an empty database (no snapshots restored), `next_version()` returns 0, causing the service to begin processing from genesis. [4](#0-3) 

While a `restore_db_snapshot` function exists in `GcsBackupRestoreOperator`, it is never called anywhere in the codebase, confirming that Restore mode is completely unimplemented despite being documented. [5](#0-4) 

**Attack Flow:**
1. Operator configures `table_info_service_mode: Restore: "bucket-name"` expecting fast snapshot restoration
2. Bucket is empty, inaccessible, or snapshots don't exist for the configured chain
3. Node starts without error or warning
4. Service silently initializes with empty database (next_version = 0)
5. Indexing loop begins processing from genesis, consuming massive CPU, memory, and disk I/O
6. On mainnet with millions of transactions, this takes days/weeks to complete
7. Operator experiences unexpected node performance degradation and resource exhaustion

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty criteria for the following reasons:

**Resource Exhaustion Impact:**
- Unexpected massive CPU usage for transaction parsing from genesis
- Excessive memory consumption during table info extraction
- Disk I/O saturation reading historical transaction data
- Network bandwidth usage fetching millions of transactions
- On mainnet, catching up from genesis could take weeks, during which the node is effectively unavailable for its intended purpose

**Operational Security Impact:**
- Violates fail-safe design principles - misconfiguration leads to resource exhaustion rather than safe failure
- Creates availability issues if the indexer node becomes unresponsive due to resource exhaustion
- Operators may mistake this for a network attack or byzantine behavior
- Documentation-reality gap creates operational security risk

While this does not directly affect consensus safety or funds, it represents a significant operational vulnerability that can cause node unavailability and violates the Resource Limits invariant that operations should have bounded resource consumption.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability is likely to occur in production environments because:

1. **Common Operational Scenario**: Operators frequently set up new indexer nodes and may configure Restore mode with an incorrect bucket name or empty bucket
2. **Documentation Encourages Usage**: The README explicitly documents Restore mode as a valid configuration option, encouraging operators to use it
3. **No Validation or Warnings**: There are no validation checks, error messages, or warnings when Restore mode is configured but fails
4. **Silent Failure**: The node starts successfully and begins processing, making the issue non-obvious until resource exhaustion occurs
5. **Testnet to Mainnet Migration**: Operators testing on testnets with different bucket names may inadvertently trigger this on mainnet

The only mitigating factor is that this requires operator action (configuration), but given the documented support for Restore mode, such configuration is expected and reasonable.

## Recommendation

**Immediate Fix:**

1. **Implement Restore Mode or Remove Documentation**: Either fully implement the restore logic in `runtime.rs` or remove Restore mode from the README until implementation is complete.

2. **Add Validation**: Before starting the indexing loop, validate that:
   - For Restore mode, the bucket exists and contains valid snapshots
   - For Restore mode, the chain ID in the snapshot matches the configured chain
   - Fail with a clear error message if validation fails

3. **Fail-Safe Behavior**: Modify the bootstrap code to handle Restore mode explicitly:

```rust
let backup_restore_operator = match node_config.indexer_table_info.table_info_service_mode {
    TableInfoServiceMode::Backup(gcs_bucket_name) => Some(Arc::new(
        GcsBackupRestoreOperator::new(gcs_bucket_name).await,
    )),
    TableInfoServiceMode::Restore(gcs_bucket_name) => {
        let operator = Arc::new(GcsBackupRestoreOperator::new(gcs_bucket_name).await);
        
        // Validate that snapshots exist
        let metadata = operator.get_metadata().await;
        if metadata.is_none() {
            panic!(
                "TableInfoServiceMode::Restore configured but no snapshots found in bucket: {}. \
                Cannot start node in Restore mode without valid snapshots. \
                Either use IndexingOnly mode to index from genesis, or ensure the bucket contains valid snapshots.",
                gcs_bucket_name
            );
        }
        
        // Perform actual restoration
        operator.restore_db_snapshot(
            chain_id.id(),
            metadata.unwrap(),
            db_path.clone(),
            node_config.storage.get_dir_paths().default_root_path(),
        ).await.expect("Failed to restore table info snapshot");
        
        Some(operator)
    }
    _ => None,
};
```

4. **Add Logging**: Log clear warnings when Restore mode is configured to help operators understand what's happening.

## Proof of Concept

**Reproduction Steps:**

1. Set up a fullnode with the following configuration in `fullnode.yaml`:
```yaml
indexer_table_info:
  parser_task_count: 10
  parser_batch_size: 100
  table_info_service_mode:
    Restore: "non-existent-bucket-name-or-empty-bucket"
```

2. Start the fullnode:
```bash
cargo run -p aptos-node --release -- -f ./fullnode.yaml
```

3. Observe the behavior:
   - Node starts without error
   - No warning about missing snapshots
   - `IndexerAsyncV2` initializes with `next_version = 0`
   - Table info service begins indexing from genesis
   - CPU and memory usage spike as it processes historical transactions

4. Check logs - you will see:
```
[Table Info] Preparing to fetch transactions
current_version = 0
```

5. Monitor resource usage - you will observe sustained high CPU, memory, and disk I/O as the service attempts to re-index the entire chain history.

**Expected Behavior:**
- Node should fail to start with a clear error: "Restore mode configured but no snapshots found in bucket"
- Or alternatively, successfully restore from a valid snapshot before starting indexing

**Actual Behavior:**
- Node starts successfully
- Silently begins indexing from genesis
- No indication that Restore mode failed or was ignored
- Massive unexpected resource consumption

---

**Notes:**

This vulnerability demonstrates a critical gap between documented functionality and actual implementation. The Restore mode is advertised as a production-ready feature but is non-functional, creating a significant operational security risk. The silent fallback to genesis indexing violates the principle of failing safely and can cause severe operational disruptions in production environments.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/runtime.rs (L93-98)
```rust
        let backup_restore_operator = match node_config.indexer_table_info.table_info_service_mode {
            TableInfoServiceMode::Backup(gcs_bucket_name) => Some(Arc::new(
                GcsBackupRestoreOperator::new(gcs_bucket_name).await,
            )),
            _ => None,
        };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L82-103)
```rust
        // TODO: fix the restore logic.
        let backup_is_enabled = match self.backup_restore_operator.clone() {
            Some(backup_restore_operator) => {
                let context = self.context.clone();
                let _task = tokio::spawn(async move {
                    loop {
                        aptos_logger::info!("[Table Info] Checking for snapshots to backup.");
                        Self::backup_snapshot_if_present(
                            context.clone(),
                            backup_restore_operator.clone(),
                        )
                        .await;
                        tokio::time::sleep(Duration::from_secs(
                            TABLE_INFO_SNAPSHOT_CHECK_INTERVAL_IN_SECS,
                        ))
                        .await;
                    }
                });
                true
            },
            None => false,
        };
```

**File:** storage/indexer/src/db_v2.rs (L142-147)
```rust
    pub fn next_version(&self) -> Version {
        self.db
            .get::<IndexerMetadataSchema>(&MetadataKey::LatestVersion)
            .unwrap()
            .map_or(0, |v| v.expect_version())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L264-314)
```rust
    pub async fn restore_db_snapshot(
        &self,
        chain_id: u64,
        metadata: BackupRestoreMetadata,
        db_path: PathBuf,
        base_path: PathBuf,
    ) -> anyhow::Result<()> {
        assert!(metadata.chain_id == chain_id, "Chain ID mismatch.");

        let epoch = metadata.epoch;
        let epoch_based_filename = generate_blob_name(chain_id, epoch);

        match self
            .gcs_client
            .download_streamed_object(
                &GetObjectRequest {
                    bucket: self.bucket_name.clone(),
                    object: epoch_based_filename.clone(),
                    ..Default::default()
                },
                &Range::default(),
            )
            .await
        {
            Ok(mut stream) => {
                // Create a temporary file and write the stream to it directly
                let temp_file_name = "snapshot.tar.gz";
                let temp_file_path = base_path.join(temp_file_name);
                let temp_file_path_clone = temp_file_path.clone();
                let mut temp_file = File::create(&temp_file_path_clone).await?;
                while let Some(chunk) = stream.next().await {
                    match chunk {
                        Ok(data) => temp_file.write_all(&data).await?,
                        Err(e) => return Err(anyhow::Error::new(e)),
                    }
                }
                temp_file.sync_all().await?;

                // Spawn blocking a thread to synchronously unpack gzipped tar file without blocking the async thread
                task::spawn_blocking(move || unpack_tar_gz(&temp_file_path_clone, &db_path))
                    .await?
                    .expect("Failed to unpack gzipped tar file");

                fs::remove_file(&temp_file_path)
                    .await
                    .context("Failed to remove temporary file after unpacking")?;
                Ok(())
            },
            Err(e) => Err(anyhow::Error::new(e)),
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/README.md (L37-45)
```markdown
* To use the restore service, 

```
  indexer_table_info:
    ...
    table_info_service_mode:
        Restore:
            your-bucket-name
```
```
