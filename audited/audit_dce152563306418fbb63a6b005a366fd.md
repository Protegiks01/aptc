# Audit Report

## Title
Event Pruning Race Condition Causing Inconsistent Query Results

## Summary
The `get_event_by_key()` function in the event store performs two separate, non-atomic database operations: first looking up event indices via an iterator, then fetching the actual event via a get operation. Between these operations, the background pruner thread can delete the event, causing NotFound errors for events that existed at query time, violating read consistency guarantees.

## Finding Description

The vulnerability exists in the event retrieval flow where queries are not protected by database snapshots or transactions. [1](#0-0) 

This function performs two separate RocksDB operations:
1. **Line 68**: `lookup_event_by_key()` creates an iterator to find `(version, index)` from `EventByKeySchema`
2. **Line 71**: `get_event_by_version_and_index()` performs a separate get operation to fetch the event from `EventSchema` [2](#0-1) [3](#0-2) 

The pruner runs continuously in a background thread without coordination with read operations: [4](#0-3) 

When pruning executes, it atomically deletes both the event indices and events: [5](#0-4) [6](#0-5) 

**Race Window:**
1. API handler calls `get_events_by_event_key()` which invokes `lookup_events_by_key()`
2. Iterator reads `EventByKeySchema` and finds `(version, index)` pairs
3. Iterator is closed, releasing its snapshot
4. **RACE**: Background pruner commits batch deleting both indices and events
5. `get_event_by_version_and_index()` attempts to fetch event from `EventSchema`
6. Event not found â†’ NotFound error returned to user [7](#0-6) 

The underlying RocksDB `get()` operation doesn't use explicit snapshots: [8](#0-7) 

There is no synchronization mechanism preventing concurrent pruning during reads: [9](#0-8) 

The locks only protect commit operations, not reads against pruning.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos Bug Bounty program category "State inconsistencies requiring intervention."

**Impact:**
- **Query Inconsistency**: Users receive NotFound errors for events that existed at query initiation time
- **Data Availability**: Event data becomes unreliable for applications depending on historical event queries
- **State Consistency Violation**: Breaks the invariant that "State transitions must be atomic and verifiable"
- **API Reliability**: REST API endpoints return inconsistent results, affecting user experience and application correctness

This does not directly affect consensus, fund security, or validator operations, but undermines the reliability of the storage layer's read guarantees.

## Likelihood Explanation

**Likelihood: Medium**

The race occurs naturally during normal operation without requiring attacker intervention:

- **Timing Dependency**: Requires overlap between API query processing and background pruning execution
- **Frequency Factors**:
  - More likely on nodes with aggressive pruning configurations
  - Higher probability for events near the pruning horizon
  - Increased likelihood on high-traffic nodes with concurrent queries and active pruning
  - Window size depends on query processing time (typically milliseconds)

While the timing window is narrow, the continuous nature of background pruning and high query volumes make this a realistic scenario that will occur in production environments.

## Recommendation

Wrap both operations in a single RocksDB snapshot to ensure read consistency:

```rust
pub fn get_event_by_key(
    &self,
    event_key: &EventKey,
    seq_num: u64,
    ledger_version: Version,
) -> Result<(Version, ContractEvent)> {
    // Create a snapshot for consistent reads
    let snapshot = self.event_db.get_snapshot();
    
    // Perform both operations using the same snapshot
    let (version, index) = self.lookup_event_by_key_with_snapshot(
        &snapshot, 
        event_key, 
        seq_num, 
        ledger_version
    )?;
    
    let event = snapshot.get::<EventSchema>(&(version, index))?
        .ok_or_else(|| AptosDbError::NotFound(
            format!("Event {} of Txn {}", index, version)
        ))?;
    
    Ok((version, event))
}
```

Alternatively, implement read-write coordination using an RwLock where:
- Pruning operations acquire write lock
- Query operations acquire read lock
- This ensures queries complete before pruning can proceed

The snapshot approach is preferred as it avoids blocking and maintains performance.

## Proof of Concept

```rust
#[cfg(test)]
mod race_condition_test {
    use super::*;
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    
    #[test]
    fn test_event_pruning_race_condition() {
        // Setup: Create database with events
        let db = setup_test_db_with_events();
        let event_store = Arc::clone(&db.event_store);
        let event_key = test_event_key();
        
        // Thread 1: Query events (simulating API call)
        let event_store_clone = Arc::clone(&event_store);
        let query_thread = thread::spawn(move || {
            // This should succeed but may fail due to race
            event_store_clone.get_event_by_key(&event_key, 0, 1000)
        });
        
        // Thread 2: Trigger pruning immediately after query starts
        let prune_thread = thread::spawn(move || {
            thread::sleep(Duration::from_micros(100));
            // Prune events including the one being queried
            db.prune_events(0, 100);
        });
        
        // Wait for both threads
        let query_result = query_thread.join().unwrap();
        prune_thread.join().unwrap();
        
        // Expected: Query should succeed with consistent snapshot
        // Actual: May fail with NotFound due to race condition
        assert!(query_result.is_ok(), 
            "Query failed due to pruning race: {:?}", query_result.err());
    }
}
```

**Steps to reproduce:**
1. Deploy AptosDB with event data near pruning window
2. Configure aggressive pruning (short retention period)
3. Issue concurrent event queries via REST API
4. Trigger background pruning
5. Observe NotFound errors in API responses for events that existed at query time
6. Check logs for EventSchema NotFound errors concurrent with pruning operations

## Notes

This vulnerability affects all event query operations including:
- `get_event_by_key()` in EventStore
- `get_events_by_event_key()` in AptosDB reader
- API endpoints: `/accounts/{address}/events/{creation_number}` and `/accounts/{address}/events/{event_handle}/{field_name}`

The issue is inherent to the separation of index lookup and event retrieval without snapshot isolation, making it a systemic problem in the event storage architecture rather than a localized bug.

### Citations

**File:** storage/aptosdb/src/event_store/mod.rs (L42-50)
```rust
    pub fn get_event_by_version_and_index(
        &self,
        version: Version,
        index: u64,
    ) -> Result<ContractEvent> {
        self.event_db
            .get::<EventSchema>(&(version, index))?
            .ok_or_else(|| AptosDbError::NotFound(format!("Event {} of Txn {}", index, version)))
    }
```

**File:** storage/aptosdb/src/event_store/mod.rs (L62-73)
```rust
    pub fn get_event_by_key(
        &self,
        event_key: &EventKey,
        seq_num: u64,
        ledger_version: Version,
    ) -> Result<(Version, ContractEvent)> {
        let (version, index) = self.lookup_event_by_key(event_key, seq_num, ledger_version)?;
        Ok((
            version,
            self.get_event_by_version_and_index(version, index)?,
        ))
    }
```

**File:** storage/aptosdb/src/event_store/mod.rs (L145-161)
```rust
    fn lookup_event_by_key(
        &self,
        event_key: &EventKey,
        seq_num: u64,
        ledger_version: Version,
    ) -> Result<(Version, u64)> {
        let indices = self.lookup_events_by_key(event_key, seq_num, 1, ledger_version)?;
        if indices.is_empty() {
            return Err(AptosDbError::NotFound(format!(
                "Event {} of seq num {}.",
                event_key, seq_num
            )));
        }
        let (_seq, version, index) = indices[0];

        Ok((version, index))
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L43-81)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let mut indexer_batch = None;

        let indices_batch = if let Some(indexer_db) = self.indexer_db() {
            if indexer_db.event_enabled() {
                indexer_batch = Some(SchemaBatch::new());
            }
            indexer_batch.as_mut()
        } else {
            Some(&mut batch)
        };
        let num_events_per_version = self.ledger_db.event_db().prune_event_indices(
            current_progress,
            target_version,
            indices_batch,
        )?;
        self.ledger_db.event_db().prune_events(
            num_events_per_version,
            current_progress,
            target_version,
            &mut batch,
        )?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        if let Some(mut indexer_batch) = indexer_batch {
            indexer_batch.put::<InternalIndexerMetadataSchema>(
                &IndexerMetadataKey::EventPrunerProgress,
                &IndexerMetadataValue::Version(target_version),
            )?;
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L224-243)
```rust
    /// Deletes a set of events in the range of version in [begin, end), and all related indices.
    pub(crate) fn prune_events(
        &self,
        num_events_per_version: Vec<usize>,
        start: Version,
        end: Version,
        db_batch: &mut SchemaBatch,
    ) -> Result<()> {
        let mut current_version = start;

        for num_events in num_events_per_version {
            for idx in 0..num_events {
                db_batch.delete::<EventSchema>(&(current_version, idx as u64))?;
            }
            current_version += 1;
        }
        self.event_store
            .prune_event_accumulator(start, end, db_batch)?;
        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L1132-1169)
```rust
        let mut event_indices = self.event_store.lookup_events_by_key(
            event_key,
            first_seq,
            real_limit,
            ledger_version,
        )?;

        // When descending, it's possible that user is asking for something beyond the latest
        // sequence number, in which case we will consider it a bad request and return an empty
        // list.
        // For example, if the latest sequence number is 100, and the caller is asking for 110 to
        // 90, we will get 90 to 100 from the index lookup above. Seeing that the last item
        // is 100 instead of 110 tells us 110 is out of bound.
        if order == Order::Descending {
            if let Some((seq_num, _, _)) = event_indices.last() {
                if *seq_num < cursor {
                    event_indices = Vec::new();
                }
            }
        }

        let mut events_with_version = event_indices
            .into_iter()
            .map(|(seq, ver, idx)| {
                let event = self.event_store.get_event_by_version_and_index(ver, idx)?;
                let v0 = match &event {
                    ContractEvent::V1(event) => event,
                    ContractEvent::V2(_) => bail!("Unexpected module event"),
                };
                ensure!(
                    seq == v0.sequence_number(),
                    "Index broken, expected seq:{}, actual:{}",
                    seq,
                    v0.sequence_number()
                );
                Ok(EventWithVersion::new(ver, event))
            })
            .collect::<Result<Vec<_>>>()?;
```

**File:** storage/schemadb/src/lib.rs (L216-232)
```rust
    pub fn get<S: Schema>(&self, schema_key: &S::Key) -> DbResult<Option<S::Value>> {
        let _timer = APTOS_SCHEMADB_GET_LATENCY_SECONDS.timer_with(&[S::COLUMN_FAMILY_NAME]);

        let k = <S::Key as KeyCodec<S>>::encode_key(schema_key)?;
        let cf_handle = self.get_cf_handle(S::COLUMN_FAMILY_NAME)?;

        let result = self.inner.get_cf(cf_handle, k).into_db_res()?;
        APTOS_SCHEMADB_GET_BYTES.observe_with(
            &[S::COLUMN_FAMILY_NAME],
            result.as_ref().map_or(0.0, |v| v.len() as f64),
        );

        result
            .map(|raw_value| <S::Value as ValueCodec<S>>::decode_value(&raw_value))
            .transpose()
            .map_err(Into::into)
    }
```

**File:** storage/aptosdb/src/db/mod.rs (L26-41)
```rust
pub struct AptosDB {
    pub(crate) ledger_db: Arc<LedgerDb>,
    pub(crate) state_kv_db: Arc<StateKvDb>,
    pub(crate) event_store: Arc<EventStore>,
    pub(crate) state_store: Arc<StateStore>,
    pub(crate) transaction_store: Arc<TransactionStore>,
    ledger_pruner: LedgerPrunerManager,
    _rocksdb_property_reporter: RocksdbPropertyReporter,
    /// This is just to detect concurrent calls to `pre_commit_ledger()`
    pre_commit_lock: std::sync::Mutex<()>,
    /// This is just to detect concurrent calls to `commit_ledger()`
    commit_lock: std::sync::Mutex<()>,
    indexer: Option<Indexer>,
    skip_index_and_usage: bool,
    update_subscriber: Option<Sender<(Instant, Version)>>,
}
```
