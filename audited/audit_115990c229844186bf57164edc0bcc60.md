# Audit Report

## Title
Indexer Timestamp Parsing Lacks Upper Bound Validation Leading to Database Corruption

## Summary
The indexer's `parse_timestamp()` function does not validate or clamp timestamps to the `MAX_TIMESTAMP_SECS` limit (year 9999-12-31), allowing far-future timestamps to be stored in the database. This creates invalid database entries that break BigQuery exports and time-based analytics queries, requiring manual intervention.

## Finding Description

The indexer processes `BlockMetadataTransaction` objects and extracts timestamps using the `parse_timestamp()` function. [1](#0-0) 

The `parse_timestamp()` function converts microsecond timestamps to `chrono::NaiveDateTime` without applying upper bound validation: [2](#0-1) 

Critically, the codebase defines `MAX_TIMESTAMP_SECS = 253_402_300_799` (December 31, 9999) as the maximum supported timestamp for BigQuery compatibility: [3](#0-2) 

A separate function `parse_timestamp_secs()` correctly implements clamping to this limit, but it is not used for block metadata transactions: [4](#0-3) 

**Attack Scenario:**

If validators experience synchronized clock misconfiguration (e.g., all set to year 2100 due to NTP failure or configuration error), they would propose and accept blocks with far-future timestamps. The consensus layer validates timestamps are within 5 minutes of each validator's clock: [5](#0-4) 

With synchronized clock skew, this validation would pass, allowing invalid timestamps into committed blocks. The Move VM's timestamp validation only checks monotonic increase, not absolute bounds: [6](#0-5) 

When the indexer processes these blocks, `parse_timestamp()` accepts any timestamp within `chrono::NaiveDateTime`'s range (approximately year -262144 to 262143), converting it without validation and storing it in the PostgreSQL `TIMESTAMP` column: [7](#0-6) 

The database insertion occurs without timestamp validation: [8](#0-7) 

## Impact Explanation

This constitutes **Medium severity** under the "State inconsistencies requiring intervention" category because:

1. **Data Integrity Violation**: The indexer database contains timestamps that violate the documented MAX_TIMESTAMP_SECS constraint, corrupting historical data
2. **BigQuery Export Failure**: Google BigQuery has a hard limit of year 9999-12-31; any timestamp beyond this causes export failures, breaking critical analytics infrastructure
3. **Query Corruption**: Time-based queries (e.g., "transactions in last 30 days") may incorrectly include or exclude data, producing wrong analytical results
4. **Manual Intervention Required**: Requires database migration scripts or manual cleanup to fix corrupted timestamps after the fact
5. **Cascading System Failures**: Downstream systems relying on accurate timestamps will malfunction

While this doesn't directly affect consensus or fund security, it breaks data integrity invariants and requires manual intervention to remediateâ€”matching the Medium severity definition.

## Likelihood Explanation

**Medium likelihood** due to:

1. **Validator Clock Skew**: NTP failures, configuration errors, or time zone misconfigurations affecting multiple validators simultaneously
2. **Genesis Misconfiguration**: If chain initialization uses an invalid timestamp, all subsequent blocks inherit the error
3. **Consensus Implementation Bugs**: Potential bugs in timestamp validation logic that could bypass the 5-minute check
4. **No Defense-in-Depth**: The indexer lacks input validation, creating a single point of failure when upstream validation fails

While requiring validator-level issues, such scenarios are realistic operational risks in distributed systems with multiple deployment environments.

## Recommendation

Add timestamp clamping to the `parse_timestamp()` function to match the protection in `parse_timestamp_secs()`:

```rust
pub fn parse_timestamp(ts: u64, version: i64) -> chrono::NaiveDateTime {
    let seconds = ts / 1000000;
    // Clamp to MAX_TIMESTAMP_SECS for BigQuery compatibility
    let seconds = std::cmp::min(seconds, MAX_TIMESTAMP_SECS as u64);
    let ns = (ts % 1000000 * 1000).try_into().unwrap_or_else(|_| {
        panic!(
            "Could not get nanoseconds for timestamp {:?} for version {}",
            ts, version
        )
    });
    #[allow(deprecated)]
    chrono::NaiveDateTime::from_timestamp_opt(seconds as i64, ns)
        .unwrap_or_else(|| panic!("Could not parse timestamp {:?} for version {}", ts, version))
}
```

This provides defense-in-depth by ensuring the indexer database never contains timestamps beyond BigQuery's supported range, even if upstream validation fails.

## Proof of Concept

```rust
#[cfg(test)]
mod timestamp_vulnerability_test {
    use super::*;
    use chrono::Datelike;

    #[test]
    fn test_far_future_timestamp_bypasses_max_limit() {
        // Timestamp representing year 10000 (315537897600000000 microseconds)
        let year_10000_timestamp: u64 = 315_537_897_600_000_000;
        let version = 12345;
        
        // This should be clamped but currently is not
        let parsed = parse_timestamp(year_10000_timestamp, version);
        
        // Verify it exceeds MAX_TIMESTAMP_SECS
        let seconds = year_10000_timestamp / 1000000;
        assert!(seconds > MAX_TIMESTAMP_SECS as u64, 
                "Timestamp should exceed MAX_TIMESTAMP_SECS");
        
        // The parsed timestamp is year 10000 (not clamped to 9999)
        assert_eq!(parsed.year(), 10000, 
                   "Should be year 10000, demonstrating lack of clamping");
        
        // This timestamp would break BigQuery exports
        println!("Vulnerability: Timestamp {} exceeds BigQuery limit of {}",
                 seconds, MAX_TIMESTAMP_SECS);
    }
    
    #[test]
    fn test_parse_timestamp_secs_has_protection() {
        // Same year 10000 timestamp in seconds
        let year_10000_secs: u64 = 315_537_897_600;
        let version = 12345;
        
        // parse_timestamp_secs correctly clamps
        let parsed = parse_timestamp_secs(year_10000_secs, version);
        
        // Verify it's clamped to year 9999
        assert_eq!(parsed.year(), 9999, 
                   "Should be clamped to year 9999");
    }
}
```

This test demonstrates that `parse_timestamp()` accepts far-future timestamps without validation, while `parse_timestamp_secs()` correctly clamps them. The fix is to apply the same clamping logic to `parse_timestamp()`.

### Citations

**File:** crates/indexer/src/models/block_metadata_transactions.rs (L69-69)
```rust
            timestamp: parse_timestamp(txn.timestamp.0, txn_version),
```

**File:** crates/indexer/src/util.rs (L11-12)
```rust
// 9999-12-31 23:59:59, this is the max supported by Google BigQuery
pub const MAX_TIMESTAMP_SECS: i64 = 253_402_300_799;
```

**File:** crates/indexer/src/util.rs (L45-56)
```rust
pub fn parse_timestamp(ts: u64, version: i64) -> chrono::NaiveDateTime {
    let seconds = ts / 1000000;
    let ns = (ts % 1000000 * 1000).try_into().unwrap_or_else(|_| {
        panic!(
            "Could not get nanoseconds for timestamp {:?} for version {}",
            ts, version
        )
    });
    #[allow(deprecated)]
    chrono::NaiveDateTime::from_timestamp_opt(seconds as i64, ns)
        .unwrap_or_else(|| panic!("Could not parse timestamp {:?} for version {}", ts, version))
}
```

**File:** crates/indexer/src/util.rs (L58-65)
```rust
pub fn parse_timestamp_secs(ts: u64, version: i64) -> chrono::NaiveDateTime {
    #[allow(deprecated)]
    chrono::NaiveDateTime::from_timestamp_opt(
        std::cmp::min(ts, MAX_TIMESTAMP_SECS as u64) as i64,
        0,
    )
    .unwrap_or_else(|| panic!("Could not parse timestamp {:?} for version {}", ts, version))
}
```

**File:** consensus/consensus-types/src/block.rs (L532-539)
```rust
            let current_ts = duration_since_epoch();

            // we can say that too far is 5 minutes in the future
            const TIMEBOUND: u64 = 300_000_000;
            ensure!(
                self.timestamp_usecs() <= (current_ts.as_micros() as u64).saturating_add(TIMEBOUND),
                "Blocks must not be too far in the future"
            );
```

**File:** aptos-move/framework/aptos-framework/sources/timestamp.move (L46-48)
```text
            // Normal block. Time must advance
            assert!(now < timestamp, error::invalid_argument(EINVALID_TIMESTAMP));
            global_timer.microseconds = timestamp;
```

**File:** crates/indexer/migrations/2022-08-08-043603_core_tables/up.sql (L94-94)
```sql
  "timestamp" TIMESTAMP NOT NULL,
```

**File:** crates/indexer/src/processors/default_processor.rs (L254-269)
```rust
fn insert_block_metadata_transactions(
    conn: &mut PgConnection,
    items_to_insert: &[BlockMetadataTransactionModel],
) -> Result<(), diesel::result::Error> {
    use schema::block_metadata_transactions::dsl::*;
    let chunks = get_chunks(
        items_to_insert.len(),
        BlockMetadataTransactionModel::field_count(),
    );
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::block_metadata_transactions::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict(version)
                .do_nothing(),
```
