# Audit Report

## Title
Cache Invalidation Race Condition in AptosDBBackend Causes Consensus Safety Violation Through Non-Deterministic Proposer Selection

## Summary
The `AptosDBBackend` implementation of the `MetadataBackend` trait contains a critical cache invalidation logic flaw that allows concurrent validators to retrieve inconsistent block metadata for the same round, leading to non-deterministic proposer selection and breaking consensus safety guarantees.

## Finding Description

The vulnerability exists in the cache validation logic within `AptosDBBackend::get_block_metadata()`. The method uses a shared cache (protected by `Mutex`) to store the latest N block events fetched from the database. When validators query for a specific round's metadata, the code checks if cached data is sufficient using the `has_larger` predicate: [1](#0-0) 

This check only verifies that the cache contains at least one event with `(epoch, round) >= (target_epoch, target_round)`, but does NOT verify that the cache actually contains the target round itself or sufficient historical events around it.

The cache always stores the LATEST N events from the database: [2](#0-1) 

This creates a race condition where:
1. Validator A queries round 100 when cache contains events [100, 99, 98, 97, 96]
2. New blocks commit, advancing the chain to round 105
3. Cache gets refreshed (by Validator B querying round 105 or by Validator A's next query)
4. Cache now contains events [105, 104, 103, 102, 101]
5. Validator C queries round 100:
   - `has_larger` check passes (105 >= 100)
   - Skips refresh, uses cached events [105, 104, 103, 102, 101]
   - Filters for events <= round 100: **EMPTY SET**
   - Returns `(vec![], HashValue::zero())` [3](#0-2) 

The empty result with `HashValue::zero()` is then used for proposer selection: [4](#0-3) 

When `use_root_hash` is enabled (default for all versions except V1): [5](#0-4) 

Different validators computing proposers for the same round will use different seeds:
- Validator A (early query): `seed = [root_hash(version_100), epoch, round]`  
- Validator C (late query): `seed = [HashValue::zero(), epoch, round]`

This produces **different proposer selections**, violating the fundamental consensus safety invariant that all honest validators must agree on the proposer for each round.

**Broken Invariant**: Consensus Safety - AptosBFT must ensure all honest validators agree on the leader for each round. This vulnerability allows validators to deterministically but inconsistently select different proposers based on cache timing, causing the network to fork or stall.

## Impact Explanation

**CRITICAL Severity** - This vulnerability directly violates consensus safety, qualifying for the highest severity category under Aptos bug bounty rules:

- **Consensus/Safety violation**: Different validators select different proposers for the same round, causing validators to vote for different blocks, leading to chain splits or inability to form quorum certificates
- **Non-recoverable without coordination**: Once validators diverge on proposer selection, they cannot recover without manual intervention or a network restart
- **Affects all validators**: Every validator running with LeaderReputation (V2+) is susceptible
- **No attacker required**: This occurs naturally during normal block production as the cache advances

The vulnerability undermines the core security property of BFT consensus - that honest validators maintain agreement on the blockchain state.

## Likelihood Explanation

**HIGH Likelihood** - This race condition manifests under normal operating conditions:

1. **Automatic triggering**: No malicious action required; occurs naturally as blocks commit
2. **Common scenario**: Validators frequently query different rounds (current round, catch-up, validation)
3. **Time window**: The vulnerability window extends from when the cache is refreshed until all validators synchronize
4. **Production configuration**: LeaderReputationV2 (which uses root hash) is the recommended and widely deployed configuration
5. **Shared backend**: The `AptosDBBackend` is wrapped in `Arc` and shared across consensus threads [6](#0-5) 

The likelihood increases with:
- High block production rate (more frequent cache refreshes)
- Network latency between validators
- Validators at slightly different round heights

## Recommendation

Fix the cache validation logic to check if the cache contains sufficient events **at or below** the target round, not just any event above it:

```rust
fn get_block_metadata(
    &self,
    target_epoch: u64,
    target_round: Round,
) -> (Vec<NewBlockEvent>, HashValue) {
    let mut locked = self.db_result.lock();
    let latest_db_version = self.aptos_db.get_latest_ledger_info_version().unwrap_or(0);
    
    if locked.is_none() {
        if let Err(e) = self.refresh_db_result(&mut locked, latest_db_version) {
            warn!(error = ?e, "[leader reputation] Fail to initialize db result");
            return (vec![], HashValue::zero());
        }
    }
    
    let (events, version, hit_end) = {
        let result = locked.as_ref().unwrap();
        (&result.0, result.1, result.2)
    };

    // FIX: Check if cache has sufficient events at or below target round
    let num_relevant_events = events
        .iter()
        .filter(|e| (e.event.epoch(), e.event.round()) <= (target_epoch, target_round))
        .count();
    
    let has_sufficient_data = num_relevant_events >= self.window_size || hit_end;

    // Refresh if we don't have enough relevant historical data
    if !has_sufficient_data && version < latest_db_version {
        let fresh_db_result = self.refresh_db_result(&mut locked, latest_db_version);
        match fresh_db_result {
            Ok((events, _version, hit_end)) => {
                self.get_from_db_result(target_epoch, target_round, &events, hit_end)
            },
            Err(e) => {
                warn!(error = ?e, "[leader reputation] Fail to refresh window");
                (vec![], HashValue::zero())
            },
        }
    } else {
        self.get_from_db_result(target_epoch, target_round, events, hit_end)
    }
}
```

Alternative: Implement per-round caching or ensure cache covers a sufficient historical window that doesn't get evicted during normal operation.

## Proof of Concept

```rust
#[cfg(test)]
mod cache_race_condition_test {
    use super::*;
    use aptos_types::account_config::NewBlockEvent;
    use std::sync::Arc;
    use std::thread;
    
    #[test]
    fn test_concurrent_queries_produce_inconsistent_results() {
        // Setup: Create AptosDBBackend with mock DB that advances
        let mock_db = Arc::new(MockDbWithAdvancingBlocks::new());
        let backend = Arc::new(AptosDBBackend::new(3, 2, mock_db.clone()));
        
        // Validator A queries round 100 (cache: [100,99,98,97,96])
        let backend_a = backend.clone();
        let handle_a = thread::spawn(move || {
            backend_a.get_block_metadata(1, 100)
        });
        
        // Simulate block production: advance to round 105
        mock_db.advance_to_round(105);
        
        // Force cache refresh by querying round 105
        backend.get_block_metadata(1, 105);
        
        // Validator C queries round 100 again (cache now: [105,104,103,102,101])
        let backend_c = backend.clone();
        let handle_c = thread::spawn(move || {
            backend_c.get_block_metadata(1, 100)
        });
        
        let (events_a, hash_a) = handle_a.join().unwrap();
        let (events_c, hash_c) = handle_c.join().unwrap();
        
        // VULNERABILITY: Same round produces different results
        assert_ne!(events_a.len(), events_c.len(), 
            "Cache race allows different validators to get inconsistent event counts");
        assert_ne!(hash_a, hash_c, 
            "Different root hashes lead to different proposer selection");
        
        // This breaks consensus as choose_index(weights, hash_a) != choose_index(weights, hash_c)
    }
}
```

## Notes

The vulnerability is particularly insidious because:
1. It's deterministic from each validator's perspective (same cache state → same result)
2. But non-deterministic across validators (different cache refresh timing → different results)
3. The mutex provides thread safety for the cache structure but not semantic consistency for consensus
4. The `DbReader` trait correctly requires `Send + Sync`, but the caching layer above it violates consensus determinism requirements

The fix must ensure that queries for the same `(epoch, round)` always return consistent results regardless of when they're executed or what other queries have occurred.

### Citations

**File:** consensus/src/liveness/leader_reputation.rs (L70-101)
```rust
    fn refresh_db_result(
        &self,
        locked: &mut MutexGuard<'_, Option<(Vec<VersionedNewBlockEvent>, u64, bool)>>,
        latest_db_version: u64,
    ) -> Result<(Vec<VersionedNewBlockEvent>, u64, bool)> {
        // assumes target round is not too far from latest commit
        let limit = self.window_size + self.seek_len;

        let events = self.aptos_db.get_latest_block_events(limit)?;

        let max_returned_version = events.first().map_or(0, |first| first.transaction_version);

        let new_block_events = events
            .into_iter()
            .map(|event| {
                Ok(VersionedNewBlockEvent {
                    event: bcs::from_bytes::<NewBlockEvent>(event.event.event_data())?,
                    version: event.transaction_version,
                })
            })
            .collect::<Result<Vec<VersionedNewBlockEvent>, bcs::Error>>()?;

        let hit_end = new_block_events.len() < limit;

        let result = (
            new_block_events,
            std::cmp::max(latest_db_version, max_returned_version),
            hit_end,
        );
        **locked = Some(result.clone());
        Ok(result)
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L103-165)
```rust
    fn get_from_db_result(
        &self,
        target_epoch: u64,
        target_round: Round,
        events: &Vec<VersionedNewBlockEvent>,
        hit_end: bool,
    ) -> (Vec<NewBlockEvent>, HashValue) {
        // Do not warn when round==0, because check will always be unsure of whether we have
        // all events from the previous epoch. If there is an actual issue, next round will log it.
        if target_round != 0 {
            let has_larger = events.first().is_some_and(|e| {
                (e.event.epoch(), e.event.round()) >= (target_epoch, target_round)
            });
            if !has_larger {
                // error, and not a fatal, in an unlikely scenario that we have many failed consecutive rounds,
                // and nobody has any newer successful blocks.
                warn!(
                    "Local history is too old, asking for {} epoch and {} round, and latest from db is {} epoch and {} round! Elected proposers are unlikely to match!!",
                    target_epoch, target_round, events.first().map_or(0, |e| e.event.epoch()), events.first().map_or(0, |e| e.event.round()))
            }
        }

        let mut max_version = 0;
        let mut result = vec![];
        for event in events {
            if (event.event.epoch(), event.event.round()) <= (target_epoch, target_round)
                && result.len() < self.window_size
            {
                max_version = std::cmp::max(max_version, event.version);
                result.push(event.event.clone());
            }
        }

        if result.len() < self.window_size && !hit_end {
            error!(
                "We are not fetching far enough in history, we filtered from {} to {}, but asked for {}. Target ({}, {}), received from {:?} to {:?}.",
                events.len(),
                result.len(),
                self.window_size,
                target_epoch,
                target_round,
                events.last().map_or((0, 0), |e| (e.event.epoch(), e.event.round())),
                events.first().map_or((0, 0), |e| (e.event.epoch(), e.event.round())),
            );
        }

        if result.is_empty() {
            warn!("No events in the requested window could be found");
            (result, HashValue::zero())
        } else {
            let root_hash = self
                .aptos_db
                .get_accumulator_root_hash(max_version)
                .unwrap_or_else(|_| {
                    error!(
                        "We couldn't fetch accumulator hash for the {} version, for {} epoch, {} round",
                        max_version, target_epoch, target_round,
                    );
                    HashValue::zero()
                });
            (result, root_hash)
        }
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L193-197)
```rust
        let has_larger = events
            .first()
            .is_some_and(|e| (e.event.epoch(), e.event.round()) >= (target_epoch, target_round));
        // check if fresher data has potential to give us different result
        if !has_larger && version < latest_db_version {
```

**File:** consensus/src/liveness/leader_reputation.rs (L695-734)
```rust
impl ProposerElection for LeaderReputation {
    fn get_valid_proposer_and_voting_power_participation_ratio(
        &self,
        round: Round,
    ) -> (Author, VotingPowerRatio) {
        let target_round = round.saturating_sub(self.exclude_round);
        let (sliding_window, root_hash) = self.backend.get_block_metadata(self.epoch, target_round);
        let voting_power_participation_ratio =
            self.compute_chain_health_and_add_metrics(&sliding_window, round);
        let mut weights =
            self.heuristic
                .get_weights(self.epoch, &self.epoch_to_proposers, &sliding_window);
        let proposers = &self.epoch_to_proposers[&self.epoch];
        assert_eq!(weights.len(), proposers.len());

        // Multiply weights by voting power:
        let stake_weights: Vec<u128> = weights
            .iter_mut()
            .enumerate()
            .map(|(i, w)| *w as u128 * self.voting_powers[i] as u128)
            .collect();

        let state = if self.use_root_hash {
            [
                root_hash.to_vec(),
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        } else {
            [
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        };

        let chosen_index = choose_index(stake_weights, state);
        (proposers[chosen_index], voting_power_participation_ratio)
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L540-544)
```rust
impl LeaderReputationType {
    pub fn use_root_hash_for_seed(&self) -> bool {
        // all versions after V1 should use root hash
        !matches!(self, Self::ProposerAndVoter(_))
    }
```

**File:** consensus/src/epoch_manager.rs (L342-346)
```rust
                let backend = Arc::new(AptosDBBackend::new(
                    window_size,
                    seek_len,
                    self.storage.aptos_db(),
                ));
```
