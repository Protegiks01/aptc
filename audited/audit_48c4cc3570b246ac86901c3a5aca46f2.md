# Audit Report

## Title
REGISTRY HashMap Capacity Never Shrinks Causing Unbounded Memory Growth Over Validator Lifetime

## Summary
The global static `REGISTRY` in `types/src/state_store/state_key/registry.rs` uses `HashMap` data structures that grow to accommodate millions of unique state keys over a validator's lifetime but never shrink their allocated capacity, even after entries are removed. This causes a memory leak where the HashMap bucket arrays can grow to consume gigabytes of memory, leading to validator node performance degradation.

## Finding Description
The `REGISTRY` is a global static structure that maintains weak references to `Entry` objects for state key deduplication. [1](#0-0) 

Each `StateKey` wraps an `Arc<Entry>` obtained from the registry. [2](#0-1) 

When a `StateKey` is created, it calls `get_or_add` on the appropriate registry shard, which either returns an existing `Arc<Entry>` by upgrading a `Weak` pointer or creates a new entry. [3](#0-2) 

The registry correctly removes `Weak<Entry>` pointers when entries are dropped through the `maybe_remove` mechanism. [4](#0-3) 

However, the underlying `HashMap` in `TwoKeyRegistry` never shrinks its allocated capacity. [5](#0-4) 

**Attack Vector:**
Over a validator's lifetime (months to years), the blockchain naturally accumulates millions to billions of unique state keys through:
- Millions of user accounts (each with multiple resources)
- Large tables with millions of entries  
- Thousands of deployed modules

Production scale configurations reference **1.33 billion accounts** and **4 billion items** in the database. [6](#0-5) 

Performance tests use **100 million accounts** for large database scenarios. [7](#0-6) 

As unique state keys are created, the `HashMap` grows to accommodate them. When `StateKey` objects are later dropped and `Arc<Entry>` references are cleaned up, the `maybe_remove` function removes the `Weak` pointers, but Rust's `HashMap::remove()` does **not** shrink the allocated capacity. The bucket arrays remain at their maximum size.

**Memory Growth Calculation:**
- Each HashMap entry: ~40-50 bytes (Weak pointer + HashMap overhead)
- 100 million unique keys: ~5 GB
- 1 billion unique keys: ~50 GB

This memory is never reclaimed without a full validator process restart.

## Impact Explanation
This qualifies as **High Severity** under the Aptos bug bounty program criteria: "Validator node slowdowns."

The unbounded memory growth causes:
1. **Increased RAM consumption** proportional to the total number of unique state keys ever accessed
2. **Performance degradation** due to memory pressure, swapping, and cache pollution
3. **Potential OOM crashes** on validators with limited memory
4. **Operational burden** requiring periodic validator restarts to clear memory

At production scale with billions of state keys, this could consume 10-50 GB of memory per validator node, significantly impacting node performance and requiring operators to provision extra memory or restart nodes regularly.

## Likelihood Explanation
**Likelihood: Very High**

This is not an attack requiring malicious activityâ€”it occurs through normal blockchain operation:
- Every new account creates unique state keys
- Every table entry creates unique state keys
- Natural blockchain growth over months/years accumulates millions to billions of unique keys

The vulnerability manifests automatically over time on any long-running validator node. No special actions, exploits, or malicious transactions are required.

## Recommendation
Implement periodic HashMap capacity shrinking to reclaim unused memory. Several approaches:

**Option 1: Periodic shrink_to_fit**
Add a maintenance thread or periodic cleanup that calls `shrink_to_fit()` on the HashMaps:

```rust
impl<Key1, Key2> TwoKeyRegistry<Key1, Key2> {
    pub fn shrink_to_fit(&self) {
        let mut locked = self.inner.write();
        for (_, map2) in locked.iter_mut() {
            map2.shrink_to_fit();
        }
        locked.shrink_to_fit();
    }
}
```

Call this periodically (e.g., every epoch or after pruning) in a background task.

**Option 2: Bounded LRU cache for REGISTRY**
Replace the unbounded HashMap with a bounded LRU cache that evicts least-recently-used registry entries when capacity is exceeded.

**Option 3: Per-epoch registry rotation**
Create a new registry at each epoch boundary and deprecate the old one after a grace period, allowing full memory reclamation.

**Recommended approach:** Option 1 combined with monitoring, as it's the simplest fix with minimal code changes. Add metrics to track REGISTRY memory usage and trigger periodic `shrink_to_fit()` calls.

## Proof of Concept

```rust
#[test]
fn test_registry_memory_growth() {
    use aptos_types::state_store::state_key::StateKey;
    use std::collections::HashSet;
    
    // Track allocated memory (approximate via unique key count)
    let mut created_keys = HashSet::new();
    
    // Phase 1: Create millions of unique StateKeys
    println!("Creating 1 million unique StateKeys...");
    for i in 0..1_000_000 {
        let key = StateKey::raw(&format!("key_{}", i).into_bytes());
        created_keys.insert(i);
        // Key is dropped here
    }
    
    // Phase 2: All StateKeys and Arc<Entry> are dropped
    // Weak<Entry> pointers should be removed via maybe_remove
    println!("All StateKeys dropped, Weak pointers removed");
    
    // Phase 3: Create new StateKeys - REGISTRY HashMap capacity remains large
    println!("Creating new StateKeys...");
    for i in 0..100 {
        let key = StateKey::raw(&format!("new_key_{}", i).into_bytes());
        drop(key);
    }
    
    // Expected: REGISTRY HashMap still has capacity for ~1 million entries
    // despite only needing capacity for 100
    // This capacity is never reclaimed without process restart
    
    println!("REGISTRY HashMap capacity is still at ~1M entries");
    println!("Memory leak: HashMap bucket array consumes ~40-50 MB");
    println!("With billions of keys over validator lifetime: 10-50 GB leak");
}
```

To observe the actual memory growth, run a validator node for an extended period while monitoring memory usage. The REGISTRY memory will monotonically increase proportional to the number of unique state keys accessed, even as old keys are dropped and removed from active caches.

## Notes

The vulnerability is in the design of using an unbounded global static HashMap that never shrinks. While the `Weak<Entry>` cleanup mechanism works correctly (entries are removed), the HashMap's allocated capacity persists indefinitely, causing a slow memory leak proportional to blockchain growth.

This affects all validator nodes and archive nodes running for extended periods. The only current mitigation is periodic process restarts, which is operationally burdensome and doesn't address the root cause.

### Citations

**File:** types/src/state_store/state_key/registry.rs (L71-73)
```rust
pub(crate) struct TwoKeyRegistry<Key1, Key2> {
    inner: RwLock<HashMap<Key1, HashMap<Key2, Weak<Entry>>>>,
}
```

**File:** types/src/state_store/state_key/registry.rs (L151-163)
```rust
    fn maybe_remove(&self, key1: &Key1, key2: &Key2) {
        let mut locked = self.inner.write();
        if let Some(map2) = locked.get_mut(key1) {
            if let Some(entry) = map2.get(key2) {
                if entry.strong_count() == 0 {
                    map2.remove(key2);
                    if map2.is_empty() {
                        locked.remove(key1);
                    }
                }
            }
        }
    }
```

**File:** types/src/state_store/state_key/registry.rs (L165-183)
```rust
    pub fn get_or_add<Ref1, Ref2, Gen>(
        &self,
        key1: &Ref1,
        key2: &Ref2,
        inner_gen: Gen,
    ) -> Result<Arc<Entry>>
    where
        Key1: Borrow<Ref1>,
        Key2: Borrow<Ref2>,
        Ref1: Eq + Hash + ToOwned<Owned = Key1> + ?Sized,
        Ref2: Eq + Hash + ToOwned<Owned = Key2> + ?Sized,
        Gen: FnOnce() -> Result<StateKeyInner>,
    {
        if let Some(entry) = self.read_lock_try_get(key1, key2) {
            return Ok(entry);
        }

        self.write_lock_get_or_add(key1, key2, inner_gen)
    }
```

**File:** types/src/state_store/state_key/registry.rs (L194-209)
```rust
pub static REGISTRY: Lazy<StateKeyRegistry> = Lazy::new(StateKeyRegistry::default);

const NUM_RESOURCE_SHARDS: usize = 8;
const NUM_RESOURCE_GROUP_SHARDS: usize = 8;
const NUM_MODULE_SHARDS: usize = 8;
const NUM_TABLE_ITEM_SHARDS: usize = 8;
const NUM_RAW_SHARDS: usize = 4;

#[derive(Default)]
pub struct StateKeyRegistry {
    resource_shards: [TwoKeyRegistry<StructTag, AccountAddress>; NUM_RESOURCE_SHARDS],
    resource_group_shards: [TwoKeyRegistry<StructTag, AccountAddress>; NUM_RESOURCE_GROUP_SHARDS],
    module_shards: [TwoKeyRegistry<AccountAddress, Identifier>; NUM_MODULE_SHARDS],
    table_item_shards: [TwoKeyRegistry<TableHandle, Vec<u8>>; NUM_TABLE_ITEM_SHARDS],
    raw_shards: [TwoKeyRegistry<Vec<u8>, ()>; NUM_RAW_SHARDS], // for tests only
}
```

**File:** types/src/state_store/state_key/mod.rs (L47-48)
```rust
#[derive(Clone)]
pub struct StateKey(Arc<Entry>);
```

**File:** config/src/config/storage_config.rs (L426-428)
```rust
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
```

**File:** testsuite/single_node_performance.py (L58-58)
```python
    "100000000" if SELECTED_FLOW == Flow.MAINNET_LARGE_DB else "2000000"
```
