# Audit Report

## Title
Database Error Ambiguity in Quorum Store Batch Retrieval Causes Validator Liveness Issues

## Summary
The `get_batch_from_db()` function in the batch store treats database errors (`Err(DbError)`) identically to missing batches (`Ok(None)`), converting both to `ExecutorError::CouldNotGetData`. This ambiguity prevents validators from distinguishing between recoverable missing data scenarios and critical database failures, potentially causing validators to enter infinite retry loops instead of alerting operators to database corruption.

## Finding Description
The vulnerability exists in the error handling pattern used when retrieving batches from the QuorumStore database. [1](#0-0) 

The problematic pattern `Ok(None) | Err(_) => { ... Err(ExecutorError::CouldNotGetData) }` conflates two fundamentally different cases:

1. **Legitimate Missing Batch** (`Ok(None)`): The batch doesn't exist in the database because it was never stored, already expired, or hasn't been received yet from peers
2. **Database Error** (`Err(DbError)`): The database operation failed due to corruption, I/O errors, deserialization failures, or other storage-layer issues

When a validator encounters either case, it enters a retry loop in the block materialization pipeline. [2](#0-1) 

**Exploitation Scenario:**

When a validator's QuorumStore database becomes corrupted or experiences I/O failures:

1. Batch metadata exists in the in-memory cache (from prior gossip/storage)
2. When attempting to read the actual payload via `get_batch_from_local()`, the database returns `Err(DbError)` due to corruption [3](#0-2) 
3. This error is converted to `ExecutorError::CouldNotGetData` (same as missing batch)
4. The validator's block preparer enters an infinite retry loop with 100ms delays
5. The batch is requested from peers, but re-persistence may fail due to the same database issue
6. No alerts are triggered about database corruption; operators believe the node is simply "waiting for batch data"
7. The validator becomes stuck on this block indefinitely, unable to make consensus progress

The error type definition confirms that `CouldNotGetData` is used for both cases: [4](#0-3) 

## Impact Explanation
**Severity: Medium**

This issue qualifies as **Medium Severity** under the "State inconsistencies requiring intervention" category because:

- **Validator Liveness Impact**: Individual validators with database corruption can become stuck in retry loops, reducing network validator count and potentially affecting consensus liveness if enough validators are affected
- **Masked Critical Failures**: Database corruption is a critical operational issue that should trigger immediate alerts and intervention, but this bug masks it as routine "missing batch" behavior
- **Requires Manual Intervention**: Operators must manually diagnose why a validator is stuck, potentially requiring database inspection or node restart
- **No Consensus Safety Violation**: Does not cause forks, double-spends, or state inconsistencies across the network
- **No Direct Fund Loss**: Does not enable theft or unauthorized minting of tokens

This does NOT qualify as Critical or High severity because:
- It affects individual validator availability, not network-wide consensus safety
- The network continues to operate with remaining healthy validators (< 1/3 Byzantine tolerance)
- No funds are at risk

## Likelihood Explanation
**Likelihood: Low to Medium**

The vulnerability requires specific conditions to manifest:

**Trigger Conditions:**
- Database corruption (disk failures, filesystem issues, software bugs in RocksDB)
- I/O errors during batch retrieval
- Deserialization errors from corrupted stored data
- Incomplete writes from prior crashes (metadata persisted, payload not persisted)

**Likelihood Factors:**
- **Low**: Modern databases and filesystems are generally reliable
- **Medium**: Storage failures do occur in production systems, especially at scale
- **Medium**: Validators operate 24/7 under high write loads, increasing corruption risk
- **High**: The retry loop provides no escape mechanism or timeout for database errors

While not exploitable by external attackers without node compromise, database issues are realistic operational scenarios that can affect any validator operator.

## Recommendation
Implement proper error discrimination to distinguish missing batches from database errors:

```rust
fn get_batch_from_db(
    &self,
    digest: &HashValue,
    is_v2: bool,
) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
    counters::GET_BATCH_FROM_DB_COUNT.inc();

    let result = if is_v2 {
        self.db.get_batch_v2(digest)
    } else {
        self.db.get_batch(digest).map(|opt| opt.map(Into::into))
    };

    match result {
        Ok(Some(value)) => Ok(value),
        Ok(None) => {
            // Batch legitimately doesn't exist - peers should have it
            debug!("Batch {} not found in local DB", digest);
            Err(ExecutorError::DataNotFound(*digest))
        },
        Err(db_error) => {
            // Critical database error - alert and potentially crash
            error!("Database error retrieving batch {}: {:?}", digest, db_error);
            counters::QUORUM_STORE_DB_ERROR_COUNT.inc();
            Err(ExecutorError::InternalError {
                error: format!("QuorumStore DB error: {}", db_error)
            })
        }
    }
}
```

Additional improvements:
1. Add monitoring counters for database errors vs. missing batches
2. Implement circuit breaker pattern for repeated database errors
3. Add alerts when database error rate exceeds threshold
4. Consider graceful degradation or node shutdown on persistent database errors

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_crypto::HashValue;
    use std::sync::Arc;

    // Mock QuorumStoreStorage that simulates database errors
    struct FailingQuorumStoreDB {
        fail_with_error: bool,
    }

    impl QuorumStoreStorage for FailingQuorumStoreDB {
        fn get_batch(&self, _digest: &HashValue) -> Result<Option<PersistedValue<BatchInfo>>, DbError> {
            if self.fail_with_error {
                // Simulate database I/O error
                Err(DbError { inner: anyhow::anyhow!("Simulated DB corruption") })
            } else {
                // Simulate missing batch
                Ok(None)
            }
        }
        
        // Other trait methods omitted for brevity...
    }

    #[test]
    fn test_database_error_vs_missing_batch_ambiguity() {
        // Case 1: Missing batch (Ok(None))
        let db_missing = Arc::new(FailingQuorumStoreDB { fail_with_error: false });
        let store_missing = BatchStore::new(/* params with db_missing */);
        let result_missing = store_missing.get_batch_from_db(&HashValue::random(), false);
        
        // Case 2: Database error (Err(DbError))
        let db_error = Arc::new(FailingQuorumStoreDB { fail_with_error: true });
        let store_error = BatchStore::new(/* params with db_error */);
        let result_error = store_error.get_batch_from_db(&HashValue::random(), false);
        
        // Both return the same error type - this is the vulnerability
        assert!(matches!(result_missing, Err(ExecutorError::CouldNotGetData)));
        assert!(matches!(result_error, Err(ExecutorError::CouldNotGetData)));
        
        // The errors are indistinguishable, but should be treated differently:
        // - Missing batch -> fetch from peers
        // - Database error -> alert operators, possibly crash
    }
}
```

## Notes
The underlying database implementation properly distinguishes these cases through its return type `Result<Option<T>, DbError>`, making the fix straightforward. The ambiguity is introduced solely by the error handling in `get_batch_from_db()`. This represents a defensive programming failure rather than a fundamental design flaw.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L545-569)
```rust
    fn get_batch_from_db(
        &self,
        digest: &HashValue,
        is_v2: bool,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        counters::GET_BATCH_FROM_DB_COUNT.inc();

        if is_v2 {
            match self.db.get_batch_v2(digest) {
                Ok(Some(value)) => Ok(value),
                Ok(None) | Err(_) => {
                    warn!("Could not get batch from db");
                    Err(ExecutorError::CouldNotGetData)
                },
            }
        } else {
            match self.db.get_batch(digest) {
                Ok(Some(value)) => Ok(value.into()),
                Ok(None) | Err(_) => {
                    warn!("Could not get batch from db");
                    Err(ExecutorError::CouldNotGetData)
                },
            }
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L571-585)
```rust
    pub(crate) fn get_batch_from_local(
        &self,
        digest: &HashValue,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        if let Some(value) = self.db_cache.get(digest) {
            if value.payload_storage_mode() == StorageMode::PersistedOnly {
                self.get_batch_from_db(digest, value.batch_info().is_v2())
            } else {
                // Available in memory.
                Ok(value.clone())
            }
        } else {
            Err(ExecutorError::CouldNotGetData)
        }
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L634-646)
```rust
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
```

**File:** execution/executor-types/src/error.rs (L41-42)
```rust
    #[error("request timeout")]
    CouldNotGetData,
```
