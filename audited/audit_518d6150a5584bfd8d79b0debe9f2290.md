# Audit Report

## Title
JWK Consensus Task Starvation Due to Blocking I/O in Async Context During Epoch Transitions

## Summary
The JWK consensus runtime performs blocking disk or network I/O operations during epoch transitions without using `tokio::task::spawn_blocking()`, which can monopolize worker threads and starve the `NetworkTask` from processing incoming RPC requests under heavy load or slow storage conditions.

## Finding Description

The `start_jwk_consensus_runtime()` function spawns two tasks on a shared 4-thread Tokio runtime: `network_task` and `epoch_manager`. [1](#0-0) 

During epoch transitions, `EpochManager::start_new_epoch()` retrieves the consensus private key by calling `self.key_storage.consensus_sk_by_pk(my_pk)` synchronously within an async function. [2](#0-1) 

This calls into `PersistentSafetyStorage::consensus_sk_by_pk()`, which performs synchronous storage reads through `self.internal_store.get()`. [3](#0-2) 

When the storage backend is `OnDiskStorage`, this results in blocking file I/O using `std::fs::File::open()` and `read_to_string()`. [4](#0-3) 

When the storage backend is `VaultStorage`, this results in blocking HTTP requests to the Vault server. [5](#0-4) 

**The Problem:** Blocking I/O in async tasks without `spawn_blocking()` occupies a worker thread until the I/O completes. With only 4 worker threads, prolonged blocking operations can prevent other tasks from being scheduled. The `NetworkTask` processes incoming RPC messages in a loop using `self.all_events.next().await`. [6](#0-5) 

Under adverse conditions (slow network-mounted storage, overloaded disks, slow Vault responses, or frequent epoch transitions in test environments), the blocking I/O can occupy multiple worker threads simultaneously, starving the `NetworkTask` and delaying RPC message processing.

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria:
- **Validator node slowdowns**: Delayed JWK consensus message processing degrades validator performance during epoch transitions
- **State inconsistencies requiring intervention**: Prolonged message delays could cause JWK consensus to lag behind the network, requiring manual investigation

The impact is limited because:
- JWK consensus is not critical to main consensus safety
- Epoch transitions are infrequent under normal operations (~2 hours)
- The runtime has 4 worker threads, providing some resilience

However, under heavy load or slow storage, this can cause measurable service degradation.

## Likelihood Explanation

**Likelihood: Medium** under specific conditions:
1. **Slow storage backends**: Network-mounted storage (NFS, EBS with poor I/O), overloaded disks, or high-latency Vault servers
2. **Rapid epoch transitions**: Test environments, devnets, or misconfigured networks with frequent reconfigurations
3. **Heavy network load**: High volume of concurrent JWK consensus RPC requests

The issue is more likely in production deployments using Vault for key storage over unreliable networks, or validators with network-mounted storage.

## Recommendation

Wrap the blocking storage operation in `tokio::task::spawn_blocking()` to prevent worker thread monopolization:

```rust
let my_sk = tokio::task::spawn_blocking({
    let key_storage = self.key_storage.clone();
    let my_pk = my_pk.clone();
    move || key_storage.consensus_sk_by_pk(&my_pk)
})
.await
.map_err(|e| anyhow!("spawn_blocking join error: {e}"))??;
```

This pattern is already used elsewhere in the codebase for blocking operations. [7](#0-6) 

**Note:** This requires `PersistentSafetyStorage` to implement `Clone` or be wrapped in `Arc`, and the method signature may need adjustment.

## Proof of Concept

A Rust reproduction would require:
1. Configuring a JWK consensus validator with `OnDiskStorage` on a slow filesystem (simulated with `sleep()` in a custom storage implementation)
2. Triggering an epoch transition via reconfig notification
3. Simultaneously sending high volumes of JWK consensus RPC requests
4. Measuring message processing latency and observing increased delays during epoch transitions

Due to the complexity of setting up a full JWK consensus environment, a simplified demonstration showing the blocking behavior:

```rust
#[tokio::test]
async fn test_blocking_io_starves_tasks() {
    let runtime = tokio::runtime::Builder::new_multi_thread()
        .worker_threads(4)
        .build()
        .unwrap();
    
    let blocking_task = runtime.spawn(async {
        // Simulate blocking I/O like OnDiskStorage::read()
        std::thread::sleep(std::time::Duration::from_secs(5));
    });
    
    let responsive_task = runtime.spawn(async {
        let start = std::time::Instant::now();
        // This should execute immediately but may be delayed
        tokio::time::sleep(std::time::Duration::from_millis(10)).await;
        start.elapsed()
    });
    
    let _ = blocking_task.await;
    let elapsed = responsive_task.await.unwrap();
    
    // Under load, responsive_task experiences delays
    assert!(elapsed > std::time::Duration::from_millis(10));
}
```

## Notes

This issue also affects the main consensus `EpochManager` which uses the same pattern. [8](#0-7)  However, the security question specifically focuses on JWK consensus task scheduling fairness.

The runtime configuration uses `.disable_lifo_slot()`, which actually improves fairness by forcing tasks through the global queue rather than thread-local queues. [9](#0-8)  However, this does not prevent blocking I/O from monopolizing worker threads.

### Citations

**File:** crates/aptos-jwk-consensus/src/lib.rs (L34-48)
```rust
    let runtime = aptos_runtimes::spawn_named_runtime("jwk".into(), Some(4));
    let (self_sender, self_receiver) = aptos_channels::new(1_024, &counters::PENDING_SELF_MESSAGES);
    let jwk_consensus_network_client = JWKConsensusNetworkClient::new(network_client);
    let epoch_manager = EpochManager::new(
        my_addr,
        safety_rules_config,
        reconfig_events,
        jwk_updated_events,
        self_sender,
        jwk_consensus_network_client,
        vtxn_pool_writer,
    );
    let (network_task, network_receiver) = NetworkTask::new(network_service_events, self_receiver);
    runtime.spawn(network_task.start());
    runtime.spawn(epoch_manager.start(network_receiver));
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L217-219)
```rust
            let my_sk = self.key_storage.consensus_sk_by_pk(my_pk).map_err(|e| {
                anyhow!("jwk-consensus new epoch handling failed with consensus sk lookup err: {e}")
            })?;
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L106-132)
```rust
    pub fn consensus_sk_by_pk(
        &self,
        pk: bls12381::PublicKey,
    ) -> Result<bls12381::PrivateKey, Error> {
        let _timer = counters::start_timer("get", CONSENSUS_KEY);
        let pk_hex = hex::encode(pk.to_bytes());
        let explicit_storage_key = format!("{}_{}", CONSENSUS_KEY, pk_hex);
        let explicit_sk = self
            .internal_store
            .get::<bls12381::PrivateKey>(explicit_storage_key.as_str())
            .map(|v| v.value);
        let default_sk = self.default_consensus_sk();
        let key = match (explicit_sk, default_sk) {
            (Ok(sk_0), _) => sk_0,
            (Err(_), Ok(sk_1)) => sk_1,
            (Err(_), Err(_)) => {
                return Err(Error::ValidatorKeyNotFound("not found!".to_string()));
            },
        };
        if key.public_key() != pk {
            return Err(Error::SecureStorageMissingDataError(format!(
                "Incorrect sk saved for {:?} the expected pk",
                pk
            )));
        }
        Ok(key)
    }
```

**File:** secure/storage/src/on_disk.rs (L53-62)
```rust
    fn read(&self) -> Result<HashMap<String, Value>, Error> {
        let mut file = File::open(&self.file_path)?;
        let mut contents = String::new();
        file.read_to_string(&mut contents)?;
        if contents.is_empty() {
            return Ok(HashMap::new());
        }
        let data = serde_json::from_str(&contents)?;
        Ok(data)
    }
```

**File:** secure/storage/vault/src/lib.rs (L255-262)
```rust
    pub fn read_secret(&self, secret: &str, key: &str) -> Result<ReadResponse<Value>, Error> {
        let request = self
            .agent
            .get(&format!("{}/v1/secret/data/{}", self.host, secret));
        let resp = self.upgrade_request(request).call();

        process_secret_read_response(secret, key, resp)
    }
```

**File:** crates/aptos-jwk-consensus/src/network.rs (L188-210)
```rust
    pub async fn start(mut self) {
        while let Some(message) = self.all_events.next().await {
            match message {
                Event::RpcRequest(peer_id, msg, protocol, response_sender) => {
                    let req = IncomingRpcRequest {
                        msg,
                        sender: peer_id,
                        response_sender: Box::new(RealRpcResponseSender {
                            inner: Some(response_sender),
                            protocol,
                        }),
                    };

                    if let Err(e) = self.rpc_tx.push(peer_id, (peer_id, req)) {
                        warn!(error = ?e, "aptos channel closed");
                    };
                },
                _ => {
                    // Ignore
                },
            }
        }
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1099-1105)
```rust
            executor
                .commit_ledger(ledger_info_with_sigs_clone)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(Some(ledger_info_with_sigs))
```

**File:** consensus/src/epoch_manager.rs (L1228-1233)
```rust
        let loaded_consensus_key = match self.load_consensus_key(&epoch_state.verifier) {
            Ok(k) => Arc::new(k),
            Err(e) => {
                panic!("load_consensus_key failed: {e}");
            },
        };
```

**File:** crates/aptos-runtimes/src/lib.rs (L47-47)
```rust
        .disable_lifo_slot()
```
