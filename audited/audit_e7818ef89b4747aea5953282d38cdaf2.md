# Audit Report

## Title
Missing Final State Root Hash Verification After Snapshot Sync Completion Allows Data Loss

## Summary
When `snapshot_sync_completed` is set to true in the metadata storage, there is no final verification that the persisted state tree root hash matches the expected root hash from the ledger info. The system relies solely on intermediate chunk-by-chunk proof verification but never performs an end-to-end verification after all data is supposedly persisted, allowing incomplete or corrupted state to be marked as complete.

## Finding Description
The state snapshot synchronization process in Aptos follows this flow:

1. State value chunks are received from peers and added via `save_state_values()` [1](#0-0) 

2. Each chunk's Merkle proof is verified during addition [2](#0-1) 

3. When a chunk with `is_last_chunk() = true` is received, the system calls `finalize_storage_and_send_commit()` [3](#0-2) 

4. This function calls `state_snapshot_receiver.finish_box()` which finalizes the Jellyfish Merkle tree without verifying the final root hash [4](#0-3) 

5. Then it calls `finalize_state_snapshot()` to save transaction metadata, again without root hash verification [5](#0-4) 

6. Finally, `snapshot_sync_completed = true` is set in metadata storage [6](#0-5) 

The critical vulnerability is that `JellyfishMerkleRestore::finish_impl()` does NOT verify that the computed root hash matches the `expected_root_hash` stored in `self.expected_root_hash`. It only freezes nodes and writes them to storage. [4](#0-3) 

This breaks the **State Consistency** and **Deterministic Execution** invariants because:

**Attack Scenario 1 - Premature Last Chunk Flag:**
A malicious or buggy peer sends a state chunk with `is_last_chunk() = true` before transmitting all state data. Each individual chunk's proof may verify correctly (showing the partial data is consistent), but the COMPLETE state tree is never fully constructed. The system finalizes the incomplete tree and marks it as complete.

**Attack Scenario 2 - Silent Write Failures:**
Database write operations fail silently due to disk issues, corruption, or resource exhaustion. The in-memory Merkle proof verification succeeds, but the actual data is not persisted to disk. Without reading back and verifying the persisted root hash, the corruption goes undetected.

**Attack Scenario 3 - Async Write Race Conditions:**
The tree restoration uses async writes. If `wait_for_async_commit()` doesn't properly wait for all writes, or if writes are queued but not executed before `finish_impl()` returns, the system may mark the snapshot as complete before all data is persisted.

## Impact Explanation
This vulnerability meets **Critical Severity** criteria:

- **Consensus/Safety Violations**: Validators with incomplete state will compute different state roots when executing transactions, breaking consensus safety guarantees and potentially causing network partitions.

- **State Inconsistency**: Different validators may have different versions of the state database, leading to non-deterministic behavior across the network.

- **Non-recoverable Network Partition**: If a significant portion of validators have incomplete state, the network may split into incompatible factions, requiring a hard fork to recover.

The impact affects the core invariant that "All validators must produce identical state roots for identical blocks." Once state databases diverge due to incomplete snapshots being marked as complete, the network cannot recover without manual intervention.

## Likelihood Explanation
This vulnerability has **High Likelihood** because:

1. **No Privileged Access Required**: Any network peer participating in state sync can send malformed chunks with incorrect `is_last_chunk()` flags.

2. **Multiple Trigger Paths**: The vulnerability can be triggered by malicious peers, bugs in data streaming logic, database write failures, or timing issues in async operations.

3. **Silent Failure Mode**: The vulnerability manifests silently - the node believes its state is correct and continues operating, only discovering the issue when state roots don't match during transaction execution.

4. **No Defensive Verification**: The code path has zero defensive checks after finalization. Once `snapshot_sync_completed = true` is set, the system trusts the state is complete.

## Recommendation
Add final state root hash verification after all state data is persisted. Specifically, in `finalize_storage_and_send_commit()`, after calling `storage.writer.finalize_state_snapshot()` and before setting `snapshot_sync_completed = true`:

```rust
// Verify the final persisted root hash matches the expected hash
let persisted_root_hash = storage
    .reader
    .get_state_snapshot_before(version + 1)
    .map_err(|e| format!("Failed to get persisted root hash: {:?}", e))?
    .ok_or_else(|| format!("State snapshot not found at version {}", version))?
    .1; // The second element is the root hash

let expected_root_hash = target_output_with_proof
    .get_output_list_with_proof()
    .proof
    .transaction_infos
    .first()
    .expect("Target transaction info should exist!")
    .ensure_state_checkpoint_hash()
    .expect("Must be at state checkpoint.");

if persisted_root_hash != expected_root_hash {
    return Err(format!(
        "State root hash verification failed! Expected: {:?}, Got: {:?}",
        expected_root_hash, persisted_root_hash
    ));
}

// Only now mark the snapshot as complete
metadata_storage.update_last_persisted_state_value_index(
    target_ledger_info,
    last_committed_state_index,
    true,
)?;
```

This ensures that:
1. The complete state tree was successfully persisted to storage
2. The persisted root hash matches the expected hash from the trusted ledger info
3. No silent write failures or incomplete data can be marked as complete

## Proof of Concept
The vulnerability can be demonstrated by:

1. Setting up a state sync session where a malicious peer controls the data stream
2. The peer sends valid state chunks (each with correct Merkle proofs)
3. After sending only 50% of the state data, the peer sends a chunk with `is_last_chunk() = true`
4. The system calls `finalize_storage_and_send_commit()`
5. `JellyfishMerkleRestore::finish_impl()` freezes the incomplete tree
6. `snapshot_sync_completed = true` is set
7. Query the state database - it will be missing 50% of the expected state
8. Attempt to execute transactions requiring the missing state - they will fail or produce incorrect results
9. The state root hash computed from this incomplete state will not match the expected hash

The system never detects this because no code path reads back the persisted root hash and compares it to the expected value after marking the snapshot as complete.

**Note**: The actual implementation of this PoC would require modifying the data streaming service to inject malicious chunks or simulating database write failures, which is beyond the scope of this report but demonstrates a clear attack vector.

### Citations

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L412-440)
```rust
    async fn save_state_values(
        &mut self,
        notification_id: NotificationId,
        state_value_chunk_with_proof: StateValueChunkWithProof,
    ) -> Result<(), Error> {
        // Get the snapshot notifier and create the storage data chunk
        let state_snapshot_notifier = self.state_snapshot_notifier.as_mut().ok_or_else(|| {
            Error::UnexpectedError("The state snapshot receiver has not been initialized!".into())
        })?;
        let storage_data_chunk =
            StorageDataChunk::States(notification_id, state_value_chunk_with_proof);

        // Notify the snapshot receiver of the storage data chunk
        if let Err(error) = send_and_monitor_backpressure(
            state_snapshot_notifier,
            metrics::STORAGE_SYNCHRONIZER_STATE_SNAPSHOT_RECEIVER,
            storage_data_chunk,
        )
        .await
        {
            Err(Error::UnexpectedError(format!(
                "Failed to send storage data chunk to state snapshot listener: {:?}",
                error
            )))
        } else {
            increment_pending_data_chunks(self.pending_data_chunks.clone());
            Ok(())
        }
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L932-954)
```rust
                            if let Err(error) = finalize_storage_and_send_commit(
                                chunk_executor,
                                &mut commit_notification_sender,
                                metadata_storage,
                                state_snapshot_receiver,
                                storage,
                                &epoch_change_proofs,
                                target_output_with_proof,
                                version,
                                &target_ledger_info,
                                last_committed_state_index,
                            )
                            .await
                            {
                                send_storage_synchronizer_error(
                                    error_notification_sender.clone(),
                                    notification_id,
                                    error,
                                )
                                .await;
                            }
                            decrement_pending_data_chunks(pending_data_chunks.clone());
                            return; // There's nothing left to do!
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1141-1147)
```rust
    metadata_storage.update_last_persisted_state_value_index(
            target_ledger_info,
            last_committed_state_index,
            true,
        ).map_err(|error| {
        format!("All states have synced, but failed to update the metadata storage at version {:?}! Error: {:?}", version, error)
    })?;
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L390-391)
```rust
        // Verify what we have added so far is all correct.
        self.verify(proof)?;
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-241)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
            restore_utils::save_ledger_infos(
                self.ledger_db.metadata_db(),
                ledger_infos,
                Some(&mut ledger_db_batch.ledger_metadata_db_batches),
            )?;

            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::LedgerCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::OverallCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;

            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;

            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;

            restore_utils::update_latest_ledger_info(self.ledger_db.metadata_db(), ledger_infos)?;
            self.state_store.reset();

            Ok(())
        })
    }
```
