# Audit Report

## Title
JWK Consensus Version Conflict in Per-Key Mode Due to Concurrent Multi-Key Updates

## Summary
In per-key consensus mode, when multiple keys of the same issuer are updated simultaneously, concurrent `start_produce()` calls create QuorumCertifiedUpdates with identical version numbers. This causes version conflicts when applied sequentially to on-chain state, resulting in rejection of valid quorum-certified updates and temporary state inconsistencies.

## Finding Description

The JWK consensus system has two modes: per-issuer and per-key. In per-key mode, individual key-level updates for the same issuer share a common version counter maintained at the issuer level.

When a single observation detects multiple key changes for the same issuer, the `KeyLevelConsensusManager::process_new_observation` function processes each changed key independently: [1](#0-0) 

Each key triggers `maybe_start_consensus()` which reads the current on-chain `base_version` and initiates consensus: [2](#0-1) 

All concurrent consensus sessions read the same `effectively_onchain.version` value (e.g., version 5) and create `KeyLevelUpdate` objects with identical `base_version`. When converted to `ProviderJWKs` for transmission, all produce `version = base_version + 1`: [3](#0-2) 

These concurrent tasks complete in non-deterministic order and push QuorumCertifiedUpdates to the channel with different session keys `(issuer, kid)`: [4](#0-3) 

The channel uses round-robin scheduling to consume messages from different keys: [5](#0-4) 

When these validator transactions execute sequentially in blocks, the Move contract enforces strict version ordering: [6](#0-5) 

**Attack Flow:**
1. Issuer A has on-chain version 5 with keys `[k1:old, k2:old, k3:old]`
2. OIDC provider rotates all keys; observation detects `[k1:new, k2:new, k3:new]`
3. Three concurrent consensus sessions start with `base_version=5`, producing updates with `version=6`
4. All three QuorumCertifiedUpdates are placed in validator transaction pool with different topics
5. Transactions execute in FIFO order:
   - First update (e.g., k2): version check `5+1==6` passes ✓, on-chain becomes version 6
   - Second update (e.g., k1): version check `6+1==6` fails ✗, rejected with `EUNEXPECTED_VERSION`
   - Third update (e.g., k3): version check `6+1==6` fails ✗, rejected

The rejected updates are discarded: [7](#0-6) 

This leaves on-chain state with only one key updated while the other quorum-certified key updates are lost until the next observation cycle (~10 seconds later).

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty classification for "State inconsistencies requiring intervention."

**Security Impact:**
- **State Inconsistency**: On-chain JWK state diverges from what validators achieved quorum consensus on
- **Compromised Key Exposure Window**: If old keys were compromised and the provider is rotating to new keys, the old keys remain on-chain and usable during the retry window (~10 seconds)
- **Resource Waste**: Validator resources spent achieving quorum consensus on updates that are subsequently rejected
- **Authentication Failures**: Applications attempting to verify JWTs with the missing keys will fail

**Violates Invariant:**
Breaks **State Consistency** invariant: "State transitions must be atomic and verifiable" - the system achieves quorum agreement on multiple key updates but only applies a subset, leaving inconsistent state.

## Likelihood Explanation

**High Likelihood** - This occurs naturally during normal operations:

1. **Common Trigger**: OIDC providers frequently rotate multiple keys simultaneously for security reasons (e.g., annual key rotation, compromise response)
2. **Automatic Occurrence**: No attacker action required; happens whenever JWK observers detect multi-key changes in a single poll
3. **Observable Pattern**: The 10-second observation interval means this can occur multiple times per hour if providers actively manage their JWKs
4. **No Mitigation**: The current code has no prevention mechanism; the issue is deterministic given concurrent multi-key observations

While not directly exploitable by attackers, this represents a critical design flaw that compromises system reliability and creates security exposure windows.

## Recommendation

Implement issuer-level version coordination in per-key mode to prevent concurrent updates with conflicting versions:

**Option 1: Serialize key-level updates per issuer**
```rust
// In KeyLevelConsensusManager, track in-progress issuer updates
issuer_update_in_progress: HashMap<Issuer, bool>

fn maybe_start_consensus(&mut self, update: KeyLevelUpdate) -> Result<()> {
    // Check if ANY consensus is in progress for this issuer
    if self.issuer_update_in_progress.get(&update.issuer).copied().unwrap_or(false) {
        return Ok(()); // Defer until issuer update completes
    }
    
    // Mark issuer as having update in progress
    self.issuer_update_in_progress.insert(update.issuer.clone(), true);
    
    // ... existing start_produce logic ...
}
```

**Option 2: Batch multi-key updates at the issuer level**
When multiple keys change, create a single consensus session that includes all changed keys, similar to per-issuer mode but with key-level granularity.

**Option 3: Use sequence numbers instead of version**
Assign unique sequence numbers to each key-level update independent of the issuer version, avoiding conflicts entirely.

Additionally, add the missing consensus-in-progress check to `IssuerLevelConsensusManager::process_new_observation` to match the protection in `KeyLevelConsensusManager::maybe_start_consensus`: [8](#0-7) 

## Proof of Concept

```rust
// Simulation test demonstrating the version conflict
#[tokio::test]
async fn test_concurrent_multi_key_version_conflict() {
    // Setup: Issuer with version 5, three keys to rotate
    let issuer = b"https://accounts.google.com".to_vec();
    let on_chain_version = 5u64;
    
    // Simulate observation detecting 3 key changes
    let k1_update = KeyLevelUpdate {
        issuer: issuer.clone(),
        base_version: on_chain_version,  // All read same version!
        kid: b"key1".to_vec(),
        to_upsert: Some(new_jwk("key1_new")),
    };
    
    let k2_update = KeyLevelUpdate {
        issuer: issuer.clone(),
        base_version: on_chain_version,  // Same base_version
        kid: b"key2".to_vec(),
        to_upsert: Some(new_jwk("key2_new")),
    };
    
    let k3_update = KeyLevelUpdate {
        issuer: issuer.clone(),
        base_version: on_chain_version,  // Same base_version
        kid: b"key3".to_vec(),
        to_upsert: Some(new_jwk("key3_new")),
    };
    
    // All produce version=6
    assert_eq!(k1_update.try_as_issuer_level_repr()?.version, 6);
    assert_eq!(k2_update.try_as_issuer_level_repr()?.version, 6);
    assert_eq!(k3_update.try_as_issuer_level_repr()?.version, 6);
    
    // When applied sequentially:
    // 1st application: version 5->6 succeeds
    // 2nd application: expects version 7 but gets 6, FAILS
    // 3rd application: expects version 7 but gets 6, FAILS
    
    // Result: Only 1 of 3 quorum-certified updates applied
    // On-chain state: version 6 with only k2:new (whichever was first)
    // Missing: k1:new and k3:new remain stale until retry
}
```

## Notes

This vulnerability affects **per-key consensus mode only**. The per-issuer mode has a related but distinct issue where the missing `consensus_already_started` check in `IssuerLevelConsensusManager::process_new_observation` could allow stale observations to be applied due to race conditions in the channel with `KLAST` queue style.

The system does eventually recover through the automatic retry mechanism triggered by `ObservedJWKsUpdated` events and periodic JWK observer polling, but the ~10 second window of inconsistent state presents a real security risk, particularly when keys are being rotated in response to compromise.

### Citations

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L128-174)
```rust
        for kid in all_kids {
            let onchain = effectively_onchain.jwks.get(&kid);
            let observed = observed_jwks_by_kid.get(&kid);
            match (onchain, observed) {
                (Some(x), Some(y)) => {
                    if x == y {
                        // No change, drop any in-progress consensus.
                        self.states_by_key.remove(&(issuer.clone(), kid.clone()));
                    } else {
                        // Update detected.
                        let update = KeyLevelUpdate {
                            issuer: issuer.clone(),
                            base_version: effectively_onchain.version,
                            kid: kid.clone(),
                            to_upsert: Some(y.clone()),
                        };
                        self.maybe_start_consensus(update)
                            .context("process_new_observation failed at upsert consensus init")?;
                    }
                },
                (None, Some(y)) => {
                    // Insert detected.
                    let update = KeyLevelUpdate {
                        issuer: issuer.clone(),
                        base_version: effectively_onchain.version,
                        kid: kid.clone(),
                        to_upsert: Some(y.clone()),
                    };
                    self.maybe_start_consensus(update)
                        .context("process_new_observation failed at upsert consensus init")?;
                },
                (Some(_), None) => {
                    // Delete detected.
                    let update = KeyLevelUpdate {
                        issuer: issuer.clone(),
                        base_version: effectively_onchain.version,
                        kid: kid.clone(),
                        to_upsert: None,
                    };
                    self.maybe_start_consensus(update)
                        .context("process_new_observation failed at deletion consensus init")?;
                },
                (None, None) => {
                    unreachable!("`kid` in `union(A, B)` but `kid` not in `A` and not in `B`?")
                },
            }
        }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L179-231)
```rust
    fn maybe_start_consensus(&mut self, update: KeyLevelUpdate) -> Result<()> {
        let consensus_already_started = match self
            .states_by_key
            .get(&(update.issuer.clone(), update.kid.clone()))
            .cloned()
        {
            Some(ConsensusState::InProgress { my_proposal, .. })
            | Some(ConsensusState::Finished { my_proposal, .. }) => {
                my_proposal.observed.to_upsert == update.to_upsert
            },
            _ => false,
        };

        if consensus_already_started {
            return Ok(());
        }

        let issuer_level_repr = update
            .try_as_issuer_level_repr()
            .context("initiate_key_level_consensus failed at repr conversion")?;
        let signature = self
            .consensus_key
            .sign(&issuer_level_repr)
            .context("crypto material error occurred during signing")?;

        let update_translated = update
            .try_as_issuer_level_repr()
            .context("maybe_start_consensus failed at update translation")?;
        let abort_handle = self
            .update_certifier
            .start_produce(
                self.epoch_state.clone(),
                update_translated,
                self.qc_update_tx.clone(),
            )
            .context("maybe_start_consensus failed at update_certifier.start_produce")?;

        self.states_by_key.insert(
            (update.issuer.clone(), update.kid.clone()),
            ConsensusState::InProgress {
                my_proposal: ObservedKeyLevelUpdate {
                    author: self.my_addr,
                    observed: update,
                    signature,
                },
                abort_handle_wrapper: QuorumCertProcessGuard {
                    handle: abort_handle,
                },
            },
        );

        Ok(())
    }
```

**File:** types/src/jwks/mod.rs (L342-357)
```rust
    pub fn try_as_issuer_level_repr(&self) -> anyhow::Result<ProviderJWKs> {
        let jwk_repr = self.to_upsert.clone().unwrap_or_else(|| {
            JWK::Unsupported(UnsupportedJWK {
                id: self.kid.clone(),
                payload: DELETE_COMMAND_INDICATOR.as_bytes().to_vec(),
            })
        });
        let version = self
            .base_version
            .checked_add(1)
            .context("KeyLevelUpdate::as_issuer_level_repr failed on version")?;
        Ok(ProviderJWKs {
            issuer: self.issuer.clone(),
            version,
            jwks: vec![JWKMoveStruct::from(jwk_repr)],
        })
```

**File:** crates/aptos-jwk-consensus/src/update_certifier.rs (L67-79)
```rust
        let task = async move {
            let qc_update = rb.broadcast(req, agg_state).await.expect("cannot fail");
            ConsensusMode::log_certify_done(epoch, &qc_update);
            let session_key = ConsensusMode::session_key_from_qc(&qc_update);
            match session_key {
                Ok(key) => {
                    let _ = qc_update_tx.push(key, qc_update);
                },
                Err(e) => {
                    error!("JWK update QCed but could not identify the session key: {e}");
                },
            }
        };
```

**File:** crates/channel/src/message_queues.rs (L154-201)
```rust
    /// pop a message from the appropriate queue in per_key_queue
    /// remove the key from the round_robin_queue if it has no more messages
    pub(crate) fn pop(&mut self) -> Option<T> {
        let key = match self.round_robin_queue.pop_front() {
            Some(v) => v,
            _ => {
                return None;
            },
        };

        let (message, is_q_empty) = self.pop_from_key_queue(&key);
        if !is_q_empty {
            self.round_robin_queue.push_back(key);
        }

        if message.is_some() {
            if let Some(c) = self.counters.as_ref() {
                c.with_label_values(&["dequeued"]).inc();
            }

            // Remove empty per-key-queues every `POPS_PER_GC` successful dequeue
            // operations.
            //
            // aptos-channel never removes keys from its PerKeyQueue (without
            // this logic). This works fine for the validator network, where we
            // have a bounded set of peers that almost never changes; however,
            // this does not work for servicing public clients, where we can have
            // large and frequent connection churn.
            //
            // Periodically removing these empty queues prevents us from causing
            // an effective memory leak when we have lots of transient peers in
            // e.g. the public-facing vfn use-case.
            //
            // This GC strategy could probably be more sophisticated, though it
            // seems to work well in some basic stress tests / micro benches.
            //
            // See: common/channel/src/bin/many_keys_stress_test.rs
            //
            // For more context, see: https://github.com/aptos-labs/aptos-core/issues/5543
            self.num_popped_since_gc += 1;
            if self.num_popped_since_gc >= POPS_PER_GC {
                self.num_popped_since_gc = 0;
                self.remove_empty_queues();
            }
        }

        message
    }
```

**File:** aptos-move/framework/aptos-framework/sources/jwks.move (L478-493)
```text
                assert!(cur_issuer_jwks.version + 1 == proposed_provider_jwks.version, error::invalid_argument(EUNEXPECTED_VERSION));
                vector::for_each(proposed_provider_jwks.jwks, |jwk|{
                    let variant_type_name = *string::bytes(copyable_any::type_name(&jwk.variant));
                    let is_delete = if (variant_type_name == b"0x1::jwks::UnsupportedJWK") {
                        let repr = copyable_any::unpack<UnsupportedJWK>(jwk.variant);
                        &repr.payload == &DELETE_COMMAND_INDICATOR
                    } else {
                        false
                    };
                    if (is_delete) {
                        remove_jwk(&mut cur_issuer_jwks, get_jwk_id(&jwk));
                    } else {
                        upsert_jwk(&mut cur_issuer_jwks, jwk);
                    }
                });
                cur_issuer_jwks.version = cur_issuer_jwks.version + 1;
```

**File:** aptos-move/aptos-vm/src/validator_txns/jwk.rs (L78-88)
```rust
            Err(Expected(failure)) => {
                // Pretend we are inside Move, and expected failures are like Move aborts.
                debug!("Processing dkg transaction expected failure: {:?}", failure);
                Ok((
                    VMStatus::MoveAbort {
                        location: AbortLocation::Script,
                        code: failure as u64,
                        message: None,
                    },
                    VMOutput::empty_with_status(TransactionStatus::Discard(StatusCode::ABORTED)),
                ))
```
