# Audit Report

## Title
Data Withholding Attack: Malicious Peers Can Cause Prolonged Synchronization Stalls Through False Advertisement

## Summary
Malicious peers can advertise complete data ranges for transactions and states without actually serving the data, causing severe synchronization delays. The lenient peer scoring system allows attackers to cause up to 41 minutes of cumulative delays before being ignored, breaking liveness guarantees and significantly impacting node synchronization performance.

## Finding Description

The state synchronization system trusts peer-advertised data ranges without verification and implements an overly lenient reputation system that fails to quickly isolate misbehaving peers.

**Attack Flow:**

1. **False Advertisement**: A malicious peer advertises complete data ranges via `StorageServerSummary`, claiming to have transactions and states for specific version ranges. [1](#0-0) 

2. **Trusted Selection**: The client's `can_service_request()` method only validates that the advertised range is a superset of the desired range—it never verifies actual data availability. [2](#0-1) [3](#0-2) 

3. **Data Withholding**: The malicious peer is selected but refuses to respond to data requests, causing RPC timeouts.

4. **Lenient Penalty**: Each timeout triggers `ErrorType::NotUseful` penalty, which only multiplies the peer's score by 0.95 (5% reduction). [4](#0-3) [5](#0-4) 

5. **Delayed Isolation**: Starting from score 50, a peer needs approximately 13-14 consecutive failures to drop below the ignore threshold of 25. [6](#0-5) [7](#0-6) 

6. **Exponential Delay Accumulation**: Each failed request undergoes exponential backoff with up to 5 retries, totaling up to 190 seconds per request (10s + 20s + 40s + 60s + 60s). [8](#0-7) [9](#0-8) [10](#0-9) 

**Total Attack Impact**: 190 seconds × 13 failures = 2,470 seconds ≈ **41 minutes of cumulative synchronization delays** before the peer is ignored.

**Aggravating Factors:**

- **Multi-fetch bypass**: If multi-fetch is disabled or there are few serviceable peers, the attack is more effective. [11](#0-10) 

- **Score recovery**: An ignored peer can recover by providing valid responses to other requests, allowing it to re-enter the serviceable peer pool. [12](#0-11) 

- **No hard disconnect**: Misbehaving peers are temporarily ignored but never forcibly disconnected, allowing persistent exploitation.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Node Slowdowns**: Nodes attempting to synchronize can experience 41+ minutes of delays, directly impacting validator performance and network participation.

2. **Significant Protocol Violations**: The attack breaks the liveness guarantee that nodes can efficiently synchronize with the network, violating fundamental state sync assumptions.

3. **Network-Wide Impact**: If multiple malicious peers coordinate this attack across the network, widespread synchronization failures can occur, particularly affecting:
   - New nodes joining the network
   - Nodes recovering from downtime
   - Nodes with limited peer connectivity

4. **Amplification Potential**: In scenarios with limited honest peers or when multi-fetch is disabled, a single malicious peer can monopolize requests and cause severe degradation.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This attack is highly likely to occur because:

1. **Low Barrier to Entry**: Any peer can join the network and advertise false data ranges—no special privileges required.

2. **Simple Execution**: The attack requires only:
   - Advertising complete data ranges in storage summary responses
   - Refusing to respond to actual data requests (silent timeout)

3. **Difficult Detection**: Timeouts can appear as network issues, making malicious behavior hard to distinguish from legitimate connectivity problems.

4. **Rational Attack Motivation**:
   - Competitors could use this to degrade rival validators
   - Malicious actors could target specific nodes during critical periods
   - Even accidental misconfigurations (buggy storage implementations) could trigger this behavior

5. **Persistent Vulnerability**: The score recovery mechanism means a clever attacker can sustain the attack indefinitely by alternating between good and bad behavior.

## Recommendation

Implement a multi-layered defense strategy:

### 1. Aggressive Peer Scoring
```rust
// In peer_states.rs
const IGNORE_PEER_THRESHOLD: f64 = 40.0; // Raised from 25.0
const NOT_USEFUL_MULTIPLIER: f64 = 0.85; // More aggressive, down from 0.95
const TIMEOUT_CONSECUTIVE_FAILURE_LIMIT: u32 = 3; // New constant

// Add consecutive failure tracking to PeerState
pub struct PeerState {
    consecutive_timeouts: u32,
    // ... existing fields
}

impl PeerState {
    fn update_score_error(&mut self, error: ErrorType) {
        match error {
            ErrorType::NotUseful => {
                self.consecutive_timeouts += 1;
                
                // Immediate ban after 3 consecutive timeouts
                if self.consecutive_timeouts >= TIMEOUT_CONSECUTIVE_FAILURE_LIMIT {
                    self.score = MIN_SCORE;
                } else {
                    self.score = f64::max(self.score * NOT_USEFUL_MULTIPLIER, MIN_SCORE);
                }
            },
            ErrorType::Malicious => {
                self.score = MIN_SCORE; // Immediate ban for malicious behavior
            }
        }
    }
    
    fn update_score_success(&mut self) {
        self.consecutive_timeouts = 0; // Reset on success
        self.score = f64::min(self.score + SUCCESSFUL_RESPONSE_DELTA, MAX_SCORE);
    }
}
```

### 2. Advertisement Verification
Add probabilistic verification of advertised data ranges:
```rust
// In client.rs - periodically verify peer advertisements
async fn verify_peer_advertisement(
    &self,
    peer: PeerNetworkId,
    advertised_range: &CompleteDataRange<Version>,
) -> Result<bool> {
    // Randomly sample a version within the advertised range
    let random_version = sample_range(advertised_range);
    
    // Request a small chunk to verify availability
    let verification_request = StorageServiceRequest::new(
        DataRequest::GetTransactionsWithProof(TransactionsWithProofRequest {
            start_version: random_version,
            end_version: random_version,
            proof_version: random_version,
            include_events: false,
        }),
        false,
    );
    
    // Short timeout for verification
    match self.send_request_to_peer(peer, verification_request, 2000).await {
        Ok(_) => Ok(true),
        Err(_) => Ok(false), // Failed verification
    }
}
```

### 3. Hard Disconnection for Persistent Failures
```rust
// Add to PeerStates
pub fn should_disconnect_peer(&self, peer: &PeerNetworkId) -> bool {
    if let Some(peer_state) = self.peer_to_state.get(peer) {
        // Disconnect if score has been at minimum for extended period
        peer_state.score <= MIN_SCORE && peer_state.consecutive_timeouts >= 5
    } else {
        false
    }
}
```

### 4. Configuration Hardening
Ensure `ignore_low_score_peers` is always enabled in production and reduce default timeout values for faster failure detection:
```rust
// In state_sync_config.rs defaults
response_timeout_ms: 5_000, // Reduced from 10_000
max_response_timeout_ms: 30_000, // Reduced from 60_000
```

## Proof of Concept

```rust
// Simulated attack demonstrating synchronization stall
// This would be integrated into state-sync test framework

#[tokio::test]
async fn test_data_withholding_attack() {
    // Setup: Create a malicious peer that advertises but doesn't serve
    let malicious_peer = create_malicious_peer_with_false_advertisement();
    
    // Create client with single peer (worst case scenario)
    let (client, _poller) = create_client_with_peers(vec![malicious_peer]);
    
    // Track total delay
    let start_time = Instant::now();
    let mut total_failures = 0;
    
    // Attempt synchronization requests
    while total_failures < 13 {
        let request = StorageServiceRequest::new(
            DataRequest::GetTransactionsWithProof(TransactionsWithProofRequest {
                start_version: 100,
                end_version: 200,
                proof_version: 200,
                include_events: false,
            }),
            false,
        );
        
        // This should timeout and take ~190 seconds with retries
        let result = client.send_request_and_decode::<_, Error>(
            request,
            10_000, // 10 second base timeout
        ).await;
        
        assert!(result.is_err());
        total_failures += 1;
    }
    
    let elapsed = start_time.elapsed();
    
    // Verify the attack caused significant delay
    assert!(elapsed.as_secs() > 2000); // Over 2000 seconds (33+ minutes)
    
    // Verify peer is now ignored
    let peer_state = client.get_peer_states()
        .get_peer_to_states()
        .get(&malicious_peer)
        .unwrap();
    assert!(peer_state.is_ignored());
}

fn create_malicious_peer_with_false_advertisement() -> PeerNetworkId {
    // Create peer that advertises complete ranges
    let peer = create_test_peer();
    
    // Advertise false data availability
    let summary = StorageServerSummary {
        data_summary: DataSummary {
            transactions: Some(CompleteDataRange::new(0, 1_000_000).unwrap()),
            states: Some(CompleteDataRange::new(0, 1_000_000).unwrap()),
            // ... other fields
        },
        // ... protocol metadata
    };
    
    // Configure peer to timeout on all requests
    configure_peer_to_timeout_on_requests(peer);
    
    peer
}
```

**Expected Results**: The test demonstrates that a single malicious peer can cause over 33 minutes of synchronization delays before being ignored, validating the vulnerability's impact on node liveness and synchronization efficiency.

---

**Notes**

This vulnerability is particularly dangerous in networks with:
- Low peer diversity (few available peers)
- Nodes behind restrictive firewalls (limited peer connectivity)  
- Bootstrap scenarios (new nodes with no peer history)
- Disabled multi-fetch configurations

The combination of trusted advertisement, lenient scoring, and exponential backoff creates a perfect storm for denial-of-service through data withholding.

### Citations

**File:** state-sync/storage-service/types/src/responses.rs (L665-686)
```rust
/// A summary of the data actually held by the storage service instance.
#[derive(Clone, Debug, Default, Deserialize, Eq, PartialEq, Serialize)]
pub struct DataSummary {
    /// The ledger info corresponding to the highest synced version in storage.
    /// This indicates the highest version and epoch that storage can prove.
    pub synced_ledger_info: Option<LedgerInfoWithSignatures>,
    /// The range of epoch ending ledger infos in storage, e.g., if the range
    /// is [(X,Y)], it means all epoch ending ledger infos for epochs X->Y
    /// (inclusive) are held.
    pub epoch_ending_ledger_infos: Option<CompleteDataRange<Epoch>>,
    /// The range of states held in storage, e.g., if the range is
    /// [(X,Y)], it means all states are held for every version X->Y
    /// (inclusive).
    pub states: Option<CompleteDataRange<Version>>,
    /// The range of transactions held in storage, e.g., if the range is
    /// [(X,Y)], it means all transactions for versions X->Y (inclusive) are held.
    pub transactions: Option<CompleteDataRange<Version>>,
    /// The range of transaction outputs held in storage, e.g., if the range
    /// is [(X,Y)], it means all transaction outputs for versions X->Y
    /// (inclusive) are held.
    pub transaction_outputs: Option<CompleteDataRange<Version>>,
}
```

**File:** state-sync/storage-service/types/src/responses.rs (L819-830)
```rust
    fn can_service_transaction_outputs(&self, desired_range: &CompleteDataRange<u64>) -> bool {
        self.transaction_outputs
            .map(|range| range.superset_of(desired_range))
            .unwrap_or(false)
    }

    /// Returns true iff the peer can service the transactions in the given range
    fn can_service_transactions(&self, desired_range: &CompleteDataRange<u64>) -> bool {
        self.transactions
            .map(|range| range.superset_of(desired_range))
            .unwrap_or(false)
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L1005-1008)
```rust
    /// Returns true iff this range is a superset of the other data range.
    pub fn superset_of(&self, other: &Self) -> bool {
        self.lowest <= other.lowest && other.highest <= self.highest
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L32-43)
```rust
/// Scores for peer rankings based on preferences and behavior.
const MAX_SCORE: f64 = 100.0;
const MIN_SCORE: f64 = 0.0;
const STARTING_SCORE: f64 = 50.0;
/// Add this score on a successful response.
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
/// Not necessarily a malicious response, but not super useful.
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
/// Likely to be a malicious response.
const MALICIOUS_MULTIPLIER: f64 = 0.8;
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L152-160)
```rust
    fn is_ignored(&self) -> bool {
        // Only ignore peers if the config allows it
        if !self.data_client_config.ignore_low_score_peers {
            return false;
        }

        // Otherwise, ignore peers with a low score
        self.score <= IGNORE_PEER_THRESHOLD
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L162-165)
```rust
    /// Updates the score of the peer according to a successful operation
    fn update_score_success(&mut self) {
        self.score = f64::min(self.score + SUCCESSFUL_RESPONSE_DELTA, MAX_SCORE);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L167-174)
```rust
    /// Updates the score of the peer according to an error
    fn update_score_error(&mut self, error: ErrorType) {
        let multiplier = match error {
            ErrorType::NotUseful => NOT_USEFUL_MULTIPLIER,
            ErrorType::Malicious => MALICIOUS_MULTIPLIER,
        };
        self.score = f64::max(self.score * multiplier, MIN_SCORE);
    }
```

**File:** config/src/config/state_sync_config.rs (L277-277)
```rust
            max_request_retry: 5,
```

**File:** config/src/config/state_sync_config.rs (L473-481)
```rust
            max_response_timeout_ms: 60_000, // 60 seconds
            max_state_chunk_size: MAX_STATE_CHUNK_SIZE,
            max_subscription_lag_secs: 20, // 20 seconds
            max_transaction_chunk_size: MAX_TRANSACTION_CHUNK_SIZE,
            max_transaction_output_chunk_size: MAX_TRANSACTION_OUTPUT_CHUNK_SIZE,
            optimistic_fetch_timeout_ms: 5000,         // 5 seconds
            progress_check_max_stall_time_secs: 86400, // 24 hours (long enough to debug any issues at runtime)
            response_timeout_ms: 10_000,               // 10 seconds
            subscription_response_timeout_ms: 15_000, // 15 seconds (longer than a regular timeout because of prefetching)
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L351-359)
```rust
            let response_timeout_ms = self.data_client_config.response_timeout_ms;
            let max_response_timeout_ms = self.data_client_config.max_response_timeout_ms;

            // Exponentially increase the timeout based on the number of
            // previous failures (but bounded by the max timeout).
            let request_timeout_ms = min(
                max_response_timeout_ms,
                response_timeout_ms * (u32::pow(2, self.request_failure_count as u32) as u64),
            );
```

**File:** state-sync/aptos-data-client/src/client.rs (L290-320)
```rust
        let multi_fetch_config = self.data_client_config.data_multi_fetch_config;
        let num_peers_for_request = if multi_fetch_config.enable_multi_fetch {
            // Calculate the total number of priority serviceable peers
            let mut num_serviceable_peers = 0;
            for (index, peers) in serviceable_peers_by_priorities.iter().enumerate() {
                // Only include the lowest priority peers if no other peers are
                // available (the lowest priority peers are generally unreliable).
                if (num_serviceable_peers == 0)
                    || (index < serviceable_peers_by_priorities.len() - 1)
                {
                    num_serviceable_peers += peers.len();
                }
            }

            // Calculate the number of peers to select for the request
            let peer_ratio_for_request =
                num_serviceable_peers / multi_fetch_config.multi_fetch_peer_bucket_size;
            let mut num_peers_for_request = multi_fetch_config.min_peers_for_multi_fetch
                + (peer_ratio_for_request * multi_fetch_config.additional_requests_per_peer_bucket);

            // Bound the number of peers by the number of serviceable peers
            num_peers_for_request = min(num_peers_for_request, num_serviceable_peers);

            // Ensure the number of peers is no larger than the maximum
            min(
                num_peers_for_request,
                multi_fetch_config.max_peers_for_multi_fetch,
            )
        } else {
            1 // Multi-fetch is disabled (only select a single peer)
        };
```
