# Audit Report

## Title
Unbounded Memory Exhaustion in BatchProofQueue via txn_summary_num_occurrences HashMap

## Summary
The `txn_summary_num_occurrences` HashMap in `BatchProofQueue` lacks size bounds, allowing a Byzantine validator to cause memory exhaustion on victim nodes by flooding them with batches containing unique transactions paired with corresponding proofs.

## Finding Description

The `BatchProofQueue` struct tracks unique transaction summaries in the `txn_summary_num_occurrences` HashMap without any size limit. [1](#0-0) 

When a proof is inserted and transaction summaries already exist, the HashMap is incremented: [2](#0-1) 

Similarly, when batches with transaction summaries are inserted and proofs already exist, the HashMap grows: [3](#0-2) 

**Critical Vulnerability Path:**

1. A Byzantine validator generates batches with unique transactions
2. The validator sends these batches to victim nodes via `BatchCoordinatorCommand::NewBatches`
3. `BatchCoordinator::handle_batches_msg` validates per-message limits (max 2,000 txns) but accepts the batches: [4](#0-3) 

4. In `persist_and_send_digests`, the batches are extracted BEFORE persistence: [5](#0-4) 

5. Even if `BatchStore.persist` fails due to quota exhaustion, the `ReceiveBatches` command is ALWAYS sent to ProofManager: [6](#0-5) 

6. ProofManager unconditionally inserts batches into BatchProofQueue: [7](#0-6) 

7. The Byzantine validator sends corresponding proofs that are also unconditionally accepted: [8](#0-7) 

8. The HashMap grows unboundedly as unique transactions accumulate

**Why Existing Defenses Fail:**

- **BatchStore Quota:** Only applies to persisted storage, not the in-memory BatchProofQueue HashMap
- **Back Pressure:** Only throttles local batch generation, not remote batch/proof reception
- **Per-Message Limits:** Allow 2,000 transactions per message but don't bound total accumulation
- **Expiration (500ms for remote batches):** If the attacker sends batches at a rate exceeding the expiration cleanup rate, memory accumulates unboundedly

The HashMap has NO size checks in insert operations and cleanup only occurs via expiration or commitment, which may lag behind insertion rate.

## Impact Explanation

This vulnerability allows a single Byzantine validator to cause **memory exhaustion** on all other validator nodes, leading to:

- Node crashes due to out-of-memory conditions
- Severe performance degradation as memory pressure triggers swapping
- Potential consensus liveness failures if multiple validators crash simultaneously

This constitutes **High Severity** per Aptos bug bounty criteria: "Validator node slowdowns" and "API crashes". [9](#0-8) 

With default configuration allowing 100 transactions per batch and no bounds on the HashMap, an attacker could accumulate millions of unique transaction entries by sending continuous batches with unique garbage transactions.

## Likelihood Explanation

**High Likelihood** for the following reasons:

1. **Low Attack Complexity:** A Byzantine validator simply needs to generate batches with unique transaction hashes (can be random/invalid transactions) and sign proofs
2. **No Rate Limiting:** The code unconditionally processes `ReceiveProofs` and `ReceiveBatches` commands without checking HashMap size
3. **Bypasses Storage Quotas:** The vulnerability exists even when BatchStore quotas are enforced because batches are sent to ProofManager before persistence status is checked
4. **BFT Assumption:** Aptos is designed to tolerate up to 1/3 Byzantine validators, but a single malicious validator can exploit this vulnerability to impact all others

## Recommendation

Implement strict bounds on `txn_summary_num_occurrences` HashMap size:

```rust
pub struct BatchProofQueue {
    // ... existing fields ...
    txn_summary_num_occurrences: HashMap<TxnSummaryWithExpiration, u64>,
    max_unique_txns: usize, // Add limit
}

impl BatchProofQueue {
    pub fn insert_batches(&mut self, batches_with_txn_summaries: Vec<...>) {
        // Before insertion, check total size
        let potential_new_txns = batches_with_txn_summaries
            .iter()
            .map(|(_, summaries)| summaries.len())
            .sum::<usize>();
        
        if self.txn_summary_num_occurrences.len() + potential_new_txns > self.max_unique_txns {
            counters::EXCEEDED_TXN_SUMMARY_QUOTA_COUNT.inc();
            return; // Reject insertion
        }
        
        // ... existing insertion logic ...
    }
}
```

Set `max_unique_txns` based on `back_pressure_total_txn_limit` or a new configuration parameter, ensuring memory usage remains bounded even under Byzantine validator attacks.

Additionally, fix the decoupling between BatchStore persistence and ProofManager notification: [10](#0-9) 

Only send `ReceiveBatches` to ProofManager for successfully persisted batches, not all batches regardless of persistence outcome.

## Proof of Concept

```rust
#[tokio::test]
async fn test_unbounded_txn_summary_growth() {
    // Setup BatchProofQueue
    let mut queue = BatchProofQueue::new(
        PeerId::random(),
        Arc::new(mock_batch_store()),
        60_000_000, // batch_expiry_gap
    );
    
    // Simulate Byzantine validator sending batches with unique transactions
    let byzantine_validator = PeerId::random();
    let mut batch_id = BatchId::new(0);
    
    // Send 1000 batches, each with 100 unique transactions
    for _ in 0..1000 {
        batch_id.increment();
        let mut txn_summaries = vec![];
        
        // Generate 100 unique transaction summaries
        for j in 0..100 {
            txn_summaries.push(TxnSummaryWithExpiration::new(
                AccountAddress::random(),
                ReplayProtector::V1 { sequence_number: j },
                current_time() + 60_000_000, // Valid for 60s
                HashValue::random(),
            ));
        }
        
        let batch_info = create_batch_info(byzantine_validator, batch_id, 100);
        
        // Insert batch summaries
        queue.insert_batches(vec![(batch_info.clone(), txn_summaries.clone())]);
        
        // Insert proof for the batch
        let proof = create_proof_of_store(batch_info);
        queue.insert_proof(proof);
    }
    
    // Verify unbounded growth
    // After 1000 batches × 100 txns = 100,000 unique transactions
    assert!(queue.txn_summary_num_occurrences.len() >= 100_000);
    
    // Memory usage would be approximately:
    // 100,000 entries × (32 bytes AccountAddress + 8 bytes seq + 8 bytes expiry + 32 bytes hash + 8 bytes count)
    // = ~8.8 MB per 100K transactions
    // An attacker sending continuously could exhaust GBs of RAM
}
```

This demonstrates how a Byzantine validator can cause unbounded memory growth in the `txn_summary_num_occurrences` HashMap, violating the **Resource Limits** invariant that "all operations must respect gas, storage, and computational limits."

### Citations

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L64-64)
```rust
    txn_summary_num_occurrences: HashMap<TxnSummaryWithExpiration, u64>,
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L216-227)
```rust
        if let Some(txn_summaries) = self
            .items
            .get(&batch_key)
            .and_then(|item| item.txn_summaries.as_ref())
        {
            for txn_summary in txn_summaries {
                *self
                    .txn_summary_num_occurrences
                    .entry(*txn_summary)
                    .or_insert(0) += 1;
            }
        }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L287-297)
```rust
            if self
                .items
                .get(&batch_key)
                .is_some_and(|item| item.proof.is_some())
            {
                for txn_summary in &txn_summaries {
                    *self
                        .txn_summary_num_occurrences
                        .entry(*txn_summary)
                        .or_insert(0) += 1;
                }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L78-135)
```rust
    fn persist_and_send_digests(
        &self,
        persist_requests: Vec<PersistedValue<BatchInfoExt>>,
        approx_created_ts_usecs: u64,
    ) {
        if persist_requests.is_empty() {
            return;
        }

        let batch_store = self.batch_store.clone();
        let network_sender = self.network_sender.clone();
        let sender_to_proof_manager = self.sender_to_proof_manager.clone();
        tokio::spawn(async move {
            let peer_id = persist_requests[0].author();
            let batches = persist_requests
                .iter()
                .map(|persisted_value| {
                    (
                        persisted_value.batch_info().clone(),
                        persisted_value.summary(),
                    )
                })
                .collect();

            if persist_requests[0].batch_info().is_v2() {
                let signed_batch_infos = batch_store.persist(persist_requests);
                if !signed_batch_infos.is_empty() {
                    if approx_created_ts_usecs > 0 {
                        observe_batch(approx_created_ts_usecs, peer_id, BatchStage::SIGNED);
                    }
                    network_sender
                        .send_signed_batch_info_msg_v2(signed_batch_infos, vec![peer_id])
                        .await;
                }
            } else {
                let signed_batch_infos = batch_store.persist(persist_requests);
                if !signed_batch_infos.is_empty() {
                    assert!(!signed_batch_infos
                        .first()
                        .expect("must not be empty")
                        .is_v2());
                    if approx_created_ts_usecs > 0 {
                        observe_batch(approx_created_ts_usecs, peer_id, BatchStage::SIGNED);
                    }
                    let signed_batch_infos = signed_batch_infos
                        .into_iter()
                        .map(|sbi| sbi.try_into().expect("Batch must be V1 batch"))
                        .collect();
                    network_sender
                        .send_signed_batch_info_msg(signed_batch_infos, vec![peer_id])
                        .await;
                }
            }
            let _ = sender_to_proof_manager
                .send(ProofManagerCommand::ReceiveBatches(batches))
                .await;
        });
    }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L137-171)
```rust
    fn ensure_max_limits(&self, batches: &[Batch<BatchInfoExt>]) -> anyhow::Result<()> {
        let mut total_txns = 0;
        let mut total_bytes = 0;
        for batch in batches.iter() {
            ensure!(
                batch.num_txns() <= self.max_batch_txns,
                "Exceeds batch txn limit {} > {}",
                batch.num_txns(),
                self.max_batch_txns,
            );
            ensure!(
                batch.num_bytes() <= self.max_batch_bytes,
                "Exceeds batch bytes limit {} > {}",
                batch.num_bytes(),
                self.max_batch_bytes,
            );

            total_txns += batch.num_txns();
            total_bytes += batch.num_bytes();
        }
        ensure!(
            total_txns <= self.max_total_txns,
            "Exceeds total txn limit {} > {}",
            total_txns,
            self.max_total_txns,
        );
        ensure!(
            total_bytes <= self.max_total_bytes,
            "Exceeds total bytes limit: {} > {}",
            total_bytes,
            self.max_total_bytes,
        );

        Ok(())
    }
```

**File:** consensus/src/quorum_store/proof_manager.rs (L65-69)
```rust
    pub(crate) fn receive_proofs(&mut self, proofs: Vec<ProofOfStore<BatchInfoExt>>) {
        for proof in proofs.into_iter() {
            self.batch_proof_queue.insert_proof(proof);
        }
        self.update_remaining_txns_and_proofs();
```

**File:** consensus/src/quorum_store/proof_manager.rs (L80-85)
```rust
    pub(crate) fn receive_batches(
        &mut self,
        batch_summaries: Vec<(BatchInfoExt, Vec<TxnSummaryWithExpiration>)>,
    ) {
        self.batch_proof_queue.insert_batches(batch_summaries);
        self.update_remaining_txns_and_proofs();
```

**File:** config/src/config/quorum_store_config.rs (L120-123)
```rust
            receiver_max_batch_txns: 100,
            receiver_max_batch_bytes: 1024 * 1024 + BATCH_PADDING_BYTES,
            receiver_max_num_batches: 20,
            receiver_max_total_txns: 2000,
```
