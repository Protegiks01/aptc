# Audit Report

## Title
Consensus Liveness Degradation via Unsynchronized Block Transaction Filter Configurations

## Summary
The `BlockTransactionFilterConfig` used for consensus proposal validation is a local node configuration that is not synchronized across validators. This design flaw allows validators with different filter configurations to disagree on proposal validity, causing voting failures and consensus delays.

## Finding Description

The Aptos consensus system includes a transaction filtering mechanism controlled by `BlockTransactionFilterConfig`, which allows validators to reject block proposals containing specific transactions. This configuration is stored locally in each validator's `NodeConfig` and is not part of the on-chain consensus configuration. [1](#0-0) 

During proposal validation, each validator independently applies its local filter configuration: [2](#0-1) 

The filter is initialized from the node's local configuration: [3](#0-2) 

Critically, the `OnChainConsensusConfig` does NOT include transaction filter settings, meaning validators have no mechanism to coordinate their filter configurations: [4](#0-3) 

When a proposer creates a DirectMempool payload, no filtering is applied during block creation—transactions are pulled directly from mempool: [5](#0-4) 

**Attack Scenario:**
1. Validator A has `filter_enabled=false` (or a permissive filter)
2. Validators B, C, D have `filter_enabled=true` with strict transaction filtering rules
3. When Validator A proposes a block containing transactions that match B/C/D's filter rules, those validators reject the proposal
4. The proposal fails to achieve a quorum, causing round timeout and consensus delay
5. This repeats whenever Validator A is the proposer

## Impact Explanation

This issue qualifies as **High Severity** under the Aptos bug bounty program because it enables:

- **Validator node slowdowns**: Repeated round timeouts degrade consensus performance
- **Significant protocol violations**: The consensus protocol assumes validators agree on what constitutes a valid proposal, but filter mismatches violate this assumption

While this does not break consensus safety (no chain splits or double-spending), it directly impacts **liveness**—a critical consensus property. Validators with mismatched configurations cannot reach consensus efficiently, causing:
- Increased block proposal latency
- Reduced transaction throughput
- Potential network instability if multiple validators have incompatible filters

## Likelihood Explanation

**Likelihood: Medium-High**

This issue can occur through:
1. **Unintentional misconfiguration**: Validators independently configuring filters without coordination
2. **Operational divergence**: Different operators applying different security policies
3. **Configuration drift**: Validators updating filters at different times
4. **Malicious exploitation**: A validator deliberately configuring permissive filters to propose blocks rejected by others

The issue is particularly likely because:
- No validation ensures filter consistency across validators
- No documentation specifies that filters must be synchronized
- The configuration system allows arbitrary local settings
- There's no on-chain governance mechanism to coordinate filter changes

## Recommendation

**Solution 1: Move filter configuration to on-chain consensus config**

Extend `OnChainConsensusConfig` to include transaction filter rules, ensuring all validators use identical filters. This requires:

```rust
// In types/src/on_chain_config/consensus_config.rs
pub enum OnChainConsensusConfig {
    V6 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
        window_size: Option<u64>,
        rand_check_enabled: bool,
        // Add synchronized filter configuration
        block_txn_filter: Option<Vec<u8>>, // Serialized BlockTransactionFilter
    },
}
```

**Solution 2: Remove filtering from proposal validation**

Apply transaction filtering only during block execution (in `BlockPreparer`), not during proposal validation. This ensures validators can disagree on which transactions to execute while still voting on proposals:

```rust
// In consensus/src/round_manager.rs, remove lines 1204-1214
// Transaction filtering happens in BlockPreparer::prepare_block instead
```

**Solution 3: Add configuration validation**

Implement a check during epoch changes that validates all validators have compatible filter configurations, rejecting epoch transitions if mismatches are detected.

**Recommended approach**: Solution 2 is preferred because it separates consensus (agreement on proposal order) from execution (transaction filtering), aligning with the decoupled execution model.

## Proof of Concept

```rust
// Test demonstrating consensus delay from filter mismatch
// Add to consensus/src/round_manager_tests/txn_filter_proposal_test.rs

#[test]
fn test_consensus_delay_from_filter_mismatch() {
    use crate::{
        network_tests::NetworkPlayground,
        round_manager::round_manager_tests::NodeSetup,
    };
    
    let runtime = consensus_runtime();
    let mut playground = NetworkPlayground::new(runtime.handle().clone());
    
    // Create validator with permissive filter (filter disabled)
    let permissive_filter_config = BlockTransactionFilterConfig::new(false, 
        BlockTransactionFilter::empty());
    
    // Create validator with strict filter (denies all transactions)
    let strict_filter_config = BlockTransactionFilterConfig::new(true,
        BlockTransactionFilter::empty().add_all_filter(false));
    
    // Create two nodes with different filter configs
    let mut node_permissive = NodeSetup::create_nodes(
        &mut playground,
        runtime.handle().clone(),
        1,
        None,
        None,
        Some(permissive_filter_config),
        None,
        None,
        None,
        false, // Use DirectMempool
    )[0];
    
    let mut node_strict = NodeSetup::create_nodes(
        &mut playground,
        runtime.handle().clone(),
        1,
        None,
        None,
        Some(strict_filter_config),
        None,
        None,
        None,
        false,
    )[0];
    
    // Node with permissive filter creates proposal
    let transactions = create_test_transactions();
    let proposal = Block::new_proposal(
        Payload::DirectMempool(transactions),
        1, 1,
        certificate_for_genesis(),
        &node_permissive.signer,
        Vec::new(),
    ).unwrap();
    
    // Permissive node accepts its own proposal
    timed_block_on(&runtime, async {
        assert!(node_permissive.round_manager
            .process_proposal(proposal.clone())
            .await
            .is_ok());
    });
    
    // Strict node rejects the same proposal due to filter mismatch
    timed_block_on(&runtime, async {
        assert!(node_strict.round_manager
            .process_proposal(proposal)
            .await
            .is_err()); // Consensus failure - no vote cast
    });
    
    // This demonstrates validators cannot reach consensus
    // In a real network, this causes round timeout and delay
}
```

## Notes

This vulnerability exists because the transaction filter system was designed as a local safety mechanism without considering the distributed consensus implications. The key insight is that **proposal validation rules must be deterministic and consistent across all validators** for consensus to function efficiently.

The filter mechanism itself is not flawed—it serves a legitimate purpose for operational security. However, its implementation as an unsynchronized local configuration violates the consensus protocol's assumption that all honest validators apply identical validation rules to proposals.

This finding affects all Aptos networks where validators might independently configure transaction filters, including mainnet, testnet, and private deployments.

### Citations

**File:** config/src/config/transaction_filters_config.rs (L90-123)
```rust
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct BlockTransactionFilterConfig {
    filter_enabled: bool, // Whether the filter is enabled
    block_transaction_filter: BlockTransactionFilter, // The block transaction filter to apply
}

impl BlockTransactionFilterConfig {
    pub fn new(filter_enabled: bool, block_transaction_filter: BlockTransactionFilter) -> Self {
        Self {
            filter_enabled,
            block_transaction_filter,
        }
    }

    /// Returns true iff the filter is enabled and not empty
    pub fn is_enabled(&self) -> bool {
        self.filter_enabled && !self.block_transaction_filter.is_empty()
    }

    /// Returns a reference to the block transaction filter
    pub fn block_transaction_filter(&self) -> &BlockTransactionFilter {
        &self.block_transaction_filter
    }
}

impl Default for BlockTransactionFilterConfig {
    fn default() -> Self {
        Self {
            filter_enabled: false,                                     // Disable the filter
            block_transaction_filter: BlockTransactionFilter::empty(), // Use an empty filter
        }
    }
}
```

**File:** consensus/src/round_manager.rs (L1204-1214)
```rust
        if let Err(error) = self
            .block_store
            .check_denied_inline_transactions(&proposal, &self.block_txn_filter_config)
        {
            counters::REJECTED_PROPOSAL_DENY_TXN_COUNT.inc();
            bail!(
                "[RoundManager] Proposal for block {} contains denied inline transactions: {}. Dropping proposal!",
                proposal.id(),
                error
            );
        }
```

**File:** consensus/src/epoch_manager.rs (L211-211)
```rust
        let consensus_txn_filter_config = node_config.transaction_filters.consensus_filter.clone();
```

**File:** types/src/on_chain_config/consensus_config.rs (L190-213)
```rust
/// The on-chain consensus config, in order to be able to add fields, we use enum to wrap the actual struct.
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
pub enum OnChainConsensusConfig {
    V1(ConsensusConfigV1),
    V2(ConsensusConfigV1),
    V3 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
    },
    V4 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
        // Execution pool block window
        window_size: Option<u64>,
    },
    V5 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
        // Execution pool block window
        window_size: Option<u64>,
        // Whether to check if we can skip generating randomness for blocks
        rand_check_enabled: bool,
    },
}
```

**File:** consensus/src/quorum_store/direct_mempool_quorum_store.rs (L89-136)
```rust
    async fn handle_block_request(
        &self,
        max_txns: u64,
        max_bytes: u64,
        return_non_full: bool,
        payload_filter: PayloadFilter,
        callback: oneshot::Sender<Result<GetPayloadResponse>>,
    ) {
        let get_batch_start_time = Instant::now();
        let exclude_txns = match payload_filter {
            PayloadFilter::DirectMempool(exclude_txns) => exclude_txns,
            PayloadFilter::InQuorumStore(_) => {
                unreachable!("Unknown payload_filter: {}", payload_filter)
            },
            PayloadFilter::Empty => Vec::new(),
        };

        let (txns, result) = match self
            .pull_internal(max_txns, max_bytes, return_non_full, exclude_txns)
            .await
        {
            Err(_) => {
                error!("GetBatch failed");
                (vec![], counters::REQUEST_FAIL_LABEL)
            },
            Ok(txns) => (txns, counters::REQUEST_SUCCESS_LABEL),
        };
        counters::quorum_store_service_latency(
            counters::GET_BATCH_LABEL,
            result,
            get_batch_start_time.elapsed(),
        );

        let get_block_response_start_time = Instant::now();
        let payload = Payload::DirectMempool(txns);
        let result = match callback.send(Ok(GetPayloadResponse::GetPayloadResponse(payload))) {
            Err(_) => {
                error!("Callback failed");
                counters::CALLBACK_FAIL_LABEL
            },
            Ok(_) => counters::CALLBACK_SUCCESS_LABEL,
        };
        counters::quorum_store_service_latency(
            counters::GET_BLOCK_RESPONSE_LABEL,
            result,
            get_block_response_start_time.elapsed(),
        );
    }
```
