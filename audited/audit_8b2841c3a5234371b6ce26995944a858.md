# Audit Report

## Title
Cross-Fork Randomness Share Contamination During Chain Reorganization

## Summary
During chain reorganizations, stale randomness broadcast tasks from the old fork can add shares to the new fork's randomness store due to insufficient fork-specific validation. This causes different validators to potentially generate different randomness values for the same round, violating consensus safety.

## Finding Description

The randomness generation system uses reliable broadcast to collect shares from validators. When a chain reorganization occurs, the system resets by dropping old broadcast tasks and clearing state. However, a critical race condition allows stale broadcast tasks to contaminate the new fork's randomness state.

**Root Cause:**

`RandMetadata` only contains epoch and round numbers, without any fork-specific identifiers like block hash. [1](#0-0) 

Randomness shares are cryptographically verified and matched based solely on this epoch+round tuple. The WVUF verification serializes only the `RandMetadata`: [2](#0-1) 

When shares are matched in the broadcast state, only the `RandMetadata` (epoch+round) is compared: [3](#0-2) 

The `retain` method in share aggregation only compares the `RandMetadata` portion: [4](#0-3) 

**Attack Path:**

1. Node is at round 100 on Fork A, with broadcast tasks spawned for recent rounds
2. Chain reorganization detected; node must sync to Fork B at round 90
3. `execution_client.reset()` is called, sending `ResetSignal::TargetRound(90)` to rand_manager: [5](#0-4) 

4. `rand_manager.process_reset(90)` clears the block queue and resets rand_store: [6](#0-5) 

5. Old broadcast tasks are aborted via DropGuard mechanism: [7](#0-6) 

6. **Race Condition:** The reset removes rounds >= 90, but `FUTURE_ROUNDS_TO_ACCEPT = 200` allows stale shares to be accepted: [8](#0-7) 

7. A stale broadcast task for round 95 (Fork A) receives a share response after reset completes but before task abortion
8. The `ShareAggregateState::add` method executes atomically (no await points) and adds the share: [9](#0-8) 

9. The share is accepted because `95 <= 90 + 200`: [10](#0-9) 

10. New blocks from Fork B arrive for round 95 with different block content
11. When Fork B's round 95 metadata is added, the `retain` check only validates that shares have matching epoch+round, keeping the Fork A share
12. Randomness aggregation mixes shares from both forks, producing different randomness values for validators that received different stale shares

The randomness becomes part of the block execution state: [11](#0-10) 

## Impact Explanation

**Severity: HIGH (Consensus Safety Violation)**

This vulnerability breaks the **Deterministic Execution** and **Consensus Safety** invariants:

1. **Consensus Safety Violation:** Different validators may produce different randomness for the same round if they experience the race condition at different times or receive different stale shares. Since randomness is included in `BlockMetadataWithRandomness`, this means different block execution results and state roots for the same block, violating the core requirement that all honest validators must agree on block content.

2. **Non-Deterministic State:** The randomness value is used in the block prologue and affects on-chain randomness APIs. Different randomness values cause different transaction execution results, producing different state roots for identical blocks.

3. **Potential Chain Split:** Differing randomness values cause validators to produce different state commitments for the same round, preventing consensus agreement and potentially causing chain divergence that requires manual intervention to recover.

Per Aptos bug bounty criteria, this qualifies as **High Severity** due to "Significant protocol violations" - it compromises consensus agreement without requiring validator collusion, creating conditions for chain divergence.

## Likelihood Explanation

**Likelihood: MEDIUM**

The vulnerability requires specific timing conditions but can occur naturally:

**Requires:**
- Chain reorganization event (occurs during network partitions or deep reorgs)
- Active broadcast tasks for rounds that exist on both forks (common in shallow reorgs)
- Precise timing: share response arrives after reset completes but before task abortion
- Stale task completes `add()` operation before abort signal takes effect

**Factors Increasing Likelihood:**
- The 200-round acceptance window (`FUTURE_ROUNDS_TO_ACCEPT = 200`) creates a large vulnerability window
- Network partitions or high latency scenarios (when reorgs are most likely to occur)
- Task abortion is asynchronous while `add()` has no await points, creating a race window
- Validators with slow network connections have larger race windows

**Factors Decreasing Likelihood:**
- Most reorgs are shallow (few rounds), though the 200-round window still allows many rounds of overlap
- DropGuard abortion is relatively fast, but not instantaneous

The vulnerability is not easily exploitable by attackers but can occur naturally during network disruptions.

## Recommendation

Bind randomness shares to specific block identifiers to prevent cross-fork contamination:

1. **Include block_id in share verification**: Modify `RandMetadata` to include a block identifier or include the full `FullRandMetadata` in cryptographic verification
2. **Stricter reset validation**: Reduce `FUTURE_ROUNDS_TO_ACCEPT` or add fork-epoch tracking to reject shares from previous forks
3. **Atomic reset**: Ensure all tasks are fully aborted before accepting new shares for reset rounds
4. **Enhanced retain logic**: Validate that shares match not just epoch+round but also the specific block_id from `FullRandMetadata`

Recommended code changes:
- Update the `retain` method to compare against `FullRandMetadata` including block_id
- Add block_id binding to the WVUF signature verification
- Implement a fork identifier in the randomness state to reject stale shares

## Proof of Concept

While a complete PoC would require orchestrating a multi-validator network with controlled chain reorganizations, the vulnerability can be demonstrated by:

1. Setting up a test network with validators at different rounds
2. Triggering a reset via `sync_to_target` 
3. Sending delayed share responses that arrive after reset but before task abortion
4. Observing that shares with matching epoch+round are retained despite being from different forks
5. Verifying that different validators produce different randomness values for the same round

The race condition window exists in the current codebase as validated through the code citations above.

### Citations

**File:** types/src/randomness.rs (L23-27)
```rust
#[derive(Clone, Serialize, Deserialize, Debug, Default, PartialEq, Eq, Hash)]
pub struct RandMetadata {
    pub epoch: u64,
    pub round: Round,
}
```

**File:** consensus/src/rand/rand_gen/types.rs (L26-26)
```rust
pub const FUTURE_ROUNDS_TO_ACCEPT: u64 = 200;
```

**File:** consensus/src/rand/rand_gen/types.rs (L65-72)
```rust
            WVUF::verify_share(
                &rand_config.vuf_pp,
                apk,
                bcs::to_bytes(&rand_metadata)
                    .map_err(|e| anyhow!("Serialization failed: {}", e))?
                    .as_slice(),
                &self.share,
            )?;
```

**File:** consensus/src/rand/rand_gen/reliable_broadcast_state.rs (L131-151)
```rust
    fn add(&self, peer: Author, share: Self::Response) -> anyhow::Result<Option<()>> {
        ensure!(share.author() == &peer, "Author does not match");
        ensure!(
            share.metadata() == &self.rand_metadata,
            "Metadata does not match: local {:?}, received {:?}",
            self.rand_metadata,
            share.metadata()
        );
        share.verify(&self.rand_config)?;
        info!(LogSchema::new(LogEvent::ReceiveReactiveRandShare)
            .epoch(share.epoch())
            .round(share.metadata().round)
            .remote_peer(*share.author()));
        let mut store = self.rand_store.lock();
        let aggregated = if store.add_share(share, PathType::Slow)? {
            Some(())
        } else {
            None
        };
        Ok(aggregated)
    }
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L91-93)
```rust
    fn retain(&mut self, rand_config: &RandConfig, rand_metadata: &FullRandMetadata) {
        self.shares
            .retain(|_, share| share.metadata() == &rand_metadata.metadata);
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L285-288)
```rust
        ensure!(
            share.metadata().round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT,
            "Share from future round"
        );
```

**File:** consensus/src/pipeline/execution_client.rs (L684-692)
```rust
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L184-194)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.rand_store.lock().reset(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L300-302)
```rust
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(task, abort_registration));
        DropGuard::new(abort_handle)
```

**File:** types/src/block_metadata_ext.rs (L24-34)
```rust
pub struct BlockMetadataWithRandomness {
    pub id: HashValue,
    pub epoch: u64,
    pub round: u64,
    pub proposer: AccountAddress,
    #[serde(with = "serde_bytes")]
    pub previous_block_votes_bitvec: Vec<u8>,
    pub failed_proposer_indices: Vec<u32>,
    pub timestamp_usecs: u64,
    pub randomness: Option<Randomness>,
}
```
