# Audit Report

## Title
Redis Connection Loss Causes Cascading Panics in Indexer File Store Processor Without Reconnection Attempts

## Summary
The indexer-grpc-file-store processor contains multiple `unwrap()` calls on Redis operations within spawned tasks that will panic when Redis connection is lost. These panics cascade up through the error handling, ultimately crashing the entire file store processor without any reconnection attempts or graceful degradation.

## Finding Description

The `run()` function in the file store processor spawns parallel tasks to fetch transactions from Redis cache and upload them to file storage. [1](#0-0) 

Within each spawned task, the code calls `cache_operator_clone.get_transactions()` and immediately unwraps the Result without error handling. The `get_transactions()` method returns `anyhow::Result<Vec<Transaction>>` which will return an error if Redis operations fail. [2](#0-1) 

The Redis operations use `mget` which will fail if the connection is lost. [3](#0-2) 

When Redis connection is lost:
1. `cache_operator_clone.get_transactions()` returns `Err` due to Redis connection failure
2. The `.unwrap()` panics the spawned task
3. `futures::future::try_join_all(tasks)` catches the panic as a `JoinError`
4. The error handling explicitly panics the entire processor [4](#0-3) 
5. This propagates to the top-level which crashes the application [5](#0-4) 

While the `redis::aio::ConnectionManager` provides automatic reconnection capabilities, it does not retry failed operations automatically. During connection loss or reconnection periods, operations return errors that must be handled by the application. [6](#0-5) 

In contrast, the data-service component properly handles Redis errors with retry logic and graceful degradation. [7](#0-6) 

## Impact Explanation

This qualifies as **High Severity** under the "API crashes" category. The indexer-grpc-file-store is a critical component of the Aptos indexer infrastructure that processes and stores transaction data for external consumption. When this processor crashes:

- The file store stops processing new transactions, creating a growing backlog
- Indexer APIs dependent on file store data become stale
- Manual intervention is required to restart the processor
- Data gaps may occur if the processor stays down during transaction processing
- No automatic recovery mechanism exists

While this does not affect consensus or validator operations, it impacts the availability of critical indexer infrastructure that downstream applications rely on.

## Likelihood Explanation

**High likelihood** - Redis connection losses are common operational scenarios that occur due to:
- Network partitions or instability
- Redis server restarts or maintenance
- Resource exhaustion on Redis instance
- Load balancer or proxy failures
- Transient network errors

The vulnerability will trigger on any Redis operation failure during the processing loop, making it highly likely to occur in production environments.

## Recommendation

Replace `unwrap()` calls with proper error handling that retries failed operations. Follow the pattern used in data-service:

```rust
// In the spawned task (around line 162-165):
let transactions = match cache_operator_clone
    .get_transactions(start_version, FILE_ENTRY_TRANSACTION_COUNT)
    .await
{
    Ok(txns) => txns,
    Err(e) => {
        tracing::error!(
            start_version = start_version,
            error = ?e,
            "[File Store] Failed to fetch transactions from cache"
        );
        return Err(anyhow::anyhow!("Cache fetch failed: {}", e));
    }
};

// In the main loop (around line 205-243):
let (first_version, last_version, first_version_encoded, last_version_encoded) =
    match futures::future::try_join_all(tasks).await {
        Ok(mut res) => {
            // ... existing success handling ...
        },
        Err(err) => {
            tracing::error!(
                error = ?err,
                "[File Store] Error processing transaction batches. Retrying..."
            );
            // Sleep and continue to retry
            tokio::time::sleep(Duration::from_millis(1000)).await;
            continue; // Continue the main loop instead of panicking
        },
    };
```

## Proof of Concept

```rust
// Reproduction steps:
// 1. Start the file-store processor with Redis connection
// 2. During processing, kill the Redis server or introduce network partition
// 3. Observe the processor panic with "Error processing transaction batches"

// To simulate in a test environment:
#[tokio::test]
async fn test_redis_connection_loss_causes_panic() {
    // Create a mock Redis connection that fails after initial setup
    let mock_conn = MockRedisConnection::new_with_delayed_failure();
    let cache_operator = CacheOperator::new(mock_conn, StorageFormat::Base64UncompressedProto);
    
    // Clone for use in spawned task (simulating the processor pattern)
    let mut cache_operator_clone = cache_operator.clone();
    
    // Spawn task that will panic on Redis error
    let task = tokio::spawn(async move {
        // This will panic when Redis connection fails
        let transactions = cache_operator_clone
            .get_transactions(0, 1000)
            .await
            .unwrap(); // <- PANICS HERE
        transactions
    });
    
    // Wait for task - will get JoinError due to panic
    let result = task.await;
    assert!(result.is_err());
    // In actual code, this propagates to line 243 which panics again
}
```

**Notes**

This vulnerability is specific to the indexer-grpc-file-store component and does not affect core consensus, validator operations, or blockchain state management. However, it represents a significant availability issue for the indexer infrastructure that serves external applications querying Aptos blockchain data. The file-store processor requires manual restart after any Redis connection issue, creating operational burden and potential data indexing gaps.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L43-58)
```rust
        // Connection to redis is a hard dependency for file store processor.
        let conn = redis::Client::open(redis_main_instance_address.0.clone())
            .with_context(|| {
                format!(
                    "Create redis client for {} failed",
                    redis_main_instance_address.0
                )
            })?
            .get_tokio_connection_manager()
            .await
            .with_context(|| {
                format!(
                    "Create redis connection to {} failed.",
                    redis_main_instance_address.0
                )
            })?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L157-165)
```rust
            for start_version in batches {
                let mut cache_operator_clone = self.cache_operator.clone();
                let mut file_store_operator_clone = self.file_store_operator.clone_box();
                let task = tokio::spawn(async move {
                    let fetch_start_time = std::time::Instant::now();
                    let transactions = cache_operator_clone
                        .get_transactions(start_version, FILE_ENTRY_TRANSACTION_COUNT)
                        .await
                        .unwrap();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L243-243)
```rust
                    Err(err) => panic!("Error processing transaction batches: {:?}", err),
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L373-380)
```rust
        let versions = (start_version..start_version + transaction_count)
            .map(|e| CacheEntry::build_key(e, self.storage_format))
            .collect::<Vec<String>>();
        let encoded_transactions: Vec<Vec<u8>> = self
            .conn
            .mget(versions)
            .await
            .context("Failed to mget from Redis")?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L398-407)
```rust
    pub async fn get_transactions(
        &mut self,
        start_version: u64,
        transaction_count: u64,
    ) -> anyhow::Result<Vec<Transaction>> {
        let (transactions, _, _) = self
            .get_transactions_with_durations(start_version, transaction_count)
            .await?;
        Ok(transactions)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/lib.rs (L57-60)
```rust
        processor
            .run()
            .await
            .expect("File store processor exited unexpectedly");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L364-369)
```rust
        Err(e) => {
            ERROR_COUNT.with_label_values(&["data_fetch_failed"]).inc();
            data_fetch_error_handling(e, start_version, chain_id).await;
            // Retry after a short sleep.
            return DataFetchSubTaskResult::NoResults;
        },
```
