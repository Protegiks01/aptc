# Audit Report

## Title
Timeout Configuration Mismatch Causes Network Fragmentation Through Peer Score Degradation

## Summary
Aptos nodes use configurable timeout values for state-sync requests, but these timeouts are not coordinated across the network. When nodes configure incompatible timeout values, repeated timeout errors cause permanent peer score degradation, leading to peer ignoring and network fragmentation into isolated clusters that cannot sync state with each other.

## Finding Description

The Aptos state-sync data client implements a peer scoring system where each peer starts with a score of 50.0. [1](#0-0)  When requests timeout, the peer's score is multiplied by 0.95 (NOT_USEFUL_MULTIPLIER). [2](#0-1)  When a peer's score drops to or below 25.0 (IGNORE_PEER_THRESHOLD), the peer is permanently ignored. [3](#0-2) 

The timeout values used for requests are configurable per-node through `AptosDataClientConfig`:
- `response_timeout_ms` (default: 10,000 ms)
- `optimistic_fetch_timeout_ms` (default: 5,000 ms) 
- `subscription_response_timeout_ms` (default: 15,000 ms)
- `max_response_timeout_ms` (default: 60,000 ms) [4](#0-3) 

When a request times out, it's classified as a "not useful" error [5](#0-4)  and triggers peer score degradation [6](#0-5) , which applies the NOT_USEFUL_MULTIPLIER to the score. [7](#0-6) 

Once a peer is ignored, it cannot service any data requests [8](#0-7)  because the `can_service_request` function returns false for ignored peers.

**The Critical Flaw:**

After approximately 14 consecutive timeouts (solving 50.0 × 0.95^n ≤ 25.0), a peer becomes permanently ignored. The timeout calculation in data streams uses exponential backoff [9](#0-8) , but if the base timeout is too short relative to actual network and processing latencies, even exponential backoff won't prevent repeated timeouts.

**Attack Scenario:**

1. Node operators in a low-latency region configure aggressive timeouts (e.g., `response_timeout_ms: 2000` ms) to "optimize" performance
2. Nodes in high-latency regions or with higher load take 3-4 seconds to respond
3. The fast-timeout nodes consistently timeout requests to slower nodes
4. After ~14 timeouts, slower nodes are permanently ignored by fast-timeout nodes
5. The network fragments into clusters based on timeout compatibility
6. State synchronization fails between clusters

This violates the critical invariant that all validators must maintain consistent state, as nodes in different clusters cannot sync from each other.

## Impact Explanation

This qualifies as **Medium Severity** under Aptos bug bounty criteria: "State inconsistencies requiring intervention."

The vulnerability causes:
- **Network fragmentation**: Honest nodes become isolated into clusters based on timeout configurations
- **State sync failures**: Nodes in different clusters cannot exchange state data
- **Potential consensus issues**: If fragmentation is severe enough, different clusters may progress independently
- **Manual intervention required**: Recovery requires restarting nodes with compatible timeout configurations

While this doesn't directly cause fund loss or immediate consensus failure, it creates state inconsistencies across the network that require operator intervention to resolve. The permanent nature of peer ignoring (once below threshold) makes this particularly severe.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability is likely to occur because:

1. **Configuration flexibility**: Node operators can customize timeout values in their YAML configs as these are standard configuration fields [10](#0-9) 

2. **Natural incentive**: Operators may legitimately reduce timeouts thinking it improves sync performance, especially when experiencing slow peers

3. **No validation**: There's no mechanism to validate or coordinate timeout values across the network

4. **Geographic distribution**: Nodes in different regions naturally have different latencies, making timeout mismatches more likely

5. **Permanent isolation**: The peer scoring mechanism has no automatic recovery once a peer is ignored, making the issue persistent once triggered

## Recommendation

Implement network-wide timeout coordination and peer score recovery mechanisms:

1. **Add timeout bounds validation** in the config sanitizer to prevent extremely aggressive timeout configurations:

```rust
// In config/src/config/state_sync_config.rs
impl ConfigSanitizer for AptosDataClientConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let data_client_config = &node_config.state_sync.aptos_data_client;
        
        // Enforce minimum timeout values to prevent network fragmentation
        const MIN_RESPONSE_TIMEOUT_MS: u64 = 5000; // 5 seconds
        const MIN_SUBSCRIPTION_TIMEOUT_MS: u64 = 10000; // 10 seconds
        
        if data_client_config.response_timeout_ms < MIN_RESPONSE_TIMEOUT_MS {
            return Err(Error::ConfigSanitizerFailed(
                "AptosDataClientConfig",
                format!("response_timeout_ms must be at least {} ms", MIN_RESPONSE_TIMEOUT_MS)
            ));
        }
        
        if data_client_config.subscription_response_timeout_ms < MIN_SUBSCRIPTION_TIMEOUT_MS {
            return Err(Error::ConfigSanitizerFailed(
                "AptosDataClientConfig",
                format!("subscription_response_timeout_ms must be at least {} ms", MIN_SUBSCRIPTION_TIMEOUT_MS)
            ));
        }
        
        Ok(())
    }
}
```

2. **Implement gradual score recovery** for ignored peers through successful storage summary polls:

```rust
// In state-sync/aptos-data-client/src/peer_states.rs
// Add a periodic score recovery mechanism
pub fn recover_ignored_peer_scores(&self) {
    for mut peer_state in self.peer_to_state.iter_mut() {
        if peer_state.is_ignored() {
            // Gradually increase score for ignored peers to allow recovery
            peer_state.score = f64::min(peer_state.score + 0.5, STARTING_SCORE);
        }
    }
}
```

3. **Distinguish between timeout types** - separate scoring for network timeouts vs. actual bad responses

4. **Add monitoring and alerts** for when peer scores drop significantly, indicating potential timeout configuration issues

## Proof of Concept

The following Rust test demonstrates the vulnerability:

```rust
#[tokio::test]
async fn test_timeout_fragmentation() {
    // Setup: Create two data clients with different timeout configs
    let fast_timeout_config = AptosDataClientConfig {
        response_timeout_ms: 2000, // 2 seconds
        optimistic_fetch_timeout_ms: 1000,
        subscription_response_timeout_ms: 3000,
        ..Default::default()
    };
    
    let normal_timeout_config = AptosDataClientConfig::default();
    
    // Create mock network where "slow_peer" takes 3 seconds to respond
    let fast_client = create_mock_data_client(fast_timeout_config);
    let slow_peer = PeerNetworkId::random();
    
    // Simulate 15 consecutive timeout events
    for _ in 0..15 {
        // Fast client sends request to slow peer
        let request = StorageServiceRequest::new(
            DataRequest::GetStorageServerSummary,
            false
        );
        
        // Request times out after 2 seconds
        let result = tokio::time::timeout(
            Duration::from_millis(2000),
            simulate_slow_response(Duration::from_secs(3))
        ).await;
        
        assert!(result.is_err()); // Timeout occurred
        
        // Client marks peer as having timeout error
        fast_client.notify_bad_response(
            0,
            slow_peer,
            &request,
            ErrorType::NotUseful
        );
    }
    
    // Verify: After 15 timeouts, peer score is below threshold
    let peer_states = fast_client.get_peer_states();
    let peer_state = peer_states.peer_to_state.get(&slow_peer).unwrap();
    
    // Score should be ~24.8 (50.0 * 0.95^15 ≈ 23.1)
    assert!(peer_state.score < IGNORE_PEER_THRESHOLD);
    
    // Verify: Peer is now ignored and cannot service requests
    assert!(peer_state.is_ignored());
    assert!(!peer_states.can_service_request(
        &slow_peer,
        TimeService::mock(),
        &request
    ));
    
    println!("Network fragmentation demonstrated: slow_peer is permanently ignored");
}
```

This demonstrates that incompatible timeout configurations lead to permanent peer isolation, fragmenting the network into non-communicating clusters.

## Notes

The vulnerability is rooted in the combination of:
1. Configurable timeouts without network-wide coordination [11](#0-10) 
2. Aggressive peer scoring that permanently ignores low-score peers [12](#0-11) 
3. Timeout errors being treated as peer failures rather than configuration mismatches [13](#0-12) 

The default configuration values are reasonable, but the lack of bounds checking and coordination allows operators to inadvertently fragment the network through well-intentioned "optimizations."

### Citations

**File:** state-sync/aptos-data-client/src/peer_states.rs (L35-35)
```rust
const STARTING_SCORE: f64 = 50.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L39-39)
```rust
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L43-43)
```rust
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L152-159)
```rust
    fn is_ignored(&self) -> bool {
        // Only ignore peers if the config allows it
        if !self.data_client_config.ignore_low_score_peers {
            return false;
        }

        // Otherwise, ignore peers with a low score
        self.score <= IGNORE_PEER_THRESHOLD
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L168-173)
```rust
    fn update_score_error(&mut self, error: ErrorType) {
        let multiplier = match error {
            ErrorType::NotUseful => NOT_USEFUL_MULTIPLIER,
            ErrorType::Malicious => MALICIOUS_MULTIPLIER,
        };
        self.score = f64::max(self.score * multiplier, MIN_SCORE);
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L217-221)
```rust
            return match peer_state.get_storage_summary_if_not_ignored() {
                Some(storage_summary) => {
                    storage_summary.can_service(&self.data_client_config, time_service, request)
                },
                None => false, // The peer is temporarily ignored
```

**File:** config/src/config/state_sync_config.rs (L413-458)
```rust
pub struct AptosDataClientConfig {
    /// Whether transaction data v2 is enabled
    pub enable_transaction_data_v2: bool,
    /// The aptos data poller config for the data client
    pub data_poller_config: AptosDataPollerConfig,
    /// The aptos data multi-fetch config for the data client
    pub data_multi_fetch_config: AptosDataMultiFetchConfig,
    /// Whether or not to ignore peers with low peer scores
    pub ignore_low_score_peers: bool,
    /// The aptos latency filtering config for the data client
    pub latency_filtering_config: AptosLatencyFilteringConfig,
    /// The interval (milliseconds) at which to refresh the latency monitor
    pub latency_monitor_loop_interval_ms: u64,
    /// Maximum number of epoch ending ledger infos per chunk
    pub max_epoch_chunk_size: u64,
    /// Maximum number of output reductions (division by 2) before transactions are returned,
    /// e.g., if 1000 outputs are requested in a single data chunk, and this is set to 1, then
    /// we'll accept anywhere between 1000 and 500 outputs. Any less, and the server should
    /// return transactions instead of outputs.
    // TODO: migrate away from this, and use cleaner chunk packing configs and logic.
    pub max_num_output_reductions: u64,
    /// Maximum lag (in seconds) we'll tolerate when sending optimistic fetch requests
    pub max_optimistic_fetch_lag_secs: u64,
    /// Maximum number of bytes to send in a single response
    pub max_response_bytes: u64,
    /// Maximum timeout (in ms) when waiting for a response (after exponential increases)
    pub max_response_timeout_ms: u64,
    /// Maximum number of state keys and values per chunk
    pub max_state_chunk_size: u64,
    /// Maximum lag (in seconds) we'll tolerate when sending subscription requests
    pub max_subscription_lag_secs: u64,
    /// Maximum number of transactions per chunk
    pub max_transaction_chunk_size: u64,
    /// Maximum number of transaction outputs per chunk
    pub max_transaction_output_chunk_size: u64,
    /// Timeout (in ms) when waiting for an optimistic fetch response
    pub optimistic_fetch_timeout_ms: u64,
    /// The duration (in seconds) after which to panic if no progress has been made
    pub progress_check_max_stall_time_secs: u64,
    /// First timeout (in ms) when waiting for a response
    pub response_timeout_ms: u64,
    /// Timeout (in ms) when waiting for a subscription response
    pub subscription_response_timeout_ms: u64,
    /// Whether or not to request compression for incoming data
    pub use_compression: bool,
}
```

**File:** config/src/config/state_sync_config.rs (L460-484)
```rust
impl Default for AptosDataClientConfig {
    fn default() -> Self {
        Self {
            enable_transaction_data_v2: true,
            data_poller_config: AptosDataPollerConfig::default(),
            data_multi_fetch_config: AptosDataMultiFetchConfig::default(),
            ignore_low_score_peers: true,
            latency_filtering_config: AptosLatencyFilteringConfig::default(),
            latency_monitor_loop_interval_ms: 100,
            max_epoch_chunk_size: MAX_EPOCH_CHUNK_SIZE,
            max_num_output_reductions: 0,
            max_optimistic_fetch_lag_secs: 20, // 20 seconds
            max_response_bytes: CLIENT_MAX_MESSAGE_SIZE_V2 as u64,
            max_response_timeout_ms: 60_000, // 60 seconds
            max_state_chunk_size: MAX_STATE_CHUNK_SIZE,
            max_subscription_lag_secs: 20, // 20 seconds
            max_transaction_chunk_size: MAX_TRANSACTION_CHUNK_SIZE,
            max_transaction_output_chunk_size: MAX_TRANSACTION_OUTPUT_CHUNK_SIZE,
            optimistic_fetch_timeout_ms: 5000,         // 5 seconds
            progress_check_max_stall_time_secs: 86400, // 24 hours (long enough to debug any issues at runtime)
            response_timeout_ms: 10_000,               // 10 seconds
            subscription_response_timeout_ms: 15_000, // 15 seconds (longer than a regular timeout because of prefetching)
            use_compression: true,
        }
    }
```

**File:** state-sync/aptos-data-client/src/client.rs (L834-867)
```rust
                let client_error = match error {
                    aptos_storage_service_client::Error::RpcError(rpc_error) => match rpc_error {
                        RpcError::NotConnected(_) => {
                            Error::DataIsUnavailable(rpc_error.to_string())
                        },
                        RpcError::TimedOut => {
                            Error::TimeoutWaitingForResponse(rpc_error.to_string())
                        },
                        _ => Error::UnexpectedErrorEncountered(rpc_error.to_string()),
                    },
                    aptos_storage_service_client::Error::StorageServiceError(err) => {
                        Error::UnexpectedErrorEncountered(err.to_string())
                    },
                    _ => Error::UnexpectedErrorEncountered(error.to_string()),
                };

                warn!(
                    (LogSchema::new(LogEntry::StorageServiceResponse)
                        .event(LogEvent::ResponseError)
                        .request_type(&request.get_label())
                        .request_id(id)
                        .peer(&peer)
                        .error(&client_error))
                );

                increment_request_counter(
                    &metrics::ERROR_RESPONSES,
                    client_error.get_label(),
                    peer,
                );

                self.notify_bad_response(id, peer, &request, ErrorType::NotUseful);
                Err(client_error)
            },
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L344-378)
```rust
        let request_timeout_ms = if data_client_request.is_optimistic_fetch_request() {
            self.data_client_config.optimistic_fetch_timeout_ms
        } else if data_client_request.is_subscription_request() {
            self.data_client_config.subscription_response_timeout_ms
        } else if !request_retry {
            self.data_client_config.response_timeout_ms
        } else {
            let response_timeout_ms = self.data_client_config.response_timeout_ms;
            let max_response_timeout_ms = self.data_client_config.max_response_timeout_ms;

            // Exponentially increase the timeout based on the number of
            // previous failures (but bounded by the max timeout).
            let request_timeout_ms = min(
                max_response_timeout_ms,
                response_timeout_ms * (u32::pow(2, self.request_failure_count as u32) as u64),
            );

            // Update the retry counter and log the request
            increment_counter_multiple_labels(
                &metrics::RETRIED_DATA_REQUESTS,
                data_client_request.get_label(),
                &request_timeout_ms.to_string(),
            );
            info!(
                (LogSchema::new(LogEntry::RetryDataRequest)
                    .stream_id(self.data_stream_id)
                    .message(&format!(
                        "Retrying data request type: {:?}, with new timeout: {:?} (ms)",
                        data_client_request.get_label(),
                        request_timeout_ms.to_string()
                    )))
            );

            request_timeout_ms
        };
```
