# Audit Report

## Title
Resource Leak in Consensus Observer Subscription Cleanup - Untracked Async Tasks and Message Queue Saturation

## Summary
When a consensus observer subscription health check fails, the cleanup process spawns untracked async tasks to notify remote peers. If these notifications fail (due to network issues or non-responsive peers), remote peers continue sending messages. Each rejected message triggers another unsubscribe attempt, spawning additional async tasks without deduplication or rate limiting. This leads to message queue saturation, task proliferation, and potential dropping of legitimate consensus messages.

## Finding Description
The vulnerability exists in the subscription termination flow across two files: [1](#0-0) 

When `check_subscription_health()` fails, it triggers the cleanup process: [2](#0-1) 

The `terminate_unhealthy_subscriptions()` function calls `unsubscribe_from_peer()`: [3](#0-2) 

**Critical Flaw**: The `unsubscribe_from_peer()` function immediately removes the subscription from the local map, then spawns an **untracked async task** via `tokio::spawn()` to send an unsubscribe RPC. This task is fire-and-forget with no handle stored or awaited.

If the RPC fails (network timeout after 5 seconds, peer unreachable, or malicious peer ignoring the request), the remote peer never receives notification and continues sending consensus messages.

Each incoming message from an unsubscribed peer is rejected here: [4](#0-3) 

**The Resource Leak**: At line 380, `verify_message_for_subscription()` calls `unsubscribe_from_peer()` again for each rejected message. Since the `.remove()` operation returns `None` (subscription already removed), this doesn't prevent the async task from being spawned. This creates:

1. **Async Task Proliferation**: If a peer sends N messages/second, N async tasks/second are spawned, each living for ~5 seconds
2. **Message Queue Saturation**: Messages fill the bounded queue (default 1000 messages): [5](#0-4) 
3. **Dropped Legitimate Messages**: When the queue is full, the push operation fails and valid messages are dropped: [6](#0-5) 

**Attack Scenario**:
1. Malicious peer gets subscribed (by being in top N optimal peers)
2. Observer detects issue and calls `check_subscription_health()` â†’ fails
3. Observer attempts unsubscribe, but attacker doesn't respond to RPC
4. Attacker floods observer with 100+ consensus messages/second
5. Each message spawns a new async task attempting unsubscribe (100 tasks/second)
6. Message queue fills to capacity (1000 messages in ~10 seconds)
7. Legitimate messages from valid subscriptions are dropped when push fails
8. Observer performance degrades, potentially forcing fallback to state sync mode

## Impact Explanation
This qualifies as **Medium Severity** per Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: Message drops can cause the observer to miss critical consensus blocks, requiring state sync recovery
- **Performance degradation**: Resource exhaustion from task proliferation and queue saturation degrades node performance
- **Limited availability impact**: While not total loss of liveness, the observer may repeatedly fall back to state sync mode

The impact is bounded because:
- Queue size is limited (1000 messages)
- No fund loss or consensus safety violation
- Observer can recover via state sync fallback
- Does not affect validator consensus (only observer nodes)

However, it represents a clear DoS vector that can be exploited by malicious peers or triggered naturally by network failures.

## Likelihood Explanation
**Likelihood: Medium**

This can occur in two scenarios:

1. **Natural occurrence**: Network partitions, peer crashes, or high latency can cause legitimate unsubscribe RPCs to timeout, triggering the leak
2. **Malicious exploitation**: An attacker who manages to get subscribed can intentionally ignore unsubscribe requests and flood the observer

**Attacker Requirements**:
- Must be accepted as a consensus publisher peer
- Must be selected for subscription (requires being in top N most optimal peers based on ping latency and distance from validators)
- Can then refuse to acknowledge unsubscribe requests

**Exploitation Complexity**: Low - once subscribed, simply ignore unsubscribe RPCs and send messages at high rate

## Recommendation

Implement proper tracking and deduplication of unsubscribe attempts:

```rust
/// Unsubscribes from the given peer by sending an unsubscribe request
fn unsubscribe_from_peer(&mut self, peer_network_id: PeerNetworkId) {
    // Check if there's already a pending unsubscribe for this peer
    if self.pending_unsubscribes.contains(&peer_network_id) {
        return; // Avoid spawning duplicate tasks
    }
    
    // Remove the peer from the active subscriptions
    let was_subscribed = self.active_observer_subscriptions
        .lock()
        .remove(&peer_network_id)
        .is_some();
    
    // Only send unsubscribe if peer was actually subscribed
    if !was_subscribed {
        return;
    }
    
    // Mark as pending unsubscribe
    self.pending_unsubscribes.insert(peer_network_id);
    
    // Send an unsubscribe request with timeout tracking
    let consensus_observer_client = self.consensus_observer_client.clone();
    let consensus_observer_config = self.consensus_observer_config;
    let pending_unsubscribes = self.pending_unsubscribes.clone();
    
    tokio::spawn(async move {
        let unsubscribe_request = ConsensusObserverRequest::Unsubscribe;
        let response = consensus_observer_client
            .send_rpc_request_to_peer(
                &peer_network_id,
                unsubscribe_request,
                consensus_observer_config.network_request_timeout_ms,
            )
            .await;
        
        // Clear pending flag regardless of success/failure
        pending_unsubscribes.lock().remove(&peer_network_id);
        
        match response {
            Ok(ConsensusObserverResponse::UnsubscribeAck) => {
                info!("Successfully unsubscribed from peer: {}", peer_network_id);
            },
            Ok(response) => {
                warn!("Got unexpected response type: {:?}", response.get_label());
            },
            Err(error) => {
                warn!("Failed to send unsubscribe request to peer: {}! Error: {:?}", 
                      peer_network_id, error);
            },
        }
    });
}
```

**Additional fixes needed**:
1. Add `pending_unsubscribes: Arc<Mutex<HashSet<PeerNetworkId>>>` to `SubscriptionManager` struct
2. Initialize in `new()` constructor
3. Consider implementing exponential backoff for retry attempts
4. Add metrics to track pending unsubscribe count

## Proof of Concept

```rust
#[tokio::test]
async fn test_unsubscribe_resource_leak() {
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::time::Duration;
    
    // Create subscription manager
    let consensus_observer_config = ConsensusObserverConfig::default();
    let (peers_and_metadata, consensus_observer_client) = 
        create_consensus_observer_client(&[NetworkId::Public]);
    let mut subscription_manager = SubscriptionManager::new(
        consensus_observer_client.clone(),
        consensus_observer_config,
        None,
        create_mock_db_reader(),
        TimeService::mock(),
    );
    
    // Add a subscription
    let malicious_peer = PeerNetworkId::random();
    create_observer_subscription(
        &mut subscription_manager,
        consensus_observer_config,
        create_mock_db_reader(),
        malicious_peer,
        TimeService::mock(),
    );
    
    // Simulate health check failure and cleanup
    subscription_manager.unsubscribe_from_peer(malicious_peer);
    
    // Track async task spawns
    let task_count = Arc::new(AtomicUsize::new(0));
    let task_count_clone = task_count.clone();
    
    // Simulate 100 messages from unsubscribed peer
    for _ in 0..100 {
        let result = subscription_manager.verify_message_for_subscription(malicious_peer);
        assert!(result.is_err()); // Should reject message
        task_count_clone.fetch_add(1, Ordering::SeqCst);
    }
    
    // Wait for tasks to accumulate
    tokio::time::sleep(Duration::from_secs(1)).await;
    
    // Verify resource leak: 100 async tasks spawned
    // In fixed version, should be only 1 task due to deduplication
    assert!(task_count.load(Ordering::SeqCst) >= 100, 
            "Resource leak: {} async tasks spawned for single peer", 
            task_count.load(Ordering::SeqCst));
}
```

**Notes**
- The vulnerability is confirmed in the codebase at the cited locations
- It affects consensus observer nodes (validator fullnodes and public fullnodes when enabled)
- The fix requires structural changes to track pending unsubscribe operations
- Monitoring should be added to detect excessive unsubscribe attempts as an indicator of attack or misconfiguration

### Citations

**File:** consensus/src/consensus_observer/observer/subscription.rs (L63-91)
```rust
    pub fn check_subscription_health(
        &mut self,
        connected_peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
        skip_peer_optimality_check: bool,
    ) -> Result<(), Error> {
        // Verify the subscription peer is still connected
        let peer_network_id = self.get_peer_network_id();
        if !connected_peers_and_metadata.contains_key(&peer_network_id) {
            return Err(Error::SubscriptionDisconnected(format!(
                "The peer: {:?} is no longer connected!",
                peer_network_id
            )));
        }

        // Verify the subscription has not timed out
        self.check_subscription_timeout()?;

        // Verify that the DB is continuing to sync and commit new data
        self.check_syncing_progress()?;

        // Verify that the subscription peer is still optimal
        self.check_subscription_peer_optimality(
            connected_peers_and_metadata,
            skip_peer_optimality_check,
        )?;

        // The subscription seems healthy
        Ok(())
    }
```

**File:** consensus/src/consensus_observer/observer/subscription_manager.rs (L270-305)
```rust
    /// Terminates any unhealthy subscriptions and returns the list of terminated subscriptions
    fn terminate_unhealthy_subscriptions(
        &mut self,
        connected_peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
    ) -> Vec<(PeerNetworkId, Error)> {
        // Go through all active subscriptions and terminate any unhealthy ones
        let mut terminated_subscriptions = vec![];
        for subscription_peer in self.get_active_subscription_peers() {
            // To avoid terminating too many subscriptions at once, we should skip
            // the peer optimality check if we've already terminated a subscription.
            let skip_peer_optimality_check = !terminated_subscriptions.is_empty();

            // Check the health of the subscription and terminate it if needed
            if let Err(error) = self.check_subscription_health(
                connected_peers_and_metadata,
                subscription_peer,
                skip_peer_optimality_check,
            ) {
                // Log the subscription termination error
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Terminating subscription to peer: {:?}! Termination reason: {:?}",
                        subscription_peer, error
                    ))
                );

                // Unsubscribe from the peer and remove the subscription
                self.unsubscribe_from_peer(subscription_peer);

                // Add the peer to the list of terminated subscriptions
                terminated_subscriptions.push((subscription_peer, error));
            }
        }

        terminated_subscriptions
    }
```

**File:** consensus/src/consensus_observer/observer/subscription_manager.rs (L307-359)
```rust
    /// Unsubscribes from the given peer by sending an unsubscribe request
    fn unsubscribe_from_peer(&mut self, peer_network_id: PeerNetworkId) {
        // Remove the peer from the active subscriptions
        self.active_observer_subscriptions
            .lock()
            .remove(&peer_network_id);

        // Send an unsubscribe request to the peer and process the response.
        // Note: we execute this asynchronously, as we don't need to wait for the response.
        let consensus_observer_client = self.consensus_observer_client.clone();
        let consensus_observer_config = self.consensus_observer_config;
        tokio::spawn(async move {
            // Send the unsubscribe request to the peer
            let unsubscribe_request = ConsensusObserverRequest::Unsubscribe;
            let response = consensus_observer_client
                .send_rpc_request_to_peer(
                    &peer_network_id,
                    unsubscribe_request,
                    consensus_observer_config.network_request_timeout_ms,
                )
                .await;

            // Process the response
            match response {
                Ok(ConsensusObserverResponse::UnsubscribeAck) => {
                    info!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Successfully unsubscribed from peer: {}!",
                            peer_network_id
                        ))
                    );
                },
                Ok(response) => {
                    // We received an invalid response
                    warn!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Got unexpected response type: {:?}",
                            response.get_label()
                        ))
                    );
                },
                Err(error) => {
                    // We encountered an error while sending the request
                    warn!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Failed to send unsubscribe request to peer: {}! Error: {:?}",
                            peer_network_id, error
                        ))
                    );
                },
            }
        });
    }
```

**File:** consensus/src/consensus_observer/observer/subscription_manager.rs (L361-386)
```rust
    /// Verifies that the message is from an active
    /// subscription. If not, an error is returned.
    pub fn verify_message_for_subscription(
        &mut self,
        message_sender: PeerNetworkId,
    ) -> Result<(), Error> {
        // Check if the message is from an active subscription
        if let Some(active_subscription) = self
            .active_observer_subscriptions
            .lock()
            .get_mut(&message_sender)
        {
            // Update the last message receive time and return early
            active_subscription.update_last_message_receive_time();
            return Ok(());
        }

        // Otherwise, the message is not from an active subscription.
        // Send another unsubscribe request, and return an error.
        self.unsubscribe_from_peer(message_sender);
        Err(Error::InvalidMessageError(format!(
            "Received message from unexpected peer, and not an active subscription: {}!",
            message_sender
        )))
    }
}
```

**File:** consensus/src/consensus_observer/network/network_handler.rs (L93-98)
```rust
        // Create a channel for sending consensus observer messages
        let (observer_message_sender, observer_message_receiver) = aptos_channel::new(
            QueueStyle::FIFO,
            consensus_observer_config.max_network_channel_size as usize,
            None,
        );
```

**File:** consensus/src/consensus_observer/network/network_handler.rs (L182-190)
```rust
        // Send the message to the consensus observer
        if let Err(error) = self.observer_message_sender.push((), network_message) {
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to forward the observer message to the consensus observer! Error: {:?}",
                    error
                ))
            );
        }
```
