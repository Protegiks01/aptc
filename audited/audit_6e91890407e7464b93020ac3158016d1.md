# Audit Report

## Title
Race Condition in Persisted State Updates Causing State-Summary Desynchronization

## Summary
A race condition exists in the state commit pipeline where `persisted_state.set()` updates the state summary synchronously but commits the state asynchronously via a background thread. This creates a window where concurrent `get_state_summary()` calls observe a newer summary version while `get_state()` returns an older state version, violating state consistency invariants.

## Finding Description

The vulnerability occurs in the interaction between `StateMerkleBatchCommitter::run()` and `PersistedState::set()`. [1](#0-0) 

When the batch committer processes state merkle commits, it first commits merkle nodes to the database, then calls `persisted_state.set(snapshot)`. [2](#0-1) 

The critical issue is in the `set()` method's implementation:
1. Line 59 immediately updates the summary: `*self.summary.lock() = summary` (synchronous)
2. Line 61 enqueues the state commit: `self.hot_state.enqueue_commit(state)` (asynchronous)

The hot state update happens in a background committer thread: [3](#0-2) 

Between the summary update (immediate) and the state commit (delayed), there's a race window where:
- `get_state_summary()` returns the NEW summary at version V2
- `get_state()` returns the OLD state at version V1 [4](#0-3) 

The execution pipeline can observe this inconsistency when creating state views and computing state checkpoints: [5](#0-4) 

Later, during ledger update: [6](#0-5) 

If the batch committer updates persisted state between these two operations, the execution observes mismatched versions, violating the invariant enforced by `assert_versions_match()`: [7](#0-6) 

The comment in the code acknowledges ordering is critical: [8](#0-7) 

However, this comment addresses the order WITHIN `set()` (summary before state), but doesn't account for the asynchronous delay in state commit completion.

## Impact Explanation

This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria:

1. **Validator node crashes**: The race can cause assertion failures in `StateSummary::update()` when `persisted.next_version() > self.next_version()`, leading to validator node panics.

2. **State inconsistencies**: During the race window, execution pipeline operations observe partially committed state where summary claims version V2 but state remains at V1, violating the atomic state transition invariant.

3. **Consensus safety risk**: If validators experience timing differences in when they observe the race, they could produce different state roots, violating deterministic execution requirements.

While not directly exploitable by unprivileged attackers, this race occurs during normal operation and can cause validator instability, potentially affecting network liveness.

## Likelihood Explanation

**High Likelihood** - This race occurs during every state commit operation when:
- The batch committer processes new snapshots (frequent operation)
- Execution pipeline concurrently accesses persisted state (continuous operation)
- The hot state committer has processing delay (depends on system load)

The race window is small but occurs repeatedly during normal blockchain operation. Under high transaction throughput, the likelihood of hitting this race increases significantly.

## Recommendation

Synchronize the summary and state updates to ensure atomicity. Replace the asynchronous `enqueue_commit()` with a synchronous blocking operation, or delay the summary update until after state commit completes:

```rust
pub fn set(&self, persisted: StateWithSummary) {
    let (state, summary) = persisted.into_inner();
    
    // First, commit the state synchronously
    self.hot_state.commit_blocking(state);
    
    // Then update the summary
    *self.summary.lock() = summary;
}
```

Alternatively, implement a double-check locking pattern where `get_state_summary()` verifies the hot state has committed before returning:

```rust
pub fn get_state_summary(&self) -> StateSummary {
    let summary = self.summary.lock().clone();
    // Ensure hot state is committed to at least the summary's version
    self.hot_state.wait_for_version(summary.version());
    summary
}
```

## Proof of Concept

```rust
// Test demonstrating the race condition
#[test]
fn test_persisted_state_race_condition() {
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    let persisted_state = PersistedState::new_empty(HotStateConfig::default());
    let persisted_state_1 = persisted_state.clone();
    let persisted_state_2 = persisted_state.clone();
    let barrier = Arc::new(Barrier::new(2));
    let barrier_1 = barrier.clone();
    let barrier_2 = barrier.clone();
    
    // Thread 1: Update persisted state
    let handle1 = thread::spawn(move || {
        let snapshot = create_test_snapshot_at_version(2);
        barrier_1.wait();
        persisted_state_1.set(snapshot);
    });
    
    // Thread 2: Read state and summary concurrently
    let handle2 = thread::spawn(move || {
        barrier_2.wait();
        thread::sleep(Duration::from_micros(100)); // Hit the race window
        let summary = persisted_state_2.get_state_summary();
        let (_, state) = persisted_state_2.get_state();
        
        // Assert should fail if race occurred
        assert_eq!(summary.version(), state.version(), 
            "Race detected: summary version {} != state version {}", 
            summary.version().unwrap(), state.version().unwrap());
    });
    
    handle1.join().unwrap();
    handle2.join().unwrap();
}
```

## Notes

This race condition violates the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs". The asynchronous state commit breaks atomicity, creating a window where the persisted state is in an inconsistent intermediate state observable by concurrent readers.

### Citations

**File:** storage/aptosdb/src/state_store/state_merkle_batch_committer.rs (L52-115)
```rust
    pub fn run(self) {
        while let Ok(msg) = self.state_merkle_batch_receiver.recv() {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["batch_committer_work"]);
            match msg {
                CommitMessage::Data(StateMerkleCommit {
                    snapshot,
                    hot_batch,
                    cold_batch,
                }) => {
                    let base_version = self.persisted_state.get_state_summary().version();
                    let current_version = snapshot
                        .version()
                        .expect("Current version should not be None");

                    // commit jellyfish merkle nodes
                    let _timer =
                        OTHER_TIMERS_SECONDS.timer_with(&["commit_jellyfish_merkle_nodes"]);
                    if let Some(hot_state_merkle_batch) = hot_batch {
                        self.commit(
                            self.state_db
                                .hot_state_merkle_db
                                .as_ref()
                                .expect("Hot state merkle db must exist."),
                            current_version,
                            hot_state_merkle_batch,
                        )
                        .expect("Hot state merkle nodes commit failed.");
                    }
                    self.commit(&self.state_db.state_merkle_db, current_version, cold_batch)
                        .expect("State merkle nodes commit failed.");

                    info!(
                        version = current_version,
                        base_version = base_version,
                        root_hash = snapshot.summary().root_hash(),
                        hot_root_hash = snapshot.summary().hot_root_hash(),
                        "State snapshot committed."
                    );
                    LATEST_SNAPSHOT_VERSION.set(current_version as i64);
                    // TODO(HotState): no pruning for hot state right now, since we always reset it
                    // upon restart.
                    self.state_db
                        .state_merkle_pruner
                        .maybe_set_pruner_target_db_version(current_version);
                    self.state_db
                        .epoch_snapshot_pruner
                        .maybe_set_pruner_target_db_version(current_version);

                    self.check_usage_consistency(&snapshot).unwrap();

                    snapshot
                        .summary()
                        .global_state_summary
                        .log_generation("buffered_state_commit");
                    self.persisted_state.set(snapshot);
                },
                CommitMessage::Sync(finish_sender) => finish_sender.send(()).unwrap(),
                CommitMessage::Exit => {
                    break;
                },
            }
        }
        trace!("State merkle batch committing thread exit.")
    }
```

**File:** storage/aptosdb/src/state_store/persisted_state.rs (L31-48)
```rust
    pub fn get_state_summary(&self) -> StateSummary {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["get_persisted_state_summary"]);

        // The back pressure is on the getting side (which is the execution side) so that it's less
        // likely for a lot of blocks locking the same old base SMT.
        SUBTREE_DROPPER.wait_for_backlog_drop(Self::MAX_PENDING_DROPS);

        self.summary.lock().clone()
    }

    #[cfg(test)]
    pub fn get_hot_state(&self) -> Arc<HotState> {
        Arc::clone(&self.hot_state)
    }

    pub fn get_state(&self) -> (Arc<dyn HotStateView>, State) {
        self.hot_state.get_committed()
    }
```

**File:** storage/aptosdb/src/state_store/persisted_state.rs (L50-62)
```rust
    pub fn set(&self, persisted: StateWithSummary) {
        let (state, summary) = persisted.into_inner();

        // n.b. Summary must be updated before committing the hot state, otherwise in the execution
        // pipeline we risk having a state generated based on a persisted version (v2) that's newer
        // than that of the summary (v1). That causes issue down the line where we commit the diffs
        // between a later snapshot (v3) and a persisted snapshot (v1) to the JMT, at which point
        // we will not be able to calculate the difference (v1 - v3) because the state links only
        // to as far as v2 (code will panic)
        *self.summary.lock() = summary;

        self.hot_state.enqueue_commit(state);
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L192-205)
```rust
    fn run(&mut self) {
        info!("HotState committer thread started.");

        while let Some(to_commit) = self.next_to_commit() {
            self.commit(&to_commit);
            *self.committed.lock() = to_commit;

            GAUGE.set_with(&["hot_state_items"], self.base.len() as i64);
            GAUGE.set_with(&["hot_state_key_bytes"], self.total_key_bytes as i64);
            GAUGE.set_with(&["hot_state_value_bytes"], self.total_value_bytes as i64);
        }

        info!("HotState committer quitting.");
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L226-233)
```rust
                let state_view = {
                    let _timer = OTHER_TIMERS.timer_with(&["get_state_view"]);
                    CachedStateView::new(
                        StateViewId::BlockExecution { block_id },
                        Arc::clone(&self.db.reader),
                        parent_output.result_state().latest().clone(),
                    )?
                };
```

**File:** execution/executor/src/block_executor/mod.rs (L315-320)
```rust
                output.set_state_checkpoint_output(DoStateCheckpoint::run(
                    &output.execution_output,
                    parent_block.output.ensure_result_state_summary()?,
                    &ProvableStateSummary::new_persisted(self.db.reader.as_ref())?,
                    None,
                )?);
```

**File:** storage/storage-interface/src/state_store/state_summary.rs (L204-210)
```rust
    pub fn assert_versions_match(&self, state: &LedgerState) {
        assert_eq!(self.next_version(), state.next_version());
        assert_eq!(
            self.last_checkpoint.next_version(),
            state.last_checkpoint().next_version()
        );
    }
```
