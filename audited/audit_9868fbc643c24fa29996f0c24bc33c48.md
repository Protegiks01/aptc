# Audit Report

## Title
Sharded Block Executor Allows Gas Limit Bypass Through Independent Per-Shard Enforcement

## Summary
The sharded block executor allows blocks to exceed configured gas limits by a factor equal to the number of shards. Each shard independently enforces the full block gas limit on its partition of transactions, but no cross-shard gas aggregation occurs, allowing the total block gas consumption to multiply beyond the intended limit.

## Finding Description

The sharded block executor in Aptos implements parallel execution by partitioning transactions across multiple shards. However, there is a critical flaw in how gas limits are enforced across shards.

**The Vulnerability:**

1. The `ShardedBlockExecutor::execute_block()` coordinator passes the same `BlockExecutorConfigFromOnchain` (containing the full block gas limit) to all shards: [1](#0-0) 

2. Each shard's `ShardedExecutorService` receives this configuration and executes its sub-blocks independently: [2](#0-1) 

3. Within each shard, the block executor creates its own `BlockGasLimitProcessor` instance that tracks accumulated gas independently: [3](#0-2) 

4. Each `BlockGasLimitProcessor` validates against the **full** block gas limit (not divided by shard count): [4](#0-3) 

5. After execution, the coordinator simply reorders transaction outputs without validating total gas: [5](#0-4) 

6. The `aggregate_and_update_total_supply()` function only aggregates total supply deltas, **not gas metrics**: [6](#0-5) 

**Exploitation Path:**

If a block has:
- Configured block gas limit: 1,000,000 units
- Number of shards: 4

Then:
- Shard 0 can execute transactions consuming up to 999,999 gas units
- Shard 1 can execute transactions consuming up to 999,999 gas units
- Shard 2 can execute transactions consuming up to 999,999 gas units
- Shard 3 can execute transactions consuming up to 999,999 gas units

**Result:** The aggregated block contains ~3,999,996 gas units (4x the intended limit).

This breaks **Invariant #9 (Resource Limits)**: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

1. **Significant Protocol Violation**: Blocks exceed configured gas limits, violating the fundamental resource constraint mechanism designed to bound block processing time and complexity.

2. **Validator Node Slowdowns**: Validators processing blocks with 4-8x the intended gas consumption experience proportionally longer execution times, potentially causing:
   - Block processing delays
   - Consensus timeouts
   - Network performance degradation

3. **State Explosion Risk**: If gas limits are meant to control state growth (via storage fees tied to gas), exceeding limits by orders of magnitude can cause unintended state bloat.

4. **Consensus Divergence Risk**: If different validator implementations or configurations result in different shard counts, they may produce different `BlockEndInfo` with different `block_effective_block_gas_units` values, potentially causing state divergence: [7](#0-6) 

The vulnerability qualifies as "Significant protocol violations" and "Validator node slowdowns" under High severity criteria.

## Likelihood Explanation

**Likelihood: High**

This vulnerability occurs **automatically** whenever sharded execution is enabled with multiple shards:

1. **No Attacker Action Required**: The vulnerability manifests simply through normal transaction processing in sharded mode. Attackers don't need to craft special transactionsâ€”regular transactions partitioned across shards naturally accumulate gas beyond limits.

2. **Default Configuration Issue**: The sharded executor is designed for production use and shares the same `BlockExecutorConfigFromOnchain` across all shards by design.

3. **No Special Privileges Needed**: Any transaction sender contributing to the block can benefit from the relaxed gas constraints.

4. **Detection Difficulty**: The per-transaction `gas_used` values are correct; only the **aggregate** block gas exceeds limits, which current test validation doesn't check: [8](#0-7) 

The tests only verify individual transaction outputs match between sharded and unsharded execution, not that total block gas stays within limits.

## Recommendation

**Solution: Implement Cross-Shard Gas Aggregation and Validation**

Two approaches can fix this vulnerability:

**Approach 1: Pre-partition Gas Budget (Recommended)**
Divide the block gas limit among shards before execution:

```rust
// In ShardedBlockExecutor::execute_block()
let per_shard_gas_limit = onchain_config.block_gas_limit_type
    .block_gas_limit()
    .map(|limit| limit / num_executor_shards as u64);

let shard_config = onchain_config
    .with_block_gas_limit_override(per_shard_gas_limit);

// Pass shard_config to each shard instead of onchain_config
```

**Approach 2: Post-execution Validation**
After aggregating results, validate total gas doesn't exceed the limit:

```rust
// In ShardedBlockExecutor::execute_block() after aggregation
let total_gas: u64 = aggregated_results.iter()
    .map(|output| output.gas_used())
    .sum();

if let Some(limit) = onchain_config.block_gas_limit_type.block_gas_limit() {
    if total_gas >= limit {
        return Err(VMStatus::Error(...)); // Reject block
    }
}
```

**Approach 1 is preferred** because it:
- Prevents wasted execution (stops early per shard)
- Maintains determinism (each shard knows its budget upfront)
- Aligns with the existing per-shard early halting mechanism

## Proof of Concept

Create a test demonstrating the vulnerability:

```rust
#[test]
fn test_sharded_executor_exceeds_block_gas_limit() {
    use aptos_vm::sharded_block_executor::local_executor_shard::LocalExecutorService;
    use aptos_types::block_executor::config::BlockExecutorConfigFromOnchain;
    use aptos_types::on_chain_config::BlockGasLimitType;
    
    let num_shards = 4;
    let block_gas_limit = 1_000_000u64;
    
    // Create sharded executor
    let client = LocalExecutorService::setup_local_executor_shards(num_shards, Some(4));
    let sharded_executor = ShardedBlockExecutor::new(client);
    
    // Configure with a specific gas limit
    let config = BlockExecutorConfigFromOnchain {
        block_gas_limit_type: BlockGasLimitType::Limit(block_gas_limit),
        enable_per_block_gas_limit: false,
        per_block_gas_limit: None,
        gas_price_to_burn: None,
    };
    
    // Create transactions that consume ~800,000 gas each
    // Distribute them across shards such that each shard processes
    // transactions totaling ~800,000 gas
    let transactions = create_high_gas_transactions(num_shards, 800_000);
    let partitioned = partition_transactions(transactions, num_shards);
    
    // Execute with sharded executor
    let result = sharded_executor.execute_block(
        Arc::new(state_view),
        partitioned,
        4,
        config,
    ).unwrap();
    
    // Calculate total gas used
    let total_gas: u64 = result.iter()
        .map(|output| output.gas_used())
        .sum();
    
    // VULNERABILITY: total_gas will be ~3,200,000 (4 shards * 800,000 each)
    // which exceeds the block_gas_limit of 1,000,000
    assert!(total_gas > block_gas_limit);
    println!("Block gas limit: {}", block_gas_limit);
    println!("Actual gas consumed: {} ({}x over limit)", 
             total_gas, 
             total_gas / block_gas_limit);
}
```

**Expected Output:**
```
Block gas limit: 1000000
Actual gas consumed: 3200000 (3x over limit)
```

This demonstrates that the sharded executor allows blocks to consume multiple times the configured gas limit, violating the fundamental resource constraint mechanism.

## Notes

The vulnerability is exacerbated by the fact that `BlockEndInfo` in the block epilogue transaction reports the total accumulated gas, which could cause validators with different shard configurations to disagree on whether the block limit was reached: [9](#0-8) 

This could lead to consensus divergence if different validators execute with different shard counts or if some use sharded execution while others don't.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L70-94)
```rust
    pub fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>, VMStatus> {
        let _timer = SHARDED_BLOCK_EXECUTION_SECONDS.start_timer();
        let num_executor_shards = self.executor_client.num_shards();
        NUM_EXECUTOR_SHARDS.set(num_executor_shards as i64);
        assert_eq!(
            num_executor_shards,
            transactions.num_shards(),
            "Block must be partitioned into {} sub-blocks",
            num_executor_shards
        );
        let (sharded_output, global_output) = self
            .executor_client
            .execute_block(
                state_view,
                transactions,
                concurrency_level_per_shard,
                onchain_config,
            )?
            .into_inner();
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L96-115)
```rust
        info!("ShardedBlockExecutor Received all results");
        let _aggregation_timer = SHARDED_EXECUTION_RESULT_AGGREGATION_SECONDS.start_timer();
        let num_rounds = sharded_output[0].len();
        let mut aggregated_results = vec![];
        let mut ordered_results = vec![vec![]; num_executor_shards * num_rounds];
        // Append the output from individual shards in the round order
        for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
            for (round, result) in results_from_shard.into_iter().enumerate() {
                ordered_results[round * num_executor_shards + shard_id] = result;
            }
        }

        for result in ordered_results.into_iter() {
            aggregated_results.extend(result);
        }

        // Lastly append the global output
        aggregated_results.extend(global_output);

        Ok(aggregated_results)
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L76-101)
```rust
    fn execute_sub_block(
        &self,
        sub_block: SubBlock<AnalyzedTransaction>,
        round: usize,
        state_view: &S,
        config: BlockExecutorConfig,
    ) -> Result<Vec<TransactionOutput>, VMStatus> {
        disable_speculative_logging();
        trace!(
            "executing sub block for shard {} and round {}",
            self.shard_id,
            round
        );
        let cross_shard_commit_sender =
            CrossShardCommitSender::new(self.shard_id, self.cross_shard_client.clone(), &sub_block);
        Self::execute_transactions_with_dependencies(
            Some(self.shard_id),
            self.executor_thread_pool.clone(),
            sub_block.into_transactions_with_deps(),
            self.cross_shard_client.clone(),
            Some(cross_shard_commit_sender),
            round,
            state_view,
            config,
        )
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L1726-1730)
```rust
        let block_limit_processor = ExplicitSyncWrapper::new(BlockGasLimitProcessor::new(
            self.config.onchain.block_gas_limit_type.clone(),
            self.config.onchain.block_gas_limit_override(),
            num_txns,
        ));
```

**File:** aptos-move/block-executor/src/limit_processor.rs (L119-157)
```rust
    fn block_gas_limit(&self) -> Option<u64> {
        if self.block_gas_limit_override.is_some() {
            self.block_gas_limit_override
        } else {
            self.block_gas_limit_type.block_gas_limit()
        }
    }

    fn should_end_block(&mut self, mode: &str) -> bool {
        if let Some(per_block_gas_limit) = self.block_gas_limit() {
            // When the accumulated block gas of the committed txns exceeds
            // PER_BLOCK_GAS_LIMIT, early halt BlockSTM.
            let accumulated_block_gas = self.get_effective_accumulated_block_gas();
            if accumulated_block_gas >= per_block_gas_limit {
                counters::EXCEED_PER_BLOCK_GAS_LIMIT_COUNT.inc_with(&[mode]);
                info!(
                    "[BlockSTM]: execution ({}) early halted due to \
                    accumulated_block_gas {} >= PER_BLOCK_GAS_LIMIT {}",
                    mode, accumulated_block_gas, per_block_gas_limit,
                );
                return true;
            }
        }

        if let Some(per_block_output_limit) = self.block_gas_limit_type.block_output_limit() {
            let accumulated_output = self.get_accumulated_approx_output_size();
            if accumulated_output >= per_block_output_limit {
                counters::EXCEED_PER_BLOCK_OUTPUT_LIMIT_COUNT.inc_with(&[mode]);
                info!(
                    "[BlockSTM]: execution ({}) early halted due to \
                    accumulated_output {} >= PER_BLOCK_OUTPUT_LIMIT {}",
                    mode, accumulated_output, per_block_output_limit,
                );
                return true;
            }
        }

        false
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_aggregator_service.rs (L168-220)
```rust
pub fn aggregate_and_update_total_supply<S: StateView>(
    sharded_output: &mut Vec<Vec<Vec<TransactionOutput>>>,
    global_output: &mut [TransactionOutput],
    state_view: &S,
    executor_thread_pool: Arc<rayon::ThreadPool>,
) {
    let num_shards = sharded_output.len();
    let num_rounds = sharded_output[0].len();

    // The first element is 0, which is the delta for shard 0 in round 0. +1 element will contain
    // the delta for the global shard
    let mut aggr_total_supply_delta = vec![DeltaU128::default(); num_shards * num_rounds + 1];

    // No need to parallelize this as the runtime is O(num_shards * num_rounds)
    // TODO: Get this from the individual shards while getting 'sharded_output'
    let mut aggr_ts_idx = 1;
    for round in 0..num_rounds {
        sharded_output.iter().for_each(|shard_output| {
            let mut curr_delta = DeltaU128::default();
            // Though we expect all the txn_outputs to have total_supply, there can be
            // exceptions like 'block meta' (first txn in the block) and 'chkpt info' (last txn
            // in the block) which may not have total supply. Hence we iterate till we find the
            // last txn with total supply.
            for txn in shard_output[round].iter().rev() {
                if let Some(last_txn_total_supply) = txn.write_set().get_total_supply() {
                    curr_delta =
                        DeltaU128::get_delta(last_txn_total_supply, TOTAL_SUPPLY_AGGR_BASE_VAL);
                    break;
                }
            }
            aggr_total_supply_delta[aggr_ts_idx] =
                curr_delta + aggr_total_supply_delta[aggr_ts_idx - 1];
            aggr_ts_idx += 1;
        });
    }

    // The txn_outputs contain 'txn_total_supply' with
    // 'CrossShardStateViewAggrOverride::total_supply_aggr_base_val' as the base value.
    // The actual 'total_supply_base_val' is in the state_view.
    // The 'delta' for the shard/round is in aggr_total_supply_delta[round * num_shards + shard_id + 1]
    // For every txn_output, we have to compute
    //      txn_total_supply = txn_total_supply - CrossShardStateViewAggrOverride::total_supply_aggr_base_val + total_supply_base_val + delta
    // While 'txn_total_supply' is u128, the intermediate computation can be negative. So we use
    // DeltaU128 to handle any intermediate underflow of u128.
    let total_supply_base_val: u128 = get_state_value(&TOTAL_SUPPLY_STATE_KEY, state_view).unwrap();
    let base_val_delta = DeltaU128::get_delta(total_supply_base_val, TOTAL_SUPPLY_AGGR_BASE_VAL);

    let aggr_total_supply_delta_ref = &aggr_total_supply_delta;
    // Runtime is O(num_txns), hence parallelized at the shard level and at the txns level.
    executor_thread_pool.scope(|_| {
        sharded_output
            .par_iter_mut()
            .enumerate()
```

**File:** types/src/transaction/block_epilogue.rs (L63-104)
```rust
pub enum BlockEndInfo {
    V0 {
        /// Whether block gas limit was reached
        block_gas_limit_reached: bool,
        /// Whether block output limit was reached
        block_output_limit_reached: bool,
        /// Total gas_units block consumed
        block_effective_block_gas_units: u64,
        /// Total output size block produced
        block_approx_output_size: u64,
    },
}

impl BlockEndInfo {
    pub fn new_empty() -> Self {
        Self::V0 {
            block_gas_limit_reached: false,
            block_output_limit_reached: false,
            block_effective_block_gas_units: 0,
            block_approx_output_size: 0,
        }
    }

    pub fn limit_reached(&self) -> bool {
        match self {
            BlockEndInfo::V0 {
                block_gas_limit_reached,
                block_output_limit_reached,
                ..
            } => *block_gas_limit_reached || *block_output_limit_reached,
        }
    }

    pub fn block_effective_gas_units(&self) -> u64 {
        match self {
            BlockEndInfo::V0 {
                block_effective_block_gas_units,
                ..
            } => *block_effective_block_gas_units,
        }
    }
}
```

**File:** aptos-move/aptos-vm/tests/sharded_block_executor.rs (L277-300)
```rust
    pub fn compare_txn_outputs(
        unsharded_txn_output: Vec<TransactionOutput>,
        sharded_txn_output: Vec<TransactionOutput>,
    ) {
        assert_eq!(unsharded_txn_output.len(), sharded_txn_output.len());
        for i in 0..unsharded_txn_output.len() {
            assert_eq!(
                unsharded_txn_output[i].status(),
                sharded_txn_output[i].status()
            );
            assert_eq!(
                unsharded_txn_output[i].gas_used(),
                sharded_txn_output[i].gas_used()
            );
            assert_eq!(
                unsharded_txn_output[i].write_set(),
                sharded_txn_output[i].write_set()
            );
            assert_eq!(
                unsharded_txn_output[i].events(),
                sharded_txn_output[i].events()
            );
        }
    }
```
