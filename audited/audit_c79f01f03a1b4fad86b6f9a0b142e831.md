# Audit Report

## Title
Aggregate Deserialization Task Limit Bypass Enables Validator DoS Through Multi-Protocol Message Flooding

## Summary
The `max_parallel_deserialization_tasks` configuration is applied independently to each registered network service, allowing the aggregate deserialization capacity across all services to significantly exceed the underlying blocking thread pool capacity (64 threads). An attacker can exploit this by sending messages to multiple protocol endpoints simultaneously, causing blocking thread pool exhaustion and validator node slowdown.

## Finding Description

The network builder's `add_client_and_service()` function registers multiple independent services (consensus, DKG, JWK consensus, mempool, peer monitoring, storage service, etc.), each receiving its own deserialization task buffer limited by `max_parallel_deserialization_tasks`. [1](#0-0) 

Each service receives the same `max_parallel_deserialization_tasks` value from the network configuration, which defaults to the number of CPU cores: [2](#0-1) 

In practice, a validator node registers 6-7 services on its validator network: [3](#0-2) 

Each service creates an independent `NetworkEvents` stream with its own buffering for parallel deserialization tasks: [4](#0-3) 

All deserialization tasks use `tokio::task::spawn_blocking()`, which shares a global blocking thread pool limited to 64 threads: [5](#0-4) 

**The Vulnerability**: With default configuration on a 16-core machine:
- Each service gets 16 parallel deserialization tasks
- 7 services × 16 tasks = **112 concurrent tasks requested**
- Only 64 blocking threads available
- **48+ tasks will queue/contend**, causing delays across ALL services

An attacker can exploit this by connecting to the validator and sending large serialized messages to multiple protocol endpoints simultaneously. The peer routing logic forwards messages based on `protocol_id` without aggregate rate limiting: [6](#0-5) 

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The aggregate deserialization capacity exceeds the system's actual thread pool capacity, enabling resource exhaustion.

## Impact Explanation

This vulnerability enables **High Severity** impact per the Aptos Bug Bounty program:

**Validator Node Slowdowns**: An attacker flooding multiple protocol endpoints causes deserialization backlog across all services, including consensus. This degrades consensus message processing, leading to validator performance degradation and potential missed block proposals.

The attack requires no special privileges—any network peer can send messages to public protocol endpoints (mempool, peer monitoring, storage service). Validator networks with mutual authentication still accept connections from other validators, and compromised validators can exploit this.

## Likelihood Explanation

**Likelihood: High**

- **Attack Requirements**: Network connectivity to the target validator (either as another validator with mutual auth, or via VFN endpoint for fullnodes)
- **Complexity**: Low - simply send large messages to multiple `protocol_id` endpoints
- **Detection Difficulty**: Medium - appears as legitimate traffic across multiple services
- **Default Configuration Vulnerable**: Yes - all nodes using default `max_parallel_deserialization_tasks = num_cpus::get()`

The vulnerability is more severe on high-core-count machines (32-64 cores) where the aggregate can reach 224-448 requested tasks competing for 64 threads.

## Recommendation

Implement a **global aggregate limit** on total deserialization tasks across all services. Options:

**Option 1: Global Semaphore**
```rust
// In NetworkBuilder
pub struct NetworkBuilder {
    // ... existing fields ...
    global_deserialization_semaphore: Option<Arc<tokio::sync::Semaphore>>,
}

// Initialize in create()
let global_limit = config.max_parallel_deserialization_tasks.unwrap_or(num_cpus::get());
network_builder.global_deserialization_semaphore = Some(Arc::new(tokio::sync::Semaphore::new(global_limit)));

// Pass to each service
fn add_service<EventsT: NewNetworkEvents>(
    &mut self,
    config: &NetworkServiceConfig,
    max_parallel_deserialization_tasks: Option<usize>,
    allow_out_of_order_delivery: bool,
) -> EventsT {
    let peer_mgr_reqs_rx = self.peer_manager_builder.add_service(config);
    EventsT::new(
        peer_mgr_reqs_rx,
        max_parallel_deserialization_tasks,
        allow_out_of_order_delivery,
        self.global_deserialization_semaphore.clone(), // NEW
    )
}
```

**Option 2: Reduce Per-Service Limits**
```rust
fn configure_num_deserialization_tasks(&mut self) {
    if self.max_parallel_deserialization_tasks.is_none() {
        // Divide by expected service count to prevent aggregate overflow
        let estimated_services = 7; // consensus, dkg, jwk, mempool, peer mon, storage, netbench
        self.max_parallel_deserialization_tasks = Some(num_cpus::get() / estimated_services);
    }
}
```

**Option 3: Cap at Blocking Thread Pool Limit**
```rust
const MAX_BLOCKING_THREADS: usize = 64;
const SAFETY_MARGIN: f64 = 0.8; // Leave 20% for other spawn_blocking usage

fn configure_num_deserialization_tasks(&mut self) {
    if self.max_parallel_deserialization_tasks.is_none() {
        let max_safe = ((MAX_BLOCKING_THREADS as f64) * SAFETY_MARGIN) as usize;
        let per_cpu = num_cpus::get();
        self.max_parallel_deserialization_tasks = Some(std::cmp::min(max_safe, per_cpu));
    }
}
```

## Proof of Concept

```rust
// Reproduction test demonstrating aggregate overflow
// File: network/framework/src/protocols/network/mod_test.rs

#[tokio::test]
async fn test_aggregate_deserialization_overflow() {
    use std::sync::Arc;
    use std::sync::atomic::{AtomicUsize, Ordering};
    use tokio::time::{sleep, Duration};
    
    const NUM_SERVICES: usize = 7;
    const TASKS_PER_SERVICE: usize = 16;
    const BLOCKING_POOL_LIMIT: usize = 64;
    
    let active_tasks = Arc::new(AtomicUsize::new(0));
    let max_concurrent = Arc::new(AtomicUsize::new(0));
    
    // Simulate 7 services each spawning 16 blocking tasks
    let mut handles = vec![];
    for service_id in 0..NUM_SERVICES {
        for task_id in 0..TASKS_PER_SERVICE {
            let active = active_tasks.clone();
            let max_conc = max_concurrent.clone();
            
            let handle = tokio::task::spawn_blocking(move || {
                let current = active.fetch_add(1, Ordering::SeqCst) + 1;
                max_conc.fetch_max(current, Ordering::SeqCst);
                
                // Simulate deserialization work
                std::thread::sleep(Duration::from_millis(100));
                
                active.fetch_sub(1, Ordering::SeqCst);
            });
            
            handles.push(handle);
        }
    }
    
    // Wait for all to complete
    for handle in handles {
        handle.await.unwrap();
    }
    
    let max_observed = max_concurrent.load(Ordering::SeqCst);
    println!("Requested concurrent tasks: {}", NUM_SERVICES * TASKS_PER_SERVICE);
    println!("Blocking pool limit: {}", BLOCKING_POOL_LIMIT);
    println!("Max concurrent observed: {}", max_observed);
    
    // Demonstrates that we requested more than available
    assert!(NUM_SERVICES * TASKS_PER_SERVICE > BLOCKING_POOL_LIMIT);
    assert!(max_observed <= BLOCKING_POOL_LIMIT);
    println!("VULNERABILITY CONFIRMED: {} tasks queued/delayed", 
             NUM_SERVICES * TASKS_PER_SERVICE - BLOCKING_POOL_LIMIT);
}
```

**Attack Script** (conceptual):
```rust
// Malicious peer sends large messages to multiple protocols simultaneously
async fn exploit_deserialization_overflow(validator_address: NetworkAddress) {
    let connection = connect_to_validator(validator_address).await;
    
    // Flood multiple protocol endpoints concurrently
    tokio::join!(
        send_large_consensus_messages(&connection, 100),
        send_large_mempool_transactions(&connection, 100),
        send_storage_rpc_requests(&connection, 100),
        send_peer_monitoring_requests(&connection, 100),
        send_dkg_messages(&connection, 100),
    );
    
    // Result: 500+ messages across 5 protocols
    // Each protocol tries to deserialize 16 in parallel
    // Total: 80 concurrent deserialization tasks
    // Available: 64 blocking threads
    // Impact: 16+ tasks queued, ALL services experience delays
}
```

**Notes**:
- The vulnerability exists even though all services receive the **same** `max_parallel_deserialization_tasks` value (not different limits as the question premise suggests)
- The core issue is lack of global coordination, allowing aggregate capacity to exceed system resources
- The configuration comment "per application" at [7](#0-6)  suggests this may be by design, but it creates a security vulnerability
- Byte-level rate limiting exists but doesn't prevent spawning excessive deserialization tasks
- This affects both validator and fullnode networks, though validators are higher-value targets

### Citations

**File:** network/builder/src/builder.rs (L431-445)
```rust
    pub fn add_client_and_service<SenderT: NewNetworkSender, EventsT: NewNetworkEvents>(
        &mut self,
        config: &NetworkApplicationConfig,
        max_parallel_deserialization_tasks: Option<usize>,
        allow_out_of_order_delivery: bool,
    ) -> (SenderT, EventsT) {
        (
            self.add_client(&config.network_client_config),
            self.add_service(
                &config.network_service_config,
                max_parallel_deserialization_tasks,
                allow_out_of_order_delivery,
            ),
        )
    }
```

**File:** config/src/config/network_config.rs (L122-123)
```rust
    /// The maximum number of parallel message deserialization tasks that can run (per application)
    pub max_parallel_deserialization_tasks: Option<usize>,
```

**File:** config/src/config/network_config.rs (L178-185)
```rust
    /// Configures the number of parallel deserialization tasks
    /// based on the number of CPU cores of the machine. This is
    /// only done if the config does not specify a value.
    fn configure_num_deserialization_tasks(&mut self) {
        if self.max_parallel_deserialization_tasks.is_none() {
            self.max_parallel_deserialization_tasks = Some(num_cpus::get());
        }
    }
```

**File:** aptos-node/src/network.rs (L292-400)
```rust
        // Register consensus (both client and server) with the network
        let network_id = network_config.network_id;
        if network_id.is_validator_network() {
            // A validator node must have only a single consensus network handle
            if consensus_network_handle.is_some() {
                panic!("There can be at most one validator network!");
            } else {
                let network_handle = register_client_and_service_with_network(
                    &mut network_builder,
                    network_id,
                    &network_config,
                    consensus_network_configuration(node_config),
                    true,
                );
                consensus_network_handle = Some(network_handle);
            }

            if dkg_network_handle.is_some() {
                panic!("There can be at most one validator network!");
            } else {
                let network_handle = register_client_and_service_with_network(
                    &mut network_builder,
                    network_id,
                    &network_config,
                    dkg_network_configuration(node_config),
                    true,
                );
                dkg_network_handle = Some(network_handle);
            }

            if jwk_consensus_network_handle.is_some() {
                panic!("There can be at most one validator network!");
            } else {
                let network_handle = register_client_and_service_with_network(
                    &mut network_builder,
                    network_id,
                    &network_config,
                    jwk_consensus_network_configuration(node_config),
                    true,
                );
                jwk_consensus_network_handle = Some(network_handle);
            }
        }

        // Register consensus observer (both client and server) with the network
        if node_config
            .consensus_observer
            .is_observer_or_publisher_enabled()
        {
            // Create the network handle for this network type
            let network_handle = register_client_and_service_with_network(
                &mut network_builder,
                network_id,
                &network_config,
                consensus_observer_network_configuration(node_config),
                false,
            );

            // Add the network handle to the set of handles
            if let Some(consensus_observer_network_handles) =
                &mut consensus_observer_network_handles
            {
                consensus_observer_network_handles.push(network_handle);
            } else {
                consensus_observer_network_handles = Some(vec![network_handle]);
            }
        }

        // Register mempool (both client and server) with the network
        let mempool_network_handle = register_client_and_service_with_network(
            &mut network_builder,
            network_id,
            &network_config,
            mempool_network_configuration(node_config),
            true,
        );
        mempool_network_handles.push(mempool_network_handle);

        // Register the peer monitoring service (both client and server) with the network
        let peer_monitoring_service_network_handle = register_client_and_service_with_network(
            &mut network_builder,
            network_id,
            &network_config,
            peer_monitoring_network_configuration(node_config),
            true,
        );
        peer_monitoring_service_network_handles.push(peer_monitoring_service_network_handle);

        // Register the storage service (both client and server) with the network
        let storage_service_network_handle = register_client_and_service_with_network(
            &mut network_builder,
            network_id,
            &network_config,
            storage_service_network_configuration(node_config),
            true,
        );
        storage_service_network_handles.push(storage_service_network_handle);

        // Register the network benchmark test service
        if let Some(app_config) = netbench_network_configuration(node_config) {
            let netbench_handle = register_client_and_service_with_network(
                &mut network_builder,
                network_id,
                &network_config,
                app_config,
                true,
            );
            netbench_handles.push(netbench_handle);
        }
```

**File:** network/framework/src/protocols/network/mod.rs (L208-243)
```rust
impl<TMessage: Message + Send + Sync + 'static> NewNetworkEvents for NetworkEvents<TMessage> {
    fn new(
        peer_mgr_notifs_rx: aptos_channel::Receiver<(PeerId, ProtocolId), ReceivedMessage>,
        max_parallel_deserialization_tasks: Option<usize>,
        allow_out_of_order_delivery: bool,
    ) -> Self {
        // Determine the number of parallel deserialization tasks to use
        let max_parallel_deserialization_tasks = max_parallel_deserialization_tasks.unwrap_or(1);

        let data_event_stream = peer_mgr_notifs_rx.map(|notification| {
            tokio::task::spawn_blocking(move || received_message_to_event(notification))
        });

        let data_event_stream: Pin<
            Box<dyn Stream<Item = Event<TMessage>> + Send + Sync + 'static>,
        > = if allow_out_of_order_delivery {
            Box::pin(
                data_event_stream
                    .buffer_unordered(max_parallel_deserialization_tasks)
                    .filter_map(|res| future::ready(res.expect("JoinError from spawn blocking"))),
            )
        } else {
            Box::pin(
                data_event_stream
                    .buffered(max_parallel_deserialization_tasks)
                    .filter_map(|res| future::ready(res.expect("JoinError from spawn blocking"))),
            )
        };

        Self {
            event_stream: data_event_stream,
            done: false,
            _marker: PhantomData,
        }
    }
}
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-51)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
        .enable_all();
```

**File:** network/framework/src/peer/mod.rs (L447-541)
```rust
    fn handle_inbound_network_message(
        &mut self,
        message: NetworkMessage,
    ) -> Result<(), PeerManagerError> {
        match &message {
            NetworkMessage::DirectSendMsg(direct) => {
                let data_len = direct.raw_msg.len();
                network_application_inbound_traffic(
                    self.network_context,
                    direct.protocol_id,
                    data_len as u64,
                );
                match self.upstream_handlers.get(&direct.protocol_id) {
                    None => {
                        counters::direct_send_messages(&self.network_context, UNKNOWN_LABEL).inc();
                        counters::direct_send_bytes(&self.network_context, UNKNOWN_LABEL)
                            .inc_by(data_len as u64);
                    },
                    Some(handler) => {
                        let key = (self.connection_metadata.remote_peer_id, direct.protocol_id);
                        let sender = self.connection_metadata.remote_peer_id;
                        let network_id = self.network_context.network_id();
                        let sender = PeerNetworkId::new(network_id, sender);
                        match handler.push(key, ReceivedMessage::new(message, sender)) {
                            Err(_err) => {
                                // NOTE: aptos_channel never returns other than Ok(()), but we might switch to tokio::sync::mpsc and then this would work
                                counters::direct_send_messages(
                                    &self.network_context,
                                    DECLINED_LABEL,
                                )
                                .inc();
                                counters::direct_send_bytes(&self.network_context, DECLINED_LABEL)
                                    .inc_by(data_len as u64);
                            },
                            Ok(_) => {
                                counters::direct_send_messages(
                                    &self.network_context,
                                    RECEIVED_LABEL,
                                )
                                .inc();
                                counters::direct_send_bytes(&self.network_context, RECEIVED_LABEL)
                                    .inc_by(data_len as u64);
                            },
                        }
                    },
                }
            },
            NetworkMessage::Error(error_msg) => {
                warn!(
                    NetworkSchema::new(&self.network_context)
                        .connection_metadata(&self.connection_metadata),
                    error_msg = ?error_msg,
                    "{} Peer {} sent an error message: {:?}",
                    self.network_context,
                    self.remote_peer_id().short_str(),
                    error_msg,
                );
            },
            NetworkMessage::RpcRequest(request) => {
                match self.upstream_handlers.get(&request.protocol_id) {
                    None => {
                        counters::direct_send_messages(&self.network_context, UNKNOWN_LABEL).inc();
                        counters::direct_send_bytes(&self.network_context, UNKNOWN_LABEL)
                            .inc_by(request.raw_request.len() as u64);
                    },
                    Some(handler) => {
                        let sender = self.connection_metadata.remote_peer_id;
                        let network_id = self.network_context.network_id();
                        let sender = PeerNetworkId::new(network_id, sender);
                        if let Err(err) = self
                            .inbound_rpcs
                            .handle_inbound_request(handler, ReceivedMessage::new(message, sender))
                        {
                            warn!(
                                NetworkSchema::new(&self.network_context)
                                    .connection_metadata(&self.connection_metadata),
                                error = %err,
                                "{} Error handling inbound rpc request: {}",
                                self.network_context,
                                err
                            );
                        }
                    },
                }
            },
            NetworkMessage::RpcResponse(_) => {
                // non-reference cast identical to this match case
                let NetworkMessage::RpcResponse(response) = message else {
                    unreachable!("NetworkMessage type changed between match and let")
                };
                self.outbound_rpcs.handle_inbound_response(response)
            },
        };
        Ok(())
    }
```
