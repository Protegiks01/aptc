# Audit Report

## Title
State-Sync Data Streaming Service Retry Storm Vulnerability Enables DoS Amplification

## Summary
The data streaming service's retry mechanism implements exponential backoff for request timeouts but has no delay between retry attempts. When storage service requests fail, they are immediately retried without any backoff delay, creating a retry storm that can amplify attacks and overwhelm storage service servers, particularly affecting validator and VFN synchronization which lacks rate limiting protection.

## Finding Description

The vulnerability exists in the data streaming service's retry logic. When a storage service request fails, the system immediately retries without any delay between attempts. [1](#0-0) 

The `resend_data_client_request` function increments the failure count and immediately calls `send_client_request(true, ...)` to retry, with no delay mechanism. [2](#0-1) 

The `send_client_request` function implements exponential backoff only for the REQUEST TIMEOUT (lines 348-378), not for the delay between retry attempts. The timeout calculation increases exponentially based on `request_failure_count`, but the request is sent immediately via `spawn_request_task`. [3](#0-2) 

When errors occur, `handle_data_client_error` immediately calls `resend_data_client_request`, triggering instant retries.

The server-side protection (RequestModerator) only applies to public network peers: [4](#0-3) 

Validator and VFN connections are explicitly NOT rate-limited (lines 56: `peer_network_id.network_id().is_public_network()`), leaving them vulnerable to retry storms.

**Attack Scenario:**
1. Attacker causes storage service requests to fail quickly (malicious server, resource exhaustion, or network manipulation)
2. Each failed request triggers immediate retry with no delay
3. With default configuration allowing 6-12 concurrent requests and 5 retries per request: [5](#0-4) 

4. A single data stream can generate 30-60 burst retry requests
5. Multiple nodes syncing simultaneously create hundreds/thousands of concurrent requests
6. Validator/VFN connections lack rate limiting, allowing unlimited retry storms
7. This overwhelms storage servers, causing cascading failures

## Impact Explanation

**Medium Severity** - This vulnerability aligns with the bug bounty's Medium severity criteria:
- **Validator node slowdowns**: Retry storms consume excessive CPU, memory, and network bandwidth
- **API crashes**: Storage service servers can be overwhelmed by burst retry traffic
- **Significant protocol violations**: Violates the Resource Limits invariant that all operations must respect computational limits

The attack can cause:
- Denial of service on storage service servers
- Degraded validator synchronization performance
- Cascading failures as nodes fall behind and generate more retries
- Network instability affecting consensus liveness (though not consensus safety)

While not causing consensus safety violations or fund loss, the availability impact on validators is significant and affects network health.

## Likelihood Explanation

**High Likelihood** of exploitation:
- No special privileges required - any node can trigger retries by causing request failures
- Validators and VFNs are not protected by rate limiting
- Natural network issues or server problems can trigger the vulnerability unintentionally
- Multiple attack vectors: malicious storage servers, resource exhaustion, network manipulation
- Amplification is automatic once failures begin
- No monitoring or alerts specifically designed for retry storms

The vulnerability can be triggered both maliciously (attacker-controlled storage server) or accidentally (legitimate server overload or network issues), making exploitation likely.

## Recommendation

Implement proper exponential backoff with delays between retry attempts:

```rust
fn resend_data_client_request(
    &mut self,
    data_client_request: &DataClientRequest,
) -> Result<(), Error> {
    // Increment the number of client failures for this request
    self.request_failure_count += 1;

    // Calculate exponential backoff delay (not just timeout)
    let base_delay_ms = 100; // 100ms base delay
    let max_delay_ms = 5000; // 5 second max delay
    let delay_ms = std::cmp::min(
        max_delay_ms,
        base_delay_ms * 2_u64.pow(self.request_failure_count.saturating_sub(1) as u32)
    );
    
    // Add jitter to prevent thundering herd (Â±25% randomization)
    let jitter_ms = (rand::random::<f64>() * 0.5 - 0.25) * delay_ms as f64;
    let delay_with_jitter_ms = ((delay_ms as f64) + jitter_ms) as u64;

    // Spawn delayed retry task
    let data_client_request_clone = data_client_request.clone();
    let stream_update_notifier = self.stream_update_notifier.clone();
    let pending_client_response = Arc::new(Mutex::new(Box::new(
        data_notification::PendingClientResponse::new(data_client_request_clone.clone()),
    )));
    
    let join_handle = tokio::spawn(async move {
        // Wait before retrying
        tokio::time::sleep(Duration::from_millis(delay_with_jitter_ms)).await;
        
        // Then send the retry request
        // ... rest of retry logic
    });
    
    self.spawned_tasks.push(join_handle);
    self.get_sent_data_requests()?.push_front(pending_client_response);
    
    Ok(())
}
```

Additional recommendations:
1. Apply rate limiting to validator/VFN connections, not just public network
2. Add circuit breaker pattern to stop retries after sustained failures
3. Implement request deduplication to prevent duplicate retries
4. Add monitoring metrics for retry rates and burst detection
5. Consider using the existing `aptos-retrier` crate's proper backoff utilities

## Proof of Concept

```rust
#[tokio::test]
async fn test_retry_storm_vulnerability() {
    use std::sync::atomic::{AtomicU64, Ordering};
    use std::sync::Arc;
    use tokio::time::{Duration, Instant};
    
    // Create a mock data client that fails quickly
    let request_count = Arc::new(AtomicU64::new(0));
    let request_count_clone = request_count.clone();
    
    let mock_client = MockAptosDataClient::new_with_callback(move |_| {
        request_count_clone.fetch_add(1, Ordering::SeqCst);
        // Return error immediately to trigger retry
        Err(aptos_data_client::error::Error::DataIsUnavailable("Test failure".into()))
    });
    
    // Create data stream configuration with retries enabled
    let config = DataStreamingServiceConfig {
        max_request_retry: 5,
        max_concurrent_requests: 6,
        ..Default::default()
    };
    
    // Create data stream
    let (mut data_stream, _listener) = DataStream::new(
        AptosDataClientConfig::default(),
        config,
        1,
        &StreamRequest::GetAllTransactions(GetAllTransactionsRequest {
            start_version: 0,
            end_version: 1000,
            proof_version: 1000,
            include_events: false,
        }),
        stream_update_notifier,
        mock_client,
        Arc::new(U64IdGenerator::new()),
        &advertised_data,
        TimeService::mock(),
    ).unwrap();
    
    // Initialize and process requests
    let start_time = Instant::now();
    data_stream.initialize_data_requests(global_data_summary).unwrap();
    
    // Allow some time for retries to occur
    tokio::time::sleep(Duration::from_millis(100)).await;
    data_stream.process_data_responses(global_data_summary).await.unwrap();
    let elapsed = start_time.elapsed();
    
    // Verify retry storm occurred
    let total_requests = request_count.load(Ordering::SeqCst);
    
    // With 6 concurrent requests and 5 retries each, we should see ~30-36 requests
    // If they all happened in < 1 second, it's a retry storm (no delay between retries)
    assert!(total_requests >= 30, "Expected retry storm with 30+ requests, got {}", total_requests);
    assert!(elapsed < Duration::from_secs(1), "Requests should burst quickly without delays, took {:?}", elapsed);
    
    println!("VULNERABILITY CONFIRMED: {} requests in {:?} (no retry delays)", total_requests, elapsed);
}
```

This PoC demonstrates that multiple requests are sent in rapid succession without any delay between retry attempts, confirming the retry storm vulnerability.

**Notes:**

The vulnerability is particularly severe for validators and VFNs because:
1. They communicate over non-public networks (Validator/VFN networks)
2. The RequestModerator explicitly excludes non-public networks from rate limiting
3. Validators require high-performance state synchronization, making retry storms more impactful
4. The lack of retry delays violates the standard practice of exponential backoff with delays (as seen in other parts of the codebase like `aptos-retrier` and REST client implementations)

### Citations

**File:** state-sync/data-streaming-service/src/data_stream.rs (L329-392)
```rust
    /// Sends a given request to the data client to be forwarded to the network
    /// and returns a pending client response. If `request_retry` is true
    /// exponential backoff takes affect (i.e., to increase the request timeout).
    fn send_client_request(
        &mut self,
        request_retry: bool,
        data_client_request: DataClientRequest,
    ) -> PendingClientResponse {
        // Create a new pending client response
        let pending_client_response = Arc::new(Mutex::new(Box::new(
            data_notification::PendingClientResponse::new(data_client_request.clone()),
        )));

        // Calculate the request timeout to use, based on the
        // request type and the number of previous failures.
        let request_timeout_ms = if data_client_request.is_optimistic_fetch_request() {
            self.data_client_config.optimistic_fetch_timeout_ms
        } else if data_client_request.is_subscription_request() {
            self.data_client_config.subscription_response_timeout_ms
        } else if !request_retry {
            self.data_client_config.response_timeout_ms
        } else {
            let response_timeout_ms = self.data_client_config.response_timeout_ms;
            let max_response_timeout_ms = self.data_client_config.max_response_timeout_ms;

            // Exponentially increase the timeout based on the number of
            // previous failures (but bounded by the max timeout).
            let request_timeout_ms = min(
                max_response_timeout_ms,
                response_timeout_ms * (u32::pow(2, self.request_failure_count as u32) as u64),
            );

            // Update the retry counter and log the request
            increment_counter_multiple_labels(
                &metrics::RETRIED_DATA_REQUESTS,
                data_client_request.get_label(),
                &request_timeout_ms.to_string(),
            );
            info!(
                (LogSchema::new(LogEntry::RetryDataRequest)
                    .stream_id(self.data_stream_id)
                    .message(&format!(
                        "Retrying data request type: {:?}, with new timeout: {:?} (ms)",
                        data_client_request.get_label(),
                        request_timeout_ms.to_string()
                    )))
            );

            request_timeout_ms
        };

        // Send the request to the network
        let join_handle = spawn_request_task(
            self.data_stream_id,
            data_client_request,
            self.aptos_data_client.clone(),
            pending_client_response.clone(),
            request_timeout_ms,
            self.stream_update_notifier.clone(),
        );
        self.spawned_tasks.push(join_handle);

        pending_client_response
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L522-538)
```rust
                },
                Err(error) => {
                    // Handle the error depending on the request type
                    if client_request.is_new_data_request() {
                        // The request was for new data. We should notify the
                        // stream engine and clear the requests queue.
                        self.notify_new_data_request_error(client_request, error)?;
                    } else {
                        // Decrease the prefetching limit on an error
                        self.dynamic_prefetching_state
                            .decrease_max_concurrent_requests();

                        // Handle the error and simply retry
                        self.handle_data_client_error(client_request, &error)?;
                    }
                    break; // We're now head of line blocked on the failed request
                },
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L729-744)
```rust
    fn resend_data_client_request(
        &mut self,
        data_client_request: &DataClientRequest,
    ) -> Result<(), Error> {
        // Increment the number of client failures for this request
        self.request_failure_count += 1;

        // Resend the client request
        let pending_client_response = self.send_client_request(true, data_client_request.clone());

        // Push the pending response to the head of the sent requests queue
        self.get_sent_data_requests()?
            .push_front(pending_client_response);

        Ok(())
    }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L50-69)
```rust
    pub fn increment_invalid_request_count(&mut self, peer_network_id: &PeerNetworkId) {
        // Increment the invalid request count
        self.invalid_request_count += 1;

        // If the peer is a PFN and has sent too many invalid requests, start ignoring it
        if self.ignore_start_time.is_none()
            && peer_network_id.network_id().is_public_network()
            && self.invalid_request_count >= self.max_invalid_requests
        {
            // TODO: at some point we'll want to terminate the connection entirely

            // Start ignoring the peer
            self.ignore_start_time = Some(self.time_service.now());

            // Log the fact that we're now ignoring the peer
            warn!(LogSchema::new(LogEntry::RequestModeratorIgnoredPeer)
                .peer_network_id(peer_network_id)
                .message("Ignoring peer due to too many invalid requests!"));
        }
    }
```

**File:** config/src/config/state_sync_config.rs (L271-277)
```rust
            max_concurrent_requests: MAX_CONCURRENT_REQUESTS,
            max_concurrent_state_requests: MAX_CONCURRENT_STATE_REQUESTS,
            max_data_stream_channel_sizes: 50,
            max_notification_id_mappings: 300,
            max_num_consecutive_subscriptions: 45, // At ~3 blocks per second, this should last ~15 seconds
            max_pending_requests: 50,
            max_request_retry: 5,
```
