# Audit Report

## Title
EventStorePruner Lacks Validation Against Catastrophic Misconfiguration Leading to Immediate Historical Data Loss

## Summary
The EventStorePruner and broader LedgerPruner system lack proper validation to prevent dangerous pruning window configurations. An operator can set `prune_window: 0` or other dangerously small values with `enable: true`, causing the node to immediately prune all historical event data up to the latest version, resulting in permanent data loss. The only protection is a warning that does not prevent the misconfiguration from being applied.

## Finding Description
The EventStorePruner relies on the `LedgerPrunerConfig.prune_window` configuration parameter to determine how many versions of historical data to retain. The configuration validation in `StorageConfig::sanitize()` only emits a warning for small prune_window values but does not enforce a minimum threshold when pruning is enabled. [1](#0-0) 

When a node operator misconfigures the system with `enable: true` and `prune_window: 0` (or any dangerously small value), the following occurs:

1. The configuration passes validation with only a warning logged
2. `LedgerPrunerManager` is initialized with the misconfigured prune_window [2](#0-1) 

3. When pruning is triggered, the target version is calculated as `latest_version - prune_window`: [3](#0-2) 

4. With `prune_window: 0`, this becomes `min_readable_version = latest_version`, causing the pruner to delete all historical data up to the current version

5. EventStorePruner and all sub-pruners execute deletion without additional validation: [4](#0-3) [5](#0-4) 

This vulnerability is confirmed by the test suite, which explicitly tests pruning with `prune_window: 0`: [6](#0-5) 

The misconfiguration breaks the **State Consistency** invariant: historical blockchain data must remain accessible for state synchronization, API queries, and verification. It also violates data availability guarantees critical for blockchain operation.

## Impact Explanation
**HIGH Severity** per Aptos bug bounty criteria:

- **Validator node slowdowns**: Nodes lose the ability to serve historical data, degrading API functionality and preventing historical queries
- **Significant protocol violations**: Historical data availability is a core protocol requirement for state sync and verification
- **State inconsistencies requiring intervention**: Requires restore from backup or re-sync from genesis to recover lost data

If multiple validators misconfigure simultaneously, the network experiences:
- Degraded state sync capabilities for new nodes
- Failed API endpoints for historical queries  
- Potential consensus issues if validators need historical data for validation
- Loss of auditability and transaction history verification

## Likelihood Explanation
**Medium-to-High Likelihood** due to multiple realistic misconfiguration scenarios:

1. **Copy-paste from test configurations**: Test files use `prune_window: 0` extensively, making it easy for operators to accidentally use test configs in production [7](#0-6) 

2. **Typo or unit confusion**: Operators may accidentally omit digits (e.g., typing `0` instead of `90000000`) or misunderstand that `0` means "no retention window" rather than "unlimited retention"

3. **Warning fatigue**: The warning is easily overlooked during node startup among other logs, especially since it doesn't prevent the node from starting

4. **No runtime enforcement**: Once the misconfiguration is active, pruning begins immediately after `batch_size` versions without additional confirmation

While this requires operator error rather than external attack, operational security vulnerabilities are explicitly in scope per the security question about "misconfiguration."

## Recommendation
Add strict validation that enforces a minimum prune_window when pruning is enabled. Modify the sanitizer to return an error instead of just warning:

```rust
// In config/src/config/storage_config.rs, ConfigSanitizer implementation
const MIN_SAFE_LEDGER_PRUNE_WINDOW: u64 = 50_000_000;

if config.storage_pruner_config.ledger_pruner_config.enable {
    if ledger_prune_window == 0 {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name,
            "Ledger pruner is enabled but prune_window is 0. This would cause immediate deletion of all historical data. Set prune_window >= 50,000,000 or disable the pruner.".to_string(),
        ));
    }
    if ledger_prune_window < MIN_SAFE_LEDGER_PRUNE_WINDOW {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name,
            format!("Ledger prune_window ({}) is too small and would harm network data availability. Minimum allowed value is {} when pruner is enabled.", ledger_prune_window, MIN_SAFE_LEDGER_PRUNE_WINDOW),
        ));
    }
}
```

Additionally, add runtime validation in `LedgerPrunerManager::new()` as a defense-in-depth measure:

```rust
// In storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs
pub fn new(
    ledger_db: Arc<LedgerDb>,
    ledger_pruner_config: LedgerPrunerConfig,
    internal_indexer_db: Option<InternalIndexerDB>,
) -> Self {
    if ledger_pruner_config.enable {
        assert!(
            ledger_pruner_config.prune_window >= 50_000_000,
            "Ledger pruner prune_window must be at least 50,000,000 when enabled, got {}",
            ledger_pruner_config.prune_window
        );
    }
    // ... rest of implementation
}
```

## Proof of Concept
The existing test explicitly demonstrates this vulnerability: [8](#0-7) 

To reproduce the vulnerability in a production-like scenario:

1. Create a node configuration file with:
```yaml
storage:
  storage_pruner_config:
    ledger_pruner_config:
      enable: true
      prune_window: 0  # Dangerous misconfiguration
      batch_size: 5000
      user_pruning_window_offset: 0
```

2. Start an Aptos node with this configuration
3. After the node processes `batch_size` (5000) versions, observe that all historical events from genesis are pruned
4. Verify data loss by querying historical events - they will return empty/error responses
5. Confirm that only the most recent versions remain accessible

The configuration will pass validation with only a warning, and the node will proceed to delete all historical blockchain data, requiring a full restore from backup or re-sync from genesis to recover.

### Citations

**File:** config/src/config/storage_config.rs (L306-323)
```rust
pub const NO_OP_STORAGE_PRUNER_CONFIG: PrunerConfig = PrunerConfig {
    ledger_pruner_config: LedgerPrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
        user_pruning_window_offset: 0,
    },
    state_merkle_pruner_config: StateMerklePrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
    },
    epoch_snapshot_pruner_config: EpochSnapshotPrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
    },
};
```

**File:** config/src/config/storage_config.rs (L708-710)
```rust
        if ledger_prune_window < 50_000_000 {
            warn!("Ledger prune_window is too small, harming network data availability.");
        }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L108-139)
```rust
    pub fn new(
        ledger_db: Arc<LedgerDb>,
        ledger_pruner_config: LedgerPrunerConfig,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Self {
        let pruner_worker = if ledger_pruner_config.enable {
            Some(Self::init_pruner(
                Arc::clone(&ledger_db),
                ledger_pruner_config,
                internal_indexer_db,
            ))
        } else {
            None
        };

        let min_readable_version =
            pruner_utils::get_ledger_pruner_progress(&ledger_db).expect("Must succeed.");

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        Self {
            ledger_db,
            prune_window: ledger_pruner_config.prune_window,
            pruner_worker,
            pruning_batch_size: ledger_pruner_config.batch_size,
            latest_version: Arc::new(Mutex::new(min_readable_version)),
            user_pruning_window_offset: ledger_pruner_config.user_pruning_window_offset,
            min_readable_version: AtomicVersion::new(min_readable_version),
        }
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L162-176)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L43-81)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let mut indexer_batch = None;

        let indices_batch = if let Some(indexer_db) = self.indexer_db() {
            if indexer_db.event_enabled() {
                indexer_batch = Some(SchemaBatch::new());
            }
            indexer_batch.as_mut()
        } else {
            Some(&mut batch)
        };
        let num_events_per_version = self.ledger_db.event_db().prune_event_indices(
            current_progress,
            target_version,
            indices_batch,
        )?;
        self.ledger_db.event_db().prune_events(
            num_events_per_version,
            current_progress,
            target_version,
            &mut batch,
        )?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        if let Some(mut indexer_batch) = indexer_batch {
            indexer_batch.put::<InternalIndexerMetadataSchema>(
                &IndexerMetadataKey::EventPrunerProgress,
                &IndexerMetadataValue::Version(target_version),
            )?;
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner_test.rs (L55-99)
```rust
fn verify_event_store_pruner(events: Vec<Vec<ContractEvent>>) {
    let tmp_dir = TempPath::new();
    let aptos_db = AptosDB::new_for_test(&tmp_dir);
    let event_store = &aptos_db.event_store;
    let mut batch = SchemaBatch::new();
    let num_versions = events.len();

    // Write events to DB
    for (version, events_for_version) in events.iter().enumerate() {
        event_store
            .put_events(
                version as u64,
                events_for_version,
                /*skip_index=*/ false,
                &mut batch,
            )
            .unwrap();
    }
    aptos_db.ledger_db.event_db().write_schemas(batch).unwrap();

    let pruner = LedgerPrunerManager::new(Arc::clone(&aptos_db.ledger_db), LedgerPrunerConfig {
        enable: true,
        prune_window: 0,
        batch_size: 1,
        user_pruning_window_offset: 0,
    });
    // start pruning events batches of size 2 and verify transactions have been pruned from DB
    for i in (0..=num_versions).step_by(2) {
        pruner
            .wake_and_wait_pruner(i as u64 /* latest_version */)
            .unwrap();
        // ensure that all events up to i has been pruned
        for j in 0..i {
            verify_events_not_in_store(j as u64, event_store);
            verify_event_by_key_not_in_store(&events, j as u64, event_store);
            verify_event_by_version_not_in_store(&events, j as u64, event_store);
        }
        // ensure all other events are valid in DB
        for j in i..num_versions {
            verify_events_in_store(&events, j as u64, event_store);
            verify_event_by_key_in_store(&events, j as u64, event_store);
            verify_event_by_version_in_store(&events, j as u64, event_store);
        }
    }
}
```
