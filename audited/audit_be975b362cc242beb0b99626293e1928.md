# Audit Report

## Title
Archive Tampering Vulnerability: Replay Verification Lacks Cryptographic Proof Validation

## Summary
The replay verification tool in `replay_on_archive.rs` fails to cryptographically verify archived transaction data against validator-signed LedgerInfo commitments. An attacker with write access to archived database files can modify historical transactions and their corresponding metadata, and the replay verification will pass because it only checks deterministic re-execution, not cryptographic authenticity.

## Finding Description

The replay verification mechanism in [1](#0-0)  loads transaction data directly from an archived database and verifies that re-execution produces matching results, but never validates the cryptographic authenticity of the archived data.

The verification flow operates as follows:

1. The `Verifier::verify()` method retrieves transactions and their "expected" outputs from the database via `backup_handler.get_transaction_iter()` [2](#0-1) 

2. The BackupHandler reads data directly from various RocksDB tables without any cryptographic verification [3](#0-2) 

3. Transactions are re-executed and outputs are compared against the expected values using `ensure_match_transaction_info()` [4](#0-3) 

4. The comparison only verifies that re-execution matches stored TransactionInfo fields (status, gas, hashes) [5](#0-4) 

**Critical Missing Security Checks:**

The code never performs the following cryptographic verifications that the Aptos security model requires:

1. **No LedgerInfo signature verification**: Although LedgerInfo objects contain BLS aggregate signatures from 2f+1 validators [6](#0-5)  and have a `verify_signatures()` method [7](#0-6) , the replay code never calls it.

2. **No accumulator proof verification**: The BackupHandler provides `get_transaction_range_proof()` to obtain cryptographic proofs [8](#0-7) , but the replay code never uses this method or verifies accumulator proofs.

3. **No waypoint anchoring**: Aptos provides waypoints as trusted checkpoints [9](#0-8) , but the replay tool never establishes or verifies against a trusted waypoint.

**Attack Scenario:**

An attacker who gains write access to archived RocksDB files (e.g., via compromised backup storage) can:

1. Modify transaction data in the `transaction_db` (change transfer amounts, recipients, smart contract calls, etc.)
2. Recompute the TransactionInfo fields (state_change_hash, event_root_hash, gas_used) to match the modified transaction's execution
3. Update the stored TransactionInfo, events, and writesets to match the modified values
4. The replay verification will **pass** because re-executing the modified transaction produces outputs matching the modified TransactionInfo

The proper verification approach is demonstrated in [10](#0-9) , which shows how to verify transaction authenticity by validating accumulator proofs against signed LedgerInfo.

## Impact Explanation

**Critical Severity** - This vulnerability violates fundamental blockchain security guarantees:

1. **Immutability Violation**: Blockchain's core promise of tamper-evident history is broken for archived data. Attackers can perform historical revisionism undetectably.

2. **Trust Compromise**: Organizations relying on archived blockchain data for:
   - Legal compliance and audit trails
   - Forensic investigations
   - Disaster recovery scenarios
   - Historical analytics
   
   Cannot trust their archives are authentic.

3. **Backup Integrity**: In disaster recovery scenarios, restoring from a tampered archive would pass verification, potentially introducing corrupted historical state into production systems.

4. **Scope of Impact**: Any entity using the db-tool for archive verification is affected, including validators, node operators, blockchain explorers, and enterprises maintaining historical records.

This meets the **Critical Severity** criteria: "Consensus/Safety violations" and could enable "State inconsistencies requiring intervention" at minimum Medium severity, but the immutability violation elevates this to Critical.

## Likelihood Explanation

**High Likelihood** due to multiple realistic attack vectors:

1. **Cloud Storage Compromise**: Many organizations store blockchain archives in cloud storage (S3, GCS, Azure) which can be compromised through:
   - Misconfigured access policies
   - Compromised credentials
   - Supply chain attacks on cloud infrastructure

2. **Backup System Attacks**: Backup infrastructure is often less secured than production systems and presents an attractive target.

3. **Insider Threats**: Personnel with access to backup storage could tamper with archives.

4. **No Detection**: The current verification provides false assurance of integrity, making detection unlikely until damage is done.

5. **Tool Usage**: The db-tool is explicitly designed for archive verification, meaning organizations are actively using this vulnerable verification mechanism.

The attack requires only:
- Write access to RocksDB files (no cryptographic keys needed)
- Understanding of RocksDB format and Aptos transaction structure
- Basic tools to modify database files

No validator collusion or complex cryptographic attacks are required.

## Recommendation

Implement cryptographic verification by:

1. **Add Waypoint Parameter**: Require a trusted waypoint (genesis waypoint or trusted checkpoint) as input:
```rust
#[clap(long, help = "Trusted waypoint for verification")]
pub trusted_waypoint: Waypoint,
```

2. **Verify LedgerInfo Signatures**: After obtaining the starting epoch's ValidatorVerifier from a trusted source, verify all LedgerInfo signatures:
```rust
// Get validator verifier from trusted genesis/waypoint
let validator_verifier = get_validator_verifier_from_trusted_source(&trusted_waypoint)?;

// For each chunk, verify the LedgerInfo
let (range_proof, ledger_info) = backup_handler.get_transaction_range_proof(
    chunk_start_version, 
    chunk_end_version
)?;

// Verify signatures
ledger_info.verify_signatures(&validator_verifier)?;
```

3. **Verify Accumulator Proofs**: For each transaction chunk, verify the accumulator range proof:
```rust
let txn_info_hashes: Vec<_> = expected_txn_infos
    .iter()
    .map(CryptoHash::hash)
    .collect();

range_proof.verify(
    ledger_info.ledger_info().transaction_accumulator_hash(),
    Some(chunk_start_version),
    &txn_info_hashes,
)?;
```

4. **Document Trust Model**: Clearly document that the tool requires a trusted waypoint source and explain the security properties this provides.

The fix should follow the pattern established in [11](#0-10)  which demonstrates proper cryptographic verification.

## Proof of Concept

```rust
// File: storage/db-tool/tests/replay_tampering_test.rs
#[test]
fn test_archive_tampering_undetected() {
    use aptos_types::transaction::{Transaction, TransactionPayload, SignedTransaction};
    use aptos_crypto::ed25519::Ed25519PrivateKey;
    use aptos_types::account_address::AccountAddress;
    
    // Setup: Create an archive with legitimate transactions
    let temp_dir = tempfile::tempdir().unwrap();
    let db_path = temp_dir.path();
    
    // Initialize database with a legitimate transaction transferring 100 coins
    let genesis_txn = create_genesis_transaction();
    let transfer_100_txn = create_transfer_transaction(
        AccountAddress::random(),
        AccountAddress::random(), 
        100
    );
    
    // Execute and commit to database
    let mut db = create_test_db(db_path);
    execute_and_commit(&mut db, vec![genesis_txn, transfer_100_txn]).unwrap();
    
    // Get the transaction info before tampering
    let original_txn_info = db.get_transaction_info(1).unwrap();
    
    // ATTACK: Directly modify RocksDB to change transaction to transfer 1000 coins
    drop(db); // Close DB for direct manipulation
    
    let modified_txn = create_transfer_transaction(
        AccountAddress::random(),
        AccountAddress::random(),
        1000  // Changed from 100 to 1000!
    );
    
    // Directly modify the transaction_db column family
    tamper_with_rocksdb(
        db_path, 
        version: 1,
        modified_txn: &modified_txn
    );
    
    // Recompute and update TransactionInfo to match modified transaction
    let modified_txn_output = execute_transaction_without_commit(&modified_txn);
    let modified_txn_info = TransactionInfo::new(
        modified_txn_output.state_change_hash(),
        modified_txn_output.event_root_hash(),
        modified_txn_output.gas_used(),
        modified_txn_output.status(),
        // ... other fields
    );
    
    update_transaction_info_in_rocksdb(db_path, version: 1, &modified_txn_info);
    update_events_in_rocksdb(db_path, version: 1, &modified_txn_output.events());
    update_writeset_in_rocksdb(db_path, version: 1, &modified_txn_output.write_set());
    
    // VERIFICATION: Run replay verification on tampered archive
    let opt = Opt {
        start_version: 0,
        end_version: 1,
        db_dir: db_path.to_path_buf(),
        chunk_size: 10,
        concurrent_replay: 1,
        timeout_secs: None,
        paranoid_type_checks: false,
        // ... other config
    };
    
    // The verification SHOULD fail but currently PASSES!
    let result = opt.run().await;
    
    // BUG: This assertion PASSES, meaning tampering was undetected
    assert!(result.is_ok(), "Tampered archive passed verification!");
    
    // The attacker successfully changed 100 to 1000 and it was not detected
    println!("CRITICAL: Archive tampering went undetected!");
    println!("Original amount: 100, Modified amount: 1000");
    println!("Replay verification incorrectly reported success");
}

fn tamper_with_rocksdb(path: &Path, version: u64, txn: &Transaction) {
    // Open RocksDB directly and modify the transaction_db column family
    let opts = rocksdb::Options::default();
    let db = rocksdb::DB::open(&opts, path).unwrap();
    let cf = db.cf_handle("transaction_db").unwrap();
    
    // Serialize and write modified transaction
    let key = version.to_be_bytes();
    let value = bcs::to_bytes(txn).unwrap();
    db.put_cf(cf, key, value).unwrap();
}

// Helper functions for test setup omitted for brevity
```

## Notes

The vulnerability exists because the replay verification was designed to check **deterministic execution** (that re-running transactions produces the same results) but not **cryptographic authenticity** (that the transactions are the ones actually committed to the blockchain). This is a critical distinction in blockchain security.

Aptos provides all the necessary cryptographic primitives (LedgerInfo signatures, accumulator proofs, waypoints), but the replay tool does not use them. The fix is straightforward: integrate the existing cryptographic verification mechanisms that are already used correctly in other parts of the codebase.

### Citations

**File:** storage/db-tool/src/replay_on_archive.rs (L91-102)
```rust
    pub async fn run(self) -> Result<()> {
        let verifier = Verifier::new(&self)?;
        let all_errors = verifier.run()?;
        if !all_errors.is_empty() {
            error!("{} failed transactions", all_errors.len());
            for e in all_errors {
                error!("Failed: {}", e);
            }
            process::exit(2);
        }
        Ok(())
    }
```

**File:** storage/db-tool/src/replay_on_archive.rs (L247-249)
```rust
        let txn_iter = self
            .backup_handler
            .get_transaction_iter(start, limit as usize)?;
```

**File:** storage/db-tool/src/replay_on_archive.rs (L394-399)
```rust
            if let Err(err) = executed_outputs[idx].ensure_match_transaction_info(
                version,
                &expected_txn_infos[idx],
                Some(&expected_writesets[idx]),
                Some(&expected_events[idx]),
            ) {
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L56-108)
```rust
        let txn_iter = self
            .ledger_db
            .transaction_db()
            .get_transaction_iter(start_version, num_transactions)?;
        let mut txn_info_iter = self
            .ledger_db
            .transaction_info_db()
            .get_transaction_info_iter(start_version, num_transactions)?;
        let mut event_vec_iter = self
            .ledger_db
            .event_db()
            .get_events_by_version_iter(start_version, num_transactions)?;
        let mut write_set_iter = self
            .ledger_db
            .write_set_db()
            .get_write_set_iter(start_version, num_transactions)?;
        let mut persisted_aux_info_iter = self
            .ledger_db
            .persisted_auxiliary_info_db()
            .get_persisted_auxiliary_info_iter(start_version, num_transactions)?;

        let zipped = txn_iter.enumerate().map(move |(idx, txn_res)| {
            let version = start_version + idx as u64; // overflow is impossible since it's check upon txn_iter construction.

            let txn = txn_res?;
            let txn_info = txn_info_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "TransactionInfo not found when Transaction exists, version {}",
                    version
                ))
            })??;
            let event_vec = event_vec_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "Events not found when Transaction exists., version {}",
                    version
                ))
            })??;
            let write_set = write_set_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "WriteSet not found when Transaction exists, version {}",
                    version
                ))
            })??;
            let persisted_aux_info = persisted_aux_info_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "PersistedAuxiliaryInfo not found when Transaction exists, version {}",
                    version
                ))
            })??;
            BACKUP_TXN_VERSION.set(version as i64);
            Ok((txn, persisted_aux_info, txn_info, event_vec, write_set))
        });
        Ok(zipped)
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L111-137)
```rust
    /// Gets the proof for a transaction chunk.
    /// N.B. the `LedgerInfo` returned will always be in the same epoch of the `last_version`.
    pub fn get_transaction_range_proof(
        &self,
        first_version: Version,
        last_version: Version,
    ) -> Result<(TransactionAccumulatorRangeProof, LedgerInfoWithSignatures)> {
        ensure!(
            last_version >= first_version,
            "Bad transaction range: [{}, {}]",
            first_version,
            last_version
        );
        let num_transactions = last_version - first_version + 1;
        let ledger_metadata_db = self.ledger_db.metadata_db();
        let epoch = ledger_metadata_db.get_epoch(last_version)?;
        let ledger_info = ledger_metadata_db.get_latest_ledger_info_in_epoch(epoch)?;
        let accumulator_proof = self
            .ledger_db
            .transaction_accumulator_db()
            .get_transaction_range_proof(
                Some(first_version),
                num_transactions,
                ledger_info.ledger_info().version(),
            )?;
        Ok((accumulator_proof, ledger_info))
    }
```

**File:** types/src/transaction/mod.rs (L1869-1928)
```rust
    pub fn ensure_match_transaction_info(
        &self,
        version: Version,
        txn_info: &TransactionInfo,
        expected_write_set: Option<&WriteSet>,
        expected_events: Option<&[ContractEvent]>,
    ) -> Result<()> {
        const ERR_MSG: &str = "TransactionOutput does not match TransactionInfo";

        let expected_txn_status: TransactionStatus = txn_info.status().clone().into();
        ensure!(
            self.status() == &expected_txn_status,
            "{}: version:{}, status:{:?}, auxiliary data:{:?}, expected:{:?}",
            ERR_MSG,
            version,
            self.status(),
            self.auxiliary_data(),
            expected_txn_status,
        );

        ensure!(
            self.gas_used() == txn_info.gas_used(),
            "{}: version:{}, gas_used:{:?}, expected:{:?}",
            ERR_MSG,
            version,
            self.gas_used(),
            txn_info.gas_used(),
        );

        let write_set_hash = CryptoHash::hash(self.write_set());
        ensure!(
            write_set_hash == txn_info.state_change_hash(),
            "{}: version:{}, write_set_hash:{:?}, expected:{:?}, write_set: {:?}, expected(if known): {:?}",
            ERR_MSG,
            version,
            write_set_hash,
            txn_info.state_change_hash(),
            self.write_set,
            expected_write_set,
        );

        let event_hashes = self
            .events()
            .iter()
            .map(CryptoHash::hash)
            .collect::<Vec<_>>();
        let event_root_hash = InMemoryEventAccumulator::from_leaves(&event_hashes).root_hash;
        ensure!(
            event_root_hash == txn_info.event_root_hash(),
            "{}: version:{}, event_root_hash:{:?}, expected:{:?}, events: {:?}, expected(if known): {:?}",
            ERR_MSG,
            version,
            event_root_hash,
            txn_info.event_root_hash(),
            self.events(),
            expected_events,
        );

        Ok(())
    }
```

**File:** types/src/ledger_info.rs (L240-246)
```rust
#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize)]
pub struct LedgerInfoWithV0 {
    ledger_info: LedgerInfo,
    /// Aggregated BLS signature of all the validators that signed the message. The bitmask in the
    /// aggregated signature can be used to find out the individual validators signing the message
    signatures: AggregateSignature,
}
```

**File:** types/src/ledger_info.rs (L303-308)
```rust
    pub fn verify_signatures(
        &self,
        validator: &ValidatorVerifier,
    ) -> ::std::result::Result<(), VerifyError> {
        validator.verify_multi_signatures(self.ledger_info(), &self.signatures)
    }
```

**File:** types/src/waypoint.rs (L24-35)
```rust
/// Waypoint keeps information about the LedgerInfo on a given version, which provides an
/// off-chain mechanism to verify the sync process right after the restart.
/// At high level, a trusted waypoint verifies the LedgerInfo for a certain epoch change.
/// For more information, please refer to the Waypoints documentation.
#[derive(Clone, Copy, Debug, Default, Eq, PartialEq)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct Waypoint {
    /// The version of the reconfiguration transaction that is being approved by this waypoint.
    version: Version,
    /// The hash of the chosen fields of LedgerInfo.
    value: HashValue,
}
```

**File:** storage/aptosdb/src/db_debugger/ledger/check_range_proof.rs (L25-70)
```rust
    pub fn run(self) -> Result<()> {
        let ledger_db = Arc::new(self.db_dir.open_ledger_db()?);
        let ledger_metadata_db = ledger_db.metadata_db();
        let ledger_info = ledger_metadata_db.get_latest_ledger_info()?;
        println!("Latest LedgerInfo: {:?}", ledger_info);

        println!("Checking Range proof...");

        let txn_infos: Vec<_> = ledger_db
            .transaction_info_db()
            .get_transaction_info_iter(self.start_version, self.num_versions)?
            .collect::<Result<_>>()?;
        ensure!(
            txn_infos.len() == self.num_versions,
            "expecting {} txns, got {}",
            self.num_versions,
            txn_infos.len(),
        );
        let txn_info_hashes: Vec<_> = txn_infos.iter().map(CryptoHash::hash).collect();

        let last_version = self.start_version + self.num_versions as u64 - 1;
        let last_version_epoch = ledger_metadata_db.get_epoch(last_version)?;
        for epoch in last_version_epoch..=ledger_info.ledger_info().epoch() {
            println!("Check against epoch {} LedgerInfo.", epoch);
            let li = ledger_metadata_db.get_latest_ledger_info_in_epoch(epoch)?;
            println!(
                "    Root hash: {:?}",
                li.ledger_info().transaction_accumulator_hash()
            );
            let range_proof = ledger_db
                .transaction_accumulator_db()
                .get_transaction_range_proof(
                    Some(self.start_version),
                    self.num_versions as u64,
                    li.ledger_info().version(),
                )?;
            range_proof.verify(
                li.ledger_info().transaction_accumulator_hash(),
                Some(self.start_version),
                &txn_info_hashes,
            )?;
        }

        println!("Done.");
        Ok(())
    }
```
