# Audit Report

## Title
Memory Exhaustion DoS via Unbounded Batch Loading During Node Recovery

## Summary
A malicious validator can cause memory exhaustion and DoS on victim validators by filling the quorum store database with maximum-sized batches, then triggering a node restart. During recovery, `get_all_batches_v2()` loads ALL persisted batches into memory simultaneously without respecting the memory quota limits enforced during normal operation, causing potential out-of-memory crashes.

## Finding Description

The quorum store implements a quota system to limit per-peer resource usage during normal operation. Each validator peer can store up to 300 MB of batch data on disk (`db_quota`) and 120 MB in memory (`memory_quota`). [1](#0-0) 

During normal operation, when batches are persisted, the quota system correctly limits storage: [2](#0-1) 

However, a critical vulnerability exists in the recovery path. When a `BatchStore` is created within the same epoch (not a new epoch), it calls `populate_cache_and_gc_expired_batches_v2()`: [3](#0-2) 

This function loads ALL batches from the database into memory at once: [4](#0-3) 

The `get_all_batches_v2()` function creates a HashMap containing all persisted batches with their full transaction payloads: [5](#0-4) 

The vulnerability occurs because:

1. During normal operation, batches are persisted to disk with full payloads via `save_batch_v2()`: [6](#0-5) 

2. Each validator peer can fill up to 300 MB of disk space with batches during normal operation (quota enforced)

3. During recovery, `get_all_batches_v2()` loads ALL these batches (from all validator peers) into a single HashMap in memory BEFORE any quota checks are applied

4. With N validators in the epoch, this can result in loading N × 300 MB into memory simultaneously

**Attack Scenario:**
1. Malicious validator sends batches with maximum allowed payload size (~1 MB per batch)
2. Batches pass verification (size limits are enforced per-batch): [7](#0-6) 

3. Batches are stored in the database, consuming up to 300 MB per validator peer
4. When a victim validator restarts (or BatchStore is recreated within the same epoch), `get_all_batches_v2()` loads all batches into memory
5. Memory usage spikes to N × 300 MB (where N is the number of validators)
6. This causes OOM conditions, crashes, or severe performance degradation

The root cause is that resource limits enforced during normal operation are bypassed during the recovery path, violating the **Resource Limits** invariant.

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:
- **"Validator node slowdowns"** - Direct impact category
- Causes memory exhaustion leading to OOM crashes or severe performance degradation
- Affects validator availability and consensus participation
- Can be triggered by a single Byzantine validator against all honest validators

**Quantified Impact:**
- With 100-validator network: 30 GB memory spike on recovery
- With 200-validator network: 60 GB memory spike on recovery
- Validators with insufficient RAM will crash (OOM kill)
- Validators with sufficient RAM experience severe performance degradation during recovery
- Repeated restarts (e.g., during upgrades, crashes, maintenance) repeatedly trigger the attack

This breaks the **Resource Limits** invariant that "all operations must respect gas, storage, and computational limits" - the recovery operation does not respect memory limits.

## Likelihood Explanation

**Likelihood: High**

The attack is highly likely to succeed because:

1. **Low barrier to entry**: Any validator in the active set can execute this attack
2. **Stealthy accumulation**: Malicious validator can gradually fill quota over time during normal operation without detection
3. **Natural trigger conditions**: Node restarts occur regularly due to:
   - Software upgrades
   - Maintenance operations  
   - Crashes or bugs
   - Infrastructure issues
4. **Deterministic impact**: Once the database is filled, every restart triggers the memory spike
5. **Wide attack surface**: Affects all validators attempting to recover within the same epoch

The attack requires:
- Being a validator in the current epoch (within threat model - BFT tolerates Byzantine validators)
- Sending batches during normal operation (normal validator behavior)
- Waiting for victim validator restarts (occurs naturally)

No complex timing, race conditions, or coordination required.

## Recommendation

**Immediate Fix**: Implement streaming/chunked loading of batches during recovery with quota enforcement:

```rust
fn populate_cache_and_gc_expired_batches_v2(
    db: Arc<dyn QuorumStoreStorage>,
    current_epoch: u64,
    last_certified_time: u64,
    expiration_buffer_usecs: u64,
    batch_store: &BatchStore,
) {
    // Stream batches instead of loading all at once
    let mut iter = db.iter_batches_v2().expect("failed to create iterator");
    let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
    let mut expired_keys = Vec::new();
    let mut loaded_count = 0;
    
    while let Some((digest, value)) = iter.next() {
        let expiration = value.expiration();
        
        if expiration < gc_timestamp {
            expired_keys.push(digest);
        } else {
            // Apply quota check during recovery
            match batch_store.insert_to_cache(&value) {
                Ok(_) => loaded_count += 1,
                Err(e) => {
                    warn!("Quota exceeded during recovery, skipping batch {}: {}", digest, e);
                    // Delete batches that exceed quota during recovery
                    expired_keys.push(digest);
                }
            }
        }
        
        // Periodically delete expired batches to free memory
        if expired_keys.len() >= 1000 {
            let batch_to_delete = std::mem::take(&mut expired_keys);
            tokio::task::spawn_blocking(move || {
                db.delete_batches_v2(batch_to_delete).ok();
            });
        }
    }
    
    info!("Loaded {} batches, deleted {} during recovery", loaded_count, expired_keys.len());
    
    if !expired_keys.is_empty() {
        tokio::task::spawn_blocking(move || {
            db.delete_batches_v2(expired_keys).ok();
        });
    }
}
```

**Additional Mitigations**:
1. Add memory usage monitoring and alerts during recovery
2. Implement aggressive garbage collection of expired batches before loading
3. Consider removing payloads from persisted batches entirely (store only metadata in DB, refetch transactions when needed)
4. Add per-epoch total storage limits in addition to per-peer limits

## Proof of Concept

```rust
#[tokio::test]
async fn test_memory_exhaustion_on_recovery() {
    use consensus::quorum_store::batch_store::BatchStore;
    use consensus::quorum_store::quorum_store_db::QuorumStoreDB;
    use aptos_types::validator_signer::ValidatorSigner;
    use std::sync::Arc;
    
    // Create temporary database
    let temp_dir = tempfile::tempdir().unwrap();
    let db = Arc::new(QuorumStoreDB::new(temp_dir.path()));
    
    // Configuration
    let memory_quota = 120_000_000; // 120 MB
    let db_quota = 300_000_000; // 300 MB
    let batch_quota = 300_000;
    let epoch = 1;
    
    // Create validator signers (simulate 10 validators)
    let num_validators = 10;
    let mut validators = Vec::new();
    for i in 0..num_validators {
        validators.push(ValidatorSigner::random([i as u8; 32]));
    }
    
    // Step 1: Create initial BatchStore
    let batch_store = Arc::new(BatchStore::new(
        epoch,
        true, // is_new_epoch
        0,    // last_certified_time
        db.clone(),
        memory_quota,
        db_quota,
        batch_quota,
        validators[0].clone(),
        Duration::from_secs(60).as_micros() as u64,
    ));
    
    // Step 2: Each validator fills their quota with maximum-sized batches
    let max_batch_size = 1024 * 1024; // ~1 MB per batch
    let batches_per_validator = 300; // Fill 300 MB per validator
    
    for (idx, validator) in validators.iter().enumerate() {
        for batch_idx in 0..batches_per_validator {
            // Create batch with large payload
            let mut txns = Vec::new();
            // Create transactions to fill ~1 MB
            let txn_size = max_batch_size / 10; // ~100 KB per txn, 10 txns = 1 MB
            for _ in 0..10 {
                let large_txn = create_large_transaction(txn_size);
                txns.push(large_txn);
            }
            
            let batch = Batch::new_v2(
                BatchId::new_for_test(idx as u64 * 1000 + batch_idx as u64),
                txns,
                epoch,
                u64::MAX, // far future expiration
                validator.author(),
                0,
                BatchKind::Standard,
            );
            
            // Persist batch
            let persist_request = PersistedValue::from(batch);
            batch_store.persist(vec![persist_request]);
        }
    }
    
    println!("Filled database with {} validators × {} batches = {} batches",
             num_validators, batches_per_validator, num_validators * batches_per_validator);
    println!("Expected memory spike: {} MB", (num_validators * 300));
    
    // Step 3: Measure memory before recovery
    let memory_before = get_current_memory_usage();
    
    // Step 4: Simulate node restart - create new BatchStore (recovery path)
    let recovered_batch_store = Arc::new(BatchStore::new(
        epoch,
        false, // NOT a new epoch - triggers recovery path
        0,
        db.clone(),
        memory_quota,
        db_quota,
        batch_quota,
        validators[0].clone(),
        Duration::from_secs(60).as_micros() as u64,
    ));
    
    // Step 5: Measure memory after recovery
    let memory_after = get_current_memory_usage();
    let memory_spike = memory_after - memory_before;
    
    println!("Memory spike during recovery: {} MB", memory_spike / (1024 * 1024));
    
    // Assert that memory spike is unreasonably large
    assert!(memory_spike > memory_quota as usize * 2,
            "Memory spike ({} bytes) should exceed memory quota significantly",
            memory_spike);
    
    // This test demonstrates the vulnerability:
    // - During normal operation, batches are limited by quota
    // - During recovery, ALL batches are loaded into memory simultaneously
    // - Memory spike far exceeds intended memory_quota limits
}

fn create_large_transaction(size: usize) -> SignedTransaction {
    // Create a transaction with large payload to reach target size
    // Implementation details omitted for brevity
    unimplemented!("Create transaction with ~{} bytes payload", size)
}

fn get_current_memory_usage() -> usize {
    // Get current process memory usage
    // Could use procinfo crate or /proc/self/status on Linux
    unimplemented!("Read current memory usage")
}
```

**Notes:**
- The vulnerability exists in the recovery code path when `is_new_epoch = false`
- Normal operation correctly enforces quotas, but recovery bypasses them
- The attack is realistic within the Byzantine fault tolerance threat model
- Impact scales linearly with validator set size

### Citations

**File:** config/src/config/quorum_store_config.rs (L133-135)
```rust
            memory_quota: 120_000_000,
            db_quota: 300_000_000,
            batch_quota: 300_000,
```

**File:** consensus/src/quorum_store/batch_store.rs (L64-84)
```rust
    pub(crate) fn update_quota(&mut self, num_bytes: usize) -> anyhow::Result<StorageMode> {
        if self.batch_balance == 0 {
            counters::EXCEEDED_BATCH_QUOTA_COUNT.inc();
            bail!("Batch quota exceeded ");
        }

        if self.db_balance >= num_bytes {
            self.batch_balance -= 1;
            self.db_balance -= num_bytes;

            if self.memory_balance >= num_bytes {
                self.memory_balance -= num_bytes;
                Ok(StorageMode::MemoryAndPersisted)
            } else {
                Ok(StorageMode::PersistedOnly)
            }
        } else {
            counters::EXCEEDED_STORAGE_QUOTA_COUNT.inc();
            bail!("Storage quota exceeded ");
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L161-175)
```rust
        } else {
            Self::populate_cache_and_gc_expired_batches_v1(
                db_clone.clone(),
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
            Self::populate_cache_and_gc_expired_batches_v2(
                db_clone,
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
```

**File:** consensus/src/quorum_store/batch_store.rs (L299-307)
```rust
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read v1 data from db");
        info!(
            epoch = current_epoch,
            "QS: Read v1 batches from storage. Len: {}, Last Cerified Time: {}",
            db_content.len(),
            last_certified_time
        );
```

**File:** consensus/src/quorum_store/batch_store.rs (L509-512)
```rust
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L133-138)
```rust
    fn get_all_batches_v2(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfoExt>>> {
        let mut iter = self.db.iter::<BatchV2Schema>()?;
        iter.seek_to_first();
        iter.map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<HashValue, PersistedValue<BatchInfoExt>>>>()
    }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L137-171)
```rust
    fn ensure_max_limits(&self, batches: &[Batch<BatchInfoExt>]) -> anyhow::Result<()> {
        let mut total_txns = 0;
        let mut total_bytes = 0;
        for batch in batches.iter() {
            ensure!(
                batch.num_txns() <= self.max_batch_txns,
                "Exceeds batch txn limit {} > {}",
                batch.num_txns(),
                self.max_batch_txns,
            );
            ensure!(
                batch.num_bytes() <= self.max_batch_bytes,
                "Exceeds batch bytes limit {} > {}",
                batch.num_bytes(),
                self.max_batch_bytes,
            );

            total_txns += batch.num_txns();
            total_bytes += batch.num_bytes();
        }
        ensure!(
            total_txns <= self.max_total_txns,
            "Exceeds total txn limit {} > {}",
            total_txns,
            self.max_total_txns,
        );
        ensure!(
            total_bytes <= self.max_total_bytes,
            "Exceeds total bytes limit: {} > {}",
            total_bytes,
            self.max_total_bytes,
        );

        Ok(())
    }
```
