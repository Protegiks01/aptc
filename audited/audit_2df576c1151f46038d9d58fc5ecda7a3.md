# Audit Report

## Title
Permanent Rate Limit Counter Corruption in Faucet Service Due to Lack of Atomic State Management Between check() and complete()

## Summary
The Aptos faucet's rate limiting system suffers from a critical state consistency vulnerability where system crashes between `check()` and `complete()` leave rate limit counters permanently incremented in Redis, causing indefinite DoS for affected users until the daily TTL expires. [1](#0-0) 

## Finding Description
The `CheckerTrait` defines a two-phase protocol for rate limiting: `check()` optimistically increments counters, and `complete()` decrements them only on server errors (HTTP 500). This design lacks atomicity and crash recovery mechanisms. [2](#0-1) 

The `RedisRatelimitChecker.check()` atomically increments the Redis counter during validation. If the system crashes after this increment but before `complete()` is called, the counter remains permanently elevated in Redis, which persists across process restarts. [3](#0-2) 

The `complete()` method only decrements on 500 errors, meaning successful or aborted requests never trigger cleanup. The execution flow in `fund_inner()` shows the vulnerability window: [4](#0-3) 

After `check()` succeeds, if a crash occurs before reaching: [5](#0-4) 

The user never receives funds but is permanently charged a rate limit slot. With `max_requests_per_day=5`, five crashes result in permanent DoS until the next day's TTL expiration. [6](#0-5) 

The code comments explicitly acknowledge abandoned attempts at atomic transactions, leaving this vulnerability unmitigated. No cleanup mechanism exists via `spawn_periodic_tasks()`. [7](#0-6) 

## Impact Explanation
**Medium Severity** - State inconsistencies requiring intervention. While this causes permanent DoS of the faucet service, it:
- Only affects testnet/devnet token distribution (not mainnet or consensus)
- Doesn't impact blockchain consensus, execution, or validator operations
- Can be manually mitigated by clearing Redis keys
- Auto-recovers after 24 hours via TTL expiration

However, it represents a clear violation of the **State Consistency** invariant where rate limit counters diverge from actual successful transactions.

## Likelihood Explanation
**High Likelihood** - System crashes are common operational events (OOM kills, panics, deployment restarts, infrastructure failures). Each crash during request processing orphans counters. In high-traffic testnet environments, this accumulates rapidly, causing widespread user lockouts.

## Recommendation
Implement one of these solutions:

**Option 1: Pre-decrement on check(), increment only in complete() on success**
- `check()`: Read counter, reject if over limit (no state modification)
- `complete()`: Increment only if `!response_is_500`
- Eliminates crash vulnerability at cost of potential race conditions

**Option 2: Write-ahead log with periodic cleanup**
- Log pending requests to Redis with short TTL (e.g., 5 minutes)
- `check()`: Create pending entry, increment counter
- `complete()`: Remove pending entry
- Periodic task: Decrement counters for expired pending entries

**Option 3: Optimistic locking with request IDs**
- Generate unique request ID in `check()`
- Store ID with incremented counter
- `complete()`: Verify ID exists before considering request complete
- Periodic cleanup of orphaned IDs

## Proof of Concept

```rust
#[tokio::test]
async fn test_crash_leaves_counter_incremented() {
    // Setup Redis ratelimit checker with max_requests_per_day = 2
    let config = RedisRatelimitCheckerConfig {
        database_address: "localhost".to_string(),
        max_requests_per_day: 2,
        // ... other config
    };
    let checker = RedisRatelimitChecker::new(config).await.unwrap();
    
    let checker_data = CheckerData {
        source_ip: "192.168.1.100".parse().unwrap(),
        // ... other fields
    };
    
    // First request: check() succeeds, increments counter
    let result1 = checker.check(checker_data.clone(), false).await.unwrap();
    assert!(result1.is_empty());
    
    // SIMULATE CRASH: complete() never called
    
    // Second request: check() succeeds, increments counter again
    let result2 = checker.check(checker_data.clone(), false).await.unwrap();
    assert!(result2.is_empty());
    
    // SIMULATE CRASH: complete() never called
    
    // Third request: Now permanently blocked (counter = 2, limit = 2)
    let result3 = checker.check(checker_data.clone(), false).await.unwrap();
    assert!(!result3.is_empty()); // BLOCKED despite receiving 0 actual funds
    
    // User is DoS'd for 24 hours despite never being funded
}
```

## Notes

This vulnerability is specific to the `RedisRatelimitChecker` implementation. The `MemoryRatelimitChecker` suffers the same issue but recovers on process restart since state is non-persistent: [8](#0-7) [9](#0-8) 

The faucet service is auxiliary infrastructure for testnet token distribution and does not participate in blockchain consensus or mainnet operations. While this represents a legitimate availability and state consistency issue, it does not threaten the core Aptos blockchain protocol.

### Citations

**File:** crates/aptos-faucet/core/src/checkers/mod.rs (L42-63)
```rust
#[async_trait]
#[enum_dispatch]
pub trait CheckerTrait: Sync + Send + 'static {
    /// Returns a list of rejection reasons for the request, if any. If dry_run
    /// is set, if this Checker would store anything based on the request, it
    /// instead will not. This is useful for the is_eligible endpoint.
    async fn check(
        &self,
        data: CheckerData,
        dry_run: bool,
    ) -> Result<Vec<RejectionReason>, AptosTapError>;

    /// If the Checker wants to do anything after the funding has completed, it
    /// may do so in this function. For example, for the storage Checkers, this
    /// function is responsible for marking a request in storage as complete,
    /// in both success and failure cases. It can also store additional metadata
    /// included in CompleteData that we might have from the call to the Funder.
    /// No dry_run flag for this, because we should never need to run this in
    /// dry_run mode.
    async fn complete(&self, _data: CompleteData) -> Result<(), AptosTapError> {
        Ok(())
    }
```

**File:** crates/aptos-faucet/core/src/checkers/mod.rs (L69-76)
```rust
    /// This function will be called once at startup. In it, the trait implementation
    /// should spawn any periodic tasks that it wants and return handles to them.
    /// If tasks want to signal that there is an issue, all they have to do is return.
    /// If the task wants to tolerate some errors, e.g. only cause the process to die
    /// if the task has failed n times, it must handle that itself and only return
    /// when it wants this to happen.
    // Sadly we can't use ! here yet: https://github.com/rust-lang/rust/issues/35121.
    fn spawn_periodic_tasks(&self, _join_set: &mut JoinSet<anyhow::Result<()>>) {}
```

**File:** crates/aptos-faucet/core/src/checkers/redis_ratelimit.rs (L113-145)
```rust

/// The RedisRatelimitChecker backend uses redis to ratelimit requests to the tap. Unlike
/// the PostgresStorage backend, it does not store full information for each
/// request. Instead, it uses counters to track limits. This is heavily inspired
/// by https://redis.com/redis-best-practices/basic-rate-limiting/.
///
/// We use a generic key (e.g. IP address or Firebase UID).
///
/// If we're not careful, it is possible for people to exceed the intended limit
/// by sending many requests simultaneously. We avoid this problem with this
/// order of operations:
///   1. Read the current value of the limit for the given key (e.g. IP / Firebase UID).
///   2. If value is greater than limit, reject.
///   3. Otherwise, increment and set TTL if necessary.
///   4. Increment returns the new value. Check if this is greater than the limit also.
///
/// Incrementing the limit is an atomic operation (meaning each client will see
/// value increment, never reading the same value), so steps 1 and 2 are not
/// actually necessary for correctness. Instead, steps 1 and 2 are just an optimization
/// to avoid incrementing the limit unnecessarily if the limit has already been
/// reached. With steps 1 and 2 we end up having more unnecessary reads when
/// they're under their limit vs more unnecessary writes when they're over their
/// limit, but we'll happily take more reads over more writes.
///
/// Note: Previously I made an attempt (d4fbf6db675e9036a967b52bf8d13e1b2566787e) at
/// doing these steps atomically, but it became very unwieldy:
///   1. Start a transaction.
///   2. Increment current value for limit for source key, set TTL if necessary.
///   3. If value is greater than limit, revert the transaction.
///
/// This second way leaves a small window for someone to slip in multiple requests,
/// therein blowing past the configured limit, but it's a very small window, so we'll
/// worry about it as a followup: https://github.com/aptos-labs/aptos-tap/issues/15.
```

**File:** crates/aptos-faucet/core/src/checkers/redis_ratelimit.rs (L261-293)
```rust
        // Atomically increment the counter for the given key, creating it and setting
        // the expiration time if it doesn't already exist.
        if !dry_run {
            let incremented_limit_value = match limit_value {
                Some(_) => conn.incr(&key, 1).await.map_err(|e| {
                    AptosTapError::new_with_error_code(
                        format!("Failed to increment redis key {}: {}", key, e),
                        AptosTapErrorCode::StorageError,
                    )
                })?,
                // If the limit value doesn't exist, create it and set the
                // expiration time.
                None => {
                    let (incremented_limit_value,): (i64,) = redis::pipe()
                        .atomic()
                        .incr(&key, 1)
                        // Expire at the end of the day roughly.
                        .expire(&key, seconds_until_next_day as usize)
                        // Only set the expiration if one isn't already set.
                        // Only works with Redis 7 sadly.
                        // .arg("NX")
                        .ignore()
                        .query_async(&mut *conn)
                        .await
                        .map_err(|e| {
                            AptosTapError::new_with_error_code(
                                format!("Failed to increment value for redis key {}: {}", key, e),
                                AptosTapErrorCode::StorageError,
                            )
                        })?;
                    incremented_limit_value
                },
            };
```

**File:** crates/aptos-faucet/core/src/checkers/redis_ratelimit.rs (L308-335)
```rust
    async fn complete(&self, data: CompleteData) -> Result<(), AptosTapError> {
        if !data.response_is_500 {
            return Ok(());
        }

        let mut conn = self
            .get_redis_connection()
            .await
            .map_err(|e| AptosTapError::new_with_error_code(e, AptosTapErrorCode::StorageError))?;

        // Generate a key corresponding to this identifier and the current day. In the
        // JWT case we re-verify the JWT. This is inefficient, but these failures are
        // extremely rare so I don't refactor for now.
        let key_prefix = self.ratelimit_key_provider.ratelimit_key_prefix();
        let key_value = self
            .ratelimit_key_provider
            .ratelimit_key_value(&data.checker_data)
            .await?;
        let (key, _) = self.get_key_and_secs_until_next_day(key_prefix, &key_value);

        let _: () = conn.decr(&key, 1).await.map_err(|e| {
            AptosTapError::new_with_error_code(
                format!("Failed to decrement value for redis key {}: {}", key, e),
                AptosTapErrorCode::StorageError,
            )
        })?;
        Ok(())
    }
```

**File:** crates/aptos-faucet/core/src/endpoints/fund.rs (L263-270)
```rust
        for checker in &self.checkers {
            rejection_reasons.extend(checker.check(checker_data.clone(), dry_run).await.map_err(
                |e| AptosTapError::new_with_error_code(e, AptosTapErrorCode::CheckerError),
            )?);
            if !rejection_reasons.is_empty() && self.return_rejections_early {
                break;
            }
        }
```

**File:** crates/aptos-faucet/core/src/endpoints/fund.rs (L332-347)
```rust
        if !bypass {
            let response_is_500 = match &fund_result {
                Ok(_) => false,
                Err(e) => e.error_code.status().is_server_error(),
            };
            let complete_data = CompleteData {
                checker_data,
                txn_hashes: txn_hashes.clone(),
                response_is_500,
            };
            for checker in &self.checkers {
                checker.complete(complete_data.clone()).await.map_err(|e| {
                    AptosTapError::new_with_error_code(e, AptosTapErrorCode::CheckerError)
                })?;
            }
        }
```

**File:** crates/aptos-faucet/core/src/checkers/memory_ratelimit.rs (L86-91)
```rust
        } else if !dry_run {
            *requests_today += 1;
        }

        Ok(vec![])
    }
```

**File:** crates/aptos-faucet/core/src/checkers/memory_ratelimit.rs (L93-102)
```rust
    async fn complete(&self, data: CompleteData) -> Result<(), AptosTapError> {
        if data.response_is_500 {
            *self
                .ip_to_requests_today
                .lock()
                .await
                .get_or_insert_mut(data.checker_data.source_ip, || 1) -= 1;
        }
        Ok(())
    }
```
