# Audit Report

## Title
Database Inconsistency Due to Non-Atomic Two-Phase Commit in EventStorePruner

## Summary
The `EventStorePruner.prune()` method performs non-atomic writes to two separate databases (indexer DB and ledger DB) without distributed transaction coordination. If the first write succeeds but the second fails, the system is left in an inconsistent state with divergent pruning progress markers.

## Finding Description

The `EventStorePruner.prune()` method violates the **State Consistency** invariant through a two-phase commit problem. The method writes to two independent RocksDB instances sequentially without any distributed transaction mechanism: [1](#0-0) 

While each individual `write_schemas()` call is atomic within its own database via RocksDB's WriteBatch mechanism [2](#0-1) , there is **no atomicity guarantee across the two databases**.

**Failure Scenario:**

1. Line 78 succeeds: Indexer DB commits with events pruned and `EventPrunerProgress` set to `target_version`
2. System failure occurs (disk error, I/O failure, process crash, OOM killer)
3. Line 80 fails: Ledger DB write never completes
4. Result: 
   - Indexer DB: progress = `target_version`, events pruned
   - Ledger DB: progress = `current_progress`, events still present

**No Recovery Mechanism:**

The pruner worker simply logs errors and continues [3](#0-2) . There is no rollback of the successful indexer DB write, and no consistency checking between the databases.

The `EventStorePruner.new()` initialization only reads progress from the ledger DB [4](#0-3) , with no verification that both databases are synchronized.

While validation tools exist [5](#0-4) , they are manual debugging utilities, not automatic recovery mechanisms.

## Impact Explanation

**Medium Severity** per Aptos bug bounty criteria: "State inconsistencies requiring intervention"

This violates **Critical Invariant #4**: "State Consistency: State transitions must be atomic and verifiable via Merkle proofs"

**Concrete Impact:**
- Divergent database states requiring manual intervention to resynchronize
- Query result inconsistencies depending on which database is accessed
- Corrupted pruning metadata that persists across restarts
- Potential for cascading failures as inconsistency grows with each pruning cycle
- No automatic detection or repair mechanism

## Likelihood Explanation

**Medium-to-High Likelihood:**

This can occur during:
- Disk I/O errors (hardware failures, filesystem corruption)
- Process termination via signals (SIGKILL, OOM killer)
- Storage quota exhaustion between the two writes
- Unexpected RocksDB errors (documented at [6](#0-5) )

The pruner runs continuously in production [7](#0-6) , making the window of vulnerability perpetual. Given typical node uptimes and hardware failure rates, this is a realistic operational scenario.

## Recommendation

Implement one of the following solutions:

**Option 1: Write-Ahead Log (WAL) Pattern**
```rust
fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
    // Prepare both batches first
    let mut batch = SchemaBatch::new();
    let mut indexer_batch = /* ... */;
    
    // Build batches...
    
    // Write intent log FIRST
    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::EventPrunerPendingCommit,
        &DbMetadataValue::Version(target_version),
    )?;
    self.ledger_db.event_db().write_schemas(batch)?;
    
    // Then commit indexer DB
    if let Some(indexer_batch) = indexer_batch {
        self.expect_indexer_db().get_inner_db_ref().write_schemas(indexer_batch)?;
    }
    
    // Clear intent log on success
    let mut cleanup_batch = SchemaBatch::new();
    cleanup_batch.delete::<DbMetadataSchema>(&DbMetadataKey::EventPrunerPendingCommit)?;
    cleanup_batch.put::<DbMetadataSchema>(
        &DbMetadataKey::EventPrunerProgress,
        &DbMetadataValue::Version(target_version),
    )?;
    self.ledger_db.event_db().write_schemas(cleanup_batch)
}
```

**Option 2: Reverse Write Order**
Write to ledger DB first, then indexer DB. This way, failure leaves indexer DB behind (safe to catch up) rather than ahead (inconsistent).

**Option 3: Consistency Check on Startup**
In `EventStorePruner::new()`, verify both databases have matching progress markers and repair if divergent.

## Proof of Concept

```rust
#[test]
fn test_event_pruner_partial_commit_inconsistency() {
    use std::sync::Arc;
    use aptos_temppath::TempPath;
    
    let tmp_dir = TempPath::new();
    let indexer_tmp_dir = TempPath::new();
    
    // Setup DBs
    let aptos_db = AptosDB::new_for_test(&tmp_dir);
    let internal_indexer_db = /* setup indexer DB */;
    
    // Populate with events
    let mut batch = SchemaBatch::new();
    for version in 0..100 {
        aptos_db.event_store.put_events(
            version,
            &vec![/* sample events */],
            false,
            &mut batch,
        ).unwrap();
    }
    aptos_db.ledger_db.event_db().write_schemas(batch).unwrap();
    
    // Create pruner
    let pruner = EventStorePruner::new(
        Arc::clone(&aptos_db.ledger_db),
        0,
        Some(internal_indexer_db.clone()),
    ).unwrap();
    
    // Mock failure: Wrap ledger_db to fail on second write
    // (This would require dependency injection or testing hooks)
    
    // Attempt prune - should fail at line 80
    let result = pruner.prune(0, 50);
    assert!(result.is_err());
    
    // Verify inconsistency
    let indexer_progress = internal_indexer_db
        .get_inner_db_ref()
        .get::<InternalIndexerMetadataSchema>(&IndexerMetadataKey::EventPrunerProgress)
        .unwrap()
        .unwrap();
    
    let ledger_progress = aptos_db.ledger_db.event_db_raw()
        .get::<DbMetadataSchema>(&DbMetadataKey::EventPrunerProgress)
        .unwrap()
        .unwrap();
    
    // BUG: Progress markers diverge!
    assert_ne!(indexer_progress.expect_version(), ledger_progress.expect_version());
    
    // Indexer thinks events are pruned, ledger still has them
    assert!(internal_indexer_db.get_events_by_version(25).unwrap().is_empty());
    assert!(!aptos_db.ledger_db.event_db().get_events_by_version(25).unwrap().is_empty());
}
```

**Notes:**
- Individual `write_schemas()` calls ARE atomic within each database per RocksDB guarantees
- The vulnerability is the **lack of distributed transaction coordination** between two independent databases
- This is a system-level consistency bug, not a cryptographic or consensus vulnerability
- Manual validation tools exist but provide no automatic recovery

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L76-80)
```rust
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L90-94)
```rust
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;
```

**File:** storage/schemadb/src/lib.rs (L289-309)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
    }

    /// Writes a group of records wrapped in a [`SchemaBatch`].
    pub fn write_schemas(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &sync_write_option())
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-68)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
```

**File:** storage/aptosdb/src/db_debugger/validation.rs (L148-155)
```rust
pub fn verify_batch_txn_events(
    txns: &TransactionListWithProofV2,
    internal_db: &DB,
    start_version: u64,
) -> Result<()> {
    verify_transactions(txns, internal_db, start_version)?;
    verify_events(txns, internal_db, start_version)
}
```

**File:** storage/storage-interface/src/errors.rs (L9-36)
```rust
/// This enum defines errors commonly used among `AptosDB` APIs.
#[derive(Clone, Debug, Error)]
pub enum AptosDbError {
    /// A requested item is not found.
    #[error("{0} not found.")]
    NotFound(String),
    /// Requested too many items.
    #[error("Too many items requested: at least {0} requested, max is {1}")]
    TooManyRequested(u64, u64),
    #[error("Missing state root node at version {0}, probably pruned.")]
    MissingRootError(u64),
    /// Other non-classified error.
    #[error("AptosDB Other Error: {0}")]
    Other(String),
    #[error("AptosDB RocksDb Error: {0}")]
    RocksDbIncompleteResult(String),
    #[error("AptosDB RocksDB Error: {0}")]
    OtherRocksDbError(String),
    #[error("AptosDB bcs Error: {0}")]
    BcsError(String),
    #[error("AptosDB IO Error: {0}")]
    IoError(String),
    #[error("AptosDB Recv Error: {0}")]
    RecvError(String),
    #[error("AptosDB ParseInt Error: {0}")]
    ParseIntError(String),
    #[error("Hot state not configured properly")]
    HotStateError,
```
