# Audit Report

## Title
DKG Indefinite Blocking on f+1 Network Failures Causes Epoch Transition Liveness Failure

## Summary
When `send_rpc()` fails for f+1 validators during Distributed Key Generation (DKG), the ReliableBroadcast protocol enters an infinite retry loop that cannot reach the required quorum threshold (2n/3+1 voting power). This causes DKG to never complete, blocking epoch transitions indefinitely and requiring manual governance intervention via `force_end_epoch()` to recover.

## Finding Description

The DKG protocol uses a ReliableBroadcast mechanism to aggregate transcripts from validators. The critical flaw exists in how the system handles persistent network failures:

**Step 1: DKG Initiation and send_rpc() calls**

The `send_rpc()` function is a simple wrapper that returns errors when RPC fails: [1](#0-0) 

**Step 2: ReliableBroadcast Infinite Retry Loop**

When `send_rpc()` fails, the ReliableBroadcast implementation enters an infinite retry loop with exponential backoff. The critical code shows retries continue indefinitely until aggregation succeeds: [2](#0-1) 

The backoff policy is an infinite iterator from `tokio-retry` crate with no maximum retry limit, only a maximum delay cap. The loop continues until aggregation completes (line 188) or runs forever retrying failed nodes.

**Step 3: Quorum Threshold Calculation**

Transcript aggregation requires reaching Byzantine fault-tolerant quorum (2n/3+1 voting power): [3](#0-2) 

The aggregation checks if sufficient voting power has been collected: [4](#0-3) 

**Step 4: Mathematical Impossibility with f+1 Failures**

For n validators with Byzantine threshold f = ⌊(n-1)/3⌋:
- If f+1 validators have persistent network failures
- Available validators: n - (f+1) = n - f - 1
- Required quorum: 2n/3 + 1
- For n=4: f=1, available=2, required=3 → **Cannot reach quorum**
- For n=7: f=2, available=4, required=5 → **Cannot reach quorum**

**Step 5: Epoch Transition Deadlock**

The epoch timeout mechanism triggers `try_start()` to initiate DKG, but if DKG is already in progress for the current epoch, it returns early without taking any action: [5](#0-4) 

The block prologue checks epoch timeout and calls `try_start()`: [6](#0-5) 

**Result:** The chain becomes stuck in the same epoch indefinitely because:
1. DKG never completes (infinite retry without reaching quorum)
2. No DKGResult transaction is created to trigger epoch change
3. Subsequent epoch timeouts call `try_start()` which sees DKG already in progress and returns early
4. The only recovery path is manual governance intervention via `force_end_epoch()`

## Impact Explanation

This qualifies as **High Severity** under Aptos bug bounty criteria for "Significant protocol violations" and "Validator node slowdowns":

**Blocked Functionality:**
- **Epoch transitions**: Chain remains stuck in same epoch indefinitely
- **Validator set rotation**: New validators cannot join, leaving validators cannot exit
- **Randomness generation**: All randomness-dependent features unavailable
- **On-chain config updates**: Gas schedules, consensus configs, feature flags cannot be updated as they take effect at epoch boundaries

**Recovery Requirements:**
- Requires governance proposal and vote to call `force_end_epoch()`
- During stuck period, validator set cannot adapt to failures
- Config updates cannot be applied even if critical

The vulnerability violates the liveness invariant that the chain should continue making progress autonomously.

## Likelihood Explanation

**Likelihood: Medium to High**

This can occur through multiple realistic scenarios:

1. **Network Partitions**: Cloud provider outages, BGP routing issues, or datacenter connectivity problems affecting f+1 validators
2. **Hardware Failures**: Simultaneous hardware failures on f+1 validator nodes
3. **Targeted DoS**: Attacker launching network-level attacks against f+1 validators during DKG window
4. **Software Crashes**: Bugs causing f+1 validator processes to crash during DKG

For a validator set of 100 nodes with f=33:
- Requires 34 validators to experience simultaneous persistent network failures
- Realistic during major cloud outages or coordinated attacks
- Once stuck, requires governance intervention (multi-day process)

## Recommendation

Implement an automatic DKG timeout mechanism to prevent indefinite blocking:

**Solution 1: DKG Session Timeout**

Add a timeout field to `DKGSessionState` and check it in `try_start()`:

```move
// In dkg.move
struct DKGSessionState has copy, store, drop {
    metadata: DKGSessionMetadata,
    start_time_us: u64,
    transcript: vector<u8>,
    timeout_us: u64,  // NEW: Add timeout duration
}

// In reconfiguration_with_dkg.move
public(friend) fun try_start() {
    let incomplete_dkg_session = dkg::incomplete_session();
    if (option::is_some(&incomplete_dkg_session)) {
        let session = option::borrow(&incomplete_dkg_session);
        let current_time = timestamp::now_microseconds();
        
        // Check if session timed out
        if (current_time - dkg::session_start_time(session) > dkg::session_timeout(session)) {
            // Timeout exceeded, clear stale session and start new one
            dkg::try_clear_incomplete_session(@aptos_framework);
        } else if (dkg::session_dealer_epoch(session) == reconfiguration::current_epoch()) {
            return
        }
    };
    // Continue with normal DKG start...
}
```

**Solution 2: ReliableBroadcast Maximum Attempt Limit**

Add a maximum retry limit to the ReliableBroadcast that aborts after exhausting attempts:

```rust
// In reliable-broadcast/src/lib.rs
pub struct ReliableBroadcast<Req: RBMessage, TBackoff, Res: RBMessage = Req> {
    // ... existing fields ...
    max_retries_per_peer: Option<usize>,  // NEW
}

// In the retry logic
let mut retry_counts: HashMap<Author, usize> = HashMap::new();
// ... in the error handling branch ...
Err(e) => {
    log_rpc_failure(e, receiver);
    
    let retry_count = retry_counts.entry(receiver).or_insert(0);
    *retry_count += 1;
    
    if let Some(max) = self.max_retries_per_peer {
        if *retry_count >= max {
            // Mark peer as permanently failed, stop retrying
            continue;
        }
    }
    // ... continue with backoff and retry ...
}
```

**Recommended Approach:** Implement both solutions - DKG timeout (e.g., 5 minutes) to automatically abort stuck sessions, plus per-peer retry limits in ReliableBroadcast to prevent indefinite resource consumption.

## Proof of Concept

**Reproduction Steps:**

1. **Setup**: Deploy Aptos testnet with n=4 validators (f=1 Byzantine threshold)

2. **Trigger DKG**: Initiate epoch transition that starts DKG protocol

3. **Simulate Network Failures**: Use network isolation tools (iptables/tc) to block RPC traffic for 2 validators (f+1):
```bash
# On validators V2 and V3, block incoming DKG RPC ports
iptables -A INPUT -p tcp --dport <DKG_RPC_PORT> -j DROP
```

4. **Observe Behavior**:
```rust
// Monitor DKG manager logs - will show infinite retry attempts:
// "[sampled] rpc to <validator> failed, error ..."
// This continues indefinitely

// Check epoch state remains unchanged:
// curl http://validator:8080/v1 | jq '.epoch'
// Returns same epoch number indefinitely

// Verify transcript aggregation stuck below quorum:
// Only 2 out of 4 validators can respond
// Quorum requires 3 validators (2*4/3 + 1 = 3)
// Aggregation never completes
```

5. **Recovery**: Requires governance to call `force_end_epoch()`:
```move
// Governance proposal to execute:
aptos_governance::force_end_epoch(aptos_framework_signer);
```

**Expected Result**: After step 3, DKG enters infinite retry loop, epoch never advances, and manual intervention is required.

**Notes**

The vulnerability stems from the design assumption that network failures are transient. The ReliableBroadcast infinite retry loop is appropriate for temporary failures but becomes problematic when f+1 validators have persistent failures exceeding the Byzantine threshold. While `force_end_epoch()` provides a recovery mechanism, requiring manual governance intervention for each DKG timeout represents a significant liveness vulnerability, especially as it blocks validator set changes, config updates, and randomness generation until resolved.

The attack vector doesn't require malicious validators—natural network partitions during cloud outages or coordinated network-level attacks can trigger this condition. The combination of infinite retry without timeout and the epoch transition guard creates a deadlock requiring out-of-band intervention.

### Citations

**File:** dkg/src/network_interface.rs (L37-47)
```rust
    pub async fn send_rpc(
        &self,
        peer: PeerId,
        message: DKGMessage,
        rpc_timeout: Duration,
    ) -> Result<DKGMessage, Error> {
        let peer_network_id = self.get_peer_network_id_for_peer(peer);
        self.network_client
            .send_to_peer_rpc(message, rpc_timeout, peer_network_id)
            .await
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L183-206)
```rust
                    Some(result) = aggregate_futures.next() => {
                        let (receiver, result) = result.expect("spawned task must succeed");
                        match result {
                            Ok(may_be_aggragated) => {
                                if let Some(aggregated) = may_be_aggragated {
                                    return Ok(aggregated);
                                }
                            },
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
                        }
                    },
                    else => unreachable!("Should aggregate with all responses")
                }
            }
        }
```

**File:** types/src/validator_verifier.rs (L206-214)
```rust
    pub fn new(validator_infos: Vec<ValidatorConsensusInfo>) -> Self {
        let total_voting_power = sum_voting_power(&validator_infos);
        let quorum_voting_power = if validator_infos.is_empty() {
            0
        } else {
            total_voting_power * 2 / 3 + 1
        };
        Self::build_index(validator_infos, quorum_voting_power, total_voting_power)
    }
```

**File:** dkg/src/transcript_aggregation/mod.rs (L122-134)
```rust
        let threshold = self.epoch_state.verifier.quorum_voting_power();
        let power_check_result = self
            .epoch_state
            .verifier
            .check_voting_power(trx_aggregator.contributors.iter(), true);
        let new_total_power = match &power_check_result {
            Ok(x) => Some(*x),
            Err(VerifyError::TooLittleVotingPower { voting_power, .. }) => Some(*voting_power),
            _ => None,
        };
        let maybe_aggregated = power_check_result
            .ok()
            .map(|_| trx_aggregator.trx.clone().unwrap());
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration_with_dkg.move (L22-40)
```text
    /// Trigger a reconfiguration with DKG.
    /// Do nothing if one is already in progress.
    public(friend) fun try_start() {
        let incomplete_dkg_session = dkg::incomplete_session();
        if (option::is_some(&incomplete_dkg_session)) {
            let session = option::borrow(&incomplete_dkg_session);
            if (dkg::session_dealer_epoch(session) == reconfiguration::current_epoch()) {
                return
            }
        };
        reconfiguration_state::on_reconfig_start();
        let cur_epoch = reconfiguration::current_epoch();
        dkg::start(
            cur_epoch,
            randomness_config::current(),
            stake::cur_validator_consensus_infos(),
            stake::next_validator_consensus_infos(),
        );
    }
```

**File:** aptos-move/framework/aptos-framework/sources/block.move (L244-246)
```text
        if (timestamp - reconfiguration::last_reconfiguration_time() >= epoch_interval) {
            reconfiguration_with_dkg::try_start();
        };
```
