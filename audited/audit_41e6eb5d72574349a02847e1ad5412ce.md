# Audit Report

## Title
Master State Desynchronization in Indexer GRPC Manager Causing Data Upload Failures and Stale Data

## Summary
The `is_master` boolean field in `GrpcManager` is statically initialized from configuration and never updated at runtime, while the `master_address` in `MetadataManager` is dynamically updated via heartbeat messages. This desynchronization can cause a node that should be master to fail to upload new indexer data, or a node that should be replica to incorrectly upload data, leading to indexer API unavailability or data corruption.

## Finding Description

The vulnerability stems from a fundamental architectural flaw where master state is tracked in two separate, unsynchronized locations:

1. **Static Master State**: The `is_master` boolean field in `GrpcManager` [1](#0-0)  is initialized once from configuration [2](#0-1)  and never updated.

2. **Dynamic Master Address**: The `master_address` in `MetadataManager` [3](#0-2)  is dynamically updated when receiving heartbeat messages from other GRPC managers [4](#0-3) .

The `is_master` field controls critical behavior:
- Whether `FileStoreUploader` starts [5](#0-4) 
- Whether `DataManager` watches file store version updates [6](#0-5) 

**Attack Scenario 1 - Master Failover Without Proper Update:**
1. Node A is configured with `is_master=true` and runs FileStoreUploader
2. Operator reconfigures Node B with `is_master=true` to become new master
3. Node B starts and sends heartbeat with `master_address=NodeB`
4. Node A receives heartbeat and updates its `MetadataManager.master_address` to NodeB
5. **Critical Bug**: Node A's `GrpcManager.is_master` remains `true`, so FileStoreUploader continues running on Node A
6. Node B's FileStoreUploader also runs
7. **Result**: Two nodes simultaneously upload to file store, causing conflicts/corruption

**Attack Scenario 2 - Master Becomes Replica:**
1. Node A is master (`is_master=true`)
2. Network topology changes require Node A to become replica
3. Node A receives heartbeat indicating different master address
4. **Critical Bug**: Node A's `is_master` remains `true`, continues uploading
5. Node A's behavior doesn't match its role as replica
6. **Result**: Incorrect node uploads data, actual master may not upload

**Attack Scenario 3 - Replica Should Become Master:**
1. Node B is replica (`is_master=false`, no FileStoreUploader running)
2. Master node fails, Node B should take over
3. Network updates indicate Node B is new master
4. **Critical Bug**: Node B's `is_master` remains `false`, FileStoreUploader never started
5. **Result**: No node uploads data, indexer stops updating, causing total API unavailability

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria for the following reasons:

**API Crashes/Unavailability**: When a node becomes master but `is_master` remains false, the FileStoreUploader never starts [7](#0-6) , causing no new blockchain data to be uploaded to the file store. This results in:
- Indexer API serving increasingly stale data
- Eventually causing complete API unavailability as cache exhausts
- Clients unable to query recent blockchain transactions

**Data Corruption**: When multiple nodes run FileStoreUploader simultaneously, they conflict when updating file store metadata [8](#0-7) , potentially corrupting the indexer database and requiring manual intervention to restore.

**Incorrect Data Serving**: The `watch_file_store_version` flag controls whether a node monitors file store updates [9](#0-8) . When desynchronized, replicas may not watch for updates, serving stale data to clients indefinitely.

## Likelihood Explanation

**Likelihood: High** - This vulnerability triggers in common operational scenarios:

1. **Master Failover**: Any planned or unplanned master failover where operator reconfigures nodes triggers this bug
2. **Rolling Updates**: During rolling updates where master assignment changes, nodes can enter desynchronized states
3. **Network Topology Changes**: Any reconfiguration of the indexer cluster topology risks desynchronization
4. **No Automatic Recovery**: Once desynchronized, there's no mechanism to detect or recover without manual intervention

The vulnerability is not theoretical - it's baked into the current architecture where heartbeat messages carry `master_address` updates but the local `is_master` flag never changes. The code even contains a comment acknowledging static master assumption [10](#0-9) , yet implements dynamic master address updates, creating this inconsistency.

## Recommendation

**Short-term Fix**: Remove dynamic `master_address` updates from heartbeat handling to match the static master assumption:

```rust
fn handle_grpc_manager_info(&self, address: GrpcAddress, info: GrpcManagerInfo) -> Result<()> {
    // Remove this line:
    // self.master_address.lock().unwrap().clone_from(&info.master_address);
    
    // Keep only the peer state tracking:
    let mut entry = self.grpc_managers.entry(address.clone()).or_insert(Peer::new(address));
    entry.value_mut().recent_states.push_back(info);
    if entry.value().recent_states.len() > MAX_NUM_OF_STATES_TO_KEEP {
        entry.value_mut().recent_states.pop_front();
    }
    Ok(())
}
```

**Long-term Fix**: Implement proper master election with dynamic role changes:

1. Add a mechanism to dynamically update `is_master` when master changes
2. Implement proper master election protocol with leader election
3. Add safe state transitions that stop FileStoreUploader before starting on new master
4. Add monitoring/alerting for master state inconsistencies
5. Implement consensus on master selection across all GRPC managers

Example implementation:
```rust
impl GrpcManager {
    pub(crate) async fn update_master_status(&mut self, new_is_master: bool) -> Result<()> {
        if self.is_master == new_is_master {
            return Ok(());
        }
        
        if new_is_master {
            // Transitioning to master
            self.start_file_store_uploader().await?;
        } else {
            // Transitioning to replica
            self.stop_file_store_uploader().await?;
        }
        
        self.is_master = new_is_master;
        IS_MASTER.set(new_is_master as i64);
        Ok(())
    }
}
```

## Proof of Concept

```rust
// Reproduction steps:

// 1. Start two GrpcManager instances
// Node A with config: is_master=true
let config_a = IndexerGrpcManagerConfig {
    chain_id: 1,
    is_master: true,
    self_advertised_address: "node-a:50051".to_string(),
    grpc_manager_addresses: vec!["node-b:50051".to_string()],
    // ... other config
};
let manager_a = GrpcManager::new(&config_a).await;

// Node B with config: is_master=false
let config_b = IndexerGrpcManagerConfig {
    chain_id: 1,
    is_master: false,
    self_advertised_address: "node-b:50051".to_string(),
    grpc_manager_addresses: vec!["node-a:50051".to_string()],
    // ... other config
};
let manager_b = GrpcManager::new(&config_b).await;

// 2. Start both managers
manager_a.start(&config_a.service_config);
manager_b.start(&config_b.service_config);

// 3. Verify initial state
assert_eq!(manager_a.is_master, true);  // Node A is master
assert_eq!(manager_b.is_master, false); // Node B is replica

// 4. Simulate master failover: restart Node B as master
let config_b_master = IndexerGrpcManagerConfig {
    chain_id: 1,
    is_master: true,  // Now master
    self_advertised_address: "node-b:50051".to_string(),
    grpc_manager_addresses: vec!["node-a:50051".to_string()],
    // ... other config
};
let manager_b_new = GrpcManager::new(&config_b_master).await;
manager_b_new.start(&config_b_master.service_config);

// 5. Node B sends heartbeat with master_address="node-b:50051"
// Node A receives it and updates MetadataManager.master_address to "node-b:50051"
// But Node A's GrpcManager.is_master stays true

// 6. Verify the desynchronization
// Node A thinks it's still master (is_master=true)
assert_eq!(manager_a.is_master, true);
// But MetadataManager knows Node B is master
let master_addr = manager_a.get_metadata_manager().master_address.lock().unwrap();
assert_eq!(master_addr.as_ref().unwrap(), "node-b:50051");

// 7. CRITICAL BUG: Both nodes now run FileStoreUploader
// Node A: is_master=true -> FileStoreUploader running
// Node B: is_master=true -> FileStoreUploader running
// Result: Conflicts in file store uploads, potential data corruption

// 8. Test stale data scenario
// Stop Node B's FileStoreUploader manually
// Node A continues serving but with is_master=true, doesn't watch file store
// Result: Node A serves stale cached data without updates from file store
```

**Notes**

This vulnerability affects the Aptos indexer infrastructure rather than core consensus, but it can cause significant operational issues including API unavailability and data corruption. The root cause is architectural - attempting to support dynamic master changes via heartbeat messages while maintaining a static `is_master` configuration flag. The system needs either full static master configuration (remove dynamic updates) or full dynamic master election (implement proper state transitions).

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L27-27)
```rust
    is_master: bool,
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L87-87)
```rust
            is_master: config.is_master,
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L112-121)
```rust
            if self.is_master {
                s.spawn(async move {
                    self.file_store_uploader
                        .lock()
                        .await
                        .start(self.data_manager.clone(), tx)
                        .await
                        .unwrap();
                });
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L135-137)
```rust
    // NOTE: We assume the master is statically configured for now.
    master_address: Mutex<Option<GrpcAddress>>,
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L552-556)
```rust
    fn handle_grpc_manager_info(&self, address: GrpcAddress, info: GrpcManagerInfo) -> Result<()> {
        self.master_address
            .lock()
            .unwrap()
            .clone_from(&info.master_address);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L179-206)
```rust
        let watch_file_store_version = !is_master;

        if is_master {
            // For master, we need to wait for the FileStoreUploader to finish the recover to get
            // the true file_store_version.
            info!("Waiting for FileStoreUploader recovering.");
            match file_store_uploader_recover_rx.await {
                Ok(_) => {},
                Err(_) => panic!("Should not happen!"),
            };
            let cache = self.cache.read().await;
            self.update_file_store_version_in_cache(&cache, /*version_can_go_backward=*/ true)
                .await;
        }

        info!("Starting DataManager loop.");

        'out: loop {
            let _timer = TIMER
                .with_label_values(&["data_manager_main_loop"])
                .start_timer();
            let cache = self.cache.read().await;
            if watch_file_store_version {
                self.update_file_store_version_in_cache(
                    &cache, /*version_can_go_backward=*/ false,
                )
                .await;
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L120-181)
```rust
    pub(crate) async fn start(
        &mut self,
        data_manager: Arc<DataManager>,
        recover_tx: Sender<()>,
    ) -> Result<()> {
        let (version, batch_metadata) = self.recover().await?;

        let mut file_store_operator = FileStoreOperatorV2::new(
            MAX_SIZE_PER_FILE,
            NUM_TXNS_PER_FOLDER,
            version,
            batch_metadata,
        );

        recover_tx.send(()).expect("Receiver should exist.");

        tokio_scoped::scope(|s| {
            let (tx, mut rx) = channel::<(_, BatchMetadata, _)>(5);
            s.spawn(async move {
                while let Some((transactions, batch_metadata, end_batch)) = rx.recv().await {
                    let bytes_to_upload = batch_metadata.files.last().unwrap().size_bytes as u64;
                    self.do_upload(transactions, batch_metadata, end_batch)
                        .await
                        .unwrap();
                    FILE_STORE_UPLOADED_BYTES.inc_by(bytes_to_upload);
                }
            });
            s.spawn(async move {
                loop {
                    let _timer = TIMER
                        .with_label_values(&["file_store_uploader_main_loop"])
                        .start_timer();
                    let next_version = file_store_operator.version();
                    let transactions = {
                        let _timer = TIMER
                            .with_label_values(&["get_transactions_from_cache"])
                            .start_timer();
                        data_manager
                            .get_transactions_from_cache(
                                next_version,
                                MAX_SIZE_PER_FILE,
                                /*update_file_store_version=*/ true,
                            )
                            .await
                    };
                    let len = transactions.len();
                    for transaction in transactions {
                        file_store_operator
                            .buffer_and_maybe_dump_transactions_to_file(transaction, tx.clone())
                            .await
                            .unwrap();
                    }
                    if len == 0 {
                        info!("No transaction was returned from cache, requested version: {next_version}.");
                        tokio::time::sleep(Duration::from_millis(200)).await;
                    }
                }
            });
        });

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L261-274)
```rust
    /// Updates the file store metadata.
    async fn update_file_store_metadata(&self, version: u64) -> Result<()> {
        FILE_STORE_VERSION.set(version as i64);
        let metadata = FileStoreMetadata {
            chain_id: self.chain_id,
            num_transactions_per_folder: NUM_TXNS_PER_FOLDER,
            version,
        };

        let raw_data = serde_json::to_vec(&metadata).map_err(anyhow::Error::msg)?;
        self.writer
            .save_raw_file(PathBuf::from(METADATA_FILE_NAME), raw_data)
            .await
    }
```
