# Audit Report

## Title
Silent Loss of Validator Transaction Fee Rewards in Sharded Block Execution Path

## Summary
The `into_transaction_outputs_forced()` function in the sharded block executor discards the `block_epilogue_txn` without executing it, causing complete loss of transaction fee distribution to validators when sharded execution is used.

## Finding Description

The vulnerability exists in how block execution results are handled in the sharded execution path versus the normal execution path.

**Normal Execution Path (Correct):** [1](#0-0) 

The normal path properly extracts both `transaction_outputs` and `block_epilogue_txn`, then adds the epilogue transaction to the transaction list for execution.

**Sharded Execution Path (Vulnerable):** [2](#0-1) 

The sharded execution calls `into_transaction_outputs_forced()` which silently discards the `block_epilogue_txn`: [3](#0-2) 

**What is Lost:** [4](#0-3) 

The `BlockEpiloguePayload::V1` contains critical `FeeDistribution` data mapping validator indices to fee amounts in Octas.

**Fee Distribution Calculation:** [5](#0-4) 

The block executor calculates fee distribution by iterating through successful transactions, computing distributable fees (after subtracting storage fees and burn amounts), and aggregating them by validator proposer index.

**Missing Execution:** [6](#0-5) 

Without the block epilogue transaction being executed, `process_block_epilogue` never runs, so the fee distribution data never reaches the Move function: [7](#0-6) [8](#0-7) 

The `stake::record_fee` function that credits pending fees to validators is never called, resulting in complete loss of transaction fee rewards for that block.

## Impact Explanation

**Severity: High** - Loss of validator transaction fee rewards

This breaks Critical Invariant #6: "Staking Security: Validator rewards and penalties must be calculated correctly."

**Impact Quantification:**
- All validators proposing transactions in blocks executed via sharded path lose 100% of their transaction fee rewards for those blocks
- Violates the economic model where validators are compensated for block proposal
- Could affect validator incentives and network security if validators realize they're not receiving expected rewards
- Cumulative loss grows with each block executed via sharded path

Per Aptos bug bounty criteria, this qualifies as **High Severity** due to:
- Significant protocol violation (incorrect reward distribution)
- Loss of funds (validator transaction fees)
- State inconsistency (pending fees not updated correctly)

## Likelihood Explanation

**Likelihood: Medium to High** (depending on production usage of sharded execution)

The vulnerability triggers automatically whenever: [9](#0-8) 

Sharded execution is selected for `ExecutableTransactions::Sharded` transactions. The code infrastructure exists for both local and remote sharded execution: [10](#0-9) 

**Factors:**
- Sharded execution infrastructure is fully implemented (not experimental code)
- Both `LocalExecutorClient` and `RemoteExecutorClient` implementations exist
- No attacker action required - happens automatically when sharded execution is used
- Silent failure - no errors or warnings, fees are simply not distributed

## Recommendation

Replace `into_transaction_outputs_forced()` with `into_inner()` and properly handle the block epilogue transaction:

```rust
// In sharded_executor_service.rs, line 156
let ret = AptosVMBlockExecutorWrapper::execute_block_on_thread_pool(
    executor_thread_pool,
    &txn_provider,
    aggr_overridden_state_view.as_ref(),
    &AptosModuleCacheManager::new(),
    config,
    TransactionSliceMetadata::unknown(),
    cross_shard_commit_sender,
)
.map(|block_output| {
    let (mut transaction_outputs, block_epilogue_txn) = block_output.into_inner();
    
    // Add block epilogue transaction output if it exists
    if let Some(epilogue_txn) = block_epilogue_txn {
        // TODO: Execute the block epilogue transaction and append its output
        // This requires refactoring to support epilogue execution in sharded context
    }
    
    transaction_outputs
});
```

**Note:** The proper fix requires architectural changes to support block epilogue execution in the sharded execution model, as the epilogue contains global state updates (fee distribution) that don't fit cleanly into the shard-based execution model.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Configure a node to use sharded block execution
2. Submit transactions with non-zero gas fees
3. Observe that the block epilogue transaction with fee distribution is generated by the block executor
4. Observe that `into_transaction_outputs_forced()` discards this transaction
5. Verify that `stake::record_fee` is never called
6. Check validator `PendingTransactionFee` - fees are not credited
7. Validators receive no transaction fee rewards despite processing the block

**Verification Steps:**
```rust
// Monitor the block execution flow
// 1. gen_block_epilogue creates fee distribution data
// 2. BlockOutput contains block_epilogue_txn with FeeDistribution
// 3. into_transaction_outputs_forced() discards it
// 4. Fee distribution never reaches stake::record_fee
// 5. Validators lose their rightful transaction fee rewards
```

## Notes

This vulnerability represents a critical economic invariant violation where validators lose their rightful compensation for block proposal when sharded execution is used. The fee distribution calculation logic works correctly, but the execution of the block epilogue transaction is bypassed entirely in the sharded path, resulting in silent loss of validator rewards.

### Citations

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L81-88)
```rust
            ExecutableTransactions::Sharded(txns) => Self::by_transaction_execution_sharded::<V>(
                txns,
                auxiliary_infos,
                parent_state,
                state_view,
                onchain_config,
                transaction_slice_metadata.append_state_checkpoint_to_block(),
            )?,
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L123-130)
```rust
        let (mut transaction_outputs, block_epilogue_txn) = block_output.into_inner();
        let (transactions, mut auxiliary_infos) = txn_provider.into_inner();
        let mut transactions = transactions
            .into_iter()
            .map(|t| t.into_inner())
            .collect_vec();
        if let Some(block_epilogue_txn) = block_epilogue_txn {
            transactions.push(block_epilogue_txn.into_inner());
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L145-156)
```rust
                let ret = AptosVMBlockExecutorWrapper::execute_block_on_thread_pool(
                    executor_thread_pool,
                    &txn_provider,
                    aggr_overridden_state_view.as_ref(),
                    // Since we execute blocks in parallel, we cannot share module caches, so each
                    // thread has its own caches.
                    &AptosModuleCacheManager::new(),
                    config,
                    TransactionSliceMetadata::unknown(),
                    cross_shard_commit_sender,
                )
                .map(BlockOutput::into_transaction_outputs_forced);
```

**File:** types/src/transaction/block_output.rs (L31-33)
```rust
    pub fn into_transaction_outputs_forced(self) -> Vec<Output> {
        self.transaction_outputs
    }
```

**File:** types/src/transaction/block_epilogue.rs (L23-27)
```rust
    V1 {
        block_id: HashValue,
        block_end_info: BlockEndInfoExt,
        fee_distribution: FeeDistribution,
    },
```

**File:** aptos-move/block-executor/src/executor.rs (L2042-2078)
```rust
        for (i, output) in outputs.enumerate().take(epilogue_txn_idx as usize) {
            // TODO(grao): Also include other transactions that is "Keep" if we are confident
            // that we successfully charge enough gas amount as it appears in the FeeStatement
            // for every corner cases.
            if !output.is_materialized_and_success() {
                continue;
            }
            let output_after_guard = output.after_materialization()?;
            let fee_statement = output_after_guard.fee_statement();

            let txn = signature_verified_block.get_txn(i as TxnIndex);
            if let Some(user_txn) = txn.try_as_signed_user_txn() {
                let auxiliary_info = signature_verified_block.get_auxiliary_info(i as TxnIndex);
                if let Some(proposer_index) = auxiliary_info.proposer_index() {
                    let gas_price = user_txn.gas_unit_price();
                    let total_gas_unit = fee_statement.gas_used();
                    // Total gas unit here includes the storage fee (deposit), which is not
                    // available for distribution. Only the execution gas and IO gas are available
                    // to distribute. Note here we deliberately NOT use the execution gas and IO
                    // gas value from the fee statement, because they might round up during the
                    // calculation and the sum of them could be larger than the actual value we
                    // burn. Instead we use the total amount (which is the total we've burnt)
                    // minus the storage deposit (round up), to avoid over distribution.
                    // We burn a fix amount of gas per gas unit.
                    let gas_price_to_burn = self.config.onchain.gas_price_to_burn();
                    if gas_price > gas_price_to_burn {
                        let gas_unit_available_to_distribute = total_gas_unit
                            .saturating_sub(fee_statement.storage_fee_used().div_ceil(gas_price));
                        if gas_unit_available_to_distribute > 0 {
                            let fee_to_distribute =
                                gas_unit_available_to_distribute * (gas_price - gas_price_to_burn);
                            *amount.entry(proposer_index).or_insert(0) += fee_to_distribute;
                        }
                    }
                }
            }
        }
```

**File:** aptos-move/aptos-vm/src/aptos_vm.rs (L2575-2588)
```rust
        let (validator_indices, amounts) = match fee_distribution {
            FeeDistribution::V0 { amount } => amount
                .into_iter()
                .map(|(validator_index, amount)| {
                    (MoveValue::U64(validator_index), MoveValue::U64(amount))
                })
                .unzip(),
        };

        let args = vec![
            MoveValue::Signer(AccountAddress::ZERO), // Run as 0x0
            MoveValue::Vector(validator_indices),
            MoveValue::Vector(amounts),
        ];
```

**File:** aptos-move/framework/aptos-framework/sources/block.move (L249-255)
```text
    fun block_epilogue(
        vm: &signer,
        fee_distribution_validator_indices: vector<u64>,
        fee_amounts_octa: vector<u64>,
    ) {
        stake::record_fee(vm, fee_distribution_validator_indices, fee_amounts_octa);
    }
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L616-635)
```text
    public(friend) fun record_fee(
        vm: &signer,
        fee_distribution_validator_indices: vector<u64>,
        fee_amounts_octa: vector<u64>,
    ) acquires PendingTransactionFee {
        // Operational constraint: can only be invoked by the VM.
        system_addresses::assert_vm(vm);

        assert!(fee_distribution_validator_indices.length() == fee_amounts_octa.length());

        let num_validators_to_distribute = fee_distribution_validator_indices.length();
        let pending_fee = borrow_global_mut<PendingTransactionFee>(@aptos_framework);
        let i = 0;
        while (i < num_validators_to_distribute) {
            let validator_index = fee_distribution_validator_indices[i];
            let fee_octa = fee_amounts_octa[i];
            pending_fee.pending_fee_by_validator.borrow_mut(&validator_index).add(fee_octa);
            i = i + 1;
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L183-222)
```rust
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        assert_eq!(transactions.num_shards(), self.num_shards());
        let (sub_blocks, global_txns) = transactions.into();
        for (i, sub_blocks_for_shard) in sub_blocks.into_iter().enumerate() {
            self.command_txs[i]
                .send(ExecutorShardCommand::ExecuteSubBlocks(
                    state_view.clone(),
                    sub_blocks_for_shard,
                    concurrency_level_per_shard,
                    onchain_config.clone(),
                ))
                .unwrap();
        }

        // This means that we are executing the global transactions concurrently with the individual shards but the
        // global transactions will be blocked for cross shard transaction results. This hopefully will help with
        // finishing the global transactions faster but we need to evaluate if this causes thread contention. If it
        // does, then we can simply move this call to the end of the function.
        let mut global_output = self.global_executor.execute_global_txns(
            global_txns,
            state_view.as_ref(),
            onchain_config,
        )?;

        let mut sharded_output = self.get_output_from_shards()?;

        sharded_aggregator_service::aggregate_and_update_total_supply(
            &mut sharded_output,
            &mut global_output,
            state_view.as_ref(),
            self.global_executor.get_executor_thread_pool(),
        );

        Ok(ShardedExecutionOutput::new(sharded_output, global_output))
```
