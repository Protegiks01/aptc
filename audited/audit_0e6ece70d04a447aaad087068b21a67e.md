# Audit Report

## Title
False Payload Availability Reporting Causes Inefficient Execution Retries and Resource Exhaustion

## Summary
The `check_payload_availability` method in `QuorumStorePayloadManager` falsely reports payload availability for `InQuorumStore`, `InQuorumStoreWithLimit`, and `QuorumStoreInlineHybrid` payload types without verifying that referenced batch data is locally available. This causes consensus to proceed with block execution immediately, leading to infinite retry loops when batch data must be fetched from the network, resulting in validator node slowdowns and resource exhaustion.

## Finding Description

The vulnerability exists in the `check_payload_availability` implementation for the `QuorumStorePayloadManager`. This method is responsible for determining whether a block's payload data is locally available before execution proceeds. [1](#0-0) 

For the following payload types, the method unconditionally returns `Ok(())` without checking local batch availability:

1. **`Payload::InQuorumStore`** - Returns `Ok(())` immediately
2. **`Payload::InQuorumStoreWithLimit`** - Returns `Ok(())` immediately  
3. **`Payload::QuorumStoreInlineHybrid/V2`** - Returns `Ok(())` with only metrics collection, not availability checking

The comment at line 405-406 states: *"The payload is considered available because it contains only proofs that guarantee network availability or inlined transactions."* However, this assumption conflates **network availability** (data exists somewhere) with **local availability** (data is accessible now).

In contrast, `OptQuorumStore` (V1/V2) correctly checks local availability using `batch_reader.exists()` and returns `Err(missing_authors)` when batches are missing locally.

**Attack Flow:**

When `check_payload_availability` falsely returns `Ok()`, the consensus round manager proceeds immediately to execution: [2](#0-1) 

The block enters the execution pipeline where `materialize_block` attempts to fetch transactions: [3](#0-2) 

When batch data is not locally available, `get_transactions` calls `batch_reader.get_batch`, which attempts network retrieval: [4](#0-3) 

If the batch fetch fails after exhausting retries (default: 10 attempts over ~50 seconds), it returns `ExecutorError::CouldNotGetData`: [5](#0-4) 

This error triggers an **infinite retry loop** in the materialize phase: [6](#0-5) 

The loop retries every 100ms indefinitely until the data arrives, the pipeline is aborted (e.g., during state sync), or the node crashes.

**Contrast with Correct Behavior:**

When `check_payload_availability` correctly returns `Err(missing_authors)` (as with `OptQuorumStore`), the round manager properly waits for payload with a timeout: [7](#0-6) 

This provides proper timeout handling and graceful failure instead of infinite retries.

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty program criteria:

1. **Validator Node Slowdowns**: When multiple blocks with missing batch data are processed concurrently, each enters its own infinite retry loop, consuming CPU resources continuously. The 100ms retry interval combined with network fetch timeouts (~5s each) creates sustained resource pressure.

2. **Significant Protocol Violations**: The consensus protocol expects payload availability checks to gate execution properly. Bypassing this check violates the intended execution flow and resource management guarantees.

3. **Resource Exhaustion**: Each block in the retry loop spawns network requests, maintains futures, and consumes thread pool resources. With typical block production rates (1-2 per second), missing data for even a few blocks creates multiplicative resource consumption.

4. **Delayed Block Processing**: Blocks stuck in retry loops cannot complete execution, delaying state commitment and blocking dependent blocks, degrading overall chain progress.

The impact is amplified during network partitions, node restarts, or high load scenarios when batch data propagation lags behind block proposals.

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of occurrence:

1. **Common Trigger Conditions**:
   - Node restarts (batch store is empty)
   - Network delays or temporary partitions
   - High transaction throughput (batches lag behind proposals)
   - New validators joining (syncing batch data)

2. **No Adversarial Action Required**: This is a natural fault condition, not requiring malicious behavior. Any network delay where block proposals arrive before their referenced batch data triggers the issue.

3. **Payload Type Usage**: `InQuorumStore`, `InQuorumStoreWithLimit`, and `QuorumStoreInlineHybrid` are actively used payload types in production, not deprecated code paths.

4. **Observable in Metrics**: The code includes metrics tracking (`CONSENSUS_PROPOSAL_PAYLOAD_BATCH_AVAILABILITY_IN_QS`) that would show frequent "missing" counts, indicating this occurs regularly.

## Recommendation

Implement consistent local availability checking for all payload types that reference batch data. The fix should mirror the `OptQuorumStore` implementation:

```rust
fn check_payload_availability(&self, block: &Block) -> Result<(), BitVec> {
    let Some(payload) = block.payload() else {
        return Ok(());
    };

    match payload {
        Payload::DirectMempool(_) => {
            unreachable!("QuorumStore doesn't support DirectMempool payload")
        },
        // FIX: Check proof batch availability for InQuorumStore
        Payload::InQuorumStore(proof_with_data) => {
            check_proof_batches_availability(&self.batch_reader, 
                &self.address_to_validator_index,
                &self.ordered_authors,
                &proof_with_data.proofs)
        },
        // FIX: Check proof batch availability for InQuorumStoreWithLimit
        Payload::InQuorumStoreWithLimit(proof_with_data) => {
            check_proof_batches_availability(&self.batch_reader,
                &self.address_to_validator_index, 
                &self.ordered_authors,
                &proof_with_data.proof_with_data.proofs)
        },
        // FIX: Check proof batch availability for hybrid payloads
        Payload::QuorumStoreInlineHybrid(inline_batches, proofs, _)
        | Payload::QuorumStoreInlineHybridV2(inline_batches, proofs, _) => {
            // Inline batches are already available (data is in block)
            // Only check proof batches
            check_proof_batches_availability(&self.batch_reader,
                &self.address_to_validator_index,
                &self.ordered_authors, 
                &proofs.proofs)
        },
        // Existing correct implementation
        Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => { /* ... */ },
        Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => { /* ... */ },
    }
}

// Helper function
fn check_proof_batches_availability(
    batch_reader: &Arc<dyn BatchReader>,
    address_to_validator_index: &HashMap<PeerId, usize>,
    ordered_authors: &[PeerId],
    proofs: &[ProofOfStore<BatchInfo>],
) -> Result<(), BitVec> {
    let mut missing_authors = BitVec::with_num_bits(ordered_authors.len() as u16);
    for proof in proofs {
        if batch_reader.exists(proof.info().digest()).is_none() {
            let index = *address_to_validator_index
                .get(&proof.info().author())
                .expect("Payload author should have been verified");
            missing_authors.set(index as u16);
        }
    }
    if missing_authors.all_zeros() {
        Ok(())
    } else {
        Err(missing_authors)
    }
}
```

This ensures all payload types properly check local batch availability before allowing execution to proceed, enabling the round manager to use the correct `wait_for_payload` path with timeout handling.

## Proof of Concept

The vulnerability can be demonstrated with the following scenario:

1. **Setup**: Start a validator node with QuorumStore enabled
2. **Trigger**: Receive a block proposal with `InQuorumStore` payload referencing batch digests not yet in local batch store
3. **Observe**: 
   - `check_payload_availability` returns `Ok()` (false positive)
   - Block proceeds to execution pipeline
   - `materialize_block` calls `get_transactions`
   - `batch_reader.get_batch` attempts network fetch
   - Network fetch times out after ~50 seconds
   - `materialize_block` retry loop begins
   - CPU usage increases from continuous retries
   - Block remains in "materializing" state indefinitely

**Metrics Evidence**:
Monitor `CONSENSUS_PROPOSAL_PAYLOAD_BATCH_AVAILABILITY_IN_QS` with label `missing` - this will show non-zero counts even though `check_payload_availability` returned `Ok()` for those blocks, confirming the false positive.

**Log Evidence**:
Observe repeated warning logs: `"[BlockPreparer] failed to prepare block {block_id}, retrying: CouldNotGetData"` occurring every 100ms for the same block, confirming the infinite retry loop.

The issue is exacerbated when multiple blocks are affected simultaneously, as each spawns its own retry loop, creating multiplicative resource consumption.

## Notes

This vulnerability represents an implementation inconsistency where newer payload types (`OptQuorumStore`) correctly implement availability checking, while older types (`InQuorumStore`, etc.) rely on an optimistic assumption that proved incorrect. The comment justifying the behavior reveals the flawed reasoning: having a proof does NOT guarantee immediate local availability, only eventual network availability. The proper gate should be local availability before execution, with network fetching reserved for the explicit `wait_for_payload` path that includes timeout handling.

### Citations

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L349-444)
```rust
    fn check_payload_availability(&self, block: &Block) -> Result<(), BitVec> {
        let Some(payload) = block.payload() else {
            return Ok(());
        };

        match payload {
            Payload::DirectMempool(_) => {
                unreachable!("QuorumStore doesn't support DirectMempool payload")
            },
            Payload::InQuorumStore(_) => Ok(()),
            Payload::InQuorumStoreWithLimit(_) => Ok(()),
            Payload::QuorumStoreInlineHybrid(inline_batches, proofs, _)
            | Payload::QuorumStoreInlineHybridV2(inline_batches, proofs, _) => {
                fn update_availability_metrics<'a>(
                    batch_reader: &Arc<dyn BatchReader>,
                    is_proof_label: &str,
                    batch_infos: impl Iterator<Item = &'a BatchInfo>,
                ) {
                    for (author, chunk) in &batch_infos.chunk_by(|info| info.author()) {
                        let (available_count, missing_count) = chunk
                            .map(|info| batch_reader.exists(info.digest()))
                            .fold((0, 0), |(available_count, missing_count), item| {
                                if item.is_some() {
                                    (available_count + 1, missing_count)
                                } else {
                                    (available_count, missing_count + 1)
                                }
                            });
                        counters::CONSENSUS_PROPOSAL_PAYLOAD_BATCH_AVAILABILITY_IN_QS
                            .with_label_values(&[
                                &author.to_hex_literal(),
                                is_proof_label,
                                "available",
                            ])
                            .inc_by(available_count as u64);
                        counters::CONSENSUS_PROPOSAL_PAYLOAD_BATCH_AVAILABILITY_IN_QS
                            .with_label_values(&[
                                &author.to_hex_literal(),
                                is_proof_label,
                                "missing",
                            ])
                            .inc_by(missing_count as u64);
                    }
                }

                update_availability_metrics(
                    &self.batch_reader,
                    "false",
                    inline_batches.iter().map(|(batch_info, _)| batch_info),
                );
                update_availability_metrics(
                    &self.batch_reader,
                    "true",
                    proofs.proofs.iter().map(|proof| proof.info()),
                );

                // The payload is considered available because it contains only proofs that guarantee network availabiliy
                // or inlined transactions.
                Ok(())
            },
            Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
            },
            Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
            },
        }
    }
```

**File:** consensus/src/round_manager.rs (L1262-1285)
```rust
        if block_store.check_payload(&proposal).is_err() {
            debug!("Payload not available locally for block: {}", proposal.id());
            counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
                .with_label_values(&["missing"])
                .inc();
            let start_time = Instant::now();
            let deadline = self.round_state.current_round_deadline();
            let future = async move {
                (
                    block_store.wait_for_payload(&proposal, deadline).await,
                    proposal,
                    start_time,
                )
            }
            .boxed();
            self.futures.push(future);
            return Ok(());
        }

        counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
            .with_label_values(&["available"])
            .inc();

        self.check_backpressure_and_process_proposal(proposal).await
```

**File:** consensus/src/block_preparer.rs (L42-69)
```rust
    pub async fn materialize_block(
        &self,
        block: &Block,
        block_qc_fut: Shared<impl Future<Output = Option<Arc<QuorumCert>>>>,
    ) -> ExecutorResult<(Vec<SignedTransaction>, Option<u64>, Option<u64>)> {
        fail_point!("consensus::prepare_block", |_| {
            use aptos_executor_types::ExecutorError;
            use std::{thread, time::Duration};
            thread::sleep(Duration::from_millis(10));
            Err(ExecutorError::CouldNotGetData)
        });
        //TODO(ibalajiarun): measure latency
        let (txns, max_txns_from_block_to_execute, block_gas_limit) = tokio::select! {
                // Poll the block qc future until a QC is received. Ignore None outcomes.
                Some(qc) = block_qc_fut => {
                    let block_voters = Some(qc.ledger_info().get_voters_bitvec().clone());
                    self.payload_manager.get_transactions(block, block_voters).await
                },
                result = self.payload_manager.get_transactions(block, None) => {
                   result
                }
        }?;
        TXNS_IN_BLOCK
            .with_label_values(&["before_filter"])
            .observe(txns.len() as f64);

        Ok((txns, max_txns_from_block_to_execute, block_gas_limit))
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L663-723)
```rust
    fn get_or_fetch_batch(
        &self,
        batch_info: BatchInfo,
        responders: Vec<PeerId>,
    ) -> Shared<Pin<Box<dyn Future<Output = ExecutorResult<Vec<SignedTransaction>>> + Send>>> {
        let mut responders = responders.into_iter().collect();

        self.inflight_fetch_requests
            .lock()
            .entry(*batch_info.digest())
            .and_modify(|fetch_unit| {
                fetch_unit.responders.lock().append(&mut responders);
            })
            .or_insert_with(|| {
                let responders = Arc::new(Mutex::new(responders));
                let responders_clone = responders.clone();

                let inflight_requests_clone = self.inflight_fetch_requests.clone();
                let batch_store = self.batch_store.clone();
                let requester = self.batch_requester.clone();

                let fut = async move {
                    let batch_digest = *batch_info.digest();
                    defer!({
                        inflight_requests_clone.lock().remove(&batch_digest);
                    });
                    // TODO(ibalajiarun): Support V2 batch
                    if let Ok(mut value) = batch_store.get_batch_from_local(&batch_digest) {
                        Ok(value.take_payload().expect("Must have payload"))
                    } else {
                        // Quorum store metrics
                        counters::MISSED_BATCHES_COUNT.inc();
                        let subscriber_rx = batch_store.subscribe(*batch_info.digest());
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
                            .await?;
                        batch_store.persist(vec![PersistedValue::new(
                            batch_info.into(),
                            Some(payload.clone()),
                        )]);
                        Ok(payload)
                    }
                }
                .boxed()
                .shared();

                tokio::spawn(fut.clone());

                BatchFetchUnit {
                    responders: responders_clone,
                    fut,
                }
            })
            .fut
            .clone()
    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L176-179)
```rust
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
        })
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L634-646)
```rust
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
```

**File:** consensus/src/block_storage/block_store.rs (L589-594)
```rust
    pub async fn wait_for_payload(&self, block: &Block, deadline: Duration) -> anyhow::Result<()> {
        let duration = deadline.saturating_sub(self.time_service.get_current_timestamp());
        tokio::time::timeout(duration, self.payload_manager.get_transactions(block, None))
            .await??;
        Ok(())
    }
```
