# Audit Report

## Title
Silent Data Loss in Backup Service Transaction Iterator Due to Missing Completeness Validation

## Summary
The `ContinuousVersionIter` iterator in the backup service fails to validate that all requested transactions were yielded when the underlying RocksDB iterator returns `None`. This allows the backup service to silently return incomplete transaction data without error indication when transactions are pruned from the database before or during iterator creation.

## Finding Description

The vulnerability exists in how `BackupHandler::get_transaction_iter()` creates multiple RocksDB iterators sequentially and wraps them with `ContinuousVersionIter` for version continuity checking. [1](#0-0) 

Each of the five iterators (transaction, transaction_info, events, write_set, auxiliary_info) is created at a different point in time, meaning each captures its own RocksDB implicit snapshot. If database pruning occurs between iterator creation, these iterators will have inconsistent snapshots.

The core bug is in `ContinuousVersionIter::next_impl()`: [2](#0-1) 

When the underlying iterator returns `None` at line 58, the code immediately returns `None` without validating that `expected_next_version == end_version`. This means if the iterator was supposed to yield 100 transactions but only yielded 50, no error is raised.

**Exploit Scenario:**

1. Node has transactions 0-1000 committed
2. Pruning is configured and actively removes old data
3. At T1: Pruner deletes transactions 150-199
4. At T2: Backup service requests transactions 100-199 (100 transactions expected)
5. All five iterators are created after T1 (post-pruning)
6. All iterators only see transactions 100-149 in their snapshots (50 transactions)
7. Iteration proceeds successfully for 50 transactions
8. All iterators return `None` (no more data available)
9. The backup endpoint completes successfully, returning only 50 transactions
10. **No error indication** - client assumes backup is complete [3](#0-2) 

The backup service HTTP endpoint uses `try_for_each` which stops when the iterator is exhausted, without validating the count matches `num_transactions`.

This breaks the **State Consistency** invariant: backup operations must either succeed completely or fail with clear error indication. Silent partial success violates data integrity guarantees.

## Impact Explanation

This vulnerability falls under **High Severity** based on the Aptos bug bounty criteria for "Significant protocol violations." 

The backup service is critical infrastructure for blockchain disaster recovery. Incomplete backups without error indication could lead to:

1. **False sense of security**: Operators believe they have complete backups when they don't
2. **Failed chain restoration**: During disaster recovery, incomplete backups cannot fully restore the chain
3. **Data loss**: Historical transaction data may be permanently lost if only incomplete backups exist
4. **Operational failures**: Monitoring systems won't detect the issue since no errors are reported

While this doesn't directly cause consensus violations or fund loss in the running chain, it compromises the ability to recover from catastrophic failures, which is a significant protocol-level concern.

## Likelihood Explanation

**Likelihood: Medium to High**

This can occur whenever:
- Database pruning is active (common in production nodes to manage storage)
- Backup requests are made for transaction ranges that overlap with pruning windows
- Timing aligns such that pruning completes between iterator creation and consumption

Factors increasing likelihood:
- Aggressive pruning configurations with short windows
- Backup services running on nodes with active pruning
- High backup frequency relative to pruning frequency
- Stale metadata in backup coordination systems

The sequential creation of five separate iterators without atomic snapshot guarantees makes this race condition realistic in production environments.

## Recommendation

Add completeness validation to `ContinuousVersionIter::next_impl()`:

```rust
fn next_impl(&mut self) -> Result<Option<T>> {
    if self.expected_next_version >= self.end_version {
        return Ok(None);
    }

    let ret = match self.inner.next().transpose()? {
        Some((version, transaction)) => {
            ensure!(
                version == self.expected_next_version,
                "{} iterator: first version {}, expecting version {}, got {} from underlying iterator.",
                std::any::type_name::<T>(),
                self.first_version,
                self.expected_next_version,
                version,
            );
            self.expected_next_version += 1;
            Some(transaction)
        },
        None => {
            // FIX: Validate that we received all expected items
            ensure!(
                self.expected_next_version >= self.end_version,
                "{} iterator: incomplete data, expected {} items but only received {}. \
                 Last yielded version: {}, expected to reach version {}",
                std::any::type_name::<T>(),
                self.end_version - self.first_version,
                self.expected_next_version - self.first_version,
                self.expected_next_version.saturating_sub(1),
                self.end_version.saturating_sub(1),
            );
            None
        }
    };

    Ok(ret)
}
```

Additionally, consider creating all backup iterators under a single explicit RocksDB snapshot to ensure consistency:

```rust
pub fn get_transaction_iter(
    &self,
    start_version: Version,
    num_transactions: usize,
) -> Result<impl Iterator<Item = Result<(...)>> + '_> {
    // Create explicit snapshot for consistency
    let snapshot = self.ledger_db.transaction_db_raw().snapshot();
    
    // Create all iterators from the same snapshot
    // (requires API changes to accept snapshot parameter)
    // ...
}
```

## Proof of Concept

```rust
#[test]
fn test_incomplete_iterator_without_error() {
    use tempfile::TempDir;
    use aptos_types::transaction::Transaction;
    
    let tmp_dir = TempDir::new().unwrap();
    let db = AptosDB::new_for_test(&tmp_dir);
    
    // Commit 100 transactions (versions 0-99)
    let txns: Vec<Transaction> = (0..100)
        .map(|_| create_test_transaction())
        .collect();
    commit_transactions(&db, 0, &txns);
    
    // Prune transactions 50-99
    let mut batch = SchemaBatch::new();
    db.ledger_db.transaction_db().prune_transactions(50, 100, &mut batch).unwrap();
    db.ledger_db.transaction_db().write_schemas(batch).unwrap();
    
    // Request 100 transactions (0-99), but only 0-49 remain after pruning
    let backup_handler = db.get_backup_handler();
    let iter = backup_handler.get_transaction_iter(0, 100).unwrap();
    
    let collected: Vec<_> = iter.collect::<Result<Vec<_>>>().unwrap();
    
    // BUG: Only 50 transactions returned, but no error was raised!
    assert_eq!(collected.len(), 50); // Should be 100 or error
    // Expected: Should return error indicating incomplete data
}
```

## Notes

This vulnerability specifically affects the backup service's data integrity guarantees. While it doesn't compromise the running blockchain's consensus or state, it undermines disaster recovery capabilities which are essential for production blockchain operations. The fix requires both immediate completeness validation and architectural improvements to ensure atomic snapshot consistency across multiple iterators.

### Citations

**File:** storage/aptosdb/src/backup/backup_handler.rs (L56-75)
```rust
        let txn_iter = self
            .ledger_db
            .transaction_db()
            .get_transaction_iter(start_version, num_transactions)?;
        let mut txn_info_iter = self
            .ledger_db
            .transaction_info_db()
            .get_transaction_info_iter(start_version, num_transactions)?;
        let mut event_vec_iter = self
            .ledger_db
            .event_db()
            .get_events_by_version_iter(start_version, num_transactions)?;
        let mut write_set_iter = self
            .ledger_db
            .write_set_db()
            .get_write_set_iter(start_version, num_transactions)?;
        let mut persisted_aux_info_iter = self
            .ledger_db
            .persisted_auxiliary_info_db()
            .get_persisted_auxiliary_info_iter(start_version, num_transactions)?;
```

**File:** storage/aptosdb/src/utils/iterators.rs (L40-62)
```rust
    fn next_impl(&mut self) -> Result<Option<T>> {
        if self.expected_next_version >= self.end_version {
            return Ok(None);
        }

        let ret = match self.inner.next().transpose()? {
            Some((version, transaction)) => {
                ensure!(
                    version == self.expected_next_version,
                    "{} iterator: first version {}, expecting version {}, got {} from underlying iterator.",
                    std::any::type_name::<T>(),
                    self.first_version,
                    self.expected_next_version,
                    version,
                );
                self.expected_next_version += 1;
                Some(transaction)
            },
            None => None,
        };

        Ok(ret)
    }
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L101-110)
```rust
    // GET transactions/<start_version>/<num_transactions>
    let bh = backup_handler.clone();
    let transactions = warp::path!(Version / usize)
        .map(move |start_version, num_transactions| {
            reply_with_bytes_sender(&bh, TRANSACTIONS, move |bh, sender| {
                bh.get_transaction_iter(start_version, num_transactions)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```
