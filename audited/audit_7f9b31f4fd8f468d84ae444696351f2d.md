# Audit Report

## Title
Infinite Retry Loop on Expired Batch Data Causes Permanent Consensus Liveness Failure

## Summary
The consensus pipeline's `materialize` phase contains an infinite retry loop that does not distinguish between transient and permanent errors. When a block references expired batch data (which has been permanently deleted from QuorumStore), the retry loop continues indefinitely, causing permanent consensus liveness failure across all affected validators.

## Finding Description

The vulnerability exists in the interaction between three components:

1. **Batch Expiration in QuorumStore**: When batch data expires, it is permanently deleted from the batch store cache and database [1](#0-0) . When validators attempt to fetch expired batches, the batch requester verifies the expiration timestamp and returns `ExecutorError::CouldNotGetData` as a permanent error [2](#0-1) .

2. **Infinite Retry Loop in Materialize Phase**: The pipeline builder's `materialize` function implements block materialization with an infinite retry loop that treats ALL errors as transient [3](#0-2) . This loop never exits on permanent failures - it only breaks on success.

3. **No Timeout or Recovery Mechanism**: The materialize task is spawned with an abort handle [4](#0-3) , but abort is only triggered during manual reset or epoch boundaries [5](#0-4) . There is no automatic timeout or detection mechanism for stuck blocks.

**Attack Path:**

1. A block is proposed containing references to batch data with expiration time T
2. Due to network delays, consensus delays, or intentional manipulation, the block reaches validators for execution after time T has passed
3. The batch data has already expired and been deleted from QuorumStore cache [6](#0-5) 
4. When `materialize_block` is called, it attempts to fetch the batch via `payload_manager.get_transactions()`
5. The batch requester returns `ExecutorError::CouldNotGetData` because the batch is expired and permanently unavailable
6. The materialize retry loop catches this error and retries indefinitely with 100ms sleep intervals
7. The block's `ledger_update_fut` never completes
8. When the execution schedule phase calls `wait_for_compute_result()`, it awaits this future indefinitely [7](#0-6) 
9. The execution pipeline is permanently blocked - no further blocks can be executed or committed
10. All validators attempting to execute this block experience the same permanent halt

This breaks the **Consensus Liveness** invariant - the network cannot make progress and requires manual intervention (reset/state sync) to recover.

## Impact Explanation

**Severity: Critical** (meets "Total loss of liveness/network availability" criteria)

This vulnerability causes:
- **Complete Network Halt**: All validators stuck on the affected block cannot execute any subsequent blocks
- **Non-Recoverable Without Intervention**: The infinite retry loop has no timeout or automatic recovery mechanism
- **Consensus Deadlock**: The pipeline blocks at the execution phase, preventing signing, committing, or advancement to new rounds
- **Requires Manual Recovery**: Operators must manually trigger reset or rely on state sync to recover, effectively requiring coordinated intervention across the network

The impact qualifies for **Critical Severity** under Aptos bug bounty criteria as it causes "Total loss of liveness/network availability" and creates a "Non-recoverable network partition."

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can occur through:

1. **Natural Network Conditions**: Legitimate network delays or partitions causing blocks to be delayed beyond batch expiration times (which may be set to relatively short windows for performance reasons)

2. **Malicious Block Timing**: A proposer could intentionally delay broadcasting a block until after its referenced batches expire, causing all validators to get stuck when attempting execution

3. **Race Conditions During Load**: Under high load, consensus may lag behind batch expiration timelines, naturally triggering this condition

4. **No Special Privileges Required**: Any proposer can trigger this condition; no validator collusion or special access is needed

The likelihood is elevated because batch expiration is a normal operational mechanism in QuorumStore, and the timing window between proposal and execution is variable based on network conditions.

## Recommendation

Implement proper error classification and handling to distinguish between transient and permanent errors:

**Solution 1: Add Timeout to Materialize Retry Loop**
```rust
async fn materialize(
    preparer: Arc<BlockPreparer>,
    block: Arc<Block>,
    qc_rx: oneshot::Receiver<Arc<QuorumCert>>,
) -> TaskResult<MaterializeResult> {
    let mut tracker = Tracker::start_waiting("materialize", &block);
    tracker.start_working();
    
    let qc_rx = async {
        match qc_rx.await {
            Ok(qc) => Some(qc),
            Err(_) => {
                warn!("[BlockPreparer] qc tx cancelled for block {}", block.id());
                None
            },
        }
    }
    .shared();
    
    // Add maximum retry limit and timeout
    const MAX_RETRIES: usize = 50; // 5 seconds total with 100ms intervals
    let mut retry_count = 0;
    
    let result = loop {
        match preparer.materialize_block(&block, qc_rx.clone()).await {
            Ok(input_txns) => break input_txns,
            Err(e) => {
                retry_count += 1;
                if retry_count >= MAX_RETRIES {
                    error!(
                        "[BlockPreparer] failed to prepare block {} after {} retries, giving up: {}",
                        block.id(),
                        retry_count,
                        e
                    );
                    // Return error to allow pipeline to abort and trigger recovery
                    return Err(TaskError::InternalError(anyhow!(
                        "Failed to materialize block after {} retries: {}", 
                        retry_count, 
                        e
                    )));
                }
                warn!(
                    "[BlockPreparer] failed to prepare block {}, retry {}/{}: {}",
                    block.id(),
                    retry_count,
                    MAX_RETRIES,
                    e
                );
                tokio::time::sleep(Duration::from_millis(100)).await;
            },
        }
    };
    Ok(result)
}
```

**Solution 2: Distinguish Permanent vs Transient Errors**

Add a new error variant to `ExecutorError` for permanent failures:
```rust
pub enum ExecutorError {
    // ... existing variants ...
    
    #[error("Permanent data unavailability: {reason}")]
    PermanentDataUnavailable { reason: String },
}
```

Modify batch expiration handling to return the permanent error variant, and update materialize to immediately fail on permanent errors without retry.

## Proof of Concept

The following Rust test demonstrates the vulnerability scenario:

```rust
#[tokio::test]
async fn test_expired_batch_causes_infinite_retry() {
    // Setup: Create a block with batch references
    let block = create_test_block_with_batches();
    
    // Simulate time passing beyond batch expiration
    advance_time_past_batch_expiration();
    
    // Trigger batch cleanup - batches are now permanently deleted
    batch_store.clear_expired_payload(current_time);
    
    // Attempt to materialize the block
    let materialize_task = tokio::spawn(async move {
        pipeline_builder.materialize(
            block_preparer.clone(),
            block.clone(),
            qc_receiver
        ).await
    });
    
    // Wait for a reasonable timeout
    let timeout_result = tokio::time::timeout(
        Duration::from_secs(10),
        materialize_task
    ).await;
    
    // Assert: The materialize task never completes (times out)
    assert!(timeout_result.is_err(), 
        "Materialize should hang indefinitely on expired batches");
    
    // Verify: Check retry metrics show continuous retries
    let retry_count = BATCH_REQUEST_RETRY_COUNT.get();
    assert!(retry_count > 50, 
        "Expected many retries, got {}", retry_count);
}
```

**Reproduction Steps:**
1. Configure QuorumStore with short batch expiration times (e.g., 5 seconds)
2. Propose a block with batch references
3. Delay the block's arrival to validators by >5 seconds (network simulation or manual delay)
4. Observe validators attempting to execute the block
5. Monitor logs showing continuous "failed to prepare block, retrying" messages
6. Verify execution pipeline is stuck - no new blocks committed
7. Only recovery is manual reset via admin intervention

## Notes

This vulnerability is particularly dangerous because:

1. **Silent Failure**: The infinite retry loop produces only warning logs, not errors, making it difficult to detect the severity of the situation

2. **Network-Wide Impact**: All honest validators executing the affected block will experience the same halt, creating a coordinated liveness failure

3. **No Self-Healing**: Unlike transient network issues that resolve themselves, expired batch data is permanently deleted and can never be recovered through retries

4. **Operational Complexity**: Recovery requires coordinated manual intervention across validators or reliance on state sync, which may itself be impacted if enough validators are affected

The root cause is the materialize phase's assumption that all `materialize_block` errors are transient and retriable, when `CouldNotGetData` from batch expiration represents a permanent, unrecoverable failure condition.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L447-471)
```rust
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
```

**File:** consensus/src/quorum_store/batch_requester.rs (L144-150)
```rust
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L457-460)
```rust
        let materialize_fut = spawn_shared_fut(
            Self::materialize(self.block_preparer.clone(), block.clone(), qc_rx),
            Some(&mut abort_handles),
        );
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L634-646)
```rust
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
```

**File:** consensus/src/pipeline/buffer_manager.rs (L552-557)
```rust
        while let Some(item) = self.buffer.pop_front() {
            for b in item.get_blocks() {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
```

**File:** consensus/src/pipeline/execution_schedule_phase.rs (L72-72)
```rust
                let (compute_result, execution_time) = b.wait_for_compute_result().await?;
```
