# Audit Report

## Title
Compression Bomb Vulnerability in Network Message Decompression Leading to Validator OOM

## Summary
The CompressedBcs encoding in `ProtocolId::from_bytes()` allows attackers to send highly compressible network messages (≤4 MiB compressed) that expand to MAX_APPLICATION_MESSAGE_SIZE (~61.88 MiB) during decompression, creating a 15x amplification attack. With 100 concurrent connections, this can trigger ~6.2 GB of memory allocations, causing validator Out-Of-Memory (OOM) crashes and network-wide liveness failures. [1](#0-0) 

## Finding Description

The vulnerability exists in the asymmetry between network frame size limits and decompression memory allocation limits:

**Compression Flow (Sending):**
The `to_bytes()` method compresses messages using LZ4 before transmission. While both raw and compressed data are checked against MAX_APPLICATION_MESSAGE_SIZE, the actual network transmission uses MAX_FRAME_SIZE (4 MiB) for framing. [2](#0-1) 

**Decompression Flow (Receiving):**
The `from_bytes()` method reads the LZ4 size prefix (first 4 bytes of compressed data), validates it against MAX_APPLICATION_MESSAGE_SIZE, and then allocates that much memory instantly. [3](#0-2) 

The decompression implementation allocates memory upfront based on the claimed size: [4](#0-3) 

**Size Limit Verification:**
The `get_decompressed_size()` function validates the claimed decompressed size but doesn't prevent the amplification attack: [5](#0-4) 

**Network Constants Creating the Vulnerability:** [6](#0-5) 

**Streaming vs Compression Asymmetry:**
Large uncompressed messages (>4 MiB) are automatically streamed in chunks, allowing gradual memory allocation: [7](#0-6) 

However, compressed messages bypass streaming because the COMPRESSED payload fits within MAX_FRAME_SIZE (4 MiB), even though the DECOMPRESSED size is 61.88 MiB. This creates the attack vector.

**Attack Scenario:**

1. Attacker opens 100 connections to a validator (MAX_INBOUND_CONNECTIONS limit) [8](#0-7) 

2. On each connection, sends a DirectSendMsg with `protocol_id = ConsensusDirectSendCompressed` containing ~4 MiB of highly compressible data (e.g., zeros) with LZ4 header claiming 61.88 MiB decompressed size

3. Each message arrives in a single frame (no streaming)

4. When consensus processes messages, it calls `protocol_id.from_bytes()` which triggers decompression: [9](#0-8) 

5. Each decompression allocates 61.88 MiB instantly (via `vec![0u8; decompressed_size]`)

6. Total allocation: 100 connections × 61.88 MiB = **6.188 GB**

7. Validator OOM crash → Network liveness failure

## Impact Explanation

This vulnerability meets **Critical Severity** criteria under the Aptos Bug Bounty program:

- **Total loss of liveness/network availability**: Multiple validators can be simultaneously crashed by the attack, causing network-wide consensus failure
- **Non-recoverable network partition**: If enough validators are taken offline, the network cannot reach consensus (requires >1/3 online validators)

The attack violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." While the decompression technically respects MAX_APPLICATION_MESSAGE_SIZE, the 15x amplification ratio (4 MiB → 61.88 MiB) allows disproportionate resource consumption relative to network input, effectively bypassing the intended resource protection through compression amplification.

## Likelihood Explanation

**Likelihood: HIGH**

- **Attacker requirements**: Only requires network access to validators (no special privileges)
- **Attack complexity**: Low - simply send crafted compressed messages
- **Detection difficulty**: Hard to distinguish from legitimate compressed traffic until OOM occurs
- **Practical feasibility**: 100 concurrent connections and 4 MiB payloads per connection (~400 MiB total network traffic) is trivial for attackers
- **No rate limiting**: No per-peer or per-protocol memory limits exist in the decompression path

## Recommendation

Implement a separate, much lower decompression size limit specifically for network messages received from untrusted peers. The limit should account for the compression amplification risk:

```rust
// In config/src/config/network_config.rs
pub const MAX_COMPRESSED_MESSAGE_DECOMPRESSION_SIZE: usize = 8 * 1024 * 1024; /* 8 MiB - only 2x amplification from 4 MiB frames */

// In network/framework/src/protocols/wire/handshake/v1/mod.rs
pub fn from_bytes<T: DeserializeOwned>(&self, bytes: &[u8]) -> anyhow::Result<T> {
    let deserialization_timer = start_serialization_timer(*self, DESERIALIZATION_LABEL);
    
    let result = match self.encoding() {
        Encoding::Bcs(limit) => self.bcs_decode(bytes, limit),
        Encoding::CompressedBcs(limit) => {
            let compression_client = self.get_compression_client();
            // Use lower limit for untrusted network decompression
            let raw_bytes = aptos_compression::decompress(
                &bytes.to_vec(),
                compression_client,
                MAX_COMPRESSED_MESSAGE_DECOMPRESSION_SIZE, // Changed from MAX_APPLICATION_MESSAGE_SIZE
            )
            .map_err(|e| anyhow! {"{:?}", e})?;
            self.bcs_decode(&raw_bytes, limit)
        },
        Encoding::Json => serde_json::from_slice(bytes).map_err(|e| anyhow!("{:?}", e)),
    };
    
    if result.is_ok() {
        deserialization_timer.observe_duration();
    }
    
    result
}
```

Additionally, implement per-connection memory tracking to prevent simultaneous large allocations across multiple peers.

## Proof of Concept

```rust
// PoC demonstrating the compression bomb attack
use aptos_compression::{compress, decompress, client::CompressionClient};
use aptos_config::config::MAX_APPLICATION_MESSAGE_SIZE;

#[test]
fn test_compression_bomb_attack() {
    // Create highly compressible data (all zeros)
    let uncompressed = vec![0u8; MAX_APPLICATION_MESSAGE_SIZE];
    
    // Compress it
    let compressed = compress(
        uncompressed.clone(),
        CompressionClient::Consensus,
        MAX_APPLICATION_MESSAGE_SIZE,
    ).expect("Compression should succeed");
    
    println!("Uncompressed size: {} bytes ({:.2} MiB)", 
        uncompressed.len(), 
        uncompressed.len() as f64 / (1024.0 * 1024.0));
    println!("Compressed size: {} bytes ({:.2} MiB)", 
        compressed.len(), 
        compressed.len() as f64 / (1024.0 * 1024.0));
    println!("Compression ratio: {:.2}x", 
        uncompressed.len() as f64 / compressed.len() as f64);
    
    // Verify compressed size fits in MAX_FRAME_SIZE
    const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024;
    assert!(compressed.len() <= MAX_FRAME_SIZE, 
        "Compressed data fits in single frame: {} <= {}", 
        compressed.len(), MAX_FRAME_SIZE);
    
    // Simulate attacker scenario: decompress triggers large allocation
    let decompressed = decompress(
        &compressed,
        CompressionClient::Consensus,
        MAX_APPLICATION_MESSAGE_SIZE,
    ).expect("Decompression should succeed");
    
    assert_eq!(decompressed.len(), MAX_APPLICATION_MESSAGE_SIZE);
    
    // Attack simulation: 100 connections × 61.88 MiB = 6.188 GB
    println!("\nAttack Impact:");
    println!("Single decompression allocation: {:.2} MiB", 
        decompressed.len() as f64 / (1024.0 * 1024.0));
    println!("100 concurrent connections: {:.2} GiB", 
        (decompressed.len() * 100) as f64 / (1024.0 * 1024.0 * 1024.0));
}
```

## Notes

The vulnerability stems from the mismatch between network framing limits (4 MiB) and application message limits (61.88 MiB) when combined with compression. While the decompression limit is technically enforced, the compression amplification creates a resource exhaustion vector that can crash validators with relatively small network payloads. This is distinct from simple network-level DoS because it exploits the protocol-level compression feature to achieve disproportionate memory consumption.

### Citations

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L196-222)
```rust
    pub fn to_bytes<T: Serialize>(&self, value: &T) -> anyhow::Result<Vec<u8>> {
        // Start the serialization timer
        let serialization_timer = start_serialization_timer(*self, SERIALIZATION_LABEL);

        // Serialize the message
        let result = match self.encoding() {
            Encoding::Bcs(limit) => self.bcs_encode(value, limit),
            Encoding::CompressedBcs(limit) => {
                let compression_client = self.get_compression_client();
                let bcs_bytes = self.bcs_encode(value, limit)?;
                aptos_compression::compress(
                    bcs_bytes,
                    compression_client,
                    MAX_APPLICATION_MESSAGE_SIZE,
                )
                .map_err(|e| anyhow!("{:?}", e))
            },
            Encoding::Json => serde_json::to_vec(value).map_err(|e| anyhow!("{:?}", e)),
        };

        // Only record the duration if serialization was successful
        if result.is_ok() {
            serialization_timer.observe_duration();
        }

        result
    }
```

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L226-242)
```rust
    pub fn from_bytes<T: DeserializeOwned>(&self, bytes: &[u8]) -> anyhow::Result<T> {
        // Start the deserialization timer
        let deserialization_timer = start_serialization_timer(*self, DESERIALIZATION_LABEL);

        // Deserialize the message
        let result = match self.encoding() {
            Encoding::Bcs(limit) => self.bcs_decode(bytes, limit),
            Encoding::CompressedBcs(limit) => {
                let compression_client = self.get_compression_client();
                let raw_bytes = aptos_compression::decompress(
                    &bytes.to_vec(),
                    compression_client,
                    MAX_APPLICATION_MESSAGE_SIZE,
                )
                .map_err(|e| anyhow! {"{:?}", e})?;
                self.bcs_decode(&raw_bytes, limit)
            },
```

**File:** crates/aptos-compression/src/lib.rs (L92-121)
```rust
pub fn decompress(
    compressed_data: &CompressedData,
    client: CompressionClient,
    max_size: usize,
) -> Result<Vec<u8>, Error> {
    // Start the decompression timer
    let start_time = Instant::now();

    // Check size of the data and initialize raw_data
    let decompressed_size = match get_decompressed_size(compressed_data, max_size) {
        Ok(size) => size,
        Err(error) => {
            let error_string = format!("Failed to get decompressed size: {}", error);
            return create_decompression_error(&client, error_string);
        },
    };
    let mut raw_data = vec![0u8; decompressed_size];

    // Decompress the data
    if let Err(error) = lz4::block::decompress_to_buffer(compressed_data, None, &mut raw_data) {
        let error_string = format!("Failed to decompress the data: {}", error);
        return create_decompression_error(&client, error_string);
    };

    // Stop the timer and update the metrics
    metrics::observe_decompression_operation_time(&client, start_time);
    metrics::update_decompression_metrics(&client, compressed_data, &raw_data);

    Ok(raw_data)
}
```

**File:** crates/aptos-compression/src/lib.rs (L150-184)
```rust
fn get_decompressed_size(
    compressed_data: &CompressedData,
    max_size: usize,
) -> Result<usize, Error> {
    // Ensure that the compressed data is at least 4 bytes long
    if compressed_data.len() < 4 {
        return Err(DecompressionError(format!(
            "Compressed data must be at least 4 bytes long! Got: {}",
            compressed_data.len()
        )));
    }

    // Parse the size prefix
    let size = (compressed_data[0] as i32)
        | ((compressed_data[1] as i32) << 8)
        | ((compressed_data[2] as i32) << 16)
        | ((compressed_data[3] as i32) << 24);
    if size < 0 {
        return Err(DecompressionError(format!(
            "Parsed size prefix in buffer must not be negative! Got: {}",
            size
        )));
    }

    // Ensure that the size is not greater than the max size limit
    let size = size as usize;
    if size > max_size {
        return Err(DecompressionError(format!(
            "Parsed size prefix in buffer is too big: {} > {}",
            size, max_size
        )));
    }

    Ok(size)
}
```

**File:** config/src/config/network_config.rs (L44-44)
```rust
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
```

**File:** config/src/config/network_config.rs (L45-50)
```rust
pub const MAX_MESSAGE_METADATA_SIZE: usize = 128 * 1024; /* 128 KiB: a buffer for metadata that might be added to messages by networking */
pub const MESSAGE_PADDING_SIZE: usize = 2 * 1024 * 1024; /* 2 MiB: a safety buffer to allow messages to get larger during serialization */
pub const MAX_APPLICATION_MESSAGE_SIZE: usize =
    (MAX_MESSAGE_SIZE - MAX_MESSAGE_METADATA_SIZE) - MESSAGE_PADDING_SIZE; /* The message size that applications should check against */
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** network/framework/src/protocols/stream/mod.rs (L253-256)
```rust
    /// Returns true if the message should be streamed
    pub fn should_stream(&self, message: &NetworkMessage) -> bool {
        message.data_len() > self.max_frame_size
    }
```

**File:** consensus/src/network.rs (L326-328)
```rust
            let response_msg =
                tokio::task::spawn_blocking(move || protocol.from_bytes(&bytes)).await??;
            Ok(response_msg)
```
