# Audit Report

## Title
Concurrent Decompression Memory Exhaustion via Legitimate Compression Ratio Exploitation in Consensus Network Layer

## Summary
An attacker can cause memory exhaustion on validator nodes by sending multiple small compressed consensus messages that each legitimately decompress to `MAX_APPLICATION_MESSAGE_SIZE` (~62 MiB). When processed concurrently through the network layer's parallel deserialization pipeline, these messages cause simultaneous memory allocations that can exhaust node resources, leading to consensus liveness failures.

## Finding Description

The vulnerability exists in the network message deserialization pipeline where compressed consensus messages are processed in parallel without aggregate memory limits.

**Vulnerability Components:**

1. **Consensus uses compressed protocols by default:** [1](#0-0) 

2. **Decompression validates size against MAX_APPLICATION_MESSAGE_SIZE (~62 MiB):** [2](#0-1) 

3. **Memory is allocated BEFORE decompression based on LZ4 header:** [3](#0-2) 

4. **Messages are deserialized concurrently in blocking tasks:** [4](#0-3) 

5. **Parallel deserialization defaults to number of CPU cores:** [5](#0-4) 

6. **Consensus DAG handler processes messages concurrently:** [6](#0-5) 

**Attack Scenario:**

1. Attacker crafts small compressed payloads (e.g., 1KB each containing highly compressible data like zeros)
2. Each payload's LZ4 header claims decompression to exactly 62 MiB (within allowed limit)
3. Attacker floods validator node with these messages
4. Network layer spawns N concurrent deserialization tasks (N = CPU cores, typically 16-64)
5. Each task reads LZ4 header and allocates 62 MiB: `vec![0u8; 62_000_000]`
6. Total instantaneous allocation: N Ã— 62 MiB = 1-4 GB
7. Continuous flooding maintains pressure, causing OOM or severe memory contention
8. Validator becomes unresponsive, failing to participate in consensus

**Broken Invariants:**
- **Resource Limits**: Operations do not respect computational/memory limits globally
- **Consensus Safety/Liveness**: Validators can be rendered unavailable through resource exhaustion

## Impact Explanation

**Critical Severity** - This meets the "Total loss of liveness/network availability" criteria:

- **Validator Unavailability**: Affected nodes become unresponsive or crash due to memory exhaustion
- **Consensus Disruption**: If sufficient validators (>1/3) are targeted, network liveness fails
- **No Privileged Access Required**: Any network peer can send these messages
- **Amplification Factor**: Small attack traffic (KB/s) causes large memory allocations (GB)
- **Difficult to Mitigate**: No per-message size violation occurs; each message is "legitimate"

On a typical 16-core validator with 32 GB RAM:
- Attack can trigger 1 GB+ allocations repeatedly
- Combined with normal operations, triggers OOM killer
- Node requires restart, causing consensus participation gaps

## Likelihood Explanation

**High Likelihood:**

- **Low Attack Complexity**: Crafting LZ4-compressed payloads with specific decompression sizes is trivial using standard libraries
- **No Authentication Bypass Required**: Attacker only needs network connectivity to validator nodes
- **Default Configuration Vulnerable**: All validators using default network configuration are affected
- **Visible Attack Surface**: Validator network addresses are often publicly known
- **Immediate Impact**: Effects are observable within seconds of attack commencement

The only barrier is network access to validator nodes, which is inherent to the P2P network architecture.

## Recommendation

Implement multi-layered protection against decompression bombs:

**1. Add Compression Ratio Check:**
```rust
// In crates/aptos-compression/src/lib.rs, modify decompress():
pub fn decompress(
    compressed_data: &CompressedData,
    client: CompressionClient,
    max_size: usize,
) -> Result<Vec<u8>, Error> {
    // ... existing code ...
    
    let decompressed_size = match get_decompressed_size(compressed_data, max_size) {
        Ok(size) => size,
        Err(error) => { /* ... */ },
    };
    
    // NEW: Enforce maximum compression ratio
    const MAX_COMPRESSION_RATIO: usize = 20; // e.g., 20:1
    if compressed_data.len() > 0 && decompressed_size / compressed_data.len() > MAX_COMPRESSION_RATIO {
        return create_decompression_error(
            &client,
            format!("Compression ratio too high: {}:1", decompressed_size / compressed_data.len())
        );
    }
    
    // ... rest of function ...
}
```

**2. Add Global Decompression Memory Budget:**
```rust
// Add to network configuration
pub struct DecompressionLimits {
    pub max_concurrent_decompressions: usize,
    pub max_total_decompression_memory: usize,
}

// Track active decompressions with a semaphore/counter
// Reject new decompressions when budget exhausted
```

**3. Reduce MAX_APPLICATION_MESSAGE_SIZE:**
Consider whether 62 MiB is necessary for consensus messages, or if a smaller limit (e.g., 16 MiB) would suffice.

**4. Add Rate Limiting:**
Implement per-peer rate limits on compressed message volume.

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[tokio::test]
async fn test_concurrent_decompression_memory_exhaustion() {
    use aptos_compression::{compress, CompressionClient};
    use aptos_config::config::MAX_APPLICATION_MESSAGE_SIZE;
    
    // Create highly compressible data that expands to MAX_APPLICATION_MESSAGE_SIZE
    let raw_data = vec![0u8; MAX_APPLICATION_MESSAGE_SIZE];
    
    // Compress it (should be very small, < 1KB)
    let compressed = compress(
        raw_data.clone(),
        CompressionClient::Consensus,
        MAX_APPLICATION_MESSAGE_SIZE,
    ).unwrap();
    
    println!("Compressed size: {} bytes", compressed.len());
    println!("Decompressed size: {} bytes", MAX_APPLICATION_MESSAGE_SIZE);
    println!("Compression ratio: {}:1", MAX_APPLICATION_MESSAGE_SIZE / compressed.len());
    
    // Simulate concurrent decompression
    let num_cores = num_cpus::get();
    let mut handles = vec![];
    
    for i in 0..num_cores {
        let compressed_clone = compressed.clone();
        let handle = tokio::spawn(async move {
            let _decompressed = aptos_compression::decompress(
                &compressed_clone,
                CompressionClient::Consensus,
                MAX_APPLICATION_MESSAGE_SIZE,
            ).unwrap();
            println!("Task {} allocated {} MiB", i, MAX_APPLICATION_MESSAGE_SIZE / 1_000_000);
        });
        handles.push(handle);
    }
    
    // Wait for all tasks
    for handle in handles {
        handle.await.unwrap();
    }
    
    println!("Total memory allocated: {} MiB", 
        (num_cores * MAX_APPLICATION_MESSAGE_SIZE) / 1_000_000);
    
    // In production, this pattern repeated continuously causes OOM
}
```

**Attack Script (Conceptual):**
```rust
// Attacker sends these messages continuously
loop {
    let payload = create_highly_compressible_payload(); // e.g., zeros
    let compressed = compress_to_max_size(payload);
    send_consensus_message(validator_addr, compressed).await;
    // No delay needed; flood as fast as possible
}
```

## Notes

This vulnerability demonstrates that per-message validation is insufficient when messages are processed concurrently. The decompression limit protects against individual oversized messages but not against the aggregate memory impact of many "legitimate" compressed messages processed simultaneously. The high default value of `MAX_APPLICATION_MESSAGE_SIZE` (~62 MiB) combined with CPU-count parallelism creates an exploitable memory exhaustion vector.

### Citations

**File:** consensus/src/network_interface.rs (L157-168)
```rust
pub const RPC: &[ProtocolId] = &[
    ProtocolId::ConsensusRpcCompressed,
    ProtocolId::ConsensusRpcBcs,
    ProtocolId::ConsensusRpcJson,
];

/// Supported protocols in preferred order (from highest priority to lowest).
pub const DIRECT_SEND: &[ProtocolId] = &[
    ProtocolId::ConsensusDirectSendCompressed,
    ProtocolId::ConsensusDirectSendBcs,
    ProtocolId::ConsensusDirectSendJson,
];
```

**File:** crates/aptos-compression/src/lib.rs (L92-121)
```rust
pub fn decompress(
    compressed_data: &CompressedData,
    client: CompressionClient,
    max_size: usize,
) -> Result<Vec<u8>, Error> {
    // Start the decompression timer
    let start_time = Instant::now();

    // Check size of the data and initialize raw_data
    let decompressed_size = match get_decompressed_size(compressed_data, max_size) {
        Ok(size) => size,
        Err(error) => {
            let error_string = format!("Failed to get decompressed size: {}", error);
            return create_decompression_error(&client, error_string);
        },
    };
    let mut raw_data = vec![0u8; decompressed_size];

    // Decompress the data
    if let Err(error) = lz4::block::decompress_to_buffer(compressed_data, None, &mut raw_data) {
        let error_string = format!("Failed to decompress the data: {}", error);
        return create_decompression_error(&client, error_string);
    };

    // Stop the timer and update the metrics
    metrics::observe_decompression_operation_time(&client, start_time);
    metrics::update_decompression_metrics(&client, compressed_data, &raw_data);

    Ok(raw_data)
}
```

**File:** crates/aptos-compression/src/lib.rs (L150-184)
```rust
fn get_decompressed_size(
    compressed_data: &CompressedData,
    max_size: usize,
) -> Result<usize, Error> {
    // Ensure that the compressed data is at least 4 bytes long
    if compressed_data.len() < 4 {
        return Err(DecompressionError(format!(
            "Compressed data must be at least 4 bytes long! Got: {}",
            compressed_data.len()
        )));
    }

    // Parse the size prefix
    let size = (compressed_data[0] as i32)
        | ((compressed_data[1] as i32) << 8)
        | ((compressed_data[2] as i32) << 16)
        | ((compressed_data[3] as i32) << 24);
    if size < 0 {
        return Err(DecompressionError(format!(
            "Parsed size prefix in buffer must not be negative! Got: {}",
            size
        )));
    }

    // Ensure that the size is not greater than the max size limit
    let size = size as usize;
    if size > max_size {
        return Err(DecompressionError(format!(
            "Parsed size prefix in buffer is too big: {} > {}",
            size, max_size
        )));
    }

    Ok(size)
}
```

**File:** network/framework/src/protocols/network/mod.rs (L217-235)
```rust
        let data_event_stream = peer_mgr_notifs_rx.map(|notification| {
            tokio::task::spawn_blocking(move || received_message_to_event(notification))
        });

        let data_event_stream: Pin<
            Box<dyn Stream<Item = Event<TMessage>> + Send + Sync + 'static>,
        > = if allow_out_of_order_delivery {
            Box::pin(
                data_event_stream
                    .buffer_unordered(max_parallel_deserialization_tasks)
                    .filter_map(|res| future::ready(res.expect("JoinError from spawn blocking"))),
            )
        } else {
            Box::pin(
                data_event_stream
                    .buffered(max_parallel_deserialization_tasks)
                    .filter_map(|res| future::ready(res.expect("JoinError from spawn blocking"))),
            )
        };
```

**File:** config/src/config/network_config.rs (L178-185)
```rust
    /// Configures the number of parallel deserialization tasks
    /// based on the number of CPU cores of the machine. This is
    /// only done if the config does not specify a value.
    fn configure_num_deserialization_tasks(&mut self) {
        if self.max_parallel_deserialization_tasks.is_none() {
            self.max_parallel_deserialization_tasks = Some(num_cpus::get());
        }
    }
```

**File:** consensus/src/dag/dag_handler.rs (L89-150)
```rust
        let mut verified_msg_stream = concurrent_map(
            dag_rpc_rx,
            executor.clone(),
            move |rpc_request: IncomingDAGRequest| {
                let epoch_state = epoch_state.clone();
                async move {
                    let epoch = rpc_request.req.epoch();
                    let result = rpc_request
                        .req
                        .try_into()
                        .and_then(|dag_message: DAGMessage| {
                            monitor!(
                                "dag_message_verify",
                                dag_message.verify(rpc_request.sender, &epoch_state.verifier)
                            )?;
                            Ok(dag_message)
                        });
                    (result, epoch, rpc_request.sender, rpc_request.responder)
                }
            },
        );

        let dag_driver_clone = dag_driver.clone();
        let node_receiver_clone = node_receiver.clone();
        let handle = tokio::spawn(async move {
            while let Some(new_round) = new_round_event.recv().await {
                monitor!("dag_on_new_round_event", {
                    dag_driver_clone.enter_new_round(new_round).await;
                    node_receiver_clone.gc();
                });
            }
        });
        defer!(handle.abort());

        let mut futures = FuturesUnordered::new();
        // A separate executor to ensure the message verification sender (above) and receiver (below) are
        // not blocking each other.
        // TODO: make this configurable
        let executor = BoundedExecutor::new(8, Handle::current());
        loop {
            select! {
                Some((msg, epoch, author, responder)) = verified_msg_stream.next() => {
                    let verified_msg_processor = verified_msg_processor.clone();
                    let f = executor.spawn(async move {
                        monitor!("dag_on_verified_msg", {
                            match verified_msg_processor.process_verified_message(msg, epoch, author, responder).await {
                                Ok(sync_status) => {
                                    if matches!(
                                        sync_status,
                                        SyncOutcome::NeedsSync(_) | SyncOutcome::EpochEnds
                                    ) {
                                        return Some(sync_status);
                                    }
                                },
                                Err(e) => {
                                    warn!(error = ?e, "error processing rpc");
                                },
                            };
                            None
                        })
                    }).await;
                    futures.push(f);
```
