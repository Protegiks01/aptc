# Audit Report

## Title
TOCTOU Race Condition in Block Ordering Allows Buffer Corruption and Consensus Safety Violation

## Summary
A Time-of-Check to Time-of-Use (TOCTOU) race condition in `BlockStore::send_for_execution` allows duplicate OrderedBlocks with the same block_id to be sent to the buffer manager. This causes linked list corruption in the buffer's internal HashMap-based storage, making blocks unreachable and causing validators to process blocks in different orders, resulting in state divergence and consensus safety violations.

## Finding Description
The vulnerability exists in the interaction between the block ordering logic and the buffer manager's internal data structure. 

The `send_for_execution` method performs a non-atomic check-then-update sequence: [1](#0-0) 

The race window exists between:
1. Reading the ordered root round (line 323)
2. Checking if the new block is ahead (lines 322-325)
3. Updating the ordered root (line 338)

If two threads execute this method concurrently with the same finality proof, both can pass the check before either updates the ordered root, causing duplicate OrderedBlocks to be sent to the buffer manager.

The buffer manager's `process_ordered_blocks` function inserts blocks into a HashMap-based linked list structure without deduplication: [2](#0-1) 

The critical issue is in the `Buffer::push_back` implementation: [3](#0-2) 

When `push_back` is called with an element having the same hash (block_id) as an existing element, `HashMap::insert` **overwrites** the previous LinkedItem entry. This breaks the linked list structure:
- The previous item's `next` pointer is lost
- Items between the overwritten entry and tail become unreachable from head
- Different validators process different subsets of blocks

The buffer iteration methods rely on the linked list structure: [4](#0-3) 

When the linked list is corrupted, `find_elem_from` will skip unreachable items, causing different validators to execute blocks in different orders or skip blocks entirely.

**Attack Scenario:**
1. Malicious validator participates in consensus for block B at round R
2. Validator sends duplicate commit messages or times messages to trigger concurrent processing
3. Two threads (e.g., RoundManager and SyncManager) concurrently call `execute_and_insert_block` or `insert_ordered_cert` with the same QC
4. Both threads enter `send_for_execution` simultaneously
5. Both read ordered_root = R-1 and pass the check `block.round > R-1`
6. Both send OrderedBlocks for round R to buffer manager
7. Buffer manager processes sequentially: first inserts block R, then overwrites it with duplicate
8. Linked list corruption: blocks between the overwritten entry and tail become unreachable
9. Different validators have different buffer states (due to timing variations)
10. Validators process different blocks → state divergence → consensus safety violation

## Impact Explanation
This is a **Critical severity** vulnerability ($1,000,000 bounty category) because it causes:

1. **Consensus Safety Violation**: Different validators produce different state roots for the same block sequence, violating the fundamental safety guarantee of BFT consensus (Invariant #2).

2. **Deterministic Execution Failure**: Validators no longer execute identical blocks deterministically (Invariant #1).

3. **Non-Recoverable State Divergence**: Once validators diverge, they cannot reach consensus on subsequent blocks, requiring a network hard fork to recover.

The impact affects all validators in the network, not just those controlled by the attacker. Even a single malicious validator with <33% stake can trigger this vulnerability to break consensus safety.

## Likelihood Explanation
**Likelihood: Medium-High**

The vulnerability is exploitable because:
1. The TOCTOU race window exists in production code
2. BlockStore is accessed concurrently (uses `Arc<RwLock<BlockTree>>` for thread safety)
3. RoundManager and SyncManager can process messages in parallel
4. A malicious validator can trigger the race by:
   - Sending duplicate/delayed consensus messages
   - Exploiting network timing to cause concurrent QC processing
   - Leveraging sync protocol to request the same block multiple times

The attack requires validator-level access but does not require >33% stake or collusion, making it realistic for a compromised or malicious validator operator.

## Recommendation
Implement atomic check-and-update using proper locking or atomic operations:

```rust
pub async fn send_for_execution(
    &self,
    finality_proof: WrappedLedgerInfo,
) -> anyhow::Result<()> {
    let block_id_to_commit = finality_proof.commit_info().id();
    
    // Acquire write lock for entire check-update sequence
    let mut inner = self.inner.write();
    
    // Check if already ordered
    ensure!(
        finality_proof.commit_info().round() > inner.ordered_root().round(),
        "Committed block round lower than or equal to root"
    );
    
    let block_to_commit = inner.get_block(block_id_to_commit)
        .ok_or_else(|| format_err!("Committed block id not found"))?;
        
    let blocks_to_commit = inner
        .path_from_ordered_root(block_id_to_commit)
        .unwrap_or_default();
    
    assert!(!blocks_to_commit.is_empty());
    
    // Update ordered root before releasing lock
    inner.update_ordered_root(block_to_commit.id());
    inner.insert_ordered_cert(finality_proof.clone());
    
    // Release lock before async operations
    drop(inner);
    
    // Clean up and send for execution
    self.pending_blocks.lock().gc(finality_proof.commit_info().round());
    update_counters_for_ordered_blocks(&blocks_to_commit);
    
    self.execution_client
        .finalize_order(blocks_to_commit, finality_proof)
        .await
        .expect("Failed to persist commit");
    
    Ok(())
}
```

Additionally, add deduplication in `process_ordered_blocks`:

```rust
async fn process_ordered_blocks(&mut self, ordered_blocks: OrderedBlocks) {
    let block_id = ordered_blocks.ordered_blocks.last()
        .expect("OrderedBlocks empty").id();
    
    // Check if block already exists in buffer
    if self.buffer.exist(&Some(block_id)) {
        warn!("Duplicate OrderedBlocks detected for block_id {}, ignoring", block_id);
        return;
    }
    
    // Continue with normal processing...
}
```

## Proof of Concept
```rust
#[cfg(test)]
mod toctou_race_test {
    use super::*;
    use std::sync::Arc;
    use tokio::task::JoinSet;
    
    #[tokio::test(flavor = "multi_thread", worker_threads = 4)]
    async fn test_concurrent_send_for_execution_race() {
        // Setup: Create BlockStore with test blocks
        let (block_store, finality_proof) = setup_test_block_store();
        let block_store = Arc::new(block_store);
        
        // Track OrderedBlocks sent to buffer manager
        let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel();
        
        // Replace execution client with mock that records sends
        let mock_client = Arc::new(MockExecutionClient::new(tx));
        block_store.set_execution_client(mock_client);
        
        // Concurrent execution: spawn 10 tasks calling send_for_execution
        let mut tasks = JoinSet::new();
        for _ in 0..10 {
            let store = block_store.clone();
            let proof = finality_proof.clone();
            tasks.spawn(async move {
                let _ = store.send_for_execution(proof).await;
            });
        }
        
        // Wait for all tasks
        while let Some(_) = tasks.join_next().await {}
        
        // Collect all OrderedBlocks sent
        let mut ordered_blocks_sent = vec![];
        while let Ok(blocks) = rx.try_recv() {
            ordered_blocks_sent.push(blocks);
        }
        
        // Verify race condition: multiple sends for same block_id
        assert!(
            ordered_blocks_sent.len() > 1,
            "Race condition: {} duplicate OrderedBlocks sent (expected 1)",
            ordered_blocks_sent.len()
        );
        
        // All should have same block_id
        let first_id = ordered_blocks_sent[0].ordered_blocks.last().unwrap().id();
        for blocks in &ordered_blocks_sent {
            assert_eq!(blocks.ordered_blocks.last().unwrap().id(), first_id);
        }
        
        println!("✗ VULNERABILITY CONFIRMED: {} duplicate sends", 
                 ordered_blocks_sent.len());
    }
}
```

**Notes**
- The TOCTOU race exists at lines 322-338 of `block_store.rs` where the check and update are not atomic
- Similar patterns exist in `execute_and_insert_block` and `insert_ordered_cert` methods
- The buffer corruption occurs because `HashMap::insert` overwrites entries with the same key
- The vulnerability breaks Consensus Safety (Invariant #2) and Deterministic Execution (Invariant #1)
- Impact is Critical severity: state divergence requires hard fork to recover

### Citations

**File:** consensus/src/block_storage/block_store.rs (L311-350)
```rust
    /// Send an ordered block id with the proof for execution, returns () on success or error
    pub async fn send_for_execution(
        &self,
        finality_proof: WrappedLedgerInfo,
    ) -> anyhow::Result<()> {
        let block_id_to_commit = finality_proof.commit_info().id();
        let block_to_commit = self
            .get_block(block_id_to_commit)
            .ok_or_else(|| format_err!("Committed block id not found"))?;

        // First make sure that this commit is new.
        ensure!(
            block_to_commit.round() > self.ordered_root().round(),
            "Committed block round lower than root"
        );

        let blocks_to_commit = self
            .path_from_ordered_root(block_id_to_commit)
            .unwrap_or_default();

        assert!(!blocks_to_commit.is_empty());

        let finality_proof_clone = finality_proof.clone();
        self.pending_blocks
            .lock()
            .gc(finality_proof.commit_info().round());

        self.inner.write().update_ordered_root(block_to_commit.id());
        self.inner
            .write()
            .insert_ordered_cert(finality_proof_clone.clone());
        update_counters_for_ordered_blocks(&blocks_to_commit);

        self.execution_client
            .finalize_order(blocks_to_commit, finality_proof.clone())
            .await
            .expect("Failed to persist commit");

        Ok(())
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L382-424)
```rust
    async fn process_ordered_blocks(&mut self, ordered_blocks: OrderedBlocks) {
        let OrderedBlocks {
            ordered_blocks,
            ordered_proof,
        } = ordered_blocks;

        info!(
            "Receive {} ordered block ends with [epoch: {}, round: {}, id: {}], the queue size is {}",
            ordered_blocks.len(),
            ordered_proof.commit_info().epoch(),
            ordered_proof.commit_info().round(),
            ordered_proof.commit_info().id(),
            self.buffer.len() + 1,
        );

        let request = self.create_new_request(ExecutionRequest {
            ordered_blocks: ordered_blocks.clone(),
        });
        if let Some(consensus_publisher) = &self.consensus_publisher {
            let message = ConsensusObserverMessage::new_ordered_block_message(
                ordered_blocks.clone(),
                ordered_proof.clone(),
            );
            consensus_publisher.publish_message(message);
        }
        self.execution_schedule_phase_tx
            .send(request)
            .await
            .expect("Failed to send execution schedule request");

        let mut unverified_votes = HashMap::new();
        if let Some(block) = ordered_blocks.last() {
            if let Some(votes) = self.pending_commit_votes.remove(&block.round()) {
                for (_, vote) in votes {
                    if vote.commit_info().id() == block.id() {
                        unverified_votes.insert(vote.author(), vote);
                    }
                }
            }
        }
        let item = BufferItem::new_ordered(ordered_blocks, ordered_proof, unverified_votes);
        self.buffer.push_back(item);
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L429-452)
```rust
    fn advance_execution_root(&mut self) -> Option<HashValue> {
        let cursor = self.execution_root;
        self.execution_root = self
            .buffer
            .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
                item.is_ordered()
            });
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
        } else {
            sample!(
                SampleRate::Frequency(2),
                info!(
                    "Advance execution root from {:?} to {:?}",
                    cursor, self.execution_root
                )
            );
            // Otherwise do nothing, because the execution wait phase is driven by the response of
            // the execution schedule phase, which is in turn fed as soon as the ordered blocks
            // come in.
            None
        }
    }
```

**File:** consensus/src/pipeline/buffer.rs (L51-64)
```rust
    pub fn push_back(&mut self, elem: T) {
        self.count = self.count.checked_add(1).unwrap();
        let t_hash = elem.hash();
        self.map.insert(t_hash, LinkedItem {
            elem: Some(elem),
            index: self.count,
            next: None,
        });
        if let Some(tail) = self.tail {
            self.map.get_mut(&tail).unwrap().next = Some(t_hash);
        }
        self.tail = Some(t_hash);
        self.head.get_or_insert(t_hash);
    }
```
