# Audit Report

## Title
File Store Metadata Corruption Due to Unverified Transaction Count in Backfiller

## Summary
The indexer-grpc-v2-file-store-backfiller does not verify that exactly `num_transactions_per_folder` transactions are received from the fullnode, leading to incomplete batches where buffered transactions are lost and batch metadata files are never created, while the progress file is incorrectly updated as if the batch completed successfully.

## Finding Description

The `backfill()` function in the file store backfiller requests a specific number of transactions but fails to validate that this exact count is received. [1](#0-0) 

The gRPC request specifies `transactions_count` but this is merely a **request**, not a guarantee. [2](#0-1) 

The server can legally return fewer transactions in several scenarios:
- When the chain hasn't progressed far enough and `highest_known_version` is less than requested [3](#0-2) 
- When the abort handle is triggered [4](#0-3) 
- When `process_next_batch()` returns empty results [5](#0-4) 

The critical flaw is in how the backfiller processes transactions. The `FileStoreOperatorV2` only flushes its buffer and writes batch metadata when `end_batch` is true, which is determined by checking if the transaction version satisfies the modulo condition. [6](#0-5) 

If fewer transactions are received than expected, the last transaction's version will not satisfy `(version + 1) % num_txns_per_folder == 0`, so:
1. Any buffered transactions that haven't reached `max_size_per_file` remain unflushed
2. The batch metadata file is never created [7](#0-6) 
3. The progress file is still updated with the full expected version range [8](#0-7) 

When a reader later attempts to read from this incomplete batch, `get_batch_metadata()` returns None because the metadata file was never written. [9](#0-8) 

This breaks the **State Consistency** invariant: the file store metadata (progress file) claims the batch is complete, but the actual batch metadata and potentially some transaction data are missing.

## Impact Explanation

This is **HIGH severity** per the Aptos bug bounty criteria because it causes:

1. **State inconsistencies requiring intervention** (Medium severity): The progress file and actual stored data become inconsistent, requiring manual cleanup and re-backfilling
2. **API crashes** (High severity): Readers attempting to serve transactions from the incomplete batch will fail when `get_batch_metadata()` returns None, effectively making the indexer gRPC service unable to serve those version ranges
3. **Data loss**: Buffered transactions that haven't reached the size threshold are permanently lost when the task completes

The vulnerability affects the reliability of the entire indexer-grpc file store infrastructure, which downstream services depend on for historical transaction data.

## Likelihood Explanation

This issue has **MEDIUM to HIGH likelihood** of occurring because:

1. **No attacker required**: The vulnerability is triggered during normal operations, not by malicious input
2. **Common scenarios**: Multiple legitimate scenarios can cause fewer transactions to be returned:
   - Backfilling near the chain tip where new transactions are still arriving
   - Network interruptions during streaming
   - Fullnode restarts or service degradation
   - Rate limiting or resource constraints on the fullnode
3. **No validation**: There is zero verification that the expected transaction count was received

The issue will manifest whenever the backfiller processes a batch where the fullnode returns fewer transactions than `num_transactions_per_folder`, which can easily happen during normal indexer operations.

## Recommendation

Add explicit transaction count verification and final buffer flushing:

```rust
// In processor.rs, after the stream processing loop (after line 199)
// Add verification and final flush logic:

let mut transactions_received = 0;
while let Some(response_item) = stream.next().await {
    match response_item {
        Ok(r) => {
            // ... existing processing ...
            for transaction in transactions {
                transactions_received += 1;
                file_store_operator
                    .buffer_and_maybe_dump_transactions_to_file(
                        transaction,
                        tx.clone(),
                    )
                    .await
                    .unwrap();
            }
        },
        // ... error handling ...
    }
}

// Verify we received the expected count
if transactions_received != num_transactions_per_folder {
    panic!(
        "Expected {} transactions but only received {} for batch starting at version {}",
        num_transactions_per_folder, transactions_received, task_version
    );
}

// Force final flush if end_batch wasn't triggered
file_store_operator.final_flush(tx.clone()).await.unwrap();
```

Additionally, add a `final_flush()` method to `FileStoreOperatorV2` that forces flushing of any remaining buffered data and ensures batch metadata is written.

## Proof of Concept

```rust
#[tokio::test]
async fn test_incomplete_batch_corruption() {
    // Setup: Create a backfiller with num_transactions_per_folder = 1000
    // and a mock fullnode that returns only 500 transactions
    
    let num_transactions_per_folder = 1000u64;
    let starting_version = 0u64;
    
    // Mock fullnode returns only 500 transactions instead of 1000
    let mock_transactions: Vec<Transaction> = (0..500)
        .map(|i| create_mock_transaction(i))
        .collect();
    
    // Simulate the backfiller processing
    let mut file_store_operator = FileStoreOperatorV2::new(
        MAX_SIZE_PER_FILE,
        num_transactions_per_folder,
        starting_version,
        BatchMetadata::default(),
    );
    
    let (tx, mut rx) = tokio::sync::mpsc::channel(10);
    
    // Process the 500 transactions
    for transaction in mock_transactions {
        file_store_operator
            .buffer_and_maybe_dump_transactions_to_file(transaction, tx.clone())
            .await
            .unwrap();
    }
    
    // Stream ends here - no more transactions
    drop(tx);
    
    // Verify the issue:
    // 1. Check that end_batch was never true (version 499 doesn't satisfy (499+1) % 1000 == 0)
    assert_eq!(file_store_operator.version(), 500);
    
    // 2. Check that buffered transactions exist but weren't flushed
    assert!(!file_store_operator.buffer.is_empty());
    
    // 3. Simulate progress file update (happens unconditionally)
    let progress_version = starting_version + num_transactions_per_folder; // 1000
    
    // 4. Try to read the batch metadata - it won't exist
    let metadata = file_store_reader.get_batch_metadata(starting_version).await;
    assert!(metadata.is_none(), "Batch metadata should not exist for incomplete batch");
    
    // 5. But progress file claims we've processed up to version 1000
    assert_eq!(progress_version, 1000);
    
    // This demonstrates the corruption: progress says 1000 but metadata doesn't exist
}
```

## Notes

This vulnerability is particularly insidious because:
1. It silently corrupts the file store without immediate errors
2. The corruption is only discovered when readers attempt to access the affected version ranges
3. Recovery requires identifying the corrupted batches and re-running the backfiller
4. The progress file provides false confidence that the backfill completed successfully

The issue stems from an incorrect assumption that the gRPC streaming interface guarantees delivery of exactly the requested transaction count, when in reality the `transactions_count` field is merely a hint to the server about how many transactions the client desires.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L163-166)
```rust
                        let request = tonic::Request::new(GetTransactionsFromNodeRequest {
                            starting_version: Some(task_version),
                            transactions_count: Some(num_transactions_per_folder),
                        });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L207-220)
```rust
                    version += self.num_transactions_per_folder;
                }
            });

            // Update the progress file.
            let progress_file = ProgressFile {
                version,
                backfill_id: self.backfill_id,
            };
            let bytes =
                serde_json::to_vec(&progress_file).context("Failed to serialize progress file.")?;
            std::fs::write(&self.progress_file_path, &bytes)
                .context("Failed to write progress file.")?;
            info!("Progress file updated to version {}.", version,);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L245-256)
```rust
        if end_batch {
            let path = self
                .file_store_reader
                .get_path_for_batch_metadata(first_version);
            batch_metadata.suffix = Some(self.backfill_id);
            self.file_store_writer
                .save_raw_file(
                    path,
                    serde_json::to_vec(&batch_metadata).map_err(anyhow::Error::msg)?,
                )
                .await?;
        }
```

**File:** protos/proto/aptos/internal/fullnode/v1/fullnode_data.proto (L42-44)
```text
  // Optional; number of transactions to return in current stream.
  // If not set, response streams infinitely.
  optional uint64 transactions_count = 2 [jstype = JS_STRING];
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L301-301)
```rust
        let end_version = std::cmp::min(self.end_version, self.highest_known_version + 1);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L553-556)
```rust
            if let Some(abort_handle) = self.abort_handle.as_ref() {
                if abort_handle.load(Ordering::SeqCst) {
                    return false;
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L143-149)
```rust
                if results.is_empty() {
                    info!(
                        start_version = starting_version,
                        chain_id = ledger_chain_id,
                        "[Indexer Fullnode] Client disconnected."
                    );
                    break;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_operator.rs (L48-61)
```rust
        let end_batch = (transaction.version + 1) % self.num_txns_per_folder == 0;
        let size_bytes = transaction.encoded_len();
        ensure!(
            self.version == transaction.version,
            "Gap is found when buffering transaction, expected: {}, actual: {}",
            self.version,
            transaction.version,
        );
        self.buffer.push(transaction);
        self.buffer_size_in_bytes += size_bytes;
        self.version += 1;
        if self.buffer_size_in_bytes >= self.max_size_per_file || end_batch {
            self.dump_transactions_to_file(end_batch, tx).await?;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L89-95)
```rust
        let batch_metadata = self.get_batch_metadata(version).await;
        if batch_metadata.is_none() {
            // TODO(grao): This is unexpected, should only happen when data is corrupted. Consider
            // make it panic!.
            error!("Failed to get the batch metadata, unable to serve the request.");
            return;
        }
```
