# Audit Report

## Title
Silent Connection Drop on Channel Send Failure Causes Consensus Liveness Vulnerability

## Summary
When the `transport_notifs_tx.send()` operation fails in `send_connection_to_peer_manager()`, the error is logged but execution continues normally, causing successfully upgraded connections to be silently dropped. This creates an asymmetric connection state where the remote peer believes it is connected while the local node has no Peer actor to handle messages, leading to message loss and potential consensus liveness failures.

## Finding Description

The vulnerability exists in the `send_connection_to_peer_manager()` function where a successfully established and upgraded connection (after Noise handshake and protocol negotiation) is sent to PeerManager via a bounded channel: [1](#0-0) 

The channel `transport_notifs_tx` is created with a bounded size (default 1024): [2](#0-1) [3](#0-2) 

When this `send()` operation fails (due to channel full or receiver dropped), only an error is logged, but the function returns normally without propagating the error. This means:

1. The Connection object is dropped without being added to PeerManager's `active_peers` map
2. No Peer actor is spawned to handle messages over this connection
3. The remote peer successfully completed the handshake and believes the connection is established
4. Messages sent by the remote peer over this connection are lost

The PeerManager event loop processes these notifications to spawn Peer actors: [4](#0-3) [5](#0-4) 

If the notification never arrives, `add_peer()` is never called: [6](#0-5) 

**Attack Scenario:**

An attacker can cause this condition by:
1. Flooding the validator with connection requests to slow down PeerManager's event loop
2. Causing the `transport_notifs_rx` channel to fill up (bounded at 1024 messages)
3. New connections continue to be accepted and upgraded by TransportHandler
4. But their NewConnection notifications fail to send and are silently dropped
5. Honest validators attempting to connect have their connections silently lost
6. Consensus messages (votes, proposals, blocks) sent over these orphaned connections are lost
7. If enough validators are affected, consensus cannot reach quorum and stalls

## Impact Explanation

This vulnerability qualifies as **HIGH severity** according to the Aptos bug bounty criteria for the following reasons:

**Validator Node Slowdowns**: The channel backlog and connection processing delays directly slow down validator operations, fitting the High severity category.

**Consensus Liveness Failure**: When consensus validators have their connections silently dropped, critical consensus messages (votes, proposals, block commits) are lost. If enough validators are affected simultaneously, the network cannot reach the required 2/3+1 quorum for consensus progress, causing a liveness failure. This represents a "Significant protocol violation" under High severity.

**Silent Failure Mode**: The most dangerous aspect is that the failure is silent - no error is propagated to callers, no automatic recovery is triggered, and the asymmetric connection state (remote thinks connected, local doesn't) makes diagnosis difficult.

**Asymmetric Connection State**: The remote peer's transport layer believes the connection is established and will attempt to send messages over it, but these messages are never received because no Peer actor exists to process them.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

The vulnerability can be triggered under realistic conditions:

1. **Normal Network Load**: During high network activity (epoch changes, catch-up synchronization, state sync), the PeerManager can legitimately become slow, filling the channel
2. **Malicious Attack**: An attacker can deliberately flood connection requests to fill the channel
3. **Bounded Channel**: The channel size of 1024 is finite and can be filled
4. **No Backpressure**: TransportHandler continues accepting and upgrading connections even when PeerManager cannot keep up

The attack requires causing PeerManager slowdown, which can be achieved through:
- Connection flooding from multiple sources
- Sending messages that cause slow processing
- Exploiting other performance bottlenecks in message handling

## Recommendation

The `send_connection_to_peer_manager()` function must handle channel send failures by closing the connection and notifying the remote peer, rather than silently dropping it.

**Recommended Fix:**

```rust
async fn send_connection_to_peer_manager(
    &mut self,
    connection: Connection<TSocket>,
    addr: &NetworkAddress,
    elapsed_time: f64,
) {
    let metadata = connection.metadata.clone();
    debug!(
        NetworkSchema::new(&self.network_context)
            .connection_metadata_with_address(&metadata)
            .network_address(addr),
        "{} {} connection from {} at {} successfully upgraded after {:.3} secs",
        self.network_context,
        metadata.origin,
        metadata.remote_peer_id.short_str(),
        metadata.addr,
        elapsed_time,
    );

    counters::connection_upgrade_time(&self.network_context, metadata.origin, SUCCEEDED_LABEL)
        .observe(elapsed_time);

    // Send the new connection to PeerManager
    let event = TransportNotification::NewConnection(connection);
    if let Err(err) = self.transport_notifs_tx.send(event).await {
        error!(
            NetworkSchema::new(&self.network_context)
                .connection_metadata_with_address(&metadata),
            error = %err,
            "Failed to notify PeerManager of new connection - channel full or closed. Closing connection."
        );
        
        // CRITICAL FIX: Extract connection back from error and close it gracefully
        // The send error contains the unsent message with the connection inside
        if let TransportNotification::NewConnection(connection) = err.into_inner() {
            self.disconnect(connection);
        }
        
        // Update metrics to track this failure
        counters::connections_rejected(&self.network_context, metadata.origin).inc();
    }
}
```

**Additional Mitigations:**

1. **Increase Channel Size**: Consider making the channel unbounded or significantly larger for critical path notifications
2. **Monitoring**: Add metrics to track channel fullness and send failures
3. **Backpressure**: Implement backpressure from PeerManager to TransportHandler when the channel is near capacity
4. **Connection Limits**: Enforce stricter connection rate limits earlier in the pipeline

## Proof of Concept

The following Rust test demonstrates the vulnerability:

```rust
#[tokio::test]
async fn test_silent_connection_drop_on_channel_full() {
    use aptos_channels;
    use aptos_metrics_core::IntGauge;
    use futures::SinkExt;
    
    // Create a small bounded channel to simulate filling
    let gauge = IntGauge::new("TEST_COUNTER", "test").unwrap();
    let (mut tx, mut rx) = aptos_channels::new::<TransportNotification<TSocket>>(2, &gauge);
    
    // Fill the channel
    tx.send(TransportNotification::Disconnected(/* ... */)).await.unwrap();
    tx.send(TransportNotification::Disconnected(/* ... */)).await.unwrap();
    
    // Simulate TransportHandler trying to send NewConnection when channel is full
    // The send will block or fail, and if not handled properly, connection is lost
    let connection = create_test_connection(); // Helper to create test connection
    let event = TransportNotification::NewConnection(connection);
    
    // This send will fail because channel is full
    let result = tx.try_send(event);
    
    // Verify that connection is silently dropped without proper handling
    assert!(result.is_err());
    // The connection inside the error is lost and never gets a Peer actor
    
    // Remote peer believes connection is established, but local node has no Peer actor
    // Messages sent by remote peer will be lost
}
```

**To reproduce in a running network:**

1. Deploy a validator node with default configuration (channel_size=1024)
2. Send 1024+ rapid connection requests to fill the `transport_notifs_rx` channel
3. Establish a legitimate connection from another validator during this time
4. Observe that the connection is accepted and upgraded but silently dropped
5. Observe that consensus messages from the affected validator are lost
6. Monitor for consensus timeout and liveness failure if enough validators are affected

## Notes

**Additional Context:**

- The vulnerability affects both inbound and outbound connections, as both flow through the same `send_connection_to_peer_manager()` path
- The issue is exacerbated during network stress (epoch changes, catch-up sync) when PeerManager is legitimately slow
- The channel uses `futures::channel::mpsc::Sender` which returns `SendError` when the receiver is dropped or channel is full
- Similar patterns exist in the codebase but most handle errors appropriately by propagating them or taking corrective action
- The `Connection` struct contains the socket and metadata, and when dropped without proper cleanup, the TCP connection is closed abruptly from the local side while the remote peer has no indication of why

**Consensus Impact:**

Consensus messages flow through the network layer as follows:
1. Consensus component sends message via `NetworkClient::send_to_peer()`
2. Message is enqueued to PeerManager's request channel
3. PeerManager looks up peer in `active_peers` and forwards to Peer actor
4. Peer actor sends over the connection socket

If a connection is silently dropped, it's not in `active_peers`, so messages fail at step 3 with a "not connected" warning but no visible error to consensus, leading to silent message loss.

### Citations

**File:** network/framework/src/peer_manager/transport.rs (L354-363)
```rust
        // Send the new connection to PeerManager
        let event = TransportNotification::NewConnection(connection);
        if let Err(err) = self.transport_notifs_tx.send(event).await {
            error!(
                NetworkSchema::new(&self.network_context)
                    .connection_metadata_with_address(&metadata),
                error = %err,
                "Failed to notify PeerManager of new connection"
            );
        }
```

**File:** network/framework/src/peer_manager/mod.rs (L147-150)
```rust
        let (transport_notifs_tx, transport_notifs_rx) = aptos_channels::new(
            channel_size,
            &counters::PENDING_CONNECTION_HANDLER_NOTIFICATIONS,
        );
```

**File:** network/framework/src/peer_manager/mod.rs (L241-242)
```rust
                connection_event = self.transport_notifs_rx.select_next_some() => {
                    self.handle_connection_event(connection_event);
```

**File:** network/framework/src/peer_manager/mod.rs (L271-273)
```rust
        match event {
            TransportNotification::NewConnection(conn) => {
                self.handle_new_connection_event(conn);
```

**File:** network/framework/src/peer_manager/mod.rs (L607-695)
```rust
    fn add_peer(&mut self, connection: Connection<TSocket>) -> Result<(), Error> {
        let conn_meta = connection.metadata.clone();
        let peer_id = conn_meta.remote_peer_id;

        // Make a disconnect if you've connected to yourself
        if self.network_context.peer_id() == peer_id {
            debug_assert!(false, "Self dials shouldn't happen");
            warn!(
                NetworkSchema::new(&self.network_context)
                    .connection_metadata_with_address(&conn_meta),
                "Received self-dial, disconnecting it"
            );
            self.disconnect(connection);
            return Ok(());
        }

        let mut send_new_peer_notification = true;

        // Check for and handle simultaneous dialing
        if let Entry::Occupied(active_entry) = self.active_peers.entry(peer_id) {
            let (curr_conn_metadata, _) = active_entry.get();
            if Self::simultaneous_dial_tie_breaking(
                self.network_context.peer_id(),
                peer_id,
                curr_conn_metadata.origin,
                conn_meta.origin,
            ) {
                let (_, peer_handle) = active_entry.remove();
                // Drop the existing connection and replace it with the new connection
                drop(peer_handle);
                info!(
                    NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                    "{} Closing existing connection with Peer {} to mitigate simultaneous dial",
                    self.network_context,
                    peer_id.short_str()
                );
                send_new_peer_notification = false;
            } else {
                info!(
                    NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                    "{} Closing incoming connection with Peer {} to mitigate simultaneous dial",
                    self.network_context,
                    peer_id.short_str()
                );
                // Drop the new connection and keep the one already stored in active_peers
                self.disconnect(connection);
                return Ok(());
            }
        }

        // TODO: Add label for peer.
        let (peer_reqs_tx, peer_reqs_rx) = aptos_channel::new(
            QueueStyle::FIFO,
            self.channel_size,
            Some(&counters::PENDING_NETWORK_REQUESTS),
        );

        // Initialize a new Peer actor for this connection.
        let peer = Peer::new(
            self.network_context,
            self.executor.clone(),
            self.time_service.clone(),
            connection,
            self.transport_notifs_tx.clone(),
            peer_reqs_rx,
            self.upstream_handlers.clone(),
            Duration::from_millis(constants::INBOUND_RPC_TIMEOUT_MS),
            constants::MAX_CONCURRENT_INBOUND_RPCS,
            constants::MAX_CONCURRENT_OUTBOUND_RPCS,
            self.max_frame_size,
            self.max_message_size,
        );
        self.executor.spawn(peer.start());

        // Save PeerRequest sender to `active_peers`.
        self.active_peers
            .insert(peer_id, (conn_meta.clone(), peer_reqs_tx));
        self.peers_and_metadata.insert_connection_metadata(
            PeerNetworkId::new(self.network_context.network_id(), peer_id),
            conn_meta.clone(),
        )?;
        // Send NewPeer notification to connection event handlers.
        if send_new_peer_notification {
            let notif =
                ConnectionNotification::NewPeer(conn_meta, self.network_context.network_id());
            self.send_conn_notification(peer_id, notif);
        }

        Ok(())
```

**File:** config/src/config/network_config.rs (L37-37)
```rust
pub const NETWORK_CHANNEL_SIZE: usize = 1024;
```
