# Audit Report

## Title
KV-Only Replay Bypasses Critical State Checkpoint Hash Validation Leading to Consensus Divergence

## Summary
The `replay_kv()` function in the transaction restore system bypasses critical state consistency validations that ChunkExecutor normally performs. Specifically, it fails to validate that the `state_checkpoint_hash` in `TransactionInfo` matches the actual state root hash computed from applying write sets, allowing corrupted or manipulated backup data to cause consensus divergence and state inconsistencies.

## Finding Description

The vulnerability exists in the KV-only replay path which directly saves transactions and key-value data to the database without involving ChunkExecutor validation.

**Normal ChunkExecutor Path:** [1](#0-0) 

During normal transaction replay through ChunkExecutor, the `update_ledger()` method performs critical validation by calling `DoStateCheckpoint::run()` which validates state checkpoint hashes: [2](#0-1) 

This validation ensures that when `known_state_checkpoints` are provided (which they are during replay), the computed state root hash matches the `state_checkpoint_hash` stored in `TransactionInfo`: [3](#0-2) 

Additionally, ChunkExecutor verifies that transaction infos match through `verify_chunk_result()`: [4](#0-3) 

**KV-Only Replay Path:**

The vulnerable code path bypasses these validations entirely: [5](#0-4) 

The `replay_kv()` function directly calls `save_transactions_and_replay_kv()`: [6](#0-5) 

Which in turn calls `restore_utils::save_transactions()` with `kv_replay=true`: [7](#0-6) 

At line 269-277, `calculate_state_and_put_updates()` is called to apply write sets directly to the state tree, but this function does NOT validate that the resulting state root hash matches the `state_checkpoint_hash` in the `TransactionInfo` objects being saved.

**Attack Scenario:**

An attacker can provide manipulated backup data where:
1. The write sets don't produce the state root hash claimed in `TransactionInfo.state_checkpoint_hash`
2. The `TransactionInfo` objects contain incorrect state checkpoint hashes

When this data is restored using KV-only replay, the node will:
- Save the incorrect `TransactionInfo` with wrong `state_checkpoint_hash` to the database
- Apply the write sets to produce a different actual state root
- End up with a state root that doesn't match what's recorded in `TransactionInfo`

This breaks the critical invariant that `TransactionInfo.state_checkpoint_hash` accurately represents the state at that version, causing:
- **Consensus Divergence**: Nodes restored from corrupted backups will have different state roots than nodes that executed transactions normally or used full replay
- **State Inconsistency**: The Merkle tree root hash stored in the database won't match what's recorded in `TransactionInfo`
- **Proof Verification Failures**: State proofs generated using the actual state root won't verify against the `state_checkpoint_hash` in committed transactions

## Impact Explanation

This is a **Critical Severity** vulnerability meeting the Aptos bug bounty criteria for "Consensus/Safety violations" and "Non-recoverable network partition."

**Consensus Safety Violation:** Different nodes can end up with different state roots for the same version if they restore from different backup sources or use different restore methods. This breaks Aptos's fundamental safety guarantee that all validators produce identical state roots for identical blocks.

**Network Partition Risk:** If a significant portion of validators restore from corrupted backups using KV-only replay, they will diverge from the canonical chain state, requiring manual intervention or a hard fork to recover.

**State Integrity Compromise:** The `TransactionInfo` structure serves as a cryptographic commitment to the transaction's effects. If its `state_checkpoint_hash` doesn't match the actual state, the entire proof system is compromised, and validators cannot trust state proofs from peers.

## Likelihood Explanation

**Likelihood: High**

This vulnerability is highly likely to be exploited in the following scenarios:

1. **Disaster Recovery**: When validators restore from backups after data loss, they commonly use KV-only replay for performance reasons (it's much faster than full re-execution)

2. **New Node Bootstrapping**: New validators joining the network often restore from snapshots, and KV-only replay is the recommended fast path

3. **Backup Corruption**: Even unintentional corruption of backup data (storage failures, transmission errors) can trigger this issue

4. **Malicious Backup Provider**: An attacker who can compromise backup storage or provide malicious backup data to validators can deliberately exploit this to cause consensus divergence

The attack requires:
- Access to provide backup data (either by compromising backup storage or providing backups to new nodes)
- Knowledge of the KV-only replay feature
- Ability to craft consistent-looking but incorrect backup data

The impact is severe because it's difficult to detect until consensus failures occur.

## Recommendation

Add state checkpoint hash validation to the KV-only replay path. The fix should validate that applying the write sets produces state root hashes that match the `state_checkpoint_hash` values in the corresponding `TransactionInfo` objects.

**Recommended Fix:**

Modify `save_transactions_impl()` in `restore_utils.rs` to validate state checkpoint hashes when `kv_replay` is true:

```rust
pub(crate) fn save_transactions_impl(
    // ... existing parameters ...
    kv_replay: bool,
) -> Result<()> {
    // ... existing transaction saving logic ...
    
    if kv_replay && first_version > 0 && state_store.get_usage(Some(first_version - 1)).is_ok() {
        let (ledger_state, _hot_state_updates) = state_store.calculate_state_and_put_updates(
            &StateUpdateRefs::index_write_sets(first_version, write_sets, write_sets.len(), vec![]),
            &mut ledger_db_batch.ledger_metadata_db_batches,
            state_kv_batches,
        )?;
        
        // VALIDATION: Verify state checkpoint hashes match TransactionInfo
        for (idx, txn_info) in txn_infos.iter().enumerate() {
            if let Some(expected_hash) = txn_info.state_checkpoint_hash() {
                let version = first_version + idx as u64;
                // Check if this version should have a checkpoint
                if ledger_state.latest().next_version() == version + 1 && ledger_state.is_checkpoint() {
                    let actual_hash = ledger_state.last_checkpoint().usage().root_hash();
                    ensure!(
                        expected_hash == actual_hash,
                        "State checkpoint hash mismatch at version {}. Expected: {}, Actual: {}",
                        version, expected_hash, actual_hash
                    );
                }
            }
        }
        
        state_store.set_state_ignoring_summary(ledger_state);
    }
    // ... rest of function ...
}
```

Additionally, consider:
1. Adding a verification mode that re-computes transaction infos and compares them with stored values
2. Logging warnings when KV-only replay is used to increase operator awareness
3. Implementing integrity checks that periodically verify state checkpoint hashes match actual state roots

## Proof of Concept

The following test demonstrates the vulnerability by showing that KV-only replay accepts mismatched state checkpoint hashes:

```rust
#[test]
fn test_kv_replay_state_checkpoint_mismatch() {
    use aptos_crypto::HashValue;
    use aptos_types::transaction::{TransactionInfo, ExecutionStatus};
    
    // Setup: Create a legitimate transaction and its outputs
    let legitimate_txn = create_test_transaction();
    let legitimate_write_set = execute_transaction(&legitimate_txn);
    let legitimate_state_root = compute_state_root(&legitimate_write_set);
    
    // Attack: Create TransactionInfo with WRONG state checkpoint hash
    let malicious_state_checkpoint = HashValue::random(); // Wrong hash!
    let malicious_txn_info = TransactionInfo::new(
        legitimate_txn.hash(),
        legitimate_write_set.hash(),
        compute_event_root(&[]),
        Some(malicious_state_checkpoint), // This doesn't match actual state!
        100, // gas_used
        ExecutionStatus::Success,
        None,
    );
    
    // Execute KV-only replay with mismatched data
    let restore_handler = setup_restore_handler();
    
    // This should FAIL but currently SUCCEEDS
    let result = restore_handler.save_transactions_and_replay_kv(
        0, // first_version
        &[legitimate_txn],
        &[PersistedAuxiliaryInfo::None],
        &[malicious_txn_info], // Contains wrong state_checkpoint_hash
        &[vec![]],
        vec![legitimate_write_set],
    );
    
    // BUG: This passes when it should fail
    assert!(result.is_ok(), "KV replay accepted mismatched state checkpoint hash!");
    
    // Verify the mismatch
    let db = restore_handler.aptosdb;
    let saved_txn_info = db.get_transaction_info(0).unwrap();
    let actual_state_root = db.get_state_merkle_root(0).unwrap();
    
    // The saved TransactionInfo has wrong hash, but actual state is correct
    assert_ne!(
        saved_txn_info.state_checkpoint_hash().unwrap(),
        actual_state_root,
        "State checkpoint hash mismatch not detected!"
    );
}
```

This demonstrates that the KV-only replay path will accept and commit transactions where the `state_checkpoint_hash` in `TransactionInfo` doesn't match the actual state root produced by applying the write sets, violating a critical consensus invariant.

## Notes

The validation gap exists because KV-only replay was designed as a performance optimization for trusted backup data, assuming that backup integrity is guaranteed by the backup system itself. However, this assumption is insufficient for a security-critical blockchain system where state consistency must be cryptographically verified at every step.

The normal ChunkExecutor path correctly performs this validation through `DoStateCheckpoint::run()`, but the optimization to bypass ChunkExecutor for performance inadvertently removed essential security checks. The fix should maintain the performance benefits of KV-only replay while adding back the critical validation logic.

### Citations

**File:** execution/executor/src/chunk_executor/mod.rs (L336-392)
```rust
    pub fn update_ledger(&self) -> Result<()> {
        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["chunk_update_ledger_total"]);

        let (parent_state_summary, parent_accumulator, chunk) =
            self.commit_queue.lock().next_chunk_to_update_ledger()?;
        let ChunkToUpdateLedger {
            output,
            chunk_verifier,
        } = chunk;

        let state_checkpoint_output = DoStateCheckpoint::run(
            &output.execution_output,
            &parent_state_summary,
            &ProvableStateSummary::new_persisted(self.db.reader.as_ref())?,
            Some(
                chunk_verifier
                    .transaction_infos()
                    .iter()
                    .map(|t| t.state_checkpoint_hash())
                    .collect_vec(),
            ),
        )?;

        let ledger_update_output = DoLedgerUpdate::run(
            &output.execution_output,
            &state_checkpoint_output,
            parent_accumulator.clone(),
        )?;

        chunk_verifier.verify_chunk_result(&parent_accumulator, &ledger_update_output)?;

        let ledger_info_opt = chunk_verifier.maybe_select_chunk_ending_ledger_info(
            &ledger_update_output,
            output.execution_output.next_epoch_state.as_ref(),
        )?;
        output.set_state_checkpoint_output(state_checkpoint_output);
        output.set_ledger_update_output(ledger_update_output);

        let first_version = output.execution_output.first_version;
        let num_txns = output.execution_output.num_transactions_to_commit();
        let executed_chunk = ExecutedChunk {
            output,
            ledger_info_opt,
        };

        self.commit_queue
            .lock()
            .save_ledger_update_output(executed_chunk)?;

        info!(
            LogSchema::new(LogEntry::ChunkExecutor)
                .first_version_in_request(Some(first_version))
                .num_txns_in_request(num_txns),
            "Calculated ledger update!",
        );
        Ok(())
    }
```

**File:** execution/executor/src/workflow/do_state_checkpoint.rs (L44-88)
```rust
    fn get_state_checkpoint_hashes(
        execution_output: &ExecutionOutput,
        known_state_checkpoints: Option<Vec<Option<HashValue>>>,
        state_summary: &LedgerStateSummary,
    ) -> Result<Vec<Option<HashValue>>> {
        let _timer = OTHER_TIMERS.timer_with(&["get_state_checkpoint_hashes"]);

        let num_txns = execution_output.to_commit.len();
        let last_checkpoint_index = execution_output
            .to_commit
            .state_update_refs()
            .last_inner_checkpoint_index();

        if let Some(known) = known_state_checkpoints {
            ensure!(
                known.len() == num_txns,
                "Bad number of known hashes. {} vs {}",
                known.len(),
                num_txns
            );
            if let Some(idx) = last_checkpoint_index {
                ensure!(
                    known[idx] == Some(state_summary.last_checkpoint().root_hash()),
                    "Root hash mismatch with known hashes passed in. {:?} vs {:?}",
                    known[idx],
                    Some(&state_summary.last_checkpoint().root_hash()),
                );
            }

            Ok(known)
        } else {
            if !execution_output.is_block {
                // We should enter this branch only in test.
                execution_output.to_commit.ensure_at_most_one_checkpoint()?;
            }

            let mut out = vec![None; num_txns];

            if let Some(index) = last_checkpoint_index {
                out[index] = Some(state_summary.last_checkpoint().root_hash());
            }

            Ok(out)
        }
    }
```

**File:** types/src/transaction/mod.rs (L2044-2047)
```rust
    /// The root hash of the Sparse Merkle Tree describing the world state at the end of this
    /// transaction. Depending on the protocol configuration, this can be generated periodical
    /// only, like per block.
    state_checkpoint_hash: Option<HashValue>,
```

**File:** execution/executor/src/chunk_executor/chunk_result_verifier.rs (L133-140)
```rust
impl ChunkResultVerifier for ReplayChunkVerifier {
    fn verify_chunk_result(
        &self,
        _parent_accumulator: &InMemoryTransactionAccumulator,
        ledger_update_output: &LedgerUpdateOutput,
    ) -> Result<()> {
        ledger_update_output.ensure_transaction_infos_match(&self.transaction_infos)
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L554-637)
```rust
    async fn replay_kv(
        &self,
        restore_handler: &RestoreHandler,
        txns_to_execute_stream: impl Stream<
            Item = Result<(
                Transaction,
                PersistedAuxiliaryInfo,
                TransactionInfo,
                WriteSet,
                Vec<ContractEvent>,
            )>,
        >,
    ) -> Result<()> {
        let (first_version, _) = self.replay_from_version.unwrap();
        restore_handler.force_state_version_for_kv_restore(first_version.checked_sub(1))?;

        let mut base_version = first_version;
        let mut offset = 0u64;
        let replay_start = Instant::now();
        let arc_restore_handler = Arc::new(restore_handler.clone());

        let db_commit_stream = txns_to_execute_stream
            .try_chunks(BATCH_SIZE)
            .err_into::<anyhow::Error>()
            .map_ok(|chunk| {
                let (txns, persisted_aux_info, txn_infos, write_sets, events): (
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                ) = chunk.into_iter().multiunzip();
                let handler = arc_restore_handler.clone();
                base_version += offset;
                offset = txns.len() as u64;
                async move {
                    let _timer = OTHER_TIMERS_SECONDS.timer_with(&["replay_txn_chunk_kv_only"]);
                    tokio::task::spawn_blocking(move || {
                        // we directly save transaction and kvs to DB without involving chunk executor
                        handler.save_transactions_and_replay_kv(
                            base_version,
                            &txns,
                            &persisted_aux_info,
                            &txn_infos,
                            &events,
                            write_sets,
                        )?;
                        // return the last version after the replaying
                        Ok(base_version + offset - 1)
                    })
                    .err_into::<anyhow::Error>()
                    .await
                }
            })
            .try_buffered_x(self.global_opt.concurrent_downloads, 1)
            .and_then(future::ready);

        let total_replayed = db_commit_stream
            .and_then(|version| async move {
                let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_txn_chunk_kv_only"]);
                tokio::task::spawn_blocking(move || {
                    // version is the latest version finishing the KV replaying
                    let total_replayed = version - first_version;
                    TRANSACTION_REPLAY_VERSION.set(version as i64);
                    info!(
                        version = version,
                        accumulative_tps =
                            (total_replayed as f64 / replay_start.elapsed().as_secs_f64()) as u64,
                        "KV replayed."
                    );
                    Ok(version)
                })
                .await?
            })
            .try_fold(0, |_total, total| future::ok(total))
            .await?;
        info!(
            total_replayed = total_replayed,
            accumulative_tps =
                (total_replayed as f64 / replay_start.elapsed().as_secs_f64()) as u64,
            "KV Replay finished."
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/backup/restore_handler.rs (L105-126)
```rust
    pub fn save_transactions_and_replay_kv(
        &self,
        first_version: Version,
        txns: &[Transaction],
        persisted_aux_info: &[PersistedAuxiliaryInfo],
        txn_infos: &[TransactionInfo],
        events: &[Vec<ContractEvent>],
        write_sets: Vec<WriteSet>,
    ) -> Result<()> {
        restore_utils::save_transactions(
            self.state_store.clone(),
            self.ledger_db.clone(),
            first_version,
            txns,
            persisted_aux_info,
            txn_infos,
            events,
            write_sets,
            None,
            true,
        )
    }
```

**File:** storage/aptosdb/src/backup/restore_utils.rs (L115-176)
```rust
pub(crate) fn save_transactions(
    state_store: Arc<StateStore>,
    ledger_db: Arc<LedgerDb>,
    first_version: Version,
    txns: &[Transaction],
    persisted_aux_info: &[PersistedAuxiliaryInfo],
    txn_infos: &[TransactionInfo],
    events: &[Vec<ContractEvent>],
    write_sets: Vec<WriteSet>,
    existing_batch: Option<(
        &mut LedgerDbSchemaBatches,
        &mut ShardedStateKvSchemaBatch,
        &mut SchemaBatch,
    )>,
    kv_replay: bool,
) -> Result<()> {
    if let Some((ledger_db_batch, state_kv_batches, _state_kv_metadata_batch)) = existing_batch {
        save_transactions_impl(
            state_store,
            ledger_db,
            first_version,
            txns,
            persisted_aux_info,
            txn_infos,
            events,
            write_sets.as_ref(),
            ledger_db_batch,
            state_kv_batches,
            kv_replay,
        )?;
    } else {
        let mut ledger_db_batch = LedgerDbSchemaBatches::new();
        let mut sharded_kv_schema_batch = state_store
            .state_db
            .state_kv_db
            .new_sharded_native_batches();
        save_transactions_impl(
            Arc::clone(&state_store),
            Arc::clone(&ledger_db),
            first_version,
            txns,
            persisted_aux_info,
            txn_infos,
            events,
            write_sets.as_ref(),
            &mut ledger_db_batch,
            &mut sharded_kv_schema_batch,
            kv_replay,
        )?;
        // get the last version and commit to the state kv db
        // commit the state kv before ledger in case of failure happens
        let last_version = first_version + txns.len() as u64 - 1;
        state_store
            .state_db
            .state_kv_db
            .commit(last_version, None, sharded_kv_schema_batch)?;

        ledger_db.write_schemas(ledger_db_batch)?;
    }

    Ok(())
}
```
