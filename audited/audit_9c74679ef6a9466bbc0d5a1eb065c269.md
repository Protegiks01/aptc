# Audit Report

## Title
Resource Exhaustion via Event-Inclusive Optimistic Fetch Requests Without Chunk Size Adjustment

## Summary
The `get_storage_request_for_missing_data()` function in optimistic fetch does not adjust chunk sizes based on the `include_events` flag, allowing malicious peers to request disproportionately large data volumes. On Mainnet nodes using the legacy implementation, this causes excessive storage I/O and CPU consumption as the serving peer repeatedly fetches and discards oversized responses.

## Finding Description

The vulnerability exists in how optimistic fetch requests calculate chunk sizes without accounting for event data costs: [1](#0-0) 

The `max_chunk_size_for_request()` function returns fixed chunk sizes based only on request type, completely ignoring whether events are included: [2](#0-1) 

At lines 114 and 123, the `include_events` flag is passed through without any chunk size reduction: [3](#0-2) [4](#0-3) 

**Attack Execution Path:**

1. Malicious peer sends optimistic fetch request with `include_events: true` (e.g., `GetNewTransactionsWithProof`)
2. Code calculates chunk size as 3000 transactions (default `max_transaction_chunk_size`)
3. Request is created for 3000 transactions with events
4. On Mainnet (using legacy implementation), serving peer calls: [5](#0-4) 

5. Legacy implementation fetches ALL requested transactions from storage first: [6](#0-5) 

The legacy implementation performs wasteful binary search retries. With events included, 3000 transactions could easily exceed 100 MiB, but the limit is only 10 MiB: [7](#0-6) [8](#0-7) 

**Mainnet Vulnerability:** The efficient implementation is intentionally disabled on Mainnet: [9](#0-8) [10](#0-9) 

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria:

1. **Resource Exhaustion**: Serving peers experience excessive storage I/O (potentially 10-100x waste reading data that exceeds limits), CPU overhead (serializing oversized responses), and memory pressure (holding responses before discard)

2. **Node Performance Degradation**: Continuous exploitation causes validator/fullnode slowdowns affecting state sync performance network-wide

3. **Amplification Factor**: Single malicious peer can send multiple concurrent optimistic fetch requests; multiple coordinated peers can amplify the attack

4. **Mainnet Impact**: The most critical network (Mainnet) uses the vulnerable legacy implementation by default, affecting production validators and fullnodes

The impact breaks the **Resource Limits** invariant (Invariant #9: "All operations must respect gas, storage, and computational limits") by allowing resource consumption disproportionate to legitimate use.

## Likelihood Explanation

**High Likelihood** of exploitation:

- **Attack Complexity**: Low - requires only sending valid storage service requests with `include_events: true`
- **Attacker Requirements**: Any network peer can exploit this; no special privileges needed
- **Detection Difficulty**: Requests appear valid; request moderator only blocks invalid requests, not resource-intensive valid ones
- **Deployment Scope**: Affects all Mainnet validators and fullnodes by default
- **Real-World Feasibility**: Events can vary significantly in size (user-emitted events, contract events); blockchain transactions with large events (e.g., NFT minting, large data structures) make this practical

## Recommendation

**Immediate Fix:** Adjust chunk size calculation to account for `include_events` flag:

```rust
fn max_chunk_size_for_request(&self, config: StorageServiceConfig) -> u64 {
    let base_chunk_size = match &self.request.data_request {
        DataRequest::GetNewTransactionOutputsWithProof(_) => {
            config.max_transaction_output_chunk_size
        },
        DataRequest::GetNewTransactionsWithProof(request) => {
            if request.include_events {
                // Reduce chunk size when events are included
                config.max_transaction_chunk_size / 4  // Conservative reduction
            } else {
                config.max_transaction_chunk_size
            }
        },
        DataRequest::GetNewTransactionsOrOutputsWithProof(request) => {
            if request.include_events {
                config.max_transaction_output_chunk_size / 4
            } else {
                config.max_transaction_output_chunk_size
            }
        },
        // ... rest of match arms
    };
    base_chunk_size
}
```

**Long-Term Fix:** Enable size-and-time-aware chunking on Mainnet: [11](#0-10) 

The new implementation incrementally builds responses and properly accounts for event sizes: [12](#0-11) 

## Proof of Concept

```rust
// Reproduction steps:
// 1. Deploy Aptos node with default Mainnet config (legacy implementation enabled)
// 2. Connect malicious peer to target node
// 3. Send continuous stream of optimistic fetch requests:

use aptos_storage_service_types::requests::{
    DataRequest, NewTransactionsWithProofRequest, StorageServiceRequest
};

// Create malicious request requesting max chunk with events
let request = StorageServiceRequest::new(
    DataRequest::GetNewTransactionsWithProof(
        NewTransactionsWithProofRequest {
            known_version: current_version,
            known_epoch: current_epoch,
            include_events: true,  // Force event inclusion
        }
    ),
    false,  // No compression to maximize processing cost
);

// Send multiple concurrent requests to amplify resource consumption
for _ in 0..10 {
    send_storage_request(target_peer, request.clone());
}

// Monitor target node:
// - Storage I/O spikes (reading 100s of MBs per request)
// - CPU usage increases (serialization overhead)
// - Memory pressure (holding large responses)
// - State sync latency degrades
// - Binary search retries visible in logs showing repeated halving
```

**Validation**: Monitor target node storage I/O metrics and observe 10-100x overhead compared to equivalent requests without `include_events` flag, with repeated fetch-and-discard cycles visible in the binary search pattern.

## Notes

This vulnerability specifically affects Mainnet production deployments due to the configuration optimizer intentionally keeping the legacy implementation enabled for stability. Non-Mainnet networks (testnet, devnet) use the efficient implementation and are less vulnerable. The issue demonstrates a classic case where chunk size limits fail to account for variable-cost data (events), allowing resource amplification attacks.

### Citations

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L84-88)
```rust
        // Bound the number of versions to fetch by the maximum chunk size
        num_versions_to_fetch = min(
            num_versions_to_fetch,
            self.max_chunk_size_for_request(config),
        );
```

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L109-116)
```rust
            DataRequest::GetNewTransactionsWithProof(request) => {
                DataRequest::GetTransactionsWithProof(TransactionsWithProofRequest {
                    proof_version: target_version,
                    start_version,
                    end_version,
                    include_events: request.include_events,
                })
            },
```

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L117-127)
```rust
            DataRequest::GetNewTransactionsOrOutputsWithProof(request) => {
                DataRequest::GetTransactionsOrOutputsWithProof(
                    TransactionsOrOutputsWithProofRequest {
                        proof_version: target_version,
                        start_version,
                        end_version,
                        include_events: request.include_events,
                        max_num_output_reductions: request.max_num_output_reductions,
                    },
                )
            },
```

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L168-192)
```rust
    fn max_chunk_size_for_request(&self, config: StorageServiceConfig) -> u64 {
        match &self.request.data_request {
            DataRequest::GetNewTransactionOutputsWithProof(_) => {
                config.max_transaction_output_chunk_size
            },
            DataRequest::GetNewTransactionsWithProof(_) => config.max_transaction_chunk_size,
            DataRequest::GetNewTransactionsOrOutputsWithProof(_) => {
                config.max_transaction_output_chunk_size
            },
            DataRequest::GetNewTransactionDataWithProof(request) => {
                match request.transaction_data_request_type {
                    TransactionDataRequestType::TransactionData(_) => {
                        config.max_transaction_chunk_size
                    },
                    TransactionDataRequestType::TransactionOutputData => {
                        config.max_transaction_output_chunk_size
                    },
                    TransactionDataRequestType::TransactionOrOutputData(_) => {
                        config.max_transaction_output_chunk_size
                    },
                }
            },
            request => unreachable!("Unexpected optimistic fetch request: {:?}", request),
        }
    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L347-371)
```rust
    fn get_transactions_with_proof_by_size(
        &self,
        proof_version: u64,
        start_version: u64,
        end_version: u64,
        include_events: bool,
        max_response_size: u64,
        use_size_and_time_aware_chunking: bool,
    ) -> Result<TransactionDataWithProofResponse, Error> {
        // Calculate the number of transactions to fetch
        let expected_num_transactions = inclusive_range_len(start_version, end_version)?;
        let max_num_transactions = self.config.max_transaction_chunk_size;
        let num_transactions_to_fetch = min(expected_num_transactions, max_num_transactions);

        // If size and time-aware chunking are disabled, use the legacy implementation
        if !use_size_and_time_aware_chunking {
            return self.get_transactions_with_proof_by_size_legacy(
                proof_version,
                start_version,
                end_version,
                num_transactions_to_fetch,
                include_events,
                max_response_size,
            );
        }
```

**File:** state-sync/storage-service/server/src/storage.rs (L420-449)
```rust
                Some((Ok(transaction), Ok(info), Ok(events), Ok(persisted_auxiliary_info))) => {
                    // Calculate the number of serialized bytes for the data items
                    let num_transaction_bytes = get_num_serialized_bytes(&transaction)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_info_bytes = get_num_serialized_bytes(&info)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_events_bytes = get_num_serialized_bytes(&events)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
                    let num_auxiliary_info_bytes =
                        get_num_serialized_bytes(&persisted_auxiliary_info).map_err(|error| {
                            Error::UnexpectedErrorEncountered(error.to_string())
                        })?;

                    // Add the data items to the lists
                    let total_serialized_bytes = num_transaction_bytes
                        + num_info_bytes
                        + num_events_bytes
                        + num_auxiliary_info_bytes;
                    if response_progress_tracker
                        .data_items_fits_in_response(true, total_serialized_bytes)
                    {
                        transactions.push(transaction);
                        transaction_infos.push(info);
                        transaction_events.push(events);
                        persisted_auxiliary_infos.push(persisted_auxiliary_info);

                        response_progress_tracker.add_data_item(total_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L515-563)
```rust
    fn get_transactions_with_proof_by_size_legacy(
        &self,
        proof_version: u64,
        start_version: u64,
        end_version: u64,
        mut num_transactions_to_fetch: u64,
        include_events: bool,
        max_response_size: u64,
    ) -> Result<TransactionDataWithProofResponse, Error> {
        while num_transactions_to_fetch >= 1 {
            let transaction_list_with_proof = self.storage.get_transactions(
                start_version,
                num_transactions_to_fetch,
                proof_version,
                include_events,
            )?;
            let response = TransactionDataWithProofResponse {
                transaction_data_response_type: TransactionDataResponseType::TransactionData,
                transaction_list_with_proof: Some(transaction_list_with_proof),
                transaction_output_list_with_proof: None,
            };
            if num_transactions_to_fetch == 1 {
                return Ok(response); // We cannot return less than a single item
            }

            // Attempt to divide up the request if it overflows the message size
            let (overflow_frame, num_bytes) =
                check_overflow_network_frame(&response, max_response_size)?;
            if !overflow_frame {
                return Ok(response);
            } else {
                metrics::increment_chunk_truncation_counter(
                    metrics::TRUNCATION_FOR_SIZE,
                    DataResponse::TransactionDataWithProof(response).get_label(),
                );
                let new_num_transactions_to_fetch = num_transactions_to_fetch / 2;
                debug!("The request for {:?} transactions was too large (num bytes: {:?}, limit: {:?}). Retrying with {:?}.",
                    num_transactions_to_fetch, num_bytes, max_response_size, new_num_transactions_to_fetch);
                num_transactions_to_fetch = new_num_transactions_to_fetch; // Try again with half the amount of data
            }
        }

        Err(Error::UnexpectedErrorEncountered(format!(
            "Unable to serve the get_transactions_with_proof request! Proof version: {:?}, \
            start version: {:?}, end version: {:?}, include events: {:?}. The data cannot fit into \
            a single network frame!",
            proof_version, start_version, end_version, include_events,
        )))
    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L1075-1090)
```rust
    fn get_transactions_with_proof(
        &self,
        proof_version: u64,
        start_version: u64,
        end_version: u64,
        include_events: bool,
    ) -> aptos_storage_service_types::Result<TransactionDataWithProofResponse, Error> {
        self.get_transactions_with_proof_by_size(
            proof_version,
            start_version,
            end_version,
            include_events,
            self.config.max_network_chunk_bytes,
            self.config.enable_size_and_time_aware_chunking,
        )
    }
```

**File:** config/src/config/state_sync_config.rs (L16-21)
```rust
// The maximum message size per state sync message
const SERVER_MAX_MESSAGE_SIZE: usize = 10 * 1024 * 1024; // 10 MiB

// The maximum message size per state sync message (for v2 data requests)
const CLIENT_MAX_MESSAGE_SIZE_V2: usize = 20 * 1024 * 1024; // 20 MiB (used for v2 data requests)
const SERVER_MAX_MESSAGE_SIZE_V2: usize = 40 * 1024 * 1024; // 40 MiB (used for v2 data requests)
```

**File:** config/src/config/state_sync_config.rs (L198-198)
```rust
            enable_size_and_time_aware_chunking: false,
```

**File:** config/src/config/state_sync_config.rs (L204-205)
```rust
            max_network_chunk_bytes: SERVER_MAX_MESSAGE_SIZE as u64,
            max_network_chunk_bytes_v2: SERVER_MAX_MESSAGE_SIZE_V2 as u64,
```

**File:** config/src/config/state_sync_config.rs (L620-630)
```rust
        // Potentially enable size and time-aware chunking for all networks except Mainnet
        let mut modified_config = false;
        if let Some(chain_id) = chain_id {
            if ENABLE_SIZE_AND_TIME_AWARE_CHUNKING
                && !chain_id.is_mainnet()
                && local_storage_config_yaml["enable_size_and_time_aware_chunking"].is_null()
            {
                storage_service_config.enable_size_and_time_aware_chunking = true;
                modified_config = true;
            }
        }
```
