# Audit Report

## Title
DKG Transcript Request Flooding via Missing Sender-Based Deduplication

## Summary
The DKG (Distributed Key Generation) manager lacks request deduplication based on sender identity, allowing a single Byzantine validator to flood honest validators with identical `DKGTranscriptRequest` messages. Each duplicate request forces the honest validator to clone and serialize their DKG transcript (approximately 14-15 KB for a 100-validator network), wasting CPU cycles and memory bandwidth without any rate limiting or deduplication mechanism.

## Finding Description

The vulnerability exists in the `process_peer_rpc_msg()` function of the DKG manager. When an honest validator receives a `DKGTranscriptRequest` RPC message, the handler performs minimal validation before responding: [1](#0-0) 

The function only checks if the message epoch matches the current epoch, then immediately responds with a cloned copy of the validator's transcript regardless of whether the same sender has already requested it: [2](#0-1) 

There is **no deduplication** mechanism to track which senders have already requested the transcript, and **no rate limiting** to restrict the number of requests from a single sender.

The `DKGTranscript` structure contains a `transcript_bytes` field that stores the BCS-serialized PVSS transcript: [3](#0-2) 

For a typical validator set, the transcript size is substantial. Based on the unweighted DAS protocol formula, the expected size is `G2_PROJ_NUM_BYTES + (num_validators + 1) * (G2_PROJ_NUM_BYTES + G1_PROJ_NUM_BYTES)`: [4](#0-3) 

For a 100-validator network, this equates to approximately 96 + (101 × 144) ≈ 14.6 KB per transcript.

**Attack Scenario:**

1. A Byzantine validator sends N identical `DKGTranscriptRequest` messages to a target honest validator (e.g., N = 1000)
2. Each request passes through the network layer and application channels:
   - Network layer enforces only per-connection concurrent RPC limits [5](#0-4) [6](#0-5) 
   
   - Application layer uses bounded channels but with sufficient capacity [7](#0-6) [8](#0-7) 

3. The honest validator processes each duplicate request, performing:
   - Memory allocation and copying for `my_transcript.clone()` (~14.6 KB × N)
   - BCS serialization of the response
   - Network transmission

4. For N = 1000 requests, this results in:
   - ~14.6 MB of memory allocations and copies
   - 1000 serialization operations
   - Significant CPU cycles wasted on duplicate work

While the `TranscriptAggregationState` implements deduplication for **responses** collected by the initiator, this deduplication is based on the transcript author, not the request sender: [9](#0-8) 

This does not protect honest validators from processing duplicate **requests** from malicious senders.

## Impact Explanation

This vulnerability allows resource exhaustion attacks against honest validators during the DKG phase, which is critical for on-chain randomness generation. The impact includes:

1. **CPU and Memory Waste**: Each duplicate request forces expensive memory operations (allocation, copying, serialization) that scale with validator set size
2. **Validator Performance Degradation**: During an active attack, honest validators may experience reduced responsiveness, potentially affecting their consensus participation
3. **DKG Protocol Disruption**: If validators are overwhelmed during DKG, the protocol's completion time may be extended or validators may fail to aggregate transcripts in time

According to the Aptos bug bounty severity categories, this qualifies as **Medium severity** because it causes limited resource exhaustion without directly compromising funds, consensus safety, or causing complete node failures. While "validator node slowdowns" is listed under High severity, the bounded nature of the channel capacities and network limits prevents this from reaching the threshold of significant, sustained slowdowns that would merit High classification.

## Likelihood Explanation

**Likelihood: Medium to High**

The attack has the following characteristics:

**Requirements:**
- Attacker must be a validator in the current epoch (has authenticated network access)
- DKG must be in progress (occurs at epoch transitions when randomness is enabled)
- No collusion required—a single Byzantine validator can execute the attack

**Feasibility:**
- The attack is trivial to execute—simply send repeated RPC requests using the DKG network protocol
- No cryptographic breaking or complex state manipulation required
- The attacker can control the rate and volume of requests subject only to network-layer limits

**Frequency:**
- DKG runs once per epoch transition when on-chain randomness is enabled
- Each DKG session lasts for a bounded time window
- An attacker could execute this attack during every DKG session

The primary limiting factors are the network layer's per-connection concurrent RPC limits and the application channel capacities, but these still allow for significant request amplification.

## Recommendation

Implement sender-based request deduplication in the `DKGManager::process_peer_rpc_msg()` function. Add a cache to track which peers have already requested the transcript in the current epoch:

```rust
// Add to DKGManager struct:
transcript_request_cache: HashSet<AccountAddress>,

// Modify process_peer_rpc_msg():
async fn process_peer_rpc_msg(&mut self, req: IncomingRpcRequest) -> Result<()> {
    let IncomingRpcRequest {
        msg,
        sender,  // Add sender field extraction
        mut response_sender,
        ..
    } = req;
    
    ensure!(
        msg.epoch() == self.epoch_state.epoch,
        "[DKG] msg not for current epoch"
    );
    
    let response = match (&self.state, &msg) {
        (InnerState::Finished { my_transcript, .. }, DKGMessage::TranscriptRequest(_))
        | (InnerState::InProgress { my_transcript, .. }, DKGMessage::TranscriptRequest(_)) => {
            // Check if this sender has already requested
            if self.transcript_request_cache.contains(&sender) {
                warn!("[DKG] Duplicate transcript request from {}, ignoring", sender);
                return Ok(());
            }
            
            // Mark sender as having requested
            self.transcript_request_cache.insert(sender);
            
            Ok(DKGMessage::TranscriptResponse(my_transcript.clone()))
        },
        _ => Err(anyhow!(
            "[DKG] msg {:?} unexpected in state {:?}",
            msg.name(),
            self.state.variant_name()
        )),
    };

    response_sender.send(response);
    Ok(())
}
```

Additionally, clear the cache when transitioning epochs or when DKG completes.

## Proof of Concept

```rust
// Pseudo-code for reproduction test
#[tokio::test]
async fn test_dkg_request_flooding() {
    // Setup: Create DKGManager for honest validator
    let (dkg_manager, rpc_tx) = setup_dkg_manager().await;
    
    // Setup: Byzantine validator identity
    let byzantine_validator = AccountAddress::random();
    
    // Attack: Send 1000 identical DKGTranscriptRequest messages
    let request = DKGTranscriptRequest::new(current_epoch);
    let mut response_count = 0;
    
    for _ in 0..1000 {
        let (response_tx, response_rx) = oneshot::channel();
        let incoming_req = IncomingRpcRequest {
            msg: DKGMessage::TranscriptRequest(request.clone()),
            sender: byzantine_validator,
            response_sender: Box::new(TestResponseSender::new(response_tx)),
        };
        
        // Send request to DKG manager
        rpc_tx.push(byzantine_validator, (byzantine_validator, incoming_req)).unwrap();
        
        // Count successful responses
        if response_rx.await.is_ok() {
            response_count += 1;
        }
    }
    
    // Verify: All 1000 duplicate requests were processed
    assert_eq!(response_count, 1000, 
        "DKG manager should have processed all duplicate requests without deduplication");
    
    // Measure: CPU time and memory allocations demonstrate waste
    // In a real PoC, instrument the clone operations to measure resource consumption
}
```

The PoC demonstrates that without deduplication, the DKG manager processes all 1000 duplicate requests from the same Byzantine sender, each triggering a transcript clone and serialization operation.

## Notes

While the network layer implements per-connection concurrent RPC limits and the application uses bounded channels, these mechanisms do not prevent request flooding from a single sender. The concurrent limit only restricts how many RPCs can be in-flight simultaneously per connection, not the total throughput over time. Once an RPC completes, the slot becomes available for another request from the same sender.

The actual performance impact depends on factors like validator hardware, network conditions, and the size of the validator set (which determines transcript size). However, the vulnerability represents a clear violation of resource efficiency invariants and provides an attack vector for Byzantine validators to degrade honest validator performance during the critical DKG phase.

### Citations

**File:** dkg/src/dkg_manager/mod.rs (L454-478)
```rust
    async fn process_peer_rpc_msg(&mut self, req: IncomingRpcRequest) -> Result<()> {
        let IncomingRpcRequest {
            msg,
            mut response_sender,
            ..
        } = req;
        ensure!(
            msg.epoch() == self.epoch_state.epoch,
            "[DKG] msg not for current epoch"
        );
        let response = match (&self.state, &msg) {
            (InnerState::Finished { my_transcript, .. }, DKGMessage::TranscriptRequest(_))
            | (InnerState::InProgress { my_transcript, .. }, DKGMessage::TranscriptRequest(_)) => {
                Ok(DKGMessage::TranscriptResponse(my_transcript.clone()))
            },
            _ => Err(anyhow!(
                "[DKG] msg {:?} unexpected in state {:?}",
                msg.name(),
                self.state.variant_name()
            )),
        };

        response_sender.send(response);
        Ok(())
    }
```

**File:** types/src/dkg/mod.rs (L49-54)
```rust
#[derive(Clone, Serialize, Deserialize, PartialEq, Eq)]
pub struct DKGTranscript {
    pub metadata: DKGTranscriptMetadata,
    #[serde(with = "serde_bytes")]
    pub transcript_bytes: Vec<u8>,
}
```

**File:** crates/aptos-dkg/tests/pvss.rs (L405-414)
```rust
fn expected_transcript_size<T: Transcript<SecretSharingConfig = ThresholdConfigBlstrs>>(
    sc: &ThresholdConfigBlstrs,
) -> usize {
    if T::scheme_name() == unweighted_protocol::DAS_SK_IN_G1 {
        G2_PROJ_NUM_BYTES
            + (sc.get_total_num_players() + 1) * (G2_PROJ_NUM_BYTES + G1_PROJ_NUM_BYTES)
    } else {
        panic!("Did not implement support for '{}' yet", T::scheme_name())
    }
}
```

**File:** network/framework/src/protocols/rpc/mod.rs (L183-183)
```rust
    max_concurrent_inbound_rpcs: u32,
```

**File:** network/framework/src/protocols/rpc/mod.rs (L212-223)
```rust
        // Drop new inbound requests if our completion queue is at capacity.
        if self.inbound_rpc_tasks.len() as u32 == self.max_concurrent_inbound_rpcs {
            // Increase counter of declined requests
            counters::rpc_messages(
                network_context,
                REQUEST_LABEL,
                INBOUND_LABEL,
                DECLINED_LABEL,
            )
            .inc();
            return Err(RpcError::TooManyPending(self.max_concurrent_inbound_rpcs));
        }
```

**File:** dkg/src/network.rs (L141-141)
```rust
        let (rpc_tx, rpc_rx) = aptos_channel::new(QueueStyle::FIFO, 10, None);
```

**File:** dkg/src/epoch_manager.rs (L227-230)
```rust
            let (dkg_rpc_msg_tx, dkg_rpc_msg_rx) = aptos_channel::new::<
                AccountAddress,
                (AccountAddress, IncomingRpcRequest),
            >(QueueStyle::FIFO, 100, None);
```

**File:** dkg/src/transcript_aggregation/mod.rs (L92-94)
```rust
        if trx_aggregator.contributors.contains(&metadata.author) {
            return Ok(None);
        }
```
