# Audit Report

## Title
Permanent Storage Corruption from Partial Writes During pre_commit_ledger Failure

## Summary
If `pre_commit_ledger` fails after some parallel database writes succeed but before `commit_ledger` is called, partial writes remain permanently in storage without any cleanup mechanism. This causes irreversible database corruption requiring manual intervention and full resync.

## Finding Description

The `pre_commit_block()` function calls `db.writer.pre_commit_ledger()` which spawns 7 parallel tasks writing to different databases: [1](#0-0) 

Inside `pre_commit_ledger`, the `calculate_and_commit_ledger_and_state_kv` function spawns parallel writes using a thread pool: [2](#0-1) 

**Critical Issue**: Each spawned task uses `.unwrap()` which panics on errors. The tasks write to:
- Events DB
- WriteSet DB  
- Transaction DB
- Auxiliary Info DB
- State KV DB + LedgerCommitProgress
- Transaction Info DB
- Transaction Accumulator DB

These writes are **NOT atomic across databases**. Each database write is individually atomic via RocksDB WriteBatch, but there's no distributed transaction. The TODO comment acknowledges this: [3](#0-2) 

**Failure Scenario**:
1. Task A (commit_events) completes successfully → Events DB written
2. Task B (commit_write_sets) completes successfully → WriteSet DB written
3. Task C (commit_transactions) fails and panics → Transaction DB not written
4. Rayon scope propagates the panic after all tasks finish
5. `pre_commit_block` returns error to consensus
6. Consensus never calls `commit_ledger`, so `OverallCommitProgress` is never written

**Recovery Failure**: On node restart, `StateStore::sync_commit_progress()` is called: [4](#0-3) 

The recovery logic reads `OverallCommitProgress`: [5](#0-4) 

If `OverallCommitProgress` is `None` (because `commit_ledger` was never called), the entire recovery block is skipped: [6](#0-5) 

**Result**: Partial writes from the failed `pre_commit_ledger` remain permanently in storage. Events DB and WriteSet DB have data for transactions that don't exist in Transaction Info DB or Transaction Accumulator DB.

This violates **State Consistency Invariant #4**: "State transitions must be atomic and verifiable via Merkle proofs."

## Impact Explanation

**Critical Severity** - This qualifies for the highest bounty tier ($1,000,000) because:

1. **Permanent Database Corruption**: The inconsistent state cannot self-heal. Reads will fail when attempting to fetch transaction info for versions that exist in Events DB but not in Transaction Info DB.

2. **Consensus Safety Violation**: Different nodes experiencing failures at different points can end up with different partial write states, violating deterministic execution and potentially causing chain splits.

3. **Non-Recoverable Without Manual Intervention**: Requires database wipe and full resync from genesis or snapshot, meeting the "requires hardfork" criteria for network partition.

4. **Breaks Core Invariant**: Violates the atomic state transition guarantee that is fundamental to blockchain correctness.

## Likelihood Explanation

**High Likelihood** of occurrence:

1. **Common Trigger Conditions**:
   - Disk full during write operations
   - I/O errors from hardware issues
   - Process crashes (OOM, SIGKILL)
   - Resource exhaustion in write thread pool
   - Any transient storage backend failure

2. **No Safeguards**: The code explicitly uses `.unwrap()` acknowledging that error handling is insufficient, as noted in the TODO comments.

3. **Production Exposure**: This affects all validator and fullnode deployments. Any node experiencing transient storage issues during block execution is vulnerable.

4. **Persistent State**: Once corruption occurs, it's permanent. The node cannot recover without manual database deletion.

## Recommendation

**Immediate Fix**: Implement proper rollback or make recovery handle `None` case:

**Option 1 - Enhanced Recovery** (Recommended):
Modify `sync_commit_progress` to handle the `None` case by truncating ALL databases to their minimum consistent version when `OverallCommitProgress` is not found but some databases have data:

```rust
pub fn sync_commit_progress(
    ledger_db: Arc<LedgerDb>,
    state_kv_db: Arc<StateKvDb>,
    state_merkle_db: Arc<StateMerkleDb>,
    crash_if_difference_is_too_large: bool,
) {
    let ledger_metadata_db = ledger_db.metadata_db();
    
    // Find the minimum safe version across all databases
    let overall_commit_progress = ledger_metadata_db
        .get_synced_version()
        .expect("DB read failed.");
    
    if overall_commit_progress.is_none() {
        // Check if any database has uncommitted data
        let ledger_commit_progress = ledger_metadata_db
            .get_ledger_commit_progress()
            .ok();
        
        if let Some(ledger_progress) = ledger_commit_progress {
            warn!(
                "Found uncommitted ledger data without OverallCommitProgress. \
                 Truncating all databases to ensure consistency."
            );
            // Truncate all databases to empty state (version 0)
            truncate_ledger_db(ledger_db.clone(), 0)
                .expect("Failed to truncate ledger db.");
            truncate_state_kv_db(&state_kv_db, ledger_progress, 0, 
                                ledger_progress as usize)
                .expect("Failed to truncate state K/V db.");
        } else {
            info!("No overall commit progress was found - clean database.");
        }
        return;
    }
    
    // ... rest of existing code
}
```

**Option 2 - Atomic Writes**:
Replace parallel writes with a single atomic transaction using RocksDB's multi-CF write batch. This requires architectural changes but provides true atomicity.

**Option 3 - Two-Phase Commit**:
Write all data to temporary locations first, then atomically rename/move them into production tables only after all writes succeed.

## Proof of Concept

```rust
// Rust test demonstrating the vulnerability
#[test]
fn test_partial_write_corruption() {
    use fail::FailScenario;
    
    let tmp_dir = TempDir::new().unwrap();
    let db = AptosDB::new_for_test(&tmp_dir);
    
    // Create a block with transactions
    let block = create_test_block(10); // 10 transactions
    
    // Set up fail point to cause one parallel task to fail
    let scenario = FailScenario::setup();
    fail::cfg("rocksdb::write_batch::commit_transaction_infos", "panic").unwrap();
    
    // Execute block - this will partially succeed
    let result = db.writer.pre_commit_ledger(block.as_chunk_to_commit(), false);
    assert!(result.is_err(), "pre_commit_ledger should fail");
    
    // Simulate node restart by dropping and reopening DB
    drop(db);
    let db_reopened = AptosDB::new_for_test(&tmp_dir);
    
    // Check database consistency
    let events_max = get_max_version_in_events_db(&db_reopened);
    let txn_info_max = get_max_version_in_txn_info_db(&db_reopened);
    
    // VULNERABILITY: Events exist but transaction infos don't
    assert_ne!(events_max, txn_info_max, 
              "Database is corrupted: inconsistent versions across tables");
    
    // Attempting to read will fail
    let result = db_reopened.get_transaction_by_version(5, true);
    assert!(result.is_err(), "Reading corrupted data should fail");
    
    scenario.teardown();
}
```

**Notes**

The vulnerability is acknowledged in the codebase via TODO comments but remains unfixed. The recovery mechanism `sync_commit_progress` only handles cases where `OverallCommitProgress` exists but is behind `LedgerCommitProgress`. It does not handle the case where `OverallCommitProgress` is `None` but partial writes exist, leaving the database in a permanently corrupted state that requires manual intervention to resolve.

### Citations

**File:** execution/executor/src/block_executor/mod.rs (L336-360)
```rust
    fn pre_commit_block(&self, block_id: HashValue) -> ExecutorResult<()> {
        let _timer = COMMIT_BLOCKS.start_timer();
        info!(
            LogSchema::new(LogEntry::BlockExecutor).block_id(block_id),
            "pre_commit_block",
        );

        let block = self.block_tree.get_block(block_id)?;

        fail_point!("executor::pre_commit_block", |_| {
            Err(anyhow::anyhow!("Injected error in pre_commit_block.").into())
        });

        let output = block.output.expect_complete_result();
        let num_txns = output.num_transactions_to_commit();
        if num_txns != 0 {
            let _timer = SAVE_TRANSACTIONS.start_timer();
            self.db
                .writer
                .pre_commit_ledger(output.as_chunk_to_commit(), false)?;
            TRANSACTIONS_SAVED.observe(num_txns as f64);
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L263-322)
```rust
    fn calculate_and_commit_ledger_and_state_kv(
        &self,
        chunk: &ChunkToCommit,
        skip_index_and_usage: bool,
    ) -> Result<HashValue> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions__work"]);

        let mut new_root_hash = HashValue::zero();
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });

        Ok(new_root_hash)
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```
