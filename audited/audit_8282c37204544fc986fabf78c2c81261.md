# Audit Report

## Title
Unverified Storage Summary Acceptance Enables Malicious Peer Manipulation of State Synchronization

## Summary
Malicious peers can provide falsified `StorageServerSummary` responses containing unverified `LedgerInfoWithSignatures` data, which are logged and used for synchronization decisions without cryptographic validation. This allows attackers to manipulate peer selection, pollute network metrics, and degrade validator synchronization performance.

## Finding Description

The state synchronization system accepts `StorageServerSummary` responses from peers without verifying the cryptographic signatures on the embedded `LedgerInfoWithSignatures` data. This breaks the fundamental security guarantee that peer-provided data must be validated before use.

**Vulnerable Code Path:**

1. The poller requests storage summaries from peers: [1](#0-0) 

2. Received summaries are logged immediately without validation: [2](#0-1) 

3. The summary is stored directly without signature verification: [3](#0-2) 

4. These unverified summaries are aggregated into the global data summary: [4](#0-3) 

5. The `highest_synced_ledger_info()` uses the unverified data: [5](#0-4) 

**The Attack:**
A malicious peer crafts a `StorageServerSummary` with a `LedgerInfoWithSignatures` claiming an artificially high version (e.g., version 999,999,999) with invalid or forged signatures. Since signature verification only occurs when actual data is fetched (not when summaries are received), this false information:

- Gets logged as `LogEntry::StorageSummaryResponse` 
- Pollutes the global data summary
- Influences peer selection for data requests
- Affects the calculation of network-wide highest synced version
- Wastes validator resources through timeouts and retries

**Signature Verification Location (too late):**
Verification only happens during actual data processing: [6](#0-5) 

This verification occurs AFTER peer selection and initial synchronization decisions have already been made based on the unverified summary.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program criteria:

**1. Validator Node Slowdowns:**
- Multiple malicious peers advertising false high versions cause validators to preferentially select them
- Each request to a malicious peer results in timeout or proof verification failure
- Retry mechanisms with exponential backoff further delay synchronization
- During network partitions or epoch transitions, this can significantly impact validator availability

**2. Significant Protocol Violations:**
- Violates the implicit security invariant that peer-provided cryptographic data must be verified before use
- The `LedgerInfoWithSignatures` struct contains BLS signatures from 2f+1 validators that should be verified
- Current implementation trusts peer-provided summaries without checking the quorum signatures

**3. Metric and Monitoring Pollution:**
- Incorrect `highest_advertised_version` metrics mislead operators
- Logs show false peer capabilities
- Makes detecting actual network issues more difficult

## Likelihood Explanation

**Likelihood: HIGH**

**Attacker Requirements:**
- Ability to run a network peer (no validator privileges required)
- Knowledge of the `StorageServerSummary` structure
- Can craft responses with arbitrary `LedgerInfoWithSignatures` data

**Attack Complexity: LOW**
- No cryptographic operations required (no need to forge valid signatures)
- Single malicious peer gets ignored after ~5-10 failed requests (score drops below 25)
- BUT an attacker can spawn multiple peers to maintain persistent attack
- Each new peer starts with score 50, requiring multiple failures before being ignored

**Practical Exploitation:**
In a network with honest peers at version 1,000 and 10 malicious peers advertising version 1,000,000:
- The global summary shows highest version as 1,000,000
- Validators waste resources attempting to fetch from malicious peers
- Synchronization delays proportional to number of malicious peers and timeout values

## Recommendation

**Immediate Fix: Validate LedgerInfo Signatures on Receipt**

Add cryptographic verification when storing peer summaries:

```rust
// In peer_states.rs, update_summary method
pub fn update_summary(&self, peer: PeerNetworkId, storage_summary: StorageServerSummary) {
    // Validate the synced_ledger_info signatures before storing
    if let Some(ref ledger_info) = storage_summary.data_summary.synced_ledger_info {
        // Get the current epoch state from local storage
        if let Ok(epoch_state) = self.get_current_epoch_state() {
            // Verify signatures match the epoch and have valid quorum
            if let Err(e) = epoch_state.verify(ledger_info) {
                warn!("Rejecting storage summary from {:?}: invalid signatures: {:?}", peer, e);
                // Immediately penalize the peer for providing invalid data
                self.update_score_error(peer, ErrorType::Malicious);
                return;
            }
        }
    }
    
    // Only store if validation passed
    self.peer_to_state
        .entry(peer)
        .or_insert(PeerState::new(self.data_client_config.clone()))
        .update_storage_summary(storage_summary);
}
```

**Additional Hardening:**
1. Rate-limit storage summary updates per peer
2. Add metrics for rejected summaries due to invalid signatures  
3. Implement cross-peer consistency checks (reject summaries claiming versions far beyond network consensus)

## Proof of Concept

```rust
// Test demonstrating the vulnerability
#[tokio::test]
async fn test_malicious_storage_summary_accepted() {
    use aptos_types::ledger_info::{LedgerInfo, LedgerInfoWithSignatures};
    use aptos_storage_service_types::responses::{StorageServerSummary, DataSummary};
    
    // Create test environment
    let (data_client, peer_id) = create_test_data_client();
    
    // Craft malicious storage summary with impossibly high version
    let malicious_ledger_info = LedgerInfo::new(
        BlockInfo::new(999_999_999, 0, HashValue::zero(), HashValue::zero(), 0, 0, None),
        HashValue::zero()
    );
    
    // Create LedgerInfoWithSignatures with INVALID/EMPTY signatures
    let malicious_ledger_info_with_sigs = LedgerInfoWithSignatures::new(
        malicious_ledger_info,
        AggregateSignature::empty() // Invalid - no actual validator signatures!
    );
    
    let malicious_summary = StorageServerSummary {
        protocol_metadata: ProtocolMetadata::default(),
        data_summary: DataSummary {
            synced_ledger_info: Some(malicious_ledger_info_with_sigs),
            ..Default::default()
        }
    };
    
    // VULNERABILITY: This is accepted without signature verification!
    data_client.update_peer_storage_summary(peer_id, malicious_summary);
    
    // The false data now pollutes the global summary
    let global_summary = data_client.get_global_data_summary();
    
    // EXPLOIT: The highest synced version is now the fake 999,999,999
    assert_eq!(
        global_summary.advertised_data.highest_synced_ledger_info()
            .unwrap().ledger_info().version(),
        999_999_999  // FALSE DATA ACCEPTED!
    );
    
    // Validators will now preferentially select this malicious peer
    // causing timeouts and synchronization delays
}
```

## Notes

The vulnerability exists because the codebase separates summary advertisement from data delivery. While data delivery verifies proofs, summary advertisement does not. This creates a window where unverified data influences critical synchronization decisions. The peer scoring system eventually mitigates persistent malicious peers, but not before performance degradation occurs.

### Citations

**File:** state-sync/aptos-data-client/src/poller.rs (L404-434)
```rust
    let poller = async move {
        // Construct the request for polling
        let data_request = DataRequest::GetStorageServerSummary;
        let use_compression = data_summary_poller.data_client_config.use_compression;
        let storage_request = StorageServiceRequest::new(data_request, use_compression);

        // Fetch the storage summary for the peer and stop the timer
        let request_timeout = data_summary_poller.data_client_config.response_timeout_ms;
        let result: crate::error::Result<StorageServerSummary> = data_summary_poller
            .data_client
            .send_request_to_peer_and_decode(peer, storage_request, request_timeout)
            .await
            .map(Response::into_payload);

        // Mark the in-flight poll as now complete
        data_summary_poller.in_flight_request_complete(&peer);

        // Check the storage summary response
        let storage_summary = match result {
            Ok(storage_summary) => storage_summary,
            Err(error) => {
                warn!(
                    (LogSchema::new(LogEntry::StorageSummaryResponse)
                        .event(LogEvent::PeerPollingError)
                        .message("Error encountered when polling peer!")
                        .error(&error)
                        .peer(&peer))
                );
                return;
            },
        };
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L324-330)
```rust
    /// Updates the storage summary for the given peer
    pub fn update_summary(&self, peer: PeerNetworkId, storage_summary: StorageServerSummary) {
        self.peer_to_state
            .entry(peer)
            .or_insert(PeerState::new(self.data_client_config.clone()))
            .update_storage_summary(storage_summary);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L338-408)
```rust
    /// Calculates a global data summary using all known storage summaries
    pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
        // Gather all storage summaries, but exclude peers that are ignored
        let storage_summaries: Vec<StorageServerSummary> = self
            .peer_to_state
            .iter()
            .filter_map(|peer_state| {
                peer_state
                    .value()
                    .get_storage_summary_if_not_ignored()
                    .cloned()
            })
            .collect();

        // If we have no peers, return an empty global summary
        if storage_summaries.is_empty() {
            return GlobalDataSummary::empty();
        }

        // Calculate the global data summary using the advertised peer data
        let mut advertised_data = AdvertisedData::empty();
        let mut max_epoch_chunk_sizes = vec![];
        let mut max_state_chunk_sizes = vec![];
        let mut max_transaction_chunk_sizes = vec![];
        let mut max_transaction_output_chunk_sizes = vec![];
        for summary in storage_summaries {
            // Collect aggregate data advertisements
            if let Some(epoch_ending_ledger_infos) = summary.data_summary.epoch_ending_ledger_infos
            {
                advertised_data
                    .epoch_ending_ledger_infos
                    .push(epoch_ending_ledger_infos);
            }
            if let Some(states) = summary.data_summary.states {
                advertised_data.states.push(states);
            }
            if let Some(synced_ledger_info) = summary.data_summary.synced_ledger_info.as_ref() {
                advertised_data
                    .synced_ledger_infos
                    .push(synced_ledger_info.clone());
            }
            if let Some(transactions) = summary.data_summary.transactions {
                advertised_data.transactions.push(transactions);
            }
            if let Some(transaction_outputs) = summary.data_summary.transaction_outputs {
                advertised_data
                    .transaction_outputs
                    .push(transaction_outputs);
            }

            // Collect preferred max chunk sizes
            max_epoch_chunk_sizes.push(summary.protocol_metadata.max_epoch_chunk_size);
            max_state_chunk_sizes.push(summary.protocol_metadata.max_state_chunk_size);
            max_transaction_chunk_sizes.push(summary.protocol_metadata.max_transaction_chunk_size);
            max_transaction_output_chunk_sizes
                .push(summary.protocol_metadata.max_transaction_output_chunk_size);
        }

        // Calculate optimal chunk sizes based on the advertised data
        let optimal_chunk_sizes = calculate_optimal_chunk_sizes(
            &self.data_client_config,
            max_epoch_chunk_sizes,
            max_state_chunk_sizes,
            max_transaction_chunk_sizes,
            max_transaction_output_chunk_sizes,
        );
        GlobalDataSummary {
            advertised_data,
            optimal_chunk_sizes,
        }
    }
```

**File:** state-sync/aptos-data-client/src/global_summary.rs (L184-198)
```rust
    pub fn highest_synced_ledger_info(&self) -> Option<LedgerInfoWithSignatures> {
        let highest_synced_position = self
            .synced_ledger_infos
            .iter()
            .map(|ledger_info_with_sigs| ledger_info_with_sigs.ledger_info().version())
            .position_max();

        if let Some(highest_synced_position) = highest_synced_position {
            self.synced_ledger_infos
                .get(highest_synced_position)
                .cloned()
        } else {
            None
        }
    }
```

**File:** types/src/epoch_state.rs (L40-50)
```rust
impl Verifier for EpochState {
    fn verify(&self, ledger_info: &LedgerInfoWithSignatures) -> anyhow::Result<()> {
        ensure!(
            self.epoch == ledger_info.ledger_info().epoch(),
            "LedgerInfo has unexpected epoch {}, expected {}",
            ledger_info.ledger_info().epoch(),
            self.epoch
        );
        ledger_info.verify_signatures(&self.verifier)?;
        Ok(())
    }
```
