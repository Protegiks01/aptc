# Audit Report

## Title
State Snapshot Backup Validation Bypass Allows Empty Chunks with Arbitrary Root Hash

## Summary
A state snapshot backup with an empty chunks array can pass validation and be restored, even when the declared `root_hash` in the manifest doesn't match the actual resulting state tree hash (`SPARSE_MERKLE_PLACEHOLDER_HASH`). This occurs because `StateSnapshotBackup` lacks the chunk validation present in other backup types, and `JellyfishMerkleRestore::finish_impl()` doesn't verify that the expected root hash matches the placeholder hash when no chunks are added.

## Finding Description
Unlike `TransactionBackup` and `EpochEndingBackup`, which have `verify()` methods that explicitly check `!self.chunks.is_empty()`, the `StateSnapshotBackup` struct has no such validation method. [1](#0-0)  In contrast, `TransactionBackup` includes this validation [2](#0-1)  and `EpochEndingBackup` also enforces it [3](#0-2) . These verify methods are called during restoration [4](#0-3) [5](#0-4) , but no such call exists for `StateSnapshotBackup`.

When a `StateSnapshotBackup` with an empty chunks array is restored, the process flows as follows:

1. The manifest is loaded without any chunk validation [6](#0-5) 

2. The proof is verified against the ledger info, confirming that `manifest.root_hash` matches the proof's claimed state root hash [7](#0-6) 

3. The restore receiver is initialized with `manifest.root_hash` as the expected root hash [8](#0-7) 

4. Since the chunks array is empty, the chunk processing loop doesn't execute any iterations [9](#0-8) 

5. `finish()` is called, which invokes `JellyfishMerkleRestore::finish_impl()` [10](#0-9) [11](#0-10) 

6. In `finish_impl()`, when `num_children == 0` (no chunks were added), it unconditionally writes `Node::Null` to storage without validating that the `expected_root_hash` equals `SPARSE_MERKLE_PLACEHOLDER_HASH` [12](#0-11) 

7. Since `Node::Null.hash()` returns `SPARSE_MERKLE_PLACEHOLDER_HASH` [13](#0-12) , the database now contains a state tree with root hash `SPARSE_MERKLE_PLACEHOLDER_HASH`, even though the manifest (and proof) claimed a different `root_hash`.

**Broken Invariant:** This violates the **State Consistency** invariant - the database contains a state tree whose actual root hash doesn't match the declared root hash for that version, creating a fundamental integrity violation.

## Impact Explanation
This is a **Medium Severity** vulnerability per the Aptos bug bounty criteria, as it causes "State inconsistencies requiring intervention."

Concrete impacts:
- **Database Corruption**: Node operators restoring from such a backup will have an incorrect state tree for the specified version
- **Consensus Risk**: If multiple validators restore from the same malformed backup, they would all have incorrect state, but the mismatch would be detected when attempting to execute transactions against this state
- **Recovery Required**: Manual intervention would be needed to detect and fix the corrupted state tree
- **Trust Compromise**: Malicious actors could distribute "valid" backups that pass all automated checks but contain incorrect state

The vulnerability doesn't directly cause fund loss or network partition, but it enables state manipulation that could cascade into more severe issues depending on how the corrupted state is used.

## Likelihood Explanation
**Likelihood: Medium**

Prerequisites for exploitation:
1. Attacker must provide a state snapshot backup manifest (either by hosting a malicious backup service or compromising a legitimate backup)
2. A valid `TransactionInfoWithProof` and `LedgerInfoWithSignatures` that verifies correctly but claims an arbitrary (non-empty-tree) root hash for a given version
3. An empty chunks array
4. A victim node operator who restores from this backup

The attack is straightforward to execute - no complex cryptographic operations or race conditions are required. The main barrier is that the attacker must either run a malicious backup service that node operators trust, or compromise an existing backup storage system. However, once a malformed backup is created, any node operator who restores from it will be affected, making the impact widespread.

## Recommendation
Add a `verify()` method to `StateSnapshotBackup` that checks for empty chunks, similar to the existing validation in `TransactionBackup` and `EpochEndingBackup`. Additionally, in `JellyfishMerkleRestore::finish_impl()`, when `num_children == 0`, validate that `expected_root_hash` equals `SPARSE_MERKLE_PLACEHOLDER_HASH` before writing `Node::Null` to storage.

## Proof of Concept
A malicious actor can create a backup manifest with:
1. A valid `TransactionInfoWithProof` and `LedgerInfoWithSignatures` (obtained from a legitimate backup for a version with non-empty state)
2. The correct `version`, `epoch`, and `root_hash` fields matching the proof
3. An empty `chunks: []` array

When a node operator restores from this backup, the validation passes due to the valid cryptographic proofs, but the resulting database will have `SPARSE_MERKLE_PLACEHOLDER_HASH` as the state root instead of the claimed `root_hash`, causing state corruption that requires manual intervention to detect and repair.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L31-51)
```rust
pub struct StateSnapshotBackup {
    /// Version at which this state snapshot is taken.
    pub version: Version,
    /// Epoch in which this state snapshot is taken.
    pub epoch: u64,
    /// Hash of the state tree root.
    pub root_hash: HashValue,
    /// All account blobs in chunks.
    pub chunks: Vec<StateSnapshotChunk>,
    /// BCS serialized
    /// `Tuple(TransactionInfoWithProof, LedgerInfoWithSignatures)`.
    ///   - The `TransactionInfoWithProof` is at `Version` above, and carries the same `root_hash`
    /// above; It proves that at specified version the root hash is as specified in a chain
    /// represented by the LedgerInfo below.
    ///   - The signatures on the `LedgerInfoWithSignatures` has a version greater than or equal to
    /// the version of this backup but is within the same epoch, so the signatures on it can be
    /// verified by the validator set in the same epoch, which can be provided by an
    /// `EpochStateBackup` recovered prior to this to the DB; Requiring it to be in the same epoch
    /// limits the requirement on such `EpochStateBackup` to no older than the same epoch.
    pub proof: FileHandle,
}
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/manifest.rs (L50-88)
```rust
    pub fn verify(&self) -> Result<()> {
        // check number of waypoints
        ensure!(
            self.first_version <= self.last_version,
            "Bad version range: [{}, {}]",
            self.first_version,
            self.last_version,
        );

        // check chunk ranges
        ensure!(!self.chunks.is_empty(), "No chunks.");

        let mut next_version = self.first_version;
        for chunk in &self.chunks {
            ensure!(
                chunk.first_version == next_version,
                "Chunk ranges not continuous. Expected first version: {}, actual: {}.",
                next_version,
                chunk.first_version,
            );
            ensure!(
                chunk.last_version >= chunk.first_version,
                "Chunk range invalid. [{}, {}]",
                chunk.first_version,
                chunk.last_version,
            );
            next_version = chunk.last_version + 1;
        }

        // check last version in chunk matches manifest
        ensure!(
            next_version - 1 == self.last_version, // okay to -1 because chunks is not empty.
            "Last version in chunks: {}, in manifest: {}",
            next_version - 1,
            self.last_version,
        );

        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/backup_types/epoch_ending/manifest.rs (L29-69)
```rust
    pub fn verify(&self) -> Result<()> {
        // check number of waypoints
        ensure!(
            self.first_epoch <= self.last_epoch
                && self.last_epoch - self.first_epoch + 1 == self.waypoints.len() as u64,
            "Malformed manifest. first epoch: {}, last epoch {}, num waypoints {}",
            self.first_epoch,
            self.last_epoch,
            self.waypoints.len(),
        );

        // check chunk ranges
        ensure!(!self.chunks.is_empty(), "No chunks.");
        let mut next_epoch = self.first_epoch;
        for chunk in &self.chunks {
            ensure!(
                chunk.first_epoch == next_epoch,
                "Chunk ranges not continuous. Expected first epoch: {}, actual: {}.",
                next_epoch,
                chunk.first_epoch,
            );
            ensure!(
                chunk.last_epoch >= chunk.first_epoch,
                "Chunk range invalid. [{}, {}]",
                chunk.first_epoch,
                chunk.last_epoch,
            );
            next_epoch = chunk.last_epoch + 1;
        }

        // check last epoch in chunk matches manifest
        ensure!(
            next_epoch - 1 == self.last_epoch, // okay to -1 because chunks is not empty.
            "Last epoch in chunks: {}, in manifest: {}",
            next_epoch - 1,
            self.last_epoch,
        );

        Ok(())
    }
}
```

**File:** storage/backup/backup-cli/src/backup_types/epoch_ending/restore.rs (L80-95)
```rust
    async fn preheat_impl(&self) -> Result<EpochEndingRestorePreheatData> {
        let manifest: EpochEndingBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        manifest.verify()?;

        let mut next_epoch = manifest.first_epoch;
        let mut waypoint_iter = manifest.waypoints.iter();

        let mut previous_li: Option<&LedgerInfoWithSignatures> = None;
        let mut ledger_infos = Vec::new();

        let mut past_target = false;
        for chunk in &manifest.chunks {
            if past_target {
                break;
            }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L345-360)
```rust

        let storage = self.storage.clone();
        let manifest_stream = manifest_handle_stream
            .map(move |hdl| {
                let storage = storage.clone();
                async move { storage.load_json_file(&hdl).await.err_notes(&hdl) }
            })
            .buffered_x(con * 3, con)
            .and_then(|m: TransactionBackup| future::ready(m.verify().map(|_| m)));

        let target_version = self.global_opt.target_version;
        let first_version = self.first_version.unwrap_or(0);
        let chunk_manifest_stream = manifest_stream
            .map_ok(|m| stream::iter(m.chunks.into_iter().map(Result::<_>::Ok)))
            .try_flatten()
            .try_filter(move |c| {
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-124)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L125-136)
```rust
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L141-145)
```rust
        let receiver = Arc::new(Mutex::new(Some(self.run_mode.get_state_restore_receiver(
            self.version,
            manifest.root_hash,
            self.restore_mode,
        )?)));
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L187-226)
```rust
        let futs_iter = chunks.into_iter().enumerate().map(|(chunk_idx, chunk)| {
            let storage = storage.clone();
            async move {
                tokio::spawn(async move {
                    let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
                    Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
                })
                .await?
            }
        });
        let con = self.concurrent_downloads;
        let mut futs_stream = stream::iter(futs_iter).buffered_x(con * 2, con);
        let mut start = None;
        while let Some((chunk_idx, chunk, mut blobs, proof)) = futs_stream.try_next().await? {
            start = start.or_else(|| Some(Instant::now()));
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["add_state_chunk"]);
            let receiver = receiver.clone();
            if self.validate_modules {
                blobs = tokio::task::spawn_blocking(move || {
                    Self::validate_modules(&blobs);
                    blobs
                })
                .await?;
            }
            tokio::task::spawn_blocking(move || {
                receiver.lock().as_mut().unwrap().add_chunk(blobs, proof)
            })
            .await??;
            leaf_idx.set(chunk.last_idx as i64);
            info!(
                chunk = chunk_idx,
                chunks_to_add = chunks_to_add,
                last_idx = chunk.last_idx,
                values_per_second = ((chunk.last_idx + 1 - start_idx) as f64
                    / start.as_ref().unwrap().elapsed().as_secs_f64())
                    as u64,
                "State chunk added.",
            );
        }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L228-228)
```rust
        tokio::task::spawn_blocking(move || receiver.lock().take().unwrap().finish()).await??;
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L260-273)
```rust
    fn finish(self) -> Result<()> {
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => self.kv_restore.lock().take().unwrap().finish()?,
            StateSnapshotRestoreMode::TreeOnly => {
                self.tree_restore.lock().take().unwrap().finish_impl()?
            },
            StateSnapshotRestoreMode::Default => {
                // for tree only mode, we also need to write the usage to DB
                self.kv_restore.lock().take().unwrap().finish()?;
                self.tree_restore.lock().take().unwrap().finish_impl()?
            },
        }
        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/node_type/mod.rs (L850-856)
```rust
    pub fn hash(&self) -> HashValue {
        match self {
            Node::Internal(internal_node) => internal_node.hash(),
            Node::Leaf(leaf_node) => leaf_node.hash(),
            Node::Null => *SPARSE_MERKLE_PLACEHOLDER_HASH,
        }
    }
```
