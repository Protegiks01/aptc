# Audit Report

## Title
Permanent Cache-File Store Desynchronization Leading to Data Loss in Indexer File Store Processor

## Summary
The indexer file store processor updates Redis cache before updating GCS file store metadata without atomic transaction semantics. If metadata update fails indefinitely while cache update succeeds, the system enters an unrecoverable state where cache eviction destroys data needed for recovery, causing permanent data loss and service failure.

## Finding Description

The vulnerability exists in the file store processor's batch processing loop where two critical state updates occur non-atomically: [1](#0-0) 

**Step 1: Cache Updated First**
The processor updates Redis cache's `file_store_latest_version` to the new batch version. This update uses the `?` operator, meaning it either succeeds completely or the function returns early. [2](#0-1) 

**Step 2: Metadata Update Enters Infinite Retry**
The processor then attempts to update the GCS metadata file. This update is wrapped in an infinite retry loop with NO timeout, maximum retry count, or circuit breaker. [3](#0-2) 

**Critical Flaw: Cache Worker Uses Stale Signal**
The cache worker relies on `file_store_latest_version` to throttle itself, checking if the file store is keeping pace: [4](#0-3) 

**Catastrophic Cache Eviction**
Meanwhile, the cache worker actively evicts old transactions based solely on cache latest version, with NO regard for `file_store_latest_version`: [5](#0-4) 

**Recovery Impossible**
On restart, the processor reads the stale metadata and attempts to re-process from that version, but those transactions have been evicted: [6](#0-5) [7](#0-6) 

The `.unwrap()` causes a panic when transactions cannot be fetched, creating permanent service failure.

## Impact Explanation

**High Severity** per Aptos bug bounty criteria due to:

1. **API Crashes**: The processor panics on restart with `.unwrap()` when cache data is missing, causing service unavailability
2. **State Inconsistencies Requiring Intervention**: Creates permanent desynchronization between cache metadata and actual file store state requiring manual data reconstruction
3. **Data Loss Potential**: Transaction data in the gap between metadata version and actual cache state can be permanently lost if not recoverable from other sources

This affects the indexer infrastructure's ability to serve historical transaction data to clients, breaking the availability guarantees of the gRPC API service.

## Likelihood Explanation

**Moderate-High Likelihood** because:

1. **Common Trigger Conditions**:
   - GCS permission changes during deployment
   - Network partitions to GCS
   - GCS quota exhaustion
   - Bucket misconfiguration

2. **No Circuit Breaker**: The infinite retry loop ensures the processor remains stuck rather than failing fast

3. **Time-Based Vulnerability Window**: Once metadata update fails, each passing minute increases the gap as cache worker continues processing and evicting data (300,000 version window = ~5 minutes at high TPS)

4. **Automatic Propagation**: No manual intervention required once triggered - cache eviction happens automatically

## Recommendation

Implement atomic state updates with proper failure handling:

```rust
// In processor.rs run() function, replace lines 258-273 with:

// Save old cache value for rollback
let old_cache_version = self.cache_operator
    .get_file_store_latest_version()
    .await?
    .unwrap_or(0);

// Try metadata update first with timeout
let metadata_result = tokio::time::timeout(
    Duration::from_secs(30),
    self.file_store_operator
        .update_file_store_metadata_with_timeout(chain_id, batch_start_version)
).await;

match metadata_result {
    Ok(Ok(_)) => {
        // Metadata updated successfully, now update cache
        self.cache_operator
            .update_file_store_latest_version(batch_start_version)
            .await?;
    },
    Ok(Err(e)) | Err(_) => {
        // Metadata update failed, do NOT update cache
        tracing::error!(
            batch_start_version = batch_start_version,
            error = ?e,
            "Failed to update file store metadata after timeout. NOT updating cache to prevent desync."
        );
        METADATA_UPLOAD_FAILURE_COUNT.inc();
        
        // Return error to retry entire batch next iteration
        return Err(anyhow::anyhow!("Metadata update failed"));
    }
}
```

Additional safeguards:

1. **Make cache eviction aware of file_store_latest_version**: Modify cache operator to only evict versions below `file_store_latest_version`
2. **Add circuit breaker**: After N consecutive metadata failures, halt processor and alert operators
3. **Add fallback recovery**: Allow processor to fetch missing transactions from actual GCS files if cache misses occur

## Proof of Concept

```rust
// Rust test demonstrating the vulnerability
#[tokio::test]
async fn test_cache_file_desync_vulnerability() {
    // Setup: Mock Redis and GCS with processor at version 100,000
    let mut mock_redis = MockRedisConnection::new(vec![]);
    let mut cache_operator = CacheOperator::new(mock_redis, StorageFormat::Base64UncompressedProto);
    let mut mock_gcs = MockGcsOperator::new();
    
    // Simulate successful batch upload to GCS
    let batch_version = 101_000;
    mock_gcs.upload_transactions(100_000..101_000).await.unwrap();
    
    // Step 1: Cache update succeeds
    cache_operator.update_file_store_latest_version(batch_version).await.unwrap();
    assert_eq!(cache_operator.get_file_store_latest_version().await.unwrap(), Some(batch_version));
    
    // Step 2: Simulate GCS metadata update failure (permissions revoked)
    mock_gcs.set_metadata_write_permission(false);
    
    // Processor enters infinite retry loop (simulated by timeout)
    let result = tokio::time::timeout(
        Duration::from_secs(1),
        mock_gcs.update_metadata(batch_version)
    ).await;
    assert!(result.is_err()); // Timeout because infinite retry
    
    // Step 3: Cache worker continues and processes 300,000 more versions
    for v in 101_000..401_000 {
        cache_operator.update_cache_transactions(vec![create_mock_txn(v)]).await.unwrap();
        // Cache eviction happens automatically at CACHE_SIZE_EVICTION_LOWER_BOUND
    }
    
    // Step 4: Versions around 100,000 are now evicted
    let evicted_result = cache_operator.get_transactions(100_000, 1000).await;
    assert!(evicted_result.is_err()); // Data evicted!
    
    // Step 5: Processor restart tries to read old metadata
    let metadata = mock_gcs.get_file_store_metadata().await.unwrap();
    assert_eq!(metadata.version, 100_000); // Still old version!
    
    // Step 6: Processor tries to fetch evicted data -> PANIC
    let panic_result = std::panic::catch_unwind(|| {
        cache_operator.get_transactions(100_000, 1000).await.unwrap()
    });
    assert!(panic_result.is_err()); // Panics on unwrap()
    
    // Result: Permanent data loss - cannot recover gap between 100,000 and 401,000
}
```

## Notes

This vulnerability specifically affects the indexer-grpc infrastructure layer. While not directly impacting consensus or on-chain state, it breaks the availability and consistency guarantees of the transaction indexing service, which is critical for ecosystem participants relying on historical data access. The issue demonstrates a violation of the atomic state transition invariant in distributed system design.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L82-98)
```rust
        let metadata = file_store_operator.get_file_store_metadata().await.unwrap();

        ensure!(metadata.chain_id == chain_id, "Chain ID mismatch.");
        let batch_start_version = metadata.version;
        // Cache config in the cache
        cache_operator.cache_setup_if_needed().await?;
        match cache_operator.get_chain_id().await? {
            Some(id) => {
                ensure!(id == chain_id, "Chain ID mismatch.");
            },
            None => {
                cache_operator.set_chain_id(chain_id).await?;
            },
        }
        cache_operator
            .update_file_store_latest_version(batch_start_version)
            .await?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L162-165)
```rust
                    let transactions = cache_operator_clone
                        .get_transactions(start_version, FILE_ENTRY_TRANSACTION_COUNT)
                        .await
                        .unwrap();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L258-273)
```rust
            self.cache_operator
                .update_file_store_latest_version(batch_start_version)
                .await?;
            while self
                .file_store_operator
                .update_file_store_metadata_with_timeout(chain_id, batch_start_version)
                .await
                .is_err()
            {
                tracing::error!(
                    batch_start_version = batch_start_version,
                    "Failed to update file store metadata. Retrying."
                );
                std::thread::sleep(std::time::Duration::from_millis(500));
                METADATA_UPLOAD_FAILURE_COUNT.inc();
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L181-190)
```rust
    pub async fn update_file_store_latest_version(
        &mut self,
        latest_version: u64,
    ) -> anyhow::Result<()> {
        let _: () = self
            .conn
            .set(FILE_STORE_LATEST_VERSION, latest_version)
            .await?;
        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L279-289)
```rust
            // Actively evict the expired cache. This is to avoid using Redis
            // eviction policy, which is probabilistic-based and may evict the
            // cache that is still needed.
            if version >= CACHE_SIZE_EVICTION_LOWER_BOUND {
                let key = CacheEntry::build_key(
                    version - CACHE_SIZE_EVICTION_LOWER_BOUND,
                    self.storage_format,
                )
                .to_string();
                redis_pipeline.cmd("DEL").arg(key).ignore();
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/gcs.rs (L162-182)
```rust
    async fn update_file_store_metadata_with_timeout(
        &mut self,
        expected_chain_id: u64,
        version: u64,
    ) -> anyhow::Result<()> {
        if let Some(metadata) = self.get_file_store_metadata().await {
            assert_eq!(metadata.chain_id, expected_chain_id, "Chain ID mismatch.");
            assert_eq!(
                metadata.storage_format, self.storage_format,
                "Storage format mismatch."
            );
        }
        if self.file_store_metadata_last_updated.elapsed().as_millis()
            < FILE_STORE_METADATA_TIMEOUT_MILLIS
        {
            bail!("File store metadata is updated too frequently.")
        }
        self.update_file_store_metadata_internal(expected_chain_id, version)
            .await?;
        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L478-499)
```rust
        // Check if the file store isn't too far away
        loop {
            let file_store_version = cache_operator
                .get_file_store_latest_version()
                .await?
                .unwrap();
            if file_store_version + FILE_STORE_VERSIONS_RESERVED < current_version {
                tokio::time::sleep(std::time::Duration::from_millis(
                    CACHE_WORKER_WAIT_FOR_FILE_STORE_MS,
                ))
                .await;
                tracing::warn!(
                    current_version = current_version,
                    file_store_version = file_store_version,
                    "[Indexer Cache] File store version is behind current version too much."
                );
                WAIT_FOR_FILE_STORE_COUNTER.inc();
            } else {
                // File store is up to date, continue cache update.
                break;
            }
        }
```
