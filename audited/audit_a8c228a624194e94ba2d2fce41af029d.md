# Audit Report

## Title
Unbounded State Accumulation via Large Package Staging Enables Validator Resource Exhaustion

## Summary
The transaction generator's `generate_transactions()` function can create unlimited chunked transactions for large packages, exploiting the absence of size limits on the `StagingArea` resource in `large_packages.move`. Attackers can stage hundreds of megabytes of data across thousands of transactions without automatic cleanup, causing validator memory exhaustion during transaction processing and state bloat.

## Finding Description

The vulnerability exists in the interaction between three components:

1. **Transaction Generation** - The `generate_transactions()` function calls `publish_transaction_payload()` with no limit on returned transactions: [1](#0-0) 

2. **Unbounded Chunking** - The `chunk_package_and_create_payloads()` function creates unlimited chunks with no cap: [2](#0-1) 

This function divides packages into 55KB chunks but has **no maximum chunk count**. A 500MB package would generate ~9,100 staging transactions.

3. **Unbounded Staging Area** - The `StagingArea` resource accumulates data without size limits: [3](#0-2) 

The `stage_code_chunk_internal()` function appends to `metadata_serialized` and adds to the `code` SmartTable without checking cumulative size: [4](#0-3) 

**Attack Scenario:**
1. Attacker creates a malicious 500MB package with 500 modules (1MB each)
2. Calls `generate_transactions()` which creates ~9,100 chunked transactions
3. Each `stage_code_chunk` transaction adds 55KB, staying under the 10MB per-transaction write limit
4. The `StagingArea` grows to 500MB across transactions with no upper bound
5. Multiple attackers can do this simultaneously from different accounts
6. When validators process subsequent staging transactions, they must load, modify, and write the growing `StagingArea`
7. The final `stage_code_chunk_and_publish_to_account` must assemble all 500MB into a vector: [5](#0-4) 

This loads 500MB into validator memory during transaction execution.

**Broken Invariants:**
- **Resource Limits**: "All operations must respect gas, storage, and computational limits" - The `StagingArea` has no size limit
- **Move VM Safety**: "Bytecode execution must respect gas limits and memory constraints" - Memory consumption is unbounded

## Impact Explanation

This qualifies as **High Severity** under Aptos bug bounty criteria:
- **Validator node slowdowns**: Processing transactions on large `StagingArea` resources becomes increasingly expensive. Multiple concurrent attacks multiply the impact.
- **State bloat attack**: Abandoned `StagingArea` resources (if publish never happens or fails) remain in state indefinitely without automatic cleanup.
- **Memory exhaustion**: The `assemble_module_code` function must load the entire staged package into memory. For a 500MB package, this consumes 500MB of validator RAM during transaction execution.

Per-transaction limits (10MB write operations, gas limits) are bypassed because the attack spreads across thousands of transactions. The cumulative effect is unbounded state growth and memory consumption during processing. [6](#0-5) 

While individual transactions respect the `max_bytes_all_write_ops_per_transaction` limit of 10MB, the `StagingArea` resource accumulates data across multiple transactions with no aggregate limit.

## Likelihood Explanation

**Likelihood: High**

The attack requires:
- No special permissions (any account can stage code)
- Significant but affordable gas costs (thousands of transactions Ã— standard gas fees)
- Basic tooling (transaction generator is part of the codebase)

The transaction generator already implements the chunking logic, making exploitation straightforward. Multiple attackers can amplify the impact by staging large packages simultaneously.

## Recommendation

Implement a hard limit on `StagingArea` size and add automatic cleanup:

**In `large_packages.move`:**

```move
// Add to module constants
const MAX_STAGING_AREA_SIZE: u64 = 50_000_000; // 50MB limit
const ESTAGING_AREA_TOO_LARGE: u64 = 10;

// Modify stage_code_chunk_internal to check size
inline fun stage_code_chunk_internal(
    owner: &signer,
    metadata_chunk: vector<u8>,
    code_indices: vector<u16>,
    code_chunks: vector<vector<u8>>
): &mut StagingArea {
    // ... existing checks ...
    
    let staging_area = borrow_global_mut<StagingArea>(owner_address);
    
    // Calculate new size before modification
    let new_metadata_size = vector::length(&staging_area.metadata_serialized) + 
                           vector::length(&metadata_chunk);
    let new_code_size = estimate_code_size(&staging_area.code, &code_chunks);
    
    assert!(
        new_metadata_size + new_code_size <= MAX_STAGING_AREA_SIZE,
        error::resource_exhausted(ESTAGING_AREA_TOO_LARGE)
    );
    
    // ... rest of function ...
}
```

**In `chunked_publish.rs`:**

```rust
const MAX_CHUNKS: usize = 1000; // ~55MB at 55KB per chunk

pub fn chunk_package_and_create_payloads(...) -> Result<Vec<TransactionPayload>, Error> {
    let mut payloads = Vec::new();
    // ... chunking logic ...
    
    if payloads.len() > MAX_CHUNKS {
        return Err(anyhow!("Package too large: {} chunks exceed maximum of {}", 
                          payloads.len(), MAX_CHUNKS));
    }
    
    Ok(payloads)
}
```

Additionally, implement automatic cleanup of old `StagingArea` resources after a timeout period.

## Proof of Concept

```move
#[test(deployer = @0xcafe)]
public fun test_staging_area_unbounded_growth(deployer: &signer) {
    // Create a large package by staging many chunks
    let metadata = vector::empty<u8>();
    let i = 0;
    
    // Stage 1000 chunks of 55KB each = 55MB total
    while (i < 1000) {
        let large_chunk = vector::empty<u8>();
        let j = 0;
        while (j < 55000) {
            vector::push_back(&mut large_chunk, 0u8);
            j = j + 1;
        };
        
        large_packages::stage_code_chunk(
            deployer,
            if (i == 0) { metadata } else { vector::empty() },
            vector[0u16], // All code goes to module index 0
            vector[large_chunk]
        );
        i = i + 1;
    };
    
    // StagingArea now contains 55MB with no size checks
    // Attempting to publish would load all 55MB into memory
    // Multiple accounts doing this simultaneously multiplies validator memory usage
}
```

**Rust-level reproduction:**

1. Use the transaction generator with a crafted 500MB package
2. Call `generate_transactions()` to create thousands of chunked transactions
3. Submit these transactions to a validator network
4. Observe increasing memory consumption and transaction processing time
5. Repeat with multiple accounts to amplify the attack

## Notes

The vulnerability stems from the design assumption that package sizes would remain reasonable. The chunked publishing mechanism was intended to handle legitimate large packages (e.g., 1-5MB) but has no safeguards against malicious abuse. The absence of automatic `StagingArea` cleanup means abandoned staging data persists indefinitely, enabling a state bloat attack even if the final publish never occurs.

### Citations

**File:** crates/transaction-generator-lib/src/publish_modules.rs (L35-64)
```rust
    fn generate_transactions(
        &mut self,
        account: &LocalAccount,
        num_to_create: usize,
    ) -> Vec<SignedTransaction> {
        let mut requests = Vec::with_capacity(num_to_create);

        // First publish the module and then use it
        let package = self
            .package_handler
            .write()
            .pick_package(&mut self.rng, account.address());

        for payload in package.publish_transaction_payload(&self.txn_factory.get_chain_id()) {
            let txn = account.sign_with_transaction_builder(self.txn_factory.payload(payload));
            requests.push(txn);
        }
        // for _ in 1..num_to_create {
        //     let request = package.use_random_transaction(&mut self.rng, account, &self.txn_factory);
        //     requests.push(request);
        // }
        // republish
        // let package = self
        //     .package_handler
        //     .write()
        //     .pick_package(&mut self.rng, account.address());
        // let txn = package.publish_transaction(account, &self.txn_factory);
        // requests.push(txn);
        requests
    }
```

**File:** aptos-move/framework/src/chunked_publish.rs (L36-110)
```rust
pub fn chunk_package_and_create_payloads(
    metadata: Vec<u8>,
    package_code: Vec<Vec<u8>>,
    publish_type: PublishType,
    object_address: Option<AccountAddress>,
    large_packages_module_address: AccountAddress,
    chunk_size: usize,
) -> Vec<TransactionPayload> {
    // Chunk the metadata
    let mut metadata_chunks = create_chunks(metadata, chunk_size);
    // Separate last chunk for special handling
    let mut metadata_chunk = metadata_chunks.pop().expect("Metadata is required");

    let mut taken_size = metadata_chunk.len();
    let mut payloads = metadata_chunks
        .into_iter()
        .map(|chunk| {
            large_packages_stage_code_chunk(chunk, vec![], vec![], large_packages_module_address)
        })
        .collect::<Vec<_>>();

    let mut code_indices: Vec<u16> = vec![];
    let mut code_chunks: Vec<Vec<u8>> = vec![];

    for (idx, module_code) in package_code.into_iter().enumerate() {
        let chunked_module = create_chunks(module_code, chunk_size);
        for chunk in chunked_module {
            if taken_size + chunk.len() > chunk_size {
                // Create a payload and reset accumulators
                let payload = large_packages_stage_code_chunk(
                    metadata_chunk,
                    code_indices.clone(),
                    code_chunks.clone(),
                    large_packages_module_address,
                );
                payloads.push(payload);

                metadata_chunk = vec![];
                code_indices.clear();
                code_chunks.clear();
                taken_size = 0;
            }

            code_indices.push(idx as u16);
            taken_size += chunk.len();
            code_chunks.push(chunk);
        }
    }

    // The final call includes staging the last metadata and code chunk, and then publishing or upgrading the package on-chain.
    let payload = match publish_type {
        PublishType::AccountDeploy => large_packages_stage_code_chunk_and_publish_to_account(
            metadata_chunk,
            code_indices,
            code_chunks,
            large_packages_module_address,
        ),
        PublishType::ObjectDeploy => large_packages_stage_code_chunk_and_publish_to_object(
            metadata_chunk,
            code_indices,
            code_chunks,
            large_packages_module_address,
        ),
        PublishType::ObjectUpgrade => large_packages_stage_code_chunk_and_upgrade_object_code(
            metadata_chunk,
            code_indices,
            code_chunks,
            object_address.expect("ObjectAddress is missing"),
            large_packages_module_address,
        ),
    };
    payloads.push(payload);

    payloads
}
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L60-64)
```text
    struct StagingArea has key {
        metadata_serialized: vector<u8>,
        code: SmartTable<u64, vector<u8>>,
        last_module_idx: u64
    }
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L156-180)
```text
        let staging_area = borrow_global_mut<StagingArea>(owner_address);

        if (!vector::is_empty(&metadata_chunk)) {
            vector::append(&mut staging_area.metadata_serialized, metadata_chunk);
        };

        let i = 0;
        while (i < vector::length(&code_chunks)) {
            let inner_code = *vector::borrow(&code_chunks, i);
            let idx = (*vector::borrow(&code_indices, i) as u64);

            if (smart_table::contains(&staging_area.code, idx)) {
                vector::append(
                    smart_table::borrow_mut(&mut staging_area.code, idx), inner_code
                );
            } else {
                smart_table::add(&mut staging_area.code, idx, inner_code);
                if (idx > staging_area.last_module_idx) {
                    staging_area.last_module_idx = idx;
                }
            };
            i = i + 1;
        };

        staging_area
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L213-225)
```text
    inline fun assemble_module_code(staging_area: &mut StagingArea): vector<vector<u8>> {
        let last_module_idx = staging_area.last_module_idx;
        let code = vector[];
        let i = 0;
        while (i <= last_module_idx) {
            vector::push_back(
                &mut code,
                *smart_table::borrow(&staging_area.code, i)
            );
            i = i + 1;
        };
        code
    }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L154-162)
```rust
            max_bytes_per_write_op: NumBytes,
            { 5.. => "max_bytes_per_write_op" },
            1 << 20, // a single state item is 1MB max
        ],
        [
            max_bytes_all_write_ops_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_write_ops_per_transaction" },
            10 << 20, // all write ops from a single transaction are 10MB max
        ],
```
