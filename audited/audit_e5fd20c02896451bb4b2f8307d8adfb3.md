# Audit Report

## Title
Partial Response Attack: Malicious Peers Can Force Excessive Network Requests Without Penalty

## Summary
A malicious storage service peer can exploit the missing data request mechanism to force victim nodes to make N times more network requests than necessary, causing resource exhaustion and slower state synchronization. The peer's reputation score increases rather than decreases during this attack, making the vulnerability sustainable and difficult to mitigate.

## Finding Description

The data streaming service contains a vulnerability where malicious peers can return partial responses to force excessive network requests without being penalized.

**The Attack Mechanism:**

When a data stream requests epoch ending ledger infos, the code at [1](#0-0)  detects when fewer items are received than requested and automatically creates a new request for the missing data.

The vulnerability exists because:

1. **Storage services always return at least one item**: The storage service implementation at [2](#0-1)  guarantees that at least one item is returned even if it exceeds size limits, allowing malicious peers to legitimately return only 1 item per request.

2. **Partial responses are treated as successful**: When a partial response is received, the peer's score is increased at [3](#0-2)  because the response passes basic type validation, even though it contains minimal data.

3. **No penalty for minimal data responses**: The peer scoring system at [4](#0-3)  only penalizes explicitly invalid responses (InvalidData, InvalidPayloadDataType, ProofVerificationError), not partial but valid responses.

4. **Missing data requests bypass retry limits**: When missing data is requested at [5](#0-4) , it's pushed to the front of the queue and the failure count is later reset at [6](#0-5) , allowing unlimited sequential partial responses.

**Attack Scenario:**

1. Victim node requests epochs 0-999 (1000 epoch ending ledger infos)
2. Malicious peer returns only epoch 0 (1 item)
3. Peer's reputation score increases from 50.0 to 51.0
4. Victim detects missing data and creates request for epochs 1-999
5. Malicious peer returns only epoch 1
6. Peer's reputation score increases to 52.0
7. Process repeats 1000 times, with peer's score reaching 1050.0

The malicious peer exploits the legitimate chunking mechanism that allows partial responses for size/time constraints, but does so deliberately to cause request amplification.

## Impact Explanation

**Medium Severity** - This vulnerability causes resource exhaustion and state sync degradation:

- **Request Amplification**: N times more network requests than necessary (e.g., 1000 requests instead of 1)
- **Bandwidth Waste**: Each request includes protocol overhead (headers, proofs, serialization)
- **CPU/Memory Exhaustion**: Each request requires processing, validation, and queue management
- **Synchronization Delay**: Sequential processing causes significantly slower state sync, affecting node liveness
- **Reputation System Failure**: Malicious peers gain reputation (+1.0 per response) instead of being penalized

This falls under the Medium severity category: "State inconsistencies requiring intervention" as nodes experiencing this attack will struggle to maintain synchronization and may require manual intervention to blacklist malicious peers.

The vulnerability does not directly cause loss of funds or consensus violations, but degrades network health and node availability, making it a legitimate security concern under the "Resource Limits" invariant: [7](#0-6) 

## Likelihood Explanation

**High Likelihood** - This attack is easy to execute and difficult to detect:

- **Low Attack Complexity**: Any peer running a storage service can exploit this by configuring small response sizes or deliberately returning minimal data
- **No Special Access Required**: Does not require validator privileges or insider access
- **No Detection Mechanism**: There's no monitoring for peers that consistently return minimal data
- **Counter-Intuitive Rewards**: The malicious peer's reputation increases, making the attack sustainable
- **Legitimate Use Case Overlap**: Partial responses can legitimately occur due to size constraints, making malicious behavior indistinguishable

The attack is particularly likely during initial state sync when nodes request large ranges of historical data.

## Recommendation

Implement a multi-layered defense:

**1. Track Average Response Completeness Per Peer**

Add peer metrics to track the ratio of returned items to requested items:
- Penalize peers that consistently return significantly fewer items than requested
- Apply penalties through the existing error type system at [8](#0-7) 

**2. Implement Minimum Expected Response Size**

Before creating missing data requests, verify the peer returned a reasonable amount:
- If less than 10% of requested items were returned without size constraints being hit, treat as potentially malicious
- Apply graduated penalties for repeatedly minimal responses

**3. Limit Sequential Missing Data Requests**

Add a counter for consecutive missing data requests from the same peer:
- After N consecutive partial responses (e.g., 5), treat as suspicious and apply penalties
- Consider this separately from the retry limit to prevent bypassing

**4. Enhanced Peer Selection**

Modify peer selection logic to prefer peers with higher completion rates when multiple peers advertise the same data.

**Example Fix for Detection:**

Add to `request_missing_data` function:
```rust
// Check if the response is suspiciously small
let response_completeness_ratio = num_received as f64 / num_requested as f64;
if response_completeness_ratio < 0.1 && !size_constrained {
    // Potential malicious partial response
    warn!("Peer returned suspiciously small response");
    self.peer_partial_response_count += 1;
    
    if self.peer_partial_response_count > 5 {
        self.notify_bad_response(response_context, ResponseError::InvalidData);
    }
}
```

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[tokio::test]
async fn test_partial_response_amplification_attack() {
    // Setup: Create a mock storage service that always returns 1 item
    let malicious_peer = create_malicious_peer_that_returns_one_item_per_request();
    
    // Setup: Create a data stream requesting 1000 epoch ending ledger infos
    let mut data_stream = create_data_stream();
    let request = EpochEndingLedgerInfosRequest {
        start_epoch: 0,
        end_epoch: 999, // Request 1000 epochs
    };
    
    // Send initial request
    data_stream.send_request(request).await;
    
    // Track number of requests made
    let mut request_count = 0;
    
    // Process responses until complete
    while !data_stream.is_complete() {
        // Malicious peer returns only 1 epoch per response
        let response = malicious_peer.handle_request_with_one_item();
        data_stream.process_response(response).await;
        request_count += 1;
        
        // Verify peer score keeps increasing (vulnerability!)
        assert!(malicious_peer.get_score() > 50.0 + request_count as f64);
    }
    
    // Vulnerability: Should make ~1-10 requests, but makes 1000
    assert!(request_count >= 1000, 
        "Vulnerability: Made {} requests instead of ~1-10", request_count);
    
    // Vulnerability: Malicious peer has highest reputation
    assert!(malicious_peer.get_score() > 1000.0,
        "Vulnerability: Malicious peer score is {}", malicious_peer.get_score());
}
```

## Notes

The vulnerability leverages legitimate protocol features (partial responses for size constraints) but exploits the lack of monitoring for peers that consistently return minimal data. The attack is particularly effective because it's indistinguishable from legitimate behavior at the protocol level, requiring behavioral analysis to detect.

The fix requires balancing legitimate partial responses (due to actual size constraints) against malicious minimal responses. A graduated penalty system based on consecutive partial responses provides the best defense while maintaining protocol flexibility.

### Citations

**File:** state-sync/data-streaming-service/src/data_stream.rs (L265-279)
```rust
    /// Creates and sends a batch of data client requests to the network
    fn create_and_send_client_requests(
        &mut self,
        global_data_summary: &GlobalDataSummary,
    ) -> Result<(), Error> {
        // Calculate the number of in-flight requests (i.e., requests that haven't completed)
        let num_pending_requests = self.get_num_pending_data_requests()?;
        let num_complete_pending_requests = self.get_num_complete_pending_requests()?;
        let num_in_flight_requests =
            num_pending_requests.saturating_sub(num_complete_pending_requests);

        // Calculate the max number of requests that can be sent now
        let max_pending_requests = self.streaming_service_config.max_pending_requests;
        let max_num_requests_to_send = max_pending_requests.saturating_sub(num_pending_requests);

```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L664-672)
```rust
            // Send the missing data request
            let pending_client_response =
                self.send_client_request(false, missing_data_request.clone());

            // Push the pending response to the front of the queue
            self.get_sent_data_requests()?
                .push_front(pending_client_response);

            return Ok(true); // Missing data was requested
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L806-807)
```rust
            // Reset the failure count. We've sent a notification and can move on.
            self.request_failure_count = 0;
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L1081-1096)
```rust
            let num_received_ledger_infos = ledger_infos.len() as u64;
            if num_received_ledger_infos < num_requested_ledger_infos {
                let start_epoch = request
                    .start_epoch
                    .checked_add(num_received_ledger_infos)
                    .ok_or_else(|| Error::IntegerOverflow("Start epoch has overflown!".into()))?;
                Ok(Some(DataClientRequest::EpochEndingLedgerInfos(
                    EpochEndingLedgerInfosRequest {
                        start_epoch,
                        end_epoch: request.end_epoch,
                    },
                )))
            } else {
                Ok(None) // The request was satisfied!
            }
        },
```

**File:** state-sync/storage-service/server/src/storage.rs (L1404-1406)
```rust
        if always_allow_first_item && self.num_items_fetched == 0 {
            true // We always include at least one item
        } else {
```

**File:** state-sync/aptos-data-client/src/client.rs (L811-817)
```rust
                // For now, record all responses that at least pass the data
                // client layer successfully. An alternative might also have the
                // consumer notify both success and failure via the callback.
                // On the one hand, scoring dynamics are simpler when each request
                // is successful or failed but not both; on the other hand, this
                // feels simpler for the consumer.
                self.peer_states.update_score_success(peer);
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L45-52)
```rust
pub enum ErrorType {
    /// A response or error that's not actively malicious but also doesn't help
    /// us make progress, e.g., timeouts, remote errors, invalid data, etc...
    NotUseful,
    /// A response or error that appears to be actively hindering progress or
    /// attempting to deceive us, e.g., invalid proof.
    Malicious,
}
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L54-62)
```rust
impl From<ResponseError> for ErrorType {
    fn from(error: ResponseError) -> Self {
        match error {
            ResponseError::InvalidData | ResponseError::InvalidPayloadDataType => {
                ErrorType::NotUseful
            },
            ResponseError::ProofVerificationError => ErrorType::Malicious,
        }
    }
```
