# Audit Report

## Title
Missing Data Validation in GCS File Store Writer Causes Indexer Service Denial of Service

## Summary
The `save_raw_file()` function in the GCS file store operator accepts arbitrary `Vec<u8>` data and writes it directly to Google Cloud Storage without any format validation, content verification, or integrity checks. When downstream services attempt to read and deserialize this data, the deserialization code uses `.expect()` calls that panic on invalid data, causing the indexer data service to fail serving those transaction versions indefinitely.

## Finding Description

The vulnerability exists across three key code paths:

**1. Writing Phase - No Validation:** [1](#0-0) 

The `save_raw_file()` implementation accepts raw bytes and directly uploads them to GCS without checking if the data represents valid compressed protobuf transactions, valid JSON metadata, or any expected format. The function blindly trusts that the input `data: Vec<u8>` is correctly formatted.

**2. Reading Phase - Panicking Deserialization:** [2](#0-1) [3](#0-2) 

Metadata deserialization uses `.expect()` which panics on invalid JSON, causing the reader to fail catastrophically rather than returning an error.

**3. Transaction Data Deserialization - Multiple Panic Points:** [4](#0-3) 

The `into_transactions_in_storage()` method contains multiple `.expect()` calls that panic on:
- LZ4 decompression failures (line 265, 269)
- Protobuf deserialization failures (line 270-271, 282-283)
- JSON deserialization failures (line 275)
- Base64 decoding failures (line 281)

**Attack Scenario:**

An attacker or corrupted system could write invalid data to GCS through several vectors:

1. **Bug in serialization logic**: A bug in `FileEntry::from_transactions()` or `serde_json::to_vec()` could produce corrupted output that passes through unchecked
2. **Memory corruption**: Runtime memory corruption before the write operation
3. **Compromised GCS credentials**: Direct write access to the GCS bucket to overwrite files
4. **Network corruption**: Data corruption during transmission (though less likely with TLS)

Once corrupted data exists in GCS, every request to the indexer data service for those affected transaction versions will trigger a panic in the spawned blocking task, causing that request to fail: [5](#0-4) 

The service will retry indefinitely but continue failing for those specific versions, effectively making that historical data permanently inaccessible.

## Impact Explanation

**Severity: Medium**

This vulnerability causes **denial of service for the indexer data service** but does NOT affect core blockchain operations:

- **Does NOT affect**: Consensus, block production, transaction execution, validator operations, or on-chain state
- **Does affect**: Historical transaction data availability through the indexer gRPC service

The impact maps to **Medium Severity** per the Aptos bug bounty program as it causes "State inconsistencies requiring intervention" - specifically, the indexer service cannot serve certain transaction ranges until manual remediation.

**Affected Services:**
- Indexer gRPC data service becomes unable to serve transactions at corrupted versions
- Wallets, block explorers, and analytics services relying on the indexer experience data gaps
- The service degrades gracefully for other version ranges but has permanent unavailability for affected ranges

**Why Not Higher Severity:**
- The indexer is an auxiliary service, not part of core blockchain consensus or execution
- No funds are at risk
- No consensus safety violations occur
- Validators continue operating normally
- Users can still submit transactions

## Likelihood Explanation

**Likelihood: Medium**

While normal operation should work correctly, several realistic scenarios could trigger this vulnerability:

1. **Software bugs**: Serialization bugs in upstream code are common programming errors
2. **Memory safety issues**: Rust's safety guarantees are strong but not absolute (unsafe code, external libraries)
3. **Operational errors**: Misconfigured GCS permissions or compromised service account credentials
4. **Supply chain attacks**: Compromised dependencies could intentionally write malformed data

The complete absence of validation makes exploitation straightforward once any of these preconditions occur. The vulnerability is latent - corrupted data can be written at any time and will only manifest when that data is requested.

## Recommendation

Implement defense-in-depth validation at both write and read phases:

**1. Add write-time validation in `save_raw_file()`:**

```rust
async fn save_raw_file(&self, file_path: PathBuf, data: Vec<u8>) -> Result<()> {
    let path = self.get_path(file_path);
    
    // Validate data format before writing
    if path.extension().and_then(|s| s.to_str()) == Some("json") {
        // Validate JSON structure
        serde_json::from_slice::<serde_json::Value>(&data)
            .map_err(|e| anyhow::anyhow!("Invalid JSON data: {}", e))?;
    } else {
        // Validate compressed protobuf format
        // Try decompressing a small header to verify format
        let mut decompressor = lz4::Decoder::new(&data[..])
            .map_err(|e| anyhow::anyhow!("Invalid LZ4 format: {}", e))?;
        let mut header = vec![0u8; 64];
        decompressor.read(&mut header)
            .map_err(|e| anyhow::anyhow!("Cannot read LZ4 header: {}", e))?;
    }
    
    trace!("Uploading validated object to {}/{}.", self.bucket_name, path.as_str());
    Object::create(self.bucket_name.as_str(), data, path.as_str(), JSON_FILE_TYPE)
        .await
        .map_err(anyhow::Error::msg)?;
    Ok(())
}
```

**2. Replace `.expect()` with proper error handling in deserialization:**

Replace all panic-inducing `.expect()` calls with `?` operator or `context()` to return errors instead of panicking:

```rust
pub fn into_transactions_in_storage(self) -> Result<TransactionsInStorage> {
    match self {
        FileEntry::Lz4CompressionProto(bytes) => {
            let mut decompressor = Decoder::new(&bytes[..])
                .context("LZ4 decompression failed - corrupted data")?;
            let mut decompressed = Vec::new();
            decompressor.read_to_end(&mut decompressed)
                .context("LZ4 decompression failed - corrupted data")?;
            TransactionsInStorage::decode(decompressed.as_slice())
                .context("Protobuf deserialization failed - corrupted data")
        },
        FileEntry::JsonBase64UncompressedProto(bytes) => {
            let file: TransactionsLegacyFile = serde_json::from_slice(bytes.as_slice())
                .context("JSON deserialization failed - corrupted data")?;
            // ... rest with ? instead of expect()
        }
    }
}
```

**3. Add data integrity checksums:**

Store and verify checksums (SHA256) for each file to detect corruption:

```rust
let checksum = sha256::digest(&data);
// Store checksum in metadata or separate file
// Verify checksum when reading
```

## Proof of Concept

```rust
// Proof of Concept demonstrating the vulnerability
#[tokio::test]
async fn test_corrupted_data_causes_panic() {
    use tempfile::TempDir;
    use std::path::PathBuf;
    
    // Setup: Create a local file store (equivalent to GCS for testing)
    let temp_dir = TempDir::new().unwrap();
    let file_store = LocalFileStore::new(temp_dir.path().to_path_buf(), None);
    
    // Attack: Write corrupted data that is NOT valid LZ4-compressed protobuf
    let corrupted_data = vec![0xFF, 0xFE, 0xFD, 0xFC]; // Invalid bytes
    let file_path = PathBuf::from("test_transactions/0");
    
    // This succeeds - no validation!
    file_store.save_raw_file(file_path.clone(), corrupted_data.clone())
        .await
        .expect("Write should succeed without validation");
    
    // Attempt to read and deserialize
    let reader = FileStoreReader::new(1, Arc::new(file_store)).await;
    
    // This will panic when trying to deserialize corrupted data
    let result = reader.get_transaction_file_at_version(0, None, 3).await;
    
    // The spawned blocking task panics, returning an error
    assert!(result.is_err());
    assert!(result.unwrap_err().to_string().contains("decompression failed"));
}
```

## Notes

**Important Context:**
- This vulnerability affects the **indexer gRPC service**, which is an auxiliary data indexing system, NOT the core Aptos blockchain consensus or execution layer
- Corrupted indexer data does NOT affect validator operations, consensus safety, transaction execution, or on-chain state
- The impact is limited to historical transaction data availability for external clients (wallets, explorers, analytics)
- The indexer service is located in `ecosystem/indexer-grpc/`, separate from core blockchain components

**Remediation Priority:**
While this is a valid security issue requiring a fix, it should be prioritized as Medium severity given that it does not compromise blockchain security guarantees or validator operations.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/gcs.rs (L120-137)
```rust
    async fn save_raw_file(&self, file_path: PathBuf, data: Vec<u8>) -> Result<()> {
        let path = self.get_path(file_path);
        trace!(
            "Uploading object to {}/{}.",
            self.bucket_name,
            path.as_str()
        );
        Object::create(
            self.bucket_name.as_str(),
            data,
            path.as_str(),
            JSON_FILE_TYPE,
        )
        .await
        .map_err(anyhow::Error::msg)?;

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L160-166)
```rust
    pub async fn get_file_store_metadata(&self) -> Option<FileStoreMetadata> {
        self.reader
            .get_raw_file(PathBuf::from(METADATA_FILE_NAME))
            .await
            .expect("Failed to get file store metadata.")
            .map(|data| serde_json::from_slice(&data).expect("Metadata JSON is invalid."))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L170-176)
```rust
    pub async fn get_batch_metadata(&self, version: u64) -> Option<BatchMetadata> {
        self.reader
            .get_raw_file(self.get_path_for_batch_metadata(version))
            .await
            .expect("Failed to get batch metadata.")
            .map(|data| serde_json::from_slice(&data).expect("Batch metadata JSON is invalid."))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L234-237)
```rust
        let transactions_in_storage = tokio::task::spawn_blocking(move || {
            FileEntry::new(bytes, StorageFormat::Lz4CompressedProto).into_transactions_in_storage()
        })
        .await?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L262-292)
```rust
    pub fn into_transactions_in_storage(self) -> TransactionsInStorage {
        match self {
            FileEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                TransactionsInStorage::decode(decompressed.as_slice())
                    .expect("proto deserialization failed.")
            },
            FileEntry::JsonBase64UncompressedProto(bytes) => {
                let file: TransactionsLegacyFile =
                    serde_json::from_slice(bytes.as_slice()).expect("json deserialization failed.");
                let transactions = file
                    .transactions_in_base64
                    .into_iter()
                    .map(|base64| {
                        let bytes: Vec<u8> =
                            base64::decode(base64).expect("base64 decoding failed.");
                        Transaction::decode(bytes.as_slice())
                            .expect("proto deserialization failed.")
                    })
                    .collect::<Vec<Transaction>>();
                TransactionsInStorage {
                    starting_version: Some(file.starting_version),
                    transactions,
                }
            },
        }
    }
```
