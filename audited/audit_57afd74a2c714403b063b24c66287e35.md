# Audit Report

## Title
Unvalidated Client-Controlled Resource Limits Enable Storage Service DoS Attack via Output Reduction Abuse

## Summary
The storage service accepts and honors the `max_num_output_reductions` parameter directly from client requests without validation, allowing malicious peers to force the server to perform up to ~12 expensive database operations per request instead of 1. This enables a resource exhaustion attack that can slow down or DoS validator and fullnode storage services.

## Finding Description

The state sync data client sends `max_num_output_reductions` as part of transaction/output requests to storage service peers. This parameter controls how many times the storage server will iteratively reduce (halve) the response chunk size before falling back to transactions. [1](#0-0) 

The critical vulnerability exists in the storage service request handling pipeline:

1. **No Validation in Handler**: The server's request handler extracts `max_num_output_reductions` from the client's request and passes it directly to the storage layer without any validation or bounds checking. [2](#0-1) 

2. **Unbounded Loop Control**: The storage layer uses this client-controlled value directly in a loop condition, allowing up to `max_num_output_reductions + 1` iterations of expensive database operations. [3](#0-2) 

3. **Request Moderator Bypasses Parameter Validation**: The request moderator only validates whether the request can be serviced based on available data ranges, not the resource limit parameters themselves. [4](#0-3) 

**Attack Execution:**
A malicious peer crafts a `TransactionsOrOutputsWithProofRequest` with `max_num_output_reductions = u64::MAX` (or any large value). The server receives this request and enters the reduction loop, which executes min(max_num_output_reductions + 1, log₂(chunk_size)) iterations. With the default `max_transaction_output_chunk_size` of 3000, this results in approximately 12 database read operations instead of the legitimate 1 operation.

Each iteration involves:
- `storage.get_transaction_outputs()` - expensive database read
- Full response serialization
- Size validation via `check_overflow_network_frame()`

**Invariant Violated:** 
Resource Limits - servers must enforce their own resource limits and not trust client-provided values for resource-intensive operations.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program:

**"Validator node slowdowns"** - A malicious peer can force storage service nodes to perform 12x more database operations than necessary. When amplified with concurrent requests from multiple malicious peers or a single peer making parallel requests, this can:

- Exhaust database connection pools
- Cause excessive CPU usage from repeated serialization of large data chunks  
- Create memory pressure from loading multiple chunk variations
- Degrade state sync performance for legitimate peers
- Potentially crash the storage service under sustained load

The attack affects:
- **Mainnet validators and fullnodes** (where `enable_size_and_time_aware_chunking = false` by default)
- Any node with legacy chunking enabled
- Critical state synchronization infrastructure [5](#0-4) 

## Likelihood Explanation

**Likelihood: HIGH**

- **Attack Complexity: Low** - Any network peer can send storage service requests; no authentication or special privileges required
- **Attacker Requirements: Minimal** - Just need to craft a request with `max_num_output_reductions` set to a high value
- **Amplification: Significant** - Each malicious request triggers 12x resource usage, and attackers can send many concurrent requests
- **Affected Systems: Critical** - Mainnet production systems are vulnerable as they use legacy chunking by default

The configuration shows this is enabled on mainnet: [6](#0-5) 

## Recommendation

Implement server-side validation and enforcement of `max_num_output_reductions`:

**1. Add validation in the request handler:**
```rust
// In handler.rs, validate before processing
fn get_transactions_or_outputs_with_proof(
    &self,
    request: &TransactionsOrOutputsWithProofRequest,
) -> aptos_storage_service_types::Result<DataResponse, Error> {
    // Validate max_num_output_reductions against server's own limit
    const MAX_ALLOWED_OUTPUT_REDUCTIONS: u64 = 5; // Reasonable server-enforced limit
    
    let validated_max_reductions = std::cmp::min(
        request.max_num_output_reductions,
        MAX_ALLOWED_OUTPUT_REDUCTIONS
    );
    
    let response = self.storage.get_transactions_or_outputs_with_proof(
        request.proof_version,
        request.start_version,
        request.end_version,
        request.include_events,
        validated_max_reductions, // Use validated value
    )?;
    // ... rest of function
}
```

**2. Add configuration for server-side limit:**
```rust
// In state_sync_config.rs
pub struct StorageServiceConfig {
    // ... existing fields ...
    
    /// Server-enforced maximum number of output reduction attempts
    /// (regardless of client request)
    pub max_server_output_reductions: u64,
}

impl Default for StorageServiceConfig {
    fn default() -> Self {
        Self {
            // ... existing defaults ...
            max_server_output_reductions: 3, // Conservative server limit
        }
    }
}
```

**3. Log suspicious requests:**
Add monitoring for requests with excessively high `max_num_output_reductions` values to detect potential attacks.

## Proof of Concept

```rust
// Malicious peer exploit demonstration
use aptos_storage_service_types::requests::{
    DataRequest, StorageServiceRequest, TransactionsOrOutputsWithProofRequest
};

// Craft malicious request
let malicious_request = StorageServiceRequest::new(
    DataRequest::GetTransactionsOrOutputsWithProof(
        TransactionsOrOutputsWithProofRequest {
            proof_version: 1000,
            start_version: 0,
            end_version: 2999, // Request max chunk size
            include_events: false,
            max_num_output_reductions: u64::MAX, // Malicious: force max iterations
        }
    ),
    true, // use_compression
);

// When sent to a storage service peer, this will cause:
// - 12 database read operations (log2(3000) ≈ 11.5)
// - 12 serialization operations
// - Significant CPU and memory usage
// - If sent concurrently (100+ requests), can DoS the storage service

// Compare with legitimate request:
let legitimate_request = StorageServiceRequest::new(
    DataRequest::GetTransactionsOrOutputsWithProof(
        TransactionsOrOutputsWithProofRequest {
            proof_version: 1000,
            start_version: 0,
            end_version: 2999,
            include_events: false,
            max_num_output_reductions: 0, // Default: only 1 attempt
        }
    ),
    true,
);
// This causes only 1 database operation before fallback
```

## Notes

The vulnerability is particularly concerning because:

1. **Default configuration is safe** (max_num_output_reductions = 0), but the server doesn't enforce this - it trusts the client's value
2. **Legacy chunking path is still used on mainnet**, making this exploitable in production
3. **The TODO comment** in the config indicates awareness that this mechanism needs migration to cleaner logic
4. **New size-aware chunking** (enabled on non-mainnet) bypasses this parameter entirely, suggesting the team recognizes the design issue

The fix should enforce server-side resource limits rather than trusting client-provided values, following standard security best practices for distributed systems.

### Citations

**File:** state-sync/aptos-data-client/src/client.rs (L1017-1024)
```rust
            DataRequest::GetNewTransactionsOrOutputsWithProof(
                NewTransactionsOrOutputsWithProofRequest {
                    known_version,
                    known_epoch,
                    include_events,
                    max_num_output_reductions: self.get_max_num_output_reductions(),
                },
            )
```

**File:** state-sync/storage-service/server/src/handler.rs (L547-557)
```rust
    fn get_transactions_or_outputs_with_proof(
        &self,
        request: &TransactionsOrOutputsWithProofRequest,
    ) -> aptos_storage_service_types::Result<DataResponse, Error> {
        let response = self.storage.get_transactions_or_outputs_with_proof(
            request.proof_version,
            request.start_version,
            request.end_version,
            request.include_events,
            request.max_num_output_reductions,
        )?;
```

**File:** state-sync/storage-service/server/src/storage.rs (L845-897)
```rust
    fn get_transactions_or_outputs_with_proof_by_size_legacy(
        &self,
        proof_version: u64,
        start_version: u64,
        end_version: u64,
        mut num_outputs_to_fetch: u64,
        include_events: bool,
        max_num_output_reductions: u64,
        max_response_size: u64,
    ) -> Result<TransactionDataWithProofResponse, Error> {
        let mut num_output_reductions = 0;
        while num_output_reductions <= max_num_output_reductions {
            let output_list_with_proof = self.storage.get_transaction_outputs(
                start_version,
                num_outputs_to_fetch,
                proof_version,
            )?;
            let response = TransactionDataWithProofResponse {
                transaction_data_response_type: TransactionDataResponseType::TransactionOutputData,
                transaction_list_with_proof: None,
                transaction_output_list_with_proof: Some(output_list_with_proof),
            };

            let (overflow_frame, num_bytes) =
                check_overflow_network_frame(&response, max_response_size)?;

            if !overflow_frame {
                return Ok(response);
            } else if num_outputs_to_fetch == 1 {
                break; // We cannot return less than a single item. Fallback to transactions
            } else {
                metrics::increment_chunk_truncation_counter(
                    metrics::TRUNCATION_FOR_SIZE,
                    DataResponse::TransactionDataWithProof(response).get_label(),
                );
                let new_num_outputs_to_fetch = num_outputs_to_fetch / 2;
                debug!("The request for {:?} outputs was too large (num bytes: {:?}, limit: {:?}). Current number of data reductions: {:?}",
                    num_outputs_to_fetch, num_bytes, max_response_size, num_output_reductions);
                num_outputs_to_fetch = new_num_outputs_to_fetch; // Try again with half the amount of data
                num_output_reductions += 1;
            }
        }

        // Return transactions only
        self.get_transactions_with_proof_by_size(
            proof_version,
            start_version,
            end_version,
            include_events,
            max_response_size,
            self.config.enable_size_and_time_aware_chunking,
        )
    }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L134-196)
```rust
    pub fn validate_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> Result<(), Error> {
        // Validate the request and time the operation
        let validate_request = || {
            // If the peer is being ignored, return an error
            if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
                if peer_state.is_ignored() {
                    return Err(Error::TooManyInvalidRequests(format!(
                        "Peer is temporarily ignored. Unable to handle request: {:?}",
                        request
                    )));
                }
            }

            // Get the latest storage server summary
            let storage_server_summary = self.cached_storage_server_summary.load();

            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }

            Ok(()) // The request is valid
        };
        utils::execute_and_time_duration(
            &metrics::STORAGE_REQUEST_VALIDATION_LATENCY,
            Some((peer_network_id, request)),
            None,
            validate_request,
            None,
        )
    }
```

**File:** config/src/config/state_sync_config.rs (L195-217)
```rust
impl Default for StorageServiceConfig {
    fn default() -> Self {
        Self {
            enable_size_and_time_aware_chunking: false,
            enable_transaction_data_v2: true,
            max_epoch_chunk_size: MAX_EPOCH_CHUNK_SIZE,
            max_invalid_requests_per_peer: 500,
            max_lru_cache_size: 500, // At ~0.6MiB per chunk, this should take no more than 0.5GiB
            max_network_channel_size: 4000,
            max_network_chunk_bytes: SERVER_MAX_MESSAGE_SIZE as u64,
            max_network_chunk_bytes_v2: SERVER_MAX_MESSAGE_SIZE_V2 as u64,
            max_num_active_subscriptions: 30,
            max_optimistic_fetch_period_ms: 5000, // 5 seconds
            max_state_chunk_size: MAX_STATE_CHUNK_SIZE,
            max_storage_read_wait_time_ms: 10_000, // 10 seconds
            max_subscription_period_ms: 30_000,    // 30 seconds
            max_transaction_chunk_size: MAX_TRANSACTION_CHUNK_SIZE,
            max_transaction_output_chunk_size: MAX_TRANSACTION_OUTPUT_CHUNK_SIZE,
            min_time_to_ignore_peers_secs: 300, // 5 minutes
            request_moderator_refresh_interval_ms: 1000, // 1 second
            storage_summary_refresh_interval_ms: 100, // Optimal for <= 10 blocks per second
        }
    }
```

**File:** config/src/config/state_sync_config.rs (L610-633)
```rust
impl ConfigOptimizer for StorageServiceConfig {
    fn optimize(
        node_config: &mut NodeConfig,
        local_config_yaml: &Value,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<bool, Error> {
        let storage_service_config = &mut node_config.state_sync.storage_service;
        let local_storage_config_yaml = &local_config_yaml["state_sync"]["storage_service"];

        // Potentially enable size and time-aware chunking for all networks except Mainnet
        let mut modified_config = false;
        if let Some(chain_id) = chain_id {
            if ENABLE_SIZE_AND_TIME_AWARE_CHUNKING
                && !chain_id.is_mainnet()
                && local_storage_config_yaml["enable_size_and_time_aware_chunking"].is_null()
            {
                storage_service_config.enable_size_and_time_aware_chunking = true;
                modified_config = true;
            }
        }

        Ok(modified_config)
    }
```
