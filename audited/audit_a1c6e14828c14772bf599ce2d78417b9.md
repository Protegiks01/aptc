# Audit Report

## Title
State Sync Notifier Race Condition Causes Consensus Liveness Failure When Both Consensus and Observer Modes Are Active

## Summary
When both consensus and consensus observer modes are enabled simultaneously, they share the same `state_sync_notifier` instance but use separate `ExecutionProxy` instances with independent synchronization locks. This allows concurrent state sync requests from both modes to race, causing the state sync driver to overwrite the first request with the second. The first caller's response callback is lost, resulting in an indefinite hang that breaks consensus liveness.

## Finding Description

The vulnerability exists in how `start_consensus()` and `start_consensus_observer()` share a single `ConsensusNotifier` instance while creating separate `ExecutionProxy` instances. [1](#0-0) 

Both functions receive clones of the same `consensus_notifier` and create separate `ExecutionProxy` instances: [2](#0-1) [3](#0-2) 

Each `ExecutionProxy` has its own `write_mutex` that only synchronizes calls within that single instance: [4](#0-3) 

When both modes call `sync_to_target()` or `sync_for_duration()` concurrently: [5](#0-4) 

Each acquires its own separate `write_mutex` lock, then both call the shared `state_sync_notifier`. The state sync driver receives both requests sequentially but can only track one active sync request at a time: [6](#0-5) 

The critical flaw occurs when the second request overwrites the first by creating a new `Arc<Mutex<...>>`: [7](#0-6) [8](#0-7) 

This complete replacement means the first request's oneshot callback is lost. When state sync completes, it responds to the second (active) request only: [9](#0-8) 

The first caller remains blocked indefinitely awaiting a response that never arrives, breaking the consensus liveness invariant.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program criteria:

1. **Validator node slowdowns/hangs**: The consensus component that issued the first sync request hangs indefinitely, waiting for a response on its orphaned oneshot channel. This prevents consensus from progressing.

2. **Consensus liveness failure**: If the full consensus engine issues a sync request first and the observer overwrites it, consensus cannot proceed, halting block production on that validator node.

3. **Significant protocol violations**: The state sync protocol assumes synchronous request-response semantics, but the race condition violates this by allowing request replacement without proper response handling.

This does not reach Critical severity as it requires both modes to be active simultaneously (not all deployments), and recovery is possible by restarting the affected node. However, it can cause extended downtime and network participation loss for affected validators.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

The vulnerability triggers under these conditions:

1. **Configuration**: Both consensus and consensus observer must be enabled, which occurs when a validator node also runs the observer for monitoring/debugging purposes
2. **Timing**: Both modes must call sync methods within a narrow time window
3. **Triggering events**: Sync requests occur during normal operation (epoch changes, catching up after network issues, explicit sync calls)

The likelihood is elevated because:
- The observer mode is increasingly deployed alongside full consensus for enhanced monitoring
- State sync requests are frequent during network instability or validator restarts
- No malicious action is required - this is a legitimate race condition in normal operation
- Once triggered, the impact is immediate and persistent until node restart

## Recommendation

Implement one of two solutions:

**Option 1 (Preferred): Single Global Sync Lock**
Wrap the `state_sync_notifier` in a synchronization primitive that prevents concurrent sync requests across all callers:

```rust
// In consensus_provider.rs
pub struct SynchronizedStateSync {
    notifier: Arc<dyn ConsensusNotificationSender>,
    sync_mutex: Arc<tokio::sync::Mutex<()>>,
}

impl ConsensusNotificationSender for SynchronizedStateSync {
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), Error> {
        let _guard = self.sync_mutex.lock().await;
        self.notifier.sync_to_target(target).await
    }
    
    async fn sync_for_duration(&self, duration: Duration) -> Result<LedgerInfoWithSignatures, Error> {
        let _guard = self.sync_mutex.lock().await;
        self.notifier.sync_for_duration(duration).await
    }
}
```

**Option 2: State Sync Request Queue**
Modify the state sync driver to queue concurrent requests instead of overwriting:

```rust
// In notification_handlers.rs
pub struct ConsensusNotificationHandler {
    consensus_listener: ConsensusNotificationListener,
    consensus_sync_requests: Arc<Mutex<VecDeque<ConsensusSyncRequest>>>, // Queue instead of single
    time_service: TimeService,
}
```

Then process requests sequentially, responding to each in order.

**Option 3: Reject Concurrent Requests**
Add validation to reject new sync requests when one is active:

```rust
pub async fn initialize_sync_target_request(...) -> Result<(), Error> {
    // Check for active request
    if self.consensus_sync_request.lock().is_some() {
        return Err(Error::ConcurrentSyncRequest(
            "Sync request already in progress".into()
        ));
    }
    // ... rest of initialization
}
```

## Proof of Concept

```rust
// Test demonstrating the race condition
// Place in: state-sync/state-sync-driver/src/tests/notification_handlers.rs

#[tokio::test]
async fn test_concurrent_sync_request_race() {
    use aptos_consensus_notifications::*;
    use aptos_types::{ledger_info::*, block_info::*, aggregate_signature::*};
    use std::sync::Arc;
    use tokio::time::{sleep, Duration};
    
    // Create consensus notifier and listener pair
    let (consensus_notifier, mut consensus_listener) = 
        new_consensus_notifier_listener_pair(5000);
    
    let notifier1 = Arc::new(consensus_notifier.clone());
    let notifier2 = Arc::new(consensus_notifier.clone());
    
    // Create two sync targets
    let target1 = LedgerInfoWithSignatures::new(
        LedgerInfo::new(BlockInfo::empty(), HashValue::zero()),
        AggregateSignature::empty(),
    );
    
    let target2 = LedgerInfoWithSignatures::new(
        LedgerInfo::new(
            BlockInfo::new(1, 0, HashValue::zero(), HashValue::zero(), 100, 0, None),
            HashValue::zero()
        ),
        AggregateSignature::empty(),
    );
    
    // Spawn first sync request (consensus)
    let notifier1_clone = notifier1.clone();
    let target1_clone = target1.clone();
    let handle1 = tokio::spawn(async move {
        notifier1_clone.sync_to_target(target1_clone).await
    });
    
    // Give first request time to be sent
    sleep(Duration::from_millis(10)).await;
    
    // Spawn second sync request (observer) - this overwrites first
    let notifier2_clone = notifier2.clone();
    let target2_clone = target2.clone();
    let handle2 = tokio::spawn(async move {
        notifier2_clone.sync_to_target(target2_clone).await
    });
    
    // Simulate state sync driver processing
    let notification1 = consensus_listener.select_next_some().await;
    let notification2 = consensus_listener.select_next_some().await;
    
    // Both notifications arrive, but only the second is stored
    // First request's callback is lost
    
    // Respond only to second notification
    if let ConsensusNotification::SyncToTarget(sync_notif) = notification2 {
        consensus_listener
            .respond_to_sync_target_notification(sync_notif, Ok(()))
            .unwrap();
    }
    
    // Second request completes successfully
    let result2 = tokio::time::timeout(Duration::from_secs(1), handle2).await;
    assert!(result2.is_ok(), "Second request should complete");
    
    // First request hangs indefinitely (will timeout)
    let result1 = tokio::time::timeout(Duration::from_secs(2), handle1).await;
    assert!(result1.is_err(), "First request should timeout - demonstrating the bug");
}
```

## Notes

This vulnerability is particularly insidious because:

1. **Silent failure**: The first caller hangs without error logs, making debugging difficult
2. **Mode confusion**: Responses intended for one mode may be received by another
3. **Production impact**: Affects validators running enhanced monitoring configurations
4. **No easy workaround**: Requires code changes to fix properly

The root cause is architectural: the assumption that sync requests are serialized within a single caller, which breaks when multiple independent callers share the same notifier. The fix must enforce global serialization or proper request queueing at either the caller or receiver side.

### Citations

**File:** aptos-node/src/lib.rs (L831-850)
```rust
        consensus::create_consensus_observer_and_publisher(
            &node_config,
            consensus_observer_network_interfaces,
            consensus_notifier.clone(),
            consensus_to_mempool_sender.clone(),
            db_rw.clone(),
            consensus_observer_reconfig_subscription,
        );

    // Create the consensus runtime (if enabled)
    let consensus_runtime = consensus::create_consensus_runtime(
        &node_config,
        db_rw.clone(),
        consensus_reconfig_subscription,
        consensus_network_interfaces,
        consensus_notifier.clone(),
        consensus_to_mempool_sender.clone(),
        vtxn_pool,
        consensus_publisher.clone(),
        &mut admin_service,
```

**File:** consensus/src/consensus_provider.rs (L65-72)
```rust
    let execution_proxy = ExecutionProxy::new(
        Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(aptos_db)),
        txn_notifier,
        state_sync_notifier,
        node_config.transaction_filters.execution_filter.clone(),
        node_config.consensus.enable_pre_commit,
        None,
    );
```

**File:** consensus/src/consensus_provider.rs (L158-165)
```rust
        let execution_proxy = ExecutionProxy::new(
            Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(aptos_db.clone())),
            txn_notifier,
            state_sync_notifier,
            node_config.transaction_filters.execution_filter.clone(),
            node_config.consensus.enable_pre_commit,
            None,
        );
```

**File:** consensus/src/state_computer.rs (L54-84)
```rust
pub struct ExecutionProxy {
    executor: Arc<dyn BlockExecutorTrait>,
    txn_notifier: Arc<dyn TxnNotifier>,
    state_sync_notifier: Arc<dyn ConsensusNotificationSender>,
    write_mutex: AsyncMutex<LogicalTime>,
    txn_filter_config: Arc<BlockTransactionFilterConfig>,
    state: RwLock<Option<MutableState>>,
    enable_pre_commit: bool,
    secret_share_config: Option<SecretShareConfig>,
}

impl ExecutionProxy {
    pub fn new(
        executor: Arc<dyn BlockExecutorTrait>,
        txn_notifier: Arc<dyn TxnNotifier>,
        state_sync_notifier: Arc<dyn ConsensusNotificationSender>,
        txn_filter_config: BlockTransactionFilterConfig,
        enable_pre_commit: bool,
        secret_share_config: Option<SecretShareConfig>,
    ) -> Self {
        Self {
            executor,
            txn_notifier,
            state_sync_notifier,
            write_mutex: AsyncMutex::new(LogicalTime::new(0, 0)),
            txn_filter_config: Arc::new(txn_filter_config),
            state: RwLock::new(None),
            enable_pre_commit,
            secret_share_config,
        }
    }
```

**File:** consensus/src/state_computer.rs (L177-233)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // The pipeline phase already committed beyond the target block timestamp, just return.
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }

        // This is to update QuorumStore with the latest known commit in the system,
        // so it can set batches expiration accordingly.
        // Might be none if called in the recovery path, or between epoch stop and start.
        if let Some(inner) = self.state.read().as_ref() {
            let block_timestamp = target.commit_info().timestamp_usecs();
            inner
                .payload_manager
                .notify_commit(block_timestamp, Vec::new());
        }

        // Inject an error for fail point testing
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Invoke state sync to synchronize to the specified target. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );

        // Update the latest logical time
        *latest_logical_time = target_logical_time;

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L212-217)
```rust
pub struct ConsensusNotificationHandler {
    // The listener for notifications from consensus
    consensus_listener: ConsensusNotificationListener,

    // The latest consensus sync request that has been received
    consensus_sync_request: Arc<Mutex<Option<ConsensusSyncRequest>>>,
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L253-258)
```rust
        // Save the request so we can notify consensus once we've hit the duration
        let consensus_sync_request =
            ConsensusSyncRequest::new_with_duration(start_time, sync_duration_notification);
        self.consensus_sync_request = Arc::new(Mutex::new(Some(consensus_sync_request)));

        Ok(())
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L312-318)
```rust
        // Save the request so we can notify consensus once we've hit the target
        let consensus_sync_request =
            ConsensusSyncRequest::new_with_target(sync_target_notification);
        self.consensus_sync_request = Arc::new(Mutex::new(Some(consensus_sync_request)));

        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L320-365)
```rust
    /// Notifies consensus of a satisfied sync request, and removes the active request.
    /// Note: this assumes that the sync request has already been checked for satisfaction.
    pub async fn handle_satisfied_sync_request(
        &mut self,
        latest_synced_ledger_info: LedgerInfoWithSignatures,
    ) -> Result<(), Error> {
        // Remove the active sync request
        let mut sync_request_lock = self.consensus_sync_request.lock();
        let consensus_sync_request = sync_request_lock.take();

        // Notify consensus of the satisfied request
        match consensus_sync_request {
            Some(ConsensusSyncRequest::SyncDuration(_, sync_duration_notification)) => {
                self.respond_to_sync_duration_notification(
                    sync_duration_notification,
                    Ok(()),
                    Some(latest_synced_ledger_info),
                )?;
            },
            Some(ConsensusSyncRequest::SyncTarget(sync_target_notification)) => {
                // Get the sync target version and latest synced version
                let sync_target = sync_target_notification.get_target();
                let sync_target_version = sync_target.ledger_info().version();
                let latest_synced_version = latest_synced_ledger_info.ledger_info().version();

                // Check if we've synced beyond the target. If so, notify consensus with an error.
                if latest_synced_version > sync_target_version {
                    let error = Err(Error::SyncedBeyondTarget(
                        latest_synced_version,
                        sync_target_version,
                    ));
                    self.respond_to_sync_target_notification(
                        sync_target_notification,
                        error.clone(),
                    )?;
                    return error;
                }

                // Otherwise, notify consensus that the target has been reached
                self.respond_to_sync_target_notification(sync_target_notification, Ok(()))?;
            },
            None => { /* Nothing needs to be done */ },
        }

        Ok(())
    }
```
