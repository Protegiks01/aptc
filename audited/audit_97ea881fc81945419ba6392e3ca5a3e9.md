# Audit Report

## Title
Snapshot Validation Bypass in Restore Mode with replay_all Flag Allows State Inconsistency via Metadata Manipulation

## Summary
The `RestoreCoordinator::run_impl()` function in `storage/backup/backup-cli/src/coordinators/restore.rs` has a critical inconsistency between Verify and Restore modes when `replay_all_mode` is enabled. In Verify mode, snapshot validation always occurs, but in Restore mode with a non-empty database, snapshot validation is completely bypassed. This allows an attacker who can manipulate backup metadata to cause nodes to restore from incorrect version numbers, leading to transaction skipping and state inconsistencies.

## Finding Description

The vulnerability exists in the mode-specific code paths at lines 314-338 of the restore coordinator: [1](#0-0) 

**Root Cause Analysis:**

The `RestoreRunMode` enum has two variants with different behavior: [2](#0-1) 

When determining the database's next expected version, the modes behave differently: [3](#0-2) 

In **Verify mode**, `db_next_version` is always 0, causing the code to always select `StateSnapshotRestoreMode::Default` at line 325 and execute snapshot validation. In **Restore mode** with `db_next_version > 0` and `replay_all_mode = true`, `restore_mode_opt` is set to `None` at line 320, causing the entire snapshot validation block (lines 328-346) to be skipped.

**Critical Validation Bypass:**

When `StateSnapshotRestoreController` is skipped, the following security-critical validations are bypassed: [4](#0-3) 

These validations ensure:
1. Transaction info proofs are valid
2. Root hash matches the expected state checkpoint hash
3. LedgerInfo signatures verify correctly against epoch history

**Exploitation Path:**

Despite skipping snapshot restoration, the system still uses `tree_snapshot.version` to determine the transaction replay starting point: [5](#0-4) 

The tree_snapshot is selected from backup metadata: [6](#0-5) 

**Attack Scenario:**

1. Attacker gains write access to backup storage or provides a malicious backup to a node operator
2. Attacker modifies backup metadata to claim a state snapshot exists at version N (e.g., 1,000,000), where N is higher than the actual latest snapshot
3. Attacker either removes the actual snapshot file or provides a corrupted/mismatched one
4. Node operator runs restore with `--replay-all` flag (common for bootstrap scenarios)
5. In **Verify mode**: The system would load the snapshot manifest, attempt validation, and fail due to proof mismatch or missing file
6. In **Restore mode**: The system skips validation entirely, accepts version N from metadata, and starts replaying transactions from version N+1
7. All transactions from version 0 to N are skipped, causing the node to have incorrect state
8. Multiple nodes restored from this backup would have identical (but wrong) state, potentially causing a consensus split from honest nodes

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per Aptos bug bounty criteria:

**State Inconsistency Requiring Intervention:**
- Nodes restored from manipulated backups will have incorrect state roots
- When these nodes attempt to sync with the network, they will detect mismatches and fail to reach consensus
- This breaks the **Deterministic Execution** invariant - nodes processing different transaction ranges produce different states
- This breaks the **State Consistency** invariant - state transitions cannot be verified via Merkle proofs because the base state is wrong

**Potential for Consensus Split:**
- If multiple nodes restore from the same malicious backup, they form a cohort with identical (wrong) state
- These nodes may accept each other's state proofs but reject honest nodes' proofs
- In extreme cases, this could cause a temporary network partition requiring manual intervention

**Limited but Real Attack Surface:**
- Requires attacker access to backup storage OR ability to convince operators to use malicious backups
- Attack is detectable if verification is run, but verification is optional per the CLI design
- Impact is contained to nodes using the malicious backup, not the entire network

## Likelihood Explanation

**Moderate Likelihood:**

**Attacker Requirements:**
- Write access to backup storage (cloud buckets, file systems) OR social engineering to provide backup to operators
- Knowledge of backup metadata format
- Ability to modify metadata files without detection

**Realistic Scenarios:**
1. **Compromised Backup Storage**: If backup storage credentials are leaked or storage is misconfigured, attackers can modify metadata
2. **Supply Chain Attack**: Malicious actor provides "helpful" backup during mainnet issues, embedding manipulated metadata
3. **Insider Threat**: Malicious infrastructure operator manipulates backups for specific nodes

**Mitigating Factors:**
- Verify mode catches the issue if used
- Operators may notice state mismatch when syncing with network
- Some backup systems have integrity checks or versioning

**Aggravating Factors:**
- `--replay-all` is common for bootstrap scenarios (new nodes, disaster recovery)
- Verification is a separate step that operators might skip due to time pressure
- Error messages from skipped validation would not clearly indicate security issue

## Recommendation

**Primary Fix: Enforce Consistent Validation**

Ensure snapshot validation occurs in both Restore and Verify modes when snapshot metadata is used for replay versioning. Modify lines 314-338 to validate the snapshot even when `replay_all_mode` is set:

```rust
// phase 2.a: if the tree is not completed, we directly restore from the latest snapshot before target
if !tree_completed {
    // For boostrap DB to latest version, we want to use default mode
    let restore_mode_opt = if db_next_version > 0 {
        if replay_all_mode {
            // SECURITY FIX: Even in replay_all mode, we must validate the snapshot
            // metadata if we're going to use tree_snapshot.version for replay
            // Use Verify mode (no actual restoration) but ensure validation runs
            if self.global_opt.run_mode.is_verify() {
                Some(StateSnapshotRestoreMode::Default)
            } else {
                // In Restore mode, validate metadata without restoring data
                // by running verification-only restoration
                Some(StateSnapshotRestoreMode::TreeOnly)
            }
        } else {
            Some(StateSnapshotRestoreMode::TreeOnly)
        }
    } else {
        Some(StateSnapshotRestoreMode::Default)
    };

    if let Some(restore_mode) = restore_mode_opt {
        info!(
            "Start restoring tree snapshot at {} with db_next_version {}",
            tree_snapshot.version, db_next_version
        );
        StateSnapshotRestoreController::new(
            StateSnapshotRestoreOpt {
                manifest_handle: tree_snapshot.manifest.clone(),
                version: tree_snapshot.version,
                validate_modules: false,
                restore_mode,
            },
            self.global_opt.clone(),
            Arc::clone(&self.storage),
            epoch_history.clone(),
        )
        .run()
        .await?;
    }
    
    // ... rest of code
}
```

**Alternative Fix: Require Explicit Verification**

Add a safety check that prevents using snapshot version for replay without validation:

```rust
// After line 196 where tree_snapshot is selected:
if replay_all_mode && self.global_opt.run_mode.name() == "restore" && db_next_version > 0 {
    // In replay_all mode with existing DB, we're not validating the snapshot
    // but we're using its version. Require explicit confirmation.
    warn!(
        "replay_all_mode is set but snapshot validation will be skipped. \
         This could lead to state inconsistency if backup metadata is manipulated. \
         Consider running verification first."
    );
    
    // Optionally: fail unless --skip-snapshot-validation flag is set
    if !self.skip_snapshot_validation_check {
        bail!(
            "Using unvalidated snapshot version {} for replay. \
             Run with --verify first or use --skip-snapshot-validation-check (unsafe)",
            tree_snapshot.version
        );
    }
}
```

**Defense in Depth:**

1. Add cryptographic signatures to backup metadata files
2. Implement metadata integrity checks during restore
3. Add warnings when Restore mode is used without prior Verify
4. Log clear warnings when snapshot validation is skipped but version is used

## Proof of Concept

```rust
// Test demonstrating the vulnerability
// File: storage/backup/backup-cli/src/coordinators/restore_test.rs

#[tokio::test]
async fn test_replay_all_skips_snapshot_validation_in_restore_mode() {
    // Setup: Create malicious backup with manipulated metadata
    let temp_dir = TempPath::new();
    let storage = Arc::new(MockBackupStorage::new());
    
    // Create metadata claiming snapshot at version 1000
    let fake_snapshot_meta = StateSnapshotBackupMeta {
        version: 1000,
        manifest: FileHandle::new("fake_snapshot_manifest"),
        // ... other fields
    };
    storage.add_metadata(fake_snapshot_meta);
    
    // Add transaction backups from version 1001 onwards
    // (skipping versions 0-1000)
    for v in 1001..2000 {
        storage.add_transaction_backup(v, /* ... */);
    }
    
    // Scenario 1: Verify mode - should FAIL when trying to validate fake snapshot
    let verify_opt = GlobalRestoreOpt {
        dry_run: true,  // Verify mode
        target_version: Some(2000),
        replay_all: true,
        // ... other options
    };
    
    let verify_result = RestoreCoordinator::new(
        restore_opt,
        verify_opt.try_into().unwrap(),
        storage.clone(),
    )
    .run()
    .await;
    
    // Verify mode should fail due to missing/invalid snapshot
    assert!(verify_result.is_err());
    assert!(verify_result.unwrap_err().to_string().contains("snapshot"));
    
    // Scenario 2: Restore mode - SUCCEEDS without validation (VULNERABILITY)
    let restore_opt = GlobalRestoreOpt {
        dry_run: false,
        db_dir: Some(temp_dir.path().to_path_buf()),
        target_version: Some(2000),
        replay_all: true,
        // ... other options
    };
    
    // Create DB with some existing data (db_next_version > 0)
    let db = AptosDB::open_kv_only(/* ... */);
    // Simulate db_next_version = 100
    db.save_transactions(/* transactions 0-99 */, /* ... */).unwrap();
    
    let restore_result = RestoreCoordinator::new(
        restore_opt,
        restore_opt.try_into().unwrap(),
        storage.clone(),
    )
    .run()
    .await;
    
    // VULNERABILITY: Restore succeeds without validating snapshot
    assert!(restore_result.is_ok());
    
    // Verify the node skipped transactions 100-1000
    let db = AptosDB::new_for_test(&temp_dir);
    assert_eq!(db.get_latest_version().unwrap(), 1999);
    
    // Transactions 100-1000 are missing - state is inconsistent!
    for v in 100..=1000 {
        assert!(db.get_transaction(v).unwrap().is_none());
    }
    
    println!("VULNERABILITY DEMONSTRATED: Restore mode accepted fake snapshot version");
    println!("without validation, causing {} transactions to be skipped", 901);
}
```

**Notes:**

1. The vulnerability is exploitable only when attackers can manipulate backup metadata (either through compromised storage or malicious backup provision)

2. The divergence between Verify and Restore modes creates an inconsistency where verification doesn't match actual behavior - a classic validation bypass pattern

3. The impact is medium rather than critical because:
   - Network-wide impact requires multiple nodes to use the malicious backup
   - Honest nodes will detect the inconsistency during sync
   - The issue is recoverable by restoring from a clean backup

4. The fix should ensure snapshot metadata validation occurs regardless of mode when that metadata is used for critical decisions like replay versioning

### Citations

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L183-196)
```rust
        let tree_snapshot = if let Some((latest_tree_version, _)) = latest_tree_version {
            let snapshot = metadata_view.select_state_snapshot(latest_tree_version)?;

            ensure!(
                snapshot.is_some() && snapshot.as_ref().unwrap().version == latest_tree_version,
                "cannot find tree snapshot {}",
                latest_tree_version
            );
            snapshot.unwrap()
        } else {
            metadata_view
                .select_state_snapshot(target_version)?
                .expect("Cannot find tree snapshot before target version")
        };
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L314-338)
```rust
            );
            // phase 2.a: if the tree is not completed, we directly restore from the latest snapshot before target
            if !tree_completed {
                // For boostrap DB to latest version, we want to use default mode
                let restore_mode_opt = if db_next_version > 0 {
                    if replay_all_mode {
                        None // the restore should already been done in the replay_all mode
                    } else {
                        Some(StateSnapshotRestoreMode::TreeOnly)
                    }
                } else {
                    Some(StateSnapshotRestoreMode::Default)
                };

                if let Some(restore_mode) = restore_mode_opt {
                    info!(
                        "Start restoring tree snapshot at {} with db_next_version {}",
                        tree_snapshot.version, db_next_version
                    );
                    StateSnapshotRestoreController::new(
                        StateSnapshotRestoreOpt {
                            manifest_handle: tree_snapshot.manifest.clone(),
                            version: tree_snapshot.version,
                            validate_modules: false,
                            restore_mode,
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L348-351)
```rust
                replay_version = Some((
                    tree_snapshot.version + 1,
                    false, /*replay entire txn including update tree and KV*/
                ));
```

**File:** storage/backup/backup-cli/src/utils/mod.rs (L169-172)
```rust
pub enum RestoreRunMode {
    Restore { restore_handler: RestoreHandler },
    Verify,
}
```

**File:** storage/backup/backup-cli/src/utils/mod.rs (L250-260)
```rust
    pub fn get_next_expected_transaction_version(&self) -> Result<Version> {
        match self {
            RestoreRunMode::Restore { restore_handler } => {
                restore_handler.get_next_expected_transaction_version()
            },
            RestoreRunMode::Verify => {
                info!("This is a dry run. Assuming resuming point at version 0.");
                Ok(0)
            },
        }
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-139)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
        if let Some(epoch_history) = self.epoch_history.as_ref() {
            epoch_history.verify_ledger_info(&li)?;
        }
```
