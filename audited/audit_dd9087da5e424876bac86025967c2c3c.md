# Audit Report

## Title
Silent Storage Commit Failure Leading to Consensus State Inconsistency

## Summary
The `wait_for_commit_ledger()` method silently ignores storage commit errors, allowing the persisting phase to report blocks as successfully committed when they actually failed to persist to storage. This violates the critical invariant that committed blocks are durably persisted, potentially causing consensus state inconsistencies and node desynchronization.

## Finding Description

The vulnerability exists in the consensus pipeline's persisting phase, where blocks are committed to storage. The error propagation chain is broken at two critical points:

**1. Error Suppression in `wait_for_commit_ledger()`:** [1](#0-0) 

The `wait_for_commit_ledger()` method explicitly discards the result of the `commit_ledger_fut` future using `let _ = fut.commit_ledger_fut.await;`. This future returns `TaskResult<CommitLedgerResult>`, which can contain errors from storage failures.

**2. Unconditional Success in Persisting Phase:** [2](#0-1) 

The `PersistingPhase::process()` method calls `wait_for_commit_ledger()` for each block but never checks for errors. It unconditionally returns `Ok(round)` on line 74, regardless of whether the actual storage commits succeeded.

**3. Storage Commit Can Fail:** [3](#0-2) 

The actual `commit_ledger` implementation can fail at multiple points (block tree checks, database writes, pruning), all returning `ExecutorResult<()>` errors. [4](#0-3) 

The storage layer's `commit_ledger` can fail during schema writes (line 107), which is the critical database persistence operation. Failures can occur due to disk full, I/O errors, or database corruption.

**4. Incorrect Consensus State Update:** [5](#0-4) 

When the BufferManager receives `Ok(round)` from the persisting phase, it unconditionally updates `highest_committed_round`, even though the actual storage commit may have failed.

**Attack Flow:**
1. Storage experiences a transient or persistent failure (disk full, I/O error, corruption)
2. The `commit_ledger` future completes with an error
3. `wait_for_commit_ledger()` discards the error with `let _`
4. Persisting phase returns `Ok(round)`, claiming success
5. BufferManager updates `highest_committed_round` to indicate blocks are committed
6. Consensus believes blocks are committed and rejects future commit proofs for those rounds as "already committed"
7. But the blocks are NOT actually in the database
8. Node has diverged from actual storage state

**Invariant Violation:**
This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." The system reports blocks as committed when they are not durably persisted, violating the fundamental guarantee that committed data is available in storage.

## Impact Explanation

**Severity: HIGH (per Aptos Bug Bounty criteria: "State inconsistencies requiring intervention")**

**Impact Categories:**

1. **Consensus State Divergence:** Nodes with failed commits believe they have advanced further than their actual storage state. This causes them to:
   - Reject valid commit proofs as "too old" [6](#0-5) 
   - Ignore commit votes for rounds they claim to have committed [7](#0-6) 

2. **Data Loss on Node Restart:** If a node restarts before the missing blocks are recovered:
   - The sync mechanism detects the mismatch [8](#0-7) 
   - If the difference exceeds 1,000,000 versions, the node crashes on startup [9](#0-8) 
   - Otherwise, it truncates back to actual committed state, losing the "committed" rounds

3. **Client Query Inconsistencies:** During the window between failed commit and node restart, clients querying this node may receive inconsistent responses compared to other validators.

4. **State Sync Failures:** Other nodes may request data from this validator for rounds it claims to have committed but doesn't actually have in storage.

This does not directly meet CRITICAL severity (no fund loss, no permanent network partition) but qualifies as **HIGH** due to significant protocol violations and state inconsistencies requiring manual intervention.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

Storage failures are relatively common in production environments:
- **Disk Full:** Especially on nodes with aggressive transaction processing
- **I/O Errors:** Hardware failures, network storage issues
- **Database Corruption:** Power failures, filesystem issues
- **Resource Contention:** High load causing write failures

The vulnerability triggers automatically whenever:
1. A storage commit operation fails (no attacker action needed)
2. The failure is transient enough that the node continues operating
3. The node doesn't restart immediately

**Exploitation Requirements:**
- No privileged access required
- No malicious validator collusion needed
- Can occur naturally through operational issues
- Can be accelerated by resource exhaustion attacks (filling disk space)

**Mitigation Factors:**
- Errors are logged (though only as warnings) [10](#0-9) 
- Node restart triggers reconciliation
- Most storage operations succeed under normal conditions

However, the silent failure mode makes detection difficult until significant divergence occurs.

## Recommendation

**Fix 1: Propagate Errors from `wait_for_commit_ledger()`**

Modify `wait_for_commit_ledger()` to return the result instead of discarding it:

```rust
// In consensus/consensus-types/src/pipelined_block.rs
pub async fn wait_for_commit_ledger(&self) -> TaskResult<CommitLedgerResult> {
    // may be aborted (e.g. by reset)
    if let Some(fut) = self.pipeline_futs() {
        fut.commit_ledger_fut.await
    } else {
        Err(TaskError::InternalError(Arc::new(anyhow::anyhow!(
            "Pipeline aborted before commit"
        ))))
    }
}
```

**Fix 2: Handle Errors in Persisting Phase**

```rust
// In consensus/src/pipeline/persisting_phase.rs
async fn process(&self, req: PersistingRequest) -> PersistingResponse {
    let PersistingRequest {
        blocks,
        commit_ledger_info,
    } = req;

    for b in &blocks {
        if let Some(tx) = b.pipeline_tx().lock().as_mut() {
            tx.commit_proof_tx
                .take()
                .map(|tx| tx.send(commit_ledger_info.clone()));
        }
        // Propagate the error instead of ignoring it
        b.wait_for_commit_ledger().await.map_err(|e| {
            ExecutorError::InternalError {
                error: format!("Commit ledger failed: {}", e),
            }
        })?;
    }

    let response = Ok(blocks.last().expect("Blocks can't be empty").round());
    if commit_ledger_info.ledger_info().ends_epoch() {
        self.commit_msg_tx
            .send_epoch_change(EpochChangeProof::new(vec![commit_ledger_info], false))
            .await;
    }
    response
}
```

**Fix 3: Handle Errors in BufferManager**

```rust
// In consensus/src/pipeline/buffer_manager.rs
// Update the pattern match to handle errors
Some(result) = self.persisting_phase_rx.next() => {
    match result {
        Ok(round) => {
            self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
            self.highest_committed_round = round;
            self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
        },
        Err(e) => {
            error!("Persisting phase failed: {}, triggering recovery", e);
            // Trigger recovery mechanism or panic to force restart
            panic!("Critical: commit persisting failed: {}", e);
        }
    }
},
```

## Proof of Concept

```rust
// Rust test to demonstrate the vulnerability
#[tokio::test]
async fn test_silent_commit_failure() {
    // Setup: Create a mock executor that fails on commit_ledger
    struct FailingExecutor;
    
    #[async_trait::async_trait]
    impl BlockExecutorTrait for FailingExecutor {
        fn commit_ledger(&self, _: LedgerInfoWithSignatures) -> ExecutorResult<()> {
            // Simulate storage failure
            Err(ExecutorError::InternalError {
                error: "Simulated disk full error".to_string(),
            })
        }
        // ... other required methods
    }
    
    // Create a PipelinedBlock with the failing executor
    let block = create_test_block_with_executor(Arc::new(FailingExecutor));
    
    // Send commit proof to trigger commit_ledger
    send_commit_proof_to_block(&block, create_test_commit_proof());
    
    // Call wait_for_commit_ledger - it should return error but currently ignores it
    block.wait_for_commit_ledger().await;
    // Currently: No error returned, execution continues
    // Expected: Should return Err(TaskError::InternalError(...))
    
    // Create persisting request
    let request = PersistingRequest {
        blocks: vec![Arc::new(block)],
        commit_ledger_info: create_test_ledger_info(),
    };
    
    // Process through persisting phase
    let persisting_phase = PersistingPhase::new(Arc::new(mock_network_sender()));
    let result = persisting_phase.process(request).await;
    
    // VULNERABILITY: Result is Ok even though commit failed!
    assert!(result.is_ok()); // This passes but shouldn't
    
    // Expected behavior: result should be Err
    // assert!(result.is_err()); // This should be the correct assertion
    
    // Verify that storage does NOT contain the committed block
    let storage_version = get_committed_version_from_storage();
    let claimed_version = result.unwrap(); // Claims this round is committed
    
    // VULNERABILITY DEMONSTRATED: Claimed version > actual storage version
    assert!(claimed_version > storage_version);
}
```

## Notes

The vulnerability has existed since the introduction of the decoupled execution pipeline. While errors are logged via the `monitor` function's `wait_and_log_error`, this runs asynchronously and independently from the persisting phase's success/failure reporting, providing observability but not correctness.

The issue is particularly insidious because:
1. It appears intentional (the comment "this may be cancelled" suggests deliberate error suppression)
2. Most commits succeed, making the bug rare in testing
3. Recovery mechanisms exist (restart reconciliation) but don't prevent the runtime inconsistency
4. The async monitoring provides a false sense of error handling

### Citations

**File:** consensus/consensus-types/src/pipelined_block.rs (L562-568)
```rust
    pub async fn wait_for_commit_ledger(&self) {
        // may be aborted (e.g. by reset)
        if let Some(fut) = self.pipeline_futs() {
            // this may be cancelled
            let _ = fut.commit_ledger_fut.await;
        }
    }
```

**File:** consensus/src/pipeline/persisting_phase.rs (L59-81)
```rust
    async fn process(&self, req: PersistingRequest) -> PersistingResponse {
        let PersistingRequest {
            blocks,
            commit_ledger_info,
        } = req;

        for b in &blocks {
            if let Some(tx) = b.pipeline_tx().lock().as_mut() {
                tx.commit_proof_tx
                    .take()
                    .map(|tx| tx.send(commit_ledger_info.clone()));
            }
            b.wait_for_commit_ledger().await;
        }

        let response = Ok(blocks.last().expect("Blocks can't be empty").round());
        if commit_ledger_info.ledger_info().ends_epoch() {
            self.commit_msg_tx
                .send_epoch_change(EpochChangeProof::new(vec![commit_ledger_info], false))
                .await;
        }
        response
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L362-395)
```rust
    fn commit_ledger(&self, ledger_info_with_sigs: LedgerInfoWithSignatures) -> ExecutorResult<()> {
        let _timer = OTHER_TIMERS.timer_with(&["commit_ledger"]);

        let block_id = ledger_info_with_sigs.ledger_info().consensus_block_id();
        info!(
            LogSchema::new(LogEntry::BlockExecutor).block_id(block_id),
            "commit_ledger"
        );

        // Check for any potential retries
        // TODO: do we still have such retries?
        let committed_block = self.block_tree.root_block();
        if committed_block.num_persisted_transactions()?
            == ledger_info_with_sigs.ledger_info().version() + 1
        {
            return Ok(());
        }

        // Confirm the block to be committed is tracked in the tree.
        self.block_tree.get_block(block_id)?;

        fail_point!("executor::commit_blocks", |_| {
            Err(anyhow::anyhow!("Injected error in commit_blocks.").into())
        });

        let target_version = ledger_info_with_sigs.ledger_info().version();
        self.db
            .writer
            .commit_ledger(target_version, Some(&ledger_info_with_sigs), None)?;

        self.block_tree.prune(ledger_info_with_sigs.ledger_info())?;

        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L78-112)
```rust
    fn commit_ledger(
        &self,
        version: Version,
        ledger_info_with_sigs: Option<&LedgerInfoWithSignatures>,
        chunk_opt: Option<ChunkToCommit>,
    ) -> Result<()> {
        gauged_api("commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_ledger"]);

            let old_committed_ver = self.get_and_check_commit_range(version)?;

            let mut ledger_batch = SchemaBatch::new();
            // Write down LedgerInfo if provided.
            if let Some(li) = ledger_info_with_sigs {
                self.check_and_put_ledger_info(version, li, &mut ledger_batch)?;
            }
            // Write down commit progress
            ledger_batch.put::<DbMetadataSchema>(
                &DbMetadataKey::OverallCommitProgress,
                &DbMetadataValue::Version(version),
            )?;
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;

            // Notify the pruners, invoke the indexer, and update in-memory ledger info.
            self.post_commit(old_committed_ver, version, ledger_info_with_sigs, chunk_opt)
        })
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L313-332)
```rust
        if self.highest_committed_round < round {
            info!(
                round = round,
                block_id = block_id,
                "Added pending commit proof."
            );
            self.pending_commit_proofs.insert(round, commit_proof);
            if self.pending_commit_proofs.len() == MAX_PENDING_COMMIT_PROOFS {
                let _ = self.pending_commit_proofs.pop_first();
            }
            true
        } else {
            debug!(
                round = round,
                highest_committed_round = self.highest_committed_round,
                block_id = block_id,
                "Commit proof too old, ignored."
            );
            false
        }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L340-345)
```rust
        if round <= self.highest_committed_round {
            true
        } else
        // Store the commit vote only if it is for one of the next 100 rounds.
        if round > self.highest_committed_round
            && self.highest_committed_round + self.max_pending_rounds_in_commit_vote_cache > round
```

**File:** consensus/src/pipeline/buffer_manager.rs (L968-972)
```rust
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
```

**File:** storage/aptosdb/src/state_store/mod.rs (L107-107)
```rust
pub const MAX_COMMIT_PROGRESS_DIFFERENCE: u64 = 1_000_000;
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L173-177)
```rust
async fn wait_and_log_error<T, F: Future<Output = TaskResult<T>>>(f: F, msg: String) {
    if let Err(TaskError::InternalError(e)) = f.await {
        warn!("{} failed: {}", msg, e);
    }
}
```
