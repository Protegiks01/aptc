# Audit Report

## Title
Database Corruption in State Pruner Causes Indefinite Pruning Failure and Storage Exhaustion

## Summary
The state key-value pruner fails to handle deserialization errors gracefully, causing it to abort the entire pruning operation when encountering corrupted database entries. This results in the pruner becoming permanently stuck, unable to advance its progress marker, leading to unbounded database growth and eventual node failure due to disk space exhaustion.

## Finding Description
The vulnerability exists in the state pruning subsystem that is responsible for cleaning up stale state values from AptosDB. When the pruner iterates through entries to delete, it deserializes each entry's index structure to determine what to prune. [1](#0-0) [2](#0-1) 

In both the sharded and non-sharded code paths, the `?` operator is used to propagate deserialization errors immediately. If a single entry fails to deserialize due to data corruption, the entire pruning operation aborts before updating the progress marker. [3](#0-2) 

The progress marker is only updated at the end of the function if all operations succeed. When deserialization fails, this update never occurs, causing the pruner to retry from the same starting position indefinitely. [4](#0-3) 

The pruner worker catches errors and logs them, then retries after a brief sleep. With no mechanism to skip corrupted entries, this creates an infinite loop where the pruner repeatedly attempts and fails at the same corrupted entry.

The same vulnerability exists in the shard pruner: [5](#0-4) [6](#0-5) 

**Attack Scenario:**
1. Database corruption occurs (disk failure, software bug, unclean shutdown, or schema migration issue)
2. A `StaleStateValueIndex` or `StaleStateValueIndexByKeyHashSchema` entry becomes corrupted at version V
3. Pruner attempts to prune from current_progress (e.g., version 1000) to target_version (e.g., version 10000)
4. During iteration, the pruner encounters the corrupted entry at version V
5. Deserialization fails, error propagates up via `?` operator
6. Progress marker remains at version 1000 (not updated)
7. Pruner worker catches error, logs it, sleeps briefly, then retries
8. On retry, pruner starts from version 1000 again
9. Process repeats indefinitely - pruner can never advance past version V
10. Database continues growing as new state values are added but old ones are never pruned
11. Disk space exhausts, node stops functioning

This breaks the **Resource Limits** invariant that "all operations must respect gas, storage, and computational limits" and compromises node availability.

## Impact Explanation
This vulnerability qualifies as **Medium Severity** under the Aptos Bug Bounty criteria: "State inconsistencies requiring intervention."

**Impact:**
- **Storage Exhaustion**: Unpruned state values accumulate indefinitely, causing the database to grow without bounds
- **Disk Space DoS**: Eventually the disk fills completely, causing the node to stop functioning
- **Manual Intervention Required**: Node operators must manually identify and remove corrupted entries or restore from backup
- **Potential Network-Wide Impact**: If corruption occurs due to a widespread software bug affecting multiple nodes, many validators could simultaneously experience pruning failures

While this doesn't directly cause fund loss or consensus violations, it creates a **state inconsistency requiring manual intervention** and leads to **node unavailability**, which are both Medium severity impacts per the bug bounty program.

## Likelihood Explanation
**Likelihood: Medium to High**

Database corruption can occur through several realistic scenarios:

1. **Hardware Failures**: Disk failures, memory corruption, or power loss during writes
2. **Software Bugs**: Bugs in the write path, schema encoding/decoding, or migration logic
3. **Unclean Shutdowns**: Node crashes during database writes leaving partially written entries
4. **Schema Migrations**: Errors during database schema upgrades [7](#0-6) 

The deserialization logic can fail if:
- Data is shorter than expected (< 16 bytes)
- StateKey decoding fails due to corrupted key data
- Invalid state key tags or malformed BCS encoding

RocksDB is configured with paranoid checks enabled, but this only detects corruption at the RocksDB level, not at the application schema level.

## Recommendation
Implement error recovery logic that allows the pruner to skip corrupted entries and continue making progress. The fix should:

1. **Catch deserialization errors** instead of propagating them immediately
2. **Log corrupted entries** with full details for manual investigation
3. **Continue iteration** past corrupted entries
4. **Update progress marker** based on successfully processed entries
5. **Emit metrics** for corruption detection monitoring

**Recommended Fix:**

```rust
pub(in crate::pruner) fn prune(
    &self,
    current_progress: Version,
    target_version: Version,
) -> Result<()> {
    let mut batch = SchemaBatch::new();
    let mut last_successful_version = current_progress;
    let mut corruption_count = 0;

    if self.state_kv_db.enabled_sharding() {
        // ... sharded path with error handling
    } else {
        let mut iter = self
            .state_kv_db
            .metadata_db()
            .iter::<StaleStateValueIndexSchema>()?;
        iter.seek(&current_progress)?;
        
        for item in iter {
            // Handle deserialization errors gracefully
            let (index, _) = match item {
                Ok(data) => data,
                Err(e) => {
                    corruption_count += 1;
                    sample!(
                        SampleRate::Duration(Duration::from_secs(60)),
                        error!(
                            error = ?e,
                            "Failed to deserialize StaleStateValueIndex during pruning, skipping corrupted entry"
                        )
                    );
                    // Skip this entry and continue
                    continue;
                }
            };
            
            if index.stale_since_version > target_version {
                break;
            }
            
            last_successful_version = index.stale_since_version;
            batch.delete::<StaleStateValueIndexSchema>(&index)?;
            batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
        }
    }

    // Update progress to the last successfully processed version
    // This allows forward progress even with some corruption
    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::StateKvPrunerProgress,
        &DbMetadataValue::Version(last_successful_version.max(current_progress)),
    )?;

    if corruption_count > 0 {
        warn!(
            corruption_count = corruption_count,
            last_successful_version = last_successful_version,
            "Encountered corrupted entries during state pruning"
        );
        // Emit metric for monitoring
        PRUNER_CORRUPTION_DETECTED.inc_by(corruption_count);
    }

    self.state_kv_db.metadata_db().write_schemas(batch)
}
```

The same pattern should be applied to the shard pruner.

Additionally, consider implementing:
- A corruption detection metric for monitoring
- A manual repair tool to identify and remove corrupted entries
- Database integrity checks at node startup
- Automated backup/restore procedures for recovery

## Proof of Concept

```rust
#[cfg(test)]
mod corruption_test {
    use super::*;
    use aptos_temppath::TempPath;
    use aptos_schemadb::DB;
    use aptos_types::state_store::state_key::StateKey;
    
    #[test]
    fn test_pruner_stuck_on_corruption() {
        // Create a test database
        let tmpdir = TempPath::new();
        let db = DB::open(
            tmpdir.path(),
            "test_db",
            vec![STALE_STATE_VALUE_INDEX_CF_NAME],
            &Default::default(),
        ).unwrap();
        
        // Insert valid entries at versions 100-105
        let mut batch = SchemaBatch::new();
        for v in 100..=105 {
            let index = StaleStateValueIndex {
                stale_since_version: v,
                version: v - 1,
                state_key: StateKey::raw(b"test_key"),
            };
            batch.put::<StaleStateValueIndexSchema>(&index, &()).unwrap();
        }
        db.write_schemas(batch).unwrap();
        
        // Manually inject corrupted entry at version 103
        // Write invalid bytes that will fail deserialization
        let corrupted_key = {
            let mut key = vec![];
            key.write_u64::<BigEndian>(103).unwrap(); // stale_since_version
            key.write_u64::<BigEndian>(102).unwrap(); // version
            key.push(255); // Invalid state key tag
            key
        };
        db.put_raw(STALE_STATE_VALUE_INDEX_CF_NAME, &corrupted_key, &[]).unwrap();
        
        // Create pruner and attempt to prune
        let state_kv_db = Arc::new(StateKvDb::new(Arc::new(db)));
        let pruner = StateKvMetadataPruner::new(state_kv_db);
        
        // First prune attempt - should fail at corrupted entry
        let result = pruner.prune(100, 110);
        assert!(result.is_err());
        
        // Progress should not have advanced past the corrupted entry
        let progress = pruner.progress().unwrap();
        assert_eq!(progress, 0); // Still at initial state, no progress made
        
        // Second attempt - still fails, demonstrating it's stuck
        let result2 = pruner.prune(100, 110);
        assert!(result2.is_err());
        
        // Pruner cannot make any progress indefinitely
        // In production, this would cause the database to grow unbounded
    }
}
```

## Notes

This vulnerability demonstrates a common resilience issue in database maintenance operations: the lack of graceful degradation when encountering data corruption. While corruption is hopefully rare in production, the consequences of this bug are severe enough that defensive programming practices should be employed.

The recommended fix allows the pruner to make forward progress while logging corrupted entries for investigation, preventing the catastrophic failure mode of unbounded storage growth. Node operators should also implement monitoring for pruner corruption metrics to detect and address issues proactively.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L44-49)
```rust
                for item in iter {
                    let (index, _) = item?;
                    if index.stale_since_version > target_version {
                        break;
                    }
                }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L57-64)
```rust
            for item in iter {
                let (index, _) = item?;
                if index.stale_since_version > target_version {
                    break;
                }
                batch.delete::<StaleStateValueIndexSchema>(&index)?;
                batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
            }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L67-72)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        self.state_kv_db.metadata_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-68)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L58-65)
```rust
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L66-71)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
```

**File:** storage/aptosdb/src/schema/stale_state_value_index/mod.rs (L51-64)
```rust
    fn decode_key(data: &[u8]) -> Result<Self> {
        const VERSION_SIZE: usize = size_of::<Version>();

        ensure_slice_len_gt(data, 2 * VERSION_SIZE)?;
        let stale_since_version = (&data[..VERSION_SIZE]).read_u64::<BigEndian>()?;
        let version = (&data[VERSION_SIZE..2 * VERSION_SIZE]).read_u64::<BigEndian>()?;
        let state_key = StateKey::decode(&data[2 * VERSION_SIZE..])?;

        Ok(Self {
            stale_since_version,
            version,
            state_key,
        })
    }
```
