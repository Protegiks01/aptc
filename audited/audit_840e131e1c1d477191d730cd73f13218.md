# Audit Report

## Title
Missing Shard ID Bounds Validation in Sharded Block Executor Causes Denial of Service

## Summary
The `ShardedExecutorService::new()` function accepts `shard_id` and `num_shards` parameters without validating the invariant that `shard_id < num_shards`. Additionally, cross-shard message routing performs unchecked array indexing using shard IDs from transaction dependencies. This allows mismatched parameters or malicious dependencies to cause executor shard crashes via out-of-bounds panics, resulting in denial of service.

## Finding Description
The sharded block executor system lacks critical bounds validation at multiple points:

**1. Missing Initialization Validation:** [1](#0-0) 

The `new()` function stores `shard_id` and `num_shards` without validating that `shard_id < num_shards`. This violates the fundamental invariant that shard IDs must be within the valid range `[0, num_shards)`.

**2. Unchecked Cross-Shard Routing:** [2](#0-1) 

The `LocalCrossShardClient` performs direct array indexing without bounds checking. The `message_txs` array is sized based on `num_shards` during initialization, but incoming shard IDs are never validated against this size. [3](#0-2) 

Similarly, `RemoteCrossShardClient` indexes into `message_txs` based on `shard_addresses.len()` without validating the incoming `shard_id` parameter.

**3. Vulnerable Code Path:** [4](#0-3) 

When transactions commit, the `CrossShardCommitSender` extracts dependent shard IDs from transaction metadata and sends cross-shard messages. Line 125-129 calls `send_cross_shard_msg()` with `dependent_shard_id` extracted from the transaction's `dependent_edges`, which are never validated.

**4. Self-Message Vulnerability:** [5](#0-4) 

After sub-block execution, the service sends a `StopMsg` to itself using `self.shard_id`. If `shard_id >= num_shards` (and consequently >= `message_txs.len()`), this causes an immediate panic.

**Attack Scenarios:**

1. **Configuration Mismatch:** Coordinator partitions for N shards, but an executor is initialized with fewer remote shard addresses. Cross-shard messages to shard IDs >= array size cause panics.

2. **Malicious Dependencies:** A compromised coordinator injects transactions with `CrossShardDependencies` containing out-of-range shard IDs (e.g., shard 999 when only 10 shards exist).

3. **Initialization Error:** Administrator provides `shard_id=5` and `num_shards=3`, violating the invariant. The first `StopMsg` sent to itself panics.

## Impact Explanation
This vulnerability enables **Denial of Service** attacks against executor shards with High severity impact:

- **Validator Node Crashes:** Out-of-bounds array access causes thread panics, potentially crashing the entire validator node process
- **Network Liveness Degradation:** If multiple shards crash simultaneously, block execution halts, stopping the network
- **API Unavailability:** Crashed executors cannot process transactions, breaking API endpoints

Per Aptos Bug Bounty criteria, this qualifies as **High Severity** ($50,000 tier):
- Validator node slowdowns/crashes
- API crashes  
- Significant protocol violations (unvalidated external input causing crashes)

The vulnerability breaks the **Resource Limits** invariant (all operations must respect computational limits) by allowing uncontrolled panics, and the **Consensus Safety** invariant by potentially causing validator unavailability.

## Likelihood Explanation
**Likelihood: Medium to High**

The vulnerability can be triggered through:

1. **Configuration Errors (High Probability):** In distributed deployments with multiple executor shard processes, misconfiguration of `num_shards` vs actual shard count is realistic. No startup validation prevents this.

2. **Partitioner Bugs (Medium Probability):** If the block partitioner has bugs or is compromised, it could generate invalid dependencies that pass through unchecked.

3. **Malicious Coordinator (Low Probability):** Requires compromising a trusted validator operator component, but explicitly asked about in the security question.

The attack requires no special privileges beyond what a coordinator or misconfigured administrator already has. The missing validation is a fundamental oversight affecting both local and remote execution modes.

## Recommendation
Add comprehensive bounds validation at multiple defense layers:

**1. Validate at Initialization:**
```rust
pub fn new(
    shard_id: ShardId,
    num_shards: usize,
    num_threads: usize,
    coordinator_client: Arc<dyn CoordinatorClient<S>>,
    cross_shard_client: Arc<dyn CrossShardClient>,
) -> Result<Self, String> {
    if shard_id >= num_shards {
        return Err(format!(
            "Invalid shard_id: {} must be less than num_shards: {}",
            shard_id, num_shards
        ));
    }
    // ... rest of initialization
    Ok(Self { ... })
}
```

**2. Validate Cross-Shard Message Routing:**
```rust
fn send_cross_shard_msg(&self, shard_id: ShardId, round: RoundId, msg: CrossShardMsg) {
    if shard_id >= self.message_txs.len() {
        error!(
            "Invalid shard_id {} >= num_shards {}. Dropping message.",
            shard_id, self.message_txs.len()
        );
        return;
    }
    if round >= self.message_txs[shard_id].len() {
        error!("Invalid round {} for shard {}. Dropping message.", round, shard_id);
        return;
    }
    self.message_txs[shard_id][round].send(msg).unwrap();
}
```

**3. Validate Transaction Dependencies:**
Add validation in `CrossShardCommitSender::new()` to check all shard IDs in dependencies are within valid range, rejecting invalid blocks early.

**4. Configuration Validation:**
Add startup checks in `ProcessExecutorService::new()` and `ExecutorService::new()` to ensure `shard_id < remote_shard_addresses.len()` and `remote_shard_addresses.len() == num_shards`.

## Proof of Concept
```rust
#[test]
#[should_panic(expected = "index out of bounds")]
fn test_shard_id_out_of_bounds_panic() {
    use aptos_vm::sharded_block_executor::local_executor_shard::LocalExecutorService;
    use aptos_types::state_store::in_memory_state_view::InMemoryStateView;
    
    // Create executor with only 3 shards (message_txs will have length 3)
    let executor = LocalExecutorService::<InMemoryStateView>::setup_local_executor_shards(
        3,  // num_shards = 3, so valid shard_ids are 0, 1, 2
        None
    );
    
    // Now create a transaction with dependency pointing to shard 5
    let mut deps = CrossShardDependencies::default();
    deps.add_dependent_edge(
        ShardedTxnIndex::new(0, 5, 0),  // shard_id = 5 (INVALID!)
        vec![storage_location]
    );
    
    let txn_with_deps = TransactionWithDependencies::new(txn, deps);
    let sub_block = SubBlock::new(0, vec![txn_with_deps]);
    
    // Execute the block - when the transaction commits, it will try to
    // send a cross-shard message to shard 5, causing:
    // message_txs[5] panic (array only has indices 0-2)
    executor.execute_block(...);
}
```

**Notes:**
- The vulnerability exists in both local and remote execution modes with identical root cause: missing bounds validation
- Remote mode is more severe as it involves network-distributed shards where configuration mismatches are more likely
- The issue affects consensus availability since crashed executor shards cannot participate in block execution
- Defense-in-depth requires validation at initialization, message routing, and dependency injection points

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L51-74)
```rust
    pub fn new(
        shard_id: ShardId,
        num_shards: usize,
        num_threads: usize,
        coordinator_client: Arc<dyn CoordinatorClient<S>>,
        cross_shard_client: Arc<dyn CrossShardClient>,
    ) -> Self {
        let executor_thread_pool = Arc::new(
            rayon::ThreadPoolBuilder::new()
                // We need two extra threads for the cross-shard commit receiver and the thread
                // that is blocked on waiting for execute block to finish.
                .thread_name(move |i| format!("sharded-executor-shard-{}-{}", shard_id, i))
                .num_threads(num_threads + 2)
                .build()
                .unwrap(),
        );
        Self {
            shard_id,
            num_shards,
            executor_thread_pool,
            coordinator_client,
            cross_shard_client,
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L164-168)
```rust
                    cross_shard_client_clone.send_cross_shard_msg(
                        shard_id,
                        round,
                        CrossShardMsg::StopMsg,
                    );
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L331-333)
```rust
    fn send_cross_shard_msg(&self, shard_id: ShardId, round: RoundId, msg: CrossShardMsg) {
        self.message_txs[shard_id][round].send(msg).unwrap()
    }
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L55-59)
```rust
    fn send_cross_shard_msg(&self, shard_id: ShardId, round: RoundId, msg: CrossShardMsg) {
        let input_message = bcs::to_bytes(&msg).unwrap();
        let tx = self.message_txs[shard_id][round].lock().unwrap();
        tx.send(Message::new(input_message)).unwrap();
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L103-135)
```rust
    fn send_remote_update_for_success(
        &self,
        txn_idx: TxnIndex,
        txn_output: &OnceCell<TransactionOutput>,
    ) {
        let edges = self.dependent_edges.get(&txn_idx).unwrap();
        let write_set = txn_output
            .get()
            .expect("Committed output must be set")
            .write_set();

        for (state_key, write_op) in write_set.expect_write_op_iter() {
            if let Some(dependent_shard_ids) = edges.get(state_key) {
                for (dependent_shard_id, round_id) in dependent_shard_ids.iter() {
                    trace!("Sending remote update for success for shard id {:?} and txn_idx: {:?}, state_key: {:?}, dependent shard id: {:?}", self.shard_id, txn_idx, state_key, dependent_shard_id);
                    let message = RemoteTxnWriteMsg(RemoteTxnWrite::new(
                        state_key.clone(),
                        Some(write_op.clone()),
                    ));
                    if *round_id == GLOBAL_ROUND_ID {
                        self.cross_shard_client.send_global_msg(message);
                    } else {
                        self.cross_shard_client.send_cross_shard_msg(
                            *dependent_shard_id,
                            *round_id,
                            message,
                        );
                    }
                }
            }
        }
    }
}
```
