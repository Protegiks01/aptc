# Audit Report

## Title
Race Condition in DKG Event Handling During Epoch Transitions Causes Silent Event Loss and Potential Quorum Failure

## Summary
The `on_dkg_start_notification()` function in `EpochManager` silently drops `DKGStartEvent` notifications when the `dkg_start_event_tx` channel is `None`. Due to a race condition in the async event loop during epoch transitions, validators can receive the `DKGStartEvent` before processing the `ReconfigNotification` that creates the channel, causing them to permanently miss the DKG start signal and fail to participate in distributed key generation.

## Finding Description

The vulnerability exists in the DKG (Distributed Key Generation) event handling mechanism during epoch transitions, particularly when randomness is first enabled or re-enabled. [1](#0-0) 

When `self.dkg_start_event_tx` is `None`, the function returns `Ok(())` without logging, storing, or retrying the event. This silent failure violates the assumption that critical DKG coordination events are reliably delivered.

The race condition occurs in the event processing loop: [2](#0-1) 

Both `dkg_start_events` and `reconfig_events` are selected concurrently via `tokio::select!`, with no ordering guarantees. During epoch transitions when randomness is first enabled, the following sequence creates the vulnerability:

**On-Chain Event Emission:**
1. Governance enables randomness, triggering epoch N→N+1 transition
2. `reconfiguration::reconfigure()` emits `NewEpochEvent` [3](#0-2) 
3. Later in epoch N+1, `block_prologue_ext()` triggers DKG start [4](#0-3) 
4. `dkg::start()` emits `DKGStartEvent` [5](#0-4) 

**Event Subscription Service Processing:**

Both events are processed and forwarded to subscribers asynchronously. [6](#0-5) 

Event and reconfiguration notifications are delivered through separate channels with no ordering guarantees.

**The Race at EpochManager:**

**Correct Order (no vulnerability):**
1. `ReconfigNotification` processed first → `on_new_epoch()` called
2. `start_new_epoch()` creates `dkg_start_event_tx` channel [7](#0-6) 
3. `DKGStartEvent` arrives → successfully forwarded to DKG manager

**Vulnerable Order (race condition):**
1. `DKGStartEvent` arrives first → `on_dkg_start_notification()` called
2. Line 109 check: `dkg_start_event_tx` is `None` (channel not created yet)
3. Event silently dropped, returns `Ok(())` at line 122
4. `ReconfigNotification` arrives later → channel created, DKG manager spawned
5. DKG manager waits indefinitely for start event that was already dropped

**Why In-Progress Session Recovery Fails:**

The `ReconfigNotification` contains on-chain state at the version where `NewEpochEvent` was emitted: [8](#0-7) 

At that version (start of epoch N+1), the DKG session for N+1→N+2 hasn't started yet. The payload doesn't contain the in-progress session, so the DKG manager starts with `in_progress_session = None`: [9](#0-8) 

Without the start event or in-progress session, the DKG manager remains in `NotStarted` state and never broadcasts its transcript.

## Impact Explanation

**High Severity** - Significant protocol violation causing temporary liveness failure.

If validators representing >33% of voting power experience the race condition (DKGStartEvent arrives before ReconfigNotification), they will not participate in DKG. The DKG protocol requires transcripts from validators representing at least ~67% of stake (2f+1 threshold): [10](#0-9) 

If quorum is not reached:
- DKG cannot complete
- The chain cannot transition to the next epoch with randomness enabled
- Liveness is impacted until the incomplete DKG session is manually cleared via `try_clear_incomplete_session()` [11](#0-10) 

Unlike validators that restart (which fetch current on-chain state and resume DKG via in-progress session recovery [12](#0-11) ), continuously-running validators that experience the race have no recovery mechanism. The `DKGStartEvent` is emitted only once per session [13](#0-12) , with no retry mechanism.

## Likelihood Explanation

**Medium-High Likelihood** during specific conditions:

The race condition occurs **every time randomness is first enabled or re-enabled** after being disabled. The `dkg_start_event_tx` channel is only created when randomness is enabled [14](#0-13) , creating a guaranteed window where it's `None`.

The event ordering depends on:
- Network propagation delays between state-sync components
- Event subscription service processing timing
- Tokio scheduler non-determinism in the `select!` macro

In a distributed network, different validators will likely experience different event orderings, with some subset experiencing the vulnerable ordering. No attacker action is required - this is a natural race condition.

The impact is amplified because there's no monitoring, alerting, or automatic recovery. Operators would need to manually detect the DKG stall and clear the session.

## Recommendation

**Immediate Fix - Buffer Events Before Channel Creation:**

Store pending `DKGStartEvent` notifications in a buffer and replay them after `dkg_start_event_tx` is created:

```rust
pub struct EpochManager<P: OnChainConfigProvider> {
    // ... existing fields ...
    dkg_start_event_tx: Option<aptos_channel::Sender<(), DKGStartEvent>>,
    pending_dkg_start_event: Option<DKGStartEvent>, // Add buffering
}

fn on_dkg_start_notification(&mut self, notification: EventNotification) -> Result<()> {
    let EventNotification {
        subscribed_events, ..
    } = notification;
    
    for event in subscribed_events {
        if let Ok(dkg_start_event) = DKGStartEvent::try_from(&event) {
            if let Some(tx) = self.dkg_start_event_tx.as_ref() {
                let _ = tx.push((), dkg_start_event);
            } else {
                // Buffer event if channel not ready
                warn!("[DKG] Channel not ready, buffering DKGStartEvent");
                self.pending_dkg_start_event = Some(dkg_start_event);
            }
            return Ok(());
        }
    }
    Ok(())
}

async fn start_new_epoch(&mut self, payload: OnChainConfigPayload<P>) -> Result<()> {
    // ... existing setup code ...
    
    if let (true, Some(my_index)) = (randomness_enabled, my_index) {
        // ... create channel ...
        let (dkg_start_event_tx, dkg_start_event_rx) =
            aptos_channel::new(QueueStyle::KLAST, 1, None);
        self.dkg_start_event_tx = Some(dkg_start_event_tx);
        
        // Replay buffered event
        if let Some(buffered_event) = self.pending_dkg_start_event.take() {
            info!("[DKG] Replaying buffered DKGStartEvent");
            if let Some(tx) = self.dkg_start_event_tx.as_ref() {
                let _ = tx.push((), buffered_event);
            }
        }
        
        // ... rest of DKG manager setup ...
    }
    Ok(())
}
```

**Additional Hardening:**
1. Add monitoring/metrics for dropped DKG events
2. Log warnings when events are buffered or dropped
3. Consider establishing ordering guarantees between reconfig and event notifications
4. Add periodic polling to check on-chain DKG state and auto-resume if needed

## Proof of Concept

The following test demonstrates the race condition (requires integration test framework):

```rust
#[tokio::test]
async fn test_dkg_start_event_race_condition() {
    // Setup: Epoch N with randomness disabled
    let mut epoch_manager = create_epoch_manager_with_randomness_disabled();
    assert!(epoch_manager.dkg_start_event_tx.is_none());
    
    // Simulate on-chain events from enabling randomness
    let new_epoch_event = create_new_epoch_event(epoch = N+1, randomness_enabled = true);
    let dkg_start_event = create_dkg_start_event(dealer_epoch = N+1);
    
    // Race condition: DKGStartEvent arrives BEFORE ReconfigNotification
    // This can happen due to async event processing non-determinism
    
    // Step 1: DKGStartEvent notification arrives first
    let event_notification = EventNotification {
        subscribed_events: vec![dkg_start_event.clone()],
        ..
    };
    epoch_manager.on_dkg_start_notification(event_notification).await.unwrap();
    
    // Verify: Event was silently dropped (dkg_start_event_tx is None)
    assert!(epoch_manager.dkg_start_event_tx.is_none());
    
    // Step 2: ReconfigNotification arrives later
    let reconfig_notification = ReconfigNotification {
        version: get_epoch_start_version(N+1),
        on_chain_configs: get_configs_with_randomness_enabled(),
    };
    epoch_manager.on_new_epoch(reconfig_notification).await.unwrap();
    
    // Verify: Channel now exists, but DKG manager never received the start event
    assert!(epoch_manager.dkg_start_event_tx.is_some());
    
    // DKG manager is waiting but will never receive the event
    // In a real network with multiple validators, if >33% experience this race,
    // DKG quorum is broken and the chain cannot progress
    
    // Expected: DKG should start successfully
    // Actual: DKG never starts - validator doesn't participate
    assert_dkg_never_starts();
}
```

**Notes:**
- This vulnerability only affects validators that were continuously running during the transition
- Validators that restart after the DKGStartEvent is emitted will correctly resume via the in-progress session recovery mechanism
- The race window is particularly significant during mainnet upgrades that enable randomness for the first time
- No malicious actor is required - this is a timing-dependent bug in the event handling architecture

### Citations

**File:** dkg/src/epoch_manager.rs (L108-123)
```rust
    fn on_dkg_start_notification(&mut self, notification: EventNotification) -> Result<()> {
        if let Some(tx) = self.dkg_start_event_tx.as_ref() {
            let EventNotification {
                subscribed_events, ..
            } = notification;
            for event in subscribed_events {
                if let Ok(dkg_start_event) = DKGStartEvent::try_from(&event) {
                    let _ = tx.push((), dkg_start_event);
                    return Ok(());
                } else {
                    debug!("[DKG] on_dkg_start_notification: failed in converting a contract event to a dkg start event!");
                }
            }
        }
        Ok(())
    }
```

**File:** dkg/src/epoch_manager.rs (L128-138)
```rust
            let handling_result = tokio::select! {
                notification = self.dkg_start_events.select_next_some() => {
                    self.on_dkg_start_notification(notification)
                },
                reconfig_notification = self.reconfig_events.select_next_some() => {
                    self.on_new_epoch(reconfig_notification).await
                },
                (peer, rpc_request) = network_receivers.rpc_rx.select_next_some() => {
                    self.process_rpc_request(peer, rpc_request)
                },
            };
```

**File:** dkg/src/epoch_manager.rs (L199-201)
```rust
        let randomness_enabled =
            consensus_config.is_vtxn_enabled() && onchain_randomness_config.randomness_enabled();
        if let (true, Some(my_index)) = (randomness_enabled, my_index) {
```

**File:** dkg/src/epoch_manager.rs (L202-205)
```rust
            let DKGState {
                in_progress: in_progress_session,
                ..
            } = payload.get::<DKGState>().unwrap_or_default();
```

**File:** dkg/src/epoch_manager.rs (L223-225)
```rust
            let (dkg_start_event_tx, dkg_start_event_rx) =
                aptos_channel::new(QueueStyle::KLAST, 1, None);
            self.dkg_start_event_tx = Some(dkg_start_event_tx);
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration.move (L151-156)
```text
        event::emit_event<NewEpochEvent>(
            &mut config_ref.events,
            NewEpochEvent {
                epoch: config_ref.epoch,
            },
        );
```

**File:** aptos-move/framework/aptos-framework/sources/block.move (L244-246)
```text
        if (timestamp - reconfiguration::last_reconfiguration_time() >= epoch_interval) {
            reconfiguration_with_dkg::try_start();
        };
```

**File:** aptos-move/framework/aptos-framework/sources/dkg.move (L81-84)
```text
        emit(DKGStartEvent {
            start_time_us,
            session_metadata: new_session_metadata,
        });
```

**File:** aptos-move/framework/aptos-framework/sources/dkg.move (L99-106)
```text
    /// Delete the currently incomplete session, if it exists.
    public fun try_clear_incomplete_session(fx: &signer) acquires DKGState {
        system_addresses::assert_aptos_framework(fx);
        if (exists<DKGState>(@aptos_framework)) {
            let dkg_state = borrow_global_mut<DKGState>(@aptos_framework);
            dkg_state.in_progress = option::none();
        }
    }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L264-272)
```rust
    fn notify_reconfiguration_subscribers(&mut self, version: Version) -> Result<(), Error> {
        if self.reconfig_subscriptions.is_empty() {
            return Ok(()); // No reconfiguration subscribers!
        }

        let new_configs = self.read_on_chain_configs(version)?;
        for (_, reconfig_subscription) in self.reconfig_subscriptions.iter_mut() {
            reconfig_subscription.notify_subscriber_of_configs(version, new_configs.clone())?;
        }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L311-325)
```rust
    fn notify_events(&mut self, version: Version, events: Vec<ContractEvent>) -> Result<(), Error> {
        if events.is_empty() {
            return Ok(()); // No events!
        }

        // Notify event subscribers and check if a reconfiguration event was processed
        let reconfig_event_processed = self.notify_event_subscribers(version, events)?;

        // If a reconfiguration event was found, also notify the reconfig subscribers
        // of the new configuration values.
        if reconfig_event_processed {
            self.notify_reconfiguration_subscribers(version)
        } else {
            Ok(())
        }
```

**File:** dkg/src/transcript_aggregation/mod.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::{counters::DKG_STAGE_SECONDS, types::DKGTranscriptRequest, DKGMessage};
use anyhow::{anyhow, ensure, Context};
use aptos_consensus_types::common::Author;
use aptos_infallible::{duration_since_epoch, Mutex};
use aptos_logger::info;
use aptos_reliable_broadcast::BroadcastStatus;
use aptos_types::{
    dkg::{DKGTrait, DKGTranscript},
    epoch_state::EpochState,
    validator_verifier::VerifyError,
};
use move_core_types::account_address::AccountAddress;
use std::{collections::HashSet, sync::Arc, time::Duration};

pub struct TranscriptAggregator<S: DKGTrait> {
    pub contributors: HashSet<AccountAddress>,
    pub trx: Option<S::Transcript>,
}

impl<S: DKGTrait> Default for TranscriptAggregator<S> {
    fn default() -> Self {
        Self {
            contributors: HashSet::new(),
            trx: None,
        }
    }
}

pub struct TranscriptAggregationState<DKG: DKGTrait> {
    start_time: Duration,
    my_addr: AccountAddress,
    valid_peer_transcript_seen: bool,
    trx_aggregator: Mutex<TranscriptAggregator<DKG>>,
    dkg_pub_params: DKG::PublicParams,
    epoch_state: Arc<EpochState>,
}

impl<DKG: DKGTrait> TranscriptAggregationState<DKG> {
    pub fn new(
        start_time: Duration,
        my_addr: AccountAddress,
        dkg_pub_params: DKG::PublicParams,
        epoch_state: Arc<EpochState>,
    ) -> Self {
        //TODO(zjma): take DKG threshold as a parameter.
        Self {
            start_time,
```

**File:** dkg/src/dkg_manager/mod.rs (L140-162)
```rust
        if let Some(session_state) = in_progress_session {
            let DKGSessionState {
                start_time_us,
                metadata,
                ..
            } = session_state;

            if metadata.dealer_epoch == self.epoch_state.epoch {
                info!(
                    epoch = self.epoch_state.epoch,
                    "Found unfinished and current DKG session. Continuing it."
                );
                if let Err(e) = self.setup_deal_broadcast(start_time_us, &metadata).await {
                    error!(epoch = self.epoch_state.epoch, "dkg resumption failed: {e}");
                }
            } else {
                info!(
                    cur_epoch = self.epoch_state.epoch,
                    dealer_epoch = metadata.dealer_epoch,
                    "Found unfinished but stale DKG session. Ignoring it."
                );
            }
        }
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration_with_dkg.move (L24-40)
```text
    public(friend) fun try_start() {
        let incomplete_dkg_session = dkg::incomplete_session();
        if (option::is_some(&incomplete_dkg_session)) {
            let session = option::borrow(&incomplete_dkg_session);
            if (dkg::session_dealer_epoch(session) == reconfiguration::current_epoch()) {
                return
            }
        };
        reconfiguration_state::on_reconfig_start();
        let cur_epoch = reconfiguration::current_epoch();
        dkg::start(
            cur_epoch,
            randomness_config::current(),
            stake::cur_validator_consensus_infos(),
            stake::next_validator_consensus_infos(),
        );
    }
```
