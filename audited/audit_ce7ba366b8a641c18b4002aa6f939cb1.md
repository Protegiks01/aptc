# Audit Report

## Title
Hot State Cache Snapshot Isolation Violation Leading to Non-Deterministic State Reads

## Summary
The `HotStateLRU` struct holds an `Arc<dyn HotStateView>` reference to a shared, mutable `HotStateBase` that can be concurrently modified by a background Committer thread. This allows execution threads to observe state updates from versions newer than the intended base version, violating snapshot isolation and potentially causing non-deterministic execution within a validator node.

## Finding Description

The vulnerability exists in the interaction between `HotStateLRU` and `HotStateBase` in the hot state caching system.

**Architecture Flow:** [1](#0-0) 

When execution requests persisted state, `get_committed()` returns an `Arc` to the shared `HotStateBase` along with `State` metadata. The `HotStateBase` contains `DashMap` shards that provide thread-safe concurrent access: [2](#0-1) 

The problem occurs when:

1. **Execution Thread** calls `get_committed()` at time T1, receiving:
   - `Arc<HotStateBase>` pointing to shared DashMaps
   - `State` metadata indicating version V

2. **Background Committer Thread** (running via `enqueue_commit()`) updates the same `HotStateBase` at time T2 (T2 > T1): [3](#0-2) 

The Committer modifies the shared DashMaps, inserting/removing entries for version V+1.

3. **Execution Thread** creates `HotStateLRU` at time T3 (T3 > T2) with:
   - The `Arc<HotStateBase>` now containing V+1 data
   - Metadata from V

4. When `HotStateLRU::get_slot()` is called: [4](#0-3) 

It queries `self.committed.get_state_slot(key)` which reads from the shared `HotStateBase`, potentially returning V+1 data when V data was expected.

**State View Hierarchy:**

The `CachedStateView` lookup order compounds this issue: [5](#0-4) 

Hot state (line 239) is checked before the versioned cold DB (line 244-246). The cold DB correctly reads at `base_version`, but the hot state cache does not respect versioning, returning whatever is currently in the shared DashMaps.

**Pipelined Execution Context:**

The production code uses pipelined execution where block N is executed while block N-1 is being committed: [6](#0-5) 

This test demonstrates the race: Thread 1 executes blocks while Thread 4 commits previous blocks concurrently, both accessing the same `PersistedState`.

**Invariant Violation:**

This breaks **Critical Invariant #1: "Deterministic Execution - All validators must produce identical state roots for identical blocks"**. A validator node experiencing this race condition during execution may observe state values from version V+1 when executing at version V+2 with base version V, while the state should reflect version V. This leads to:

- **Inconsistent snapshot**: Reading key K1 might return V+1 data while key K2 returns V data
- **Non-deterministic execution**: Same block produces different results based on commit timing
- **State root divergence**: The affected validator computes an incorrect state root

## Impact Explanation

**Severity: Critical** (Consensus/Safety Violations)

This vulnerability can cause consensus failures through non-deterministic state reads. While DashMap provides memory safety, it does not provide snapshot isolation. The hot state cache is designed to be an optimization layer, but its lack of versioning means it can serve incorrect data:

- **Cross-key inconsistency**: Within a single execution, different keys might be read from different versions (V vs V+1) depending on when the commit updates each shard
- **State root divergence**: The validator computing state at the wrong version will produce an incorrect state root, causing it to reject or produce incorrect blocks
- **Consensus disruption**: If this affects multiple validators at critical moments, it can prevent consensus progress

The vulnerability is particularly severe because:
1. It affects core state management used by all transactions
2. It can cause subtle, timing-dependent consensus failures that are difficult to diagnose
3. It violates the fundamental assumption that state views provide consistent snapshots

## Likelihood Explanation

**Likelihood: Medium-High**

The race condition occurs naturally in the pipelined execution model:

- The execution pipeline explicitly allows concurrent execution and commit operations for performance
- Every block execution that retrieves persisted state while commits are processing is vulnerable
- The window of vulnerability exists from when `get_committed()` returns until execution completes
- Higher transaction throughput increases the likelihood of overlapping execution and commit

The issue manifests when:
1. Block N-1 is being committed (Committer thread updating HotStateBase)
2. Block N begins execution (gets Arc to HotStateBase with version N-2 metadata)
3. Commit completes, updating HotStateBase to version N-1
4. Block N's execution reads from hot state, observing N-1 data with N-2 base version

## Recommendation

**Fix 1: Implement versioned hot state snapshots**

Modify `get_committed()` to return a true snapshot instead of a live reference:

```rust
pub fn get_committed(&self) -> (Arc<dyn HotStateView>, State) {
    let state = self.committed.lock().clone();
    // Create a snapshot of current HotStateBase entries
    let snapshot = Arc::new(HotStateSnapshot::from_base(&self.base, &state));
    (snapshot, state)
}
```

Where `HotStateSnapshot` captures the current state and remains immutable.

**Fix 2: Add version validation to HotStateBase reads**

Extend `HotStateView` trait to include version information:

```rust
pub trait HotStateView: Send + Sync {
    fn get_state_slot(&self, state_key: &StateKey) -> Option<StateSlot>;
    fn version(&self) -> Option<Version>; // Add version tracking
}
```

Then validate in `get_slot()` that the returned slot's version doesn't exceed the base version.

**Fix 3: Use immutable snapshots in execution pipeline**

Modify the commit flow to ensure execution always works with immutable snapshots that are not affected by concurrent commits. This could involve copy-on-write or reference counting with version tags.

## Proof of Concept

The existing test demonstrates the vulnerability pattern: [7](#0-6) 

To expose the bug, add instrumentation to track version mismatches:

```rust
// In HotStateLRU::get_slot(), add assertion:
pub(crate) fn get_slot(&self, key: &StateKey) -> Option<StateSlot> {
    if let Some(slot) = self.pending.get(key) {
        return Some(slot.clone());
    }
    if let Some(slot) = self.overlay.get(key) {
        return Some(slot);
    }
    let slot = self.committed.get_state_slot(key);
    
    // Add version check (for testing)
    if let Some(ref s) = slot {
        if s.is_hot() {
            let slot_version = s.expect_hot_since_version();
            // If slot_version > expected_base_version, we have the bug
            assert!(slot_version <= self.expected_base_version,
                "Hot state returned data from version {} but base is {}",
                slot_version, self.expected_base_version);
        }
    }
    slot
}
```

Run the `replay_chunks_pipelined` test with high concurrency to trigger the race condition. The assertion will fail when hot state returns data from a version newer than the base version.

## Notes

This is a **snapshot isolation** bug in the hot state caching layer. While the cold storage (DB) provides correct versioned reads, the hot state cache lacks versioning and can serve stale or future data depending on concurrent commit timing. The issue is fundamental to the current architecture where `Arc<HotStateBase>` is a shared, mutable structure rather than an immutable snapshot.

The vulnerability affects the core state read path used by all transaction execution, making it a critical consensus safety issue. The fix requires architectural changes to ensure hot state caching provides the same snapshot isolation guarantees as the underlying versioned storage.

### Citations

**File:** storage/aptosdb/src/state_store/hot_state.rs (L100-105)
```rust
impl HotStateView for HotStateBase<StateKey, StateSlot> {
    fn get_state_slot(&self, state_key: &StateKey) -> Option<StateSlot> {
        let shard_id = state_key.get_shard_id();
        self.get_from_shard(shard_id, state_key).map(|v| v.clone())
    }
}
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L131-136)
```rust
    pub fn get_committed(&self) -> (Arc<dyn HotStateView>, State) {
        let state = self.committed.lock().clone();
        let base = self.base.clone();

        (base, state)
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L235-275)
```rust
    fn commit(&mut self, to_commit: &State) {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["hot_state_commit"]);

        let mut n_insert = 0;
        let mut n_update = 0;
        let mut n_evict = 0;

        let delta = to_commit.make_delta(&self.committed.lock());
        for shard_id in 0..NUM_STATE_SHARDS {
            for (key, slot) in delta.shards[shard_id].iter() {
                if slot.is_hot() {
                    let key_size = key.size();
                    self.total_key_bytes += key_size;
                    self.total_value_bytes += slot.size();
                    if let Some(old_slot) = self.base.shards[shard_id].insert(key, slot) {
                        self.total_key_bytes -= key_size;
                        self.total_value_bytes -= old_slot.size();
                        n_update += 1;
                    } else {
                        n_insert += 1;
                    }
                } else if let Some((key, old_slot)) = self.base.shards[shard_id].remove(&key) {
                    self.total_key_bytes -= key.size();
                    self.total_value_bytes -= old_slot.size();
                    n_evict += 1;
                }
            }
            self.heads[shard_id] = to_commit.latest_hot_key(shard_id);
            self.tails[shard_id] = to_commit.oldest_hot_key(shard_id);
            assert_eq!(
                self.base.shards[shard_id].len(),
                to_commit.num_hot_items(shard_id)
            );

            debug_assert!(self.validate_lru(shard_id).is_ok());
        }

        COUNTER.inc_with_by(&["hot_state_insert"], n_insert);
        COUNTER.inc_with_by(&["hot_state_update"], n_update);
        COUNTER.inc_with_by(&["hot_state_evict"], n_evict);
    }
```

**File:** storage/storage-interface/src/state_store/hot_state.rs (L145-155)
```rust
    pub(crate) fn get_slot(&self, key: &StateKey) -> Option<StateSlot> {
        if let Some(slot) = self.pending.get(key) {
            return Some(slot.clone());
        }

        if let Some(slot) = self.overlay.get(key) {
            return Some(slot);
        }

        self.committed.get_state_slot(key)
    }
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L236-250)
```rust
        let ret = if let Some(slot) = self.speculative.get_state_slot(state_key) {
            COUNTER.inc_with(&["sv_hit_speculative"]);
            slot
        } else if let Some(slot) = self.hot.get_state_slot(state_key) {
            COUNTER.inc_with(&["sv_hit_hot"]);
            slot
        } else if let Some(base_version) = self.base_version() {
            COUNTER.inc_with(&["sv_cold"]);
            StateSlot::from_db_get(
                self.cold
                    .get_state_value_with_version_by_version(state_key, base_version)?,
            )
        } else {
            StateSlot::ColdVacant
        };
```

**File:** storage/aptosdb/src/state_store/tests/speculative_state_workflow.rs (L543-571)
```rust
        let (hot_state, persisted_state) = persisted_state.get_state();
        let state_view = CachedStateView::new_with_config(
            StateViewId::Miscellaneous,
            state_by_version.clone(),
            hot_state.clone(),
            persisted_state.clone(),
            parent_state.deref().clone(),
        );
        let read_keys = block.all_reads().collect_vec();
        pool.install(|| {
            read_keys.par_iter().for_each(|k| {
                let value = state_view.get_state_value(k).unwrap();
                let expected_value = parent_state.version().and_then(|version| {
                    state_by_version
                        .get_state_value_with_version_by_version(k, version)
                        .unwrap()
                        .map(|(_ver, val)| val)
                });
                assert_eq!(value, expected_value)
            });
        });
        let memorized_reads = state_view.into_memorized_reads();

        let (next_state, hot_state_updates) = parent_state.update_with_memorized_reads(
            hot_state.clone(),
            &persisted_state,
            block.update_refs(),
            &memorized_reads,
        );
```

**File:** storage/aptosdb/src/state_store/tests/speculative_state_workflow.rs (L732-805)
```rust
fn replay_chunks_pipelined(chunks: Vec<Chunk>, state_by_version: Arc<StateByVersion>) {
    let empty = LedgerStateWithSummary::new_empty(TEST_CONFIG);
    let current_state = Arc::new(Mutex::new(empty.clone()));

    let persisted_state = PersistedState::new_empty(TEST_CONFIG);
    persisted_state.hack_reset(empty.deref().clone());

    let (to_summary_update, from_state_update) = channel();
    let (to_db_commit, from_summary_update) = channel();
    let (to_buffered_state_commit, from_buffered_state_commit) = channel();

    let mut threads = vec![];

    {
        let empty = empty.clone();
        let state_by_version = state_by_version.clone();
        let persisted_state = persisted_state.clone();
        threads.push(spawn(move || {
            update_state(
                chunks,
                state_by_version,
                empty,
                persisted_state,
                to_summary_update,
            );
        }));
    }

    {
        let empty = empty.clone();
        let state_by_version = state_by_version.clone();
        let persisted_state = persisted_state.clone();
        threads.push(spawn(move || {
            update_state_summary(
                state_by_version,
                empty.clone(),
                persisted_state,
                from_state_update,
                to_db_commit,
            );
        }));
    }

    {
        let empty = empty.clone();
        let state_by_version = state_by_version.clone();
        let current_state = current_state.clone();
        threads.push(spawn(move || {
            send_to_state_buffer(
                empty,
                state_by_version,
                from_summary_update,
                to_buffered_state_commit,
                current_state,
            );
        }));
    }

    {
        let state_by_version = state_by_version.clone();
        let persisted_state = persisted_state.clone();
        threads.push(spawn(move || {
            commit_state_buffer(
                state_by_version,
                from_buffered_state_commit,
                persisted_state,
            );
        }));
    }

    threads
        .into_iter()
        .for_each(|t| t.join().expect("join() failed."))
}
```
