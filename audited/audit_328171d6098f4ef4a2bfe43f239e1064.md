# Audit Report

## Title
Synchronous Quorum Store Request Processing Causes Starvation of Client Transactions and Network Events in Mempool Coordinator

## Summary
The mempool coordinator processes quorum store requests synchronously in its main event loop without yield points, while client transaction submissions and network events are handled asynchronously. During high consensus activity, the synchronous processing of quorum store batch requests—which involves lock acquisition, garbage collection, and iteration through potentially millions of transactions—blocks the coordinator loop, causing significant latency spikes and potential timeouts for client API requests and network transaction propagation.

## Finding Description

The mempool coordinator uses a `futures::select!` macro to multiplex between multiple event sources in its main loop. However, there is a critical asymmetry in how different event types are processed: [1](#0-0) 

The coordinator handles three main event types differently:

1. **Client events** are processed by spawning async tasks on a bounded executor that yields control
2. **Network events** are processed by spawning async tasks on a bounded executor that yields control  
3. **Quorum store requests** are processed **synchronously** by calling `tasks::process_quorum_store_request()` directly without any `.await` points

The quorum store request handler performs blocking operations: [2](#0-1) 

This function:
- Acquires the mempool lock (blocking until available)
- Performs garbage collection by expiration time
- Calls `get_batch()` which iterates through the priority queue [3](#0-2) 

The critical issue is that the mempool can hold up to **2 million transactions** by default: [4](#0-3) 

During high consensus activity, the batch generator polls every **25-50ms** and requests up to **1500 transactions** per pull: [5](#0-4) 

The channel buffer between consensus and mempool is only **size 1**: [6](#0-5) [7](#0-6) 

**Attack Scenario:**
During periods of high transaction throughput:
1. The mempool accumulates many transactions (hundreds of thousands to millions)
2. Consensus batch generator sends GetBatchRequest every 50-250ms requesting 1500 transactions
3. Each request processes synchronously, iterating through the large priority queue and holding the mempool lock
4. The synchronous processing takes 50-200ms depending on mempool size
5. While processing, the coordinator loop is completely blocked
6. Client transaction submissions pile up in the `client_events` channel
7. Network broadcast messages pile up in the `events` stream
8. API requests timeout after waiting too long for mempool to accept transactions
9. Network transaction propagation is delayed, affecting overall network performance

## Impact Explanation

This vulnerability meets **High Severity** criteria per the Aptos bug bounty program:

1. **"Validator node slowdowns"** - The blocking synchronous processing directly causes validator nodes to slow down during high-load periods. The mempool coordinator cannot process other events while handling quorum store requests, degrading overall node performance.

2. **"API crashes"** - Client API requests that submit transactions to mempool will timeout when the coordinator is blocked processing quorum store requests. Modern APIs typically have 2-5 second timeouts, and if the coordinator is blocked for 100-200ms per request with requests arriving every 50-250ms, a client request could wait through multiple quorum store request cycles before being processed, easily exceeding timeout thresholds.

The issue affects all validator nodes during normal high-load operation, not just specific configurations. It degrades the user experience by causing transaction submission failures and delays network synchronization.

## Likelihood Explanation

**Likelihood: HIGH**

This issue occurs naturally during normal high-load operation without requiring any malicious actors:

1. **High transaction volume** - When the network experiences high usage (thousands of TPS), the mempool fills up
2. **Fast block production** - Aptos targets sub-second block times, meaning consensus frequently pulls batches
3. **Normal configuration** - The issue exists with default configuration parameters
4. **No attacker required** - This is a design flaw that manifests under load, not an attack vector

The conditions for this issue are common in production blockchain networks during peak usage. The mempool can easily accumulate hundreds of thousands of transactions, and consensus naturally requests batches frequently during active block production. The combination of synchronous processing and frequent requests creates a recurring bottleneck.

## Recommendation

Convert `process_quorum_store_request` to an async function that spawns work on the bounded executor, similar to how client events and network events are handled. This ensures fair processing across all event types and prevents any single event source from monopolizing the coordinator loop.

**Fix approach:**

1. Make `process_quorum_store_request` async and spawn it on the bounded executor
2. Add appropriate yield points to allow the async runtime to schedule other tasks
3. Consider adding timeout protection for quorum store request processing

**Code changes:**

In `coordinator.rs`, change the handler to spawn async:
```rust
msg = quorum_store_requests.select_next_some() => {
    let smp_clone = smp.clone();
    bounded_executor.spawn(
        tasks::process_quorum_store_request_async(smp_clone, msg)
    ).await;
},
```

In `tasks.rs`, convert to async function:
```rust
pub(crate) async fn process_quorum_store_request_async<NetworkClient, TransactionValidator>(
    smp: SharedMempool<NetworkClient, TransactionValidator>,
    req: QuorumStoreRequest,
) where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg>,
    TransactionValidator: TransactionValidation,
{
    // Existing logic but allow async executor to yield during processing
    // by adding tokio::task::yield_now().await at appropriate points
}
```

Alternatively, add explicit yield points in the get_batch iteration to allow other tasks to run periodically during long batch retrievals.

## Proof of Concept

The following scenario demonstrates the vulnerability:

**Setup:**
1. Start an Aptos validator node with default configuration
2. Submit transactions to fill mempool to ~500,000 transactions
3. Enable block production at normal rate (1-2 second block times)

**Reproduction steps:**

```rust
// Rust test demonstrating the blocking behavior
#[tokio::test]
async fn test_quorum_store_request_blocks_coordinator() {
    // Create mempool with 500,000 transactions
    let mempool = setup_mempool_with_txns(500_000).await;
    
    // Measure time to process a single quorum store request
    let start = Instant::now();
    let request = QuorumStoreRequest::GetBatchRequest(
        1500,  // max_txns
        4_000_000,  // max_bytes (4MB)
        true,  // return_non_full
        BTreeMap::new(),  // no excludes
        callback_sender,
    );
    
    // This call blocks synchronously
    tasks::process_quorum_store_request(&smp, request);
    let duration = start.elapsed();
    
    // Assert processing took significant time (>50ms with large mempool)
    assert!(duration > Duration::from_millis(50));
    
    // During this time, simulate client requests arriving
    // They will queue up and experience high latency
    
    // Demonstrate that frequent requests create sustained blocking
    for _ in 0..10 {
        tokio::time::sleep(Duration::from_millis(50)).await;
        // Send another request - coordinator is blocked repeatedly
        send_quorum_store_request();
    }
    
    // Client requests submitted during this period will timeout
}
```

**Expected behavior:**
- Quorum store request processing takes 50-200ms depending on mempool size
- During this time, no client events or network events can be processed
- With requests arriving every 50-250ms, the coordinator spends significant time blocked
- Client API requests experience multi-second delays and timeout

**Observed impact:**
- Transaction submission API returns timeout errors
- Network transaction propagation is delayed
- Validator performance metrics show high latency in mempool operations
- Node dashboards show mempool event processing latency spikes

## Notes

This vulnerability is a design issue rather than an implementation bug. The asymmetry between synchronous quorum store request handling and asynchronous handling of other event types creates an unintentional priority inversion. The issue is exacerbated by:

1. The small channel buffer size (1) which provides minimal buffering
2. The large potential mempool size (2M transactions) making iteration expensive  
3. The high frequency of consensus batch requests (every 25-250ms)
4. The lack of yield points in the synchronous processing path

The fix requires architectural changes to ensure all event handlers in the coordinator loop are async and can yield, maintaining fairness across event sources regardless of processing time.

### Citations

**File:** mempool/src/shared_mempool/coordinator.rs (L106-129)
```rust
    loop {
        let _timer = counters::MAIN_LOOP.start_timer();
        ::futures::select! {
            msg = client_events.select_next_some() => {
                handle_client_request(&mut smp, &bounded_executor, msg).await;
            },
            msg = quorum_store_requests.select_next_some() => {
                tasks::process_quorum_store_request(&smp, msg);
            },
            reconfig_notification = mempool_reconfig_events.select_next_some() => {
                handle_mempool_reconfig_event(&mut smp, &bounded_executor, reconfig_notification.on_chain_configs).await;
            },
            (peer, backoff) = scheduled_broadcasts.select_next_some() => {
                tasks::execute_broadcast(peer, backoff, &mut smp, &mut scheduled_broadcasts, executor.clone()).await;
            },
            (network_id, event) = events.select_next_some() => {
                handle_network_event(&bounded_executor, &mut smp, network_id, event).await;
            },
            _ = update_peers_interval.tick().fuse() => {
                handle_update_peers(peers_and_metadata.clone(), &mut smp, &mut scheduled_broadcasts, executor.clone()).await;
            },
            complete => break,
        }
    }
```

**File:** mempool/src/shared_mempool/tasks.rs (L630-675)
```rust
pub(crate) fn process_quorum_store_request<NetworkClient, TransactionValidator>(
    smp: &SharedMempool<NetworkClient, TransactionValidator>,
    req: QuorumStoreRequest,
) where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg>,
    TransactionValidator: TransactionValidation,
{
    // Start latency timer
    let start_time = Instant::now();

    let (resp, callback, counter_label) = match req {
        QuorumStoreRequest::GetBatchRequest(
            max_txns,
            max_bytes,
            return_non_full,
            exclude_transactions,
            callback,
        ) => {
            let txns;
            {
                let lock_timer = counters::mempool_service_start_latency_timer(
                    counters::GET_BLOCK_LOCK_LABEL,
                    counters::REQUEST_SUCCESS_LABEL,
                );
                let mut mempool = smp.mempool.lock();
                lock_timer.observe_duration();

                {
                    let _gc_timer = counters::mempool_service_start_latency_timer(
                        counters::GET_BLOCK_GC_LABEL,
                        counters::REQUEST_SUCCESS_LABEL,
                    );
                    // gc before pulling block as extra protection against txns that may expire in consensus
                    // Note: this gc operation relies on the fact that consensus uses the system time to determine block timestamp
                    let curr_time = aptos_infallible::duration_since_epoch();
                    mempool.gc_by_expiration_time(curr_time);
                }

                let max_txns = cmp::max(max_txns, 1);
                let _get_batch_timer = counters::mempool_service_start_latency_timer(
                    counters::GET_BLOCK_GET_BATCH_LABEL,
                    counters::REQUEST_SUCCESS_LABEL,
                );
                txns =
                    mempool.get_batch(max_txns, max_bytes, return_non_full, exclude_transactions);
            }
```

**File:** mempool/src/core_mempool/mempool.rs (L425-507)
```rust
    pub(crate) fn get_batch(
        &self,
        max_txns: u64,
        max_bytes: u64,
        return_non_full: bool,
        exclude_transactions: BTreeMap<TransactionSummary, TransactionInProgress>,
    ) -> Vec<SignedTransaction> {
        let start_time = Instant::now();
        let exclude_size = exclude_transactions.len();
        let mut inserted = HashSet::new();

        let gas_end_time = start_time.elapsed();

        let mut result = vec![];
        // Helper DS. Helps to mitigate scenarios where account submits several transactions
        // with increasing gas price (e.g. user submits transactions with sequence number 1, 2
        // and gas_price 1, 10 respectively)
        // Later txn has higher gas price and will be observed first in priority index iterator,
        // but can't be executed before first txn. Once observed, such txn will be saved in
        // `skipped` DS and rechecked once it's ancestor becomes available
        let mut skipped = HashSet::new();
        let mut total_bytes = 0;
        let mut txn_walked = 0usize;
        // iterate over the queue of transactions based on gas price
        'main: for txn in self.transactions.iter_queue() {
            txn_walked += 1;
            let txn_ptr = TxnPointer::from(txn);

            // TODO: removed gas upgraded logic. double check if it's needed
            if exclude_transactions.contains_key(&txn_ptr) {
                continue;
            }
            let txn_replay_protector = txn.replay_protector;
            match txn_replay_protector {
                ReplayProtector::SequenceNumber(txn_seq) => {
                    let txn_in_sequence = txn_seq > 0
                        && Self::txn_was_chosen(
                            txn.address,
                            txn_seq - 1,
                            &inserted,
                            &exclude_transactions,
                        );
                    let account_sequence_number =
                        self.transactions.get_account_sequence_number(&txn.address);
                    // include transaction if it's "next" for given account or
                    // we've already sent its ancestor to Consensus.
                    if txn_in_sequence || account_sequence_number == Some(&txn_seq) {
                        inserted.insert((txn.address, txn_replay_protector));
                        result.push((txn.address, txn_replay_protector));
                        if (result.len() as u64) == max_txns {
                            break;
                        }
                        // check if we can now include some transactions
                        // that were skipped before for given account
                        let (skipped_txn_sender, mut skipped_txn_seq_num) =
                            (txn.address, txn_seq + 1);
                        while skipped.remove(&(skipped_txn_sender, skipped_txn_seq_num)) {
                            inserted.insert((
                                skipped_txn_sender,
                                ReplayProtector::SequenceNumber(skipped_txn_seq_num),
                            ));
                            result.push((
                                skipped_txn_sender,
                                ReplayProtector::SequenceNumber(skipped_txn_seq_num),
                            ));
                            if (result.len() as u64) == max_txns {
                                break 'main;
                            }
                            skipped_txn_seq_num += 1;
                        }
                    } else {
                        skipped.insert((txn.address, txn_seq));
                    }
                },
                ReplayProtector::Nonce(_) => {
                    inserted.insert((txn.address, txn_replay_protector));
                    result.push((txn.address, txn_replay_protector));
                    if (result.len() as u64) == max_txns {
                        break;
                    }
                },
            };
        }
```

**File:** config/src/config/mempool_config.rs (L121-123)
```rust
            capacity: 2_000_000,
            capacity_bytes: 2 * 1024 * 1024 * 1024,
            capacity_per_user: 100,
```

**File:** config/src/config/quorum_store_config.rs (L110-117)
```rust
            batch_generation_poll_interval_ms: 25,
            batch_generation_min_non_empty_interval_ms: 50,
            batch_generation_max_interval_ms: 250,
            sender_max_batch_txns: DEFEAULT_MAX_BATCH_TXNS,
            // TODO: on next release, remove BATCH_PADDING_BYTES
            sender_max_batch_bytes: 1024 * 1024 - BATCH_PADDING_BYTES,
            sender_max_num_batches: DEFAULT_MAX_NUM_BATCHES,
            sender_max_total_txns: 1500,
```

**File:** aptos-node/src/services.rs (L47-47)
```rust
const INTRA_NODE_CHANNEL_BUFFER_SIZE: usize = 1;
```

**File:** aptos-node/src/services.rs (L185-186)
```rust
    let (consensus_to_mempool_sender, consensus_to_mempool_receiver) =
        mpsc::channel(INTRA_NODE_CHANNEL_BUFFER_SIZE);
```
