# Audit Report

## Title
Silent Progress Regression in State KV Shard Pruner Masks Critical Synchronization Failures

## Summary
The `StateKvShardPruner::new()` function silently regresses shard pruner progress when a shard is ahead of the metadata pruner, masking critical synchronization problems and creating hidden state inconsistencies that can lead to consensus divergence across validators.

## Finding Description

The vulnerability exists in the initialization logic of `StateKvShardPruner::new()`. When a shard's pruner progress exceeds the metadata pruner progress (an anomalous condition indicating synchronization failure), the catch-up logic performs a backward progress regression without validation or error reporting. [1](#0-0) 

The `get_or_initialize_subpruner_progress` function retrieves the shard's stored progress, which could be greater than `metadata_progress` in crash-recovery or race condition scenarios. [2](#0-1) 

The subsequent catch-up call on line 42 invokes `prune(progress, metadata_progress)` where `progress > metadata_progress`. [3](#0-2) 

In the `prune()` function, when `current_progress > target_version`, the iterator seeks forward to `current_progress` but immediately breaks since all entries have `stale_since_version > target_version`. However, the function unconditionally writes `target_version` as the new progress, regressing the shard's progress backward. [4](#0-3) 

**How Shards Get Ahead:**

During normal pruning operations, the metadata pruner updates first, followed by parallel shard pruning: [5](#0-4) 

If a crash occurs after a shard completes its write but before the metadata pruner's write is durably persisted, the shard will be ahead upon restart. This violates the **State Consistency** invariant.

**The Security Impact:**

When a shard has progress = 1000 but metadata has progress = 500:
1. The shard has already deleted state data for versions 0-1000
2. Progress regression sets shard progress back to 500
3. The system now claims data exists for versions 501-1000 (it doesn't)
4. `min_readable_version` is based on metadata progress (500) [6](#0-5) 

State queries check against `min_readable_version` from metadata, not individual shards: [7](#0-6) 

This creates a scenario where:
- Queries for versions 501-1000 pass the pruning check (since 501 >= 500)
- But the actual data was already deleted from the shard
- Queries return unexpected None values instead of the expected state

If different validators experience this condition differently (some have the bug manifest, others don't), they will have inconsistent state availability, leading to **consensus divergence** when executing transactions that depend on this state.

This breaks the **Deterministic Execution** invariant: validators will produce different execution results and different state roots for identical blocks, potentially causing chain splits.

## Impact Explanation

**Medium Severity** - State inconsistencies requiring intervention (per Aptos Bug Bounty criteria).

The vulnerability causes:
1. **Silent state corruption**: Data that should be available is missing
2. **Hidden synchronization failures**: Critical anomalies are masked without logging
3. **Potential consensus divergence**: Validators with different data availability cannot maintain consensus
4. **Loss of system observability**: Operators cannot detect or diagnose the underlying issue
5. **Violates state consistency invariant**: State transitions are no longer reliably verifiable

While this doesn't directly lead to fund theft, it can cause validator nodes to diverge in their execution results, requiring manual intervention to restore network consistency.

## Likelihood Explanation

**Medium-High Likelihood** in production environments:

1. **Crash scenarios**: Validator crashes during pruning operations are common in production
2. **Race conditions**: The parallel shard pruning with non-atomic cross-DB writes creates natural race windows
3. **Silent failure**: No warnings or errors alert operators to the problem
4. **Cascading impact**: Once masked, the issue persists across restarts
5. **Distributed systems**: With multiple validators, probability increases that at least one experiences this condition

The anomaly (shard ahead of metadata) can occur through legitimate system failures, not requiring attacker action. However, the masking behavior exacerbates the security impact by preventing detection and remediation.

## Recommendation

Add explicit validation in `StateKvShardPruner::new()` to detect and handle the anomalous condition:

```rust
pub(in crate::pruner) fn new(
    shard_id: usize,
    db_shard: Arc<DB>,
    metadata_progress: Version,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        &db_shard,
        &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
        metadata_progress,
    )?;
    
    // CRITICAL: Detect shard ahead of metadata
    if progress > metadata_progress {
        return Err(anyhow::anyhow!(
            "StateKvShardPruner shard {} has progress {} ahead of metadata progress {}. \
             This indicates a synchronization failure or data corruption. \
             Manual intervention required to restore consistency.",
            shard_id,
            progress,
            metadata_progress
        ));
    }
    
    let myself = Self { shard_id, db_shard };
    
    if progress < metadata_progress {
        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;
    }
    
    Ok(myself)
}
```

Additionally, the pruning workflow should use atomic transactions across metadata and shards, or implement a two-phase commit protocol to prevent the anomaly from occurring.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_temppath::TempPath;
    use aptos_schemadb::DB;
    
    #[test]
    fn test_shard_ahead_of_metadata_regression() {
        // Setup: Create shard DB with progress ahead of metadata
        let tmpdir = TempPath::new();
        let db = Arc::new(DB::open(
            tmpdir.path(),
            "test_shard",
            &[DbMetadataSchema::name(), StaleStateValueIndexByKeyHashSchema::name()],
        ).unwrap());
        
        // Simulate crash scenario: shard has progress 1000
        db.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(0),
            &DbMetadataValue::Version(1000)
        ).unwrap();
        
        // Metadata has progress 500
        let metadata_progress = 500;
        
        // Initialize shard pruner - this should detect the anomaly
        let result = StateKvShardPruner::new(0, db.clone(), metadata_progress);
        
        // Current behavior: silently regresses progress to 500
        assert!(result.is_ok());
        let shard_progress: DbMetadataValue = db.get::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(0)
        ).unwrap().unwrap();
        assert_eq!(shard_progress.expect_version(), 500); // Progress regressed!
        
        // Expected behavior: should return error
        // assert!(result.is_err());
        // assert!(result.unwrap_err().to_string().contains("ahead of metadata"));
    }
}
```

## Notes

The vulnerability is particularly concerning because:
1. It silently masks critical failures in a distributed consensus system
2. The logging at line 37-41 indicates "Catching up" when actually regressing progress
3. No monitoring or alerting exists for this condition
4. The issue can persist indefinitely, accumulating state inconsistencies over time

The same pattern exists in `StateMerkleShardPruner`, suggesting a systemic issue requiring comprehensive remediation across all shard pruner implementations.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L30-34)
```rust
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L37-42)
```rust
        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-59)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L64-81)
```rust
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L94-99)
```rust
        let min_readable_version =
            pruner_utils::get_state_kv_pruner_progress(&state_kv_db).expect("Must succeed.");

        PRUNER_VERSIONS
            .with_label_values(&["state_kv_pruner", "min_readable"])
            .set(min_readable_version as i64);
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L305-314)
```rust
    pub(super) fn error_if_state_kv_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.state_store.state_kv_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
```
