# Audit Report

## Title
Non-Atomic Message Delivery in Consensus Broadcasts Causes Validator Exclusion and Liveness Degradation

## Summary
The `send_to_many()` function in the consensus network layer lacks atomic delivery guarantees. When broadcasting critical consensus messages (votes, proposals) to validators with heterogeneous protocol support, partial message delivery can occur, causing systematic validator exclusion and consensus liveness issues.

## Finding Description

The consensus layer broadcasts critical messages (votes, proposals, timeout votes) using `send_to_many()`, which delegates to the underlying network layer's `send_to_peers()` function. This function contains two critical flaws that enable partial message delivery:

**Flaw 1: Non-Atomic Delivery Across Protocol Groups** [1](#0-0) 

The `send_to_peers()` function groups peers by protocol and iterates through each group. If an early protocol group's send succeeds but a later group's send fails, messages have already been delivered to the first group, yet the function returns an error. This violates atomicity - either all peers should receive the message or none should.

**Flaw 2: Silent Peer Exclusion** [2](#0-1) 

The `group_peers_by_protocol()` function silently drops peers without matching protocols, only logging them periodically. These peers never receive the message, but no error is returned to the caller.

**Consensus Layer Vulnerability** [3](#0-2) 

The `broadcast_without_self()` function, used for broadcasting votes and proposals, only logs warnings when `send_to_many()` fails. The consensus layer continues as if the broadcast succeeded, unaware that some validators may not have received the message.

**Attack Scenario:**

1. During a rolling upgrade, validators V1, V2 support protocol A (e.g., `ConsensusDirectSendBcs`), while V3, V4 support protocol B (e.g., `ConsensusDirectSendCompressed`)
2. V1 broadcasts a `VoteMsg` 
3. Protocol A group send succeeds → V2 receives the vote
4. Protocol B group send fails (channel error) → V3, V4 don't receive the vote
5. V1 and V2 count this vote toward quorum; V3 and V4 timeout waiting
6. If this pattern repeats across multiple votes, V3 and V4 are systematically excluded from consensus participation
7. With >1/3 voting power excluded, consensus stalls completely

**Invariant Violated:**
- **Consensus Safety/Liveness**: The assumption that broadcast messages are delivered to all validators or handled with proper retry logic is violated, enabling systematic validator exclusion.

## Impact Explanation

This vulnerability meets **High Severity** criteria per the Aptos bug bounty program:

- **Validator node slowdowns**: Validators without matching protocols experience timeouts and degraded performance
- **Significant protocol violations**: Breaks the broadcast message delivery guarantee expected by AptosBFT
- **Temporary loss of liveness**: If enough validators are excluded (>1/3 voting power), consensus cannot make progress

While this doesn't cause permanent safety violations (no double-spending or chain splits), it can cause **temporary network unavailability** during protocol transitions, which is a significant operational risk for a production blockchain.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability manifests in realistic operational scenarios:

1. **Rolling Upgrades**: Validators legitimately support different protocol versions during network upgrades, creating the protocol heterogeneity required for this issue
2. **Configuration Errors**: Misconfigured validators may fail to negotiate common protocols
3. **Network Failures**: The underlying channel can fail mid-send if disconnections occur

The vulnerability does NOT require:
- Malicious validator behavior
- 51% attack or Byzantine majorities  
- Exploitation of cryptographic weaknesses
- Social engineering or insider access

It can occur naturally during normal operations, making it a HIGH likelihood issue during upgrade windows.

## Recommendation

**Fix: Implement All-or-Nothing Broadcast Semantics**

1. **Pre-validation Before Sending**: Before initiating any sends, validate that ALL target peers have compatible protocols:

```rust
fn send_to_peers(&self, message: Message, peers: Vec<PeerNetworkId>) -> Result<(), Error> {
    // PRE-VALIDATION: Check all peers have compatible protocols
    let mut peers_without_protocol = Vec::new();
    for peer in &peers {
        if self.get_preferred_protocol_for_peer(peer, &self.direct_send_protocols_and_preferences).is_err() {
            peers_without_protocol.push(peer);
        }
    }
    
    if !peers_without_protocol.is_empty() {
        return Err(Error::NetworkError(format!(
            "Cannot broadcast: {} peers lack compatible protocols: {:?}",
            peers_without_protocol.len(), peers_without_protocol
        )));
    }
    
    // Now proceed with sends knowing all peers are reachable
    let peers_per_protocol = self.group_peers_by_protocol(peers);
    for (protocol_id, peers) in peers_per_protocol {
        for (network_id, peers) in &peers.iter().chunk_by(|peer_network_id| peer_network_id.network_id()) {
            let network_sender = self.get_sender_for_network_id(&network_id)?;
            let peer_ids = peers.map(|peer_network_id| peer_network_id.peer_id());
            network_sender.send_to_many(peer_ids, protocol_id, message.clone())?;
        }
    }
    Ok(())
}
```

2. **Consensus Layer: Proper Error Handling** [4](#0-3) 

Replace warning-only logging with proper error propagation and retry logic:

```rust
pub fn broadcast_without_self(&self, msg: ConsensusMsg) -> Result<(), Error> {
    // ... peer collection logic ...
    
    self.consensus_network_client
        .send_to_many(other_validators, msg)
        .map_err(|err| {
            error!(error = ?err, "Critical: Failed to broadcast consensus message");
            err
        })?;
    
    Ok(())
}
```

3. **Protocol Enforcement During Epoch Changes**: Ensure all validators in an epoch support the same protocol set before starting consensus.

## Proof of Concept

```rust
#[test]
fn test_partial_broadcast_during_protocol_mismatch() {
    use aptos_channels::aptos_channel;
    use aptos_config::network_id::{NetworkId, PeerNetworkId};
    use aptos_types::PeerId;
    use std::collections::HashMap;

    // Setup: Create network client with multiple protocols
    let protocols = vec![
        ProtocolId::ConsensusDirectSendBcs,
        ProtocolId::ConsensusDirectSendCompressed,
    ];
    
    // Create 4 validators
    let v1 = PeerNetworkId::new(NetworkId::Validator, PeerId::random());
    let v2 = PeerNetworkId::new(NetworkId::Validator, PeerId::random());
    let v3 = PeerNetworkId::new(NetworkId::Validator, PeerId::random());
    let v4 = PeerNetworkId::new(NetworkId::Validator, PeerId::random());
    
    // Setup: V1, V2 support only BCS protocol
    // Setup: V3, V4 support only Compressed protocol
    // This simulates a rolling upgrade scenario
    
    let peers_and_metadata = setup_peers_with_mixed_protocols(
        vec![(v1, vec![ProtocolId::ConsensusDirectSendBcs]),
             (v2, vec![ProtocolId::ConsensusDirectSendBcs]),
             (v3, vec![ProtocolId::ConsensusDirectSendCompressed]),
             (v4, vec![ProtocolId::ConsensusDirectSendCompressed])],
    );
    
    let network_client = NetworkClient::new(protocols, vec![], network_senders, peers_and_metadata);
    
    // Simulate: Broadcast a vote to all 4 validators
    let vote_msg = ConsensusMsg::VoteMsg(Box::new(create_test_vote()));
    
    // Close the channel for the Compressed protocol network to simulate failure
    drop_compressed_protocol_sender();
    
    // Attempt broadcast
    let result = network_client.send_to_peers(
        vote_msg,
        vec![v1, v2, v3, v4]
    );
    
    // VULNERABILITY: The function returns an error, BUT:
    // - V1 and V2 (BCS protocol group) already received the message
    // - V3 and V4 (Compressed protocol group) did NOT receive the message
    // This is PARTIAL DELIVERY
    
    assert!(result.is_err(), "Send should fail for compressed group");
    
    // Verify: V1, V2 received message
    assert!(check_peer_received_message(v1, vote_msg.clone()));
    assert!(check_peer_received_message(v2, vote_msg.clone()));
    
    // Verify: V3, V4 did NOT receive message
    assert!(!check_peer_received_message(v3, vote_msg.clone()));
    assert!(!check_peer_received_message(v4, vote_msg.clone()));
    
    // Impact: V1, V2 will count the vote; V3, V4 will timeout
    // If this happens repeatedly, consensus stalls
}
```

## Notes

This vulnerability is particularly concerning because:

1. **It occurs during normal operations**: Rolling upgrades are a standard operational procedure
2. **Silent failures**: The consensus layer doesn't detect that only a subset of validators received messages
3. **Cascading effects**: Repeated partial deliveries can systematically exclude validators from multiple rounds
4. **No recovery mechanism**: There's no automatic retry or fallback when partial delivery occurs

The fix requires coordination between the network layer (ensuring atomic delivery) and the consensus layer (proper error handling and retry logic).

### Citations

**File:** network/framework/src/application/interface.rs (L160-191)
```rust
    fn group_peers_by_protocol(
        &self,
        peers: Vec<PeerNetworkId>,
    ) -> HashMap<ProtocolId, Vec<PeerNetworkId>> {
        // Sort peers by protocol
        let mut peers_per_protocol = HashMap::new();
        let mut peers_without_a_protocol = vec![];
        for peer in peers {
            match self
                .get_preferred_protocol_for_peer(&peer, &self.direct_send_protocols_and_preferences)
            {
                Ok(protocol) => peers_per_protocol
                    .entry(protocol)
                    .or_insert_with(Vec::new)
                    .push(peer),
                Err(_) => peers_without_a_protocol.push(peer),
            }
        }

        // We only periodically log any unavailable peers (to prevent log spamming)
        if !peers_without_a_protocol.is_empty() {
            sample!(
                SampleRate::Duration(Duration::from_secs(10)),
                warn!(
                    "[sampled] Unavailable peers (without a common network protocol): {:?}",
                    peers_without_a_protocol
                )
            );
        }

        peers_per_protocol
    }
```

**File:** network/framework/src/application/interface.rs (L243-258)
```rust
    fn send_to_peers(&self, message: Message, peers: Vec<PeerNetworkId>) -> Result<(), Error> {
        let peers_per_protocol = self.group_peers_by_protocol(peers);

        // Send to all peers in each protocol group and network
        for (protocol_id, peers) in peers_per_protocol {
            for (network_id, peers) in &peers
                .iter()
                .chunk_by(|peer_network_id| peer_network_id.network_id())
            {
                let network_sender = self.get_sender_for_network_id(&network_id)?;
                let peer_ids = peers.map(|peer_network_id| peer_network_id.peer_id());
                network_sender.send_to_many(peer_ids, protocol_id, message.clone())?;
            }
        }
        Ok(())
    }
```

**File:** consensus/src/network.rs (L387-408)
```rust
    pub fn broadcast_without_self(&self, msg: ConsensusMsg) {
        fail_point!("consensus::send::any", |_| ());

        let self_author = self.author;
        let mut other_validators: Vec<_> = self
            .validators
            .get_ordered_account_addresses_iter()
            .filter(|author| author != &self_author)
            .collect();
        self.sort_peers_by_latency(&mut other_validators);

        counters::CONSENSUS_SENT_MSGS
            .with_label_values(&[msg.name()])
            .inc_by(other_validators.len() as u64);
        // Broadcast message over direct-send to all other validators.
        if let Err(err) = self
            .consensus_network_client
            .send_to_many(other_validators, msg)
        {
            warn!(error = ?err, "Error broadcasting message");
        }
    }
```
