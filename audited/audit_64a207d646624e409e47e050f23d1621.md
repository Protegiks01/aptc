# Audit Report

## Title
Cross-Shard Message Loss Due to Stale Cached Dependencies in Sharded Execution

## Summary
The sharded block executor caches cross-shard dependency information based on static write hints during initialization, but this cached data can become stale when transactions are re-executed with different actual write-sets, potentially causing missing cross-shard messages and state inconsistencies across shards.

## Finding Description

The vulnerability exists in the sharded block executor's cross-shard messaging system. The system builds a cache of dependent edges (which shards need to receive messages for which state keys) during initialization based on static write hints from transaction analysis. However, this cache is never updated during transaction re-execution, creating a mismatch between predicted and actual writes.

**The Flow:**

1. **Initialization Phase** - `CrossShardCommitSender` is created once per sub-block with cached `dependent_edges`: [1](#0-0) 

The `dependent_edges` HashMap is populated from the sub-block's static `cross_shard_dependencies`, which are derived from write hints during partitioning: [2](#0-1) 

2. **Write Hints Are Static Predictions** - The write hints are computed once during transaction analysis, not during execution: [3](#0-2) 

3. **Message Sending Uses Cached Dependencies** - When a transaction commits, it sends cross-shard messages based on the CACHED `dependent_edges` but iterates over the ACTUAL runtime write-set: [4](#0-3) 

**Critical Issue at Line 115:** If a state key in the actual write-set is NOT in the cached `dependent_edges` (because it wasn't in the original write hints), `edges.get(state_key)` returns `None`, and NO cross-shard message is sent!

4. **Re-execution Can Change Actual Writes** - During parallel execution with BlockSTM, transactions are re-executed when validation fails: [5](#0-4) 

**The Vulnerability:** The same `CrossShardCommitSender` instance (with stale cached dependencies) is used for all transaction incarnations within a sub-block. If re-execution produces different writes than originally predicted by write hints, dependent shards will not receive the necessary updates, leading to stale reads and state divergence.

## Impact Explanation

**Severity: HIGH** (per Aptos Bug Bounty - "Significant protocol violations")

This vulnerability breaks the **Deterministic Execution** invariant - different validators may produce different state roots for identical blocks if they execute with different shard assignments or timing.

**Concrete Impact:**
- **State Divergence:** Dependent shards read stale data when cross-shard messages are not sent for actual writes
- **Consensus Risk:** Different validators executing the same block may disagree on state, potentially causing consensus failures
- **Non-Determinism:** The same transaction sequence could produce different outputs depending on execution scheduling

**Current Mitigation:** The impact is currently LIMITED because write hint analysis only supports specific transaction types (coin transfer, create account), and unsupported types would panic: [6](#0-5) 

However, this is a **latent vulnerability** that would become exploitable if:
1. Additional transaction types are supported without complete write hint analysis
2. Dynamic execution paths cause writes not predicted by static analysis
3. Move code evolution introduces new write patterns

## Likelihood Explanation

**Current Likelihood: LOW** - The current implementation prevents exploitation by:
- Only supporting transactions with complete, conservative write hint analysis
- Panicking on unsupported transaction types
- Designing write hints to be "strictly overestimated" (including all possible writes)

**Future Likelihood: HIGH** - As the system evolves to support more transaction types or more complex Move execution patterns, the likelihood increases significantly. The architectural flaw would manifest when write hints become incomplete.

## Recommendation

**Solution:** Replace cached static dependencies with dynamic runtime dependency tracking.

**Recommended Fix:**

1. **Option A (Immediate Fix):** Validate at commit time that all actual writes were predicted by write hints, panic if mismatch detected:

```rust
fn send_remote_update_for_success(
    &self,
    txn_idx: TxnIndex,
    txn_output: &OnceCell<TransactionOutput>,
) {
    let edges = self.dependent_edges.get(&txn_idx).unwrap();
    let write_set = txn_output
        .get()
        .expect("Committed output must be set")
        .write_set();

    for (state_key, write_op) in write_set.expect_write_op_iter() {
        if let Some(dependent_shard_ids) = edges.get(state_key) {
            // Send messages...
        } else {
            // CRITICAL: Actual write not in predicted dependencies!
            panic!(
                "Invariant violation: StateKey {:?} written by txn {} but not in write hints. \
                This indicates incomplete static analysis.",
                state_key, txn_idx
            );
        }
    }
}
```

2. **Option B (Robust Fix):** Build dependencies dynamically from actual execution:
   - Store actual write-sets during execution
   - Build cross-shard messages from actual writes, not predicted hints
   - Eliminate reliance on static write hint analysis

3. **Option C (Defense in Depth):** Add runtime verification that write hints match actual writes for all supported transaction types, failing execution if mismatch occurs.

## Proof of Concept

```rust
// Conceptual PoC - would require full integration test infrastructure

#[test]
fn test_stale_cross_shard_dependencies() {
    // Setup: 2 shards, transaction T1 in shard 0, T2 in shard 1
    // T1 write hints predict write to StateKey A
    // But during re-execution, T1 actually writes to StateKey B (dynamic behavior)
    // T2 depends on reading StateKey B
    
    // 1. Initialize with write hints containing only StateKey A
    let sub_block = SubBlock {
        transactions: vec![
            TransactionWithDependencies {
                txn: t1,
                cross_shard_dependencies: {
                    dependent_edges: {
                        edges: { T2_shard_1: [StateKey_A] }
                    }
                }
            }
        ]
    };
    
    // 2. CrossShardCommitSender caches dependencies for StateKey A only
    let sender = CrossShardCommitSender::new(shard_0, client, &sub_block);
    
    // 3. Execute T1 - due to re-execution with different read values,
    //    T1 actually writes to StateKey B instead of StateKey A
    let actual_write_set = execute_transaction(t1);
    assert!(actual_write_set.contains(StateKey_B));
    assert!(!actual_write_set.contains(StateKey_A));
    
    // 4. On commit, sender.on_transaction_committed() is called
    //    It iterates over actual_write_set containing StateKey B
    //    But sender.dependent_edges only has mapping for StateKey A
    //    Result: edges.get(StateKey_B) returns None
    //    NO MESSAGE SENT TO SHARD 1!
    
    // 5. T2 in shard 1 executes and tries to read StateKey B
    //    Without the cross-shard message, T2 reads stale/wrong value
    //    STATE DIVERGENCE!
}
```

## Notes

While this vulnerability is architecturally present, its **current exploitability is limited** by defensive programming that restricts supported transaction types. The issue represents a **design flaw** that should be addressed before expanding transaction support or when the system inevitably evolves to handle more complex execution patterns.

The vulnerability is particularly concerning because:
1. It's subtle and would not be caught by standard testing
2. It manifests only under specific execution interleavings
3. It breaks core blockchain invariants (determinism, state consistency)
4. The current mitigation (panic on unsupported types) is fragile and will break as the system evolves

**Recommended Action:** Implement Option A (validation panic) immediately as defense-in-depth, and plan Option B (dynamic dependency tracking) for long-term robustness.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L61-101)
```rust
impl CrossShardCommitSender {
    pub fn new(
        shard_id: ShardId,
        cross_shard_client: Arc<dyn CrossShardClient>,
        sub_block: &SubBlock<AnalyzedTransaction>,
    ) -> Self {
        let mut dependent_edges = HashMap::new();
        let mut num_dependent_edges = 0;
        for (txn_idx, txn_with_deps) in sub_block.txn_with_index_iter() {
            let mut storage_locations_to_target = HashMap::new();
            for (txn_id_with_shard, storage_locations) in txn_with_deps
                .cross_shard_dependencies
                .dependent_edges()
                .iter()
            {
                for storage_location in storage_locations {
                    storage_locations_to_target
                        .entry(storage_location.clone().into_state_key())
                        .or_insert_with(HashSet::new)
                        .insert((txn_id_with_shard.shard_id, txn_id_with_shard.round_id));
                    num_dependent_edges += 1;
                }
            }
            if !storage_locations_to_target.is_empty() {
                dependent_edges.insert(txn_idx as TxnIndex, storage_locations_to_target);
            }
        }

        trace!(
            "CrossShardCommitSender::new: shard_id: {:?}, num_dependent_edges: {:?}",
            shard_id,
            num_dependent_edges
        );

        Self {
            shard_id,
            cross_shard_client,
            dependent_edges,
            index_offset: sub_block.start_index as TxnIndex,
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L103-134)
```rust
    fn send_remote_update_for_success(
        &self,
        txn_idx: TxnIndex,
        txn_output: &OnceCell<TransactionOutput>,
    ) {
        let edges = self.dependent_edges.get(&txn_idx).unwrap();
        let write_set = txn_output
            .get()
            .expect("Committed output must be set")
            .write_set();

        for (state_key, write_op) in write_set.expect_write_op_iter() {
            if let Some(dependent_shard_ids) = edges.get(state_key) {
                for (dependent_shard_id, round_id) in dependent_shard_ids.iter() {
                    trace!("Sending remote update for success for shard id {:?} and txn_idx: {:?}, state_key: {:?}, dependent shard id: {:?}", self.shard_id, txn_idx, state_key, dependent_shard_id);
                    let message = RemoteTxnWriteMsg(RemoteTxnWrite::new(
                        state_key.clone(),
                        Some(write_op.clone()),
                    ));
                    if *round_id == GLOBAL_ROUND_ID {
                        self.cross_shard_client.send_global_msg(message);
                    } else {
                        self.cross_shard_client.send_cross_shard_msg(
                            *dependent_shard_id,
                            *round_id,
                            message,
                        );
                    }
                }
            }
        }
    }
```

**File:** execution/block-partitioner/src/v2/state.rs (L323-348)
```rust
        // Build dependent edges.
        for &key_idx in self.write_sets[ori_txn_idx].read().unwrap().iter() {
            if Some(txn_idx) == self.last_writer(key_idx, SubBlockIdx { round_id, shard_id }) {
                let start_of_next_sub_block = ShardedTxnIndexV2::new(round_id, shard_id + 1, 0);
                let next_writer = self.first_writer(key_idx, start_of_next_sub_block);
                let end_follower = match next_writer {
                    None => ShardedTxnIndexV2::new(self.num_rounds(), self.num_executor_shards, 0), // Guaranteed to be greater than any invalid idx...
                    Some(idx) => ShardedTxnIndexV2::new(idx.round_id(), idx.shard_id() + 1, 0),
                };
                for follower_txn_idx in
                    self.all_txns_in_sub_block_range(key_idx, start_of_next_sub_block, end_follower)
                {
                    let final_sub_blk_idx =
                        self.final_sub_block_idx(follower_txn_idx.sub_block_idx);
                    let dst_txn_idx = ShardedTxnIndex {
                        txn_index: *self.final_idxs_by_pre_partitioned
                            [follower_txn_idx.pre_partitioned_txn_idx]
                            .read()
                            .unwrap(),
                        shard_id: final_sub_blk_idx.shard_id,
                        round_id: final_sub_blk_idx.round_id,
                    };
                    deps.add_dependent_edge(dst_txn_idx, vec![self.storage_location(key_idx)]);
                }
            }
        }
```

**File:** types/src/transaction/analyzed_transaction.rs (L23-37)
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct AnalyzedTransaction {
    transaction: SignatureVerifiedTransaction,
    /// Set of storage locations that are read by the transaction - this doesn't include location
    /// that are written by the transactions to avoid duplication of locations across read and write sets
    /// This can be accurate or strictly overestimated.
    pub read_hints: Vec<StorageLocation>,
    /// Set of storage locations that are written by the transaction. This can be accurate or strictly
    /// overestimated.
    pub write_hints: Vec<StorageLocation>,
    /// A transaction is predictable if neither the read_hint or the write_hint have wildcards.
    predictable_transaction: bool,
    /// The hash of the transaction - this is cached for performance reasons.
    hash: HashValue,
}
```

**File:** types/src/transaction/analyzed_transaction.rs (L266-269)
```rust
                _ => todo!(
                    "Only coin transfer and create account transactions are supported for now"
                ),
            }
```

**File:** aptos-move/block-executor/src/executor.rs (L896-978)
```rust
    fn execute_txn_after_commit(
        txn: &T,
        auxiliary_info: &A,
        txn_idx: TxnIndex,
        incarnation: Incarnation,
        scheduler: SchedulerWrapper,
        versioned_cache: &MVHashMap<T::Key, T::Tag, T::Value, DelayedFieldID>,
        last_input_output: &TxnLastInputOutput<T, E::Output>,
        start_shared_counter: u32,
        shared_counter: &AtomicU32,
        executor: &E,
        base_view: &S,
        global_module_cache: &GlobalModuleCache<
            ModuleId,
            CompiledModule,
            Module,
            AptosModuleExtension,
        >,
        runtime_environment: &RuntimeEnvironment,
        block_gas_limit_type: &BlockGasLimitType,
    ) -> Result<(), PanicError> {
        let parallel_state = ParallelState::new(
            versioned_cache,
            scheduler,
            start_shared_counter,
            shared_counter,
            incarnation,
        );

        match scheduler.as_v2() {
            None => {
                // We are ignoring _needs_suffix_validation, as the caller will reduce the
                // validation index unconditionally after execute_txn_after_commit call.
                Self::execute(
                    txn_idx,
                    incarnation,
                    txn,
                    auxiliary_info,
                    None,
                    last_input_output,
                    versioned_cache,
                    executor,
                    base_view,
                    global_module_cache,
                    runtime_environment,
                    parallel_state,
                    block_gas_limit_type,
                )?;
            },
            Some((scheduler, worker_id)) => {
                Self::execute_v2(
                    worker_id,
                    txn_idx,
                    incarnation,
                    txn,
                    auxiliary_info,
                    last_input_output,
                    versioned_cache,
                    executor,
                    base_view,
                    global_module_cache,
                    runtime_environment,
                    parallel_state,
                    scheduler,
                    block_gas_limit_type,
                )?;
            },
        }

        if !Self::validate_and_commit_delayed_fields(
            txn_idx,
            versioned_cache,
            last_input_output,
            scheduler.is_v2(),
        )? {
            return Err(code_invariant_error(format!(
                "Delayed field validation after re-execution failed for txn {}",
                txn_idx
            )));
        }

        Ok(())
    }
```
