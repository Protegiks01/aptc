# Audit Report

## Title
State Synchronization Desynchronization Leading to Missing State Chunks and Denial of Service

## Summary
The `StateStreamEngine` maintains two separate indices (`next_stream_index` and `next_request_index`) that can desynchronize when processing partial responses from malicious peers, causing permanent gaps in state chunk requests and blocking state synchronization indefinitely.

## Finding Description
The vulnerability exists in the `StateStreamEngine::transform_client_response_into_notification()` function where the stream engine updates `next_stream_index` based on the actual data received in responses, while `next_request_index` was already updated eagerly when the request was created. This breaks the **State Consistency** invariant that state transitions must be atomic and verifiable.

**Root Cause:**

The stream engine uses two indices to track progress:
- `next_request_index`: Updated EAGERLY when requests are created (based on `request.end_index + 1`) [1](#0-0) 
- `next_stream_index`: Updated LAZILY when responses are received (based on `response.last_index + 1`) [2](#0-1) 

When a malicious or resource-constrained peer returns a partial response where `last_index < request.end_index` (which is legitimate per the protocol's size-based chunking), the indices desynchronize:

**Attack Execution:**
1. Node requests state values for indices [0, 99]
2. `update_request_tracking()` immediately sets `next_request_index = 100` [3](#0-2) 
3. Malicious peer responds with `StateValueChunkWithProof{first_index: 0, last_index: 50, raw_values: [51 items]}`
4. `transform_client_response_into_notification()` verifies the response is not empty [4](#0-3)  and bounds the last_index [5](#0-4) 
5. Stream engine updates `next_stream_index = 51`
6. Next request batch starts from `next_request_index = 100`, creating requests for indices [100, 199]
7. When the response for [100, 199] arrives, `verify_client_request_indices(51, 100, 199)` fails [6](#0-5)  because `start_index (100) != expected_next_index (51)`
8. Request is retried indefinitely with the same parameters, but verification continues to fail
9. **States 51-99 are never requested and state reconstruction remains incomplete**

The protocol explicitly allows partial responses due to size constraints enforced by the storage service [7](#0-6) , making this a realistic attack vector.

## Impact Explanation
This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria:

1. **Validator Node Slowdown/Failure**: Nodes attempting to sync state become stuck in an error loop, unable to complete state synchronization
2. **Significant Protocol Violation**: Breaks the state consistency invariant - nodes cannot reconstruct complete state snapshots
3. **Availability Impact**: While not total network unavailability, malicious peers can prevent new nodes from joining the network and existing nodes from fast-syncing

The attack affects state synchronization during:
- Node bootstrapping from genesis
- Fast sync to catch up with network
- State snapshot restoration

Nodes remain functional for consensus if already synced, but cannot recover from state divergence or onboard new validators.

## Likelihood Explanation
**Likelihood: High**

1. **Ease of Exploitation**: Any network peer can trigger this by simply returning legitimate partial responses (no special crafting needed beyond choosing `last_index`)
2. **No Authentication Required**: The stream engine processes responses from any peer without requiring validator privileges
3. **Natural Occurrence**: Even non-malicious peers may trigger this accidentally due to size limits, making it a reliability issue as well
4. **Persistent Effect**: Once triggered, the desynchronization persists until stream reset, and the gap in state chunks remains

## Recommendation
Synchronize both indices by updating `next_request_index` lazily after response validation, not eagerly during request creation. Modify the flow to:

1. Keep `next_request_index` aligned with `next_stream_index` when creating requests
2. Only advance `next_request_index` after successfully processing a response
3. Add validation that partial responses don't create gaps

**Proposed Fix in `StateStreamEngine`:**

```rust
// In update_request_tracking, remove the eager update
fn update_request_tracking(
    &mut self,
    client_requests: &[DataClientRequest],
) -> Result<(), Error> {
    // Track pending requests but don't update next_request_index yet
    self.pending_requests.extend(client_requests.clone());
    Ok(())
}

// In transform_client_response_into_notification, update both indices together
// After line 341:
if last_received_index < request.end_index {
    // Partial response detected - adjust next_request_index to match
    self.next_request_index = self.next_stream_index;
}
```

Alternatively, reset `next_request_index = next_stream_index` at the start of `create_data_client_requests()` to ensure they remain synchronized.

## Proof of Concept
```rust
#[tokio::test]
async fn test_state_stream_desynchronization() {
    // Setup: Create StateStreamEngine with 1000 total states
    let mut engine = StateStreamEngine::new(&GetAllStatesRequest {
        version: 100,
        start_index: 0,
    }).unwrap();
    engine.number_of_states = Some(1000);
    
    // Step 1: Create first batch of requests (indices 0-99)
    let requests = engine.create_data_client_requests(
        10, 10, 0, &mock_global_summary(), mock_id_gen()
    ).unwrap();
    assert_eq!(requests[0], StateValuesWithProof { 
        start_index: 0, end_index: 99, .. 
    });
    // next_request_index is now 100
    assert_eq!(engine.next_request_index, 100);
    
    // Step 2: Process malicious partial response (only indices 0-50)
    let partial_response = ResponsePayload::StateValuesWithProof(
        StateValueChunkWithProof {
            first_index: 0,
            last_index: 50,  // Partial! Should be 99
            raw_values: vec![...; 51],  // Only 51 items
            ..
        }
    );
    engine.transform_client_response_into_notification(
        &requests[0], partial_response, mock_id_gen()
    ).unwrap();
    // next_stream_index is now 51
    assert_eq!(engine.next_stream_index, 51);
    
    // Step 3: Create next batch - starts from next_request_index=100, not 51!
    let next_requests = engine.create_data_client_requests(
        10, 10, 0, &mock_global_summary(), mock_id_gen()
    ).unwrap();
    assert_eq!(next_requests[0].start_index, 100);
    // GAP CREATED: States 51-99 never requested!
    
    // Step 4: Process response for [100, 199] - verification fails
    let next_response = ResponsePayload::StateValuesWithProof(
        StateValueChunkWithProof {
            first_index: 100,
            last_index: 199,
            raw_values: vec![...; 100],
            ..
        }
    );
    let result = engine.transform_client_response_into_notification(
        &next_requests[0], next_response, mock_id_gen()
    );
    // Verification fails: expected start_index 51, got 100
    assert!(result.is_err());
    assert!(result.unwrap_err().to_string().contains(
        "The start index did not match the expected next index"
    ));
    
    // Stream is now stuck - states 51-99 will never be requested
}
```

## Notes
This vulnerability demonstrates a fundamental flaw in the state streaming protocol where optimistic request batching assumptions don't account for legitimate partial responses. The issue is exacerbated by the lack of coordination between the two index tracking mechanisms. While the bootstrapper layer has additional validations [8](#0-7) , these only verify individual chunk consistency, not the gap between requested and received ranges at the stream engine level.

### Citations

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L201-217)
```rust
    fn update_request_tracking(
        &mut self,
        client_requests: &[DataClientRequest],
    ) -> Result<(), Error> {
        for client_request in client_requests {
            match client_request {
                StateValuesWithProof(request) => {
                    self.next_request_index =
                        request.end_index.checked_add(1).ok_or_else(|| {
                            Error::IntegerOverflow("Next request index has overflown!".into())
                        })?;
                },
                request => invalid_client_request!(request, self),
            }
        }
        Ok(())
    }
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L318-327)
```rust
                // Identify the last received state index and bound it appropriately
                let last_received_index = match &client_response_payload {
                    ResponsePayload::StateValuesWithProof(state_values_with_proof) => {
                        // Verify that we received at least one state value
                        if state_values_with_proof.raw_values.is_empty() {
                            return Err(Error::AptosDataClientResponseIsInvalid(format!(
                                "Received an empty state values response! Request: {:?}",
                                client_request
                            )));
                        }
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L334-335)
```rust
                let last_received_index =
                    bound_by_range(last_received_index, request.start_index, request.end_index);
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L337-341)
```rust
                // Update the next stream index
                self.next_stream_index = last_received_index.checked_add(1).ok_or_else(|| {
                    Error::IntegerOverflow("Next stream index has overflown!".into())
                })?;

```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L2011-2030)
```rust
fn verify_client_request_indices(
    expected_next_index: u64,
    start_index: u64,
    end_index: u64,
) -> Result<(), Error> {
    if start_index != expected_next_index {
        return Err(Error::UnexpectedErrorEncountered(format!(
            "The start index did not match the expected next index! Given: {:?}, expected: {:?}",
            start_index, expected_next_index
        )));
    }

    if end_index < expected_next_index {
        return Err(Error::UnexpectedErrorEncountered(format!(
            "The end index was less than the expected next index! Given: {:?}, expected: {:?}",
            end_index, expected_next_index
        )));
    }

    Ok(())
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1083-1115)
```rust
    pub fn get_value_chunk_with_proof(
        self: &Arc<Self>,
        version: Version,
        first_index: usize,
        chunk_size: usize,
    ) -> Result<StateValueChunkWithProof> {
        let state_key_values: Vec<(StateKey, StateValue)> = self
            .get_value_chunk_iter(version, first_index, chunk_size)?
            .collect::<Result<Vec<_>>>()?;
        self.get_value_chunk_proof(version, first_index, state_key_values)
    }

    pub fn get_value_chunk_iter(
        self: &Arc<Self>,
        version: Version,
        first_index: usize,
        chunk_size: usize,
    ) -> Result<impl Iterator<Item = Result<(StateKey, StateValue)>> + Send + Sync + use<>> {
        let store = Arc::clone(self);
        let value_chunk_iter = JellyfishMerkleIterator::new_by_index(
            Arc::clone(&self.state_merkle_db),
            version,
            first_index,
        )?
        .take(chunk_size)
        .map(move |res| {
            res.and_then(|(_, (key, version))| {
                Ok((key.clone(), store.expect_value_by_version(&key, version)?))
            })
        });

        Ok(value_chunk_iter)
    }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L937-956)
```rust
        // Verify the end index and number of state values is valid
        let expected_num_state_values = state_value_chunk_with_proof
            .last_index
            .checked_sub(state_value_chunk_with_proof.first_index)
            .and_then(|version| version.checked_add(1)) // expected_num_state_values = last_index - first_index + 1
            .ok_or_else(|| {
                Error::IntegerOverflow("The expected number of state values has overflown!".into())
            })?;
        let num_state_values = state_value_chunk_with_proof.raw_values.len() as u64;
        if expected_num_state_values != num_state_values {
            self.reset_active_stream(Some(NotificationAndFeedback::new(
                notification_id,
                NotificationFeedback::InvalidPayloadData,
            )))
            .await?;
            return Err(Error::VerificationError(format!(
                "The expected number of state values was invalid! Expected: {:?}, received: {:?}",
                expected_num_state_values, num_state_values,
            )));
        }
```
