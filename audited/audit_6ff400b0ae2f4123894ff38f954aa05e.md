# Audit Report

## Title
Consensus Pipeline Liveness Failure: Execution Error Responses Cause Pipeline Halt Due to Missing Retry Mechanism

## Summary
The consensus pipeline's BufferManager fails to retry blocks when execution errors occur. When the execution phase returns an error response, the block remains in "Ordered" state but no retry request is scheduled, causing the consensus pipeline to stall until epoch boundary or manual reset.

## Finding Description

The consensus pipeline processes blocks through multiple phases: execution_schedule → execution_wait → signing → persisting. [1](#0-0) 

The `ExecutionResponse` type contains an `ExecutorResult` which can be either successful results or executor errors. [2](#0-1) 

When the BufferManager receives an execution error response, it logs the error and returns early WITHOUT advancing the buffer item state. [3](#0-2) 

The block remains in "Ordered" state. The `advance_execution_root()` method is designed to detect this situation and return `Some(block_id)` to indicate retry is needed. [4](#0-3) 

However, in the main event loop, the return value from `advance_execution_root()` is completely ignored. [5](#0-4) 

This contrasts with the signing phase, which properly implements retry logic when the signing_root hasn't changed. [6](#0-5) 

**Execution Flow:**
1. Ordered blocks sent to ExecutionSchedulePhase
2. `wait_for_compute_result()` called on each block [7](#0-6) 
3. If execution fails, error propagates to ExecutionWaitPhase [8](#0-7) 
4. BufferManager processes error response but doesn't schedule retry
5. Block remains stuck in "Ordered" state
6. Pipeline cannot progress past failed block

Execution errors include `CouldNotGetData`, `BlockNotFound`, `InternalError`, and others. [9](#0-8) 

These errors are logged with specific handling for different types. [10](#0-9) 

## Impact Explanation

**Severity: HIGH** per Aptos bug bounty criteria.

This vulnerability causes validator liveness failure:

1. **Validator Unresponsiveness**: The affected validator cannot process blocks until epoch boundary or manual reset
2. **Reduced Network Resilience**: If multiple validators encounter similar errors, network capacity degrades
3. **Manual Intervention Required**: Recovery requires either waiting for epoch boundary or manual reset signal [11](#0-10) 

This qualifies as "Validator Node Slowdowns (High)" per Aptos criteria: significant performance degradation affecting consensus participation. While automatic recovery occurs at epoch boundaries [12](#0-11) , within-epoch failures require operator intervention.

## Likelihood Explanation

**Likelihood: MEDIUM**

Execution errors can occur through realistic scenarios:

1. **Database Errors**: Converted to `InternalError` [13](#0-12) 
2. **Request Timeouts**: `CouldNotGetData` errors during high load [14](#0-13) 
3. **State Inconsistencies**: `BlockNotFound` when speculation results unavailable [15](#0-14) 

The missing retry mechanism is a clear logic bug - `advance_execution_root()` is designed to return retry signals, but this value is never used.

## Recommendation

Implement execution retry logic similar to the signing phase:

```rust
// In BufferManager.start() event loop, after line 957:
async fn advance_signing_root(&mut self) {
    // ... existing code ...
    if cursor == self.signing_root {
        let sender = self.signing_phase_tx.clone();
        Self::spawn_retry_request(sender, request, Duration::from_millis(100));
    }
}
```

Apply the same pattern to execution by capturing and using the return value from `advance_execution_root()`:

```rust
Some(response) = self.execution_wait_phase_rx.next() => {
    self.process_execution_response(response).await;
    if let Some(block_id) = self.advance_execution_root() {
        // Schedule retry for failed execution
        let sender = self.execution_schedule_phase_tx.clone();
        let item = self.buffer.get(&Some(block_id));
        let request = self.create_new_request(ExecutionRequest {
            ordered_blocks: item.get_blocks().clone(),
        });
        Self::spawn_retry_request(sender, request, Duration::from_millis(100));
    }
    if self.signing_root.is_none() {
        self.advance_signing_root().await;
    }
}
```

## Proof of Concept

The vulnerability can be demonstrated by examining the asymmetry between signing and execution retry logic. The test `test_execution_retry` validates block preparation retry (different phase) but not execution phase retry. [16](#0-15) 

The block preparation phase has explicit retry logic in an infinite loop. [17](#0-16) 

However, the execution phase (after ordering) lacks any retry mechanism, as evidenced by the ignored return value at line 957 of buffer_manager.rs.

## Notes

This is a valid logic vulnerability where the retry mechanism is partially implemented (`advance_execution_root()` returns retry signal) but never utilized. The asymmetry with the signing phase (which has working retry) confirms this is an implementation gap rather than intentional design. Recovery mechanisms exist (epoch boundaries, manual reset) but do not provide automatic within-epoch retry for transient execution errors.

### Citations

**File:** consensus/src/pipeline/buffer_manager.rs (L102-104)
```rust
/// BufferManager handles the states of ordered blocks and
/// interacts with the execution phase, the signing phase, and
/// the persisting phase.
```

**File:** consensus/src/pipeline/buffer_manager.rs (L436-438)
```rust
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
```

**File:** consensus/src/pipeline/buffer_manager.rs (L478-480)
```rust
            if cursor == self.signing_root {
                let sender = self.signing_phase_tx.clone();
                Self::spawn_retry_request(sender, request, Duration::from_millis(100));
```

**File:** consensus/src/pipeline/buffer_manager.rs (L530-533)
```rust
                if commit_proof.ledger_info().ends_epoch() {
                    // the epoch ends, reset to avoid executing more blocks, execute after
                    // this persisting request will result in BlockNotFound
                    self.reset().await;
```

**File:** consensus/src/pipeline/buffer_manager.rs (L578-596)
```rust
    /// It pops everything in the buffer and if reconfig flag is set, it stops the main loop
    async fn process_reset_request(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        info!("Receive reset");

        match signal {
            ResetSignal::Stop => self.stop = true,
            ResetSignal::TargetRound(round) => {
                self.highest_committed_round = round;
                self.latest_round = round;

                let _ = self.drain_pending_commit_proof_till(round);
            },
        }

        self.reset().await;
        let _ = tx.send(ResetAck::default());
        info!("Reset finishes");
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L617-626)
```rust
        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
```

**File:** consensus/src/pipeline/buffer_manager.rs (L954-960)
```rust
                Some(response) = self.execution_wait_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_wait_response", {
                    self.process_execution_response(response).await;
                    self.advance_execution_root();
                    if self.signing_root.is_none() {
                        self.advance_signing_root().await;
                    }});
```

**File:** consensus/src/pipeline/execution_wait_phase.rs (L35-38)
```rust
pub struct ExecutionResponse {
    pub block_id: HashValue,
    pub inner: ExecutorResult<Vec<Arc<PipelinedBlock>>>,
}
```

**File:** consensus/src/pipeline/execution_wait_phase.rs (L49-56)
```rust
    async fn process(&self, req: ExecutionWaitRequest) -> ExecutionResponse {
        let ExecutionWaitRequest { block_id, fut } = req;

        ExecutionResponse {
            block_id,
            inner: fut.await,
        }
    }
```

**File:** consensus/src/pipeline/execution_schedule_phase.rs (L72-73)
```rust
                let (compute_result, execution_time) = b.wait_for_compute_result().await?;
                b.set_compute_result(compute_result, execution_time);
```

**File:** execution/executor-types/src/error.rs (L13-43)
```rust
pub enum ExecutorError {
    #[error("Cannot find speculation result for block id {0}")]
    BlockNotFound(HashValue),

    #[error("Cannot get data for batch id {0}")]
    DataNotFound(HashValue),

    #[error(
        "Bad num_txns_to_commit. first version {}, num to commit: {}, target version: {}",
        first_version,
        to_commit,
        target_version
    )]
    BadNumTxnsToCommit {
        first_version: Version,
        to_commit: usize,
        target_version: Version,
    },

    #[error("Internal error: {:?}", error)]
    InternalError { error: String },

    #[error("Serialization error: {0}")]
    SerializationError(String),

    #[error("Received Empty Blocks")]
    EmptyBlocks,

    #[error("request timeout")]
    CouldNotGetData,
}
```

**File:** execution/executor-types/src/error.rs (L53-58)
```rust
impl From<AptosDbError> for ExecutorError {
    fn from(error: AptosDbError) -> Self {
        Self::InternalError {
            error: format!("{}", error),
        }
    }
```

**File:** consensus/src/counters.rs (L1189-1210)
```rust
    match e {
        ExecutorError::CouldNotGetData => {
            counter.with_label_values(&["CouldNotGetData"]).inc();
            warn!(
                block_id = block_id,
                "Execution error - CouldNotGetData {}", block_id
            );
        },
        ExecutorError::BlockNotFound(block_id) => {
            counter.with_label_values(&["BlockNotFound"]).inc();
            warn!(
                block_id = block_id,
                "Execution error BlockNotFound {}", block_id
            );
        },
        e => {
            counter.with_label_values(&["UnexpectedError"]).inc();
            warn!(
                block_id = block_id,
                "Execution error {:?} for {}", e, block_id
            );
        },
```

**File:** testsuite/smoke-test/src/consensus/consensus_fault_tolerance.rs (L312-349)
```rust
async fn test_execution_retry() {
    let num_validators = 4;

    let swarm = create_swarm(num_validators, 1).await;
    let (validator_clients, public_info) = {
        (
            swarm.get_validator_clients_with_names(),
            swarm.aptos_public_info(),
        )
    };
    test_consensus_fault_tolerance(
        validator_clients,
        public_info,
        3,
        5.0,
        1,
        Box::new(FailPointFailureInjection::new(Box::new(move |cycle, _| {
            (
                vec![(
                    cycle % num_validators,
                    "consensus::prepare_block".to_string(),
                    format!("{}%return", 50),
                )],
                true,
            )
        }))),
        Box::new(
            move |_, executed_epochs, executed_rounds, executed_transactions, _, _| {
                successful_criteria(executed_epochs, executed_rounds, executed_transactions);
                Ok(())
            },
        ),
        true,
        false,
    )
    .await
    .unwrap();
}
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L634-646)
```rust
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
```
