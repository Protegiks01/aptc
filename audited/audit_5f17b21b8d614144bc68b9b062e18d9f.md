# Audit Report

## Title
Unrecoverable Background Task Termination in Peer Location Updater Due to Missing Panic Safety

## Summary
The `PeerLocationUpdater::run()` function spawns a background task that can permanently terminate if `query_peer_locations()` panics instead of returning an error, causing the telemetry service to stop updating peer location data forever without any recovery mechanism.

## Finding Description
The `PeerLocationUpdater::run()` function in the Aptos telemetry service spawns a background task to periodically update peer location information from BigQuery. [1](#0-0) 

The task contains an infinite loop that calls `query_peer_locations()` and handles the result with a `match` statement that only catches `Ok` and `Err` cases. If `query_peer_locations()` panics instead of returning an error, the panic will propagate through the `await` point and exit the loop, causing the tokio task to terminate. tokio::spawn will catch the panic to prevent process termination, but the task will not be restarted.

Potential panic sources in `query_peer_locations()` include:

1. **Lazy metric initialization failures**: The metrics `BIG_QUERY_REQUEST_TOTAL` and `BIG_QUERY_REQUEST_FAILURES_TOTAL` are defined as `Lazy<IntCounter>` with `.unwrap()` during registration. [2](#0-1) 

2. **Third-party library panics**: The BigQuery client library or ResultSet implementation could panic due to unexpected data, resource exhaustion, or internal bugs.

3. **Unexpected runtime conditions**: System time errors or other environmental issues could trigger panics in supporting code.

When `query_peer_locations()` is called, it increments metrics before making the BigQuery request. [3](#0-2) 

If any panic occurs during metric access, BigQuery interaction, or result processing, the background task terminates permanently. The service continues running, but peer location data becomes stale and is never updated again.

## Impact Explanation
This issue qualifies as **Medium Severity** per Aptos bug bounty criteria because it causes:

1. **Service degradation**: The telemetry service continues accepting requests but serves increasingly stale peer location data
2. **State inconsistencies requiring intervention**: Manual service restart is needed to resume location updates
3. **Limited observability impact**: Monitoring and geographic telemetry features become unreliable

While this doesn't directly affect blockchain consensus, transaction execution, or funds security, it degrades a production service that provides operational visibility into the Aptos network. The issue requires manual intervention (service restart) to recover, making it more severe than a transient error.

## Likelihood Explanation
**Likelihood: Low-Medium**

Panics are uncommon in production Rust code, but several realistic scenarios could trigger this issue:

1. **First deployment or metric registration conflicts**: If metrics fail to register (duplicate names, resource limits), the first call to `.inc()` will panic
2. **Third-party library bugs**: The BigQuery client could panic on malformed responses or network anomalies
3. **Resource exhaustion**: Out-of-memory or other resource limits could cause panics in allocation-heavy operations
4. **System time anomalies**: Though rare, system clock issues could trigger panics in timestamp operations

Once triggered, the impact is permanent until manual intervention, increasing the effective severity despite moderate likelihood.

## Recommendation
Implement panic recovery using `std::panic::catch_unwind` or refactor to use a more robust task supervision pattern. Here's the recommended fix:

```rust
pub fn run(self) -> anyhow::Result<()> {
    tokio::spawn(async move {
        loop {
            // Wrap the entire query operation in catch_unwind
            let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
                tokio::runtime::Handle::current().block_on(async {
                    query_peer_locations(&self.client).await
                })
            }));
            
            match result {
                Ok(Ok(locations)) => {
                    let mut peer_locations = self.peer_locations.write();
                    *peer_locations = locations;
                },
                Ok(Err(e)) => {
                    aptos_logger::error!("Failed to query peer locations: {}", e);
                },
                Err(panic_err) => {
                    aptos_logger::error!(
                        "Panic in query_peer_locations (task continuing): {:?}", 
                        panic_err
                    );
                },
            }
            tokio::time::sleep(Duration::from_secs(3600)).await;
        }
    });
    Ok(())
}
```

Alternatively, use a task supervision framework or ensure all operations return Results rather than panicking.

## Proof of Concept

```rust
#[cfg(test)]
mod panic_safety_tests {
    use super::*;
    use std::sync::Arc;
    use aptos_infallible::RwLock;
    use std::collections::HashMap;
    
    #[tokio::test]
    async fn test_panic_terminates_background_task() {
        // Create a mock BigQuery client that will cause a panic
        // (In reality, you'd need to inject a panic into query_peer_locations)
        
        let peer_locations = Arc::new(RwLock::new(HashMap::new()));
        
        // Simulate the task spawning and panic
        let handle = tokio::spawn(async move {
            // Simulate what happens in the run() loop
            loop {
                // This simulates a panic in query_peer_locations
                panic!("Simulated BigQuery panic");
                #[allow(unreachable_code)]
                tokio::time::sleep(Duration::from_secs(1)).await;
            }
        });
        
        // Wait a bit to ensure the task has started
        tokio::time::sleep(Duration::from_millis(100)).await;
        
        // Verify the task has terminated due to panic
        assert!(handle.is_finished());
        
        // The task is permanently dead and will never restart
        // In the real service, peer_locations would never be updated again
    }
}
```

**Notes**

This vulnerability affects the **aptos-telemetry-service**, which is an operational/monitoring component rather than a core blockchain consensus or execution component. While it doesn't directly impact consensus safety, transaction execution, or funds security, it does cause permanent service degradation requiring manual intervention.

The issue demonstrates a common Rust pattern mistake: using `match` on Results doesn't catch panics, which can permanently terminate background tasks spawned with `tokio::spawn`. The telemetry service would continue running and accepting requests, but peer location data would become increasingly stale without any error indication beyond log messages at panic time.

Similar panic safety issues may exist in other background tasks like `PeerSetCacheUpdater` [4](#0-3)  and `PrometheusExporter` [5](#0-4) , which use the same pattern of spawning tasks without panic recovery.

### Citations

**File:** crates/aptos-telemetry-service/src/peer_location.rs (L40-56)
```rust
    pub fn run(self) -> anyhow::Result<()> {
        tokio::spawn(async move {
            loop {
                match query_peer_locations(&self.client).await {
                    Ok(locations) => {
                        let mut peer_locations = self.peer_locations.write();
                        *peer_locations = locations;
                    },
                    Err(e) => {
                        aptos_logger::error!("Failed to query peer locations: {}", e);
                    },
                }
                tokio::time::sleep(Duration::from_secs(3600)).await; // 1 hour
            }
        });
        Ok(())
    }
```

**File:** crates/aptos-telemetry-service/src/peer_location.rs (L107-117)
```rust
    BIG_QUERY_REQUEST_TOTAL.inc();

    let mut res = client
        .job()
        .query(ANALYTICS_PROJECT_ID, req)
        .await
        .map_err(|e| {
            BIG_QUERY_REQUEST_FAILURES_TOTAL.inc();
            aptos_logger::error!("Failed to query peer locations: {}", e);
            e
        })?;
```

**File:** crates/aptos-telemetry-service/src/metrics.rs (L195-209)
```rust
pub(crate) static BIG_QUERY_REQUEST_TOTAL: Lazy<IntCounter> = Lazy::new(|| {
    register_int_counter!(
        "big_query_request_total",
        "Total number of big query requests"
    )
    .unwrap()
});

pub(crate) static BIG_QUERY_REQUEST_FAILURES_TOTAL: Lazy<IntCounter> = Lazy::new(|| {
    register_int_counter!(
        "big_query_request_failures_total",
        "Total number of big query request failures"
    )
    .unwrap()
});
```

**File:** crates/aptos-telemetry-service/src/metrics.rs (L384-395)
```rust
    pub fn run(self) {
        tokio::spawn(async move {
            let mut interval = time::interval(METRICS_EXPORT_FREQUENCY);
            loop {
                interval.tick().await;
                match self.gather_and_send().await {
                    Ok(()) => debug!("service metrics exported successfully"),
                    Err(err) => error!("error exporting metrics {}", err),
                }
            }
        });
    }
```

**File:** crates/aptos-telemetry-service/src/validator_cache.rs (L51-59)
```rust
    pub fn run(self) {
        let mut interval = time::interval(self.update_interval);
        tokio::spawn(async move {
            loop {
                self.update().await;
                interval.tick().await;
            }
        });
    }
```
