# Audit Report

## Title
TOCTOU Race Between Pruner and State Iterator Creation Causes Consensus-Breaking State Reads

## Summary
A critical Time-Of-Check-Time-Of-Use (TOCTOU) race condition exists between the version pruning check and RocksDB iterator creation in `get_prefixed_state_value_iterator()`. The pruner can delete state data after the version check passes but before the iterator's implicit snapshot is taken, causing validators to read incomplete state and break consensus determinism.

## Finding Description

The vulnerability occurs in the state reading critical path when validators execute blocks and need to iterate over state values. [1](#0-0) 

The flow proceeds as follows:

1. **Version Check**: `error_if_state_kv_pruned()` atomically reads `min_readable_version` and verifies `version >= min_readable_version` [2](#0-1) 

2. **Iterator Creation**: Control flows through `state_store.get_prefixed_state_value_iterator()` [3](#0-2) 

3. **RocksDB Iterator**: Finally creates the underlying RocksDB iterator [4](#0-3) 

**The Race Window**: Between steps 1 and 3, the background pruner thread can:
- Advance `min_readable_version` from V to V+1
- Delete stale state indices where `stale_since_version <= V+1`  
- Commit these deletions to RocksDB [5](#0-4) 

When the RocksDB iterator is created in step 3, it gets an **implicit snapshot** that sees the database state AFTER the pruning commit. The deleted data appears as tombstones in this snapshot.

**Concrete Attack Scenario**:

Assume `prune_window = 1000`, current version = 2004:
- `min_readable_version = 1004` (versions >= 1004 are readable)
- State key K has versions: [1000 (stale@1003), 1003 (stale@1005), 1005 (current)]

Validator execution timeline:
1. **T1**: Validator needs to read state at version 1004 for block execution
2. **T2**: Check passes: `1004 >= min_readable(1004)` âœ“
3. **T3**: New block commits at version 2005
4. **T4**: Pruner updates: `min_readable = 1005`, deletes indices with `stale_since <= 1005`
   - Deletes: version 1000 (stale@1003) and version 1003 (stale@1005)
5. **T5**: RocksDB iterator created with snapshot AFTER T4
6. **T6**: Iterator seeks version 1004, expects to find version 1003
7. **T7**: Iterator returns None - version 1003 was deleted
8. **T8**: Validator executes with missing state, computes wrong state root

Different validators hitting this race at different times will see different state, violating the **Deterministic Execution** invariant.

The pruner manager's version advancement logic: [6](#0-5) 

## Impact Explanation

This vulnerability causes **consensus-breaking state inconsistencies**:

1. **Deterministic Execution Violation**: Validators executing the same block at different times may see different state values depending on race timing, producing different state roots.

2. **Consensus Safety Break**: If validator A reads complete state and validator B reads incomplete state (due to the race), they will disagree on the block's state root, preventing consensus.

3. **Potential Chain Halt**: Widespread race conditions during high-load periods could cause enough validators to disagree that consensus cannot proceed.

4. **State Corruption**: Validators making decisions based on incomplete state may incorrectly validate transactions, execute smart contracts with missing dependencies, or compute invalid state transitions.

This qualifies as **HIGH severity** per Aptos bug bounty criteria: "Significant protocol violations" causing validator node issues and consensus disruption. While not reaching Critical severity (which requires permanent network partition), the impact on validator operation and consensus integrity is severe.

## Likelihood Explanation

**High Likelihood** - This race occurs naturally during normal operation:

1. **Continuous Pruning**: The pruner runs continuously in a background thread with ~1ms sleep intervals [7](#0-6) 

2. **Frequent State Reads**: Validators constantly read state during block execution, especially for iterating over account resources or modules

3. **Narrow Race Window**: The vulnerability is most likely at version boundaries where `version == min_readable_version`, which occurs whenever reading at the edge of the prune window

4. **No Synchronization**: There is no locking or coordination between the pruner thread and read operations - only atomic reads of version numbers

5. **Production Trigger**: High transaction throughput increases both pruning frequency and state read frequency, amplifying race probability

The race is non-deterministic but will occur probabilistically in production, especially under load.

## Recommendation

**Immediate Fix**: Create the RocksDB iterator snapshot BEFORE the version check, or use an explicit snapshot that's established at check time:

```rust
pub fn get_prefixed_state_value_iterator(
    &self,
    key_prefix: &StateKeyPrefix,
    cursor: Option<&StateKey>,
    version: Version,
) -> Result<Box<dyn Iterator<Item = Result<(StateKey, StateValue)>> + '_>> {
    gauged_api("get_prefixed_state_value_iterator", || {
        ensure!(
            !self.state_kv_db.enabled_sharding(),
            "This API is not supported with sharded DB"
        );
        
        // Create ReadOptions with explicit snapshot BEFORE the check
        let mut read_opts = ReadOptions::default();
        let snapshot = self.state_kv_db.metadata_db().inner.snapshot();
        read_opts.set_snapshot(&snapshot);
        
        // Now perform the check - if it passes, the snapshot is already captured
        self.error_if_state_kv_pruned("StateValue", version)?;

        // Create iterator with the pre-established snapshot
        let iter = PrefixedStateValueIterator::new_with_snapshot(
            &self.state_kv_db,
            key_prefix,
            cursor,
            version,
            read_opts,
        )?;
        
        Ok(Box::new(iter) as Box<dyn Iterator<Item = Result<(StateKey, StateValue)>>>)
    })
}
```

**Alternative Fix**: Add atomic coordination between the pruner and read operations:

1. Maintain a reader count or snapshot registry
2. Pruner checks for active readers at boundary versions before pruning
3. Delay pruning if readers exist at critical versions

**Long-term Fix**: Implement read-write locks around the prune window boundaries or use a versioned snapshot manager that guarantees snapshot consistency.

## Proof of Concept

```rust
#[cfg(test)]
mod pruning_race_test {
    use super::*;
    use std::sync::{Arc, atomic::{AtomicBool, Ordering}};
    use std::thread;
    use std::time::Duration;

    #[test]
    fn test_iterator_pruning_race_condition() {
        // Setup: Create AptosDB with small prune window
        let tmpdir = TempDir::new().unwrap();
        let db = AptosDB::new_for_test_with_prune_window(&tmpdir, 100);
        
        // Write initial state versions
        for v in 0..200 {
            let mut batch = SchemaBatch::new();
            batch.put::<StateValueSchema>(
                &(test_state_key(), v),
                &Some(test_state_value(v))
            ).unwrap();
            db.state_kv_db.metadata_db().write_schemas(batch).unwrap();
        }
        
        // Current version = 200, min_readable should be ~100
        let boundary_version = db.state_store.state_kv_pruner.get_min_readable_version();
        
        let race_detected = Arc::new(AtomicBool::new(false));
        let race_detected_clone = Arc::clone(&race_detected);
        
        // Thread 1: Continuously try to create iterators at boundary version
        let db_clone = Arc::clone(&db);
        let reader_thread = thread::spawn(move || {
            for _ in 0..1000 {
                match db_clone.get_prefixed_state_value_iterator(
                    &StateKeyPrefix::from(test_account()),
                    None,
                    boundary_version,
                ) {
                    Ok(mut iter) => {
                        // Try to consume iterator
                        let count = iter.count();
                        if count == 0 {
                            // Race detected: check passed but iterator sees no data
                            race_detected_clone.store(true, Ordering::SeqCst);
                        }
                    },
                    Err(_) => {
                        // Expected if version already pruned
                    }
                }
                thread::sleep(Duration::from_micros(10));
            }
        });
        
        // Thread 2: Advance pruner continuously
        let pruner_thread = thread::spawn(move || {
            for v in 201..300 {
                // Simulate new version commits
                db.state_store.state_kv_pruner.maybe_set_pruner_target_db_version(v);
                thread::sleep(Duration::from_micros(50));
            }
        });
        
        reader_thread.join().unwrap();
        pruner_thread.join().unwrap();
        
        assert!(
            race_detected.load(Ordering::SeqCst),
            "Race condition not triggered: iterator should see missing data after check passes"
        );
    }
}
```

## Notes

This vulnerability specifically affects non-sharded database configurations. The sharded path uses an indexer-based iterator that may have different race characteristics. The fix should address both code paths.

The issue is particularly dangerous because it's silent - validators don't crash or log errors, they simply read incomplete state and continue execution, leading to consensus divergence that's difficult to debug.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L78-96)
```rust
    fn get_prefixed_state_value_iterator(
        &self,
        key_prefix: &StateKeyPrefix,
        cursor: Option<&StateKey>,
        version: Version,
    ) -> Result<Box<dyn Iterator<Item = Result<(StateKey, StateValue)>> + '_>> {
        gauged_api("get_prefixed_state_value_iterator", || {
            ensure!(
                !self.state_kv_db.enabled_sharding(),
                "This API is not supported with sharded DB"
            );
            self.error_if_state_kv_pruned("StateValue", version)?;

            Ok(Box::new(
                self.state_store
                    .get_prefixed_state_value_iterator(key_prefix, cursor, version)?,
            )
                as Box<dyn Iterator<Item = Result<(StateKey, StateValue)>>>)
        })
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L305-315)
```rust
    pub(super) fn error_if_state_kv_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.state_store.state_kv_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L732-746)
```rust
    pub fn get_prefixed_state_value_iterator(
        &self,
        key_prefix: &StateKeyPrefix,
        first_key_opt: Option<&StateKey>,
        desired_version: Version,
    ) -> Result<PrefixedStateValueIterator<'_>> {
        // this can only handle non-sharded db scenario.
        // For sharded db, should look at API side using internal indexer to handle this request
        PrefixedStateValueIterator::new(
            &self.state_kv_db,
            key_prefix.clone(),
            first_key_opt.cloned(),
            desired_version,
        )
    }
```

**File:** storage/aptosdb/src/utils/iterators.rs (L114-146)
```rust
    pub fn new(
        db: &'a StateKvDb,
        key_prefix: StateKeyPrefix,
        first_key: Option<StateKey>,
        desired_version: Version,
    ) -> Result<Self> {
        let mut read_opts = ReadOptions::default();
        // Without this, iterators are not guaranteed a total order of all keys, but only keys for the same prefix.
        // For example,
        // aptos/abc|2
        // aptos/abc|1
        // aptos/abc|0
        // aptos/abd|1
        // if we seek('aptos/'), and call next, we may not reach `aptos/abd/1` because the prefix extractor we adopted
        // here will stick with prefix `aptos/abc` and return `None` or any arbitrary result after visited all the
        // keys starting with `aptos/abc`.
        read_opts.set_total_order_seek(true);
        let mut kv_iter = db
            .metadata_db()
            .iter_with_opts::<StateValueSchema>(read_opts)?;
        if let Some(first_key) = &first_key {
            kv_iter.seek(&(first_key.clone(), u64::MAX))?;
        } else {
            kv_iter.seek(&&key_prefix)?;
        };
        Ok(Self {
            kv_iter: Some(kv_iter),
            key_prefix,
            prev_key: None,
            desired_version,
            is_finished: false,
        })
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L128-142)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["state_kv_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L42-69)
```rust
impl PrunerWorkerInner {
    fn new(pruner: Arc<dyn DBPruner>, batch_size: usize) -> Arc<Self> {
        Arc::new(Self {
            pruning_time_interval_in_ms: if cfg!(test) { 100 } else { 1 },
            pruner,
            batch_size,
            quit_worker: AtomicBool::new(false),
        })
    }

    // Loop that does the real pruning job.
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```
