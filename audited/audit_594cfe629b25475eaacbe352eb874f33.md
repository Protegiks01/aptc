# Audit Report

## Title
Race Condition in Quorum Store Batch Persistence Leading to Cache-Database Inconsistency and Potential Batch Loss

## Summary
A race condition exists in the QuorumStore batch persistence mechanism where concurrent writes to the same batch digest can result in the in-memory cache containing a different (newer) expiration time than the persistent database. Upon node restart, the stale database state is loaded, causing batches to expire prematurely and potentially become unavailable when needed for block execution.

## Finding Description

The vulnerability exists in the `BatchStore::persist_inner()` method where cache updates and database writes are not atomic. When multiple components process the same batch concurrently (with different expiration times due to receive-time-based calculation), the following race condition occurs: [1](#0-0) 

**Attack Flow:**

1. **Concurrent Batch Reception**: The same batch is received multiple times from different peers. Each reception triggers a separate tokio task via `BatchCoordinator::persist_and_send_digests()`: [2](#0-1) 

2. **Expiration Calculation**: Remote batches calculate expiration based on local receive time, meaning the same digest can have different expirations: [3](#0-2) 

3. **Non-Atomic Cache-DB Update**: The `insert_to_cache()` method uses DashMap for thread-safe cache updates, but the subsequent database write happens after releasing the lock: [4](#0-3) 

4. **Race Condition**:
   - Thread A: Updates cache with expiration T1, releases lock, initiates DB write
   - Thread B: Updates cache with expiration T2 (T2 > T1), releases lock, initiates DB write  
   - Cache ends up with T2 (correct, higher expiration)
   - Database may end up with T1 if that write completes last (incorrect, lower expiration)

5. **Database Write Using Relaxed Consistency**: All writes use `write_schemas_relaxed()` without synchronization: [5](#0-4) [6](#0-5) 

6. **Corruption Persists on Restart**: When the node restarts, it loads batch state from the database, making the stale expiration the source of truth: [7](#0-6) 

**Invariant Broken**: This violates the **State Consistency** invariant - state transitions must be atomic and storage must maintain consistency between cache and persistent layers.

## Impact Explanation

This qualifies as **Medium to High Severity** based on the following impact:

1. **Data Corruption**: Cache-database inconsistency violates storage correctness guarantees
2. **Premature Batch Expiration**: Batches expire earlier than intended, causing them to be garbage collected before use
3. **Block Execution Failure**: If a batch is referenced in a committed block but has been prematurely removed, execution cannot proceed
4. **Potential Liveness Issues**: If multiple nodes experience this corruption simultaneously (during network-wide batch gossip), the batch may become unavailable across the network

While nodes can request missing batches from peers (providing some resilience), widespread corruption during synchronized batch broadcasts could lead to permanent batch loss and require manual intervention.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability triggers under these conditions:
- Same batch received multiple times (common during gossip)
- Receives occur close enough in time to create expiration differences
- Multiple concurrent persist operations execute (happens with multiple BatchCoordinator workers)
- Node restarts before batch is used (converts cache-only corruption to persistent corruption)

The test suite acknowledges this scenario exists: [8](#0-7) [9](#0-8) 

## Recommendation

**Fix: Implement atomic cache-database updates with proper locking**

Add a per-digest mutex to ensure cache update and database write are atomic:

```rust
// In BatchStore struct, add:
persist_locks: Arc<DashMap<HashValue, Arc<Mutex<()>>>>,

// In persist_inner(), wrap both operations:
fn persist_inner(&self, batch_info: BatchInfoExt, persist_request: PersistedValue<BatchInfoExt>) 
    -> Option<SignedBatchInfo<BatchInfoExt>> {
    let digest = *persist_request.digest();
    let lock = self.persist_locks
        .entry(digest)
        .or_insert_with(|| Arc::new(Mutex::new(())))
        .clone();
    
    let _guard = lock.lock();
    
    match self.save(&persist_request) {
        Ok(needs_db) => {
            if needs_db {
                // DB write now happens under the same lock
                if !batch_info.is_v2() {
                    self.db.save_batch(persist_request.try_into().expect("Must be V1"))
                        .expect("Could not write to DB");
                } else {
                    self.db.save_batch_v2(persist_request)
                        .expect("Could not write to DB");
                }
            }
            // Continue with signature generation...
        },
        Err(e) => None,
    }
}
```

**Alternative: Use database transactions with proper isolation**

Switch from `write_schemas_relaxed()` to `write_schemas()` for batch persistence to ensure durability, or implement proper transaction semantics where cache and DB updates are committed together.

## Proof of Concept

```rust
#[tokio::test(flavor = "multi_thread")]
async fn test_concurrent_persist_race_condition() {
    let batch_store = batch_store_for_test(2000);
    let digest = HashValue::random();
    
    // Simulate concurrent persist operations with different expirations
    let handles: Vec<_> = (0..10).map(|i| {
        let store = batch_store.clone();
        let d = digest;
        tokio::spawn(async move {
            let expiration = 100 + i * 10; // Different expirations
            let value = request_for_test(&d, expiration, 10, None);
            store.persist(vec![value])
        })
    }).collect();
    
    for handle in handles {
        handle.await.unwrap();
    }
    
    // Force reload from DB (simulating restart)
    let db = batch_store.db.clone();
    let db_value = db.get_batch_v2(&digest).unwrap().unwrap();
    let cache_value = batch_store.get_batch_from_local(&digest).unwrap();
    
    // Assert: DB and cache should have same expiration, but may not due to race
    assert_eq!(db_value.expiration(), cache_value.expiration(), 
        "Cache-DB inconsistency detected!");
}
```

**Notes:**

This vulnerability is particularly concerning because:
- The system explicitly handles same-digest-different-expiration scenarios (as shown in `insert_to_cache()`)
- Multiple BatchCoordinator workers are spawned concurrently by design
- The relaxed write consistency prioritizes performance over durability

The impact depends on timing and network conditions, but represents a real risk to storage consistency and batch availability in production environments.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L292-336)
```rust
    fn populate_cache_and_gc_expired_batches_v2(
        db: Arc<dyn QuorumStoreStorage>,
        current_epoch: u64,
        last_certified_time: u64,
        expiration_buffer_usecs: u64,
        batch_store: &BatchStore,
    ) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read v1 data from db");
        info!(
            epoch = current_epoch,
            "QS: Read v1 batches from storage. Len: {}, Last Cerified Time: {}",
            db_content.len(),
            last_certified_time
        );

        let mut expired_keys = Vec::new();
        let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
        for (digest, value) in db_content {
            let expiration = value.expiration();
            trace!(
                "QS: Batchreader recovery content exp {:?}, digest {}",
                expiration,
                digest
            );

            if expiration < gc_timestamp {
                expired_keys.push(digest);
            } else {
                batch_store
                    .insert_to_cache(&value)
                    .expect("Storage limit exceeded upon BatchReader construction");
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        tokio::task::spawn_blocking(move || {
            db.delete_batches_v2(expired_keys)
                .expect("Deletion of expired keys should not fail");
        });
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L358-417)
```rust
    pub(crate) fn insert_to_cache(
        &self,
        value: &PersistedValue<BatchInfoExt>,
    ) -> anyhow::Result<bool> {
        let digest = *value.digest();
        let author = value.author();
        let expiration_time = value.expiration();

        {
            // Acquire dashmap internal lock on the entry corresponding to the digest.
            let cache_entry = self.db_cache.entry(digest);

            if let Occupied(entry) = &cache_entry {
                match entry.get().expiration().cmp(&expiration_time) {
                    std::cmp::Ordering::Equal => return Ok(false),
                    std::cmp::Ordering::Greater => {
                        debug!(
                            "QS: already have the digest with higher expiration {}",
                            digest
                        );
                        return Ok(false);
                    },
                    std::cmp::Ordering::Less => {},
                }
            };
            let value_to_be_stored = if self
                .peer_quota
                .entry(author)
                .or_insert(QuotaManager::new(
                    self.db_quota,
                    self.memory_quota,
                    self.batch_quota,
                ))
                .update_quota(value.num_bytes() as usize)?
                == StorageMode::PersistedOnly
            {
                PersistedValue::new(value.batch_info().clone(), None)
            } else {
                value.clone()
            };

            match cache_entry {
                Occupied(entry) => {
                    let (k, prev_value) = entry.replace_entry(value_to_be_stored);
                    debug_assert!(k == digest);
                    self.free_quota(prev_value);
                },
                Vacant(slot) => {
                    slot.insert(value_to_be_stored);
                },
            }
        }

        // Add expiration for the inserted entry, no need to be atomic w. insertion.
        #[allow(clippy::unwrap_used)]
        {
            self.expirations.lock().add_item(digest, expiration_time);
        }
        Ok(true)
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L488-528)
```rust
    fn persist_inner(
        &self,
        batch_info: BatchInfoExt,
        persist_request: PersistedValue<BatchInfoExt>,
    ) -> Option<SignedBatchInfo<BatchInfoExt>> {
        assert!(
            &batch_info == persist_request.batch_info(),
            "Provided batch info doesn't match persist request batch info"
        );
        match self.save(&persist_request) {
            Ok(needs_db) => {
                trace!("QS: sign digest {}", persist_request.digest());
                if needs_db {
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
                }
                if !batch_info.is_v2() {
                    self.generate_signed_batch_info(batch_info.info().clone())
                        .ok()
                        .map(|inner| inner.into())
                } else {
                    self.generate_signed_batch_info(batch_info).ok()
                }
            },
            Err(e) => {
                debug!("QS: failed to store to cache {:?}", e);
                None
            },
        }
    }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L78-135)
```rust
    fn persist_and_send_digests(
        &self,
        persist_requests: Vec<PersistedValue<BatchInfoExt>>,
        approx_created_ts_usecs: u64,
    ) {
        if persist_requests.is_empty() {
            return;
        }

        let batch_store = self.batch_store.clone();
        let network_sender = self.network_sender.clone();
        let sender_to_proof_manager = self.sender_to_proof_manager.clone();
        tokio::spawn(async move {
            let peer_id = persist_requests[0].author();
            let batches = persist_requests
                .iter()
                .map(|persisted_value| {
                    (
                        persisted_value.batch_info().clone(),
                        persisted_value.summary(),
                    )
                })
                .collect();

            if persist_requests[0].batch_info().is_v2() {
                let signed_batch_infos = batch_store.persist(persist_requests);
                if !signed_batch_infos.is_empty() {
                    if approx_created_ts_usecs > 0 {
                        observe_batch(approx_created_ts_usecs, peer_id, BatchStage::SIGNED);
                    }
                    network_sender
                        .send_signed_batch_info_msg_v2(signed_batch_infos, vec![peer_id])
                        .await;
                }
            } else {
                let signed_batch_infos = batch_store.persist(persist_requests);
                if !signed_batch_infos.is_empty() {
                    assert!(!signed_batch_infos
                        .first()
                        .expect("must not be empty")
                        .is_v2());
                    if approx_created_ts_usecs > 0 {
                        observe_batch(approx_created_ts_usecs, peer_id, BatchStage::SIGNED);
                    }
                    let signed_batch_infos = signed_batch_infos
                        .into_iter()
                        .map(|sbi| sbi.try_into().expect("Batch must be V1 batch"))
                        .collect();
                    network_sender
                        .send_signed_batch_info_msg(signed_batch_infos, vec![peer_id])
                        .await;
                }
            }
            let _ = sender_to_proof_manager
                .send(ProofManagerCommand::ReceiveBatches(batches))
                .await;
        });
    }
```

**File:** consensus/src/quorum_store/batch_generator.rs (L392-401)
```rust
    pub(crate) fn handle_remote_batch(
        &mut self,
        author: PeerId,
        batch_id: BatchId,
        txns: Vec<SignedTransaction>,
    ) {
        let expiry_time_usecs = aptos_infallible::duration_since_epoch().as_micros() as u64
            + self.config.remote_batch_expiry_gap_when_init_usecs;
        self.insert_batch(author, batch_id, txns, expiry_time_usecs);
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L82-89)
```rust
    /// Relaxed writes instead of sync writes.
    pub fn put<S: Schema>(&self, key: &S::Key, value: &S::Value) -> Result<(), DbError> {
        // Not necessary to use a batch, but we'd like a central place to bump counters.
        let mut batch = self.db.new_native_batch();
        batch.put::<S>(key, value)?;
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** storage/schemadb/src/lib.rs (L311-318)
```rust
    /// Writes without sync flag in write option.
    /// If this flag is false, and the machine crashes, some recent
    /// writes may be lost.  Note that if it is just the process that
    /// crashes (i.e., the machine does not reboot), no writes will be
    /// lost even if sync==false.
    pub fn write_schemas_relaxed(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &WriteOptions::default())
    }
```

**File:** consensus/src/quorum_store/tests/batch_store_test.rs (L67-89)
```rust
async fn test_insert_expire() {
    let batch_store = batch_store_for_test(30);

    let digest = HashValue::random();

    assert_ok_eq!(
        batch_store.insert_to_cache(&request_for_test(&digest, 15, 10, None)),
        true
    );
    assert_ok_eq!(
        batch_store.insert_to_cache(&request_for_test(&digest, 30, 10, None)),
        true
    );
    assert_ok_eq!(
        batch_store.insert_to_cache(&request_for_test(&digest, 25, 10, None)),
        false
    );
    let expired = batch_store.clear_expired_payload(27);
    assert!(expired.is_empty());
    let expired = batch_store.clear_expired_payload(29);
    assert!(expired.is_empty());
    assert_eq!(batch_store.clear_expired_payload(30), vec![digest]);
}
```

**File:** consensus/src/quorum_store/tests/batch_store_test.rs (L91-143)
```rust
#[tokio::test(flavor = "multi_thread")]
async fn test_extend_expiration_vs_save() {
    let num_experiments = 2000;
    let batch_store = batch_store_for_test(2001);

    let batch_store_clone1 = batch_store.clone();
    let batch_store_clone2 = batch_store.clone();

    let digests: Vec<HashValue> = (0..num_experiments).map(|_| HashValue::random()).collect();
    let later_exp_values: Vec<PersistedValue<BatchInfoExt>> = (0..num_experiments)
        .map(|i| {
            // Pre-insert some of them.
            if i % 2 == 0 {
                assert_ok!(batch_store.save(&request_for_test(
                    &digests[i],
                    i as u64 + 30,
                    1,
                    None
                )));
            }

            request_for_test(&digests[i], i as u64 + 40, 1, None)
        })
        .collect();

    // Marshal threads to start at the same time.
    let start_flag = Arc::new(AtomicUsize::new(0));
    let start_clone1 = start_flag.clone();
    let start_clone2 = start_flag.clone();

    let save_error = Arc::new(AtomicBool::new(false));
    let save_error_clone1 = save_error.clone();
    let save_error_clone2 = save_error.clone();

    // Thread that extends expiration by saving.
    spawn_blocking(move || {
        for (i, later_exp_value) in later_exp_values.into_iter().enumerate() {
            // Wait until both threads are ready for next experiment.
            loop {
                let flag_val = start_clone1.load(Ordering::Acquire);
                if flag_val == 3 * i + 1 || flag_val == 3 * i + 2 {
                    break;
                }
            }

            if batch_store_clone1.save(&later_exp_value).is_err() {
                // Save in a separate flag and break so test doesn't hang.
                save_error_clone1.store(true, Ordering::Release);
                break;
            }
            start_clone1.fetch_add(1, Ordering::Relaxed);
        }
    });
```
