# Audit Report

## Title
Byzantine Validators Can Withhold Batch Data Despite Valid ProofOfStore, Causing Block Execution Failures

## Summary
The `check_payload_availability()` function for `InQuorumStore` payloads accepts blocks with `ProofOfStore` without verifying that the actual batch transaction data is locally available or can be fetched from the network. This allows Byzantine validators to create valid blocks containing `ProofOfStore` objects for batches whose data they withhold, causing honest validators to accept these blocks but fail to execute them, resulting in consensus liveness degradation.

## Finding Description

The vulnerability stems from a false assumption in the data availability checking logic. When a block proposal with an `InQuorumStore` payload is received, the system performs two critical checks:

1. **At proposal acceptance time** (in `process_proposal()`): The code calls `check_payload_availability()` to verify the payload can be processed. [1](#0-0) 

2. **The check for InQuorumStore payloads**: The implementation immediately returns `Ok(())` without any actual availability verification: [2](#0-1) 

The code contains a comment revealing the flawed assumption: [3](#0-2) 

**Why this assumption is incorrect:**

A `ProofOfStore` only contains:
- `BatchInfo` (metadata: digest, author, num_txns, expiration)
- `AggregateSignature` (proving >2/3 validators signed the metadata) [4](#0-3) 

The `ProofOfStore` cryptographic verification only checks signature validity: [5](#0-4) 

**The Attack Scenario:**

1. A Byzantine proposer creates a batch and distributes it to validators
2. Honest validators receive, store, and sign the batch (creating `SignedBatchInfo`)
3. The Byzantine proposer collects >2/3 signatures and creates a valid `ProofOfStore`
4. **Critical step**: The Byzantine proposer deletes the batch data locally or ensures other signers won't respond
5. The proposer includes the `ProofOfStore` in a block proposal
6. Honest validators receive the proposal and call `check_payload_availability()`, which returns `Ok()` for `InQuorumStore` without checking local availability
7. Validators accept and vote for the block
8. **During execution**, `get_transactions()` attempts to fetch the batch from signers: [6](#0-5) 

9. The batch fetching logic retries with timeout: [7](#0-6) 

10. If all signers are unresponsive (Byzantine, offline, or data expired), the request times out and returns `ExecutorError::CouldNotGetData`: [8](#0-7) 

11. Block execution fails, causing consensus liveness issues

**Contrast with OptQuorumStore:**

The `OptQuorumStore` payload type correctly checks local batch availability before accepting the block: [9](#0-8) 

This proves the system designers recognized the need for availability checking but failed to apply it consistently to `InQuorumStore` payloads.

## Impact Explanation

This vulnerability has **Critical** severity according to Aptos bug bounty criteria:

**"Total loss of liveness/network availability"**: If a Byzantine validator repeatedly proposes blocks with unavailable batch data, honest validators will accept these blocks but fail to execute them. This causes:
- Round timeouts as execution cannot complete
- Wasted computational resources on retry attempts
- Degraded transaction throughput
- Potential network stalls if enough consecutive rounds fail

**"Consensus/Safety violations"**: While this primarily affects liveness, it violates the fundamental consensus invariant that accepted blocks must be executable. The protocol allows blocks into the committed chain that cannot be processed, breaking the deterministic execution guarantee.

The attack requires the Byzantine actor to be a block proposer (rotates among validators), making it feasible for any compromised validator to exploit. With multiple Byzantine validators (<1/3 of stake), they can orchestrate sustained liveness attacks during their proposer turns.

## Likelihood Explanation

**Likelihood: Medium to High**

The attack is feasible under standard Byzantine fault tolerance assumptions:

**Preconditions:**
1. Attacker controls at least one validator node (<1/3 threshold assumed in BFT)
2. Attacker's validator is selected as proposer for a round (regular occurrence)
3. Attacker can create batches and gather signatures from honest validators

**Execution complexity: Low**
- The attacker simply withholds batch data after obtaining valid signatures
- No cryptographic breaks or complex timing attacks required
- The vulnerability is deterministic once preconditions are met

**Factors increasing likelihood:**
- Batch expiration creates a natural time window for data unavailability
- Network partitions or validator downtime make signers unresponsive
- Garbage collection of expired batches ensures data loss over time

**Factors decreasing likelihood:**
- Retry mechanism with multiple peers provides resilience if honest signers remain online
- Batch expiry checks skip some expired batches rather than failing execution

However, the fundamental flaw remains: the system accepts blocks it cannot execute, violating a core consensus invariant.

## Recommendation

**Fix: Add data availability verification for InQuorumStore payloads**

Modify `check_payload_availability()` to verify that batch data for `InQuorumStore` payloads is either:
1. Available locally in the batch store, OR
2. Can be fetched from a sufficient number of responsive signers

**Proposed implementation:**

```rust
Payload::InQuorumStore(proof_with_data) => {
    // Verify each proof's batch data is available locally
    for proof in &proof_with_data.proofs {
        if self.batch_reader.exists(proof.digest()).is_none() {
            // Batch not available locally - attempt immediate fetch from signers
            // If fetch fails quickly, reject the block
            return Err(BitVec::with_num_bits(self.ordered_authors.len() as u16));
        }
    }
    Ok(())
}
```

**Alternative approach (more conservative):**

Add an availability check with a short timeout during block validation:

```rust
Payload::InQuorumStore(proof_with_data) => {
    let mut missing_batches = Vec::new();
    for proof in &proof_with_data.proofs {
        if self.batch_reader.exists(proof.digest()).is_none() {
            missing_batches.push(proof.info().clone());
        }
    }
    
    if !missing_batches.is_empty() {
        // Attempt quick fetch with short timeout (e.g., 100ms)
        // If fetch fails, reject block
        let fetch_result = self.quick_fetch_batches(missing_batches).await;
        if fetch_result.is_err() {
            return Err(self.compute_missing_authors(&missing_batches));
        }
    }
    Ok(())
}
```

This approach mirrors the `OptQuorumStore` logic and ensures blocks are only accepted when their data is verifiably available.

## Proof of Concept

**Reproduction steps (conceptual, as this requires Byzantine validator behavior):**

```rust
// Step 1: Byzantine validator creates batch
let batch_id = create_batch(vec![transaction1, transaction2]);
let batch_info = BatchInfo::new(
    byzantine_author,
    batch_id,
    epoch,
    expiration,
    batch_digest,
    num_txns,
    num_bytes,
    gas_bucket_start,
);

// Step 2: Distribute to honest validators, collect signatures
let signed_batch_infos = collect_signatures_from_honest_validators(batch_info);

// Step 3: Create valid ProofOfStore
let proof_of_store = create_proof_from_signatures(batch_info, signed_batch_infos);
assert!(proof_of_store.verify(&validator_verifier, &cache).is_ok());

// Step 4: Delete local batch data
byzantine_batch_store.delete_batch(batch_digest);

// Step 5: Include ProofOfStore in block proposal
let payload = Payload::InQuorumStore(ProofWithData {
    proofs: vec![proof_of_store],
});
let block = create_block_proposal(payload);

// Step 6: Broadcast to honest validators
broadcast_block(block);

// Expected result: Honest validators accept block (check_payload_availability returns Ok)
// But execution fails when trying to fetch unavailable batch data
// Verify: block_execution_result.is_err() && 
//         error == ExecutorError::CouldNotGetData
```

**Testing approach:**
1. Modify a test validator to withhold batch data after gathering signatures
2. Observe that `check_payload_availability()` passes for the malicious block
3. Observe that block execution fails with `CouldNotGetData` error
4. Verify that applying the recommended fix causes the block to be rejected at proposal time

---

**Notes:**

This vulnerability demonstrates a critical gap between cryptographic proof of past data possession (ProofOfStore signatures) and guaranteed current data availability. The system incorrectly assumes that signatures imply network availability, when in reality they only prove that signers possessed the data at signing time. The inconsistency between `OptQuorumStore` (which checks availability) and `InQuorumStore` (which doesn't) confirms this is an oversight rather than intentional design.

### Citations

**File:** consensus/src/round_manager.rs (L1262-1278)
```rust
        if block_store.check_payload(&proposal).is_err() {
            debug!("Payload not available locally for block: {}", proposal.id());
            counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
                .with_label_values(&["missing"])
                .inc();
            let start_time = Instant::now();
            let deadline = self.round_state.current_round_deadline();
            let future = async move {
                (
                    block_store.wait_for_payload(&proposal, deadline).await,
                    proposal,
                    start_time,
                )
            }
            .boxed();
            self.futures.push(future);
            return Ok(());
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L354-359)
```rust
        match payload {
            Payload::DirectMempool(_) => {
                unreachable!("QuorumStore doesn't support DirectMempool payload")
            },
            Payload::InQuorumStore(_) => Ok(()),
            Payload::InQuorumStoreWithLimit(_) => Ok(()),
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L405-407)
```rust
                // The payload is considered available because it contains only proofs that guarantee network availabiliy
                // or inlined transactions.
                Ok(())
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L409-424)
```rust
            Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L456-467)
```rust
            Payload::InQuorumStore(proof_with_data) => {
                let transactions = process_qs_payload(
                    proof_with_data,
                    self.batch_reader.clone(),
                    block,
                    &self.ordered_authors,
                )
                .await?;
                BlockTransactionPayload::new_in_quorum_store(
                    transactions,
                    proof_with_data.proofs.clone(),
                )
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L618-622)
```rust
#[derive(Deserialize, Serialize, Clone, Debug, PartialEq, Eq)]
pub struct ProofOfStore<T> {
    info: T,
    multi_signature: AggregateSignature,
}
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L635-652)
```rust
    pub fn verify(&self, validator: &ValidatorVerifier, cache: &ProofCache) -> anyhow::Result<()> {
        let batch_info_ext: BatchInfoExt = self.info.clone().into();
        if let Some(signature) = cache.get(&batch_info_ext) {
            if signature == self.multi_signature {
                return Ok(());
            }
        }
        let result = validator
            .verify_multi_signatures(&self.info, &self.multi_signature)
            .context(format!(
                "Failed to verify ProofOfStore for batch: {:?}",
                self.info
            ));
        if result.is_ok() {
            cache.insert(batch_info_ext, self.multi_signature.clone());
        }
        result
    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L101-180)
```rust
    pub(crate) async fn request_batch(
        &self,
        digest: HashValue,
        expiration: u64,
        responders: Arc<Mutex<BTreeSet<PeerId>>>,
        mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
    ) -> ExecutorResult<Vec<SignedTransaction>> {
        let validator_verifier = self.validator_verifier.clone();
        let mut request_state = BatchRequesterState::new(responders, self.retry_limit);
        let network_sender = self.network_sender.clone();
        let request_num_peers = self.request_num_peers;
        let my_peer_id = self.my_peer_id;
        let epoch = self.epoch;
        let retry_interval = Duration::from_millis(self.retry_interval_ms as u64);
        let rpc_timeout = Duration::from_millis(self.rpc_timeout_ms as u64);

        monitor!("batch_request", {
            let mut interval = time::interval(retry_interval);
            let mut futures = FuturesUnordered::new();
            let request = BatchRequest::new(my_peer_id, epoch, digest);
            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        // send batch request to a set of peers of size request_num_peers
                        if let Some(request_peers) = request_state.next_request_peers(request_num_peers) {
                            for peer in request_peers {
                                futures.push(network_sender.request_batch(request.clone(), peer, rpc_timeout));
                            }
                        } else if futures.is_empty() {
                            // end the loop when the futures are drained
                            break;
                        }
                    },
                    Some(response) = futures.next() => {
                        match response {
                            Ok(BatchResponse::Batch(batch)) => {
                                counters::RECEIVED_BATCH_RESPONSE_COUNT.inc();
                                let payload = batch.into_transactions();
                                return Ok(payload);
                            }
                            // Short-circuit if the chain has moved beyond expiration
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
                            }
                            Ok(BatchResponse::BatchV2(_)) => {
                                error!("Batch V2 response is not supported");
                            }
                            Err(e) => {
                                counters::RECEIVED_BATCH_RESPONSE_ERROR_COUNT.inc();
                                debug!("QS: batch request error, digest:{}, error:{:?}", digest, e);
                            }
                        }
                    },
                    result = &mut subscriber_rx => {
                        match result {
                            Ok(persisted_value) => {
                                counters::RECEIVED_BATCH_FROM_SUBSCRIPTION_COUNT.inc();
                                let (_, maybe_payload) = persisted_value.unpack();
                                return Ok(maybe_payload.expect("persisted value must exist"));
                            }
                            Err(err) => {
                                debug!("channel closed: {}", err);
                            }
                        };
                    },
                }
            }
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
        })
    }
```
