# Audit Report

## Title
BytesSender Buffer Lacks Maximum Size Enforcement Leading to Unbounded Memory Allocation

## Summary
The `BytesSender` struct in the backup service lacks maximum size enforcement on its `BytesMut` buffer. While `TARGET_BATCH_SIZE` exists as a batching threshold, individual large records can cause the buffer to grow to arbitrary sizes (potentially 20MB+ per request), enabling memory exhaustion attacks through concurrent backup requests. [1](#0-0) 

## Finding Description

The `BytesSender::send_bytes()` method extends the buffer **before** checking if it exceeds `TARGET_BATCH_SIZE`, allowing individual large records to bypass any meaningful size control: [2](#0-1) 

When backup endpoints serialize database records via `send_size_prefixed_bcs_bytes()`, large transaction tuples (Transaction, PersistedAuxiliaryInfo, TransactionInfo, Vec<ContractEvent>, WriteSet) can exceed 20MB when BCS-serialized, as transactions support:
- Write operations: up to 10MB total per transaction
- Events: up to 10MB total per transaction  
- Transaction payload: up to 64KB (1MB for governance) [3](#0-2) 

The attack path:
1. Attacker previously submits transactions with maximum-size write sets and events (requires gas payment but is permissible)
2. Attacker floods backup service with concurrent requests (e.g., `/transactions/{start}/{count}` endpoint)
3. Each request spawns a blocking task, up to 64 concurrent tasks due to tokio runtime limit
4. Each `BytesSender` buffer grows to ~20MB+ when processing large transaction records
5. Combined with channel buffers (100 batches per sender), total memory consumption can reach: 64 requests × (20MB buffer + 100 × average_batch_size) [4](#0-3) [5](#0-4) 

The backup service has no server-side rate limiting or authentication, making it accessible to any network peer who can reach the endpoint: [6](#0-5) 

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:
- **Validator node slowdowns**: Memory exhaustion causes severe performance degradation as the system enters swap or triggers garbage collection pressure
- **API crashes**: Out-of-memory (OOM) conditions can terminate the node process entirely
- This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits"

While the 64 concurrent task limit provides some bound (worst case ~1.3GB+ for buffers alone), this is sufficient to significantly impact nodes, especially when combined with normal operational memory usage. Production nodes must handle this backup service alongside consensus, execution, and storage operations.

## Likelihood Explanation

**Medium Likelihood**:
- **Requirements**: Attacker must previously create large transactions (requires gas payment but feasible)
- **Accessibility**: Backup service typically exposed for snapshot providers and archival nodes
- **Simplicity**: Attack requires only HTTP GET requests to public endpoints
- **Detection**: High memory usage may be attributed to legitimate large backups initially

The bounded concurrency (64 tasks) reduces severity from Critical to High, but the issue remains exploitable for causing node instability.

## Recommendation

Implement explicit maximum buffer size enforcement in `BytesSender`:

```rust
pub struct BytesSender {
    const MAX_BUFFER_SIZE: usize = 5 * 1024 * 1024; // 5MB limit
    buffer: BytesMut,
    bytes_tx: tokio::sync::mpsc::Sender<BytesResult>,
    endpoint: &'static str,
}

pub fn send_bytes(&mut self, bytes: Bytes) -> DbResult<()> {
    // Check BEFORE extending to prevent oversized allocation
    if self.buffer.len() + bytes.len() > Self::MAX_BUFFER_SIZE {
        self.flush_buffer()?;
    }
    
    // If single record exceeds MAX_BUFFER_SIZE, flush immediately after
    self.buffer.extend(bytes);
    
    if self.buffer.len() >= Self::TARGET_BATCH_SIZE {
        self.flush_buffer()?
    }
    
    Ok(())
}
```

Additionally, consider:
1. Adding server-side rate limiting/authentication to backup endpoints
2. Implementing request-level memory budgets using `BoundedExecutor`
3. Monitoring and alerting on backup service memory usage

## Proof of Concept

```rust
#[cfg(test)]
mod memory_exhaustion_test {
    use super::*;
    use aptos_types::transaction::{Transaction, WriteSet};
    use std::sync::Arc;
    
    #[tokio::test]
    async fn test_large_record_memory_spike() {
        // Setup: Create database with large transaction records
        let tmpdir = TempPath::new();
        let db = Arc::new(AptosDB::new_for_test(&tmpdir));
        
        // Insert transaction with maximum allowed write set (10MB)
        let large_write_set = create_max_size_write_set(); // 10MB
        let large_events = create_max_size_events(); // 10MB
        let txn = create_transaction_with_data(large_write_set, large_events);
        
        // Start backup service
        let port = get_available_port();
        let addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), port);
        let _rt = start_backup_service(addr, db.clone());
        
        // Attack: Make concurrent requests to transaction endpoint
        let handles: Vec<_> = (0..64).map(|_| {
            tokio::spawn(async move {
                reqwest::get(format!("http://127.0.0.1:{}/transactions/0/100", port))
                    .await
            })
        }).collect();
        
        // Monitor memory usage - should spike significantly
        // Each BytesSender buffer will grow to ~20MB per request
        // 64 concurrent requests = ~1.3GB minimum
        
        for handle in handles {
            handle.await.unwrap();
        }
    }
}
```

## Notes

The vulnerability stems from treating `TARGET_BATCH_SIZE` as a batching hint rather than a strict memory limit. The design comment states the buffer provides "more predictable" memory usage, but without maximum size enforcement, predictability is lost when processing large legitimate records. The 64-thread concurrency bound mitigates but does not eliminate the risk of memory exhaustion attacks.

### Citations

**File:** storage/backup/backup-service/src/handlers/bytes_sender.rs (L13-19)
```rust
pub(super) struct BytesSender {
    /// Buffers bytes instead of relying on the channel's backlog to provide backpressure, so
    /// the max pending bytes are more predictable.
    buffer: BytesMut,
    bytes_tx: tokio::sync::mpsc::Sender<BytesResult>,
    endpoint: &'static str,
}
```

**File:** storage/backup/backup-service/src/handlers/bytes_sender.rs (L44-52)
```rust
    pub fn send_bytes(&mut self, bytes: Bytes) -> DbResult<()> {
        self.buffer.extend(bytes);

        if self.buffer.len() >= Self::TARGET_BATCH_SIZE {
            self.flush_buffer()?
        }

        Ok(())
    }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L160-172)
```rust
            { 5.. => "max_bytes_all_write_ops_per_transaction" },
            10 << 20, // all write ops from a single transaction are 10MB max
        ],
        [
            max_bytes_per_event: NumBytes,
            { 5.. => "max_bytes_per_event" },
            1 << 20, // a single event is 1MB max
        ],
        [
            max_bytes_all_events_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_events_per_transaction"},
            10 << 20, // all events from a single transaction are 10MB max
        ],
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L103-110)
```rust
    let transactions = warp::path!(Version / usize)
        .map(move |start_version, num_transactions| {
            reply_with_bytes_sender(&bh, TRANSACTIONS, move |bh, sender| {
                bh.get_transaction_iter(start_version, num_transactions)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** storage/backup/backup-service/src/lib.rs (L12-30)
```rust
pub fn start_backup_service(address: SocketAddr, db: Arc<AptosDB>) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler);

    let runtime = aptos_runtimes::spawn_named_runtime("backup".into(), None);

    // Ensure that we actually bind to the socket first before spawning the
    // server tasks. This helps in tests to prevent races where a client attempts
    // to make a request before the server task is actually listening on the
    // socket.
    //
    // Note: we need to enter the runtime context first to actually bind, since
    //       tokio TcpListener can only be bound inside a tokio context.
    let _guard = runtime.enter();
    let server = warp::serve(routes).bind(address);
    runtime.handle().spawn(server);
    info!("Backup service spawned.");
    runtime
}
```
