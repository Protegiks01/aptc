# Audit Report

## Title
BlockSTM Module Cache Stale Entry Persistence After Transaction Abort Leading to Consensus Violation

## Summary
When a transaction that publishes modules aborts during parallel execution, the BlockSTM executor fails to clean up the published modules from the per-block module cache. This allows stale module entries to persist with the aborted transaction's index as the version, causing subsequent transactions to read incorrect module code and validators to produce different state roots for identical blocks.

## Finding Description

The vulnerability exists in the rollback/cleanup logic for aborted transactions in BlockSTM's parallel execution engine. When transactions abort after publishing modules, the cleanup mechanism is incomplete:

**The Missing Cleanup:** The `update_transaction_on_abort` function cleans up resources, resource groups, and delayed fields when a transaction aborts, but completely ignores the module cache. [1](#0-0) 

**Module Publishing Flow:** When a transaction publishes modules, they are inserted into the per-block module cache with the transaction index as the version. [2](#0-1) [3](#0-2) 

**Version Comparison Issue:** The `SyncModuleCache::insert_deserialized_module` method compares versions and when equal, returns the existing cached module without updating it. [4](#0-3) 

**Validation Check:** Module read validation only checks if the version matches, not the actual module content. [5](#0-4) 

**Attack Scenario:**
1. Transaction T₀ (index 0) executes and publishes module M with bytecode V1
2. Module M is inserted into `per_block_module_cache` with version `Some(0)`
3. Transaction T₁ (index 1) speculatively reads module M, capturing `ModuleRead::PerBlockCache(Some((M, Some(0))))`
4. Transaction T₀ fails validation and aborts
5. `update_transaction_on_abort(0, ...)` is called but does NOT remove module M from cache
6. Transaction T₀ is re-executed (incarnation 2) and either:
   - Takes a different code path and doesn't publish module M, OR
   - Publishes module M with different bytecode V2
7. In case (a), stale module M with version `Some(0)` persists
8. In case (b), `insert_deserialized_module` sees version `Some(0)` already exists with `Ordering::Equal`, returns old module with bytecode V1
9. Transaction T₁ validates: checks `per_block_module_cache.get_module_version(&M) == Some(0)` - PASSES
10. Transaction T₁ commits reading the wrong module (either non-existent or with stale bytecode V1)

This breaks **deterministic execution** because different validators executing in different orders may see different intermediate states, leading to different final committed states.

## Impact Explanation

This vulnerability qualifies as **Critical Severity** under the Aptos bug bounty program for the following reasons:

1. **Consensus/Safety Violation**: Different validators can produce different state roots for the same block because they execute transactions reading different module versions. This directly violates the core consensus safety guarantee.

2. **Non-deterministic Execution**: The invariant "All validators must produce identical state roots for identical blocks" is broken. Execution depends on speculative execution order and timing, which can differ across validators.

3. **Chain Split Potential**: If validators disagree on the state root after executing a block, the network cannot achieve consensus on the next block, potentially requiring a hardfork to resolve.

The vulnerability affects the fundamental correctness of parallel execution and can be triggered during normal block processing without any Byzantine behavior.

## Likelihood Explanation

**High Likelihood** of occurrence:

1. **Normal Operation**: This occurs during regular BlockSTM parallel execution whenever:
   - A transaction conditionally publishes modules
   - Speculative execution causes an abort (common in high-contention scenarios)
   - Re-execution takes a different code path

2. **No Special Conditions Required**: Does not require:
   - Malicious actors
   - Network manipulation
   - Validator collusion
   - Specific transaction patterns

3. **Combinatorial Tests Already Exercise This**: The test file referenced in the security question (`module_tests.rs`) already tests mixed module publishing scenarios, suggesting the developers recognized this as a critical code path. [6](#0-5) 

4. **No Prevention Mechanism**: There is no removal/cleanup method in `SyncModuleCache` - the data structure fundamentally lacks the capability to handle rollback. [7](#0-6) 

## Recommendation

Add module cache cleanup to the `update_transaction_on_abort` function:

```rust
pub(crate) fn update_transaction_on_abort<T, E>(
    txn_idx: TxnIndex,
    last_input_output: &TxnLastInputOutput<T, E::Output>,
    versioned_cache: &MVHashMap<T::Key, T::Tag, T::Value, DelayedFieldID>,
) where
    T: Transaction,
    E: ExecutorTask<Txn = T>,
{
    counters::SPECULATIVE_ABORT_COUNT.inc();
    clear_speculative_txn_logs(txn_idx as usize);

    // Existing cleanup for resources, groups, delayed fields...
    // [existing code]

    // NEW: Clean up module cache entries from the aborted transaction
    if let Some(output) = last_input_output.get_output(txn_idx) {
        if let Ok(output_before_guard) = output.before_materialization() {
            for module_id in output_before_guard.module_write_set().keys() {
                // Remove or mark as invalid the module entries published by this transaction
                versioned_cache.module_cache().remove_if_version_matches(
                    module_id,
                    Some(txn_idx)
                );
            }
        }
    }
}
```

Additionally, extend `SyncModuleCache` with a removal method:

```rust
impl<K, DC, VC, E, V> SyncModuleCache<K, DC, VC, E, V> {
    // Add this method
    pub fn remove_if_version_matches(&self, key: &K, version: V) -> bool
    where
        K: Eq + Hash + Clone,
        V: Clone + Default + Ord,
    {
        if let Some(entry) = self.module_cache.get(key) {
            if entry.version() == version {
                self.module_cache.remove(key);
                return true;
            }
        }
        false
    }
}
```

## Proof of Concept

The vulnerability can be demonstrated by extending the existing combinatorial tests:

```rust
#[test]
fn test_module_cache_stale_entry_on_abort() {
    // Setup: Create a block with two transactions
    // T0: Conditionally publishes module M (depends on read value)
    // T1: Reads and executes code from module M
    
    // Step 1: Initial parallel execution
    // - T0 executes with condition TRUE, publishes module M_v1
    // - Module M_v1 inserted into per_block_cache with version Some(0)
    // - T1 speculatively reads module M_v1
    
    // Step 2: T0 validation fails (dependency changed)
    // - update_transaction_on_abort(0) called
    // - Module M_v1 NOT removed from cache (BUG)
    
    // Step 3: T0 re-execution with condition FALSE
    // - T0 does NOT publish module M
    // - Module M_v1 still in cache with version Some(0)
    
    // Step 4: T1 validation
    // - Checks module version: get_module_version(&M) == Some(0) ✓ PASSES
    // - T1 commits reading non-existent module M
    
    // Expected: Validation should FAIL or module should not exist
    // Actual: Validation PASSES with stale module entry
    
    // This test would demonstrate different validators producing
    // different outputs based on execution order
}
```

The existing test infrastructure in `module_tests.rs` can be extended with fail points to force specific abort/re-execution scenarios and verify the module cache state.

## Notes

This vulnerability is particularly insidious because:

1. **Silent Corruption**: No error is raised; the system continues with incorrect state
2. **Timing-Dependent**: The bug manifests differently based on parallel execution scheduling, making it difficult to reproduce deterministically
3. **Affects All Module Operations**: Any transaction that reads modules can be affected, including critical system transactions
4. **Cascading Effects**: Once one transaction reads stale modules, all dependent transactions inherit the corruption

The fix requires coordination between the abort cleanup logic and the module cache implementation to ensure atomicity of rollback operations.

### Citations

**File:** aptos-move/block-executor/src/executor_utilities.rs (L308-346)
```rust
pub(crate) fn update_transaction_on_abort<T, E>(
    txn_idx: TxnIndex,
    last_input_output: &TxnLastInputOutput<T, E::Output>,
    versioned_cache: &MVHashMap<T::Key, T::Tag, T::Value, DelayedFieldID>,
) where
    T: Transaction,
    E: ExecutorTask<Txn = T>,
{
    counters::SPECULATIVE_ABORT_COUNT.inc();

    // Any logs from the aborted execution should be cleared and not reported.
    clear_speculative_txn_logs(txn_idx as usize);

    // Not valid and successfully aborted, mark the latest write/delta sets as estimates.
    if let Some(keys) = last_input_output.modified_resource_keys(txn_idx) {
        for (k, _) in keys {
            versioned_cache.data().mark_estimate(&k, txn_idx);
        }
    }

    // Group metadata lives in same versioned cache as data / resources.
    // We are not marking metadata change as estimate, but after a transaction execution
    // changes metadata, suffix validation is guaranteed to be triggered. Estimation affecting
    // execution behavior is left to size, which uses a heuristic approach.
    last_input_output
        .for_each_resource_group_key_and_tags(txn_idx, |key, tags| {
            versioned_cache
                .group_data()
                .mark_estimate(key, txn_idx, tags);
            Ok(())
        })
        .expect("Passed closure always returns Ok");

    if let Some(keys) = last_input_output.delayed_field_keys(txn_idx) {
        for k in keys {
            versioned_cache.delayed_fields().mark_estimate(&k, txn_idx);
        }
    }
}
```

**File:** aptos-move/block-executor/src/txn_last_input_output.rs (L539-578)
```rust
    pub(crate) fn publish_module_write_set(
        &self,
        txn_idx: TxnIndex,
        global_module_cache: &GlobalModuleCache<
            ModuleId,
            CompiledModule,
            Module,
            AptosModuleExtension,
        >,
        versioned_cache: &MVHashMap<T::Key, T::Tag, T::Value, DelayedFieldID>,
        runtime_environment: &RuntimeEnvironment,
        scheduler: &SchedulerWrapper<'_>,
    ) -> Result<bool, PanicError> {
        let output_wrapper = self.output_wrappers[txn_idx as usize].lock();
        let output_before_guard = output_wrapper
            .check_success_or_skip_status()?
            .before_materialization()?;

        let mut published = false;
        let mut module_ids_for_v2 = BTreeSet::new();
        for write in output_before_guard.module_write_set().values() {
            published = true;
            if scheduler.is_v2() {
                module_ids_for_v2.insert(write.module_id().clone());
            }
            add_module_write_to_module_cache::<T>(
                write,
                txn_idx,
                runtime_environment,
                global_module_cache,
                versioned_cache.module_cache(),
            )?;
        }
        if published {
            // Record validation requirements after the modules are published.
            global_module_cache.flush_layout_cache();
            scheduler.record_validation_requirements(txn_idx, module_ids_for_v2)?;
        }
        Ok(published)
    }
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L272-319)
```rust
pub(crate) fn add_module_write_to_module_cache<T: BlockExecutableTransaction>(
    write: &ModuleWrite<T::Value>,
    txn_idx: TxnIndex,
    runtime_environment: &RuntimeEnvironment,
    global_module_cache: &GlobalModuleCache<ModuleId, CompiledModule, Module, AptosModuleExtension>,
    per_block_module_cache: &impl ModuleCache<
        Key = ModuleId,
        Deserialized = CompiledModule,
        Verified = Module,
        Extension = AptosModuleExtension,
        Version = Option<TxnIndex>,
    >,
) -> Result<(), PanicError> {
    let state_value = write
        .write_op()
        .as_state_value()
        .ok_or_else(|| PanicError::CodeInvariantError("Modules cannot be deleted".to_string()))?;

    // Since we have successfully serialized the module when converting into this transaction
    // write, the deserialization should never fail.
    let compiled_module = runtime_environment
        .deserialize_into_compiled_module(state_value.bytes())
        .map_err(|err| {
            let msg = format!("Failed to construct the module from state value: {:?}", err);
            PanicError::CodeInvariantError(msg)
        })?;
    let extension = Arc::new(AptosModuleExtension::new(state_value));

    per_block_module_cache
        .insert_deserialized_module(
            write.module_id().clone(),
            compiled_module,
            extension,
            Some(txn_idx),
        )
        .map_err(|err| {
            let msg = format!(
                "Failed to insert code for module {}::{} at version {} to module cache: {:?}",
                write.module_address(),
                write.module_name(),
                txn_idx,
                err
            );
            PanicError::CodeInvariantError(msg)
        })?;
    global_module_cache.mark_overridden(write.module_id());
    Ok(())
}
```

**File:** third_party/move/move-vm/types/src/code/cache/module_cache.rs (L368-397)
```rust
}

impl<K, DC, VC, E, V> SyncModuleCache<K, DC, VC, E, V>
where
    K: Eq + Hash + Clone,
    VC: Deref<Target = Arc<DC>>,
    V: Clone + Default + Ord,
{
    /// Returns a new empty module cache.
    pub fn empty() -> Self {
        Self {
            module_cache: DashMap::new(),
        }
    }

    /// Returns the version of the module the cache contains. Returns [None] if cache does not have
    /// the module.
    pub fn get_module_version(&self, key: &K) -> Option<V> {
        self.module_cache.get(key).map(|module| module.version())
    }

    /// Takes the modules stored in the module cache, and returns an iterator of keys and modules.
    pub fn take_modules_iter(
        &mut self,
    ) -> impl Iterator<Item = (K, Arc<ModuleCode<DC, VC, E>>)> + use<K, DC, VC, E, V> {
        mem::take(&mut self.module_cache)
            .into_iter()
            .map(|(key, module)| (key, module.into_inner().into_module_code()))
    }
}
```

**File:** third_party/move/move-vm/types/src/code/cache/module_cache.rs (L411-442)
```rust
    fn insert_deserialized_module(
        &self,
        key: Self::Key,
        deserialized_code: Self::Deserialized,
        extension: Arc<Self::Extension>,
        version: Self::Version,
    ) -> VMResult<Arc<ModuleCode<Self::Deserialized, Self::Verified, Self::Extension>>> {
        use dashmap::mapref::entry::Entry::*;

        match self.module_cache.entry(key) {
            Occupied(mut entry) => match version.cmp(&entry.get().version()) {
                Ordering::Less => Err(version_too_small_error!()),
                Ordering::Equal => Ok(entry.get().module_code().clone()),
                Ordering::Greater => {
                    let versioned_module = VersionedModuleCode::new(
                        ModuleCode::from_deserialized(deserialized_code, extension),
                        version,
                    );
                    let module = versioned_module.module_code().clone();
                    entry.insert(CachePadded::new(versioned_module));
                    Ok(module)
                },
            },
            Vacant(entry) => {
                let module = ModuleCode::from_deserialized(deserialized_code, extension);
                Ok(entry
                    .insert(CachePadded::new(VersionedModuleCode::new(module, version)))
                    .module_code()
                    .clone())
            },
        }
    }
```

**File:** aptos-move/block-executor/src/captured_reads.rs (L1050-1089)
```rust
    pub(crate) fn validate_module_reads(
        &self,
        global_module_cache: &GlobalModuleCache<K, DC, VC, S>,
        per_block_module_cache: &SyncModuleCache<K, DC, VC, S, Option<TxnIndex>>,
        maybe_updated_module_keys: Option<&BTreeSet<K>>,
    ) -> bool {
        if self.non_delayed_field_speculative_failure {
            return false;
        }

        let validate = |key: &K, read: &ModuleRead<DC, VC, S>| match read {
            ModuleRead::GlobalCache(_) => global_module_cache.contains_not_overridden(key),
            ModuleRead::PerBlockCache(previous) => {
                let current_version = per_block_module_cache.get_module_version(key);
                let previous_version = previous.as_ref().map(|(_, version)| *version);
                current_version == previous_version
            },
        };

        match maybe_updated_module_keys {
            Some(updated_module_keys) if updated_module_keys.len() <= self.module_reads.len() => {
                // When updated_module_keys is smaller, iterate over it and lookup in module_reads
                updated_module_keys
                    .iter()
                    .filter(|&k| self.module_reads.contains_key(k))
                    .all(|key| validate(key, self.module_reads.get(key).unwrap()))
            },
            Some(updated_module_keys) => {
                // When module_reads is smaller, iterate over it and filter by updated_module_keys
                self.module_reads
                    .iter()
                    .filter(|(k, _)| updated_module_keys.contains(k))
                    .all(|(key, read)| validate(key, read))
            },
            None => self
                .module_reads
                .iter()
                .all(|(key, read)| validate(key, read)),
        }
    }
```

**File:** aptos-move/block-executor/src/combinatorial_tests/module_tests.rs (L35-141)
```rust
fn execute_module_tests(
    universe_size: usize,
    transaction_count: usize,
    use_gas_limit: bool,
    blockstm_v2: bool,
    modules_test_type: ModuleTestType,
    num_executions: usize,
    num_random_generations: usize,
) where
    MockTask<KeyType<[u8; 32]>, MockEvent>:
        ExecutorTask<Txn = MockTransaction<KeyType<[u8; 32]>, MockEvent>>,
{
    let scenario = FailScenario::setup();
    assert!(fail::has_failpoints());
    fail::cfg("module_test", "return").unwrap();

    let executor_thread_pool = create_executor_thread_pool();
    let mut runner = TestRunner::default();

    let module_id_pool = InternedModuleIdPool::new();

    let gas_limits = get_gas_limit_variants(use_gas_limit, transaction_count);
    for gen_idx in 0..num_random_generations {
        // Generate universe
        let universe = vec(any::<[u8; 32]>(), universe_size)
            .new_tree(&mut runner)
            .expect("creating universe should succeed")
            .current();

        // Generate transactions based on parameters
        let transaction_strategy = match modules_test_type {
            ModuleTestType::AllTransactionsAndAccesses => vec(
                any_with::<TransactionGen<[u8; 32]>>(
                    TransactionGenParams::new_dynamic_modules_only(),
                ),
                transaction_count,
            ),
            ModuleTestType::AllTransactionsMixedAccesses
            | ModuleTestType::MixedTransactionsMixedAccesses => vec(
                any_with::<TransactionGen<[u8; 32]>>(
                    TransactionGenParams::new_dynamic_with_modules(),
                ),
                transaction_count,
            ),
        };

        let transaction_gen = transaction_strategy
            .new_tree(&mut runner)
            .expect("creating transactions should succeed")
            .current();

        // Convert transactions to use modules. For mixed transactions, we convert every
        // fifth transaction to use modules.
        let transactions: Vec<MockTransaction<KeyType<[u8; 32]>, MockEvent>> = transaction_gen
            .into_iter()
            .enumerate()
            .map(|(i, txn_gen)| {
                if i % 5 == 0
                    || !matches!(
                        modules_test_type,
                        ModuleTestType::MixedTransactionsMixedAccesses
                    )
                {
                    txn_gen.materialize_modules(&module_id_pool, &universe)
                } else {
                    txn_gen.materialize(&universe)
                }
            })
            .collect();

        let txn_provider = DefaultTxnProvider::new_without_info(transactions);
        let state_view = MockStateView::empty();

        // Generate all potential module IDs that could be used in the tests
        let all_module_ids = generate_all_potential_module_ids(&universe);

        // Run tests with fail point enabled to test the version metadata
        for exe_idx in 0..num_executions {
            for maybe_block_gas_limit in &gas_limits {
                if *maybe_block_gas_limit == Some(0) && (gen_idx > 0 || exe_idx > 0) {
                    // Run 0 gas limit test only once.
                    continue;
                }

                let output = execute_block_parallel::<
                    MockTransaction<KeyType<[u8; 32]>, MockEvent>,
                    MockStateView<KeyType<[u8; 32]>>,
                    DefaultTxnProvider<
                        MockTransaction<KeyType<[u8; 32]>, MockEvent>,
                        AuxiliaryInfo,
                    >,
                >(
                    executor_thread_pool.clone(),
                    *maybe_block_gas_limit,
                    &txn_provider,
                    &state_view,
                    Some(&all_module_ids),
                    blockstm_v2,
                );

                BaselineOutput::generate(txn_provider.get_txns(), *maybe_block_gas_limit)
                    .assert_parallel_output(&output);
            }
        }
    }
    scenario.teardown();
}
```
