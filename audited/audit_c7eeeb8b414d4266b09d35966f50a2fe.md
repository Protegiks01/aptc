# Audit Report

## Title
Atomic Pruning Violation: Parallel Sub-Pruner Failures Cause Foreign Key Integrity Violations in Sharded Storage Mode

## Summary
The LedgerPruner's parallel execution of sub-pruners in sharded storage mode lacks atomic commit coordination. When one sub-pruner fails after others have already committed to their separate RocksDB instances, orphaned data remains that references deleted transactions, violating database referential integrity and breaking state synchronization.

## Finding Description

This is a **logic vulnerability** in the storage layer's pruning coordination mechanism that affects the default production configuration.

**Storage Sharding Architecture:** When storage sharding is enabled (which is the default and mandatory for mainnet/testnet), each ledger component is stored in a separate physical RocksDB instance. The LedgerDb opens distinct databases for transaction_db, write_set_db, event_db, transaction_info_db, transaction_auxiliary_data_db, persisted_auxiliary_info_db, and transaction_accumulator_db. [1](#0-0) 

Storage sharding is enabled by default and explicitly required for production networks: [2](#0-1) [3](#0-2) 

**Parallel Execution Without Atomic Coordination:** The LedgerPruner executes all 7 sub-pruners in parallel using rayon's `par_iter().try_for_each()`: [4](#0-3) 

**Independent Commits:** Each sub-pruner independently commits its deletions AND progress metadata to its own separate database instance. For example:

TransactionPruner commits to transaction_db: [5](#0-4) 

WriteSetPruner commits to write_set_db: [6](#0-5) 

EventStorePruner commits to event_db: [7](#0-6) 

**The Critical Flaw:** When `try_for_each` encounters an error from any sub-pruner, it returns immediately via the `?` operator. However, sub-pruners that already completed have permanently committed their changes to separate RocksDB instances. There is no distributed transaction mechanism, no two-phase commit, and no rollback capability across these independent databases.

**Attack Scenario:**
1. LedgerPruner attempts to prune versions 1000-1100
2. TransactionPruner successfully deletes transactions and commits TransactionPrunerProgress=1100 to transaction_db
3. EventStorePruner successfully deletes events and commits EventPrunerProgress=1100 to event_db
4. WriteSetPruner encounters a disk I/O error and fails before committing
5. The parallel execution returns an error, and line 86-87 never execute
6. LedgerPruner's overall progress remains at 1000
7. **Result:** Transactions and events for versions 1000-1100 are deleted, but write sets still exist

**State Synchronization Failure:** When new nodes attempt to synchronize, the `get_transaction_outputs` method queries all databases for each version: [8](#0-7) 

Each database query uses the `?` operator, so if any data is missing (transactions deleted but write sets remaining), the query returns `AptosDbError::NotFound`: [9](#0-8) 

This causes the entire `get_transaction_outputs` operation to fail, breaking state synchronization for new nodes.

**Non-Idempotent Retry Behavior:** The TransactionPruner needs to read transactions before pruning to extract their hashes for index deletion: [10](#0-9) [11](#0-10) 

If transactions are already deleted in a partial pruning scenario, the retry will fail when trying to read them, preventing automatic recovery.

## Impact Explanation

This is a **HIGH severity** vulnerability per Aptos bug bounty criteria:

**State Synchronization Failures:** New nodes attempting to synchronize blockchain state will fail when encountering partially pruned versions. The `get_transaction_outputs` method requires all related data (transaction, events, write_set, transaction_info) to be present for each version. Partial pruning causes `NotFound` errors, preventing nodes from syncing historical data. This qualifies as "Significant protocol violations" per HIGH severity criteria.

**API Crashes:** The REST API and state storage service will return errors when querying partially pruned data, meeting the "API crashes" criteria for HIGH severity.

**Database Referential Integrity Violations:** Write sets reference transactions by version number. Events reference transactions by version number. Partial pruning creates orphaned references where write sets or events exist but their corresponding transactions do not. This violates the core database consistency invariant.

**Network Liveness Impact:** While not total network failure (which would be CRITICAL), this significantly degrades the network's ability to onboard new validator nodes or recover nodes from snapshots, impacting decentralization and resilience.

## Likelihood Explanation

This vulnerability has **MEDIUM likelihood** of occurring in production:

**Triggering Conditions:**
- Disk I/O errors (occur in cloud environments and hardware failures)
- Database corruption (can occur during crashes or power failures)
- Resource exhaustion (disk full, too many open files)
- Any transient database write failure in one sub-pruner while others succeed

**Frequency:** Pruning runs continuously in the background on validator nodes. With 7 independent sub-pruners committing to separate databases in parallel, the probability of at least one failing while others succeed is non-negligible over time. However, infrastructure is generally reliable, making this MEDIUM (not HIGH) likelihood.

**Detection Difficulty:** The error would be logged, but the inconsistent state persists. Nodes continue operating normally for recent versions, but historical queries and new node synchronization will fail unpredictably.

## Recommendation

Implement atomic coordination across sub-pruner commits using one of these approaches:

**Option 1: Sequential Execution with Single Commit**
- Execute sub-pruners sequentially rather than in parallel
- Accumulate all deletions in separate batches
- Commit all batches atomically or not at all
- Update all progress markers only after all commits succeed

**Option 2: Metadata-Based Recovery**
- Before parallel execution, record intent-to-prune in metadata
- After all sub-pruners succeed, clear intent marker
- On startup, check for incomplete pruning and either complete or rollback
- Use sub-pruner progress markers to determine which pruners need to catch up

**Option 3: Checkpointing with Rollback**
- Create RocksDB checkpoints before pruning batch
- If any sub-pruner fails, restore from checkpoints
- This requires additional disk space but provides true atomicity

The recommended approach is Option 1 (sequential execution) as it provides the strongest consistency guarantees with minimal complexity overhead, though it may have performance implications that need evaluation.

## Proof of Concept

A proof of concept would require simulating disk I/O failures during pruning operations. This can be achieved by:

1. Instrumenting the WriteSetPruner to inject failures after other pruners succeed
2. Running the pruner with a controlled test database
3. Observing that subsequent `get_transaction_outputs` calls fail with NotFound errors
4. Verifying that the database is in an inconsistent state with mismatched progress markers

The vulnerability can be reproduced in integration tests by mocking RocksDB write failures in one sub-pruner while allowing others to succeed, then verifying that state synchronization fails for the affected version range.

### Citations

**File:** storage/aptosdb/src/ledger_db/mod.rs (L184-278)
```rust
            s.spawn(|_| {
                let event_db_raw = Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(EVENT_DB_NAME),
                        EVENT_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                );
                event_db = Some(EventDb::new(
                    event_db_raw.clone(),
                    EventStore::new(event_db_raw),
                ));
            });
            s.spawn(|_| {
                persisted_auxiliary_info_db = Some(PersistedAuxiliaryInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME),
                        PERSISTED_AUXILIARY_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_accumulator_db = Some(TransactionAccumulatorDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME),
                        TRANSACTION_ACCUMULATOR_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_auxiliary_data_db = Some(TransactionAuxiliaryDataDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME),
                        TRANSACTION_AUXILIARY_DATA_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )))
            });
            s.spawn(|_| {
                transaction_db = Some(TransactionDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_DB_NAME),
                        TRANSACTION_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_info_db = Some(TransactionInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_INFO_DB_NAME),
                        TRANSACTION_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                write_set_db = Some(WriteSetDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(WRITE_SET_DB_NAME),
                        WRITE_SET_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
```

**File:** config/src/config/storage_config.rs (L233-233)
```rust
            enable_storage_sharding: true,
```

**File:** config/src/config/storage_config.rs (L664-668)
```rust
            if (chain_id.is_testnet() || chain_id.is_mainnet())
                && config_yaml["rocksdb_configs"]["enable_storage_sharding"].as_bool() != Some(true)
            {
                panic!("Storage sharding (AIP-97) is not enabled in node config. Please follow the guide to migration your node, and set storage.rocksdb_configs.enable_storage_sharding to true explicitly in your node config. https://aptoslabs.notion.site/DB-Sharding-Migration-Public-Full-Nodes-1978b846eb7280b29f17ceee7d480730");
            }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L39-46)
```rust
        let candidate_transactions =
            self.get_pruning_candidate_transactions(current_progress, target_version)?;
        self.ledger_db
            .transaction_db()
            .prune_transaction_by_hash_indices(
                candidate_transactions.iter().map(|(_, txn)| txn.hash()),
                &mut batch,
            )?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L54-73)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        if let Some(indexer_db) = self.internal_indexer_db.as_ref() {
            if indexer_db.transaction_enabled() {
                let mut index_batch = SchemaBatch::new();
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut index_batch)?;
                index_batch.put::<InternalIndexerMetadataSchema>(
                    &IndexerMetadataKey::TransactionPrunerProgress,
                    &IndexerMetadataValue::Version(target_version),
                )?;
                indexer_db.get_inner_db_ref().write_schemas(index_batch)?;
            } else {
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut batch)?;
            }
        }
        self.ledger_db.transaction_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L106-131)
```rust
    fn get_pruning_candidate_transactions(
        &self,
        start: Version,
        end: Version,
    ) -> Result<Vec<(Version, Transaction)>> {
        ensure!(end >= start, "{} must be >= {}", end, start);

        let mut iter = self
            .ledger_db
            .transaction_db_raw()
            .iter::<TransactionSchema>()?;
        iter.seek(&start)?;

        // The capacity is capped by the max number of txns we prune in a single batch. It's a
        // relatively small number set in the config, so it won't cause high memory usage here.
        let mut txns = Vec::with_capacity((end - start) as usize);
        for item in iter {
            let (version, txn) = item?;
            if version >= end {
                break;
            }
            txns.push((version, txn));
        }

        Ok(txns)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/write_set_pruner.rs (L28-32)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::WriteSetPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_db.write_set_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L66-80)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        if let Some(mut indexer_batch) = indexer_batch {
            indexer_batch.put::<InternalIndexerMetadataSchema>(
                &IndexerMetadataKey::EventPrunerProgress,
                &IndexerMetadataValue::Version(target_version),
            )?;
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L391-420)
```rust
            let (txn_infos, txns_and_outputs, persisted_aux_info) = (start_version
                ..start_version + limit)
                .map(|version| {
                    let txn_info = self
                        .ledger_db
                        .transaction_info_db()
                        .get_transaction_info(version)?;
                    let events = self.ledger_db.event_db().get_events_by_version(version)?;
                    let write_set = self.ledger_db.write_set_db().get_write_set(version)?;
                    let txn = self.ledger_db.transaction_db().get_transaction(version)?;
                    let auxiliary_data = self
                        .ledger_db
                        .transaction_auxiliary_data_db()
                        .get_transaction_auxiliary_data(version)?
                        .unwrap_or_default();
                    let txn_output = TransactionOutput::new(
                        write_set,
                        events,
                        txn_info.gas_used(),
                        txn_info.status().clone().into(),
                        auxiliary_data,
                    );
                    let persisted_aux_info = self
                        .ledger_db
                        .persisted_auxiliary_info_db()
                        .get_persisted_auxiliary_info(version)?
                        .unwrap_or(PersistedAuxiliaryInfo::None);
                    Ok((txn_info, (txn, txn_output), persisted_aux_info))
                })
                .collect::<Result<Vec<_>>>()?
```

**File:** storage/aptosdb/src/ledger_db/transaction_db.rs (L56-60)
```rust
    pub(crate) fn get_transaction(&self, version: Version) -> Result<Transaction> {
        self.db
            .get::<TransactionSchema>(&version)?
            .ok_or_else(|| AptosDbError::NotFound(format!("Txn {version}")))
    }
```
