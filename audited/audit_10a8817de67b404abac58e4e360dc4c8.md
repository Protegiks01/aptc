# Audit Report

## Title
Silent Pruning Failure Leading to Unbounded Database Growth and Node Crash

## Summary
The ledger pruner's error handling in `pruner_worker.rs` silently ignores persistent database write failures, continuing to retry indefinitely without updating pruner progress. This causes the database to grow unbounded, eventually exhausting disk space and crashing the validator node, leading to consensus unavailability and potential validator slashing.

## Finding Description

The vulnerability exists in the pruner worker's error handling logic. When the `prune()` function returns an error (propagated via `?` operators from database write operations), the error is logged but the worker continues retrying without updating progress. [1](#0-0) [2](#0-1) 

These `?` operators propagate errors from database write operations in sub-pruners upward to the caller: [3](#0-2) 

The critical flaw is at the error handling: when `prune()` fails, the code logs the error, sleeps briefly, then **continues the loop** without updating pruner progress. Progress is only updated on successful pruning: [4](#0-3) 

**Attack Path (Natural Failure Scenario):**
1. A database write error occurs (disk corruption, I/O failure, insufficient permissions, low disk space, RocksDB bug)
2. The error propagates through sub-pruner write operations like `write_schemas()`: [5](#0-4) 
3. Error is caught in the pruner worker but only logged (sampled at 1 error/second)
4. Worker retries indefinitely without updating `progress`
5. New blocks continue arriving, advancing `target_version`
6. Database grows unbounded as no data is pruned
7. Disk space exhausts completely
8. Node crashes or becomes unresponsive
9. Validator fails to participate in consensus and may be slashed

**Broken Invariants:**
- **Resource Limits**: Storage grows unbounded, violating "all operations must respect storage limits"
- **Node Availability**: Critical system component (pruner) silently fails without failing the node
- **Operational Integrity**: No circuit breaker, health check failure, or alert mechanism to detect pruner malfunction

## Impact Explanation

This qualifies as **HIGH SEVERITY** per Aptos Bug Bounty criteria:

1. **"Validator node slowdowns"** - As disk fills up, I/O operations become progressively slower, degrading node performance before complete failure

2. **"API crashes"** - When disk is completely full, database write operations fail catastrophically, causing the node to crash and APIs to become unavailable

3. **"Significant protocol violations"** - Node cannot participate in consensus when crashed, violating network liveness requirements

**Additional Impacts:**
- **Validator Penalties**: Validators experience downtime and may be slashed for failing to participate in consensus
- **Network Degradation**: If multiple nodes are affected by the same underlying issue (e.g., common RocksDB bug, file system corruption pattern), network liveness degrades
- **No Automatic Recovery**: Requires manual operator intervention to diagnose and fix, as the error is only logged, not alerted

The vulnerability does not require attacker action - it's a defensive programming failure where normal operational errors are mishandled.

## Likelihood Explanation

**MEDIUM-HIGH Likelihood:**

**Common Triggering Conditions:**
- Disk corruption or bad sectors
- File system errors or permissions changes after system updates
- RocksDB internal errors or version-specific bugs
- I/O subsystem failures (hardware, driver, kernel issues)
- Transient low disk space creating a downward spiral (write fails → can't prune → disk fills more → more write failures)

**Exacerbating Factors:**
- No exponential backoff or circuit breaker - infinite immediate retries
- No maximum retry count before failing loudly
- No dedicated metric for pruning errors (only generic logs)
- No automated alert when `target_version - progress` exceeds threshold
- Disk space alerts only fire when already critical (<50GB): [6](#0-5) 

**Real-World Scenarios:**
- Production validators experiencing file system corruption
- Cloud volumes with degraded I/O performance
- Nodes recovering from previous disk space issues
- Systems with aggressive file system policies or quotas

The issue is particularly insidious because it manifests as silent degradation rather than immediate failure, making it harder to detect until catastrophic.

## Recommendation

**Immediate Fixes:**

1. **Add Pruning Error Metrics** - Create a dedicated counter for pruner failures:
   - Add `PRUNER_ERRORS` metric in `storage/aptosdb/src/metrics.rs`
   - Increment on each prune failure
   - Alert when error rate exceeds threshold

2. **Implement Circuit Breaker** - Fail the node after consecutive pruning failures:
   ```rust
   const MAX_CONSECUTIVE_FAILURES: usize = 10;
   let mut consecutive_failures = 0;
   
   loop {
       let pruner_result = self.pruner.prune(self.batch_size);
       if pruner_result.is_err() {
           consecutive_failures += 1;
           error!(
               error = ?pruner_result.err().unwrap(),
               consecutive_failures = consecutive_failures,
               "Pruner has error."
           );
           
           if consecutive_failures >= MAX_CONSECUTIVE_FAILURES {
               panic!("Pruner failed {} times consecutively. Node must shut down to prevent disk exhaustion.", MAX_CONSECUTIVE_FAILURES);
           }
           
           sleep(Duration::from_millis(self.pruning_time_interval_in_ms * consecutive_failures));
           continue;
       }
       consecutive_failures = 0; // Reset on success
       // ... rest of logic
   }
   ```

3. **Add Health Check** - Expose pruner lag as a health endpoint:
   - Monitor `target_version - progress` 
   - Fail health check if lag exceeds configured threshold
   - Allow orchestration systems to restart unhealthy nodes

4. **Add Proactive Alert** - Alert on pruner lag before disk exhaustion:
   ```yaml
   - alert: PrunerLaggingBehind
     expr: (aptos_pruner_versions{tag="target"} - aptos_pruner_versions{tag="progress"}) > 10000000
     for: 5m
     labels:
       severity: warning
   ```

**Long-Term Improvements:**
- Implement exponential backoff with jitter for retries
- Add pruner self-diagnostic mode that tests write capabilities at startup
- Consider separating pruner failures from critical path vs. alerting-only mode
- Document operational runbooks for pruner failure scenarios

## Proof of Concept

The following Rust test demonstrates the vulnerability (pseudo-code showing the conceptual issue):

```rust
#[cfg(test)]
mod test {
    use super::*;
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::sync::Arc;
    
    // Mock pruner that always fails
    struct FailingPruner {
        prune_attempts: Arc<AtomicUsize>,
        progress_val: AtomicVersion,
        target_val: AtomicVersion,
    }
    
    impl DBPruner for FailingPruner {
        fn name(&self) -> &'static str { "test_failing_pruner" }
        
        fn prune(&self, _max_versions: usize) -> Result<Version> {
            self.prune_attempts.fetch_add(1, Ordering::SeqCst);
            // Simulate persistent database error
            Err(AptosDbError::Other("Simulated disk I/O error".to_string()))
        }
        
        fn progress(&self) -> Version {
            self.progress_val.load(Ordering::SeqCst)
        }
        
        fn set_target_version(&self, target: Version) {
            self.target_val.store(target, Ordering::SeqCst);
        }
        
        fn target_version(&self) -> Version {
            self.target_val.load(Ordering::SeqCst)
        }
        
        fn record_progress(&self, version: Version) {
            self.progress_val.store(version, Ordering::SeqCst);
        }
    }
    
    #[test]
    fn test_silent_pruning_failure() {
        let prune_attempts = Arc::new(AtomicUsize::new(0));
        let failing_pruner = Arc::new(FailingPruner {
            prune_attempts: Arc::clone(&prune_attempts),
            progress_val: AtomicVersion::new(0),
            target_val: AtomicVersion::new(1000),
        });
        
        // Create pruner worker
        let inner = Arc::new(PrunerWorkerInner {
            pruning_time_interval_in_ms: 1,
            pruner: failing_pruner.clone() as Arc<dyn DBPruner>,
            batch_size: 100,
            quit_worker: AtomicBool::new(false),
        });
        
        // Run worker for a short time
        let inner_clone = Arc::clone(&inner);
        let handle = std::thread::spawn(move || {
            // Simulate work loop for limited iterations
            for _ in 0..50 {
                if inner_clone.quit_worker.load(Ordering::SeqCst) { break; }
                let result = inner_clone.pruner.prune(inner_clone.batch_size);
                if result.is_err() {
                    std::thread::sleep(Duration::from_millis(1));
                    continue;
                }
            }
        });
        
        // Let it run briefly
        std::thread::sleep(Duration::from_millis(100));
        inner.quit_worker.store(true, Ordering::SeqCst);
        handle.join().unwrap();
        
        // Verify the bug: many prune attempts but progress never advances
        assert!(prune_attempts.load(Ordering::SeqCst) > 10, 
                "Pruner should have retried many times");
        assert_eq!(failing_pruner.progress(), 0,
                   "BUG: Progress should still be 0 despite many retry attempts");
        assert_eq!(failing_pruner.target_version(), 1000,
                   "Target remains at 1000 while progress stuck at 0");
        
        // In production, this gap would grow indefinitely until disk exhaustion
        println!("Gap between target and progress: {}", 
                 failing_pruner.target_version() - failing_pruner.progress());
    }
}
```

**To reproduce in production:**
1. Deploy a validator node with pruning enabled
2. Simulate persistent database errors (e.g., revoke write permissions on RocksDB directory, or use LD_PRELOAD to inject write failures)
3. Observe logs showing repeated "Pruner has error" messages
4. Monitor `aptos_pruner_versions` metrics showing `progress` stuck while `target` advances
5. Watch disk usage grow without bounds
6. Eventually node crashes when disk fills completely

The node will continue operating normally until disk exhaustion, with no circuit breaker or loud failure to alert operators.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L75-76)
```rust
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L86-87)
```rust
            progress = current_batch_target_version;
            self.record_progress(progress);
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_metadata_pruner.rs (L42-56)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();
        for version in current_progress..target_version {
            batch.delete::<VersionDataSchema>(&version)?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::LedgerPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_metadata_db.write_schemas(batch)
    }
```

**File:** terraform/helm/monitoring/files/rules/alerts.yml (L115-125)
```yaml
      summary: "Less than 50 GB of free space on Aptos Node."
    annotations:
      description: "A validator or fullnode pod has less than 50 GB of disk space -- that's dangerously low. \
        1. A warning level alert of disk space less than 200GB should've fired a few days ago at least, search on slack and understand why it's not dealt with.
        2. Search in the code for the runbook of the warning alert, quickly go through that too determine if it's a bug. Involve the storage team and other team accordingly.
      If no useful information is found, evaluate the trend of disk usage increasing, how long can we run further? If it can't last the night, you have these options to mitigate this:
        1. Expand the disk if it's a cloud volume.
        2. Shorten the pruner windows. Before that, find the latest version of these https://github.com/aptos-labs/aptos-core/blob/48cc64df8a64f2d13012c10d8bd5bf25d94f19dc/config/src/config/storage_config.rs#L166-L218 \
          and read carefully the comments on the prune window config entries -- set safe values.
        3. If you believe this is happening on nodes that are not run by us, involve the PE / Community / Ecosystem teams to coordinate efforts needed on those nodes.
      "
```
