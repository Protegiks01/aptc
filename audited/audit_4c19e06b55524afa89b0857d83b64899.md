# Audit Report

## Title
Memory Exhaustion via Unbounded StateValue Size in Backup Restore Process

## Summary
The state snapshot restore functionality lacks size validation on individual StateValue entries, allowing an attacker to cause memory exhaustion by providing backup files containing gigabyte-sized StateValues. This bypasses the 1MB write operation limit enforced during normal transaction execution, leading to validator node crashes during restore operations.

## Finding Description

During normal transaction execution, Aptos enforces a maximum size limit on StateValue write operations of approximately 1MB (1,048,576 bytes), configured via `max_bytes_per_write_op` in the gas parameters. [1](#0-0) [2](#0-1) 

However, the backup restoration process does not enforce this limit. The vulnerability exists in the `read_state_value()` function which reads state values from backup files without size validation. [3](#0-2) 

The `read_record_bytes()` implementation reads a 4-byte size prefix (u32, allowing values up to 4GB) and allocates a buffer of that size without any bounds checking. [4](#0-3) 

**Attack Path:**

1. Attacker creates or modifies a backup file with maliciously large StateValues (e.g., multiple GB each)
2. Node operator initiates restore from this backup (via compromised storage, social engineering, or malicious backup source)
3. The restore process calls `read_state_value()` which deserializes the entire chunk into memory
4. For each record, `read_record_bytes()` allocates a BytesMut buffer equal to the attacker-controlled size
5. Multiple gigabyte-sized StateValues in a single chunk cause memory exhaustion
6. The memory allocation occurs **before** cryptographic proof verification in `add_chunk()` [5](#0-4) 

The proof verification happens in `add_chunk_impl()` only after the chunk is already fully loaded into memory: [6](#0-5) 

The `should_cut_chunk()` function only limits total chunk size but does **not** prevent individual records from exceeding reasonable bounds: [7](#0-6) 

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." While normal execution enforces a 1MB limit on StateValues, the restore process allows arbitrarily large values up to 4GB each.

## Impact Explanation

**High Severity** per Aptos bug bounty criteria: **Validator node slowdowns** and crashes.

An attacker providing malicious backup files can cause:
- Immediate memory exhaustion leading to OOM (Out of Memory) kills
- Node crashes requiring manual intervention to recover
- Extended downtime during restore operations affecting validator participation
- Potential cascade effects if multiple validators attempt restore from the same compromised backup source

While this doesn't directly cause consensus violations or fund loss, it significantly impacts network availability and validator operations, qualifying as High severity under "Validator node slowdowns" and "API crashes."

## Likelihood Explanation

**Medium Likelihood**

The attack requires:
- Attacker to control or compromise the backup storage source (S3, GCS, Azure, or local filesystem)
- Node operator to initiate a restore operation from the malicious backup
- No additional privileges or validator access

Scenarios enabling this attack:
1. **Compromised backup storage**: S3/GCS bucket credentials leaked or misconfigured permissions
2. **Social engineering**: Convincing operators to restore from attacker-controlled backup source
3. **Supply chain attack**: Compromising backup infrastructure or tooling
4. **Insider threat**: Malicious operator creating poisoned backups

The likelihood is elevated because backup restoration is a common operational procedure during:
- New validator setup
- Disaster recovery
- Network upgrades requiring state bootstrapping

## Recommendation

Implement size validation for individual StateValue entries during backup restoration, consistent with the limits enforced during normal transaction execution.

**Recommended Fix:**

1. Add a constant for maximum StateValue size in restoration context:
```rust
const MAX_STATE_VALUE_SIZE: usize = 1 << 20; // 1MB, consistent with max_bytes_per_write_op
```

2. Modify `read_record_bytes()` to accept and enforce a size limit:
```rust
async fn read_record_bytes(&mut self, max_size: Option<usize>) -> Result<Option<Bytes>> {
    let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
    // read record size
    let mut size_buf = BytesMut::with_capacity(4);
    self.read_full_buf_or_none(&mut size_buf).await?;
    if size_buf.is_empty() {
        return Ok(None);
    }

    let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
    
    // Validate size limit
    if let Some(max) = max_size {
        if record_size > max {
            bail!("Record size {} exceeds maximum allowed size {}", record_size, max);
        }
    }
    
    if record_size == 0 {
        return Ok(Some(Bytes::new()));
    }

    // read record
    let mut record_buf = BytesMut::with_capacity(record_size);
    self.read_full_buf_or_none(&mut record_buf).await?;
    if record_buf.is_empty() {
        bail!("Hit EOF when reading record.")
    }

    Ok(Some(record_buf.freeze()))
}
```

3. Update `read_state_value()` to pass the size limit:
```rust
async fn read_state_value(
    storage: &Arc<dyn BackupStorage>,
    file_handle: FileHandle,
) -> Result<Vec<(StateKey, StateValue)>> {
    let mut file = storage.open_for_read(&file_handle).await?;
    let mut chunk = vec![];
    
    // Use a generous limit (10MB) to allow some overhead beyond transaction limits
    // while still preventing extreme memory exhaustion attacks
    const MAX_RECORD_SIZE: usize = 10 << 20; // 10MB
    
    while let Some(record_bytes) = file.read_record_bytes(Some(MAX_RECORD_SIZE)).await? {
        chunk.push(bcs::from_bytes(&record_bytes)?);
    }

    Ok(chunk)
}
```

This ensures that maliciously large StateValues are rejected early, before memory allocation, while still allowing legitimate large values that may slightly exceed transaction limits.

## Proof of Concept

**Rust Test to Reproduce:**

```rust
#[tokio::test]
async fn test_oversized_state_value_memory_exhaustion() {
    use bytes::{BufMut, BytesMut};
    use std::io::Cursor;
    
    // Create a malicious backup file with gigabyte-sized StateValue
    let malicious_size: u32 = 2_000_000_000; // 2GB
    let mut malicious_backup = BytesMut::new();
    
    // Write size prefix
    malicious_backup.put_slice(&malicious_size.to_be_bytes());
    
    // Write minimal valid BCS-encoded (StateKey, StateValue)
    // In practice, this would be a valid but enormous StateValue
    let mut fake_record = vec![0u8; 1000]; // Header
    fake_record.extend(vec![0u8; malicious_size as usize - 1000]); // Payload
    malicious_backup.extend_from_slice(&fake_record);
    
    // Attempt to read - this will allocate 2GB of memory
    let mut cursor = Cursor::new(malicious_backup.freeze());
    let result = cursor.read_record_bytes().await;
    
    // Without size validation, this succeeds and allocates 2GB
    // With fix, this should fail with size limit error
    assert!(result.is_ok());
    let bytes = result.unwrap().unwrap();
    assert_eq!(bytes.len(), malicious_size as usize);
    
    // In a real scenario with multiple such records:
    // - Memory exhaustion occurs
    // - OOM killer terminates the process
    // - Node crashes before proof verification
}
```

**Steps to Exploit:**

1. Create malicious backup file with record size prefix of 0x77359400 (2GB)
2. Configure backup restore to point to malicious storage location
3. Initiate restore: `aptos-db-tool restore state-snapshot --state-manifest <malicious-manifest>`
4. Monitor memory consumption - node allocates 2GB per malicious StateValue
5. With 64 such values in a chunk (128MB default chunk size allows this): 128GB memory allocation
6. Node crashes with OOM before completing restore

**Notes**

This vulnerability represents a **defense-in-depth failure** where size limits enforced during normal operations are not consistently applied during administrative operations like backup restoration. The attack exploits the trust assumption that backup files are legitimate, but in practice, backup storage can be compromised through various vectors.

The fix should balance security (preventing memory exhaustion) with operational flexibility (allowing restoration of legitimately large state values that may have been created before stricter limits were enforced).

### Citations

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L157-161)
```rust
        ],
        [
            max_bytes_all_write_ops_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_write_ops_per_transaction" },
            10 << 20, // all write ops from a single transaction are 10MB max
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L105-107)
```rust
                if write_op_size > self.max_bytes_per_write_op {
                    return storage_write_limit_reached(None);
                }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L190-215)
```rust
                tokio::spawn(async move {
                    let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
                    Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
                })
                .await?
            }
        });
        let con = self.concurrent_downloads;
        let mut futs_stream = stream::iter(futs_iter).buffered_x(con * 2, con);
        let mut start = None;
        while let Some((chunk_idx, chunk, mut blobs, proof)) = futs_stream.try_next().await? {
            start = start.or_else(|| Some(Instant::now()));
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["add_state_chunk"]);
            let receiver = receiver.clone();
            if self.validate_modules {
                blobs = tokio::task::spawn_blocking(move || {
                    Self::validate_modules(&blobs);
                    blobs
                })
                .await?;
            }
            tokio::task::spawn_blocking(move || {
                receiver.lock().as_mut().unwrap().add_chunk(blobs, proof)
            })
            .await??;
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L253-266)
```rust
    async fn read_state_value(
        storage: &Arc<dyn BackupStorage>,
        file_handle: FileHandle,
    ) -> Result<Vec<(StateKey, StateValue)>> {
        let mut file = storage.open_for_read(&file_handle).await?;

        let mut chunk = vec![];

        while let Some(record_bytes) = file.read_record_bytes().await? {
            chunk.push(bcs::from_bytes(&record_bytes)?);
        }

        Ok(chunk)
    }
```

**File:** storage/backup/backup-cli/src/utils/read_record_bytes.rs (L44-67)
```rust
    async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
        let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
        // read record size
        let mut size_buf = BytesMut::with_capacity(4);
        self.read_full_buf_or_none(&mut size_buf).await?;
        if size_buf.is_empty() {
            return Ok(None);
        }

        // empty record
        let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
        if record_size == 0 {
            return Ok(Some(Bytes::new()));
        }

        // read record
        let mut record_buf = BytesMut::with_capacity(record_size);
        self.read_full_buf_or_none(&mut record_buf).await?;
        if record_buf.is_empty() {
            bail!("Hit EOF when reading record.")
        }

        Ok(Some(record_buf.freeze()))
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L336-392)
```rust
    /// Restores a chunk of states. This function will verify that the given chunk is correct
    /// using the proof and root hash, then write things to storage. If the chunk is invalid, an
    /// error will be returned and nothing will be written to storage.
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

```

**File:** storage/backup/backup-cli/src/utils/mod.rs (L411-413)
```rust
pub(crate) fn should_cut_chunk(chunk: &[u8], record: &[u8], max_chunk_size: usize) -> bool {
    !chunk.is_empty() && chunk.len() + record.len() + size_of::<u32>() > max_chunk_size
}
```
