# Audit Report

## Title
BoundedExecutor Task Queue Saturation Enables DoS on Critical Consensus Operations

## Summary
A single shared `BoundedExecutor` with limited capacity (default: 16 tasks) is used across multiple critical consensus components including randomness generation verification, commit message verification, and reliable broadcast aggregation. An attacker can saturate this executor by flooding RPC requests, causing verification tasks to block indefinitely and starving critical consensus operations like commit message processing, resulting in validator node slowdowns and potential consensus stalls.

## Finding Description

The consensus system creates a single `BoundedExecutor` instance that is shared across multiple critical components: [1](#0-0) 

This executor is then cloned and passed to multiple components that compete for the same limited pool of execution permits (default capacity: 16): [2](#0-1) 

**Critical Issue #1: RandManager Verification Blocks on Full Executor**

The RandManager's verification task processes incoming randomness generation RPC requests using the bounded executor: [3](#0-2) 

When the executor is at capacity, the `.await` on line 234-259 causes the entire verification loop to **block**, preventing any new randomness messages from being processed.

**Critical Issue #2: BufferManager Commit Message Verification Blocks on Full Executor**

Similarly, the BufferManager spawns a verification task that processes incoming commit messages: [4](#0-3) 

This verification task also uses `bounded_executor.spawn().await`, which blocks when the executor is saturated.

**Critical Issue #3: ReliableBroadcast Aggregation Consumes Executor Capacity**

The ReliableBroadcast mechanism, used for consensus message aggregation, also spawns tasks on the same executor: [5](#0-4) 

**Attack Scenario:**

1. An attacker floods a validator node with randomness generation RPC requests (or multiple validators send legitimate but high-volume traffic)
2. Each incoming message triggers a `bounded_executor.spawn().await` call in the RandManager verification loop
3. With only 16 available permits, the executor quickly becomes saturated
4. The BufferManager's commit message verification task attempts to spawn a new verification task but blocks waiting for an available permit
5. Commit votes and proofs cannot be verified, preventing block finalization
6. The consensus pipeline stalls as blocks cannot be committed

The vulnerability exists because:
- All components share the same `BoundedExecutor` instance (via cloning the Arc)
- The `spawn()` method is `async` and **blocks** the caller when no permits are available
- There is no prioritization - all tasks compete equally for executor capacity
- The default capacity (16) is small relative to potential message volume from a network of validators

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty criteria:

**Primary Impact: Validator Node Slowdowns**
- When the BoundedExecutor is saturated, critical consensus verification tasks block indefinitely
- This prevents the node from processing commit messages, effectively slowing down or halting consensus participation
- Multiple affected nodes could impact overall network liveness

**Secondary Impact: Potential Consensus Liveness Failure**
- If commit message verification is blocked across multiple validators, blocks cannot be finalized
- This violates the consensus liveness property, though it doesn't break safety
- Recovery requires the attack to cease or manual intervention to clear the backlog

The vulnerability affects the **Resource Limits** invariant (#9) - the shared executor does not properly isolate or prioritize critical consensus operations, allowing non-critical tasks to starve critical ones.

## Likelihood Explanation

**Likelihood: High**

**Attacker Requirements:**
- No special privileges required - any network peer can send RPC requests
- No validator access needed - external actors can trigger the vulnerability
- Low complexity - simple flood of RPC messages can saturate the executor

**Triggering Conditions:**
- Normal network operation with high validator count can naturally cause high message volume
- Legitimate but bursty traffic (e.g., during network congestion or epoch transitions) could inadvertently trigger the issue
- Malicious actors can deliberately flood randomness or commit message RPCs

**Practical Considerations:**
- The default capacity of 16 is relatively small for a production blockchain network
- Multiple components competing for the same executor makes saturation more likely
- The blocking behavior of `spawn().await` amplifies the impact - one saturated executor affects all components

## Recommendation

**Immediate Fix: Separate BoundedExecutors for Critical vs Non-Critical Operations**

Create separate `BoundedExecutor` instances with different capacities for different priority levels:

```rust
// In consensus_provider.rs start_consensus()
let critical_bounded_executor = BoundedExecutor::new(
    node_config.consensus.num_critical_executor_tasks as usize, // e.g., 32
    runtime.handle().clone(),
);

let general_bounded_executor = BoundedExecutor::new(
    node_config.consensus.num_general_executor_tasks as usize, // e.g., 16
    runtime.handle().clone(),
);

// Use critical_bounded_executor for BufferManager commit message verification
// Use general_bounded_executor for RandManager, SecretShareManager
```

**Alternative Fix: Use try_spawn() with Retry Logic**

For non-critical operations like RandManager verification, use `try_spawn()` instead of `spawn()`: [6](#0-5) 

Modify the verification tasks to use `try_spawn()` and implement backpressure or dropping of low-priority messages when the executor is saturated.

**Long-term Fix: Task Prioritization**

Implement a priority-aware executor that guarantees capacity for critical consensus operations while allowing best-effort processing of other tasks.

## Proof of Concept

```rust
// Rust integration test demonstrating the vulnerability
#[tokio::test]
async fn test_bounded_executor_starvation() {
    use aptos_bounded_executor::BoundedExecutor;
    use tokio::sync::mpsc;
    use std::time::Duration;
    
    // Create a BoundedExecutor with small capacity (simulating production config)
    let runtime = tokio::runtime::Runtime::new().unwrap();
    let executor = BoundedExecutor::new(16, runtime.handle().clone());
    
    // Simulate RandManager verification task that blocks on spawn
    let executor_clone = executor.clone();
    let (rand_tx, mut rand_rx) = mpsc::unbounded_channel::<i32>();
    tokio::spawn(async move {
        while let Some(_msg) = rand_rx.recv().await {
            // This will block when executor is full
            executor_clone.spawn(async {
                tokio::time::sleep(Duration::from_millis(100)).await;
            }).await;
        }
    });
    
    // Simulate BufferManager commit verification task
    let executor_clone2 = executor.clone();
    let (commit_tx, mut commit_rx) = mpsc::unbounded_channel::<i32>();
    let commit_blocked = std::sync::Arc::new(std::sync::atomic::AtomicBool::new(false));
    let commit_blocked_clone = commit_blocked.clone();
    tokio::spawn(async move {
        while let Some(_msg) = commit_rx.recv().await {
            commit_blocked_clone.store(true, std::sync::atomic::Ordering::SeqCst);
            // This will block when executor is full - critical path blocked!
            executor_clone2.spawn(async {
                tokio::time::sleep(Duration::from_millis(10)).await;
            }).await;
            commit_blocked_clone.store(false, std::sync::atomic::Ordering::SeqCst);
        }
    });
    
    // Attack: Flood with rand messages to saturate executor
    for _ in 0..100 {
        rand_tx.send(1).unwrap();
    }
    
    tokio::time::sleep(Duration::from_millis(50)).await;
    
    // Try to send critical commit message
    commit_tx.send(1).unwrap();
    
    // Wait and verify commit processing is blocked
    tokio::time::sleep(Duration::from_millis(200)).await;
    
    // Commit verification should be blocked because executor is saturated
    // In production, this means consensus cannot progress
    assert!(commit_blocked.load(std::sync::atomic::Ordering::SeqCst), 
            "Critical commit message verification should be blocked by saturated executor");
}
```

**Notes:**
- The vulnerability is confirmed by the shared use of a single `BoundedExecutor` across all consensus components
- The blocking `.await` behavior on `spawn()` is the root cause - it prevents verification loops from making progress when the executor is full
- The default capacity of 16 tasks is insufficient for handling concurrent operations from multiple consensus subsystems under load
- This can be triggered by legitimate high-volume traffic or deliberate flooding of RPC requests
- The fix requires either separating executors by priority or implementing non-blocking task submission with backpressure for non-critical operations

### Citations

**File:** consensus/src/consensus_provider.rs (L81-84)
```rust
    let bounded_executor = BoundedExecutor::new(
        node_config.consensus.num_bounded_executor_tasks as usize,
        runtime.handle().clone(),
    );
```

**File:** config/src/config/consensus_config.rs (L379-379)
```rust
            num_bounded_executor_tasks: 16,
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L234-259)
```rust
            bounded_executor
                .spawn(async move {
                    match bcs::from_bytes::<RandMessage<S, D>>(rand_gen_msg.req.data()) {
                        Ok(msg) => {
                            if msg
                                .verify(
                                    &epoch_state_clone,
                                    &config_clone,
                                    &fast_config_clone,
                                    rand_gen_msg.sender,
                                )
                                .is_ok()
                            {
                                let _ = tx.unbounded_send(RpcRequest {
                                    req: msg,
                                    protocol: rand_gen_msg.protocol,
                                    response_sender: rand_gen_msg.response_sender,
                                });
                            }
                        },
                        Err(e) => {
                            warn!("Invalid rand gen message: {}", e);
                        },
                    }
                })
                .await;
```

**File:** consensus/src/pipeline/buffer_manager.rs (L923-932)
```rust
                bounded_executor
                    .spawn(async move {
                        match commit_msg.req.verify(sender, &epoch_state_clone.verifier) {
                            Ok(_) => {
                                let _ = tx.unbounded_send(commit_msg);
                            },
                            Err(e) => warn!("Invalid commit message: {}", e),
                        }
                    })
                    .await;
```

**File:** crates/reliable-broadcast/src/lib.rs (L171-180)
```rust
                        let future = executor.spawn(async move {
                            (
                                    receiver,
                                    result
                                        .and_then(|msg| {
                                            msg.try_into().map_err(|e| anyhow::anyhow!("{:?}", e))
                                        })
                                        .and_then(|ack| aggregating.add(receiver, ack)),
                            )
                        }).await;
```

**File:** crates/bounded-executor/src/executor.rs (L59-68)
```rust
    pub fn try_spawn<F>(&self, future: F) -> Result<JoinHandle<F::Output>, F>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        match self.try_acquire_permit() {
            Some(permit) => Ok(self.executor.spawn(future_with_permit(future, permit))),
            None => Err(future),
        }
    }
```
