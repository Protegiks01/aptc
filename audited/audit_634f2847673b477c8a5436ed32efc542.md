# Audit Report

## Title
Batch Persistence Without BatchGenerator Tracking Causes Resource Waste and Validator Slowdowns

## Summary
When remote batches fail to send to BatchGenerator (channel full or receiver dropped), they are still persisted to BatchStore, creating an inconsistency where BatchStore has batches that BatchGenerator doesn't track. This causes duplicate batch creation, resource waste, and validator performance degradation.

## Finding Description

In `handle_batches_msg()`, the code attempts to send remote batches to BatchGenerator for transaction tracking, but unconditionally persists them regardless of send success: [1](#0-0) 

The channel is bounded with configurable size (default 1000): [2](#0-1) [3](#0-2) 

**The Vulnerability Flow:**

1. **Normal Case**: Remote batch → Send to BatchGenerator succeeds → Batch tracked in `txns_in_progress_sorted` → Persisted to BatchStore → Transactions excluded from future mempool pulls

2. **Failure Case**: Remote batch → Send to BatchGenerator **fails** (channel full/receiver dropped) → Batch **NOT** tracked → Still persisted to BatchStore → Transactions **NOT** excluded from mempool pulls

BatchGenerator uses `txns_in_progress_sorted` to prevent duplicate proposals: [4](#0-3) [5](#0-4) 

When the send fails, transactions from the remote batch are not inserted into `batches_in_progress` or `txns_in_progress_sorted`: [6](#0-5) 

**Resulting State Inconsistency:**
- Remote batch persisted in BatchStore (counts against remote author's quota)
- Transactions NOT tracked in BatchGenerator's `txns_in_progress_sorted`
- Local node pulls same transactions from mempool
- Local node creates duplicate batch (counts against local node's quota)
- Both batches exist, consume storage, and must be processed by consensus

The quota system confirms both batches consume resources: [7](#0-6) 

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria for:

1. **Validator Node Slowdowns**: Duplicate batches force nodes to:
   - Process duplicate proofs
   - Broadcast duplicate batch information  
   - Execute duplicate batch retrievals
   - Persist duplicate data to storage

2. **Resource Exhaustion**:
   - Local node's batch quota consumed faster (300,000 batch limit)
   - Local node's storage quota consumed faster (300MB DB quota, 120MB memory quota)
   - Network bandwidth wasted on duplicate broadcasts
   - CPU cycles wasted on duplicate processing

3. **Protocol Violation**: The Quorum Store protocol assumes transaction deduplication across batches. This bug violates **Invariant #9 (Resource Limits)** by allowing unbounded resource consumption through duplicate batches.

## Likelihood Explanation

**High Likelihood** during normal operations:

1. **High Network Activity**: With 10 remote batch coordinators (default `num_workers_for_remote_batches`) processing batches concurrently, the channel (size 1000) can fill during network spikes.

2. **Epoch Transitions**: During epoch changes, BatchGenerator may shut down while BatchCoordinators continue processing network messages, causing all sends to fail.

3. **Backpressure Conditions**: When the system is under backpressure, batches accumulate faster than BatchGenerator can process them.

**Reproduction Scenario**: A network with 100+ validators all broadcasting batches during high TPS periods can easily generate >1000 pending commands, filling the channel legitimately.

## Recommendation

The fix requires ensuring persistence only happens after successful BatchGenerator notification, or implementing a rollback mechanism:

**Option 1 - Persist Only on Success:**
```rust
let mut persist_requests = vec![];
for batch in batches.into_iter() {
    if let Err(e) = self
        .sender_to_batch_generator
        .send(BatchGeneratorCommand::RemoteBatch(batch.clone()))
        .await
    {
        error!("Failed to send batch to batch generator, dropping batch: {}", e);
        continue; // Skip persistence for failed batches
    }
    persist_requests.push(batch.into());
}
```

**Option 2 - Increase Channel Size and Add Backpressure:**
- Increase `channel_size` based on `num_workers_for_remote_batches`
- Add backpressure to network layer when channel is near capacity
- Implement retry logic with exponential backoff

**Option 3 - Make Send Synchronous (Preferred):**
Replace the bounded async channel with a synchronous mechanism that blocks BatchCoordinator when BatchGenerator is overloaded, providing natural backpressure.

## Proof of Concept

```rust
// Rust test demonstrating the issue
#[tokio::test]
async fn test_batch_persistence_without_tracking() {
    // 1. Create BatchCoordinator with small channel size
    let (tx, _rx_dropped) = tokio::sync::mpsc::channel(1);
    // Receiver immediately dropped - all sends will fail
    
    let batch_store = Arc::new(create_test_batch_store());
    let coordinator = BatchCoordinator::new(
        /* ... */
        tx, // This will fail on send
        batch_store.clone(),
        /* ... */
    );
    
    // 2. Send batch message - send to generator fails but persist succeeds
    let test_batch = create_test_batch(vec![test_transaction()]);
    coordinator.handle_batches_msg(peer_id, vec![test_batch]).await;
    
    // 3. Verify batch is in BatchStore
    let digest = test_batch.digest();
    assert!(batch_store.exists(&digest).is_some()); // ✓ Persisted
    
    // 4. Verify transactions NOT tracked in BatchGenerator
    // (would need access to BatchGenerator's txns_in_progress_sorted)
    // Result: transactions can be re-pulled from mempool
}
```

**Notes**

The TODO comment at line 230 suggests developer awareness of potential ordering issues, though they considered the opposite direction. The current implementation prioritizes persistence over tracking consistency, which violates the deduplication guarantee that BatchGenerator provides. This is exacerbated by the bounded channel design which can legitimately fill during high load, making this a reliability issue rather than solely an attack vector.

### Citations

**File:** consensus/src/quorum_store/batch_coordinator.rs (L228-244)
```rust
        let mut persist_requests = vec![];
        for batch in batches.into_iter() {
            // TODO: maybe don't message batch generator if the persist is unsuccessful?
            if let Err(e) = self
                .sender_to_batch_generator
                .send(BatchGeneratorCommand::RemoteBatch(batch.clone()))
                .await
            {
                warn!("Failed to send batch to batch generator: {}", e);
            }
            persist_requests.push(batch.into());
        }
        counters::RECEIVED_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        if author != self.my_peer_id {
            counters::RECEIVED_REMOTE_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        }
        self.persist_and_send_digests(persist_requests, approx_created_ts_usecs);
```

**File:** config/src/config/quorum_store_config.rs (L108-108)
```rust
            channel_size: 1000,
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L179-180)
```rust
        let (batch_generator_cmd_tx, batch_generator_cmd_rx) =
            tokio::sync::mpsc::channel(config.channel_size);
```

**File:** consensus/src/quorum_store/batch_generator.rs (L123-171)
```rust
    fn insert_batch(
        &mut self,
        author: PeerId,
        batch_id: BatchId,
        txns: Vec<SignedTransaction>,
        expiry_time_usecs: u64,
    ) {
        if self.batches_in_progress.contains_key(&(author, batch_id)) {
            return;
        }

        let txns_in_progress: Vec<_> = txns
            .par_iter()
            .with_min_len(optimal_min_len(txns.len(), 32))
            .map(|txn| {
                (
                    TransactionSummary::new(
                        txn.sender(),
                        txn.replay_protector(),
                        txn.committed_hash(),
                    ),
                    TransactionInProgress::new(txn.gas_unit_price()),
                )
            })
            .collect();

        let mut txns = vec![];
        for (summary, info) in txns_in_progress {
            let txn_info = self
                .txns_in_progress_sorted
                .entry(summary)
                .or_insert_with(|| TransactionInProgress::new(info.gas_unit_price));
            txn_info.increment();
            txn_info.gas_unit_price = info.gas_unit_price.max(txn_info.gas_unit_price);
            txns.push(summary);
        }
        let updated_expiry_time_usecs = self
            .batches_in_progress
            .get(&(author, batch_id))
            .map_or(expiry_time_usecs, |batch_in_progress| {
                expiry_time_usecs.max(batch_in_progress.expiry_time_usecs)
            });
        self.batches_in_progress.insert(
            (author, batch_id),
            BatchInProgress::new(txns, updated_expiry_time_usecs),
        );
        self.batch_expirations
            .add_item((author, batch_id), updated_expiry_time_usecs);
    }
```

**File:** consensus/src/quorum_store/batch_generator.rs (L342-360)
```rust
    pub(crate) async fn handle_scheduled_pull(
        &mut self,
        max_count: u64,
    ) -> Vec<Batch<BatchInfoExt>> {
        counters::BATCH_PULL_EXCLUDED_TXNS.observe(self.txns_in_progress_sorted.len() as f64);
        trace!(
            "QS: excluding txs len: {:?}",
            self.txns_in_progress_sorted.len()
        );

        let mut pulled_txns = self
            .mempool_proxy
            .pull_internal(
                max_count,
                self.config.sender_max_total_bytes as u64,
                self.txns_in_progress_sorted.clone(),
            )
            .await
            .unwrap_or_default();
```

**File:** consensus/src/quorum_store/batch_generator.rs (L392-401)
```rust
    pub(crate) fn handle_remote_batch(
        &mut self,
        author: PeerId,
        batch_id: BatchId,
        txns: Vec<SignedTransaction>,
    ) {
        let expiry_time_usecs = aptos_infallible::duration_since_epoch().as_micros() as u64
            + self.config.remote_batch_expiry_gap_when_init_usecs;
        self.insert_batch(author, batch_id, txns, expiry_time_usecs);
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L64-84)
```rust
    pub(crate) fn update_quota(&mut self, num_bytes: usize) -> anyhow::Result<StorageMode> {
        if self.batch_balance == 0 {
            counters::EXCEEDED_BATCH_QUOTA_COUNT.inc();
            bail!("Batch quota exceeded ");
        }

        if self.db_balance >= num_bytes {
            self.batch_balance -= 1;
            self.db_balance -= num_bytes;

            if self.memory_balance >= num_bytes {
                self.memory_balance -= num_bytes;
                Ok(StorageMode::MemoryAndPersisted)
            } else {
                Ok(StorageMode::PersistedOnly)
            }
        } else {
            counters::EXCEEDED_STORAGE_QUOTA_COUNT.inc();
            bail!("Storage quota exceeded ");
        }
    }
```
