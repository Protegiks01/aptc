# Audit Report

## Title
Consensus Observer Node Denial of Service via Infinite Retry Loop in Payload Retrieval

## Summary
The consensus observer's payload manager contains an infinite retry loop when retrieving block transactions. When a race condition causes payloads to be removed from storage between verification and execution, the node enters an infinite retry loop, causing a complete denial of service for that consensus observer node.

## Finding Description

The vulnerability exists in the interaction between three components:

1. **Payload Verification and Removal Race Condition:** In `consensus_observer.rs`, when an ordered block is received, the system checks if all payloads exist before processing [1](#0-0) . However, payloads can be removed asynchronously via the commit callback [2](#0-1) .

2. **Asynchronous Pipeline Execution:** The block enters the execution pipeline where the materialize phase runs asynchronously [3](#0-2) . There's a time gap between payload verification and actual retrieval.

3. **Infinite Retry Loop:** When `get_transactions_for_observer()` returns an error (missing or unverified payload) [4](#0-3) , the error propagates to `materialize_block()` [5](#0-4) , which is then caught in an **infinite retry loop with no abort condition** [6](#0-5) .

**Attack Scenario:**
1. Consensus observer receives ordered block at round N
2. Payloads exist at verification time (line 706 check passes)
3. Block enters execution pipeline and materialize phase starts waiting
4. Meanwhile, blocks at rounds N+1, N+2, ... N+K are received and committed quickly
5. Commit callback removes all payloads up to round N+K (including block N) [7](#0-6) 
6. When block N's materialize phase tries to retrieve transactions, payload is missing
7. System enters infinite retry loop, logging warnings every 100ms but never aborting
8. Consensus observer node is completely stuck, unable to process any new blocks

This can occur naturally during high throughput when blocks commit faster than they execute, or can be triggered by a malicious publisher sending blocks without coordination.

## Impact Explanation

**Severity: High** (up to $50,000 per Aptos Bug Bounty)

This vulnerability causes **validator node slowdown / complete unresponsiveness** which qualifies as High Severity under the Aptos Bug Bounty program.

**Specific Impacts:**
- **Consensus Observer Liveness Failure:** The affected node becomes completely stuck in the retry loop and cannot process any new blocks
- **Resource Exhaustion:** CPU is consumed by the infinite loop, logging warnings every 100ms indefinitely  
- **Network Degradation:** Affected consensus observers cannot serve their downstream clients, reducing network reliability
- **No Recovery Mechanism:** The loop has no timeout, maximum retry count, or abort condition - the node must be manually restarted

This breaks the **Consensus Liveness** invariant - the system must make progress even under adverse conditions.

## Likelihood Explanation

**Likelihood: High**

This vulnerability has high likelihood because:

1. **Natural Occurrence:** The race condition can happen during normal operation when the network is under high load and blocks are being committed faster than they're being executed locally
2. **No Coordination:** There's no synchronization mechanism between payload cleanup and pipeline execution
3. **Timing Window:** The window between payload verification (line 706) and retrieval (in materialize phase) can be significant if parent blocks are slow to execute
4. **Consensus Observer Design:** The consensus observer architecture inherently has this timing gap as payloads are verified early but used later in the async pipeline

An attacker can also deliberately trigger this by:
- Acting as a consensus publisher and sending blocks at high rate
- Timing block messages to maximize the race condition window
- No special privileges required - just network access to send messages

## Recommendation

Replace the infinite retry loop with a bounded retry mechanism that aborts after a reasonable number of attempts:

```rust
async fn materialize(
    preparer: Arc<BlockPreparer>,
    block: Arc<Block>,
    qc_rx: oneshot::Receiver<Arc<QuorumCert>>,
) -> TaskResult<MaterializeResult> {
    let mut tracker = Tracker::start_waiting("materialize", &block);
    tracker.start_working();

    let qc_rx = async {
        match qc_rx.await {
            Ok(qc) => Some(qc),
            Err(_) => {
                warn!("[BlockPreparer] qc tx cancelled for block {}", block.id());
                None
            },
        }
    }
    .shared();
    
    // Add maximum retry count and timeout
    const MAX_RETRIES: u32 = 50; // 5 seconds total with 100ms sleep
    let mut retry_count = 0;
    
    let result = loop {
        match preparer.materialize_block(&block, qc_rx.clone()).await {
            Ok(input_txns) => break Ok(input_txns),
            Err(e) => {
                retry_count += 1;
                if retry_count >= MAX_RETRIES {
                    error!(
                        "[BlockPreparer] Failed to prepare block {} after {} retries: {}",
                        block.id(),
                        MAX_RETRIES,
                        e
                    );
                    return Err(TaskError::InternalError(anyhow!(
                        "Failed to materialize block after {} retries: {}",
                        MAX_RETRIES,
                        e
                    )));
                }
                warn!(
                    "[BlockPreparer] failed to prepare block {}, retrying ({}/{}): {}",
                    block.id(),
                    retry_count,
                    MAX_RETRIES,
                    e
                );
                tokio::time::sleep(Duration::from_millis(100)).await;
            },
        }
    };
    result
}
```

Additionally, improve the coordination between payload cleanup and pipeline execution:
- Don't remove payloads for blocks that are currently in the pipeline
- Add reference counting or keep payloads until pipeline completion
- Add better logging to detect when this race condition occurs

## Proof of Concept

This vulnerability can be demonstrated through the following sequence:

```rust
// Pseudo-code demonstrating the race condition
#[tokio::test]
async fn test_infinite_retry_loop_dos() {
    // Setup consensus observer with payload store
    let observer = ConsensusObserver::new(...);
    let payload_store = observer.block_payload_store();
    
    // Create ordered block at round 100
    let block_round_100 = create_ordered_block(100);
    let block_payload_100 = create_block_payload(100);
    
    // Insert payload (simulating normal flow)
    payload_store.insert_block_payload(block_payload_100, true);
    
    // Verify payload exists (this is the line 706 check)
    assert!(payload_store.all_payloads_exist(&[block_round_100]));
    
    // Start processing block (enters pipeline)
    observer.process_ordered_block(block_round_100).await;
    
    // Simulate rapid commits of later blocks
    for round in 101..110 {
        let block = create_ordered_block(round);
        observer.process_ordered_block(block).await;
        
        // Simulate commit callback
        let commit_decision = create_commit_decision(round);
        observer.handle_committed_blocks(commit_decision.commit_proof());
    }
    
    // At this point, payload for round 100 is removed
    assert!(!payload_store.all_payloads_exist(&[block_round_100]));
    
    // The materialize phase for block 100 tries to get transactions
    // Result: Infinite retry loop - node becomes unresponsive
    // The task will hang forever with warnings logged every 100ms
    
    // Monitor: CPU usage increases, node stops processing new blocks
    // Only recovery: Manual node restart
}
```

The vulnerability is confirmed by examining the code flow where errors are **NOT** properly propagated to cause task failure, but instead trigger infinite retries [8](#0-7) .

### Citations

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L706-713)
```rust
        if self.all_payloads_exist(pending_block_with_metadata.ordered_block().blocks()) {
            self.process_ordered_block(pending_block_with_metadata)
                .await;
        } else {
            self.observer_block_data
                .lock()
                .insert_pending_block(pending_block_with_metadata);
        }
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L182-189)
```rust
    fn handle_committed_blocks(&mut self, ledger_info: LedgerInfoWithSignatures) {
        // Remove the committed blocks from the payload and ordered block stores
        self.block_payload_store.remove_blocks_for_epoch_round(
            ledger_info.commit_info().epoch(),
            ledger_info.commit_info().round(),
        );
        self.ordered_block_store
            .remove_blocks_for_commit(&ledger_info);
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L457-460)
```rust
        let materialize_fut = spawn_shared_fut(
            Self::materialize(self.block_preparer.clone(), block.clone(), qc_rx),
            Some(&mut abort_handles),
        );
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L634-646)
```rust
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
```

**File:** consensus/src/payload_manager/co_payload_manager.rs (L49-57)
```rust
        Entry::Vacant(_) => {
            // This shouldn't happen (the payload should already be present)
            let error = format!(
                "Missing payload data for block epoch {}, round {}!",
                block.epoch(),
                block.round()
            );
            return Err(InternalError { error });
        },
```

**File:** consensus/src/block_preparer.rs (L54-63)
```rust
        let (txns, max_txns_from_block_to_execute, block_gas_limit) = tokio::select! {
                // Poll the block qc future until a QC is received. Ignore None outcomes.
                Some(qc) = block_qc_fut => {
                    let block_voters = Some(qc.ledger_info().get_voters_bitvec().clone());
                    self.payload_manager.get_transactions(block, block_voters).await
                },
                result = self.payload_manager.get_transactions(block, None) => {
                   result
                }
        }?;
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L112-119)
```rust
    pub fn remove_blocks_for_epoch_round(&self, epoch: u64, round: Round) {
        // Determine the round to split off
        let split_off_round = round.saturating_add(1);

        // Remove the blocks from the payload store
        let mut block_payloads = self.block_payloads.lock();
        *block_payloads = block_payloads.split_off(&(epoch, split_off_round));
    }
```
