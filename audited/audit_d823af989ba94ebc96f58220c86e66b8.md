# Audit Report

## Title
Deadlock in Sharded Block Executor Due to Panic Before StopMsg Delivery

## Summary
The `execute_transactions_with_dependencies()` function spawns two threads within a rayon scope, where one thread blocks waiting for a `StopMsg` from the other. If the second thread panics before sending the `StopMsg`, the first thread deadlocks indefinitely, causing the validator node to hang and lose liveness. [1](#0-0) 

## Finding Description

The vulnerability exists in the thread coordination mechanism used for cross-shard execution. The function creates two scoped spawns:

1. **First spawn** (CrossShardCommitReceiver): Runs a loop that blocks on `receive_cross_shard_msg()`, which internally calls `recv().unwrap()` on a crossbeam channel. This is a **blocking call** that waits indefinitely for messages. [2](#0-1) [3](#0-2) 

2. **Second spawn** (Block executor): Executes the block and must send a `StopMsg` to terminate the first spawn's loop before completing. [4](#0-3) 

**Critical flaw**: If the second spawn panics **before** successfully sending the `StopMsg` (lines 164-173), the following deadlock sequence occurs:

1. The second spawn panics and is caught by rayon's scope handler
2. The first spawn remains blocked on `recv()`, waiting for a `StopMsg` that will never arrive
3. Rayon's scope semantics dictate that it must wait for **all** spawned tasks to complete before propagating any panics
4. The scope waits indefinitely for the first spawn to complete
5. **The entire executor service deadlocks** - the function never returns
6. The validator node becomes unresponsive and cannot process further blocks

**Panic scenarios before StopMsg**:
- Any uncaught panic in `execute_block_on_thread_pool()` 
- Assertion failures or explicit `panic!()` calls in the execution path
- The `send_cross_shard_msg()` itself contains `.unwrap()` calls that could panic if channels are in unexpected states [5](#0-4) 

While the block executor has panic handling that converts some panics to errors, uncaught panics (like those from `.unwrap()` calls on channel operations, array bounds violations, or assertion failures) will cause this deadlock.

## Impact Explanation

This is a **Medium severity** vulnerability per Aptos bug bounty criteria because:

- **Liveness Failure**: A deadlocked executor service cannot process blocks, causing the validator node to become completely unresponsive
- **Availability Impact**: Affected nodes cannot participate in consensus, reducing network capacity
- **Recovery Required**: Operators must manually restart the affected validator nodes
- **No Direct Fund Loss**: While serious, this doesn't directly cause loss of funds or consensus safety violations
- **State Inconsistency**: The node may be stuck mid-block execution, requiring careful recovery procedures

The impact aligns with Medium severity: "State inconsistencies requiring intervention" - the deadlocked node requires manual intervention (restart) and may have partially-executed state requiring cleanup.

## Likelihood Explanation

**Likelihood: Medium to Low**

While production code aims to minimize panics, the likelihood is non-negligible because:

**Factors increasing likelihood:**
- Multiple `.unwrap()` calls in the critical path create panic opportunities
- Complex concurrent execution increases the chance of race conditions leading to unexpected states
- Edge cases in cross-shard messaging could trigger channel errors
- Assertion failures in debug or production code
- Array/vector index out of bounds in transaction processing

**Factors decreasing likelihood:**
- Most VM panics are caught and converted to errors by the block executor
- Production Rust code typically handles errors explicitly
- The code would need to encounter truly unexpected states

However, the **severity of a single occurrence** (complete node unavailability) makes this vulnerability significant even with moderate likelihood.

## Recommendation

Implement proper cleanup mechanisms to prevent deadlock on panic:

**Solution 1: Use timeout on receive**
Replace blocking `recv()` with `recv_timeout()` to prevent indefinite blocking:

```rust
fn receive_cross_shard_msg(&self, current_round: RoundId) -> CrossShardMsg {
    self.message_rxs[current_round]
        .recv_timeout(Duration::from_secs(60))
        .expect("Cross-shard message receive timeout")
}
```

**Solution 2: Use panic guard with cleanup** (Preferred)
Wrap the second spawn with a panic guard that ensures StopMsg delivery:

```rust
s.spawn(move |_| {
    // Set up panic guard
    let _guard = CallOnDrop::new(|| {
        // Ensure StopMsg is sent even on panic
        if let Some(shard_id) = shard_id {
            let _ = cross_shard_client_clone.send_cross_shard_msg(
                shard_id, round, CrossShardMsg::StopMsg
            );
        } else {
            let _ = cross_shard_client_clone.send_global_msg(CrossShardMsg::StopMsg);
        }
    });
    
    // Execute block...
    let ret = AptosVMBlockExecutorWrapper::execute_block_on_thread_pool(...);
    
    // Send StopMsg on success (guard won't duplicate)
    std::mem::forget(_guard);
    // ... existing StopMsg send code ...
});
```

**Solution 3: Use select! with panic channel**
Create a panic notification channel that allows early termination of the receiver loop on panic.

**Additional hardening:**
- Replace `.unwrap()` calls with proper error handling
- Add timeout mechanisms to all blocking operations
- Implement health checks that can detect and recover from deadlocked states

## Proof of Concept

```rust
#[test]
fn test_deadlock_on_panic_before_stopmsg() {
    use std::sync::Arc;
    use std::time::Duration;
    use crossbeam_channel::unbounded;
    
    // Simulate the channel setup
    let (msg_tx, msg_rx) = unbounded();
    let (callback_tx, callback_rx) = oneshot::channel();
    
    // Simulate the executor thread pool
    let executor_pool = Arc::new(
        rayon::ThreadPoolBuilder::new()
            .num_threads(4)
            .build()
            .unwrap()
    );
    
    let executor_pool_clone = executor_pool.clone();
    let msg_rx_clone = msg_rx.clone();
    
    // This should deadlock
    let result = std::panic::catch_unwind(|| {
        executor_pool.scope(|s| {
            // First spawn: waits for StopMsg
            s.spawn(move |_| {
                loop {
                    match msg_rx_clone.recv() {
                        Ok(msg) => {
                            if msg == "STOP" {
                                break;
                            }
                        }
                        Err(_) => break,
                    }
                }
            });
            
            // Second spawn: panics before sending StopMsg
            s.spawn(move |_| {
                // Simulate work
                std::thread::sleep(Duration::from_millis(100));
                
                // PANIC before sending StopMsg
                panic!("Simulated panic before StopMsg");
                
                // This line never executes
                #[allow(unreachable_code)]
                msg_tx.send("STOP").unwrap();
            });
        });
    });
    
    // In a real deadlock scenario, this test would hang indefinitely
    // For testing purposes, we use a timeout
    let timeout_result = std::thread::spawn(move || {
        std::thread::sleep(Duration::from_secs(5));
        eprintln!("DEADLOCK DETECTED: Scope did not complete within timeout");
    }).join();
    
    assert!(result.is_err(), "Expected panic but scope completed");
}
```

**To reproduce in the actual codebase:**
1. Modify `execute_transactions_with_dependencies()` to inject a panic after line 156 but before line 164
2. Run the sharded executor with this modification
3. Observe that the executor service hangs indefinitely
4. Monitor thread states - the receiver thread will be blocked on `recv()` forever

## Notes

This vulnerability specifically affects the **sharded block executor** implementation used for parallel transaction execution. The issue demonstrates a fundamental problem with the coordination pattern: using blocking channel operations without timeout mechanisms in panic-prone concurrent code creates deadlock opportunities.

The vulnerability is exacerbated by the extensive use of `.unwrap()` calls throughout the codebase, which convert errors into panics. While the VM's panic handling catches some execution panics, infrastructure-level panics (channel errors, index bounds, assertions) can still trigger the deadlock condition.

**Affected invariants:**
- **Liveness/Availability**: Validator nodes must remain responsive and process blocks continuously
- **Resource Management**: Thread resources must be properly cleaned up even in error conditions

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L103-183)
```rust
    pub fn execute_transactions_with_dependencies(
        shard_id: Option<ShardId>, // None means execution on global shard
        executor_thread_pool: Arc<rayon::ThreadPool>,
        transactions: Vec<TransactionWithDependencies<AnalyzedTransaction>>,
        cross_shard_client: Arc<dyn CrossShardClient>,
        cross_shard_commit_sender: Option<CrossShardCommitSender>,
        round: usize,
        state_view: &S,
        config: BlockExecutorConfig,
    ) -> Result<Vec<TransactionOutput>, VMStatus> {
        let (callback, callback_receiver) = oneshot::channel();

        let cross_shard_state_view = Arc::new(CrossShardStateView::create_cross_shard_state_view(
            state_view,
            &transactions,
        ));

        let cross_shard_state_view_clone = cross_shard_state_view.clone();
        let cross_shard_client_clone = cross_shard_client.clone();

        let aggr_overridden_state_view = Arc::new(AggregatorOverriddenStateView::new(
            cross_shard_state_view.as_ref(),
            TOTAL_SUPPLY_AGGR_BASE_VAL,
        ));

        let signature_verified_transactions: Vec<SignatureVerifiedTransaction> = transactions
            .into_iter()
            .map(|txn| txn.into_txn().into_txn())
            .collect();
        let executor_thread_pool_clone = executor_thread_pool.clone();

        executor_thread_pool.clone().scope(|s| {
            s.spawn(move |_| {
                CrossShardCommitReceiver::start(
                    cross_shard_state_view_clone,
                    cross_shard_client,
                    round,
                );
            });
            s.spawn(move |_| {
                let txn_provider =
                    DefaultTxnProvider::new_without_info(signature_verified_transactions);
                let ret = AptosVMBlockExecutorWrapper::execute_block_on_thread_pool(
                    executor_thread_pool,
                    &txn_provider,
                    aggr_overridden_state_view.as_ref(),
                    // Since we execute blocks in parallel, we cannot share module caches, so each
                    // thread has its own caches.
                    &AptosModuleCacheManager::new(),
                    config,
                    TransactionSliceMetadata::unknown(),
                    cross_shard_commit_sender,
                )
                .map(BlockOutput::into_transaction_outputs_forced);
                if let Some(shard_id) = shard_id {
                    trace!(
                        "executed sub block for shard {} and round {}",
                        shard_id,
                        round
                    );
                    // Send a self message to stop the cross-shard commit receiver.
                    cross_shard_client_clone.send_cross_shard_msg(
                        shard_id,
                        round,
                        CrossShardMsg::StopMsg,
                    );
                } else {
                    trace!("executed block for global shard and round {}", round);
                    // Send a self message to stop the cross-shard commit receiver.
                    cross_shard_client_clone.send_global_msg(CrossShardMsg::StopMsg);
                }
                callback.send(ret).unwrap();
                executor_thread_pool_clone.spawn(move || {
                    // Explicit async drop
                    drop(txn_provider);
                });
            });
        });

        block_on(callback_receiver).unwrap()
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L26-45)
```rust
    pub fn start<S: StateView + Sync + Send>(
        cross_shard_state_view: Arc<CrossShardStateView<S>>,
        cross_shard_client: Arc<dyn CrossShardClient>,
        round: RoundId,
    ) {
        loop {
            let msg = cross_shard_client.receive_cross_shard_msg(round);
            match msg {
                RemoteTxnWriteMsg(txn_commit_msg) => {
                    let (state_key, write_op) = txn_commit_msg.take();
                    cross_shard_state_view
                        .set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
                },
                CrossShardMsg::StopMsg => {
                    trace!("Cross shard commit receiver stopped for round {}", round);
                    break;
                },
            }
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L331-333)
```rust
    fn send_cross_shard_msg(&self, shard_id: ShardId, round: RoundId, msg: CrossShardMsg) {
        self.message_txs[shard_id][round].send(msg).unwrap()
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L335-337)
```rust
    fn receive_cross_shard_msg(&self, current_round: RoundId) -> CrossShardMsg {
        self.message_rxs[current_round].recv().unwrap()
    }
```
