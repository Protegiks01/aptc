# Audit Report

## Title
Epoch Snapshot Pruner Progress Marker Desynchronization Between Metadata and Shard Databases

## Summary
The `epoch_snapshot_pruner` writes its global progress marker to the metadata database before shard pruners complete their work, creating a critical synchronization gap. If the system crashes during this window, the progress marker incorrectly indicates that data has been fully pruned when shard databases still contain unpruned nodes, violating state consistency guarantees.

## Finding Description

The vulnerability exists in the coordination between the metadata pruner and shard pruners within the `StateMerklePruner<StaleNodeIndexCrossEpochSchema>` implementation.

**The Critical Flow:** [1](#0-0) 

During pruning, the system executes:
1. Calls `metadata_pruner.maybe_prune_single_version()` which processes stale indices from the metadata DB
2. Inside this call, the global progress marker is written immediately: [2](#0-1) 

3. Only AFTER this progress is committed does `prune_shards()` begin processing the shard databases

The progress marker uses `S::progress_metadata_key(None)`, which for `StaleNodeIndexCrossEpochSchema` translates to `DbMetadataKey::EpochEndingStateMerklePrunerProgress`: [3](#0-2) 

**The Vulnerability Window:**

When sharding is enabled (16 separate shard databases), the metadata DB stores top-level Merkle tree nodes while shards store leaf nodes: [4](#0-3) 

Node routing determines placement: [5](#0-4) 

If a crash occurs after the metadata pruner writes `progress = V` but before shard pruners complete:
- Metadata DB: Top-level nodes for versions ≤ V are deleted, progress marker = V
- Shard DBs: Leaf nodes for versions ≤ V still exist, but global progress says they're pruned

**Impact on State Queries:**

The system checks if data is pruned via: [6](#0-5) 

With the desynchronized progress marker:
1. A query for epoch-ending version `V_query` where `V_query < progress` arrives
2. The check passes if `V_query` is an epoch-ending version (line 293)
3. Query attempts to traverse the Merkle tree
4. **Top-level nodes are missing** (deleted from metadata DB)
5. **Leaf nodes still exist** (not yet deleted from shard DBs)
6. Tree traversal fails with "node not found" errors despite the data being partially present

This breaks the invariant that epoch-ending versions within the epoch snapshot pruner's window must be accessible.

## Impact Explanation

This qualifies as **Medium Severity** under Aptos Bug Bounty criteria:
- **State inconsistencies requiring intervention**: The database enters an inconsistent state where the progress marker doesn't reflect actual pruning completion
- **Availability impact**: Epoch-ending state queries may fail unexpectedly, breaking state sync and snapshot functionality
- **Data integrity**: Violates the guarantee that `min_readable_version` accurately represents available data

The vulnerability doesn't directly cause fund loss or consensus failure, but it compromises state store reliability and could require manual database recovery.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability triggers whenever:
1. Sharded state merkle DB is enabled (production configuration)
2. Epoch snapshot pruner is actively pruning
3. Any crash, restart, or abnormal termination occurs during the pruning window

Given that:
- Background pruning runs continuously on validator nodes
- The metadata pruning completes quickly (single DB transaction) while shard pruning takes longer (16 parallel operations)
- System crashes, OOM kills, or forced restarts are common in production
- The vulnerability window exists on EVERY pruning cycle

The race condition will occur regularly in production deployments.

## Recommendation

**Fix: Delay global progress marker update until all shard pruners complete**

Modify the pruning flow to only update the global progress marker after shard pruning succeeds:

```rust
// In state_merkle_metadata_pruner.rs, remove the progress write from maybe_prune_single_version
pub(in crate::pruner) fn maybe_prune_single_version(
    &self,
    current_progress: Version,
    target_version: Version,
) -> Result<Option<Version>> {
    // ... existing logic ...
    
    let mut batch = SchemaBatch::new();
    indices.into_iter().try_for_each(|index| {
        batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
        batch.delete::<S>(&index)
    })?;
    
    // REMOVE THIS: Do NOT write progress here
    // batch.put::<DbMetadataSchema>(&S::progress_metadata_key(None), ...)?;
    
    self.metadata_db.write_schemas(batch)?;
    // ... rest ...
}
```

```rust
// In mod.rs, write progress AFTER shard pruning completes
fn prune(&self, batch_size: usize) -> Result<Version> {
    // ... existing loop ...
    while progress < target_version {
        if let Some(target_version_for_this_round) = self
            .metadata_pruner
            .maybe_prune_single_version(progress, target_version)?
        {
            self.prune_shards(progress, target_version_for_this_round, batch_size)?;
            
            // Write progress marker ONLY after shards complete
            self.metadata_db.put::<DbMetadataSchema>(
                &S::progress_metadata_key(None),
                &DbMetadataValue::Version(target_version_for_this_round)
            )?;
            
            progress = target_version_for_this_round;
            self.record_progress(target_version_for_this_round);
        }
    }
}
```

This ensures atomicity: the global progress marker only advances once ALL pruning work completes.

## Proof of Concept

```rust
#[test]
fn test_epoch_snapshot_pruner_progress_desync() {
    // Setup sharded StateMerkleDb
    let tmpdir = TempPath::new();
    let db = StateMerkleDb::new(/* sharding enabled */);
    
    // Create epoch snapshot pruner with StaleNodeIndexCrossEpochSchema
    let pruner = StateMerklePruner::<StaleNodeIndexCrossEpochSchema>::new(Arc::new(db)).unwrap();
    
    // Commit state at versions 100, 200, 300 with previous_epoch_ending_version
    // This creates StaleNodeIndexCrossEpochSchema entries
    for version in [100, 200, 300] {
        commit_test_state(&db, version, Some(version - 1));
    }
    
    // Set pruner target to 200
    pruner.set_target_version(200);
    
    // Manually invoke metadata pruner (simulating the pruning flow)
    let metadata_db = db.metadata_db();
    let metadata_pruner = StateMerkleMetadataPruner::<StaleNodeIndexCrossEpochSchema>::new(metadata_db);
    
    // This writes progress = 200 to metadata DB
    metadata_pruner.maybe_prune_single_version(0, 200).unwrap();
    
    // Verify progress marker shows 200
    let progress = metadata_db.get::<DbMetadataSchema>(
        &DbMetadataKey::EpochEndingStateMerklePrunerProgress
    ).unwrap().unwrap().expect_version();
    assert_eq!(progress, 200);
    
    // SIMULATE CRASH: shard pruners never ran
    
    // Restart: Check if shard 0 still has unpruned data
    let shard_0 = db.db_shard(0);
    let mut iter = shard_0.iter::<StaleNodeIndexCrossEpochSchema>().unwrap();
    iter.seek(&StaleNodeIndex { stale_since_version: 100, node_key: NodeKey::new_empty_path(0) }).unwrap();
    
    // BUG: Shard still has entries <= 200, but progress marker says they're pruned!
    let (index, _) = iter.next().unwrap().unwrap();
    assert!(index.stale_since_version <= 200, "Shard has unpruned data despite progress = 200");
    
    // Attempt to query epoch-ending version 150
    // This should work according to progress marker, but will fail due to missing top-level nodes
    let result = query_state_at_version(&db, 150);
    assert!(result.is_err(), "Query fails due to inconsistent state");
}
```

This demonstrates the desynchronization where the progress marker indicates completion while shards remain unpruned, violating state consistency guarantees.

## Notes

This vulnerability specifically affects the `epoch_snapshot_pruner` (using `StaleNodeIndexCrossEpochSchema`) but the same pattern exists in the regular `state_merkle_pruner` (using `StaleNodeIndexSchema`). Both should be fixed with the same synchronization approach.

The issue is particularly critical for epoch snapshots because these are relied upon for state synchronization and must maintain availability guarantees. The desynchronization breaks the contract that `min_readable_version` accurately represents what data is accessible.

### Citations

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L76-84)
```rust
        while progress < target_version {
            if let Some(target_version_for_this_round) = self
                .metadata_pruner
                .maybe_prune_single_version(progress, target_version)?
            {
                self.prune_shards(progress, target_version_for_this_round, batch_size)?;
                progress = target_version_for_this_round;
                info!(name = S::name(), progress = progress);
                self.record_progress(target_version_for_this_round);
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs (L66-71)
```rust
        batch.put::<DbMetadataSchema>(
            &S::progress_metadata_key(None),
            &DbMetadataValue::Version(target_version_for_this_round),
        )?;

        self.metadata_db.write_schemas(batch)?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/generics.rs (L33-40)
```rust
impl StaleNodeIndexSchemaTrait for StaleNodeIndexCrossEpochSchema {
    fn progress_metadata_key(shard_id: Option<usize>) -> DbMetadataKey {
        if let Some(shard_id) = shard_id {
            DbMetadataKey::EpochEndingStateMerkleShardPrunerProgress(shard_id)
        } else {
            DbMetadataKey::EpochEndingStateMerklePrunerProgress
        }
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L72-77)
```rust
pub struct StateMerkleDb {
    // Stores metadata and top levels (non-sharded part) of tree nodes.
    state_merkle_metadata_db: Arc<DB>,
    // Stores sharded part of tree nodes.
    state_merkle_db_shards: [Arc<DB>; NUM_STATE_SHARDS],
    enable_sharding: bool,
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L589-595)
```rust
    fn db_by_key(&self, node_key: &NodeKey) -> &DB {
        if let Some(shard_id) = node_key.get_shard_id() {
            self.db_shard(shard_id)
        } else {
            self.metadata_db()
        }
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L273-302)
```rust
    pub(super) fn error_if_state_merkle_pruned(
        &self,
        data_type: &str,
        version: Version,
    ) -> Result<()> {
        let min_readable_version = self
            .state_store
            .state_db
            .state_merkle_pruner
            .get_min_readable_version();
        if version >= min_readable_version {
            return Ok(());
        }

        let min_readable_epoch_snapshot_version = self
            .state_store
            .state_db
            .epoch_snapshot_pruner
            .get_min_readable_version();
        if version >= min_readable_epoch_snapshot_version {
            self.ledger_db.metadata_db().ensure_epoch_ending(version)
        } else {
            bail!(
                "{} at version {} is pruned. snapshots are available at >= {}, epoch snapshots are available at >= {}",
                data_type,
                version,
                min_readable_version,
                min_readable_epoch_snapshot_version,
            )
        }
```
