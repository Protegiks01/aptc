# Audit Report

## Title
Stale Merkle Tree Node Caches Persist After StateStore Reset Leading to State Corruption

## Summary
The `reset_state_store()` function fails to clear Merkle tree node caches in `StateMerkleDb` when resetting the state store during backup/restore operations. This allows stale cached nodes to persist and be returned for subsequent queries, potentially causing state root hash mismatches, proof verification failures, and consensus divergence.

## Finding Description

The `reset_state_store()` function is called during restore operations to reset the state store to a clean state. However, the reset is incomplete. [1](#0-0) 

This calls `StateStore::reset()`: [2](#0-1) 

The reset process:
1. Calls `buffered_state.lock().quit()` to stop the buffered state committer thread
2. Recreates the buffered state from the latest snapshot
3. **Crucially, reuses the same `state_db` Arc**, which contains the `state_merkle_db`

The `StateMerkleDb` structure contains two types of caches for Merkle tree nodes: [3](#0-2) 

These caches are populated during tree operations: [4](#0-3) 

When querying nodes, the system checks these caches first: [5](#0-4) 

**The Critical Issue:** During restore operations, the system can overwrite data at existing versions using `StateSnapshotRestore::new_overwrite()`. The test demonstrates this: [6](#0-5) 

This creates the vulnerability scenario:
1. A node has version N with Merkle tree nodes cached in `version_caches` and `lru_cache`
2. During restore, `new_overwrite()` writes completely different tree data to version N
3. `reset_state_store()` is called but does NOT clear the Merkle node caches
4. The caches still contain the old nodes for version N
5. Subsequent queries for version N return stale cached nodes instead of reading the new data from disk
6. This causes incorrect state root hash calculations and Merkle proof generation

Neither `VersionedNodeCache` nor `LruNodeCache` has any method to clear cached entries: [7](#0-6) [8](#0-7) 

## Impact Explanation

This vulnerability meets **High Severity** criteria per the Aptos bug bounty program:

**State Inconsistency**: Stale cached nodes can cause nodes to compute incorrect state root hashes for the same version, breaking the critical invariant that "All validators must produce identical state roots for identical blocks."

**Potential Consensus Divergence**: If different validator nodes have different cached state when performing restore operations, they could diverge on state root hashes, leading to consensus failures.

**Restore Operation Corruption**: During backup/restore scenarios (critical for node operators recovering from failures), the stale caches can cause:
- Incorrect Merkle proof generation
- State root hash verification failures  
- State synchronization errors between nodes
- Need for manual intervention to recover

This is particularly severe because restore operations are used in critical recovery scenarios where correctness is paramount.

## Likelihood Explanation

**Likelihood: Medium-to-High**

The vulnerability triggers in the following realistic scenario:

1. **Common Operation**: Node operators regularly perform backup/restore operations for disaster recovery, node migrations, or testing
2. **Automatic Trigger**: The `reset_state_store()` is automatically called during transaction replay in restore operations
3. **No Special Permissions Needed**: This occurs during normal restore workflows without requiring any malicious action
4. **Persistent Impact**: Once triggered, the stale caches remain until the node is restarted or naturally evicted

The likelihood is elevated because:
- Restore operations with overwrite are a documented feature
- The `reset_state_store()` is called automatically in the restore workflow
- Node operators performing incremental restores or re-restoring to the same version would hit this
- The bug requires no attacker interaction - it's a logic error in normal operation

## Recommendation

Add cache clearing to the `StateStore::reset()` method. Specifically:

1. **Clear the version_caches**: Add a method to `VersionedNodeCache` to clear all cached versions
2. **Clear the lru_cache**: Add a method to `LruNodeCache` to clear all entries
3. **Call these during reset**: Ensure `StateMerkleDb` caches are cleared before recreating the buffered state

**Proposed Fix:**

```rust
// In storage/aptosdb/src/versioned_node_cache.rs
impl VersionedNodeCache {
    pub fn clear(&self) {
        self.inner.write().clear();
    }
}

// In storage/aptosdb/src/lru_node_cache.rs
impl LruNodeCache {
    pub fn clear(&self) {
        for shard in &self.shards {
            shard.lock().clear();
        }
    }
}

// In storage/aptosdb/src/state_merkle_db.rs
impl StateMerkleDb {
    pub(crate) fn clear_caches(&self) {
        // Clear version caches for all shards
        for cache in self.version_caches.values() {
            cache.clear();
        }
        // Clear LRU cache if enabled
        if let Some(lru_cache) = &self.lru_cache {
            lru_cache.clear();
        }
    }
}

// In storage/aptosdb/src/state_store/mod.rs
pub fn reset(&self) {
    self.buffered_state.lock().quit();
    
    // CRITICAL FIX: Clear all Merkle node caches before recreating buffered state
    self.state_db.state_merkle_db.clear_caches();
    if let Some(hot_merkle_db) = &self.state_db.hot_state_merkle_db {
        hot_merkle_db.clear_caches();
    }
    
    *self.buffered_state.lock() = Self::create_buffered_state_from_latest_snapshot(
        &self.state_db,
        self.buffered_state_target_items,
        false,
        true,
        self.current_state.clone(),
        self.persisted_state.clone(),
        self.hot_state_config,
    )
    .expect("buffered state creation failed.");
}
```

## Proof of Concept

```rust
// This test demonstrates the vulnerability
#[test]
fn test_stale_cache_after_reset() {
    use std::sync::Arc;
    use aptos_crypto::HashValue;
    use aptos_types::transaction::Version;
    
    // 1. Setup: Create AptosDB and write initial state at version 100
    let tmpdir = tempfile::tempdir().unwrap();
    let db = Arc::new(AptosDB::new_for_test(&tmpdir));
    let restore_handler = db.get_restore_handler();
    
    // Write initial tree data to version 100 with root hash H1
    let version: Version = 100;
    let initial_tree_data = create_test_tree_data_set_a();
    write_state_to_version(&db, version, &initial_tree_data);
    
    // Query version 100 - this populates the caches
    let initial_root_hash = db.state_merkle_db.get_root_hash(version).unwrap();
    
    // Verify caches are populated by checking metrics
    assert!(db.state_merkle_db.version_caches.get(&None).unwrap().get_version(version).is_some());
    
    // 2. Restore: Overwrite version 100 with completely different data (root hash H2)
    let different_tree_data = create_test_tree_data_set_b(); 
    let expected_new_root_hash = calculate_expected_root(different_tree_data);
    
    let state_restore = StateSnapshotRestore::new_overwrite(
        &db.state_merkle_db,
        &db,
        version,
        expected_new_root_hash,
        StateSnapshotRestoreMode::Default,
    ).unwrap();
    
    // Write the new data
    state_restore.add_chunk(different_tree_data, version).unwrap();
    state_restore.finish().unwrap();
    
    // 3. Reset state store (simulating what happens in transaction replay)
    restore_handler.reset_state_store();
    
    // 4. BUG: Query version 100 again - should get new root hash H2
    // but due to stale cache, may get H1 instead!
    let actual_root_hash = db.state_merkle_db.get_root_hash(version).unwrap();
    
    // This assertion may FAIL due to stale cache returning old nodes
    assert_eq!(actual_root_hash, expected_new_root_hash, 
        "Stale cache returned incorrect root hash! Expected {:?} but got {:?}", 
        expected_new_root_hash, actual_root_hash);
    
    // Reading directly from disk (without cache) shows the correct value
    let root_from_disk = read_root_directly_from_disk(&db, version);
    assert_eq!(root_from_disk, expected_new_root_hash);
    
    // This proves the cache is stale and inconsistent with disk
    assert_ne!(actual_root_hash, root_from_disk,
        "Cache returned different value than disk - STALE CACHE DETECTED");
}
```

**Notes:**
The provided PoC outline demonstrates the vulnerability by:
1. Writing initial state to a version and populating caches
2. Overwriting that version with different data using restore
3. Calling reset_state_store() which doesn't clear caches
4. Showing that cached queries return stale data while disk has correct data

This breaks the critical invariant that state queries must return the current on-disk state, not stale cached values.

### Citations

**File:** storage/aptosdb/src/backup/restore_handler.rs (L57-59)
```rust
    pub fn reset_state_store(&self) {
        self.state_store.reset();
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L707-719)
```rust
    pub fn reset(&self) {
        self.buffered_state.lock().quit();
        *self.buffered_state.lock() = Self::create_buffered_state_from_latest_snapshot(
            &self.state_db,
            self.buffered_state_target_items,
            false,
            true,
            self.current_state.clone(),
            self.persisted_state.clone(),
            self.hot_state_config,
        )
        .expect("buffered state creation failed.");
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L72-82)
```rust
pub struct StateMerkleDb {
    // Stores metadata and top levels (non-sharded part) of tree nodes.
    state_merkle_metadata_db: Arc<DB>,
    // Stores sharded part of tree nodes.
    state_merkle_db_shards: [Arc<DB>; NUM_STATE_SHARDS],
    enable_sharding: bool,
    // shard_id -> cache.
    version_caches: HashMap<Option<usize>, VersionedNodeCache>,
    // `None` means the cache is not enabled.
    lru_cache: Option<LruNodeCache>,
}
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L483-496)
```rust
        if self.cache_enabled() {
            self.version_caches
                .get(&Some(shard_id))
                .unwrap()
                .add_version(
                    version,
                    tree_update_batch
                        .node_batch
                        .iter()
                        .flatten()
                        .cloned()
                        .collect(),
                );
        }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L856-897)
```rust
    fn get_node_option(&self, node_key: &NodeKey, tag: &str) -> Result<Option<Node>> {
        let start_time = Instant::now();
        if !self.cache_enabled() {
            let node_opt = self
                .db_by_key(node_key)
                .get::<JellyfishMerkleNodeSchema>(node_key)?;
            NODE_CACHE_SECONDS
                .observe_with(&[tag, "cache_disabled"], start_time.elapsed().as_secs_f64());
            return Ok(node_opt);
        }
        if let Some(node_cache) = self
            .version_caches
            .get(&node_key.get_shard_id())
            .unwrap()
            .get_version(node_key.version())
        {
            let node = node_cache.get(node_key).cloned();
            NODE_CACHE_SECONDS.observe_with(
                &[tag, "versioned_cache_hit"],
                start_time.elapsed().as_secs_f64(),
            );
            return Ok(node);
        }

        if let Some(lru_cache) = &self.lru_cache {
            if let Some(node) = lru_cache.get(node_key) {
                NODE_CACHE_SECONDS
                    .observe_with(&[tag, "lru_cache_hit"], start_time.elapsed().as_secs_f64());
                return Ok(Some(node));
            }
        }

        let node_opt = self
            .db_by_key(node_key)
            .get::<JellyfishMerkleNodeSchema>(node_key)?;
        if let Some(lru_cache) = &self.lru_cache {
            if let Some(node) = &node_opt {
                lru_cache.put(node_key.clone(), node.clone());
            }
        }
        NODE_CACHE_SECONDS.observe_with(&[tag, "cache_miss"], start_time.elapsed().as_secs_f64());
        Ok(node_opt)
```

**File:** storage/aptosdb/src/state_restore/restore_test.rs (L220-228)
```rust
    fn test_overwrite(
        btree in arb_btree_map(1),
        target_version in 0u64..2000,
    ) {
        let restore_db = Arc::new(MockSnapshotStore::new(true /* allow_overwrite */));
        restore_without_interruption(&btree, target_version, &restore_db, true);
        // overwrite, an entirely different tree
        restore_without_interruption(&btree, target_version, &restore_db, false);
    }
```

**File:** storage/aptosdb/src/versioned_node_cache.rs (L19-41)
```rust
pub(crate) struct VersionedNodeCache {
    inner: RwLock<VecDeque<(Version, Arc<NodeCache>)>>,
}

impl fmt::Debug for VersionedNodeCache {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let entries = self.inner.read();
        writeln!(f, "Total versions: {}.", entries.len())?;
        for entry in entries.iter() {
            writeln!(f, "Version {} has {} elements.", entry.0, entry.1.len())?;
        }
        Ok(())
    }
}

impl VersionedNodeCache {
    pub(crate) const NUM_VERSIONS_TO_CACHE: usize = 2;

    pub fn new() -> Self {
        Self {
            inner: RwLock::new(Default::default()),
        }
    }
```

**File:** storage/aptosdb/src/lru_node_cache.rs (L13-29)
```rust
pub(crate) struct LruNodeCache {
    shards: [Mutex<LruCache<NibblePath, (Version, Node)>>; NUM_SHARDS],
}

impl fmt::Debug for LruNodeCache {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        writeln!(f, "LruCache with {NUM_SHARDS} shards.")
    }
}

impl LruNodeCache {
    pub fn new(max_nodes_per_shard: NonZeroUsize) -> Self {
        Self {
            // `arr!()` doesn't allow a const in place of the integer literal
            shards: arr_macro::arr![Mutex::new(LruCache::new(max_nodes_per_shard)); 256],
        }
    }
```
