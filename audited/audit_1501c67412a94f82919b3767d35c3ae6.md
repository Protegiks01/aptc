# Audit Report

## Title
Silent Batch Loss in Quorum Store Due to Unhandled Persistence Failures in Batch Generator

## Summary
The `batch_generator.rs` file at line 491 calls `batch_writer.persist()` but completely ignores its return value, which indicates whether batches were successfully persisted. While the question specifically asks about "persistent disk failures," disk write failures actually cause validator panics (not silent loss) due to `.expect()` calls. However, **quota exhaustion failures ARE silent** and can cause network-wide batch loss, breaking the critical data availability invariant. [1](#0-0) 

## Finding Description

The vulnerability exists in the batch generation flow:

**Step 1: Ignored Persistence Return Value**
When a validator creates new batches from mempool transactions, it calls `persist()` but discards the return value indicating success/failure. [2](#0-1) 

**Step 2: Unconditional Broadcast**
Regardless of persistence success, batches are broadcast to all validators. [3](#0-2) 

**Step 3: Silent Failure in Quota Exhaustion**
The `persist()` method calls `persist_inner()` which calls `save()`. When storage quota is exceeded, `save()` returns `Err`, causing `persist_inner()` to return `None` with only a debug log. [4](#0-3) 

**Step 4: Quota Management Per Author**
Each validator maintains separate quota limits per batch author. If an aggressive validator creates many batches, it can exceed quota limits on multiple validators simultaneously. [5](#0-4) 

**Attack Scenario:**
1. Validator A creates large batches rapidly, exceeding its quota allocation on multiple validators
2. Validator A's `persist()` fails silently (quota exceeded), returns empty vector
3. Validator A broadcasts batches to all validators anyway
4. Other validators receive batches but also fail to persist (quota exceeded for author A)
5. ALL validators have now failed to persist these batches
6. Batches are completely lost from the network
7. If these batches are referenced in a ProofOfStore and included in blocks, execution will fail when trying to retrieve them

**Clarification on Disk Failures:**
The question asks about "persistent disk failures." However, disk write failures trigger `.expect()` panics that crash the validator, which is NOT silent. [6](#0-5) 

The ACTUAL silent failure occurs with quota exhaustion, not disk failures.

## Impact Explanation

**Severity: CRITICAL** (up to $1,000,000 per Aptos Bug Bounty)

This vulnerability breaks the **Data Availability** and **Liveness** invariants:

1. **Total Loss of Liveness**: If batches referenced in blocks cannot be retrieved, block execution stalls completely. The network cannot make progress and requires manual intervention or a hard fork to recover.

2. **Network Partition Risk**: Different validators may have different subsets of batches depending on when they hit quota limits. This creates inconsistency in available data across the network.

3. **Consensus Violation**: Validators may sign batches (creating ProofOfStore) without having the actual transaction data, violating the guarantee that 2f+1 validators attest to batch availability.

The impact qualifies as "Total loss of liveness/network availability" under Critical Severity criteria.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

The attack requires:
- A validator creating many large batches rapidly (easy for any validator)
- Block proposal/execution being slower than batch creation (can occur under load)
- Quota limits being reached on multiple validators (realistic with default configurations)

No special privileges or Byzantine behavior are required. A single honest-but-aggressive validator under heavy load can trigger this condition. The issue becomes more likely as network load increases.

The quota limits are designed to prevent resource exhaustion, but the lack of error handling means the protection mechanism itself causes data loss rather than graceful degradation.

## Recommendation

**Immediate Fix:**
Add proper error handling at line 491 to check the persist return value and only broadcast batches that were successfully persisted:

```rust
let signed_batch_infos = self.batch_writer.persist(persist_requests);
let successfully_persisted_batches: Vec<_> = batches
    .into_iter()
    .zip(signed_batch_infos.into_iter())
    .filter_map(|(batch, signed_info_opt)| {
        signed_info_opt.map(|_| batch)
    })
    .collect();

if !successfully_persisted_batches.is_empty() {
    counters::BATCH_CREATION_PERSIST_LATENCY.observe_duration(persist_start.elapsed());
    
    if self.config.enable_batch_v2 {
        network_sender.broadcast_batch_msg_v2(successfully_persisted_batches).await;
    } else {
        let batches = successfully_persisted_batches.into_iter().map(|batch| {
            batch.try_into().expect("Cannot send V2 batch with flag disabled")
        }).collect();
        network_sender.broadcast_batch_msg(batches).await;
    }
} else {
    warn!("Failed to persist all batches, quota may be exceeded");
    counters::BATCH_PERSIST_FAILED_COUNT.inc();
}
```

**Additional Recommendations:**
1. Change panic on disk failures to graceful degradation with alerts
2. Implement backpressure when approaching quota limits
3. Add monitoring for persist failure rates
4. Consider dynamic quota adjustment based on network conditions

## Proof of Concept

```rust
// Proof of Concept: Quota Exhaustion Causing Silent Batch Loss
// This demonstrates how quota limits can cause silent failures

#[cfg(test)]
mod quota_exhaustion_test {
    use super::*;
    
    #[tokio::test]
    async fn test_silent_batch_loss_on_quota_exhaustion() {
        // Setup: Create batch generator with low quota
        let small_quota = 1000; // Small quota for testing
        let batch_store = Arc::new(BatchStore::new(
            epoch,
            true,
            0,
            db,
            small_quota, // memory quota
            small_quota, // db quota  
            2,           // batch quota
            validator_signer,
            60_000_000,
        ));
        
        // Step 1: Fill up quota with existing batches
        // (Create batches until quota is nearly exhausted)
        
        // Step 2: Attempt to persist new large batches
        let large_txns = create_large_transactions(1000);
        let batch = create_batch(large_txns);
        let persist_requests = vec![batch.into()];
        
        // Step 3: Call persist - will silently fail with quota exceeded
        let result = batch_store.persist(persist_requests);
        
        // VULNERABILITY: result is empty but no error is propagated
        assert!(result.is_empty(), "Persist failed silently");
        
        // Step 4: Batch would still be broadcast despite not being persisted
        // This breaks data availability invariant
        
        // Step 5: Later when execution needs this batch, it will fail
        // because no validator has it persisted
        let batch_digest = batch.digest();
        let local_result = batch_store.get_batch_from_local(&batch_digest);
        assert!(local_result.is_err(), "Batch not available locally");
        
        // If all validators hit quota, batch is completely lost!
    }
}
```

## Notes

While the security question specifically asks about "persistent disk failures," the actual vulnerability is with **quota exhaustion causing silent failures**, not disk failures. Disk failures cause validator panics through `.expect()` calls, which are NOT silent. However, the ignored return value at line 491 enables a CRITICAL vulnerability where quota exhaustion can cause network-wide batch loss without any validator being aware. This is arguably more dangerous than disk failures since it's silent and doesn't trigger alerts or recovery mechanisms.

The comparison with `batch_coordinator.rs` is instructive: when receiving batches from the network, the coordinator DOES check the persist return value and only sends SignedBatchInfo on success. [7](#0-6) 

This inconsistency in error handling between batch creation and batch reception is the root cause of the vulnerability.

### Citations

**File:** consensus/src/quorum_store/batch_generator.rs (L486-492)
```rust
                            let persist_start = Instant::now();
                            let mut persist_requests = vec![];
                            for batch in batches.clone().into_iter() {
                                persist_requests.push(batch.into());
                            }
                            self.batch_writer.persist(persist_requests);
                            counters::BATCH_CREATION_PERSIST_LATENCY.observe_duration(persist_start.elapsed());
```

**File:** consensus/src/quorum_store/batch_generator.rs (L494-501)
```rust
                            if self.config.enable_batch_v2 {
                                network_sender.broadcast_batch_msg_v2(batches).await;
                            } else {
                                let batches = batches.into_iter().map(|batch| {
                                    batch.try_into().expect("Cannot send V2 batch with flag disabled")
                                }).collect();
                                network_sender.broadcast_batch_msg(batches).await;
                            }
```

**File:** consensus/src/quorum_store/batch_store.rs (L64-84)
```rust
    pub(crate) fn update_quota(&mut self, num_bytes: usize) -> anyhow::Result<StorageMode> {
        if self.batch_balance == 0 {
            counters::EXCEEDED_BATCH_QUOTA_COUNT.inc();
            bail!("Batch quota exceeded ");
        }

        if self.db_balance >= num_bytes {
            self.batch_balance -= 1;
            self.db_balance -= num_bytes;

            if self.memory_balance >= num_bytes {
                self.memory_balance -= num_bytes;
                Ok(StorageMode::MemoryAndPersisted)
            } else {
                Ok(StorageMode::PersistedOnly)
            }
        } else {
            counters::EXCEEDED_STORAGE_QUOTA_COUNT.inc();
            bail!("Storage quota exceeded ");
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L505-512)
```rust
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
```

**File:** consensus/src/quorum_store/batch_store.rs (L523-527)
```rust
            Err(e) => {
                debug!("QS: failed to store to cache {:?}", e);
                None
            },
        }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L103-111)
```rust
                let signed_batch_infos = batch_store.persist(persist_requests);
                if !signed_batch_infos.is_empty() {
                    if approx_created_ts_usecs > 0 {
                        observe_batch(approx_created_ts_usecs, peer_id, BatchStage::SIGNED);
                    }
                    network_sender
                        .send_signed_batch_info_msg_v2(signed_batch_infos, vec![peer_id])
                        .await;
                }
```
