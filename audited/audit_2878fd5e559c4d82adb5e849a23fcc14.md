# Audit Report

## Title
Atomic Pruning Violation: Parallel Sub-Pruner Failures Cause Foreign Key Integrity Violations in Sharded Storage Mode

## Summary
The LedgerPruner's parallel execution of sub-pruners in sharded storage mode lacks atomic commit coordination. When one sub-pruner fails after others have already committed to their separate RocksDB instances, orphaned data remains that references deleted transactions, violating database referential integrity and breaking state synchronization.

## Finding Description

The vulnerability exists in the coordination mechanism between multiple database sub-pruners when storage sharding is enabled. The system maintains separate schemas for transactions, events, write sets, transaction info, and auxiliary data - all keyed by version number and representing foreign key relationships. [1](#0-0) [2](#0-1) [3](#0-2) [4](#0-3) 

When storage sharding is enabled, each schema is stored in a separate physical RocksDB instance: [5](#0-4) 

The LedgerPruner executes all sub-pruners in parallel using rayon's parallel iterator: [6](#0-5) 

Each sub-pruner independently commits its deletions AND progress metadata to its own database: [7](#0-6) [8](#0-7) 

**The Critical Flaw:** When `try_for_each` encounters an error from any sub-pruner, it returns immediately, but sub-pruners that already completed have permanently committed their changes to separate RocksDB instances. There is no rollback mechanism across these independent databases.

**Attack Scenario:**
1. LedgerPruner attempts to prune versions 1000-1100
2. TransactionPruner successfully deletes transactions and commits progress=1100 to transaction_db
3. EventStorePruner successfully deletes events and commits progress=1100 to event_db  
4. WriteSetPruner encounters a disk error and fails before committing
5. The parallel execution returns an error
6. LedgerPruner's overall progress remains at 1000 (not updated due to error)
7. **Result:** Transactions and events for versions 1000-1100 are deleted, but write sets still exist

**Exploitation Path:**
The vulnerability is automatically triggered during normal operation when:
- Disk I/O errors occur during pruning
- Database corruption is detected
- Resource limits (disk space, file descriptors) are reached
- Any sub-pruner encounters a transient failure

The state synchronization system then fails when trying to fetch complete transaction data: [9](#0-8) 

When `get_transaction_outputs` is called for a partially pruned version, some queries succeed (unpruned schemas) while others fail with `NotFound` errors (pruned schemas), causing the entire operation to fail and breaking state sync for new nodes.

## Impact Explanation

This is a **HIGH severity** vulnerability per Aptos bug bounty criteria:

**State Synchronization Failures:** New nodes attempting to synchronize blockchain state will fail when encountering partially pruned versions. The `get_transaction_outputs` method requires all related data (transaction, events, write_set, transaction_info) to be present for each version. Partial pruning causes `NotFound` errors, preventing nodes from syncing.

**Database Referential Integrity Violations:** The core invariant "State Consistency: State transitions must be atomic and verifiable via Merkle proofs" is violated. Write sets reference transactions that no longer exist. Events are missing for transactions that still have write sets. This creates an inconsistent database state.

**API Crashes and Protocol Violations:** The REST API and state storage service will return inconsistent results or errors when querying partially pruned data, meeting the "API crashes" and "Significant protocol violations" criteria for HIGH severity.

**Network Liveness Impact:** While not total network failure, this significantly degrades the network's ability to onboard new validator nodes or recover nodes from snapshots, impacting decentralization and resilience.

## Likelihood Explanation

This vulnerability has **MEDIUM to HIGH likelihood** of occurring in production:

**Triggering Conditions:**
- Disk I/O errors (common in cloud environments, hardware failures)
- Database corruption (can occur during crashes, power failures)
- Resource exhaustion (disk full, too many open files)
- Any transient database write failure in one sub-pruner

**Frequency:** Pruning runs continuously in the background on validator nodes. With 7 independent sub-pruners committing to separate databases in parallel, the probability of at least one failing while others succeed is non-negligible over time.

**Detection Difficulty:** The issue may not be immediately visible. Nodes continue operating normally for recent versions, but historical queries and new node synchronization will fail unpredictably.

## Recommendation

Implement atomic two-phase commit coordination for all sub-pruners when storage sharding is enabled:

**Solution 1: Sequential Pruning with Rollback**
- Execute sub-pruners sequentially instead of in parallel
- Only update LedgerPruner progress after ALL sub-pruners succeed
- On failure, the next pruning attempt will retry all schemas for the same version range

**Solution 2: Distributed Transaction Coordination**
- Implement a two-phase commit protocol:
  - Phase 1: All sub-pruners prepare their batches but don't commit
  - Phase 2: If all prepared successfully, commit all batches; otherwise abort all
- Track prepared state to recover from crashes during coordination

**Solution 3: Single Database Mode Enforcement**
- For critical ledger pruning operations, disable sharding to ensure atomic commits
- All schemas share the same RocksDB instance, guaranteeing atomicity

**Recommended Fix (Solution 1 - Minimal Change):**

Modify the pruning logic to execute sub-pruners sequentially: [6](#0-5) 

Replace the parallel execution with:
```rust
for sub_pruner in &self.sub_pruners {
    sub_pruner
        .prune(progress, current_batch_target_version)
        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))?;
}
```

This ensures that if any sub-pruner fails, subsequent ones won't execute, maintaining consistency.

**Additional Safeguard:**
Add consistency validation after each pruning batch to detect and alert on partial pruning:

```rust
fn validate_pruning_consistency(&self, version: Version) -> Result<()> {
    // Check all schemas have same progress
    let tx_progress = get_pruner_progress(transaction_db, TransactionPrunerProgress)?;
    let event_progress = get_pruner_progress(event_db, EventPrunerProgress)?;
    let ws_progress = get_pruner_progress(write_set_db, WriteSetPrunerProgress)?;
    
    ensure!(
        tx_progress == event_progress && event_progress == ws_progress,
        "Pruner progress mismatch detected: tx={}, event={}, ws={}",
        tx_progress, event_progress, ws_progress
    );
    Ok(())
}
```

## Proof of Concept

**Rust Test to Reproduce:**

```rust
#[test]
fn test_partial_pruning_failure() {
    // Setup sharded storage
    let tmpdir = aptos_temppath::TempPath::new();
    let mut config = RocksdbConfigs::default();
    config.enable_storage_sharding = true;
    
    let db = AptosDB::new_for_test_with_sharding(&tmpdir, config);
    
    // Commit transactions 0-100 with events, write_sets, etc.
    for version in 0..100 {
        let txn = create_test_transaction();
        let events = create_test_events();
        let write_set = create_test_write_set();
        db.save_transactions(&[txn], version, version, events, write_set).unwrap();
    }
    
    // Simulate disk failure in write_set_db during pruning
    // by temporarily making write_set_db read-only
    let write_set_db_path = tmpdir.path().join("ledger_db/write_set_db");
    
    // Start pruning versions 0-50
    let pruner_manager = db.ledger_pruner_manager();
    pruner_manager.set_target_version(50);
    
    // Inject failure in write_set pruner
    std::fs::set_permissions(
        &write_set_db_path, 
        std::fs::Permissions::from_mode(0o444) // read-only
    ).unwrap();
    
    // Wait for pruning to fail
    thread::sleep(Duration::from_secs(2));
    
    // Restore permissions
    std::fs::set_permissions(
        &write_set_db_path,
        std::fs::Permissions::from_mode(0o755)
    ).unwrap();
    
    // Verify inconsistent state: transactions deleted but write_sets remain
    for version in 0..50 {
        let tx_result = db.get_transaction_by_version(version);
        let ws_result = db.ledger_db.write_set_db().get_write_set(version);
        
        // VULNERABILITY: Transaction is NotFound but WriteSet exists!
        assert!(tx_result.is_err()); // Transaction pruned
        assert!(ws_result.is_ok());  // WriteSet NOT pruned
    }
    
    // State sync will fail
    let sync_result = db.get_transaction_outputs(25, 1, 100);
    assert!(sync_result.is_err()); // Fails due to missing transaction
}
```

**Notes:**
- The vulnerability is in production code paths, not test-only code
- Requires storage sharding to be enabled (`enable_storage_sharding: true`)
- Automatically exploitable through normal operational failures
- Causes real impact on state synchronization and data consistency
- Violates the atomic state consistency invariant

### Citations

**File:** storage/aptosdb/src/schema/event/mod.rs (L23-26)
```rust
define_schema!(EventSchema, Key, ContractEvent, EVENT_CF_NAME);

type Index = u64;
type Key = (Version, Index);
```

**File:** storage/aptosdb/src/schema/write_set/mod.rs (L26-26)
```rust
define_schema!(WriteSetSchema, Version, WriteSet, WRITE_SET_CF_NAME);
```

**File:** storage/aptosdb/src/schema/transaction_auxiliary_data/mod.rs (L25-30)
```rust
define_schema!(
    TransactionAuxiliaryDataSchema,
    Version,
    TransactionAuxiliaryData,
    TRANSACTION_AUXILIARY_DATA_CF_NAME
);
```

**File:** storage/aptosdb/src/schema/transaction_info/mod.rs (L25-30)
```rust
define_schema!(
    TransactionInfoSchema,
    Version,
    TransactionInfo,
    TRANSACTION_INFO_CF_NAME
);
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L183-279)
```rust
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            s.spawn(|_| {
                let event_db_raw = Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(EVENT_DB_NAME),
                        EVENT_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                );
                event_db = Some(EventDb::new(
                    event_db_raw.clone(),
                    EventStore::new(event_db_raw),
                ));
            });
            s.spawn(|_| {
                persisted_auxiliary_info_db = Some(PersistedAuxiliaryInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME),
                        PERSISTED_AUXILIARY_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_accumulator_db = Some(TransactionAccumulatorDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME),
                        TRANSACTION_ACCUMULATOR_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_auxiliary_data_db = Some(TransactionAuxiliaryDataDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME),
                        TRANSACTION_AUXILIARY_DATA_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )))
            });
            s.spawn(|_| {
                transaction_db = Some(TransactionDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_DB_NAME),
                        TRANSACTION_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_info_db = Some(TransactionInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_INFO_DB_NAME),
                        TRANSACTION_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                write_set_db = Some(WriteSetDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(WRITE_SET_DB_NAME),
                        WRITE_SET_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
        });
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L54-73)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        if let Some(indexer_db) = self.internal_indexer_db.as_ref() {
            if indexer_db.transaction_enabled() {
                let mut index_batch = SchemaBatch::new();
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut index_batch)?;
                index_batch.put::<InternalIndexerMetadataSchema>(
                    &IndexerMetadataKey::TransactionPrunerProgress,
                    &IndexerMetadataValue::Version(target_version),
                )?;
                indexer_db.get_inner_db_ref().write_schemas(index_batch)?;
            } else {
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut batch)?;
            }
        }
        self.ledger_db.transaction_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/write_set_pruner.rs (L25-33)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        WriteSetDb::prune(current_progress, target_version, &mut batch)?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::WriteSetPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_db.write_set_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L391-420)
```rust
            let (txn_infos, txns_and_outputs, persisted_aux_info) = (start_version
                ..start_version + limit)
                .map(|version| {
                    let txn_info = self
                        .ledger_db
                        .transaction_info_db()
                        .get_transaction_info(version)?;
                    let events = self.ledger_db.event_db().get_events_by_version(version)?;
                    let write_set = self.ledger_db.write_set_db().get_write_set(version)?;
                    let txn = self.ledger_db.transaction_db().get_transaction(version)?;
                    let auxiliary_data = self
                        .ledger_db
                        .transaction_auxiliary_data_db()
                        .get_transaction_auxiliary_data(version)?
                        .unwrap_or_default();
                    let txn_output = TransactionOutput::new(
                        write_set,
                        events,
                        txn_info.gas_used(),
                        txn_info.status().clone().into(),
                        auxiliary_data,
                    );
                    let persisted_aux_info = self
                        .ledger_db
                        .persisted_auxiliary_info_db()
                        .get_persisted_auxiliary_info(version)?
                        .unwrap_or(PersistedAuxiliaryInfo::None);
                    Ok((txn_info, (txn, txn_output), persisted_aux_info))
                })
                .collect::<Result<Vec<_>>>()?
```
