# Audit Report

## Title
Lack of File-Level Locking in Indexer Table Info Backup Operations Allows Concurrent Access Corruption

## Summary
The `fs_ops.rs` file in the indexer-grpc-table-info service lacks any file-level or directory-level locking mechanisms to prevent concurrent filesystem operations on the same database paths. If multiple fullnode instances operate on the same data directory, concurrent backup or restore operations can corrupt shared state through race conditions in file writes and directory manipulations.

## Finding Description

The indexer-grpc-table-info service provides backup and restore functionality for the table info database through filesystem operations in `fs_ops.rs`. This file contains three critical functions that perform filesystem operations without any locking: [1](#0-0) [2](#0-1) 

**Critical Issue 1: Concurrent Backup Operations**

The `create_tar_gz` function builds a compressed archive in memory and writes it to disk with no atomic write protection. If two processes call this function with identical parameters, both will write to the same file path simultaneously, potentially corrupting the backup file. [3](#0-2) 

**Critical Issue 2: Non-Atomic Restore Operations**

The `unpack_tar_gz` function performs a multi-step restore process where all concurrent operations use the same temporary directory path. The sequence of `remove_dir_all` followed by `rename` is not atomic, creating a window for race conditions. [4](#0-3) 

**Developer Acknowledgment**

The codebase contains an explicit TODO comment acknowledging this concurrency issue: [5](#0-4) 

**Attack Scenario:**

1. Operator misconfigures deployment and starts two fullnode instances pointing to the same data directory
2. Both instances run in Backup mode with the indexer table info service enabled
3. At epoch boundary, both instances create snapshots with identical names
4. Both instances concurrently call `backup_the_snapshot_and_cleanup` which invokes `create_tar_gz`
5. Both processes write compressed data to the same tar.gz file path simultaneously
6. File writes interleave, producing a corrupted backup archive
7. The corrupted backup is uploaded to GCS
8. Future restore operations fail or restore corrupted data

## Impact Explanation

This issue falls into the **Medium Severity** category per the Aptos bug bounty program criteria: "State inconsistencies requiring intervention."

While the indexer-grpc-table-info service is auxiliary infrastructure (not part of core consensus), backup corruption can lead to:

- **Data Recovery Failure**: Operators relying on backups for disaster recovery will find backups corrupted and unusable
- **Service Degradation**: Failed restore operations require manual intervention and extended downtime
- **Operational Risk**: Silent corruption may go undetected until restoration is attempted during an emergency

However, this does NOT affect:
- Consensus safety or blockchain integrity
- Validator operations or staking
- Fund security or transaction processing  
- Core fullnode availability

The impact is limited because the table info indexer is used for metadata queries, not critical blockchain operations.

## Likelihood Explanation

**Likelihood: Low to Medium**

The vulnerability requires specific conditions:
1. Multiple fullnode instances configured with the same data directory (operator error or malicious configuration)
2. Both instances running in Backup mode simultaneously
3. Concurrent access to the same snapshot at the same time

This is more likely in scenarios such as:
- Orchestration system bugs causing duplicate deployments
- Manual operator errors during node management
- Intentional attack by someone with deployment access

It is **NOT** exploitable by:
- External network peers
- Transaction senders
- Smart contract deployers
- Anyone without server/deployment access

## Recommendation

Implement file-level locking using platform-appropriate mechanisms:

**Option 1: Advisory File Locking**
```rust
use fs2::FileExt;
use std::fs::OpenOptions;

pub fn create_tar_gz_with_lock(dir_path: PathBuf, backup_file_name: &str) -> Result<PathBuf, anyhow::Error> {
    let tar_file_name = format!("{}.tar.gz", backup_file_name);
    let tar_file_path = dir_path.join(&tar_file_name);
    let lock_file_path = dir_path.join(format!("{}.lock", backup_file_name));
    
    // Acquire exclusive lock
    let lock_file = OpenOptions::new()
        .create(true)
        .write(true)
        .open(&lock_file_path)?;
    lock_file.lock_exclusive()?;
    
    // Perform backup operations...
    let result = create_tar_gz_internal(dir_path, backup_file_name);
    
    // Release lock (automatic on drop)
    drop(lock_file);
    std::fs::remove_file(&lock_file_path).ok();
    
    result
}
```

**Option 2: Process-Level Lock File**
Create a PID file at service startup to prevent multiple instances:
```rust
pub fn acquire_service_lock(data_dir: &Path) -> Result<File, anyhow::Error> {
    let lock_path = data_dir.join("table_info_service.lock");
    let lock_file = OpenOptions::new()
        .create(true)
        .write(true)
        .open(&lock_path)?;
    
    lock_file.try_lock_exclusive()
        .context("Another instance is already running")?;
    
    // Write PID for debugging
    write!(lock_file, "{}", std::process::id())?;
    
    Ok(lock_file)
}
```

**Option 3: Atomic Operations**
Use atomic write patterns with temporary files and atomic rename:
```rust
pub fn atomic_write(path: &Path, data: &[u8]) -> Result<()> {
    let temp_path = path.with_extension("tmp");
    std::fs::write(&temp_path, data)?;
    std::fs::rename(&temp_path, path)?; // Atomic on same filesystem
    Ok(())
}
```

## Proof of Concept

```rust
// Proof of concept demonstrating concurrent write corruption
use std::fs;
use std::path::PathBuf;
use std::thread;
use std::time::Duration;
use tempfile::tempdir;

#[test]
fn test_concurrent_backup_corruption() {
    let temp_dir = tempdir().unwrap();
    let backup_path = temp_dir.path().join("test_backup.tar.gz");
    
    // Spawn two threads that write to the same file concurrently
    let path1 = backup_path.clone();
    let path2 = backup_path.clone();
    
    let handle1 = thread::spawn(move || {
        let data = vec![0xAA; 1000000]; // 1MB of 0xAA
        fs::write(&path1, data).unwrap();
    });
    
    let handle2 = thread::spawn(move || {
        thread::sleep(Duration::from_millis(5)); // Slight delay to maximize race
        let data = vec![0xBB; 1000000]; // 1MB of 0xBB
        fs::write(&path2, data).unwrap();
    });
    
    handle1.join().unwrap();
    handle2.join().unwrap();
    
    // Read the resulting file - it should be either all 0xAA or all 0xBB
    // But due to race conditions, it may contain mixed data or wrong length
    let result = fs::read(&backup_path).unwrap();
    
    // Check if corruption occurred (contains mixed bytes)
    let all_aa = result.iter().all(|&b| b == 0xAA);
    let all_bb = result.iter().all(|&b| b == 0xBB);
    
    // In a properly locked system, this should always be true
    // Without locking, this assertion may fail
    assert!(all_aa || all_bb, "File corruption detected: mixed content");
    assert_eq!(result.len(), 1000000, "File corruption detected: wrong length");
}
```

## Notes

**Important Clarifications:**

1. **Service Scope**: This vulnerability affects the indexer-grpc-table-info service, which is auxiliary infrastructure for table metadata indexing, NOT part of the core consensus or blockchain state machine.

2. **Current Production Status**: Investigation reveals that the restore functionality (`restore_db_snapshot`) is defined but never actually called in production code. The Restore mode is configured but not implemented in the service bootstrap logic: [6](#0-5) 

Only Backup mode is implemented, so the `unpack_tar_gz` race condition is theoretical but not currently active.

3. **RocksDB Protection**: While RocksDB provides database-level locking when opened, this protection only applies AFTER the database is opened. The filesystem operations (create, remove, rename directories) happen before RocksDB locking takes effect, leaving a window for corruption.

4. **Exploitation Requirements**: This is NOT a remote exploit. It requires either:
   - Server access to start multiple instances
   - Deployment configuration control
   - Operator error in infrastructure setup

5. **Severity Justification**: While the bug is real and acknowledged by developers (TODO comment), it does NOT meet the **strict validation criteria** for Critical or High severity because:
   - Not exploitable by unprivileged external attackers
   - Does not affect consensus, validator operations, or fund security
   - Limited to auxiliary service availability and backup integrity
   - Requires operator-level access or misconfiguration

Given these limitations, this issue is borderline for bug bounty eligibility under current criteria.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/fs_ops.rs (L56-98)
```rust
pub fn create_tar_gz(dir_path: PathBuf, backup_file_name: &str) -> Result<PathBuf, anyhow::Error> {
    // Create a buffer to write the tar.gz archive.
    let gz_encoder = GzEncoder::new(Vec::new(), Compression::fast());
    let tar_data = BufWriter::new(gz_encoder);
    let mut tar_builder = Builder::new(tar_data);
    aptos_logger::info!(
        dir_path = dir_path.to_str(),
        backup_file_name = backup_file_name,
        "[Table Info] Creating a tar.gz archive from the db snapshot directory"
    );
    tar_builder
        .append_dir_all(".", &dir_path)
        .context("Tar building failed.")?;
    aptos_logger::info!("[Table Info] Directory contents appended to the tar.gz archive");
    // Finish writing the tar archive and get the compressed GzEncoder back
    let tar_data = tar_builder
        .into_inner()
        .context("Unwrap the tar builder failed.")?;
    let gz_encoder = tar_data
        .into_inner()
        .context("Failed to get the compressed buffer.")?;

    // Finish the compression process
    let compressed_data = gz_encoder
        .finish()
        .context("Failed to build the compressed bytes.")?;

    let tar_file_name = format!("{}.tar.gz", backup_file_name);
    let tar_file_path = dir_path.join(&tar_file_name);
    aptos_logger::info!(
        dir_path = dir_path.to_str(),
        backup_file_name = backup_file_name,
        tar_file_path = tar_file_path.to_str(),
        tar_file_name = tar_file_name,
        "[Table Info] Prepare to compress the db snapshot directory"
    );
    // Write the tar.gz archive to a file
    std::fs::write(&tar_file_path, compressed_data)
        .context("Failed to write the compressed data.")?;
    aptos_logger::info!("[Table Info] Tar.gz archive created successfully");

    Ok(tar_file_path)
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/fs_ops.rs (L101-113)
```rust
pub fn unpack_tar_gz(temp_file_path: &PathBuf, target_db_path: &PathBuf) -> anyhow::Result<()> {
    let temp_dir_path = target_db_path.with_extension("tmp");
    fs::create_dir(&temp_dir_path)?;

    let file = File::open(temp_file_path)?;
    let gz_decoder = GzDecoder::new(file);
    let mut archive = Archive::new(gz_decoder);
    archive.unpack(&temp_dir_path)?;

    fs::remove_dir_all(target_db_path).unwrap_or(());
    fs::rename(&temp_dir_path, target_db_path)?; // Atomically replace the directory
    Ok(())
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L599-599)
```rust
    // TODO: add checks to handle concurrent backup jobs.
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/runtime.rs (L93-98)
```rust
        let backup_restore_operator = match node_config.indexer_table_info.table_info_service_mode {
            TableInfoServiceMode::Backup(gcs_bucket_name) => Some(Arc::new(
                GcsBackupRestoreOperator::new(gcs_bucket_name).await,
            )),
            _ => None,
        };
```
