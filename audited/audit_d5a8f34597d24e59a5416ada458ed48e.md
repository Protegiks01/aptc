# Audit Report

## Title
LRU Cache Eviction Allows Rate Limit Bypass in Faucet Memory Rate Limiter

## Summary
The `MemoryRatelimitChecker` in the Aptos faucet uses a bounded LRU cache to track per-IP request counts. When the cache reaches capacity (default 1,000,000 entries), the least recently used entries are evicted, permanently losing their request count state. An attacker can exploit this by deliberately triggering cache eviction to reset their rate limit counter, enabling unlimited faucet fund drainage from a fixed set of IP addresses.

## Finding Description

The `MemoryRatelimitChecker` implements rate limiting using an in-memory LRU cache with a configurable maximum capacity [1](#0-0) . The default capacity is set to 1,000,000 entries [2](#0-1) .

The vulnerability exists in the `check()` function where `get_or_insert_mut()` is called on the LRU cache [3](#0-2) . When an IP address is not present in the cache (either because it's new OR because it was previously evicted), the function inserts it with an initial count of 1. This means evicted IPs have their request counters completely reset.

**Attack Path:**
1. Attacker makes requests from target IP address(es) up to near the daily limit (e.g., 9 out of 10 allowed requests)
2. Attacker generates requests from 1,000,000+ unique IP addresses to fill the LRU cache to capacity
3. As new IPs are added beyond capacity, the LRU eviction policy removes the least recently used entries, including the attacker's target IP(s)
4. Attacker returns to their target IP address(es) - since the IP was evicted, `get_or_insert_mut()` treats it as a new IP with counter = 1
5. Attacker can now make another full cycle of requests from the same IP
6. Repeat steps 2-5 indefinitely to bypass rate limits

The attacker gains the ability to make unlimited requests from a small set of controlled IPs by cycling them through cache eviction, despite the per-IP daily rate limit.

**Technical Details:**
- The LRU cache stores IP-to-count mappings [4](#0-3) 
- When capacity is exceeded, the standard `lru::LruCache` implementation automatically evicts the least recently used entry
- The daily reset mechanism clears the entire cache [5](#0-4) , but this doesn't prevent intra-day eviction-based bypasses

## Impact Explanation

This vulnerability allows **Limited funds loss** qualifying for **Medium Severity** ($10,000) under the Aptos bug bounty program.

**Direct Impact:**
- Attackers can drain faucet funds beyond intended rate limits by repeatedly cycling IP addresses through cache eviction
- The faucet's primary security control (per-IP rate limiting) becomes bypassable with sufficient IP addresses
- Legitimate users may experience degraded service if attackers fill the cache with junk entries

**Funds Loss Calculation:**
If the faucet allows 10 requests per IP per day at 1 APT per request, and an attacker controls 100 target IPs:
- Normal limit: 100 IPs × 10 requests × 1 APT = 1,000 APT per day
- With eviction bypass: 100 IPs × unlimited cycles × 10 requests × 1 APT = unlimited APT drainage (constrained only by total faucet balance and attacker's ability to generate eviction traffic)

**Severity Justification:**
While this is not "Loss of Funds" requiring validator compromise or consensus manipulation, it enables **limited funds loss through rate limit bypass**, allowing attackers to extract more funds than intended from the faucet system. This meets the Medium severity criteria of "Limited funds loss or manipulation."

## Likelihood Explanation

**Likelihood: HIGH**

**Attacker Requirements:**
1. Access to 1,000,000+ unique IP addresses to trigger eviction
2. Ability to make HTTP requests to the faucet endpoint
3. No authentication or privileged access required

**Feasibility:**
- **IPv6 Addresses**: Each /64 IPv6 subnet provides 2^64 addresses; a single cloud server can easily generate millions of unique source IPs
- **Cloud Providers**: AWS, GCP, and Azure allow programmatic IP rotation and elastic IP allocation
- **Botnets**: Existing botnets commonly have millions of compromised devices with unique IPs
- **Tor Network**: While Tor exit nodes are limited, they can be combined with IPv6 for amplification
- **VPN Services**: Commercial VPN providers offer thousands of exit IPs

**Attack Complexity: LOW**
- Simple HTTP requests - no cryptographic operations required
- No need to compromise validators or understand consensus
- Attack can be automated with basic scripts
- No race conditions or timing-sensitive operations

**Natural Occurrence:**
Even without malicious intent, a high-traffic public faucet serving >1M unique IPs per day would naturally experience cache evictions, making rate limiting ineffective for all users.

## Recommendation

**Immediate Fix - Add Eviction Penalty:**
Modify the LRU cache eviction behavior to treat evicted IPs as having exhausted their rate limit, not as fresh IPs:

```rust
pub struct MemoryRatelimitChecker {
    pub max_requests_per_day: u32,
    pub ip_to_requests_today: Mutex<LruCache<IpAddr, u32>>,
    
    // Add: Track recently evicted IPs that should be rate limited
    pub evicted_ips: Mutex<HashSet<IpAddr>>,
    
    pub current_day: AtomicU64,
}
```

In the `check()` function:
```rust
// Before get_or_insert_mut, check if IP was recently evicted
let evicted_ips = self.evicted_ips.lock().await;
if evicted_ips.contains(&data.source_ip) {
    return Ok(vec![RejectionReason::new(
        format!("IP {} has exhausted daily limit", data.source_ip),
        RejectionReasonCode::UsageLimitExhausted,
    )]);
}
drop(evicted_ips);
```

Implement custom eviction callback to track evicted entries.

**Better Long-term Solution:**
1. **Use RedisRatelimitChecker**: The Redis-based implementation doesn't suffer from eviction issues because it uses persistent storage with TTL-based expiry [6](#0-5) 

2. **Hybrid Approach**: Use memory cache as write-through cache with persistent backend, falling back to rejection on cache miss rather than assuming zero count

3. **Increase Capacity**: If memory permits, significantly increase `max_entries_in_map` beyond 1M, though this only delays the problem

4. **Rate Limit Globally**: Add a global request rate limiter independent of per-IP tracking to prevent cache-filling attacks

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::net::{IpAddr, Ipv6Addr};
    
    #[tokio::test]
    async fn test_rate_limit_bypass_via_eviction() {
        // Create checker with small capacity for testing
        let config = MemoryRatelimitCheckerConfig {
            max_requests_per_day: 5,
            max_entries_in_map: NonZeroUsize::new(100).unwrap(),
        };
        let checker = MemoryRatelimitChecker::new(config);
        
        // Target IP that attacker wants to reuse
        let target_ip = IpAddr::V6(Ipv6Addr::new(0x2001, 0xdb8, 0, 0, 0, 0, 0, 1));
        
        // Step 1: Make requests from target IP up to limit
        let target_data = CheckerData {
            source_ip: target_ip,
            receiver: AccountAddress::ZERO,
            headers: Arc::new(HeaderMap::new()),
            time_request_received_secs: 0,
        };
        
        // Make 4 requests (limit is 5)
        for i in 0..4 {
            let result = checker.check(target_data.clone(), false).await.unwrap();
            assert!(result.is_empty(), "Request {} should succeed", i);
        }
        
        // Verify we're near the limit (5th request should still succeed)
        let result = checker.check(target_data.clone(), false).await.unwrap();
        assert!(result.is_empty(), "5th request should succeed");
        
        // 6th request should be rate limited
        let result = checker.check(target_data.clone(), false).await.unwrap();
        assert!(!result.is_empty(), "6th request should be rate limited");
        
        // Step 2: Fill cache with 100+ different IPs to trigger eviction of target_ip
        for i in 0..101 {
            let filler_ip = IpAddr::V6(Ipv6Addr::new(0x2001, 0xdb8, 0, 0, 0, 0, 1, i as u16));
            let filler_data = CheckerData {
                source_ip: filler_ip,
                receiver: AccountAddress::ZERO,
                headers: Arc::new(HeaderMap::new()),
                time_request_received_secs: 0,
            };
            checker.check(filler_data, false).await.unwrap();
        }
        
        // Step 3: Return to target IP - rate limit should be reset due to eviction
        let result = checker.check(target_data.clone(), false).await.unwrap();
        
        // VULNERABILITY: This request succeeds even though we already made 6 requests
        // from this IP today, because the counter was reset when the IP was evicted
        assert!(result.is_empty(), "VULNERABILITY: Rate limit bypassed via eviction!");
        
        // We can now make 4 more requests before hitting the limit again
        for i in 0..4 {
            let result = checker.check(target_data.clone(), false).await.unwrap();
            assert!(result.is_empty(), "Bypass request {} should succeed", i);
        }
        
        // Total requests from target_ip: 6 (before eviction) + 5 (after eviction) = 11
        // Expected maximum: 5
        // Actual bypassed: 11 - 5 = 6 extra requests (120% over limit)
    }
}
```

**Test Execution:**
```bash
cd crates/aptos-faucet/core
cargo test test_rate_limit_bypass_via_eviction
```

This test demonstrates that an IP can exceed its daily rate limit by triggering cache eviction through requests from many other IPs.

## Notes

- The vulnerability is specific to `MemoryRatelimitChecker`; the `RedisRatelimitChecker` implementation is not affected because it uses persistent storage with atomic operations [7](#0-6) 
- In production deployments using `MemoryRatelimitChecker`, this vulnerability is immediately exploitable
- The issue affects any faucet instance where traffic naturally exceeds the cache capacity, even without malicious attackers
- There is also a minor off-by-one issue where `get_or_insert_mut(ip, || 1)` followed by increment results in max_requests_per_day - 1 allowed requests instead of max_requests_per_day, but this is separate from the main eviction vulnerability

### Citations

**File:** crates/aptos-faucet/core/src/checkers/memory_ratelimit.rs (L19-21)
```rust
    #[serde(default = "MemoryRatelimitCheckerConfig::default_max_entries_in_map")]
    pub max_entries_in_map: NonZeroUsize,
}
```

**File:** crates/aptos-faucet/core/src/checkers/memory_ratelimit.rs (L24-26)
```rust
    fn default_max_entries_in_map() -> NonZeroUsize {
        NonZeroUsize::new(1000000).unwrap()
    }
```

**File:** crates/aptos-faucet/core/src/checkers/memory_ratelimit.rs (L37-37)
```rust
    pub ip_to_requests_today: Mutex<LruCache<IpAddr, u32>>,
```

**File:** crates/aptos-faucet/core/src/checkers/memory_ratelimit.rs (L53-63)
```rust
    async fn clear_if_new_day(&self) {
        if days_since_tap_epoch(get_current_time_secs())
            > self.current_day.load(std::sync::atomic::Ordering::Relaxed)
        {
            self.current_day.store(
                days_since_tap_epoch(get_current_time_secs()),
                std::sync::atomic::Ordering::Relaxed,
            );
            self.ip_to_requests_today.lock().await.clear();
        }
    }
```

**File:** crates/aptos-faucet/core/src/checkers/memory_ratelimit.rs (L77-77)
```rust
        let requests_today = ip_to_requests_today.get_or_insert_mut(data.source_ip, || 1);
```

**File:** crates/aptos-faucet/core/src/checkers/redis_ratelimit.rs (L114-149)
```rust
/// The RedisRatelimitChecker backend uses redis to ratelimit requests to the tap. Unlike
/// the PostgresStorage backend, it does not store full information for each
/// request. Instead, it uses counters to track limits. This is heavily inspired
/// by https://redis.com/redis-best-practices/basic-rate-limiting/.
///
/// We use a generic key (e.g. IP address or Firebase UID).
///
/// If we're not careful, it is possible for people to exceed the intended limit
/// by sending many requests simultaneously. We avoid this problem with this
/// order of operations:
///   1. Read the current value of the limit for the given key (e.g. IP / Firebase UID).
///   2. If value is greater than limit, reject.
///   3. Otherwise, increment and set TTL if necessary.
///   4. Increment returns the new value. Check if this is greater than the limit also.
///
/// Incrementing the limit is an atomic operation (meaning each client will see
/// value increment, never reading the same value), so steps 1 and 2 are not
/// actually necessary for correctness. Instead, steps 1 and 2 are just an optimization
/// to avoid incrementing the limit unnecessarily if the limit has already been
/// reached. With steps 1 and 2 we end up having more unnecessary reads when
/// they're under their limit vs more unnecessary writes when they're over their
/// limit, but we'll happily take more reads over more writes.
///
/// Note: Previously I made an attempt (d4fbf6db675e9036a967b52bf8d13e1b2566787e) at
/// doing these steps atomically, but it became very unwieldy:
///   1. Start a transaction.
///   2. Increment current value for limit for source key, set TTL if necessary.
///   3. If value is greater than limit, revert the transaction.
///
/// This second way leaves a small window for someone to slip in multiple requests,
/// therein blowing past the configured limit, but it's a very small window, so we'll
/// worry about it as a followup: https://github.com/aptos-labs/aptos-tap/issues/15.
pub struct RedisRatelimitChecker {
    args: RedisRatelimitCheckerConfig,
    db_pool: Pool,
    ratelimit_key_provider: RatelimitKeyProvider,
```

**File:** crates/aptos-faucet/core/src/checkers/redis_ratelimit.rs (L225-304)
```rust
impl CheckerTrait for RedisRatelimitChecker {
    async fn check(
        &self,
        data: CheckerData,
        dry_run: bool,
    ) -> Result<Vec<RejectionReason>, AptosTapError> {
        let mut conn = self
            .get_redis_connection()
            .await
            .map_err(|e| AptosTapError::new_with_error_code(e, AptosTapErrorCode::StorageError))?;

        // Generate a key corresponding to this identifier and the current day.
        let key_prefix = self.ratelimit_key_provider.ratelimit_key_prefix();
        let key_value = self
            .ratelimit_key_provider
            .ratelimit_key_value(&data)
            .await?;
        let (key, seconds_until_next_day) =
            self.get_key_and_secs_until_next_day(key_prefix, &key_value);

        // Get the value for the key, indicating how many non-500 requests we have
        // serviced for it today.
        let limit_value: Option<i64> = conn.get(&key).await.map_err(|e| {
            AptosTapError::new_with_error_code(
                format!("Failed to get value for redis key {}: {}", key, e),
                AptosTapErrorCode::StorageError,
            )
        })?;

        // If the limit value is greater than what we allow per day, signal that we
        // should reject this request.
        if let Some(rejection_reason) = self.check_limit_value(limit_value, seconds_until_next_day)
        {
            return Ok(vec![rejection_reason]);
        }

        // Atomically increment the counter for the given key, creating it and setting
        // the expiration time if it doesn't already exist.
        if !dry_run {
            let incremented_limit_value = match limit_value {
                Some(_) => conn.incr(&key, 1).await.map_err(|e| {
                    AptosTapError::new_with_error_code(
                        format!("Failed to increment redis key {}: {}", key, e),
                        AptosTapErrorCode::StorageError,
                    )
                })?,
                // If the limit value doesn't exist, create it and set the
                // expiration time.
                None => {
                    let (incremented_limit_value,): (i64,) = redis::pipe()
                        .atomic()
                        .incr(&key, 1)
                        // Expire at the end of the day roughly.
                        .expire(&key, seconds_until_next_day as usize)
                        // Only set the expiration if one isn't already set.
                        // Only works with Redis 7 sadly.
                        // .arg("NX")
                        .ignore()
                        .query_async(&mut *conn)
                        .await
                        .map_err(|e| {
                            AptosTapError::new_with_error_code(
                                format!("Failed to increment value for redis key {}: {}", key, e),
                                AptosTapErrorCode::StorageError,
                            )
                        })?;
                    incremented_limit_value
                },
            };

            // Check limit again, to ensure there wasn't a get / set race.
            if let Some(rejection_reason) =
                self.check_limit_value(Some(incremented_limit_value), seconds_until_next_day)
            {
                return Ok(vec![rejection_reason]);
            }
        }

        Ok(vec![])
    }
```
