# Audit Report

## Title
TokenDataId Hash Collision via Double-Colon Delimiter Injection in Indexer

## Summary
The indexer's TokenDataIdType hash computation uses a Display format with `::` as a delimiter, but the Move framework allows collection and token names to contain `::` characters. This enables attackers to create different TokenDataIds that produce identical hashes, causing database conflicts and data corruption in the `current_token_datas` table.

## Finding Description

The Aptos Token V1 framework stores token metadata using a `TokenDataId` struct with three fields: creator address, collection name, and token name. While the Move framework validates the maximum length of these strings (128 bytes), it does not restrict special characters. [1](#0-0) 

The Move string validation only checks UTF-8 encoding and does not prevent special characters like `::`: [2](#0-1) [3](#0-2) 

The indexer's Rust implementation formats TokenDataId as a string using `::` as a delimiter: [4](#0-3) 

The hash is computed from this Display string: [5](#0-4) [6](#0-5) 

**The vulnerability:** Two different TokenDataIds can produce the same hash:
- TokenDataId A: `{creator: 0x1, collection: "MyNFT", name: "Token::1"}`  
  Display: `"0x0000...001::MyNFT::Token::1"`  
  Hash: SHA256("0x0000...001::MyNFT::Token::1")

- TokenDataId B: `{creator: 0x1, collection: "MyNFT::Token", name: "1"}`  
  Display: `"0x0000...001::MyNFT::Token::1"`  
  Hash: SHA256("0x0000...001::MyNFT::Token::1")

Both produce identical hashes but represent different tokens. The `current_token_datas` table uses this hash as the sole primary key: [7](#0-6) 

When a hash collision occurs, the upsert operation overwrites the first token's data with the second: [8](#0-7) 

The upsert updates all fields including `collection_name` and `name` when the `last_transaction_version` condition is met, causing the first token's data to be permanently lost from the current state view.

## Impact Explanation

This vulnerability meets **Medium severity** criteria per the Aptos bug bounty program: "State inconsistencies requiring intervention."

**Specific impacts:**
1. **Data corruption**: Token metadata for legitimate tokens is overwritten in the indexer
2. **User-facing application failures**: DApps querying the indexer will display incorrect token data or fail to find tokens
3. **Asset invisibility**: Users cannot view or interact with their tokens through indexer-dependent interfaces
4. **Database integrity violation**: The uniqueness invariant of TokenDataId is broken in the indexer

While this vulnerability only affects the off-chain indexer (not on-chain consensus), the indexer is critical infrastructure for user-facing applications, wallets, and marketplaces. The impact requires manual intervention to fix (reindexing) and causes real-time data availability issues.

## Likelihood Explanation

**Likelihood: Medium**

The attack requires:
1. Creating a collection with `::` in the name (e.g., "Collection::Name")
2. Creating a token under that collection
3. Understanding that this will collide with another collection/token combination

**Factors increasing likelihood:**
- No special privileges required - any user can create tokens
- `::` is a common programming delimiter that users might naturally include
- The attack is deterministic and repeatable
- Minimal technical sophistication needed

**Factors decreasing likelihood:**
- Requires knowledge of the collision vulnerability
- Both tokens must be created by the same creator address
- Users must intentionally craft collision strings

The vulnerability is more likely to occur accidentally than through malicious exploitation, but intentional attacks are feasible.

## Recommendation

**Immediate fix:** Sanitize collection and token names by disallowing `::` sequences in the Move framework validation:

```move
public fun create_token_data_id(
    creator: address,
    collection: String,
    name: String,
): TokenDataId {
    assert!(collection.length() <= MAX_COLLECTION_NAME_LENGTH, error::invalid_argument(ECOLLECTION_NAME_TOO_LONG));
    assert!(name.length() <= MAX_NFT_NAME_LENGTH, error::invalid_argument(ENFT_NAME_TOO_LONG));
    // Add validation to prevent delimiter injection
    assert!(!string_contains(collection, string::utf8(b"::")), error::invalid_argument(EINVALID_COLLECTION_NAME));
    assert!(!string_contains(name, string::utf8(b"::")), error::invalid_argument(EINVALID_TOKEN_NAME));
    TokenDataId { creator, collection, name }
}
```

**Alternative fix:** Change the hash computation to use BCS serialization of the struct instead of the Display string:

```rust
impl TokenDataIdType {
    pub fn to_hash(&self) -> String {
        // Use BCS serialization instead of Display string
        let bcs_bytes = bcs::to_bytes(&(
            &self.creator,
            &self.collection,
            &self.name
        )).expect("BCS serialization failed");
        hex::encode(sha2::Sha256::digest(&bcs_bytes))
    }
}
```

This ensures that different struct values always produce different hashes, regardless of string content.

## Proof of Concept

```move
#[test_only]
module test_addr::collision_poc {
    use aptos_framework::account;
    use aptos_token::token;
    use std::string;
    
    #[test(creator = @0xcafe)]
    fun test_hash_collision(creator: &signer) {
        account::create_account_for_test(@0xcafe);
        
        // Create first collection and token
        token::create_collection(
            creator,
            string::utf8(b"MyCollection"),
            string::utf8(b"Test collection"),
            string::utf8(b"https://example.com"),
            1000,
            vector[false, false, false]
        );
        
        let token_data_id_1 = token::create_token_data_id(
            @0xcafe,
            string::utf8(b"MyCollection"),
            string::utf8(b"Token::V1")  // Contains ::
        );
        
        // Create second collection with :: in name
        token::create_collection(
            creator,
            string::utf8(b"MyCollection::Token"),  // Contains ::
            string::utf8(b"Test collection 2"),
            string::utf8(b"https://example.com"),
            1000,
            vector[false, false, false]
        );
        
        let token_data_id_2 = token::create_token_data_id(
            @0xcafe,
            string::utf8(b"MyCollection::Token"),
            string::utf8(b"V1")
        );
        
        // In the indexer, both would produce:
        // Display: "0x00000000000000000000000000000000000000000000000000000000000cafe::MyCollection::Token::V1"
        // Hash: SHA256 of the above string
        // Result: Hash collision in current_token_datas table
    }
}
```

**Notes**

This vulnerability demonstrates a classical delimiter injection issue where the separator character (`::`) is not properly escaped or forbidden in the input data. The root cause is the mismatch between the Move framework's permissive string validation and the indexer's assumption that `::` is a safe delimiter for string concatenation-based hashing. The fix should be implemented at the Move framework level to prevent such strings from being created in the first place.

### Citations

**File:** aptos-move/framework/aptos-token/sources/token.move (L1538-1546)
```text
    public fun create_token_data_id(
        creator: address,
        collection: String,
        name: String,
    ): TokenDataId {
        assert!(collection.length() <= MAX_COLLECTION_NAME_LENGTH, error::invalid_argument(ECOLLECTION_NAME_TOO_LONG));
        assert!(name.length() <= MAX_NFT_NAME_LENGTH, error::invalid_argument(ENFT_NAME_TOO_LONG));
        TokenDataId { creator, collection, name }
    }
```

**File:** third_party/move/move-stdlib/sources/string.move (L17-21)
```text
    /// Creates a new string from a sequence of bytes. Aborts if the bytes do not represent valid utf8.
    public fun utf8(bytes: vector<u8>): String {
        assert!(internal_check_utf8(&bytes), EINVALID_UTF8);
        String{bytes}
    }
```

**File:** third_party/move/move-stdlib/src/natives/string.rs (L39-54)
```rust
fn native_check_utf8(
    gas_params: &CheckUtf8GasParameters,
    _context: &mut NativeContext,
    _ty_args: &[Type],
    mut args: VecDeque<Value>,
) -> PartialVMResult<NativeResult> {
    debug_assert!(args.len() == 1);
    let s_arg = pop_arg!(args, VectorRef);
    let s_ref = s_arg.as_bytes_ref();
    let ok = std::str::from_utf8(s_ref.as_slice()).is_ok();
    // TODO: extensible native cost tables

    let cost = gas_params.base + gas_params.per_byte * NumBytes::new(s_ref.as_slice().len() as u64);

    NativeResult::map_partial_vm_result_one(cost, Ok(Value::bool(ok)))
}
```

**File:** crates/indexer/src/models/token_models/token_utils.rs (L46-48)
```rust
    pub fn to_hash(&self) -> String {
        hash_str(&self.to_string())
    }
```

**File:** crates/indexer/src/models/token_models/token_utils.rs (L67-77)
```rust
impl fmt::Display for TokenDataIdType {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        write!(
            f,
            "{}::{}::{}",
            standardize_address(self.creator.as_str()),
            self.collection,
            self.name
        )
    }
}
```

**File:** crates/indexer/src/util.rs (L19-21)
```rust
pub fn hash_str(val: &str) -> String {
    hex::encode(sha2::Sha256::digest(val.as_bytes()))
}
```

**File:** crates/indexer/migrations/2022-09-20-055651_add_current_token_data/up.sql (L26-28)
```sql
CREATE TABLE current_token_datas (
  -- sha256 of creator + collection_name + name
  token_data_id_hash VARCHAR(64) UNIQUE PRIMARY KEY NOT NULL,
```

**File:** crates/indexer/src/processors/token_processor.rs (L412-450)
```rust
fn insert_current_token_datas(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentTokenData],
) -> Result<(), diesel::result::Error> {
    use schema::current_token_datas::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentTokenData::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_token_datas::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict(token_data_id_hash)
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    name.eq(excluded(name)),
                    maximum.eq(excluded(maximum)),
                    supply.eq(excluded(supply)),
                    largest_property_version.eq(excluded(largest_property_version)),
                    metadata_uri.eq(excluded(metadata_uri)),
                    payee_address.eq(excluded(payee_address)),
                    royalty_points_numerator.eq(excluded(royalty_points_numerator)),
                    royalty_points_denominator.eq(excluded(royalty_points_denominator)),
                    maximum_mutable.eq(excluded(maximum_mutable)),
                    uri_mutable.eq(excluded(uri_mutable)),
                    description_mutable.eq(excluded(description_mutable)),
                    properties_mutable.eq(excluded(properties_mutable)),
                    royalty_mutable.eq(excluded(royalty_mutable)),
                    default_properties.eq(excluded(default_properties)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    collection_data_id_hash.eq(excluded(collection_data_id_hash)),
                    description.eq(excluded(description)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_token_datas.last_transaction_version <= excluded.last_transaction_version "),
        )?;
```
