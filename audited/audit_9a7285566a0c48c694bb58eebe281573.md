# Audit Report

## Title
Race Condition in BlockExecutor::ledger_update Causes Node Panic via OnceCell Double-Set

## Summary
The `BlockExecutor::ledger_update` method contains a Time-of-Check to Time-of-Use (TOCTOU) race condition that allows concurrent threads to simultaneously process the same block, resulting in a panic when both threads attempt to set the same `OnceCell` fields. This can crash validator nodes and disrupt consensus liveness.

## Finding Description

The `BlockExecutor` publicly exposes a thread-safe API through `Arc<dyn BlockExecutorTrait>`, but the `ledger_update` method lacks proper synchronization for the critical path where block computation results are stored. [1](#0-0) 

The vulnerability exists in the `ledger_update` method which only acquires a **read lock** on the `inner` field: [2](#0-1) 

This allows multiple threads to concurrently enter `BlockExecutorInner::ledger_update` for the same `block_id`. The method contains a TOCTOU vulnerability: [3](#0-2) 

The check at line 291 (`get_complete_result()`) is **not atomic** with the subsequent set operations. If two threads pass this check before either completes the set operations, both will attempt to set the `OnceCell` fields in `PartialStateComputeResult`: [4](#0-3) 

The `expect()` calls will **panic** when the second thread attempts to set an already-set `OnceCell`, crashing the executor thread and potentially bringing down the validator node.

**Attack Scenario:**
1. Thread A calls `executor.ledger_update(block_X, parent_X)`
2. Thread B concurrently calls `executor.ledger_update(block_X, parent_X)` (same block)
3. Both acquire read locks on `inner` (succeeds concurrently)
4. Both retrieve the same `Block` from `block_tree`
5. Both check `get_complete_result()` → returns `None` for both
6. Thread A calls `output.set_state_checkpoint_output(...)` → succeeds
7. Thread B calls `output.set_state_checkpoint_output(...)` → **panics with "StateCheckpointOutput already set"**
8. Validator node crashes

The TODO comment at line 290 acknowledges retry scenarios but provides no protection: [5](#0-4) 

## Impact Explanation

This vulnerability meets **High Severity** criteria per the Aptos bug bounty program:

- **Validator node crashes**: A panic in the executor thread will crash the node, causing immediate loss of liveness for that validator
- **API crashes**: The publicly exposed `BlockExecutor` API crashes when used concurrently, violating thread-safety expectations
- **Consensus disruption**: If multiple validators are affected (e.g., through network conditions causing retries or error scenarios), this could impact consensus operation

While the consensus pipeline's use of `.shared()` futures normally prevents this race in production, the **public API design is fundamentally unsafe** for concurrent access. External tools, benchmarks, error recovery logic, or future code changes could inadvertently trigger this race condition.

## Likelihood Explanation

**Medium-to-High likelihood** depending on deployment scenario:

1. **Current production risk**: Low-Medium - The consensus pipeline appears to use proper future coordination, but error conditions, retries, or edge cases in epoch transitions could trigger concurrent calls

2. **API design risk**: High - The public API accepts `&self` and only uses read locks, creating the false impression of thread-safety while containing a critical race condition

3. **Future risk**: High - Any code (internal tools, benchmarks, monitoring systems, future features) that uses `BlockExecutor` directly could easily trigger this bug

The codebase search revealed similar `OnceCell` patterns with warnings like "must be called in quiescence", indicating the developers are aware of these concurrency risks in other parts of the codebase.

## Recommendation

**Fix 1: Extend execution_lock to cover ledger_update (Recommended)**

Change `ledger_update` to acquire the `execution_lock` before processing: [6](#0-5) 

Modify the method to acquire the execution lock:

```rust
fn ledger_update(
    &self,
    block_id: HashValue,
    parent_block_id: HashValue,
) -> ExecutorResult<StateComputeResult> {
    let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "ledger_update"]);
    
    // Acquire execution lock to prevent concurrent processing of same block
    let _exec_guard = self.execution_lock.lock();
    
    self.inner
        .read()
        .as_ref()
        .ok_or_else(|| ExecutorError::InternalError {
            error: "BlockExecutor is not reset".into(),
        })?
        .ledger_update(block_id, parent_block_id)
}
```

**Fix 2: Use try_set pattern in PartialStateComputeResult**

Change the panic-on-set to return early if already set:

```rust
pub fn set_state_checkpoint_output(&self, state_checkpoint_output: StateCheckpointOutput) -> Result<()> {
    self.state_checkpoint_output
        .set(state_checkpoint_output)
        .map_err(|_| anyhow!("StateCheckpointOutput already set - concurrent ledger_update detected"))
}
```

Then handle the error in `ledger_update` by returning the already-completed result.

**Fix 3: Document thread-safety requirements**

Add explicit documentation to `BlockExecutorTrait` specifying that `ledger_update` must not be called concurrently for the same `block_id`.

## Proof of Concept

```rust
#[test]
fn test_concurrent_ledger_update_race() {
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    // Setup: Create executor with a block already executed
    let executor = Arc::new(TestExecutor::new());
    let parent_block_id = executor.committed_block_id();
    let block_id = gen_block_id(1);
    
    let txns: Vec<_> = (0..10)
        .map(|i| encode_mint_transaction(gen_address(i), 100))
        .collect();
    
    // Execute block first
    executor.execute_and_update_state(
        (block_id, block(txns)).into(),
        parent_block_id,
        TEST_BLOCK_EXECUTOR_ONCHAIN_CONFIG,
    ).unwrap();
    
    // Now spawn two threads that both call ledger_update
    let barrier = Arc::new(Barrier::new(2));
    let mut handles = vec![];
    
    for _ in 0..2 {
        let exec = executor.clone();
        let bar = barrier.clone();
        let handle = thread::spawn(move || {
            bar.wait(); // Synchronize to maximize race condition
            exec.ledger_update(block_id, parent_block_id)
        });
        handles.push(handle);
    }
    
    // At least one thread should panic with "already set"
    let results: Vec<_> = handles.into_iter()
        .map(|h| h.join())
        .collect();
    
    // Expected: One succeeds, one panics
    // Actual vulnerability: Second thread panics, crashes node
    assert!(results.iter().any(|r| r.is_err()), 
        "Expected panic from concurrent ledger_update");
}
```

**Notes:**
- This vulnerability affects the thread-safety guarantees of the publicly exposed `BlockExecutor` API
- The `execution_lock` currently only protects `execute_and_update_state`, not `ledger_update`
- The consensus pipeline's use of `.shared()` futures provides some protection, but does not address the fundamental API design flaw
- Similar `OnceCell` usage patterns exist elsewhere in the codebase, suggesting this is a systemic concurrency concern
- The check at line 291 is defensive programming that is insufficient without proper locking

### Citations

**File:** execution/executor/src/lib.rs (L13-14)
```rust
pub mod block_executor;
pub mod chunk_executor;
```

**File:** execution/executor/src/block_executor/mod.rs (L52-52)
```rust
    execution_lock: Mutex<()>,
```

**File:** execution/executor/src/block_executor/mod.rs (L115-129)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "ledger_update"]);

        self.inner
            .read()
            .as_ref()
            .ok_or_else(|| ExecutorError::InternalError {
                error: "BlockExecutor is not reset".into(),
            })?
            .ledger_update(block_id, parent_block_id)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L260-334)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _timer = UPDATE_LEDGER.start_timer();
        info!(
            LogSchema::new(LogEntry::BlockExecutor).block_id(block_id),
            "ledger_update"
        );
        let committed_block_id = self.committed_block_id();
        let mut block_vec = self
            .block_tree
            .get_blocks_opt(&[block_id, parent_block_id])?;
        let parent_block = block_vec
            .pop()
            .expect("Must exist.")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
        // At this point of time two things must happen
        // 1. The block tree must also have the current block id with or without the ledger update output.
        // 2. We must have the ledger update output of the parent block.
        // Above is not ture if the block is on a forked branch.
        let block = block_vec
            .pop()
            .expect("Must exist")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
        parent_block.ensure_has_child(block_id)?;
        let output = &block.output;
        let parent_out = &parent_block.output;

        // TODO(aldenhu): remove, assuming no retries.
        if let Some(complete_result) = block.output.get_complete_result() {
            info!(block_id = block_id, "ledger_update already done.");
            return Ok(complete_result);
        }

        if parent_block_id != committed_block_id && parent_out.has_reconfiguration() {
            info!(block_id = block_id, "ledger_update for reconfig suffix.");

            // Parent must have done all state checkpoint and ledger update since this method
            // is being called.
            output.set_state_checkpoint_output(
                parent_out
                    .ensure_state_checkpoint_output()?
                    .reconfig_suffix(),
            );
            output.set_ledger_update_output(
                parent_out.ensure_ledger_update_output()?.reconfig_suffix(),
            );
        } else {
            THREAD_MANAGER.get_non_exe_cpu_pool().install(|| {
                // TODO(aldenhu): remove? no known strategy to recover from this failure
                fail_point!("executor::block_state_checkpoint", |_| {
                    Err(anyhow::anyhow!("Injected error in block state checkpoint."))
                });
                output.set_state_checkpoint_output(DoStateCheckpoint::run(
                    &output.execution_output,
                    parent_block.output.ensure_result_state_summary()?,
                    &ProvableStateSummary::new_persisted(self.db.reader.as_ref())?,
                    None,
                )?);
                output.set_ledger_update_output(DoLedgerUpdate::run(
                    &output.execution_output,
                    output.ensure_state_checkpoint_output()?,
                    parent_out
                        .ensure_ledger_update_output()?
                        .transaction_accumulator
                        .clone(),
                )?);
                Result::<_>::Ok(())
            })?;
        }

        Ok(block.output.expect_complete_result())
    }
```

**File:** execution/executor/src/types/partial_state_compute_result.rs (L76-91)
```rust
    pub fn set_state_checkpoint_output(&self, state_checkpoint_output: StateCheckpointOutput) {
        self.state_checkpoint_output
            .set(state_checkpoint_output)
            .expect("StateCheckpointOutput already set");
    }

    pub fn ensure_ledger_update_output(&self) -> Result<&LedgerUpdateOutput> {
        self.ledger_update_output
            .get()
            .context("LedgerUpdateOutput not set.")
    }

    pub fn set_ledger_update_output(&self, ledger_update_output: LedgerUpdateOutput) {
        self.ledger_update_output
            .set(ledger_update_output)
            .expect("LedgerUpdateOutput already set");
```
