# Audit Report

## Title
Non-Atomic Store Updates in ObserverBlockData Can Cause State Desynchronization Under Failure Conditions

## Summary
The `ObserverBlockData` struct manages three independent stores (block_payload_store, ordered_block_store, and pending_block_store) that are updated sequentially without atomicity guarantees. If operations fail mid-sequence due to panics, OOM, or process termination, the stores become desynchronized, creating an inconsistent view of blocks.

## Finding Description

The `ObserverBlockData` struct contains three separate stores that must remain synchronized: [1](#0-0) 

Critical methods update these stores sequentially without transaction semantics:

**1. clear_block_data()** - Clears all three stores sequentially: [2](#0-1) 

**2. handle_committed_blocks()** - Updates two stores and root: [3](#0-2) 

**3. update_blocks_for_state_sync_commit()** - Updates root and two stores: [4](#0-3) 

**Critical vulnerability in PendingBlockStore.remove_ready_block():** [5](#0-4) 

This code clears both internal maps, then rebuilds them in a loop. If `first_block()` panics (which uses `.expect()` and can panic on empty blocks vector), the two internal maps become permanently desynchronized: [6](#0-5) 

The code even acknowledges this synchronization risk: [7](#0-6) 

## Impact Explanation

**Severity Assessment: Medium (State inconsistencies requiring intervention)**

This issue does NOT meet Critical severity because:
- Consensus Observers are passive components that watch consensus but don't participate in voting
- A desynchronized observer cannot directly violate consensus safety or cause network-wide failures
- Does not result in loss of funds or consensus violations

However, it qualifies as **Medium severity** because:
- Creates state inconsistencies that persist across operations
- Affected observer nodes may serve incorrect data to clients
- Requires manual intervention to recover (node restart)
- Could affect validator nodes running observers, impacting API availability
- Violates the **State Consistency** invariant (state transitions must be atomic)

## Likelihood Explanation

**Likelihood: Low-Medium**

The desynchronization can occur when:
1. **Panic during BTreeMap operations** - Memory allocation failures during `split_off()`
2. **Process crash/termination** - Mid-update if process is killed
3. **Mutex poisoning** - If BlockPayloadStore's mutex is poisoned from another thread's panic
4. **Panic in rebuild loop** - If `first_block()` is called on invalid OrderedBlock (theoretically prevented by validation)

However, external attackers cannot directly trigger these conditions without:
- Node compromise (already has RCE)
- Resource exhaustion (DoS attacks are out of scope)
- Validation bypass to inject invalid blocks

The issue is more likely to manifest under:
- Heavy load causing memory pressure
- Abnormal node termination
- Software bugs causing unexpected panics

## Recommendation

Implement atomic transaction semantics or rollback capability for multi-store updates:

```rust
// Option 1: Make updates atomic by wrapping all stores in a single transaction-like structure
pub fn clear_block_data(&mut self) -> Result<LedgerInfoWithSignatures, Error> {
    // Save state for potential rollback
    let payload_backup = self.block_payload_store.clone();
    let ordered_backup = self.ordered_block_store.clone();
    let pending_backup = self.pending_block_store.clone();
    
    // Attempt all operations
    let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
        self.block_payload_store.clear_all_payloads();
        self.ordered_block_store.clear_all_ordered_blocks();
        self.pending_block_store.clear_missing_blocks();
    }));
    
    // Rollback on failure
    if result.is_err() {
        self.block_payload_store = payload_backup;
        self.ordered_block_store = ordered_backup;
        self.pending_block_store = pending_backup;
        return Err(Error::InternalError("Failed to clear block data".into()));
    }
    
    Ok(self.root())
}

// Option 2: Fix PendingBlockStore to rebuild atomically
pub fn remove_ready_block(...) -> Option<Arc<PendingBlockWithMetadata>> {
    // Build new hash map BEFORE clearing
    let mut new_hash_map = BTreeMap::new();
    for pending_block in blocks_at_higher_rounds.values() {
        // This might panic, so do it before clearing
        let first_block = pending_block.ordered_block().first_block();
        new_hash_map.insert(first_block.id(), pending_block.clone());
    }
    
    // Now clear and update atomically
    self.blocks_without_payloads.clear();
    self.blocks_without_payloads_by_hash.clear();
    self.blocks_without_payloads = blocks_at_higher_rounds;
    self.blocks_without_payloads_by_hash = new_hash_map;
    
    ready_block
}

// Option 3: Add verification after updates
fn verify_stores_synchronized(&self) -> Result<(), Error> {
    // Check payload store vs ordered store consistency
    // Check pending store internal consistency
    // Return error if desynchronized
}
```

## Proof of Concept

```rust
#[test]
fn test_store_desynchronization_on_panic() {
    use std::panic;
    
    // Create observer block data
    let mut observer_block_data = ObserverBlockData::new_with_root(
        ConsensusObserverConfig::default(),
        create_test_ledger_info(0, 0),
    );
    
    // Insert valid blocks into stores
    insert_test_blocks(&mut observer_block_data);
    
    // Verify stores are synchronized
    assert_eq!(
        observer_block_data.get_block_payloads().lock().len(),
        observer_block_data.get_all_ordered_blocks().len()
    );
    
    // Simulate panic during clear_block_data by injecting panic in one store
    // (In reality, this could happen due to OOM during BTreeMap operations)
    let result = panic::catch_unwind(panic::AssertUnwindSafe(|| {
        // Clear first store
        observer_block_data.block_payload_store.clear_all_payloads();
        
        // Simulate panic before other stores are cleared
        panic!("Simulated OOM or unexpected panic");
        
        // These would normally execute but won't due to panic
        observer_block_data.ordered_block_store.clear_all_ordered_blocks();
        observer_block_data.pending_block_store.clear_missing_blocks();
    }));
    
    // Verify panic occurred
    assert!(result.is_err());
    
    // Verify stores are now DESYNCHRONIZED
    let payload_count = observer_block_data.get_block_payloads().lock().len();
    let ordered_count = observer_block_data.get_all_ordered_blocks().len();
    
    // payload_store is cleared but ordered_block_store still has blocks
    assert_eq!(payload_count, 0);
    assert!(ordered_count > 0);
    
    // This demonstrates the desynchronization vulnerability
    println!("VULNERABILITY CONFIRMED: Stores desynchronized after partial update failure");
}
```

**Note**: While this is a real issue in code quality and robustness, it does NOT meet the **Critical** severity threshold as labeled in the question, because it requires internal failures (panics, OOM, crashes) rather than external attacker-controlled triggers, and consensus observers are passive components that don't directly affect consensus safety.

### Citations

**File:** consensus/src/consensus_observer/observer/block_data.rs (L40-52)
```rust
pub struct ObserverBlockData {
    // The block payload store (containing the block transaction payloads)
    block_payload_store: BlockPayloadStore,

    // The ordered block store (containing ordered blocks that are ready for execution)
    ordered_block_store: OrderedBlockStore,

    // The pending block store (containing pending blocks that are without payloads)
    pending_block_store: PendingBlockStore,

    // The latest ledger info
    root: LedgerInfoWithSignatures,
}
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L93-105)
```rust
    pub fn clear_block_data(&mut self) -> LedgerInfoWithSignatures {
        // Clear the payload store
        self.block_payload_store.clear_all_payloads();

        // Clear the ordered blocks
        self.ordered_block_store.clear_all_ordered_blocks();

        // Clear the pending blocks
        self.pending_block_store.clear_missing_blocks();

        // Return the root ledger info
        self.root()
    }
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L182-219)
```rust
    fn handle_committed_blocks(&mut self, ledger_info: LedgerInfoWithSignatures) {
        // Remove the committed blocks from the payload and ordered block stores
        self.block_payload_store.remove_blocks_for_epoch_round(
            ledger_info.commit_info().epoch(),
            ledger_info.commit_info().round(),
        );
        self.ordered_block_store
            .remove_blocks_for_commit(&ledger_info);

        // Verify the ledger info is for the same epoch
        let root_commit_info = self.root.commit_info();
        if ledger_info.commit_info().epoch() != root_commit_info.epoch() {
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Received commit callback for a different epoch! Ledger info: {:?}, Root: {:?}",
                    ledger_info.commit_info(),
                    root_commit_info
                ))
            );
            return;
        }

        // Update the root ledger info. Note: we only want to do this if
        // the new ledger info round is greater than the current root
        // round. Otherwise, this can race with the state sync process.
        if ledger_info.commit_info().round() > root_commit_info.round() {
            info!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Updating the root ledger info! Old root: (epoch: {:?}, round: {:?}). New root: (epoch: {:?}, round: {:?})",
                root_commit_info.epoch(),
                root_commit_info.round(),
                ledger_info.commit_info().epoch(),
                ledger_info.commit_info().round(),
            ))
        );
            self.root = ledger_info;
        }
    }
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L275-291)
```rust
    pub fn update_blocks_for_state_sync_commit(&mut self, commit_decision: &CommitDecision) {
        // Get the commit proof, epoch and round
        let commit_proof = commit_decision.commit_proof();
        let commit_epoch = commit_decision.epoch();
        let commit_round = commit_decision.round();

        // Update the root
        self.update_root(commit_proof.clone());

        // Update the block payload store
        self.block_payload_store
            .remove_blocks_for_epoch_round(commit_epoch, commit_round);

        // Update the ordered block store
        self.ordered_block_store
            .remove_blocks_for_commit(commit_proof);
    }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L159-170)
```rust
        // Verify that both stores have the same number of entries.
        // If not, log an error as this should never happen.
        let num_pending_blocks = self.blocks_without_payloads.len() as u64;
        let num_pending_blocks_by_hash = self.blocks_without_payloads_by_hash.len() as u64;
        if num_pending_blocks != num_pending_blocks_by_hash {
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "The pending block stores have different numbers of entries: {} and {} (by hash)",
                    num_pending_blocks, num_pending_blocks_by_hash
                ))
            );
        }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L243-252)
```rust
        // Clear all blocks from the pending block stores
        self.clear_missing_blocks();

        // Update the pending block stores to only include the blocks at higher rounds
        self.blocks_without_payloads = blocks_at_higher_rounds;
        for pending_block in self.blocks_without_payloads.values() {
            let first_block = pending_block.ordered_block().first_block();
            self.blocks_without_payloads_by_hash
                .insert(first_block.id(), pending_block.clone());
        }
```

**File:** consensus/src/consensus_observer/network/observer_message.rs (L199-205)
```rust
    /// Returns a copy of the first ordered block
    pub fn first_block(&self) -> Arc<PipelinedBlock> {
        self.blocks
            .first()
            .cloned()
            .expect("At least one block is expected!")
    }
```
