# Audit Report

## Title
Hash Collision Vulnerability in Token Data Indexer via Delimiter Injection in Collection/Name Fields

## Summary
The token indexer's `token_data_id_hash` calculation uses an unescaped delimiter (`::`) to concatenate creator address, collection name, and token name. Since collection and name fields can contain `::` characters, an attacker can create distinct tokens that produce identical hash values, causing data corruption in the `current_token_datas` table and potential royalty theft. [1](#0-0) 

## Finding Description

The vulnerability exists in the hash calculation for `TokenDataId`, not in `standardize_address()` itself. The `standardize_address()` function correctly normalizes address representations: [2](#0-1) 

However, the `TokenDataIdType::Display` implementation concatenates fields using `::` as a delimiter: [1](#0-0) 

This string is then hashed: [3](#0-2) 

The Move framework only validates field length, not content: [4](#0-3) 

**Attack Scenario:**
1. Creator creates Token A: `{creator: 0x1, collection: "Art", name: "Piece::Rare"}`
   - Hash input: `"0x0000...0001::Art::Piece::Rare"`
   
2. Same creator creates Token B: `{creator: 0x1, collection: "Art::Piece", name: "Rare"}`
   - Hash input: `"0x0000...0001::Art::Piece::Rare"`
   - **Identical hash to Token A!**

3. When Token B is indexed, the upsert logic triggers: [5](#0-4) 

Since `current_token_datas` uses `token_data_id_hash` as the sole primary key: [6](#0-5) 

Token A's data is completely overwritten with Token B's data, including `creator_address`, `collection_name`, `name`, and critically, `payee_address`.

## Impact Explanation

**Medium Severity** per Aptos bug bounty criteria:
- **State inconsistencies requiring intervention**: Token A's metadata is permanently lost from `current_token_datas`, requiring database restoration
- **Limited funds loss or manipulation**: Royalty payments for Token A would be redirected to Token B's `payee_address`
- NFT marketplaces querying the indexer API would display wrong metadata for Token A
- Applications cannot distinguish between the two tokens using `token_data_id_hash` queries

While the on-chain state remains correct, many dApps depend on the indexer for token queries, making this a significant data integrity issue.

## Likelihood Explanation

**Medium likelihood:**
- Requires the same creator to create tokens with specific `::` character placement in collection/name fields
- Could occur accidentally if creators use `::` as a naming convention (e.g., "Series::Episode" or "Artist::Work")
- Could be exploited intentionally by malicious creators to redirect their own token royalties or confuse marketplace listings
- Not cross-creator exploitable (requires same creator address)

## Recommendation

Implement proper escaping or use a different hash format that prevents delimiter collisions:

**Option 1: Escape delimiter in fields**
```rust
impl fmt::Display for TokenDataIdType {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        write!(
            f,
            "{}::{}::{}",
            standardize_address(self.creator.as_str()),
            self.collection.replace("::", "\\::\\"),
            self.name.replace("::", "\\::\\")
        )
    }
}
```

**Option 2: Use structured hashing (recommended)**
```rust
pub fn to_hash(&self) -> String {
    let mut hasher = sha2::Sha256::new();
    hasher.update(standardize_address(self.creator.as_str()).as_bytes());
    hasher.update(&[0u8]); // field separator
    hasher.update(self.collection.as_bytes());
    hasher.update(&[0u8]); // field separator
    hasher.update(self.name.as_bytes());
    hex::encode(hasher.finalize())
}
```

**Option 3: Add validation in Move contract**
```move
public fun create_token_data_id(
    creator: address,
    collection: String,
    name: String,
): TokenDataId {
    assert!(collection.length() <= MAX_COLLECTION_NAME_LENGTH, error::invalid_argument(ECOLLECTION_NAME_TOO_LONG));
    assert!(name.length() <= MAX_NFT_NAME_LENGTH, error::invalid_argument(ENFT_NAME_TOO_LONG));
    // Add validation to prevent :: in collection and name
    assert!(!string::contains(&collection, &string::utf8(b"::")), error::invalid_argument(EINVALID_COLLECTION_NAME));
    assert!(!string::contains(&name, &string::utf8(b"::")), error::invalid_argument(EINVALID_TOKEN_NAME));
    TokenDataId { creator, collection, name }
}
```

## Proof of Concept

```move
// Move test demonstrating the collision
#[test(creator = @0x1)]
fun test_token_hash_collision(creator: &signer) {
    // Create Token A
    token::create_collection(creator, string::utf8(b"Art"), /* ... */);
    let token_a_id = token::create_token_data_id(
        signer::address_of(creator),
        string::utf8(b"Art"),
        string::utf8(b"Piece::Rare")
    );
    
    // Create Token B with different collection/name but same hash
    token::create_collection(creator, string::utf8(b"Art::Piece"), /* ... */);
    let token_b_id = token::create_token_data_id(
        signer::address_of(creator),
        string::utf8(b"Art::Piece"),
        string::utf8(b"Rare")
    );
    
    // On-chain: token_a_id != token_b_id (different structs)
    // In indexer: hash(token_a_id) == hash(token_b_id) (collision!)
    // Result: Token A data overwritten in current_token_datas
}
```

**Note:** The original security question asked specifically about `standardize_address()` handling. The `standardize_address()` function itself works correctly and cannot be exploited for address collisions. The vulnerability lies in the delimiter-based hash format, not in address standardization.

### Citations

**File:** crates/indexer/src/models/token_models/token_utils.rs (L46-48)
```rust
    pub fn to_hash(&self) -> String {
        hash_str(&self.to_string())
    }
```

**File:** crates/indexer/src/models/token_models/token_utils.rs (L67-77)
```rust
impl fmt::Display for TokenDataIdType {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        write!(
            f,
            "{}::{}::{}",
            standardize_address(self.creator.as_str()),
            self.collection,
            self.name
        )
    }
}
```

**File:** crates/indexer/src/util.rs (L14-17)
```rust
/// Standardizes all addresses and table handles to be length 66 (0x-64 length hash)
pub fn standardize_address(handle: &str) -> String {
    format!("0x{:0>64}", &handle[2..])
}
```

**File:** aptos-move/framework/aptos-token/sources/token.move (L1543-1545)
```text
        assert!(collection.length() <= MAX_COLLECTION_NAME_LENGTH, error::invalid_argument(ECOLLECTION_NAME_TOO_LONG));
        assert!(name.length() <= MAX_NFT_NAME_LENGTH, error::invalid_argument(ENFT_NAME_TOO_LONG));
        TokenDataId { creator, collection, name }
```

**File:** crates/indexer/src/processors/token_processor.rs (L425-450)
```rust
                .on_conflict(token_data_id_hash)
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    name.eq(excluded(name)),
                    maximum.eq(excluded(maximum)),
                    supply.eq(excluded(supply)),
                    largest_property_version.eq(excluded(largest_property_version)),
                    metadata_uri.eq(excluded(metadata_uri)),
                    payee_address.eq(excluded(payee_address)),
                    royalty_points_numerator.eq(excluded(royalty_points_numerator)),
                    royalty_points_denominator.eq(excluded(royalty_points_denominator)),
                    maximum_mutable.eq(excluded(maximum_mutable)),
                    uri_mutable.eq(excluded(uri_mutable)),
                    description_mutable.eq(excluded(description_mutable)),
                    properties_mutable.eq(excluded(properties_mutable)),
                    royalty_mutable.eq(excluded(royalty_mutable)),
                    default_properties.eq(excluded(default_properties)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    collection_data_id_hash.eq(excluded(collection_data_id_hash)),
                    description.eq(excluded(description)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_token_datas.last_transaction_version <= excluded.last_transaction_version "),
        )?;
```

**File:** crates/indexer/migrations/2022-09-20-055651_add_current_token_data/up.sql (L26-48)
```sql
CREATE TABLE current_token_datas (
  -- sha256 of creator + collection_name + name
  token_data_id_hash VARCHAR(64) UNIQUE PRIMARY KEY NOT NULL,
  creator_address VARCHAR(66) NOT NULL,
  collection_name VARCHAR(128) NOT NULL,
  name VARCHAR(128) NOT NULL,
  maximum NUMERIC NOT NULL,
  supply NUMERIC NOT NULL,
  largest_property_version NUMERIC NOT NULL,
  metadata_uri VARCHAR(512) NOT NULL,
  payee_address VARCHAR(66) NOT NULL,
  royalty_points_numerator NUMERIC NOT NULL,
  royalty_points_denominator NUMERIC NOT NULL,
  maximum_mutable BOOLEAN NOT NULL,
  uri_mutable BOOLEAN NOT NULL,
  description_mutable BOOLEAN NOT NULL,
  properties_mutable BOOLEAN NOT NULL,
  royalty_mutable BOOLEAN NOT NULL,
  default_properties jsonb NOT NULL,
  last_transaction_version BIGINT NOT NULL,
  inserted_at TIMESTAMP NOT NULL DEFAULT NOW()
);
CREATE INDEX curr_td_crea_cn_name_index ON current_token_datas (creator_address, collection_name, name);
```
