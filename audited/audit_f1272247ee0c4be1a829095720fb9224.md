# Audit Report

## Title
Epoch Transition Race Condition: Batches from Previous Epoch Can Be Persisted and Immediately Garbage Collected

## Summary
During epoch transitions, when `BatchStore::new()` is called with `is_new_epoch=true`, a race condition allows batches from the previous epoch to be persisted to the database after garbage collection (GC) has been initiated, causing these batches to be deleted despite passing validation checks. This occurs because (1) the GC spawns asynchronously without synchronization, (2) batch persistence lacks epoch validation, and (3) the database is shared across epoch transitions.

## Finding Description

The vulnerability manifests through the following execution flow:

**Phase 1: Epoch Transition with Shared Database**

The QuorumStore database is shared across epochs via `Arc<dyn QuorumStoreStorage>` cloning: [1](#0-0) 

When a new epoch starts, `BatchStore::new()` is called with `is_new_epoch=true`, which spawns garbage collection asynchronously: [2](#0-1) 

The GC is spawned via `tokio::task::spawn_blocking()`, meaning it executes asynchronously on a thread pool. The `BatchStore` is immediately returned and becomes operational before GC completes.

**Phase 2: Lack of Epoch Validation in Batch Persistence**

When batches arrive from the network, they are processed by `BatchCoordinator::handle_batches_msg()`: [3](#0-2) 

This function performs validation on transaction limits and filters, but contains **no epoch validation**. Batches are then persisted via: [4](#0-3) 

The `save()` function in `BatchStore` only validates expiration time, not epoch: [5](#0-4) 

**Phase 3: Race Condition Exploitation**

The race window exists because:

1. During epoch N-1 shutdown, batches may still be in processing queues
2. New epoch N's `BatchStore` is created while old epoch's batches are being persisted
3. The shared database means both old and new `BatchStore` instances write to the same storage
4. GC reads all batches and deletes those with `epoch < current_epoch`: [6](#0-5) 

**Phase 4: Consequence - Data Loss**

If a batch is stored with `StorageMode::PersistedOnly` (when memory quota is exceeded), the metadata remains in cache but the payload is only in the database: [7](#0-6) 

When fetching such a batch after GC deletion: [8](#0-7) 

The fetch from database fails, returning `ExecutorError::CouldNotGetData`, which propagates to block execution: [9](#0-8) 

## Impact Explanation

This vulnerability qualifies as **High Severity** based on the following criteria:

1. **Consensus Impact**: If a validator requires a batch that was deleted due to this race, block execution will fail with `ExecutorError::CouldNotGetData`, potentially causing the validator to fall behind or requiring manual intervention.

2. **Liveness Degradation**: Validators affected by this race may experience slowdowns or failures in block processing, reducing network liveness.

3. **State Consistency Violation**: The system invariant that "successfully persisted batches remain accessible until proper expiration" is violated. Batches that pass all validation checks (`save()` returns `Ok`) are immediately deleted by GC.

4. **Non-Deterministic Failure**: The race condition creates non-deterministic behavior depending on timing, making the system less predictable and reliable.

While this doesn't directly cause fund loss or total network partition, it represents a **significant protocol violation** affecting validator operation, which aligns with High Severity per the Aptos bug bounty criteria.

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability will occur when:

1. **Epoch transitions happen** (regular occurrence in Aptos)
2. **Network has batches in flight** during the transition window
3. **Memory quotas are constrained**, causing `StorageMode::PersistedOnly` storage
4. **GC completes before shutdown fully drains queues**

The race window is narrow but real because:
- `tokio::task::spawn_blocking()` provides no synchronization guarantees
- Shutdown coordination with old epoch happens via channels, but database writes are not atomic with respect to GC
- Network delays naturally cause batches from previous epochs to arrive during transitions

The likelihood increases under:
- High network load conditions
- Slower disk I/O (delayed batch persistence)
- High validator count (more network messages in flight)

## Recommendation

**Primary Fix: Add Epoch Validation in BatchStore**

Add epoch validation in the `save()` function:

```rust
pub(crate) fn save(&self, value: &PersistedValue<BatchInfoExt>) -> anyhow::Result<bool> {
    // Add epoch validation
    let batch_epoch = value.batch_info().epoch();
    let current_epoch = self.epoch();
    if batch_epoch != current_epoch {
        counters::NUM_BATCH_WRONG_EPOCH_WHEN_SAVE.inc();
        bail!(
            "Incorrect epoch {} (expected {}) for batch {}",
            batch_epoch,
            current_epoch,
            value.digest(),
        );
    }
    
    let last_certified_time = self.last_certified_time();
    if value.expiration() > last_certified_time {
        // existing logic...
    }
    // ...
}
```

**Secondary Fix: Synchronize GC with Epoch Start**

Wait for GC completion before making the `BatchStore` operational:

```rust
if is_new_epoch {
    let gc_handle = tokio::task::spawn_blocking(move || {
        Self::gc_previous_epoch_batches_from_db_v1(db_clone.clone(), epoch);
        Self::gc_previous_epoch_batches_from_db_v2(db_clone, epoch);
    });
    // Block until GC completes
    gc_handle.await.expect("GC task failed");
}
```

**Tertiary Fix: Add Epoch to BatchCoordinator Validation**

Add epoch check in `handle_batches_msg()`:

```rust
pub(crate) async fn handle_batches_msg(&mut self, author: PeerId, batches: Vec<Batch<BatchInfoExt>>) {
    // Validate epoch matches current epoch
    if let Some(batch) = batches.first() {
        if batch.epoch() != self.current_epoch {
            error!("Batch from wrong epoch {} (expected {})", batch.epoch(), self.current_epoch);
            counters::RECEIVED_BATCH_WRONG_EPOCH.inc();
            return;
        }
    }
    // existing validation logic...
}
```

## Proof of Concept

The following test demonstrates the race condition (conceptual, requires test framework):

```rust
#[tokio::test]
async fn test_epoch_transition_batch_gc_race() {
    // Setup: Create BatchStore for epoch 1
    let db = Arc::new(MockQuorumStoreStorage::new());
    let batch_store_epoch1 = Arc::new(BatchStore::new(
        1, // epoch
        false, // not new epoch
        1000, // last_certified_time
        db.clone(),
        1000, 1000, 10,
        test_signer(),
        60_000_000,
    ));
    
    // Create and persist batch from epoch 1
    let batch_epoch1 = create_test_batch(1, 2000); // epoch 1, expiration 2000
    batch_store_epoch1.save(&batch_epoch1).unwrap();
    
    // Simulate epoch transition: Create new BatchStore for epoch 2 with is_new_epoch=true
    // This spawns GC to delete epoch < 2 batches
    let batch_store_epoch2 = Arc::new(BatchStore::new(
        2, // epoch 2
        true, // NEW EPOCH - triggers GC
        1500, // last_certified_time advanced
        db.clone(), // SHARED database
        1000, 1000, 10,
        test_signer(),
        60_000_000,
    ));
    
    // Race: Try to persist another epoch 1 batch while GC is running
    // This simulates a batch arriving late during epoch transition
    let late_batch_epoch1 = create_test_batch(1, 2000);
    
    // Without epoch validation, this succeeds
    let persist_result = batch_store_epoch2.save(&late_batch_epoch1);
    assert!(persist_result.is_ok()); // BUG: Should fail epoch check
    
    // Wait for GC to complete
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Try to fetch the batch - it's been GC'd from database
    let fetch_result = batch_store_epoch2.get_batch_from_local(late_batch_epoch1.digest());
    
    // If stored as PersistedOnly, fetch fails
    assert!(fetch_result.is_err()); // Batch deleted by GC despite successful save()
}
```

**Notes**

While `EpochManager::check_epoch()` provides epoch filtering at the network message level, this is insufficient defense-in-depth: [10](#0-9) 

The vulnerability exists because:
1. The database is shared across epoch transitions, creating a window where old and new `BatchStore` instances operate concurrently on the same storage
2. The asynchronous GC spawn provides no synchronization with ongoing persistence operations
3. No epoch validation exists at the persistence layer, violating defense-in-depth principles

This represents a fundamental design issue in epoch transition handling that should be addressed through the recommended mitigations.

### Citations

**File:** consensus/src/epoch_manager.rs (L752-752)
```rust
                self.quorum_store_storage.clone(),
```

**File:** consensus/src/epoch_manager.rs (L1641-1647)
```rust
            | ConsensusMsg::BatchMsg(_)
            | ConsensusMsg::BatchRequestMsg(_)
            | ConsensusMsg::SignedBatchInfo(_)
            | ConsensusMsg::ProofOfStoreMsg(_) => {
                let event: UnverifiedEvent = msg.into();
                if event.epoch()? == self.epoch() {
                    return Ok(Some(event));
```

**File:** consensus/src/quorum_store/batch_store.rs (L156-160)
```rust
        if is_new_epoch {
            tokio::task::spawn_blocking(move || {
                Self::gc_previous_epoch_batches_from_db_v1(db_clone.clone(), epoch);
                Self::gc_previous_epoch_batches_from_db_v2(db_clone, epoch);
            });
```

**File:** consensus/src/quorum_store/batch_store.rs (L181-210)
```rust
    fn gc_previous_epoch_batches_from_db_v1(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db.get_all_batches().expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L383-397)
```rust
            let value_to_be_stored = if self
                .peer_quota
                .entry(author)
                .or_insert(QuotaManager::new(
                    self.db_quota,
                    self.memory_quota,
                    self.batch_quota,
                ))
                .update_quota(value.num_bytes() as usize)?
                == StorageMode::PersistedOnly
            {
                PersistedValue::new(value.batch_info().clone(), None)
            } else {
                value.clone()
            };
```

**File:** consensus/src/quorum_store/batch_store.rs (L419-439)
```rust
    pub(crate) fn save(&self, value: &PersistedValue<BatchInfoExt>) -> anyhow::Result<bool> {
        let last_certified_time = self.last_certified_time();
        if value.expiration() > last_certified_time {
            fail_point!("quorum_store::save", |_| {
                // Skip caching and storing value to the db
                Ok(false)
            });
            counters::GAP_BETWEEN_BATCH_EXPIRATION_AND_CURRENT_TIME_WHEN_SAVE.observe(
                Duration::from_micros(value.expiration() - last_certified_time).as_secs_f64(),
            );

            return self.insert_to_cache(value);
        }
        counters::NUM_BATCH_EXPIRED_WHEN_SAVE.inc();
        bail!(
            "Incorrect expiration {} in epoch {}, last committed timestamp {}",
            value.expiration(),
            self.epoch(),
            last_certified_time,
        );
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L571-585)
```rust
    pub(crate) fn get_batch_from_local(
        &self,
        digest: &HashValue,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        if let Some(value) = self.db_cache.get(digest) {
            if value.payload_storage_mode() == StorageMode::PersistedOnly {
                self.get_batch_from_db(digest, value.batch_info().is_v2())
            } else {
                // Available in memory.
                Ok(value.clone())
            }
        } else {
            Err(ExecutorError::CouldNotGetData)
        }
    }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L173-245)
```rust
    pub(crate) async fn handle_batches_msg(
        &mut self,
        author: PeerId,
        batches: Vec<Batch<BatchInfoExt>>,
    ) {
        if let Err(e) = self.ensure_max_limits(&batches) {
            error!("Batch from {}: {}", author, e);
            counters::RECEIVED_BATCH_MAX_LIMIT_FAILED.inc();
            return;
        }

        let Some(batch) = batches.first() else {
            error!("Empty batch received from {}", author.short_str().as_str());
            return;
        };

        // Filter the transactions in the batches. If any transaction is rejected,
        // the message will be dropped, and all batches will be rejected.
        if self.transaction_filter_config.is_enabled() {
            let transaction_filter = &self.transaction_filter_config.batch_transaction_filter();
            for batch in batches.iter() {
                for transaction in batch.txns() {
                    if !transaction_filter.allows_transaction(
                        batch.batch_info().batch_id(),
                        batch.author(),
                        batch.digest(),
                        transaction,
                    ) {
                        error!(
                            "Transaction {}, in batch {}, from {}, was rejected by the filter. Dropping {} batches!",
                            transaction.committed_hash(),
                            batch.batch_info().batch_id(),
                            author.short_str().as_str(),
                            batches.len()
                        );
                        counters::RECEIVED_BATCH_REJECTED_BY_FILTER.inc();
                        return;
                    }
                }
            }
        }

        let approx_created_ts_usecs = batch
            .info()
            .expiration()
            .saturating_sub(self.batch_expiry_gap_when_init_usecs);

        if approx_created_ts_usecs > 0 {
            observe_batch(
                approx_created_ts_usecs,
                batch.author(),
                BatchStage::RECEIVED,
            );
        }

        let mut persist_requests = vec![];
        for batch in batches.into_iter() {
            // TODO: maybe don't message batch generator if the persist is unsuccessful?
            if let Err(e) = self
                .sender_to_batch_generator
                .send(BatchGeneratorCommand::RemoteBatch(batch.clone()))
                .await
            {
                warn!("Failed to send batch to batch generator: {}", e);
            }
            persist_requests.push(batch.into());
        }
        counters::RECEIVED_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        if author != self.my_peer_id {
            counters::RECEIVED_REMOTE_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        }
        self.persist_and_send_digests(persist_requests, approx_created_ts_usecs);
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L111-122)
```rust
    async fn request_and_wait_transactions(
        batches: Vec<(BatchInfo, Vec<PeerId>)>,
        block_timestamp: u64,
        batch_reader: Arc<dyn BatchReader>,
    ) -> ExecutorResult<Vec<SignedTransaction>> {
        let futures = Self::request_transactions(batches, block_timestamp, batch_reader);
        let mut all_txns = Vec::new();
        for result in futures::future::join_all(futures).await {
            all_txns.append(&mut result?);
        }
        Ok(all_txns)
    }
```
