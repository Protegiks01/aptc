# Audit Report

## Title
TOCTOU Race Condition in Transaction Accumulator Pruning Causes Proof Verification Failures

## Summary
A Time-Of-Check-Time-Of-Use (TOCTOU) race condition exists between the `error_if_ledger_pruned()` version check and the actual data read in `get_accumulator_root_hash()`. The pruner can delete transaction accumulator root hashes and nodes after a read operation passes the version check but before it accesses the data, causing legitimate proof verification operations to fail unexpectedly.

## Finding Description

The vulnerability manifests in the following sequence: [1](#0-0) 

When `get_accumulator_root_hash()` is called, it first validates the version using `error_if_ledger_pruned()`: [2](#0-1) 

This check reads `min_readable_version` atomically and ensures `version >= min_readable_version`. However, there is no lock or transaction held that prevents the pruner from updating `min_readable_version` and deleting data between this check and the actual data access.

The pruner manager updates `min_readable_version` **before** triggering the actual deletion: [3](#0-2) 

The pruner worker then asynchronously executes the deletion: [4](#0-3) 

This deletes both the cached root hash entries (line 151) and the underlying accumulator nodes (lines 166-167). When the original reader tries to access the data in `get_root_hash()`: [5](#0-4) 

If the schema returns `None` (line 132), it attempts to recompute from accumulator nodes, but those have also been deleted. The `HashReader` implementation returns an error: [6](#0-5) 

The RocksDB `get()` operation at line 222 in schemadb does **not** use snapshot isolation: [7](#0-6) 

This means reads see the current state, not a consistent snapshot from when the version check passed.

**Race Condition Timeline:**
1. Thread A (reader) checks: `version=850 >= min_readable_version=800` ✓ (passes)
2. Thread B (pruner manager) updates: `min_readable_version = 900`
3. Thread C (pruner worker) deletes versions [800, 900)
4. Thread A attempts to read version 850 → **fails with "does not exist" error**

## Impact Explanation

**Severity: High** (potentially approaching Critical under specific conditions)

This vulnerability affects multiple critical subsystems:

1. **State Synchronization Failures**: Nodes performing state sync rely on accumulator root hash verification. When this fails mid-verification, sync operations are interrupted, causing nodes to fall behind or repeatedly retry.

2. **Proof Verification Breakdown**: The accumulator root hash is essential for verifying `TransactionAccumulatorProof`, used throughout state sync and consensus operations. Failures break the trust chain for transaction verification.

3. **API Service Degradation**: Public API endpoints that serve proofs or verify transactions will return unexpected errors to clients, breaking applications that rely on these APIs.

4. **Validator Node Disruption**: If validators encounter this race during proof verification needed for consensus operations, it could cause temporary validator unavailability or slowdown.

While this does not directly cause consensus **safety** violations (all nodes run the same buggy code), it impacts **liveness** and **availability**. Under heavy load with aggressive pruning, this could affect multiple validators simultaneously, approaching the Critical category of "Total loss of liveness/network availability."

The bug violates **Invariant #4: State Consistency** - state transitions must be verifiable via Merkle proofs, but this race makes proof verification non-deterministically fail for valid versions.

## Likelihood Explanation

**Likelihood: Medium**

While the race window is narrow (microseconds between check and read), several factors increase occurrence probability:

1. **Continuous Operation**: Pruning runs continuously on active nodes with finite retention windows
2. **High Query Volume**: API nodes and syncing nodes frequently query historical accumulator roots
3. **Burst Pruning**: When a node catches up after downtime, it may prune large ranges rapidly, widening the race window
4. **No Synchronization**: The complete lack of coordination between readers and the pruner means races are purely probabilistic

In production networks with aggressive pruning (short retention windows) and high API traffic, this race could manifest multiple times per hour across the network. State sync operations are particularly vulnerable as they make repeated accumulator queries during chunk verification.

## Recommendation

Implement proper synchronization between read operations and pruning. The recommended fix uses **read-write locks** or **epoch-based reclamation**:

**Option 1: RocksDB Snapshot Isolation**
Modify `get_accumulator_root_hash()` to use RocksDB snapshots that are captured atomically with the version check, ensuring a consistent view throughout the read operation.

**Option 2: Deferred Pruning with Grace Period**
Introduce a grace period where `min_readable_version` is updated well before actual deletion occurs. For example:
- Maintain `min_readable_version` (advertised to readers)
- Maintain `actual_pruning_version` (lagging by grace period, e.g., 1000 versions)
- Only prune up to `actual_pruning_version`
- This ensures that data remains available for a safety margin after readers see the updated `min_readable_version`

**Option 3: Atomic Check-and-Read**
Hold a read lock during the entire `get_accumulator_root_hash()` operation, preventing the pruner from deleting data while any reader holds a reference to a version.

## Proof of Concept

```rust
// Reproduction test demonstrating the race condition
// Add to storage/aptosdb/src/pruner/ledger_pruner/test.rs

#[test]
fn test_race_condition_accumulator_root_hash_pruning() {
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    let tmpdir = aptos_temppath::TempPath::new();
    let db = setup_db(&tmpdir);
    
    // Commit transactions 0-999
    for i in 0..1000 {
        commit_transaction(&db, i);
    }
    
    // Configure aggressive pruning
    let prune_window = 100;
    db.ledger_pruner.set_prune_window(prune_window);
    
    let db_clone = Arc::clone(&db);
    let barrier = Arc::new(Barrier::new(2));
    let barrier_clone = Arc::clone(&barrier);
    
    // Reader thread: tries to read version 850
    let reader = thread::spawn(move || {
        barrier_clone.wait();
        
        // This should pass the check when min_readable_version=800
        let result = db_clone.get_accumulator_root_hash(850);
        result
    });
    
    // Pruner thread: updates min_readable_version and prunes
    let pruner = thread::spawn(move || {
        barrier.wait();
        
        // Update min_readable_version to 900 and trigger pruning
        db.ledger_pruner.maybe_set_pruner_target_db_version(1000);
        thread::sleep(Duration::from_millis(1)); // Let pruner run
    });
    
    let reader_result = reader.join().unwrap();
    pruner.join().unwrap();
    
    // Race condition: reader may fail even though version 850 was valid
    // when it passed the check
    match reader_result {
        Err(e) if e.to_string().contains("does not exist") => {
            println!("RACE CONDITION TRIGGERED: Reader failed despite passing version check");
            panic!("Vulnerability confirmed");
        },
        Ok(_) => println!("No race in this execution (timing dependent)"),
        Err(e) if e.to_string().contains("pruned") => {
            println!("Check failed (expected in some timings)");
        },
        Err(e) => panic!("Unexpected error: {}", e),
    }
}
```

## Notes

This vulnerability is a classic TOCTOU race condition in the storage layer. While it doesn't directly compromise consensus **safety** (byzantine fault tolerance), it severely impacts **liveness** and **availability** - both critical for blockchain operation. The lack of any synchronization mechanism between the version check and data access means this race can occur during normal operations without any malicious actor involvement.

The issue is particularly concerning because:
1. It affects state sync, a critical operation for new nodes and recovering validators
2. It impacts proof verification, which is fundamental to blockchain trust
3. The failure mode is non-deterministic, making it difficult to diagnose in production
4. Multiple nodes could be affected simultaneously during periods of high pruning activity

The fix requires careful consideration of performance implications, as adding synchronization to hot read paths could impact throughput. The recommended grace period approach (Option 2) provides safety without significantly impacting read performance.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L832-838)
```rust
    fn get_accumulator_root_hash(&self, version: Version) -> Result<HashValue> {
        gauged_api("get_accumulator_root_hash", || {
            self.error_if_ledger_pruned("Transaction accumulator", version)?;
            self.ledger_db
                .transaction_accumulator_db()
                .get_root_hash(version)
        })
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L261-270)
```rust
    pub(super) fn error_if_ledger_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.ledger_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L162-176)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L129-137)
```rust
    pub fn get_root_hash(&self, version: Version) -> Result<HashValue> {
        if let Some(hash) = self
            .db
            .get::<TransactionAccumulatorRootHashSchema>(&version)?
        {
            return Ok(hash);
        }
        Accumulator::get_root_hash(self, version + 1).map_err(Into::into)
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L149-172)
```rust
    pub(crate) fn prune(begin: Version, end: Version, db_batch: &mut SchemaBatch) -> Result<()> {
        for version_to_delete in begin..end {
            db_batch.delete::<TransactionAccumulatorRootHashSchema>(&version_to_delete)?;
            // The even version will be pruned in the iteration of version + 1.
            if version_to_delete % 2 == 0 {
                continue;
            }

            let first_ancestor_that_is_a_left_child =
                Self::find_first_ancestor_that_is_a_left_child(version_to_delete);

            // This assertion is true because we skip the leaf nodes with address which is a
            // a multiple of 2.
            assert!(!first_ancestor_that_is_a_left_child.is_leaf());

            let mut current = first_ancestor_that_is_a_left_child;
            while !current.is_leaf() {
                db_batch.delete::<TransactionAccumulatorSchema>(&current.left_child())?;
                db_batch.delete::<TransactionAccumulatorSchema>(&current.right_child())?;
                current = current.right_child();
            }
        }
        Ok(())
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L195-201)
```rust
impl HashReader for TransactionAccumulatorDb {
    fn get(&self, position: Position) -> Result<HashValue, anyhow::Error> {
        self.db
            .get::<TransactionAccumulatorSchema>(&position)?
            .ok_or_else(|| anyhow!("{} does not exist.", position))
    }
}
```

**File:** storage/schemadb/src/lib.rs (L216-232)
```rust
    pub fn get<S: Schema>(&self, schema_key: &S::Key) -> DbResult<Option<S::Value>> {
        let _timer = APTOS_SCHEMADB_GET_LATENCY_SECONDS.timer_with(&[S::COLUMN_FAMILY_NAME]);

        let k = <S::Key as KeyCodec<S>>::encode_key(schema_key)?;
        let cf_handle = self.get_cf_handle(S::COLUMN_FAMILY_NAME)?;

        let result = self.inner.get_cf(cf_handle, k).into_db_res()?;
        APTOS_SCHEMADB_GET_BYTES.observe_with(
            &[S::COLUMN_FAMILY_NAME],
            result.as_ref().map_or(0.0, |v| v.len() as f64),
        );

        result
            .map(|raw_value| <S::Value as ValueCodec<S>>::decode_value(&raw_value))
            .transpose()
            .map_err(Into::into)
    }
```
