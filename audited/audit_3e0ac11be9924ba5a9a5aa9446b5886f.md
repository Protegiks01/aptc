# Audit Report

## Title
Indexer gRPC Service Crash Due to Missing Blob Files Despite Valid Metadata

## Summary
The indexer-grpc file store system lacks consistency verification between `FileStoreMetadata.version` and the actual existence of blob files. When blob files are manually deleted or lost due to cloud storage failures, the metadata continues to reference non-existent versions, causing the indexer service to panic when attempting to serve historical transactions to clients.

## Finding Description

The indexer-grpc system stores transaction data in blob files (cloud storage) and maintains a metadata file tracking the current version. The vulnerability occurs in the file reading path where the system assumes all blob files referenced by metadata must exist.

**Critical Code Path:**

When an external indexer requests historical transactions, the flow is:

1. `HistoricalDataService` receives a request for transactions starting at version Y [1](#0-0) 

2. `FileStoreReader.get_transaction_batch()` is called to fetch the transactions [2](#0-1) 

3. The method calls `get_transaction_file_at_version()` which attempts to read the blob file [3](#0-2) 

4. **Panic occurs** if the file doesn't exist after retries [4](#0-3) 

**The Inconsistency Problem:**

The metadata update process does not verify blob file existence: [5](#0-4) 

The system uploads blobs and updates metadata separately with no atomicity guarantee or post-update verification: [6](#0-5) 

**Triggering Scenario:**
1. Normal operation: Blobs for versions [0-999], [1000-1999], [2000-2999] are uploaded
2. Metadata is updated to `version: 3000`
3. Cloud storage incident: Blob file for [1000-1999] is lost or manually deleted
4. External indexer requests transactions starting from version 1500
5. Service reads metadata (version=3000), confirms it can serve version 1500
6. Service attempts to read blob file for version 1000
7. File doesn't exist â†’ **Panic: "File should exist: {path}"**
8. Spawned task crashes, indexer client receives no response

## Impact Explanation

This qualifies as **High Severity** per the Aptos bug bounty criteria:
- **API crashes**: The indexer gRPC service spawned task panics, terminating the client streaming session
- **Service disruption**: External indexers cannot sync historical data from affected version ranges
- **Cascading failures**: Multiple indexer clients attempting to resume from the missing range will all fail

While this doesn't affect consensus or validator operations (the indexer-grpc system is auxiliary), it breaks the availability guarantee for the historical data API, which is critical for ecosystem applications relying on blockchain indexing.

## Likelihood Explanation

**Likelihood: Medium to High**

This can occur through:
1. **Operational errors**: Administrator manually deleting old blobs for storage management without updating metadata
2. **Cloud storage failures**: GCS data loss incidents (rare but documented)
3. **Backup/restore operations**: Incomplete restore that recovers metadata but not all blobs
4. **Storage quota policies**: Automated cleanup scripts removing old blobs

The vulnerability does NOT require:
- Attacker access to GCS (though someone with such access could exploit it)
- Code modifications
- Network attacks

The lack of consistency checks and graceful degradation makes this likely to cause service disruption in production environments with long-running deployments.

## Recommendation

Implement a multi-layered defense:

**1. Add blob existence verification when updating metadata:**
```rust
async fn update_file_store_metadata(&self, version: u64) -> Result<()> {
    // Verify critical blobs exist before updating metadata
    let num_blobs_to_verify = 5; // Verify recent blobs
    for i in 0..num_blobs_to_verify {
        let check_version = version.saturating_sub((i + 1) * NUM_TXNS_PER_FOLDER);
        if check_version > 0 {
            let batch_metadata = self.reader.get_batch_metadata(check_version).await;
            if let Some(metadata) = batch_metadata {
                // Verify first file in batch exists
                let first_file_version = metadata.files.first().unwrap().first_version;
                let path = self.reader.get_path_for_version(first_file_version, metadata.suffix);
                if self.writer.get_raw_file(path).await?.is_none() {
                    anyhow::bail!("Blob verification failed: missing file for version {}", first_file_version);
                }
            }
        }
    }
    
    // Proceed with metadata update
    FILE_STORE_VERSION.set(version as i64);
    // ... rest of existing code
}
```

**2. Replace panic with graceful error handling:**
```rust
async fn get_transaction_file_at_version(
    &self,
    version: u64,
    suffix: Option<u64>,
    retries: u8,
) -> Result<Vec<Transaction>> {
    let mut retries = retries;
    let bytes = loop {
        let path = self.get_path_for_version(version, suffix);
        match self.reader.get_raw_file(path.clone()).await {
            Ok(Some(bytes)) => break bytes,
            Ok(None) => {
                // Return error instead of panic
                return Err(anyhow::anyhow!(
                    "Blob file missing for version {}: {:?}. Metadata may be inconsistent.",
                    version, path
                ));
            },
            Err(err) => {
                if retries == 0 {
                    return Err(err);
                }
                retries -= 1;
                tokio::time::sleep(std::time::Duration::from_millis(10)).await;
            },
        }
    };
    // ... rest of existing code
}
```

**3. Add periodic consistency verification background task:**
- Scan metadata periodically
- Verify blob files exist for all referenced versions
- Alert operators of inconsistencies
- Optionally rebuild metadata from actual blob inventory

## Proof of Concept

**Rust reproduction steps:**

```rust
#[tokio::test]
async fn test_metadata_blob_inconsistency() {
    use tempfile::TempDir;
    use std::path::PathBuf;
    
    // Setup local file store
    let temp_dir = TempDir::new().unwrap();
    let file_store = LocalFileStore::new(temp_dir.path().to_path_buf());
    
    // Initialize file store with metadata
    let metadata = FileStoreMetadata {
        chain_id: 1,
        num_transactions_per_folder: 100000,
        version: 3000,
    };
    file_store.save_raw_file(
        PathBuf::from("metadata.json"),
        serde_json::to_vec(&metadata).unwrap()
    ).await.unwrap();
    
    // Create batch metadata referencing blob that doesn't exist
    let batch_metadata = BatchMetadata {
        files: vec![FileMetadata {
            first_version: 1000,
            last_version: 1999,
            size_bytes: 1024,
        }],
        suffix: None,
    };
    file_store.save_raw_file(
        PathBuf::from("0/metadata.json"),
        serde_json::to_vec(&batch_metadata).unwrap()
    ).await.unwrap();
    
    // DO NOT create the actual blob file for version 1000
    // Simulating deletion or loss
    
    // Try to read transactions
    let reader = FileStoreReader::new(1, Arc::new(file_store)).await;
    
    // This will PANIC with "File should exist"
    let result = reader.get_transaction_file_at_version(1000, None, 3).await;
    assert!(result.is_err(), "Expected error due to missing blob file");
    
    // Verify the error message indicates missing file
    let err_msg = format!("{:?}", result.unwrap_err());
    assert!(err_msg.contains("File should exist") || err_msg.contains("missing"));
}
```

## Notes

This vulnerability represents a **data consistency failure** between metadata and actual blob storage. While it requires elevated access (GCS permissions) or infrastructure failures to trigger, the lack of defensive checks makes the system fragile. The panic-based error handling converts operational issues into service outages, violating the availability guarantees expected from a production indexing service.

The fix should focus on:
1. Defensive verification before metadata updates
2. Graceful degradation instead of panics
3. Operational tools for detecting and recovering from inconsistencies

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L170-179)
```rust
                file_store_reader
                    .get_transaction_batch(
                        next_version,
                        /*retries=*/ 3,
                        /*max_files=*/ None,
                        filter,
                        Some(ending_version),
                        tx,
                    )
                    .await;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L77-95)
```rust
    pub async fn get_transaction_batch(
        &self,
        version: u64,
        retries: u8,
        max_files: Option<usize>,
        filter: Option<BooleanTransactionFilter>,
        ending_version: Option<u64>,
        tx: Sender<(Vec<Transaction>, usize, Timestamp, (u64, u64))>,
    ) {
        trace!(
            "Getting transactions from file store, version: {version}, max_files: {max_files:?}."
        );
        let batch_metadata = self.get_batch_metadata(version).await;
        if batch_metadata.is_none() {
            // TODO(grao): This is unexpected, should only happen when data is corrupted. Consider
            // make it panic!.
            error!("Failed to get the batch metadata, unable to serve the request.");
            return;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L213-232)
```rust
    async fn get_transaction_file_at_version(
        &self,
        version: u64,
        suffix: Option<u64>,
        retries: u8,
    ) -> Result<Vec<Transaction>> {
        let mut retries = retries;
        let bytes = loop {
            let path = self.get_path_for_version(version, suffix);
            match self.reader.get_raw_file(path.clone()).await {
                Ok(bytes) => break bytes.unwrap_or_else(|| panic!("File should exist: {path:?}.")),
                Err(err) => {
                    if retries == 0 {
                        return Err(err);
                    }
                    retries -= 1;
                    tokio::time::sleep(std::time::Duration::from_millis(10)).await;
                },
            }
        };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L183-259)
```rust
    async fn do_upload(
        &mut self,
        transactions: Vec<Transaction>,
        batch_metadata: BatchMetadata,
        end_batch: bool,
    ) -> Result<()> {
        let _timer = TIMER.with_label_values(&["do_upload"]).start_timer();

        let first_version = transactions.first().unwrap().version;
        let last_version = transactions.last().unwrap().version;
        let data_file = {
            let _timer = TIMER
                .with_label_values(&["do_upload__prepare_file"])
                .start_timer();
            FileEntry::from_transactions(transactions, StorageFormat::Lz4CompressedProto)
        };
        let path = self.reader.get_path_for_version(first_version, None);

        info!("Dumping transactions [{first_version}, {last_version}] to file {path:?}.");

        {
            let _timer = TIMER
                .with_label_values(&["do_upload__save_file"])
                .start_timer();
            self.writer
                .save_raw_file(path, data_file.into_inner())
                .await?;
        }

        let mut update_batch_metadata = false;
        let max_update_frequency = self.writer.max_update_frequency();
        if self.last_batch_metadata_update_time.is_none()
            || Instant::now() - self.last_batch_metadata_update_time.unwrap()
                >= MIN_UPDATE_FREQUENCY
        {
            update_batch_metadata = true;
        } else if end_batch {
            update_batch_metadata = true;
            tokio::time::sleep_until(
                self.last_batch_metadata_update_time.unwrap() + max_update_frequency,
            )
            .await;
        }

        if !update_batch_metadata {
            return Ok(());
        }

        let batch_metadata_path = self.reader.get_path_for_batch_metadata(first_version);
        {
            let _timer = TIMER
                .with_label_values(&["do_upload__update_batch_metadata"])
                .start_timer();
            self.writer
                .save_raw_file(
                    batch_metadata_path,
                    serde_json::to_vec(&batch_metadata).map_err(anyhow::Error::msg)?,
                )
                .await?;
        }

        if end_batch {
            self.last_batch_metadata_update_time = None;
        } else {
            self.last_batch_metadata_update_time = Some(Instant::now());
        }

        if Instant::now() - self.last_metadata_update_time >= max_update_frequency {
            let _timer = TIMER
                .with_label_values(&["do_upload__update_metadata"])
                .start_timer();
            self.update_file_store_metadata(last_version + 1).await?;
            self.last_metadata_update_time = Instant::now();
        }

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L261-274)
```rust
    /// Updates the file store metadata.
    async fn update_file_store_metadata(&self, version: u64) -> Result<()> {
        FILE_STORE_VERSION.set(version as i64);
        let metadata = FileStoreMetadata {
            chain_id: self.chain_id,
            num_transactions_per_folder: NUM_TXNS_PER_FOLDER,
            version,
        };

        let raw_data = serde_json::to_vec(&metadata).map_err(anyhow::Error::msg)?;
        self.writer
            .save_raw_file(PathBuf::from(METADATA_FILE_NAME), raw_data)
            .await
    }
```
