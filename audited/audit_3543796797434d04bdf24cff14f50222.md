# Audit Report

## Title
HTTP Connection Pool Exhaustion via Public Client Field in ApiIndexProvider

## Summary
The `ApiIndexProvider` exposes its internal `reqwest::Client` as a public field, allowing external code to bypass the `OutputCache` protection mechanism and make unbounded concurrent HTTP requests. This can exhaust the connection pool, causing legitimate `provide()` calls to timeout or fail unpredictably.

## Finding Description

The `ApiIndexProvider` struct contains a public `client` field that provides direct access to the underlying `reqwest::Client`. [1](#0-0) 

This design explicitly acknowledges the security concern in comments, describing it as "backdoor access" that is "ideally avoidable". The `provide()` method is protected by an `OutputCache` mechanism [2](#0-1)  that prevents excessive API calls within a configurable TTL window using locks to prevent concurrent fetches. [3](#0-2) 

However, code with access to the `ApiIndexProvider` instance can bypass this protection entirely by accessing the `client` field directly. This is already happening in the TPS checker: [4](#0-3) 

The underlying `reqwest::Client` uses a connection pool per client instance. [5](#0-4)  The client is configured with only a 10-second timeout and no explicit connection pool limits. [6](#0-5) 

**Attack Scenario:**
A malicious or buggy checker (or any code using the node-checker library, [7](#0-6) ) could spawn hundreds or thousands of concurrent requests using the public `client` field. These requests would compete for the same connection pool used by legitimate `provide()` calls, causing timeouts and failures.

## Impact Explanation

This vulnerability represents a **Medium severity** issue under the Aptos bug bounty criteria for the following reasons:

1. **Resource Exhaustion**: The vulnerability enables connection pool exhaustion attacks that can cause the node-checker service to fail unpredictably
2. **State Inconsistencies**: When `provide()` calls fail due to connection exhaustion, the node-checker cannot properly validate node health, leading to inconsistent state reporting that may require manual intervention
3. **Service Availability**: The node-checker is critical infrastructure for validator operators to monitor node health and compliance

While this doesn't directly impact consensus or cause loss of funds, it affects critical monitoring infrastructure that validators depend on for operational security.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability is likely to occur because:

1. **Existing Usage**: The TPS checker already accesses the client directly, demonstrating this pattern is in active use
2. **Library Exposure**: The node-checker is exported as a library crate, allowing external code to import and use it
3. **No Safeguards**: There are no connection pool limits, rate limiting, or circuit breakers configured
4. **Public API**: The `client` field and `ApiIndexProvider` struct are both public, making this an intentional (though acknowledged as problematic) part of the API

The comment explicitly acknowledges this as an "ideally avoidable" pattern, indicating the developers recognize the risk but have not mitigated it.

## Recommendation

**Solution 1: Make the client field private**

Change the `client` field from public to private in `ApiIndexProvider`:

```rust
#[derive(Clone, Debug)]
pub struct ApiIndexProvider {
    pub config: ApiIndexProviderConfig,
    
    // Make this private
    client: Client,
    
    output_cache: Arc<OutputCache<IndexResponse>>,
}
```

If specific "backdoor" functionality is legitimately needed (like building URLs in the TPS checker), provide dedicated public methods:

```rust
impl ApiIndexProvider {
    /// Get the base URL for this API endpoint
    pub fn get_base_url(&self) -> Url {
        self.client.build_path("/").unwrap()
    }
}
```

**Solution 2: Add connection pool limits**

Configure explicit connection pool limits in the `ClientBuilder`: [6](#0-5) 

```rust
pub fn build(self) -> Client {
    let version_path_base = get_version_path_with_base(self.base_url.clone());
    
    Client {
        inner: self
            .reqwest_builder
            .default_headers(self.headers)
            .timeout(self.timeout)
            .pool_max_idle_per_host(10) // Limit idle connections
            .pool_idle_timeout(Duration::from_secs(90))
            .cookie_store(true)
            .build()
            .unwrap(),
        base_url: self.base_url,
        version_path_base,
    }
}
```

**Recommended approach**: Implement both solutions for defense in depth.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;
    use tokio::time::Duration;
    
    #[tokio::test]
    async fn test_connection_pool_exhaustion() {
        // Create a provider instance
        let config = ApiIndexProviderConfig::default();
        let client = Client::new(Url::parse("http://localhost:8080").unwrap());
        let provider = Arc::new(ApiIndexProvider::new(config, client));
        
        // Spawn many concurrent requests using direct client access
        let mut handles = vec![];
        for _ in 0..1000 {
            let provider_clone = provider.clone();
            handles.push(tokio::spawn(async move {
                // Direct client access bypasses OutputCache
                let _ = provider_clone.client.get_index().await;
            }));
        }
        
        // Meanwhile, try to use the provider normally
        let legitimate_call = provider.provide();
        
        // The legitimate call may timeout or fail due to connection exhaustion
        match tokio::time::timeout(Duration::from_secs(15), legitimate_call).await {
            Ok(Ok(_)) => println!("Legitimate call succeeded"),
            Ok(Err(e)) => println!("Legitimate call failed: {}", e),
            Err(_) => println!("Legitimate call timed out - connection pool exhausted"),
        }
        
        // Wait for spawned tasks
        for handle in handles {
            let _ = handle.await;
        }
    }
}
```

This PoC demonstrates how concurrent direct client access can impact legitimate `provide()` calls by competing for the same connection pool resources.

## Notes

The vulnerability is explicitly acknowledged in the codebase comments as "backdoor access" that is "ideally avoidable", indicating awareness of the design flaw. [8](#0-7)  The fact that this pattern is already in use by the TPS checker demonstrates this is not a theoretical concern but an active pattern that could be abused.

### Citations

**File:** ecosystem/node-checker/src/provider/api_index.rs (L28-31)
```rust
    // We make this public to allow convenient (but ideally avoidable) backdoor
    // access to the client for use cases where the memoization / retrying support
    // offered by a Provider isn't relevant.
    pub client: Client,
```

**File:** ecosystem/node-checker/src/provider/api_index.rs (L55-64)
```rust
    async fn provide(&self) -> Result<Self::Output, ProviderError> {
        self.output_cache
            .get(
                self.client
                    .get_index()
                    .map_ok(|r| r.into_inner())
                    .map_err(|e| ProviderError::RetryableEndpointError("/", e.into())),
            )
            .await
    }
```

**File:** ecosystem/node-checker/src/provider/cache.rs (L35-54)
```rust
    pub async fn get(
        &self,
        func: impl Future<Output = Result<T, ProviderError>>,
    ) -> Result<T, ProviderError> {
        // If the cache isn't too old and there is a value, return it.
        if self.last_run.read().await.elapsed() < self.cache_ttl {
            if let Some(last_output) = &*self.last_output.read().await {
                return Ok(last_output.clone());
            }
        }

        // Otherwise fetch the value and update the cache. We take the locks while
        // fetching the new value so we don't waste effort fetching it multiple times.
        let mut last_output = self.last_output.write().await;
        let mut last_run = self.last_run.write().await;
        let new_output = func.await?;
        *last_output = Some(new_output.clone());
        *last_run = Instant::now();
        Ok(new_output)
    }
```

**File:** ecosystem/node-checker/src/checker/tps.rs (L118-118)
```rust
        let target_url = target_api_index_provider.client.build_path("/").unwrap();
```

**File:** crates/aptos-rest-client/src/lib.rs (L81-85)
```rust
pub struct Client {
    inner: ReqwestClient,
    base_url: Url,
    version_path_base: String,
}
```

**File:** crates/aptos-rest-client/src/client_builder.rs (L95-109)
```rust
    pub fn build(self) -> Client {
        let version_path_base = get_version_path_with_base(self.base_url.clone());

        Client {
            inner: self
                .reqwest_builder
                .default_headers(self.headers)
                .timeout(self.timeout)
                .cookie_store(true)
                .build()
                .unwrap(),
            base_url: self.base_url,
            version_path_base,
        }
    }
```

**File:** ecosystem/node-checker/Cargo.toml (L40-42)
```text
[lib]
name = "aptos_node_checker_lib"
path = "src/lib.rs"
```
