# Audit Report

## Title
Unvalidated Duration Ratio in QuorumStore Backpressure Configuration Enables Resource Exhaustion Through Misconfiguration

## Summary
The `QuorumStoreBackPressureConfig` struct lacks validation for the ratio between `increase_duration_ms` and `decrease_duration_ms`, allowing extreme ratios (e.g., 1:1,000,000) that break the AIMD backpressure algorithm's ability to respond to congestion, potentially causing validator node resource exhaustion and consensus degradation.

## Finding Description

The QuorumStore backpressure system uses an Additive Increase Multiplicative Decrease (AIMD) algorithm to dynamically adjust transaction pull rates. The configuration structure defines two critical timing parameters without ratio validation: [1](#0-0) 

The AIMD algorithm implementation adjusts `dynamic_pull_txn_per_s` based on backpressure signals: [2](#0-1) 

When backpressured (`txn_count == true`), the rate should decrease multiplicatively: [3](#0-2) 

When not backpressured, the rate increases additively: [4](#0-3) 

**Exploitation Path:**

With a malicious/erroneous configuration setting `increase_duration_ms = 1` and `decrease_duration_ms = 1000000`:

1. System starts at `(160 + 12000)/2 = 6080` txns/s
2. Rate increases every 1ms by 2000 txns/s, quickly reaching 12,000 txns/s maximum
3. High transaction volume triggers backpressure when `remaining_total_txn_num > 36,000`: [5](#0-4) 

4. **Critical Issue:** Rate decrease only occurs every 1,000,000ms (16.67 minutes), but the system continues pulling 12,000 txns/s during this entire period
5. During 16.67 minutes: ~12,000,000 transactions pulled from mempool and stored in `batches_in_progress` HashMap with no hard limit: [6](#0-5) 

6. When batch quota is exceeded, persists fail silently, but batch generation continues: [7](#0-6) [8](#0-7) 

7. Memory exhaustion occurs as `batches_in_progress` grows unbounded, eventually crashing the validator node

## Impact Explanation

**Severity Assessment: Medium (Configuration Hardening Issue)**

While this issue can cause validator node crashes and consensus degradation, it **fails the "unprivileged attacker" requirement** from the validation checklist. This configuration can only be set by:
- Node operators with filesystem access to node configuration
- Administrators deploying validator nodes

According to the trust model, validator operators are trusted actors. This makes it a **configuration validation/hardening issue** rather than an exploitable vulnerability by untrusted actors.

However, it represents a genuine risk for:
- Accidental misconfigurations during deployment
- Human error in parameter tuning
- Lack of defense-in-depth against operator mistakes

The impact would include:
- Single validator node unavailability (not network-wide)
- Memory exhaustion requiring node restart
- Temporary consensus participation loss
- Potential slashing if downtime exceeds thresholds

## Likelihood Explanation

**Likelihood: Low to Medium**

- Requires operator access to modify node configuration (high barrier)
- Default values are safe (both 1000ms), reducing accidental misconfiguration risk
- No validation or warnings in config sanitizer to catch extreme ratios
- Operators tuning for performance could accidentally create dangerous ratios
- No runtime detection or circuit breakers to prevent resource exhaustion

## Recommendation

Add configuration validation in the `ConfigSanitizer` implementation to enforce reasonable duration ratios:

```rust
impl ConfigSanitizer for QuorumStoreConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        // Existing validations...
        Self::sanitize_send_recv_batch_limits(&sanitizer_name, &node_config.consensus.quorum_store)?;
        Self::sanitize_batch_total_limits(&sanitizer_name, &node_config.consensus.quorum_store)?;

        // NEW: Validate backpressure duration ratio
        let bp = &node_config.consensus.quorum_store.back_pressure;
        let max_ratio = 1000; // Allow up to 1000:1 ratio
        
        if bp.increase_duration_ms == 0 || bp.decrease_duration_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "Backpressure durations must be non-zero".to_string(),
            ));
        }
        
        let ratio = bp.increase_duration_ms.max(bp.decrease_duration_ms) 
                  / bp.increase_duration_ms.min(bp.decrease_duration_ms);
        
        if ratio > max_ratio {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                format!("Backpressure duration ratio {} exceeds maximum {}", ratio, max_ratio),
            ));
        }

        Ok(())
    }
}
```

Additionally, add runtime safeguards:
1. Hard limit on `batches_in_progress` size with circuit breaker
2. Alert/metric when backpressure persists beyond threshold
3. Document safe ratio ranges in configuration comments

## Proof of Concept

```rust
#[test]
fn test_extreme_backpressure_duration_ratio() {
    use aptos_config::config::{ConsensusConfig, NodeConfig, QuorumStoreConfig, QuorumStoreBackPressureConfig};
    use aptos_types::chain_id::ChainId;
    
    // Create a node config with extreme duration ratio
    let node_config = NodeConfig {
        consensus: ConsensusConfig {
            quorum_store: QuorumStoreConfig {
                back_pressure: QuorumStoreBackPressureConfig {
                    increase_duration_ms: 1,
                    decrease_duration_ms: 1000000,
                    ..Default::default()
                },
                ..Default::default()
            },
            ..Default::default()
        },
        ..Default::default()
    };

    // This should fail validation but currently passes
    let result = QuorumStoreConfig::sanitize(
        &node_config,
        NodeType::Validator,
        Some(ChainId::mainnet()),
    );
    
    // Currently passes when it should fail
    assert!(result.is_ok(), "Extreme ratio not detected");
    
    // With fix, this should return ConfigSanitizerFailed error
}
```

## Notes

This finding represents a **configuration hardening recommendation** rather than a directly exploitable vulnerability, as it requires privileged operator access. While it doesn't meet the strict criteria for a security bounty (requires trusted actor compromise), it represents important defense-in-depth that prevents operator mistakes from causing node failures.

The lack of validation violates best practices for safety-critical system configuration and should be addressed to improve operational robustness.

### Citations

**File:** config/src/config/quorum_store_config.rs (L16-27)
```rust
#[derive(Clone, Copy, Debug, Deserialize, PartialEq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct QuorumStoreBackPressureConfig {
    pub backlog_txn_limit_count: u64,
    pub backlog_per_validator_batch_limit_count: u64,
    pub decrease_duration_ms: u64,
    pub increase_duration_ms: u64,
    pub decrease_fraction: f64,
    pub dynamic_min_txn_per_s: u64,
    pub dynamic_max_txn_per_s: u64,
    pub additive_increase_when_no_backpressure: u64,
}
```

**File:** consensus/src/quorum_store/batch_generator.rs (L68-75)
```rust
    batches_in_progress: HashMap<(PeerId, BatchId), BatchInProgress>,
    txns_in_progress_sorted: BTreeMap<TransactionSummary, TransactionInProgress>,
    batch_expirations: TimeExpirations<(PeerId, BatchId)>,
    latest_block_timestamp: u64,
    last_end_batch_time: Instant,
    // quorum store back pressure, get updated from proof manager
    back_pressure: BackPressure,
}
```

**File:** consensus/src/quorum_store/batch_generator.rs (L413-422)
```rust
        let back_pressure_decrease_duration =
            Duration::from_millis(self.config.back_pressure.decrease_duration_ms);
        let back_pressure_increase_duration =
            Duration::from_millis(self.config.back_pressure.increase_duration_ms);
        let mut back_pressure_decrease_latest = start;
        let mut back_pressure_increase_latest = start;
        let mut dynamic_pull_txn_per_s = (self.config.back_pressure.dynamic_min_txn_per_s
            + self.config.back_pressure.dynamic_max_txn_per_s)
            / 2;

```

**File:** consensus/src/quorum_store/batch_generator.rs (L434-443)
```rust
                    if self.back_pressure.txn_count {
                        // multiplicative decrease, every second
                        if back_pressure_decrease_latest.elapsed() >= back_pressure_decrease_duration {
                            back_pressure_decrease_latest = tick_start;
                            dynamic_pull_txn_per_s = std::cmp::max(
                                (dynamic_pull_txn_per_s as f64 * self.config.back_pressure.decrease_fraction) as u64,
                                self.config.back_pressure.dynamic_min_txn_per_s,
                            );
                            trace!("QS: dynamic_max_pull_txn_per_s: {}", dynamic_pull_txn_per_s);
                        }
```

**File:** consensus/src/quorum_store/batch_generator.rs (L448-456)
```rust
                        // additive increase, every second
                        if back_pressure_increase_latest.elapsed() >= back_pressure_increase_duration {
                            back_pressure_increase_latest = tick_start;
                            dynamic_pull_txn_per_s = std::cmp::min(
                                dynamic_pull_txn_per_s + self.config.back_pressure.additive_increase_when_no_backpressure,
                                self.config.back_pressure.dynamic_max_txn_per_s,
                            );
                            trace!("QS: dynamic_max_pull_txn_per_s: {}", dynamic_pull_txn_per_s);
                        }
```

**File:** consensus/src/quorum_store/proof_manager.rs (L245-265)
```rust
    pub(crate) fn qs_back_pressure(&self) -> BackPressure {
        if self.remaining_total_txn_num > self.back_pressure_total_txn_limit
            || self.remaining_total_proof_num > self.back_pressure_total_proof_limit
        {
            sample!(
                SampleRate::Duration(Duration::from_millis(200)),
                info!(
                    "Quorum store is back pressured with {} txns, limit: {}, proofs: {}, limit: {}",
                    self.remaining_total_txn_num,
                    self.back_pressure_total_txn_limit,
                    self.remaining_total_proof_num,
                    self.back_pressure_total_proof_limit
                );
            );
        }

        BackPressure {
            txn_count: self.remaining_total_txn_num > self.back_pressure_total_txn_limit,
            proof_count: self.remaining_total_proof_num > self.back_pressure_total_proof_limit,
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L64-84)
```rust
    pub(crate) fn update_quota(&mut self, num_bytes: usize) -> anyhow::Result<StorageMode> {
        if self.batch_balance == 0 {
            counters::EXCEEDED_BATCH_QUOTA_COUNT.inc();
            bail!("Batch quota exceeded ");
        }

        if self.db_balance >= num_bytes {
            self.batch_balance -= 1;
            self.db_balance -= num_bytes;

            if self.memory_balance >= num_bytes {
                self.memory_balance -= num_bytes;
                Ok(StorageMode::MemoryAndPersisted)
            } else {
                Ok(StorageMode::PersistedOnly)
            }
        } else {
            counters::EXCEEDED_STORAGE_QUOTA_COUNT.inc();
            bail!("Storage quota exceeded ");
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L522-527)
```rust
            },
            Err(e) => {
                debug!("QS: failed to store to cache {:?}", e);
                None
            },
        }
```
