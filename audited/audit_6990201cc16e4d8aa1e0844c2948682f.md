# Audit Report

## Title
Non-Atomic File Writes Cause Permanent Indexer Failure on Crash During Metadata Updates

## Summary
The `LocalFileStore::save_raw_file()` function uses `tokio::fs::write()` which does not perform atomic write operations. When the indexer process crashes during critical metadata file writes (metadata.json or batch metadata files), partially written files are left on disk. On restart, the indexer attempts to deserialize these corrupted JSON files and panics, causing permanent failure that requires manual intervention to restore service.

## Finding Description
The indexer-grpc-utils local file store implementation violates atomicity guarantees for critical state files. The vulnerability exists in the `save_raw_file()` function: [1](#0-0) 

The `tokio::fs::write()` function directly writes data to the target file without using the atomic write-then-rename pattern. This creates a race condition where process crashes (SIGKILL, power failure, OOM killer) during writes leave partially written files.

The indexer stores three types of critical files:
1. **metadata.json** - Contains chain_id, version, and configuration
2. **Batch metadata files** - Contains transaction file locations and version ranges  
3. **Transaction data files** - Compressed protobuf transaction data

During the recovery process at startup, the indexer reads and deserializes these files: [2](#0-1) [3](#0-2) 

Both functions use `.expect()` which panics on deserialization failure. The recovery process explicitly depends on these files: [4](#0-3) 

**Attack Scenario:**
1. Indexer is actively writing metadata files (happens on every batch completion and periodically)
2. Process receives SIGKILL, system crashes, or OOM killer terminates the process mid-write
3. Critical JSON file (metadata.json or batch metadata) is left partially written with truncated/malformed JSON
4. Operator restarts the indexer service
5. During initialization, `FileStoreUploader::new()` or `recover()` attempts to deserialize the corrupted file
6. Deserialization fails, triggering panic with "Metadata JSON is invalid." or "Batch metadata JSON is invalid."
7. Indexer cannot start, blockchain data becomes unavailable to downstream consumers
8. Manual intervention required: operator must identify and delete/restore corrupted files

**Evidence of Atomic Write Awareness:**
The codebase demonstrates knowledge of atomic write requirements in other critical components: [5](#0-4) 

This secure storage implementation uses the standard write-to-temp-then-rename pattern, but the indexer file store does not.

## Impact Explanation
This qualifies as **High Severity** per the Aptos bug bounty criteria:

- **API crashes**: The indexer gRPC API becomes completely unavailable and cannot restart
- **Significant protocol violations**: Breaks the state consistency invariant - indexer state recovery expects atomic file operations but receives corrupted state
- **Service availability impact**: The indexer is critical infrastructure that provides blockchain data access to wallets, explorers, dApps, and other services. Complete indexer failure affects the entire ecosystem's ability to query historical transaction data

This does not reach Critical severity because:
- No funds are at risk
- Consensus/validator operations are not affected  
- Recovery is possible with manual intervention (though it requires identifying and fixing corrupted files)

The severity is elevated from Medium to High because:
- The failure is permanent (not transient) - requires manual intervention
- The impact scope is broad - affects all downstream services depending on indexer data
- The likelihood is non-trivial in production environments with system crashes, OOM conditions, or rolling updates

## Likelihood Explanation
**Likelihood: Medium-High**

This vulnerability will trigger whenever:
- Process crashes (SIGKILL) during metadata file write operations
- System experiences power failure or hardware crash
- OOM killer terminates the indexer process
- Kubernetes/orchestration performs ungraceful pod termination
- System maintenance causes abrupt shutdown

Metadata files are updated frequently:
- `metadata.json` updates on every batch completion and every 10+ seconds (based on `MIN_UPDATE_FREQUENCY`)
- Batch metadata files update multiple times per batch as files are written [6](#0-5) 

In production environments with high transaction throughput, metadata writes occur frequently, creating multiple opportunities for crash-during-write scenarios. The window of vulnerability increases with file size and storage I/O latency.

## Recommendation
Implement atomic file writes using the write-then-rename pattern, similar to the existing pattern in `secure/storage/src/on_disk.rs`:

```rust
#[async_trait::async_trait]
impl IFileStoreWriter for LocalFileStore {
    async fn save_raw_file(&self, file_path: PathBuf, data: Vec<u8>) -> Result<()> {
        let file_path = self.path.join(file_path);
        if let Some(parent) = file_path.parent() {
            tokio::fs::create_dir_all(parent).await?;
        }
        
        // Write to temporary file
        let temp_path = file_path.with_extension("tmp");
        tokio::fs::write(&temp_path, data).await?;
        
        // Ensure data is flushed to disk (optional but recommended for durability)
        let file = tokio::fs::OpenOptions::new()
            .write(true)
            .open(&temp_path)
            .await?;
        file.sync_all().await?;
        
        // Atomically rename temp file to final destination
        tokio::fs::rename(&temp_path, &file_path)
            .await
            .map_err(anyhow::Error::msg)
    }

    fn max_update_frequency(&self) -> Duration {
        Duration::from_secs(0)
    }
}
```

The atomic rename operation (POSIX `rename()` system call) guarantees that:
- The target file either contains the complete new content or the complete old content
- No partial/corrupted content is visible at the target path
- Even if the process crashes immediately after rename, the file is complete

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;
    use tempfile::tempdir;

    #[tokio::test]
    async fn test_non_atomic_write_corruption() {
        // Setup: Create a temporary directory for the local file store
        let temp_dir = tempdir().unwrap();
        let store = LocalFileStore::new(temp_dir.path().to_path_buf());
        
        // Simulate a valid metadata file
        let metadata = serde_json::json!({
            "chain_id": 1,
            "num_transactions_per_folder": 100000,
            "version": 12345
        });
        let valid_data = serde_json::to_vec(&metadata).unwrap();
        
        // Write initial valid metadata
        store.save_raw_file(
            PathBuf::from("metadata.json"),
            valid_data.clone()
        ).await.unwrap();
        
        // Simulate partial write by manually creating corrupted file
        // (In real scenario, this happens when process crashes mid-write)
        let metadata_path = temp_dir.path().join("metadata.json");
        let corrupted_json = b"{\"chain_id\": 1, \"num_transa"; // Truncated JSON
        fs::write(&metadata_path, corrupted_json).unwrap();
        
        // Attempt to read the corrupted metadata (simulating restart/recovery)
        let reader = FileStoreReader::new(1, Arc::new(store)).await;
        
        // This should panic with "Metadata JSON is invalid."
        // Demonstrating that corrupted files cause unrecoverable failure
        let result = std::panic::catch_unwind(|| {
            tokio::runtime::Runtime::new().unwrap().block_on(async {
                reader.get_file_store_metadata().await
            })
        });
        
        assert!(result.is_err(), "Should panic on corrupted JSON file");
    }
    
    #[tokio::test]
    async fn test_atomic_write_prevents_corruption() {
        // This test demonstrates the fix using atomic writes
        let temp_dir = tempdir().unwrap();
        let file_path = temp_dir.path().join("test.json");
        let temp_path = temp_dir.path().join("test.json.tmp");
        
        let data = b"{\"valid\": \"json\"}";
        
        // Atomic write pattern
        tokio::fs::write(&temp_path, data).await.unwrap();
        tokio::fs::rename(&temp_path, &file_path).await.unwrap();
        
        // Even if process crashes here, file_path either:
        // - Doesn't exist (if crash before rename)
        // - Contains complete data (if crash after rename)
        // Never contains partial data
        
        let read_data = tokio::fs::read(&file_path).await.unwrap();
        assert_eq!(read_data, data);
    }
}
```

**Notes**

This vulnerability affects the local file store implementation specifically. The GCS (Google Cloud Storage) implementation is not vulnerable as cloud storage services handle atomic write operations internally. However, local file stores are commonly used in development, testing, and certain production deployments, making this a real-world concern.

The fix is straightforward and follows established patterns already present in the Aptos codebase. The performance impact is negligible - the additional rename operation is a metadata-only operation that completes in microseconds.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/local.rs (L61-69)
```rust
    async fn save_raw_file(&self, file_path: PathBuf, data: Vec<u8>) -> Result<()> {
        let file_path = self.path.join(file_path);
        if let Some(parent) = file_path.parent() {
            tokio::fs::create_dir_all(parent).await?;
        }
        tokio::fs::write(file_path, data)
            .await
            .map_err(anyhow::Error::msg)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L160-166)
```rust
    pub async fn get_file_store_metadata(&self) -> Option<FileStoreMetadata> {
        self.reader
            .get_raw_file(PathBuf::from(METADATA_FILE_NAME))
            .await
            .expect("Failed to get file store metadata.")
            .map(|data| serde_json::from_slice(&data).expect("Metadata JSON is invalid."))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L170-176)
```rust
    pub async fn get_batch_metadata(&self, version: u64) -> Option<BatchMetadata> {
        self.reader
            .get_raw_file(self.get_path_for_batch_metadata(version))
            .await
            .expect("Failed to get batch metadata.")
            .map(|data| serde_json::from_slice(&data).expect("Batch metadata JSON is invalid."))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L86-118)
```rust
    /// Recovers the batch metadata in memory buffer for the unfinished batch from file store.
    async fn recover(&self) -> Result<(u64, BatchMetadata)> {
        let _timer = TIMER.with_label_values(&["recover"]).start_timer();

        let mut version = self
            .reader
            .get_latest_version()
            .await
            .expect("Latest version must exist.");
        info!("Starting recovering process, current version in storage: {version}.");
        let mut num_folders_checked = 0;
        let mut buffered_batch_metadata_to_recover = BatchMetadata::default();
        while let Some(batch_metadata) = self.reader.get_batch_metadata(version).await {
            let batch_last_version = batch_metadata.files.last().unwrap().last_version;
            version = batch_last_version;
            if version % NUM_TXNS_PER_FOLDER != 0 {
                buffered_batch_metadata_to_recover = batch_metadata;
                break;
            }
            num_folders_checked += 1;
            if num_folders_checked >= MAX_NUM_FOLDERS_TO_CHECK_FOR_RECOVERY {
                panic!(
                    "File store metadata is way behind batch metadata, data might be corrupted."
                );
            }
        }

        self.update_file_store_metadata(version).await?;

        info!("Finished recovering process, recovered at version: {version}.");

        Ok((version, buffered_batch_metadata_to_recover))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L212-248)
```rust
        let mut update_batch_metadata = false;
        let max_update_frequency = self.writer.max_update_frequency();
        if self.last_batch_metadata_update_time.is_none()
            || Instant::now() - self.last_batch_metadata_update_time.unwrap()
                >= MIN_UPDATE_FREQUENCY
        {
            update_batch_metadata = true;
        } else if end_batch {
            update_batch_metadata = true;
            tokio::time::sleep_until(
                self.last_batch_metadata_update_time.unwrap() + max_update_frequency,
            )
            .await;
        }

        if !update_batch_metadata {
            return Ok(());
        }

        let batch_metadata_path = self.reader.get_path_for_batch_metadata(first_version);
        {
            let _timer = TIMER
                .with_label_values(&["do_upload__update_batch_metadata"])
                .start_timer();
            self.writer
                .save_raw_file(
                    batch_metadata_path,
                    serde_json::to_vec(&batch_metadata).map_err(anyhow::Error::msg)?,
                )
                .await?;
        }

        if end_batch {
            self.last_batch_metadata_update_time = None;
        } else {
            self.last_batch_metadata_update_time = Some(Instant::now());
        }
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```
