# Audit Report

## Title
Race Condition in Indexer State View Reading During Concurrent Pre-Commit Operations

## Summary
The indexer's `index()` function creates a state view at `last_version` without isolation guarantees, allowing concurrent `pre_commit_ledger()` operations to write data at that version while the annotator reads from it. This race condition can cause the annotator to read inconsistent module definitions and extract incorrect table type information.

## Finding Description

The vulnerability exists in the interaction between three components:

1. **Indexer state view creation** [1](#0-0) 

The `index()` function calculates `last_version = first_version + write_sets.len()` and creates a state view at this version, which is one version BEYOND the last transaction being indexed.

2. **Concurrent locking mechanism** [2](#0-1) 

`pre_commit_ledger()` uses `pre_commit_lock`, while `commit_ledger()` uses `commit_lock` (separate locks). [3](#0-2) 

The comment explicitly states "Pre-committing and committing in concurrency is allowed" - these operations use different locks and can execute simultaneously.

3. **Non-isolated database reads** [4](#0-3) 

The `get_state_value_with_version_by_version()` function creates iterators with default `ReadOptions` without snapshot isolation, allowing reads to see concurrent writes.

4. **Module reading during annotation** [5](#0-4) 

When the annotator parses Move values, it loads module definitions from the state view to resolve type information.

**Attack Scenario:**

- **Thread 1**: Commits transactions 0-2, then calls `post_commit()` â†’ `indexer.index(db_reader, 0, [ws0, ws1, ws2])`
  - Creates state_view at version 3 (= 0 + 3)
  - Begins parsing write_sets using the annotator

- **Thread 2** (concurrently): Executes `pre_commit_ledger()` for transactions 3-5
  - Acquires `pre_commit_lock` (different lock!)
  - Writes module upgrades and new state at versions 3-5 to disk via `calculate_and_commit_ledger_and_state_kv()`

- **Thread 1** (continued): Annotator reads module M at version 3 to parse table types
  - May read the UPGRADED module from Thread 2's pre-commit
  - Extracts incorrect type information for tables created in transactions 0-2
  - Stores wrong table metadata permanently in indexer database

## Impact Explanation

**Severity: Medium** - State inconsistencies requiring intervention

This qualifies as Medium severity per the Aptos bug bounty criteria because:

1. **State Consistency Violation**: The indexer stores incorrect table type metadata persistently, breaking the invariant that indexer data accurately reflects blockchain state.

2. **Scope of Impact**: While the indexer is auxiliary infrastructure (not part of consensus), it's critical for:
   - Block explorers displaying transaction details
   - REST APIs serving table data to applications
   - Developer tools parsing on-chain state
   
3. **Cascading Failures**: Incorrect table info can cause:
   - API crashes when attempting to parse table items with wrong types
   - Data corruption in applications relying on indexer metadata
   - Inconsistent views across different nodes' indexers

4. **Non-recoverable**: Once wrong table info is committed to the indexer database, it persists until manual intervention or re-indexing from genesis.

This does NOT qualify as High or Critical because:
- Does not affect consensus or validator operations
- Does not cause fund loss or blockchain state corruption
- Main ledger remains correct; only auxiliary metadata is affected

## Likelihood Explanation

**Likelihood: Medium to High** in production environments

**Factors increasing likelihood:**

1. **High-throughput scenarios**: During periods of high transaction volume, `pre_commit_ledger()` and `commit_ledger()` operations overlap frequently by design (the separate locks exist specifically to allow this concurrency).

2. **Small race window**: The vulnerability window is brief (microseconds during module reads), but occurs on EVERY indexing operation when concurrent pre-commits are active.

3. **Module upgrades**: The issue manifests specifically when:
   - A transaction batch creates/uses tables with certain types
   - A concurrent pre-commit upgrades related modules with different types
   - This is less common but happens during framework upgrades or module updates

**Factors decreasing likelihood:**

1. **Specific timing required**: The concurrent pre-commit must write exactly at `last_version` during the annotator's module read operations.

2. **Module read dependency**: Only affects cases where the annotator needs to load module definitions (not all write_set parsing requires this).

## Recommendation

Implement snapshot-based isolation for indexer state view reads:

**Option 1: Use RocksDB Snapshots**

```rust
pub fn index(
    &self,
    db_reader: Arc<dyn DbReader>,
    first_version: Version,
    write_sets: &[&WriteSet],
) -> Result<()> {
    let last_version = first_version + write_sets.len() as Version;
    
    // Create a snapshot at the correct version (last committed transaction)
    // Use last_version - 1 to read state AFTER the last transaction in write_sets
    let snapshot_version = if write_sets.is_empty() {
        first_version.saturating_sub(1)
    } else {
        last_version - 1
    };
    
    let state_view = db_reader.state_view_at_version(Some(snapshot_version))?;
    let annotator = AptosValueAnnotator::new(&state_view);
    self.index_with_annotator(&annotator, first_version, write_sets)
}
```

**Option 2: Use commit_lock During Indexing**

Extend `commit_lock` protection to cover the entire post_commit operation including indexing, preventing concurrent pre-commits: [6](#0-5) 

Move the indexer call inside the critical section protected by commit_lock scope.

**Option 3: Read from Committed Version**

Modify the version calculation to read from the actual last committed transaction rather than the next version:

```rust
let last_committed_version = first_version + write_sets.len() as Version - 1;
let state_view = db_reader.state_view_at_version(Some(last_committed_version))?;
```

**Recommended approach**: Option 1 or Option 3, as they provide isolation without reducing concurrency. Option 2 would eliminate the performance benefit of concurrent pre-commit and commit operations.

## Proof of Concept

```rust
// Rust integration test demonstrating the race condition
#[test]
fn test_indexer_concurrent_precommit_race() {
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    // Setup: Create AptosDB and Indexer
    let (db, indexer) = setup_test_db_and_indexer();
    
    // Commit initial transactions 0-2
    let chunk_0_2 = create_chunk_with_table_creation(0, 3);
    db.pre_commit_ledger(chunk_0_2.clone(), false).unwrap();
    db.commit_ledger(2, None, Some(chunk_0_2)).unwrap();
    
    let barrier = Arc::new(Barrier::new(2));
    let barrier_clone = barrier.clone();
    let db_clone = db.clone();
    
    // Thread 1: Index transactions 0-2
    let indexer_handle = thread::spawn(move || {
        barrier.wait(); // Synchronize start
        
        let write_sets = get_write_sets_for_versions(0, 3);
        indexer.index(db_clone.clone(), 0, &write_sets)
    });
    
    // Thread 2: Pre-commit transactions 3-5 with module upgrade
    let precommit_handle = thread::spawn(move || {
        barrier_clone.wait(); // Synchronize start
        
        // Create chunk that upgrades module at version 3
        let chunk_3_5 = create_chunk_with_module_upgrade(3, 3);
        db.pre_commit_ledger(chunk_3_5, false)
    });
    
    // Wait for both threads
    let index_result = indexer_handle.join().unwrap();
    let precommit_result = precommit_handle.join().unwrap();
    
    // Verify: Check if indexer extracted incorrect table info
    // due to reading upgraded module from concurrent pre-commit
    let table_info = indexer.get_table_info(test_table_handle).unwrap();
    assert_ne!(table_info.key_type, expected_original_key_type,
        "Race condition: indexer read upgraded module instead of original");
}
```

**Note**: This PoC is conceptual. Actual reproduction requires precise timing control and access to internal AptosDB/Indexer test utilities. The race window is narrow, making deterministic reproduction challenging without instrumentation.

---

**Notes:**

This vulnerability represents a classic read-write race condition in a concurrent database system. While the impact is limited to auxiliary indexer metadata (not affecting consensus or core blockchain security), it violates state consistency guarantees and can cause downstream failures in applications relying on accurate table type information. The fix requires implementing proper snapshot isolation for indexer reads or adjusting the version calculation to ensure reads don't race with concurrent writes.

### Citations

**File:** storage/indexer/src/lib.rs (L83-93)
```rust
    pub fn index(
        &self,
        db_reader: Arc<dyn DbReader>,
        first_version: Version,
        write_sets: &[&WriteSet],
    ) -> Result<()> {
        let last_version = first_version + write_sets.len() as Version;
        let state_view = db_reader.state_view_at_version(Some(last_version))?;
        let annotator = AptosValueAnnotator::new(&state_view);
        self.index_with_annotator(&annotator, first_version, write_sets)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L44-76)
```rust
    fn pre_commit_ledger(&self, chunk: ChunkToCommit, sync_commit: bool) -> Result<()> {
        gauged_api("pre_commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .pre_commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["pre_commit_ledger"]);

            chunk
                .state_summary
                .latest()
                .global_state_summary
                .log_generation("db_save");

            self.pre_commit_validation(&chunk)?;
            let _new_root_hash =
                self.calculate_and_commit_ledger_and_state_kv(&chunk, self.skip_index_and_usage)?;

            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions__others"]);

            self.state_store.buffered_state().lock().update(
                chunk.result_ledger_state_with_summary(),
                chunk.estimated_total_state_updates(),
                sync_commit || chunk.is_reconfig,
            )?;

            Ok(())
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L78-112)
```rust
    fn commit_ledger(
        &self,
        version: Version,
        ledger_info_with_sigs: Option<&LedgerInfoWithSignatures>,
        chunk_opt: Option<ChunkToCommit>,
    ) -> Result<()> {
        gauged_api("commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_ledger"]);

            let old_committed_ver = self.get_and_check_commit_range(version)?;

            let mut ledger_batch = SchemaBatch::new();
            // Write down LedgerInfo if provided.
            if let Some(li) = ledger_info_with_sigs {
                self.check_and_put_ledger_info(version, li, &mut ledger_batch)?;
            }
            // Write down commit progress
            ledger_batch.put::<DbMetadataSchema>(
                &DbMetadataKey::OverallCommitProgress,
                &DbMetadataValue::Version(version),
            )?;
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;

            // Notify the pruners, invoke the indexer, and update in-memory ledger info.
            self.post_commit(old_committed_ver, version, ledger_info_with_sigs, chunk_opt)
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L603-672)
```rust
    fn post_commit(
        &self,
        old_committed_version: Option<Version>,
        version: Version,
        ledger_info_with_sigs: Option<&LedgerInfoWithSignatures>,
        chunk_opt: Option<ChunkToCommit>,
    ) -> Result<()> {
        // If commit succeeds and there are at least one transaction written to the storage, we
        // will inform the pruner thread to work.
        if old_committed_version.is_none() || version > old_committed_version.unwrap() {
            let first_version = old_committed_version.map_or(0, |v| v + 1);
            let num_txns = version + 1 - first_version;

            COMMITTED_TXNS.inc_by(num_txns);
            LATEST_TXN_VERSION.set(version as i64);
            if let Some(update_sender) = &self.update_subscriber {
                update_sender
                    .send((Instant::now(), version))
                    .map_err(|err| {
                        AptosDbError::Other(format!("Failed to send update to subscriber: {}", err))
                    })?;
            }
            // Activate the ledger pruner and state kv pruner.
            // Note the state merkle pruner is activated when state snapshots are persisted
            // in their async thread.
            self.ledger_pruner
                .maybe_set_pruner_target_db_version(version);
            self.state_store
                .state_kv_pruner
                .maybe_set_pruner_target_db_version(version);

            // Note: this must happen after txns have been saved to db because types can be newly
            // created in this same chunk of transactions.
            if let Some(indexer) = &self.indexer {
                let _timer = OTHER_TIMERS_SECONDS.timer_with(&["indexer_index"]);
                // n.b. txns_to_commit can be partial, when the control was handed over from consensus to state sync
                // where state sync won't send the pre-committed part to the DB again.
                if let Some(chunk) = chunk_opt
                    && chunk.len() == num_txns as usize
                {
                    let write_sets = chunk
                        .transaction_outputs
                        .iter()
                        .map(|t| t.write_set())
                        .collect_vec();
                    indexer.index(self.state_store.clone(), first_version, &write_sets)?;
                } else {
                    let write_sets: Vec<_> = self
                        .ledger_db
                        .write_set_db()
                        .get_write_set_iter(first_version, num_txns as usize)?
                        .try_collect()?;
                    let write_set_refs = write_sets.iter().collect_vec();
                    indexer.index(self.state_store.clone(), first_version, &write_set_refs)?;
                };
            }
        }

        // Once everything is successfully persisted, update the latest in-memory ledger info.
        if let Some(x) = ledger_info_with_sigs {
            self.ledger_db
                .metadata_db()
                .set_latest_ledger_info(x.clone());

            LEDGER_VERSION.set(x.ledger_info().version() as i64);
            NEXT_BLOCK_EPOCH.set(x.ledger_info().next_block_epoch() as i64);
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L374-400)
```rust
    pub(crate) fn get_state_value_with_version_by_version(
        &self,
        state_key: &StateKey,
        version: Version,
    ) -> Result<Option<(Version, StateValue)>> {
        let mut read_opts = ReadOptions::default();

        // We want `None` if the state_key changes in iteration.
        read_opts.set_prefix_same_as_start(true);
        if !self.enabled_sharding() {
            let mut iter = self
                .db_shard(state_key.get_shard_id())
                .iter_with_opts::<StateValueSchema>(read_opts)?;
            iter.seek(&(state_key.clone(), version))?;
            Ok(iter
                .next()
                .transpose()?
                .and_then(|((_, version), value_opt)| value_opt.map(|value| (version, value))))
        } else {
            let mut iter = self
                .db_shard(state_key.get_shard_id())
                .iter_with_opts::<StateValueByKeyHashSchema>(read_opts)?;
            iter.seek(&(state_key.hash(), version))?;
            Ok(iter
                .next()
                .transpose()?
                .and_then(|((_, version), value_opt)| value_opt.map(|value| (version, value))))
```

**File:** aptos-move/aptos-resource-viewer/src/module_view.rs (L56-87)
```rust
    fn view_compiled_module(&self, module_id: &ModuleId) -> anyhow::Result<Option<Self::Item>> {
        let mut module_cache = self.module_cache.borrow_mut();
        if let Some(module) = module_cache.get(module_id) {
            return Ok(Some(module.clone()));
        }

        let state_key = StateKey::module_id(module_id);
        Ok(
            match self
                .state_view
                .get_state_value_bytes(&state_key)
                .map_err(|e| anyhow!("Error retrieving module {:?}: {:?}", module_id, e))?
            {
                Some(bytes) => {
                    let compiled_module =
                        CompiledModule::deserialize_with_config(&bytes, &self.deserializer_config)
                            .map_err(|status| {
                                anyhow!(
                                    "Module {:?} deserialize with error code {:?}",
                                    module_id,
                                    status
                                )
                            })?;

                    let compiled_module = Arc::new(compiled_module);
                    module_cache.insert(module_id.clone(), compiled_module.clone());
                    Some(compiled_module)
                },
                None => None,
            },
        )
    }
```
