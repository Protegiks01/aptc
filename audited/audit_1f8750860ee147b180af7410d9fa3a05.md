# Audit Report

## Title
Backup Service Blocking Thread Pool Exhaustion Leading to Validator Node Performance Degradation

## Summary
The backup service's `reply_with_bytes_sender()` function spawns unbounded blocking tasks without rate limiting or concurrency controls. An attacker can flood backup endpoints with requests to exhaust the 64-thread blocking pool, causing all threads to be occupied with long-running database iteration operations. While the backup service has an isolated runtime, the shared AptosDB instance means 64 concurrent database read operations create I/O contention that can degrade validator node performance.

## Finding Description

The backup service accepts HTTP requests on endpoints that trigger database iteration operations through `reply_with_bytes_sender()`. [1](#0-0) 

Each request spawns a blocking task via `tokio::task::spawn_blocking` without any concurrency control. The affected endpoints include:
- `/state_snapshot/<version>` - iterates through all state items
- `/state_snapshot_chunk/<version>/<start>/<limit>` - iterates through state chunks  
- `/transactions/<start>/<count>` - iterates through transactions
- `/epoch_ending_ledger_infos/<start>/<end>` - iterates through epoch info [2](#0-1) 

The backup service runtime is limited to 64 blocking threads: [3](#0-2) 

In production deployments, the backup service is exposed on all network interfaces: [4](#0-3) 

**Attack Path:**
1. Attacker sends 100+ concurrent requests to `/state_snapshot/0` or similar endpoints
2. First 64 requests occupy all blocking threads with long-running database iterations
3. Remaining requests queue indefinitely in memory
4. Each blocking thread holds database iterators and performs intensive I/O operations
5. The 64 concurrent database reads create I/O contention on the shared AptosDB instance
6. Consensus and execution operations that require database access experience slowdowns
7. Legitimate backup requests cannot proceed as the blocking pool remains saturated

The backup service shares the same AptosDB instance with critical node components: [5](#0-4) 

Database iteration creates resource-intensive operations: [6](#0-5) 

## Impact Explanation

This qualifies as **High Severity** under "Validator node slowdowns" criteria. While the backup service has an isolated runtime, the 64 concurrent long-running database read operations create measurable I/O contention on the shared database. This can slow down time-critical consensus operations including block execution, state commitment, and transaction validation.

The attack is sustained - once 64 threads are occupied with multi-hour state iteration operations, the blocking pool remains exhausted until those operations complete or timeout. This creates a persistent performance degradation window during which the validator node operates at reduced efficiency.

## Likelihood Explanation

**High likelihood** if the backup service is network-exposed:
- No authentication required on backup endpoints
- No rate limiting implemented
- Attack requires only standard HTTP GET requests
- Backup operations are inherently long-running (hours for full state snapshots)
- Production configurations expose the service on `0.0.0.0:6186`

The attacker needs no special privileges, just network access to port 6186.

## Recommendation

Implement multi-layered protections:

1. **Add request-level concurrency limiting:**
```rust
// In handlers/utils.rs
static ACTIVE_BACKUP_TASKS: Lazy<Semaphore> = Lazy::new(|| Semaphore::new(8));

pub(super) fn reply_with_bytes_sender<F>(
    backup_handler: &BackupHandler,
    endpoint: &'static str,
    f: F,
) -> Box<dyn Reply>
where
    F: FnOnce(BackupHandler, &mut bytes_sender::BytesSender) -> DbResult<()> + Send + 'static,
{
    // Acquire permit before spawning
    let permit = match ACTIVE_BACKUP_TASKS.try_acquire() {
        Ok(permit) => permit,
        Err(_) => {
            return Box::new(warp::http::StatusCode::SERVICE_UNAVAILABLE)
        }
    };
    
    let (sender, stream) = bytes_sender::BytesSender::new(endpoint);
    let bh = backup_handler.clone();
    
    let _join_handle = tokio::task::spawn_blocking(move || {
        let _permit = permit; // Hold permit for task lifetime
        let _timer = BACKUP_TIMER.timer_with(&[&format!("backup_service_bytes_sender_{}", endpoint)]);
        abort_on_error(f)(bh, sender)
    });

    Box::new(Response::new(Body::wrap_stream(stream)))
}
```

2. **Add authentication** to the backup service endpoints
3. **Add rate limiting** per client IP address
4. **Monitor** active backup task counts and alert on sustained high usage
5. **Document** that backup_service_address should use `127.0.0.1:6186` unless explicitly needed for external backup infrastructure

## Proof of Concept

```rust
#[cfg(test)]
mod blocking_pool_exhaustion_test {
    use super::*;
    use aptos_config::utils::get_available_port;
    use aptos_temppath::TempPath;
    use std::net::{IpAddr, Ipv4Addr};
    use std::sync::Arc;
    use std::thread;
    use std::time::{Duration, Instant};

    #[test]
    fn test_blocking_pool_exhaustion_causes_backup_service_dos() {
        let tmpdir = TempPath::new();
        let db = Arc::new(AptosDB::new_for_test(&tmpdir));
        let port = get_available_port();
        let _rt = start_backup_service(
            SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), port), 
            db
        );

        // Spawn 70 concurrent requests (exceeds 64 thread limit)
        let handles: Vec<_> = (0..70)
            .map(|i| {
                let port_clone = port;
                thread::spawn(move || {
                    let start = Instant::now();
                    let url = format!("http://127.0.0.1:{}/state_snapshot/0", port_clone);
                    let result = reqwest::blocking::get(&url);
                    (i, start.elapsed(), result.is_err())
                })
            })
            .collect();

        // Collect results
        let results: Vec<_> = handles.into_iter()
            .map(|h| h.join().unwrap())
            .collect();

        // First 64 requests should complete (or fail fast due to empty DB)
        // Remaining 6 requests should be severely delayed or timeout
        let (fast, slow): (Vec<_>, Vec<_>) = results.iter()
            .partition(|(_, elapsed, _)| *elapsed < Duration::from_secs(5));

        // Demonstrate that requests beyond the blocking pool limit are delayed
        assert!(slow.len() > 0, 
            "Expected some requests to be delayed due to blocking pool exhaustion");
        
        println!("Fast requests: {}, Slow/timeout requests: {}", 
            fast.len(), slow.len());
    }
}
```

## Notes

The vulnerability exists in the production codebase and is exploitable when the backup service is network-accessible. The shared database resource means that even with runtime isolation, the backup service can create I/O contention affecting validator performance. The lack of any concurrency control, rate limiting, or authentication on these resource-intensive endpoints represents a significant operational security gap.

The fix requires adding semaphore-based concurrency limiting at minimum, with additional authentication and rate limiting recommended for defense in depth. The default configuration should bind to localhost only unless external backup infrastructure explicitly requires network exposure.

### Citations

**File:** storage/backup/backup-service/src/handlers/utils.rs (L46-65)
```rust
pub(super) fn reply_with_bytes_sender<F>(
    backup_handler: &BackupHandler,
    endpoint: &'static str,
    f: F,
) -> Box<dyn Reply>
where
    F: FnOnce(BackupHandler, &mut bytes_sender::BytesSender) -> DbResult<()> + Send + 'static,
{
    let (sender, stream) = bytes_sender::BytesSender::new(endpoint);

    // spawn and forget, error propagates through the `stream: TryStream<_>`
    let bh = backup_handler.clone();
    let _join_handle = tokio::task::spawn_blocking(move || {
        let _timer =
            BACKUP_TIMER.timer_with(&[&format!("backup_service_bytes_sender_{}", endpoint)]);
        abort_on_error(f)(bh, sender)
    });

    Box::new(Response::new(Body::wrap_stream(stream)))
}
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L48-56)
```rust
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-27)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;
```

**File:** terraform/helm/fullnode/files/fullnode-base.yaml (L68-68)
```yaml
  backup_service_address: "0.0.0.0:6186"
```

**File:** aptos-node/src/storage.rs (L69-71)
```rust
            let (db_arc, db_rw) = DbReaderWriter::wrap(db);
            let db_backup_service =
                start_backup_service(node_config.storage.backup_service_address, db_arc.clone());
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L41-59)
```rust
    pub fn get_transaction_iter(
        &self,
        start_version: Version,
        num_transactions: usize,
    ) -> Result<
        impl Iterator<
                Item = Result<(
                    Transaction,
                    PersistedAuxiliaryInfo,
                    TransactionInfo,
                    Vec<ContractEvent>,
                    WriteSet,
                )>,
            > + '_,
    > {
        let txn_iter = self
            .ledger_db
            .transaction_db()
            .get_transaction_iter(start_version, num_transactions)?;
```
