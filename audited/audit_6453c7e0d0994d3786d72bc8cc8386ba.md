# Audit Report

## Title
Unbounded Memory Allocation in Consensus Recovery Causes Validator OOM on Startup

## Summary
The consensus recovery mechanism in `persistent_liveness_storage.rs` performs unbounded memory allocation when formatting blocks and quorum certificates for logging and error messages. If ConsensusDB contains an abnormally large number of blocks (due to pruning bugs, corruption, or other failure modes), validator nodes will exhaust memory and crash on every startup attempt, causing permanent liveness failure.

## Finding Description

There are **two distinct locations** where unbounded memory allocation occurs during consensus recovery:

**Location 1 (Primary Vulnerability - Lines 535-547):** [1](#0-0) 

The `start()` function unconditionally formats ALL blocks and quorum certificates from ConsensusDB for info logging **before** any recovery validation occurs. This happens on **every** startup, regardless of whether recovery succeeds or fails.

**Location 2 (Secondary Vulnerability - Lines 365-383):** [2](#0-1) 

The error handler in `RecoveryData::new()` formats ALL blocks and quorum certificates when `find_root()` fails, creating diagnostic error messages with complete block/QC listings.

**Attack Vector:**

ConsensusDB loads all persisted blocks and quorum certificates without bounds checking: [3](#0-2) 

The `get_all()` method retrieves every block and QC from the database: [4](#0-3) 

There is no upper limit on blocks that can be saved to ConsensusDB: [5](#0-4) 

**Memory Consumption Calculation:**

Each Block's Display implementation produces approximately 200-300 bytes: [6](#0-5) 

Each QuorumCert's Display contains signatures and could be 500+ bytes: [7](#0-6) 

For 1 million blocks:
- Block strings: ~250 MB
- Vec<String> overhead: ~8-16 MB  
- Concatenated string: another ~250 MB
- QC strings: ~500 MB
- QC concatenated: another ~500 MB
- **Total: ~1.5 GB additional memory allocation**

This compounds with the already-loaded blocks/QCs in memory, easily causing OOM on validators with limited resources.

**Failure Scenarios Leading to Block Accumulation:**

1. **Pruning Logic Bug**: A bug in `find_blocks_to_prune` or pruning execution could prevent old blocks from being removed
2. **Database Corruption**: File system errors or manual manipulation could result in millions of stale blocks
3. **Epoch Transition Failures**: Failures during epoch changes could leave orphaned blocks
4. **Network Partition Recovery**: Extended network issues followed by recovery attempts

This breaks the **Resource Limits** invariant (#9): "All operations must respect gas, storage, and computational limits" - the formatting operations have no resource bounds.

## Impact Explanation

**Severity: High to Critical**

This vulnerability causes validator **liveness failure**:

- **Immediate Impact**: Validator node crashes with OOM on every startup attempt when ConsensusDB contains excessive blocks
- **No Recovery Path**: The node cannot recover automatically - it will crash repeatedly on startup
- **Network Impact**: If multiple validators experience similar conditions (shared bug, coordinated attack), network consensus could be disrupted
- **Operational Burden**: Requires manual database cleanup or code patching to recover

Per Aptos bug bounty criteria:
- **High Severity** ($50k): "Validator node slowdowns" - this is more severe than slowdowns, it's complete startup failure
- Approaches **Critical Severity** ($1M): "Total loss of liveness/network availability" - if multiple validators are affected simultaneously

The first location (lines 535-547) is particularly severe because it executes **unconditionally** on every startup, making the vulnerability deterministic once conditions are met.

## Likelihood Explanation

**Likelihood: Medium**

While requiring ConsensusDB to accumulate millions of blocks is not the normal case, several realistic scenarios can trigger this:

1. **Software Bugs**: Pruning logic bugs are common in production systems handling persistent state
2. **Cascade Failures**: One validator experiencing issues could have similar root causes affecting others
3. **Upgrade/Migration Issues**: Schema changes or version migrations could leave stale data
4. **File System Corruption**: Storage hardware failures can corrupt RocksDB

The vulnerability does NOT require:
- Attacker with validator privileges
- Network-level attacks
- Cryptographic breaks
- Consensus protocol manipulation

It can be triggered by internal system bugs or environmental failures, making it a **latent reliability vulnerability** rather than requiring active exploitation.

## Recommendation

**Immediate Fix**: Add bounds checking to limit the number of blocks/QCs formatted in logging and error messages:

```rust
// In start() function, replace lines 535-547:
const MAX_BLOCKS_TO_LOG: usize = 100;
let blocks_to_log = blocks.iter().take(MAX_BLOCKS_TO_LOG);
let blocks_repr: Vec<String> = blocks_to_log.map(|b| format!("\n\t{}", b)).collect();
info!(
    "Restored {} blocks from ConsensusDB (showing first {}): {}",
    blocks.len(),
    MAX_BLOCKS_TO_LOG.min(blocks.len()),
    blocks_repr.concat()
);

let qc_to_log = quorum_certs.iter().take(MAX_BLOCKS_TO_LOG);
let qc_repr: Vec<String> = qc_to_log.map(|qc| format!("\n\t{}", qc)).collect();
info!(
    "Restored {} quorum certs from ConsensusDB (showing first {}): {}",
    quorum_certs.len(),
    MAX_BLOCKS_TO_LOG.min(quorum_certs.len()),
    qc_repr.concat()
);
```

**In RecoveryData::new(), replace lines 366-383:**

```rust
.with_context(|| {
    const MAX_BLOCKS_IN_ERROR: usize = 50;
    blocks.sort_by_key(|block| block.round());
    quorum_certs.sort_by_key(|qc| qc.certified_block().round());
    
    let blocks_summary = if blocks.len() <= MAX_BLOCKS_IN_ERROR {
        blocks.iter().map(|b| format!("\n{}", b)).collect::<Vec<_>>().concat()
    } else {
        format!("\n{} blocks total (showing first {} and last {}):\n{}...{}",
            blocks.len(),
            MAX_BLOCKS_IN_ERROR/2,
            MAX_BLOCKS_IN_ERROR/2,
            blocks.iter().take(MAX_BLOCKS_IN_ERROR/2).map(|b| format!("\n{}", b)).collect::<Vec<_>>().concat(),
            blocks.iter().rev().take(MAX_BLOCKS_IN_ERROR/2).map(|b| format!("\n{}", b)).collect::<Vec<_>>().concat()
        )
    };
    
    let qcs_summary = if quorum_certs.len() <= MAX_BLOCKS_IN_ERROR {
        quorum_certs.iter().map(|qc| format!("\n{}", qc)).collect::<Vec<_>>().concat()
    } else {
        format!("\n{} QCs total (showing first {} and last {})",
            quorum_certs.len(), MAX_BLOCKS_IN_ERROR/2, MAX_BLOCKS_IN_ERROR/2)
    };
    
    format!(
        "\nRoot: {}\nBlocks in db: {}\nQuorum Certs in db: {}\n",
        ledger_recovery_data.storage_ledger.ledger_info(),
        blocks_summary,
        qcs_summary
    )
})?;
```

**Additional Hardening**: Add a sanity check when loading from ConsensusDB:

```rust
// In ConsensusDB::get_data()
const MAX_CONSENSUS_BLOCKS_WARNING: usize = 10000;
let consensus_blocks = self.get_all::<BlockSchema>()?
    .into_iter()
    .map(|(_, block)| block)
    .collect::<Vec<_>>();

if consensus_blocks.len() > MAX_CONSENSUS_BLOCKS_WARNING {
    warn!(
        "ConsensusDB contains {} blocks (expected < {}). This may indicate a pruning issue.",
        consensus_blocks.len(),
        MAX_CONSENSUS_BLOCKS_WARNING
    );
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod oom_vulnerability_test {
    use super::*;
    use aptos_consensus_types::block::Block;
    use aptos_consensus_types::quorum_cert::QuorumCert;
    use aptos_crypto::HashValue;
    use tempfile::TempDir;
    
    #[test]
    #[ignore] // Run manually - will consume significant memory
    fn test_oom_on_recovery_with_million_blocks() {
        let temp_dir = TempDir::new().unwrap();
        let db = ConsensusDB::new(temp_dir.path());
        
        // Simulate corrupted DB with 1 million blocks
        const BLOCK_COUNT: usize = 1_000_000;
        
        println!("Creating {} blocks...", BLOCK_COUNT);
        for i in 0..BLOCK_COUNT {
            let block = Block::new_for_testing(
                HashValue::random(),
                i as u64, // round
                i as u64, // timestamp
                QuorumCert::certificate_for_genesis(),
                vec![], // payload
            );
            
            db.put::<BlockSchema>(&block.id(), &block).unwrap();
            
            if i % 100_000 == 0 {
                println!("Created {} blocks", i);
            }
        }
        
        println!("Attempting recovery with {} blocks in DB...", BLOCK_COUNT);
        
        // This will attempt to format all 1M blocks for logging
        // Expected: OOM or extremely long execution time
        let storage = StorageWriteProxy::new_for_testing(temp_dir.path());
        let _result = storage.start(false, None);
        
        // If we reach here without OOM, memory usage should be extreme
        println!("Recovery completed (or OOM occurred)");
    }
}
```

**Expected Behavior**: Running this test will demonstrate memory exhaustion when attempting to format millions of blocks during startup recovery. The test will either OOM or consume multiple gigabytes of memory in the formatting operations at lines 535-547 and potentially 365-383.

---

**Notes**

The vulnerability is particularly insidious because:
1. It occurs in error-handling and logging code paths that are meant to aid debugging
2. The failure mode (OOM) prevents normal recovery mechanisms from operating
3. There's no automatic mitigation - manual intervention is required to clean the database
4. It can be triggered by non-malicious system bugs rather than active attacks

This represents a violation of defensive programming principles where error handlers should be more robust than normal code paths, not less.

### Citations

**File:** consensus/src/persistent_liveness_storage.rs (L365-383)
```rust
            .with_context(|| {
                // for better readability
                blocks.sort_by_key(|block| block.round());
                quorum_certs.sort_by_key(|qc| qc.certified_block().round());
                format!(
                    "\nRoot: {}\nBlocks in db: {}\nQuorum Certs in db: {}\n",
                    ledger_recovery_data.storage_ledger.ledger_info(),
                    blocks
                        .iter()
                        .map(|b| format!("\n{}", b))
                        .collect::<Vec<String>>()
                        .concat(),
                    quorum_certs
                        .iter()
                        .map(|qc| format!("\n{}", qc))
                        .collect::<Vec<String>>()
                        .concat(),
                )
            })?;
```

**File:** consensus/src/persistent_liveness_storage.rs (L535-547)
```rust
        let blocks_repr: Vec<String> = blocks.iter().map(|b| format!("\n\t{}", b)).collect();
        info!(
            "The following blocks were restored from ConsensusDB : {}",
            blocks_repr.concat()
        );
        let qc_repr: Vec<String> = quorum_certs
            .iter()
            .map(|qc| format!("\n\t{}", qc))
            .collect();
        info!(
            "The following quorum certs were restored from ConsensusDB: {}",
            qc_repr.concat()
        );
```

**File:** consensus/src/consensusdb/mod.rs (L80-106)
```rust
    pub fn get_data(
        &self,
    ) -> Result<(
        Option<Vec<u8>>,
        Option<Vec<u8>>,
        Vec<Block>,
        Vec<QuorumCert>,
    )> {
        let last_vote = self.get_last_vote()?;
        let highest_2chain_timeout_certificate = self.get_highest_2chain_timeout_certificate()?;
        let consensus_blocks = self
            .get_all::<BlockSchema>()?
            .into_iter()
            .map(|(_, block)| block)
            .collect();
        let consensus_qcs = self
            .get_all::<QCSchema>()?
            .into_iter()
            .map(|(_, qc)| qc)
            .collect();
        Ok((
            last_vote,
            highest_2chain_timeout_certificate,
            consensus_blocks,
            consensus_qcs,
        ))
    }
```

**File:** consensus/src/consensusdb/mod.rs (L121-137)
```rust
    pub fn save_blocks_and_quorum_certificates(
        &self,
        block_data: Vec<Block>,
        qc_data: Vec<QuorumCert>,
    ) -> Result<(), DbError> {
        if block_data.is_empty() && qc_data.is_empty() {
            return Err(anyhow::anyhow!("Consensus block and qc data is empty!").into());
        }
        let mut batch = SchemaBatch::new();
        block_data
            .iter()
            .try_for_each(|block| batch.put::<BlockSchema>(&block.id(), block))?;
        qc_data
            .iter()
            .try_for_each(|qc| batch.put::<QCSchema>(&qc.certified_block().id(), qc))?;
        self.commit(batch)
    }
```

**File:** consensus/src/consensusdb/mod.rs (L201-205)
```rust
    pub fn get_all<S: Schema>(&self) -> Result<Vec<(S::Key, S::Value)>, DbError> {
        let mut iter = self.db.iter::<S>()?;
        iter.seek_to_first();
        Ok(iter.collect::<Result<Vec<(S::Key, S::Value)>, AptosDbError>>()?)
    }
```

**File:** consensus/consensus-types/src/block.rs (L64-80)
```rust
impl Display for Block {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        let author = self
            .author()
            .map(|addr| format!("{}", addr))
            .unwrap_or_else(|| "(NIL)".to_string());
        write!(
            f,
            "[id: {}, author: {}, epoch: {}, round: {:02}, parent_id: {}, timestamp: {}]",
            self.id,
            author,
            self.epoch(),
            self.round(),
            self.parent_id(),
            self.timestamp_usecs(),
        )
    }
```

**File:** consensus/consensus-types/src/quorum_cert.rs (L25-32)
```rust
impl Display for QuorumCert {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        write!(
            f,
            "QuorumCert: [{}, {}]",
            self.vote_data, self.signed_ledger_info
        )
    }
```
