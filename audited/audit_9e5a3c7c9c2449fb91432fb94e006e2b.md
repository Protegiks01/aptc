# Audit Report

## Title
State Sync Race Condition Causing JWK Consensus Config Divergence and Network Partition

## Summary
A critical vulnerability exists where validators with state sync lag can load different `OnChainJWKConsensusConfig` values during epoch transitions, causing consensus split when validating `ObservedJWKUpdate` validator transactions. This occurs due to a fallback mechanism that uses deprecated resources when the primary config fails to load, potentially returning different values than fully-synced validators.

## Finding Description

The vulnerability manifests in the epoch transition flow where validators load on-chain configurations. The critical code path is: [1](#0-0) 

When this fetch fails (due to state sync lag or database unavailability), the code falls back to deprecated resources: [2](#0-1) 

This fallback constructs the config from `Features` flag and `SupportedOIDCProviders` resources: [3](#0-2) [4](#0-3) 

The divergence creates a consensus split because validators validate proposals differently: [5](#0-4) [6](#0-5) [7](#0-6) 

**Attack Scenario:**

During an epoch transition where JWK consensus config changes (e.g., governance proposal enables JWK consensus):

1. **Governance proposal executes:** Calls `set_for_next_epoch()` with new config, then `reconfigure()` [8](#0-7) 

2. **Epoch transition applies config:** The `on_new_epoch()` function extracts and applies buffered config [9](#0-8) 

3. **ReconfigNotification created:** State sync creates notification with `DbBackedOnChainConfig` [10](#0-9) 

4. **Config read divergence:** When validators process the notification:
   - **Validator A** (fully synced): Successfully reads `JWKConsensusConfig` → `V1(providers)` → `jwk_consensus_enabled() = true`
   - **Validator B** (state sync lagging): Fails to read `JWKConsensusConfig`, falls back to `Features` flag → might get `Off` → `jwk_consensus_enabled() = false`

5. **Consensus split occurs:** When a proposal contains `ObservedJWKUpdate`:
   - Validator A accepts (votes for the proposal)
   - Validator B rejects (refuses to vote)
   - Neither group can form 2f+1 quorum → **network partition**

The config read happens through `DbBackedOnChainConfig.get()` which directly queries the database at a specific version: [11](#0-10) 

If a validator's database hasn't fully synced to the reconfiguration version when processing the notification, the read fails and triggers the fallback, causing divergence.

## Impact Explanation

**Critical Severity** - This meets the "Non-recoverable network partition (requires hardfork)" criterion:

1. **Complete Consensus Halt:** Validators split into two groups with incompatible validation rules. Neither group can form 2f+1 quorum, halting block production completely.

2. **Non-Recoverable Without Intervention:** Unlike temporary liveness failures, this split is permanent because each validator's config is locked for the entire epoch. Normal consensus recovery mechanisms cannot resolve this.

3. **Affects Core Protocol Safety:** Violates the fundamental invariant that all validators must execute identical validation logic for the same block. This breaks AptosBFT consensus safety guarantees.

4. **Requires Coordinated Recovery:** Would likely require emergency validator coordination, emergency governance proposal, or hardfork to resolve.

## Likelihood Explanation

**Medium-High Likelihood** during epoch transitions with config changes:

1. **Triggering Conditions Are Realistic:**
   - State sync lag is common during high network load or for validators with slower infrastructure
   - Epoch transitions with JWK config changes happen during governance-driven upgrades
   - The fallback mechanism is legitimate code path, not an edge case

2. **No Explicit Synchronization:** The code lacks validation that all validators have synced to the reconfiguration version before processing the notification. There's no wait mechanism ensuring database consistency.

3. **Race Condition Window:** A timing window exists between when state sync commits blocks and when consensus processes reconfig notifications. Fast validators process notifications while slower ones are still syncing.

4. **Production Occurrence:** This could manifest during major network upgrades when JWK consensus is being enabled/modified via governance proposals.

## Recommendation

Implement strict version synchronization before loading epoch configs:

```rust
// In epoch_manager.rs, before loading configs:
async fn start_new_epoch(&mut self, payload: OnChainConfigPayload<P>) {
    let reconfig_version = payload.version();
    
    // NEW: Wait for database to reach reconfiguration version
    self.wait_for_sync_to_version(reconfig_version).await?;
    
    // Existing config loading logic...
    let onchain_jwk_consensus_config: Result<OnChainJWKConsensusConfig> = payload.get();
    
    // REMOVE or LOG ERROR instead of silent fallback:
    let jwk_consensus_config = onchain_jwk_consensus_config
        .expect("JWKConsensusConfig must be readable at reconfiguration version");
    
    // Or with retry logic:
    let jwk_consensus_config = retry_with_timeout(|| payload.get(), TIMEOUT)
        .await
        .expect("Failed to load JWKConsensusConfig after retries");
}
```

Alternative: Add version validation in `DbBackedOnChainConfig::get()` to ensure database has reached the required version before reading.

**Critical Fix:** Remove or restrict the fallback mechanism to only genesis/bootstrap scenarios, never during normal epoch transitions. Log errors loudly instead of silently using potentially stale deprecated resources.

## Proof of Concept

```rust
// Reproduction steps (pseudocode for integration test):

// Setup: Two validators A and B starting at epoch N
// Validator A: Fast sync, fully caught up
// Validator B: Slow sync, lagging by 100 versions

// Step 1: Execute governance proposal to enable JWK consensus
governance_proposal_enable_jwk_consensus();
// This triggers epoch N -> N+1 transition at version V

// Step 2: Validator A processes reconfiguration
// - State sync at version V
// - Loads OnChainJWKConsensusConfig successfully
// - jwk_consensus_config = V1(providers)

// Step 3: Validator B processes reconfiguration notification  
// - State sync still at version V-100
// - Attempts to load OnChainJWKConsensusConfig at version V
// - Database read fails (version not yet synced)
// - Falls back to Features flag
// - jwk_consensus_config = Off (or different value)

// Step 4: Leader proposes block with ObservedJWKUpdate
let proposal = create_proposal_with_vtxn(ValidatorTransaction::ObservedJWKUpdate(...));

// Step 5: Validators validate proposal
// Validator A: is_vtxn_expected() returns true -> votes
// Validator B: is_vtxn_expected() returns false -> rejects with "unexpected validator txn"

// Result: Consensus split, no quorum can be formed
assert_eq!(validator_a.can_vote(proposal), true);
assert_eq!(validator_b.can_vote(proposal), false);
assert!(network_is_partitioned());
```

## Notes

This vulnerability exists at the intersection of state sync timing and consensus configuration loading. The fallback mechanism, while intended for backward compatibility with networks that haven't initialized `JWKConsensusConfig`, creates a dangerous code path during normal epoch transitions. The lack of explicit version synchronization between state sync and consensus components allows validators to proceed with potentially divergent configs, breaking the fundamental consensus invariant of deterministic validation.

### Citations

**File:** consensus/src/epoch_manager.rs (L1184-1184)
```rust
        let onchain_jwk_consensus_config: anyhow::Result<OnChainJWKConsensusConfig> = payload.get();
```

**File:** consensus/src/epoch_manager.rs (L1223-1226)
```rust
        let jwk_consensus_config = onchain_jwk_consensus_config.unwrap_or_else(|_| {
            // `jwk_consensus_config` not yet initialized, falling back to the old configs.
            Self::equivalent_jwk_consensus_config_from_deprecated_resources(&payload)
        });
```

**File:** consensus/src/epoch_manager.rs (L1963-1969)
```rust
    fn equivalent_jwk_consensus_config_from_deprecated_resources(
        payload: &OnChainConfigPayload<P>,
    ) -> OnChainJWKConsensusConfig {
        let features = payload.get::<Features>().ok();
        let oidc_providers = payload.get::<SupportedOIDCProviders>().ok();
        OnChainJWKConsensusConfig::from((features, oidc_providers))
    }
```

**File:** types/src/on_chain_config/jwk_consensus_config.rs (L112-132)
```rust
impl From<(Option<Features>, Option<SupportedOIDCProviders>)> for OnChainJWKConsensusConfig {
    fn from(
        (features, supported_oidc_providers): (Option<Features>, Option<SupportedOIDCProviders>),
    ) -> Self {
        if let Some(features) = features {
            if features.is_enabled(FeatureFlag::JWK_CONSENSUS) {
                let oidc_providers = supported_oidc_providers
                    .unwrap_or_default()
                    .providers
                    .into_iter()
                    .filter_map(|deprecated| OIDCProvider::try_from(deprecated).ok())
                    .collect();
                OnChainJWKConsensusConfig::V1(ConfigV1 { oidc_providers })
            } else {
                OnChainJWKConsensusConfig::Off
            }
        } else {
            OnChainJWKConsensusConfig::Off
        }
    }
}
```

**File:** consensus/src/util/mod.rs (L15-24)
```rust
pub fn is_vtxn_expected(
    randomness_config: &OnChainRandomnessConfig,
    jwk_consensus_config: &OnChainJWKConsensusConfig,
    vtxn: &ValidatorTransaction,
) -> bool {
    match vtxn {
        ValidatorTransaction::DKGResult(_) => randomness_config.randomness_enabled(),
        ValidatorTransaction::ObservedJWKUpdate(_) => jwk_consensus_config.jwk_consensus_enabled(),
    }
}
```

**File:** consensus/src/round_manager.rs (L1126-1137)
```rust
        if let Some(vtxns) = proposal.validator_txns() {
            for vtxn in vtxns {
                let vtxn_type_name = vtxn.type_name();
                ensure!(
                    is_vtxn_expected(&self.randomness_config, &self.jwk_consensus_config, vtxn),
                    "unexpected validator txn: {:?}",
                    vtxn_type_name
                );
                vtxn.verify(self.epoch_state.verifier.as_ref())
                    .context(format!("{} verify failed", vtxn_type_name))?;
            }
        }
```

**File:** consensus/src/dag/rb_handler.rs (L122-131)
```rust
        for vtxn in node.validator_txns() {
            let vtxn_type_name = vtxn.type_name();
            ensure!(
                is_vtxn_expected(&self.randomness_config, &self.jwk_consensus_config, vtxn),
                "unexpected validator transaction: {:?}",
                vtxn_type_name
            );
            vtxn.verify(self.epoch_state.verifier.as_ref())
                .context(format!("{} verification failed", vtxn_type_name))?;
        }
```

**File:** aptos-move/framework/aptos-framework/sources/configs/jwk_consensus_config.move (L62-65)
```text
    public fun set_for_next_epoch(framework: &signer, config: JWKConsensusConfig) {
        system_addresses::assert_aptos_framework(framework);
        config_buffer::upsert(config);
    }
```

**File:** aptos-move/framework/aptos-framework/sources/configs/jwk_consensus_config.move (L68-78)
```text
    public(friend) fun on_new_epoch(framework: &signer) acquires JWKConsensusConfig {
        system_addresses::assert_aptos_framework(framework);
        if (config_buffer::does_exist<JWKConsensusConfig>()) {
            let new_config = config_buffer::extract_v2<JWKConsensusConfig>();
            if (exists<JWKConsensusConfig>(@aptos_framework)) {
                *borrow_global_mut<JWKConsensusConfig>(@aptos_framework) = new_config;
            } else {
                move_to(framework, new_config);
            };
        }
    }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L281-307)
```rust
    fn read_on_chain_configs(
        &self,
        version: Version,
    ) -> Result<OnChainConfigPayload<DbBackedOnChainConfig>, Error> {
        let db_state_view = &self
            .storage
            .read()
            .reader
            .state_view_at_version(Some(version))
            .map_err(|error| {
                Error::UnexpectedErrorEncountered(format!(
                    "Failed to create account state view {:?}",
                    error
                ))
            })?;
        let epoch = ConfigurationResource::fetch_config(&db_state_view)
            .ok_or_else(|| {
                Error::UnexpectedErrorEncountered("Configuration resource does not exist!".into())
            })?
            .epoch();

        // Return the new on-chain config payload (containing all found configs at this version).
        Ok(OnChainConfigPayload::new(
            epoch,
            DbBackedOnChainConfig::new(self.storage.read().reader.clone(), version),
        ))
    }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L397-412)
```rust
impl OnChainConfigProvider for DbBackedOnChainConfig {
    fn get<T: OnChainConfig>(&self) -> Result<T> {
        let bytes = self
            .reader
            .get_state_value_by_version(&StateKey::on_chain_config::<T>()?, self.version)?
            .ok_or_else(|| {
                anyhow!(
                    "no config {} found in aptos root account state",
                    T::CONFIG_ID
                )
            })?
            .bytes()
            .clone();

        T::deserialize_into_config(&bytes)
    }
```
