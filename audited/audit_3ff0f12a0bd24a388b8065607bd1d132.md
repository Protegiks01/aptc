# Audit Report

## Title
Missing Configuration Validation: Zero prune_window Causes Historical State Unavailability and Node Dysfunction

## Summary
The `StateKvPrunerManager::new()` function and related pruner managers accept `prune_window=0` without validation when pruning is enabled. This causes the minimum readable version to equal the latest version, making all historical state queries fail and breaking state synchronization, effectively rendering the node non-functional for network participation.

## Finding Description

The `prune_window` parameter controls how many historical versions to retain in the database. When a pruner manager is initialized with `enable=true` and `prune_window=0`, no validation prevents this invalid configuration from being accepted. [1](#0-0) 

The configuration structure allows `prune_window` to be any u64 value, including zero: [2](#0-1) 

The configuration sanitizer only issues warnings for small values but does not enforce a minimum or reject zero: [3](#0-2) 

When the pruner calculates the minimum readable version, it uses saturating subtraction: [4](#0-3) 

With `prune_window=0`, the calculation becomes `min_readable_version = latest_version - 0 = latest_version`, meaning only the current version is considered readable.

State access validation checks fail for any historical query: [5](#0-4) 

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs" - historical states become unavailable even though they exist in the database, breaking verifiability and state synchronization protocols that require historical state access.

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos Bug Bounty criteria: "State inconsistencies requiring intervention."

**Concrete Impact:**
- Node cannot serve historical state queries (all fail with "pruned" error)
- State synchronization fails - other nodes cannot sync from this node
- Fast sync mode breaks - requires historical epoch snapshots
- APIs return errors for any non-latest version queries
- Node effectively becomes isolated from the network

**Network-Wide Impact if Multiple Nodes Misconfigured:**
- Reduced network data availability
- State sync failures across the network
- Potential liveness degradation if enough nodes affected
- Requires manual intervention to correct configurations and potentially restore nodes

The issue does not cause direct fund loss or consensus safety violations, but it causes significant operational disruption and state availability failures that require manual intervention.

## Likelihood Explanation

**Likelihood: Low to Medium**

This vulnerability requires a configuration error by a node operator. Scenarios include:

1. **Human Error:** Operator misunderstands the prune_window parameter and sets it to 0 thinking it disables pruning (when they should set `enable=false`)
2. **Configuration Template Errors:** Automated configuration generation scripts contain bugs
3. **Copy-Paste Errors:** Using test configurations (which deliberately use prune_window=0) in production
4. **Migration Errors:** Incorrect configuration during node upgrades or migrations

The existing test code demonstrates that `prune_window=0` is used intentionally in test scenarios: [6](#0-5) 

This increases the likelihood that operators might mistakenly use similar configurations in production, not realizing the severe consequences.

## Recommendation

Add mandatory configuration validation in the sanitizer to reject `prune_window=0` when pruning is enabled:

```rust
// In config/src/config/storage_config.rs, within sanitize() function:

if config.storage_pruner_config.ledger_pruner_config.enable 
    && config.storage_pruner_config.ledger_pruner_config.prune_window == 0 {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "ledger_pruner_config.prune_window cannot be 0 when pruning is enabled. Set a minimum value (recommended: >= 50_000_000) or disable pruning with enable=false.".to_string(),
    ));
}

if config.storage_pruner_config.state_merkle_pruner_config.enable 
    && config.storage_pruner_config.state_merkle_pruner_config.prune_window == 0 {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "state_merkle_pruner_config.prune_window cannot be 0 when pruning is enabled. Set a minimum value (recommended: >= 100_000) or disable pruning with enable=false.".to_string(),
    ));
}

if config.storage_pruner_config.epoch_snapshot_pruner_config.enable 
    && config.storage_pruner_config.epoch_snapshot_pruner_config.prune_window == 0 {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "epoch_snapshot_pruner_config.prune_window cannot be 0 when pruning is enabled. Set a minimum value (recommended: >= 50_000_000) or disable pruning with enable=false.".to_string(),
    ));
}
```

Additionally, convert the existing warnings to hard errors for values below recommended minimums to prevent production misconfigurations.

## Proof of Concept

```rust
// Add this test to storage/aptosdb/src/pruner/state_kv_pruner/test.rs

#[test]
#[should_panic(expected = "is pruned")]
fn test_zero_prune_window_breaks_historical_queries() {
    use crate::db::AptosDB;
    use aptos_config::config::LedgerPrunerConfig;
    use aptos_temppath::TempPath;
    use aptos_types::state_store::{StateKey, StateValue};
    
    let tmp_dir = TempPath::new();
    let aptos_db = AptosDB::new_for_test(&tmp_dir);
    let state_store = &aptos_db.state_store;
    
    // Insert values at versions 0, 1, 2
    let key = StateKey::raw(b"test_key");
    for version in 0..3 {
        let value = StateValue::from(vec![version as u8]);
        state_store.commit_block_for_test(
            version,
            [vec![(key.clone(), Some(value))].into_iter()]
        );
    }
    
    // Create pruner with prune_window=0 and enable=true
    let pruner = StateKvPrunerManager::new(
        aptos_db.state_kv_db.clone(),
        LedgerPrunerConfig {
            enable: true,
            prune_window: 0,  // INVALID: should be rejected
            batch_size: 1,
            user_pruning_window_offset: 0,
        }
    );
    
    // Wake pruner at version 2
    pruner.wake_and_wait_pruner(2).unwrap();
    
    // Now min_readable_version = 2 - 0 = 2
    // Queries for versions 0 and 1 should fail even though data exists
    let result = state_store.get_state_value_with_proof_by_version(&key, 0);
    
    // This will panic with "is pruned" error, demonstrating the vulnerability
    result.expect("Version 0 should be readable but is marked as pruned");
}
```

**Notes**

- The vulnerability stems from missing input validation at the configuration layer, allowing an invalid state (`enable=true` with `prune_window=0`) to be accepted
- While this requires operator access to modify configurations, proper input validation is a defense-in-depth security measure that prevents human errors from causing operational failures
- The same pattern affects all pruner managers (LedgerPruner, StateMerklePruner, EpochSnapshotPruner) as they all use the same configuration validation logic
- The NO_OP_STORAGE_PRUNER_CONFIG constant demonstrates the intended pattern: when `prune_window=0`, `enable` should be `false` [7](#0-6)

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L84-108)
```rust
    pub fn new(state_kv_db: Arc<StateKvDb>, state_kv_pruner_config: LedgerPrunerConfig) -> Self {
        let pruner_worker = if state_kv_pruner_config.enable {
            Some(Self::init_pruner(
                Arc::clone(&state_kv_db),
                state_kv_pruner_config,
            ))
        } else {
            None
        };

        let min_readable_version =
            pruner_utils::get_state_kv_pruner_progress(&state_kv_db).expect("Must succeed.");

        PRUNER_VERSIONS
            .with_label_values(&["state_kv_pruner", "min_readable"])
            .set(min_readable_version as i64);

        Self {
            state_kv_db,
            prune_window: state_kv_pruner_config.prune_window,
            pruner_worker,
            pruning_batch_size: state_kv_pruner_config.batch_size,
            min_readable_version: AtomicVersion::new(min_readable_version),
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L128-142)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["state_kv_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** config/src/config/storage_config.rs (L306-323)
```rust
pub const NO_OP_STORAGE_PRUNER_CONFIG: PrunerConfig = PrunerConfig {
    ledger_pruner_config: LedgerPrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
        user_pruning_window_offset: 0,
    },
    state_merkle_pruner_config: StateMerklePrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
    },
    epoch_snapshot_pruner_config: EpochSnapshotPrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
    },
};
```

**File:** config/src/config/storage_config.rs (L327-341)
```rust
pub struct LedgerPrunerConfig {
    /// Boolean to enable/disable the ledger pruner. The ledger pruner is responsible for pruning
    /// everything else except for states (e.g. transactions, events etc.)
    pub enable: bool,
    /// This is the default pruning window for any other store except for state store. State store
    /// being big in size, we might want to configure a smaller window for state store vs other
    /// store.
    pub prune_window: u64,
    /// Batch size of the versions to be sent to the ledger pruner - this is to avoid slowdown due to
    /// issuing too many DB calls and batch prune instead. For ledger pruner, this means the number
    /// of versions to prune a time.
    pub batch_size: usize,
    /// The offset for user pruning window to adjust
    pub user_pruning_window_offset: u64,
}
```

**File:** config/src/config/storage_config.rs (L708-716)
```rust
        if ledger_prune_window < 50_000_000 {
            warn!("Ledger prune_window is too small, harming network data availability.");
        }
        if state_merkle_prune_window < 100_000 {
            warn!("State Merkle prune_window is too small, node might stop functioning.");
        }
        if epoch_snapshot_prune_window < 50_000_000 {
            warn!("Epoch snapshot prune_window is too small, harming network data availability.");
        }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L305-315)
```rust
    pub(super) fn error_if_state_kv_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.state_store.state_kv_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/test.rs (L54-63)
```rust
fn create_state_merkle_pruner_manager(
    state_merkle_db: &Arc<StateMerkleDb>,
    prune_batch_size: usize,
) -> StateMerklePrunerManager<StaleNodeIndexSchema> {
    StateMerklePrunerManager::new(Arc::clone(state_merkle_db), StateMerklePrunerConfig {
        enable: true,
        prune_window: 0,
        batch_size: prune_batch_size,
    })
}
```
