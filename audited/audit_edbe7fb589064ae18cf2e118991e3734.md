# Audit Report

## Title
Missing Duplicate Ciphertext ID Validation in Batch Encryption Digest Computation Enables Transaction Censorship

## Summary
The `digest()` function in the FPTX batch encryption scheme fails to validate for duplicate ciphertext IDs, allowing malicious actors to cause encrypted transactions to fail decryption. The empty error message at line 107 is currently dead code but indicates missing validation logic that should detect and reject duplicate IDs.

## Finding Description

The batch encryption system used for encrypted transactions in Aptos consensus has a critical validation gap. In the `digest()` function, ciphertext IDs are collected and processed without checking for duplicates: [1](#0-0) 

The `IdSet::from_slice()` method accepts duplicate IDs without validation: [2](#0-1) 

The implementation's `add()` method simply pushes IDs to a vector without deduplication: [3](#0-2) 

When duplicate IDs exist, the polynomial construction creates repeated roots (e.g., `(X - id1)² · (X - id2)`), and evaluation proofs are computed for all positions. However, when proofs are stored in a HashMap, only one proof per unique ID is retained (the last occurrence): [4](#0-3) 

This causes earlier occurrences of duplicate IDs to receive incorrect evaluation proofs. During decryption, these transactions fail verification: [5](#0-4) 

Failed transactions are marked as `FailedDecryption`, and when the VM attempts execution, it discards them with a misleading error: [6](#0-5) 

**Attack Path:**
1. Attacker creates multiple encrypted transactions with identical ciphertext IDs (by reusing verification keys or hash collisions)
2. Validator includes multiple transactions with duplicate IDs in a block
3. Consensus pipeline computes digest over ciphertexts with duplicate IDs
4. HashMap overwrites earlier proofs, leaving first occurrences with incorrect proofs
5. Decryption fails deterministically for first occurrences
6. Valid transactions are censored from execution

This breaks the **Deterministic Execution** invariant in an unintended way - while execution is deterministic (all validators censor the same transactions), valid transactions are incorrectly rejected.

## Impact Explanation

This is a **Medium Severity** vulnerability per Aptos bug bounty criteria because it causes:

1. **State inconsistencies requiring intervention**: Valid encrypted transactions are incorrectly failed and censored, requiring users to resubmit with different ciphertext IDs. The empty error message provides no diagnostic information.

2. **Transaction censorship**: An attacker can selectively cause specific encrypted transactions to fail by submitting duplicates, effectively censoring legitimate transactions from execution.

3. **Protocol violation**: The batch encryption scheme's cryptographic correctness relies on unique IDs for polynomial evaluation, but this requirement is not enforced.

While the issue is deterministic (preventing consensus divergence), it violates the expected behavior that valid encrypted transactions should be processed successfully. The impact is limited to encrypted transaction availability rather than fund loss or consensus safety.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability can manifest in several scenarios:

1. **Malicious user submission**: Users can deliberately create encrypted transactions with duplicate IDs by reusing verification keys
2. **Hash collisions**: Though unlikely with proper cryptographic hashing, accidental ID collisions could occur over time
3. **Implementation errors**: Code changes or bugs in ciphertext generation could inadvertently produce duplicates

The attack requires no special privileges - any transaction submitter can potentially trigger it. However, there may be upstream validation in the mempool or transaction verification that prevents duplicates from reaching the digest computation (not confirmed in this analysis).

## Recommendation

**Immediate Fix:**

1. Add duplicate detection in `IdSet::from_slice()` with proper error message:

```rust
pub fn from_slice(ids: &[Id]) -> Option<Self> {
    let mut result = Self::with_capacity(ids.len())?;
    let mut seen = std::collections::HashSet::new();
    
    for id in ids {
        if !seen.insert(*id) {
            return None; // Duplicate detected
        }
        result.add(id);
    }
    Some(result)
}
```

2. Update the error message in `fptx.rs` line 107:

```rust
.ok_or(anyhow!("Duplicate ciphertext IDs detected in batch - each ciphertext must have a unique ID"))?;
```

3. Add validation at block proposal time to reject blocks containing encrypted transactions with duplicate ciphertext IDs.

**Comprehensive Fix:**

Add upstream validation in the transaction verification pipeline before transactions reach consensus, ensuring duplicate ciphertext IDs are rejected early with clear error messages.

## Proof of Concept

```rust
#[test]
fn test_duplicate_ciphertext_ids_cause_decryption_failure() {
    use crate::shared::ids::{Id, IdSet};
    use ark_std::rand::thread_rng;
    use ark_ff::UniformRand;
    
    let mut rng = thread_rng();
    
    // Create three IDs where id1 appears twice
    let id1 = Id::new(Fr::rand(&mut rng));
    let id2 = Id::new(Fr::rand(&mut rng));
    let ids_with_duplicate = vec![id1, id2, id1]; // id1 is duplicated
    
    // This should fail but currently succeeds
    let id_set = IdSet::from_slice(&ids_with_duplicate);
    assert!(id_set.is_some(), "IdSet accepts duplicate IDs without validation");
    
    // Compute polynomial coefficients
    let id_set = id_set.unwrap().compute_poly_coeffs();
    
    // Create digest key for testing
    let digest_key = DigestKey::new(&mut rng, 8, 1).unwrap();
    
    // Compute evaluation proofs
    let proofs = id_set.compute_all_eval_proofs_with_setup(&digest_key, 0);
    
    // The HashMap will only contain 2 entries (id1 and id2), not 3
    assert_eq!(proofs.len(), 2, "HashMap collapsed duplicates");
    
    // The first occurrence of id1 will get the wrong proof
    // (proof for position 2 instead of position 0)
    // This would cause decryption failure in real usage
    
    println!("Vulnerability confirmed: Duplicate IDs accepted, proofs collapsed in HashMap");
}
```

**Notes:**
- The empty error message at line 107 is currently unreachable because `with_capacity()` always returns `Some(...)`, making this dead code
- The real vulnerability is the missing duplicate validation, not the empty error message itself
- The empty message would only become relevant if validation was added but indicates poor error handling design
- This analysis assumes no upstream validation exists in mempool or block proposal - additional investigation needed to confirm full exploitability
- While execution is deterministic across validators, the behavior (censoring valid transactions) is incorrect and violates protocol expectations

### Citations

**File:** crates/aptos-batch-encryption/src/schemes/fptx.rs (L105-110)
```rust
        let mut ids: IdSet<UncomputedCoeffs> =
            IdSet::from_slice(&cts.iter().map(|ct| ct.id()).collect::<Vec<Id>>())
                .ok_or(anyhow!(""))?;

        digest_key.digest(&mut ids, round)
    }
```

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L63-69)
```rust
    pub fn from_slice(ids: &[Id]) -> Option<Self> {
        let mut result = Self::with_capacity(ids.len())?;
        for id in ids {
            result.add(id);
        }
        Some(result)
    }
```

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L84-89)
```rust
    pub fn add(&mut self, id: &Id) {
        if self.poly_roots.len() >= self.capacity {
            panic!("Number of ids must be less than capacity");
        }
        self.poly_roots.push(id.root_x);
    }
```

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L140-145)
```rust
        HashMap::from_iter(
            self.as_vec()
                .into_iter()
                .zip(pfs)
                .collect::<Vec<(Id, G1Affine)>>(),
        )
```

**File:** consensus/src/pipeline/decryption_pipeline_builder.rs (L121-148)
```rust
        let decrypted_txns = encrypted_txns
            .into_par_iter()
            .zip(txn_ciphertexts)
            .map(|(mut txn, ciphertext)| {
                let eval_proof = proofs.get(&ciphertext.id()).expect("must exist");
                if let Ok(payload) = FPTXWeighted::decrypt_individual::<DecryptedPayload>(
                    &decryption_key.key,
                    &ciphertext,
                    &digest,
                    &eval_proof,
                ) {
                    let (executable, nonce) = payload.unwrap();
                    txn.payload_mut()
                        .as_encrypted_payload_mut()
                        .map(|p| {
                            p.into_decrypted(eval_proof, executable, nonce)
                                .expect("must happen")
                        })
                        .expect("must exist");
                } else {
                    txn.payload_mut()
                        .as_encrypted_payload_mut()
                        .map(|p| p.into_failed_decryption(eval_proof).expect("must happen"))
                        .expect("must exist");
                }
                txn
            })
            .collect();
```

**File:** aptos-move/aptos-vm/src/aptos_vm.rs (L2061-2064)
```rust
        let executable = match txn.executable_ref() {
            Ok(executable) => executable,
            Err(_) => return unwrap_or_discard!(Err(deprecated_module_bundle!())),
        };
```
