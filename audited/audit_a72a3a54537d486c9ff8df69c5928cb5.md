# Audit Report

## Title
Memory Exhaustion and Database Corruption in Ledger Truncation Due to Unbounded Batch Accumulation

## Summary
The `delete_per_version_data_impl()` function accumulates delete operations in an unbounded `SchemaBatch` when processing large version ranges, causing memory exhaustion and node crashes. Additionally, a critical ordering bug commits the progress marker before the actual data deletion, leading to database corruption if the deletion fails.

## Finding Description

The vulnerability exists in the ledger database truncation logic where massive version ranges cause two critical issues:

**Issue 1: Unbounded Memory Accumulation** [1](#0-0) 

The function iterates through all versions from `start_version` to `latest_version` and accumulates delete operations in a single `SchemaBatch` with no size limits. The `SchemaBatch` is implemented as a `HashMap<ColumnFamilyName, Vec<WriteOp>>` with no memory bounds: [2](#0-1) 

For large version ranges (e.g., 1+ million versions), this accumulates millions of delete operations in memory before committing. Each version requires deletions across multiple schemas (TransactionSchema, TransactionInfoSchema, VersionDataSchema, WriteSetSchema, etc.), multiplying the memory footprint.

**Issue 2: Critical Ordering Bug - Progress Before Deletion** [3](#0-2) 

The progress marker is committed to the database (line 358) BEFORE the actual deletion batch (line 360). If the deletion fails due to memory exhaustion, timeout, or crash, the progress marker indicates truncation is complete while the data remains in the database, causing permanent database corruption.

**Issue 3: No Batching in Ledger Truncation**

Unlike `truncate_state_kv_db` which implements batching: [4](#0-3) 

The `truncate_ledger_db` function has no batching mechanism despite the `ledger_db_batch_size` CLI parameter existing: [5](#0-4) 

This parameter is never actually used, and all truncation happens in a single batch: [6](#0-5) 

**Attack Vector**

During `StateStore` initialization, if version difference exceeds `MAX_COMMIT_PROGRESS_DIFFERENCE` (1 million versions): [7](#0-6) 

The node panics with `crash_if_difference_is_too_large=true`: [8](#0-7) 

However, when using the db_debugger tool with `crash_if_difference_is_too_large=false`, no limit is enforced, allowing unbounded memory accumulation.

## Impact Explanation

**High Severity** - This meets the Aptos bug bounty criteria for High Severity:

1. **Validator Node Crashes**: Memory exhaustion from accumulating millions of delete operations causes out-of-memory crashes during node sync operations
2. **Database Corruption**: The progress-before-deletion ordering bug creates permanent database inconsistency requiring manual intervention or restoration from backup
3. **Validator Unavailability**: Affected validators cannot start up or sync, impacting network availability

While the 1M version limit prevents the worst cases during normal initialization, validators that fall behind or operators using the truncate tool can still trigger memory exhaustion. The database corruption issue affects all truncation operations regardless of size.

## Likelihood Explanation

**Medium-to-High Likelihood**:
- Validators syncing after downtime may accumulate large version gaps
- Manual truncation operations by operators using the db_debugger tool
- The 1M version limit still allows processing large batches that consume significant memory
- No timeout or recovery mechanism exists if the operation fails partway through

## Recommendation

**1. Implement batching for ledger truncation:**

```rust
pub(crate) fn truncate_ledger_db(
    ledger_db: Arc<LedgerDb>, 
    target_version: Version,
    batch_size: usize
) -> Result<()> {
    let transaction_store = TransactionStore::new(Arc::clone(&ledger_db));
    let mut current_version = get_ledger_latest_version(ledger_db)?;
    
    while current_version > target_version {
        let batch_start = std::cmp::max(
            current_version.saturating_sub(batch_size as u64),
            target_version + 1
        );
        truncate_ledger_db_single_batch(&ledger_db, &transaction_store, batch_start)?;
        current_version = batch_start - 1;
    }
    Ok(())
}
```

**2. Fix the ordering bug - commit progress AFTER successful deletion:**

```rust
fn truncate_ledger_db_single_batch(
    ledger_db: &LedgerDb,
    transaction_store: &TransactionStore,
    start_version: Version,
) -> Result<()> {
    let mut batch = LedgerDbSchemaBatches::new();
    
    delete_transaction_index_data(ledger_db, transaction_store, start_version, &mut batch.transaction_db_batches)?;
    delete_per_epoch_data(&ledger_db.metadata_db_arc(), start_version, &mut batch.ledger_metadata_db_batches)?;
    delete_per_version_data(ledger_db, start_version, &mut batch)?;
    delete_event_data(ledger_db, start_version, &mut batch.event_db_batches)?;
    truncate_transaction_accumulator(ledger_db.transaction_accumulator_db_raw(), start_version, &mut batch.transaction_accumulator_db_batches)?;
    
    // Commit data deletion FIRST
    ledger_db.write_schemas(batch)?;
    
    // Then update progress marker AFTER successful deletion
    let mut progress_batch = SchemaBatch::new();
    progress_batch.put::<DbMetadataSchema>(
        &DbMetadataKey::LedgerCommitProgress,
        &DbMetadataValue::Version(start_version - 1),
    )?;
    ledger_db.metadata_db().write_schemas(progress_batch)?;
    
    Ok(())
}
```

**3. Actually use the `ledger_db_batch_size` parameter in the CLI tool.**

## Proof of Concept

```rust
#[test]
fn test_truncation_memory_exhaustion() {
    use aptos_temppath::TempPath;
    use crate::db::AptosDB;
    
    let tmp_dir = TempPath::new();
    let db = AptosDB::new_for_test(&tmp_dir);
    
    // Commit 2 million versions
    for i in 0..2_000_000 {
        let txns = vec![/* create test transaction */];
        db.save_transactions_for_test(&txns, i, None, true).unwrap();
    }
    
    drop(db);
    
    // Attempt truncation - should cause memory exhaustion
    let (ledger_db, _, _, _) = AptosDB::open_dbs(&tmp_dir, /* configs */);
    
    // This will accumulate 2M versions worth of delete operations in memory
    truncate_ledger_db(ledger_db, 0).unwrap(); // Will likely OOM
}
```

## Notes

This vulnerability breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable." The progress-before-deletion ordering creates a window where the database state is inconsistent with its progress markers, violating atomicity guarantees essential for consensus correctness.

### Citations

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L73-79)
```rust
pub(crate) fn truncate_ledger_db(ledger_db: Arc<LedgerDb>, target_version: Version) -> Result<()> {
    let transaction_store = TransactionStore::new(Arc::clone(&ledger_db));

    let start_version = target_version + 1;
    truncate_ledger_db_single_batch(&ledger_db, &transaction_store, start_version)?;
    Ok(())
}
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L353-360)
```rust
    let mut progress_batch = SchemaBatch::new();
    progress_batch.put::<DbMetadataSchema>(
        &DbMetadataKey::LedgerCommitProgress,
        &DbMetadataValue::Version(start_version - 1),
    )?;
    ledger_db.metadata_db().write_schemas(progress_batch)?;

    ledger_db.write_schemas(batch)
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L494-518)
```rust
fn delete_per_version_data_impl<S>(
    ledger_db: &DB,
    start_version: Version,
    batch: &mut SchemaBatch,
) -> Result<()>
where
    S: Schema<Key = Version>,
{
    let mut iter = ledger_db.iter::<S>()?;
    iter.seek_to_last();
    if let Some((latest_version, _)) = iter.next().transpose()? {
        if latest_version >= start_version {
            info!(
                start_version = start_version,
                latest_version = latest_version,
                cf_name = S::COLUMN_FAMILY_NAME,
                "Truncate per version data."
            );
            for version in start_version..=latest_version {
                batch.delete::<S>(&version)?;
            }
        }
    }
    Ok(())
}
```

**File:** storage/schemadb/src/batch.rs (L129-133)
```rust
#[derive(Debug, Default)]
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}
```

**File:** storage/aptosdb/src/state_store/mod.rs (L107-107)
```rust
pub const MAX_COMMIT_PROGRESS_DIFFERENCE: u64 = 1_000_000;
```

**File:** storage/aptosdb/src/state_store/mod.rs (L444-449)
```rust
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");
```

**File:** storage/aptosdb/src/state_store/mod.rs (L461-467)
```rust
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");
```

**File:** storage/aptosdb/src/db_debugger/truncate/mod.rs (L34-35)
```rust
    #[clap(long, default_value_t = 1000)]
    ledger_db_batch_size: usize,
```
