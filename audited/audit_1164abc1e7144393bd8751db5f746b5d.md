# Audit Report

## Title
Transaction Deduper Allows Multiple Transactions with Same Sequence Number, Enabling Consensus Bandwidth DoS

## Summary
The `TxnHashAndAuthenticatorDeduper` in the consensus layer only filters transactions with identical `(hash, authenticator)` pairs, but allows multiple different transactions from the same sender with the same sequence number to pass through consensus. This wastes block space and validator resources, creating a denial-of-service vector where attackers can spam the network with transaction variations that ultimately fail execution.

## Finding Description
The transaction deduplication mechanism in the consensus layer is designed to filter duplicate transactions before they consume block space. However, the current implementation has a critical flaw in how it identifies duplicates. [1](#0-0) 

The deduper correctly identifies transactions with the same `(sender, replay_protector)` as **possible duplicates** in Step 1. For sequence-number-based transactions, this means transactions with the same sender and sequence number are flagged.

However, in Step 3, the deduper only filters transactions that have the **exact same** `(transaction_hash, authenticator)` pair: [2](#0-1) 

This means if an attacker creates two transactions with:
- Same sender (Alice)
- Same sequence number (5)
- Different payloads (transfer to Bob vs transfer to Charlie)
- Different signatures (signed with different entropy or gas prices)

Both transactions will have different hashes and pass through the deduper. They will both be included in consensus blocks, consume block space, undergo signature verification, and reach execution. During execution, only the first transaction will succeed (sequence number check in prologue), while the second will fail. [3](#0-2) 

The test explicitly demonstrates and **approves** this behavior with the comment "Should not be filtered" on line 246, indicating this is intentional design rather than an oversight.

The prologue correctly enforces that only one transaction per sequence number succeeds: [4](#0-3) 

**Attack Scenario:**
1. Attacker creates 100 variations of the same transaction (same sender, same sequence number, different payloads)
2. Attacker submits all 100 to different validators or mempool nodes
3. All 100 transactions pass through the deduper and enter consensus blocks
4. All validators must process all 100 transactions (filtering, deduplication, shuffling, signature verification)
5. During execution, only the first succeeds; the remaining 99 fail with `PROLOGUE_ESEQUENCE_NUMBER_TOO_OLD`
6. This wastes 99x block space and validator computational resources

## Impact Explanation
This vulnerability falls under **High Severity** per the Aptos bug bounty criteria for the following reasons:

1. **Validator node slowdowns**: An attacker can force validators to process hundreds of variations of the same transaction, consuming CPU cycles for signature verification, deduplication processing, and execution prologue checks. This directly causes validator slowdowns.

2. **Consensus bandwidth waste**: Block space is a critical resource in blockchain systems. Allowing multiple transactions with the same sequence number to occupy block slots that could be used for valid transactions reduces network throughput.

3. **Denial of Service vector**: A coordinated attack with thousands of duplicate-sequence-number transactions could severely degrade network performance, affecting legitimate users' ability to submit transactions.

While this does not cause:
- Consensus safety violations (all validators reach the same state deterministically)
- Loss of funds (execution layer correctly enforces sequence number uniqueness)
- Network partition

It does constitute a "Significant protocol violation" as the consensus layer should prevent obviously invalid transactions from consuming block space.

## Likelihood Explanation
**Likelihood: High**

This vulnerability is highly likely to be exploited because:

1. **No special access required**: Any user can submit transactions to the network
2. **Simple to exploit**: Creating multiple transactions with the same sequence number but different payloads requires only basic transaction construction
3. **Low cost to attacker**: Failed transactions still consume some gas, but the attacker can use low gas prices
4. **High impact per attempt**: Each batch of duplicate transactions multiplies resource consumption across all validators

The attack does not require:
- Validator privileges
- Consensus majority
- Complex timing or race conditions
- Insider knowledge

## Recommendation
The deduper should filter transactions based on `(sender, sequence_number)` pairs **before** checking `(hash, authenticator)` pairs. Only the **first** transaction with a given `(sender, sequence_number)` should be kept.

**Recommended fix** in `consensus/src/txn_hash_and_authenticator_deduper.rs`:

```rust
fn dedup(&self, transactions: Vec<SignedTransaction>) -> Vec<SignedTransaction> {
    let _timer = TXN_DEDUP_SECONDS.start_timer();
    let mut seen_seq = HashMap::new();
    let mut seen_hashes = HashSet::new();
    let mut num_duplicates: usize = 0;
    
    let filtered: Vec<_> = transactions
        .into_iter()
        .filter(|txn| {
            let sender = txn.sender();
            let replay_protector = txn.replay_protector();
            
            // First, check if we've seen this (sender, sequence_number) before
            if let Some(_) = seen_seq.insert((sender, replay_protector), ()) {
                // Already seen this (sender, sequence_number), filter it out
                num_duplicates += 1;
                return false;
            }
            
            // Then check for exact duplicate (hash, authenticator)
            let hash_and_authenticator = (txn.committed_hash(), txn.authenticator());
            if seen_hashes.insert(hash_and_authenticator) {
                true
            } else {
                num_duplicates += 1;
                false
            }
        })
        .collect();
    
    TXN_DEDUP_FILTERED.observe(num_duplicates as f64);
    filtered
}
```

This ensures:
1. Only one transaction per `(sender, sequence_number)` passes through consensus
2. Exact duplicates (same hash + signature) are still filtered
3. Block space is not wasted on transactions that will definitely fail execution
4. Validator resources are protected from DoS via duplicate sequence numbers

## Proof of Concept
The existing test demonstrates the vulnerability: [3](#0-2) 

To create a PoC exploit demonstrating the DoS impact, modify this test to create 1000 variations instead of 2, and measure the processing time increase. The current implementation will process all 1000, while the recommended fix would process only 1.

### Citations

**File:** consensus/src/txn_hash_and_authenticator_deduper.rs (L44-55)
```rust
        for (i, txn) in transactions.iter().enumerate() {
            match seen.get(&(txn.sender(), txn.replay_protector())) {
                None => {
                    seen.insert((txn.sender(), txn.replay_protector()), i);
                },
                Some(first_index) => {
                    is_possible_duplicate = true;
                    possible_duplicates[*first_index] = true;
                    possible_duplicates[i] = true;
                },
            }
        }
```

**File:** consensus/src/txn_hash_and_authenticator_deduper.rs (L74-90)
```rust
        let mut seen_hashes = HashSet::new();
        let mut num_duplicates: usize = 0;
        let filtered: Vec<_> = hash_and_authenticators
            .into_iter()
            .zip(transactions)
            .filter_map(|(maybe_hash, txn)| match maybe_hash {
                None => Some(txn),
                Some(hash_and_authenticator) => {
                    if seen_hashes.insert(hash_and_authenticator) {
                        Some(txn)
                    } else {
                        num_duplicates += 1;
                        None
                    }
                },
            })
            .collect();
```

**File:** consensus/src/txn_hash_and_authenticator_deduper.rs (L236-256)
```rust
    fn test_repeated_sequence_number() {
        let deduper = TxnHashAndAuthenticatorDeduper::new();

        let sender = Account::new();
        let receiver = Account::new();

        let txn_0a = empty_txn(sender.addr, 0, 100)
            .sign(&sender.privkey, sender.pubkey.clone())
            .unwrap()
            .into_inner();
        // Different txn, same sender and sequence number. Should not be filtered.
        let txn_0b = peer_to_peer_txn(sender.addr, receiver.addr, 0, 100)
            .sign(&sender.privkey, sender.pubkey)
            .unwrap()
            .into_inner();
        let txns = block(vec![&txn_0a, &txn_0b, &txn_0a]);
        let expected = block(vec![&txn_0a, &txn_0b]);
        let deduped_txns = deduper.dedup(txns);
        assert_eq!(expected.len(), deduped_txns.len());
        assert_eq!(expected, deduped_txns);
    }
```

**File:** aptos-move/framework/aptos-framework/sources/transaction_validation.move (L238-241)
```text
            assert!(
                txn_sequence_number == account_sequence_number,
                error::invalid_argument(PROLOGUE_ESEQUENCE_NUMBER_TOO_NEW)
            );
```
