# Audit Report

## Title
Incorrect Ordering in EpochEndingBackupMeta Allows Attacker to Force Selection of Malicious Backup During Restore

## Summary
The derived `Ord` implementation on `EpochEndingBackupMeta` sorts backups by ascending `last_version`, causing the system to preferentially select backups with lower version coverage when multiple backups exist for the same epoch range. An attacker with write access to backup storage can exploit this to force selection of a malicious backup over the legitimate one, leading to denial of service or restoration of incorrect chain state.

## Finding Description

The `EpochEndingBackupMeta` struct derives `Ord` and `PartialOrd` traits, which creates a default ordering based on field declaration order: `(first_epoch, last_epoch, first_version, last_version, manifest)`. [1](#0-0) 

When the backup system loads metadata from storage, it collects ALL metadata files without filtering: [2](#0-1) 

The `MetadataView` constructor then sorts these backups using the derived ordering and removes only exact duplicates: [3](#0-2) 

During restore, `select_epoch_ending_backups` iterates through the sorted backups and enforces epoch continuity: [4](#0-3) 

**Attack Scenario:**

1. Legitimate backup exists: `first_epoch=0, last_epoch=100, first_version=0, last_version=10000`
2. Attacker with write access to backup storage creates malicious metadata:
   - `first_epoch=0, last_epoch=100, first_version=0, last_version=5000`
   - Points to a backup from a minority fork or crafted state
3. When comparing these backups:
   - `first_epoch`: 0 == 0 (tie)
   - `last_epoch`: 100 == 100 (tie)
   - `first_version`: 0 == 0 (tie)
   - `last_version`: 5000 < 10000, so **malicious backup sorts FIRST**
4. During restore:
   - Malicious backup is selected first (passes `first_epoch == next_epoch` check)
   - After processing malicious backup, `next_epoch = 101`
   - Legitimate backup is rejected (`first_epoch=0 != next_epoch=101`)

The malicious backup is now exclusively used for epochs 0-100. If trusted waypoints are provided, verification fails (DoS). If no trusted waypoints and the malicious backup has valid signatures (e.g., from a minority fork), incorrect chain state is restored.

## Impact Explanation

This vulnerability meets **Medium severity** criteria under the Aptos bug bounty program:

**With Trusted Waypoints (recommended deployment):**
- Restore operations fail when malicious backup doesn't match waypoints
- Results in denial of service for restore operations
- Requires manual intervention to identify and remove malicious metadata
- Impact: **Medium** (state inconsistencies requiring intervention)

**Without Trusted Waypoints (insecure deployment):**
- Malicious backup with valid signatures may pass verification
- Node restores from minority fork instead of canonical chain
- Results in incorrect blockchain state that diverges from network consensus
- Impact: **Medium-High** (limited funds loss or manipulation, state inconsistencies)

The severity is Medium rather than High because:
1. Requires attacker to compromise backup storage credentials
2. Recommended deployments use trusted waypoints, limiting impact to DoS
3. Does not directly affect running validator nodes or network consensus

## Likelihood Explanation

**Likelihood: Low-Medium**

**Prerequisites for exploitation:**
1. **Compromised backup storage access** - Attacker needs write permissions to cloud storage (S3, GCS) or local filesystem where backups are stored
2. **Credible backup creation** - Attacker must create valid backup metadata with proper manifest structure
3. **Absence of trusted waypoints** (for full exploitation) - Victim must perform restore without specifying trusted waypoints

**Factors increasing likelihood:**
- Backup storage credentials are often shared across teams and may have weak access controls
- Cloud storage misconfigurations are common attack vectors
- Operators may skip trusted waypoint verification for convenience during testing or emergency recovery

**Factors decreasing likelihood:**
- Production deployments should use trusted waypoints (best practice)
- Malicious backup still requires valid epoch ending ledger info signatures to pass verification
- Creating a credible backup from a minority fork requires significant resources

## Recommendation

**Fix the ordering logic to prefer backups with higher version coverage when epoch ranges are identical:**

```rust
// In storage/backup/backup-cli/src/metadata/mod.rs

// Remove the derived Ord and PartialOrd, implement manually:
#[derive(Clone, Debug, Deserialize, Serialize, Eq, PartialEq)]
pub struct EpochEndingBackupMeta {
    pub first_epoch: u64,
    pub last_epoch: u64,
    pub first_version: Version,
    pub last_version: Version,
    pub manifest: FileHandle,
}

impl Ord for EpochEndingBackupMeta {
    fn cmp(&self, other: &Self) -> Ordering {
        // Compare by first_epoch first (ascending - process earlier epochs first)
        match self.first_epoch.cmp(&other.first_epoch) {
            Ordering::Equal => {
                // If first_epoch is equal, prefer longer epoch ranges (descending last_epoch)
                match other.last_epoch.cmp(&self.last_epoch) {
                    Ordering::Equal => {
                        // If epoch ranges are identical, prefer higher last_version (descending)
                        // This ensures more complete backups are selected
                        match other.last_version.cmp(&self.last_version) {
                            Ordering::Equal => {
                                // Compare remaining fields in ascending order
                                self.first_version.cmp(&other.first_version)
                                    .then_with(|| self.manifest.cmp(&other.manifest))
                            }
                            ordering => ordering,
                        }
                    }
                    ordering => ordering,
                }
            }
            ordering => ordering,
        }
    }
}

impl PartialOrd for EpochEndingBackupMeta {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        Some(self.cmp(other))
    }
}
```

**Additional safeguards:**

1. **Detect and reject overlapping backups:**
```rust
// In MetadataView::new(), after sorting:
for window in epoch_ending_backups.windows(2) {
    if window[0].first_epoch == window[1].first_epoch 
        && window[0].last_epoch == window[1].last_epoch {
        warn!(
            "Detected multiple backups for epoch range {}-{}, keeping highest version",
            window[0].first_epoch, window[0].last_epoch
        );
        // The corrected ordering already prefers higher last_version
    }
}
```

2. **Enforce trusted waypoints in production:**
    - Document requirement for trusted waypoints in restore operations
    - Add warning logs when restore proceeds without trusted waypoints

## Proof of Concept

```rust
// storage/backup/backup-cli/src/metadata/mod.rs - Add to test module

#[cfg(test)]
mod tests {
    use super::*;
    use crate::storage::FileHandle;

    #[test]
    fn test_backup_ordering_with_overlapping_ranges() {
        // Create two backups with same epoch range but different version ranges
        let backup_low_version = EpochEndingBackupMeta {
            first_epoch: 0,
            last_epoch: 100,
            first_version: 0,
            last_version: 5000,  // Lower version
            manifest: FileHandle::new("backup_low".to_string()),
        };

        let backup_high_version = EpochEndingBackupMeta {
            first_epoch: 0,
            last_epoch: 100,
            first_version: 0,
            last_version: 10000,  // Higher version (should be preferred)
            manifest: FileHandle::new("backup_high".to_string()),
        };

        // Test current (buggy) ordering - lower version sorts first
        let mut backups = vec![backup_high_version.clone(), backup_low_version.clone()];
        backups.sort();
        
        // BUG: Current implementation selects backup with LOWER version
        assert_eq!(backups[0].last_version, 5000, 
            "VULNERABILITY: Backup with lower version is sorted first!");
        assert_eq!(backups[1].last_version, 10000);

        // After fix: backup with HIGHER version should sort first
        // Uncomment after implementing the fix:
        // assert_eq!(backups[0].last_version, 10000);
        // assert_eq!(backups[1].last_version, 5000);
    }
}
```

Run with: `cargo test -p backup-cli test_backup_ordering_with_overlapping_ranges`

**Notes:**
- This vulnerability requires compromised backup storage access but represents a real security risk
- The counterintuitive ordering prioritizes backups with LOWER version coverage, which is incorrect
- Production deployments should always use trusted waypoints to mitigate this issue
- The fix requires custom `Ord` implementation to prefer higher version coverage for identical epoch ranges

### Citations

**File:** storage/backup/backup-cli/src/metadata/mod.rs (L175-182)
```rust
#[derive(Clone, Debug, Deserialize, Serialize, Eq, PartialEq, Ord, PartialOrd)]
pub struct EpochEndingBackupMeta {
    pub first_epoch: u64,
    pub last_epoch: u64,
    pub first_version: Version,
    pub last_version: Version,
    pub manifest: FileHandle,
}
```

**File:** storage/backup/backup-cli/src/metadata/cache.rs (L193-213)
```rust
    let mut metadata_vec = Vec::new();
    for h in new_remote_hashes.into_iter().chain(up_to_date_local_hashes) {
        let cached_file = cache_dir.join(h);
        metadata_vec.extend(
            OpenOptions::new()
                .read(true)
                .open(&cached_file)
                .await
                .err_notes(&cached_file)?
                .load_metadata_lines()
                .await
                .err_notes(&cached_file)?
                .into_iter(),
        )
    }
    info!(
        total_time = timer.elapsed().as_secs(),
        "Metadata cache loaded.",
    );

    Ok(MetadataView::new(metadata_vec, remote_file_handles))
```

**File:** storage/backup/backup-cli/src/metadata/view.rs (L29-51)
```rust
    pub(crate) fn new(metadata_vec: Vec<Metadata>, file_handles: Vec<FileHandle>) -> Self {
        let mut epoch_ending_backups = Vec::new();
        let mut state_snapshot_backups = Vec::new();
        let mut transaction_backups = Vec::new();
        let mut identity = None;
        let mut compaction_timestamps = Vec::new();

        for meta in metadata_vec {
            match meta {
                Metadata::EpochEndingBackup(e) => epoch_ending_backups.push(e),
                Metadata::StateSnapshotBackup(s) => state_snapshot_backups.push(s),
                Metadata::TransactionBackup(t) => transaction_backups.push(t),
                Metadata::Identity(i) => identity = Some(i),
                Metadata::CompactionTimestamps(t) => compaction_timestamps.push(t),
            }
        }
        epoch_ending_backups.sort_unstable();
        epoch_ending_backups.dedup();
        state_snapshot_backups.sort_unstable();
        state_snapshot_backups.dedup();
        transaction_backups.sort_unstable();
        transaction_backups.dedup();

```

**File:** storage/backup/backup-cli/src/metadata/view.rs (L171-196)
```rust
    pub fn select_epoch_ending_backups(
        &self,
        target_version: Version,
    ) -> Result<Vec<EpochEndingBackupMeta>> {
        // This can be more flexible, but for now we assume and check backups are continuous in
        // range (which is always true when we backup from a single backup coordinator)
        let mut next_epoch = 0;
        let mut res = Vec::new();
        for backup in self.epoch_ending_backups.iter().sorted() {
            if backup.first_version > target_version {
                break;
            }

            ensure!(
                backup.first_epoch == next_epoch,
                "Epoch ending backup ranges not continuous, expecting epoch {}, got {}.",
                next_epoch,
                backup.first_epoch,
            );
            res.push(backup.clone());

            next_epoch = backup.last_epoch + 1;
        }

        Ok(res)
    }
```
