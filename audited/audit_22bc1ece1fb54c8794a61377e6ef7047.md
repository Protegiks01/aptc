# Audit Report

## Title
Non-Atomic Cross-Database Writes in LedgerDb Enable State Inconsistency During Auxiliary Data Commits

## Summary
When storage sharding is enabled (default configuration), the `LedgerDb::write_schemas()` method commits data to 8 separate RocksDB databases sequentially without distributed transaction coordination. If any database write fails after earlier ones succeed, auxiliary data and other ledger components can be partially committed, violating atomic transaction semantics and creating irrecoverable state inconsistencies.

## Finding Description

The vulnerability exists in the sequential write pattern used by `LedgerDb::write_schemas()` when storage sharding is enabled. [1](#0-0) 

When this method is called, it writes to 8 separate RocksDB instances in sequence:
1. write_set_db
2. transaction_info_db  
3. transaction_db
4. persisted_auxiliary_info_db
5. event_db
6. transaction_accumulator_db
7. **transaction_auxiliary_data_db** (TransactionAuxiliaryDataSchema)
8. ledger_metadata_db

Each `write_schemas()` call commits to a separate RocksDB instance using synchronous writes. [2](#0-1) 

Storage sharding is enabled by default. [3](#0-2) 

**Attack Scenario:**

During state synchronization, `finalize_state_snapshot()` is called to commit snapshot data. [4](#0-3) 

The comment claims atomicity, but this is misleading. If the 4th write (persisted_auxiliary_info_db) fails due to disk exhaustion, I/O error, or filesystem corruption:
- Versions will have write_sets, transaction_info, and transactions committed
- Same versions will be **missing** persisted_auxiliary_info, events, transaction_accumulator updates, transaction_auxiliary_data, and metadata

This directly answers the security question: **YES, some versions can have auxiliary data while others don't**, breaking all-or-nothing semantics.

The developers acknowledge this issue exists but haven't fixed it. [5](#0-4) 

**Recovery Amplifies the Problem:**

When nodes restart, `sync_commit_progress()` attempts recovery by truncating inconsistent data. [6](#0-5) 

However, the truncation itself uses the same non-atomic `LedgerDb::write_schemas()` method! [7](#0-6) 

If truncation fails partway through, the inconsistency worsens.

## Impact Explanation

This is **High Severity** under the Aptos bug bounty criteria for the following reasons:

1. **State Inconsistencies Requiring Intervention**: Different nodes experiencing failures at different points will have divergent partial states with no automatic recovery mechanism. Manual intervention would be required to restore consistency.

2. **Potential Consensus Impact**: While not an immediate consensus safety violation, nodes with inconsistent auxiliary data, missing transaction accumulator updates, or incomplete metadata could compute different state roots when processing subsequent transactions, leading to consensus divergence over time.

3. **Significant Protocol Violation**: This breaks the fundamental **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." The Merkle accumulator becomes unverifiable when transaction accumulator updates are missing for some versions.

4. **Active Code Path**: The vulnerable `finalize_state_snapshot()` is actively used during state synchronization. [8](#0-7) 

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can be triggered without attacker involvement through environmental failures:

1. **Disk Space Exhaustion**: Nodes syncing large state snapshots can exhaust disk space during the sequential writes, causing later database writes to fail while earlier ones have committed.

2. **I/O Errors**: Hardware failures, network-attached storage disruptions, or filesystem corruption can cause individual RocksDB writes to fail.

3. **Process Crashes**: If the node process crashes or is killed between database writes (e.g., OOM killer, administrative shutdown), partial state persists.

4. **No Attacker Required**: This is an environmental vulnerability - the bug is inherent in the design and will trigger whenever any database write fails after earlier ones succeed.

The default configuration uses storage sharding, making all production nodes vulnerable.

## Recommendation

Implement one of the following solutions:

**Option 1: Two-Phase Commit Protocol**
Implement a distributed transaction coordinator that:
1. Prepares all batches in memory
2. Calls prepare/lock on all databases
3. Only commits if all prepares succeed
4. Rolls back all databases on any failure

**Option 2: Write-Ahead Log (WAL)**
Create a single WAL that atomically records the intent to commit all batches, then apply them asynchronously. On recovery, replay incomplete WAL entries.

**Option 3: Single Database Mode (Short-term)**
When sharding is enabled, merge all `LedgerDbSchemaBatches` into a single RocksDB batch before committing. This requires all ledger components to share one RocksDB instance, sacrificing sharding performance benefits but ensuring atomicity.

**Immediate Mitigation:**
Add consistency checks during startup that detect partial commits and automatically trigger recovery. Enhance error handling to panic on any database write failure to prevent partial commits from persisting.

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
// File: storage/aptosdb/src/ledger_db/ledger_db_atomicity_test.rs

#[cfg(test)]
mod atomicity_test {
    use super::*;
    use aptos_schemadb::SchemaBatch;
    use aptos_temppath::TempPath;
    use std::sync::Arc;

    #[test]
    fn test_non_atomic_ledger_db_writes() {
        // Setup: Create LedgerDb with sharding enabled
        let tmpdir = TempPath::new();
        let rocksdb_configs = RocksdbConfigs {
            enable_storage_sharding: true,
            ..Default::default()
        };
        
        let ledger_db = Arc::new(LedgerDb::new(
            tmpdir.path(),
            rocksdb_configs,
            None,
            None,
            false,
        ).unwrap());

        // Create batches with data for version 100
        let mut batches = LedgerDbSchemaBatches::new();
        
        // Add data to multiple databases
        batches.transaction_db_batches.put::<TransactionSchema>(
            &100u64,
            &Transaction::dummy()
        ).unwrap();
        
        batches.persisted_auxiliary_info_db_batches.put::<PersistedAuxiliaryInfoSchema>(
            &100u64,
            &PersistedAuxiliaryInfo::V1 { transaction_index: 0 }
        ).unwrap();

        // Simulate failure: Fill disk or corrupt one DB directory
        // This causes the 4th write (persisted_auxiliary_info_db) to fail
        // while earlier writes succeed
        
        let result = ledger_db.write_schemas(batches);
        
        // Expected: Either all data committed or none
        // Actual: Partial commit possible - transaction exists but auxiliary info missing
        
        // Verify inconsistency:
        let has_transaction = ledger_db.transaction_db()
            .get_transaction(100).unwrap().is_some();
        let has_aux_info = ledger_db.persisted_auxiliary_info_db()
            .get_persisted_auxiliary_info(100).unwrap().is_some();
            
        // VULNERABILITY: These should always match, but may not if writes fail
        assert_eq!(has_transaction, has_aux_info, 
            "Atomicity violated: version 100 has transaction={} but auxiliary_info={}",
            has_transaction, has_aux_info);
    }
}
```

## Notes

The vulnerability specifically affects the `TransactionAuxiliaryDataSchema` mentioned in the security question, as it is written at step 7 of 8 in the sequential write process. Any failure of earlier writes means auxiliary data won't be written; any failure at this step leaves earlier data committed without auxiliary data.

The issue is most severe during `finalize_state_snapshot()` operations in state sync, where large batches of data are committed. The recovery mechanism itself is vulnerable to the same atomicity issue, creating a cascading failure scenario.

### Citations

**File:** storage/aptosdb/src/ledger_db/mod.rs (L281-281)
```rust
        // TODO(grao): Handle data inconsistency.
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L531-548)
```rust
    pub fn write_schemas(&self, schemas: LedgerDbSchemaBatches) -> Result<()> {
        self.write_set_db
            .write_schemas(schemas.write_set_db_batches)?;
        self.transaction_info_db
            .write_schemas(schemas.transaction_info_db_batches)?;
        self.transaction_db
            .write_schemas(schemas.transaction_db_batches)?;
        self.persisted_auxiliary_info_db
            .write_schemas(schemas.persisted_auxiliary_info_db_batches)?;
        self.event_db.write_schemas(schemas.event_db_batches)?;
        self.transaction_accumulator_db
            .write_schemas(schemas.transaction_accumulator_db_batches)?;
        self.transaction_auxiliary_data_db
            .write_schemas(schemas.transaction_auxiliary_data_db_batches)?;
        // TODO: remove this after sharding migration
        self.ledger_metadata_db
            .write_schemas(schemas.ledger_metadata_db_batches)
    }
```

**File:** storage/schemadb/src/lib.rs (L289-309)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
    }

    /// Writes a group of records wrapped in a [`SchemaBatch`].
    pub fn write_schemas(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &sync_write_option())
    }
```

**File:** config/src/config/storage_config.rs (L219-238)
```rust
impl Default for RocksdbConfigs {
    fn default() -> Self {
        Self {
            ledger_db_config: RocksdbConfig::default(),
            state_merkle_db_config: RocksdbConfig::default(),
            state_kv_db_config: RocksdbConfig {
                bloom_filter_bits: Some(10.0),
                bloom_before_level: Some(2),
                ..Default::default()
            },
            index_db_config: RocksdbConfig {
                max_open_files: 1000,
                ..Default::default()
            },
            enable_storage_sharding: true,
            high_priority_background_threads: 4,
            low_priority_background_threads: 2,
            shared_block_cache_size: Self::DEFAULT_BLOCK_CACHE_SIZE,
        }
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L220-223)
```rust
            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;
```

**File:** storage/aptosdb/src/state_store/mod.rs (L438-449)
```rust
            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L360-360)
```rust
    ledger_db.write_schemas(batch)
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1129-1136)
```rust
    storage
        .writer
        .finalize_state_snapshot(
            version,
            target_output_with_proof.clone(),
            epoch_change_proofs,
        )
        .map_err(|error| format!("Failed to finalize the state snapshot! Error: {:?}", error))?;
```
