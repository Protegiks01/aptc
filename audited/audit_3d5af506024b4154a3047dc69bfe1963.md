# Audit Report

## Title
Lack of Runtime Validation for Multiple Master Nodes Enables File Store Corruption Through Configuration Errors

## Summary
The `is_master` check at line 112 in `grpc_manager.rs` correctly prevents replica nodes (configured with `is_master: false`) from uploading to the file store under normal operation. However, the system lacks runtime validation to detect and prevent configuration errors where multiple nodes are mistakenly configured with `is_master: true`, which can lead to concurrent uploads and file store corruption.

## Finding Description
The `GrpcManager::start()` function uses a simple boolean check to determine whether to spawn the `FileStoreUploader` task: [1](#0-0) 

The `is_master` field is set once during initialization from the configuration and never changes: [2](#0-1) 

While this check has no race conditions (the boolean field is immutable), the system makes an explicit assumption documented in the code: [3](#0-2) 

**The vulnerability occurs when this assumption is violated through configuration errors:**

1. Every `GrpcManager` instance creates a `FileStoreUploader`, regardless of master/replica status: [4](#0-3) 

2. If multiple nodes are configured with `is_master: true` (configuration error during deployment), all instances will start uploading concurrently to the same file store

3. The file store implementations have no distributed locking: [5](#0-4) 

4. Multiple masters will race to update critical metadata files: [6](#0-5) 

5. The `MetadataManager` tracks `master_address` via heartbeats but does not enforce single-master semantics: [7](#0-6) 

## Impact Explanation
**Severity: Medium to High (operational impact, not exploitable by unprivileged attacker)**

If multiple nodes are configured as masters:
- Concurrent writes to `METADATA_FILE_NAME` cause non-deterministic final state
- Batch metadata files can become inconsistent with actual transaction files
- File store version tracking may become corrupted, triggering panics in replica nodes
- Recovery requires manual intervention to identify and fix corrupted metadata
- Indexer service disruption affects downstream applications relying on transaction data

The impact on replica nodes reading corrupted data can trigger this panic condition: [8](#0-7) 

## Likelihood Explanation
**Likelihood: Low to Medium (requires configuration error, not an exploit)**

This issue can occur through:
- Human error during manual deployment (copy-paste configuration mistakes)
- Infrastructure-as-code bugs affecting multiple nodes
- Failover scenarios where old master not properly decommissioned before new master starts
- Automated deployment systems with race conditions during rollout

**However, this does NOT meet bug bounty criteria** because:
- Requires operator-level access to configuration (trusted role)
- Not exploitable by unprivileged attacker
- The code works correctly as designedâ€”the issue is lack of defensive validation
- This is an operational reliability concern, not a security vulnerability

## Recommendation
Add runtime validation to detect and prevent multiple masters:

1. **Implement distributed leader election or locking:**
   - Use file store atomic operations to claim master role
   - Create a lock file in the file store that only one node can hold
   - Periodically refresh the lock with heartbeat

2. **Add startup validation:**
   - On startup, check if another master exists via heartbeat responses
   - Abort if `master_address` from other nodes doesn't match `self_advertised_address`

3. **Add runtime monitoring:**
   - Compare `is_master` flag with cluster consensus view of master
   - Alert/panic if mismatch detected

4. **Metadata integrity checks:**
   - Add version vectors or checksums to detect concurrent modifications
   - Implement optimistic locking for metadata updates

## Proof of Concept
This issue cannot be demonstrated as a security exploit because it requires operator configuration access. However, it can be reproduced operationally:

**Steps to Reproduce (Operational Test):**
1. Deploy two `indexer-grpc-manager` instances
2. Configure both with `is_master: true` in their config files
3. Point both to same GCS bucket or local file store
4. Start both instances simultaneously
5. Observe concurrent uploads and metadata conflicts in logs
6. Verify file store metadata inconsistencies using file store checker tools

**Expected Result:** File store metadata becomes corrupted, requiring manual recovery.

---

**Note:** While this represents a real operational risk that should be addressed through better defensive programming, it does not qualify as a bug bounty vulnerability per the stated criteria because it is not exploitable by an unprivileged attacker and requires trusted operator access to configuration systems.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L33-42)
```rust
        let file_store_uploader = Mutex::new(
            FileStoreUploader::new(chain_id, config.file_store_config.clone())
                .await
                .unwrap_or_else(|e| {
                    panic!(
                        "Failed to create filestore uploader, config: {:?}, error: {e:?}",
                        config.file_store_config
                    )
                }),
        );
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L82-88)
```rust
        Self {
            chain_id,
            file_store_uploader,
            metadata_manager,
            data_manager,
            is_master: config.is_master,
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L112-121)
```rust
            if self.is_master {
                s.spawn(async move {
                    self.file_store_uploader
                        .lock()
                        .await
                        .start(self.data_manager.clone(), tx)
                        .await
                        .unwrap();
                });
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L135-136)
```rust
    // NOTE: We assume the master is statically configured for now.
    master_address: Mutex<Option<GrpcAddress>>,
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L552-556)
```rust
    fn handle_grpc_manager_info(&self, address: GrpcAddress, info: GrpcManagerInfo) -> Result<()> {
        self.master_address
            .lock()
            .unwrap()
            .clone_from(&info.master_address);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/gcs.rs (L120-137)
```rust
    async fn save_raw_file(&self, file_path: PathBuf, data: Vec<u8>) -> Result<()> {
        let path = self.get_path(file_path);
        trace!(
            "Uploading object to {}/{}.",
            self.bucket_name,
            path.as_str()
        );
        Object::create(
            self.bucket_name.as_str(),
            data,
            path.as_str(),
            JSON_FILE_TYPE,
        )
        .await
        .map_err(anyhow::Error::msg)?;

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L261-273)
```rust
    /// Updates the file store metadata.
    async fn update_file_store_metadata(&self, version: u64) -> Result<()> {
        FILE_STORE_VERSION.set(version as i64);
        let metadata = FileStoreMetadata {
            chain_id: self.chain_id,
            num_transactions_per_folder: NUM_TXNS_PER_FOLDER,
            version,
        };

        let raw_data = serde_json::to_vec(&metadata).map_err(anyhow::Error::msg)?;
        self.writer
            .save_raw_file(PathBuf::from(METADATA_FILE_NAME), raw_data)
            .await
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L415-417)
```rust
            if !version_can_go_backward && file_store_version_before_update > file_store_version {
                panic!("File store version is going backward, data might be corrupted. {file_store_version_before_update} v.s. {file_store_version}");
            };
```
