# Audit Report

## Title
Shutdown Deadlock in QuorumStore Coordinator Due to Circular Channel Dependency

## Summary
A critical deadlock vulnerability exists in the QuorumStore shutdown sequence where NetworkListener can become blocked on sending to ProofManager's bounded channel, while QuorumStoreCoordinator waits for NetworkListener acknowledgment before shutting down ProofManager. This circular dependency causes indefinite hang during shutdown, requiring forceful node termination.

## Finding Description
The vulnerability stems from a circular dependency in the shutdown ordering of QuorumStore components:

**Component Architecture:**
- NetworkListener receives network messages and forwards them to ProofManager via a bounded `tokio::sync::mpsc::channel` with capacity 1000 [1](#0-0) 
- All components (NetworkListener, BatchCoordinator, QuorumStoreCoordinator) share the same bounded channel to send commands to ProofManager [2](#0-1) 

**Shutdown Sequence:**
The shutdown process is designed to shut down NetworkListener first, then ProofManager last [3](#0-2) , with the coordinator waiting for NetworkListener acknowledgment before proceeding [4](#0-3) 

**Deadlock Scenario:**
1. During normal operation, NetworkListener processes `VerifiedEvent::ProofOfStoreMsg` messages and sends them to ProofManager using blocking `.send().await` [5](#0-4) 

2. If ProofManager's channel becomes full (1000 messages queued), NetworkListener's send operation blocks indefinitely waiting for channel capacity

3. When a shutdown is initiated, QuorumStoreCoordinator sends `VerifiedEvent::Shutdown` to NetworkListener via non-blocking `aptos_channel` [6](#0-5) 

4. **Critical Issue:** NetworkListener cannot process the shutdown message because its async task is blocked on the previous `send().await` operation at the ProofManager channel

5. QuorumStoreCoordinator waits indefinitely for NetworkListener to acknowledge shutdown [4](#0-3) 

6. ProofManager never receives shutdown command because coordinator is blocked waiting for NetworkListener

7. **Deadlock:** NetworkListener waits for ProofManager to drain channel → Coordinator waits for NetworkListener ack → ProofManager never gets shutdown signal to drain channel

**Root Cause:**
The use of bounded channels with blocking sends creates a dependency from NetworkListener to ProofManager, but the shutdown ordering assumes NetworkListener can immediately respond to shutdown signals without any blocking dependencies.

## Impact Explanation
**Severity: CRITICAL** - Total loss of liveness/network availability

This vulnerability results in:
- **Node Hang During Shutdown:** Affected validator nodes cannot gracefully shut down, hanging indefinitely
- **Requires Forceful Termination:** Operators must use SIGKILL or equivalent to terminate hung processes
- **Epoch Transition Failures:** If shutdown occurs during epoch transitions, the node cannot rejoin the network cleanly
- **Cascading Failures:** Multiple validators experiencing high network load during coordinated shutdown (e.g., network-wide upgrade) could trigger widespread deadlocks
- **Loss of Network Availability:** If enough validators hang during shutdown, the network loses liveness until manual intervention

According to Aptos bug bounty criteria, "Total loss of liveness/network availability" qualifies as **Critical Severity** (up to $1,000,000).

## Likelihood Explanation
**Likelihood: MEDIUM-HIGH**

The vulnerability is triggered when:
1. ProofManager's bounded channel (capacity 1000) becomes full
2. NetworkListener attempts to send another message
3. A shutdown is initiated while NetworkListener is blocked

**Triggering Conditions:**
- **High Network Traffic:** During periods of high transaction throughput or network congestion, the ProofManager channel can accumulate messages faster than they're processed
- **Burst Traffic Patterns:** Sudden spikes in ProofOfStore messages from peers (e.g., after network partition recovery) can rapidly fill the channel
- **Graceful Shutdown During Load:** Operators initiating shutdown during peak traffic
- **Coordinated Validator Restarts:** Network-wide upgrades where multiple validators restart simultaneously under load

**Attack Vector:**
A malicious peer could deliberately flood the network with valid `ProofOfStoreMsg` messages to fill ProofManager's channel, then trigger coordinated shutdown attempts, causing widespread validator hangs.

## Recommendation
Implement one of the following fixes:

**Option 1: Use Non-Blocking Sends with Error Handling (Recommended)**
Replace blocking `send().await` with `try_send()` in NetworkListener to avoid blocking on full channels:

```rust
// In network_listener.rs, line 99-103
match self.proof_manager_tx.try_send(cmd) {
    Ok(()) => {},
    Err(tokio::sync::mpsc::error::TrySendError::Full(_)) => {
        warn!("ProofManager channel full, dropping ProofOfStoreMsg");
        counters::DROPPED_PROOF_OF_STORE_MSGS.inc();
    },
    Err(tokio::sync::mpsc::error::TrySendError::Closed(_)) => {
        // Channel closed during shutdown, this is expected
        break;
    }
}
```

**Option 2: Use Send with Timeout**
Add timeout to all blocking sends to NetworkListener:

```rust
match tokio::time::timeout(
    Duration::from_secs(5),
    self.proof_manager_tx.send(cmd)
).await {
    Ok(Ok(())) => {},
    Ok(Err(_)) => break, // Channel closed
    Err(_) => {
        warn!("ProofManager send timeout, dropping message");
        counters::PROOF_MANAGER_SEND_TIMEOUT.inc();
    }
}
```

**Option 3: Reverse Shutdown Order**
Shut down ProofManager before NetworkListener to ensure NetworkListener never blocks on a closed channel. However, this violates the "senders before receivers" principle stated in the comment [3](#0-2) 

**Recommended Fix:** Option 1 provides the best balance of safety and simplicity, preventing deadlock while maintaining message delivery under normal conditions.

## Proof of Concept

```rust
// Add to consensus/src/quorum_store/tests/quorum_store_test.rs

#[tokio::test(flavor = "multi_thread")]
async fn test_shutdown_deadlock_scenario() {
    use tokio::sync::mpsc;
    use std::time::Duration;
    
    // Create bounded channel with small capacity to simulate full condition
    let (proof_manager_tx, mut proof_manager_rx) = mpsc::channel(2);
    let (shutdown_tx, shutdown_rx) = futures_channel::oneshot::channel();
    
    // Simulate ProofManager not processing messages (blocked/slow)
    // Do NOT spawn a task to drain proof_manager_rx
    
    // Simulate NetworkListener trying to send messages
    let network_listener = tokio::spawn(async move {
        // Send messages until channel is full
        for i in 0..5 {
            println!("NetworkListener: Attempting send {}", i);
            match proof_manager_tx.send(format!("Message {}", i)).await {
                Ok(()) => println!("NetworkListener: Sent message {}", i),
                Err(e) => {
                    println!("NetworkListener: Send failed: {:?}", e);
                    break;
                }
            }
        }
        
        // At this point, channel should be full and next send will block
        println!("NetworkListener: Channel full, waiting for shutdown signal");
        
        // Simulate receiving shutdown signal (but we're blocked above)
        // In real code, this would be via network_msg_rx.next().await
        // but we're blocked on the send above, so we never get here
        shutdown_tx.send(()).ok();
    });
    
    // Simulate coordinator waiting for NetworkListener shutdown acknowledgment
    println!("Coordinator: Waiting for NetworkListener shutdown ack...");
    let timeout_result = tokio::time::timeout(
        Duration::from_secs(2),
        shutdown_rx
    ).await;
    
    // This should timeout, demonstrating the deadlock
    assert!(timeout_result.is_err(), "Should timeout waiting for NetworkListener ack");
    println!("Coordinator: DEADLOCK - NetworkListener never acknowledged shutdown!");
    
    // Clean up
    network_listener.abort();
    
    // Verify channel still has messages (ProofManager never drained)
    let remaining = proof_manager_rx.try_recv();
    assert!(remaining.is_ok(), "Channel should still have messages");
}
```

**Expected Output:**
```
NetworkListener: Attempting send 0
NetworkListener: Sent message 0
NetworkListener: Attempting send 1
NetworkListener: Sent message 1
NetworkListener: Attempting send 2
NetworkListener: Channel full, waiting for shutdown signal
Coordinator: Waiting for NetworkListener shutdown ack...
Coordinator: DEADLOCK - NetworkListener never acknowledged shutdown!
```

This demonstrates that NetworkListener cannot respond to shutdown when blocked on a full channel, causing coordinator to hang indefinitely.

**Notes:**
- The vulnerability requires ProofManager's channel to be full (1000 messages by default), which can occur during high network load or malicious flooding
- The deadlock is deterministic once the channel fills and shutdown is initiated while NetworkListener is blocked
- Current implementation has no timeout or fallback mechanism to detect or recover from this condition
- The issue affects all validator nodes during graceful shutdown under load conditions

### Citations

**File:** config/src/config/quorum_store_config.rs (L108-108)
```rust
            channel_size: 1000,
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L183-184)
```rust
        let (proof_manager_cmd_tx, proof_manager_cmd_rx) =
            tokio::sync::mpsc::channel(config.channel_size);
```

**File:** consensus/src/quorum_store/quorum_store_coordinator.rs (L86-91)
```rust
                        // Note: Shutdown is done from the back of the quorum store pipeline to the
                        // front, so senders are always shutdown before receivers. This avoids sending
                        // messages through closed channels during shutdown.
                        // Oneshots that send data in the reverse order of the pipeline must assume that
                        // the receiver could be unavailable during shutdown, and resolve this without
                        // panicking.
```

**File:** consensus/src/quorum_store/quorum_store_coordinator.rs (L95-104)
```rust
                        match self.quorum_store_msg_tx.push(
                            self.my_peer_id,
                            (
                                self.my_peer_id,
                                VerifiedEvent::Shutdown(network_listener_shutdown_tx),
                            ),
                        ) {
                            Ok(()) => info!("QS: shutdown network listener sent"),
                            Err(err) => panic!("Failed to send to NetworkListener, Err {:?}", err),
                        };
```

**File:** consensus/src/quorum_store/quorum_store_coordinator.rs (L105-107)
```rust
                        network_listener_shutdown_rx
                            .await
                            .expect("Failed to stop NetworkListener");
```

**File:** consensus/src/quorum_store/network_listener.rs (L99-103)
```rust
                        let cmd = ProofManagerCommand::ReceiveProofs(*proofs);
                        self.proof_manager_tx
                            .send(cmd)
                            .await
                            .expect("could not push Proof proof_of_store");
```
