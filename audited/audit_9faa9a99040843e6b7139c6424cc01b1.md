# Audit Report

## Title
Peer Scoring Cascade Failure: Network-Wide State Sync Liveness Failure via Systematic Bad Response Notifications

## Summary
A bug in the data client's response validation logic can trigger a cascading failure where all peers are systematically penalized and eventually ignored, causing the global data summary to become empty and halting state synchronization network-wide. This results in a non-recoverable network partition requiring coordinated manual intervention or a hardfork to restore.

## Finding Description

The vulnerability exists in the state sync data streaming service's bad response notification mechanism. When `handle_sanity_check_failure()` detects a response type mismatch, it calls `notify_bad_response()` with `ResponseError::InvalidPayloadDataType`: [1](#0-0) 

This notification triggers a peer scoring update through the response callback: [2](#0-1) 

The callback implementation in the data client invokes `update_score_error()`: [3](#0-2) [4](#0-3) 

For `InvalidPayloadDataType` errors, the peer's score is multiplied by 0.95 (NOT_USEFUL_MULTIPLIER): [5](#0-4) [6](#0-5) 

**Attack Scenario:**

If a bug exists in the sanity check function that causes it to consistently return false for all peers: [7](#0-6) 

Then every response from every peer will fail validation, triggering bad response notifications for all peers. After approximately 30-40 failed responses, each peer's score will drop below the IGNORE_PEER_THRESHOLD of 25.0: [8](#0-7) 

Peers below this threshold are excluded from the global data summary calculation: [9](#0-8) [10](#0-9) 

When all peers are ignored, the global data summary becomes empty, and state sync halts: [11](#0-10) 

**Network-Wide Cascade:**

If this validation bug is deployed across all nodes in the network (e.g., through a software update), every node will independently:
1. Fail sanity checks for responses from all peers
2. Systematically degrade all peer scores
3. Eventually ignore all peers
4. Reach an empty global data summary
5. Halt state synchronization

This results in a complete network partition where no node can sync with any other node, causing total liveness failure.

## Impact Explanation

This vulnerability meets **Critical Severity** criteria per the Aptos bug bounty program:

1. **Non-recoverable network partition (requires hardfork)**: Once all nodes have ignored all their peers, there is no automatic recovery mechanism. The peer scoring system has no circuit breaker to detect this condition and reset scores. Recovery requires either:
   - Manual disabling of `ignore_low_score_peers` configuration across all nodes (coordinated intervention)
   - A hardfork with the bug fixed [12](#0-11) [13](#0-12) 

2. **Total loss of liveness/network availability**: State sync is critical for blockchain operation. Without it, nodes cannot:
   - Bootstrap new validators
   - Catch up after downtime
   - Synchronize with the network
   - Maintain consensus on the latest state

The network effectively becomes non-functional for any node that falls behind or attempts to join.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability is triggered by software bugs, not malicious actors. The likelihood assessment factors include:

1. **Common Bug Pattern**: Response validation logic is complex and bugs in type checking or payload matching are not uncommon in large codebases.

2. **Single Point of Failure**: A bug in either:
   - The sanity check validation logic (lines 1292-1379 in data_stream.rs)
   - The response payload generation logic in peers
   
   Can trigger this cascade.

3. **No Defensive Programming**: The system lacks:
   - Circuit breakers to detect mass peer ignoring
   - Automatic score reset on systematic failures
   - Differentiation between "local bug" vs "bad peer" scenarios
   - Upper bounds on ignored peer percentages

4. **Network-Wide Deployment**: Software updates are deployed uniformly across the network, meaning a bug affecting validation logic would impact all nodes simultaneously.

5. **Historical Precedent**: Similar peer scoring and reputation system bugs have caused network issues in other blockchain systems.

## Recommendation

Implement a **circuit breaker pattern** with the following safeguards:

1. **Add a maximum ignored peers threshold:**
```rust
// In peer_states.rs
const MAX_IGNORED_PEERS_PERCENTAGE: f64 = 0.75; // Never ignore more than 75% of peers

pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
    // Calculate total and ignored peer counts
    let total_peers = self.peer_to_state.len();
    let ignored_peers = self.peer_to_state.iter()
        .filter(|entry| entry.value().is_ignored())
        .count();
    
    // If too many peers are ignored, log a critical warning and temporarily
    // include some ignored peers (with lowest version lag) to prevent partition
    if total_peers > 0 && (ignored_peers as f64 / total_peers as f64) > MAX_IGNORED_PEERS_PERCENTAGE {
        error!(LogSchema::new(LogEntry::PeerStates)
            .event(LogEvent::TooManyIgnoredPeers)
            .message(&format!(
                "CRITICAL: {}% of peers are ignored ({}/{}). Possible systematic validation bug!",
                (ignored_peers as f64 / total_peers as f64) * 100.0,
                ignored_peers,
                total_peers
            )));
        
        // Reset scores of the least-bad ignored peers to prevent total partition
        self.reset_best_ignored_peers(MAX_IGNORED_PEERS_PERCENTAGE);
    }
    
    // Continue with normal summary calculation
    // ...
}
```

2. **Add systematic failure detection:**
```rust
// Track consecutive failures across all peers
pub struct PeerStates {
    // ... existing fields ...
    systematic_failure_count: Arc<AtomicU64>,
    last_systematic_failure_check: Arc<Mutex<Instant>>,
}

pub fn update_score_error(&self, peer: PeerNetworkId, error: ErrorType) {
    // ... existing score update ...
    
    // Check for systematic failures across all peers
    self.check_systematic_failure();
}

fn check_systematic_failure(&self) {
    let failure_count = self.systematic_failure_count.fetch_add(1, Ordering::SeqCst);
    
    // If too many peers are failing too quickly, trigger an alert
    if failure_count > (self.peer_to_state.len() * 10) as u64 {
        let mut last_check = self.last_systematic_failure_check.lock();
        let elapsed = last_check.elapsed();
        
        if elapsed < Duration::from_secs(60) {
            error!(LogSchema::new(LogEntry::PeerStates)
                .event(LogEvent::SystematicFailure)
                .message(&format!(
                    "CRITICAL: Systematic validation failure detected! {} failures across {} peers in {:?}",
                    failure_count, self.peer_to_state.len(), elapsed
                )));
            
            // Reset counters and potentially disable ignore_low_score_peers temporarily
        }
        
        *last_check = Instant::now();
        self.systematic_failure_count.store(0, Ordering::SeqCst);
    }
}
```

3. **Add configuration for emergency score reset:**
```rust
// In AptosDataClientConfig
pub enable_automatic_score_reset: bool, // Default: true
pub score_reset_threshold: f64, // Percentage of ignored peers before reset (default: 0.75)
```

## Proof of Concept

To demonstrate this vulnerability, one can introduce a bug in the sanity check function:

```rust
// Modified sanity_check_client_response_type in data_stream.rs
fn sanity_check_client_response_type(
    data_client_request: &DataClientRequest,
    data_client_response: &Response<ResponsePayload>,
) -> bool {
    // BUG: Always return false to simulate a validation bug
    false
    
    // Original code would be here...
}
```

**Expected behavior with this bug:**
1. All data client responses fail sanity checks
2. All peers receive `notify_bad_response()` calls
3. After ~40 responses per peer, all peer scores drop below 25.0
4. All peers are ignored (`is_ignored()` returns true)
5. `calculate_global_data_summary()` returns empty summary
6. State sync driver detects empty summary and stops syncing
7. Network-wide state sync halt

**Test steps:**
1. Deploy modified code with the bug to a test network
2. Observe peer scores degrading: `metrics::IGNORED_PEERS` increasing
3. Monitor logs for "The global data summary is empty! It's likely that we have no active peers."
4. Verify state sync halts across all nodes
5. Attempt manual recovery by disabling `ignore_low_score_peers` config
6. Observe that this requires coordinated configuration change across all nodes

## Notes

This vulnerability is particularly insidious because:

1. **Silent Degradation**: The system doesn't immediately fail - it gradually ignores more peers until complete failure occurs.

2. **Symmetric Failure**: Since all nodes run the same buggy code, they all experience the same issue simultaneously, preventing any node from serving as a recovery point.

3. **Configuration-Based Mitigation**: The only built-in mitigation (`ignore_low_score_peers = false`) must be manually configured, requiring human intervention during an emergency.

4. **No Automatic Recovery**: Unlike transient network issues, this failure persists until manual intervention occurs.

The core issue is the lack of defensive programming patterns (circuit breakers, systematic failure detection, automatic score resets) in a critical network synchronization component. The peer scoring system was designed to handle individual bad actors, not systematic validation failures affecting all peers simultaneously.

### Citations

**File:** state-sync/data-streaming-service/src/data_stream.rs (L695-708)
```rust
    /// Handles a client response that failed sanity checks
    fn handle_sanity_check_failure(
        &mut self,
        data_client_request: &DataClientRequest,
        response_context: &ResponseContext,
    ) -> Result<(), Error> {
        error!(LogSchema::new(LogEntry::ReceivedDataResponse)
            .stream_id(self.data_stream_id)
            .event(LogEvent::Error)
            .message("Encountered a client response that failed the sanity checks!"));

        self.notify_bad_response(response_context, ResponseError::InvalidPayloadDataType);
        self.resend_data_client_request(data_client_request)
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L746-764)
```rust
    /// Notifies the Aptos data client of a bad client response
    fn notify_bad_response(
        &self,
        response_context: &ResponseContext,
        response_error: ResponseError,
    ) {
        let response_id = response_context.id;
        info!(LogSchema::new(LogEntry::ReceivedDataResponse)
            .stream_id(self.data_stream_id)
            .event(LogEvent::Error)
            .message(&format!(
                "Notifying the data client of a bad response. Response id: {:?}, error: {:?}",
                response_id, response_error
            )));

        response_context
            .response_callback
            .notify_bad_response(response_error);
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L1292-1379)
```rust
fn sanity_check_client_response_type(
    data_client_request: &DataClientRequest,
    data_client_response: &Response<ResponsePayload>,
) -> bool {
    match data_client_request {
        DataClientRequest::EpochEndingLedgerInfos(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::EpochEndingLedgerInfos(_)
            )
        },
        DataClientRequest::NewTransactionOutputsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionOutputsWithProof(_)
            )
        },
        DataClientRequest::NewTransactionsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionsWithProof(_)
            )
        },
        DataClientRequest::NewTransactionsOrOutputsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionsWithProof(_)
            ) || matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionOutputsWithProof(_)
            )
        },
        DataClientRequest::NumberOfStates(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NumberOfStates(_)
            )
        },
        DataClientRequest::StateValuesWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::StateValuesWithProof(_)
            )
        },
        DataClientRequest::SubscribeTransactionsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionsWithProof(_)
            )
        },
        DataClientRequest::SubscribeTransactionOutputsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionOutputsWithProof(_)
            )
        },
        DataClientRequest::SubscribeTransactionsOrOutputsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionsWithProof(_)
            ) || matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionOutputsWithProof(_)
            )
        },
        DataClientRequest::TransactionsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::TransactionsWithProof(_)
            )
        },
        DataClientRequest::TransactionOutputsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::TransactionOutputsWithProof(_)
            )
        },
        DataClientRequest::TransactionsOrOutputsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::TransactionsWithProof(_)
            ) || matches!(
                data_client_response.payload,
                ResponsePayload::TransactionOutputsWithProof(_)
            )
        },
    }
}
```

**File:** state-sync/aptos-data-client/src/client.rs (L872-880)
```rust
    fn notify_bad_response(
        &self,
        _id: ResponseId,
        peer: PeerNetworkId,
        _request: &StorageServiceRequest,
        error_type: ErrorType,
    ) {
        self.peer_states.update_score_error(peer, error_type);
    }
```

**File:** state-sync/aptos-data-client/src/client.rs (L1240-1245)
```rust
impl ResponseCallback for AptosNetResponseCallback {
    fn notify_bad_response(&self, error: ResponseError) {
        let error_type = ErrorType::from(error);
        self.data_client
            .notify_bad_response(self.id, self.peer, &self.request, error_type);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L32-43)
```rust
/// Scores for peer rankings based on preferences and behavior.
const MAX_SCORE: f64 = 100.0;
const MIN_SCORE: f64 = 0.0;
const STARTING_SCORE: f64 = 50.0;
/// Add this score on a successful response.
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
/// Not necessarily a malicious response, but not super useful.
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
/// Likely to be a malicious response.
const MALICIOUS_MULTIPLIER: f64 = 0.8;
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L54-62)
```rust
impl From<ResponseError> for ErrorType {
    fn from(error: ResponseError) -> Self {
        match error {
            ResponseError::InvalidData | ResponseError::InvalidPayloadDataType => {
                ErrorType::NotUseful
            },
            ResponseError::ProofVerificationError => ErrorType::Malicious,
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L142-160)
```rust
    /// Returns the storage summary iff the peer is not below the ignore threshold
    pub fn get_storage_summary_if_not_ignored(&self) -> Option<&StorageServerSummary> {
        if self.is_ignored() {
            None
        } else {
            self.storage_summary.as_ref()
        }
    }

    /// Returns true iff the peer is currently ignored
    fn is_ignored(&self) -> bool {
        // Only ignore peers if the config allows it
        if !self.data_client_config.ignore_low_score_peers {
            return false;
        }

        // Otherwise, ignore peers with a low score
        self.score <= IGNORE_PEER_THRESHOLD
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L168-174)
```rust
    fn update_score_error(&mut self, error: ErrorType) {
        let multiplier = match error {
            ErrorType::NotUseful => NOT_USEFUL_MULTIPLIER,
            ErrorType::Malicious => MALICIOUS_MULTIPLIER,
        };
        self.score = f64::max(self.score * multiplier, MIN_SCORE);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L339-355)
```rust
    pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
        // Gather all storage summaries, but exclude peers that are ignored
        let storage_summaries: Vec<StorageServerSummary> = self
            .peer_to_state
            .iter()
            .filter_map(|peer_state| {
                peer_state
                    .value()
                    .get_storage_summary_if_not_ignored()
                    .cloned()
            })
            .collect();

        // If we have no peers, return an empty global summary
        if storage_summaries.is_empty() {
            return GlobalDataSummary::empty();
        }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L671-678)
```rust
        // Fetch the global data summary and verify we have active peers
        let global_data_summary = self.aptos_data_client.get_global_data_summary();
        if global_data_summary.is_empty() {
            trace!(LogSchema::new(LogEntry::Driver).message(
                "The global data summary is empty! It's likely that we have no active peers."
            ));
            return self.check_auto_bootstrapping().await;
        }
```

**File:** config/src/config/state_sync_config.rs (L420-421)
```rust
    /// Whether or not to ignore peers with low peer scores
    pub ignore_low_score_peers: bool,
```

**File:** config/src/config/state_sync_config.rs (L466-466)
```rust
            ignore_low_score_peers: true,
```
