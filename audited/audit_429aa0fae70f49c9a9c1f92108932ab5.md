# Audit Report

## Title
Duplicate Transactions After Shuffle Can Reach Executor Without Detection

## Summary
If the transaction shuffler contains a buggy implementation that duplicates transactions, these duplicates are NOT caught before reaching the executor. The deduplication logic executes BEFORE the shuffler, not after, creating a gap in defensive validation that could cause consensus breaks during rolling upgrades when validators run different shuffler versions.

## Finding Description

In `prepare_block()`, the transaction processing pipeline follows this order: [1](#0-0) 

The critical issue is that **deduplication happens at line 99, BEFORE shuffle at line 103**. If the shuffler implementation has a bug that creates duplicate transactions, there is no subsequent deduplication to catch them.

The deduper uses `(transaction_hash, authenticator)` pairs to identify duplicates: [2](#0-1) 

Once duplicates pass the shuffle stage, they flow directly to the executor with no additional validation. The executor does not validate transaction uniqueness within a block, and neither does the VM layer.

When duplicate transactions reach execution, the VM's sequence number validation will cause the second duplicate to fail: [3](#0-2) 

The first duplicate transaction executes successfully and increments the account's sequence number. The second duplicate then fails with `PROLOGUE_ESEQUENCE_NUMBER_TOO_OLD`.

**Critical Consensus Break Scenario**: During rolling upgrades, if some validators run a buggy shuffler version while others run a fixed version:
- Validators with the bug will have duplicate transactions in their execution input
- Validators without the bug will have unique transactions  
- They will execute DIFFERENT transaction sets
- This produces DIFFERENT state roots
- **Consensus safety is violated** - validators cannot agree on block execution results

## Impact Explanation

**High Severity** - This breaks the **Deterministic Execution** invariant (Invariant #1):

1. **Consensus Safety Violation**: During version mismatches, different validators produce different state roots for the same block, violating the fundamental BFT consensus safety guarantee. This can cause network-wide disagreement requiring emergency intervention or hardfork.

2. **Resource Exhaustion**: Even when all validators have the same bug, duplicate transactions waste computational resources - gas is computed for transactions that ultimately fail, slowing block processing.

3. **Block Integrity Violation**: Blocks contain duplicate transactions, violating the implicit invariant that block transactions should be unique.

This meets the **High Severity** criteria per Aptos bug bounty: "Significant protocol violations" and potential for "Validator node slowdowns." In worst-case scenarios with validator divergence, it approaches **Critical Severity**: "Consensus/Safety violations."

## Likelihood Explanation

**Medium Likelihood**:

- The shuffler is deterministic and configured via on-chain parameters: [4](#0-3) 

- The shuffler implementation is complex with priority queues and delayed transaction logic: [5](#0-4) 

- Risk increases during shuffler implementation changes or upgrades
- Rolling upgrades create a window where validators temporarily run different versions
- The lack of post-shuffle validation means bugs go undetected until execution

## Recommendation

**Add deduplication AFTER shuffle** to provide defensive validation:

```rust
pub async fn prepare_block(
    &self,
    block: &Block,
    txns: Vec<SignedTransaction>,
    max_txns_from_block_to_execute: Option<u64>,
    block_gas_limit: Option<u64>,
) -> (Vec<SignedTransaction>, Option<u64>) {
    let start_time = Instant::now();
    
    let txn_filter_config = self.txn_filter_config.clone();
    let txn_deduper = self.txn_deduper.clone();
    let txn_shuffler = self.txn_shuffler.clone();
    
    // ... existing code ...
    
    let result = tokio::task::spawn_blocking(move || {
        let filtered_txns = filter_block_transactions(/* ... */);
        let deduped_txns = txn_deduper.dedup(filtered_txns);
        let mut shuffled_txns = {
            let _timer = TXN_SHUFFLE_SECONDS.start_timer();
            txn_shuffler.shuffle(deduped_txns)
        };
        
        // ADD: Second deduplication pass after shuffle for safety
        let final_txns = txn_deduper.dedup(shuffled_txns);
        
        if let Some(max_txns_from_block_to_execute) = max_txns_from_block_to_execute {
            final_txns.truncate(max_txns_from_block_to_execute as usize);
        }
        final_txns
    })
    .await
    .expect("Failed to spawn blocking task");
    
    (result, block_gas_limit)
}
```

Alternatively, add an assertion in debug builds to detect duplicates:

```rust
#[cfg(debug_assertions)]
{
    use std::collections::HashSet;
    let mut seen = HashSet::new();
    for txn in &shuffled_txns {
        let key = (txn.committed_hash(), txn.authenticator());
        assert!(seen.insert(key), "Shuffler produced duplicate transaction!");
    }
}
```

## Proof of Concept

This vulnerability requires creating a buggy shuffler implementation. Here's a test demonstrating the issue:

```rust
#[test]
fn test_duplicate_after_shuffle_reaches_executor() {
    use crate::block_preparer::BlockPreparer;
    use crate::transaction_shuffler::TransactionShuffler;
    use aptos_types::transaction::SignedTransaction;
    
    // Create a buggy shuffler that duplicates the first transaction
    struct BuggyShuffler;
    impl TransactionShuffler for BuggyShuffler {
        fn shuffle(&self, mut txns: Vec<SignedTransaction>) -> Vec<SignedTransaction> {
            if !txns.is_empty() {
                // BUG: Duplicate the first transaction
                let duplicate = txns[0].clone();
                txns.push(duplicate);
            }
            txns
        }
        // ... other trait methods ...
    }
    
    let preparer = BlockPreparer::new(
        payload_manager,
        filter_config,
        Arc::new(TxnHashAndAuthenticatorDeduper::new()), // Deduper
        Arc::new(BuggyShuffler), // Buggy shuffler
    );
    
    let (result_txns, _) = preparer.prepare_block(&block, input_txns, None, None).await;
    
    // The duplicate should be present (no post-shuffle dedup)
    assert!(has_duplicates(&result_txns), "Duplicate was not caught!");
}
```

## Notes

The vulnerability stems from a defensive programming gap: the system assumes shuffler correctness without validation. While the VM's sequence number validation prevents state corruption (second duplicate fails), this occurs AFTER wasted execution. During version mismatches, this causes consensus divergence - different validators execute different transaction sets, producing different state roots and breaking consensus safety.

### Citations

**File:** consensus/src/block_preparer.rs (L91-113)
```rust
            let filtered_txns = filter_block_transactions(
                txn_filter_config,
                block_id,
                block_author,
                block_epoch,
                block_timestamp_usecs,
                txns,
            );
            let deduped_txns = txn_deduper.dedup(filtered_txns);
            let mut shuffled_txns = {
                let _timer = TXN_SHUFFLE_SECONDS.start_timer();

                txn_shuffler.shuffle(deduped_txns)
            };

            if let Some(max_txns_from_block_to_execute) = max_txns_from_block_to_execute {
                shuffled_txns.truncate(max_txns_from_block_to_execute as usize);
            }
            TXNS_IN_BLOCK
                .with_label_values(&["after_filter"])
                .observe(shuffled_txns.len() as f64);
            MAX_TXNS_FROM_BLOCK_TO_EXECUTE.observe(shuffled_txns.len() as f64);
            shuffled_txns
```

**File:** consensus/src/txn_hash_and_authenticator_deduper.rs (L38-94)
```rust
impl TransactionDeduper for TxnHashAndAuthenticatorDeduper {
    fn dedup(&self, transactions: Vec<SignedTransaction>) -> Vec<SignedTransaction> {
        let _timer = TXN_DEDUP_SECONDS.start_timer();
        let mut seen = HashMap::new();
        let mut is_possible_duplicate = false;
        let mut possible_duplicates = vec![false; transactions.len()];
        for (i, txn) in transactions.iter().enumerate() {
            match seen.get(&(txn.sender(), txn.replay_protector())) {
                None => {
                    seen.insert((txn.sender(), txn.replay_protector()), i);
                },
                Some(first_index) => {
                    is_possible_duplicate = true;
                    possible_duplicates[*first_index] = true;
                    possible_duplicates[i] = true;
                },
            }
        }
        if !is_possible_duplicate {
            TXN_DEDUP_FILTERED.observe(0 as f64);
            return transactions;
        }

        let num_txns = transactions.len();

        let hash_and_authenticators: Vec<_> = possible_duplicates
            .into_par_iter()
            .zip(&transactions)
            .with_min_len(optimal_min_len(num_txns, 48))
            .map(|(need_hash, txn)| match need_hash {
                true => Some((txn.committed_hash(), txn.authenticator())),
                false => None,
            })
            .collect();

        // TODO: Possibly parallelize. See struct comment.
        let mut seen_hashes = HashSet::new();
        let mut num_duplicates: usize = 0;
        let filtered: Vec<_> = hash_and_authenticators
            .into_iter()
            .zip(transactions)
            .filter_map(|(maybe_hash, txn)| match maybe_hash {
                None => Some(txn),
                Some(hash_and_authenticator) => {
                    if seen_hashes.insert(hash_and_authenticator) {
                        Some(txn)
                    } else {
                        num_duplicates += 1;
                        None
                    }
                },
            })
            .collect();

        TXN_DEDUP_FILTERED.observe(num_duplicates as f64);
        filtered
    }
```

**File:** aptos-move/framework/aptos-framework/sources/transaction_validation.move (L233-241)
```text
            assert!(
                txn_sequence_number >= account_sequence_number,
                error::invalid_argument(PROLOGUE_ESEQUENCE_NUMBER_TOO_OLD)
            );

            assert!(
                txn_sequence_number == account_sequence_number,
                error::invalid_argument(PROLOGUE_ESEQUENCE_NUMBER_TOO_NEW)
            );
```

**File:** types/src/on_chain_config/execution_config.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::{
    block_executor::config::BlockExecutorConfigFromOnchain, on_chain_config::OnChainConfig,
};
use anyhow::{format_err, Result};
use serde::{Deserialize, Serialize};

/// The on-chain execution config, in order to be able to add fields, we use enum to wrap the actual struct.
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
pub enum OnChainExecutionConfig {
    V1(ExecutionConfigV1),
    V2(ExecutionConfigV2),
    V3(ExecutionConfigV3),
    /// To maintain backwards compatibility on replay, we must ensure that any new features resolve
    /// to previous behavior (before OnChainExecutionConfig was registered) in case of Missing.
    Missing,
    // Reminder: Add V4 and future versions here, after Missing (order matters for enums).
    V4(ExecutionConfigV4),
    V5(ExecutionConfigV5),
    V6(ExecutionConfigV6),
    V7(ExecutionConfigV7),
}

/// The public interface that exposes all values with safe fallback.
impl OnChainExecutionConfig {
    /// The type of the transaction shuffler being used.
    pub fn transaction_shuffler_type(&self) -> TransactionShufflerType {
        match &self {
            OnChainExecutionConfig::Missing => TransactionShufflerType::NoShuffling,
            OnChainExecutionConfig::V1(config) => config.transaction_shuffler_type.clone(),
            OnChainExecutionConfig::V2(config) => config.transaction_shuffler_type.clone(),
            OnChainExecutionConfig::V3(config) => config.transaction_shuffler_type.clone(),
            OnChainExecutionConfig::V4(config) => config.transaction_shuffler_type.clone(),
            OnChainExecutionConfig::V5(config) => config.transaction_shuffler_type.clone(),
            OnChainExecutionConfig::V6(config) => config.transaction_shuffler_type.clone(),
            OnChainExecutionConfig::V7(config) => config.transaction_shuffler_type.clone(),
        }
    }

    /// The per-block gas limit being used.
    pub fn block_gas_limit_type(&self) -> BlockGasLimitType {
        match &self {
            OnChainExecutionConfig::Missing => BlockGasLimitType::NoLimit,
            OnChainExecutionConfig::V1(_config) => BlockGasLimitType::NoLimit,
            OnChainExecutionConfig::V2(config) => config
                .block_gas_limit
                .map_or(BlockGasLimitType::NoLimit, BlockGasLimitType::Limit),
            OnChainExecutionConfig::V3(config) => config
```

**File:** consensus/src/transaction_shuffler/use_case_aware/delayed_queue.rs (L306-405)
```rust
    pub fn pop_head(&mut self, only_if_ready: bool) -> Option<Txn> {
        // See if any delayed txn exists. If not, return None.
        let use_case_entry = match self.use_cases_by_delay.first_entry() {
            None => {
                return None;
            },
            Some(occupied_entry) => occupied_entry,
        };
        let use_case_delay_key = use_case_entry.key();

        // Check readiness.
        if only_if_ready && use_case_delay_key.try_delay_till > self.output_idx {
            return None;
        }

        // Gonna return the front txn of the front account of the front use case.

        // First, both the use case and account need to be removed from the priority queues.
        let use_case_delay_key = *use_case_delay_key;
        let use_case_key = use_case_entry.remove();
        let use_case = self.use_cases.expect_mut(&use_case_key);
        let (account_delay_key, address) = use_case.expect_pop_head_account();
        assert!(account_delay_key.try_delay_till <= use_case_delay_key.try_delay_till);
        assert_eq!(account_delay_key.input_idx, use_case_delay_key.input_idx);

        // Pop first txn from account (for returning it later).
        let account = self.accounts.expect_mut(&address);
        let txn = account.expect_dequeue_txn();

        // Update priorities.
        account.update_try_delay_till(self.output_idx + 1 + self.config.sender_spread_factor());
        use_case.update_try_delay_till(
            self.output_idx + 1 + self.config.use_case_spread_factor(&use_case_key),
        );

        // Add account and original use case back to delay queues.

        if account.is_empty() {
            self.account_placeholders_by_delay
                .strict_insert(account.delay_key(), address);
            if use_case.is_empty() {
                self.use_case_placeholders_by_delay
                    .strict_insert(use_case.delay_key(), use_case_key.clone());
            } else {
                self.use_cases_by_delay
                    .strict_insert(use_case.delay_key(), use_case_key.clone());
            }
        } else {
            // See if account now belongs to a different use case.
            let new_use_case_key = account.expect_use_case_key();
            if new_use_case_key == use_case_key {
                use_case.add_account(address, account);
                self.use_cases_by_delay
                    .strict_insert(use_case.delay_key(), use_case_key.clone());
            } else {
                // Account now belongs to a different use case.

                // Add original use case back to delay queue.
                if use_case.is_empty() {
                    self.use_case_placeholders_by_delay
                        .strict_insert(use_case.delay_key(), use_case_key.clone());
                } else {
                    self.use_cases_by_delay
                        .strict_insert(use_case.delay_key(), use_case_key.clone());
                }

                // Add the account to the new use case.
                match self.use_cases.entry(new_use_case_key.clone()) {
                    hash_map::Entry::Occupied(mut occupied_entry) => {
                        // Existing use case, remove from priority queues.
                        let new_use_case = occupied_entry.get_mut();
                        if new_use_case.is_empty() {
                            self.use_case_placeholders_by_delay
                                .strict_remove(&new_use_case.delay_key());
                        } else {
                            self.use_cases_by_delay
                                .strict_remove(&new_use_case.delay_key());
                        }
                        // Add account to use case.
                        new_use_case.add_account(address, account);
                        // Add new use case back to delay queue.
                        self.use_cases_by_delay
                            .strict_insert(new_use_case.delay_key(), new_use_case_key.clone());
                    },
                    hash_map::Entry::Vacant(entry) => {
                        // Use case not tracked previously, try_delay_till = output_idx + 1
                        let new_use_case = entry.insert(UseCase::new_with_account(
                            self.output_idx + 1,
                            address,
                            account,
                        ));
                        self.use_cases_by_delay
                            .strict_insert(new_use_case.delay_key(), new_use_case_key.clone());
                    },
                }
            }
        }

        Some(txn.txn)
    }
```
