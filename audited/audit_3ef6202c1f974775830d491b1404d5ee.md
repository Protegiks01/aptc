# Audit Report

## Title
Race Condition in Data Stream Progress Updates Causes Sync Failures Due to Stale Global Data Summary

## Summary
The `update_progress_of_data_stream()` function retrieves a snapshot of the global data summary at line 345 which can become stale while concurrent background refreshes are updating the cached summary. This stale snapshot is then used to make data availability decisions and create data client requests, potentially requesting data that is no longer available in the network, causing stream failures and sync interruptions. [1](#0-0) 

## Finding Description
The data streaming service maintains a cached global data summary in an `Arc<ArcSwap<GlobalDataSummary>>` that reflects what data is currently advertised by peers in the network. This cache is refreshed asynchronously by a background task every 50ms (default configuration). [2](#0-1) 

The background refresher task continuously updates this cache: [3](#0-2) 

When `update_progress_of_data_stream()` is called, it loads a snapshot of the global data summary using `get_global_data_summary()`: [4](#0-3) 

This snapshot is then used to initialize data requests or process data responses: [5](#0-4) 

**The Race Condition:**
1. At time T0: `update_progress_of_data_stream()` calls `get_global_data_summary()` obtaining snapshot S1
2. At time T1 (concurrent): Background refresher updates the cache with snapshot S2 reflecting newer network state (e.g., peer P1 went offline, data pruned)
3. At time T2: The stream engine uses stale snapshot S1 to select target ledger info and create data requests
4. At time T3: Data requests are sent to the network, but the data indicated in S1 is no longer available
5. Requests fail repeatedly with exponential backoff
6. After `max_request_retry` failures (default: 5), the stream terminates

**Critical Flow in ContinuousTransactionStreamEngine:**

The stale summary is used to select the target ledger info from advertised data: [6](#0-5) 

The `select_target_ledger_info` method uses the advertised data from the stale summary: [7](#0-6) 

**Lack of Refresh During Ongoing Stream Processing:**

Unlike new stream creation which manually refreshes the summary, ongoing stream updates do not: [8](#0-7) 

**Inadequate Error Handling:**

When data requests fail, the stream retries without refreshing the global data summary, continuing to use the same stale information: [9](#0-8) 

The retry mechanism simply increments a counter and resends the same request: [10](#0-9) 

## Impact Explanation
This vulnerability causes **Medium Severity** impact according to Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: When streams fail due to stale data, nodes cannot complete state synchronization, requiring manual stream restarts or service restarts
- **Sync disruption**: Nodes fall behind in syncing the latest blockchain state, creating operational issues
- **Cascading failures**: If multiple peers experience network churn simultaneously, multiple streams can fail, severely degrading sync performance

This does NOT cause:
- Consensus breaks (all nodes experience the same issue if data truly unavailable)
- Fund loss or theft
- Network-wide liveness failures (isolated to individual syncing nodes)

The default configuration makes this more likely: [11](#0-10) 

With `global_summary_refresh_interval_ms: 50` and `progress_check_interval_ms: 50`, the race window is significant.

## Likelihood Explanation
**High Likelihood** - This race condition can occur naturally during normal network operation:

1. **Network Churn**: Peers regularly join/leave the network, changing advertised data
2. **Data Pruning**: Nodes prune old data, reducing advertised ranges
3. **Peer Failures**: Network partitions or peer crashes remove advertised data
4. **Timing**: With both refresh and progress checks happening every 50ms, race windows occur frequently

The vulnerability is **deterministic** - when the timing aligns (which happens regularly in production), the issue manifests. No special attacker capability is required beyond normal network participation.

## Recommendation

**Solution**: Refresh the global data summary at the beginning of `update_progress_of_data_stream()` to ensure the most current network state is used for decision-making.

```rust
async fn update_progress_of_data_stream(
    &mut self,
    data_stream_id: &DataStreamId,
) -> Result<(), Error> {
    // Refresh the global data summary before using it
    refresh_global_data_summary(
        self.aptos_data_client.clone(),
        self.global_data_summary.clone(),
    );
    
    let global_data_summary = self.get_global_data_summary();
    
    // ... rest of the function
}
```

**Alternative**: Implement retry logic that refreshes the global data summary on failures:

```rust
fn handle_data_client_error(
    &mut self,
    data_client_request: &DataClientRequest,
    data_client_error: &aptos_data_client::error::Error,
) -> Result<(), Error> {
    // Log the error
    warn!(...);
    
    // Refresh global data summary to get latest network state
    // before retrying (would require passing in the summary or data client)
    
    self.resend_data_client_request(data_client_request)
}
```

**Best Practice**: Combine both approaches - refresh on each progress update AND on failures to minimize stale data usage.

## Proof of Concept

```rust
#[tokio::test]
async fn test_stale_global_summary_causes_request_failure() {
    use std::sync::Arc;
    use arc_swap::ArcSwap;
    use aptos_data_client::global_summary::{GlobalDataSummary, AdvertisedData};
    
    // Create a streaming service with initial global data summary
    let mut streaming_service = create_test_streaming_service();
    
    // Create a data stream that references data at version 1000
    let initial_summary = create_summary_with_data_up_to_version(1000);
    streaming_service.global_data_summary.store(Arc::new(initial_summary));
    
    // Start a continuous transaction stream
    let stream_id = create_continuous_stream(&mut streaming_service, 900, 1000);
    
    // Simulate network churn: Update global summary to remove advertised data
    // This simulates a peer going offline or pruning data
    let updated_summary = create_summary_with_data_up_to_version(950);
    streaming_service.global_data_summary.store(Arc::new(updated_summary));
    
    // Now call update_progress_of_data_stream
    // The function will load a snapshot (which might be stale if timing aligns)
    // and try to request data up to version 1000
    let result = streaming_service
        .update_progress_of_data_stream(&stream_id)
        .await;
    
    // The stream should fail because:
    // 1. It targets version 1000 based on potentially stale snapshot
    // 2. Data is only available up to version 950
    // 3. Requests fail repeatedly
    // 4. After max_request_retry, stream terminates
    
    assert!(result.is_err() || stream_fails_after_retries(&streaming_service, stream_id));
}
```

**To reproduce in production:**
1. Start a node syncing via continuous transaction streams
2. During sync, force a peer advertising high-version data to disconnect
3. Observe that ongoing streams may fail with "NoDataToFetch" or similar errors
4. Check metrics for `RETRIED_DATA_REQUESTS` and eventual stream termination
5. Node requires stream restart to recover

The race condition window is approximately 50ms (default refresh interval), during which stale summaries are actively used for critical sync decisions.

### Citations

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L64-65)
```rust
    // Cached global data summary
    global_data_summary: Arc<ArcSwap<GlobalDataSummary>>,
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L156-159)
```rust
    /// Returns the global data summary
    fn get_global_data_summary(&self) -> GlobalDataSummary {
        self.global_data_summary.load().clone().deref().clone()
    }
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L265-269)
```rust
        // Refresh the cached global data summary
        refresh_global_data_summary(
            self.aptos_data_client.clone(),
            self.global_data_summary.clone(),
        );
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L341-345)
```rust
    async fn update_progress_of_data_stream(
        &mut self,
        data_stream_id: &DataStreamId,
    ) -> Result<(), Error> {
        let global_data_summary = self.get_global_data_summary();
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L366-381)
```rust
        // Drive data stream progress
        if !data_stream.data_requests_initialized() {
            // Initialize the request batch by sending out data client requests
            data_stream.initialize_data_requests(global_data_summary)?;
            info!(
                (LogSchema::new(LogEntry::InitializeStream)
                    .stream_id(*data_stream_id)
                    .event(LogEvent::Success)
                    .message("Data stream initialized."))
            );
        } else {
            // Process any data client requests that have received responses
            data_stream
                .process_data_responses(global_data_summary)
                .await?;
        }
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L408-428)
```rust
/// Spawns a task that periodically refreshes the global data summary
fn spawn_global_data_summary_refresher<T: AptosDataClientInterface + Send + Clone + 'static>(
    data_streaming_service_config: DataStreamingServiceConfig,
    aptos_data_client: T,
    cached_global_data_summary: Arc<ArcSwap<GlobalDataSummary>>,
) {
    tokio::spawn(async move {
        loop {
            // Refresh the cached global data summary
            refresh_global_data_summary(
                aptos_data_client.clone(),
                cached_global_data_summary.clone(),
            );

            // Sleep for a while before refreshing the cache again
            let sleep_duration_ms =
                data_streaming_service_config.global_summary_refresh_interval_ms;
            tokio::time::sleep(Duration::from_millis(sleep_duration_ms)).await;
        }
    });
}
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L498-535)
```rust
    fn select_target_ledger_info(
        &self,
        advertised_data: &AdvertisedData,
    ) -> Result<Option<LedgerInfoWithSignatures>, Error> {
        // Check if the stream has a final target ledger info
        match &self.request {
            StreamRequest::ContinuouslyStreamTransactions(request) => {
                if let Some(target) = &request.target {
                    return Ok(Some(target.clone()));
                }
            },
            StreamRequest::ContinuouslyStreamTransactionOutputs(request) => {
                if let Some(target) = &request.target {
                    return Ok(Some(target.clone()));
                }
            },
            StreamRequest::ContinuouslyStreamTransactionsOrOutputs(request) => {
                if let Some(target) = &request.target {
                    return Ok(Some(target.clone()));
                }
            },
            request => invalid_stream_request!(request),
        };

        // We don't have a final target, select the highest to make progress
        if let Some(highest_synced_ledger_info) = advertised_data.highest_synced_ledger_info() {
            let (next_request_version, _) = self.next_request_version_and_epoch;
            if next_request_version > highest_synced_ledger_info.ledger_info().version() {
                Ok(None) // We're already at the highest synced ledger info. There's no known target.
            } else {
                Ok(Some(highest_synced_ledger_info))
            }
        } else {
            Err(Error::DataIsUnavailable(
                "Unable to find the highest synced ledger info!".into(),
            ))
        }
    }
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L1186-1222)
```rust
        // If we don't have a syncing target, try to select one
        let (next_request_version, next_request_epoch) = self.next_request_version_and_epoch;
        if self.current_target_ledger_info.is_none() {
            // Try to select a new ledger info from the advertised data
            if let Some(target_ledger_info) =
                self.select_target_ledger_info(&global_data_summary.advertised_data)?
            {
                if target_ledger_info.ledger_info().epoch() > next_request_epoch {
                    // There was an epoch change. Request an epoch ending ledger info.
                    info!(
                        (LogSchema::new(LogEntry::AptosDataClient)
                            .event(LogEvent::Pending)
                            .message(&format!(
                                "Requested an epoch ending ledger info for epoch: {:?}",
                                next_request_epoch
                            )))
                    );
                    self.end_of_epoch_requested = true;
                    return Ok(vec![DataClientRequest::EpochEndingLedgerInfos(
                        EpochEndingLedgerInfosRequest {
                            start_epoch: next_request_epoch,
                            end_epoch: next_request_epoch,
                        },
                    )]);
                } else {
                    debug!(
                        (LogSchema::new(LogEntry::ReceivedDataResponse)
                            .event(LogEvent::Success)
                            .message(&format!(
                                "Setting new target ledger info. Version: {:?}, Epoch: {:?}",
                                target_ledger_info.ledger_info().version(),
                                target_ledger_info.ledger_info().epoch()
                            )))
                    );
                    self.current_target_ledger_info = Some(target_ledger_info);
                }
            }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L709-725)
```rust

    /// Handles an error returned by the data client in relation to a request
    fn handle_data_client_error(
        &mut self,
        data_client_request: &DataClientRequest,
        data_client_error: &aptos_data_client::error::Error,
    ) -> Result<(), Error> {
        // Log the error
        warn!(LogSchema::new(LogEntry::ReceivedDataResponse)
            .stream_id(self.data_stream_id)
            .event(LogEvent::Error)
            .error(&data_client_error.clone().into())
            .message("Encountered a data client error!"));

        // TODO(joshlind): can we identify the best way to react to the error?
        self.resend_data_client_request(data_client_request)
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L729-744)
```rust
    fn resend_data_client_request(
        &mut self,
        data_client_request: &DataClientRequest,
    ) -> Result<(), Error> {
        // Increment the number of client failures for this request
        self.request_failure_count += 1;

        // Resend the client request
        let pending_client_response = self.send_client_request(true, data_client_request.clone());

        // Push the pending response to the head of the sent requests queue
        self.get_sent_data_requests()?
            .push_front(pending_client_response);

        Ok(())
    }
```

**File:** config/src/config/state_sync_config.rs (L265-280)
```rust
impl Default for DataStreamingServiceConfig {
    fn default() -> Self {
        Self {
            dynamic_prefetching: DynamicPrefetchingConfig::default(),
            enable_subscription_streaming: false,
            global_summary_refresh_interval_ms: 50,
            max_concurrent_requests: MAX_CONCURRENT_REQUESTS,
            max_concurrent_state_requests: MAX_CONCURRENT_STATE_REQUESTS,
            max_data_stream_channel_sizes: 50,
            max_notification_id_mappings: 300,
            max_num_consecutive_subscriptions: 45, // At ~3 blocks per second, this should last ~15 seconds
            max_pending_requests: 50,
            max_request_retry: 5,
            max_subscription_stream_lag_secs: 10, // 10 seconds
            progress_check_interval_ms: 50,
        }
```
