# Audit Report

## Title
Unauthenticated Cross-Shard Message Injection Enables State Corruption and Consensus Failure in Remote Sharded Execution

## Summary
The remote sharded block executor lacks authentication and sender verification for cross-shard messages. A compromised or malicious shard process can send arbitrary state values to other shards via forged `RemoteTxnWriteMsg` messages, causing dependent transactions to execute with corrupted state. This breaks deterministic execution across validators and leads to consensus failure.

## Finding Description

The `ProcessExecutorService` architecture enables distributed block execution across multiple remote shard processes. [1](#0-0) 

When transactions have cross-shard dependencies, the `CrossShardCommitSender` sends state values from one shard to another via `RemoteTxnWriteMsg` messages. [2](#0-1) 

However, the message sending mechanism has **no sender authentication**. Any shard can send messages to any other shard: [3](#0-2) 

The receiving shard accepts messages without verifying the sender identity: [4](#0-3) 

The underlying gRPC service extracts the sender address but **discards it** before forwarding to handlers: [5](#0-4) 

Messages are processed in `CrossShardCommitReceiver` without any validation: [6](#0-5) 

State values are set unconditionally with no protection against overwrites: [7](#0-6) 

**Attack Scenario:**

1. Block partitioned across Shards 0, 1, and 2
2. Transaction T1 in Shard 0 writes `StateKey K` with legitimate value `V_legitimate`
3. Transaction T2 in Shard 1 depends on reading `K`
4. Compromised Shard 2 sends forged `RemoteTxnWriteMsg(K, V_malicious)` to Shard 1
5. Shard 1 receives the malicious message (via race condition or overwrites legitimate value)
6. T2 executes with `V_malicious` instead of `V_legitimate`
7. Shard 1 returns incorrect execution results
8. Different validators experience different message arrival orders, producing different state roots
9. Consensus failure occurs due to state root mismatch

The vulnerability breaks the **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks."

## Impact Explanation

This is a **Critical Severity** vulnerability (up to $1,000,000 per Aptos Bug Bounty) under the "Consensus/Safety violations" category.

**Impact:**
- **Complete consensus failure**: Different validators produce different state roots for identical blocks
- **Network partition**: Validators cannot reach consensus on block validity
- **Non-recoverable without hard fork**: Requires manual intervention to restore consensus
- **Chain split risk**: Network may bifurcate if subsets of validators accept different states

The attack directly violates AptosBFT safety guarantees by enabling Byzantine behavior at the execution layer, bypassing consensus-level protections.

## Likelihood Explanation

**Likelihood: Medium**

**Requirements:**
- Attacker must compromise one shard process (via RCE, memory corruption, or insider access)
- Remote sharded execution must be enabled (production deployment feature)
- Block must have cross-shard transaction dependencies

**Feasibility:**
- Attack execution is straightforward once a shard is compromised
- No cryptographic operations required
- Race conditions make exploitation reliable
- Message injection requires only network access to other shards

**Mitigating Factors:**
- All shards typically controlled by single validator operator
- However, compromise of one process enables full attack
- Defense-in-depth principle violated (no isolation between shards)

The explicit security question assumes a "malicious shard" scenario, making this threat model directly in scope.

## Recommendation

Implement **sender authentication and message validation** for all cross-shard communications:

**1. Add message authentication:**
```rust
pub struct AuthenticatedCrossShardMsg {
    pub sender_shard_id: ShardId,
    pub message: CrossShardMsg,
    pub signature: Signature, // HMAC or Ed25519 signature
}
```

**2. Add sender verification in CrossShardCommitReceiver:**
```rust
pub fn start<S: StateView + Sync + Send>(
    cross_shard_state_view: Arc<CrossShardStateView<S>>,
    cross_shard_client: Arc<dyn CrossShardClient>,
    round: RoundId,
    expected_senders: HashMap<StateKey, ShardId>, // Map state keys to authorized sender shards
) {
    loop {
        let msg = cross_shard_client.receive_cross_shard_msg(round);
        match msg {
            RemoteTxnWriteMsg(txn_commit_msg) => {
                let (state_key, write_op) = txn_commit_msg.peek();
                
                // VALIDATE SENDER
                if let Some(expected_sender) = expected_senders.get(&state_key) {
                    if msg.sender_shard_id != *expected_sender {
                        error!("Received cross-shard message from unauthorized shard");
                        continue; // Reject message
                    }
                } else {
                    error!("Received cross-shard message for unexpected state key");
                    continue;
                }
                
                // SET VALUE ONLY ONCE
                if cross_shard_state_view.is_already_set(&state_key) {
                    warn!("Attempted duplicate set for state key");
                    continue;
                }
                
                let (state_key, write_op) = txn_commit_msg.take();
                cross_shard_state_view.set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
            },
            CrossShardMsg::StopMsg => break,
        }
    }
}
```

**3. Add idempotency protection in RemoteStateValue:**
```rust
pub fn set_value(&self, value: Option<StateValue>) -> Result<(), Error> {
    let (lock, cvar) = &*self.value_condition;
    let mut status = lock.lock().unwrap();
    
    // PREVENT OVERWRITES
    match *status {
        RemoteValueStatus::Ready(_) => {
            return Err(Error::AlreadySet);
        },
        RemoteValueStatus::Waiting => {
            *status = RemoteValueStatus::Ready(value);
            cvar.notify_all();
            Ok(())
        }
    }
}
```

**4. Pass sender information through network layer:**

Modify gRPC handler to preserve sender address: [8](#0-7) 

Change to pass `NetworkMessage` (with sender field) instead of just `Message`.

## Proof of Concept

```rust
// File: execution/executor-service/src/tests.rs
#[test]
fn test_cross_shard_message_injection_attack() {
    use aptos_types::state_store::state_key::StateKey;
    use aptos_types::write_set::WriteOp;
    use aptos_vm::sharded_block_executor::messages::{CrossShardMsg, RemoteTxnWrite};
    
    // Setup: 3 shards with cross-shard dependencies
    let num_shards = 3;
    let shard_addresses = create_test_shard_addresses(num_shards);
    
    // Create legitimate transaction T1 in Shard 0 writing state key K
    let state_key = StateKey::raw(b"test_balance");
    let legitimate_value = WriteOp::Value(b"balance:1000".to_vec());
    
    // Shard 0 sends legitimate message to Shard 1
    let legitimate_msg = CrossShardMsg::RemoteTxnWriteMsg(
        RemoteTxnWrite::new(state_key.clone(), Some(legitimate_value.clone()))
    );
    
    // ATTACK: Malicious Shard 2 sends forged message to Shard 1
    let malicious_value = WriteOp::Value(b"balance:999999".to_vec());
    let malicious_msg = CrossShardMsg::RemoteTxnWriteMsg(
        RemoteTxnWrite::new(state_key.clone(), Some(malicious_value.clone()))
    );
    
    // Both messages arrive at Shard 1
    // No sender verification - both are accepted
    // Last message wins due to overwrite in RemoteStateValue::set_value()
    
    // Expected: Only legitimate message from Shard 0 should be accepted
    // Actual: Both messages accepted, causing non-deterministic execution
    
    // Result: Different validators may see different arrival orders
    // Leading to different state roots and consensus failure
}
```

**Notes**

This vulnerability exists specifically in the **remote sharded execution** architecture where shards run as separate processes. [9](#0-8) 

The local in-process execution mode is not affected as all shards share the same trusted process space. [10](#0-9) 

The cross-shard dependency mapping is computed during partitioning and should be used to validate which shard is authorized to send values for specific state keys. [11](#0-10)

### Citations

**File:** execution/executor-service/src/process_executor_service.rs (L11-14)
```rust
/// An implementation of the remote executor service that runs in a standalone process.
pub struct ProcessExecutorService {
    executor_service: ExecutorService,
}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L26-45)
```rust
    pub fn start<S: StateView + Sync + Send>(
        cross_shard_state_view: Arc<CrossShardStateView<S>>,
        cross_shard_client: Arc<dyn CrossShardClient>,
        round: RoundId,
    ) {
        loop {
            let msg = cross_shard_client.receive_cross_shard_msg(round);
            match msg {
                RemoteTxnWriteMsg(txn_commit_msg) => {
                    let (state_key, write_op) = txn_commit_msg.take();
                    cross_shard_state_view
                        .set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
                },
                CrossShardMsg::StopMsg => {
                    trace!("Cross shard commit receiver stopped for round {}", round);
                    break;
                },
            }
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L62-101)
```rust
    pub fn new(
        shard_id: ShardId,
        cross_shard_client: Arc<dyn CrossShardClient>,
        sub_block: &SubBlock<AnalyzedTransaction>,
    ) -> Self {
        let mut dependent_edges = HashMap::new();
        let mut num_dependent_edges = 0;
        for (txn_idx, txn_with_deps) in sub_block.txn_with_index_iter() {
            let mut storage_locations_to_target = HashMap::new();
            for (txn_id_with_shard, storage_locations) in txn_with_deps
                .cross_shard_dependencies
                .dependent_edges()
                .iter()
            {
                for storage_location in storage_locations {
                    storage_locations_to_target
                        .entry(storage_location.clone().into_state_key())
                        .or_insert_with(HashSet::new)
                        .insert((txn_id_with_shard.shard_id, txn_id_with_shard.round_id));
                    num_dependent_edges += 1;
                }
            }
            if !storage_locations_to_target.is_empty() {
                dependent_edges.insert(txn_idx as TxnIndex, storage_locations_to_target);
            }
        }

        trace!(
            "CrossShardCommitSender::new: shard_id: {:?}, num_dependent_edges: {:?}",
            shard_id,
            num_dependent_edges
        );

        Self {
            shard_id,
            cross_shard_client,
            dependent_edges,
            index_offset: sub_block.start_index as TxnIndex,
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L103-134)
```rust
    fn send_remote_update_for_success(
        &self,
        txn_idx: TxnIndex,
        txn_output: &OnceCell<TransactionOutput>,
    ) {
        let edges = self.dependent_edges.get(&txn_idx).unwrap();
        let write_set = txn_output
            .get()
            .expect("Committed output must be set")
            .write_set();

        for (state_key, write_op) in write_set.expect_write_op_iter() {
            if let Some(dependent_shard_ids) = edges.get(state_key) {
                for (dependent_shard_id, round_id) in dependent_shard_ids.iter() {
                    trace!("Sending remote update for success for shard id {:?} and txn_idx: {:?}, state_key: {:?}, dependent shard id: {:?}", self.shard_id, txn_idx, state_key, dependent_shard_id);
                    let message = RemoteTxnWriteMsg(RemoteTxnWrite::new(
                        state_key.clone(),
                        Some(write_op.clone()),
                    ));
                    if *round_id == GLOBAL_ROUND_ID {
                        self.cross_shard_client.send_global_msg(message);
                    } else {
                        self.cross_shard_client.send_cross_shard_msg(
                            *dependent_shard_id,
                            *round_id,
                            message,
                        );
                    }
                }
            }
        }
    }
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L55-59)
```rust
    fn send_cross_shard_msg(&self, shard_id: ShardId, round: RoundId, msg: CrossShardMsg) {
        let input_message = bcs::to_bytes(&msg).unwrap();
        let tx = self.message_txs[shard_id][round].lock().unwrap();
        tx.send(Message::new(input_message)).unwrap();
    }
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L61-66)
```rust
    fn receive_cross_shard_msg(&self, current_round: RoundId) -> CrossShardMsg {
        let rx = self.message_rxs[current_round].lock().unwrap();
        let message = rx.recv().unwrap();
        let msg: CrossShardMsg = bcs::from_bytes(&message.to_bytes()).unwrap();
        msg
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L92-116)
```rust
impl NetworkMessageService for GRPCNetworkMessageServiceServerWrapper {
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/remote_state_value.rs (L22-27)
```rust
    pub fn set_value(&self, value: Option<StateValue>) {
        let (lock, cvar) = &*self.value_condition;
        let mut status = lock.lock().unwrap();
        *status = RemoteValueStatus::Ready(value);
        cvar.notify_all();
    }
```

**File:** execution/executor-service/src/remote_executor_service.rs (L14-19)
```rust
/// the remote executor client and executes the block locally and returns the result.
pub struct ExecutorService {
    shard_id: ShardId,
    controller: NetworkController,
    executor_service: Arc<ShardedExecutorService<RemoteStateViewClient>>,
}
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L31-36)
```rust
/// Executor service that runs on local machine and waits for commands from the coordinator and executes
/// them in parallel.
pub struct LocalExecutorService<S: StateView + Sync + Send + 'static> {
    join_handle: Option<thread::JoinHandle<()>>,
    phantom: std::marker::PhantomData<S>,
}
```
