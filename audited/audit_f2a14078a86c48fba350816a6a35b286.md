# Audit Report

## Title
Unhandled Database Error in Ledger Pruner Initialization Causes Validator Startup Failure

## Summary
The `init_pruner()` function in the ledger pruner manager uses `.expect()` to unwrap the result of `LedgerPruner::new()`, causing a panic if pruner initialization fails. Database corruption or resource exhaustion during validator restart will trigger this panic, preventing the validator from starting and causing complete downtime. [1](#0-0) 

## Finding Description

When a validator restarts, `AptosDB::open()` is called during the initialization sequence. This eventually invokes `LedgerPrunerManager::new()`, which conditionally creates pruner workers if pruning is enabled. The `init_pruner()` helper function creates the `LedgerPruner` instance using `.expect()`, converting any error into an immediate panic. [2](#0-1) 

The `LedgerPruner::new()` function creates multiple sub-pruners, each of which can fail during initialization. Each sub-pruner calls `get_or_initialize_subpruner_progress()` to read/write database metadata, and then performs catch-up pruning to synchronize with the metadata pruner's progress. [3](#0-2) 

The `get_or_initialize_subpruner_progress()` utility function performs database read and write operations that can fail due to:
- Database corruption (RocksDB errors, filesystem corruption)
- I/O errors (disk failures, permission issues)
- Resource exhaustion (disk space full, file descriptor exhaustion) [4](#0-3) 

Additionally, each sub-pruner's `new()` method calls `prune()` to catch up to the current metadata progress, which involves multiple database operations that can also fail under the same conditions. [5](#0-4) 

**Failure Scenario:**

1. A validator is running normally with pruning enabled
2. An environmental issue occurs (power failure, disk error, filesystem corruption)
3. The database becomes corrupted or the disk fills up
4. The validator restarts (for recovery, upgrade, or maintenance)
5. During startup, `AptosDB::open()` → `LedgerPrunerManager::new()` → `init_pruner()` → `LedgerPruner::new()`
6. When initializing a sub-pruner, either:
   - `get_or_initialize_subpruner_progress()` fails on database read/write operations, OR
   - The catch-up `prune()` call fails on database operations
7. The error propagates back to `LedgerPruner::new()`, which returns `Err(...)`
8. The `.expect()` at line 148 panics, crashing the validator startup
9. The validator cannot start, resulting in complete downtime

The database opening sequence shows this happens in the critical startup path: [6](#0-5) [7](#0-6) 

## Impact Explanation

This issue qualifies as **High Severity** under the Aptos bug bounty program, which explicitly includes "Validator node slowdowns" and "API crashes" as High severity impacts. A validator that fails to start due to this panic experiences complete downtime, which is more severe than a slowdown.

**Concrete impacts:**
- **Validator Unavailability**: The affected validator cannot participate in consensus, reducing network decentralization
- **Missed Rewards**: The validator cannot propose blocks or vote, missing potential rewards
- **Potential Penalties**: Extended downtime may trigger slashing or reputation penalties
- **Manual Intervention Required**: Recovery requires operator intervention to repair the database or clear disk space
- **Network Impact**: If multiple validators are affected simultaneously (e.g., during coordinated upgrades with pre-existing corruption), consensus could be impacted

While this cannot be directly triggered remotely by an attacker, it represents a critical operational vulnerability where environmental failures (which are expected and should be handled gracefully) instead cause catastrophic validator failure.

## Likelihood Explanation

**Likelihood: Medium to High** in production environments.

Database corruption and resource exhaustion are realistic operational scenarios:

1. **Database Corruption**: Can occur from:
   - Hardware failures (disk errors, bad sectors)
   - Power loss during write operations
   - Filesystem bugs or corruption
   - RocksDB bugs or crashes
   - System crashes during critical database operations

2. **Resource Exhaustion**:
   - Disk space can fill from transaction data growth, state bloat, logs
   - File descriptors can be exhausted under heavy load
   - Memory pressure can cause I/O errors

3. **Restart Frequency**: Validators restart regularly for:
   - Software upgrades (routine maintenance)
   - Configuration changes
   - Recovery from other issues
   - System maintenance or reboots

The combination of restart frequency with environmental failure probability makes this a realistic concern, especially in large validator networks where at least some nodes will experience these conditions over time.

## Recommendation

Replace `.expect()` with proper error handling that allows the error to propagate up the call stack. The error should be logged and returned, allowing the validator startup code to handle it gracefully (e.g., retry, alert operators, fail with a clear error message).

**Recommended fix for `ledger_pruner_manager.rs`:**

```rust
fn init_pruner(
    ledger_db: Arc<LedgerDb>,
    ledger_pruner_config: LedgerPrunerConfig,
    internal_indexer_db: Option<InternalIndexerDB>,
) -> Result<PrunerWorker> {  // Change return type to Result
    let pruner = Arc::new(
        LedgerPruner::new(ledger_db, internal_indexer_db)?  // Use ? instead of expect
    );

    PRUNER_WINDOW
        .with_label_values(&["ledger_pruner"])
        .set(ledger_pruner_config.prune_window as i64);

    PRUNER_BATCH_SIZE
        .with_label_values(&["ledger_pruner"])
        .set(ledger_pruner_config.batch_size as i64);

    Ok(PrunerWorker::new(pruner, ledger_pruner_config.batch_size, "ledger"))
}
```

**Update the caller in `LedgerPrunerManager::new()`:**

```rust
pub fn new(
    ledger_db: Arc<LedgerDb>,
    ledger_pruner_config: LedgerPrunerConfig,
    internal_indexer_db: Option<InternalIndexerDB>,
) -> Result<Self> {  // Change return type to Result
    let pruner_worker = if ledger_pruner_config.enable {
        Some(Self::init_pruner(
            Arc::clone(&ledger_db),
            ledger_pruner_config,
            internal_indexer_db,
        )?)  // Propagate error instead of panicking
    } else {
        None
    };

    let min_readable_version =
        pruner_utils::get_ledger_pruner_progress(&ledger_db)?;  // Already returns Result

    PRUNER_VERSIONS
        .with_label_values(&["ledger_pruner", "min_readable"])
        .set(min_readable_version as i64);

    Ok(Self {
        ledger_db,
        prune_window: ledger_pruner_config.prune_window,
        pruner_worker,
        pruning_batch_size: ledger_pruner_config.batch_size,
        latest_version: Arc::new(Mutex::new(min_readable_version)),
        user_pruning_window_offset: ledger_pruner_config.user_pruning_window_offset,
        min_readable_version: AtomicVersion::new(min_readable_version),
    })
}
```

This allows the error to propagate to `AptosDB::open()`, which returns a `Result`, enabling the validator startup code to handle database initialization failures gracefully with proper error messages and potential recovery mechanisms.

The same pattern should be applied to `LedgerMetadataPruner::new()` which also uses `.expect()` on line 126 of `mod.rs`.

## Proof of Concept

**Simulating disk space exhaustion during validator startup:**

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_temppath::TempPath;
    use aptos_config::config::LedgerPrunerConfig;
    
    #[test]
    #[should_panic(expected = "Failed to create ledger pruner")]
    fn test_pruner_init_panic_on_disk_full() {
        // Create a test database
        let tmpdir = TempPath::new();
        let db = Arc::new(LedgerDb::new_for_test(&tmpdir));
        
        // Simulate disk full by making the filesystem read-only
        // or by mocking the database to return I/O errors
        
        // This will panic when trying to initialize the pruner
        let config = LedgerPrunerConfig {
            enable: true,
            prune_window: 1000,
            batch_size: 100,
            user_pruning_window_offset: 0,
        };
        
        // This call will panic due to database write failure
        let _ = LedgerPrunerManager::new(db, config, None);
    }
}
```

**Simulating database corruption:**

```bash
# Start a validator with pruning enabled
# Stop the validator
# Corrupt the RocksDB database files
dd if=/dev/urandom of=/path/to/db/ledger.db/CURRENT bs=1024 count=1
# Attempt to restart the validator
# The validator will panic during startup at line 148
```

The panic can be verified by examining the validator logs, which will show:
```
thread 'main' panicked at 'Failed to create ledger pruner.': <error details>
```

## Notes

This vulnerability represents a failure in defensive programming where environmental errors that should be handled gracefully instead cause catastrophic failure. While not directly exploitable by remote attackers, it creates an operational security risk where validators can be taken offline by environmental conditions that are expected to occur in production systems. The fix is straightforward: replace `.expect()` with proper error propagation, allowing higher-level code to handle failures appropriately.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L146-149)
```rust
        let pruner = Arc::new(
            LedgerPruner::new(ledger_db, internal_indexer_db)
                .expect("Failed to create ledger pruner."),
        );
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L86-90)
```rust
        let ledger_pruner = LedgerPrunerManager::new(
            Arc::clone(&ledger_db),
            pruner_config.ledger_pruner_config,
            internal_indexer_db,
        );
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L118-143)
```rust
    pub fn new(
        ledger_db: Arc<LedgerDb>,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        info!(name = LEDGER_PRUNER_NAME, "Initializing...");

        let ledger_metadata_pruner = Box::new(
            LedgerMetadataPruner::new(ledger_db.metadata_db_arc())
                .expect("Failed to initialize ledger_metadata_pruner."),
        );

        let metadata_progress = ledger_metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created ledger metadata pruner, start catching up all sub pruners."
        );

        let transaction_store = Arc::new(TransactionStore::new(Arc::clone(&ledger_db)));

        let event_store_pruner = Box::new(EventStorePruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db.clone(),
        )?);
        let persisted_auxiliary_info_pruner = Box::new(PersistedAuxiliaryInfoPruner::new(
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-60)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L85-109)
```rust
    pub(in crate::pruner) fn new(
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;

        let myself = EventStorePruner {
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up EventStorePruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** aptos-node/src/storage.rs (L63-73)
```rust
    let (aptos_db_reader, db_rw, backup_service) = match FastSyncStorageWrapper::initialize_dbs(
        node_config,
        internal_indexer_db.clone(),
        update_sender,
    )? {
        Either::Left(db) => {
            let (db_arc, db_rw) = DbReaderWriter::wrap(db);
            let db_backup_service =
                start_backup_service(node_config.storage.backup_service_address, db_arc.clone());
            maybe_apply_genesis(&db_rw, node_config)?;
            (db_arc as Arc<dyn DbReader>, db_rw, Some(db_backup_service))
```

**File:** storage/aptosdb/src/fast_sync_storage_wrapper.rs (L48-59)
```rust
        let mut db_main = AptosDB::open(
            config.storage.get_dir_paths(),
            /*readonly=*/ false,
            config.storage.storage_pruner_config,
            config.storage.rocksdb_configs,
            config.storage.enable_indexer,
            config.storage.buffered_state_target_items,
            config.storage.max_num_nodes_per_lru_cache_shard,
            internal_indexer_db,
            config.storage.hot_state_config,
        )
        .map_err(|err| anyhow!("fast sync DB failed to open {}", err))?;
```
