# Audit Report

## Title
V2 Batch Metadata Orphaning Due to Incorrect Deletion Method in Expiration Cleanup

## Summary
The `update_certified_timestamp()` and `gc_previous_epoch_batches_from_db_v2()` functions incorrectly call `delete_batches()` instead of `delete_batches_v2()` when cleaning up expired v2 batches. This causes BatchInfoExt entries containing extended metadata (ExtraBatchInfo and BatchKind) to remain orphaned in the database indefinitely, leading to storage bloat and state inconsistency between memory and persistent storage.

## Finding Description

The Aptos consensus quorum store implements two batch storage systems: the legacy v1 system using `BatchInfo` and the newer v2 system using `BatchInfoExt` with extended metadata. [1](#0-0) 

The `BatchInfoExt` enum has a V2 variant that contains additional metadata in the form of `ExtraBatchInfo`, which includes the `BatchKind` field. [2](#0-1) 

When the batch generator creates batches with `enable_batch_v2` configuration enabled, it creates v2 batches with this extended metadata. [3](#0-2) 

These v2 batches are persisted to the database using `save_batch_v2()`, which stores them in the "batch_v2" column family via `BatchV2Schema`. [4](#0-3) 

The database layer implements separate deletion methods:
- `delete_batches()` deletes from the "batch" column family (v1 schema) [5](#0-4) 
- `delete_batches_v2()` deletes from the "batch_v2" column family (v2 schema) [6](#0-5) 

**Bug #1 - Runtime Expiration Cleanup:**

When batches expire during normal operation, `update_certified_timestamp()` calls `clear_expired_payload()` to remove expired batches from the in-memory cache, then attempts to delete them from the database. However, it incorrectly calls `delete_batches()` instead of `delete_batches_v2()`. [7](#0-6) 

This means v2 batches are removed from memory but remain in the "batch_v2" database column family, as the deletion targets the wrong column family.

**Bug #2 - Epoch Change Cleanup:**

When a new epoch begins, `gc_previous_epoch_batches_from_db_v2()` reads v2 batches from previous epochs using `get_all_batches_v2()`, but then calls `delete_batches()` instead of `delete_batches_v2()`. [8](#0-7) 

This function is invoked during epoch transitions. [9](#0-8) 

**Attack Scenario:**

1. Network operates with `enable_batch_v2 = true` configuration
2. Validators create v2 batches containing extended metadata (BatchKind, ExtraBatchInfo)
3. These batches are stored in the "batch_v2" column family
4. When batches expire or epochs change, the cleanup logic removes them from memory but fails to delete them from the database
5. Over time, orphaned v2 batch metadata accumulates in the "batch_v2" column family
6. This leads to unbounded database growth and potential disk space exhaustion

This breaks the **State Consistency** invariant: the in-memory cache and persistent database should be synchronized, but orphaned metadata creates a persistent discrepancy.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per Aptos bug bounty criteria for the following reasons:

1. **State Inconsistencies Requiring Intervention**: The bug creates a divergence between in-memory state (which correctly expires batches) and persistent state (which retains orphaned metadata). Per the bounty program, "State inconsistencies requiring intervention" fall under Medium severity.

2. **Storage Exhaustion**: Continuous accumulation of orphaned metadata can lead to disk space exhaustion on validator nodes, potentially causing:
   - Degraded node performance as database size grows
   - Failed database writes when disk space is exhausted
   - Node crashes or unavailability
   - Increased backup/recovery times

3. **Validator Node Impact**: While this doesn't immediately crash nodes, the gradual storage exhaustion could escalate to **High Severity** ("Validator node slowdowns") if left unaddressed over multiple epochs.

The impact is limited to storage bloat and state inconsistency rather than consensus safety violations or fund theft, placing it firmly in the Medium category with potential escalation to High if disk exhaustion impacts validator availability.

## Likelihood Explanation

**HIGH Likelihood** - This vulnerability will manifest deterministically under normal operation:

1. **Configuration-Dependent**: The bug only affects deployments with `enable_batch_v2 = true`, but once enabled, it triggers on every batch expiration.

2. **Automatic Triggering**: No attacker action is required. Normal consensus operation automatically triggers the bug:
   - Batches expire continuously during operation [10](#0-9) 
   - Epoch transitions periodically clean up old batches

3. **Continuous Accumulation**: Every expired v2 batch leaves orphaned metadata, meaning the storage leak grows monotonically over time.

4. **No Mitigating Factors**: There is no automatic cleanup mechanism for these orphaned entries. The only cleanup that works correctly is `populate_cache_and_gc_expired_batches_v2()`, which only runs on non-epoch-change restarts. [11](#0-10) 

## Recommendation

**Fix #1 - Runtime Expiration Cleanup:**

Modify `update_certified_timestamp()` to distinguish between v1 and v2 batches before deletion:

```rust
pub fn update_certified_timestamp(&self, certified_time: u64) {
    trace!("QS: batch reader updating time {:?}", certified_time);
    self.last_certified_time
        .fetch_max(certified_time, Ordering::SeqCst);

    let expired_keys = self.clear_expired_payload(certified_time);
    
    // Separate expired keys by batch version
    let mut v1_keys = Vec::new();
    let mut v2_keys = Vec::new();
    
    for key in expired_keys {
        // Check which column family contains this key
        if let Ok(Some(_)) = self.db.get_batch(&key) {
            v1_keys.push(key);
        } else if let Ok(Some(_)) = self.db.get_batch_v2(&key) {
            v2_keys.push(key);
        }
    }
    
    if !v1_keys.is_empty() {
        if let Err(e) = self.db.delete_batches(v1_keys) {
            debug!("Error deleting v1 batches: {:?}", e)
        }
    }
    
    if !v2_keys.is_empty() {
        if let Err(e) = self.db.delete_batches_v2(v2_keys) {
            debug!("Error deleting v2 batches: {:?}", e)
        }
    }
}
```

**Fix #2 - Epoch Change Cleanup:**

Modify `gc_previous_epoch_batches_from_db_v2()` to use the correct deletion method: [12](#0-11) 

Change line 241 from:
```rust
db.delete_batches(expired_keys)
```

To:
```rust
db.delete_batches_v2(expired_keys)
```

**Alternative Simpler Fix:**

Track batch version in the `BatchStore` cache and use that information during cleanup, or maintain the batch version during the `clear_expired_payload()` process to avoid additional database lookups.

## Proof of Concept

```rust
#[cfg(test)]
mod test_v2_batch_orphaning {
    use super::*;
    use aptos_consensus_types::proof_of_store::{BatchInfoExt, BatchKind};
    use aptos_crypto::HashValue;
    use aptos_types::quorum_store::BatchId;
    use std::sync::Arc;
    
    #[test]
    fn test_v2_batch_metadata_orphaning() {
        // Setup test database
        let db_dir = tempfile::tempdir().unwrap();
        let db = Arc::new(QuorumStoreDB::new(db_dir.path()));
        
        // Create a v2 batch with extended metadata
        let batch_info = BatchInfoExt::new_v2(
            PeerId::random(),
            BatchId::new_for_test(1),
            1, // epoch
            1000000, // expiration (1 second in microseconds)
            HashValue::random(),
            10, // num_txns
            1000, // num_bytes
            0, // gas_bucket_start
            BatchKind::Normal, // Extended metadata
        );
        
        let persisted_value = PersistedValue::new(batch_info.clone(), None);
        let digest = *persisted_value.digest();
        
        // Save v2 batch to database
        db.save_batch_v2(persisted_value.clone()).unwrap();
        
        // Verify it's stored in batch_v2 column family
        assert!(db.get_batch_v2(&digest).unwrap().is_some());
        assert!(db.get_batch(&digest).unwrap().is_none()); // Not in v1 CF
        
        // Simulate expiration cleanup using wrong deletion method
        // (mimicking the bug in update_certified_timestamp)
        db.delete_batches(vec![digest]).unwrap();
        
        // BUG: v2 batch still exists in database despite deletion attempt
        assert!(db.get_batch_v2(&digest).unwrap().is_some(), 
                "VULNERABILITY: V2 batch metadata orphaned - still in DB after delete_batches()");
        
        // Correct cleanup should use delete_batches_v2
        db.delete_batches_v2(vec![digest]).unwrap();
        
        // Now it's properly cleaned up
        assert!(db.get_batch_v2(&digest).unwrap().is_none(),
                "V2 batch properly deleted with correct method");
    }
    
    #[test]
    fn test_epoch_change_orphaning() {
        let db_dir = tempfile::tempdir().unwrap();
        let db = Arc::new(QuorumStoreDB::new(db_dir.path()));
        
        // Create multiple v2 batches in epoch 1
        let mut digests = Vec::new();
        for i in 0..100 {
            let batch_info = BatchInfoExt::new_v2(
                PeerId::random(),
                BatchId::new_for_test(i),
                1, // old epoch
                1000000,
                HashValue::random(),
                10,
                1000,
                0,
                BatchKind::Normal,
            );
            let persisted_value = PersistedValue::new(batch_info, None);
            digests.push(*persisted_value.digest());
            db.save_batch_v2(persisted_value).unwrap();
        }
        
        // Simulate epoch change cleanup with wrong method
        // (mimicking gc_previous_epoch_batches_from_db_v2 bug)
        db.delete_batches(digests.clone()).unwrap();
        
        // BUG: All 100 v2 batches still exist
        let remaining = db.get_all_batches_v2().unwrap();
        assert_eq!(remaining.len(), 100,
                   "VULNERABILITY: All v2 batches orphaned after epoch change cleanup");
        
        // Correct cleanup
        db.delete_batches_v2(digests).unwrap();
        assert_eq!(db.get_all_batches_v2().unwrap().len(), 0);
    }
}
```

**Notes**

This vulnerability demonstrates a classic storage leak caused by incorrect API usage across database schema boundaries. The system maintains parallel v1 and v2 storage systems, but cleanup logic fails to respect this separation, causing metadata to accumulate indefinitely in the v2 column family. The bug is particularly insidious because it only affects deployments using the newer v2 batch format, and the orphaned data doesn't immediately impact consensus correctnessâ€”it silently degrades storage and performance over time.

### Citations

**File:** consensus/consensus-types/src/proof_of_store.rs (L195-203)
```rust
pub enum BatchInfoExt {
    V1 {
        info: BatchInfo,
    },
    V2 {
        info: BatchInfo,
        extra: ExtraBatchInfo,
    },
}
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L335-348)
```rust
#[derive(
    Clone, Debug, Deserialize, Serialize, CryptoHasher, BCSCryptoHash, PartialEq, Eq, Hash,
)]
pub struct ExtraBatchInfo {
    pub batch_kind: BatchKind,
}

#[derive(
    Clone, Debug, Deserialize, Serialize, CryptoHasher, BCSCryptoHash, PartialEq, Eq, Hash,
)]
pub enum BatchKind {
    Normal,
    Encrypted,
}
```

**File:** consensus/src/quorum_store/batch_generator.rs (L190-201)
```rust
        if self.config.enable_batch_v2 {
            // TODO(ibalajiarun): Specify accurate batch kind
            let batch_kind = BatchKind::Normal;
            Batch::new_v2(
                batch_id,
                txns,
                self.epoch,
                expiry_time,
                self.my_peer_id,
                bucket_start,
                batch_kind,
            )
```

**File:** consensus/src/quorum_store/batch_store.rs (L156-160)
```rust
        if is_new_epoch {
            tokio::task::spawn_blocking(move || {
                Self::gc_previous_epoch_batches_from_db_v1(db_clone.clone(), epoch);
                Self::gc_previous_epoch_batches_from_db_v2(db_clone, epoch);
            });
```

**File:** consensus/src/quorum_store/batch_store.rs (L161-175)
```rust
        } else {
            Self::populate_cache_and_gc_expired_batches_v1(
                db_clone.clone(),
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
            Self::populate_cache_and_gc_expired_batches_v2(
                db_clone,
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
```

**File:** consensus/src/quorum_store/batch_store.rs (L212-243)
```rust
    fn gc_previous_epoch_batches_from_db_v2(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L501-513)
```rust
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L93-101)
```rust
    fn delete_batches(&self, digests: Vec<HashValue>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        for digest in digests.iter() {
            trace!("QS: db delete digest {}", digest);
            batch.delete::<BatchSchema>(digest)?;
        }
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L123-131)
```rust
    fn delete_batches_v2(&self, digests: Vec<HashValue>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        for digest in digests.iter() {
            trace!("QS: db delete digest {}", digest);
            batch.delete::<BatchV2Schema>(digest)?;
        }
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L168-170)
```rust
    fn notify_commit(&self, block_timestamp: u64, payloads: Vec<Payload>) {
        self.batch_reader
            .update_certified_timestamp(block_timestamp);
```
