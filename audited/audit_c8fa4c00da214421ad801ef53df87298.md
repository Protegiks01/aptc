# Audit Report

## Title
Non-Deterministic DKG Transcript Verification Violates Consensus Determinism Invariant

## Summary
The DKG transcript verification uses `thread_rng()` to generate random challenge scalars during block execution, creating non-deterministic behavior across validators. This violates the fundamental consensus requirement that all validators must execute identical blocks deterministically and produce identical state roots.

## Finding Description

The production DKG implementation's transcript verification logic uses non-deterministic randomness during consensus block execution. When validators execute blocks containing `DKGResult` transactions, each validator independently generates different random challenges for cryptographic verification.

**Execution Flow:**

1. ValidatorTransactions (including DKGResult) are executed during block processing between BlockMetadata and UserTransactions [1](#0-0) 

2. The AptosVM dispatches DKGResult transactions to the DKG processing module [2](#0-1) 

3. The VM validates the transcript cryptographically before publishing on-chain [3](#0-2) 

4. This calls `RealDKG::verify_transcript()` which verifies the weighted transcript [4](#0-3) 

5. The weighted transcript verification uses **non-deterministic `thread_rng()`** to generate random challenge scalars: [5](#0-4) 

6. These random scalars are used in batch signature verification, low-degree tests, and multi-pairing checks: [6](#0-5) 

**Broken Invariant:** This violates the "Deterministic Execution" invariant—all validators must produce identical state roots for identical blocks. The comment acknowledges the risk but dismisses it, stating "Creates bad RNG risks but we deem that acceptable"—this assessment fails to consider consensus implications.

**Theoretical Consensus Divergence Scenario:**
While batched verification with random linear combinations is designed to have negligible soundness error (typically <2^-128), the use of different random challenges per validator creates a theoretical possibility that:
- Validator A generates challenges {α₁, α₂, ..., αₙ} and verification passes
- Validator B generates challenges {α'₁, α'₂, ..., α'ₙ} and verification fails (or vice versa)
- Validators compute different state roots
- No 2f+1 quorum can form, causing consensus deadlock

A proper deterministic Fiat-Shamir implementation exists in the codebase and should be used instead: [7](#0-6) 

## Impact Explanation

**Severity: HIGH to CRITICAL** (depending on cryptographic implementation quality)

This vulnerability violates fundamental consensus safety properties:

1. **Consensus Invariant Violation**: All blockchain consensus systems require deterministic execution. Using `thread_rng()` during block execution categorically violates this invariant, regardless of the practical probability of divergence.

2. **Potential Network Partition**: If verification outcomes diverge (however unlikely), the network cannot reach 2f+1 consensus on subsequent blocks, requiring manual intervention or hard fork to resolve.

3. **No Automatic Recovery**: There are no checks, fallbacks, or recovery mechanisms to detect or prevent execution non-determinism at the consensus layer.

4. **Production Code Path**: This affects the real DKG implementation used during validator set changes, not test-only code paths.

While the practical probability of divergence is very low with correct cryptographic implementations (soundness error typically <2^-128), the principle of deterministic execution in consensus systems is absolute. Over the lifetime of a blockchain with potentially thousands of DKG sessions, even negligible probabilities become concerning.

## Likelihood Explanation

**Likelihood: LOW to MEDIUM** (practical probability of divergence)

The non-determinism is guaranteed to occur (each validator generates different random values), but the likelihood of this causing actual consensus divergence depends on:

- **Cryptographic Implementation Quality**: With correct pairing implementations, different random challenges should produce identical accept/reject decisions with overwhelming probability
- **DKG Session Frequency**: More DKG sessions increase the cumulative probability of encountering edge cases
- **No Evidence of Occurrence**: No documented instances of this causing actual consensus failures

However, the **likelihood of violating consensus principles is 100%**—the code definitively uses non-deterministic randomness during consensus execution, which is architecturally unacceptable regardless of practical exploitability.

## Recommendation

Replace `thread_rng()` with deterministic Fiat-Shamir challenge generation derived from the transcript itself. The codebase already includes the necessary infrastructure via the `merlin::Transcript` type.

**Fix approach:**
1. Initialize a `merlin::Transcript` with the domain separator
2. Append all transcript elements deterministically
3. Derive challenge scalars using `challenge_scalars()` methods
4. Use these deterministic challenges for batch verification

This ensures all validators derive identical challenges from identical transcript data.

## Proof of Concept

The non-determinism can be demonstrated by running the verification twice and observing different random values:

```rust
// File: crates/aptos-dkg/src/pvss/das/weighted_protocol.rs:295-297
// Each call generates different random values:
let mut rng = rand::thread_rng();
let extra = random_scalars(2 + W * 3, &mut rng);
```

Since `thread_rng()` is unseeded and uses OS entropy, each validator will generate different values during the same block execution, violating deterministic execution requirements.

**Notes:**

While the theoretical vulnerability is valid, the practical likelihood of consensus divergence is very low due to the high soundness of batched cryptographic verification protocols. However, any violation of deterministic execution in consensus-critical code represents an unacceptable architectural flaw that should be fixed regardless of practical exploitability. The severity assessment depends on whether we prioritize principle violations (CRITICAL) or practical exploitability (HIGH).

### Citations

**File:** consensus/consensus-types/src/block.rs (L1-150)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::{
    block_data::{BlockData, BlockType},
    common::{Author, Payload, Round},
    opt_block_data::OptBlockData,
    payload::{OptQuorumStorePayload, TDataInfo},
    quorum_cert::QuorumCert,
};
use anyhow::{bail, ensure, format_err, Result};
use aptos_bitvec::BitVec;
use aptos_crypto::{bls12381, hash::CryptoHash, HashValue};
use aptos_infallible::duration_since_epoch;
use aptos_types::{
    account_address::AccountAddress,
    block_info::BlockInfo,
    block_metadata::BlockMetadata,
    block_metadata_ext::BlockMetadataExt,
    epoch_state::EpochState,
    ledger_info::LedgerInfo,
    randomness::Randomness,
    transaction::{SignedTransaction, Transaction, Version},
    validator_signer::ValidatorSigner,
    validator_txn::ValidatorTransaction,
    validator_verifier::ValidatorVerifier,
};
use mirai_annotations::debug_checked_verify_eq;
use serde::{Deserialize, Deserializer, Serialize};
use std::{
    convert::TryFrom,
    fmt::{self, Display, Formatter},
    iter::once,
};

#[path = "block_test_utils.rs"]
#[cfg(any(test, feature = "fuzzing"))]
pub mod block_test_utils;

#[cfg(test)]
#[path = "block_test.rs"]
pub mod block_test;

#[derive(Serialize, Clone, PartialEq, Eq)]
/// Block has the core data of a consensus block that should be persistent when necessary.
/// Each block must know the id of its parent and keep the QuorurmCertificate to that parent.
pub struct Block {
    /// This block's id as a hash value, it is generated at call time
    #[serde(skip)]
    id: HashValue,
    /// The container for the actual block
    block_data: BlockData,
    /// Signature that the hash of this block has been authored by the owner of the private key,
    /// this is only set within Proposal blocks
    signature: Option<bls12381::Signature>,
}

impl fmt::Debug for Block {
    fn fmt(&self, f: &mut Formatter) -> fmt::Result {
        write!(f, "{}", self)
    }
}

impl Display for Block {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        let author = self
            .author()
            .map(|addr| format!("{}", addr))
            .unwrap_or_else(|| "(NIL)".to_string());
        write!(
            f,
            "[id: {}, author: {}, epoch: {}, round: {:02}, parent_id: {}, timestamp: {}]",
            self.id,
            author,
            self.epoch(),
            self.round(),
            self.parent_id(),
            self.timestamp_usecs(),
        )
    }
}

impl Block {
    pub fn author(&self) -> Option<Author> {
        self.block_data.author()
    }

    pub fn epoch(&self) -> u64 {
        self.block_data.epoch()
    }

    pub fn id(&self) -> HashValue {
        self.id
    }

    // Is this block a parent of the parameter block?
    #[cfg(test)]
    pub fn is_parent_of(&self, block: &Self) -> bool {
        block.parent_id() == self.id
    }

    pub fn parent_id(&self) -> HashValue {
        self.block_data.parent_id()
    }

    pub fn payload(&self) -> Option<&Payload> {
        self.block_data.payload()
    }

    pub fn payload_size(&self) -> usize {
        match self.block_data.payload() {
            None => 0,
            Some(payload) => match payload {
                Payload::InQuorumStore(pos) => pos.proofs.len(),
                Payload::DirectMempool(_txns) => 0,
                Payload::InQuorumStoreWithLimit(pos) => pos.proof_with_data.proofs.len(),
                Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _)
                | Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _) => {
                    inline_batches.len() + proof_with_data.proofs.len()
                },
                Payload::OptQuorumStore(opt_quorum_store_payload) => {
                    opt_quorum_store_payload.num_txns()
                },
            },
        }
    }

    /// Returns the number of proofs, the number of txns in the proofs, and the bytes of txns in the proofs
    pub fn proof_stats(&self) -> (usize, usize, usize) {
        match self.block_data.payload() {
            None => (0, 0, 0),
            Some(payload) => match payload {
                Payload::InQuorumStore(pos) => (pos.num_proofs(), pos.num_txns(), pos.num_bytes()),
                Payload::DirectMempool(_txns) => (0, 0, 0),
                Payload::InQuorumStoreWithLimit(pos) => (
                    pos.proof_with_data.num_proofs(),
                    pos.proof_with_data.num_txns(),
                    pos.proof_with_data.num_bytes(),
                ),
                Payload::QuorumStoreInlineHybrid(_inline_batches, proof_with_data, _)
                | Payload::QuorumStoreInlineHybridV2(_inline_batches, proof_with_data, _) => (
                    proof_with_data.num_proofs(),
                    proof_with_data.num_txns(),
                    proof_with_data.num_bytes(),
                ),
                Payload::OptQuorumStore(opt_quorum_store_payload) => match opt_quorum_store_payload
                {
                    OptQuorumStorePayload::V1(p) => (
                        p.proof_with_data().num_proofs(),
                        p.proof_with_data().num_txns(),
```

**File:** aptos-move/aptos-vm/src/validator_txns/mod.rs (L15-37)
```rust
impl AptosVM {
    pub(crate) fn process_validator_transaction(
        &self,
        resolver: &impl AptosMoveResolver,
        module_storage: &impl AptosModuleStorage,
        txn: ValidatorTransaction,
        log_context: &AdapterLogSchema,
    ) -> Result<(VMStatus, VMOutput), VMStatus> {
        let session_id = SessionId::validator_txn(&txn);
        match txn {
            ValidatorTransaction::DKGResult(dkg_node) => {
                self.process_dkg_result(resolver, module_storage, log_context, session_id, dkg_node)
            },
            ValidatorTransaction::ObservedJWKUpdate(jwk_update) => self.process_jwk_update(
                resolver,
                module_storage,
                log_context,
                session_id,
                jwk_update,
            ),
        }
    }
}
```

**File:** aptos-move/aptos-vm/src/validator_txns/dkg.rs (L52-149)
```rust
    pub(crate) fn process_dkg_result(
        &self,
        resolver: &impl AptosMoveResolver,
        module_storage: &impl AptosModuleStorage,
        log_context: &AdapterLogSchema,
        session_id: SessionId,
        dkg_transcript: DKGTranscript,
    ) -> Result<(VMStatus, VMOutput), VMStatus> {
        match self.process_dkg_result_inner(
            resolver,
            module_storage,
            log_context,
            session_id,
            dkg_transcript,
        ) {
            Ok((vm_status, vm_output)) => Ok((vm_status, vm_output)),
            Err(Expected(failure)) => {
                // Pretend we are inside Move, and expected failures are like Move aborts.
                Ok((
                    VMStatus::MoveAbort {
                        location: AbortLocation::Script,
                        code: failure as u64,
                        message: None,
                    },
                    VMOutput::empty_with_status(TransactionStatus::Discard(StatusCode::ABORTED)),
                ))
            },
            Err(Unexpected(vm_status)) => Err(vm_status),
        }
    }

    fn process_dkg_result_inner(
        &self,
        resolver: &impl AptosMoveResolver,
        module_storage: &impl AptosModuleStorage,
        log_context: &AdapterLogSchema,
        session_id: SessionId,
        dkg_node: DKGTranscript,
    ) -> Result<(VMStatus, VMOutput), ExecutionFailure> {
        let dkg_state =
            OnChainConfig::fetch_config(resolver).ok_or(Expected(MissingResourceDKGState))?;
        let config_resource = ConfigurationResource::fetch_config(resolver)
            .ok_or(Expected(MissingResourceConfiguration))?;
        let DKGState { in_progress, .. } = dkg_state;
        let in_progress_session_state =
            in_progress.ok_or(Expected(MissingResourceInprogressDKGSession))?;

        // Check epoch number.
        if dkg_node.metadata.epoch != config_resource.epoch() {
            return Err(Expected(EpochNotCurrent));
        }

        // Deserialize transcript and verify it.
        let pub_params = DefaultDKG::new_public_params(&in_progress_session_state.metadata);
        let transcript = bcs::from_bytes::<<DefaultDKG as DKGTrait>::Transcript>(
            dkg_node.transcript_bytes.as_slice(),
        )
        .map_err(|_| Expected(TranscriptDeserializationFailed))?;

        DefaultDKG::verify_transcript(&pub_params, &transcript)
            .map_err(|_| Expected(TranscriptVerificationFailed))?;

        // All check passed, invoke VM to publish DKG result on chain.
        let mut gas_meter = UnmeteredGasMeter;
        let mut session = self.new_session(resolver, session_id, None);
        let args = vec![
            MoveValue::Signer(AccountAddress::ONE),
            dkg_node.transcript_bytes.as_move_value(),
        ];

        let traversal_storage = TraversalStorage::new();
        session
            .execute_function_bypass_visibility(
                &RECONFIGURATION_WITH_DKG_MODULE,
                FINISH_WITH_DKG_RESULT,
                vec![],
                serialize_values(&args),
                &mut gas_meter,
                &mut TraversalContext::new(&traversal_storage),
                module_storage,
            )
            .map_err(|e| {
                expect_only_successful_execution(e, FINISH_WITH_DKG_RESULT.as_str(), log_context)
            })
            .map_err(|r| Unexpected(r.unwrap_err()))?;

        let output = get_system_transaction_output(
            session,
            module_storage,
            &self
                .storage_gas_params(log_context)
                .map_err(Unexpected)?
                .change_set_configs,
        )
        .map_err(Unexpected)?;

        Ok((VMStatus::Executed, output))
    }
```

**File:** types/src/dkg/real_dkg/mod.rs (L331-401)
```rust
    /// NOTE: this is used in VM.
    fn verify_transcript(
        params: &Self::PublicParams,
        trx: &Self::Transcript,
    ) -> anyhow::Result<()> {
        // Verify dealer indices are valid.
        let dealers = trx
            .main
            .get_dealers()
            .iter()
            .map(|player| player.id)
            .collect::<Vec<usize>>();
        let num_validators = params.session_metadata.dealer_validator_set.len();
        ensure!(
            dealers.iter().all(|id| *id < num_validators),
            "real_dkg::verify_transcript failed with invalid dealer index."
        );

        let all_eks = params.pvss_config.eks.clone();

        let addresses = params.verifier.get_ordered_account_addresses();
        let dealers_addresses = dealers
            .iter()
            .filter_map(|&pos| addresses.get(pos))
            .cloned()
            .collect::<Vec<_>>();

        let spks = dealers_addresses
            .iter()
            .filter_map(|author| params.verifier.get_public_key(author))
            .collect::<Vec<_>>();

        let aux = dealers_addresses
            .iter()
            .map(|address| (params.pvss_config.epoch, address))
            .collect::<Vec<_>>();

        trx.main.verify(
            &params.pvss_config.wconfig,
            &params.pvss_config.pp,
            &spks,
            &all_eks,
            &aux,
        )?;

        // Verify fast path is present if and only if fast_wconfig is present.
        ensure!(
            trx.fast.is_some() == params.pvss_config.fast_wconfig.is_some(),
            "real_dkg::verify_transcript failed with mismatched fast path flag in trx and params."
        );

        if let Some(fast_trx) = trx.fast.as_ref() {
            let fast_dealers = fast_trx
                .get_dealers()
                .iter()
                .map(|player| player.id)
                .collect::<Vec<usize>>();
            ensure!(
                dealers == fast_dealers,
                "real_dkg::verify_transcript failed with inconsistent dealer index."
            );
        }

        if let (Some(fast_trx), Some(fast_wconfig)) =
            (trx.fast.as_ref(), params.pvss_config.fast_wconfig.as_ref())
        {
            fast_trx.verify(fast_wconfig, &params.pvss_config.pp, &spks, &all_eks, &aux)?;
        }

        Ok(())
    }
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L295-297)
```rust
        // Deriving challenges by flipping coins: less complex to implement & less likely to get wrong. Creates bad RNG risks but we deem that acceptable.
        let mut rng = rand::thread_rng();
        let extra = random_scalars(2 + W * 3, &mut rng);
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L302-374)
```rust
        batch_verify_soks::<G1Projective, A>(
            self.soks.as_slice(),
            g_1,
            &self.V[W],
            spks,
            auxs,
            sok_vrfy_challenge,
        )?;

        let ldt = LowDegreeTest::random(
            &mut rng,
            sc.get_threshold_weight(),
            W + 1,
            true,
            sc.get_batch_evaluation_domain(),
        );
        ldt.low_degree_test_on_g1(&self.V)?;

        //
        // Correctness of encryptions check
        //

        let alphas_betas_and_gammas = &extra[0..W * 3 + 1];
        let (alphas_and_betas, gammas) = alphas_betas_and_gammas.split_at(2 * W + 1);
        let (alphas, betas) = alphas_and_betas.split_at(W + 1);
        assert_eq!(alphas.len(), W + 1);
        assert_eq!(betas.len(), W);
        assert_eq!(gammas.len(), W);

        let lc_VR_hat = G2Projective::multi_exp_iter(
            self.V_hat.iter().chain(self.R_hat.iter()),
            alphas_and_betas.iter(),
        );
        let lc_VRC = G1Projective::multi_exp_iter(
            self.V.iter().chain(self.R.iter()).chain(self.C.iter()),
            alphas_betas_and_gammas.iter(),
        );
        let lc_V_hat = G2Projective::multi_exp_iter(self.V_hat.iter().take(W), gammas.iter());
        let mut lc_R_hat = Vec::with_capacity(n);

        for i in 0..n {
            let p = sc.get_player(i);
            let weight = sc.get_player_weight(&p);
            let s_i = sc.get_player_starting_index(&p);

            lc_R_hat.push(g2_multi_exp(
                &self.R_hat[s_i..s_i + weight],
                &gammas[s_i..s_i + weight],
            ));
        }

        let h = pp.get_encryption_public_params().message_base();
        let g_2_neg = g_2.neg();
        let eks = eks
            .iter()
            .map(Into::<G1Projective>::into)
            .collect::<Vec<G1Projective>>();
        // The vector of left-hand-side ($\mathbb{G}_2$) inputs to each pairing in the multi-pairing.
        let lhs = [g_1, &lc_VRC, h].into_iter().chain(&eks);
        // The vector of right-hand-side ($\mathbb{G}_2$) inputs to each pairing in the multi-pairing.
        let rhs = [&lc_VR_hat, &g_2_neg, &lc_V_hat]
            .into_iter()
            .chain(&lc_R_hat);

        let res = multi_pairing(lhs, rhs);
        if res != Gt::identity() {
            bail!(
                "Expected zero during multi-pairing check for {} {}, but got {}",
                sc,
                <Self as traits::Transcript>::scheme_name(),
                res
            );
        }
```

**File:** crates/aptos-dkg/src/fiat_shamir.rs (L1-57)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

//! For what it's worth, I don't understand why the `merlin` library wants the user to first define
//! a trait with their 'append' operations and then implement that trait on `Transcript`.
//! I also don't understand how that doesn't break the orphan rule in Rust.
//! I suspect the reason they want the developer to do things these ways is to force them to cleanly
//! define all the things that are appended to the transcript.

use crate::{
    range_proofs::traits::BatchedRangeProof, sigma_protocol, sigma_protocol::homomorphism,
};
use ark_ec::{pairing::Pairing, CurveGroup};
use ark_ff::PrimeField;
use ark_serialize::CanonicalSerialize;
use merlin::Transcript;
use serde::Serialize;

/// Helper trait for deriving random scalars from a transcript.
///
/// Not every Fiat–Shamir call needs higher-level operations
/// (like appending PVSS information), but most do require scalar
/// derivation. This basic trait provides that functionality.
///
/// ⚠️ This trait is intentionally private: functions like `challenge_scalars`
/// should **only** be used internally to ensure properly
/// labelled scalar generation across Fiat-Shamir protocols.
trait ScalarProtocol<F: PrimeField> {
    fn challenge_full_scalars(&mut self, label: &[u8], num_scalars: usize) -> Vec<F>;

    fn challenge_full_scalar(&mut self, label: &[u8]) -> F {
        self.challenge_full_scalars(label, 1)[0]
    }

    fn challenge_128bit_scalars(&mut self, label: &[u8], num_scalars: usize) -> Vec<F>;
}

impl<F: PrimeField> ScalarProtocol<F> for Transcript {
    fn challenge_full_scalars(&mut self, label: &[u8], num_scalars: usize) -> Vec<F> {
        let byte_size = (F::MODULUS_BIT_SIZE as usize) / 8;
        let mut buf = vec![0u8; 2 * num_scalars * byte_size];
        self.challenge_bytes(label, &mut buf);

        buf.chunks(2 * byte_size)
            .map(|chunk| F::from_le_bytes_mod_order(chunk))
            .collect()
    }

    fn challenge_128bit_scalars(&mut self, label: &[u8], num_scalars: usize) -> Vec<F> {
        let mut buf = vec![0u8; num_scalars * 16];
        self.challenge_bytes(label, &mut buf);

        buf.chunks(16)
            .map(|chunk| F::from_le_bytes_mod_order(chunk.try_into().unwrap()))
            .collect()
    }
}
```
