# Audit Report

## Title
Silent Channel Failure in Persisting Phase Causes False Commit Success During Pipeline Abort

## Summary
The persisting phase silently ignores send errors when notifying blocks of commit proofs via oneshot channels. When `commit_proof_tx.send()` fails (receiver dropped due to pipeline abort), the error is discarded through `.map()`, causing the persisting phase to report success even though blocks were never actually committed to storage. [1](#0-0) 

## Finding Description

The vulnerability occurs in the persisting phase's block commit notification mechanism. When blocks are ready to be persisted:

1. The persisting phase attempts to send the `commit_ledger_info` to each block's pipeline via a oneshot channel
2. If the receiver has been dropped (e.g., during `abort_pipeline_for_state_sync` or buffer reset), `send()` returns an error
3. The error is silently ignored because `.map()` only operates on the `Option` layer, not the `Result` returned by `send()` [2](#0-1) 

The commit proof notification is critical because the `commit_ledger` phase waits on it to actually persist blocks to storage: [3](#0-2) 

When the send fails:
- Line 1089: `commit_proof_fut.await?` returns an error (receiver was cancelled/dropped)
- The `executor.commit_ledger()` call (lines 1099-1104) **never executes**
- Blocks are **not persisted to storage**

However, the persisting phase's `wait_for_commit_ledger()` ignores this error: [4](#0-3) 

This causes the persisting phase to return `Ok(round)` even though commit failed, misleading the buffer manager into thinking blocks were successfully persisted.

**Race Condition Scenario:**

During state synchronization: [5](#0-4) 

1. `abort_pipeline_for_state_sync()` aborts all block pipelines (line 509)
2. This drops the oneshot receivers for `commit_proof_tx`
3. Concurrently, persisting phase tries to send commit proofs
4. Send fails silently, but persisting returns `Ok(round)`
5. Buffer manager updates `highest_committed_round` incorrectly [6](#0-5) 

The abort mechanism that creates this race: [7](#0-6) 

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria: "State inconsistencies requiring intervention."

While state sync automatically resolves the inconsistency, there is a critical window where:
- Buffer manager believes blocks are committed (incorrect `highest_committed_round`)
- Storage does not contain these blocks
- The node's view of committed state diverges from reality

This temporarily breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs."

The impact is mitigated by:
- Automatic state sync recovery
- Reset mechanism that eventually corrects the state
- No permanent chain split or fund loss

However, the bug represents a protocol violation where commit success is incorrectly reported, which could complicate debugging and monitoring during state sync scenarios.

## Likelihood Explanation

**High likelihood** during normal operation:
- Occurs every time state sync is triggered via `abort_pipeline_for_state_sync`
- Occurs during epoch transitions when pipelines are aborted
- Occurs during block tree pruning operations

The race condition happens naturally during protocol operations, not requiring attacker manipulation. Any validator node experiencing state sync will hit this code path.

## Recommendation

Replace the silent `.map()` with explicit error handling:

```rust
if let Some(tx) = b.pipeline_tx().lock().as_mut() {
    if let Some(sender) = tx.commit_proof_tx.take() {
        if let Err(proof) = sender.send(commit_ledger_info.clone()) {
            warn!(
                "Failed to send commit proof for block {}: receiver dropped. \
                This may indicate pipeline was aborted during persisting.",
                b.id()
            );
            // Consider returning error instead of continuing
            return Err(ExecutorError::InternalError {
                error: format!(
                    "Commit proof notification failed for block {}", 
                    b.id()
                ),
            }.into());
        }
    }
}
```

Alternatively, check the result of `wait_for_commit_ledger()` and propagate errors properly instead of using `let _ =`.

## Proof of Concept

```rust
#[tokio::test]
async fn test_persisting_phase_silent_channel_failure() {
    use consensus::pipeline::persisting_phase::{PersistingPhase, PersistingRequest};
    use consensus_types::pipelined_block::PipelinedBlock;
    use tokio::sync::oneshot;
    
    // Create a block with pipeline channels
    let block = Arc::new(PipelinedBlock::new(/* ... */));
    
    // Setup pipeline with commit_proof channel
    let (commit_proof_tx, commit_proof_rx) = oneshot::channel();
    let pipeline_tx = PipelineInputTx {
        commit_proof_tx: Some(commit_proof_tx),
        // ... other fields
    };
    block.set_pipeline_tx(pipeline_tx);
    
    // Simulate pipeline abort: drop the receiver
    drop(commit_proof_rx);
    
    // Create persisting request
    let request = PersistingRequest {
        blocks: vec![block.clone()],
        commit_ledger_info: /* valid ledger info */,
    };
    
    let persisting_phase = PersistingPhase::new(/* network sender */);
    
    // Process should succeed despite channel failure
    let result = persisting_phase.process(request).await;
    
    // BUG: Result is Ok even though commit_proof send failed
    assert!(result.is_ok());
    
    // Verify block was NOT actually committed to storage
    // (commit_ledger phase never executed)
}
```

## Notes

While this issue has Medium severity impact due to automatic recovery via state sync, it represents a fundamental flaw in error handling that violates the principle of explicit failure propagation. The silent failure makes debugging difficult and could mask more serious issues during incident response.

### Citations

**File:** consensus/src/pipeline/persisting_phase.rs (L59-82)
```rust
    async fn process(&self, req: PersistingRequest) -> PersistingResponse {
        let PersistingRequest {
            blocks,
            commit_ledger_info,
        } = req;

        for b in &blocks {
            if let Some(tx) = b.pipeline_tx().lock().as_mut() {
                tx.commit_proof_tx
                    .take()
                    .map(|tx| tx.send(commit_ledger_info.clone()));
            }
            b.wait_for_commit_ledger().await;
        }

        let response = Ok(blocks.last().expect("Blocks can't be empty").round());
        if commit_ledger_info.ledger_info().ends_epoch() {
            self.commit_msg_tx
                .send_epoch_change(EpochChangeProof::new(vec![commit_ledger_info], false))
                .await;
        }
        response
    }
}
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1079-1106)
```rust
    async fn commit_ledger(
        pre_commit_fut: TaskFuture<PreCommitResult>,
        commit_proof_fut: TaskFuture<LedgerInfoWithSignatures>,
        parent_block_commit_fut: TaskFuture<CommitLedgerResult>,
        executor: Arc<dyn BlockExecutorTrait>,
        block: Arc<Block>,
    ) -> TaskResult<CommitLedgerResult> {
        let mut tracker = Tracker::start_waiting("commit_ledger", &block);
        parent_block_commit_fut.await?;
        pre_commit_fut.await?;
        let ledger_info_with_sigs = commit_proof_fut.await?;

        // it's committed as prefix
        if ledger_info_with_sigs.commit_info().id() != block.id() {
            return Ok(None);
        }

        tracker.start_working();
        let ledger_info_with_sigs_clone = ledger_info_with_sigs.clone();
        tokio::task::spawn_blocking(move || {
            executor
                .commit_ledger(ledger_info_with_sigs_clone)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(Some(ledger_info_with_sigs))
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L562-568)
```rust
    pub async fn wait_for_commit_ledger(&self) {
        // may be aborted (e.g. by reset)
        if let Some(fut) = self.pipeline_futs() {
            // this may be cancelled
            let _ = fut.commit_ledger_fut.await;
        }
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L504-514)
```rust
        // abort any pending executor tasks before entering state sync
        // with zaptos, things can run before hitting buffer manager
        if let Some(block_store) = maybe_block_store {
            monitor!(
                "abort_pipeline_for_state_sync",
                block_store.abort_pipeline_for_state_sync().await
            );
        }
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```

**File:** consensus/src/pipeline/buffer_manager.rs (L968-973)
```rust
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
                },
```

**File:** consensus/src/block_storage/block_store.rs (L617-627)
```rust
    pub async fn abort_pipeline_for_state_sync(&self) {
        let blocks = self.inner.read().get_all_blocks();
        // the blocks are not ordered by round here, so we need to abort all then wait
        let futs: Vec<_> = blocks
            .into_iter()
            .filter_map(|b| b.abort_pipeline())
            .collect();
        for f in futs {
            f.wait_until_finishes().await;
        }
    }
```
