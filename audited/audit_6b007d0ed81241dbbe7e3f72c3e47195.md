# Audit Report

## Title
Partial Reconfiguration Notification Failure Causes Validator Liveness Loss During Epoch Transitions

## Summary
The `notify_reconfiguration_subscribers()` function in the event notification service fails to implement fault-tolerant notification delivery. When any single subscriber's channel is closed (e.g., due to component crash), the notification loop aborts early using the `?` operator, preventing all remaining subscribers from receiving critical epoch change notifications. This causes validators to become permanently stuck waiting for notifications, resulting in validator liveness loss.

## Finding Description

The vulnerability exists in the reconfiguration notification delivery mechanism. The function iterates through all reconfiguration subscribers and calls `notify_subscriber_of_configs()` with the `?` operator, which causes immediate return on the first error, preventing subsequent subscribers from receiving notifications. [1](#0-0) 

**Critical Components Affected:**

Multiple critical validator components subscribe to reconfiguration notifications and block waiting for them:

1. **Consensus** - During epoch transitions, consensus syncs to the new epoch and then blocks indefinitely waiting for reconfiguration notifications. The critical blocking point occurs in `initiate_new_epoch()` where it awaits a notification that may never arrive if the notification loop exited early. [2](#0-1) 

The `await_reconfig_notification()` implementation blocks with an `.expect()` that panics if the sender is dropped: [3](#0-2) 

2. **DKG (Distributed Key Generation)** - blocks waiting for reconfig at startup with the same pattern: [4](#0-3) 

3. **JWK Consensus** - blocks waiting for reconfig at startup with the same pattern: [5](#0-4) 

**Root Cause - Channel Push Failure:**

When a subscriber component crashes or its listener is dropped, the underlying channel's receiver is marked as dropped. The push operation fails with "Channel is closed" error when `receiver_dropped` is true: [6](#0-5) 

**No Cleanup Mechanism:**

The service maintains a `reconfig_subscriptions` HashMap but provides no mechanism to detect or remove dead subscribers. The subscription creation shows no cleanup logic: [7](#0-6) 

**Attack Scenario:**

1. Validator node starts with all components subscribing to reconfigurations during initialization: [8](#0-7) 

2. During operation, one component (e.g., consensus observer) crashes or panics, dropping its `ReconfigNotificationListener`

3. An epoch transition occurs, triggering `notify_events()` which calls `notify_reconfiguration_subscribers()`: [9](#0-8) 

4. The notification loop encounters the dead subscriber and fails. Due to HashMap's non-deterministic iteration order, this could occur before or after notifying other critical components.

5. Critical components like consensus, DKG, or JWK consensus may not receive the notification.

6. These components remain blocked indefinitely at `await_reconfig_notification()`, unable to process the new epoch.

7. The validator cannot participate in consensus for the new epoch, causing validator downtime and network liveness degradation.

**Error Handling Does Not Prevent Impact:**

The error is logged but doesn't crash the node: [10](#0-9) 

However, the affected validator components remain permanently stuck, requiring manual node restart.

## Impact Explanation

**Severity: CRITICAL** (qualifies for up to $1,000,000 per Aptos bug bounty)

This vulnerability meets the **"Total loss of liveness/network availability"** criterion:

1. **Validator Becomes Non-Operational**: When consensus doesn't receive epoch notifications at line 567 of epoch_manager.rs, the validator cannot participate in the new epoch. The validator is effectively offline for block production and validation.

2. **No Automatic Recovery**: The stuck components require manual node restart. There is no self-healing mechanism in the codebase.

3. **Network-Wide Impact Potential**: If multiple validators experience this simultaneously (e.g., if a commonly-used component like consensus observer has a bug that causes crashes), multiple validators could become stuck. If >1/3 of staking power becomes stuck, the network loses liveness entirely.

4. **Breaks Consensus Liveness Invariant**: Validators must be able to transition between epochs to maintain consensus liveness. This bug directly prevents epoch transitions on affected validators.

5. **Critical Timing**: Epoch transitions are critical network events. Missing these notifications during validator set changes, configuration updates, or governance actions causes prolonged validator downtime.

## Likelihood Explanation

**Likelihood: HIGH**

1. **No Special Privileges Required**: This occurs from normal component crashes/panics, not from malicious action. Any component can crash due to bugs, resource exhaustion, or unexpected conditions.

2. **Multiple Vulnerable Components**: Six different components subscribe to reconfigurations (mempool, consensus observer, consensus, DKG, JWK consensus, on-chain discovery). Any one failing affects others.

3. **Non-Deterministic Failure**: Rust's HashMap iteration order is non-deterministic, so any failing subscriber could block any other subscriber depending on iteration order at runtime.

4. **Production Environment**: In production networks, components do crash occasionally due to bugs or unexpected conditions. This is not a theoretical scenario but a realistic operational condition.

5. **No Existing Safeguards**: The codebase has no dead subscriber cleanup, no timeout mechanisms in the await operations, and no retry logic for reconfiguration notifications.

## Recommendation

Implement fault-tolerant notification delivery by continuing to notify all subscribers even if some fail:

```rust
fn notify_reconfiguration_subscribers(&mut self, version: Version) -> Result<(), Error> {
    if self.reconfig_subscriptions.is_empty() {
        return Ok(()); // No reconfiguration subscribers!
    }

    let new_configs = self.read_on_chain_configs(version)?;
    let mut errors = Vec::new();
    
    // Continue notifying all subscribers even if some fail
    for (subscription_id, reconfig_subscription) in self.reconfig_subscriptions.iter_mut() {
        if let Err(e) = reconfig_subscription.notify_subscriber_of_configs(version, new_configs.clone()) {
            warn!("Failed to notify subscriber {}: {:?}", subscription_id, e);
            errors.push((*subscription_id, e));
        }
    }
    
    // Optionally clean up dead subscriptions
    for (subscription_id, _) in &errors {
        self.reconfig_subscriptions.remove(subscription_id);
    }
    
    Ok(())
}
```

Additionally, implement timeout mechanisms in the blocking await operations and add subscriber cleanup when channels are detected as closed.

## Proof of Concept

While a full Rust test would require complex setup, the vulnerability can be verified by:

1. Starting a validator node with all components subscribed
2. Simulating a component crash (e.g., killing consensus observer process)
3. Triggering an epoch transition
4. Observing that consensus/DKG/JWK consensus become stuck (depending on HashMap iteration order)
5. Verifying that the validator cannot participate in the new epoch

The code evidence provided above demonstrates the vulnerability exists in the current codebase without requiring additional exploitation code.

## Notes

This vulnerability represents a critical fault tolerance gap in the Aptos validator architecture. While component crashes are normal operational events that should be handled gracefully, the current implementation causes cascading failures where one component's crash can prevent critical components from receiving epoch notifications, leading to validator liveness loss. The non-deterministic nature of HashMap iteration makes this particularly insidious, as the impact varies based on runtime conditions.

### Citations

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L171-198)
```rust
    pub fn subscribe_to_reconfigurations(
        &mut self,
    ) -> Result<ReconfigNotificationListener<DbBackedOnChainConfig>, Error> {
        let (notification_sender, notification_receiver) =
            aptos_channel::new(QueueStyle::KLAST, RECONFIG_NOTIFICATION_CHANNEL_SIZE, None);

        // Create a new reconfiguration subscription
        let subscription_id = self.get_new_subscription_id();
        let reconfig_subscription = ReconfigSubscription {
            notification_sender,
        };

        // Store the new subscription
        if self
            .reconfig_subscriptions
            .insert(subscription_id, reconfig_subscription)
            .is_some()
        {
            return Err(Error::UnexpectedErrorEncountered(format!(
                "Duplicate reconfiguration subscription found! This should not occur! ID: {}",
                subscription_id,
            )));
        }

        Ok(ReconfigNotificationListener {
            notification_receiver,
        })
    }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L264-275)
```rust
    fn notify_reconfiguration_subscribers(&mut self, version: Version) -> Result<(), Error> {
        if self.reconfig_subscriptions.is_empty() {
            return Ok(()); // No reconfiguration subscribers!
        }

        let new_configs = self.read_on_chain_configs(version)?;
        for (_, reconfig_subscription) in self.reconfig_subscriptions.iter_mut() {
            reconfig_subscription.notify_subscriber_of_configs(version, new_configs.clone())?;
        }

        Ok(())
    }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L310-331)
```rust
impl EventNotificationSender for EventSubscriptionService {
    fn notify_events(&mut self, version: Version, events: Vec<ContractEvent>) -> Result<(), Error> {
        if events.is_empty() {
            return Ok(()); // No events!
        }

        // Notify event subscribers and check if a reconfiguration event was processed
        let reconfig_event_processed = self.notify_event_subscribers(version, events)?;

        // If a reconfiguration event was found, also notify the reconfig subscribers
        // of the new configuration values.
        if reconfig_event_processed {
            self.notify_reconfiguration_subscribers(version)
        } else {
            Ok(())
        }
    }

    fn notify_initial_configs(&mut self, version: Version) -> Result<(), Error> {
        self.notify_reconfiguration_subscribers(version)
    }
}
```

**File:** consensus/src/epoch_manager.rs (L544-569)
```rust
    async fn initiate_new_epoch(&mut self, proof: EpochChangeProof) -> anyhow::Result<()> {
        let ledger_info = proof
            .verify(self.epoch_state())
            .context("[EpochManager] Invalid EpochChangeProof")?;
        info!(
            LogSchema::new(LogEvent::NewEpoch).epoch(ledger_info.ledger_info().next_block_epoch()),
            "Received verified epoch change",
        );

        // shutdown existing processor first to avoid race condition with state sync.
        self.shutdown_current_processor().await;
        *self.pending_blocks.lock() = PendingBlocks::new();
        // make sure storage is on this ledger_info too, it should be no-op if it's already committed
        // panic if this doesn't succeed since the current processors are already shutdown.
        self.execution_client
            .sync_to_target(ledger_info.clone())
            .await
            .context(format!(
                "[EpochManager] State sync to new epoch {}",
                ledger_info
            ))
            .expect("Failed to sync to new epoch");

        monitor!("reconfig", self.await_reconfig_notification().await);
        Ok(())
    }
```

**File:** consensus/src/epoch_manager.rs (L1912-1920)
```rust
    async fn await_reconfig_notification(&mut self) {
        let reconfig_notification = self
            .reconfig_events
            .next()
            .await
            .expect("Reconfig sender dropped, unable to start new epoch");
        self.start_new_epoch(reconfig_notification.on_chain_configs)
            .await;
    }
```

**File:** dkg/src/epoch_manager.rs (L125-155)
```rust
    pub async fn start(mut self, mut network_receivers: NetworkReceivers) {
        self.await_reconfig_notification().await;
        loop {
            let handling_result = tokio::select! {
                notification = self.dkg_start_events.select_next_some() => {
                    self.on_dkg_start_notification(notification)
                },
                reconfig_notification = self.reconfig_events.select_next_some() => {
                    self.on_new_epoch(reconfig_notification).await
                },
                (peer, rpc_request) = network_receivers.rpc_rx.select_next_some() => {
                    self.process_rpc_request(peer, rpc_request)
                },
            };

            if let Err(e) = handling_result {
                error!("{}", e);
            }
        }
    }

    async fn await_reconfig_notification(&mut self) {
        let reconfig_notification = self
            .reconfig_events
            .next()
            .await
            .expect("Reconfig sender dropped, unable to start new epoch");
        self.start_new_epoch(reconfig_notification.on_chain_configs)
            .await
            .unwrap();
    }
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L122-152)
```rust
    pub async fn start(mut self, mut network_receivers: NetworkReceivers) {
        self.await_reconfig_notification().await;
        loop {
            let handle_result = tokio::select! {
                reconfig_notification = self.reconfig_events.select_next_some() => {
                    self.on_new_epoch(reconfig_notification).await
                },
                event = self.jwk_updated_events.select_next_some() => {
                    self.process_onchain_event(event)
                },
                (peer, rpc_request) = network_receivers.rpc_rx.select_next_some() => {
                    self.process_rpc_request(peer, rpc_request)
                }
            };

            if let Err(e) = handle_result {
                error!("{}", e);
            }
        }
    }

    async fn await_reconfig_notification(&mut self) {
        let reconfig_notification = self
            .reconfig_events
            .next()
            .await
            .expect("Reconfig sender dropped, unable to start new epoch");
        self.start_new_epoch(reconfig_notification.on_chain_configs)
            .await
            .unwrap();
    }
```

**File:** crates/channel/src/aptos_channel.rs (L85-112)
```rust
    pub fn push(&self, key: K, message: M) -> Result<()> {
        self.push_with_feedback(key, message, None)
    }

    /// Same as `push`, but this function also accepts a oneshot::Sender over which the sender can
    /// be notified when the message eventually gets delivered or dropped.
    pub fn push_with_feedback(
        &self,
        key: K,
        message: M,
        status_ch: Option<oneshot::Sender<ElementStatus<M>>>,
    ) -> Result<()> {
        let mut shared_state = self.shared_state.lock();
        ensure!(!shared_state.receiver_dropped, "Channel is closed");
        debug_assert!(shared_state.num_senders > 0);

        let dropped = shared_state.internal_queue.push(key, (message, status_ch));
        // If this or an existing message had to be dropped because of the queue being full, we
        // notify the corresponding status channel if it was registered.
        if let Some((dropped_val, Some(dropped_status_ch))) = dropped {
            // Ignore errors.
            let _err = dropped_status_ch.send(ElementStatus::Dropped(dropped_val));
        }
        if let Some(w) = shared_state.waker.take() {
            w.wake();
        }
        Ok(())
    }
```

**File:** aptos-node/src/state_sync.rs (L42-100)
```rust
pub fn create_event_subscription_service(
    node_config: &NodeConfig,
    db_rw: &DbReaderWriter,
) -> (
    EventSubscriptionService,
    ReconfigNotificationListener<DbBackedOnChainConfig>,
    Option<ReconfigNotificationListener<DbBackedOnChainConfig>>,
    Option<ReconfigNotificationListener<DbBackedOnChainConfig>>,
    Option<(
        ReconfigNotificationListener<DbBackedOnChainConfig>,
        EventNotificationListener,
    )>, // (reconfig_events, dkg_start_events) for DKG
    Option<(
        ReconfigNotificationListener<DbBackedOnChainConfig>,
        EventNotificationListener,
    )>, // (reconfig_events, jwk_updated_events) for JWK consensus
) {
    // Create the event subscription service
    let mut event_subscription_service =
        EventSubscriptionService::new(Arc::new(RwLock::new(db_rw.clone())));

    // Create a reconfiguration subscription for mempool
    let mempool_reconfig_subscription = event_subscription_service
        .subscribe_to_reconfigurations()
        .expect("Mempool must subscribe to reconfigurations");

    // Create a reconfiguration subscription for consensus observer (if enabled)
    let consensus_observer_reconfig_subscription =
        if node_config.consensus_observer.observer_enabled {
            Some(
                event_subscription_service
                    .subscribe_to_reconfigurations()
                    .expect("Consensus observer must subscribe to reconfigurations"),
            )
        } else {
            None
        };

    // Create a reconfiguration subscription for consensus
    let consensus_reconfig_subscription = if node_config.base.role.is_validator() {
        Some(
            event_subscription_service
                .subscribe_to_reconfigurations()
                .expect("Consensus must subscribe to reconfigurations"),
        )
    } else {
        None
    };

    // Create reconfiguration subscriptions for DKG
    let dkg_subscriptions = if node_config.base.role.is_validator() {
        let reconfig_events = event_subscription_service
            .subscribe_to_reconfigurations()
            .expect("DKG must subscribe to reconfigurations");
        let dkg_start_events = event_subscription_service
            .subscribe_to_events(vec![], vec!["0x1::dkg::DKGStartEvent".to_string()])
            .expect("Consensus must subscribe to DKG events");
        Some((reconfig_events, dkg_start_events))
    } else {
```

**File:** state-sync/state-sync-driver/src/utils.rs (L356-370)
```rust
    if let Err(error) = CommitNotification::handle_transaction_notification(
        committed_transactions.events,
        committed_transactions.transactions,
        latest_synced_version,
        latest_synced_ledger_info,
        mempool_notification_handler,
        event_subscription_service,
        storage_service_notification_handler,
    )
    .await
    {
        error!(LogSchema::new(LogEntry::SynchronizerNotification)
            .error(&error)
            .message("Failed to handle a transaction commit notification!"));
    }
```
