# Audit Report

## Title
Oversized Epoch Proofs Cause Permanent Epoch Synchronization Failure

## Summary
The legacy epoch sync implementation in `get_epoch_ending_ledger_infos_by_size_legacy()` returns oversized single-epoch proofs without size validation, causing permanent synchronization failures when epoch proofs exceed network message limits. This prevents nodes from ever synchronizing past the problematic epoch, creating a non-recoverable network partition.

## Finding Description

The vulnerability exists in the legacy epoch synchronization path, which is **enabled by default** (as `enable_size_and_time_aware_chunking` defaults to `false`). [1](#0-0) 

When the storage service attempts to fetch epoch ending ledger infos, it calls the legacy implementation which employs a binary search strategy to fit data within `max_network_chunk_bytes` (default 10 MiB): [2](#0-1) 

The critical flaw occurs at lines 318-320 where the function returns a single epoch proof **without size validation**: [3](#0-2) 

This bypasses the overflow check that occurs for multi-epoch batches. When the oversized proof is processed:

1. If compression is enabled, the `compress()` function validates against `MAX_APPLICATION_MESSAGE_SIZE` (~61.875 MiB): [4](#0-3) 

2. If the raw serialized epoch proof exceeds this limit OR doesn't compress well, compression fails with a `CompressionError`

3. The error propagates to the client, which retries via the error handling mechanism: [5](#0-4) 

4. Each retry increments `request_failure_count` and re-sends the same request: [6](#0-5) 

5. After exhausting retry attempts, synchronization permanently halts: [7](#0-6) 

**Attack Path:**
- An epoch's `EpochChangeProof` grows beyond limits (e.g., due to large validator set, extensive metadata in `EpochState`, or configuration changes)
- Node requests epoch sync starting from this epoch
- Server attempts to serve the request, eventually reducing to 1 epoch
- Returns oversized proof without validation (line 318-320)
- Compression/serialization fails every retry attempt
- Node cannot progress past this epoch â€” **permanent sync failure**

The error at lines 339-343 is actually unreachable (since line 318-320 returns early), but the real vulnerability is the missing size check that allows oversized responses. [8](#0-7) 

## Impact Explanation

**Severity: CRITICAL** (per Aptos Bug Bounty: "Non-recoverable network partition (requires hardfork)")

**Impact:**
- **Consensus Liveness**: New validators cannot join the network if they must sync past the problematic epoch
- **Network Partition**: Nodes that fall behind cannot catch up, creating permanent chain split risk
- **Validator Operations**: Validator rotation becomes impossible if new validators cannot sync
- **Recovery Cost**: Requires coordinated hardfork or manual intervention across all affected nodes
- **Cascading Failure**: As more nodes fall behind and attempt to resync, they all hit the same permanent failure

This meets the **Critical** severity threshold as it causes "Non-recoverable network partition (requires hardfork)" and "Total loss of liveness/network availability" for affected nodes.

## Likelihood Explanation

**Likelihood: MEDIUM-LOW** (but increasing over time)

**Factors Increasing Likelihood:**
- Default configuration uses vulnerable legacy implementation
- Natural network growth increases validator set sizes
- Mainnet comment suggests feature may be disabled in production: [9](#0-8) 
- Protocol upgrades may add metadata to epoch proofs
- Misconfigured lower `max_network_chunk_bytes` values
- Networks with thousands of validators (future scaling)

**Current Mitigations:**
- Typical validator sets (~200) unlikely to exceed 10 MiB per epoch
- BLS signatures are compact (96 bytes)
- Most epoch metadata is reasonably sized

**Future Risk:**
- As Aptos scales to more validators, epoch proof sizes grow
- Governance changes may add epoch metadata
- Network configuration errors could trigger the bug

## Recommendation

**Immediate Fix:** Enforce size validation even for single-epoch requests in the legacy implementation:

```rust
fn get_epoch_ending_ledger_infos_by_size_legacy(
    &self,
    start_epoch: u64,
    expected_end_epoch: u64,
    mut num_ledger_infos_to_fetch: u64,
    max_response_size: u64,
) -> Result<EpochChangeProof, Error> {
    while num_ledger_infos_to_fetch >= 1 {
        let end_epoch = start_epoch
            .checked_add(num_ledger_infos_to_fetch)
            .ok_or_else(|| {
                Error::UnexpectedErrorEncountered("End epoch has overflown!".into())
            })?;
        let epoch_change_proof = self
            .storage
            .get_epoch_ending_ledger_infos(start_epoch, end_epoch)?;
        
        // REMOVE THE EARLY RETURN WITHOUT SIZE CHECK
        // if num_ledger_infos_to_fetch == 1 {
        //     return Ok(epoch_change_proof);
        // }

        // Attempt to divide up the request if it overflows the message size
        let (overflow_frame, num_bytes) =
            check_overflow_network_frame(&epoch_change_proof, max_response_size)?;
        if !overflow_frame {
            return Ok(epoch_change_proof);
        } else if num_ledger_infos_to_fetch == 1 {
            // NEW: Return terminal error if single epoch proof is oversized
            return Err(Error::UnexpectedErrorEncountered(format!(
                "Single epoch proof too large to serve! Epoch: {:?}, Size: {:?} bytes, \
                Limit: {:?} bytes. This requires protocol-level intervention.",
                start_epoch, num_bytes, max_response_size
            )));
        } else {
            metrics::increment_chunk_truncation_counter(
                metrics::TRUNCATION_FOR_SIZE,
                DataResponse::EpochEndingLedgerInfos(epoch_change_proof).get_label(),
            );
            let new_num_ledger_infos_to_fetch = num_ledger_infos_to_fetch / 2;
            debug!("The request for {:?} ledger infos was too large...", num_ledger_infos_to_fetch);
            num_ledger_infos_to_fetch = new_num_ledger_infos_to_fetch;
        }
    }

    // This is now reachable only via logic error
    Err(Error::UnexpectedErrorEncountered(format!(
        "Unable to serve the get_epoch_ending_ledger_infos request! Start epoch: {:?}, \
        expected end epoch: {:?}. The data cannot fit into a single network frame!",
        start_epoch, expected_end_epoch
    )))
}
```

**Long-term Solutions:**
1. **Enable new implementation by default**: Set `enable_size_and_time_aware_chunking: true` in production configs
2. **Protocol-level epoch proof compression**: Implement more efficient epoch proof serialization
3. **Chunked epoch proofs**: Allow splitting single epoch proofs into multiple sub-proofs if they contain large validator sets
4. **Increase message limits**: Raise `MAX_APPLICATION_MESSAGE_SIZE` if network can handle it
5. **Monitoring**: Add alerts when epoch proof sizes approach limits

## Proof of Concept

```rust
#[cfg(test)]
mod test_oversized_epoch_proof {
    use super::*;
    use aptos_types::{
        aggregate_signature::AggregateSignature,
        block_info::BlockInfo,
        epoch_change::EpochChangeProof,
        epoch_state::EpochState,
        ledger_info::{LedgerInfo, LedgerInfoWithSignatures},
        validator_verifier::random_validator_verifier,
    };
    use aptos_crypto::hash::HashValue;
    use std::sync::Arc;

    #[test]
    fn test_oversized_single_epoch_proof_causes_permanent_sync_failure() {
        // Create a mock storage with an oversized epoch proof
        // Simulate an epoch with 10,000 validators (future scaling scenario)
        let num_validators = 10_000;
        let (signers, verifier) = random_validator_verifier(num_validators, None, true);
        let verifier = Arc::new(verifier);
        
        // Create epoch ending ledger info with large validator set
        let epoch_state = EpochState {
            epoch: 100,
            verifier: verifier.clone(),
        };
        
        let ledger_info = LedgerInfo::new(
            BlockInfo::new(
                100,
                0,
                HashValue::zero(),
                HashValue::zero(),
                10000,
                0,
                Some(epoch_state),
            ),
            HashValue::zero(),
        );
        
        // Create aggregated signature
        let partial_sigs = signers.iter()
            .map(|s| (s.author(), s.sign(&ledger_info).unwrap()))
            .collect();
        let partial_signatures = PartialSignatures::new(partial_sigs);
        let aggregated_sig = verifier
            .aggregate_signatures(partial_signatures.signatures_iter())
            .unwrap();
        
        let ledger_info_with_sigs = LedgerInfoWithSignatures::new(
            ledger_info,
            aggregated_sig,
        );
        
        // Create epoch proof
        let epoch_proof = EpochChangeProof::new(vec![ledger_info_with_sigs], false);
        
        // Serialize and check size
        let serialized = bcs::to_bytes(&epoch_proof).unwrap();
        println!("Epoch proof size: {} bytes", serialized.len());
        
        // With 10k validators, this will likely exceed limits
        // The legacy implementation would return this without checking,
        // causing compression to fail and permanent sync failure
        
        assert!(serialized.len() > 10 * 1024 * 1024, 
            "Epoch proof should exceed 10 MiB limit with large validator set");
    }
}
```

**Notes:**
- The PoC demonstrates that with sufficient validators, epoch proofs can exceed size limits
- In production, this would cause the exact failure sequence described
- The legacy code path (default configuration) has no defense against this scenario
- Recovery requires manual intervention or protocol upgrade

### Citations

**File:** config/src/config/state_sync_config.rs (L12-14)
```rust
// Whether to enable size and time-aware chunking (for non-production networks).
// Note: once this becomes stable, we should enable it for all networks (e.g., Mainnet).
const ENABLE_SIZE_AND_TIME_AWARE_CHUNKING: bool = true;
```

**File:** config/src/config/state_sync_config.rs (L198-198)
```rust
            enable_size_and_time_aware_chunking: false,
```

**File:** state-sync/storage-service/server/src/storage.rs (L222-229)
```rust
        // If size and time-aware chunking are disabled, use the legacy implementation
        if !use_size_and_time_aware_chunking {
            return self.get_epoch_ending_ledger_infos_by_size_legacy(
                start_epoch,
                expected_end_epoch,
                num_ledger_infos_to_fetch,
                max_response_size,
            );
```

**File:** state-sync/storage-service/server/src/storage.rs (L318-320)
```rust
            if num_ledger_infos_to_fetch == 1 {
                return Ok(epoch_change_proof); // We cannot return less than a single item
            }
```

**File:** state-sync/storage-service/server/src/storage.rs (L339-343)
```rust
        Err(Error::UnexpectedErrorEncountered(format!(
            "Unable to serve the get_epoch_ending_ledger_infos request! Start epoch: {:?}, \
            expected end epoch: {:?}. The data cannot fit into a single network frame!",
            start_epoch, expected_end_epoch
        )))
```

**File:** crates/aptos-compression/src/lib.rs (L52-60)
```rust
    // Ensure that the raw data size is not greater than the max bytes limit
    if raw_data.len() > max_bytes {
        let error_string = format!(
            "Raw data size greater than max bytes limit: {}, max: {}",
            raw_data.len(),
            max_bytes
        );
        return create_compression_error(&client, error_string);
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L446-453)
```rust
        if self.stream_engine.is_stream_complete()
            || self.request_failure_count >= self.streaming_service_config.max_request_retry
            || self.send_failure
        {
            if !self.send_failure && self.stream_end_notification_id.is_none() {
                self.send_end_of_stream_notification().await?;
            }
            return Ok(()); // There's nothing left to do
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L710-724)
```rust
    /// Handles an error returned by the data client in relation to a request
    fn handle_data_client_error(
        &mut self,
        data_client_request: &DataClientRequest,
        data_client_error: &aptos_data_client::error::Error,
    ) -> Result<(), Error> {
        // Log the error
        warn!(LogSchema::new(LogEntry::ReceivedDataResponse)
            .stream_id(self.data_stream_id)
            .event(LogEvent::Error)
            .error(&data_client_error.clone().into())
            .message("Encountered a data client error!"));

        // TODO(joshlind): can we identify the best way to react to the error?
        self.resend_data_client_request(data_client_request)
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L727-744)
```rust
    /// Resends a failed data client request and pushes the pending notification
    /// to the head of the pending notifications batch.
    fn resend_data_client_request(
        &mut self,
        data_client_request: &DataClientRequest,
    ) -> Result<(), Error> {
        // Increment the number of client failures for this request
        self.request_failure_count += 1;

        // Resend the client request
        let pending_client_response = self.send_client_request(true, data_client_request.clone());

        // Push the pending response to the head of the sent requests queue
        self.get_sent_data_requests()?
            .push_front(pending_client_response);

        Ok(())
    }
```
