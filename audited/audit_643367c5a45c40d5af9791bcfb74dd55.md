# Audit Report

## Title
DKG NetworkTask Channel Overflow Enables Liveness Attack via Message Drop During Critical Transcript Broadcasting Phase

## Summary
The DKG (Distributed Key Generation) NetworkTask uses a hardcoded internal RPC channel with capacity of only 10 messages. During DKG transcript broadcasting phases, this tiny channel can be systematically overflowed by an attacker flooding RPC requests, causing legitimate DKG transcript requests from honest validators to be dropped due to FIFO queue overflow behavior. This prevents DKG completion and blocks randomness generation indefinitely, constituting a high-severity liveness attack.

## Finding Description

The vulnerability exists in the DKG network message processing pipeline. When the DKG NetworkTask is initialized, it creates an internal RPC channel with a hardcoded capacity of 10 messages to queue incoming RPC requests before they are forwarded to the DKG manager: [1](#0-0) 

This channel uses FIFO (First-In-First-Out) queue style, which has a critical behavior: when the queue is full, **the newest message is dropped**, not the oldest: [2](#0-1) 

During DKG phases, validators broadcast `DKGTranscriptRequest` messages to all other validators using ReliableBroadcast to collect transcripts: [3](#0-2) 

The attack works as follows:

1. **Normal DKG Operation**: During DKG, each validator broadcasts transcript requests to all N-1 other validators. Each validator's NetworkTask receives these RPC requests and queues them in the internal channel (capacity 10) before forwarding to DKG manager.

2. **Attack Initiation**: An attacker (malicious validator or network peer with RPC access) floods a target validator's DKG RPC endpoint with crafted RPC requests.

3. **Channel Overflow**: With only 10 slots available, the NetworkTask channel fills up rapidly. When full, new incoming legitimate DKG transcript requests from honest validators are **dropped** (FIFO behavior drops newest).

4. **Message Loss**: The dropped transcript requests never reach the DKG manager, preventing the victim validator from participating in transcript aggregation.

5. **DKG Failure**: Since DKG requires 2f+1 validators to successfully aggregate transcripts (quorum), if the attacker targets enough validators or causes enough message drops, the network cannot reach quorum: [4](#0-3) 

6. **Persistent Attack**: Even though ReliableBroadcast implements retry logic with exponential backoff, if the attacker maintains continuous flooding, the channel remains full and retries also get dropped: [5](#0-4) 

7. **Randomness Generation Blocked**: Without DKG completion, the randomness generation system is blocked indefinitely, preventing any on-chain features that depend on randomness.

Note that while the DKG configuration specifies `max_network_channel_size` (default 256): [6](#0-5) 

This configuration is used for the application-level network service channel, but the NetworkTask's internal RPC processing channel is hardcoded to 10 and ignores this configuration.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program criteria:

**Primary Impact: Validator Node Slowdowns / Significant Protocol Violations**
- DKG is a critical consensus subsystem for randomness generation
- Preventing DKG completion blocks all randomness-dependent features
- Affects network liveness without requiring 51% attack or validator collusion

**Specific Impacts:**
1. **Liveness Attack**: Randomness generation blocked indefinitely during active DKG sessions
2. **Protocol Violation**: DKG protocol assumes reliable message delivery under Byzantine conditions (up to f failures), but this channel bottleneck violates that assumption
3. **Validator Disruption**: Targeted validators cannot participate in DKG, causing repeated DKG failures
4. **Resource Exhaustion**: Continuous retry attempts from ReliableBroadcast waste network bandwidth and computational resources

The impact is severe enough to require operator intervention and potentially network-wide coordination to mitigate, but does not directly cause fund loss or permanent state corruption (hence HIGH, not CRITICAL severity).

## Likelihood Explanation

**Likelihood: HIGH**

The attack is highly likely to succeed because:

1. **Low Barrier to Entry**: 
   - Attack requires only sending RPC requests to DKG protocol endpoints
   - No validator private keys or stake required
   - Can be executed by any peer with network access to validator nodes

2. **Tiny Channel Size**: 
   - Capacity of 10 is extremely small for distributed protocol handling N validators
   - With 100 validators, each broadcasting to 99 peers, channels naturally experience high load
   - Even legitimate traffic from 11 concurrent peers would overflow the channel

3. **FIFO Drop Behavior**: 
   - Newest messages dropped means continuous flooding keeps channel full
   - Legitimate retries from ReliableBroadcast also get dropped
   - No prioritization mechanism for legitimate vs. attack traffic

4. **Timing Window**: 
   - DKG phases are predictable (occur during epoch transitions)
   - Attacker knows exactly when to initiate flooding
   - Attack window is several seconds to minutes (entire DKG duration)

5. **No Apparent Rate Limiting**: 
   - No per-peer rate limiting visible at DKG protocol level
   - General network rate limiting (if configured) may not be sufficient
   - No backpressure or circuit breaker mechanisms observed

**Attack Complexity: LOW**
- Simple to execute (send many RPC requests)
- No cryptographic bypasses or complex state manipulation needed
- Readily automatable with basic scripting

## Recommendation

**Immediate Mitigation (Quick Fix):**

Increase the hardcoded channel size in NetworkTask to match or exceed the configured `max_network_channel_size` from DKG config:

```rust
// dkg/src/network.rs, line 141
// BEFORE:
let (rpc_tx, rpc_rx) = aptos_channel::new(QueueStyle::FIFO, 10, None);

// AFTER:
let (rpc_tx, rpc_rx) = aptos_channel::new(
    QueueStyle::FIFO,
    // Use the configured max_network_channel_size or a reasonable minimum
    256.max(100), // At minimum 100, preferably use config value
    None
);
```

**Recommended Fix (Pass Config Value):**

Modify NetworkTask to accept channel size as a parameter from the DKG configuration:

```rust
// In dkg/src/epoch_manager.rs
// Pass the configured channel size when creating NetworkTask
let network_task = NetworkTask::new(
    network_service_events,
    self_receiver,
    node_config.dkg.max_network_channel_size, // Pass config value
);

// In dkg/src/network.rs
impl NetworkTask {
    pub fn new(
        network_service_events: NetworkServiceEvents<DKGMessage>,
        self_receiver: aptos_channels::Receiver<Event<DKGMessage>>,
        rpc_channel_size: usize, // Add parameter
    ) -> (NetworkTask, NetworkReceivers) {
        let (rpc_tx, rpc_rx) = aptos_channel::new(
            QueueStyle::FIFO,
            rpc_channel_size, // Use configured value
            None
        );
        // ... rest of implementation
    }
}
```

**Long-Term Improvements:**

1. **Add Per-Peer Rate Limiting**: Implement token bucket rate limiting per peer at the DKG protocol level to prevent flooding
2. **Priority Queue**: Use priority queue to ensure legitimate validator RPC requests are processed before others
3. **Backpressure Mechanism**: Add flow control to signal senders when channel is approaching capacity
4. **Monitoring**: Add metrics for channel utilization and dropped message rates
5. **Dynamic Sizing**: Consider dynamic channel sizing based on validator set size (N validators → channel size ≥ N)

## Proof of Concept

**Conceptual PoC (Attack Simulation):**

```rust
// This demonstrates the attack concept
// Attacker code (would run externally):

use aptos_types::PeerId;
use dkg::DKGMessage;
use std::time::Duration;

async fn dkg_liveness_attack(
    target_validator: PeerId,
    network_client: NetworkClient,
) {
    // During DKG phase (epoch transition)
    loop {
        // Flood with crafted DKG RPC requests
        for _ in 0..100 {
            let fake_request = DKGMessage::TranscriptRequest(
                DKGTranscriptRequest::new(current_epoch)
            );
            
            // Send RPC without waiting for response
            let _ = network_client.send_rpc(
                target_validator,
                fake_request,
                Duration::from_secs(1),
            );
        }
        
        // Small delay to maintain flooding rate
        tokio::time::sleep(Duration::from_millis(10)).await;
    }
}

// Expected Result:
// - NetworkTask internal channel (size 10) fills up immediately
// - Legitimate DKG transcript requests from honest validators are dropped
// - Victim validator cannot aggregate transcripts
// - If enough validators are attacked, DKG fails to reach quorum
// - Randomness generation is blocked
```

**Validation Steps:**

1. Deploy Aptos testnet with 100 validators
2. Instrument NetworkTask to log channel utilization and dropped messages
3. Initiate DKG phase (trigger epoch transition)
4. From external node, flood target validator with 1000+ DKG RPC requests/second
5. Observe:
   - NetworkTask channel fills to capacity (10 messages)
   - Legitimate transcript requests from honest validators are dropped
   - DKG aggregation fails to reach quorum on victim validator
   - DKG times out without completing
6. Repeat across multiple validators to prevent network-wide DKG completion

**Notes:**
This PoC requires integration testing infrastructure with multiple validators. The attack is straightforward once network access to validator DKG endpoints is obtained. The vulnerability is confirmed by code analysis showing the 10-message channel bottleneck and FIFO drop behavior.

### Citations

**File:** dkg/src/network.rs (L141-141)
```rust
        let (rpc_tx, rpc_rx) = aptos_channel::new(QueueStyle::FIFO, 10, None);
```

**File:** crates/channel/src/message_queues.rs (L134-147)
```rust
        if key_message_queue.len() >= self.max_queue_size.get() {
            if let Some(c) = self.counters.as_ref() {
                c.with_label_values(&["dropped"]).inc();
            }
            match self.queue_style {
                // Drop the newest message for FIFO
                QueueStyle::FIFO => Some(message),
                // Drop the oldest message for LIFO
                QueueStyle::LIFO | QueueStyle::KLAST => {
                    let oldest = key_message_queue.pop_front();
                    key_message_queue.push_back(message);
                    oldest
                },
            }
```

**File:** dkg/src/agg_trx_producer.rs (L56-67)
```rust
        let req = DKGTranscriptRequest::new(epoch_state.epoch);
        let agg_state = Arc::new(TranscriptAggregationState::<DKG>::new(
            start_time,
            my_addr,
            params,
            epoch_state,
        ));
        let task = async move {
            let agg_trx = rb
                .broadcast(req, agg_state)
                .await
                .expect("broadcast cannot fail");
```

**File:** dkg/src/transcript_aggregation/mod.rs (L122-134)
```rust
        let threshold = self.epoch_state.verifier.quorum_voting_power();
        let power_check_result = self
            .epoch_state
            .verifier
            .check_voting_power(trx_aggregator.contributors.iter(), true);
        let new_total_power = match &power_check_result {
            Ok(x) => Some(*x),
            Err(VerifyError::TooLittleVotingPower { voting_power, .. }) => Some(*voting_power),
            _ => None,
        };
        let maybe_aggregated = power_check_result
            .ok()
            .map(|_| trx_aggregator.trx.clone().unwrap());
```

**File:** crates/reliable-broadcast/src/lib.rs (L185-200)
```rust
                        match result {
                            Ok(may_be_aggragated) => {
                                if let Some(aggregated) = may_be_aggragated {
                                    return Ok(aggregated);
                                }
                            },
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
```

**File:** config/src/config/dkg_config.rs (L8-18)
```rust
pub struct DKGConfig {
    pub max_network_channel_size: usize,
}

impl Default for DKGConfig {
    fn default() -> Self {
        Self {
            max_network_channel_size: 256,
        }
    }
}
```
