# Audit Report

## Title
Time-of-Check-Time-of-Use (TOCTOU) Race Condition in Data Stream Creation Allows Unfulfillable Streams

## Summary
A race condition exists between stream validation and stream execution in the data streaming service. Streams are validated against a snapshot of advertised data at creation time, but when the stream later creates actual data requests, it uses a fresh global data summary that may no longer contain the required data. This allows streams to be created that can never be fulfilled, causing them to retry repeatedly for ~250 seconds before terminating, wasting node resources and degrading performance.

## Finding Description

The vulnerability exists in the stream creation and initialization flow:

**Stream Creation (validation):** [1](#0-0) 

At line 266-269, the global data summary is refreshed. At line 273, `advertised_data` is captured from this summary. At line 287, the stream is validated using `ensure_data_is_available(&advertised_data)`. [2](#0-1) 

The validation checks if data is available in the advertised_data snapshot via `is_remaining_data_available()`.

**Stream Initialization (request creation):** [3](#0-2) 

Later, when the stream actually sends requests, at line 345 a FRESH `global_data_summary` is fetched. This is passed to `initialize_data_requests()` at line 369 and `process_data_responses()` at line 379.

**The TOCTOU Race:**

Between stream creation (T1) and stream initialization (T2), the advertised data can become stale when: [4](#0-3) 

Peers disconnect (naturally or maliciously), triggering `garbage_collect_peer_states()` which removes their advertised data. The `calculate_global_data_summary()` then rebuilds the global summary excluding disconnected peers.

**Request Creation Without Availability Check:** [5](#0-4) 

The `create_data_client_request_batch()` function blindly creates requests based on stream state without checking if data is actually available from connected peers.

**Retry Loop on Unavailable Data:** [6](#0-5) 

When requests fail due to unavailable data, `handle_data_client_error()` increments `request_failure_count` and retries with exponential backoff. [7](#0-6) 

The stream continues retrying until `request_failure_count >= max_request_retry` (default: 5). [8](#0-7) 

With `max_request_retry: 5`, exponential backoff timeouts (10s, 20s, 40s, 60s, 60s, 60s), the stream remains blocked for approximately 250 seconds (~4 minutes) before terminating.

## Impact Explanation

**High Severity - Validator Node Slowdowns:**

1. **Resource Exhaustion**: Each unfulfillable stream consumes network connections, memory for pending requests, and CPU cycles for retry logic
2. **State Sync Delays**: Blocked streams prevent nodes from making state sync progress, causing them to lag behind the network
3. **Amplification**: Multiple streams can be created simultaneously, multiplying the resource impact
4. **Availability Impact**: Severe resource exhaustion can degrade node responsiveness and affect consensus participation

This meets the **High severity** criteria per the Aptos bug bounty program: "Validator node slowdowns" and "Significant protocol violations."

## Likelihood Explanation

**High Likelihood:**

1. **Natural Occurrence**: Peer disconnections happen regularly in distributed networks due to network partitions, node restarts, or maintenance
2. **No Special Privileges**: Any peer can disconnect, triggering the condition
3. **Timing Window**: The race window exists from stream creation until first request initialization, which can be 50-100ms based on `progress_check_interval_ms`
4. **Malicious Exploitation**: An attacker can deliberately:
   - Connect as a peer and advertise data
   - Wait for streams to be created
   - Disconnect immediately, causing streams to fail
   - Repeat to create multiple unfulfillable streams

## Recommendation

**Fix: Re-validate data availability when initializing requests**

Modify `update_progress_of_data_stream()` to re-check data availability before initializing requests:

```rust
async fn update_progress_of_data_stream(
    &mut self,
    data_stream_id: &DataStreamId,
) -> Result<(), Error> {
    let global_data_summary = self.get_global_data_summary();
    let data_stream = self.get_data_stream(data_stream_id)?;
    
    // ... existing send_failure check ...
    
    if !data_stream.data_requests_initialized() {
        // RE-VALIDATE: Check data availability with current advertised data
        // before initializing requests
        if let Err(error) = data_stream.ensure_data_is_available(
            &global_data_summary.advertised_data
        ) {
            // Data no longer available, terminate stream immediately
            warn!(LogSchema::new(LogEntry::TerminateStream)
                .stream_id(*data_stream_id)
                .error(&error)
                .message("Data became unavailable, terminating stream."));
            self.data_streams.remove(data_stream_id);
            return Err(error);
        }
        
        data_stream.initialize_data_requests(global_data_summary)?;
        // ... rest of initialization ...
    } else {
        // ... existing process_data_responses ...
    }
    Ok(())
}
```

**Alternative Fix: Periodic re-validation during processing**

Add periodic availability checks in `process_data_responses()` to catch stale data mid-stream.

## Proof of Concept

```rust
#[tokio::test]
async fn test_toctou_unfulfillable_stream() {
    use aptos_config::config::DataStreamingServiceConfig;
    use std::time::Duration;
    
    // Create streaming service with mock data client
    let (streaming_client, mut streaming_service) = 
        create_streaming_client_and_server(None, false, false, true, false);
    
    // Add a peer that advertises transaction data at version 1000
    let peer = create_mock_peer_with_data(0, 1000);
    streaming_service.aptos_data_client.add_peer(peer.clone());
    
    // Refresh to include the peer's advertised data
    refresh_global_data_summary(
        streaming_service.aptos_data_client.clone(),
        streaming_service.global_data_summary.clone()
    );
    
    // Request a stream for transactions 0-1000
    // This will validate against advertised data including the peer
    let stream_request = StreamRequest::GetAllTransactions(
        GetAllTransactionsRequest {
            start_version: 0,
            end_version: 1000,
            proof_version: 1000,
            include_events: false,
        }
    );
    let (request_message, response_receiver) = 
        create_stream_request_message(stream_request);
    
    // Create the stream (validation passes at T1)
    streaming_service.handle_stream_request_message(
        request_message,
        streaming_service.stream_update_notifier.clone()
    );
    let stream_listener = response_receiver.await.unwrap().unwrap();
    let stream_id = stream_listener.data_stream_id;
    
    // RACE CONDITION: Disconnect the peer BEFORE stream initializes requests
    // This simulates the TOCTOU window
    streaming_service.aptos_data_client.disconnect_peer(peer);
    
    // Refresh to remove the peer from advertised data
    refresh_global_data_summary(
        streaming_service.aptos_data_client.clone(),
        streaming_service.global_data_summary.clone()
    );
    
    // Now try to drive stream progress (at T2)
    // The stream will try to initialize with FRESH global_data_summary
    // that no longer contains the required data
    let start_time = Instant::now();
    let mut retry_count = 0;
    
    while Instant::now() - start_time < Duration::from_secs(300) {
        if let Err(error) = streaming_service
            .update_progress_of_data_stream(&stream_id)
            .await 
        {
            retry_count += 1;
            assert!(matches!(error, Error::DataIsUnavailable(_)) || 
                    error.to_string().contains("timeout"));
        }
        
        // Check if stream was terminated due to max retries
        if !streaming_service.data_streams.contains_key(&stream_id) {
            break;
        }
        
        tokio::time::sleep(Duration::from_millis(100)).await;
    }
    
    // VULNERABILITY CONFIRMED: Stream retried multiple times before terminating
    assert!(retry_count >= 5, "Stream should retry at least 5 times");
    
    // Stream should eventually be removed after max_request_retry
    assert!(!streaming_service.data_streams.contains_key(&stream_id),
        "Stream should be terminated after max retries");
    
    // IMPACT: Stream was blocked for extended period (~250 seconds)
    let elapsed = Instant::now() - start_time;
    assert!(elapsed >= Duration::from_secs(200),
        "Stream should be blocked for significant time due to retries");
}
```

## Notes

The vulnerability is exacerbated by:
- Background global data summary refresher running every 50ms, increasing likelihood of stale data
- No mechanism to detect or handle mid-stream data unavailability
- Exponential backoff amplifying the blocking time
- Multiple streams can be created, multiplying the resource impact

The root cause is the fundamental TOCTOU pattern: checking data availability at stream creation time but using different data at request execution time. The fix requires atomic validation-and-execution or re-validation before execution.

### Citations

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L265-287)
```rust
        // Refresh the cached global data summary
        refresh_global_data_summary(
            self.aptos_data_client.clone(),
            self.global_data_summary.clone(),
        );

        // Create a new data stream
        let stream_id = self.stream_id_generator.next();
        let advertised_data = self.get_global_data_summary().advertised_data.clone();
        let (data_stream, stream_listener) = DataStream::new(
            self.data_client_config,
            self.streaming_service_config,
            stream_id,
            &request_message.stream_request,
            stream_update_notifier,
            self.aptos_data_client.clone(),
            self.notification_id_generator.clone(),
            &advertised_data,
            self.time_service.clone(),
        )?;

        // Verify the data stream can be fulfilled using the currently advertised data
        data_stream.ensure_data_is_available(&advertised_data)?;
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L341-384)
```rust
    async fn update_progress_of_data_stream(
        &mut self,
        data_stream_id: &DataStreamId,
    ) -> Result<(), Error> {
        let global_data_summary = self.get_global_data_summary();

        // If there was a send failure, terminate the stream
        let data_stream = self.get_data_stream(data_stream_id)?;
        if data_stream.send_failure() {
            info!(
                (LogSchema::new(LogEntry::TerminateStream)
                    .stream_id(*data_stream_id)
                    .event(LogEvent::Success)
                    .message("There was a send failure, terminating the stream."))
            );
            metrics::DATA_STREAM_SEND_FAILURE.inc();
            if self.data_streams.remove(data_stream_id).is_none() {
                return Err(Error::UnexpectedErrorEncountered(format!(
                    "Failed to terminate stream id {:?} for send failure! Stream not found.",
                    data_stream_id
                )));
            }
            return Ok(());
        }

        // Drive data stream progress
        if !data_stream.data_requests_initialized() {
            // Initialize the request batch by sending out data client requests
            data_stream.initialize_data_requests(global_data_summary)?;
            info!(
                (LogSchema::new(LogEntry::InitializeStream)
                    .stream_id(*data_stream_id)
                    .event(LogEvent::Success)
                    .message("Data stream initialized."))
            );
        } else {
            // Process any data client requests that have received responses
            data_stream
                .process_data_responses(global_data_summary)
                .await?;
        }

        Ok(())
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L442-454)
```rust
    pub async fn process_data_responses(
        &mut self,
        global_data_summary: GlobalDataSummary,
    ) -> Result<(), Error> {
        if self.stream_engine.is_stream_complete()
            || self.request_failure_count >= self.streaming_service_config.max_request_retry
            || self.send_failure
        {
            if !self.send_failure && self.stream_end_notification_id.is_none() {
                self.send_end_of_stream_notification().await?;
            }
            return Ok(()); // There's nothing left to do
        }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L710-744)
```rust
    /// Handles an error returned by the data client in relation to a request
    fn handle_data_client_error(
        &mut self,
        data_client_request: &DataClientRequest,
        data_client_error: &aptos_data_client::error::Error,
    ) -> Result<(), Error> {
        // Log the error
        warn!(LogSchema::new(LogEntry::ReceivedDataResponse)
            .stream_id(self.data_stream_id)
            .event(LogEvent::Error)
            .error(&data_client_error.clone().into())
            .message("Encountered a data client error!"));

        // TODO(joshlind): can we identify the best way to react to the error?
        self.resend_data_client_request(data_client_request)
    }

    /// Resends a failed data client request and pushes the pending notification
    /// to the head of the pending notifications batch.
    fn resend_data_client_request(
        &mut self,
        data_client_request: &DataClientRequest,
    ) -> Result<(), Error> {
        // Increment the number of client failures for this request
        self.request_failure_count += 1;

        // Resend the client request
        let pending_client_response = self.send_client_request(true, data_client_request.clone());

        // Push the pending response to the head of the sent requests queue
        self.get_sent_data_requests()?
            .push_front(pending_client_response);

        Ok(())
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L866-877)
```rust
    pub fn ensure_data_is_available(&self, advertised_data: &AdvertisedData) -> Result<(), Error> {
        if !self
            .stream_engine
            .is_remaining_data_available(advertised_data)?
        {
            return Err(Error::DataIsUnavailable(format!(
                "Unable to satisfy stream engine: {:?}, with advertised data: {:?}",
                self.stream_engine, advertised_data
            )));
        }
        Ok(())
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L333-350)
```rust
    pub fn garbage_collect_peer_states(&self, connected_peers: HashSet<PeerNetworkId>) {
        self.peer_to_state
            .retain(|peer_network_id, _| connected_peers.contains(peer_network_id));
    }

    /// Calculates a global data summary using all known storage summaries
    pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
        // Gather all storage summaries, but exclude peers that are ignored
        let storage_summaries: Vec<StorageServerSummary> = self
            .peer_to_state
            .iter()
            .filter_map(|peer_state| {
                peer_state
                    .value()
                    .get_storage_summary_if_not_ignored()
                    .cloned()
            })
            .collect();
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L2049-2098)
```rust
fn create_data_client_request_batch(
    start_index: u64,
    end_index: u64,
    max_number_of_requests: u64,
    optimal_chunk_size: u64,
    stream_engine: StreamEngine,
) -> Result<Vec<DataClientRequest>, Error> {
    if start_index > end_index {
        return Ok(vec![]);
    }

    // Calculate the total number of items left to satisfy the stream
    let mut total_items_to_fetch = end_index
        .checked_sub(start_index)
        .and_then(|e| e.checked_add(1)) // = end_index - start_index + 1
        .ok_or_else(|| Error::IntegerOverflow("Total items to fetch has overflown!".into()))?;

    // Iterate until we've requested all transactions or hit the maximum number of requests
    let mut data_client_requests = vec![];
    let mut num_requests_made = 0;
    let mut next_index_to_request = start_index;
    while total_items_to_fetch > 0 && num_requests_made < max_number_of_requests {
        // Calculate the number of items to fetch in this request
        let num_items_to_fetch = cmp::min(total_items_to_fetch, optimal_chunk_size);

        // Calculate the start and end indices for the request
        let request_start_index = next_index_to_request;
        let request_end_index = request_start_index
            .checked_add(num_items_to_fetch)
            .and_then(|e| e.checked_sub(1)) // = request_start_index + num_items_to_fetch - 1
            .ok_or_else(|| Error::IntegerOverflow("End index to fetch has overflown!".into()))?;

        // Create the data client requests
        let data_client_request =
            create_data_client_request(request_start_index, request_end_index, &stream_engine)?;
        data_client_requests.push(data_client_request);

        // Update the local loop state
        next_index_to_request = request_end_index
            .checked_add(1)
            .ok_or_else(|| Error::IntegerOverflow("Next index to request has overflown!".into()))?;
        total_items_to_fetch = total_items_to_fetch
            .checked_sub(num_items_to_fetch)
            .ok_or_else(|| Error::IntegerOverflow("Total items to fetch has overflown!".into()))?;
        num_requests_made = num_requests_made.checked_add(1).ok_or_else(|| {
            Error::IntegerOverflow("Number of payload requests has overflown!".into())
        })?;
    }

    Ok(data_client_requests)
```

**File:** config/src/config/state_sync_config.rs (L265-280)
```rust
impl Default for DataStreamingServiceConfig {
    fn default() -> Self {
        Self {
            dynamic_prefetching: DynamicPrefetchingConfig::default(),
            enable_subscription_streaming: false,
            global_summary_refresh_interval_ms: 50,
            max_concurrent_requests: MAX_CONCURRENT_REQUESTS,
            max_concurrent_state_requests: MAX_CONCURRENT_STATE_REQUESTS,
            max_data_stream_channel_sizes: 50,
            max_notification_id_mappings: 300,
            max_num_consecutive_subscriptions: 45, // At ~3 blocks per second, this should last ~15 seconds
            max_pending_requests: 50,
            max_request_retry: 5,
            max_subscription_stream_lag_secs: 10, // 10 seconds
            progress_check_interval_ms: 50,
        }
```
