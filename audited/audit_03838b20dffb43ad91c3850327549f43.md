# Audit Report

## Title
Admin Service Database Operations Lack Timeout Protection Leading to Thread Pool Exhaustion and Service Unavailability

## Summary
The admin service HTTP handlers for consensus database dumps execute unbounded blocking database operations without timeout protection. Slow or hanging database reads can exhaust the limited blocking thread pool (64 threads), causing the entire admin service to become unresponsive and preventing critical operational monitoring during incidents.

## Finding Description

The `handle_dump_consensus_db_request()` function performs database operations without timeout protection: [1](#0-0) 

This calls `spawn_blocking()` which is a thin wrapper around tokio's blocking task spawner: [2](#0-1) 

The underlying database operation reads **all** blocks and quorum certificates from storage: [3](#0-2) 

The `get_all()` method performs a full table scan collecting all entries into memory: [4](#0-3) 

The Tokio runtime has a hard limit of 64 concurrent blocking threads: [5](#0-4) 

**Attack/Failure Scenario:**
1. Admin service is running (default on testnet/devnet, or enabled on mainnet with authentication)
2. An operator (or attacker on testnet) sends multiple requests to `/debug/consensus/consensusdb`, `/debug/consensus/quorumstoredb`, or `/debug/consensus/block`
3. Each request triggers a full database scan that may take minutes on a validator with large consensus history
4. After 64 concurrent slow requests, the blocking thread pool is exhausted
5. All subsequent admin service requests (including to other endpoints) hang indefinitely waiting for available blocking threads
6. The admin service becomes completely unresponsive until existing operations complete

This same issue affects two other handlers: [6](#0-5) [7](#0-6) 

The admin service configuration shows that authentication is NOT enforced on non-mainnet networks: [8](#0-7) [9](#0-8) 

## Impact Explanation

This qualifies as **High Severity** per the Aptos bug bounty criteria:

- **Validator node slowdowns**: The admin service becomes completely unresponsive, preventing operators from accessing critical debugging endpoints during incidents
- **API crashes**: If database operations consume excessive memory or CPU, the admin service thread pool exhaustion can cascade to affect other node operations

The admin service is critical operational infrastructure used for:
- Real-time debugging during consensus issues (`/debug/consensus/*`)
- Performance profiling (`/profilez`, `/threadz`)
- Memory analysis (`/malloc/stats`)

During a network incident, operators rely on these endpoints to diagnose issues. If the admin service is hung due to unbounded database operations, operators cannot investigate problems, significantly extending incident response time and potentially causing validator penalties or network degradation.

## Likelihood Explanation

**High Likelihood** of occurrence:

1. **Legitimate operational trigger**: Even authorized operators can accidentally trigger this by requesting large database dumps during normal debugging operations
2. **Database growth**: As validators run longer, consensus databases grow larger, making operations progressively slower
3. **Disk I/O issues**: Any disk performance degradation (hardware issues, high load) can cause operations to hang indefinitely
4. **Testnet/Devnet exposure**: Enabled by default without authentication on non-mainnet networks, allowing any network participant to trigger the issue
5. **Multiple vulnerable endpoints**: Three separate handlers share this vulnerability, increasing the attack surface

The issue requires no special conditionsâ€”simply invoking the endpoint is sufficient if the database is large or experiencing I/O slowness.

## Recommendation

Implement timeout protection for all blocking database operations in admin service handlers:

```rust
use tokio::time::{timeout, Duration};

pub async fn handle_dump_consensus_db_request(
    _req: Request<Body>,
    consensus_db: Arc<dyn PersistentLivenessStorage>,
) -> hyper::Result<Response<Body>> {
    info!("Dumping consensus db.");

    // Add configurable timeout (e.g., 30 seconds for admin operations)
    const ADMIN_DB_OPERATION_TIMEOUT: Duration = Duration::from_secs(30);
    
    match timeout(
        ADMIN_DB_OPERATION_TIMEOUT,
        spawn_blocking(move || dump_consensus_db(consensus_db.as_ref()))
    ).await {
        Ok(Ok(result)) => {
            info!("Finished dumping consensus db.");
            let headers: Vec<(_, HeaderValue)> =
                vec![(CONTENT_LENGTH, HeaderValue::from(result.len()))];
            Ok(reply_with(headers, result))
        },
        Ok(Err(e)) => {
            info!("Failed to dump consensus db: {e:?}");
            Ok(reply_with_status(
                StatusCode::INTERNAL_SERVER_ERROR,
                e.to_string(),
            ))
        },
        Err(_) => {
            info!("Consensus db dump timed out");
            Ok(reply_with_status(
                StatusCode::REQUEST_TIMEOUT,
                "Database operation timed out".to_string(),
            ))
        },
    }
}
```

Apply the same pattern to:
- `handle_dump_quorum_store_db_request()` (line 57)
- `handle_dump_block_request()` (line 106-114)

Additionally, consider implementing pagination or streaming responses for large database dumps to avoid loading entire datasets into memory.

## Proof of Concept

**Rust Reproduction Steps:**

1. Start an Aptos validator node with admin service enabled
2. Let the consensus database accumulate significant history (or simulate with pre-populated test data)
3. Execute the following attack script:

```rust
use std::time::Duration;
use tokio::time::sleep;

#[tokio::main]
async fn main() {
    let client = reqwest::Client::new();
    let admin_endpoint = "http://localhost:9102/debug/consensus/consensusdb";
    
    // Launch 65+ concurrent requests to exhaust the 64-thread blocking pool
    let mut handles = vec![];
    
    for i in 0..70 {
        let client = client.clone();
        let endpoint = admin_endpoint.to_string();
        
        let handle = tokio::spawn(async move {
            println!("Sending request {}", i);
            let start = std::time::Instant::now();
            
            match client.get(&endpoint).send().await {
                Ok(resp) => {
                    println!("Request {} completed in {:?} with status {}", 
                             i, start.elapsed(), resp.status());
                },
                Err(e) => {
                    println!("Request {} failed: {:?}", i, e);
                }
            }
        });
        
        handles.push(handle);
        
        // Stagger requests slightly
        sleep(Duration::from_millis(100)).await;
    }
    
    // Wait for all requests
    for handle in handles {
        handle.await.unwrap();
    }
}
```

**Expected Result**: After ~64 concurrent requests, subsequent requests will hang indefinitely or timeout at the HTTP client level, and the admin service becomes unresponsive to all endpoints.

**Shell-based PoC:**
```bash
# Launch 70 concurrent requests
for i in {1..70}; do
  curl -s "http://localhost:9102/debug/consensus/consensusdb" > /dev/null &
  echo "Launched request $i (PID: $!)"
  sleep 0.1
done

# Try to access other admin endpoints - they should hang
curl -v "http://localhost:9102/threadz"  # This will hang if pool is exhausted
```

## Notes

This vulnerability affects all three consensus database dump endpoints. The issue is exacerbated on long-running validators with large consensus databases. While mainnet nodes should have authentication enabled (preventing unauthorized exploitation), the issue can still be triggered accidentally by legitimate operators or due to infrastructure issues (slow disks, database corruption). On testnet and devnet, the admin service is enabled without authentication by default, making this trivially exploitable by any network participant.

The fix is straightforward and follows standard best practices for preventing unbounded blocking operations in async services.

### Citations

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L23-23)
```rust
    match spawn_blocking(move || dump_consensus_db(consensus_db.as_ref())).await {
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L57-57)
```rust
    match spawn_blocking(move || dump_quorum_store_db(quorum_store_db.as_ref(), digest)).await {
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L106-114)
```rust
    match spawn_blocking(move || {
        if bcs {
            dump_blocks_bcs(consensus_db.as_ref(), quorum_store_db.as_ref(), block_id)
                .map(Into::<Body>::into)
        } else {
            dump_blocks(consensus_db.as_ref(), quorum_store_db.as_ref(), block_id).map(Into::into)
        }
    })
    .await
```

**File:** crates/aptos-system-utils/src/utils.rs (L14-22)
```rust
pub async fn spawn_blocking<F, T>(func: F) -> Result<T>
where
    F: FnOnce() -> Result<T> + Send + 'static,
    T: Send + 'static,
{
    tokio::task::spawn_blocking(func)
        .await
        .map_err(Error::msg)?
}
```

**File:** consensus/src/consensusdb/mod.rs (L80-106)
```rust
    pub fn get_data(
        &self,
    ) -> Result<(
        Option<Vec<u8>>,
        Option<Vec<u8>>,
        Vec<Block>,
        Vec<QuorumCert>,
    )> {
        let last_vote = self.get_last_vote()?;
        let highest_2chain_timeout_certificate = self.get_highest_2chain_timeout_certificate()?;
        let consensus_blocks = self
            .get_all::<BlockSchema>()?
            .into_iter()
            .map(|(_, block)| block)
            .collect();
        let consensus_qcs = self
            .get_all::<QCSchema>()?
            .into_iter()
            .map(|(_, qc)| qc)
            .collect();
        Ok((
            last_vote,
            highest_2chain_timeout_certificate,
            consensus_blocks,
            consensus_qcs,
        ))
    }
```

**File:** consensus/src/consensusdb/mod.rs (L201-205)
```rust
    pub fn get_all<S: Schema>(&self) -> Result<Vec<(S::Key, S::Value)>, DbError> {
        let mut iter = self.db.iter::<S>()?;
        iter.seek_to_first();
        Ok(iter.collect::<Result<Vec<(S::Key, S::Value)>, AptosDbError>>()?)
    }
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** config/src/config/admin_service_config.rs (L21-22)
```rust
    // If empty, will allow all requests without authentication. (Not allowed on mainnet.)
    pub authentication_configs: Vec<AuthenticationConfig>,
```

**File:** config/src/config/admin_service_config.rs (L93-103)
```rust
        if node_config.admin_service.enabled.is_none() {
            // Only enable the admin service if the chain is not mainnet
            let admin_service_enabled = if let Some(chain_id) = chain_id {
                !chain_id.is_mainnet()
            } else {
                false // We cannot determine the chain ID, so we disable the admin service
            };
            node_config.admin_service.enabled = Some(admin_service_enabled);

            modified_config = true; // The config was modified
        }
```
