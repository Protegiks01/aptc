# Audit Report

## Title
Missing Batch Expiration Validation Enables Resource Exhaustion DoS Attack via Expired Remote Batches

## Summary
Byzantine validators can exploit the missing expiration validation in `BatchMsg` verification to force honest validators to repeatedly waste resources processing batches that will ultimately be rejected as expired. The asymmetry between `remote_batch_expiry_gap_when_init_usecs` (500ms) and `batch_expiry_gap_when_init_usecs` (60s) exacerbates this issue by creating a wide window for attack.

## Finding Description

The quorum store batch verification flow contains a critical validation gap: while `SignedBatchInfo` messages validate that batch expiration is not too far in the future, `BatchMsg` messages (which contain the actual batch data and transactions) perform **no expiration validation whatsoever**. [1](#0-0) 

The `BatchMsg::verify()` function only validates:
- Message is not empty
- Batch count limits
- Author is in validator set
- Author matches sender
- Payload consistency via `Batch::verify()` [2](#0-1) 

The `Batch::verify()` function only validates payload consistency (hash, counts, gas buckets) but **never checks the batch expiration field**.

This allows Byzantine validators to send batches with arbitrarily short or even past expiration times. These batches pass all network-level verification and are processed through expensive operations:

1. **Network I/O**: Full batch data received (up to 4MB per message)
2. **Persistence**: Batches written to storage [3](#0-2) 

3. **Transaction Tracking**: Sent to `BatchGenerator` where transactions are marked as in-progress with internal expiry of only 500ms [4](#0-3) 

4. **Repeated Filtering**: During proposal generation, transactions are repeatedly filtered based on expiration [5](#0-4) 

The asymmetry creates additional exploitation vectors:
- `SignedBatchInfo` verification allows expiration up to `current_time + 60s` [6](#0-5) [7](#0-6) 

- But remote batch transactions are only tracked internally for 500ms [8](#0-7) 

This 59.5 second gap allows Byzantine validators to send batches that pass verification but become useless almost immediately, yet honest validators have already consumed resources processing them.

**Attack Flow:**
1. Byzantine validator creates batch with `expiration = current_time + 2s` (or even `current_time - 10s`)
2. Sends `BatchMsg` to honest validators
3. `BatchMsg::verify()` passes (no expiration check)
4. Honest validators persist batch to storage (disk I/O)
5. Honest validators mark transactions as in-progress (CPU/memory)
6. After 500ms, internal tracking expires, but batch data remains in storage
7. During proposal generation, honest validators repeatedly filter expired transactions (CPU waste)
8. Byzantine validator repeats attack continuously

## Impact Explanation

This vulnerability represents a **Medium Severity** issue per the Aptos bug bounty criteria, specifically "Validator node slowdowns."

**Resource Exhaustion Impacts:**
- **Network Bandwidth**: Byzantine validators can send up to 4MB batches repeatedly
- **Disk I/O**: Each batch is persisted to storage before expiration is checked
- **CPU Cycles**: Transaction filtering, signature aggregation, and batch processing
- **Memory**: Tracking batches and transactions in various data structures

The attack is rate-limited by network capacity and per-validator quotas, but a coordinated attack by multiple Byzantine validators (up to 1/3 of stake) could significantly degrade honest validator performance, potentially affecting consensus liveness and block production rates.

## Likelihood Explanation

**High Likelihood** - The attack requires only:
- Byzantine validator with < 1/3 total stake (standard Byzantine assumption)
- Ability to craft batches with arbitrary expiration times (trivial)
- Network connectivity to send `BatchMsg` messages (normal operation)

No special privileges, insider access, or complex coordination is required. The attack is straightforward to execute and difficult to distinguish from legitimate traffic until resources are exhausted.

## Recommendation

Add expiration validation to `BatchMsg` and `Batch` verification:

```rust
// In consensus/src/quorum_store/types.rs, Batch::verify()
pub fn verify(&self, current_block_timestamp: u64) -> anyhow::Result<()> {
    // Existing checks...
    
    // Add expiration validation
    ensure!(
        self.expiration() > current_block_timestamp,
        "Batch expiration {} is not greater than block timestamp {}",
        self.expiration(),
        current_block_timestamp
    );
    
    // Existing payload consistency checks...
    Ok(())
}

// In consensus/src/quorum_store/types.rs, BatchMsg::verify()
pub fn verify(
    &self,
    peer_id: PeerId,
    max_num_batches: usize,
    verifier: &ValidatorVerifier,
    current_block_timestamp: u64,
) -> anyhow::Result<()> {
    // Existing checks...
    
    for batch in self.batches.iter() {
        // Existing author checks...
        batch.verify(current_block_timestamp)?  // Pass block timestamp
    }
    Ok(())
}
```

Additionally, consider using the `remote_batch_expiry_gap_when_init_usecs` value consistently for all remote batch expiration calculations instead of mixing it with the 60s value.

## Proof of Concept

```rust
#[tokio::test]
async fn test_expired_batch_resource_exhaustion() {
    use aptos_types::transaction::SignedTransaction;
    use consensus::quorum_store::types::{Batch, BatchMsg};
    use aptos_consensus_types::proof_of_store::BatchInfoExt;
    
    // Simulate Byzantine validator
    let byzantine_peer = PeerId::random();
    let epoch = 1;
    
    // Create batch with expiration in the past (relative to block timestamp)
    let current_block_timestamp = aptos_infallible::duration_since_epoch().as_micros() as u64;
    let expired_expiration = current_block_timestamp - 10_000_000; // 10 seconds ago
    
    // Create transactions
    let txns: Vec<SignedTransaction> = vec![/* ... */];
    
    // Create batch with expired timestamp
    let batch = Batch::<BatchInfoExt>::new_v1(
        BatchId::new(1),
        txns,
        epoch,
        expired_expiration, // Already expired!
        byzantine_peer,
        0,
    );
    
    // Create BatchMsg
    let batch_msg = BatchMsg::new(vec![batch]);
    
    // Verify - this will PASS despite expired batch!
    let verifier = /* ... */;
    assert!(batch_msg.verify(byzantine_peer, 10, &verifier).is_ok());
    
    // Batch will be processed, consuming resources:
    // - Network bandwidth to receive
    // - Disk I/O to persist
    // - CPU to process
    // Only to be rejected later when checked against block_timestamp
    
    // Demonstrate repeated attack
    for i in 0..100 {
        let batch = create_expired_batch(i);
        // Each batch passes verification and consumes resources
        // before being rejected as expired
    }
}
```

## Notes

The vulnerability stems from an incomplete validation pipeline where expiration checks exist at the signature level (`SignedBatchInfo`) but not at the data level (`BatchMsg`/`Batch`). The asymmetry between 500ms and 60s expiry gaps creates a wide attack window where batches can be "valid" for signature collection but "useless" for actual consensus due to rapid internal expiry. This design appears to optimize for reducing transaction duplication across validators but inadvertently opens a resource exhaustion vector.

### Citations

**File:** consensus/src/quorum_store/types.rs (L262-290)
```rust
    pub fn verify(&self) -> anyhow::Result<()> {
        ensure!(
            self.payload.author() == self.author(),
            "Payload author doesn't match the info"
        );
        ensure!(
            self.payload.hash() == *self.digest(),
            "Payload hash doesn't match the digest"
        );
        ensure!(
            self.payload.num_txns() as u64 == self.num_txns(),
            "Payload num txns doesn't match batch info"
        );
        ensure!(
            self.payload.num_bytes() as u64 == self.num_bytes(),
            "Payload num bytes doesn't match batch info"
        );
        for txn in self.payload.txns() {
            ensure!(
                txn.gas_unit_price() >= self.gas_bucket_start(),
                "Payload gas unit price doesn't match batch info"
            );
            ensure!(
                !txn.payload().is_encrypted_variant(),
                "Encrypted transaction is not supported yet"
            );
        }
        Ok(())
    }
```

**File:** consensus/src/quorum_store/types.rs (L433-461)
```rust
    pub fn verify(
        &self,
        peer_id: PeerId,
        max_num_batches: usize,
        verifier: &ValidatorVerifier,
    ) -> anyhow::Result<()> {
        ensure!(!self.batches.is_empty(), "Empty message");
        ensure!(
            self.batches.len() <= max_num_batches,
            "Too many batches: {} > {}",
            self.batches.len(),
            max_num_batches
        );
        let epoch_authors = verifier.address_to_validator_index();
        for batch in self.batches.iter() {
            ensure!(
                epoch_authors.contains_key(&batch.author()),
                "Invalid author {} for batch {} in current epoch",
                batch.author(),
                batch.digest()
            );
            ensure!(
                batch.author() == peer_id,
                "Batch author doesn't match sender"
            );
            batch.verify()?
        }
        Ok(())
    }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L173-244)
```rust
    pub(crate) async fn handle_batches_msg(
        &mut self,
        author: PeerId,
        batches: Vec<Batch<BatchInfoExt>>,
    ) {
        if let Err(e) = self.ensure_max_limits(&batches) {
            error!("Batch from {}: {}", author, e);
            counters::RECEIVED_BATCH_MAX_LIMIT_FAILED.inc();
            return;
        }

        let Some(batch) = batches.first() else {
            error!("Empty batch received from {}", author.short_str().as_str());
            return;
        };

        // Filter the transactions in the batches. If any transaction is rejected,
        // the message will be dropped, and all batches will be rejected.
        if self.transaction_filter_config.is_enabled() {
            let transaction_filter = &self.transaction_filter_config.batch_transaction_filter();
            for batch in batches.iter() {
                for transaction in batch.txns() {
                    if !transaction_filter.allows_transaction(
                        batch.batch_info().batch_id(),
                        batch.author(),
                        batch.digest(),
                        transaction,
                    ) {
                        error!(
                            "Transaction {}, in batch {}, from {}, was rejected by the filter. Dropping {} batches!",
                            transaction.committed_hash(),
                            batch.batch_info().batch_id(),
                            author.short_str().as_str(),
                            batches.len()
                        );
                        counters::RECEIVED_BATCH_REJECTED_BY_FILTER.inc();
                        return;
                    }
                }
            }
        }

        let approx_created_ts_usecs = batch
            .info()
            .expiration()
            .saturating_sub(self.batch_expiry_gap_when_init_usecs);

        if approx_created_ts_usecs > 0 {
            observe_batch(
                approx_created_ts_usecs,
                batch.author(),
                BatchStage::RECEIVED,
            );
        }

        let mut persist_requests = vec![];
        for batch in batches.into_iter() {
            // TODO: maybe don't message batch generator if the persist is unsuccessful?
            if let Err(e) = self
                .sender_to_batch_generator
                .send(BatchGeneratorCommand::RemoteBatch(batch.clone()))
                .await
            {
                warn!("Failed to send batch to batch generator: {}", e);
            }
            persist_requests.push(batch.into());
        }
        counters::RECEIVED_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        if author != self.my_peer_id {
            counters::RECEIVED_REMOTE_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        }
        self.persist_and_send_digests(persist_requests, approx_created_ts_usecs);
```

**File:** consensus/src/quorum_store/batch_generator.rs (L392-401)
```rust
    pub(crate) fn handle_remote_batch(
        &mut self,
        author: PeerId,
        batch_id: BatchId,
        txns: Vec<SignedTransaction>,
    ) {
        let expiry_time_usecs = aptos_infallible::duration_since_epoch().as_micros() as u64
            + self.config.remote_batch_expiry_gap_when_init_usecs;
        self.insert_batch(author, batch_id, txns, expiry_time_usecs);
    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L642-646)
```rust
                                    .filter(|txn_summary| {
                                        !filtered_txns.contains(txn_summary)
                                            && block_timestamp.as_secs()
                                                < txn_summary.expiration_timestamp_secs
                                    })
```

**File:** consensus/src/epoch_manager.rs (L1583-1584)
```rust
            let max_batch_expiry_gap_usecs =
                self.config.quorum_store.batch_expiry_gap_when_init_usecs;
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L469-479)
```rust
        if self.expiration()
            > aptos_infallible::duration_since_epoch().as_micros() as u64
                + max_batch_expiry_gap_usecs
        {
            bail!(
                "Batch expiration too far in future: {} > {}",
                self.expiration(),
                aptos_infallible::duration_since_epoch().as_micros() as u64
                    + max_batch_expiry_gap_usecs
            );
        }
```

**File:** config/src/config/quorum_store_config.rs (L131-132)
```rust
            batch_expiry_gap_when_init_usecs: Duration::from_secs(60).as_micros() as u64,
            remote_batch_expiry_gap_when_init_usecs: Duration::from_millis(500).as_micros() as u64,
```
