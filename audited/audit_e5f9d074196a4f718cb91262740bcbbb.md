# Audit Report

## Title
DKG Network Stream Starvation via Biased select() Combinator

## Summary
The `select()` stream combinator in the DKG NetworkTask uses biased polling that prioritizes `network_events` over `self_receiver`, allowing a Byzantine validator to prevent target validators from processing self-sent messages required by the ReliableBroadcast protocol, causing DKG session failures.

## Finding Description

The DKG (Distributed Key Generation) network layer uses a biased stream combinator that violates fairness guarantees required for the ReliableBroadcast protocol. [1](#0-0) 

The `futures::stream::select()` combinator implements **biased polling** - it preferentially polls the first stream (`network_events`) and only polls the second stream (`self_receiver`) when the first returns `Poll::Pending`. This creates a starvation vulnerability.

**Why this matters for DKG:**

The DKG protocol uses ReliableBroadcast to disseminate transcript requests to all validators, including sending a message to itself: [2](#0-1) 

When a validator sends an RPC to itself, it routes through the `self_sender` channel: [3](#0-2) 

The self-message is placed in `self_receiver` and must be processed by the NetworkTask to complete the RPC. However, if `network_events` continuously has messages ready (due to flooding by a Byzantine validator), the biased `select()` will never poll `self_receiver`.

**Attack Path:**

1. Byzantine validator floods target with authenticated DKG RPC requests (validators have mutual authentication on the validator network)
2. The `network_events` stream continuously has items ready
3. Due to biased polling, `self_receiver` is never polled
4. Target validator attempts ReliableBroadcast, sending self-RPC
5. Self-message waits in `self_receiver` but is never processed
6. RPC times out after 1 second (default configuration): [4](#0-3) 

7. ReliableBroadcast retries with exponential backoff (100ms → 200ms → 400ms → 800ms → 1600ms → 3000ms max)
8. All retries timeout due to continued starvation
9. DKG transcript aggregation fails for the target validator
10. If enough validators are targeted, DKG session cannot complete, preventing epoch transition [5](#0-4) 

The same vulnerability exists in the consensus layer and JWK consensus, which use identical patterns: [6](#0-5) [7](#0-6) 

## Impact Explanation

This vulnerability achieves **Medium severity** per Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: A sustained attack during DKG prevents affected validators from completing the protocol, requiring manual intervention or extended epoch duration
- **Liveness impact**: While not complete network unavailability, preventing DKG completion blocks epoch transitions, which are critical for validator set updates, on-chain randomness, and protocol upgrades
- **Violates Byzantine Fault Tolerance**: A single Byzantine validator can DoS another validator's DKG participation, when the protocol should tolerate up to 1/3 Byzantine validators

The impact is limited to Medium (not High/Critical) because:
- Does not cause fund loss or consensus safety violations
- Does not cause permanent network partition
- Requires sustained attack during specific DKG windows
- Attack must target multiple validators to completely prevent epoch transition

## Likelihood Explanation

**Likelihood: Medium-High**

**Attacker Requirements:**
- Must be a validator with network authentication (requires stake)
- Must identify DKG session timing
- Must sustain message flooding during DKG window

**Feasibility:**
- Byzantine validators are within the threat model (< 1/3 assumption)
- No technical complexity - simply send valid DKG TranscriptRequest messages repeatedly
- The biased polling behavior is deterministic and always occurs
- DKG sessions happen at every epoch transition (regular occurrence)

**Mitigating Factors:**
- Requires validator access (stake requirement)
- Must be timed during DKG sessions
- May trigger monitoring alerts if excessive RPC rates are detected

## Recommendation

Replace the biased `select()` combinator with a fair stream merger. The Rust futures ecosystem does not provide a built-in fair select for streams, so implement round-robin or weighted fair polling.

**Option 1: Manual fair polling using tokio::select! with round-robin**

```rust
// In NetworkTask::new()
let all_events = /* create custom fair stream */;

// In NetworkTask::start() - rewrite to use tokio::select! with manual fairness
let mut network_events_fused = network_events.fuse();
let mut self_receiver_fused = self_receiver.fuse();
let mut prefer_self = false; // Round-robin flag

while !shutdown {
    let message = if prefer_self {
        tokio::select! {
            biased; // Explicitly biased, but we control the order
            msg = self_receiver_fused.next() => {
                prefer_self = false;
                msg
            }
            msg = network_events_fused.next() => {
                prefer_self = true;
                msg
            }
        }
    } else {
        tokio::select! {
            biased;
            msg = network_events_fused.next() => {
                prefer_self = true;
                msg
            }
            msg = self_receiver_fused.next() => {
                prefer_self = false;
                msg
            }
        }
    };
    // Process message...
}
```

**Option 2: Add prioritization for self_receiver**

Implement a custom stream that gives self_receiver higher priority or guaranteed polling frequency to prevent starvation.

**Option 3: Separate task for self_receiver**

Process `self_receiver` in a dedicated task to guarantee it's never starved, sending messages to the same `rpc_tx` channel.

The same fix should be applied to consensus and JWK consensus network layers.

## Proof of Concept

```rust
// Proof of Concept: Demonstrate self-receiver starvation
// This would be added as a test in dkg/src/network.rs

#[tokio::test]
async fn test_self_receiver_starvation() {
    use futures::stream::{self, StreamExt};
    use aptos_channels::aptos_channel;
    use tokio::time::{Duration, timeout};
    
    // Create channels simulating network_events and self_receiver
    let (network_tx, network_rx) = aptos_channel::new(QueueStyle::FIFO, 100, None);
    let (self_tx, self_rx) = aptos_channel::new(QueueStyle::FIFO, 10, None);
    
    // Create biased select (same as production code)
    let mut all_events = Box::new(select(network_rx, self_rx));
    
    // Flood network_events with messages
    let flood_handle = tokio::spawn(async move {
        for i in 0..1000 {
            let _ = network_tx.push((), format!("network_msg_{}", i));
            tokio::time::sleep(Duration::from_micros(100)).await;
        }
    });
    
    // Send one self message
    let _ = self_tx.push((), "self_msg".to_string());
    
    // Try to receive the self message within reasonable time
    let mut received_self = false;
    let mut network_count = 0;
    
    let result = timeout(Duration::from_secs(2), async {
        while let Some(msg) = all_events.next().await {
            if msg.contains("self_msg") {
                received_self = true;
                break;
            }
            network_count += 1;
            if network_count > 100 {
                break; // Processed 100 network messages without seeing self message
            }
        }
    }).await;
    
    flood_handle.abort();
    
    // Assert that self message was starved
    assert!(!received_self, "Self message should be starved by network flood");
    assert!(network_count > 50, "Should process many network messages while starving self");
}
```

**Notes:**

This vulnerability affects the liveness guarantees of the DKG protocol. While Byzantine validators are expected in the threat model, the system should maintain progress with up to 1/3 Byzantine validators. A single Byzantine validator being able to prevent another validator from completing DKG violates this assumption.

The issue is particularly concerning during epoch transitions, where DKG is critical for generating shared randomness and updating the validator set. Sustained attacks could delay epoch transitions indefinitely, requiring manual intervention.

The same pattern exists in consensus and JWK consensus, indicating a systemic issue with stream handling in network layers that rely on self-messaging for ReliableBroadcast protocols.

### Citations

**File:** dkg/src/network.rs (L69-80)
```rust
        if receiver == self.author() {
            let (tx, rx) = oneshot::channel();
            let protocol = RPC[0];
            let self_msg = Event::RpcRequest(self.author, msg.clone(), RPC[0], tx);
            self.self_sender.clone().send(self_msg).await?;
            if let Ok(Ok(Ok(bytes))) = timeout(timeout_duration, rx).await {
                let response_msg =
                    tokio::task::spawn_blocking(move || protocol.from_bytes(&bytes)).await??;
                Ok(response_msg)
            } else {
                bail!("self rpc failed");
            }
```

**File:** dkg/src/network.rs (L150-153)
```rust
        // Collect all the network events into a single stream
        let network_events: Vec<_> = network_and_events.into_values().collect();
        let network_events = select_all(network_events).fuse();
        let all_events = Box::new(select(network_events, self_receiver));
```

**File:** crates/reliable-broadcast/src/lib.rs (L146-147)
```rust
                    let send_fut = if receiver == self_author {
                        network_sender.send_rb_rpc(receiver, message, rpc_timeout_duration)
```

**File:** config/src/config/dag_consensus_config.rs (L120-120)
```rust
            rpc_timeout_ms: 1000,
```

**File:** dkg/src/agg_trx_producer.rs (L64-67)
```rust
            let agg_trx = rb
                .broadcast(req, agg_state)
                .await
                .expect("broadcast cannot fail");
```

**File:** consensus/src/network.rs (L782-782)
```rust
        let all_events = Box::new(select(network_events, self_receiver));
```

**File:** crates/aptos-jwk-consensus/src/network.rs (L181-181)
```rust
        let all_events = Box::new(select(network_events, self_receiver));
```
