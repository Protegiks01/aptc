# Audit Report

## Title
DAG Consensus Node Crash Due to Unvalidated Duplicate Pipeline Backpressure Latency Limits

## Summary
The `DagHealthConfig.pipeline_backpressure_config` field accepts duplicate `back_pressure_pipeline_latency_limit_ms` values through YAML configuration without validation. At runtime during DAG consensus initialization, these duplicate values cause a panic in `PipelineBackpressureConfig::new()`, crashing the validator node and preventing consensus participation.

## Finding Description

The vulnerability exists in the configuration validation and runtime initialization flow for DAG consensus:

**Configuration Structure**: [1](#0-0) 

The `DagHealthConfig` contains a `pipeline_backpressure_config` field that is a vector of `PipelineBackpressureValues`. Each value has a `back_pressure_pipeline_latency_limit_ms` field that should be unique.

**Missing Validation**: [2](#0-1) 

The `DagConsensusConfig::sanitize()` method only validates `DagPayloadConfig` and does not check for duplicate latency limits in `pipeline_backpressure_config`.

**PipelineBackpressureValues Definition**: [3](#0-2) 

**Runtime Panic**: [4](#0-3) 

During initialization, `PipelineBackpressureConfig::new()` converts the vector to a `BTreeMap` using `back_pressure_pipeline_latency_limit_ms` as the key. Line 122 contains an assertion that panics if duplicate keys exist (causing the BTreeMap to have fewer entries than the original vector).

**Call Site**: [5](#0-4) 

The panic occurs during DAG consensus bootstrap when initializing `PipelineLatencyBasedBackpressure`.

**Attack Path**:
1. Validator operator creates a YAML config with duplicate `back_pressure_pipeline_latency_limit_ms` values (e.g., two entries with 1200ms)
2. Config passes sanitization checks (no validation exists)
3. Node starts and begins DAG consensus initialization
4. `PipelineBackpressureConfig::new()` is called
5. Assertion fails on line 122, causing a panic
6. Validator node crashes and cannot participate in consensus

This is identical to the pattern found in `ChainHealthBackoffConfig`: [6](#0-5) 

## Impact Explanation

This is a **High Severity** vulnerability (per Aptos bug bounty categories: "Validator node slowdowns, API crashes, Significant protocol violations").

**Impact**:
- Validator node unavailability: The node crashes during startup and cannot participate in consensus
- Consensus liveness degradation: If multiple validators have this misconfiguration, network liveness could be impacted
- Non-recoverable without config fix: The node will continue to crash on restart until the configuration is corrected
- Production disruption: Operators would need emergency intervention to fix configs and restart nodes

While this requires configuration access, it represents a significant robustness failure where:
1. Invalid configurations pass validation but cause runtime panics
2. The failure mode is a crash rather than graceful error handling
3. There's no clear error message indicating the configuration issue

## Likelihood Explanation

**Likelihood: Medium**

This can occur through:
1. **Honest misconfiguration**: Operators copying/pasting config sections may accidentally create duplicates
2. **Config management errors**: Automated config generation tools could produce invalid configs
3. **Insider threat**: A malicious insider with config access could deliberately crash nodes
4. **Compromised config files**: If an attacker gains access to validator configs, they could sabotage nodes

The vulnerability is particularly concerning because:
- Configuration errors are common in production systems
- The error is not caught at config load time (fails fast principle violation)
- No warning or validation exists to prevent this mistake
- Similar pattern exists elsewhere in the codebase (ChainHealthBackoffConfig)

## Recommendation

Add validation to `DagConsensusConfig::sanitize()` to check for duplicate `back_pressure_pipeline_latency_limit_ms` values:

```rust
impl ConfigSanitizer for DagConsensusConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        
        DagPayloadConfig::sanitize(node_config, node_type, chain_id)?;
        
        // Validate pipeline_backpressure_config for duplicate latency limits
        let mut latency_limits = std::collections::HashSet::new();
        for value in &node_config.dag_consensus.health_config.pipeline_backpressure_config {
            if !latency_limits.insert(value.back_pressure_pipeline_latency_limit_ms) {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    format!(
                        "Duplicate back_pressure_pipeline_latency_limit_ms: {}",
                        value.back_pressure_pipeline_latency_limit_ms
                    ),
                ));
            }
        }
        
        // Validate chain_backoff_config for duplicate voting power percentages
        let mut voting_power_percentages = std::collections::HashSet::new();
        for value in &node_config.dag_consensus.health_config.chain_backoff_config {
            if !voting_power_percentages.insert(
                value.backoff_if_below_participating_voting_power_percentage
            ) {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    format!(
                        "Duplicate backoff_if_below_participating_voting_power_percentage: {}",
                        value.backoff_if_below_participating_voting_power_percentage
                    ),
                ));
            }
        }

        Ok(())
    }
}
```

Additionally, consider replacing the `assert_eq!` with a proper error return in `PipelineBackpressureConfig::new()` and `ChainHealthBackoffConfig::new()` for more graceful error handling.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_config::config::{DagConsensusConfig, NodeConfig, PipelineBackpressureValues};
    
    #[test]
    #[should_panic(expected = "assertion failed")]
    fn test_duplicate_pipeline_backpressure_latency_limits_cause_panic() {
        // Create a node config with duplicate latency limits
        let node_config = NodeConfig {
            dag_consensus: DagConsensusConfig {
                health_config: DagHealthConfig {
                    pipeline_backpressure_config: vec![
                        PipelineBackpressureValues {
                            back_pressure_pipeline_latency_limit_ms: 1200,
                            max_sending_block_txns_after_filtering_override: 1800,
                            max_sending_block_bytes_override: 5 * 1024 * 1024,
                            backpressure_proposal_delay_ms: 50,
                        },
                        PipelineBackpressureValues {
                            // DUPLICATE: Same latency limit as above
                            back_pressure_pipeline_latency_limit_ms: 1200,
                            max_sending_block_txns_after_filtering_override: 1000,
                            max_sending_block_bytes_override: 3 * 1024 * 1024,
                            backpressure_proposal_delay_ms: 100,
                        },
                    ],
                    ..Default::default()
                },
                ..Default::default()
            },
            ..Default::default()
        };
        
        // This should pass sanitization (no validation exists)
        let sanitize_result = DagConsensusConfig::sanitize(
            &node_config,
            NodeType::Validator,
            None,
        );
        assert!(sanitize_result.is_ok(), "Config should pass sanitization");
        
        // But runtime initialization will panic
        let config = PipelineBackpressureConfig::new(
            node_config.dag_consensus.health_config.pipeline_backpressure_config.clone(),
            None,
        );
        // Panic occurs here due to assertion failure
    }
}
```

## Notes

This vulnerability follows the same pattern as `ChainHealthBackoffConfig`, suggesting a systematic issue in configuration validation across the codebase. Both should be addressed to improve system robustness. The fix should be applied to both DAG consensus and traditional consensus configurations to prevent similar issues.

### Citations

**File:** config/src/config/dag_consensus_config.rs (L141-155)
```rust
pub struct DagHealthConfig {
    pub chain_backoff_config: Vec<ChainHealthBackoffValues>,
    pub voter_pipeline_latency_limit_ms: u64,
    pub pipeline_backpressure_config: Vec<PipelineBackpressureValues>,
}

impl Default for DagHealthConfig {
    fn default() -> Self {
        Self {
            chain_backoff_config: Vec::new(),
            voter_pipeline_latency_limit_ms: 30_000,
            pipeline_backpressure_config: Vec::new(),
        }
    }
}
```

**File:** config/src/config/dag_consensus_config.rs (L169-179)
```rust
impl ConfigSanitizer for DagConsensusConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        DagPayloadConfig::sanitize(node_config, node_type, chain_id)?;

        Ok(())
    }
}
```

**File:** config/src/config/consensus_config.rs (L196-208)
```rust
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
pub struct PipelineBackpressureValues {
    // At what latency does this backpressure level activate
    pub back_pressure_pipeline_latency_limit_ms: u64,
    pub max_sending_block_txns_after_filtering_override: u64,
    pub max_sending_block_bytes_override: u64,
    // If there is backpressure, giving some more breathing room to go through the backlog,
    // and making sure rounds don't go extremely fast (even if they are smaller blocks)
    // Set to a small enough value, so it is unlikely to affect proposer being able to finish the round in time.
    // If we want to dynamically increase it beyond quorum_store_poll_time,
    // we need to adjust timeouts other nodes use for the backpressured round.
    pub backpressure_proposal_delay_ms: u64,
}
```

**File:** consensus/src/liveness/proposal_generator.rs (L58-66)
```rust
    pub fn new(backoffs: Vec<ChainHealthBackoffValues>) -> Self {
        let original_len = backoffs.len();
        let backoffs = backoffs
            .into_iter()
            .map(|v| (v.backoff_if_below_participating_voting_power_percentage, v))
            .collect::<BTreeMap<_, _>>();
        assert_eq!(original_len, backoffs.len());
        Self { backoffs }
    }
```

**File:** consensus/src/liveness/proposal_generator.rs (L113-127)
```rust
    pub fn new(
        backoffs: Vec<PipelineBackpressureValues>,
        execution: Option<ExecutionBackpressureConfig>,
    ) -> Self {
        let original_len = backoffs.len();
        let backoffs = backoffs
            .into_iter()
            .map(|v| (v.back_pressure_pipeline_latency_limit_ms, v))
            .collect::<BTreeMap<_, _>>();
        assert_eq!(original_len, backoffs.len());
        Self {
            backoffs,
            execution,
        }
    }
```

**File:** consensus/src/dag/bootstrap.rs (L622-633)
```rust
        let pipeline_health = PipelineLatencyBasedBackpressure::new(
            Duration::from_millis(self.config.health_config.voter_pipeline_latency_limit_ms),
            PipelineBackpressureConfig::new(
                self.config
                    .health_config
                    .pipeline_backpressure_config
                    .clone(),
                // TODO: add pipeline backpressure based on execution speed to DAG config
                None,
            ),
            ordered_notifier.clone(),
        );
```
