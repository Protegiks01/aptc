# Audit Report

## Title
Resource Exhaustion via Stale Active Subscriber Metadata in Consensus Publisher

## Summary
The consensus publisher maintains an in-memory set of active subscribers that is only synchronized with actual peer connection states via periodic garbage collection (default: 60 seconds). This creates a vulnerability window where disconnected peers remain in the active subscriber set, causing validators to waste CPU resources on serializing consensus messages for peers that cannot receive them.

## Finding Description
The vulnerability exists in the consensus publisher's subscription management mechanism. When a peer subscribes to consensus updates, they are added to the `active_subscribers` HashSet. [1](#0-0) 

However, when a peer disconnects, they are NOT immediately removed from this set. Instead, removal only occurs during periodic garbage collection. [2](#0-1) 

The garbage collection interval is configured at 60 seconds by default. [3](#0-2) 

During message publication, the publisher iterates through all `active_subscribers` and queues messages for serialization. [4](#0-3) 

The serialization happens in background tasks with bounded parallelism. [5](#0-4) 

**Attack Flow:**
1. Attacker connects to validator and sends a Subscribe RPC
2. Validator adds attacker to `active_subscribers` 
3. Attacker immediately disconnects
4. For the next 60 seconds, the validator continues to:
   - Queue consensus messages for the disconnected peer
   - Serialize messages in parallel tasks (consuming CPU)
   - Attempt to send via `send_serialized_message_to_peer()` (which fails silently)
5. Attacker can repeat with multiple peer identities to amplify resource consumption

The actual network send fails when the peer is disconnected. [6](#0-5) 

However, by the time the send is attempted, CPU resources have already been consumed on serialization, and memory has been used for queuing.

## Impact Explanation
This qualifies as **Medium severity** under the Aptos bug bounty criteria for the following reasons:

1. **Validator Performance Degradation**: Validators running the consensus publisher waste CPU resources on serializing messages for disconnected peers. With many disconnected subscribers, this can cause measurable performance impact.

2. **Resource Exhaustion Attack Vector**: An attacker can amplify the impact by:
   - Using multiple peer identities
   - Repeatedly connecting/disconnecting
   - Timing disconnections to maximize the 60-second windows

3. **Bounded but Exploitable**: While the impact is bounded by `max_parallel_serialization_tasks` and `max_network_channel_size`, the 60-second window allows sustained resource waste that accumulates across multiple disconnected peers.

This does not meet Critical or High severity because:
- No funds are at risk
- Consensus safety/liveness is not compromised
- The validator continues to operate (just less efficiently)

## Likelihood Explanation
**Likelihood: High**

- **Low attack complexity**: Attacker only needs to establish network connections and send Subscribe RPCs
- **No special privileges required**: Any peer can subscribe to consensus updates
- **Easy to repeat**: Attacker can automate connect/disconnect cycles
- **Natural occurrence**: Legitimate peer disconnections (network issues, restarts) also trigger this behavior, meaning the vulnerability activates even without malicious intent

## Recommendation
**Solution 1: Proactive Connection State Verification**

Before queuing messages for serialization, verify that peers are still connected by checking the current connection state from `PeersAndMetadata`:

```rust
pub fn publish_message(&self, message: ConsensusObserverDirectSend) {
    // Get connected peers
    let connected_peers = match self.consensus_observer_client
        .get_peers_and_metadata()
        .get_connected_peers_and_metadata() {
        Ok(peers) => peers.keys().cloned().collect::<HashSet<_>>(),
        Err(_) => HashSet::new(),
    };
    
    // Filter active subscribers to only connected peers
    let active_subscribers = self.get_active_subscribers();
    let connected_subscribers: Vec<_> = active_subscribers
        .intersection(&connected_peers)
        .cloned()
        .collect();
    
    // Send only to connected subscribers
    for peer_network_id in connected_subscribers {
        // ... rest of sending logic
    }
}
```

**Solution 2: Reduce Garbage Collection Interval**

Set `garbage_collection_interval_ms` to a much lower value (e.g., 5 seconds) to minimize the staleness window.

**Solution 3: Subscribe to Connection Notifications**

Use the `PeersAndMetadata::subscribe()` mechanism to receive immediate notifications of peer disconnections and proactively remove them from `active_subscribers`.

## Proof of Concept

```rust
// This PoC demonstrates the vulnerability
// Add to consensus/src/consensus_observer/publisher/consensus_publisher.rs test module

#[tokio::test]
async fn test_stale_subscriber_resource_waste() {
    use std::time::{Duration, Instant};
    
    // Create a network client with a single peer
    let network_id = NetworkId::Public;
    let peers_and_metadata = PeersAndMetadata::new(&[network_id]);
    let network_client = NetworkClient::new(
        vec![], vec![], hashmap![], peers_and_metadata.clone()
    );
    let consensus_observer_client = Arc::new(ConsensusObserverClient::new(network_client));
    
    // Create consensus publisher with default config (60s garbage collection)
    let (consensus_publisher, mut outbound_receiver) = ConsensusPublisher::new(
        ConsensusObserverConfig::default(),
        consensus_observer_client,
    );
    
    // Peer subscribes
    let peer = PeerNetworkId::new(network_id, PeerId::random());
    let conn_metadata = ConnectionMetadata::mock(peer.peer_id());
    peers_and_metadata.insert_connection_metadata(peer, conn_metadata.clone()).unwrap();
    process_subscription_for_peer(&consensus_publisher, &peer);
    
    // Peer disconnects (update connection state)
    peers_and_metadata.update_connection_state(
        peer, 
        ConnectionState::Disconnected
    ).unwrap();
    
    // Publish 100 messages to demonstrate resource waste
    let start = Instant::now();
    for _ in 0..100 {
        let msg = ConsensusObserverMessage::new_ordered_block_message(
            vec![],
            LedgerInfoWithSignatures::new(
                LedgerInfo::new(BlockInfo::empty(), HashValue::zero()),
                AggregateSignature::empty(),
            ),
        );
        consensus_publisher.publish_message(msg);
    }
    
    // Messages are queued for disconnected peer
    // They will be serialized (wasting CPU) then fail to send
    let mut count = 0;
    while let Some(_) = outbound_receiver.next().await {
        count += 1;
        if count >= 100 {
            break;
        }
    }
    
    let elapsed = start.elapsed();
    println!("Wasted {} seconds serializing messages for disconnected peer", 
             elapsed.as_secs_f64());
    
    // Verify peer is still in active_subscribers (stale!)
    assert!(consensus_publisher.get_active_subscribers().contains(&peer));
}
```

## Notes
The vulnerability is confirmed in the codebase. The peer metadata retrieved via `get_peers_and_metadata()` is properly cached and updated, [7](#0-6)  but the consensus publisher maintains a separate `active_subscribers` set that only synchronizes via periodic garbage collection. This architectural separation creates the staleness window that enables resource exhaustion attacks.

### Citations

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L93-96)
```rust
    /// Adds the given subscriber to the set of active subscribers
    fn add_active_subscriber(&self, peer_network_id: PeerNetworkId) {
        self.active_subscribers.write().insert(peer_network_id);
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L98-155)
```rust
    /// Garbage collect inactive subscriptions by removing peers that are no longer connected
    fn garbage_collect_subscriptions(&self) {
        // Get the set of active subscribers
        let active_subscribers = self.get_active_subscribers();

        // Get the connected peers and metadata
        let peers_and_metadata = self.consensus_observer_client.get_peers_and_metadata();
        let connected_peers_and_metadata =
            match peers_and_metadata.get_connected_peers_and_metadata() {
                Ok(connected_peers_and_metadata) => connected_peers_and_metadata,
                Err(error) => {
                    // We failed to get the connected peers and metadata
                    warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::UnexpectedError)
                        .message(&format!(
                            "Failed to get connected peers and metadata! Error: {:?}",
                            error
                        )));
                    return;
                },
            };

        // Identify the active subscribers that are no longer connected
        let connected_peers: HashSet<PeerNetworkId> =
            connected_peers_and_metadata.keys().cloned().collect();
        let disconnected_subscribers: HashSet<PeerNetworkId> = active_subscribers
            .difference(&connected_peers)
            .cloned()
            .collect();

        // Remove any subscriptions from peers that are no longer connected
        for peer_network_id in &disconnected_subscribers {
            self.remove_active_subscriber(peer_network_id);
            info!(LogSchema::new(LogEntry::ConsensusPublisher)
                .event(LogEvent::Subscription)
                .message(&format!(
                    "Removed peer subscription due to disconnection! Peer: {:?}",
                    peer_network_id
                )));
        }

        // Update the number of active subscribers for each network
        let active_subscribers = self.get_active_subscribers();
        for network_id in peers_and_metadata.get_registered_networks() {
            // Calculate the number of active subscribers for the network
            let num_active_subscribers = active_subscribers
                .iter()
                .filter(|peer_network_id| peer_network_id.network_id() == network_id)
                .count() as i64;

            // Update the active subscriber metric
            metrics::set_gauge(
                &metrics::PUBLISHER_NUM_ACTIVE_SUBSCRIBERS,
                &network_id,
                num_active_subscribers,
            );
        }
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L210-232)
```rust
    /// Publishes a direct send message to all active subscribers. Note: this method
    /// is non-blocking (to avoid blocking callers during publishing, e.g., consensus).
    pub fn publish_message(&self, message: ConsensusObserverDirectSend) {
        // Get the active subscribers
        let active_subscribers = self.get_active_subscribers();

        // Send the message to all active subscribers
        for peer_network_id in &active_subscribers {
            // Send the message to the outbound receiver for publishing
            let mut outbound_message_sender = self.outbound_message_sender.clone();
            if let Err(error) =
                outbound_message_sender.try_send((*peer_network_id, message.clone()))
            {
                // The message send failed
                warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::SendDirectSendMessage)
                        .message(&format!(
                            "Failed to send outbound message to the receiver for peer {:?}! Error: {:?}",
                            peer_network_id, error
                    )));
            }
        }
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L277-350)
```rust
/// Spawns a message serialization task that serializes outbound publisher
/// messages in parallel but guarantees in order sends to the receiver.
fn spawn_message_serializer_and_sender(
    consensus_observer_client: Arc<
        ConsensusObserverClient<NetworkClient<ConsensusObserverMessage>>,
    >,
    consensus_observer_config: ConsensusObserverConfig,
    outbound_message_receiver: mpsc::Receiver<(PeerNetworkId, ConsensusObserverDirectSend)>,
) {
    tokio::spawn(async move {
        // Create the message serialization task
        let consensus_observer_client_clone = consensus_observer_client.clone();
        let serialization_task =
            outbound_message_receiver.map(move |(peer_network_id, message)| {
                // Spawn a new blocking task to serialize the message
                let consensus_observer_client_clone = consensus_observer_client_clone.clone();
                tokio::task::spawn_blocking(move || {
                    let message_label = message.get_label();
                    let serialized_message = consensus_observer_client_clone
                        .serialize_message_for_peer(&peer_network_id, message);
                    (peer_network_id, serialized_message, message_label)
                })
            });

        // Execute the serialization task with in-order buffering
        let consensus_observer_client_clone = consensus_observer_client.clone();
        serialization_task
            .buffered(consensus_observer_config.max_parallel_serialization_tasks)
            .map(|serialization_result| {
                // Attempt to send the serialized message to the peer
                match serialization_result {
                    Ok((peer_network_id, serialized_message, message_label)) => {
                        match serialized_message {
                            Ok(serialized_message) => {
                                // Send the serialized message to the peer
                                if let Err(error) = consensus_observer_client_clone
                                    .send_serialized_message_to_peer(
                                        &peer_network_id,
                                        serialized_message,
                                        message_label,
                                    )
                                {
                                    // We failed to send the message
                                    warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                                        .event(LogEvent::SendDirectSendMessage)
                                        .message(&format!(
                                            "Failed to send message to peer: {:?}. Error: {:?}",
                                            peer_network_id, error
                                        )));
                                }
                            },
                            Err(error) => {
                                // We failed to serialize the message
                                warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                                    .event(LogEvent::SendDirectSendMessage)
                                    .message(&format!(
                                        "Failed to serialize message for peer: {:?}. Error: {:?}",
                                        peer_network_id, error
                                    )));
                            },
                        }
                    },
                    Err(error) => {
                        // We failed to spawn the serialization task
                        warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                            .event(LogEvent::SendDirectSendMessage)
                            .message(&format!("Failed to spawn the serializer task: {:?}", error)));
                    },
                }
            })
            .collect::<()>()
            .await;
    });
}
```

**File:** config/src/config/consensus_observer_config.rs (L71-71)
```rust
            garbage_collection_interval_ms: 60_000,            // 60 seconds
```

**File:** network/framework/src/peer_manager/mod.rs (L538-546)
```rust
        } else {
            warn!(
                NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                protocol_id = %protocol_id,
                "{} Can't send message to peer.  Peer {} is currently not connected",
                self.network_context,
                peer_id.short_str()
            );
        }
```

**File:** network/framework/src/application/storage.rs (L206-207)
```rust
        // Update the cached peers and metadata
        self.set_cached_peers_and_metadata(peers_and_metadata.clone());
```
