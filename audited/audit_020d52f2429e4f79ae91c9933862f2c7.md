# Audit Report

## Title
Node-Checker Execution Validation Gap: Health Checks Pass Despite Invalid Transaction Execution

## Summary
The Aptos node-checker's TPS and TransactionCorrectness checkers have complementary validation gaps that allow a node to pass health checks while producing invalid transaction execution results. The TPS checker only validates transaction commitment and success status without verifying execution correctness, while the TransactionCorrectness checker only samples a single historical transaction without validating ongoing execution.

## Finding Description

The node-checker implements two critical checkers for execution validation, but neither provides comprehensive verification:

**1. TPS Checker Validation Gap** [1](#0-0) 

The TPS checker emits transactions and validates throughput, checking only that `rate.committed >= minimum_tps`. The underlying transaction emitter performs status validation: [2](#0-1) 

This validation only checks `transaction_info.status().is_success()`, verifying the transaction didn't abort but NOT validating:
- Write set correctness (state_change_hash)
- Event correctness (event_root_hash)  
- State checkpoint hash accuracy
- Gas calculation accuracy
- Execution determinism

**2. TransactionCorrectness Checker Validation Gap** [3](#0-2) 

This checker only validates ONE transaction at `middle_shared_version` from the existing ledger, comparing only the `accumulator_root_hash` against a baseline. It does NOT:
- Validate the NEW transactions emitted by the TPS checker
- Perform continuous execution monitoring
- Check individual execution components (write sets, events, gas)
- Sample multiple transactions across the ledger

**3. Concurrent Execution Without Coordination** [4](#0-3) 

The checkers run concurrently via `try_join_all`, meaning they operate independently without coordinating their validation windows.

**Attack Scenario:**

A node with a buggy Move VM implementation could:
1. Execute transactions without aborting (status = "Keep(ExecutionStatus::Success)")
2. Produce INCORRECT write sets, events, or state changes due to VM bugs
3. Pass TPS checker: transactions are committed with success status, meeting throughput requirements
4. Pass TransactionCorrectness checker: the single sampled historical transaction was executed before the bug manifested

When MISCONFIGURED:
- `minimum_tps: 0` allows any throughput
- `required: false` on TransactionCorrectness means failures don't fail the overall check
- Missing baseline configuration skips TransactionCorrectness entirely

## Impact Explanation

This issue represents a **monitoring and observability gap** rather than a consensus safety violation. While serious, the impact is limited because:

1. **Consensus Layer Still Validates**: The actual consensus implementation properly validates execution via: [5](#0-4) [6](#0-5) 

These functions validate write sets, events, gas, and status comprehensively.

2. **False Positive, Not False Negative**: A validator with execution bugs would still be rejected by consensus but wouldn't detect the issue via node-checker health checks.

3. **Operational Impact**: Validator operators relying on node-checker would:
   - Not detect VM execution bugs early
   - Submit invalid proposals that get rejected
   - Experience poor performance without understanding why
   - Potentially face slashing or reputation damage

**Severity Assessment**: This qualifies as **Medium Severity** under the bug bounty program:
- Not a consensus safety violation (Critical)
- Not a direct validator slowdown (High)  
- Creates state monitoring inconsistencies requiring operational intervention (Medium)
- Validator operators may deploy buggy nodes believing they're healthy

## Likelihood Explanation

**Likelihood: Medium**

This gap would manifest when:
1. A node has a VM implementation bug causing incorrect execution
2. The bug doesn't cause transaction aborts (maintains "success" status)
3. Node-checker is used as the primary health validation
4. Configuration uses low `minimum_tps` or makes TransactionCorrectness non-required

This is realistic because:
- VM bugs producing incorrect but non-aborting execution are plausible
- Operators commonly configure lower TPS thresholds for non-mainnet environments
- The node-checker is explicitly designed as a health validation tool

## Recommendation

**Primary Fix**: Enhance the TPS checker to validate execution correctness:

```rust
// In ecosystem/node-checker/src/checker/tps.rs
// After line 148, add execution validation:

// Sample random transactions from those committed
let sample_size = min(10, stats.committed as usize);
let sample_versions: Vec<u64> = (/* select random versions from committed range */);

for version in sample_versions {
    let baseline_txn = baseline_client.get_transaction_by_version_bcs(version).await?;
    let target_txn = target_client.get_transaction_by_version_bcs(version).await?;
    
    // Compare TransactionInfo components
    if baseline_txn.state_change_hash != target_txn.state_change_hash ||
       baseline_txn.event_root_hash != target_txn.event_root_hash ||
       baseline_txn.gas_used != target_txn.gas_used {
        return Ok(vec![Self::build_result(
            "Transaction execution diverged from baseline".to_string(),
            0,
            format!("Transaction at version {} has incorrect execution results", version),
        )]);
    }
}
```

**Secondary Fix**: Enhance TransactionCorrectness to sample multiple transactions:

```rust
// In ecosystem/node-checker/src/checker/transaction_correctness.rs
// After line 167, sample multiple versions instead of one:

let sample_versions = vec![
    oldest_shared_version,
    middle_shared_version,
    latest_shared_version,
];

for version in sample_versions {
    // Validate each version...
}
```

**Configuration Guidance**: Document minimum safe configuration:
- `minimum_tps` should be > 0 and realistic for the network
- `required: true` for TransactionCorrectness in production
- Always configure baseline node for comparison

## Proof of Concept

```rust
// Test demonstrating the gap
#[tokio::test]
async fn test_node_checker_execution_gap() {
    // Setup: Create two nodes - one with correct execution, one with buggy VM
    let baseline_node = setup_node_with_correct_vm();
    let buggy_node = setup_node_with_buggy_vm(); // Produces wrong write sets but doesn't abort
    
    // Configure node-checker with minimal requirements
    let tps_config = TpsCheckerConfig {
        minimum_tps: 1, // Very low threshold
        ..Default::default()
    };
    
    let correctness_config = TransactionCorrectnessCheckerConfig {
        common: CommonCheckerConfig {
            required: false, // Non-required check
        },
    };
    
    // Run health check
    let result = run_node_checker(
        &baseline_node,
        &buggy_node,
        vec![
            CheckerConfig::Tps(tps_config),
            CheckerConfig::TransactionCorrectness(correctness_config),
        ],
    ).await;
    
    // BUG: Health check passes even though buggy_node produces invalid execution
    assert_eq!(result.summary_score, 100);
    
    // Verify buggy node actually produces wrong results
    let txn_on_baseline = baseline_node.get_transaction(version).await;
    let txn_on_buggy = buggy_node.get_transaction(version).await;
    assert_ne!(txn_on_baseline.write_set, txn_on_buggy.write_set); // Different execution!
}
```

## Notes

This is fundamentally a **monitoring system limitation** rather than a consensus protocol vulnerability. The Aptos consensus layer maintains proper execution validation through `ensure_match_transaction_info` and related mechanisms. However, node operators relying on the node-checker for health validation face a false sense of security when their nodes have execution bugs that don't cause transaction aborts. This represents a significant operational risk and monitoring gap that should be addressed to prevent validators from unknowingly running buggy nodes.

### Citations

**File:** ecosystem/node-checker/src/checker/tps.rs (L141-164)
```rust
        let stats = emit_transactions_with_cluster(
            &cluster,
            &self.config.emit_config,
            self.config
                .emit_workload_configs
                .args_to_transaction_mix_per_phase(),
        )
        .await
        .map_err(TpsCheckerError::TransactionEmitterError)?;

        // AKA stats per second.
        let rate = stats.rate();

        if rate.submitted < (self.config.minimum_tps as f64) {
            return Err(TpsCheckerError::InsufficientSubmittedTransactionsError(
                rate.submitted as u64,
                self.config.minimum_tps,
            )
            .into());
        }

        let mut description = format!("The minimum TPS (transactions per second) \
            required of nodes is {}, your node hit: {} (out of {} transactions submitted per second).", self.config.minimum_tps, rate.committed, rate.submitted);
        let evaluation_result = if rate.committed >= (self.config.minimum_tps as f64) {
```

**File:** crates/transaction-emitter-lib/src/emitter/transaction_executor.rs (L260-274)
```rust
        Ok(result) => {
            let transaction_info = &result.inner().info;
            if !transaction_info.status().is_success() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(60)),
                    warn_detailed_error("waiting on a", rest_client, txn, Ok(transaction_info))
                        .await
                );
                anyhow::bail!(
                    "Transaction failed execution with VM status {:?}",
                    transaction_info.status()
                );
            }
        },
    }
```

**File:** ecosystem/node-checker/src/checker/transaction_correctness.rs (L165-210)
```rust
        // Select a version in the middle of shared oldest and latest version.
        let middle_shared_version =
            (oldest_shared_version.saturating_add(latest_shared_version)) / 2;

        // We've asserted that both nodes are sufficiently up to date relative
        // to each other, we should be able to pull the same transaction from
        // both nodes.

        let middle_baseline_transaction = Self::get_transaction_by_version(
            &baseline_api_index_provider.client,
            middle_shared_version,
            "baseline",
        )
        .await?;
        let middle_baseline_accumulator_root_hash =
            Self::unwrap_accumulator_root_hash(&middle_baseline_transaction)?;

        let evaluation = match Self::get_transaction_by_version(
            &target_api_index_provider.client,
            middle_shared_version,
            "latest",
        )
        .await
        {
            Ok(middle_target_transaction) => {
                match Self::unwrap_accumulator_root_hash(&middle_target_transaction) {
                    Ok(middle_target_accumulator_root_hash) => {
                        if middle_baseline_accumulator_root_hash
                            == middle_target_accumulator_root_hash
                        {
                            Self::build_result(
                                "Target node produced valid recent transaction".to_string(),
                                100,
                                format!(
                                    "We were able to pull the same transaction (version: {}) \
                                    from both your node and the baseline node. Great! This \
                                    implies that your node is returning valid transaction data.",
                                    middle_shared_version,
                                ),
                            )
                        } else {
                            Self::build_result(
                                "Target node produced recent transaction, but it was invalid"
                                    .to_string(),
                                0,
                                format!(
```

**File:** ecosystem/node-checker/src/runner/sync_runner.rs (L155-164)
```rust
        // Call each of the Checkers without awaiting them yet.
        let mut futures = Vec::new();
        for checker in &self.checkers {
            futures.push(self.call_check(checker, &provider_collection));
        }

        // Run all the Checkers concurrently and collect their results.
        let check_results: Vec<CheckResult> =
            try_join_all(futures).await?.into_iter().flatten().collect();

```

**File:** execution/executor/src/chunk_executor/mod.rs (L592-651)
```rust
    fn verify_execution(
        &self,
        transactions: &[Transaction],
        persisted_aux_info: &[PersistedAuxiliaryInfo],
        transaction_infos: &[TransactionInfo],
        write_sets: &[WriteSet],
        event_vecs: &[Vec<ContractEvent>],
        begin_version: Version,
        end_version: Version,
        verify_execution_mode: &VerifyExecutionMode,
    ) -> Result<Version> {
        // Execute transactions.
        let parent_state = self.commit_queue.lock().latest_state().clone();
        let state_view = self.state_view(parent_state.latest())?;
        let txns = transactions
            .iter()
            .take((end_version - begin_version) as usize)
            .cloned()
            .map(|t| t.into())
            .collect::<Vec<SignatureVerifiedTransaction>>();

        let auxiliary_info = persisted_aux_info
            .iter()
            .take((end_version - begin_version) as usize)
            .map(|persisted_aux_info| AuxiliaryInfo::new(*persisted_aux_info, None))
            .collect::<Vec<_>>();
        // State sync executor shouldn't have block gas limit.
        let execution_output = DoGetExecutionOutput::by_transaction_execution::<V>(
            &V::new(),
            txns.into(),
            auxiliary_info,
            &parent_state,
            state_view,
            BlockExecutorConfigFromOnchain::new_no_block_limit(),
            TransactionSliceMetadata::chunk(begin_version, end_version),
        )?;
        // not `zip_eq`, deliberately
        for (version, txn_out, txn_info, write_set, events) in multizip((
            begin_version..end_version,
            &execution_output.to_commit.transaction_outputs,
            transaction_infos.iter(),
            write_sets.iter(),
            event_vecs.iter(),
        )) {
            if let Err(err) = txn_out.ensure_match_transaction_info(
                version,
                txn_info,
                Some(write_set),
                Some(events),
            ) {
                return if verify_execution_mode.is_lazy_quit() {
                    error!("(Not quitting right away.) {}", err);
                    verify_execution_mode.mark_seen_error();
                    Ok(version + 1)
                } else {
                    Err(err)
                };
            }
        }
        Ok(end_version)
```

**File:** types/src/transaction/mod.rs (L1869-1928)
```rust
    pub fn ensure_match_transaction_info(
        &self,
        version: Version,
        txn_info: &TransactionInfo,
        expected_write_set: Option<&WriteSet>,
        expected_events: Option<&[ContractEvent]>,
    ) -> Result<()> {
        const ERR_MSG: &str = "TransactionOutput does not match TransactionInfo";

        let expected_txn_status: TransactionStatus = txn_info.status().clone().into();
        ensure!(
            self.status() == &expected_txn_status,
            "{}: version:{}, status:{:?}, auxiliary data:{:?}, expected:{:?}",
            ERR_MSG,
            version,
            self.status(),
            self.auxiliary_data(),
            expected_txn_status,
        );

        ensure!(
            self.gas_used() == txn_info.gas_used(),
            "{}: version:{}, gas_used:{:?}, expected:{:?}",
            ERR_MSG,
            version,
            self.gas_used(),
            txn_info.gas_used(),
        );

        let write_set_hash = CryptoHash::hash(self.write_set());
        ensure!(
            write_set_hash == txn_info.state_change_hash(),
            "{}: version:{}, write_set_hash:{:?}, expected:{:?}, write_set: {:?}, expected(if known): {:?}",
            ERR_MSG,
            version,
            write_set_hash,
            txn_info.state_change_hash(),
            self.write_set,
            expected_write_set,
        );

        let event_hashes = self
            .events()
            .iter()
            .map(CryptoHash::hash)
            .collect::<Vec<_>>();
        let event_root_hash = InMemoryEventAccumulator::from_leaves(&event_hashes).root_hash;
        ensure!(
            event_root_hash == txn_info.event_root_hash(),
            "{}: version:{}, event_root_hash:{:?}, expected:{:?}, events: {:?}, expected(if known): {:?}",
            ERR_MSG,
            version,
            event_root_hash,
            txn_info.event_root_hash(),
            self.events(),
            expected_events,
        );

        Ok(())
    }
```
