# Audit Report

## Title
Permanent Validator Sync Failure When Falling Behind Pruning Window During Repeated Stream Errors

## Summary
A critical liveness vulnerability exists in the continuous sync mechanism where repeated streaming service errors can cause a validator to fall behind the pruning window (80M versions), resulting in permanent inability to sync without manual snapshot restoration, effectively ejecting the validator from the network.

## Finding Description
The vulnerability exists in the interaction between the data streaming service's error handling, retry limits, and the storage pruning mechanism. When a continuously syncing validator encounters repeated errors, the following failure cascade occurs:

**1. Stream Failure and Retry Exhaustion:**
The data streaming service limits retries to 5 attempts via `max_request_retry` configuration: [1](#0-0) 

When this limit is reached, the stream terminates: [2](#0-1) 

**2. Pruning Window Limitation:**
The epoch snapshot pruner maintains a default window of only 80 million versions: [3](#0-2) 

This means peers only serve data within this window. Data older than `latest_version - 80M` is permanently pruned: [4](#0-3) 

**3. Data Availability Check Failure:**
When the continuous syncer attempts to create a new stream after reset, it checks data availability: [5](#0-4) 

The check verifies that the next required version is within advertised data ranges: [6](#0-5) 

If the validator has fallen behind by more than the pruning window, this check fails with `Error::DataIsUnavailable`: [7](#0-6) 

**4. No Automatic Recovery:**
The system has no automatic fallback mechanism to switch to snapshot sync when this occurs. The `OutputFallbackHandler` only handles fallback between transaction execution and output application modes, not to snapshot/fast sync: [8](#0-7) 

**Attack Scenario:**
A production validator experiencing extended network issues (partition, peer unavailability, or DDoS) can trigger this vulnerability through natural failure conditions:
1. Validator is at version V (e.g., 100M) during continuous sync
2. Network partition or peer failures cause repeated stream errors
3. After 5 retry failures per request, streams terminate repeatedly
4. During this period (potentially hours), the network progresses to V + 80M+ (e.g., 180M+)
5. The validator's required data (at version 100M) is now pruned from all peers
6. Stream creation permanently fails with `DataIsUnavailable`
7. Validator cannot participate in consensus without manual snapshot restoration

## Impact Explanation
This vulnerability meets **Critical Severity** criteria per the Aptos Bug Bounty program:

**Total Loss of Liveness/Network Availability:** A validator experiencing this vulnerability is permanently unable to sync and participate in consensus. This constitutes a complete loss of validator functionality.

**Validator Ejection:** The affected validator is effectively ejected from the active validator set, reducing the network's decentralization and security guarantees. Multiple validators experiencing this simultaneously could threaten network liveness.

**Requires Manual Intervention:** Recovery requires manual snapshot restoration from backups or trusted sources, which:
- May not always be available or trusted
- Requires significant operational overhead
- Creates downtime during restoration
- May require coordinator intervention for production networks

**No Automatic Mitigation:** Unlike temporary network issues that self-resolve, this represents a permanent failure state with no automated recovery path.

## Likelihood Explanation
This vulnerability has **HIGH likelihood** in production environments:

**Realistic Trigger Conditions:**
- Network partitions (common in distributed systems)
- Extended peer unavailability or churn
- DDoS attacks on validator nodes
- Infrastructure failures (cloud provider issues, ISP problems)
- Correlated failures during network upgrades

**Time Window Analysis:**
At Aptos's target throughput (~5,000 TPS), the 80M version pruning window represents approximately:
- 80,000,000 versions / 5,000 TPS = 16,000 seconds = ~4.4 hours

A validator needs to be unable to sync for only ~4-5 hours before falling into the unrecoverable state. This is a realistic timeframe for extended network issues.

**No Special Privileges Required:**
This can occur through natural network failures without requiring malicious actors or compromised validators.

## Recommendation
Implement automatic fallback to snapshot sync when a validator detects it has fallen beyond the pruning window. The fix should:

**1. Add Detection Logic:**
Add a check in the continuous syncer to detect when the validator's synced version is beyond the advertised data range:

```rust
// In continuous_syncer.rs, within initialize_active_data_stream()
fn should_fallback_to_snapshot_sync(
    &self,
    highest_synced_version: Version,
    advertised_data: &AdvertisedData,
) -> bool {
    // Check if our version is outside the available range
    let transactions_range = &advertised_data.transactions;
    !AdvertisedData::contains_range(
        highest_synced_version,
        highest_synced_version,
        transactions_range,
    )
}
```

**2. Automatic Mode Switching:**
Modify the stream initialization logic to automatically switch to snapshot sync mode:

```rust
async fn initialize_active_data_stream(
    &mut self,
    consensus_sync_request: Arc<Mutex<Option<ConsensusSyncRequest>>>,
) -> Result<(), Error> {
    let (highest_synced_version, highest_synced_epoch) =
        self.get_highest_synced_version_and_epoch()?;
    
    let advertised_data = self.get_advertised_data();
    
    // Check if we've fallen beyond the pruning window
    if self.should_fallback_to_snapshot_sync(highest_synced_version, &advertised_data) {
        warn!("Validator has fallen beyond pruning window. Switching to snapshot sync mode.");
        // Trigger snapshot sync through bootstrapper
        return self.trigger_snapshot_recovery();
    }
    
    // Continue with normal continuous sync...
}
```

**3. Add Configuration Safety:**
Add a configuration sanity check to warn operators about the pruning window risk:

```rust
// In state_sync_config.rs sanitizer
fn sanitize(...) -> Result<(), Error> {
    let pruning_window = storage_config.epoch_snapshot_prune_window;
    let max_stream_wait = state_sync_config.max_stream_wait_time_ms;
    let max_retries = streaming_config.max_request_retry;
    
    // Warn if retries could exhaust pruning window
    if should_warn_about_pruning_window_risk(...) {
        warn!("Pruning window may be insufficient for retry limits. Consider increasing epoch_snapshot_prune_window.");
    }
    Ok(())
}
```

## Proof of Concept

The following scenario demonstrates the vulnerability:

**Setup:**
1. Deploy a validator node with default pruning configuration (80M version window)
2. Start the validator in continuous sync mode at version V

**Execution:**
```rust
// Simulate the failure scenario
#[tokio::test]
async fn test_permanent_sync_failure_beyond_pruning_window() {
    // 1. Setup validator at version 100M
    let validator_version = 100_000_000u64;
    let mut continuous_syncer = setup_continuous_syncer(validator_version);
    
    // 2. Simulate network advancing to 180M+ (beyond pruning window)
    let network_version = 180_000_000u64;
    let pruning_window = 80_000_000u64;
    
    // 3. Create advertised data that only contains recent versions
    let lowest_available = network_version - pruning_window + 1;
    let advertised_data = create_advertised_data(
        lowest_available,  // 100,000,001 (just past validator's position)
        network_version,   // 180,000,000
    );
    
    // 4. Attempt to initialize stream
    let result = continuous_syncer
        .initialize_active_data_stream(Arc::new(Mutex::new(None)))
        .await;
    
    // 5. Verify permanent failure
    assert!(matches!(result, Err(Error::DataIsUnavailable(_))));
    
    // 6. Verify no automatic recovery mechanism exists
    // Stream will fail repeatedly with same error
    for _ in 0..10 {
        let retry_result = continuous_syncer
            .initialize_active_data_stream(Arc::new(Mutex::new(None)))
            .await;
        assert!(matches!(retry_result, Err(Error::DataIsUnavailable(_))));
    }
    
    // Validator is now permanently stuck without manual intervention
}
```

**Expected Behavior:**
The validator should automatically detect it has fallen beyond the pruning window and trigger snapshot sync recovery.

**Actual Behavior:**
The validator permanently fails to create new data streams with `DataIsUnavailable` error, requiring manual snapshot restoration to recover.

**Notes**
This vulnerability represents a critical gap in the state sync resilience mechanisms. The interaction between fixed retry limits, continuous progression of the network, and aggressive pruning creates a scenario where validators can become permanently unable to sync through standard mechanisms. The 4-5 hour window before permanent failure is realistic for production network issues, making this a high-priority fix for validator liveness guarantees.

### Citations

**File:** config/src/config/state_sync_config.rs (L277-277)
```rust
            max_request_retry: 5,
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L442-454)
```rust
    pub async fn process_data_responses(
        &mut self,
        global_data_summary: GlobalDataSummary,
    ) -> Result<(), Error> {
        if self.stream_engine.is_stream_complete()
            || self.request_failure_count >= self.streaming_service_config.max_request_retry
            || self.send_failure
        {
            if !self.send_failure && self.stream_end_notification_id.is_none() {
                self.send_end_of_stream_notification().await?;
            }
            return Ok(()); // There's nothing left to do
        }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L866-877)
```rust
    pub fn ensure_data_is_available(&self, advertised_data: &AdvertisedData) -> Result<(), Error> {
        if !self
            .stream_engine
            .is_remaining_data_available(advertised_data)?
        {
            return Err(Error::DataIsUnavailable(format!(
                "Unable to satisfy stream engine: {:?}, with advertised data: {:?}",
                self.stream_engine, advertised_data
            )));
        }
        Ok(())
    }
```

**File:** config/src/config/storage_config.rs (L415-430)
```rust
impl Default for EpochSnapshotPrunerConfig {
    fn default() -> Self {
        Self {
            enable: true,
            // This is based on ~5K TPS * 2h/epoch * 2 epochs. -- epoch ending snapshots are used
            // by state sync in fast sync mode.
            // The setting is in versions, not epochs, because this makes it behave more like other
            // pruners: a slower network will have longer history in db with the same pruner
            // settings, but the disk space take will be similar.
            // settings.
            prune_window: 80_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L145-176)
```rust
    /// version, V, the node also contains all state values at V.
    fn fetch_state_values_range(
        &self,
        latest_version: Version,
        transactions_range: &Option<CompleteDataRange<Version>>,
    ) -> aptos_storage_service_types::Result<Option<CompleteDataRange<Version>>, Error> {
        let pruner_enabled = self.storage.is_state_merkle_pruner_enabled()?;
        if !pruner_enabled {
            return Ok(*transactions_range);
        }
        let pruning_window = self.storage.get_epoch_snapshot_prune_window()?;

        if latest_version > pruning_window as Version {
            // lowest_state_version = latest_version - pruning_window + 1;
            let mut lowest_state_version = latest_version
                .checked_sub(pruning_window as Version)
                .ok_or_else(|| {
                    Error::UnexpectedErrorEncountered("Lowest state version has overflown!".into())
                })?;
            lowest_state_version = lowest_state_version.checked_add(1).ok_or_else(|| {
                Error::UnexpectedErrorEncountered("Lowest state version has overflown!".into())
            })?;

            // Create the state range
            let state_range = CompleteDataRange::new(lowest_state_version, latest_version)
                .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
            return Ok(Some(state_range));
        }

        // No pruning has occurred. Return the transactions range.
        Ok(*transactions_range)
    }
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L286-287)
```rust
        // Verify the data stream can be fulfilled using the currently advertised data
        data_stream.ensure_data_is_available(&advertised_data)?;
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L1291-1310)
```rust
    fn is_remaining_data_available(&self, advertised_data: &AdvertisedData) -> Result<bool, Error> {
        let advertised_ranges = match &self.request {
            StreamRequest::ContinuouslyStreamTransactions(_) => &advertised_data.transactions,
            StreamRequest::ContinuouslyStreamTransactionOutputs(_) => {
                &advertised_data.transaction_outputs
            },
            StreamRequest::ContinuouslyStreamTransactionsOrOutputs(_) => {
                &advertised_data.transaction_outputs
            },
            request => invalid_stream_request!(request),
        };

        // Verify we can satisfy the next version
        let (next_request_version, _) = self.next_request_version_and_epoch;
        Ok(AdvertisedData::contains_range(
            next_request_version,
            next_request_version,
            advertised_ranges,
        ))
    }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L500-522)
```rust
    /// Handles the storage synchronizer error sent by the driver
    pub async fn handle_storage_synchronizer_error(
        &mut self,
        notification_and_feedback: NotificationAndFeedback,
    ) -> Result<(), Error> {
        // Reset the active stream
        self.reset_active_stream(Some(notification_and_feedback))
            .await?;

        // Fallback to output syncing if we need to
        if let ContinuousSyncingMode::ExecuteTransactionsOrApplyOutputs =
            self.get_continuous_syncing_mode()
        {
            self.output_fallback_handler.fallback_to_outputs();
            metrics::set_gauge(
                &metrics::DRIVER_FALLBACK_MODE,
                ExecutingComponent::ContinuousSyncer.get_label(),
                1,
            );
        }

        Ok(())
    }
```
