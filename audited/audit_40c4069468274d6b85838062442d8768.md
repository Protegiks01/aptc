# Audit Report

## Title
Race Condition in Reconfig Subscription During Node Startup Allows Services to Miss Initial Epoch Configuration

## Summary
A race condition exists in the node startup sequence where reconfig (reconfiguration) subscriptions can deliver epoch transition notifications before dependent services are ready to consume them. Due to the KLAST (keep-last) queue with size 1, if state sync processes blocks containing reconfiguration events during startup, newer epoch notifications overwrite the initial configuration, causing critical services (mempool, consensus, DKG, JWK consensus) to start in an incorrect epoch state.

## Finding Description

The vulnerability occurs in the `setup_environment_and_start_node()` function where the following sequence creates a race window: [1](#0-0) 

All reconfig subscriptions are created at this point, establishing channels with KLAST queue style and size 1. [2](#0-1) 

The critical issue is that state sync is started next and immediately calls `notify_initial_configs()`: [3](#0-2) 

Inside state sync initialization, the initial configs are sent synchronously before the driver is spawned: [4](#0-3) 

However, state sync is then spawned and starts running asynchronously: [5](#0-4) 

**The Race Window**: Between state sync starting (lib.rs line 769) and services starting (lib.rs lines 801-851), state sync can process blocks containing NewEpochEvents. When this happens: [6](#0-5) 

This triggers reconfig notification to all subscribers: [7](#0-6) 

Because the channel uses KLAST with size 1, the initial notification is **dropped** and replaced with the newer epoch configuration.

**Victim Services:**

Mempool expects to receive the initial reconfig and blocks until it arrives: [8](#0-7) 

If state sync processes multiple epochs before mempool starts, mempool receives only the latest epoch config, missing all intermediate epoch transitions. The same applies to DKG, JWK consensus, and most critically, consensus itself which starts even later after waiting for state sync initialization: [9](#0-8) [10](#0-9) 

## Impact Explanation

This is **High Severity** per Aptos bug bounty criteria for the following reasons:

1. **Consensus Safety Risk**: Consensus starting with epoch N+k instead of epoch N violates epoch transition assumptions. It may use an incorrect validator set, wrong consensus parameters, or invalid quorum thresholds.

2. **Significant Protocol Violation**: Services operating with inconsistent epoch states break deterministic execution invariants. Different nodes might observe different epoch transitions depending on their startup timing.

3. **Validator Node Issues**: During network partitions or when validators restart and need to catch up through multiple epochs, this race is highly likely to manifest, causing undefined behavior.

4. **State Inconsistency**: Mempool, DKG, and JWK consensus missing intermediate epoch reconfigurations can lead to state divergence between nodes.

This does not meet Critical severity because it requires specific timing (node startup + multi-epoch sync) and likely doesn't directly cause fund loss, but it represents a significant consensus and liveness risk.

## Likelihood Explanation

**High Likelihood** in the following scenarios:

1. **Validator Restarts During Epoch Transitions**: When a validator node restarts and needs to sync past epoch boundaries, state sync will process NewEpochEvents while services are starting.

2. **Network Partitions**: Nodes rejoining after network partitions often need to sync multiple epochs, triggering this race.

3. **New Node Bootstrap**: New validators or full nodes joining the network process many epochs during initial sync, making this race almost guaranteed.

4. **Fast Sync Mode**: Nodes using fast sync to catch up will process epoch transitions rapidly during bootstrapping.

The race window spans from lib.rs line 769 (state sync start) to line 841-851 (consensus start), which includes network setup, mempool initialization, and state sync initialization blocking call - a significant time window where state sync actively processes blocks.

## Recommendation

**Solution**: Ensure reconfig subscriptions are not notified until their consuming services are ready. Implement a two-phase initialization:

1. **Phase 1**: Create all subscriptions but hold notifications
2. **Phase 2**: After all services start, deliver the initial reconfig notification atomically

**Code Fix** (in `aptos-node/src/lib.rs`):

```rust
pub fn setup_environment_and_start_node(
    mut node_config: NodeConfig,
    remote_log_rx: Option<mpsc::Receiver<TelemetryLog>>,
    logger_filter_update_job: Option<LoggerFilterUpdater>,
    api_port_tx: Option<oneshot::Sender<u16>>,
    indexer_grpc_port_tx: Option<oneshot::Sender<u16>>,
) -> anyhow::Result<AptosHandle> {
    // ... existing setup code ...
    
    // Create event subscription service WITHOUT initial notification
    let (
        mut event_subscription_service,
        mempool_reconfig_subscription,
        consensus_observer_reconfig_subscription,
        consensus_reconfig_subscription,
        dkg_subscriptions,
        jwk_consensus_subscriptions,
    ) = state_sync::create_event_subscription_service(&node_config, &db_rw);
    
    // Start state sync WITHOUT calling notify_initial_configs internally
    // (Modify driver_factory.rs to skip notify_initial_configs)
    let (aptos_data_client, state_sync_runtimes, mempool_listener, consensus_notifier) =
        state_sync::start_state_sync_without_initial_notify(...)?;
    
    // ... start all services (mempool, DKG, JWK, consensus) ...
    
    // Wait for state sync initialization
    state_sync_runtimes.block_until_initialized();
    
    // NOW deliver initial reconfig notification after all services are listening
    let synced_version = db_rw.reader.get_latest_state_checkpoint_version()
        .expect("Failed to get synced version")
        .expect("No synced version found");
    event_subscription_service.notify_initial_configs(synced_version)
        .expect("Failed to notify initial configs");
    
    // ... continue with node startup ...
}
```

Additionally, modify `state-sync/state-sync-driver/src/driver_factory.rs` to NOT call `notify_initial_configs` during driver creation, moving that responsibility to the node startup coordinator.

## Proof of Concept

**Setup**: Start a validator node that needs to sync through multiple epoch transitions during bootstrap.

**Steps to Reproduce**:

1. Deploy a test network with epoch duration of 60 seconds
2. Let the network run through 3-4 epoch transitions
3. Start a new validator node from genesis
4. Add logging to track reconfig notifications:

```rust
// In mempool/src/shared_mempool/coordinator.rs around line 95
let initial_reconfig = mempool_reconfig_events
    .next()
    .await
    .expect("Reconfig sender dropped, unable to start mempool");
    
// Add logging
info!("Mempool received initial reconfig for epoch: {}", 
      initial_reconfig.on_chain_configs.epoch());
```

5. Add similar logging in state sync notification handler:

```rust
// In state-sync/state-sync-driver/src/notification_handlers.rs around line 109
info!("State sync sending reconfig notification for version: {}, epoch: {:?}", 
      latest_synced_version, /* extract epoch from events */);
```

**Expected Vulnerable Behavior**:
- State sync logs show multiple epoch reconfigurations being sent (epoch 1, 2, 3)
- Mempool logs show only receiving the final epoch (epoch 3)
- Mempool misses epochs 1 and 2 transitions
- Consensus similarly starts in epoch 3 without processing epoch 1 and 2

**Expected Fixed Behavior**:
- All services receive initial reconfig for epoch 1
- Services properly process sequential epoch transitions
- No missed epoch configurations

**Notes**
This vulnerability specifically affects the node startup sequence and epoch transition handling. The KLAST queue design is intentional for runtime efficiency (only latest config matters), but during startup, the initial synchronous notification gets lost in the race. The fix requires coordinating the initial notification delivery with service readiness, ensuring the startup invariant that "all services begin from a consistent epoch state" is maintained.

### Citations

**File:** aptos-node/src/lib.rs (L726-734)
```rust
    // Create an event subscription service (and reconfig subscriptions for consensus and mempool)
    let (
        mut event_subscription_service,
        mempool_reconfig_subscription,
        consensus_observer_reconfig_subscription,
        consensus_reconfig_subscription,
        dkg_subscriptions,
        jwk_consensus_subscriptions,
    ) = state_sync::create_event_subscription_service(&node_config, &db_rw);
```

**File:** aptos-node/src/lib.rs (L762-769)
```rust
    let (aptos_data_client, state_sync_runtimes, mempool_listener, consensus_notifier) =
        state_sync::start_state_sync_and_get_notification_handles(
            &node_config,
            storage_service_network_interfaces,
            genesis_waypoint,
            event_subscription_service,
            db_rw.clone(),
        )?;
```

**File:** aptos-node/src/lib.rs (L824-827)
```rust
    // Wait until state sync has been initialized
    debug!("Waiting until state sync is initialized!");
    state_sync_runtimes.block_until_initialized();
    debug!("State sync initialization complete.");
```

**File:** aptos-node/src/lib.rs (L841-851)
```rust
    let consensus_runtime = consensus::create_consensus_runtime(
        &node_config,
        db_rw.clone(),
        consensus_reconfig_subscription,
        consensus_network_interfaces,
        consensus_notifier.clone(),
        consensus_to_mempool_sender.clone(),
        vtxn_pool,
        consensus_publisher.clone(),
        &mut admin_service,
    );
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L40-40)
```rust
const RECONFIG_NOTIFICATION_CHANNEL_SIZE: usize = 1; // Note: this should be 1 to ensure only the latest reconfig is consumed
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L311-326)
```rust
    fn notify_events(&mut self, version: Version, events: Vec<ContractEvent>) -> Result<(), Error> {
        if events.is_empty() {
            return Ok(()); // No events!
        }

        // Notify event subscribers and check if a reconfiguration event was processed
        let reconfig_event_processed = self.notify_event_subscribers(version, events)?;

        // If a reconfiguration event was found, also notify the reconfig subscribers
        // of the new configuration values.
        if reconfig_event_processed {
            self.notify_reconfiguration_subscribers(version)
        } else {
            Ok(())
        }
    }
```

**File:** state-sync/state-sync-driver/src/driver_factory.rs (L102-118)
```rust
        // Notify subscribers of the initial on-chain config values
        match storage.reader.get_latest_state_checkpoint_version() {
            Ok(Some(synced_version)) => {
                if let Err(error) =
                    event_subscription_service.notify_initial_configs(synced_version)
                {
                    panic!(
                        "Failed to notify subscribers of initial on-chain configs: {:?}",
                        error
                    )
                }
            },
            Ok(None) => {
                panic!("Latest state checkpoint version not found.")
            },
            Err(error) => panic!("Failed to fetch the initial synced version: {:?}", error),
        }
```

**File:** state-sync/state-sync-driver/src/driver_factory.rs (L184-189)
```rust
        // Spawn the driver
        if let Some(driver_runtime) = &driver_runtime {
            driver_runtime.spawn(state_sync_driver.start_driver());
        } else {
            tokio::spawn(state_sync_driver.start_driver());
        }
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L106-110)
```rust
        // Notify the event subscription service of the events
        event_subscription_service
            .lock()
            .notify_events(latest_synced_version, events)?;

```

**File:** mempool/src/shared_mempool/coordinator.rs (L95-104)
```rust
    let initial_reconfig = mempool_reconfig_events
        .next()
        .await
        .expect("Reconfig sender dropped, unable to start mempool");
    handle_mempool_reconfig_event(
        &mut smp,
        &bounded_executor,
        initial_reconfig.on_chain_configs,
    )
    .await;
```
