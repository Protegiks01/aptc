# Audit Report

## Title
Version Tracking Desynchronization in FileStoreUploader Recovery Leading to System Deadlock and Data Gaps

## Summary
The oneshot channel synchronization mechanism between FileStoreUploader and DataManager fails to correctly handle backward version updates during recovery. When FileStoreUploader rolls back to an earlier version due to incomplete batches, DataManager's use of `fetch_max()` prevents the cache from tracking the correct file store version, resulting in a permanent deadlock where the uploader cannot make progress and data gaps emerge in the indexer system.

## Finding Description

The vulnerability occurs in the initialization and recovery flow of the indexer-grpc-manager:

**Phase 1 - Initialization (Before Recovery):** [1](#0-0) 

DataManager is initialized by reading the current file store version (e.g., 100000). The Cache is created with both `start_version` and `file_store_version` set to this value.

**Phase 2 - Recovery Process:** [2](#0-1) 

FileStoreUploader's `recover()` function scans for incomplete batches. If it finds one (e.g., at version 95000), it updates the file store metadata to this lower version and returns it. [3](#0-2) 

The recovery signal is sent via the oneshot channel after updating metadata to the recovered version.

**Phase 3 - Synchronization Failure:** [4](#0-3) 

DataManager receives the signal and calls `update_file_store_version_in_cache` with `version_can_go_backward=true`. [5](#0-4) 

The critical bug: `fetch_max()` is used to update the cached version. Since `fetch_max()` only updates to the maximum value, when the file store version has gone backward (95000 < 100000), the atomic variable remains at 100000. The `version_can_go_backward` flag is checked for panicking but doesn't force an update of the atomic value.

**Phase 4 - System Deadlock:** [6](#0-5) 

FileStoreUploader tries to fetch transactions starting from its recovered version (95000), but: [7](#0-6) 

The cache returns empty because `start_version` (100000) > requested version (95000). [8](#0-7) 

GC cannot prune data because `file_store_version` (100000) equals `start_version` (100000), so the condition on line 68 never becomes true. The cache fills up indefinitely, and the system deadlocks.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** per Aptos bug bounty criteria:

1. **Validator node slowdowns**: The indexer-grpc-manager becomes completely stuck, unable to process new transactions or serve historical data
2. **Significant protocol violations**: Data integrity is compromised with a permanent gap in the range [95000, 100000)
3. **Service unavailability**: The indexer service becomes unresponsive, requiring manual intervention and potentially data restoration from backups

The impact extends to all downstream consumers of the indexer data, including wallets, explorers, and analytics platforms that rely on complete transaction history.

## Likelihood Explanation

This vulnerability has **MEDIUM-HIGH likelihood**:

**Triggering Conditions:**
- Occurs during any abnormal shutdown or crash of the master node during batch upload
- Does not require attacker action - normal operational failures trigger it
- Common scenarios include: OOM errors, network interruptions during upload, or process restarts

**Exploitation Requirements:**
- No special privileges needed
- Happens automatically during the recovery process
- Cannot be prevented by operators once the initial crash has occurred

The vulnerability is particularly insidious because the `version_can_go_backward=true` flag gives false confidence that backward version updates are handled correctly, when in fact they silently fail.

## Recommendation

Replace the `fetch_max()` operation with explicit version setting when `version_can_go_backward=true`:

```rust
async fn update_file_store_version_in_cache(
    &self,
    cache: &RwLockReadGuard<'_, Cache>,
    version_can_go_backward: bool,
) {
    let file_store_version = self.file_store_reader.get_latest_version().await;
    if let Some(file_store_version) = file_store_version {
        let file_store_version_before_update = if version_can_go_backward {
            // During recovery, explicitly set the version even if it goes backward
            cache.file_store_version.swap(file_store_version, Ordering::SeqCst)
        } else {
            // In normal operation, only allow version to increase
            let old_version = cache.file_store_version.fetch_max(file_store_version, Ordering::SeqCst);
            if old_version > file_store_version {
                panic!("File store version is going backward, data might be corrupted. {old_version} v.s. {file_store_version}");
            }
            old_version
        };
        FILE_STORE_VERSION_IN_CACHE.set(file_store_version as i64);
        info!("Updated file_store_version in cache from {file_store_version_before_update} to {file_store_version}.");
    }
}
```

Additionally, consider adjusting `cache.start_version` during recovery if it exceeds the recovered file store version to prevent the gap scenario.

## Proof of Concept

```rust
// Reproduction steps for Rust integration test
#[tokio::test]
async fn test_version_tracking_desynchronization() {
    // 1. Initialize file store with metadata at version 100000
    let file_store_config = create_test_file_store_config();
    let file_store = file_store_config.create_filestore().await;
    
    let metadata = FileStoreMetadata {
        chain_id: 1,
        num_transactions_per_folder: 100000,
        version: 100000,
    };
    file_store.save_raw_file(
        PathBuf::from(METADATA_FILE_NAME),
        serde_json::to_vec(&metadata).unwrap()
    ).await.unwrap();
    
    // 2. Create DataManager - it reads version 100000
    let data_manager = DataManager::new(
        1,
        file_store_config.clone(),
        cache_config,
        metadata_manager.clone(),
        false,
    ).await;
    
    // Verify initial cache version
    assert_eq!(data_manager.get_file_store_version().await, 100000);
    
    // 3. Simulate incomplete batch by creating batch metadata at version 95000
    // (omitted for brevity - create partial batch files)
    
    // 4. Create FileStoreUploader and run recovery
    let mut uploader = FileStoreUploader::new(1, file_store_config).await.unwrap();
    let (tx, rx) = oneshot::channel();
    
    // 5. Start uploader in background (it will recover to 95000)
    tokio::spawn(async move {
        uploader.start(data_manager.clone(), tx).await.unwrap();
    });
    
    // 6. DataManager receives signal and updates
    rx.await.unwrap();
    let cache = data_manager.cache.read().await;
    data_manager.update_file_store_version_in_cache(&cache, true).await;
    
    // 7. BUG: Cache still shows version 100000 despite file store being at 95000
    assert_eq!(cache.file_store_version.load(Ordering::SeqCst), 100000);
    assert_eq!(data_manager.get_file_store_version().await, 95000);
    
    // 8. Demonstrate deadlock: uploader cannot fetch transactions
    let txns = data_manager.get_transactions_from_cache(95000, 1000, true).await;
    assert_eq!(txns.len(), 0); // Empty! Creates permanent gap
}
```

## Notes

This vulnerability specifically affects the master node configuration of the indexer-grpc-manager. Non-master nodes do not experience this issue as they continuously watch the file store version without the recovery synchronization mechanism. [9](#0-8) 

The root cause is the semantic mismatch between the `version_can_go_backward` parameter (intended to allow backward version updates) and the `fetch_max()` implementation (which fundamentally cannot decrease values). This represents a dangerous false sense of security in the code.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L63-80)
```rust
    fn maybe_gc(&mut self) -> bool {
        if self.cache_size <= self.max_cache_size {
            return true;
        }

        while self.start_version < self.file_store_version.load(Ordering::SeqCst)
            && self.cache_size > self.target_cache_size
        {
            let transaction = self.transactions.pop_front().unwrap();
            self.cache_size -= transaction.encoded_len();
            self.start_version += 1;
        }

        CACHE_SIZE.set(self.cache_size as i64);
        CACHE_START_VERSION.set(self.start_version as i64);

        self.cache_size <= self.max_cache_size
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L156-172)
```rust
    pub(crate) async fn new(
        chain_id: u64,
        file_store_config: IndexerGrpcFileStoreConfig,
        cache_config: CacheConfig,
        metadata_manager: Arc<MetadataManager>,
        allow_fn_fallback: bool,
    ) -> Self {
        let file_store = file_store_config.create_filestore().await;
        let file_store_reader = FileStoreReader::new(chain_id, file_store).await;
        let file_store_version = file_store_reader.get_latest_version().await.unwrap();
        Self {
            cache: RwLock::new(Cache::new(cache_config, file_store_version)),
            file_store_reader,
            metadata_manager,
            allow_fn_fallback,
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L174-192)
```rust
    pub(crate) async fn start(
        &self,
        is_master: bool,
        file_store_uploader_recover_rx: Receiver<()>,
    ) {
        let watch_file_store_version = !is_master;

        if is_master {
            // For master, we need to wait for the FileStoreUploader to finish the recover to get
            // the true file_store_version.
            info!("Waiting for FileStoreUploader recovering.");
            match file_store_uploader_recover_rx.await {
                Ok(_) => {},
                Err(_) => panic!("Should not happen!"),
            };
            let cache = self.cache.read().await;
            self.update_file_store_version_in_cache(&cache, /*version_can_go_backward=*/ true)
                .await;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L374-384)
```rust
    pub(crate) async fn get_transactions_from_cache(
        &self,
        start_version: u64,
        max_size: usize,
        update_file_store_version: bool,
    ) -> Vec<Transaction> {
        self.cache
            .read()
            .await
            .get_transactions(start_version, max_size, update_file_store_version)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L403-419)
```rust
    async fn update_file_store_version_in_cache(
        &self,
        cache: &RwLockReadGuard<'_, Cache>,
        version_can_go_backward: bool,
    ) {
        let file_store_version = self.file_store_reader.get_latest_version().await;
        if let Some(file_store_version) = file_store_version {
            let file_store_version_before_update = cache
                .file_store_version
                .fetch_max(file_store_version, Ordering::SeqCst);
            FILE_STORE_VERSION_IN_CACHE.set(file_store_version as i64);
            info!("Updated file_store_version in cache to {file_store_version}.");
            if !version_can_go_backward && file_store_version_before_update > file_store_version {
                panic!("File store version is going backward, data might be corrupted. {file_store_version_before_update} v.s. {file_store_version}");
            };
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L87-118)
```rust
    async fn recover(&self) -> Result<(u64, BatchMetadata)> {
        let _timer = TIMER.with_label_values(&["recover"]).start_timer();

        let mut version = self
            .reader
            .get_latest_version()
            .await
            .expect("Latest version must exist.");
        info!("Starting recovering process, current version in storage: {version}.");
        let mut num_folders_checked = 0;
        let mut buffered_batch_metadata_to_recover = BatchMetadata::default();
        while let Some(batch_metadata) = self.reader.get_batch_metadata(version).await {
            let batch_last_version = batch_metadata.files.last().unwrap().last_version;
            version = batch_last_version;
            if version % NUM_TXNS_PER_FOLDER != 0 {
                buffered_batch_metadata_to_recover = batch_metadata;
                break;
            }
            num_folders_checked += 1;
            if num_folders_checked >= MAX_NUM_FOLDERS_TO_CHECK_FOR_RECOVERY {
                panic!(
                    "File store metadata is way behind batch metadata, data might be corrupted."
                );
            }
        }

        self.update_file_store_metadata(version).await?;

        info!("Finished recovering process, recovered at version: {version}.");

        Ok((version, buffered_batch_metadata_to_recover))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L120-134)
```rust
    pub(crate) async fn start(
        &mut self,
        data_manager: Arc<DataManager>,
        recover_tx: Sender<()>,
    ) -> Result<()> {
        let (version, batch_metadata) = self.recover().await?;

        let mut file_store_operator = FileStoreOperatorV2::new(
            MAX_SIZE_PER_FILE,
            NUM_TXNS_PER_FOLDER,
            version,
            batch_metadata,
        );

        recover_tx.send(()).expect("Receiver should exist.");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L147-177)
```rust
            s.spawn(async move {
                loop {
                    let _timer = TIMER
                        .with_label_values(&["file_store_uploader_main_loop"])
                        .start_timer();
                    let next_version = file_store_operator.version();
                    let transactions = {
                        let _timer = TIMER
                            .with_label_values(&["get_transactions_from_cache"])
                            .start_timer();
                        data_manager
                            .get_transactions_from_cache(
                                next_version,
                                MAX_SIZE_PER_FILE,
                                /*update_file_store_version=*/ true,
                            )
                            .await
                    };
                    let len = transactions.len();
                    for transaction in transactions {
                        file_store_operator
                            .buffer_and_maybe_dump_transactions_to_file(transaction, tx.clone())
                            .await
                            .unwrap();
                    }
                    if len == 0 {
                        info!("No transaction was returned from cache, requested version: {next_version}.");
                        tokio::time::sleep(Duration::from_millis(200)).await;
                    }
                }
            });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L106-126)
```rust
        let (tx, rx) = channel();
        tokio_scoped::scope(|s| {
            s.spawn(async move {
                self.metadata_manager.start().await.unwrap();
            });
            s.spawn(async move { self.data_manager.start(self.is_master, rx).await });
            if self.is_master {
                s.spawn(async move {
                    self.file_store_uploader
                        .lock()
                        .await
                        .start(self.data_manager.clone(), tx)
                        .await
                        .unwrap();
                });
            }
            s.spawn(async move {
                info!("Starting GrpcManager at {}.", service_config.listen_address);
                server.serve(service_config.listen_address).await.unwrap();
            });
        });
```
