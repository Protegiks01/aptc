# Audit Report

## Title
Memory Exhaustion via Unbounded BCS Deserialization in State Snapshot Restoration

## Summary
The state snapshot restoration process deserializes `SparseMerkleRangeProof` objects from backup files without size limits, allowing an attacker with access to backup storage to craft malicious proofs containing arbitrarily large vectors that cause memory exhaustion and node crashes during restoration.

## Finding Description

The vulnerability exists in the state snapshot restoration workflow where chunk proofs are deserialized without size validation. [1](#0-0) 

This line loads the chunk proof using `load_bcs_file`, which is implemented as: [2](#0-1) 

The implementation uses `bcs::from_bytes` **without any size limits**, unlike protected code paths in the network layer that use `bcs::from_bytes_with_limit`: [3](#0-2) 

The deserialized type `SparseMerkleRangeProof` contains an unbounded vector: [4](#0-3) 

Each `HashValue` is 32 bytes in size: [5](#0-4) 

**Attack Path:**
1. Attacker crafts a malicious state snapshot chunk proof with `right_siblings` containing millions/billions of entries (e.g., 100 million entries = 3.2 GB)
2. Attacker replaces legitimate backup files with malicious ones in backup storage, or performs MITM during backup retrieval
3. Validator node attempts to restore from backup
4. `load_bcs_file` reads the entire malicious file and attempts BCS deserialization
5. BCS deserialization allocates a `Vec<HashValue>` with the attacker-specified length
6. Memory exhaustion occurs (e.g., 100M × 32 bytes = 3.2 GB), triggering OOM kill or system crash
7. Node becomes unavailable and restoration fails

**Verification Does Not Prevent This:**
The proof verification happens **after** deserialization and memory allocation: [6](#0-5) 

Even though verification would eventually fail for oversized proofs (Sparse Merkle Trees with 256-bit keys have maximum height 256, so valid proofs have ≤256 siblings), the memory exhaustion occurs during deserialization at line 342, before verification at line 391.

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

**Severity: HIGH**

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:
- **"Validator node slowdowns"**: The vulnerability causes complete node crashes during restoration, which is more severe than slowdowns
- **"API crashes"**: Similar impact - node becomes unavailable

The impact includes:
1. **Denial of Service**: Nodes attempting restoration crash and cannot complete the process
2. **Disaster Recovery Prevention**: If backups are poisoned before a network disaster, recovery becomes impossible without manual intervention
3. **Operational Disruption**: Requires node restarts and backup file sanitization

While this does not directly break consensus (restoration is an offline operation), it affects network availability and recovery capabilities, making it a significant security issue.

## Likelihood Explanation

**Likelihood: MEDIUM**

The attack requires:
- **Access to backup storage** OR **MITM capability** during backup retrieval
- **Low technical complexity**: Crafting malicious BCS data is straightforward
- **Backup storage security**: Typically protected, but varies by deployment

The likelihood is MEDIUM because:
- Backup storage is usually secured, but not always cryptographically authenticated
- Compromise of backup infrastructure is a realistic threat model
- Once access is gained, exploitation is trivial
- Impact is amplified during disaster scenarios when backups are critically needed

## Recommendation

Implement size limits for BCS deserialization in backup restoration. Add a reasonable upper bound based on the theoretical maximum proof size:

**Fix in `storage/backup/backup-cli/src/utils/storage_ext.rs`:**
```rust
async fn load_bcs_file<T: DeserializeOwned>(&self, file_handle: &FileHandleRef) -> Result<T> {
    // For SparseMerkleRangeProof: max tree height is 256, 
    // so max siblings is 256 * 32 bytes = 8KB
    // Add reasonable overhead for other types, set limit to 1MB
    const MAX_BCS_SIZE: usize = 1_000_000;
    let bytes = self.read_all(file_handle).await?;
    Ok(bcs::from_bytes_with_limit(&bytes, MAX_BCS_SIZE)?)
}
```

**Additional validation in proof verification:**
Add early bounds checking in `storage/jellyfish-merkle/src/restore/mod.rs`:
```rust
pub fn add_chunk_impl(
    &mut self,
    mut chunk: Vec<(&K, HashValue)>,
    proof: SparseMerkleRangeProof,
) -> Result<()> {
    // Add early validation
    ensure!(
        proof.right_siblings().len() <= HashValue::LENGTH_IN_BITS,
        "Invalid proof: too many right siblings (max 256, got {})",
        proof.right_siblings().len()
    );
    
    // ... rest of function
}
```

## Proof of Concept

```rust
// PoC: Create malicious backup file that triggers memory exhaustion
use aptos_types::proof::SparseMerkleRangeProof;
use aptos_crypto::HashValue;

fn create_malicious_proof() -> Vec<u8> {
    // Create proof with 100 million entries (3.2 GB)
    let malicious_siblings: Vec<HashValue> = (0..100_000_000)
        .map(|_| HashValue::random())
        .collect();
    
    let proof = SparseMerkleRangeProof::new(malicious_siblings);
    
    // Serialize to BCS
    bcs::to_bytes(&proof).expect("Serialization should succeed")
}

#[test]
fn test_memory_exhaustion_attack() {
    let malicious_data = create_malicious_proof();
    println!("Malicious proof size: {} bytes (~3.2 GB)", malicious_data.len());
    
    // Attempting to deserialize this will cause memory exhaustion
    // In production, this would crash the node during backup restoration
    let result: Result<SparseMerkleRangeProof, _> = bcs::from_bytes(&malicious_data);
    
    // This deserialization will attempt to allocate 3.2 GB of memory
    // causing OOM or system crash
}
```

To demonstrate the fix works:
```rust
#[test]
fn test_size_limit_prevents_attack() {
    let malicious_data = create_malicious_proof();
    
    // With size limit, deserialization fails safely
    const MAX_SIZE: usize = 1_000_000;
    let result: Result<SparseMerkleRangeProof, _> = 
        bcs::from_bytes_with_limit(&malicious_data, MAX_SIZE);
    
    assert!(result.is_err(), "Oversized proof should be rejected");
}
```

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L192-192)
```rust
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
```

**File:** storage/backup/backup-cli/src/utils/storage_ext.rs (L31-32)
```rust
    async fn load_bcs_file<T: DeserializeOwned>(&self, file_handle: &FileHandleRef) -> Result<T> {
        Ok(bcs::from_bytes(&self.read_all(file_handle).await?)?)
```

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L260-261)
```rust
    fn bcs_decode<T: DeserializeOwned>(&self, bytes: &[u8], limit: usize) -> anyhow::Result<T> {
        bcs::from_bytes_with_limit(bytes, limit).map_err(|e| anyhow!("{:?}", e))
```

**File:** types/src/proof/definition.rs (L762-767)
```rust
#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize)]
pub struct SparseMerkleRangeProof {
    /// The vector of siblings on the right of the path from root to last leaf. The ones near the
    /// bottom are at the beginning of the vector. In the above example, it's `[X, h]`.
    right_siblings: Vec<HashValue>,
}
```

**File:** crates/aptos-crypto/src/hash.rs (L130-131)
```rust
    /// The length of the hash in bytes.
    pub const LENGTH: usize = 32;
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L339-391)
```rust
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;
```
