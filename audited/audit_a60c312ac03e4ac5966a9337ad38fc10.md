# Audit Report

## Title
Epoch Manipulation Attack Allows Malicious Blocks to Evade Garbage Collection in Consensus Observer

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) vulnerability in the consensus observer's pending block store allows blocks with artificially inflated epoch values to bypass garbage collection while legitimate blocks are prematurely removed. This occurs because epoch validation happens after insertion into the pending store, and the garbage collection mechanism removes blocks with the lowest (epoch, round) keys first, causing legitimate current-epoch blocks to be evicted before malicious future-epoch blocks.

## Finding Description

The vulnerability exists in the consensus observer's block processing pipeline where epoch validation is deferred until after blocks are inserted into the pending store. When quorum store is enabled and payloads are not yet available, incoming blocks bypass epoch validation during insertion.

**Technical Flow:**

The pending block store uses a BTreeMap keyed by `(u64, Round)` tuples representing (epoch, round): [1](#0-0) 

When an `OrderedBlock` message arrives, it undergoes structural validation that explicitly does NOT check the cryptographic proof or epoch: [2](#0-1) 

The out-of-date check only verifies if `(block_epoch, block_round) <= (last_ordered_block.epoch(), last_ordered_block.round())`. Blocks with future epochs (e.g., epoch 999999 when current is epoch 5) pass this check: [3](#0-2) 

When quorum store is enabled but payloads don't exist, blocks are inserted into the pending store WITHOUT epoch or signature validation: [4](#0-3) 

Garbage collection triggers automatically after insertion and uses `pop_first()` to remove entries with the LOWEST (epoch, round) keys: [5](#0-4) 

Since BTreeMap orders entries in ascending order, legitimate blocks with current epoch values are removed before malicious blocks with inflated epochs.

Epoch validation only occurs in `process_ordered_block()`, which is called AFTER insertion and only when all payloads exist: [6](#0-5) 

## Impact Explanation

**Severity: HIGH**

This vulnerability causes denial of service on consensus observer infrastructure:

1. **Validator Fullnode Degradation**: While validators themselves only run publishers, Validator Fullnodes (VFNs) run both observers and publishers. This attack degrades VFN functionality: [7](#0-6) 

2. **Observer Dysfunction**: The consensus observer fails its core function - legitimate blocks are garbage collected before processing, forcing fallback to slower state sync mechanisms.

3. **Resource Exhaustion**: The pending store fills with unprocessable blocks with invalid epochs, consuming memory (default limit: 150 blocks, configurable up to 300 for test networks): [8](#0-7) 

4. **Persistent DoS**: Once malicious blocks fill the store, legitimate blocks cannot be retained, creating a persistent denial of service until subscriptions are reset.

This qualifies as HIGH severity under Aptos bug bounty criteria for causing significant protocol violations and infrastructure degradation.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

The vulnerability requires:

1. **Quorum Store Enabled**: Attack only works when quorum store is enabled, as otherwise payloads exist inline and epoch validation occurs immediately: [9](#0-8) 

2. **Peer Subscription**: The observer must subscribe to a malicious or compromised peer. Observers select peers based on distance from validators and latency metrics: [10](#0-9) 

3. **Simple Exploitation**: Once subscribed to, sending malformed OrderedBlock messages requires only structural validity, no cryptographic operations.

4. **No Detection**: There are no epoch bounds checks in the insertion path before garbage collection triggers.

The attack is feasible when quorum store is enabled and an observer subscribes to a compromised peer or when a legitimate peer has a bug that sends incorrect epoch values.

## Recommendation

Implement epoch validation before insertion into the pending block store:

```rust
// In process_ordered_block_message(), after structural validation and before insertion
let epoch_state = self.get_epoch_state();
let first_block_epoch = ordered_block.first_block().epoch();

// Validate epoch is within reasonable bounds
if first_block_epoch != epoch_state.epoch && 
   first_block_epoch != epoch_state.epoch + 1 {
    error!("Received block with invalid epoch: {} (current: {})", 
           first_block_epoch, epoch_state.epoch);
    increment_invalid_message_counter(&peer_network_id, metrics::ORDERED_BLOCK_LABEL);
    return;
}
```

Additionally, modify garbage collection to prioritize removal based on validation failure status rather than purely epoch ordering, or implement bounds checking on epoch values during insertion.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Running a consensus observer with quorum store enabled
2. Establishing a subscription from the observer to a malicious publisher
3. Sending OrderedBlock messages with epochs set to `u64::MAX`
4. Sending legitimate OrderedBlock messages with current epoch
5. Observing that when the pending store reaches capacity, legitimate blocks are garbage collected while malicious blocks persist

The code paths traced above demonstrate this behavior will occur deterministically given these conditions.

---

**Notes:**

- The report's claim that "any peer can establish a subscription" is inaccurate - observers initiate subscriptions to publishers, not vice versa
- The impact is on fullnode infrastructure (primarily VFNs) rather than validator nodes directly, though VFNs are part of the validator ecosystem
- The core TOCTOU vulnerability and garbage collection logic flaw are valid and exploitable when preconditions are met
- This represents a genuine logic bug in the consensus observer implementation that violates the intended resource management invariants

### Citations

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L67-72)
```rust
    blocks_without_payloads: BTreeMap<(u64, Round), Arc<PendingBlockWithMetadata>>,

    // A map of ordered blocks that are without payloads. The key is
    // the hash of the first block in the ordered block.
    // Note: this is the same as blocks_without_payloads, but with a different key.
    blocks_without_payloads_by_hash: BTreeMap<HashValue, Arc<PendingBlockWithMetadata>>,
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L176-194)
```rust
        // Remove the oldest blocks if the store is too large
        for _ in 0..num_blocks_to_remove {
            if let Some((oldest_epoch_round, pending_block)) =
                self.blocks_without_payloads.pop_first()
            {
                // Log a warning message for the removed block
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "The pending block store is too large: {:?} blocks. Removing the block for the oldest epoch and round: {:?}",
                        num_pending_blocks, oldest_epoch_round
                    ))
                );

                // Remove the block from the hash store
                let first_block = pending_block.ordered_block().first_block();
                self.blocks_without_payloads_by_hash
                    .remove(&first_block.id());
            }
        }
```

**File:** consensus/src/consensus_observer/network/observer_message.rs (L225-266)
```rust
    /// Verifies the ordered blocks and returns an error if the data is invalid.
    /// Note: this does not check the ordered proof.
    pub fn verify_ordered_blocks(&self) -> Result<(), Error> {
        // Verify that we have at least one ordered block
        if self.blocks.is_empty() {
            return Err(Error::InvalidMessageError(
                "Received empty ordered block!".to_string(),
            ));
        }

        // Verify the last block ID matches the ordered proof block ID
        if self.last_block().id() != self.proof_block_info().id() {
            return Err(Error::InvalidMessageError(
                format!(
                    "Last ordered block ID does not match the ordered proof ID! Number of blocks: {:?}, Last ordered block ID: {:?}, Ordered proof ID: {:?}",
                    self.blocks.len(),
                    self.last_block().id(),
                    self.proof_block_info().id()
                )
            ));
        }

        // Verify the blocks are correctly chained together (from the last block to the first)
        let mut expected_parent_id = None;
        for block in self.blocks.iter().rev() {
            if let Some(expected_parent_id) = expected_parent_id {
                if block.id() != expected_parent_id {
                    return Err(Error::InvalidMessageError(
                        format!(
                            "Block parent ID does not match the expected parent ID! Block ID: {:?}, Expected parent ID: {:?}",
                            block.id(),
                            expected_parent_id
                        )
                    ));
                }
            }

            expected_parent_id = Some(block.parent_id());
        }

        Ok(())
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L157-165)
```rust
    fn all_payloads_exist(&self, blocks: &[Arc<PipelinedBlock>]) -> bool {
        // If quorum store is disabled, all payloads exist (they're already in the blocks)
        if !self.observer_epoch_state.is_quorum_store_enabled() {
            return true;
        }

        // Otherwise, check if all the payloads exist in the payload store
        self.observer_block_data.lock().all_payloads_exist(blocks)
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L677-691)
```rust
        // Determine if the block is behind the last ordered block, or if it is already pending
        let last_ordered_block = self.observer_block_data.lock().get_last_ordered_block();
        let block_out_of_date =
            first_block_epoch_round <= (last_ordered_block.epoch(), last_ordered_block.round());
        let block_pending = self
            .observer_block_data
            .lock()
            .existing_pending_block(&ordered_block);

        // If the block is out of date or already pending, ignore it
        if block_out_of_date || block_pending {
            // Update the metrics for the dropped ordered block
            update_metrics_for_dropped_ordered_block_message(peer_network_id, &ordered_block);
            return;
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L704-713)
```rust
        // If all payloads exist, process the block. Otherwise, store it
        // in the pending block store and wait for the payloads to arrive.
        if self.all_payloads_exist(pending_block_with_metadata.ordered_block().blocks()) {
            self.process_ordered_block(pending_block_with_metadata)
                .await;
        } else {
            self.observer_block_data
                .lock()
                .insert_pending_block(pending_block_with_metadata);
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L728-752)
```rust
        let epoch_state = self.get_epoch_state();
        if ordered_block.proof_block_info().epoch() == epoch_state.epoch {
            if let Err(error) = ordered_block.verify_ordered_proof(&epoch_state) {
                // Log the error and update the invalid message counter
                error!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Failed to verify ordered proof! Ignoring: {:?}, from peer: {:?}. Error: {:?}",
                        ordered_block.proof_block_info(),
                        peer_network_id,
                        error
                    ))
                );
                increment_invalid_message_counter(&peer_network_id, metrics::ORDERED_BLOCK_LABEL);
                return;
            }
        } else {
            // Drop the block and log an error (the block should always be for the current epoch)
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Received ordered block for a different epoch! Ignoring: {:?}",
                    ordered_block.proof_block_info()
                ))
            );
            return;
        };
```

**File:** config/src/config/consensus_observer_config.rs (L72-73)
```rust
            max_num_pending_blocks: 150, // 150 blocks (sufficient for existing production networks)
            progress_check_interval_ms: 5_000, // 5 seconds
```

**File:** config/src/config/consensus_observer_config.rs (L119-128)
```rust
            NodeType::ValidatorFullnode => {
                if ENABLE_ON_VALIDATOR_FULLNODES
                    && !observer_manually_set
                    && !publisher_manually_set
                {
                    // Enable both the observer and the publisher for VFNs
                    consensus_observer_config.observer_enabled = true;
                    consensus_observer_config.publisher_enabled = true;
                    modified_config = true;
                }
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L283-295)
```rust
pub fn sort_peers_by_subscription_optimality(
    peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) -> Vec<PeerNetworkId> {
    // Group peers and latencies by validator distance, i.e., distance -> [(peer, latency)]
    let mut unsupported_peers = Vec::new();
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for (peer_network_id, peer_metadata) in peers_and_metadata {
        // Verify that the peer supports consensus observer
        if !supports_consensus_observer(peer_metadata) {
            unsupported_peers.push(*peer_network_id);
            continue; // Skip the peer
        }

```
