# Audit Report

## Title
Concurrent Batch Processing Race Condition Causes Object Deletions to be Attributed to Wrong Owner in Indexer

## Summary
The Aptos indexer processes transaction batches concurrently using multiple processor tasks. When an object ownership transfer and deletion occur in sequential batches, a race condition can cause the deletion to be attributed to the previous owner instead of the current owner due to stale database reads during concurrent processing.

## Finding Description

The indexer architecture spawns multiple concurrent processor tasks (default 5) that process transaction batches in parallel. [1](#0-0) 

Each processor task processes a batch of transactions, building an in-memory `object_mapping` HashMap to track object state changes within that batch. [2](#0-1) 

When processing object deletions via `Object::from_delete_resource()`, the code first checks if the object exists in the current batch's `object_mapping`. If not found, it falls back to querying the database to retrieve the object's previous owner. [3](#0-2) 

The developer explicitly acknowledges this limitation with the comment: "This is actually not great because object owner can change." [4](#0-3) 

**Race Condition Scenario:**

1. Batch A (versions 1000-1999) contains an ownership transfer: ObjectCore resource updated, transferring object X from Alice to Bob at version 1500
2. Batch B (versions 2000-2999) contains a deletion: ObjectGroup resource deleted for object X at version 2500
3. Both batches are fetched sequentially but **processed concurrently** by different threads
4. Thread 2 (processing Batch B) may start before Thread 1 (processing Batch A) commits to the database
5. When Thread 2 processes the deletion:
   - Checks its local `object_mapping` - doesn't contain object X (it's from a different batch)
   - Falls back to database query via `get_object_owner()` [5](#0-4) 
   - Database query executes under READ COMMITTED isolation (PostgreSQL default)
   - Thread 1's ownership transfer transaction hasn't committed yet
   - Query returns **stale data**: Alice as owner
6. Thread 2 records the deletion with Alice as owner (incorrect)
7. Thread 1 later commits the ownership transfer to Bob
8. **Permanent inconsistency**: Indexer shows object deleted by Alice when it was actually owned by Bob at deletion time

The ownership transfer itself works correctly on-chain: when transferring ownership, the ObjectCore resource's `owner` field is directly modified. [6](#0-5) 

This creates a WriteResource change that should be processed by the indexer. [7](#0-6) 

However, when deletions occur, only the ObjectGroup resource is deleted (not ObjectCore), which is why the code checks for "0x1::object::ObjectGroup" specifically. [8](#0-7) 

## Impact Explanation

This vulnerability causes **data integrity issues in the indexer**, which qualifies as **High Severity** under "Significant protocol violations" or "API crashes" categories. While the blockchain state itself remains correct (consensus is unaffected), the indexer provides critical infrastructure:

1. **Historical Data Corruption**: Block explorers, analytics platforms, and applications querying object deletion history will receive incorrect ownership information
2. **Audit Trail Compromise**: Forensic analysis and compliance tracking become unreliable
3. **Application Logic Errors**: dApps using indexer APIs for ownership validation may make incorrect decisions
4. **Data Trust Erosion**: Users lose confidence in indexed data accuracy

The impact is limited to off-chain indexer data rather than on-chain state, which prevents this from being Critical severity. However, given the indexer's role as the primary data access layer for the Aptos ecosystem, this represents a significant protocol infrastructure vulnerability.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

This race condition will occur whenever:
1. An object ownership transfer and deletion happen in separate transaction batches
2. The batches are assigned to different concurrent processor threads
3. The deletion processing starts before the transfer processing commits to the database

With the default configuration of 5 concurrent processor tasks and typical blockchain activity involving object transfers and deletions (NFTs, tokens, etc.), this condition will occur regularly during normal operation. The vulnerability is **not** exploitable by external attackers but occurs naturally due to the concurrent processing architecture.

The frequency depends on:
- Batch size configuration
- Transaction throughput
- Object lifecycle patterns (transfers followed by deletions)
- Database commit latencies

## Recommendation

**Solution 1: Enforce Sequential Processing for Object-Related Batches**

Implement a coordination mechanism to ensure batches containing object operations are processed in strict version order:

```rust
// In runtime.rs or tailer.rs
// Add a mutex or semaphore that ensures object-related batches
// wait for all previous batches to commit before processing deletions
```

**Solution 2: Include All Previous Batch Updates in object_mapping**

Maintain a global object_mapping across batches that persists until database commit, ensuring deletion processing sees all recent updates:

```rust
// Share object_mapping across processor tasks with proper synchronization
// Clear only after confirming database commit
```

**Solution 3: Use Higher Isolation Level for Owner Queries**

Query the database with SERIALIZABLE isolation or use explicit locking:

```rust
// In get_object_owner()
conn.transaction::<_, Error, _>(|c| {
    // Use FOR UPDATE to lock the row
    current_objects::table
        .filter(current_objects::object_address.eq(object_address))
        .for_update()
        .first::<CurrentObjectQuery>(c)
})
```

**Recommended Approach**: Implement Solution 1 (sequential processing) combined with Solution 3 (isolation) for defense in depth. This maintains performance while ensuring correctness.

## Proof of Concept

The following demonstrates the race condition architecture:

**Setup:**
1. Deploy an object on-chain
2. Transfer ownership in transaction at version V
3. Delete object in transaction at version V+1000 (ensuring different batches)
4. Configure indexer with `processor_tasks: 5` and `batch_size: 500`
5. Observe concurrent processing

**Expected Incorrect Behavior:**
```
Batch A (V-500 to V+499): Contains ownership transfer Aliceâ†’Bob
Batch B (V+500 to V+1499): Contains deletion
Thread 1: Processing Batch B
Thread 2: Processing Batch A (slower)

Thread 1 query:
SELECT owner_address FROM current_objects WHERE object_address = X;
-- Returns: "Alice" (stale data, Thread 2 hasn't committed)

Thread 1 inserts:
INSERT INTO objects (..., owner_address) VALUES (..., "Alice");

Result: Deletion attributed to Alice despite Bob being actual owner
```

**Validation:**
Query the indexer after processing:
```sql
-- Find the transfer event
SELECT * FROM objects WHERE object_address = X AND is_deleted = false ORDER BY transaction_version DESC LIMIT 1;
-- Shows: owner_address = "Bob"

-- Find the deletion event  
SELECT * FROM objects WHERE object_address = X AND is_deleted = true ORDER BY transaction_version DESC LIMIT 1;
-- Shows: owner_address = "Alice" (INCORRECT - should be "Bob")
```

This demonstrates the permanent inconsistency in indexed data due to the race condition during concurrent batch processing.

## Notes

This vulnerability affects the **indexer component only** and does not impact blockchain consensus, on-chain state, or fund security. The blockchain itself maintains correct state; only the off-chain indexed representation is affected. However, given the indexer's critical role in the Aptos ecosystem for data access, this represents a significant infrastructure reliability issue that should be addressed to maintain data integrity guarantees.

### Citations

**File:** crates/indexer/src/runtime.rs (L210-219)
```rust
        let mut tasks = vec![];
        for _ in 0..processor_tasks {
            let other_tailer = tailer.clone();
            let task = tokio::spawn(async move { other_tailer.process_next_batch().await });
            tasks.push(task);
        }
        let batches = match futures::future::try_join_all(tasks).await {
            Ok(res) => res,
            Err(err) => panic!("Error processing transaction batches: {:?}", err),
        };
```

**File:** crates/indexer/src/processors/default_processor.rs (L531-576)
```rust
        let mut all_objects = vec![];
        let mut all_current_objects = HashMap::new();
        for txn in &transactions {
            let (changes, txn_version) = match txn {
                Transaction::UserTransaction(user_txn) => (
                    user_txn.info.changes.clone(),
                    user_txn.info.version.0 as i64,
                ),
                Transaction::BlockMetadataTransaction(bmt_txn) => {
                    (bmt_txn.info.changes.clone(), bmt_txn.info.version.0 as i64)
                },
                _ => continue,
            };

            for (index, wsc) in changes.iter().enumerate() {
                let index = index as i64;
                match wsc {
                    WriteSetChange::WriteResource(inner) => {
                        if let Some((object, current_object)) =
                            &Object::from_write_resource(inner, txn_version, index).unwrap()
                        {
                            all_objects.push(object.clone());
                            all_current_objects
                                .insert(object.object_address.clone(), current_object.clone());
                        }
                    },
                    WriteSetChange::DeleteResource(inner) => {
                        // Passing all_current_objects into the function so that we can get the owner of the deleted
                        // resource if it was handled in the same batch
                        if let Some((object, current_object)) = Object::from_delete_resource(
                            inner,
                            txn_version,
                            index,
                            &all_current_objects,
                            &mut conn,
                        )
                        .unwrap()
                        {
                            all_objects.push(object.clone());
                            all_current_objects
                                .insert(object.object_address.clone(), current_object.clone());
                        }
                    },
                    _ => {},
                }
            }
```

**File:** crates/indexer/src/models/v2_objects.rs (L118-119)
```rust
        if delete_resource.resource.to_string() == "0x1::object::ObjectGroup" {
            let resource = MoveResource::from_delete_resource(
```

**File:** crates/indexer/src/models/v2_objects.rs (L125-139)
```rust
            let previous_object = if let Some(object) = object_mapping.get(&resource.address) {
                object.clone()
            } else {
                match Self::get_object_owner(conn, &resource.address) {
                    Ok(owner) => owner,
                    Err(_) => {
                        aptos_logger::error!(
                            transaction_version = txn_version,
                            lookup_key = &resource.address,
                            "Missing object owner for object. You probably should backfill db.",
                        );
                        return Ok(None);
                    },
                }
            };
```

**File:** crates/indexer/src/models/v2_objects.rs (L166-192)
```rust
    /// This is actually not great because object owner can change. The best we can do now though
    fn get_object_owner(
        conn: &mut PgPoolConnection,
        object_address: &str,
    ) -> anyhow::Result<CurrentObject> {
        let mut retried = 0;
        while retried < QUERY_RETRIES {
            retried += 1;
            match CurrentObjectQuery::get_by_address(object_address, conn) {
                Ok(res) => {
                    return Ok(CurrentObject {
                        object_address: res.object_address,
                        owner_address: res.owner_address,
                        state_key_hash: res.state_key_hash,
                        allow_ungated_transfer: res.allow_ungated_transfer,
                        last_guid_creation_num: res.last_guid_creation_num,
                        last_transaction_version: res.last_transaction_version,
                        is_deleted: res.is_deleted,
                    })
                },
                Err(_) => {
                    std::thread::sleep(std::time::Duration::from_millis(QUERY_RETRY_DELAY_MS));
                },
            }
        }
        Err(anyhow::anyhow!("Failed to get object owner"))
    }
```

**File:** aptos-move/framework/aptos-framework/sources/object.move (L508-509)
```text
        object.owner = to;
    }
```
