# Audit Report

## Title
Secret Share Loss Due to Missing Reset During State Synchronization

## Summary
During state synchronization, the execution client's `reset()` function fails to reset the `SecretShareManager`, causing stale secret shares to remain in memory. When consensus resumes after sync, attempts to add new shares for rounds that already have stale shares in `PendingDecision` state trigger panics, preventing validators from participating in the randomness beacon and potentially stalling consensus.

## Finding Description

The vulnerability exists in the consensus reset mechanism during state synchronization. The critical flaw is that **the `reset()` function does not reset the SecretShareManager**, while it does reset the RandManager and BufferManager. [1](#0-0) 

The reset function only extracts and uses `reset_tx_to_rand_manager` and `reset_tx_to_buffer_manager`, completely omitting `reset_tx_to_secret_share_manager` despite it being available in the BufferManagerHandle structure. [2](#0-1) 

This creates a state management flaw where:

1. **Pre-Sync State**: Node processes block A at round R, computing and storing a secret share with metadata `(round: R, block_id: A, digest: DA)`. The share transitions to `PendingDecision` state in the `secret_share_map`.

2. **State Sync Event**: Node falls behind (common during network delays, slow execution, or validator restarts) and triggers state synchronization via `sync_to_target`. [3](#0-2) 

3. **Incomplete Reset**: The reset function is called, but only rand_manager and buffer_manager are reset. The SecretShareManager continues running with its old state, including stale shares in `secret_share_map`.

4. **Post-Sync Processing**: After sync completes and consensus resumes, when the node processes blocks (potentially including rounds it had previously started processing with different block contents), it attempts to add new secret shares.

5. **Share Addition Failure**: The `add_self_share` function is called with the new share. [4](#0-3) 

This calls `add_share_with_metadata` which encounters the existing `PendingDecision` item and explicitly bails: [5](#0-4) 

The error propagates back to the `expect` statement, causing a **panic that crashes the consensus thread**.

6. **RPC Request Failures**: When other validators request this node's share for a round with specific metadata, the `get_self_share` function filters by exact metadata match: [6](#0-5) 

Since `SecretShareMetadata` requires all fields to match exactly (epoch, round, timestamp, block_id, digest): [7](#0-6) 

Stale shares with old metadata fail the filter at line 302, returning `Ok(None)` and triggering a warning: [8](#0-7) 

7. **No Cleanup Mechanism**: There is no garbage collection for old rounds in the `secret_share_map`, so stale shares persist indefinitely within an epoch.

Furthermore, when `process_reset` IS called on SecretShareManager (during epoch end, not during state sync), it also fails to clear the map: [9](#0-8) 

## Impact Explanation

**Severity: High** (up to $50,000 per Aptos Bug Bounty)

This vulnerability causes **Validator Node Crashes and Consensus Liveness Issues**, qualifying for HIGH severity under the "Validator node slowdowns" category:

1. **Immediate Crash**: The panic at line 146 crashes the consensus thread for affected validators, removing them from participation until restart.

2. **Share Aggregation Failure**: Even if the panic is somehow caught, validators with stale shares cannot provide valid shares for RPC requests, preventing threshold achievement.

3. **Cascade Effect**: State sync is often triggered simultaneously across multiple validators (e.g., during network partitions or periods of high load), causing multiple nodes to fail concurrently.

4. **Randomness Beacon Failure**: Without sufficient valid shares, the secret sharing threshold cannot be reached, preventing randomness generation.

5. **Consensus Impact**: Since Aptos randomness is integrated into the consensus protocol for leader election and transaction ordering, randomness beacon failure can halt or significantly degrade block production.

6. **Persistence**: The issue persists until the affected validator restarts its consensus component or a new epoch begins, potentially causing extended periods of degraded consensus performance.

## Likelihood Explanation

**Likelihood: High**

This vulnerability has high likelihood of occurring in production:

1. **Frequent Trigger**: State synchronization is a common operation in distributed blockchain networks:
   - Validators falling behind due to temporary performance issues
   - Network delays or partitions
   - Validator restarts or upgrades
   - Catching up after being offline [10](#0-9) 

2. **No Malicious Action Required**: This is purely a protocol-level bug triggered by normal consensus operations. No attacker involvement is necessary.

3. **Deterministic Failure**: Once a validator has processed shares for a round and then undergoes state sync without proper cleanup, the failure is deterministic when those rounds are encountered again.

4. **Production-Realistic Scenarios**: The conditions for this bug (partial block processing followed by state sync) occur regularly in real-world deployments under normal network conditions.

## Recommendation

**Fix 1: Include SecretShareManager in reset() function**

Modify the `reset()` function in `execution_client.rs` to also reset the SecretShareManager:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager, reset_tx_to_secret_share_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
            handle.reset_tx_to_secret_share_manager.clone(),  // ADD THIS
        )
    };

    // Reset rand manager
    if let Some(mut reset_tx) = reset_tx_to_rand_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::RandResetDropped)?;
        ack_rx.await.map_err(|_| Error::RandResetDropped)?;
    }

    // Reset secret share manager - ADD THIS BLOCK
    if let Some(mut reset_tx) = reset_tx_to_secret_share_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::SecretShareResetDropped)?;
        ack_rx.await.map_err(|_| Error::SecretShareResetDropped)?;
    }

    // Reset buffer manager
    if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
        let (tx, rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::ResetDropped)?;
        rx.await.map_err(|_| Error::ResetDropped)?;
    }

    Ok(())
}
```

**Fix 2: Clear secret_share_map during reset**

Modify `process_reset` in `SecretShareManager` to clear the `secret_share_map`:

```rust
fn process_reset(&mut self, request: ResetRequest) {
    let ResetRequest { tx, signal } = request;
    let target_round = match signal {
        ResetSignal::Stop => 0,
        ResetSignal::TargetRound(round) => round,
    };
    self.block_queue = BlockQueue::new();
    
    // Clear stale shares - ADD THIS
    {
        let mut store = self.secret_share_store.lock();
        store.secret_share_map.clear();  // Clear all stale shares
        store.update_highest_known_round(target_round);
    }
    
    self.stop = matches!(signal, ResetSignal::Stop);
    let _ = tx.send(ResetAck::default());
}
```

**Fix 3: Add garbage collection for old rounds**

Implement periodic cleanup of shares for rounds below a threshold:

```rust
impl SecretShareStore {
    pub fn gc_rounds_before(&mut self, min_round: Round) {
        self.secret_share_map.retain(|round, _| *round >= min_round);
    }
}
```

## Proof of Concept

The vulnerability can be demonstrated through the following scenario:

1. Start a validator node with secret sharing enabled
2. Allow the node to process several blocks, computing secret shares
3. Trigger state synchronization (e.g., by pausing the node briefly then resuming)
4. Observe that `reset()` is called but `SecretShareManager` is not reset
5. When new blocks arrive, attempts to add shares for previously-seen rounds will panic at the `expect("Add self dec share should succeed")` line

The panic can be verified by examining validator logs for the error message or by tracing through the code execution during state sync events.

**Notes:**
- This vulnerability affects the consensus liveness guarantee and validator availability
- The issue is deterministic once triggered and does not require precise timing
- Multiple validators can be affected simultaneously during network-wide sync events
- The bug exists in the core consensus pipeline, not in test code or external utilities
- Recovery requires validator restart or epoch transition

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L124-131)
```rust
struct BufferManagerHandle {
    pub execute_tx: Option<UnboundedSender<OrderedBlocks>>,
    pub commit_tx:
        Option<aptos_channel::Sender<AccountAddress, (AccountAddress, IncomingCommitRequest)>>,
    pub reset_tx_to_buffer_manager: Option<UnboundedSender<ResetRequest>>,
    pub reset_tx_to_rand_manager: Option<UnboundedSender<ResetRequest>>,
    pub reset_tx_to_secret_share_manager: Option<UnboundedSender<ResetRequest>>,
}
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L65-77)
```rust
    pub fn need_sync_for_ledger_info(&self, li: &LedgerInfoWithSignatures) -> bool {
        const MAX_PRECOMMIT_GAP: u64 = 200;
        let block_not_exist = self.ordered_root().round() < li.commit_info().round()
            && !self.block_exists(li.commit_info().id());
        // TODO move min gap to fallback (30) to config, and if configurable make sure the value is
        // larger than buffer manager MAX_BACKLOG (20)
        let max_commit_gap = 30.max(2 * self.vote_back_pressure_limit);
        let min_commit_round = li.commit_info().round().saturating_sub(max_commit_gap);
        let current_commit_round = self.commit_root().round();

        if let Some(pre_commit_status) = self.pre_commit_status() {
            let mut status_guard = pre_commit_status.lock();
            if block_not_exist || status_guard.round() < min_commit_round {
```

**File:** consensus/src/block_storage/sync_manager.rs (L512-514)
```rust
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L144-147)
```rust
            secret_share_store.update_highest_known_round(block.round());
            secret_share_store
                .add_self_share(self_secret_share.clone())
                .expect("Add self dec share should succeed");
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L299-304)
```rust
                    Ok(None) => {
                        warn!(
                            "Self secret share could not be found for RPC request {}",
                            request.metadata().round
                        );
                    },
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L175-177)
```rust
            SecretShareItem::PendingDecision { .. } => {
                bail!("Cannot add self share in PendingDecision state");
            },
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L288-303)
```rust
    pub fn get_self_share(
        &mut self,
        metadata: &SecretShareMetadata,
    ) -> anyhow::Result<Option<SecretShare>> {
        ensure!(
            metadata.round <= self.highest_known_round,
            "Request share from future round {}, highest known round {}",
            metadata.round,
            self.highest_known_round
        );
        Ok(self
            .secret_share_map
            .get(&metadata.round)
            .and_then(|item| item.get_self_share())
            .filter(|share| &share.metadata == metadata))
    }
```

**File:** types/src/secret_sharing.rs (L32-39)
```rust
#[derive(Clone, Serialize, Deserialize, Debug, Default, PartialEq, Eq, Hash)]
pub struct SecretShareMetadata {
    pub epoch: u64,
    pub round: Round,
    pub timestamp: u64,
    pub block_id: HashValue,
    pub digest: Digest,
}
```
