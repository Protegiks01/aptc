# Audit Report

## Title
Unreachable Error Handler in Legacy Storage Service Causes Permanent State Sync Failure on Oversized Transaction Outputs

## Summary
The legacy storage service functions contain unreachable terminal error handlers that were intended to prevent oversized single items from being served. Due to early return logic, these protections never execute, allowing oversized responses to be returned. When a single transaction output exceeds network compression limits (~62 MiB), nodes enter an infinite retry loop, causing permanent state sync failure without the ability to progress past the problematic version.

## Finding Description

The vulnerability exists in four legacy storage service functions that share identical flawed logic: [1](#0-0) 

The critical flaw is in the execution order at lines 758-760. When `num_outputs_to_fetch == 1`, the function returns immediately **before** checking if the response overflows the network frame size limit. This early return bypasses the intended protection, making the terminal error at lines 779-783 **unreachable** under normal circumstances.

**Execution Flow:**
1. Function starts with binary search, halving the chunk size when overflow detected
2. When `num_outputs_to_fetch` reaches 1, line 758 checks this condition
3. Line 759 returns the response immediately, bypassing overflow check at line 763
4. Oversized response proceeds to `StorageServiceResponse::new()` for compression [2](#0-1) 

5. If compressed size exceeds `MAX_APPLICATION_MESSAGE_SIZE` (~62 MiB), compression fails
6. Error propagates to client as `StorageServiceError::InternalError` [3](#0-2) 

7. Data streaming client receives error and retries via `handle_data_client_error` [4](#0-3) 

8. After 5 retries (default `max_request_retry`), stream terminates with EndOfStream [5](#0-4) 

9. Continuous syncer resets and creates new stream from same version [6](#0-5) 

10. **Infinite loop**: Node continuously requests same range, hits same oversized output, fails, and restarts

**Same Pattern in All Legacy Functions:**
- `get_epoch_ending_ledger_infos_by_size_legacy()` (lines 318-343)
- `get_transactions_with_proof_by_size_legacy()` (lines 536-562)  
- `get_transaction_outputs_with_proof_by_size_legacy()` (lines 758-783)
- `get_state_value_chunk_with_proof_by_size_legacy()` (lines 1005-1031)

**Why Oversized Items Can Occur:**
Transaction outputs can legitimately exceed compression limits due to:
- Large write sets from batch resource creation
- Mass event emission (e.g., governance proposals affecting many accounts)
- Smart contract deployments with extensive metadata
- State migrations creating numerous large resources

Network limits defined at: [7](#0-6) [8](#0-7) 

## Impact Explanation

**Severity: High** (Validator Node Unavailability)

This vulnerability causes **permanent state sync failure** with the following impacts:

1. **Node Liveness Loss**: Affected nodes cannot sync past the problematic version, preventing them from:
   - Processing new transactions
   - Participating in consensus (for validator nodes)
   - Serving API requests with current data

2. **Network Degradation**: If multiple nodes encounter the same oversized transaction:
   - Reduced validator set participation
   - Decreased network fault tolerance
   - Potential consensus delays if sufficient validators affected

3. **No Automatic Recovery**: The node enters an infinite retry loop with no self-healing mechanism:
   - Stream recreation attempts identical request
   - Node restart does not help
   - Manual intervention or configuration changes required

4. **Attack Surface**: While not requiring malicious intent, this can be exploited:
   - Adversary submits large legitimate transactions
   - Targets specific versions to disrupt node synchronization
   - Particularly effective during network upgrades when many nodes sync

**Why This Meets High Severity:**
Per Aptos bug bounty criteria, "Validator node slowdowns" and "Significant protocol violations" qualify as High severity. Complete inability to sync constitutes both slowdown (infinite) and protocol violation (inability to maintain state consistency invariant).

## Likelihood Explanation

**Likelihood: Medium to High**

**Factors Increasing Likelihood:**
1. **Natural Occurrence**: Large transactions occur organically in production:
   - Governance proposals affecting many accounts
   - DEX liquidity pool initializations
   - NFT collection mints with extensive metadata
   - Protocol upgrades with state migrations

2. **Default Configuration**: Legacy implementation is still default in some configurations: [9](#0-8) 

3. **Cascading Effect**: Once one node is affected, other syncing nodes will hit the same version

4. **No Warning**: No pre-execution validation of transaction output size, so oversized outputs are discovered only during sync

**Mitigation Factors:**
- Transactions approaching 62 MiB compressed are relatively rare
- Gas limits indirectly constrain transaction size
- New implementation exists (though has similar issue with first-item handling)

## Recommendation

**Immediate Fix: Remove Early Return or Add Size Check**

The early return at line 758-760 should be modified to check response size BEFORE returning:

```rust
fn get_transaction_outputs_with_proof_by_size_legacy(
    &self,
    proof_version: u64,
    start_version: u64,
    end_version: u64,
    mut num_outputs_to_fetch: u64,
    max_response_size: u64,
) -> Result<TransactionDataWithProofResponse, Error> {
    while num_outputs_to_fetch >= 1 {
        let output_list_with_proof = self.storage.get_transaction_outputs(
            start_version,
            num_outputs_to_fetch,
            proof_version,
        )?;
        let response = TransactionDataWithProofResponse {
            transaction_data_response_type: TransactionDataResponseType::TransactionOutputData,
            transaction_list_with_proof: None,
            transaction_output_list_with_proof: Some(output_list_with_proof),
        };
        
        // Check overflow BEFORE checking if this is the last item
        let (overflow_frame, num_bytes) =
            check_overflow_network_frame(&response, max_response_size)?;
        
        if !overflow_frame {
            return Ok(response);
        } else if num_outputs_to_fetch == 1 {
            // Single item too large - return error instead of oversized response
            return Err(Error::UnexpectedErrorEncountered(format!(
                "Unable to serve the get_transaction_outputs_with_proof request! \
                Proof version: {:?}, start version: {:?}, end version: {:?}. \
                Single transaction output at version {:?} exceeds maximum size ({:?} bytes, limit: {:?})",
                proof_version, start_version, end_version, start_version, num_bytes, max_response_size
            )));
        } else {
            // Continue halving
            metrics::increment_chunk_truncation_counter(
                metrics::TRUNCATION_FOR_SIZE,
                DataResponse::TransactionDataWithProof(response).get_label(),
            );
            let new_num_outputs_to_fetch = num_outputs_to_fetch / 2;
            debug!("The request for {:?} outputs was too large (num bytes: {:?}, limit: {:?}). Retrying with {:?}.",
                num_outputs_to_fetch, num_bytes, max_response_size, new_num_outputs_to_fetch);
            num_outputs_to_fetch = new_num_outputs_to_fetch;
        }
    }
    
    // This should now be reachable for truly oversized single items
    Err(Error::UnexpectedErrorEncountered(format!(
        "Unable to serve the get_transaction_outputs_with_proof request! \
        Proof version: {:?}, start version: {:?}, end version: {:?}. \
        The data cannot fit into a single network frame!",
        proof_version, start_version, end_version
    )))
}
```

**Apply same fix to all four legacy functions.**

**Long-term Solution:**
1. **Enable size-aware chunking by default** across all networks
2. **Add pre-transaction validation** to reject transactions likely to produce oversized outputs
3. **Implement graceful degradation** for unavoidable oversized items (e.g., skip with checkpoint mechanism)
4. **Monitor and alert** on oversized transaction outputs in production

## Proof of Concept

**Scenario Setup:**
1. Deploy a Move module that creates a transaction with output size > 62 MiB (compressed)
2. Configure node with `enable_size_and_time_aware_chunking = false`
3. Attempt to sync past this transaction

**Reproduction Steps:**

```rust
// Test in state-sync/storage-service/server/src/tests/

#[tokio::test]
async fn test_oversized_single_output_causes_infinite_retry() {
    // Setup storage with one oversized transaction output
    let (mut mock_db, _) = MockDatabaseReader::new();
    
    // Create a transaction output with size exceeding MAX_APPLICATION_MESSAGE_SIZE
    let oversized_write_set = create_write_set_with_size(70 * 1024 * 1024); // 70 MiB
    let oversized_output = TransactionOutput::new(
        oversized_write_set,
        vec![], // events
        1000, // gas_used
        TransactionStatus::Keep(ExecutionStatus::Success),
        TransactionAuxiliaryData::None,
    );
    
    mock_db.expect_get_transaction_outputs()
        .returning(move |start, _limit, _proof| {
            Ok(create_output_list_with_proof(start, vec![oversized_output.clone()]))
        });
    
    let storage_reader = StorageReader::new(
        StorageServiceConfig {
            enable_size_and_time_aware_chunking: false, // Use legacy implementation
            ..Default::default()
        },
        Arc::new(mock_db),
        TimeService::mock(),
    );
    
    // Attempt to fetch - should enter retry loop
    let result = storage_reader.get_transaction_outputs_with_proof(
        100, // proof_version
        50,  // start_version
        50,  // end_version (single output)
    );
    
    // Legacy implementation returns oversized response without error
    // This will fail at compression layer when sent over network
    assert!(result.is_ok());
    let response = result.unwrap();
    
    // Verify response would fail compression
    let compression_result = StorageServiceResponse::new(
        DataResponse::TransactionOutputsWithProof(
            response.transaction_output_list_with_proof.unwrap().into()
        ),
        true // enable compression
    );
    
    // Should fail due to size exceeding MAX_APPLICATION_MESSAGE_SIZE
    assert!(compression_result.is_err());
}
```

**Expected Behavior:** Node should return explicit error for oversized single items instead of entering infinite retry loop.

**Actual Behavior:** Node returns oversized response, fails at compression, retries infinitely, causing permanent sync failure.

## Notes

This vulnerability demonstrates a critical disconnect between intended protection (terminal error handlers) and actual implementation (early return bypass). The unreachable error code at lines 779-783 suggests the original developers recognized the risk of oversized single items but the protection was inadvertently disabled by the early return logic at line 758-760.

The issue affects all nodes using legacy storage service implementation and cannot be recovered without manual intervention or configuration changes. This violates the **State Consistency** invariant that requires continuous sync progress and the **Resource Limits** invariant that should constrain response sizes.

### Citations

**File:** state-sync/storage-service/server/src/storage.rs (L739-784)
```rust
    fn get_transaction_outputs_with_proof_by_size_legacy(
        &self,
        proof_version: u64,
        start_version: u64,
        end_version: u64,
        mut num_outputs_to_fetch: u64,
        max_response_size: u64,
    ) -> Result<TransactionDataWithProofResponse, Error> {
        while num_outputs_to_fetch >= 1 {
            let output_list_with_proof = self.storage.get_transaction_outputs(
                start_version,
                num_outputs_to_fetch,
                proof_version,
            )?;
            let response = TransactionDataWithProofResponse {
                transaction_data_response_type: TransactionDataResponseType::TransactionOutputData,
                transaction_list_with_proof: None,
                transaction_output_list_with_proof: Some(output_list_with_proof),
            };
            if num_outputs_to_fetch == 1 {
                return Ok(response); // We cannot return less than a single item
            }

            // Attempt to divide up the request if it overflows the message size
            let (overflow_frame, num_bytes) =
                check_overflow_network_frame(&response, max_response_size)?;
            if !overflow_frame {
                return Ok(response);
            } else {
                metrics::increment_chunk_truncation_counter(
                    metrics::TRUNCATION_FOR_SIZE,
                    DataResponse::TransactionDataWithProof(response).get_label(),
                );
                let new_num_outputs_to_fetch = num_outputs_to_fetch / 2;
                debug!("The request for {:?} outputs was too large (num bytes: {:?}, limit: {:?}). Retrying with {:?}.",
                    num_outputs_to_fetch, num_bytes, max_response_size, new_num_outputs_to_fetch);
                num_outputs_to_fetch = new_num_outputs_to_fetch; // Try again with half the amount of data
            }
        }

        Err(Error::UnexpectedErrorEncountered(format!(
            "Unable to serve the get_transaction_outputs_with_proof request! Proof version: {:?}, \
            start version: {:?}, end version: {:?}. The data cannot fit into a single network frame!",
            proof_version, start_version, end_version
        )))
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L74-93)
```rust
    pub fn new(data_response: DataResponse, perform_compression: bool) -> Result<Self, Error> {
        if perform_compression {
            // Serialize and compress the raw data
            let raw_data = bcs::to_bytes(&data_response)
                .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
            let compressed_data = aptos_compression::compress(
                raw_data,
                CompressionClient::StateSync,
                MAX_APPLICATION_MESSAGE_SIZE,
            )?;

            // Create the compressed response
            let label = data_response.get_label().to_string() + COMPRESSION_SUFFIX_LABEL;
            Ok(StorageServiceResponse::CompressedResponse(
                label,
                compressed_data,
            ))
        } else {
            Ok(StorageServiceResponse::RawResponse(data_response))
        }
```

**File:** state-sync/storage-service/server/src/handler.rs (L195-202)
```rust
        // Transform the request error into a storage service error (for the client)
        process_result.map_err(|error| match error {
            Error::InvalidRequest(error) => StorageServiceError::InvalidRequest(error),
            Error::TooManyInvalidRequests(error) => {
                StorageServiceError::TooManyInvalidRequests(error)
            },
            error => StorageServiceError::InternalError(error.to_string()),
        })
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L710-725)
```rust
    /// Handles an error returned by the data client in relation to a request
    fn handle_data_client_error(
        &mut self,
        data_client_request: &DataClientRequest,
        data_client_error: &aptos_data_client::error::Error,
    ) -> Result<(), Error> {
        // Log the error
        warn!(LogSchema::new(LogEntry::ReceivedDataResponse)
            .stream_id(self.data_stream_id)
            .event(LogEvent::Error)
            .error(&data_client_error.clone().into())
            .message("Encountered a data client error!"));

        // TODO(joshlind): can we identify the best way to react to the error?
        self.resend_data_client_request(data_client_request)
    }
```

**File:** config/src/config/state_sync_config.rs (L12-14)
```rust
// Whether to enable size and time-aware chunking (for non-production networks).
// Note: once this becomes stable, we should enable it for all networks (e.g., Mainnet).
const ENABLE_SIZE_AND_TIME_AWARE_CHUNKING: bool = true;
```

**File:** config/src/config/state_sync_config.rs (L16-21)
```rust
// The maximum message size per state sync message
const SERVER_MAX_MESSAGE_SIZE: usize = 10 * 1024 * 1024; // 10 MiB

// The maximum message size per state sync message (for v2 data requests)
const CLIENT_MAX_MESSAGE_SIZE_V2: usize = 20 * 1024 * 1024; // 20 MiB (used for v2 data requests)
const SERVER_MAX_MESSAGE_SIZE_V2: usize = 40 * 1024 * 1024; // 40 MiB (used for v2 data requests)
```

**File:** config/src/config/state_sync_config.rs (L254-277)
```rust
    /// Maximum number of retries for a single client request before a data
    /// stream will terminate.
    pub max_request_retry: u64,

    /// Maximum lag (in seconds) we'll tolerate when sending subscription requests
    pub max_subscription_stream_lag_secs: u64,

    /// The interval (milliseconds) at which to check the progress of each stream.
    pub progress_check_interval_ms: u64,
}

impl Default for DataStreamingServiceConfig {
    fn default() -> Self {
        Self {
            dynamic_prefetching: DynamicPrefetchingConfig::default(),
            enable_subscription_streaming: false,
            global_summary_refresh_interval_ms: 50,
            max_concurrent_requests: MAX_CONCURRENT_REQUESTS,
            max_concurrent_state_requests: MAX_CONCURRENT_STATE_REQUESTS,
            max_data_stream_channel_sizes: 50,
            max_notification_id_mappings: 300,
            max_num_consecutive_subscriptions: 45, // At ~3 blocks per second, this should last ~15 seconds
            max_pending_requests: 50,
            max_request_retry: 5,
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L468-491)
```rust
    /// Handles the end of stream notification or an invalid payload by
    /// terminating the stream appropriately.
    async fn handle_end_of_stream_or_invalid_payload(
        &mut self,
        data_notification: DataNotification,
    ) -> Result<(), Error> {
        // Calculate the feedback based on the notification
        let notification_feedback = match data_notification.data_payload {
            DataPayload::EndOfStream => NotificationFeedback::EndOfStream,
            _ => NotificationFeedback::PayloadTypeIsIncorrect,
        };
        let notification_and_feedback =
            NotificationAndFeedback::new(data_notification.notification_id, notification_feedback);

        // Reset the stream
        self.reset_active_stream(Some(notification_and_feedback))
            .await?;

        // Return an error if the payload was invalid
        match data_notification.data_payload {
            DataPayload::EndOfStream => Ok(()),
            _ => Err(Error::InvalidPayload("Unexpected payload type!".into())),
        }
    }
```

**File:** config/src/config/network_config.rs (L47-50)
```rust
pub const MAX_APPLICATION_MESSAGE_SIZE: usize =
    (MAX_MESSAGE_SIZE - MAX_MESSAGE_METADATA_SIZE) - MESSAGE_PADDING_SIZE; /* The message size that applications should check against */
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```
