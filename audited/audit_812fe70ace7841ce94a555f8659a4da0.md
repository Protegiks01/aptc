# Audit Report

## Title
Non-Atomic Broadcast in Consensus Network Layer Breaks Message Delivery Guarantees

## Summary
The `send_to_many` function used by `broadcast_without_self()` is not atomic and can result in partial message delivery to validators without proper error propagation, violating consensus broadcast assumptions and potentially causing liveness failures.

## Finding Description

The consensus layer's `broadcast_without_self()` function delegates to `send_to_many()`, which iterates through recipients and pushes messages sequentially. This implementation has two critical flaws:

**1. Non-Atomic Channel Push Operations** [1](#0-0) 

The implementation loops through recipients, pushing messages one-by-one. If the channel is unexpectedly shutdown mid-loop, the function returns early with an error, but messages already enqueued to earlier recipients remain delivered. This creates partial delivery where some validators receive the message while others don't.

**2. Silent Message Dropping on Queue Congestion** [2](#0-1) 

Even when `push()` succeeds (returns `Ok`), messages can be silently dropped if the per-peer queue is at capacity (default 1024 messages). With FIFO queue style, the newest message is dropped and `Some(T)` is returned, but this dropped message is NOT propagated as an error to the caller. [3](#0-2) 

The dropped message only triggers a status channel notification if one was provided, but `send_to_many` doesn't use status channels.

**3. Inadequate Error Handling** [4](#0-3) 

When `send_to_many` fails, the error is merely logged as a warning. The counter is incremented before the send, leading to inaccurate metrics. The sender proceeds without knowing the broadcast status.

**Attack Scenario:**

A malicious actor could exploit this by:
1. Flooding the network to fill per-peer message queues
2. When a validator broadcasts a critical consensus message (vote, proposal, sync info), some messages are silently dropped
3. Validators develop inconsistent views of the protocol state
4. Consensus stalls or validators timeout waiting for messages others received

## Impact Explanation

This constitutes **High Severity** under the Aptos bug bounty criteria:

- **Significant protocol violations**: The consensus protocol assumes reliable eventual message delivery in the partial synchrony model. Partial delivery without sender notification violates this assumption.

- **Validator node slowdowns**: Queue congestion leading to dropped consensus messages causes validators to timeout, miss proposals, and require sync operations, degrading performance.

- **Potential consensus liveness failures**: If critical messages like proposals or quorum certificates fail to reach enough validators, the network may fail to make progress until recovery mechanisms engage.

While this primarily affects **liveness** rather than **safety** (thanks to SafetyRules and other protections), it still represents a significant deviation from protocol assumptions that could be exploited for denial-of-service or compounded with other vulnerabilities.

## Likelihood Explanation

**Medium to High Likelihood:**

- **Normal operation**: Under typical conditions with healthy networks, this may not manifest frequently due to the large queue size (1024).

- **Attack scenarios**: An adversary can deliberately trigger this by:
  - Flooding the network with valid but unnecessary messages
  - Targeting specific validators' queues before critical consensus events
  - Exploiting periods of high transaction volume

- **No special privileges required**: Any network participant can send messages that fill queues; the attack doesn't require validator access.

- **Cascading effects**: Once queues fill, legitimate consensus messages from honest validators are dropped, amplifying the impact.

## Recommendation

Implement reliable broadcast semantics with proper error handling and retry logic:

```rust
pub fn send_to_many(
    &self,
    recipients: impl Iterator<Item = PeerId>,
    protocol_id: ProtocolId,
    mdata: Bytes,
) -> Result<(), PeerManagerError> {
    let msg = Message { protocol_id, mdata };
    let recipients: Vec<_> = recipients.collect();
    
    // Track which sends succeeded
    let mut failed_recipients = Vec::new();
    
    for recipient in recipients {
        match self.inner.push(
            (recipient, protocol_id),
            PeerManagerRequest::SendDirectSend(recipient, msg.clone()),
        ) {
            Ok(_) => {},
            Err(e) => {
                failed_recipients.push(recipient);
                // Log but continue to attempt all sends
                warn!("Failed to send to {:?}: {:?}", recipient, e);
            }
        }
    }
    
    // Return error with partial failure information
    if !failed_recipients.is_empty() {
        return Err(PeerManagerError::PartialBroadcastFailure(failed_recipients));
    }
    
    Ok(())
}
```

Additionally, in `broadcast_without_self`:

```rust
pub fn broadcast_without_self(&self, msg: ConsensusMsg) {
    // ... existing code ...
    
    if let Err(err) = self
        .consensus_network_client
        .send_to_many(other_validators, msg)
    {
        error!(error = ?err, "CRITICAL: Broadcast failed - may impact consensus");
        counters::CONSENSUS_BROADCAST_FAILURES.inc();
        // Consider implementing retry logic here
    }
}
```

Implement proper monitoring and alerting for broadcast failures, and consider using the existing `ReliableBroadcast` abstraction for consensus-critical messages.

## Proof of Concept

```rust
// Reproduction test demonstrating partial delivery
#[tokio::test]
async fn test_partial_broadcast_on_channel_shutdown() {
    let (sender, mut receiver) = aptos_channel::new(QueueStyle::FIFO, 10, None);
    let peer_mgr_sender = PeerManagerRequestSender::new(sender);
    
    // Setup validators
    let validators: Vec<PeerId> = (0..5).map(|_| PeerId::random()).collect();
    
    // Close receiver mid-broadcast to simulate channel shutdown
    tokio::spawn(async move {
        tokio::time::sleep(Duration::from_millis(5)).await;
        drop(receiver); // Close channel
    });
    
    let msg = Message {
        protocol_id: ProtocolId::ConsensusDirectSendBcs,
        mdata: Bytes::from("test"),
    };
    
    // Attempt broadcast - should fail partway through
    let result = peer_mgr_sender.send_to_many(
        validators.iter().cloned(),
        ProtocolId::ConsensusDirectSendBcs,
        msg.mdata,
    );
    
    // Assert: Some messages were sent before failure
    // In real scenario: first 1-2 validators receive message, others don't
    assert!(result.is_err(), "Expected partial failure");
    
    // The problem: Caller doesn't know which validators received the message!
}
```

## Notes

While AptosBFT includes recovery mechanisms (timeouts, sync info, block retrieval) that mitigate the liveness impact of message loss, the **assumption of reliable broadcast** is documented in the README as part of the partial synchrony model. [5](#0-4) 

The comment in the code explicitly acknowledges lack of delivery guarantees: [6](#0-5) 

However, the gap between this best-effort delivery and the protocol's expectations creates a vulnerability surface that could be exploited for targeted denial-of-service attacks or compounded with other issues to cause more severe impacts.

### Citations

**File:** network/framework/src/peer_manager/senders.rs (L68-86)
```rust
    pub fn send_to_many(
        &self,
        recipients: impl Iterator<Item = PeerId>,
        protocol_id: ProtocolId,
        mdata: Bytes,
    ) -> Result<(), PeerManagerError> {
        let msg = Message { protocol_id, mdata };
        for recipient in recipients {
            // We return `Err` early here if the send fails. Since sending will
            // only fail if the queue is unexpectedly shutdown (i.e., receiver
            // dropped early), we know that we can't make further progress if
            // this send fails.
            self.inner.push(
                (recipient, protocol_id),
                PeerManagerRequest::SendDirectSend(recipient, msg.clone()),
            )?;
        }
        Ok(())
    }
```

**File:** crates/channel/src/message_queues.rs (L112-152)
```rust
    pub(crate) fn push(&mut self, key: K, message: T) -> Option<T> {
        if let Some(c) = self.counters.as_ref() {
            c.with_label_values(&["enqueued"]).inc();
        }

        let key_message_queue = self
            .per_key_queue
            .entry(key.clone())
            // Only allocate a small initial queue for a new key. Previously, we
            // allocated a queue with all `max_queue_size_per_key` entries;
            // however, this breaks down when we have lots of transient peers.
            // For example, many of our queues have a max capacity of 1024. To
            // handle a single rpc from a transient peer, we would end up
            // allocating ~ 96 b * 1024 ~ 64 Kib per queue.
            .or_insert_with(|| VecDeque::with_capacity(1));

        // Add the key to our round-robin queue if it's not already there
        if key_message_queue.is_empty() {
            self.round_robin_queue.push_back(key);
        }

        // Push the message to the actual key message queue
        if key_message_queue.len() >= self.max_queue_size.get() {
            if let Some(c) = self.counters.as_ref() {
                c.with_label_values(&["dropped"]).inc();
            }
            match self.queue_style {
                // Drop the newest message for FIFO
                QueueStyle::FIFO => Some(message),
                // Drop the oldest message for LIFO
                QueueStyle::LIFO | QueueStyle::KLAST => {
                    let oldest = key_message_queue.pop_front();
                    key_message_queue.push_back(message);
                    oldest
                },
            }
        } else {
            key_message_queue.push_back(message);
            None
        }
    }
```

**File:** crates/channel/src/aptos_channel.rs (L91-112)
```rust
    pub fn push_with_feedback(
        &self,
        key: K,
        message: M,
        status_ch: Option<oneshot::Sender<ElementStatus<M>>>,
    ) -> Result<()> {
        let mut shared_state = self.shared_state.lock();
        ensure!(!shared_state.receiver_dropped, "Channel is closed");
        debug_assert!(shared_state.num_senders > 0);

        let dropped = shared_state.internal_queue.push(key, (message, status_ch));
        // If this or an existing message had to be dropped because of the queue being full, we
        // notify the corresponding status channel if it was registered.
        if let Some((dropped_val, Some(dropped_status_ch))) = dropped {
            // Ignore errors.
            let _err = dropped_status_ch.send(ElementStatus::Dropped(dropped_val));
        }
        if let Some(w) = shared_state.waker.take() {
            w.wake();
        }
        Ok(())
    }
```

**File:** consensus/src/network.rs (L358-362)
```rust
    /// Tries to send the given msg to all the participants.
    ///
    /// The future is fulfilled as soon as the message is put into the mpsc channel to network
    /// internal (to provide back pressure), it does not indicate the message is delivered or sent
    /// out.
```

**File:** consensus/src/network.rs (L387-408)
```rust
    pub fn broadcast_without_self(&self, msg: ConsensusMsg) {
        fail_point!("consensus::send::any", |_| ());

        let self_author = self.author;
        let mut other_validators: Vec<_> = self
            .validators
            .get_ordered_account_addresses_iter()
            .filter(|author| author != &self_author)
            .collect();
        self.sort_peers_by_latency(&mut other_validators);

        counters::CONSENSUS_SENT_MSGS
            .with_label_values(&[msg.name()])
            .inc_by(other_validators.len() as u64);
        // Broadcast message over direct-send to all other validators.
        if let Err(err) = self
            .consensus_network_client
            .send_to_many(other_validators, msg)
        {
            warn!(error = ?err, "Error broadcasting message");
        }
    }
```

**File:** consensus/README.md (L14-19)
```markdown
Agreement on the database state must be reached between validators, even if
there are Byzantine faults. The Byzantine failures model allows some validators
to arbitrarily deviate from the protocol without constraint, with the exception
of being computationally bound (and thus not able to break cryptographic assumptions). Byzantine faults are worst-case errors where validators collude and behave maliciously to try to sabotage system behavior. A consensus protocol that tolerates Byzantine faults caused by malicious or hacked validators can also mitigate arbitrary hardware and software failures.

AptosBFT assumes that a set of 3f + 1 votes is distributed among a set of validators that may be honest or Byzantine. AptosBFT remains safe, preventing attacks such as double spends and forks when at most f votes are controlled by Byzantine validators &mdash; also implying that at least 2f+1 votes are honest.  AptosBFT remains live, committing transactions from clients, as long as there exists a global stabilization time (GST), after which all messages between honest validators are delivered to other honest validators within a maximal network delay $\Delta$ (this is the partial synchrony model introduced in [DLS](https://groups.csail.mit.edu/tds/papers/Lynch/jacm88.pdf)). In addition to traditional guarantees, AptosBFT maintains safety when validators crash and restart â€” even if all valida ... (truncated)
```
