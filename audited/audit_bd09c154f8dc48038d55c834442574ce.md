# Audit Report

## Title
Critical Database Corruption and Node Unrecoverability Due to Non-Atomic Pruning Operations During Disk Full Scenario

## Summary
The `LedgerPruner.prune()` function contains a critical atomicity flaw where metadata pruning and data pruning are performed in separate non-atomic operations. If disk becomes full after metadata pruning succeeds but before sub-pruner operations complete, the database enters an inconsistent state that renders the node unrecoverable without manual intervention, violating the State Consistency invariant.

## Finding Description
The vulnerability exists in the pruning logic where two separate atomic operations are performed sequentially without transactional guarantees across both operations. [1](#0-0) 

The pruning process executes in this order:
1. **Line 75-76**: `ledger_metadata_pruner.prune()` executes and commits atomically, deleting `VersionData` entries and updating `LedgerPrunerProgress` to the target version
2. **Lines 78-84**: Multiple sub-pruners execute in parallel, each performing their own atomic deletions (transactions, events, etc.) and updating their individual progress markers

Each individual operation is atomic via RocksDB's `write_schemas()`: [2](#0-1) 

However, there is **no atomicity guarantee across these separate write operations**. When disk becomes full:

**Failure Scenario:**
- `ledger_metadata_pruner.prune(1000, 2000)` succeeds → `LedgerPrunerProgress = 2000`, `VersionData[1000-1999]` deleted
- `TransactionPruner.prune(1000, 2000)` fails due to disk full → `TransactionPrunerProgress = 1000`, transactions still exist
- Database now has inconsistent state: metadata claims data is pruned, but actual transaction data remains

**Recovery Failure Path:**
On node restart, the pruner initialization attempts recovery: [3](#0-2) 

Each sub-pruner initializes with a catch-up mechanism: [4](#0-3) 

The catch-up calls `myself.prune(old_progress, metadata_progress)` which requires disk space for RocksDB WAL operations. If disk remains full, this fails. The failure propagates through `.expect()` panics: [5](#0-4) 

**Deadlock State:**
- Cannot complete pruning without free disk space (RocksDB WAL requires space even for deletes)
- Cannot free disk space without completing pruning
- Node cannot start due to initialization panic
- Requires manual intervention to free disk space externally

## Impact Explanation
**Critical Severity** - This qualifies as "Non-recoverable network partition (requires hardfork)" or at minimum "Total loss of liveness/network availability":

1. **Affected Scope**: Any validator node running low on disk space is vulnerable
2. **Unrecoverable State**: Node enters crash loop, cannot self-recover
3. **Network Impact**: If multiple validators hit this simultaneously during a network-wide pruning cycle, could cause significant validator set reduction affecting consensus
4. **Database Corruption**: Inconsistent state where metadata and actual data are out of sync, violating State Consistency invariant
5. **Manual Intervention Required**: Operators must manually free disk space before node can restart

This breaks the critical invariant: **State Consistency - State transitions must be atomic and verifiable via Merkle proofs**. The database contains orphaned transaction data without corresponding metadata, creating an inconsistent state that cannot be automatically recovered.

## Likelihood Explanation
**High Likelihood** in production environments:

1. **Common Scenario**: Disk space exhaustion is a frequent operational issue in blockchain nodes with continuous data growth
2. **Predictable Trigger**: Pruning operations are regular, scheduled processes that handle large data deletions
3. **No Safeguards**: Code has no pre-flight disk space checks or transaction-level atomicity across operations
4. **Silent Failure Window**: The failure occurs between two operations with no rollback mechanism
5. **Deployment Reality**: Many operators run nodes with minimal disk headroom for cost optimization

The vulnerability activates whenever disk becomes full at the precise window between metadata pruning completion and sub-pruner execution, making it a timing-dependent but realistic failure mode.

## Recommendation

Implement proper transactional atomicity across all pruning operations:

**Option 1: Two-Phase Commit Protocol**
```rust
fn prune(&self, max_versions: usize) -> Result<Version> {
    let mut progress = self.progress();
    let target_version = self.target_version();

    while progress < target_version {
        let current_batch_target_version = 
            min(progress + max_versions as Version, target_version);

        // Phase 1: Build all batches WITHOUT committing
        let mut metadata_batch = SchemaBatch::new();
        self.ledger_metadata_pruner.prepare_batch(
            progress, current_batch_target_version, &mut metadata_batch)?;
        
        let mut sub_batches: Vec<SchemaBatch> = Vec::new();
        for sub_pruner in &self.sub_pruners {
            let mut batch = SchemaBatch::new();
            sub_pruner.prepare_batch(progress, current_batch_target_version, &mut batch)?;
            sub_batches.push(batch);
        }

        // Phase 2: Check disk space before committing
        if !has_sufficient_disk_space()? {
            return Err(AptosDbError::InsufficientDiskSpace);
        }

        // Phase 3: Commit all batches (if any fails, rollback all)
        self.ledger_metadata_db.write_schemas(metadata_batch)?;
        for (sub_pruner, batch) in self.sub_pruners.iter().zip(sub_batches) {
            sub_pruner.write_batch(batch)
                .map_err(|e| {
                    // Attempt rollback on failure
                    self.rollback_to_version(progress);
                    e
                })?;
        }

        progress = current_batch_target_version;
        self.record_progress(progress);
    }
    Ok(target_version)
}
```

**Option 2: Progress-First Strategy** (Safer but less optimal)
Update sub-pruner progress BEFORE updating metadata progress:
```rust
fn prune(&self, max_versions: usize) -> Result<Version> {
    // ... existing setup ...
    
    // First: Prune all sub-pruners and update their progress
    THREAD_MANAGER.get_background_pool().install(|| {
        self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
            sub_pruner.prune(progress, current_batch_target_version)
                .map_err(|err| anyhow!("{} failed: {err}", sub_pruner.name()))
        })
    })?;
    
    // Only after all sub-pruners succeed: prune metadata
    self.ledger_metadata_pruner.prune(progress, current_batch_target_version)?;
    
    // ... rest of function ...
}
```

**Option 3: Add Disk Space Pre-Check**
```rust
fn prune(&self, max_versions: usize) -> Result<Version> {
    // Pre-flight check
    let required_space = self.estimate_wal_space_needed(max_versions);
    let available_space = get_available_disk_space()?;
    
    if available_space < required_space * 2 { // 2x safety margin
        return Err(AptosDbError::InsufficientDiskSpace(
            format!("Need {} bytes, have {} bytes", required_space, available_space)
        ));
    }
    
    // ... existing prune logic ...
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_temppath::TempPath;
    use std::fs;
    use std::sync::Arc;

    #[test]
    fn test_disk_full_during_pruning_causes_unrecoverable_state() {
        // Setup: Create a test database with some data
        let tmpdir = TempPath::new();
        let db = setup_test_db(&tmpdir);
        
        // Populate database with transactions
        for version in 0..2000 {
            write_test_transaction(&db, version);
        }
        
        // Enable pruning
        let pruner = LedgerPruner::new(
            db.ledger_db_arc(),
            None,
        ).unwrap();
        
        // Set target to prune versions 0-1000
        pruner.set_target_version(1000);
        
        // Simulate disk becoming full during pruning by:
        // 1. Starting prune operation
        // 2. Injecting disk full error after metadata pruner succeeds
        //    but before all sub-pruners complete
        
        // Mock RocksDB to return disk full error for specific write
        let result = simulate_disk_full_during_prune(&pruner);
        assert!(result.is_err());
        
        // Verify inconsistent state:
        let metadata_progress = pruner.ledger_metadata_pruner.progress().unwrap();
        let transaction_progress = get_transaction_pruner_progress(&db);
        
        // Metadata shows 1000 pruned, but transactions show less
        assert_eq!(metadata_progress, 1000);
        assert!(transaction_progress < 1000);
        
        // Attempt to restart node - this should PANIC
        drop(pruner);
        drop(db);
        
        // Try to reopen DB - initialization will fail
        let result = std::panic::catch_unwind(|| {
            setup_test_db(&tmpdir) // Will panic during LedgerPruner::new()
        });
        
        assert!(result.is_err(), "Node should panic and be unrecoverable");
        
        // Verify the panic message contains pruner initialization failure
        // This demonstrates the node is in unrecoverable state
    }
    
    fn simulate_disk_full_during_prune(pruner: &LedgerPruner) -> Result<()> {
        // Implementation would inject disk full error via mock
        // or filesystem quota to trigger failure between operations
        unimplemented!("Requires RocksDB mocking or filesystem simulation")
    }
}
```

**Notes**

The vulnerability stems from a fundamental design flaw where multiple independent atomic operations are used to maintain a single logical state transition. The lack of cross-database transactional guarantees means partial failures leave the system in an inconsistent, unrecoverable state. This is particularly critical because:

1. The recovery mechanism (catch-up pruning during initialization) requires the same resource (disk space) that caused the initial failure
2. The `.expect()` panic strategy prevents graceful degradation
3. There's no rollback mechanism to restore consistency
4. Operators have no warning before the failure occurs

The recommended fix requires implementing proper two-phase commit semantics or reordering operations to ensure crash-consistency, similar to database write-ahead logging protocols.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L75-84)
```rust
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L117-143)
```rust
impl LedgerPruner {
    pub fn new(
        ledger_db: Arc<LedgerDb>,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        info!(name = LEDGER_PRUNER_NAME, "Initializing...");

        let ledger_metadata_pruner = Box::new(
            LedgerMetadataPruner::new(ledger_db.metadata_db_arc())
                .expect("Failed to initialize ledger_metadata_pruner."),
        );

        let metadata_progress = ledger_metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created ledger metadata pruner, start catching up all sub pruners."
        );

        let transaction_store = Arc::new(TransactionStore::new(Arc::clone(&ledger_db)));

        let event_store_pruner = Box::new(EventStorePruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db.clone(),
        )?);
        let persisted_auxiliary_info_pruner = Box::new(PersistedAuxiliaryInfoPruner::new(
```

**File:** storage/schemadb/src/lib.rs (L289-309)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
    }

    /// Writes a group of records wrapped in a [`SchemaBatch`].
    pub fn write_schemas(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &sync_write_option())
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L84-104)
```rust
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_db_raw(),
            &DbMetadataKey::TransactionPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionPruner {
            transaction_store,
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L146-149)
```rust
        let pruner = Arc::new(
            LedgerPruner::new(ledger_db, internal_indexer_db)
                .expect("Failed to create ledger pruner."),
        );
```
