# Audit Report

## Title
Memory Exhaustion DoS via Batch Payload Storage During Node Recovery

## Summary
The quorum store batch persistence mechanism saves full transaction payloads to disk even when they exceed memory quota, but loads ALL batches with payloads into memory simultaneously during node recovery. This allows malicious validators to cause out-of-memory crashes by persisting maximum-sized batches, making target validators unable to restart.

## Finding Description

The vulnerability exists in the batch storage and recovery mechanism of the Aptos quorum store consensus system. The issue manifests through a mismatch between how batches are saved versus how they are loaded during recovery.

**Saving Path (Normal Operation):**

When a validator receives batches from peers, the `BatchCoordinator` validates them against size limits, then calls `BatchStore::persist()`. The `persist_inner()` function calls `insert_to_cache()` which checks quota and may strip the payload for memory efficiency, but then saves the **original full payload** to disk: [1](#0-0) 

The critical issue is at lines 505-512: `save_batch()` is called with the original `persist_request` containing the full payload, regardless of whether `insert_to_cache()` stripped it from the in-memory cache.

**Recovery Path (Node Restart):**

During node startup or epoch transition, `populate_cache_and_gc_expired_batches_v1()` loads ALL batches from disk into a HashMap in memory at once: [2](#0-1) 

Line 252-254 calls `get_all_batches()` which deserializes every batch with full payloads into memory before any quota checks: [3](#0-2) 

**The Attack:**

1. Malicious validators (within Byzantine tolerance of ≤1/3) create and send batches with maximum allowed payloads (~1 MB per batch)
2. Each batch passes validation checks in `ensure_max_limits()`: [4](#0-3) 

3. Batches are saved with full payloads to disk via `save_batch()`: [5](#0-4) 

4. Each validator can persist up to their `db_quota` (300 MB) of batch data: [6](#0-5) 

5. With 100-200 validators in a typical network, if each stores 300 MB, total on-disk batches = 30-60 GB

6. When a node restarts, `get_all_batches()` attempts to load all batches into memory simultaneously, causing OOM crash and preventing node restart

**Broken Invariant:**
This violates the "Resource Limits" invariant (#9): "All operations must respect gas, storage, and computational limits." The recovery process loads unbounded amounts of data into memory without respecting quota limits.

## Impact Explanation

This is a **High Severity** DoS vulnerability per Aptos bug bounty criteria:
- **Validator node slowdowns**: Nodes become unresponsive during failed recovery attempts
- **Total loss of liveness**: Affected validators cannot restart and rejoin the network
- **Network availability**: If enough validators (>1/3) are affected, the network cannot make progress

The impact is severe because:
1. Attack persists across restarts - crashed nodes cannot recover
2. Requires manual intervention (database cleanup) to restore service
3. Can be executed by Byzantine validators within fault tolerance threshold
4. Affects network liveness if coordinated across multiple validators

## Likelihood Explanation

**Likelihood: High**

The attack is highly feasible because:

1. **Low attacker requirements**: Any validator (or coalition of ≤1/3 validators) can execute this attack
2. **Bypasses validation**: All batches pass legitimate size validation checks
3. **Persistent effect**: Once batches are on disk, the vulnerability triggers on every restart
4. **No detection**: Looks like legitimate quorum store traffic during the attack
5. **No rate limiting**: Validators can continuously send maximum-sized batches

The attack does not require:
- Protocol bugs or cryptographic breaks
- Exploitation of implementation errors
- Coordination beyond standard Byzantine behavior
- Sophisticated timing or race conditions

## Recommendation

**Immediate Fix**: Modify `populate_cache_and_gc_expired_batches_v1()` and `v2()` to load batches incrementally with quota checks:

```rust
fn populate_cache_and_gc_expired_batches_v1(
    db: Arc<dyn QuorumStoreStorage>,
    current_epoch: u64,
    last_certified_time: u64,
    expiration_buffer_usecs: u64,
    batch_store: &BatchStore,
) {
    let mut iter = db.db.iter::<BatchSchema>().expect("failed to create iterator");
    iter.seek_to_first();
    
    let mut expired_keys = Vec::new();
    let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
    
    // Load batches incrementally instead of all at once
    for item in iter {
        let (digest, value) = item.expect("failed to read batch");
        
        if value.expiration() < gc_timestamp {
            expired_keys.push(digest);
        } else {
            // insert_to_cache has quota checks - if quota exceeded, skip with warning
            if let Err(e) = batch_store.insert_to_cache(&value.into()) {
                warn!("Skipping batch {} during recovery due to quota: {:?}", digest, e);
                expired_keys.push(digest); // Clean up batches that exceed quota
            }
        }
    }
    
    // ... rest of cleanup
}
```

**Long-term Fix**: 
1. Store batch payloads separately from metadata, allowing selective loading
2. Implement payload size limits in the database schema
3. Add recovery-time memory budget checks before loading batches
4. Consider separate disk quotas for metadata vs. payloads

## Proof of Concept

```rust
// PoC test showing the vulnerability
#[tokio::test]
async fn test_recovery_memory_exhaustion() {
    use aptos_types::transaction::SignedTransaction;
    use aptos_crypto::HashValue;
    
    // Setup: Create database with many large batches from different validators
    let db = QuorumStoreDB::new(temp_dir.path());
    let validators = (0..100).map(|_| PeerId::random()).collect::<Vec<_>>();
    
    for validator in validators {
        // Each validator saves batches up to their quota (300 MB)
        let mut total_size = 0;
        while total_size < 300_000_000 {
            // Create batch with ~1 MB payload (within receiver_max_batch_bytes limit)
            let txns = create_large_transactions(1_000_000); // 1 MB of transactions
            let batch_info = BatchInfo::new(
                validator,
                BatchId::new_for_test(total_size as u64),
                1, // epoch
                u64::MAX, // expiration
                HashValue::random(),
                txns.len() as u64,
                1_000_000, // num_bytes
                0, // gas_bucket_start
            );
            
            let persisted_value = PersistedValue::new(batch_info, Some(txns));
            db.save_batch(persisted_value).expect("save failed");
            total_size += 1_000_000;
        }
    }
    
    // Attack: Attempt to restart node by loading all batches
    // Expected: OOM crash when trying to load 30 GB into memory
    let result = std::panic::catch_unwind(|| {
        let all_batches = db.get_all_batches();
        // This line attempts to load ~30 GB (100 validators * 300 MB) into memory
        println!("Loaded {} batches", all_batches.unwrap().len());
    });
    
    assert!(result.is_err(), "Node should crash due to OOM");
}
```

**Notes**

The vulnerability is exacerbated by the fact that the `expect()` call at line 278 of `batch_store.rs` will panic if quota is exceeded during recovery, providing no graceful degradation path. This makes the DoS complete and unrecoverable without manual database intervention.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L245-280)
```rust
    fn populate_cache_and_gc_expired_batches_v1(
        db: Arc<dyn QuorumStoreStorage>,
        current_epoch: u64,
        last_certified_time: u64,
        expiration_buffer_usecs: u64,
        batch_store: &BatchStore,
    ) {
        let db_content = db
            .get_all_batches()
            .expect("failed to read v1 data from db");
        info!(
            epoch = current_epoch,
            "QS: Read v1 batches from storage. Len: {}, Last Cerified Time: {}",
            db_content.len(),
            last_certified_time
        );

        let mut expired_keys = Vec::new();
        let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
        for (digest, value) in db_content {
            let expiration = value.expiration();

            trace!(
                "QS: Batchreader recovery content exp {:?}, digest {}",
                expiration,
                digest
            );

            if expiration < gc_timestamp {
                expired_keys.push(digest);
            } else {
                batch_store
                    .insert_to_cache(&value.into())
                    .expect("Storage limit exceeded upon BatchReader construction");
            }
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L488-513)
```rust
    fn persist_inner(
        &self,
        batch_info: BatchInfoExt,
        persist_request: PersistedValue<BatchInfoExt>,
    ) -> Option<SignedBatchInfo<BatchInfoExt>> {
        assert!(
            &batch_info == persist_request.batch_info(),
            "Provided batch info doesn't match persist request batch info"
        );
        match self.save(&persist_request) {
            Ok(needs_db) => {
                trace!("QS: sign digest {}", persist_request.digest());
                if needs_db {
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L103-108)
```rust
    fn get_all_batches(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfo>>> {
        let mut iter = self.db.iter::<BatchSchema>()?;
        iter.seek_to_first();
        iter.map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<HashValue, PersistedValue<BatchInfo>>>>()
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L110-117)
```rust
    fn save_batch(&self, batch: PersistedValue<BatchInfo>) -> Result<(), DbError> {
        trace!(
            "QS: db persists digest {} expiration {:?}",
            batch.digest(),
            batch.expiration()
        );
        self.put::<BatchSchema>(batch.digest(), &batch)
    }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L137-171)
```rust
    fn ensure_max_limits(&self, batches: &[Batch<BatchInfoExt>]) -> anyhow::Result<()> {
        let mut total_txns = 0;
        let mut total_bytes = 0;
        for batch in batches.iter() {
            ensure!(
                batch.num_txns() <= self.max_batch_txns,
                "Exceeds batch txn limit {} > {}",
                batch.num_txns(),
                self.max_batch_txns,
            );
            ensure!(
                batch.num_bytes() <= self.max_batch_bytes,
                "Exceeds batch bytes limit {} > {}",
                batch.num_bytes(),
                self.max_batch_bytes,
            );

            total_txns += batch.num_txns();
            total_bytes += batch.num_bytes();
        }
        ensure!(
            total_txns <= self.max_total_txns,
            "Exceeds total txn limit {} > {}",
            total_txns,
            self.max_total_txns,
        );
        ensure!(
            total_bytes <= self.max_total_bytes,
            "Exceeds total bytes limit: {} > {}",
            total_bytes,
            self.max_total_bytes,
        );

        Ok(())
    }
```

**File:** config/src/config/quorum_store_config.rs (L105-146)
```rust
impl Default for QuorumStoreConfig {
    fn default() -> QuorumStoreConfig {
        QuorumStoreConfig {
            channel_size: 1000,
            proof_timeout_ms: 10000,
            batch_generation_poll_interval_ms: 25,
            batch_generation_min_non_empty_interval_ms: 50,
            batch_generation_max_interval_ms: 250,
            sender_max_batch_txns: DEFEAULT_MAX_BATCH_TXNS,
            // TODO: on next release, remove BATCH_PADDING_BYTES
            sender_max_batch_bytes: 1024 * 1024 - BATCH_PADDING_BYTES,
            sender_max_num_batches: DEFAULT_MAX_NUM_BATCHES,
            sender_max_total_txns: 1500,
            // TODO: on next release, remove DEFAULT_MAX_NUM_BATCHES * BATCH_PADDING_BYTES
            sender_max_total_bytes: 4 * 1024 * 1024 - DEFAULT_MAX_NUM_BATCHES * BATCH_PADDING_BYTES,
            receiver_max_batch_txns: 100,
            receiver_max_batch_bytes: 1024 * 1024 + BATCH_PADDING_BYTES,
            receiver_max_num_batches: 20,
            receiver_max_total_txns: 2000,
            receiver_max_total_bytes: 4 * 1024 * 1024
                + DEFAULT_MAX_NUM_BATCHES
                + BATCH_PADDING_BYTES,
            batch_request_num_peers: 5,
            batch_request_retry_limit: 10,
            batch_request_retry_interval_ms: 500,
            batch_request_rpc_timeout_ms: 5000,
            batch_expiry_gap_when_init_usecs: Duration::from_secs(60).as_micros() as u64,
            remote_batch_expiry_gap_when_init_usecs: Duration::from_millis(500).as_micros() as u64,
            memory_quota: 120_000_000,
            db_quota: 300_000_000,
            batch_quota: 300_000,
            back_pressure: QuorumStoreBackPressureConfig::default(),
            // number of batch coordinators to handle QS batch messages, should be >= 1
            num_workers_for_remote_batches: 10,
            batch_buckets: DEFAULT_BUCKETS.to_vec(),
            allow_batches_without_pos_in_proposal: true,
            enable_opt_quorum_store: true,
            opt_qs_minimum_batch_age_usecs: Duration::from_millis(50).as_micros() as u64,
            enable_payload_v2: false,
            enable_batch_v2: false,
        }
    }
```
