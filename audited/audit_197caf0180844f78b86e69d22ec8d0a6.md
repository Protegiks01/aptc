# Audit Report

## Title
Memory Leak in Indexer gRPC Data Service v2 - Orphaned StreamProgressSamples on Task Cancellation

## Summary
The `ConnectionManager` in the indexer-grpc-data-service-v2 lacks automatic cleanup of `StreamProgressSamples` when streaming tasks are cancelled, panicked, or aborted. This causes unbounded memory accumulation as orphaned entries remain in the `active_streams` DashMap indefinitely until service restart.

## Finding Description

The `ConnectionManager` maintains a `DashMap` of active streams, where each entry contains an `ActiveStream` and associated `StreamProgressSamples`: [1](#0-0) 

When streams complete normally, cleanup occurs via `remove_active_stream`: [2](#0-1) [3](#0-2) 

However, when async streaming tasks are cancelled, panicked, or dropped before reaching the cleanup code, the `remove_active_stream` call never executes. This occurs in common scenarios:

1. **Client disconnection during streaming** - Network errors or abrupt client termination
2. **Server shutdown/restart** - Running tasks are cancelled without cleanup
3. **Task panics** - Unwrap failures or other panics prevent cleanup execution
4. **Explicit task abortion** - Manual cancellation operations

The `update_stream_progress` method contains an `.unwrap()` that will panic if the stream doesn't exist: [4](#0-3) 

Critical finding: The `ConnectionManager` has **no periodic garbage collection mechanism** to detect and remove orphaned streams. The `start()` method only sends heartbeats: [5](#0-4) 

In contrast, similar systems in the Aptos codebase implement proper cleanup. The state-sync storage service has explicit timeout checking and garbage collection: [6](#0-5) [7](#0-6) 

Each leaked entry retains:
- `ActiveStream` struct (~100 bytes with strings)
- `StreamProgressSamples` with up to 120 samples (~4KB total)
- Persistent DashMap entry overhead

## Impact Explanation

This constitutes a **Medium severity** memory leak issue affecting the indexer-grpc-data-service-v2, a critical component for blockchain data access. While not directly impacting consensus or validator operations, the indexer services are essential infrastructure for ecosystem applications and data consumers.

The impact includes:
- **Unbounded memory growth** over extended service uptime (weeks/months)
- **Service degradation** under memory pressure
- **Potential OOM crashes** requiring emergency restarts
- **Data availability disruption** for ecosystem participants

For a busy indexer service handling thousands of streams daily with typical connection patterns (network errors, client churn), this could accumulate significant memory (hundreds of MB to GB) over weeks, requiring operational intervention.

## Likelihood Explanation

**High likelihood** - This occurs automatically in production environments through:

1. **Normal client behavior** - Clients disconnect due to network issues, timeouts, or intentional termination
2. **Deployment operations** - Service restarts/updates cancel active tasks
3. **Load patterns** - High connection churn in production environments
4. **Error conditions** - Any panic in streaming code paths leaves orphaned entries

The vulnerability requires no attacker action - it's an inherent operational issue that will manifest in any production deployment with sustained traffic.

## Recommendation

Implement periodic garbage collection similar to the state-sync storage service pattern:

```rust
// Add to StreamProgressSamples or wrap in a struct with timestamp
struct TrackedStreamProgress {
    samples: StreamProgressSamples,
    last_activity: Instant,
}

// Add to ConnectionManager
impl ConnectionManager {
    pub(crate) fn remove_stale_streams(&self, max_idle_duration: Duration) {
        let now = Instant::now();
        self.active_streams.retain(|_id, (stream, progress)| {
            // Check if stream has been idle too long
            if let Ok(elapsed) = now.duration_since(progress.last_activity) {
                elapsed < max_idle_duration
            } else {
                true // Keep if timestamp error
            }
        });
    }
    
    // Modify start() to include periodic cleanup
    pub(crate) async fn start(&self) {
        loop {
            // ... existing heartbeat code ...
            
            // Add garbage collection
            self.remove_stale_streams(Duration::from_secs(300)); // 5 min timeout
            
            tokio::time::sleep(Duration::from_secs(1)).await;
        }
    }
}
```

Additionally:
1. Use RAII guards or scope guards to ensure cleanup on task drop
2. Add metrics for tracking orphaned streams
3. Implement alerting for abnormal active_streams growth
4. Consider using weak references or timeout-based caching

## Proof of Concept

```rust
// Rust test demonstrating the memory leak
#[tokio::test]
async fn test_stream_cancellation_memory_leak() {
    use std::sync::Arc;
    use tokio::time::{sleep, Duration};
    
    // Create connection manager
    let connection_manager = Arc::new(
        ConnectionManager::new(
            1, // chain_id
            vec!["http://localhost:50051".to_string()],
            "http://localhost:50052".to_string(),
            true, // is_live_data_service
        ).await
    );
    
    // Insert multiple streams
    for i in 0..100 {
        let id = format!("stream_{}", i);
        connection_manager.insert_active_stream(&id, 0, Some(1000));
    }
    
    // Verify streams exist
    assert_eq!(connection_manager.active_streams.len(), 100);
    
    // Simulate task cancellations by NOT calling remove_active_stream
    // In real scenario, this happens when tasks panic or are cancelled
    // The streams remain in memory indefinitely
    
    sleep(Duration::from_secs(60)).await;
    
    // Streams are still in memory - no automatic cleanup
    assert_eq!(connection_manager.active_streams.len(), 100);
    
    // Expected behavior: periodic GC should have removed stale streams
    // Actual behavior: streams remain until explicit removal or restart
}
```

## Notes

While the `StreamProgressSamples` struct itself has bounded growth (max 120 samples), the vulnerability lies in the **entry-level accumulation** - each orphaned stream entry persists indefinitely in the `active_streams` DashMap. The lack of timeout-based garbage collection violates the **Resource Limits** invariant, as the system should respect memory constraints and prevent unbounded growth through operational patterns.

This issue affects production availability rather than blockchain consensus or asset security, but remains a legitimate operational vulnerability requiring remediation for long-running indexer services.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L101-108)
```rust
pub(crate) struct ConnectionManager {
    chain_id: u64,
    grpc_manager_connections: DashMap<String, GrpcManagerClient<Channel>>,
    self_advertised_address: String,
    known_latest_version: AtomicU64,
    active_streams: DashMap<String, (ActiveStream, StreamProgressSamples)>,
    is_live_data_service: bool,
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L145-166)
```rust
    pub(crate) async fn start(&self) {
        loop {
            for entry in self.grpc_manager_connections.iter() {
                let address = entry.key();
                let mut retries = 0;
                loop {
                    let result = self.heartbeat(address).await;
                    if result.is_ok() {
                        break;
                    }
                    retries += 1;
                    if retries > MAX_HEARTBEAT_RETRIES {
                        warn!("Failed to send heartbeat to GrpcManager at {address}, last error: {result:?}.");
                        break;
                    }
                }
                continue;
            }

            tokio::time::sleep(Duration::from_secs(1)).await;
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L227-233)
```rust
    pub(crate) fn update_stream_progress(&self, id: &str, version: u64, size_bytes: u64) {
        self.active_streams
            .get_mut(id)
            .unwrap()
            .1
            .maybe_add_sample(version, size_bytes);
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L276-279)
```rust
        self.connection_manager
            .update_stream_progress(&id, next_version, size_bytes);
        self.connection_manager.remove_active_stream(&id);
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L236-239)
```rust
        self.connection_manager
            .update_stream_progress(&id, next_version, size_bytes);
        self.connection_manager.remove_active_stream(&id);
    }
```

**File:** state-sync/storage-service/server/src/subscription.rs (L418-433)
```rust
    fn is_expired(&self, timeout_ms: u64) -> bool {
        // Determine the time when the stream was first blocked
        let time_when_first_blocked =
            if let Some(subscription_request) = self.first_pending_request() {
                subscription_request.request_start_time // The stream is blocked on the first pending request
            } else {
                self.last_stream_update_time // The stream is idle and hasn't been updated in a while
            };

        // Verify the stream hasn't been blocked for too long
        let current_time = self.time_service.now();
        let elapsed_time = current_time
            .duration_since(time_when_first_blocked)
            .as_millis();
        elapsed_time > (timeout_ms as u128)
    }
```

**File:** state-sync/storage-service/server/src/subscription.rs (L791-795)
```rust
    // Remove the expired subscriptions
    remove_expired_subscriptions(subscriptions.clone(), peers_with_expired_subscriptions);

    // Remove the invalid subscriptions
    remove_invalid_subscriptions(subscriptions.clone(), peers_with_invalid_subscriptions);
```
