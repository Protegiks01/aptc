# Audit Report

## Title
Cross-Shard Deadlock Due to Missing Messages for Unpredicted Write Set Differences

## Summary
The `send_remote_update_for_success()` function in the sharded block executor only sends cross-shard update messages for state keys that appear in the actual transaction write set, but not for state keys that have dependent edges but are absent from the write set. This causes dependent shards to deadlock indefinitely when write hints overestimate actual writes or when transactions produce empty/incomplete write sets.

## Finding Description

The sharded block executor uses a prediction-based dependency system where `write_hints` from transaction analysis determine which shards depend on a transaction's results. These hints create `dependent_edges` that map state keys to waiting shards. [1](#0-0) 

The documentation explicitly states that write hints "can be accurate or strictly overestimated," indicating that predicted writes may not match actual execution results.

When a transaction commits, `send_remote_update_for_success()` is supposed to notify dependent shards of the write results. However, the implementation iterates over the actual write set instead of the dependent edges: [2](#0-1) 

The critical flaw is at line 114: the function iterates over `write_set.expect_write_op_iter()`. This means only state keys present in the actual write set trigger message sends. If a state key exists in `dependent_edges` (line 108) but not in the write set, no message is ever sent for that key.

Meanwhile, dependent shards initialize waiting state for all required keys based on their `required_edges`: [3](#0-2) 

Each required key gets a `RemoteStateValue::waiting()` that blocks until `set_value()` is called: [4](#0-3) 

The `get_value()` method uses a condition variable that waits indefinitely (line 32-33). If `set_value()` is never called because no message was sent, the shard deadlocks permanently.

**The Protocol Supports This Scenario:**

The `RemoteTxnWrite` message protocol is explicitly designed to handle missing writes: [5](#0-4) 

Line 16 states: "The write op is None if the transaction is aborted." The protocol accepts `Option<WriteOp>`, and the receiver correctly handles `None` values. However, the sender never utilizes this capability for keys that have dependent edges but no actual writes.

**When This Occurs:**

The dependent edges are built from write hints during partitioning: [6](#0-5) 

If the actual execution produces a write set that is a subset of the predicted writes (empty or partially empty), the mismatch causes the deadlock.

## Impact Explanation

This vulnerability causes **total loss of liveness** in the sharded block executor:

- Dependent shards block indefinitely in `RemoteStateValue::get_value()` waiting for messages that never arrive
- The blocked shard cannot process any further transactions
- This cascades to halt the entire blockchain execution
- Recovery requires node restart and would repeatedly fail if the same transactions are re-executed

This meets **Critical Severity** criteria under the Aptos bug bounty program:
- "Total loss of liveness/network availability" 
- Requires system intervention to recover
- Affects consensus and block production

The invariant broken is: **State Consistency** - the cross-shard state synchronization mechanism fails, causing state divergence and execution halts.

## Likelihood Explanation

The likelihood depends on whether write hints can overestimate in practice:

**Current State:**
The current implementation only supports specific transaction types (coin transfers, account creation) with precise hints. For these, hints match execution. However, the architecture explicitly anticipates overestimation scenarios.

**Future Risk:**
When the sharded executor is completed:
- More complex transaction types with conditional logic will have less predictable write patterns
- Transaction abort handling (currently unimplemented) would produce empty write sets while dependent edges exist [7](#0-6) 

The `todo!()` at line 150 shows abort handling is incomplete, indicating this feature is still under development.

**Exploitation:** Currently low likelihood due to limited transaction support, but becomes high likelihood once the feature is production-ready with broader transaction type coverage.

## Recommendation

The fix is to iterate over `dependent_edges` instead of the write set, sending messages for all expected keys:

```rust
fn send_remote_update_for_success(
    &self,
    txn_idx: TxnIndex,
    txn_output: &OnceCell<TransactionOutput>,
) {
    let edges = self.dependent_edges.get(&txn_idx).unwrap();
    let write_set = txn_output
        .get()
        .expect("Committed output must be set")
        .write_set();

    // Iterate over dependent_edges, not write_set
    for (state_key, dependent_shard_ids) in edges.iter() {
        // Get the write_op if it exists in the write set, None otherwise
        let write_op = write_set
            .expect_write_op_iter()
            .find(|(key, _)| key == state_key)
            .map(|(_, op)| op.clone());
        
        for (dependent_shard_id, round_id) in dependent_shard_ids.iter() {
            trace!("Sending remote update for shard id {:?} and txn_idx: {:?}, state_key: {:?}, dependent shard id: {:?}", 
                   self.shard_id, txn_idx, state_key, dependent_shard_id);
            let message = RemoteTxnWriteMsg(RemoteTxnWrite::new(
                state_key.clone(),
                write_op.clone(), // May be None if key wasn't written
            ));
            if *round_id == GLOBAL_ROUND_ID {
                self.cross_shard_client.send_global_msg(message);
            } else {
                self.cross_shard_client.send_cross_shard_msg(
                    *dependent_shard_id,
                    *round_id,
                    message,
                );
            }
        }
    }
}
```

This ensures all dependent shards receive messages (with `None` for missing writes), preventing deadlock.

## Proof of Concept

Due to the current limited transaction type support and incomplete abort handling, a direct PoC requires extending the codebase. However, the logical flow demonstrating the vulnerability is:

```rust
// Conceptual PoC (requires sharded executor setup)
#[test]
fn test_missing_write_deadlock() {
    // 1. Create transaction T1 in Shard A with write_hints = [K1, K2]
    // 2. Create transaction T2 in Shard B with read_hints = [K1]
    // 3. Partitioner creates dependent_edge: T1 -> (Shard B, K1)
    // 4. Shard B initializes RemoteStateValue::waiting() for K1
    
    // 5. Execute T1, but it only writes to K2 (not K1)
    //    - write_set = {K2: SomeValue}
    //    - dependent_edges = {K1: [(ShardB, round)], K2: [...]}
    
    // 6. send_remote_update_for_success() is called
    //    - Iterates over write_set: only K2
    //    - Sends message for K2, but NOT for K1
    
    // 7. Shard B attempts to read K1
    //    - Calls RemoteStateValue::get_value() for K1
    //    - Blocks forever on condvar at line 32-33 of remote_state_value.rs
    
    // Expected: Shard B should receive message with None for K1
    // Actual: Shard B deadlocks indefinitely
}
```

The vulnerability is confirmed by the architectural mismatch: the protocol supports `Option<WriteOp>` for missing writes, but the sender implementation doesn't use it correctly.

**Notes**

This issue represents a critical design flaw in the cross-shard messaging logic that violates the principle of complete notification for all dependencies. While currently limited in exploitability due to the experimental nature of the sharded executor (evidenced by `todo!()` markers), it will cause production failures once the feature is completed. The explicit support for overestimated hints in the architecture and the `Option<WriteOp>` protocol design confirm this scenario is expected and must be handled correctly.

### Citations

**File:** types/src/transaction/analyzed_transaction.rs (L26-32)
```rust
    /// Set of storage locations that are read by the transaction - this doesn't include location
    /// that are written by the transactions to avoid duplication of locations across read and write sets
    /// This can be accurate or strictly overestimated.
    pub read_hints: Vec<StorageLocation>,
    /// Set of storage locations that are written by the transaction. This can be accurate or strictly
    /// overestimated.
    pub write_hints: Vec<StorageLocation>,
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L103-134)
```rust
    fn send_remote_update_for_success(
        &self,
        txn_idx: TxnIndex,
        txn_output: &OnceCell<TransactionOutput>,
    ) {
        let edges = self.dependent_edges.get(&txn_idx).unwrap();
        let write_set = txn_output
            .get()
            .expect("Committed output must be set")
            .write_set();

        for (state_key, write_op) in write_set.expect_write_op_iter() {
            if let Some(dependent_shard_ids) = edges.get(state_key) {
                for (dependent_shard_id, round_id) in dependent_shard_ids.iter() {
                    trace!("Sending remote update for success for shard id {:?} and txn_idx: {:?}, state_key: {:?}, dependent shard id: {:?}", self.shard_id, txn_idx, state_key, dependent_shard_id);
                    let message = RemoteTxnWriteMsg(RemoteTxnWrite::new(
                        state_key.clone(),
                        Some(write_op.clone()),
                    ));
                    if *round_id == GLOBAL_ROUND_ID {
                        self.cross_shard_client.send_global_msg(message);
                    } else {
                        self.cross_shard_client.send_cross_shard_msg(
                            *dependent_shard_id,
                            *round_id,
                            message,
                        );
                    }
                }
            }
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L149-151)
```rust
    fn on_execution_aborted(&self, _txn_idx: TxnIndex) {
        todo!("on_transaction_aborted not supported for sharded execution yet")
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_state_view.rs (L58-71)
```rust
    pub fn create_cross_shard_state_view(
        base_view: &'a S,
        transactions: &[TransactionWithDependencies<AnalyzedTransaction>],
    ) -> CrossShardStateView<'a, S> {
        let mut cross_shard_state_key = HashSet::new();
        for txn in transactions {
            for (_, storage_locations) in txn.cross_shard_dependencies.required_edges_iter() {
                for storage_location in storage_locations {
                    cross_shard_state_key.insert(storage_location.clone().into_state_key());
                }
            }
        }
        CrossShardStateView::new(cross_shard_state_key, base_view)
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/remote_state_value.rs (L29-39)
```rust
    pub fn get_value(&self) -> Option<StateValue> {
        let (lock, cvar) = &*self.value_condition;
        let mut status = lock.lock().unwrap();
        while let RemoteValueStatus::Waiting = *status {
            status = cvar.wait(status).unwrap();
        }
        match &*status {
            RemoteValueStatus::Ready(value) => value.clone(),
            RemoteValueStatus::Waiting => unreachable!(),
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/messages.rs (L13-26)
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct RemoteTxnWrite {
    state_key: StateKey,
    // The write op is None if the transaction is aborted.
    write_op: Option<WriteOp>,
}

impl RemoteTxnWrite {
    pub fn new(state_key: StateKey, write_op: Option<WriteOp>) -> Self {
        Self {
            state_key,
            write_op,
        }
    }
```

**File:** execution/block-partitioner/src/v2/state.rs (L323-348)
```rust
        // Build dependent edges.
        for &key_idx in self.write_sets[ori_txn_idx].read().unwrap().iter() {
            if Some(txn_idx) == self.last_writer(key_idx, SubBlockIdx { round_id, shard_id }) {
                let start_of_next_sub_block = ShardedTxnIndexV2::new(round_id, shard_id + 1, 0);
                let next_writer = self.first_writer(key_idx, start_of_next_sub_block);
                let end_follower = match next_writer {
                    None => ShardedTxnIndexV2::new(self.num_rounds(), self.num_executor_shards, 0), // Guaranteed to be greater than any invalid idx...
                    Some(idx) => ShardedTxnIndexV2::new(idx.round_id(), idx.shard_id() + 1, 0),
                };
                for follower_txn_idx in
                    self.all_txns_in_sub_block_range(key_idx, start_of_next_sub_block, end_follower)
                {
                    let final_sub_blk_idx =
                        self.final_sub_block_idx(follower_txn_idx.sub_block_idx);
                    let dst_txn_idx = ShardedTxnIndex {
                        txn_index: *self.final_idxs_by_pre_partitioned
                            [follower_txn_idx.pre_partitioned_txn_idx]
                            .read()
                            .unwrap(),
                        shard_id: final_sub_blk_idx.shard_id,
                        round_id: final_sub_blk_idx.round_id,
                    };
                    deps.add_dependent_edge(dst_txn_idx, vec![self.storage_location(key_idx)]);
                }
            }
        }
```
