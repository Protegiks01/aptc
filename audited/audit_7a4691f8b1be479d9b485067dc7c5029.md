# Audit Report

## Title
Indexer Fails to Detect Duplicate MintEvent Indices Across Transactions

## Summary
The Aptos indexer does not store or validate the `index` field from `MintEvent` structures when processing token minting activities. This means if duplicate MintEvent indices appear across different transactions for the same collection, the indexer will accept both without detection, leading to data integrity issues in the indexed state.

## Finding Description

The `MintEvent` struct contains an `index` field representing the sequential mint number of a token within a collection: [1](#0-0) 

This index is supposed to be unique per token in a collection, as enforced by the on-chain Move framework: [2](#0-1) 

However, when the indexer processes MintEvents, it completely discards this index field. The event processing code matches on `MintEvent` but ignores its contents: [3](#0-2) 

The `TokenActivityV2` database record only stores `transaction_version` and `event_index` (the position of the event within the transaction): [4](#0-3) 

The database schema confirms this primary key structure: [5](#0-4) 

The insert operation only prevents duplicate (transaction_version, event_index) pairs, not duplicate MintEvent indices: [6](#0-5) 

**Breaking Security Guarantees:**
This violates the data integrity expectation that the indexer should accurately reflect on-chain token semantics. While the on-chain framework maintains index uniqueness, the indexer fails to validate or even store this critical field.

## Impact Explanation

This issue constitutes a **Medium severity** vulnerability under "State inconsistencies requiring intervention."

While the indexer is an off-chain component, it serves as the primary data source for:
- NFT marketplaces querying token ownership and metadata
- Analytics tools tracking collection supply and minting activity  
- User-facing applications displaying token information
- Audit and compliance tools verifying token provenance

If duplicate MintEvent indices appear in the blockchain (whether through a framework bug, VM inconsistency, or other upstream issue), the indexer will silently accept them, causing:

1. **Data corruption** where two different tokens appear to have the same mint index
2. **Supply tracking errors** as external systems rely on indices to count unique tokens
3. **Token identity confusion** since index is meant to uniquely identify tokens within collections
4. **Loss of data integrity guarantees** that downstream consumers expect

The impact is limited to data quality rather than direct fund loss or consensus failure, but represents a significant deviation from expected behavior for a critical infrastructure component.

## Likelihood Explanation

**Likelihood: Low under normal operation, but High if upstream vulnerability exists**

Under normal conditions, the Move framework's `increment_supply` function prevents duplicate indices through atomic aggregator operations and monotonic counters. However:

1. **Defensive programming gap**: The indexer should validate semantic invariants as a defense-in-depth measure
2. **Upstream bug amplification**: If any bug in the Move VM, framework, or consensus produces duplicate indices, the indexer won't catch it
3. **Silent failure mode**: The issue manifests as data corruption rather than an obvious error
4. **No validation layer**: The indexer accepts all events without semantic validation

The probability increases if there are:
- Race conditions in parallel minting for ConcurrentSupply collections
- VM non-determinism bugs affecting supply counters
- Consensus disagreement on transaction ordering
- Framework upgrade bugs introducing regressions

## Recommendation

**Add MintEvent index validation and storage:**

1. Store the `MintEvent.index` field in the `token_activities_v2` table
2. Add a uniqueness constraint on `(collection_id, mint_index)` for MintEvent types
3. Implement validation logic to detect and reject duplicate indices

**Proposed schema change:**
```sql
ALTER TABLE token_activities_v2 
ADD COLUMN mint_index NUMERIC;

CREATE UNIQUE INDEX token_mint_idx 
ON token_activities_v2 (event_account_address, mint_index) 
WHERE type = '0x4::collection::MintEvent';
```

**Proposed code change in v2_token_activities.rs:**
```rust
// Extract and store the mint index when processing MintEvent
V2TokenEvent::MintEvent(inner) => TokenActivityHelperV2 {
    from_address: Some(object_core.get_owner_address()),
    to_address: None,
    token_amount: BigDecimal::one(),
    before_value: None,
    after_value: None,
    mint_index: Some(inner.index.clone()),  // ADD THIS
},
```

Add validation in the insert function to check for duplicate indices and log/reject them.

## Proof of Concept

```rust
// Rust test demonstrating the issue
#[test]
fn test_duplicate_mint_index_not_detected() {
    // Create two MintEvents with same index but different transactions
    let mint_event_1 = MintEvent {
        index: BigDecimal::from(42),
        token: "0xabc".to_string(),
    };
    
    let mint_event_2 = MintEvent {
        index: BigDecimal::from(42),  // SAME INDEX
        token: "0xdef".to_string(),   // Different token
    };
    
    // Process both events in different transactions
    let activity_1 = TokenActivityV2::get_nft_v2_from_parsed_event(
        &create_event(mint_event_1, 100, 0),  // txn_version=100, event_index=0
        /* ... */
    );
    
    let activity_2 = TokenActivityV2::get_nft_v2_from_parsed_event(
        &create_event(mint_event_2, 101, 0),  // txn_version=101, event_index=0
        /* ... */
    );
    
    // Both are inserted successfully - NO DUPLICATE DETECTION
    insert_token_activities_v2(&mut conn, &[activity_1, activity_2]);
    
    // Query shows two different tokens with same mint index - DATA CORRUPTION
    let results = query_by_collection(collection_id);
    assert_eq!(results.len(), 2);
    assert_eq!(results[0].mint_index, 42);
    assert_eq!(results[1].mint_index, 42);  // DUPLICATE!
}
```

**Note:** This PoC demonstrates the indexer's acceptance of duplicates. Actual creation of duplicate indices on-chain would require a separate vulnerability in the Move framework, which is not demonstrated here.

### Citations

**File:** crates/indexer/src/models/token_models/v2_token_utils.rs (L328-338)
```rust
pub struct MintEvent {
    #[serde(deserialize_with = "deserialize_from_string")]
    pub index: BigDecimal,
    token: String,
}

impl MintEvent {
    pub fn get_token_address(&self) -> String {
        standardize_address(&self.token)
    }
}
```

**File:** aptos-move/framework/aptos-token-objects/sources/collection.move (L373-443)
```text
    friend fun increment_supply(
        collection: &Object<Collection>,
        token: address,
    ): Option<AggregatorSnapshot<u64>> acquires FixedSupply, UnlimitedSupply, ConcurrentSupply {
        let collection_addr = object::object_address(collection);
        if (exists<ConcurrentSupply>(collection_addr)) {
            let supply = &mut ConcurrentSupply[collection_addr];
            assert!(
                aggregator_v2::try_add(&mut supply.current_supply, 1),
                error::out_of_range(ECOLLECTION_SUPPLY_EXCEEDED),
            );
            aggregator_v2::add(&mut supply.total_minted, 1);
            event::emit(
                Mint {
                    collection: collection_addr,
                    index: aggregator_v2::snapshot(&supply.total_minted),
                    token,
                },
            );
            option::some(aggregator_v2::snapshot(&supply.total_minted))
        } else if (exists<FixedSupply>(collection_addr)) {
            let supply = &mut FixedSupply[collection_addr];
            supply.current_supply += 1;
            supply.total_minted += 1;
            assert!(
                supply.current_supply <= supply.max_supply,
                error::out_of_range(ECOLLECTION_SUPPLY_EXCEEDED),
            );
            if (std::features::module_event_migration_enabled()) {
                event::emit(
                    Mint {
                        collection: collection_addr,
                        index: aggregator_v2::create_snapshot(supply.total_minted),
                        token,
                    },
                );
            } else {
                event::emit_event(&mut supply.mint_events,
                    MintEvent {
                        index: supply.total_minted,
                        token,
                    },
                );
            };
            option::some(aggregator_v2::create_snapshot<u64>(supply.total_minted))
        } else if (exists<UnlimitedSupply>(collection_addr)) {
            let supply = &mut UnlimitedSupply[collection_addr];
            supply.current_supply += 1;
            supply.total_minted += 1;
            if (std::features::module_event_migration_enabled()) {
                event::emit(
                    Mint {
                        collection: collection_addr,
                        index: aggregator_v2::create_snapshot(supply.total_minted),
                        token,
                    },
                );
            } else {
                event::emit_event(
                    &mut supply.mint_events,
                    MintEvent {
                        index: supply.total_minted,
                        token,
                    },
                );
            };
            option::some(aggregator_v2::create_snapshot<u64>(supply.total_minted))
        } else {
            option::none()
        }
    }
```

**File:** crates/indexer/src/models/token_models/v2_token_activities.rs (L22-41)
```rust
#[derive(Debug, Deserialize, FieldCount, Identifiable, Insertable, Serialize)]
#[diesel(primary_key(transaction_version, event_index))]
#[diesel(table_name = token_activities_v2)]
pub struct TokenActivityV2 {
    pub transaction_version: i64,
    pub event_index: i64,
    pub event_account_address: String,
    pub token_data_id: String,
    pub property_version_v1: BigDecimal,
    pub type_: String,
    pub from_address: Option<String>,
    pub to_address: Option<String>,
    pub token_amount: BigDecimal,
    pub before_value: Option<String>,
    pub after_value: Option<String>,
    pub entry_function_id_str: Option<String>,
    pub token_standard: String,
    pub is_fungible_v2: Option<bool>,
    pub transaction_timestamp: chrono::NaiveDateTime,
}
```

**File:** crates/indexer/src/models/token_models/v2_token_activities.rs (L164-171)
```rust
                let token_activity_helper = match token_event {
                    V2TokenEvent::MintEvent(_) => TokenActivityHelperV2 {
                        from_address: Some(object_core.get_owner_address()),
                        to_address: None,
                        token_amount: BigDecimal::one(),
                        before_value: None,
                        after_value: None,
                    },
```

**File:** crates/indexer/src/schema.rs (L720-746)
```rust
diesel::table! {
    token_activities_v2 (transaction_version, event_index) {
        transaction_version -> Int8,
        event_index -> Int8,
        #[max_length = 66]
        event_account_address -> Varchar,
        #[max_length = 66]
        token_data_id -> Varchar,
        property_version_v1 -> Numeric,
        #[sql_name = "type"]
        type_ -> Varchar,
        #[max_length = 66]
        from_address -> Nullable<Varchar>,
        #[max_length = 66]
        to_address -> Nullable<Varchar>,
        token_amount -> Numeric,
        before_value -> Nullable<Text>,
        after_value -> Nullable<Text>,
        #[max_length = 100]
        entry_function_id_str -> Nullable<Varchar>,
        #[max_length = 10]
        token_standard -> Varchar,
        is_fungible_v2 -> Nullable<Bool>,
        transaction_timestamp -> Timestamp,
        inserted_at -> Timestamp,
    }
}
```

**File:** crates/indexer/src/processors/token_processor.rs (L798-817)
```rust
fn insert_token_activities_v2(
    conn: &mut PgConnection,
    items_to_insert: &[TokenActivityV2],
) -> Result<(), diesel::result::Error> {
    use schema::token_activities_v2::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), TokenActivityV2::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::token_activities_v2::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((transaction_version, event_index))
                .do_nothing(),
            None,
        )?;
    }
    Ok(())
}
```
