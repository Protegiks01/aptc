# Audit Report

## Title
Remote Executor Service Memory Exhaustion via Backpressure Bypass

## Summary
The `NetworkMessageServiceServer` gRPC service always reports readiness via `poll_ready()` returning `Poll::Ready(Ok(()))`, bypassing Tower's backpressure mechanism. Combined with unbounded channel buffering and sequential CPU-intensive transaction processing, this allows an attacker to exhaust server memory by flooding the service with execution commands, causing validator node crashes and denial of service.

## Finding Description

The vulnerability exists in the remote executor service's gRPC implementation, which handles distributed transaction execution across shards. The issue spans three interconnected components:

**1. Disabled Backpressure Mechanism**

The auto-generated gRPC service implementation always returns ready status without checking actual service capacity: [1](#0-0) 

This violates the Tower service contract where `poll_ready` should return `Poll::Pending` when the service is overloaded, preventing the framework from applying backpressure.

**2. Unbounded Message Queue**

Incoming execution commands are routed through unbounded crossbeam channels: [2](#0-1) 

The `unbounded()` call creates a channel with no capacity limits, allowing unlimited message accumulation in memory.

**3. Sequential Processing Bottleneck**

The executor service processes commands sequentially in a blocking loop with CPU-intensive transaction execution: [3](#0-2) 

The service receives one command, executes an entire block of transactions (CPU-intensive), then receives the next command. This creates a processing bottleneck.

**4. No Rate Limiting or Authentication**

The gRPC service accepts messages without authentication or rate limiting: [4](#0-3) 

Messages are immediately queued via `handler.send(msg).unwrap()` on line 107, which never blocks due to the unbounded channel.

**Attack Path:**

1. Attacker identifies a remote executor service endpoint (deployed via the standalone executable)
2. Attacker sends rapid bursts of `SimpleMsgExchange` gRPC requests containing `execute_command` messages (up to 80MB each per `MAX_MESSAGE_SIZE`)
3. `poll_ready` always returns `Ready`, so Tonic accepts all incoming requests without backpressure
4. Messages accumulate in the unbounded inbound channel while the executor is busy processing previous commands sequentially
5. Memory grows unboundedly as the queue fills faster than the sequential processor can drain it
6. Eventually the server runs out of memory (OOM), crashes, and becomes unavailable

This breaks **Invariant #9 (Resource Limits)**: "All operations must respect gas, storage, and computational limits" - the unbounded queue allows unlimited memory consumption without any resource controls.

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty program criteria:

- **Validator node slowdowns**: As memory fills, the node experiences degraded performance before eventual crash
- **Service availability disruption**: The executor service becomes unresponsive, preventing distributed transaction execution
- **Denial of Service**: Repeated exploitation can keep executor shards offline, disrupting the execution layer

The impact is severe because:
- The executor service is critical infrastructure for transaction processing
- No authentication means any network peer can exploit this
- Memory exhaustion leads to unrecoverable crashes requiring manual intervention
- In a sharded execution model, taking down executor shards degrades overall system throughput

This does NOT qualify as Critical severity because it doesn't directly cause consensus violations, fund loss, or permanent network partition - it's a service-level denial of service.

## Likelihood Explanation

**Likelihood: High**

The attack is highly likely to succeed because:

1. **No attacker prerequisites**: No authentication, authorization, or special permissions required
2. **Simple exploitation**: Standard gRPC client tools can send the malicious requests
3. **Guaranteed success**: The unbounded queue and disabled backpressure ensure memory will exhaust given sufficient requests
4. **Low detection threshold**: The attack uses legitimate protocol messages, making it hard to distinguish from normal traffic without rate limiting

The main uncertainty is whether executor service endpoints are exposed to untrusted networks in production deployments. However, the existence of a standalone executable taking network addresses as parameters suggests distributed deployment scenarios where network exposure is possible.

## Recommendation

Implement multi-layered protection:

**1. Implement Proper Backpressure in poll_ready:**

Override the generated code to check actual service readiness:

```rust
fn poll_ready(
    &mut self,
    _cx: &mut Context<'_>,
) -> Poll<std::result::Result<(), Self::Error>> {
    // Check if inbound handlers can accept more messages
    // Return Poll::Pending if queues are full
    let handlers = self.inbound_handlers.lock().unwrap();
    for (_, handler) in handlers.iter() {
        if handler.len() > MAX_QUEUE_SIZE {
            return Poll::Pending;
        }
    }
    Poll::Ready(Ok(()))
}
```

**2. Use Bounded Channels:**

Replace unbounded channels with bounded ones: [2](#0-1) 

Change line 129 to:
```rust
let (inbound_sender, inbound_receiver) = bounded(MAX_INBOUND_QUEUE_SIZE);
```

Where `MAX_INBOUND_QUEUE_SIZE` should be tuned based on expected load (e.g., 100-1000 messages).

**3. Add Rate Limiting:**

Apply per-peer rate limiting using the existing `aptos-rate-limiter` crate:

```rust
use aptos_rate_limiter::{Bucket, Config};

// In GRPCNetworkMessageServiceServerWrapper
struct GRPCNetworkMessageServiceServerWrapper {
    rate_limiter: Arc<Mutex<HashMap<SocketAddr, Bucket>>>,
    // ... existing fields
}

// In simple_msg_exchange:
async fn simple_msg_exchange(&self, request: Request<NetworkMessage>) 
    -> Result<Response<Empty>, Status> {
    let remote_addr = request.remote_addr().ok_or_else(|| 
        Status::unauthenticated("Missing remote address"))?;
    
    let mut limiters = self.rate_limiter.lock().unwrap();
    let bucket = limiters.entry(remote_addr)
        .or_insert_with(|| Bucket::new(Config::default()));
    
    if !bucket.acquire_tokens(1) {
        return Err(Status::resource_exhausted("Rate limit exceeded"));
    }
    // ... rest of existing code
}
```

**4. Add Authentication:**

Implement mutual TLS or token-based authentication to restrict access to trusted peers only.

## Proof of Concept

```rust
// Add to execution/executor-service/src/tests.rs

#[test]
fn test_memory_exhaustion_via_unbounded_queue() {
    use std::thread;
    use std::time::Duration;
    use aptos_config::utils;
    use std::net::{IpAddr, Ipv4Addr, SocketAddr};
    
    // Setup executor service
    let server_addr = SocketAddr::new(
        IpAddr::V4(Ipv4Addr::LOCALHOST), 
        utils::get_available_port()
    );
    
    let mut network_controller = NetworkController::new(
        "test_executor".to_string(),
        server_addr,
        5000
    );
    
    let inbound_rx = network_controller.create_inbound_channel(
        "execute_command_0".to_string()
    );
    
    network_controller.start();
    thread::sleep(Duration::from_millis(100));
    
    // Create malicious client
    let rt = Runtime::new().unwrap();
    let mut client = GRPCNetworkMessageServiceClientWrapper::new(&rt, server_addr);
    
    // Flood with large messages
    let large_payload = vec![0u8; 10 * 1024 * 1024]; // 10MB each
    let mut handles = vec![];
    
    // Send 1000 messages as fast as possible
    for i in 0..1000 {
        let payload = large_payload.clone();
        let mut client_clone = client.clone();
        let handle = rt.spawn(async move {
            client_clone.send_message(
                server_addr,
                Message::new(payload),
                &MessageType::new("execute_command_0".to_string())
            ).await;
        });
        handles.push(handle);
    }
    
    // Verify queue is filling up (memory consumption increasing)
    // In production, this would lead to OOM
    thread::sleep(Duration::from_secs(5));
    
    // Attempt to measure queue depth
    // Since we can't directly access the channel length with unbounded channels,
    // we measure by attempting to receive with timeout
    let mut queued_count = 0;
    while inbound_rx.try_recv().is_ok() {
        queued_count += 1;
    }
    
    // With proper backpressure, queued_count should be bounded
    // Without it, it will grow unboundedly
    assert!(queued_count > 500, 
        "Unbounded queue allowed {} messages to accumulate", queued_count);
    
    network_controller.shutdown();
}
```

## Notes

This vulnerability demonstrates a classic backpressure bypass pattern. The auto-generated gRPC code from `tonic-build` does not implement proper service readiness checks, making it critical to add custom backpressure logic when wrapping generated services. The combination of disabled backpressure, unbounded buffering, and sequential processing creates a perfect storm for memory exhaustion attacks.

The issue is particularly concerning for the remote executor service because it handles transaction execution workloads, where processing latency can be significant, making the sequential bottleneck more pronounced. Any deployment exposing this service to untrusted networks is vulnerable to trivial denial-of-service attacks.

### Citations

**File:** protos/rust/src/pb/aptos.remote_executor.v1.tonic.rs (L204-209)
```rust
        fn poll_ready(
            &mut self,
            _cx: &mut Context<'_>,
        ) -> Poll<std::result::Result<(), Self::Error>> {
            Poll::Ready(Ok(()))
        }
```

**File:** secure/net/src/network_controller/mod.rs (L128-137)
```rust
    pub fn create_inbound_channel(&mut self, message_type: String) -> Receiver<Message> {
        let (inbound_sender, inbound_receiver) = unbounded();

        self.inbound_handler
            .lock()
            .unwrap()
            .register_handler(message_type, inbound_sender);

        inbound_receiver
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-272)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
        let exe_time = SHARDED_EXECUTOR_SERVICE_SECONDS
            .get_metric_with_label_values(&[&self.shard_id.to_string(), "execute_block"])
            .unwrap()
            .get_sample_sum();
        info!(
            "Shard {} is shutting down; On shard execution tps {} txns/s ({} txns / {} s)",
            self.shard_id,
            (num_txns as f64 / exe_time),
            num_txns,
            exe_time
        );
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L93-116)
```rust
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
}
```
