# Audit Report

## Title
Storage Leak: Orphaned Stale State Data After Sharding Migration

## Summary
The state pruner fails to clean up stale state data from pre-migration schemas when nodes transition from non-sharding to sharding mode, causing unbounded storage growth in ledger_db that can eventually lead to disk exhaustion and node failure.

## Finding Description

The state KV pruner uses different database schemas depending on whether sharding is enabled: [1](#0-0) 

When **sharding is disabled**, the pruner deletes from `StaleStateValueIndexSchema` and `StateValueSchema` in the metadata database (ledger_db). When **sharding is enabled**, it only iterates through `StaleStateValueIndexByKeyHashSchema` in sharded databases but performs no deletions in this code path - the actual deletion happens in separate shard pruners. [2](#0-1) 

State writes are mutually exclusive - data goes to either the non-sharded schemas OR the sharded schemas based on the current configuration. [3](#0-2) 

The sharding configuration is set at node initialization and determines which database structure is used. [4](#0-3) 

For mainnet and testnet, nodes are required to enable sharding, indicating a mandatory migration occurred.

**The vulnerability:** When a node that previously ran with sharding disabled (accumulating data in `StaleStateValueIndexSchema` and `StateValueSchema` in ledger_db) is restarted with sharding enabled:

1. The pruner checks `enabled_sharding()` and takes the sharding code path
2. The sharding path only accesses `StaleStateValueIndexByKeyHashSchema` in shard databases
3. The old data in ledger_db's `StaleStateValueIndexSchema` and `StateValueSchema` is never touched
4. This orphaned data accumulates forever, consuming disk space without bound [5](#0-4) 

The ledger_db always contains these column families regardless of sharding mode, so the orphaned data persists.

## Impact Explanation

This qualifies as **Medium Severity** under "State inconsistencies requiring intervention":

- **Storage Exhaustion**: All mainnet/testnet nodes that underwent the sharding migration have accumulated stale data that grows unbounded
- **Node Availability**: Eventually, disk space exhaustion will cause validator nodes to fail
- **Network Degradation**: Performance degrades as the database grows without limit
- **Manual Intervention Required**: No automatic cleanup mechanism exists; operators must manually clean up orphaned data

While this doesn't directly affect consensus correctness, it impacts validator availability which is critical for network liveness.

## Likelihood Explanation

**Likelihood: HIGH** - This occurs automatically on all nodes that:
1. Ran with sharding disabled before the mandatory migration (all pre-AIP-97 mainnet/testnet nodes)
2. Continue to write new state values, creating stale entries that never get pruned from the old schemas
3. Have been running post-migration for extended periods, accumulating orphaned data

The storage leak is continuous and affects all production networks.

## Recommendation

Add a one-time migration cleanup step that:

1. Detects orphaned data in the old schemas when sharding is enabled
2. Deletes entries from `StaleStateValueIndexSchema` and `StateValueSchema` in ledger_db
3. Can be run as a background task to avoid blocking node operation

Example fix in `state_kv_metadata_pruner.rs`:

```rust
pub(in crate::pruner) fn prune(
    &self,
    current_progress: Version,
    target_version: Version,
) -> Result<()> {
    let mut batch = SchemaBatch::new();

    if self.state_kv_db.enabled_sharding() {
        // ... existing sharding path ...
        
        // MIGRATION CLEANUP: Also prune any orphaned data from pre-migration schemas
        // This can be removed after all mainnet nodes have been cleaned up
        let mut legacy_iter = self
            .state_kv_db
            .metadata_db()
            .iter::<StaleStateValueIndexSchema>()?;
        legacy_iter.seek(&current_progress)?;
        for item in legacy_iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexSchema>(&index)?;
            batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
        }
    } else {
        // ... existing non-sharding path ...
    }

    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::StateKvPrunerProgress,
        &DbMetadataValue::Version(target_version),
    )?;

    self.state_kv_db.metadata_db().write_schemas(batch)
}
```

## Proof of Concept

To verify the issue exists on a node that underwent migration:

```rust
// Check for orphaned data in ledger_db
use aptos_db::schema::stale_state_value_index::StaleStateValueIndexSchema;

// On a node with sharding enabled, check ledger_db
let ledger_db = /* get ledger_db reference */;
let mut iter = ledger_db.iter::<StaleStateValueIndexSchema>()?;
iter.seek_to_first()?;

let mut count = 0;
for item in iter {
    let (index, _) = item?;
    count += 1;
    if count % 10000 == 0 {
        println!("Found {} orphaned stale indices", count);
    }
}

println!("Total orphaned entries: {}", count);
// On affected nodes, this will show thousands to millions of orphaned entries
// that consume disk space but are never pruned
```

---

## Notes

This vulnerability specifically affects the transition from non-sharded to sharded storage mode. The pruner's conditional logic correctly handles either mode independently, but fails to account for orphaned data from the previous mode after migration. This is a classic data migration issue where cleanup of the old format was not implemented.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L35-64)
```rust
        if self.state_kv_db.enabled_sharding() {
            let num_shards = self.state_kv_db.num_shards();
            // NOTE: This can be done in parallel if it becomes the bottleneck.
            for shard_id in 0..num_shards {
                let mut iter = self
                    .state_kv_db
                    .db_shard(shard_id)
                    .iter::<StaleStateValueIndexByKeyHashSchema>()?;
                iter.seek(&current_progress)?;
                for item in iter {
                    let (index, _) = item?;
                    if index.stale_since_version > target_version {
                        break;
                    }
                }
            }
        } else {
            let mut iter = self
                .state_kv_db
                .metadata_db()
                .iter::<StaleStateValueIndexSchema>()?;
            iter.seek(&current_progress)?;
            for item in iter {
                let (index, _) = item?;
                if index.stale_since_version > target_version {
                    break;
                }
                batch.delete::<StaleStateValueIndexSchema>(&index)?;
                batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
            }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L985-1015)
```rust
    fn put_state_kv_index(
        batch: &mut NativeBatch,
        enable_sharding: bool,
        stale_since_version: Version,
        version: Version,
        key: &StateKey,
    ) {
        if enable_sharding {
            batch
                .put::<StaleStateValueIndexByKeyHashSchema>(
                    &StaleStateValueByKeyHashIndex {
                        stale_since_version,
                        version,
                        state_key_hash: key.hash(),
                    },
                    &(),
                )
                .unwrap();
        } else {
            batch
                .put::<StaleStateValueIndexSchema>(
                    &StaleStateValueIndex {
                        stale_since_version,
                        version,
                        state_key: (*key).clone(),
                    },
                    &(),
                )
                .unwrap();
        }
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L62-71)
```rust
        let sharding = rocksdb_configs.enable_storage_sharding;
        if !sharding {
            info!("State K/V DB is not enabled!");
            return Ok(Self {
                state_kv_metadata_db: Arc::clone(&ledger_db),
                state_kv_db_shards: arr![Arc::clone(&ledger_db); 16],
                hot_state_kv_db_shards: None,
                enabled_sharding: false,
            });
        }
```

**File:** config/src/config/storage_config.rs (L664-668)
```rust
            if (chain_id.is_testnet() || chain_id.is_mainnet())
                && config_yaml["rocksdb_configs"]["enable_storage_sharding"].as_bool() != Some(true)
            {
                panic!("Storage sharding (AIP-97) is not enabled in node config. Please follow the guide to migration your node, and set storage.rocksdb_configs.enable_storage_sharding to true explicitly in your node config. https://aptoslabs.notion.site/DB-Sharding-Migration-Public-Full-Nodes-1978b846eb7280b29f17ceee7d480730");
            }
```

**File:** storage/aptosdb/src/db_options.rs (L14-39)
```rust
pub(super) fn ledger_db_column_families() -> Vec<ColumnFamilyName> {
    vec![
        /* empty cf */ DEFAULT_COLUMN_FAMILY_NAME,
        BLOCK_BY_VERSION_CF_NAME,
        BLOCK_INFO_CF_NAME,
        EPOCH_BY_VERSION_CF_NAME,
        EVENT_ACCUMULATOR_CF_NAME,
        EVENT_BY_KEY_CF_NAME,
        EVENT_BY_VERSION_CF_NAME,
        EVENT_CF_NAME,
        LEDGER_INFO_CF_NAME,
        PERSISTED_AUXILIARY_INFO_CF_NAME,
        STALE_STATE_VALUE_INDEX_CF_NAME,
        STATE_VALUE_CF_NAME,
        TRANSACTION_CF_NAME,
        TRANSACTION_ACCUMULATOR_CF_NAME,
        TRANSACTION_ACCUMULATOR_HASH_CF_NAME,
        TRANSACTION_AUXILIARY_DATA_CF_NAME,
        ORDERED_TRANSACTION_BY_ACCOUNT_CF_NAME,
        TRANSACTION_SUMMARIES_BY_ACCOUNT_CF_NAME,
        TRANSACTION_BY_HASH_CF_NAME,
        TRANSACTION_INFO_CF_NAME,
        VERSION_DATA_CF_NAME,
        WRITE_SET_CF_NAME,
        DB_METADATA_CF_NAME,
    ]
```
