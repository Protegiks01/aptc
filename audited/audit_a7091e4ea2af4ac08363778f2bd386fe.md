# Audit Report

## Title
Silent RPC Failure in Secret Share Manager Causes Indefinite Retries and Resource Exhaustion

## Summary
When a validator receives a secret share request (`RequestShare` RPC) but doesn't have the requested share available, it logs a warning but fails to send any response to the requesting validator. This causes the requester's RPC to timeout after 10 seconds and triggers infinite retry attempts with exponential backoff, wasting network bandwidth and potentially delaying consensus progress.

## Finding Description

The vulnerability exists in the `handle_incoming_msg()` function where secret share requests are processed. When a validator cannot find the requested share in its local store, two problematic scenarios occur: [1](#0-0) 

In both the `Ok(None)` case (share not found) and the `Err(e)` case (validation error), only a warning is logged and the `response_sender` oneshot channel is never used to send a response back to the requester.

**When does this occur?**

The most common scenario happens during validator initialization or epoch transitions. When a `SecretShareManager` starts, it receives the `highest_committed_round` parameter and immediately sets its internal `highest_known_round` to this value: [2](#0-1) [3](#0-2) 

At this point, the `secret_share_map` is empty (new manager instance), but the validator will accept share requests for any round up to `highest_committed_round`. When such a request arrives:

1. The future-round validation check passes: [4](#0-3) 

2. But the map lookup returns `None`: [5](#0-4) 

3. The function returns `Ok(None)`, triggering the silent failure path.

**What happens on the requester's side?**

The requesting validator uses `ReliableBroadcast` to send the RPC with a configured timeout: [6](#0-5) 

When no response is received within 10 seconds, the RPC protocol layer times out and returns an error. The `ReliableBroadcast` system treats this as a transient failure and retries indefinitely with exponential backoff: [7](#0-6) 

With the configured backoff parameters (base=2ms, factor=100, max_delay=10s), the retry pattern becomes:
- First retry: wait 2ms, timeout after 10s
- Second retry: wait 200ms, timeout after 10s  
- Third retry: wait 10s (capped), timeout after 10s
- Subsequent retries: wait 10s, timeout after 10s (20s cycle)

This continues until either:
- The target validator processes the block and generates the share (which may never happen for already-committed rounds)
- Enough other validators respond to meet the reconstruction threshold: [8](#0-7) 

## Impact Explanation

This issue qualifies as **Medium Severity** per the Aptos bug bounty criteria:

1. **Resource Exhaustion**: Each failed RPC wastes 10 seconds of waiting time plus network bandwidth. With multiple validators experiencing this issue simultaneously, the cumulative waste is significant.

2. **Consensus Delays**: Secret share aggregation is critical for randomness generation in consensus. Repeated timeouts delay block finalization, reducing network throughput and increasing latency.

3. **Potential Liveness Impact**: If more than `n - t` validators are missing shares (where `n` is total validators and `t` is threshold), the aggregation can never complete, effectively blocking consensus progress until validators are restarted or state is manually recovered.

4. **Silent Failure**: The lack of explicit error responses makes this issue difficult to diagnose and monitor in production environments, compounding operational challenges.

5. **State Inconsistency**: This creates temporary state inconsistencies where validators have different views of share availability, violating the assumption that all honest validators should be able to participate in secret reconstruction.

The impact is limited to performance degradation and resource waste under normal conditions, but can escalate to liveness failures under specific circumstances (multiple simultaneous validator restarts, network partitions).

## Likelihood Explanation

**High Likelihood** - This issue will manifest in production with high probability:

1. **Validator Restarts**: Common operational events like upgrades, crashes, or configuration changes trigger the vulnerable code path whenever a `SecretShareManager` is reinitialized.

2. **Epoch Transitions**: Every epoch change creates a new `SecretShareManager` instance, exposing the vulnerability window where `highest_known_round` is set but `secret_share_map` is empty.

3. **No Attack Required**: This is a protocol bug that occurs during normal operation without any malicious actor involvement.

4. **Network Conditions**: Under network partitions or high latency scenarios, the probability increases as validators may be at different consensus heights.

The vulnerability is deterministic and will occur whenever the specific conditions (initialized manager receiving requests for rounds it hasn't processed) are met.

## Recommendation

Send an explicit error response when the share is not available, allowing the requester to handle the failure appropriately instead of waiting for a timeout.

**Recommended Fix:**

Modify `handle_incoming_msg()` to send an error response in both failure cases:

```rust
fn handle_incoming_msg(&self, rpc: SecretShareRpc) {
    let SecretShareRpc {
        msg,
        protocol,
        response_sender,
    } = rpc;
    match msg {
        SecretShareMessage::RequestShare(request) => {
            let result = self
                .secret_share_store
                .lock()
                .get_self_share(request.metadata());
            match result {
                Ok(Some(share)) => {
                    self.process_response(
                        protocol,
                        response_sender,
                        SecretShareMessage::Share(share),
                    );
                },
                Ok(None) => {
                    warn!(
                        "Self secret share could not be found for RPC request {}",
                        request.metadata().round
                    );
                    // Send error response instead of silent failure
                    let _ = response_sender.send(Err(RpcError::ApplicationError(
                        anyhow::anyhow!("Share not available for round {}", request.metadata().round)
                    )));
                },
                Err(e) => {
                    warn!("[SecretShareManager] Failed to get share: {}", e);
                    // Send error response
                    let _ = response_sender.send(Err(RpcError::ApplicationError(e)));
                },
            }
        },
        SecretShareMessage::Share(share) => {
            // ... existing code ...
        },
    }
}
```

**Alternative Fix (Protocol-Level):**

Add an error variant to `SecretShareMessage` enum to support explicit negative responses:

```rust
pub enum SecretShareMessage {
    RequestShare(RequestSecretShare),
    Share(SecretShare),
    ShareNotAvailable(SecretShareMetadata), // New variant
}
```

This would allow the requester to distinguish between "share doesn't exist" and "network/timeout failure", enabling smarter retry logic.

## Proof of Concept

```rust
// Simulation scenario demonstrating the vulnerability:
// 
// Setup: 4 validators in an epoch
// - Validator A: Just restarted, highest_committed_round = 1000, secret_share_map is empty
// - Validator B: Running normally, processing block for round 1001, needs shares from round 1000
//
// Expected behavior: Validator A should respond with an error/not-found message
// Actual behavior: Validator A logs warning, sends no response, causing B to timeout and retry
//
// Steps to reproduce:
//
// 1. Initialize Validator A's SecretShareManager with highest_known_round = 1000
//    (simulating recovery from committed state)
//
// 2. Validator B sends RequestShare RPC for round 1000 to Validator A
//
// 3. Validator A's handle_incoming_msg() is called:
//    - get_self_share(round=1000) is called
//    - Check passes: 1000 <= 1000 (highest_known_round)
//    - Map lookup: secret_share_map.get(1000) returns None
//    - Result: Ok(None)
//    - Action: Log warning, no response sent
//
// 4. Validator B waits for response:
//    - After 10 seconds: RPC timeout
//    - ReliableBroadcast retry logic triggered
//    - Exponential backoff: 2ms -> 200ms -> 10s -> 10s ...
//    - Continues indefinitely until threshold met or A processes block 1000
//
// Verification:
// - Monitor network traffic: Repeated RPC requests to Validator A every ~20s
// - Check logs: Warning messages on A, timeout errors on B
// - Measure impact: Consensus delay proportional to retry cycles
// - Resource consumption: Network bandwidth, CPU for serialization/deserialization
```

**Notes:**

The vulnerability is exacerbated by the fact that for already-committed rounds, the validator may never process those blocks again (they're in the past), meaning the retries will continue indefinitely until either:
1. The requesting validator gathers enough shares from other validators to meet the threshold
2. Manual intervention (restart, state sync) occurs
3. The round becomes irrelevant (consensus moves forward despite missing randomness)

This creates a subtle but persistent resource drain that accumulates over time, especially in networks with frequent validator restarts or high churn rates.

### Citations

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L299-304)
```rust
                    Ok(None) => {
                        warn!(
                            "Self secret share could not be found for RPC request {}",
                            request.metadata().round
                        );
                    },
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L337-339)
```rust
            self.secret_share_store
                .lock()
                .update_highest_known_round(highest_known_round);
```

**File:** consensus/src/pipeline/execution_client.rs (L296-301)
```rust
        tokio::spawn(secret_share_manager.start(
            ordered_block_rx,
            secret_sharing_msg_rx,
            reset_secret_share_manager_rx,
            self.bounded_executor.clone(),
            highest_committed_round,
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L44-46)
```rust
        if self.total_weight < secret_share_config.threshold() {
            return Either::Left(self);
        }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L292-297)
```rust
        ensure!(
            metadata.round <= self.highest_known_round,
            "Request share from future round {}, highest known round {}",
            metadata.round,
            self.highest_known_round
        );
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L298-302)
```rust
        Ok(self
            .secret_share_map
            .get(&metadata.round)
            .and_then(|item| item.get_self_share())
            .filter(|share| &share.metadata == metadata))
```

**File:** config/src/config/consensus_config.rs (L373-378)
```rust
            rand_rb_config: ReliableBroadcastConfig {
                backoff_policy_base_ms: 2,
                backoff_policy_factor: 100,
                backoff_policy_max_delay_ms: 10000,
                rpc_timeout_ms: 10000,
            },
```

**File:** crates/reliable-broadcast/src/lib.rs (L191-200)
```rust
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
```
