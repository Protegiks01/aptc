# Audit Report

## Title
Indexer GRPC Data Service Cache Serves Transactions from Wrong Fork Without Detection

## Summary
The in-memory cache in `indexer-grpc-data-service-v2` stores and serves transactions based solely on version numbers without validating fork consistency. When blockchain forks occur, the cache can overwrite correct transactions with wrong-fork transactions and serve them to clients with no detection mechanism, leading to data corruption in downstream indexer systems.

## Finding Description

The `InMemoryCache` component in the indexer GRPC data service stores transactions indexed only by version number using modulo-based slot allocation. It lacks any fork detection or validation mechanism.

**Critical Missing Validations:**

1. **Cache Storage Without Fork Validation** - The `DataManager::update_data()` function stores transactions using only version-based indexing: [1](#0-0) 

The cache overwrites any existing transaction at a slot without checking if it belongs to the same fork.

2. **Cache Retrieval Without Fork Validation** - The `InMemoryCache::get_data()` function retrieves transactions based solely on version: [2](#0-1) 

No validation of `accumulator_root_hash` or other fork indicators occurs.

3. **Upstream Fetch Without Fork Validation** - The `DataClient::fetch_transactions()` only validates version number match: [3](#0-2) 

Line 37 checks only `transactions.first().unwrap().version == starting_version`, not fork consistency.

**Available But Unused Fork Detection Data:**

Each transaction contains an `accumulator_root_hash` in its `TransactionInfo`: [4](#0-3) 

This field represents the transaction accumulator state up to that transaction and would differ between forks. However, the cache never validates this field.

**Attack Scenario:**

1. Cache is populated with transactions V1000-V1100 from canonical fork A
2. Network partition occurs, or indexer connects to different fullnode
3. Upstream serves transactions V1050-V1150 from fork B (diverged at V1050)
4. Cache's `update_data()` overwrites slots for V1050-V1100 with fork B transactions
5. Client calls `get_data(1060, 1080, ...)` and receives fork B transactions
6. Client has no indication these are from wrong fork
7. Downstream databases and analytics systems get corrupted with wrong-fork data

**Proof of Fork Detection in Core System:**

The core Aptos system has proper fork detection via `TransactionInfoListWithProof::verify_extends_ledger()`: [5](#0-4) 

Line 968-971 specifically detects forks by comparing accumulator root hashes. However, this validation is not used in the indexer cache layer.

## Impact Explanation

**Severity: HIGH** (aligns with "Significant protocol violations" and "API crashes" categories)

**Direct Impacts:**

1. **Data Integrity Violation**: Indexer clients receive transactions from wrong fork, breaking the fundamental guarantee of serving canonical chain data
2. **Downstream Corruption**: Databases, analytics systems, and applications consuming indexer data become corrupted with wrong-fork transactions
3. **Silent Failure**: No error detection means corruption persists unnoticed until manual discovery
4. **Financial Risk**: Applications making financial decisions based on indexer data (DeFi protocols, exchanges, wallets) operate on incorrect state

**Scope:**

- Affects all clients of the indexer-grpc-data-service-v2 live data service
- Impacts any system building on top of indexer data (block explorers, analytics platforms, monitoring systems)
- Can persist across service restarts if wrong-fork data is not cleared

This does not directly affect consensus or validator operations, but severely impacts the indexer infrastructure that the Aptos ecosystem relies on for data access.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

**Triggering Conditions:**

1. **Network Partitions**: Natural forks can occur during network partitions when validator sets temporarily diverge
2. **Multiple Fullnode Connections**: Indexer services typically connect to multiple fullnodes for redundancy. If these fullnodes are on different forks temporarily, the indexer can fetch conflicting data
3. **Peer Rotation**: The connection manager randomly selects peers for requests, potentially switching between nodes on different forks
4. **No Prevention Mechanism**: Zero validation exists to prevent or detect this scenario

**Real-World Scenarios:**

- During network instability or datacenter failures
- When fullnodes have different sync states
- During chain reorganizations (though rare in BFT)
- If malicious fullnode is added to peer list

The TODO comment in the code suggests error handling is incomplete: [6](#0-5) 

This indicates the developers are aware of insufficient error handling in this code path.

## Recommendation

**Implement Fork Consistency Validation:**

1. **Track Expected Accumulator Root Hash**: Maintain the expected `accumulator_root_hash` for each version in the cache based on the canonical chain.

2. **Validate on Update**: In `DataManager::update_data()`, verify that incoming transactions' `accumulator_root_hash` values are consistent with the expected chain:

```rust
// In update_data(), after line 75:
for (i, transaction) in transactions.into_iter().enumerate().skip(num_to_skip as usize) {
    let version = start_version + i as u64;
    
    // Validate fork consistency
    if let Some(expected_root) = self.expected_accumulator_roots.get(&version) {
        if let Some(info) = &transaction.info {
            if info.accumulator_root_hash != *expected_root {
                error!("Fork detected at version {}: expected root {:?}, got {:?}", 
                       version, expected_root, info.accumulator_root_hash);
                COUNTER.with_label_values(&["fork_detected"]).inc();
                return; // Reject entire batch
            }
        }
    }
    
    // Store expected root for next transaction
    if let Some(info) = &transaction.info {
        self.expected_accumulator_roots.insert(version + 1, info.accumulator_root_hash.clone());
    }
    
    // ... existing storage logic
}
```

3. **Validate on Fetch**: In `DataClient::fetch_transactions()`, check accumulator root hash consistency across the returned transaction list.

4. **Add Fork Detection Metrics**: Expose metrics when fork detection occurs to enable monitoring and alerting.

5. **Graceful Degradation**: On fork detection, clear potentially corrupted cache data and re-fetch from a trusted source.

## Proof of Concept

```rust
// Rust test demonstrating the vulnerability
#[tokio::test]
async fn test_cache_serves_wrong_fork_without_detection() {
    use aptos_protos::transaction::v1::{Transaction, TransactionInfo};
    
    // Setup: Create cache with transactions from fork A
    let mut cache = DataManager::new(1000, 100, 1_000_000);
    
    // Simulate transactions from fork A (versions 1000-1010)
    let fork_a_txns: Vec<Transaction> = (1000..1010)
        .map(|v| Transaction {
            version: v,
            info: Some(TransactionInfo {
                accumulator_root_hash: vec![0xAA; 32], // Fork A root
                ..Default::default()
            }),
            ..Default::default()
        })
        .collect();
    
    cache.update_data(1000, fork_a_txns);
    
    // Simulate receiving transactions from fork B at overlapping versions
    let fork_b_txns: Vec<Transaction> = (1005..1015)
        .map(|v| Transaction {
            version: v,
            info: Some(TransactionInfo {
                accumulator_root_hash: vec![0xBB; 32], // Fork B root - DIFFERENT!
                ..Default::default()
            }),
            ..Default::default()
        })
        .collect();
    
    // VULNERABILITY: update_data() accepts fork B transactions without validation
    cache.update_data(1005, fork_b_txns);
    
    // Retrieve transaction at version 1007
    let txn = cache.get_data(1007).as_ref().unwrap();
    
    // PROOF OF VULNERABILITY: Cache now serves fork B transaction without detection
    assert_eq!(txn.info.as_ref().unwrap().accumulator_root_hash, vec![0xBB; 32]);
    // Client receives wrong fork data with no error or indication!
    
    // Expected behavior: Should have detected fork and rejected fork B transactions
    // or returned error when serving potentially forked data
}
```

## Notes

This vulnerability is specific to the indexer infrastructure layer and does not affect core consensus or validator operations. However, it critically undermines data integrity guarantees for the broader Aptos ecosystem that depends on indexer services for blockchain data access. The fix requires implementing the same fork detection mechanisms used in the core state synchronization layer (`verify_extends_ledger`) but at the indexer cache level.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_manager.rs (L80-86)
```rust
            let version = start_version + i as u64;
            let slot_index = version as usize % self.num_slots;
            if let Some(transaction) = self.data[slot_index].take() {
                size_decreased += transaction.encoded_len();
            }
            size_increased += transaction.encoded_len();
            self.data[version as usize % self.num_slots] = Some(Box::new(transaction));
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/in_memory_cache.rs (L40-101)
```rust
    pub(super) async fn get_data(
        &'a self,
        starting_version: u64,
        ending_version: u64,
        max_num_transactions_per_batch: usize,
        max_bytes_per_batch: usize,
        filter: &Option<BooleanTransactionFilter>,
    ) -> Option<(Vec<Transaction>, usize, u64)> {
        let _timer = TIMER.with_label_values(&["cache_get_data"]).start_timer();

        while starting_version >= self.data_manager.read().await.end_version {
            trace!("Reached head, wait...");
            let num_transactions = self
                .fetch_manager
                .fetching_latest_data_task
                .read()
                .await
                .as_ref()
                .unwrap()
                .clone()
                .await;

            trace!("Done waiting, got {num_transactions} transactions at head.");
        }

        loop {
            let data_manager = self.data_manager.read().await;

            trace!("Getting data from cache, requested_version: {starting_version}, oldest available version: {}.", data_manager.start_version);
            if starting_version < data_manager.start_version {
                return None;
            }

            if data_manager.get_data(starting_version).is_none() {
                drop(data_manager);
                self.fetch_manager.fetch_past_data(starting_version).await;
                continue;
            }

            let mut total_bytes = 0;
            let mut version = starting_version;
            let ending_version = ending_version.min(data_manager.end_version);

            let mut result = Vec::new();
            while version < ending_version
                && total_bytes < max_bytes_per_batch
                && result.len() < max_num_transactions_per_batch
            {
                if let Some(transaction) = data_manager.get_data(version).as_ref() {
                    // NOTE: We allow 1 more txn beyond the size limit here, for simplicity.
                    if filter.is_none() || filter.as_ref().unwrap().matches(transaction) {
                        total_bytes += transaction.encoded_len();
                        result.push(transaction.as_ref().clone());
                    }
                    version += 1;
                } else {
                    break;
                }
            }
            trace!("Data was sent from cache, last version: {}.", version - 1);
            return Some((result, total_bytes, version - 1));
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_client.rs (L18-43)
```rust
    pub(super) async fn fetch_transactions(&self, starting_version: u64) -> Vec<Transaction> {
        trace!("Fetching transactions from GrpcManager, start_version: {starting_version}.");

        let request = GetTransactionsRequest {
            starting_version: Some(starting_version),
            transactions_count: None,
            batch_size: None,
            transaction_filter: None,
        };
        loop {
            let mut client = self
                .connection_manager
                .get_grpc_manager_client_for_request();
            let response = client.get_transactions(request.clone()).await;
            if let Ok(response) = response {
                let transactions = response.into_inner().transactions;
                if transactions.is_empty() {
                    return vec![];
                }
                if transactions.first().unwrap().version == starting_version {
                    return transactions;
                }
            }
            // TODO(grao): Error handling.
        }
    }
```

**File:** protos/proto/aptos/transaction/v1/transaction.proto (L169-179)
```text
message TransactionInfo {
  bytes hash = 1;
  bytes state_change_hash = 2;
  bytes event_root_hash = 3;
  optional bytes state_checkpoint_hash = 4;
  uint64 gas_used = 5 [jstype = JS_STRING];
  bool success = 6;
  string vm_status = 7;
  bytes accumulator_root_hash = 8;
  repeated WriteSetChange changes = 9;
}
```

**File:** types/src/proof/definition.rs (L927-978)
```rust
    pub fn verify_extends_ledger(
        &self,
        num_txns_in_ledger: LeafCount,
        root_hash: HashValue,
        first_transaction_info_version: Option<Version>,
    ) -> Result<usize> {
        if let Some(first_version) = first_transaction_info_version {
            ensure!(
                first_version <= num_txns_in_ledger,
                "Transaction list too new. Expected version: {}. First transaction version: {}.",
                num_txns_in_ledger,
                first_version
            );
            let num_overlap_txns = (num_txns_in_ledger - first_version) as usize;
            if num_overlap_txns > self.transaction_infos.len() {
                // Entire chunk is in the past, hard to verify if there's a fork.
                // A fork will need to be detected later.
                return Ok(self.transaction_infos.len());
            }
            let overlap_txn_infos = &self.transaction_infos[..num_overlap_txns];

            // Left side of the proof happens to be the frozen subtree roots of the accumulator
            // right before the list of txns are applied.
            let frozen_subtree_roots_from_proof = self
                .ledger_info_to_transaction_infos_proof
                .left_siblings()
                .iter()
                .rev()
                .cloned()
                .collect::<Vec<_>>();
            let accu_from_proof = InMemoryTransactionAccumulator::new(
                frozen_subtree_roots_from_proof,
                first_version,
            )?
            .append(
                &overlap_txn_infos
                    .iter()
                    .map(CryptoHash::hash)
                    .collect::<Vec<_>>()[..],
            );
            // The two accumulator root hashes should be identical.
            ensure!(
                accu_from_proof.root_hash() == root_hash,
                "Fork happens because the current synced_trees doesn't match the txn list provided."
            );
            Ok(num_overlap_txns)
        } else {
            // Assuming input is empty
            ensure!(self.transaction_infos.is_empty());
            Ok(0)
        }
    }
```
