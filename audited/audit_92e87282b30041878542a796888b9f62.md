# Audit Report

## Title
Race Condition in StateSummary::update() Causes Validator Node Crashes During Concurrent Fork Processing

## Summary
A race condition exists in `StateSummary::update()` where the version ordering assertion `persisted.next_version() <= self.next_version()` can be violated when processing forked blocks concurrently. This occurs when one fork commits and updates the persisted state while another fork is mid-computation, causing validator nodes to panic and crash. [1](#0-0) 

## Finding Description

The vulnerability exists in the state checkpoint computation pipeline where blocks can be processed concurrently through asynchronous execution. The critical assertion at line 96 enforces that the persisted state version must not exceed the parent state version being built upon. [2](#0-1) 

The race condition manifests in the following scenario:

1. **Fork Creation**: Two competing blocks (Block B and Block C) are both children of Block A, creating a fork at the same height. Both blocks begin execution and enter the ledger update phase.

2. **Concurrent Ledger Update Processing**: The consensus pipeline spawns `ledger_update` as a blocking task for both blocks. These execute concurrently: [3](#0-2) 

3. **Race Window**: In `BlockExecutorInner::ledger_update()`, there's a critical race window between fetching the parent state summary and fetching the persisted state summary: [4](#0-3) 

4. **The Race**: 
   - Thread 1 (Block B): Retrieves `parent_state_summary` from Block A (version N)
   - **PAUSE** - Thread 1 is preempted
   - Thread 2 (Block C): Completes its full ledger update pipeline
   - Thread 2 (Block C): Commits to disk via `pre_commit_block` and `commit_ledger`, updating `persisted_state` to version M (where M > N) [5](#0-4) 

   - Thread 1 (Block B): Resumes and retrieves `persisted_state_summary` (now at version M)
   - Thread 1 (Block B): Calls `DoStateCheckpoint::run()` with `parent_state_summary.next_version() = N+1` and `persisted.next_version() = M+1`
   - Thread 1 (Block B): Enters `StateSummary::update()` where the assertion `M+1 <= N+1` **FAILS**, causing a panic

5. **Non-Abortable Execution**: The blocking task cannot be aborted mid-execution in Tokio, meaning even if the consensus layer decides to abandon Block B's fork after Block C commits, Block B's ledger_update will continue executing until it hits the assertion failure. [6](#0-5) 

The vulnerability breaks the **State Consistency** and **Deterministic Execution** invariants by allowing a scenario where state checkpoint computation fails catastrophically instead of being handled gracefully.

## Impact Explanation

**Severity: High** (potentially Critical)

This vulnerability causes validator node crashes with the following impacts:

1. **Validator Liveness Impact**: When the assertion fails, the thread panics, potentially crashing the entire validator node process. This directly impacts network availability.

2. **Reproducible Crashes**: In networks with high contention or Byzantine behavior, fork scenarios occur naturally. Each time competing forks are processed concurrently with unfortunate timing, validators can crash repeatedly.

3. **Multi-Validator Impact**: Multiple validators processing the same forked blocks under similar network conditions will all experience the same race, potentially causing widespread crashes.

4. **Network Partition Risk**: If a significant fraction of validators crash simultaneously due to this race condition, the network could lose liveness or require manual intervention to recover.

This meets the **High Severity** criteria ("Validator node slowdowns" - though actual crashes are more severe than slowdowns) and borders on **Critical Severity** ("Total loss of liveness/network availability") if multiple validators are affected simultaneously.

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability is realistic and can occur without malicious intent:

1. **Natural Occurrence**: Forks occur naturally in BFT consensus due to:
   - Network delays causing validators to receive different proposals first
   - Multiple validators proposing for the same slot
   - Near-simultaneous proposal broadcasts

2. **Timing Requirements**: The race window is narrow but achievable:
   - Block B must fetch parent state summary
   - Block C must commit (3-phase process: ledger_update → pre_commit → commit_ledger)
   - Block B must fetch persisted state summary after Block C commits
   - With async execution and blocking tasks, this timing is realistic under load

3. **No Byzantine Requirements**: This can happen with honest validators under normal network conditions, increasing likelihood.

4. **Amplification Under Stress**: Network congestion, high block production rates, or Byzantine validators (within the <1/3 threshold) increase fork probability and race timing likelihood.

## Recommendation

**Immediate Fix**: Replace the assertion with graceful error handling that detects stale parent state:

```rust
fn update(
    &self,
    persisted: &ProvableStateSummary,
    hot_updates: &[HotStateShardUpdates; NUM_STATE_SHARDS],
    updates: &BatchedStateUpdateRefs,
) -> Result<Self> {
    let _timer = TIMER.timer_with(&["state_summary__update"]);

    assert_ne!(self.hot_state_summary.root_hash(), *CORRUPTION_SENTINEL);
    assert_ne!(self.global_state_summary.root_hash(), *CORRUPTION_SENTINEL);

    // Persisted must be before or at my version.
    // If persisted is ahead, this block is on a dead fork - return error instead of panicking
    if persisted.next_version() > self.next_version() {
        return Err(anyhow::anyhow!(
            "State summary update rejected: persisted state (version {}) is ahead of parent state (version {}). \
             This block is likely on an abandoned fork.",
            persisted.next_version(),
            self.next_version()
        ));
    }
    
    // Updates must start at exactly my version.
    assert_eq!(updates.first_version(), self.next_version());
    
    // ... rest of function
}
```

**Long-term Fix**: Implement proper fork detection and cancellation:

1. Add a version check in `ledger_update` before calling `DoStateCheckpoint::run()` to detect if the block is on a stale fork
2. Implement cooperative cancellation in the blocking task using a cancellation token
3. Add monitoring/metrics to detect and log fork-related races for observability [7](#0-6) 

## Proof of Concept

```rust
// This PoC demonstrates the race condition through a simulated scenario
// Add to: storage/aptosdb/src/state_store/tests/speculative_state_workflow.rs

#[test]
fn test_concurrent_fork_ledger_update_race() {
    use std::sync::{Arc, Mutex};
    use std::thread;
    use std::time::Duration;
    
    let hot_state_config = TEST_CONFIG;
    
    // Setup: Block A (parent) at version 100
    let block_a_state = State::new_at_version(
        Some(100),
        StateStorageUsage::zero(),
        hot_state_config,
    );
    let block_a_summary = StateSummary::new_at_version(
        Some(100),
        SparseMerkleTree::new_empty(),
        SparseMerkleTree::new_empty(),
        hot_state_config,
    );
    
    // Shared persisted state (simulating the global persisted state)
    let persisted_state = Arc::new(Mutex::new(block_a_summary.clone()));
    
    // Block B: child of A (fork 1) - will be victim of race
    let block_b_parent_summary = block_a_summary.clone();
    let persisted_state_b = Arc::clone(&persisted_state);
    
    // Block C: child of A (fork 2) - will commit and cause race
    let persisted_state_c = Arc::clone(&persisted_state);
    
    let thread_b = thread::spawn(move || {
        // Simulate Block B starting ledger_update
        let parent_summary = block_b_parent_summary; // Fetched at version 101
        
        // RACE WINDOW: Sleep to simulate Block C committing during this time
        thread::sleep(Duration::from_millis(100));
        
        // Fetch persisted state (now updated by Block C to version 150)
        let persisted = persisted_state_b.lock().unwrap().clone();
        
        // Create dummy updates starting at parent's next version
        let updates = BatchedStateUpdateRefs::new(
            101, // first_version = parent.next_version()
            vec![],
            vec![],
        );
        
        let hot_updates = arr![HotStateShardUpdates::new_empty(); 16];
        
        // This should panic with assertion failure if race occurs
        let result = parent_summary.update(
            &ProvableStateSummary::new(persisted, &FakeDb),
            &hot_updates,
            &updates,
        );
        
        result
    });
    
    let thread_c = thread::spawn(move || {
        // Simulate Block C committing
        thread::sleep(Duration::from_millis(50));
        
        // Block C commits, updating persisted state to version 150
        let block_c_summary = StateSummary::new_at_version(
            Some(150),
            SparseMerkleTree::new_empty(),
            SparseMerkleTree::new_empty(),
            hot_state_config,
        );
        
        *persisted_state_c.lock().unwrap() = block_c_summary;
        Ok(())
    });
    
    // Block C completes successfully
    thread_c.join().unwrap().unwrap();
    
    // Block B should panic with assertion failure
    let result_b = thread_b.join();
    match result_b {
        Ok(Ok(_)) => panic!("Expected assertion failure, but update succeeded"),
        Ok(Err(e)) => println!("Got expected error: {}", e),
        Err(panic) => {
            // Expected: panic due to assertion failure
            println!("Got expected panic (assertion failure)");
            // In production, this would crash the validator node
        }
    }
}
```

**Notes:**
- This vulnerability can cause validator nodes to crash during normal consensus operation when processing competing forks
- The race condition is timing-dependent but realistic under network load
- Impact is amplified when multiple validators process the same forked blocks
- The fix requires replacing the bare assertion with graceful error handling to prevent crashes on stale forks

### Citations

**File:** storage/storage-interface/src/state_store/state_summary.rs (L84-111)
```rust
    fn update(
        &self,
        persisted: &ProvableStateSummary,
        hot_updates: &[HotStateShardUpdates; NUM_STATE_SHARDS],
        updates: &BatchedStateUpdateRefs,
    ) -> Result<Self> {
        let _timer = TIMER.timer_with(&["state_summary__update"]);

        assert_ne!(self.hot_state_summary.root_hash(), *CORRUPTION_SENTINEL);
        assert_ne!(self.global_state_summary.root_hash(), *CORRUPTION_SENTINEL);

        // Persisted must be before or at my version.
        assert!(persisted.next_version() <= self.next_version());
        // Updates must start at exactly my version.
        assert_eq!(updates.first_version(), self.next_version());

        let (hot_smt_result, smt_result) = rayon::join(
            || self.update_hot_state_summary(persisted, hot_updates),
            || self.update_global_state_summary(persisted, updates),
        );

        Ok(Self {
            next_version: updates.next_version(),
            hot_state_summary: hot_smt_result?,
            global_state_summary: smt_result?,
            hot_state_config: self.hot_state_config,
        })
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L874-921)
```rust
    async fn ledger_update(
        rand_check: TaskFuture<RandResult>,
        execute_fut: TaskFuture<ExecuteResult>,
        parent_block_ledger_update_fut: TaskFuture<LedgerUpdateResult>,
        executor: Arc<dyn BlockExecutorTrait>,
        block: Arc<Block>,
    ) -> TaskResult<LedgerUpdateResult> {
        let mut tracker = Tracker::start_waiting("ledger_update", &block);
        let (_, _, prev_epoch_end_timestamp) = parent_block_ledger_update_fut.await?;
        let execution_time = execute_fut.await?;

        tracker.start_working();
        let block_clone = block.clone();
        let result = tokio::task::spawn_blocking(move || {
            executor
                .ledger_update(block_clone.id(), block_clone.parent_id())
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        let timestamp = block.timestamp_usecs();
        observe_block(timestamp, BlockStage::EXECUTED);
        let epoch_end_timestamp =
            if result.has_reconfiguration() && !result.compute_status_for_input_txns().is_empty() {
                Some(timestamp)
            } else {
                prev_epoch_end_timestamp
            };
        // check for randomness consistency
        let (_, has_randomness) = rand_check.await?;
        if !has_randomness {
            let mut label = "consistent";
            for event in result.execution_output.subscribable_events.get(None) {
                if event.type_tag() == RANDOMNESS_GENERATED_EVENT_MOVE_TYPE_TAG.deref() {
                    error!(
                            "[Pipeline] Block {} {} {} generated randomness event without has_randomness being true!",
                            block.id(),
                            block.epoch(),
                            block.round()
                        );
                    label = "inconsistent";
                    break;
                }
            }
            counters::RAND_BLOCK.with_label_values(&[label]).inc();
        }
        Ok((result, execution_time, epoch_end_timestamp))
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L260-334)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _timer = UPDATE_LEDGER.start_timer();
        info!(
            LogSchema::new(LogEntry::BlockExecutor).block_id(block_id),
            "ledger_update"
        );
        let committed_block_id = self.committed_block_id();
        let mut block_vec = self
            .block_tree
            .get_blocks_opt(&[block_id, parent_block_id])?;
        let parent_block = block_vec
            .pop()
            .expect("Must exist.")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
        // At this point of time two things must happen
        // 1. The block tree must also have the current block id with or without the ledger update output.
        // 2. We must have the ledger update output of the parent block.
        // Above is not ture if the block is on a forked branch.
        let block = block_vec
            .pop()
            .expect("Must exist")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
        parent_block.ensure_has_child(block_id)?;
        let output = &block.output;
        let parent_out = &parent_block.output;

        // TODO(aldenhu): remove, assuming no retries.
        if let Some(complete_result) = block.output.get_complete_result() {
            info!(block_id = block_id, "ledger_update already done.");
            return Ok(complete_result);
        }

        if parent_block_id != committed_block_id && parent_out.has_reconfiguration() {
            info!(block_id = block_id, "ledger_update for reconfig suffix.");

            // Parent must have done all state checkpoint and ledger update since this method
            // is being called.
            output.set_state_checkpoint_output(
                parent_out
                    .ensure_state_checkpoint_output()?
                    .reconfig_suffix(),
            );
            output.set_ledger_update_output(
                parent_out.ensure_ledger_update_output()?.reconfig_suffix(),
            );
        } else {
            THREAD_MANAGER.get_non_exe_cpu_pool().install(|| {
                // TODO(aldenhu): remove? no known strategy to recover from this failure
                fail_point!("executor::block_state_checkpoint", |_| {
                    Err(anyhow::anyhow!("Injected error in block state checkpoint."))
                });
                output.set_state_checkpoint_output(DoStateCheckpoint::run(
                    &output.execution_output,
                    parent_block.output.ensure_result_state_summary()?,
                    &ProvableStateSummary::new_persisted(self.db.reader.as_ref())?,
                    None,
                )?);
                output.set_ledger_update_output(DoLedgerUpdate::run(
                    &output.execution_output,
                    output.ensure_state_checkpoint_output()?,
                    parent_out
                        .ensure_ledger_update_output()?
                        .transaction_accumulator
                        .clone(),
                )?);
                Result::<_>::Ok(())
            })?;
        }

        Ok(block.output.expect_complete_result())
    }
```

**File:** storage/aptosdb/src/state_store/state_merkle_batch_committer.rs (L56-106)
```rust
                CommitMessage::Data(StateMerkleCommit {
                    snapshot,
                    hot_batch,
                    cold_batch,
                }) => {
                    let base_version = self.persisted_state.get_state_summary().version();
                    let current_version = snapshot
                        .version()
                        .expect("Current version should not be None");

                    // commit jellyfish merkle nodes
                    let _timer =
                        OTHER_TIMERS_SECONDS.timer_with(&["commit_jellyfish_merkle_nodes"]);
                    if let Some(hot_state_merkle_batch) = hot_batch {
                        self.commit(
                            self.state_db
                                .hot_state_merkle_db
                                .as_ref()
                                .expect("Hot state merkle db must exist."),
                            current_version,
                            hot_state_merkle_batch,
                        )
                        .expect("Hot state merkle nodes commit failed.");
                    }
                    self.commit(&self.state_db.state_merkle_db, current_version, cold_batch)
                        .expect("State merkle nodes commit failed.");

                    info!(
                        version = current_version,
                        base_version = base_version,
                        root_hash = snapshot.summary().root_hash(),
                        hot_root_hash = snapshot.summary().hot_root_hash(),
                        "State snapshot committed."
                    );
                    LATEST_SNAPSHOT_VERSION.set(current_version as i64);
                    // TODO(HotState): no pruning for hot state right now, since we always reset it
                    // upon restart.
                    self.state_db
                        .state_merkle_pruner
                        .maybe_set_pruner_target_db_version(current_version);
                    self.state_db
                        .epoch_snapshot_pruner
                        .maybe_set_pruner_target_db_version(current_version);

                    self.check_usage_consistency(&snapshot).unwrap();

                    snapshot
                        .summary()
                        .global_state_summary
                        .log_generation("buffered_state_commit");
                    self.persisted_state.set(snapshot);
```
