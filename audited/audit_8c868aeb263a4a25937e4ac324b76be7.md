# Audit Report

## Title
Critical Consensus Safety Violation: BlockExecutor Silently Ignores Root Hash Mismatch After Pre-Commit, Allowing Continued Execution in Diverged State

## Summary
The BlockExecutor's `commit_ledger` validation can fail with a root hash mismatch error after `pre_commit_block` has already written transaction data to the database. Unlike ChunkExecutor which panics in this scenario to force recovery, BlockExecutor returns an error that is silently discarded by the consensus pipeline, allowing the node to continue execution with a state that diverges from consensus agreement. This violates the fundamental deterministic execution invariant and can lead to network splits.

## Finding Description

The vulnerability exists in the interaction between three components:

1. **Pre-commit Phase** - Transaction data is written to database: [1](#0-0) 

The pre_commit_ledger spawns parallel database writes that use `.unwrap()`, meaning data is committed to storage.

2. **Commit Validation** - Root hash is validated against quorum-agreed value: [2](#0-1) 

If the node's computed root hash doesn't match the LedgerInfo's root hash (signed by quorum), this returns an error indicating the node's execution diverged from consensus.

3. **Error Discarded** - The commit_ledger error is silently ignored: [3](#0-2) 

The `let _ =` explicitly discards the result, so consensus never knows the commit failed.

**Critical Contrast**: ChunkExecutor (state sync path) has proper safety handling: [4](#0-3) 

ChunkExecutor panics if any error occurs with pending pre-committed data, forcing a reboot and database truncation. BlockExecutor lacks this protection.

**Exploitation Path:**
1. Node executes block and computes state root X
2. `pre_commit_block` writes all transaction data to database
3. Node receives LedgerInfo with quorum signatures containing root hash Y (where Y â‰  X)
4. `commit_ledger` validation detects mismatch at check_and_put_ledger_info
5. Error propagates through consensus pipeline
6. Error is discarded in wait_for_commit_ledger
7. Node continues consensus with diverged state, building on incorrect root

## Impact Explanation

**Critical Severity - Consensus/Safety Violation**

This vulnerability breaks two fundamental Aptos invariants:

1. **Deterministic Execution**: All validators must produce identical state roots for identical blocks. A node with a diverged root continuing execution violates this guarantee.

2. **Consensus Safety**: AptosBFT must prevent chain splits. If multiple nodes experience root hash mismatches and continue execution, they could form conflicting chains.

**Potential Consequences:**
- **Network Split**: Nodes with diverged states could fork the chain
- **Double-Spend**: Different validators might commit different transaction orderings
- **Permanent Corruption**: Node state permanently diverges, requiring manual intervention or hard fork
- **Loss of Liveness**: Honest validators cannot reach consensus when some nodes are on different state roots

This meets the Critical severity criteria: "Consensus/Safety violations" and potentially "Non-recoverable network partition (requires hardfork)".

## Likelihood Explanation

**Likelihood: Low to Medium**

The vulnerability requires a root hash mismatch to occur, which could happen due to:

1. **Non-deterministic Execution Bugs**: Rare but documented in blockchain systems (e.g., floating point operations, threading issues, hardware differences)
2. **Database Corruption**: Storage layer corruption could cause incorrect state reads
3. **Memory Corruption**: Bit flips or memory errors during computation
4. **Software Bugs**: Implementation errors in state computation logic
5. **Malicious Ledger Info**: If quorum is compromised (outside normal threat model)

While the triggering condition is uncommon in normal operation, the impact when it occurs is catastrophic. The system is designed to detect and recover from such conditions (as evidenced by ChunkExecutor's panic behavior), but BlockExecutor lacks this protection.

The likelihood increases during:
- Protocol upgrades with execution changes
- High transaction load with complex state updates
- Epoch transitions with significant state checkpoints
- Nodes running on degraded hardware

## Recommendation

Implement the same panic-on-error-with-pending-pre-commit logic that ChunkExecutor uses:

```rust
// In execution/executor/src/block_executor/mod.rs
impl<V> BlockExecutorInner<V>
where
    V: VMBlockExecutor,
{
    fn commit_ledger(&self, ledger_info_with_sigs: LedgerInfoWithSignatures) -> ExecutorResult<()> {
        // Track if pre_commit_block has been called for this block
        let has_pending_pre_commit = /* check if pre_commit_block was called */;
        
        let result = self.db
            .writer
            .commit_ledger(target_version, Some(&ledger_info_with_sigs), None);
        
        // If validation fails after pre-commit, panic to force recovery
        if let Err(error) = &result {
            if has_pending_pre_commit {
                panic!(
                    "Hit error with pending pre-committed ledger, panicking. Block: {}, Error: {:?}",
                    ledger_info_with_sigs.ledger_info().consensus_block_id(),
                    error
                );
            }
        }
        
        result
    }
}
```

Additionally, the consensus pipeline should not discard commit_ledger errors:

```rust
// In consensus/consensus-types/src/pipelined_block.rs
pub async fn wait_for_commit_ledger(&self) -> TaskResult<()> {
    if let Some(fut) = self.pipeline_futs() {
        // Propagate error instead of discarding
        fut.commit_ledger_fut.await?;
    }
    Ok(())
}
```

This ensures that critical validation failures trigger node recovery through panic/reboot and database truncation: [5](#0-4) 

## Proof of Concept

```rust
// Reproduction test for execution/executor/src/tests/block_executor_tests.rs
#[test]
#[should_panic(expected = "Root hash pre-committed doesn't match LedgerInfo")]
fn test_block_executor_panic_on_root_hash_mismatch() {
    let db = create_db();
    let executor = BlockExecutor::<MockVM>::new(db);
    
    // Execute and pre-commit block 1
    let block1 = create_block_with_transactions(1..=5);
    executor.execute_and_update_state(block1.clone(), genesis_id, config).unwrap();
    let result1 = executor.ledger_update(block1.id(), genesis_id).unwrap();
    executor.pre_commit_block(block1.id()).unwrap();
    
    // Create LedgerInfo with WRONG root hash (simulating divergence)
    let wrong_root_hash = HashValue::random();
    let wrong_ledger_info = create_ledger_info_with_sig(
        result1.version(),
        wrong_root_hash,  // Different from computed root
        block1.id()
    );
    
    // This should panic but currently returns error that gets ignored
    let result = executor.commit_ledger(wrong_ledger_info);
    
    // In current implementation, this doesn't panic - it returns error
    // that gets discarded in consensus pipeline
    // Expected behavior: Should panic like ChunkExecutor does
    assert!(result.is_err());
}
```

The test demonstrates that while ChunkExecutor has protection (test reference): [6](#0-5) 

BlockExecutor lacks equivalent safety guarantees, allowing consensus to silently proceed with diverged state.

## Notes

This vulnerability represents a critical gap in the BlockExecutor's safety guarantees compared to ChunkExecutor. The database recovery mechanism via `sync_commit_progress` can only help if the node reboots, but the current implementation allows continued execution without triggering recovery. The fix should ensure that any validation failure after pre-commit forces immediate node recovery to maintain consensus safety.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L263-322)
```rust
    fn calculate_and_commit_ledger_and_state_kv(
        &self,
        chunk: &ChunkToCommit,
        skip_index_and_usage: bool,
    ) -> Result<HashValue> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions__work"]);

        let mut new_root_hash = HashValue::zero();
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });

        Ok(new_root_hash)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L556-569)
```rust
        // Verify the root hash.
        let db_root_hash = self
            .ledger_db
            .transaction_accumulator_db()
            .get_root_hash(version)?;
        let li_root_hash = ledger_info_with_sig
            .ledger_info()
            .transaction_accumulator_hash();
        ensure!(
            db_root_hash == li_root_hash,
            "Root hash pre-committed doesn't match LedgerInfo. pre-commited: {:?} vs in LedgerInfo: {:?}",
            db_root_hash,
            li_root_hash,
        );
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L562-568)
```rust
    pub async fn wait_for_commit_ledger(&self) {
        // may be aborted (e.g. by reset)
        if let Some(fut) = self.pipeline_futs() {
            // this may be cancelled
            let _ = fut.commit_ledger_fut.await;
        }
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L96-105)
```rust
        let has_pending_pre_commit = inner.has_pending_pre_commit.load(Ordering::Acquire);
        f(inner).map_err(|error| {
            if has_pending_pre_commit {
                panic!(
                    "Hit error with pending pre-committed ledger, panicking. {:?}",
                    error,
                );
            }
            error
        })
```

**File:** storage/aptosdb/src/state_store/mod.rs (L408-502)
```rust
    // We commit the overall commit progress at the last, and use it as the source of truth of the
    // commit progress.
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```

**File:** execution/executor/src/tests/chunk_executor_tests.rs (L364-379)
```rust
#[test]
#[should_panic(expected = "Hit error with pending pre-committed ledger, panicking.")]
fn test_panic_on_mismatch_with_pre_committed() {
    // See comments on `commit_1_pre_commit_2_return_3()`
    let (db, _chunk3, _ledger_info2, _ledger_info3) = commit_1_pre_commit_2_return_3();

    let (bad_chunks, bad_ledger_info) = create_transaction_chunks(vec![1..=7, 8..=12]);
    // bad chunk has txn 8-12
    let bad_chunk = bad_chunks[1].clone();

    let chunk_executor = ChunkExecutor::<MockVM>::new(db);
    // chunk executor knows there's pre-committed txns in the DB and when a verified chunk
    // doesn't match the pre-committed root hash it panics in hope that pre-committed versions
    // get truncated on reboot
    let _res = chunk_executor.execute_chunk(bad_chunk, &bad_ledger_info, None);
}
```
