# Audit Report

## Title
Blocking Thread Pool Exhaustion via Unbounded Per-Peer Outbound RPC Serialization

## Summary
The network layer uses `tokio::task::spawn_blocking` for RPC message serialization with a global limit of 64 blocking threads, but enforces only per-peer limits (100 concurrent outbound RPCs per peer) without a global limit across all peers. With up to 100 inbound peer connections, an attacker can trigger up to 10,000 concurrent serialization tasks competing for 64 threads, exhausting the blocking thread pool and causing node-wide performance degradation affecting all critical operations including message deserialization and consensus participation.

## Finding Description

The vulnerability exists in the architectural mismatch between per-peer RPC concurrency limits and the global blocking thread pool capacity: [1](#0-0) 

The `send_rpc()` function uses `spawn_blocking` to offload CPU-intensive serialization to a blocking thread pool. However, the blocking thread pool has a global limit: [2](#0-1) 

Each peer connection has its own `OutboundRpcs` instance with a per-peer limit: [3](#0-2) 

The system allows up to 100 inbound connections: [4](#0-3) 

When creating each peer, the per-peer limit is applied independently: [5](#0-4) 

Serialization for compressed protocols is CPU-intensive: [6](#0-5) 

**Attack Path:**
1. Attacker connects as multiple malicious peers (or controls nodes that the victim connects to) - up to 100 inbound connections
2. Attacker sends requests that trigger the victim node to send outbound RPCs (e.g., state sync requests, consensus messages, storage service queries)
3. Each peer can trigger up to 100 concurrent outbound RPCs, totaling 10,000 potential concurrent serialization tasks
4. With only 64 blocking threads available, serialization tasks queue indefinitely when the pool is saturated
5. Since the blocking thread pool is shared globally, ALL `spawn_blocking` operations are affected, including:
   - Inbound message deserialization [7](#0-6) 
   - Response deserialization [8](#0-7) 
6. The node becomes slow/unresponsive, unable to process messages, participate in consensus, or serve requests

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program ("Validator node slowdowns"). The impact includes:

- **Validator Degradation**: Validators become slow to process consensus messages, potentially causing them to miss voting rounds or fall behind
- **Network-Wide Disruption**: If multiple validators are targeted simultaneously, consensus liveness could be affected
- **Cascading Failures**: The blocking affects all message processing (inbound/outbound), creating a complete operational slowdown
- **No Resource Limits Enforcement**: Violates the critical invariant "Resource Limits: All operations must respect gas, storage, and computational limits"

While not causing permanent damage or fund loss, this creates severe operational impact on network availability and validator performance, which is explicitly categorized as HIGH severity.

## Likelihood Explanation

**Likelihood: HIGH**

The attack is highly feasible because:

1. **Low Attacker Requirements**: Any network peer can connect (no privileged access needed)
2. **Easy to Trigger**: Many legitimate protocol operations trigger outbound RPCs (state sync, consensus, storage service)
3. **No Authentication Barrier**: Inbound connections are accepted up to the limit
4. **Scalable Attack**: Attacker can run multiple nodes or compromise existing nodes to multiply the effect
5. **Deterministic Exploitation**: The math is clear - 10,000 potential tasks vs 64 threads guarantees exhaustion
6. **No Rate Limiting**: Beyond per-peer limits, there's no global throttling on serialization workload

## Recommendation

Implement a **global limit on total concurrent outbound RPCs across all peers** and **global limit on spawn_blocking serialization tasks**:

1. Add a global semaphore limiting total outbound RPC serialization tasks across all peers (e.g., 256 total)
2. When the global limit is reached, apply backpressure by failing new RPC requests with `RpcError::TooManyPending`
3. Consider using a dedicated thread pool for RPC serialization separate from other blocking tasks
4. Add monitoring metrics for blocking thread pool utilization

**Code Fix** (conceptual):
- Add a global `Arc<Semaphore>` in the network layer initialization
- Before calling `spawn_blocking` in `send_rpc()`, acquire a permit from the global semaphore
- Return `RpcError::TooManyPending` if the permit cannot be acquired
- Release the permit after serialization completes

Alternatively, use a bounded channel or async serialization with backpressure to prevent unbounded queueing on the blocking thread pool.

## Proof of Concept

```rust
// Reproduction test (conceptual - to be added to network/framework/src/peer/test.rs)
#[tokio::test]
async fn test_blocking_thread_pool_exhaustion() {
    // Setup: Create 100 mock peers connected to victim node
    let mut peers = Vec::new();
    for i in 0..100 {
        let peer = create_mock_peer(i);
        peers.push(peer);
    }
    
    // Attack: Each peer triggers 100 concurrent outbound RPCs
    let mut handles = Vec::new();
    for peer in peers {
        for j in 0..100 {
            let handle = tokio::spawn(async move {
                // Create a large message with compressed protocol
                let large_msg = create_large_consensus_message();
                // Trigger outbound RPC - this will call spawn_blocking
                peer.send_rpc(ProtocolId::ConsensusRpcCompressed, large_msg, Duration::from_secs(30)).await
            });
            handles.push(handle);
        }
    }
    
    // Expected: First 64 RPCs serialize normally
    // Next 9,936 RPCs queue on the blocking thread pool
    // All spawn_blocking operations (including deserialization) are blocked
    // Node becomes unresponsive
    
    // Measure: Track time for critical operations
    let start = Instant::now();
    let _ = deserialize_inbound_message().await; // Should be slow/timeout
    let elapsed = start.elapsed();
    
    assert!(elapsed > Duration::from_secs(10), "Node should be degraded");
}
```

**Notes:**
- The vulnerability stems from architectural oversight: per-peer limits without global aggregation
- The blocking thread pool is shared across ALL runtime operations, amplifying the impact
- Compressed protocol serialization (BCS compression) is CPU-intensive, making the attack more effective
- The per-peer limit of 100 was likely chosen for reasonable operation, but 100 peers Ã— 100 RPCs = 10,000 tasks far exceeds the 64-thread capacity
- This affects both validators and fullnodes, though validators have more restricted peer sets which slightly reduces likelihood in practice

### Citations

**File:** network/framework/src/protocols/network/mod.rs (L217-219)
```rust
        let data_event_stream = peer_mgr_notifs_rx.map(|notification| {
            tokio::task::spawn_blocking(move || received_message_to_event(notification))
        });
```

**File:** network/framework/src/protocols/network/mod.rs (L435-450)
```rust
    pub async fn send_rpc(
        &self,
        recipient: PeerId,
        protocol: ProtocolId,
        req_msg: TMessage,
        timeout: Duration,
    ) -> Result<TMessage, RpcError> {
        // Serialize the request using a blocking task
        let req_data = tokio::task::spawn_blocking(move || protocol.to_bytes(&req_msg))
            .await??
            .into();

        // Send the request and wait for the response
        self.send_rpc_raw(recipient, protocol, req_data, timeout)
            .await
    }
```

**File:** network/framework/src/protocols/network/mod.rs (L468-470)
```rust
        // Deserialize the response using a blocking task
        let res_msg = tokio::task::spawn_blocking(move || protocol.from_bytes(&res_data)).await??;
        Ok(res_msg)
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** network/framework/src/constants.rs (L12-15)
```rust
/// Limit on concurrent Outbound RPC requests before backpressure is applied
pub const MAX_CONCURRENT_OUTBOUND_RPCS: u32 = 100;
/// Limit on concurrent Inbound RPC requests before backpressure is applied
pub const MAX_CONCURRENT_INBOUND_RPCS: u32 = 100;
```

**File:** config/src/config/network_config.rs (L44-44)
```rust
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
```

**File:** network/framework/src/peer_manager/mod.rs (L673-675)
```rust
            Duration::from_millis(constants::INBOUND_RPC_TIMEOUT_MS),
            constants::MAX_CONCURRENT_INBOUND_RPCS,
            constants::MAX_CONCURRENT_OUTBOUND_RPCS,
```

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L196-214)
```rust
    pub fn to_bytes<T: Serialize>(&self, value: &T) -> anyhow::Result<Vec<u8>> {
        // Start the serialization timer
        let serialization_timer = start_serialization_timer(*self, SERIALIZATION_LABEL);

        // Serialize the message
        let result = match self.encoding() {
            Encoding::Bcs(limit) => self.bcs_encode(value, limit),
            Encoding::CompressedBcs(limit) => {
                let compression_client = self.get_compression_client();
                let bcs_bytes = self.bcs_encode(value, limit)?;
                aptos_compression::compress(
                    bcs_bytes,
                    compression_client,
                    MAX_APPLICATION_MESSAGE_SIZE,
                )
                .map_err(|e| anyhow!("{:?}", e))
            },
            Encoding::Json => serde_json::to_vec(value).map_err(|e| anyhow!("{:?}", e)),
        };
```
