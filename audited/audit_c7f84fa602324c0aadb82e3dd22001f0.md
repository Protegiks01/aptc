# Audit Report

## Title
Silent Message Drop in Network Channel Layer Causes Consensus to Assume Successful Delivery

## Summary
The `aptos_channel::Sender::push()` method returns `Ok(())` even when messages are silently dropped due to queue capacity limits. This breaks error propagation throughout the network stack, causing consensus and other critical components to incorrectly assume message delivery success when votes, proposals, or other critical messages are actually dropped.

## Finding Description

The vulnerability exists in the error propagation chain from the channel layer up to consensus: [1](#0-0) 

The `Sender::push()` method calls `push_with_feedback()` with `None` for the status channel. When the per-key queue is full, `PerKeyQueue::push()` drops a message: [2](#0-1) 

When `queue_style` is `FIFO` (the default for consensus), and the queue reaches capacity (`max_queue_size`), the newest message is dropped and returned as `Some(message)`. However, in `push_with_feedback()`, this dropped message is only reported if a status channel was provided. Since `push()` passes `None`, the condition at line 104 fails, and the function returns `Ok(())` despite the message being dropped.

This `Ok(())` propagates through the entire network stack: [3](#0-2) [4](#0-3) [5](#0-4) 

Finally, consensus code attempts to handle errors but never receives them: [6](#0-5) 

The error check at line 426 never triggers because the underlying channel returns `Ok(())` for dropped messages. The comments at lines 515-519 acknowledge that delivery is not guaranteed, but there's a critical difference between "message queued but might fail network delivery" and "message silently dropped from queue before even attempting delivery."

**Attack Scenario:**

1. Under high network load or consensus activity, per-peer message queues fill up (default: 1024 messages per peer)
2. When a validator tries to send a vote or proposal to a peer with a full queue, the message is silently dropped
3. The sender receives `Ok(())` and assumes the message was sent
4. The receiver never gets the vote/proposal
5. Consensus waits indefinitely for messages that will never arrive, causing liveness degradation
6. If drops are asymmetric across validators, it can cause inconsistent views of consensus state

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per Aptos bug bounty criteria:

- **"State inconsistencies requiring intervention"** - Asymmetric message drops can cause validators to have different views of which votes/proposals were sent, requiring manual investigation and potential intervention to restore consensus progress

The impact includes:

1. **Consensus Liveness Issues**: Validators may wait for votes that were silently dropped, stalling consensus rounds
2. **Silent Failures**: No visibility into dropped messages makes debugging difficult
3. **Broken Error Handling**: All error handling code in consensus that checks for send failures is bypassed
4. **Broad Scope**: Affects all network-based components (consensus, state sync, mempool, DKG, JWK consensus)

While this doesn't directly violate consensus safety (Byzantine fault tolerance still holds), it degrades liveness, which is critical for blockchain operation. The queue configuration confirms the risk: [7](#0-6) [8](#0-7) 

## Likelihood Explanation

**Moderate to High Likelihood:**

- Queue capacity is finite (1024 messages per peer by default)
- Occurs naturally under high load without requiring attacker intervention
- Can be triggered by:
  - Network congestion or validator performance issues
  - Burst traffic during epoch transitions
  - DDoS-like conditions on individual validators
- No special privileges required to trigger
- Affects production systems under stress conditions

The FIFO queue style means the newest (potentially most critical) messages are dropped when queues fill.

## Recommendation

**Fix Option 1: Propagate Queue Full Errors**

Modify `Sender::push()` to detect and return an error when messages are dropped:

```rust
pub fn push(&self, key: K, message: M) -> Result<()> {
    let mut shared_state = self.shared_state.lock();
    ensure!(!shared_state.receiver_dropped, "Channel is closed");
    
    let dropped = shared_state.internal_queue.push(key, (message, None));
    
    // Return error if message was dropped due to full queue
    if dropped.is_some() {
        return Err(anyhow::anyhow!("Message queue full for key, message dropped"));
    }
    
    if let Some(w) = shared_state.waker.take() {
        w.wake();
    }
    Ok(())
}
```

**Fix Option 2: Use push_with_feedback() Everywhere**

Modify network senders to use `push_with_feedback()` with status channels to detect drops:

```rust
pub fn send_to(
    &self,
    peer_id: PeerId,
    protocol_id: ProtocolId,
    mdata: Bytes,
) -> Result<(), PeerManagerError> {
    let (status_tx, status_rx) = oneshot::channel();
    self.inner.push(
        (peer_id, protocol_id),
        PeerManagerRequest::SendDirectSend(peer_id, Message { protocol_id, mdata }),
        Some(status_tx)
    )?;
    
    // Check if message was dropped
    if let Ok(ElementStatus::Dropped(_)) = status_rx.try_recv() {
        return Err(PeerManagerError::QueueFull);
    }
    Ok(())
}
```

**Recommended approach:** Implement Fix Option 1 as it's simpler and catches the issue at the root cause.

## Proof of Concept

```rust
#[test]
fn test_silent_message_drop_returns_ok() {
    use crate::aptos_channel;
    use crate::message_queues::QueueStyle;
    
    // Create channel with capacity of 2 messages per key
    let (sender, _receiver) = aptos_channel::new(QueueStyle::FIFO, 2, None);
    
    // Fill the queue for key "validator1"
    assert!(sender.push("validator1", "vote1").is_ok());
    assert!(sender.push("validator1", "vote2").is_ok());
    
    // This message gets silently dropped but returns Ok()!
    // In FIFO mode, the newest message is dropped
    let result = sender.push("validator1", "vote3");
    
    // BUG: This returns Ok(()) even though "vote3" was dropped
    assert!(result.is_ok()); // This passes, demonstrating the bug
    
    // The caller has no way to know that vote3 was never queued
    // In consensus, this would mean a validator thinks it sent a vote
    // but the vote was actually silently dropped
}

#[test]
fn test_consensus_vote_silent_drop_scenario() {
    use crate::aptos_channel;
    use crate::message_queues::QueueStyle;
    use futures::stream::StreamExt;
    
    // Simulate consensus network channel (1024 capacity per peer)
    let (sender, mut receiver) = aptos_channel::new(QueueStyle::FIFO, 1024, None);
    
    let peer_id = "validator_node_1";
    
    // Simulate high load: fill the queue with 1024 messages
    for i in 0..1024 {
        assert!(sender.push(peer_id, format!("proposal_{}", i)).is_ok());
    }
    
    // Now try to send a critical vote - it will be silently dropped
    let critical_vote_result = sender.push(peer_id, "CRITICAL_VOTE_FOR_BLOCK_100");
    
    // BUG: Consensus thinks the vote was sent successfully
    assert!(critical_vote_result.is_ok());
    
    // But if we consume all messages, the vote is not there
    let mut received_messages = Vec::new();
    for _ in 0..1024 {
        if let Some(msg) = receiver.select_next_some().now_or_never() {
            received_messages.push(msg);
        }
    }
    
    // The critical vote was never queued!
    assert!(!received_messages.contains(&"CRITICAL_VOTE_FOR_BLOCK_100".to_string()));
    assert_eq!(received_messages.len(), 1024); // Only the first 1024 messages
    
    // This demonstrates how consensus can think it sent a vote
    // but the vote never reaches the receiver, causing liveness issues
}
```

## Notes

This vulnerability affects the fundamental reliability assumption of the network layer. While the consensus comments acknowledge that messages might not be delivered over the network, there's no acknowledgment that messages can be silently dropped before even entering the network queue. This breaks the implicit contract that `Result<(), Error>` accurately represents whether the operation succeeded, and specifically breaks the error handling code in consensus that attempts to log failed sends.

### Citations

**File:** crates/channel/src/aptos_channel.rs (L85-112)
```rust
    pub fn push(&self, key: K, message: M) -> Result<()> {
        self.push_with_feedback(key, message, None)
    }

    /// Same as `push`, but this function also accepts a oneshot::Sender over which the sender can
    /// be notified when the message eventually gets delivered or dropped.
    pub fn push_with_feedback(
        &self,
        key: K,
        message: M,
        status_ch: Option<oneshot::Sender<ElementStatus<M>>>,
    ) -> Result<()> {
        let mut shared_state = self.shared_state.lock();
        ensure!(!shared_state.receiver_dropped, "Channel is closed");
        debug_assert!(shared_state.num_senders > 0);

        let dropped = shared_state.internal_queue.push(key, (message, status_ch));
        // If this or an existing message had to be dropped because of the queue being full, we
        // notify the corresponding status channel if it was registered.
        if let Some((dropped_val, Some(dropped_status_ch))) = dropped {
            // Ignore errors.
            let _err = dropped_status_ch.send(ElementStatus::Dropped(dropped_val));
        }
        if let Some(w) = shared_state.waker.take() {
            w.wake();
        }
        Ok(())
    }
```

**File:** crates/channel/src/message_queues.rs (L112-152)
```rust
    pub(crate) fn push(&mut self, key: K, message: T) -> Option<T> {
        if let Some(c) = self.counters.as_ref() {
            c.with_label_values(&["enqueued"]).inc();
        }

        let key_message_queue = self
            .per_key_queue
            .entry(key.clone())
            // Only allocate a small initial queue for a new key. Previously, we
            // allocated a queue with all `max_queue_size_per_key` entries;
            // however, this breaks down when we have lots of transient peers.
            // For example, many of our queues have a max capacity of 1024. To
            // handle a single rpc from a transient peer, we would end up
            // allocating ~ 96 b * 1024 ~ 64 Kib per queue.
            .or_insert_with(|| VecDeque::with_capacity(1));

        // Add the key to our round-robin queue if it's not already there
        if key_message_queue.is_empty() {
            self.round_robin_queue.push_back(key);
        }

        // Push the message to the actual key message queue
        if key_message_queue.len() >= self.max_queue_size.get() {
            if let Some(c) = self.counters.as_ref() {
                c.with_label_values(&["dropped"]).inc();
            }
            match self.queue_style {
                // Drop the newest message for FIFO
                QueueStyle::FIFO => Some(message),
                // Drop the oldest message for LIFO
                QueueStyle::LIFO | QueueStyle::KLAST => {
                    let oldest = key_message_queue.pop_front();
                    key_message_queue.push_back(message);
                    oldest
                },
            }
        } else {
            key_message_queue.push_back(message);
            None
        }
    }
```

**File:** network/framework/src/peer_manager/senders.rs (L44-55)
```rust
    pub fn send_to(
        &self,
        peer_id: PeerId,
        protocol_id: ProtocolId,
        mdata: Bytes,
    ) -> Result<(), PeerManagerError> {
        self.inner.push(
            (peer_id, protocol_id),
            PeerManagerRequest::SendDirectSend(peer_id, Message { protocol_id, mdata }),
        )?;
        Ok(())
    }
```

**File:** network/framework/src/protocols/network/mod.rs (L406-415)
```rust
    pub fn send_to_raw(
        &self,
        recipient: PeerId,
        protocol: ProtocolId,
        message: Bytes,
    ) -> Result<(), NetworkError> {
        self.peer_mgr_reqs_tx
            .send_to(recipient, protocol, message)?;
        Ok(())
    }
```

**File:** network/framework/src/application/interface.rs (L229-234)
```rust
    fn send_to_peer(&self, message: Message, peer: PeerNetworkId) -> Result<(), Error> {
        let network_sender = self.get_sender_for_network_id(&peer.network_id())?;
        let direct_send_protocol_id = self
            .get_preferred_protocol_for_peer(&peer, &self.direct_send_protocols_and_preferences)?;
        Ok(network_sender.send_to(peer.peer_id(), direct_send_protocol_id, message)?)
    }
```

**File:** consensus/src/network.rs (L411-433)
```rust
    async fn send(&self, msg: ConsensusMsg, recipients: Vec<Author>) {
        fail_point!("consensus::send::any", |_| ());
        let network_sender = self.consensus_network_client.clone();
        let mut self_sender = self.self_sender.clone();
        for peer in recipients {
            if self.author == peer {
                let self_msg = Event::Message(self.author, msg.clone());
                if let Err(err) = self_sender.send(self_msg).await {
                    warn!(error = ?err, "Error delivering a self msg");
                }
                continue;
            }
            counters::CONSENSUS_SENT_MSGS
                .with_label_values(&[msg.name()])
                .inc();
            if let Err(e) = network_sender.send_to(peer, msg.clone()) {
                warn!(
                    remote_peer = peer,
                    error = ?e, "Failed to send a msg {:?} to peer", msg
                );
            }
        }
    }
```

**File:** aptos-node/src/network.rs (L57-72)
```rust
pub fn consensus_network_configuration(node_config: &NodeConfig) -> NetworkApplicationConfig {
    let direct_send_protocols: Vec<ProtocolId> =
        aptos_consensus::network_interface::DIRECT_SEND.into();
    let rpc_protocols: Vec<ProtocolId> = aptos_consensus::network_interface::RPC.into();

    let network_client_config =
        NetworkClientConfig::new(direct_send_protocols.clone(), rpc_protocols.clone());
    let network_service_config = NetworkServiceConfig::new(
        direct_send_protocols,
        rpc_protocols,
        aptos_channel::Config::new(node_config.consensus.max_network_channel_size)
            .queue_style(QueueStyle::FIFO)
            .counters(&aptos_consensus::counters::PENDING_CONSENSUS_NETWORK_EVENTS),
    );
    NetworkApplicationConfig::new(network_client_config, network_service_config)
}
```

**File:** config/src/config/consensus_config.rs (L220-223)
```rust
impl Default for ConsensusConfig {
    fn default() -> ConsensusConfig {
        ConsensusConfig {
            max_network_channel_size: 1024,
```
