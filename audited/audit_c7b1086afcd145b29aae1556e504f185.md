# Audit Report

## Title
Ledger Pruner Non-Atomic Operations Leading to Validator Node Restart Denial of Service

## Summary
The `LedgerPruner::prune()` function executes `ledger_metadata_pruner` before sub-pruners without atomic transaction guarantees. If metadata pruning succeeds but sub-pruner operations fail, the database enters an inconsistent state. On node restart, the initialization process attempts to reconcile this inconsistency by forcing sub-pruners to "catch up" to the metadata progress, which panics if the underlying failure persists, preventing validator node restart. [1](#0-0) 

## Finding Description
The `prune()` function violates **State Consistency** invariant #4: "State transitions must be atomic and verifiable via Merkle proofs." 

The pruning sequence is:
1. `ledger_metadata_pruner.prune()` executes and commits immediately to its database, deleting `VersionData` entries and updating `LedgerPrunerProgress` [2](#0-1) 

2. Sub-pruners execute in parallel. Each commits to separate databases independently [3](#0-2) 

3. If any sub-pruner fails, the error propagates but ledger_metadata_pruner changes are already committed

**Attack Scenario:**
- Environmental condition causes sub-pruner failure (disk I/O error, corruption, resource exhaustion on specific database instance)
- Node operator restarts node (routine maintenance, crash recovery, upgrade)
- During initialization, `LedgerPruner::new()` reads `metadata_progress` and forces each sub-pruner to catch up by calling `prune(old_progress, metadata_progress)` [4](#0-3) 

- If the underlying failure persists, sub-pruner initialization fails
- The failure propagates through `.expect("Failed to create ledger pruner.")` causing panic [5](#0-4) 

- Node cannot complete initialization, validator becomes permanently unavailable

While queries during normal operation are protected by `min_readable_version` checks that prevent accessing the inconsistent state, there is one unchecked code path: [6](#0-5) [7](#0-6) 

This calls `get_usage()` without pruning checks and uses `.unwrap()`, which could panic if VersionData was deleted.

## Impact Explanation
**High Severity** per Aptos Bug Bounty criteria:
- "Validator node slowdowns" - exceeds this, causing complete unavailability
- "API crashes" - node panics during initialization

A validator experiencing this issue becomes permanently unavailable until manual database repair or intervention. If multiple validators experience similar failures simultaneously (e.g., due to common storage infrastructure issues), network consensus liveness could be impacted.

## Likelihood Explanation
**Medium-High Likelihood:**
- Hardware failures in production validator infrastructure are not uncommon
- Disk I/O errors, space exhaustion, or corruption can affect specific RocksDB instances
- Node restarts are routine (software upgrades, configuration changes, crash recovery)
- Once the inconsistent state occurs, every restart attempt will fail
- No automated recovery mechanism exists

The vulnerability is not directly attacker-exploitable through network transactions, but occurs naturally in production environments under adverse conditions.

## Recommendation
Implement atomic transaction guarantees across all pruner operations:

```rust
fn prune(&self, max_versions: usize) -> Result<Version> {
    let mut progress = self.progress();
    let target_version = self.target_version();

    while progress < target_version {
        let current_batch_target_version =
            min(progress + max_versions as Version, target_version);

        // Collect all batches before committing any
        let mut all_batches = Vec::new();
        
        // Prepare metadata pruner batch
        let metadata_batch = self.ledger_metadata_pruner
            .prepare_prune_batch(progress, current_batch_target_version)?;
        all_batches.push(metadata_batch);
        
        // Prepare sub-pruner batches
        for sub_pruner in &self.sub_pruners {
            let batch = sub_pruner.prepare_prune_batch(progress, current_batch_target_version)?;
            all_batches.push(batch);
        }
        
        // Commit all batches atomically or rollback on failure
        self.commit_all_batches_atomically(all_batches)?;
        
        progress = current_batch_target_version;
        self.record_progress(progress);
    }

    Ok(target_version)
}
```

Alternative: Add error recovery logic in sub-pruner initialization to handle partial pruning gracefully instead of panicking.

## Proof of Concept
```rust
// Simulated test scenario (requires mocking RocksDB failures)
#[test]
fn test_partial_pruning_failure_prevents_restart() {
    // 1. Setup: Initialize LedgerPruner with normal databases
    let ledger_db = create_test_ledger_db();
    let pruner = LedgerPruner::new(ledger_db, None).unwrap();
    
    // 2. Trigger partial pruning failure
    // Mock: Make ledger_metadata_pruner succeed but transaction_info_pruner fail
    mock_database_io_error_for_transaction_info_db();
    
    // 3. Attempt pruning - metadata succeeds, sub-pruner fails
    pruner.set_target_version(100);
    let result = pruner.prune(50);
    assert!(result.is_err()); // Pruning fails
    
    // 4. Verify inconsistent state:
    // - metadata_progress = 100 (in DB)
    // - transaction_info_progress = 0 (in DB)
    // - pruner.progress = 0 (atomic)
    
    // 5. Simulate node restart - try to create new LedgerPruner
    let result = LedgerPruner::new(ledger_db, None);
    
    // Expected: Panic during TransactionInfoPruner::new() 
    // when it tries prune(0, 100) and fails again
    assert!(result.is_err()); // Would panic in real code due to .expect()
}
```

---

**Notes:**

While this represents a valid robustness issue in the codebase that violates the State Consistency invariant and can cause validator unavailability in production, it does **not** meet the strict "exploitable by unprivileged attacker" criterion from the validation checklist. The issue occurs due to environmental failures (hardware, OS-level) rather than through attacker-controlled inputs or transactions.

The specific question asked about "dangling references" - the analysis confirms there are no traditional dangling pointer references between data structures, as `VersionData` and transaction data are independent. However, there is a database consistency violation and a potential node restart DoS under failure conditions.

Given the strict validation requirements emphasizing attacker exploitability, this finding is **borderline** and may not qualify for the bug bounty program despite representing a real production risk.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L75-84)
```rust
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_metadata_pruner.rs (L42-56)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();
        for version in current_progress..target_version {
            batch.delete::<VersionDataSchema>(&version)?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::LedgerPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_metadata_db.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_info_pruner.rs (L41-54)
```rust
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_info_db_raw(),
            &DbMetadataKey::TransactionInfoPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionInfoPruner { ledger_db };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionInfoPruner."
        );
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L146-149)
```rust
        let pruner = Arc::new(
            LedgerPruner::new(ledger_db, internal_indexer_db)
                .expect("Failed to create ledger pruner."),
        );
```

**File:** storage/aptosdb/src/state_store/state_merkle_batch_committer.rs (L100-100)
```rust
                    self.check_usage_consistency(&snapshot).unwrap();
```

**File:** storage/aptosdb/src/state_store/state_merkle_batch_committer.rs (L141-141)
```rust
        let usage_from_ledger_db = self.state_db.ledger_db.metadata_db().get_usage(version)?;
```
