# Audit Report

## Title
Validator Consensus Participation with Corrupted Blocks Due to Missing Validation During Recovery

## Summary
When a validator restarts and recovers state from ConsensusDB, blocks loaded from persistent storage are not validated for signature correctness, timestamp consistency, or data integrity. If `find_root()` succeeds but blocks contain corrupted data, the validator starts in normal mode with invalid blocks and participates in consensus, potentially causing safety violations and chain splits.

## Finding Description

During consensus recovery, the system loads blocks from ConsensusDB and attempts to reconstruct the consensus state. The recovery flow executes as follows: [1](#0-0) 

The `start()` method retrieves blocks from the database and calls `RecoveryData::new()`. If this succeeds, it returns `FullRecoveryData`, causing the validator to start normally. If it fails, it falls back to `PartialRecoveryData` and enters recovery mode. [2](#0-1) 

`RecoveryData::new()` calls `find_root()` to locate the root block: [3](#0-2) 

The `find_root()` methods only verify that:
1. A block with the root ID exists in the blocks vector
2. A QC exists for that block  
3. Parent relationships form a valid tree structure [4](#0-3) 

**Critical Gap**: `find_root()` performs NO validation of block content integrity, signatures, timestamps, or epochs. It only checks block IDs match expected values.

When blocks are deserialized from the database, the custom `Deserialize` implementation only computes the block ID by hashing block_data: [5](#0-4) 

The blocks are then inserted into BlockStore without validation: [6](#0-5) 

Neither `insert_block()` nor `insert_committed_block()` validates signatures or block data integrity: [7](#0-6) 

In contrast, blocks received from the network ARE validated: [8](#0-7) [9](#0-8) 

**The Vulnerability**: If ConsensusDB becomes corrupted (disk corruption, malicious file modification, or software bug) such that blocks have invalid signatures, incorrect timestamps, wrong epochs, or corrupted payload BUT valid IDs and parent hashes, then:

1. `find_root()` succeeds because IDs match
2. `RecoveryData::new()` succeeds and returns `FullRecoveryData`
3. Validator starts in normal mode (not recovery mode)  
4. Corrupted blocks are inserted into BlockStore without validation
5. Validator participates in consensus with corrupted state

This breaks the **Consensus Safety** invariant because different validators with different corrupted states will diverge, and breaks **Deterministic Execution** because blocks don't match their cryptographic commitments.

## Impact Explanation

This is **Critical Severity** under the Aptos Bug Bounty program for the following reasons:

**Consensus Safety Violations**: If different validators recover with different corrupted block data, they will have inconsistent views of the chain state. This can cause validators to vote on or propose conflicting blocks, violating the fundamental safety guarantee of AptosBFT consensus.

**Chain Splits**: Different validators operating with different corrupted data will sign conflicting statements, potentially leading to chain splits that cannot be resolved without manual intervention or a hard fork.

**Byzantine Behavior by Honest Nodes**: A validator with corrupted blocks may inadvertently violate safety rules by signing blocks with invalid timestamps or epochs, effectively becoming Byzantine despite being honest.

**Non-recoverable Network Partition**: If a significant portion of validators recover with corrupted data, the network may partition in ways that require hard fork intervention, meeting the Critical severity criteria of "Non-recoverable network partition (requires hardfork)".

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can occur through:

1. **Disk Corruption**: Hardware failures, power loss, or filesystem bugs can corrupt ConsensusDB files while preserving structural integrity (valid block IDs and parent hashes) but corrupting data fields.

2. **Software Bugs**: Bugs in the consensus DB write path, serialization logic, or RocksDB could write malformed data that passes structural checks but has invalid content.

3. **Malicious File Modification**: An attacker with filesystem access (compromised validator node, insider threat) could modify ConsensusDB files to inject corrupted blocks while maintaining valid ID structure.

4. **Crash During Write**: If the validator crashes during block persistence, partially written data may have valid IDs but corrupted fields.

The attack requires no special privileges beyond causing database corruption, which can happen through various failure modes in production systems. The lack of validation during recovery means this vulnerability is always present.

## Recommendation

Add comprehensive validation of blocks loaded from persistent storage during recovery. Specifically:

1. **Validate all blocks during recovery**: Before constructing `RecoveryData`, validate each block's signature and well-formedness:

```rust
// In RecoveryData::new() after loading blocks, before calling find_root()
for block in &blocks {
    if !block.is_genesis_block() {
        block.validate_signature(&epoch_state.verifier)
            .context(format!("Invalid signature for recovered block {}", block.id()))?;
        block.verify_well_formed()
            .context(format!("Block {} is not well-formed", block.id()))?;
    }
}
```

2. **Validate QC-block consistency**: Ensure each QC's certified block info matches the actual block data:

```rust
for qc in &quorum_certs {
    if let Some(block) = blocks.iter().find(|b| b.id() == qc.certified_block().id()) {
        ensure!(
            block.block_info().match_ordered_only(qc.certified_block()),
            "Recovered QC for block {} has mismatched block info",
            qc.certified_block().id()
        );
    }
}
```

3. **Add integrity checksums**: Store cryptographic checksums of blocks in ConsensusDB and verify them during reads.

4. **Fall back to recovery mode on validation failure**: If ANY block fails validation, return `PartialRecoveryData` to trigger recovery manager sync from peers instead of starting with corrupted state.

## Proof of Concept

The following outlines steps to reproduce this vulnerability:

```rust
// Pseudo-code for reproducing the vulnerability

// 1. Setup: Start a validator node and let it commit some blocks
let mut validator = setup_validator_node();
validator.commit_blocks(10); // Commit 10 blocks
validator.shutdown();

// 2. Corrupt ConsensusDB: Modify a block's timestamp while preserving its ID
let db_path = validator.consensus_db_path();
let mut db = ConsensusDB::new(db_path);
let blocks = db.get_all::<BlockSchema>().unwrap();
let mut block_to_corrupt = blocks[5].clone();

// Corrupt the timestamp but keep the same parent_id and other ID-determining fields
let original_id = block_to_corrupt.id();
// Modify timestamp in the block_data (requires access to internals)
// The block ID is computed from block_data hash, so we need to ensure
// the serialized form maintains the ID but has corrupted timestamp

// 3. Restart validator - it will load corrupted block without validation
validator.restart();

// 4. Observe: Validator participates in consensus with corrupted timestamp
// This can cause it to propose or vote on blocks with invalid timestamps,
// violating consensus safety

// Expected: Validator should detect corruption and enter recovery mode
// Actual: Validator starts normally and participates with corrupted data
```

A complete PoC would require:
1. Creating a test validator node with ConsensusDB
2. Persisting several blocks with valid signatures
3. Modifying one block's timestamp in the database (this requires careful modification to corrupt data while preserving serialization structure)
4. Restarting the node and observing it starts in FullRecoveryData mode instead of PartialRecoveryData
5. Demonstrating the validator signs messages based on corrupted block state

## Notes

**Additional Context**:

- The vulnerability affects only the recovery path; blocks received from the network during normal operation are properly validated via `ProposalMsg::verify()`.

- The issue is particularly dangerous because `find_root()` succeeding gives false confidence that the recovered state is valid, when in fact only the structural properties (IDs, parent relationships) have been verified.

- Even if only one validator has corrupted data, that validator becoming Byzantine due to corruption can contribute to consensus failures if combined with other Byzantine nodes or failures.

- The proper mitigation is defense-in-depth: validate ALL data loaded from persistent storage, not just data received from potentially malicious peers over the network.

### Citations

**File:** consensus/src/persistent_liveness_storage.rs (L102-201)
```rust
    pub fn find_root_with_window(
        &self,
        blocks: &mut Vec<Block>,
        quorum_certs: &mut Vec<QuorumCert>,
        order_vote_enabled: bool,
        window_size: u64,
    ) -> Result<RootInfo> {
        // We start from the block that storage's latest ledger info, if storage has end-epoch
        // LedgerInfo, we generate the virtual genesis block
        let (latest_commit_id, latest_ledger_info_sig) =
            if self.storage_ledger.ledger_info().ends_epoch() {
                let genesis =
                    Block::make_genesis_block_from_ledger_info(self.storage_ledger.ledger_info());
                let genesis_qc = QuorumCert::certificate_for_genesis_from_ledger_info(
                    self.storage_ledger.ledger_info(),
                    genesis.id(),
                );
                let genesis_ledger_info = genesis_qc.ledger_info().clone();
                let genesis_id = genesis.id();
                blocks.push(genesis);
                quorum_certs.push(genesis_qc);
                (genesis_id, genesis_ledger_info)
            } else {
                (
                    self.storage_ledger.ledger_info().consensus_block_id(),
                    self.storage_ledger.clone(),
                )
            };

        // sort by (epoch, round) to guarantee the topological order of parent <- child
        blocks.sort_by_key(|b| (b.epoch(), b.round()));

        let latest_commit_idx = blocks
            .iter()
            .position(|block| block.id() == latest_commit_id)
            .ok_or_else(|| format_err!("unable to find root: {}", latest_commit_id))?;
        let commit_block = blocks[latest_commit_idx].clone();
        let commit_block_quorum_cert = quorum_certs
            .iter()
            .find(|qc| qc.certified_block().id() == commit_block.id())
            .ok_or_else(|| format_err!("No QC found for root: {}", commit_block.id()))?
            .clone();

        let (root_ordered_cert, root_commit_cert) = if order_vote_enabled {
            // We are setting ordered_root same as commit_root. As every committed block is also ordered, this is fine.
            // As the block store inserts all the fetched blocks and quorum certs and execute the blocks, the block store
            // updates highest_ordered_cert accordingly.
            let root_ordered_cert =
                WrappedLedgerInfo::new(VoteData::dummy(), latest_ledger_info_sig.clone());
            (root_ordered_cert.clone(), root_ordered_cert)
        } else {
            let root_ordered_cert = quorum_certs
                .iter()
                .find(|qc| qc.commit_info().id() == commit_block.id())
                .ok_or_else(|| format_err!("No LI found for root: {}", latest_commit_id))?
                .clone()
                .into_wrapped_ledger_info();
            let root_commit_cert = root_ordered_cert
                .create_merged_with_executed_state(latest_ledger_info_sig)
                .expect("Inconsistent commit proof and evaluation decision, cannot commit block");
            (root_ordered_cert, root_commit_cert)
        };

        let window_start_round = calculate_window_start_round(commit_block.round(), window_size);
        let mut id_to_blocks = HashMap::new();
        blocks.iter().for_each(|block| {
            id_to_blocks.insert(block.id(), block);
        });

        let mut current_block = &commit_block;
        while !current_block.is_genesis_block()
            && current_block.quorum_cert().certified_block().round() >= window_start_round
        {
            if let Some(parent_block) = id_to_blocks.get(&current_block.parent_id()) {
                current_block = *parent_block;
            } else {
                bail!("Parent block not found for block {}", current_block.id());
            }
        }
        let window_start_id = current_block.id();

        let window_start_idx = blocks
            .iter()
            .position(|block| block.id() == window_start_id)
            .ok_or_else(|| format_err!("unable to find window root: {}", window_start_id))?;
        let window_start_block = blocks.remove(window_start_idx);

        info!(
            "Commit block is {}, window block is {}",
            commit_block, window_start_block
        );

        Ok(RootInfo {
            commit_root_block: Box::new(commit_block),
            window_root_block: Some(Box::new(window_start_block)),
            quorum_cert: commit_block_quorum_cert,
            ordered_cert: root_ordered_cert,
            commit_cert: root_commit_cert,
        })
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L278-296)
```rust
    pub fn find_root(
        &self,
        blocks: &mut Vec<Block>,
        quorum_certs: &mut Vec<QuorumCert>,
        order_vote_enabled: bool,
        window_size: Option<u64>,
    ) -> Result<RootInfo> {
        info!(
            "The last committed block id as recorded in storage: {}",
            self.storage_ledger
        );

        match window_size {
            None => self.find_root_without_window(blocks, quorum_certs, order_vote_enabled),
            Some(window_size) => {
                self.find_root_with_window(blocks, quorum_certs, order_vote_enabled, window_size)
            },
        }
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L347-419)
```rust
impl RecoveryData {
    pub fn new(
        last_vote: Option<Vote>,
        ledger_recovery_data: LedgerRecoveryData,
        mut blocks: Vec<Block>,
        root_metadata: RootMetadata,
        mut quorum_certs: Vec<QuorumCert>,
        highest_2chain_timeout_cert: Option<TwoChainTimeoutCertificate>,
        order_vote_enabled: bool,
        window_size: Option<u64>,
    ) -> Result<Self> {
        let root = ledger_recovery_data
            .find_root(
                &mut blocks,
                &mut quorum_certs,
                order_vote_enabled,
                window_size,
            )
            .with_context(|| {
                // for better readability
                blocks.sort_by_key(|block| block.round());
                quorum_certs.sort_by_key(|qc| qc.certified_block().round());
                format!(
                    "\nRoot: {}\nBlocks in db: {}\nQuorum Certs in db: {}\n",
                    ledger_recovery_data.storage_ledger.ledger_info(),
                    blocks
                        .iter()
                        .map(|b| format!("\n{}", b))
                        .collect::<Vec<String>>()
                        .concat(),
                    quorum_certs
                        .iter()
                        .map(|qc| format!("\n{}", qc))
                        .collect::<Vec<String>>()
                        .concat(),
                )
            })?;

        // If execution pool is enabled, use the window_root, else use the commit_root
        let (root_id, epoch) = match &root.window_root_block {
            None => {
                let commit_root_id = root.commit_root_block.id();
                let epoch = root.commit_root_block.epoch();
                (commit_root_id, epoch)
            },
            Some(window_root_block) => {
                let window_start_id = window_root_block.id();
                let epoch = window_root_block.epoch();
                (window_start_id, epoch)
            },
        };
        let blocks_to_prune = Some(Self::find_blocks_to_prune(
            root_id,
            &mut blocks,
            &mut quorum_certs,
        ));

        Ok(RecoveryData {
            last_vote: match last_vote {
                Some(v) if v.epoch() == epoch => Some(v),
                _ => None,
            },
            root,
            root_metadata,
            blocks,
            quorum_certs,
            blocks_to_prune,
            highest_2chain_timeout_certificate: match highest_2chain_timeout_cert {
                Some(tc) if tc.epoch() == epoch => Some(tc),
                _ => None,
            },
        })
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L519-596)
```rust
    fn start(&self, order_vote_enabled: bool, window_size: Option<u64>) -> LivenessStorageData {
        info!("Start consensus recovery.");
        let raw_data = self
            .db
            .get_data()
            .expect("unable to recover consensus data");

        let last_vote = raw_data
            .0
            .map(|bytes| bcs::from_bytes(&bytes[..]).expect("unable to deserialize last vote"));

        let highest_2chain_timeout_cert = raw_data.1.map(|b| {
            bcs::from_bytes(&b).expect("unable to deserialize highest 2-chain timeout cert")
        });
        let blocks = raw_data.2;
        let quorum_certs: Vec<_> = raw_data.3;
        let blocks_repr: Vec<String> = blocks.iter().map(|b| format!("\n\t{}", b)).collect();
        info!(
            "The following blocks were restored from ConsensusDB : {}",
            blocks_repr.concat()
        );
        let qc_repr: Vec<String> = quorum_certs
            .iter()
            .map(|qc| format!("\n\t{}", qc))
            .collect();
        info!(
            "The following quorum certs were restored from ConsensusDB: {}",
            qc_repr.concat()
        );
        // find the block corresponding to storage latest ledger info
        let latest_ledger_info = self
            .aptos_db
            .get_latest_ledger_info()
            .expect("Failed to get latest ledger info.");
        let accumulator_summary = self
            .aptos_db
            .get_accumulator_summary(latest_ledger_info.ledger_info().version())
            .expect("Failed to get accumulator summary.");
        let ledger_recovery_data = LedgerRecoveryData::new(latest_ledger_info);

        match RecoveryData::new(
            last_vote,
            ledger_recovery_data.clone(),
            blocks,
            accumulator_summary.into(),
            quorum_certs,
            highest_2chain_timeout_cert,
            order_vote_enabled,
            window_size,
        ) {
            Ok(mut initial_data) => {
                (self as &dyn PersistentLivenessStorage)
                    .prune_tree(initial_data.take_blocks_to_prune())
                    .expect("unable to prune dangling blocks during restart");
                if initial_data.last_vote.is_none() {
                    self.db
                        .delete_last_vote_msg()
                        .expect("unable to cleanup last vote");
                }
                if initial_data.highest_2chain_timeout_certificate.is_none() {
                    self.db
                        .delete_highest_2chain_timeout_certificate()
                        .expect("unable to cleanup highest 2-chain timeout cert");
                }
                info!(
                    "Starting up the consensus state machine with recovery data - [last_vote {}], [highest timeout certificate: {}]",
                    initial_data.last_vote.as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                    initial_data.highest_2chain_timeout_certificate().as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                );

                LivenessStorageData::FullRecoveryData(initial_data)
            },
            Err(e) => {
                error!(error = ?e, "Failed to construct recovery data");
                LivenessStorageData::PartialRecoveryData(ledger_recovery_data)
            },
        }
    }
```

**File:** consensus/consensus-types/src/block.rs (L425-464)
```rust
    pub fn validate_signature(&self, validator: &ValidatorVerifier) -> anyhow::Result<()> {
        match self.block_data.block_type() {
            BlockType::Genesis => bail!("We should not accept genesis from others"),
            BlockType::NilBlock { .. } => self.quorum_cert().verify(validator),
            BlockType::Proposal { author, .. } => {
                let signature = self
                    .signature
                    .as_ref()
                    .ok_or_else(|| format_err!("Missing signature in Proposal"))?;
                let (res1, res2) = rayon::join(
                    || validator.verify(*author, &self.block_data, signature),
                    || self.quorum_cert().verify(validator),
                );
                res1?;
                res2
            },
            BlockType::ProposalExt(proposal_ext) => {
                let signature = self
                    .signature
                    .as_ref()
                    .ok_or_else(|| format_err!("Missing signature in Proposal"))?;
                let (res1, res2) = rayon::join(
                    || validator.verify(*proposal_ext.author(), &self.block_data, signature),
                    || self.quorum_cert().verify(validator),
                );
                res1?;
                res2
            },
            BlockType::OptimisticProposal(p) => {
                // Note: Optimistic proposal is not signed by proposer unlike normal proposal
                let (res1, res2) = rayon::join(
                    || p.grandparent_qc().verify(validator),
                    || self.quorum_cert().verify(validator),
                );
                res1?;
                res2
            },
            BlockType::DAGBlock { .. } => bail!("We should not accept DAG block from others"),
        }
    }
```

**File:** consensus/consensus-types/src/block.rs (L469-550)
```rust
    pub fn verify_well_formed(&self) -> anyhow::Result<()> {
        ensure!(
            !self.is_genesis_block(),
            "We must not accept genesis from others"
        );
        let parent = self.quorum_cert().certified_block();
        ensure!(
            parent.round() < self.round(),
            "Block must have a greater round than parent's block"
        );
        ensure!(
            parent.epoch() == self.epoch(),
            "block's parent should be in the same epoch"
        );
        if parent.has_reconfiguration() {
            ensure!(
                self.payload().is_none_or(|p| p.is_empty()),
                "Reconfiguration suffix should not carry payload"
            );
        }

        if let Some(payload) = self.payload() {
            payload.verify_epoch(self.epoch())?;
        }

        if let Some(failed_authors) = self.block_data().failed_authors() {
            // when validating for being well formed,
            // allow for missing failed authors,
            // for whatever reason (from different max configuration, etc),
            // but don't allow anything that shouldn't be there.
            //
            // we validate the full correctness of this field in round_manager.process_proposal()
            let succ_round = self.round() + u64::from(self.is_nil_block());
            let skipped_rounds = succ_round.checked_sub(parent.round() + 1);
            ensure!(
                skipped_rounds.is_some(),
                "Block round is smaller than block's parent round"
            );
            ensure!(
                failed_authors.len() <= skipped_rounds.unwrap() as usize,
                "Block has more failed authors than missed rounds"
            );
            let mut bound = parent.round();
            for (round, _) in failed_authors {
                ensure!(
                    bound < *round && *round < succ_round,
                    "Incorrect round in failed authors"
                );
                bound = *round;
            }
        }

        if self.is_nil_block() || parent.has_reconfiguration() {
            ensure!(
                self.timestamp_usecs() == parent.timestamp_usecs(),
                "Nil/reconfig suffix block must have same timestamp as parent"
            );
        } else {
            ensure!(
                self.timestamp_usecs() > parent.timestamp_usecs(),
                "Blocks must have strictly increasing timestamps"
            );

            let current_ts = duration_since_epoch();

            // we can say that too far is 5 minutes in the future
            const TIMEBOUND: u64 = 300_000_000;
            ensure!(
                self.timestamp_usecs() <= (current_ts.as_micros() as u64).saturating_add(TIMEBOUND),
                "Blocks must not be too far in the future"
            );
        }
        ensure!(
            !self.quorum_cert().ends_epoch(),
            "Block cannot be proposed in an epoch that has ended"
        );
        debug_checked_verify_eq!(
            self.id(),
            self.block_data.hash(),
            "Block id mismatch the hash"
        );
        Ok(())
```

**File:** consensus/consensus-types/src/block.rs (L641-663)
```rust
impl<'de> Deserialize<'de> for Block {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: Deserializer<'de>,
    {
        #[derive(Deserialize)]
        #[serde(rename = "Block")]
        struct BlockWithoutId {
            block_data: BlockData,
            signature: Option<bls12381::Signature>,
        }

        let BlockWithoutId {
            block_data,
            signature,
        } = BlockWithoutId::deserialize(deserializer)?;

        Ok(Block {
            id: block_data.hash(),
            block_data,
            signature,
        })
    }
```

**File:** consensus/src/block_storage/block_store.rs (L282-305)
```rust
        for block in blocks {
            if block.round() <= root_block_round {
                block_store
                    .insert_committed_block(block)
                    .await
                    .unwrap_or_else(|e| {
                        panic!(
                            "[BlockStore] failed to insert committed block during build {:?}",
                            e
                        )
                    });
            } else {
                block_store.insert_block(block).await.unwrap_or_else(|e| {
                    panic!("[BlockStore] failed to insert block during build {:?}", e)
                });
            }
        }
        for qc in quorum_certs {
            block_store
                .insert_single_quorum_cert(qc)
                .unwrap_or_else(|e| {
                    panic!("[BlockStore] failed to insert quorum during build{:?}", e)
                });
        }
```

**File:** consensus/src/block_storage/block_store.rs (L397-438)
```rust
    pub async fn insert_committed_block(
        &self,
        block: Block,
    ) -> anyhow::Result<Arc<PipelinedBlock>> {
        ensure!(
            self.get_block(block.id()).is_none(),
            "Recovered block already exists"
        );

        // We don't know if the blocks in the window for a committed block will
        // be available in memory so we set the OrderedBlockWindow to empty
        let pipelined_block = PipelinedBlock::new_ordered(block, OrderedBlockWindow::empty());
        self.insert_block_inner(pipelined_block).await
    }

    pub async fn insert_block(&self, block: Block) -> anyhow::Result<Arc<PipelinedBlock>> {
        if let Some(existing_block) = self.get_block(block.id()) {
            return Ok(existing_block);
        }
        ensure!(
            self.inner.read().ordered_root().round() < block.round(),
            "Block with old round"
        );

        let block_window = self
            .inner
            .read()
            .get_ordered_block_window(&block, self.window_size)?;
        let blocks = block_window.blocks();
        for block in blocks {
            if let Some(payload) = block.payload() {
                self.payload_manager.prefetch_payload_data(
                    payload,
                    block.author().expect("Payload block must have author"),
                    block.timestamp_usecs(),
                );
            }
        }

        let pipelined_block = PipelinedBlock::new_ordered(block, block_window);
        self.insert_block_inner(pipelined_block).await
    }
```
