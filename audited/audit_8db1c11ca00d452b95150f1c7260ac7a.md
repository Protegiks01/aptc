# Audit Report

## Title
Unbounded Memory Consumption During State KV Shard Pruner Initialization Causes OOM and Node Startup Failure

## Summary
The `StateKvShardPruner::new()` function performs unbounded catch-up pruning during initialization without batching, accumulating all delete operations in memory. When the version gap is large (millions of versions), this causes out-of-memory crashes or extremely long initialization times, preventing validator nodes from starting.

## Finding Description

When database sharding is enabled for state KV storage, each shard maintains its own pruner progress. During initialization, if a shard pruner's progress lags behind the metadata pruner's progress, a catch-up operation is performed to synchronize them. [1](#0-0) 

The vulnerability occurs at line 42 where `myself.prune(progress, metadata_progress)` is called with the full version range. The `prune()` function accumulates all delete operations in a single `SchemaBatch` before writing to the database: [2](#0-1) 

The `SchemaBatch` structure accumulates all operations in memory using a `HashMap<ColumnFamilyName, Vec<WriteOp>>`: [3](#0-2) 

Each stale state value generates **two** delete operations (one for the index, one for the value), and all are held in memory until the batch is written. There are no size limits enforced: [4](#0-3) 

In contrast, normal pruning operations use batching with a default batch size of 5,000 versions: [5](#0-4) [6](#0-5) 

**Attack Scenario:**

1. A validator node has been running for months with millions of versions (e.g., 100M versions on mainnet)
2. Operator enables sharding for the first time or restores from backup
3. Metadata pruner progress is at current height (e.g., 100M)
4. Shard pruner progress initializes to 0 or a very old value
5. During initialization, `StateKvShardPruner::new()` attempts to prune the entire range
6. With ~6 state values touched per transaction and high update rates, millions of stale entries exist
7. All delete operations accumulate in memory (estimated 10-100GB for realistic scenarios)
8. Node crashes with OOM or initialization takes hours, preventing startup

## Impact Explanation

**Severity: HIGH** (Validator node slowdowns/crashes, per Aptos Bug Bounty)

This vulnerability causes:
- **Validator Downtime**: Nodes fail to initialize, causing validator unavailability
- **OOM Crashes**: Memory exhaustion during startup prevents node recovery
- **Initialization Timeout**: Even if OOM doesn't occur, the operation may take hours
- **Network Impact**: Multiple validators experiencing this simultaneously reduces network availability

The issue violates the documented invariant: **"Resource Limits: All operations must respect gas, storage, and computational limits"** by performing unbounded memory accumulation.

Based on benchmark data showing "A 10k transaction block (touching 60k state values)", approximately 6 state values are updated per transaction. Over a 10M version gap with moderate state churn, this could easily generate 50-100M stale entries requiring 7.5-15GB of memory just for batch operations. [7](#0-6) 

## Likelihood Explanation

**Likelihood: HIGH**

This issue occurs in the following realistic scenarios:

1. **First-time sharding enablement**: When sharding is enabled on an existing production node
2. **Node recovery**: After crash recovery where shard progress metadata is lost
3. **Backup restoration**: Restoring a node from backup with outdated shard progress
4. **Long downtime**: Shard pruner falling significantly behind during extended maintenance

These are normal operational scenarios, not edge cases. The issue is deterministic and will always manifest when the version gap is sufficiently large.

## Recommendation

Implement batched catch-up pruning during initialization, consistent with normal pruning operations:

```rust
pub(in crate::pruner) fn new(
    shard_id: usize,
    db_shard: Arc<DB>,
    metadata_progress: Version,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        &db_shard,
        &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
        metadata_progress,
    )?;
    let myself = Self { shard_id, db_shard };

    info!(
        progress = progress,
        metadata_progress = metadata_progress,
        "Catching up state kv shard {shard_id}."
    );
    
    // FIX: Batch the catch-up pruning
    const CATCHUP_BATCH_SIZE: u64 = 5_000; // Match normal batch size
    let mut current_progress = progress;
    while current_progress < metadata_progress {
        let batch_target = std::cmp::min(
            current_progress + CATCHUP_BATCH_SIZE,
            metadata_progress
        );
        myself.prune(current_progress, batch_target)?;
        current_progress = batch_target;
    }

    Ok(myself)
}
```

This ensures catch-up operations respect the same memory constraints as normal pruning.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_temppath::TempPath;
    use aptos_schemadb::DB;
    use aptos_types::transaction::Version;

    #[test]
    #[should_panic(expected = "out of memory")]
    fn test_catchup_oom_vulnerability() {
        // Setup: Create a test database
        let tmpdir = TempPath::new();
        let db = Arc::new(DB::open(
            tmpdir.path(),
            "test_db",
            vec!["stale_state_value_index_by_key_hash", "state_value_by_key_hash"],
            &rocksdb::Options::default()
        ).unwrap());

        // Populate database with millions of stale entries
        let num_stale_entries = 10_000_000; // 10M entries
        for version in 0..num_stale_entries {
            let index = StaleStateValueByKeyHashIndex {
                stale_since_version: version,
                version: version,
                state_key_hash: HashValue::random(),
            };
            db.put::<StaleStateValueIndexByKeyHashSchema>(&index, &()).unwrap();
        }

        // Trigger the vulnerability: large gap between progress (0) and metadata (10M)
        // This will attempt to accumulate all 10M delete operations in memory
        let result = StateKvShardPruner::new(
            0,  // shard_id
            db,
            num_stale_entries  // metadata_progress is 10M versions ahead
        );
        
        // Expected: OOM crash or extremely long operation time
        // With ~150 bytes per entry, this requires ~1.5GB of memory
        assert!(result.is_err());
    }
}
```

**Notes:**
- The vulnerability is architecture-specific and affects the storage layer's pruning subsystem
- The issue only manifests when sharding is enabled for state KV storage
- Similar patterns should be audited in `StateMerkleShardPruner` and other shard-based pruners
- The fix maintains deterministic behavior while preventing resource exhaustion
- Consider adding monitoring for catch-up operation duration and memory consumption

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L25-45)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/schemadb/src/batch.rs (L129-173)
```rust
#[derive(Debug, Default)]
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}

impl SchemaBatch {
    /// Creates an empty batch.
    pub fn new() -> Self {
        Self::default()
    }

    /// keep these on the struct itself so that we don't need to update each call site.
    pub fn put<S: Schema>(&mut self, key: &S::Key, value: &S::Value) -> DbResult<()> {
        <Self as WriteBatch>::put::<S>(self, key, value)
    }

    pub fn delete<S: Schema>(&mut self, key: &S::Key) -> DbResult<()> {
        <Self as WriteBatch>::delete::<S>(self, key)
    }
}

impl WriteBatch for SchemaBatch {
    fn stats(&mut self) -> &mut SampledBatchStats {
        &mut self.stats
    }

    fn raw_put(&mut self, cf_name: ColumnFamilyName, key: Vec<u8>, value: Vec<u8>) -> DbResult<()> {
        self.rows
            .entry(cf_name)
            .or_default()
            .push(WriteOp::Value { key, value });

        Ok(())
    }

    fn raw_delete(&mut self, cf_name: ColumnFamilyName, key: Vec<u8>) -> DbResult<()> {
        self.rows
            .entry(cf_name)
            .or_default()
            .push(WriteOp::Deletion { key });

        Ok(())
    }
}
```

**File:** storage/schemadb/src/lib.rs (L289-304)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L49-86)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_pruner__prune"]);

        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning state kv data."
            );
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning state kv data is done.");
        }

        Ok(target_version)
    }
```

**File:** config/src/config/storage_config.rs (L387-395)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
```

**File:** config/src/config/storage_config.rs (L406-411)
```rust
            // Still, defaulting to 1M to be super safe.
            prune_window: 1_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
```
