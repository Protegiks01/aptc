# Audit Report

## Title
State Snapshot Restore Race Condition Allows Unverified Data Persistence

## Summary
A critical Time-of-Check to Time-of-Use (TOCTOU) race condition exists in the state snapshot restoration process. When restoring in Default mode, key-value data is written to the database in parallel with cryptographic proof verification, allowing invalid state data to be permanently persisted if verification fails after the write completes.

## Finding Description

The vulnerability exists in the `StateSnapshotRestore::add_chunk` implementation where KV write operations and proof verification execute in parallel threads. [1](#0-0) 

In Default mode (the default restore mode), the code uses `IO_POOL.join(kv_fn, tree_fn)` to execute both operations in parallel. The `kv_fn` closure writes state data directly to the database via `StateValueRestore::add_chunk`, which commits the data synchronously: [2](#0-1) 

The write is committed immediately to the database via `write_kv_batch`: [3](#0-2) 

Meanwhile, `tree_fn` performs cryptographic verification using the Sparse Merkle proof: [4](#0-3) 

**Attack Scenario:**

1. Attacker provides a malicious state snapshot backup with mismatched `chunk.blobs` and `chunk.proof` files
2. The restore process reads both files separately: [5](#0-4) 

3. Both `kv_fn` and `tree_fn` execute in parallel
4. If `kv_fn` completes first, invalid blob data is committed to the state database
5. When `tree_fn` subsequently fails verification, an error is returned but the invalid data remains permanently in the database with no rollback mechanism

This breaks the fundamental invariant: **"State Consistency: State transitions must be atomic and verifiable via Merkle proofs"**

Additionally, in KvOnly mode, there is NO cryptographic verification whatsoever: [6](#0-5) 

## Impact Explanation

**Critical Severity** - This vulnerability allows an attacker to corrupt the state database with cryptographically invalid data, which constitutes:

1. **State Consistency Violation**: Bypasses Merkle tree verification that guarantees state integrity
2. **Database Corruption**: Invalid state data persists permanently in AptosDB
3. **Consensus Impact**: Nodes with corrupted state may compute different state roots, breaking consensus safety
4. **Non-recoverable State**: Once written, invalid data cannot be automatically detected or rolled back

This meets **Critical Severity** criteria per Aptos Bug Bounty:
- Consensus/Safety violations (corrupted state leads to divergent state roots)
- State inconsistencies that are non-recoverable without manual intervention
- Potential for network-wide state corruption if malicious backups are distributed

## Likelihood Explanation

**High Likelihood** because:

1. **Common Attack Vector**: State snapshot restoration is a standard operational procedure for new nodes joining the network or nodes recovering from failures
2. **No Special Privileges Required**: Any attacker who can provide backup files to a node (through compromised backup storage, malicious backup service, or man-in-the-middle attacks) can exploit this
3. **Race Condition Guaranteed**: Due to parallel execution, the race condition occurs on every chunk in Default mode
4. **No Detection**: The invalid data persists silently in the database with no automatic detection mechanism

The attack only requires:
- Access to provide malicious backup files to a victim node
- Ability to create mismatched `chunk.blobs` and `chunk.proof` files

## Recommendation

**Fix: Enforce Sequential Verification-Before-Write**

Modify `StateSnapshotRestore::add_chunk` to perform verification BEFORE any writes:

```rust
fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
    match self.restore_mode {
        StateSnapshotRestoreMode::KvOnly => {
            // SECURITY FIX: Always verify proof even in KvOnly mode
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .verify_chunk_without_write(
                    chunk.iter().map(|(k, v)| (k, v.hash())).collect(), 
                    proof
                )?;
            
            self.kv_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk(chunk)
        },
        StateSnapshotRestoreMode::TreeOnly => {
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
        },
        StateSnapshotRestoreMode::Default => {
            // SECURITY FIX: Verify FIRST, then write in parallel
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)?;
            
            // Only write KV data AFTER verification succeeds
            self.kv_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk(chunk)
        },
    }
}
```

**Alternative: Implement Transactional Rollback**

Add a transaction mechanism that allows rolling back KV writes if verification fails. However, this is more complex and has performance implications.

## Proof of Concept

```rust
// PoC: Demonstrate race condition with mismatched files

#[test]
fn test_mismatched_chunk_corruption() {
    use aptos_types::state_store::{state_key::StateKey, state_value::StateValue};
    use aptos_crypto::HashValue;
    
    // Setup restore receiver in Default mode
    let restore_handler = setup_restore_handler();
    let mut receiver = restore_handler.get_state_restore_receiver(
        0, 
        HashValue::zero(),
        StateSnapshotRestoreMode::Default
    ).unwrap();
    
    // Create mismatched chunk data
    let valid_key = StateKey::raw(b"valid_key");
    let malicious_value = StateValue::new_legacy(b"malicious_data".to_vec());
    let chunk = vec![(valid_key.clone(), malicious_value)];
    
    // Create proof for DIFFERENT data (causes verification failure)
    let different_key = StateKey::raw(b"different_key");
    let different_value = StateValue::new_legacy(b"different_data".to_vec());
    let proof = create_proof_for_chunk(vec![(different_key, different_value)]);
    
    // Attempt to add mismatched chunk
    let result = receiver.add_chunk(chunk, proof);
    
    // Verification should fail
    assert!(result.is_err());
    
    // BUT: Query database - malicious data may already be persisted!
    let db_value = query_state_kv_db(&valid_key, 0);
    
    // VULNERABILITY: Invalid data was written despite verification failure
    if db_value.is_some() {
        panic!("VULNERABILITY CONFIRMED: Unverified data persisted in database!");
    }
}
```

## Notes

This vulnerability is particularly critical because:

1. **Silent Corruption**: Invalid data persists without any warnings or detection
2. **Trust Boundary Violation**: The system trusts that verification happens before writes, but parallel execution violates this assumption
3. **Cascading Failures**: Corrupted state at version N affects all subsequent state computations
4. **Default Mode Affected**: The vulnerability exists in the DEFAULT restore mode, affecting most restoration operations

The fix must ensure cryptographic verification completes successfully BEFORE any state data is written to the database, maintaining the atomic "verify-then-write" invariant.

### Citations

**File:** storage/aptosdb/src/state_restore/mod.rs (L88-127)
```rust
    pub fn add_chunk(&mut self, mut chunk: Vec<(K, V)>) -> Result<()> {
        // load progress
        let progress_opt = self.db.get_progress(self.version)?;

        // skip overlaps
        if let Some(progress) = progress_opt {
            let idx = chunk
                .iter()
                .position(|(k, _v)| CryptoHash::hash(k) > progress.key_hash)
                .unwrap_or(chunk.len());
            chunk = chunk.split_off(idx);
        }

        // quit if all skipped
        if chunk.is_empty() {
            return Ok(());
        }

        // save
        let mut usage = progress_opt.map_or(StateStorageUsage::zero(), |p| p.usage);
        let (last_key, _last_value) = chunk.last().unwrap();
        let last_key_hash = CryptoHash::hash(last_key);

        // In case of TreeOnly Restore, we only restore the usage of KV without actually writing KV into DB
        for (k, v) in chunk.iter() {
            usage.add_item(k.key_size() + v.value_size());
        }

        // prepare the sharded kv batch
        let kv_batch: StateValueBatch<K, Option<V>> = chunk
            .into_iter()
            .map(|(k, v)| ((k, self.version), Some(v)))
            .collect();

        self.db.write_kv_batch(
            self.version,
            &kv_batch,
            StateSnapshotProgress::new(last_key_hash, usage),
        )
    }
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L228-258)
```rust
    fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
        let kv_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_add_chunk"]);
            self.kv_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk(chunk.clone())
        };

        let tree_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["jmt_add_chunk"]);
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
        };
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => kv_fn()?,
            StateSnapshotRestoreMode::TreeOnly => tree_fn()?,
            StateSnapshotRestoreMode::Default => {
                // We run kv_fn with TreeOnly to restore the usage of DB
                let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
                r1?;
                r2?;
            },
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1244-1279)
```rust
    fn write_kv_batch(
        &self,
        version: Version,
        node_batch: &StateValueBatch,
        progress: StateSnapshotProgress,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_writer_write_chunk"]);
        let mut batch = SchemaBatch::new();
        let mut sharded_schema_batch = self.state_kv_db.new_sharded_native_batches();

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateSnapshotKvRestoreProgress(version),
            &DbMetadataValue::StateSnapshotProgress(progress),
        )?;

        if self.internal_indexer_db.is_some()
            && self
                .internal_indexer_db
                .as_ref()
                .unwrap()
                .statekeys_enabled()
        {
            let keys = node_batch.keys().map(|key| key.0.clone()).collect();
            self.internal_indexer_db
                .as_ref()
                .unwrap()
                .write_keys_to_indexer_db(&keys, version, progress)?;
        }
        self.shard_state_value_batch(
            &mut sharded_schema_batch,
            node_batch,
            self.state_kv_db.enabled_sharding(),
        )?;
        self.state_kv_db
            .commit(version, Some(batch), sharded_schema_batch)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L339-413)
```rust
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }

        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L187-197)
```rust
        let futs_iter = chunks.into_iter().enumerate().map(|(chunk_idx, chunk)| {
            let storage = storage.clone();
            async move {
                tokio::spawn(async move {
                    let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
                    Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
                })
                .await?
            }
        });
```
