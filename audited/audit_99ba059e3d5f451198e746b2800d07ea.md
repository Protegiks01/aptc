# Audit Report

## Title
Missing Execution Timeout Enables Permanent Consensus Liveness Failure

## Summary
The execution pipeline in `ExecutionWaitPhase::process()` awaits execution futures without any timeout mechanism, creating a critical vulnerability where any execution bug that causes hanging will permanently block the consensus pipeline with no recovery path.

## Finding Description
The decoupled execution pipeline processes blocks sequentially through the `ExecutionWaitPhase`, which awaits execution futures with no timeout protection. This breaks the consensus liveness invariant when execution hangs.

**Vulnerable Code Path:**

The `ExecutionWaitPhase::process()` method directly awaits the execution future without timeout: [1](#0-0) 

This future is created by `ExecutionSchedulePhase` which spawns blocking operations for block execution: [2](#0-1) 

The execution is wrapped in `spawn_blocking` calls that cannot be interrupted: [3](#0-2) 

Similarly for ledger updates: [4](#0-3) 

**Sequential Processing Amplifies Impact:**

The `PipelinePhase` processes requests sequentially in its main loop: [5](#0-4) 

When execution hangs at line 99, ALL subsequent blocks are blocked from processing.

**Abort Mechanism is Ineffective:**

While there is an abort mechanism via `abort_pipeline()`: [6](#0-5) 

Tokio's abort cannot interrupt code running in `spawn_blocking`. The reset mechanism must wait for futures to complete: [7](#0-6) 

But `wait_until_finishes()` has no timeout either: [8](#0-7) 

**Trigger Scenarios:**

While Move VM has gas metering protecting against infinite loops in bytecode, execution can hang due to:
1. Bugs in native function implementations (e.g., in cryptography operations)
2. Database deadlocks or corruption in RocksDB operations  
3. Race conditions in the executor's block tree management
4. Unexpected long-running operations in `spawn_blocking` that exceed reasonable bounds

## Impact Explanation
**Critical Severity** - Total Loss of Liveness/Network Availability

If execution hangs due to any implementation bug:
1. The `ExecutionWaitPhase` permanently blocks on line 54 of `execution_wait_phase.rs`
2. All subsequent blocks cannot be processed (sequential pipeline)
3. No new blocks can be committed across the entire network
4. Round timeouts do not abort execution - they only move to next round
5. The reset mechanism cannot recover (waits indefinitely for hung futures)
6. **Network-wide permanent liveness failure** - requires node restarts or hardfork

This meets the "Total loss of liveness/network availability" category for Critical severity ($1,000,000 tier).

## Likelihood Explanation
**Medium-High Likelihood** of occurrence if triggerable bugs exist:

- The lack of defensive timeout is a **guaranteed** code path issue
- Execution bugs in native functions or database operations are **realistic** (complex cryptography, concurrent database access)
- A single malformed block can affect **all validators** deterministically
- No privileged access required - bugs can be triggered by transaction content that passes through normal consensus ordering

The likelihood depends on existence of triggerable execution bugs, but the **impact is guaranteed catastrophic** when such bugs manifest.

## Recommendation
Add timeout protection at the `ExecutionWaitPhase` level:

```rust
async fn process(&self, req: ExecutionWaitRequest) -> ExecutionResponse {
    let ExecutionWaitRequest { block_id, fut } = req;
    
    // Add configurable timeout (e.g., 30-60 seconds)
    const EXECUTION_TIMEOUT: Duration = Duration::from_secs(30);
    
    let result = match tokio::time::timeout(EXECUTION_TIMEOUT, fut).await {
        Ok(execution_result) => execution_result,
        Err(_timeout) => {
            error!("Execution timeout for block {}", block_id);
            Err(ExecutorError::InternalError {
                error: format!("Execution timeout after {:?}", EXECUTION_TIMEOUT),
            })
        }
    };

    ExecutionResponse {
        block_id,
        inner: result,
    }
}
```

Additionally:
1. Add timeout configuration to `ConsensusConfig`
2. Implement monitoring metrics for execution duration
3. Add circuit breaker logic to abort after N consecutive timeouts
4. Log detailed diagnostics when timeouts occur for debugging

## Proof of Concept
While I cannot provide a complete PoC without identifying the specific execution bug, the vulnerability can be demonstrated conceptually:

```rust
// Simulated hang scenario (would need actual bug to trigger in production)
#[tokio::test]
async fn test_execution_hang_blocks_pipeline() {
    // Create execution future that hangs indefinitely
    let hanging_fut = Box::pin(async {
        // Simulate spawn_blocking operation that never completes
        tokio::task::spawn_blocking(|| {
            std::thread::sleep(Duration::from_secs(u64::MAX));
        }).await.unwrap();
        
        Ok(vec![])
    });
    
    let req = ExecutionWaitRequest {
        block_id: HashValue::random(),
        fut: hanging_fut,
    };
    
    let phase = ExecutionWaitPhase;
    
    // This will hang forever - no timeout protection
    // In production, this blocks ALL subsequent block processing
    tokio::select! {
        _ = phase.process(req) => {
            panic!("Should never complete");
        }
        _ = tokio::time::sleep(Duration::from_secs(5)) => {
            println!("Confirmed: execution hung with no timeout recovery");
        }
    }
}
```

## Notes
The root cause is the absence of defensive timeout mechanisms in consensus-critical async operations. While the Move VM includes gas metering for bytecode execution, this does not protect against hangs in:
- Native Rust function implementations  
- Database operations
- Executor internal operations running in `spawn_blocking`

The sequential pipeline design means a single hanging block prevents all subsequent blocks from processing, making this a network-wide liveness failure rather than a single-node issue.

### Citations

**File:** consensus/src/pipeline/execution_wait_phase.rs (L49-56)
```rust
    async fn process(&self, req: ExecutionWaitRequest) -> ExecutionResponse {
        let ExecutionWaitRequest { block_id, fut } = req;

        ExecutionResponse {
            block_id,
            inner: fut.await,
        }
    }
```

**File:** consensus/src/pipeline/execution_schedule_phase.rs (L70-79)
```rust
        let fut = async move {
            for b in ordered_blocks.iter_mut() {
                let (compute_result, execution_time) = b.wait_for_compute_result().await?;
                b.set_compute_result(compute_result, execution_time);
            }
            Ok(ordered_blocks)
        }
        .boxed();

        ExecutionWaitRequest { block_id, fut }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L856-868)
```rust
        let start = Instant::now();
        tokio::task::spawn_blocking(move || {
            executor
                .execute_and_update_state(
                    (block.id(), txns, auxiliary_info).into(),
                    block.parent_id(),
                    onchain_execution_config,
                )
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(start.elapsed())
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L887-893)
```rust
        let result = tokio::task::spawn_blocking(move || {
            executor
                .ledger_update(block_clone.id(), block_clone.parent_id())
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```

**File:** consensus/src/pipeline/pipeline_phase.rs (L88-109)
```rust
    pub async fn start(mut self) {
        // main loop
        while let Some(counted_req) = self.rx.next().await {
            let CountedRequest { req, guard: _guard } = counted_req;
            if self.reset_flag.load(Ordering::SeqCst) {
                continue;
            }
            let response = {
                let _timer = BUFFER_MANAGER_PHASE_PROCESS_SECONDS
                    .with_label_values(&[T::NAME])
                    .start_timer();
                self.processor.process(req).await
            };
            if let Some(tx) = &mut self.maybe_tx {
                if tx.send(response).await.is_err() {
                    debug!("Failed to send response, buffer manager probably dropped");
                    break;
                }
            }
        }
    }
}
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L104-113)
```rust
    pub async fn wait_until_finishes(self) {
        let _ = join5(
            self.execute_fut,
            self.ledger_update_fut,
            self.pre_commit_fut,
            self.commit_ledger_fut,
            self.notify_state_sync_fut,
        )
        .await;
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L528-547)
```rust
    pub fn abort_pipeline(&self) -> Option<PipelineFutures> {
        if let Some(abort_handles) = self.pipeline_abort_handle.lock().take() {
            let mut aborted = false;
            for handle in abort_handles {
                if !handle.is_finished() {
                    handle.abort();
                    aborted = true;
                }
            }
            if aborted {
                info!(
                    "[Pipeline] Aborting pipeline for block {} {} {}",
                    self.id(),
                    self.epoch(),
                    self.round()
                );
            }
        }
        self.pipeline_futs.lock().take()
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L546-576)
```rust
    async fn reset(&mut self) {
        while let Some((_, block)) = self.pending_commit_blocks.pop_first() {
            // Those blocks don't have any dependencies, should be able to finish commit_ledger.
            // Abort them can cause error on epoch boundary.
            block.wait_for_commit_ledger().await;
        }
        while let Some(item) = self.buffer.pop_front() {
            for b in item.get_blocks() {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
        self.buffer = Buffer::new();
        self.execution_root = None;
        self.signing_root = None;
        self.previous_commit_time = Instant::now();
        self.commit_proof_rb_handle.take();
        // purge the incoming blocks queue
        while let Ok(Some(blocks)) = self.block_rx.try_next() {
            for b in blocks.ordered_blocks {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
        // Wait for ongoing tasks to finish before sending back ack.
        while self.ongoing_tasks.load(Ordering::SeqCst) > 0 {
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
    }
```
