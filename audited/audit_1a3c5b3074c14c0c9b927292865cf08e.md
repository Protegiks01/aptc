# Audit Report

## Title
State Snapshot Restore Lacks Final Root Hash Verification Leading to Database Inconsistency

## Summary
The `StateSnapshotRestoreController::run_impl()` function validates the manifest's root hash against the transaction proof, but the underlying `JellyfishMerkleRestore::finish_impl()` does not verify that the final reconstructed tree matches the expected root hash. An attacker who controls the backup source can provide a manifest with an empty or incomplete chunks array, causing the restore to write an inconsistent state tree to storage without detection. [1](#0-0) 

## Finding Description

When a state snapshot restore is performed, the manifest is deserialized and validated: [2](#0-1) 

The validation confirms that:
1. The transaction proof is valid
2. The manifest's `root_hash` matches the state root hash from the proof

However, the validation does **not** check that the `chunks` array is non-empty or complete. The manifest structure allows an empty chunks array: [3](#0-2) 

When chunks is empty, the iteration proceeds safely (no undefined behavior in Rust): [4](#0-3) 

The while loop simply doesn't execute, and `finish()` is called on the receiver: [5](#0-4) 

In `JellyfishMerkleRestore::finish_impl()`, when no chunks were added to a fresh restore, the function writes a `Node::Null` to storage: [6](#0-5) 

**Critical Issue**: `finish_impl()` writes the tree to storage without verifying that the final root hash matches `self.expected_root_hash`. A `Node::Null` has hash value `SPARSE_MERKLE_PLACEHOLDER_HASH`, which will not match any legitimate non-empty state root.

The expected root hash is stored but never validated at completion: [7](#0-6) 

**Attack Scenario**:
1. Attacker compromises backup storage or tricks operator into using malicious backup
2. Attacker provides manifest with valid `version`, `epoch`, `root_hash`, and `proof` from a legitimate state checkpoint
3. Attacker provides an **empty** `chunks` array
4. Node performs restore:
   - Validates proof ✓
   - Validates root_hash in proof ✓  
   - Processes zero chunks
   - Writes `Node::Null` or incomplete tree to storage ✓
   - Returns success without verifying final tree matches expected root hash
5. Database now contains state tree with wrong root hash

## Impact Explanation

**Severity: Medium to High**

This violates the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs."

The database is left in an inconsistent state where:
- The validated root hash indicates a specific state checkpoint
- But the actual tree in storage is empty or incomplete
- Subsequent state queries will fail or return incorrect results
- State sync with other nodes will fail due to root hash mismatch

**Medium Severity Impact:**
- State inconsistencies requiring manual intervention to repair
- Nodes affected must re-sync from scratch or restore from a valid backup
- Limited to nodes that restore from the compromised backup source

**Potential High Severity Impact:**
- If multiple validators restore from the same compromised source during recovery, they could have consistent-with-each-other but incorrect state
- Could cause temporary consensus issues if a minority of validators have wrong state
- Significant protocol violation requiring coordinated recovery

## Likelihood Explanation

**Likelihood: Medium**

**Attack Requirements:**
1. Attacker must compromise backup storage OR
2. Attacker must social engineer validator operator to restore from malicious source
3. Victim must perform state snapshot restore operation

**Mitigating Factors:**
- Backup sources are typically trusted infrastructure
- State restores are infrequent operations (disaster recovery, new validator setup)
- Nodes will fail to sync with network after restore, alerting operators
- Does not affect normal operation, only restore scenarios

**Amplifying Factors:**
- No validation prevents the attack once backup source is compromised
- Silent failure - restore reports success despite writing wrong state
- Multiple nodes could be affected if using same compromised backup

## Recommendation

Add final root hash verification in `JellyfishMerkleRestore::finish_impl()` before returning success:

```rust
pub fn finish_impl(mut self) -> Result<()> {
    self.wait_for_async_commit()?;
    
    // ... existing code to write tree ...
    
    // ADDED: Verify final root hash matches expected
    let root_node = self.store.get_node_option(&NodeKey::new_empty_path(self.version), "finish")?
        .ok_or_else(|| anyhow!("Root node not found after restore"))?;
    
    let actual_root_hash = root_node.hash();
    ensure!(
        actual_root_hash == self.expected_root_hash,
        "State restore produced incorrect root hash. Expected: {}, Actual: {}",
        self.expected_root_hash,
        actual_root_hash
    );
    
    Ok(())
}
```

Additionally, add validation in `StateSnapshotRestoreController::run_impl()` to ensure chunks array is non-empty when root hash indicates non-empty tree:

```rust
// After line 136, add:
if manifest.root_hash != *SPARSE_MERKLE_PLACEHOLDER_HASH {
    ensure!(
        !manifest.chunks.is_empty(),
        "Manifest has non-empty root hash but empty chunks array"
    );
}
```

## Proof of Concept

Create a malicious manifest file `malicious_manifest.json`:

```json
{
  "version": 1000,
  "epoch": 10,
  "root_hash": "0x1234567890abcdef...",
  "chunks": [],
  "proof": "proof_file_handle"
}
```

Where `root_hash` is copied from a legitimate state checkpoint and `proof` is a valid proof file proving that root hash.

Execute restore:
```bash
aptos-backup-cli state-snapshot restore \
  --state-manifest malicious_manifest.json \
  --state-into-version 1000
```

The restore will complete successfully but the database will contain `Node::Null` instead of the expected state tree, causing all subsequent operations to fail or produce incorrect results.

**Notes**

The security question's premise about "undefined behavior during iteration" is incorrect - Rust's safety guarantees prevent undefined behavior. The actual vulnerability is the lack of final root hash verification in `finish_impl()`, which allows inconsistent state to be written to storage. This requires attacker control over the backup source (supply chain attack scenario), making it a real but contextual vulnerability rather than a direct protocol exploit.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-136)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L162-174)
```rust
        tgt_leaf_idx.set(manifest.chunks.last().map_or(0, |c| c.last_idx as i64));
        let total_chunks = manifest.chunks.len();

        let resume_point_opt = receiver.lock().as_mut().unwrap().previous_key_hash()?;
        let chunks = if let Some(resume_point) = resume_point_opt {
            manifest
                .chunks
                .into_iter()
                .skip_while(|chunk| chunk.last_key <= resume_point)
                .collect()
        } else {
            manifest.chunks
        };
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L228-228)
```rust
        tokio::task::spawn_blocking(move || receiver.lock().take().unwrap().finish()).await??;
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L29-51)
```rust
/// State snapshot backup manifest, representing a complete state view at specified version.
#[derive(Deserialize, Serialize)]
pub struct StateSnapshotBackup {
    /// Version at which this state snapshot is taken.
    pub version: Version,
    /// Epoch in which this state snapshot is taken.
    pub epoch: u64,
    /// Hash of the state tree root.
    pub root_hash: HashValue,
    /// All account blobs in chunks.
    pub chunks: Vec<StateSnapshotChunk>,
    /// BCS serialized
    /// `Tuple(TransactionInfoWithProof, LedgerInfoWithSignatures)`.
    ///   - The `TransactionInfoWithProof` is at `Version` above, and carries the same `root_hash`
    /// above; It proves that at specified version the root hash is as specified in a chain
    /// represented by the LedgerInfo below.
    ///   - The signatures on the `LedgerInfoWithSignatures` has a version greater than or equal to
    /// the version of this backup but is within the same epoch, so the signatures on it can be
    /// verified by the validator set in the same epoch, which can be provided by an
    /// `EpochStateBackup` recovered prior to this to the DB; Requiring it to be in the same epoch
    /// limits the requirement on such `EpochStateBackup` to no older than the same epoch.
    pub proof: FileHandle,
}
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L189-235)
```rust
    pub fn new<D: 'static + TreeReader<K> + TreeWriter<K>>(
        store: Arc<D>,
        version: Version,
        expected_root_hash: HashValue,
        async_commit: bool,
    ) -> Result<Self> {
        let tree_reader = Arc::clone(&store);
        let (finished, partial_nodes, previous_leaf) = if let Some(root_node) =
            tree_reader.get_node_option(&NodeKey::new_empty_path(version), "restore")?
        {
            info!("Previous restore is complete, checking root hash.");
            ensure!(
                root_node.hash() == expected_root_hash,
                "Previous completed restore has root hash {}, expecting {}",
                root_node.hash(),
                expected_root_hash,
            );
            (true, vec![], None)
        } else if let Some((node_key, leaf_node)) = tree_reader.get_rightmost_leaf(version)? {
            // If the system crashed in the middle of the previous restoration attempt, we need
            // to recover the partial nodes to the state right before the crash.
            (
                false,
                Self::recover_partial_nodes(tree_reader.as_ref(), version, node_key)?,
                Some(leaf_node),
            )
        } else {
            (
                false,
                vec![InternalInfo::new_empty(NodeKey::new_empty_path(version))],
                None,
            )
        };

        Ok(Self {
            store,
            version,
            partial_nodes,
            frozen_nodes: HashMap::new(),
            previous_leaf,
            num_keys_received: 0,
            expected_root_hash,
            finished,
            async_commit,
            async_commit_result: None,
        })
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```
