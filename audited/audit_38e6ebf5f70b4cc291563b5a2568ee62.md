# Audit Report

## Title
Consensus Notification Channel Termination Does Not Account for Pending Sync Requests, Causing Coordination Failure

## Summary
The `is_terminated()` check in `ConsensusNotificationHandler` only verifies if the underlying notification channel is closed, but does not check if there are active sync requests pending. This allows the channel to terminate while state-sync is still processing a sync request from consensus, breaking the coordination protocol between consensus and state-sync components.

## Finding Description

The `ConsensusNotificationHandler` implements `FusedStream` with an `is_terminated()` method that delegates to the underlying `consensus_listener`: [1](#0-0) 

However, the handler maintains a separate `consensus_sync_request` field that stores pending sync requests: [2](#0-1) 

The vulnerability occurs when:
1. Consensus sends a sync request (`sync_to_target` or `sync_for_duration`) to state-sync
2. State-sync receives and stores it in `consensus_sync_request`
3. Before completion, the consensus notification channel closes (e.g., during epoch transition at [3](#0-2) )
4. `is_terminated()` returns true, causing the driver's `select!` loop to stop matching on consensus notifications: [4](#0-3) 

5. State-sync continues processing the old sync request via periodic `check_sync_request_progress()` calls but cannot receive new consensus notifications
6. When satisfied, the response fails because the callback receiver was dropped with the old consensus components
7. Consensus never receives confirmation of sync completion, potentially causing it to wait indefinitely

The coordination protocol is broken: state-sync completes the synchronization but consensus has no way to know, leading to liveness issues.

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program criteria for "Validator node slowdowns". When the channel terminates with pending sync requests:

- Consensus may block waiting for a state-sync response that never arrives
- State-sync cannot receive new consensus notifications or commit notifications
- The validator node cannot make forward progress until manual intervention or restart
- This affects validator liveness and network participation

The issue becomes critical during epoch transitions where [5](#0-4)  shuts down processors and immediately calls `sync_to_target`, potentially racing with pending sync requests.

## Likelihood Explanation

This vulnerability has **moderate likelihood** of occurring because:

1. **Epoch transitions are frequent** - They happen regularly in the Aptos network
2. **Race condition window** - If a sync request is in flight during `shutdown_current_processor()`, the callback receiver gets dropped
3. **No defensive checks** - The code doesn't verify sync request state before marking the stream as terminated
4. **Cascading failures** - Once triggered, the node requires restart to recover

However, it requires specific timing: a sync request must be pending exactly when epoch transition occurs.

## Recommendation

Add a check in `is_terminated()` to account for pending sync requests:

```rust
impl FusedStream for ConsensusNotificationHandler {
    fn is_terminated(&self) -> bool {
        // Only report terminated if both the channel is closed 
        // AND there are no pending sync requests
        self.consensus_listener.is_terminated() 
            && !self.active_sync_request()
    }
}
```

Additionally, implement proper cleanup in epoch transitions to either:
1. Wait for pending sync requests to complete before shutting down processors
2. Explicitly cancel pending sync requests with error responses before channel closure
3. Maintain the notification channel across epoch boundaries

## Proof of Concept

```rust
#[tokio::test]
async fn test_terminated_with_pending_sync_request() {
    use aptos_consensus_notifications::*;
    use aptos_types::{ledger_info::*, block_info::BlockInfo};
    use futures::StreamExt;
    
    // Create consensus notification pair
    let (consensus_notifier, consensus_listener) = 
        new_consensus_notifier_listener_pair(1000);
    let time_service = TimeService::mock();
    
    let mut handler = ConsensusNotificationHandler::new(
        consensus_listener,
        time_service.clone()
    );
    
    // Send a sync target notification
    let target = LedgerInfoWithSignatures::new(
        LedgerInfo::new(BlockInfo::empty(), HashValue::zero()),
        AggregateSignature::empty()
    );
    
    let sync_future = consensus_notifier.sync_to_target(target.clone());
    
    // Wait for notification to be received
    tokio::time::sleep(Duration::from_millis(100)).await;
    if let Some(ConsensusNotification::SyncToTarget(notification)) = 
        handler.next().await 
    {
        // Initialize the sync request
        handler.initialize_sync_target_request(
            notification, 
            0, 
            target
        ).await.unwrap();
        
        // Sync request is now pending
        assert!(handler.active_sync_request());
    }
    
    // Drop the consensus_notifier to close the channel
    drop(consensus_notifier);
    drop(sync_future);
    
    // Wait for channel to fully terminate
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // BUG: is_terminated() returns true even though sync request is pending
    assert!(handler.is_terminated()); // This passes
    assert!(handler.active_sync_request()); // This also passes!
    
    // This demonstrates the vulnerability: the stream is terminated
    // but we still have a pending sync request that consensus is waiting for
}
```

## Notes

This vulnerability specifically affects the coordination between consensus and state-sync during epoch transitions or abnormal shutdowns. The `ConsensusNotificationListener` channel termination at [6](#0-5)  only checks the underlying `mpsc::UnboundedReceiver` state, not the higher-level synchronization protocol state maintained by the handler.

### Citations

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L212-221)
```rust
pub struct ConsensusNotificationHandler {
    // The listener for notifications from consensus
    consensus_listener: ConsensusNotificationListener,

    // The latest consensus sync request that has been received
    consensus_sync_request: Arc<Mutex<Option<ConsensusSyncRequest>>>,

    // The time service
    time_service: TimeService,
}
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L462-466)
```rust
impl FusedStream for ConsensusNotificationHandler {
    fn is_terminated(&self) -> bool {
        self.consensus_listener.is_terminated()
    }
}
```

**File:** consensus/src/epoch_manager.rs (L544-569)
```rust
    async fn initiate_new_epoch(&mut self, proof: EpochChangeProof) -> anyhow::Result<()> {
        let ledger_info = proof
            .verify(self.epoch_state())
            .context("[EpochManager] Invalid EpochChangeProof")?;
        info!(
            LogSchema::new(LogEvent::NewEpoch).epoch(ledger_info.ledger_info().next_block_epoch()),
            "Received verified epoch change",
        );

        // shutdown existing processor first to avoid race condition with state sync.
        self.shutdown_current_processor().await;
        *self.pending_blocks.lock() = PendingBlocks::new();
        // make sure storage is on this ledger_info too, it should be no-op if it's already committed
        // panic if this doesn't succeed since the current processors are already shutdown.
        self.execution_client
            .sync_to_target(ledger_info.clone())
            .await
            .context(format!(
                "[EpochManager] State sync to new epoch {}",
                ledger_info
            ))
            .expect("Failed to sync to new epoch");

        monitor!("reconfig", self.await_reconfig_notification().await);
        Ok(())
    }
```

**File:** consensus/src/epoch_manager.rs (L637-670)
```rust
    async fn shutdown_current_processor(&mut self) {
        if let Some(close_tx) = self.round_manager_close_tx.take() {
            // Release the previous RoundManager, especially the SafetyRule client
            let (ack_tx, ack_rx) = oneshot::channel();
            close_tx
                .send(ack_tx)
                .expect("[EpochManager] Fail to drop round manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop round manager");
        }
        self.round_manager_tx = None;

        if let Some(close_tx) = self.dag_shutdown_tx.take() {
            // Release the previous RoundManager, especially the SafetyRule client
            let (ack_tx, ack_rx) = oneshot::channel();
            close_tx
                .send(ack_tx)
                .expect("[EpochManager] Fail to drop DAG bootstrapper");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop DAG bootstrapper");
        }
        self.dag_shutdown_tx = None;

        // Shutdown the previous rand manager
        self.rand_manager_msg_tx = None;

        // Shutdown the previous secret share manager
        self.secret_share_manager_tx = None;

        // Shutdown the previous buffer manager, to release the SafetyRule client
        self.execution_client.end_epoch().await;

```

**File:** state-sync/state-sync-driver/src/driver.rs (L221-239)
```rust
        loop {
            ::futures::select! {
                notification = self.client_notification_listener.select_next_some() => {
                    self.handle_client_notification(notification).await;
                },
                notification = self.commit_notification_listener.select_next_some() => {
                    self.handle_snapshot_commit_notification(notification).await;
                }
                notification = self.consensus_notification_handler.select_next_some() => {
                    self.handle_consensus_or_observer_notification(notification).await;
                }
                notification = self.error_notification_listener.select_next_some() => {
                    self.handle_error_notification(notification).await;
                }
                _ = progress_check_interval.select_next_some() => {
                    self.drive_progress().await;
                }
            }
        }
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L279-283)
```rust
impl FusedStream for ConsensusNotificationListener {
    fn is_terminated(&self) -> bool {
        self.notification_receiver.is_terminated()
    }
}
```
