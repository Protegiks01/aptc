# Audit Report

## Title
Unauthenticated Backup Service Exposes Complete Blockchain State and Validator Configuration Data

## Summary
The backup service (`start_backup_service()`) starts an HTTP server without any authentication, authorization, TLS client certificate validation, or IP whitelisting mechanisms. In production deployments, this service is configured to bind to all network interfaces (0.0.0.0:6186), allowing any network peer to download the entire blockchain state, including validator configurations containing consensus public keys and network addresses. [1](#0-0) 

## Finding Description
The `start_backup_service()` function creates a warp HTTP server that exposes multiple endpoints for blockchain backup operations without implementing any access control mechanisms: [2](#0-1) 

These endpoints expose:

1. **Complete State Snapshots** - The `/state_snapshot/<version>` and `/state_snapshot_chunk/<version>/<start_idx>/<limit>` endpoints expose all state key-value pairs, including validator configurations: [3](#0-2) 

2. **Validator Configuration Data** - State snapshots include `ValidatorConfig` resources containing sensitive network topology information: [4](#0-3) 

3. **Transaction History** - The `/transactions/<start_version>/<num_transactions>` endpoint exposes complete transaction data with auxiliary information: [5](#0-4) 

4. **Database State** - The `/db_state` endpoint reveals current epoch and version information: [6](#0-5) 

While the default configuration binds to localhost only (127.0.0.1:6186), production deployments override this to bind to all interfaces: [7](#0-6) [8](#0-7) 

The Kubernetes service configuration exposes port 6186 within the cluster with no authentication layer: [9](#0-8) 

No authentication middleware exists in the request handling pipeline - only logging and error handling: [10](#0-9) 

## Impact Explanation
This vulnerability qualifies as **Low Severity** under the Aptos bug bounty program as it constitutes an information disclosure issue. However, it's important to note that:

1. **All blockchain data is public by design** - The blockchain state, transactions, and validator configurations are meant to be accessible to network participants
2. **Validator metadata is intentionally public** - Consensus public keys and network addresses must be published for the network to function
3. **This is working as designed** - The backup service is intended for operators to backup their node data

The security concern is not about exposing "private" data (as there is none), but rather:
- Lack of rate limiting could enable resource exhaustion attacks against validator nodes
- Unauthenticated access in misconfigured deployments violates the principle of least privilege
- Operational overhead for node operators who must implement infrastructure-level controls

This does NOT constitute Critical or High severity because:
- No funds can be stolen or minted
- No consensus violations occur
- No protocol-level security guarantees are broken
- The exposed data is inherently public

## Likelihood Explanation
**Likelihood: Low to Medium** depending on deployment configuration.

The default configuration (localhost binding) prevents external access, making exploitation impossible without deployment misconfigurations. However, the official Kubernetes/Terraform templates bind to 0.0.0.0, creating a window for exploitation if operators don't implement proper network segmentation.

An attacker would need:
1. Network connectivity to the backup service port (6186)
2. Knowledge that the service is running (easily determined by port scanning)
3. No additional infrastructure controls (firewall rules, VPNs, etc.)

The attack requires only basic HTTP client capabilities and no special privileges.

## Recommendation
While the exposed data is public by design, implementing defense-in-depth access controls would follow security best practices:

**Option 1: Add Authentication Middleware** (Similar to existing Aptos services)
Implement token-based authentication in the backup service, following the pattern used in other Aptos components. This would require modifying the route handlers to validate authentication tokens.

**Option 2: Document Security Requirements**
Add clear documentation to the backup service configuration warning operators that:
- The service should only be exposed on internal/trusted networks
- Infrastructure-level controls (firewalls, VPNs, security groups) are required
- Rate limiting should be implemented at the load balancer/proxy level

**Option 3: Disable by Default in Production**
Change the default behavior to require explicit opt-in for the backup service, with warnings about security implications.

**Recommended Implementation:**
Add a configuration option for authentication and update the deployment templates to include firewall rules:

```rust
// Add to StorageConfig
pub struct BackupServiceConfig {
    pub address: SocketAddr,
    pub enable_authentication: bool,
    pub auth_token: Option<String>,
}
```

## Proof of Concept
```bash
#!/bin/bash
# POC: Unauthenticated access to backup service endpoints

# Target: A fullnode with backup service exposed on 0.0.0.0:6186
BACKUP_SERVICE_URL="http://<node-ip>:6186"

# 1. Get database state (no authentication required)
curl "${BACKUP_SERVICE_URL}/db_state"

# 2. Download state snapshot at version 1000 (contains validator configs)
curl "${BACKUP_SERVICE_URL}/state_snapshot/1000" -o state_snapshot.bin

# 3. Download transaction data
curl "${BACKUP_SERVICE_URL}/transactions/0/1000" -o transactions.bin

# 4. Get epoch ending ledger infos (validator set information)
curl "${BACKUP_SERVICE_URL}/epoch_ending_ledger_infos/0/10" -o epochs.bin

# All requests succeed without any authentication
# The downloaded data includes ValidatorConfig resources with:
# - consensus_pubkey (BLS public keys)
# - network_addresses (P2P networking endpoints)
# - fullnode_addresses (State sync endpoints)
```

**Note:** This POC demonstrates the lack of access control but does not constitute a severe vulnerability since all exposed data is part of the public blockchain state by design.

### Citations

**File:** storage/backup/backup-service/src/lib.rs (L12-30)
```rust
pub fn start_backup_service(address: SocketAddr, db: Arc<AptosDB>) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler);

    let runtime = aptos_runtimes::spawn_named_runtime("backup".into(), None);

    // Ensure that we actually bind to the socket first before spawning the
    // server tasks. This helps in tests to prevent races where a client attempts
    // to make a request before the server task is actually listening on the
    // socket.
    //
    // Note: we need to enter the runtime context first to actually bind, since
    //       tokio TcpListener can only be bound inside a tokio context.
    let _guard = runtime.enter();
    let server = warp::serve(routes).bind(address);
    runtime.handle().spawn(server);
    info!("Backup service spawned.");
    runtime
}
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L27-147)
```rust
pub(crate) fn get_routes(backup_handler: BackupHandler) -> BoxedFilter<(impl Reply,)> {
    // GET db_state
    let bh = backup_handler.clone();
    let db_state = warp::path::end()
        .map(move || reply_with_bcs_bytes(DB_STATE, &bh.get_db_state()?))
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_range_proof/<version>/<end_key>
    let bh = backup_handler.clone();
    let state_range_proof = warp::path!(Version / HashValue)
        .map(move |version, end_key| {
            reply_with_bcs_bytes(
                STATE_RANGE_PROOF,
                &bh.get_account_state_range_proof(end_key, version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot/<version>
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_item_count/<version>
    let bh = backup_handler.clone();
    let state_item_count = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(
                STATE_ITEM_COUNT,
                &(bh.get_state_item_count(version)? as u64),
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot_chunk/<version>/<start_idx>/<limit>
    let bh = backup_handler.clone();
    let state_snapshot_chunk = warp::path!(Version / usize / usize)
        .map(move |version, start_idx, limit| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT_CHUNK, move |bh, sender| {
                bh.get_state_item_iter(version, start_idx, limit)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_root_proof/<version>
    let bh = backup_handler.clone();
    let state_root_proof = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(STATE_ROOT_PROOF, &bh.get_state_root_proof(version)?)
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET epoch_ending_ledger_infos/<start_epoch>/<end_epoch>/
    let bh = backup_handler.clone();
    let epoch_ending_ledger_infos = warp::path!(u64 / u64)
        .map(move |start_epoch, end_epoch| {
            reply_with_bytes_sender(&bh, EPOCH_ENDING_LEDGER_INFOS, move |bh, sender| {
                bh.get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transactions/<start_version>/<num_transactions>
    let bh = backup_handler.clone();
    let transactions = warp::path!(Version / usize)
        .map(move |start_version, num_transactions| {
            reply_with_bytes_sender(&bh, TRANSACTIONS, move |bh, sender| {
                bh.get_transaction_iter(start_version, num_transactions)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transaction_range_proof/<first_version>/<last_version>
    let bh = backup_handler;
    let transaction_range_proof = warp::path!(Version / Version)
        .map(move |first_version, last_version| {
            reply_with_bcs_bytes(
                TRANSACTION_RANGE_PROOF,
                &bh.get_transaction_range_proof(first_version, last_version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // Route by endpoint name.
    let routes = warp::any()
        .and(warp::path(DB_STATE).and(db_state))
        .or(warp::path(STATE_RANGE_PROOF).and(state_range_proof))
        .or(warp::path(STATE_SNAPSHOT).and(state_snapshot))
        .or(warp::path(STATE_ITEM_COUNT).and(state_item_count))
        .or(warp::path(STATE_SNAPSHOT_CHUNK).and(state_snapshot_chunk))
        .or(warp::path(STATE_ROOT_PROOF).and(state_root_proof))
        .or(warp::path(EPOCH_ENDING_LEDGER_INFOS).and(epoch_ending_ledger_infos))
        .or(warp::path(TRANSACTIONS).and(transactions))
        .or(warp::path(TRANSACTION_RANGE_PROOF).and(transaction_range_proof));

    // Serve all routes for GET only.
    warp::get()
        .and(routes)
        .with(warp::log::custom(|info| {
            let endpoint = info.path().split('/').nth(1).unwrap_or("-");
            LATENCY_HISTOGRAM.observe_with(
                &[endpoint, info.status().as_str()],
                info.elapsed().as_secs_f64(),
            )
        }))
        .boxed()
}
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L40-109)
```rust
    /// Gets an iterator that yields a range of transactions.
    pub fn get_transaction_iter(
        &self,
        start_version: Version,
        num_transactions: usize,
    ) -> Result<
        impl Iterator<
                Item = Result<(
                    Transaction,
                    PersistedAuxiliaryInfo,
                    TransactionInfo,
                    Vec<ContractEvent>,
                    WriteSet,
                )>,
            > + '_,
    > {
        let txn_iter = self
            .ledger_db
            .transaction_db()
            .get_transaction_iter(start_version, num_transactions)?;
        let mut txn_info_iter = self
            .ledger_db
            .transaction_info_db()
            .get_transaction_info_iter(start_version, num_transactions)?;
        let mut event_vec_iter = self
            .ledger_db
            .event_db()
            .get_events_by_version_iter(start_version, num_transactions)?;
        let mut write_set_iter = self
            .ledger_db
            .write_set_db()
            .get_write_set_iter(start_version, num_transactions)?;
        let mut persisted_aux_info_iter = self
            .ledger_db
            .persisted_auxiliary_info_db()
            .get_persisted_auxiliary_info_iter(start_version, num_transactions)?;

        let zipped = txn_iter.enumerate().map(move |(idx, txn_res)| {
            let version = start_version + idx as u64; // overflow is impossible since it's check upon txn_iter construction.

            let txn = txn_res?;
            let txn_info = txn_info_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "TransactionInfo not found when Transaction exists, version {}",
                    version
                ))
            })??;
            let event_vec = event_vec_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "Events not found when Transaction exists., version {}",
                    version
                ))
            })??;
            let write_set = write_set_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "WriteSet not found when Transaction exists, version {}",
                    version
                ))
            })??;
            let persisted_aux_info = persisted_aux_info_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "PersistedAuxiliaryInfo not found when Transaction exists, version {}",
                    version
                ))
            })??;
            BACKUP_TXN_VERSION.set(version as i64);
            Ok((txn, persisted_aux_info, txn_info, event_vec, write_set))
        });
        Ok(zipped)
    }
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L144-162)
```rust
    /// Iterate through items in a state snapshot
    pub fn get_state_item_iter(
        &self,
        version: Version,
        start_idx: usize,
        limit: usize,
    ) -> Result<impl Iterator<Item = Result<(StateKey, StateValue)>> + Send + use<>> {
        let iterator = self
            .state_store
            .get_state_key_and_value_iter(version, start_idx)?
            .take(limit)
            .enumerate()
            .map(move |(idx, res)| {
                BACKUP_STATE_SNAPSHOT_VERSION.set(version as i64);
                BACKUP_STATE_SNAPSHOT_LEAF_IDX.set((start_idx + idx) as i64);
                res
            });
        Ok(Box::new(iterator))
    }
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L174-184)
```rust
    /// Gets the epoch, committed version, and synced version of the DB.
    pub fn get_db_state(&self) -> Result<Option<DbState>> {
        Ok(self
            .ledger_db
            .metadata_db()
            .get_latest_ledger_info_option()
            .map(|li| DbState {
                epoch: li.ledger_info().epoch(),
                committed_version: li.ledger_info().version(),
            }))
    }
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L160-168)
```text
    /// Validator info stored in validator address.
    struct ValidatorConfig has key, copy, store, drop {
        consensus_pubkey: vector<u8>,
        network_addresses: vector<u8>,
        // to make it compatible with previous definition, remove later
        fullnode_addresses: vector<u8>,
        // Index in the active set if the validator corresponding to this stake pool is active.
        validator_index: u64,
    }
```

**File:** config/src/config/storage_config.rs (L433-436)
```rust
impl Default for StorageConfig {
    fn default() -> StorageConfig {
        StorageConfig {
            backup_service_address: SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 6186),
```

**File:** terraform/helm/fullnode/files/fullnode-base.yaml (L67-68)
```yaml
storage:
  backup_service_address: "0.0.0.0:6186"
```

**File:** terraform/helm/fullnode/templates/service.yaml (L42-56)
```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "aptos-fullnode.fullname" . }}
  labels:
    {{- include "aptos-fullnode.labels" . | nindent 4 }}
spec:
  selector:
    {{- include "aptos-fullnode.selectorLabels" . | nindent 4 }}
    app.kubernetes.io/name: fullnode
  ports:
  - name: backup
    port: 6186
  - name: metrics
    port: 9101
```

**File:** storage/backup/backup-service/src/handlers/utils.rs (L82-97)
```rust
/// Return 500 on any error raised by the request handler.
pub(super) fn unwrap_or_500(result: DbResult<Box<dyn Reply>>) -> Box<dyn Reply> {
    match result {
        Ok(resp) => resp,
        Err(e) => {
            warn!("Request handler exception: {:#}", e);
            Box::new(warp::http::StatusCode::INTERNAL_SERVER_ERROR)
        },
    }
}

/// Return 400 on any rejections (parameter parsing errors).
pub(super) async fn handle_rejection(err: Rejection) -> DbResult<impl Reply, Infallible> {
    warn!("bad request: {:?}", err);
    Ok(warp::http::StatusCode::BAD_REQUEST)
}
```
