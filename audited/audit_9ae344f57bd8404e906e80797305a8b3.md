# Audit Report

## Title
Memory Exhaustion in State Snapshot Proof Deserialization During Backup Restore

## Summary
The backup restore system loads entire proof files into memory before validating their size, allowing an attacker to cause memory exhaustion by providing maliciously crafted backup files with gigabyte-sized proof data. This affects nodes performing state snapshot restoration and can lead to crashes or out-of-memory conditions.

## Finding Description

The state snapshot backup system stores cryptographic proofs in separate files referenced by `FileHandle` objects. During restoration, these proof files are deserialized to verify state integrity. However, the deserialization path reads entire files into memory before any size validation occurs.

**Attack Flow:**

1. **Malicious Backup Creation**: An attacker creates a state snapshot backup with an extremely large proof file (e.g., 2-4 GB) containing a BCS-serialized tuple of `(TransactionInfoWithProof, LedgerInfoWithSignatures)` with maliciously oversized vectors:
   - `TransactionInfoWithProof` contains `AccumulatorProof` with a `siblings: Vec<HashValue>` field that could have millions of entries
   - `LedgerInfoWithSignatures` contains `AggregateSignature` with a `validator_bitmask: BitVec` backed by a `Vec<u8>` that could be excessively large

2. **Restore Initiation**: A node operator attempts to restore from this backup using the state snapshot restore command.

3. **Unchecked File Read**: The system calls `load_bcs_file(&manifest.proof)` which executes: [1](#0-0) 
   
   This calls `read_all()` which reads the entire file into memory: [2](#0-1) 

4. **Memory Exhaustion**: Before any size validation, `file.read_to_end(&mut bytes)` allocates memory for the entire multi-gigabyte file, causing memory exhaustion.

5. **Late Validation**: Size checks only occur during BCS deserialization (e.g., `BitVec` checks `MAX_BUCKETS` limit) or proof verification (e.g., `AccumulatorProof::verify()` checks `MAX_ACCUMULATOR_PROOF_DEPTH`), but by this point the memory has already been allocated. [3](#0-2) [4](#0-3) 

**Vulnerable Code Paths:**

The main proof deserialization occurs in the restore controller: [5](#0-4) 

Additionally, chunk proofs are loaded with the same vulnerability: [6](#0-5) 

The file opening operation has no size validation: [7](#0-6) 

**Broken Invariants:**
- **Resource Limits**: "All operations must respect gas, storage, and computational limits" - violated by unbounded memory allocation
- **Node Availability**: Nodes can crash during legitimate restore operations if provided malicious backups

## Impact Explanation

This vulnerability qualifies as **Medium Severity** based on Aptos bug bounty criteria:

1. **State inconsistencies requiring intervention**: Nodes crash during restore, preventing state synchronization completion
2. **Validator node slowdowns/crashes**: If validators attempt to restore from compromised backups, they experience memory exhaustion
3. **Limited DoS impact**: The attack requires operators to use malicious backup sources, limiting scope

The impact is not Critical because:
- Does not result in funds loss or consensus safety violations
- Requires node operators to actively restore from attacker-controlled backups
- Recovery is possible by restarting the node and using valid backup sources

However, it represents a real operational security risk:
- Nodes performing disaster recovery could be crashed
- Malicious backup hosting services could weaponize this
- Multiple concurrent restore attempts multiply the memory pressure

## Likelihood Explanation

**Likelihood: Medium to High**

**Factors increasing likelihood:**
1. **Common Operation**: State snapshot restore is a standard operation for new validators and nodes recovering from failures
2. **External Data Dependency**: Backup files may come from untrusted or compromised sources (cloud storage, backup services, peers)
3. **No Authentication**: The code validates proof correctness but not file size before loading
4. **Easy to Exploit**: Creating oversized BCS files is straightforward - just serialize large vectors
5. **Concurrent Downloads**: The restore uses concurrent downloads (buffered streams), potentially loading multiple large files simultaneously and multiplying memory impact

**Factors decreasing likelihood:**
1. **Requires Backup Access**: Attacker must either compromise backup storage or convince operators to use malicious backups
2. **Not Persistent**: Effect is temporary - node can recover by restarting with valid backups

## Recommendation

**Implement file size validation before reading:**

```rust
// In storage/backup/backup-cli/src/utils/storage_ext.rs

const MAX_PROOF_FILE_SIZE: u64 = 10 * 1024 * 1024; // 10 MB reasonable for proofs
const MAX_CHUNK_PROOF_SIZE: u64 = 5 * 1024 * 1024; // 5 MB for chunk proofs

async fn read_all_with_limit(&self, file_handle: &FileHandleRef, max_size: u64) -> Result<Vec<u8>> {
    let mut file = self.open_for_read(file_handle).await?;
    
    // Check file size via metadata if available
    if let Ok(metadata) = file.get_ref().metadata().await {
        ensure!(
            metadata.len() <= max_size,
            "File size {} exceeds maximum allowed size {}",
            metadata.len(),
            max_size
        );
    }
    
    // Use take() to enforce size limit during read
    let mut limited_reader = file.take(max_size + 1);
    let mut bytes = Vec::new();
    limited_reader.read_to_end(&mut bytes).await?;
    
    ensure!(
        bytes.len() as u64 <= max_size,
        "File content size {} exceeds maximum allowed size {}",
        bytes.len(),
        max_size
    );
    
    Ok(bytes)
}

async fn load_bcs_file_with_limit<T: DeserializeOwned>(
    &self, 
    file_handle: &FileHandleRef,
    max_size: u64
) -> Result<T> {
    Ok(bcs::from_bytes(&self.read_all_with_limit(file_handle, max_size).await?)?)
}
```

**Update call sites:**
```rust
// In restore.rs for main proof
let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
    self.storage.load_bcs_file_with_limit(&manifest.proof, MAX_PROOF_FILE_SIZE).await?;

// For chunk proofs
let proof = storage.load_bcs_file_with_limit(&chunk.proof, MAX_CHUNK_PROOF_SIZE).await?;
```

**Additional hardening:**
1. Add memory budget tracking across concurrent downloads
2. Implement streaming BCS deserialization for large structures (longer-term)
3. Add configuration option for operators to adjust limits based on their environment

## Proof of Concept

```rust
// This PoC can be added to storage/backup/backup-cli/src/backup_types/state_snapshot/tests.rs

#[tokio::test]
async fn test_memory_exhaustion_attack() {
    use aptos_crypto::HashValue;
    use aptos_types::proof::AccumulatorProof;
    use aptos_types::transaction::TransactionInfo;
    use std::io::Write;
    use tempfile::TempDir;
    
    // Create a malicious proof with oversized siblings vector
    let mut huge_siblings = Vec::new();
    for _ in 0..1_000_000 { // 1 million hash values = ~32 MB
        huge_siblings.push(HashValue::random());
    }
    
    let malicious_proof = AccumulatorProof::<TestAccumulatorHasher>::new(huge_siblings);
    let txn_info = TransactionInfo::new(
        HashValue::random(),
        HashValue::random(),
        HashValue::random(),
        Some(HashValue::random()),
        0,
        crate::transaction::ExecutionStatus::Success,
    );
    
    let txn_info_with_proof = TransactionInfoWithProof::new(malicious_proof, txn_info);
    let li = LedgerInfoWithSignatures::new(
        LedgerInfo::new(BlockInfo::empty(), HashValue::zero()),
        AggregateSignature::empty()
    );
    
    // Serialize to BCS - this will be ~32+ MB
    let malicious_data = bcs::to_bytes(&(txn_info_with_proof, li)).unwrap();
    println!("Malicious proof size: {} bytes", malicious_data.len());
    
    // Write to a temporary file
    let temp_dir = TempDir::new().unwrap();
    let proof_file_path = temp_dir.path().join("malicious_proof.bcs");
    let mut file = std::fs::File::create(&proof_file_path).unwrap();
    file.write_all(&malicious_data).unwrap();
    
    // Attempt to load - this will consume excessive memory
    let storage = Arc::new(LocalFs::new(temp_dir.path().to_path_buf()));
    let file_handle = "malicious_proof.bcs".to_string();
    
    // This should ideally fail with size limit error, but currently
    // it will allocate all the memory before failing on proof verification
    let result: Result<(TransactionInfoWithProof, LedgerInfoWithSignatures)> = 
        storage.load_bcs_file(&file_handle).await;
    
    // Currently this succeeds in loading (memory exhaustion risk!)
    // After fix, this should fail with file size limit error
    match result {
        Ok(_) => println!("WARNING: Successfully loaded oversized proof - memory exhaustion risk!"),
        Err(e) => println!("Expected failure: {}", e),
    }
}
```

**To demonstrate actual memory impact:**
1. Increase `huge_siblings` to 10-50 million entries (320 MB - 1.6 GB)
2. Run with memory profiling: `valgrind --tool=massif` or `/usr/bin/time -v`
3. Observe memory spike during `read_to_end()` call before deserialization validation
4. With concurrent downloads enabled, run multiple restore operations simultaneously to multiply the effect

## Notes

This vulnerability affects all backup storage backends (LocalFS, CommandAdapter) since they all use the same `BackupStorageExt::load_bcs_file()` method. The issue is particularly concerning because:

1. **Compound Effect**: State snapshots can have many chunks, each with its own proof file, allowing multiple concurrent large file reads
2. **Trust Boundary**: Backup files cross trust boundaries - they may come from third-party services, cloud storage, or peer nodes
3. **Limited Validation**: While the cryptographic proofs are verified after loading, there's no early-exit for obvious resource abuse

The recommended fix adds defense-in-depth by validating file sizes before allocating memory, preventing resource exhaustion attacks while maintaining the existing cryptographic verification guarantees.

### Citations

**File:** storage/backup/backup-cli/src/utils/storage_ext.rs (L24-29)
```rust
    async fn read_all(&self, file_handle: &FileHandleRef) -> Result<Vec<u8>> {
        let mut file = self.open_for_read(file_handle).await?;
        let mut bytes = Vec::new();
        file.read_to_end(&mut bytes).await?;
        Ok(bytes)
    }
```

**File:** storage/backup/backup-cli/src/utils/storage_ext.rs (L31-33)
```rust
    async fn load_bcs_file<T: DeserializeOwned>(&self, file_handle: &FileHandleRef) -> Result<T> {
        Ok(bcs::from_bytes(&self.read_all(file_handle).await?)?)
    }
```

**File:** crates/aptos-bitvec/src/lib.rs (L247-249)
```rust
        if v.len() > MAX_BUCKETS {
            return Err(D::Error::custom(format!("BitVec too long: {}", v.len())));
        }
```

**File:** types/src/proof/definition.rs (L74-79)
```rust
        ensure!(
            self.siblings.len() <= MAX_ACCUMULATOR_PROOF_DEPTH,
            "Accumulator proof has more than {} ({}) siblings.",
            MAX_ACCUMULATOR_PROOF_DEPTH,
            self.siblings.len()
        );
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L125-127)
```rust
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L192-192)
```rust
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
```

**File:** storage/backup/backup-cli/src/storage/local_fs/mod.rs (L98-109)
```rust
    async fn open_for_read(
        &self,
        file_handle: &FileHandleRef,
    ) -> Result<Box<dyn AsyncRead + Send + Unpin>> {
        let path = self.dir.join(file_handle);
        let file = OpenOptions::new()
            .read(true)
            .open(&path)
            .await
            .err_notes(&path)?;
        Ok(Box::new(file))
    }
```
