# Audit Report

## Title
Consensus Observer Permanent Liveness Failure Due to Unhandled State Sync Errors in Fallback Mode

## Summary
When `sync_for_fallback()` spawns an async task to perform state synchronization, if `sync_for_duration()` returns an error, the task logs the error and returns early without sending a `StateSyncNotification::FallbackSyncCompleted` notification. This causes the consensus observer to remain indefinitely stuck in fallback mode, as the `fallback_sync_handle` is never cleared and `in_fallback_mode()` continues to return true, preventing all subsequent progress checks and message processing.

## Finding Description

The vulnerability exists in the error handling path of the `sync_for_fallback()` function. When this function is called, it spawns an async task that performs state synchronization for a specified duration: [1](#0-0) 

The critical flaw occurs when `execution_client.sync_for_duration()` returns an error. In this case, the task logs the error and returns immediately without sending the completion notification: [2](#0-1) 

However, the `fallback_sync_handle` has already been set before the task executes: [3](#0-2) 

The observer checks if it's in fallback mode during its periodic progress checks: [4](#0-3) 

The `in_fallback_mode()` method returns true as long as `fallback_sync_handle` is set: [5](#0-4) 

The only way to clear the fallback handle is through the notification handler: [6](#0-5) 

Since the notification is never sent when an error occurs, `clear_active_fallback_sync()` is never called, and the observer remains permanently stuck in fallback mode, unable to process any consensus messages or make progress.

**Attack Scenario:**

State sync failures can occur due to multiple realistic conditions:
1. Network connectivity issues between the observer and state sync peers
2. State sync timeout errors from the notification sender
3. Executor reset failures after state sync attempts
4. State sync driver being overwhelmed or experiencing internal errors [7](#0-6) [8](#0-7) 

**Invariant Violation:**

This bug violates the **Consensus Liveness** invariant. Consensus observer nodes must be able to recover from temporary failures and continue processing consensus messages. By getting permanently stuck in fallback mode, the observer can no longer participate in consensus observation, effectively removing it from the network.

## Impact Explanation

**Severity: High** (per Aptos Bug Bounty criteria: "Validator node slowdowns" and "Significant protocol violations")

This vulnerability causes **permanent liveness failure** of consensus observer nodes:

1. **Node Unavailability**: Once stuck in fallback mode, the observer stops processing all consensus messages indefinitely
2. **No Automatic Recovery**: There is no timeout mechanism or alternative recovery path
3. **Cascading Impact**: If multiple observers encounter this issue during network instability, it reduces the overall redundancy and reliability of the consensus observation network
4. **Silent Failure**: The node continues running but is effectively non-functional, potentially masking the issue from operators

While this doesn't directly affect validator consensus (observers don't participate in voting), it impacts the overall network health and monitoring infrastructure that operators rely on.

## Likelihood Explanation

**Likelihood: High**

State sync failures are common in production blockchain environments due to:

1. **Network Instability**: Temporary network partitions, high latency, or peer disconnections can cause state sync operations to fail
2. **Resource Contention**: During periods of high network load, state sync may timeout or fail to respond within expected timeframes
3. **Synchronization Issues**: When the observer falls significantly behind (triggering fallback mode), the state sync system may already be under stress
4. **No Retry Logic**: The current implementation has no retry mechanism for failed state sync operations

The identical vulnerability pattern also exists in `sync_to_commit()`: [9](#0-8) 

This increases the overall likelihood as both code paths can trigger the permanent stuck state.

## Recommendation

**Fix: Ensure notifications are sent even when errors occur, or implement proper cleanup on failure.**

The task should always clear the fallback/commit sync handles, even when errors occur. Here are two recommended approaches:

**Option 1 (Recommended): Always send notification with error status**
Modify the notification enum to include error cases, and always send a notification regardless of success or failure. The handler can then decide how to proceed.

**Option 2: Clear handle on error**
Add cleanup logic to clear the sync handle when errors occur. This could be done by:
- Passing a channel to signal completion/failure back to the manager
- Using a defer-like pattern to ensure cleanup
- Adding a timeout mechanism that auto-clears handles after a maximum duration

**Fixed code pattern for sync_for_fallback():**

```rust
// After the error logging at line 159, add cleanup:
if let Err(error) = sync_notification_sender.send(
    StateSyncNotification::fallback_sync_completed(/* default or last known ledger info */)
) {
    error!(LogSchema::new(LogEntry::ConsensusObserver)
        .message(&format!("Failed to send fallback failure notification: {:?}", error)));
}
// Then clear metrics and return
```

Or implement a separate error notification path that properly clears the handle in the manager.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use crate::pipeline::execution_client::DummyExecutionClient;
    use aptos_types::{aggregate_signature::AggregateSignature, ledger_info::LedgerInfo};
    use std::sync::Arc;

    // Mock execution client that always fails sync_for_duration
    struct FailingExecutionClient;

    #[async_trait::async_trait]
    impl TExecutionClient for FailingExecutionClient {
        async fn sync_for_duration(
            &self,
            _duration: Duration,
        ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
            // Simulate a state sync failure
            Err(StateSyncError::from(anyhow::anyhow!("Simulated sync failure")))
        }
        
        // Implement other required trait methods with defaults...
    }

    #[tokio::test]
    async fn test_fallback_mode_stuck_on_sync_error() {
        // Create a state sync manager with a failing execution client
        let consensus_observer_config = ConsensusObserverConfig::default();
        let (state_sync_notification_sender, mut notification_receiver) = 
            tokio::sync::mpsc::unbounded_channel();
        let mut state_sync_manager = StateSyncManager::new(
            consensus_observer_config,
            Arc::new(FailingExecutionClient),
            state_sync_notification_sender,
        );

        // Verify observer is not initially in fallback mode
        assert!(!state_sync_manager.in_fallback_mode());

        // Trigger fallback sync
        state_sync_manager.sync_for_fallback();

        // Observer should now be in fallback mode
        assert!(state_sync_manager.in_fallback_mode());

        // Wait for the task to complete (it will fail)
        tokio::time::sleep(Duration::from_millis(200)).await;

        // Try to receive notification (should timeout as none was sent)
        let notification_result = tokio::time::timeout(
            Duration::from_millis(100),
            notification_receiver.recv()
        ).await;
        
        // Verify no notification was received
        assert!(notification_result.is_err(), "No notification should be sent on error");

        // VULNERABILITY: Observer remains stuck in fallback mode indefinitely
        assert!(state_sync_manager.in_fallback_mode(), 
            "BUG: Observer stuck in fallback mode after sync failure!");
    }
}
```

**Notes**

This vulnerability represents a critical operational risk for consensus observer deployments. The lack of any recovery mechanism means that once triggered, manual intervention (node restart) is required to restore functionality. The same error handling pattern exists in both `sync_for_fallback()` and `sync_to_commit()`, doubling the attack surface. Production environments experiencing network instability or state sync issues will likely trigger this bug, leading to degraded network observability and monitoring capabilities.

### Citations

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L100-103)
```rust
    /// Returns true iff state sync is currently executing in fallback mode
    pub fn in_fallback_mode(&self) -> bool {
        self.fallback_sync_handle.is_some()
    }
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L136-186)
```rust
        tokio::spawn(Abortable::new(
            async move {
                // Update the state sync metrics now that we're syncing for the fallback
                metrics::set_gauge_with_label(
                    &metrics::OBSERVER_STATE_SYNC_EXECUTING,
                    metrics::STATE_SYNCING_FOR_FALLBACK,
                    1, // We're syncing for the fallback
                );

                // Get the fallback duration
                let fallback_duration =
                    Duration::from_millis(consensus_observer_config.observer_fallback_duration_ms);

                // Sync for the fallback duration
                let latest_synced_ledger_info = match execution_client
                    .clone()
                    .sync_for_duration(fallback_duration)
                    .await
                {
                    Ok(latest_synced_ledger_info) => latest_synced_ledger_info,
                    Err(error) => {
                        error!(LogSchema::new(LogEntry::ConsensusObserver)
                            .message(&format!("Failed to sync for fallback! Error: {:?}", error)));
                        return;
                    },
                };

                // Notify consensus observer that we've synced for the fallback
                let state_sync_notification =
                    StateSyncNotification::fallback_sync_completed(latest_synced_ledger_info);
                if let Err(error) = sync_notification_sender.send(state_sync_notification) {
                    error!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Failed to send state sync notification for fallback! Error: {:?}",
                            error
                        ))
                    );
                }

                // Clear the state sync metrics now that we're done syncing
                metrics::set_gauge_with_label(
                    &metrics::OBSERVER_STATE_SYNC_EXECUTING,
                    metrics::STATE_SYNCING_FOR_FALLBACK,
                    0, // We're no longer syncing for the fallback
                );
            },
            abort_registration,
        ));

        // Save the sync task handle
        self.fallback_sync_handle = Some(DropGuard::new(abort_handle));
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L219-231)
```rust
                if let Err(error) = execution_client
                    .clone()
                    .sync_to_target(commit_decision.commit_proof().clone())
                    .await
                {
                    error!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Failed to sync to commit decision: {:?}! Error: {:?}",
                            commit_decision, error
                        ))
                    );
                    return;
                }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L172-177)
```rust
        // If we've fallen back to state sync, we should wait for it to complete
        if self.state_sync_manager.in_fallback_mode() {
            info!(LogSchema::new(LogEntry::ConsensusObserver)
                .message("Waiting for state sync to complete fallback syncing!",));
            return;
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L916-965)
```rust
    /// Processes the state sync notification for the fallback sync
    async fn process_fallback_sync_notification(
        &mut self,
        latest_synced_ledger_info: LedgerInfoWithSignatures,
    ) {
        // Get the epoch and round for the latest synced ledger info
        let ledger_info = latest_synced_ledger_info.ledger_info();
        let epoch = ledger_info.epoch();
        let round = ledger_info.round();

        // Log the state sync notification
        info!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Received state sync notification for fallback completion! Epoch {}, round: {}!",
                epoch, round
            ))
        );

        // Verify that there is an active fallback sync
        if !self.state_sync_manager.in_fallback_mode() {
            // Log the error and return early
            error!(LogSchema::new(LogEntry::ConsensusObserver).message(
                "Failed to process fallback sync notification! No active fallback sync found!"
            ));
            return;
        }

        // Reset the fallback manager state
        self.observer_fallback_manager
            .reset_syncing_progress(&latest_synced_ledger_info);

        // Update the root with the latest synced ledger info
        self.observer_block_data
            .lock()
            .update_root(latest_synced_ledger_info);

        // If the epoch has changed, end the current epoch and start the latest one
        let current_epoch_state = self.get_epoch_state();
        if epoch > current_epoch_state.epoch {
            // Wait for the latest epoch to start
            self.execution_client.end_epoch().await;
            self.wait_for_epoch_start().await;
        };

        // Reset the pending block state
        self.clear_pending_block_state().await;

        // Reset the state sync manager for the synced fallback
        self.state_sync_manager.clear_active_fallback_sync();
    }
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L24-32)
```rust
#[derive(Clone, Debug, Deserialize, Error, PartialEq, Eq, Serialize)]
pub enum Error {
    #[error("Notification failed: {0}")]
    NotificationError(String),
    #[error("Hit the timeout waiting for state sync to respond to the notification!")]
    TimeoutWaitingForStateSync,
    #[error("Unexpected error encountered: {0}")]
    UnexpectedErrorEncountered(String),
}
```

**File:** consensus/src/state_computer.rs (L132-174)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        // Grab the logical time lock
        let mut latest_logical_time = self.write_mutex.lock().await;

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by the BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // Inject an error for fail point testing
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Invoke state sync to synchronize for the specified duration. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_for_duration",
            self.state_sync_notifier.sync_for_duration(duration).await
        );

        // Update the latest logical time
        if let Ok(latest_synced_ledger_info) = &result {
            let ledger_info = latest_synced_ledger_info.ledger_info();
            let synced_logical_time = LogicalTime::new(ledger_info.epoch(), ledger_info.round());
            *latest_logical_time = synced_logical_time;
        }

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```
