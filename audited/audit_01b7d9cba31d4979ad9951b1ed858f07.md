# Audit Report

## Title
Peer Reputation Score Reset via Garbage Collection Timing Manipulation

## Summary
Malicious peers can evade the reputation-based defense system by exploiting the garbage collection mechanism to reset their peer scores from low values (indicating past malicious behavior) back to the default starting score of 50.0, allowing them to continue serving data despite previous detection.

## Finding Description

The garbage collection logic in `garbage_collect_peer_states()` removes all peer state data for disconnected peers, including their accumulated reputation scores. When a malicious peer reconnects, a fresh `PeerState` is created with `STARTING_SCORE` (50.0), effectively erasing all historical evidence of malicious behavior. [1](#0-0) 

The GC retrieves a snapshot of currently connected peers and removes all others from the peer state map: [2](#0-1) 

When a peer reconnects and is polled, `update_summary()` creates a new state with default scoring if the peer doesn't exist: [3](#0-2) 

The new `PeerState` is initialized with `STARTING_SCORE`: [4](#0-3) 

**Attack Flow:**
1. Malicious peer performs bad actions (sends invalid proofs, malicious data, causes proof verification errors)
2. Peer's score drops below `IGNORE_PEER_THRESHOLD` (25.0) through the scoring system
3. Peer is now ignored and cannot service requests
4. Peer disconnects from the network
5. GC runs periodically (every ~100ms by default) and removes the peer's state including its low score
6. Peer reconnects to the network
7. When polled or when sending data, `update_summary()` creates a fresh `PeerState` with score of 50.0
8. Malicious peer successfully evades reputation system and can continue attacks [5](#0-4) 

The existing test `disconnected_peers_garbage_collection` confirms this behavior but does not verify score persistence: [6](#0-5) 

## Impact Explanation

**Medium Severity** - This vulnerability allows malicious peers to evade reputation-based defenses, leading to:

1. **State Sync Degradation**: Nodes may repeatedly receive bad data from peers who reset their scores, causing state sync delays and failures
2. **Resource Exhaustion**: Nodes waste bandwidth and computation repeatedly requesting from malicious peers who appear legitimate
3. **Network Reliability**: The reputation system's effectiveness is undermined, reducing overall network health

While this does not directly cause consensus failures or fund loss, it significantly degrades the state synchronization system's security guarantees. Malicious peers can maintain continuous disruption by cycling through disconnect-reconnect patterns every few minutes, preventing proper peer blacklisting.

This meets the **Medium severity** criteria: "State inconsistencies requiring intervention" - operators would need to manually ban peers at the network level as the reputation system can be circumvented.

## Likelihood Explanation

**High Likelihood**:
- Attack requires only network-level peer disconnect/reconnect capability (no validator privileges needed)
- GC runs predictably every 100ms, making timing straightforward
- No rate limiting on reconnections exists in the codebase
- Attacker can automate the exploit with simple scripts
- The scoring thresholds are well-defined constants, making the attack deterministic

The attack is practical because:
1. Peer disconnection/reconnection is normal network behavior
2. No authentication or penalty persists across connections
3. The 100ms GC interval is frequent enough to exploit regularly

## Recommendation

Implement persistent peer reputation tracking across connection sessions:

**Solution 1: Score Decay with Time-Based Memory**
```rust
pub struct PeerState {
    // ... existing fields ...
    score: f64,
    last_disconnect_time: Option<Instant>,
    reconnection_count: u32,
}

impl PeerState {
    pub fn new_with_history(
        data_client_config: Arc<AptosDataClientConfig>,
        previous_score: Option<f64>,
        last_disconnect_time: Option<Instant>,
    ) -> Self {
        let score = if let (Some(prev_score), Some(disconnect_time)) = (previous_score, last_disconnect_time) {
            // Gradually decay penalty over time (e.g., recover 10 points per hour)
            let time_since_disconnect = Instant::now().duration_since(disconnect_time);
            let recovery = time_since_disconnect.as_secs() as f64 / 360.0; // 10 points per hour
            f64::min(prev_score + recovery, STARTING_SCORE)
        } else {
            STARTING_SCORE
        };
        
        Self {
            data_client_config,
            score,
            // ... initialize other fields ...
        }
    }
}
```

**Solution 2: Maintain Disconnected Peer History**
```rust
pub struct PeerStates {
    data_client_config: Arc<AptosDataClientConfig>,
    peer_to_state: Arc<DashMap<PeerNetworkId, PeerState>>,
    // Add history tracking
    disconnected_peer_history: Arc<DashMap<PeerNetworkId, (f64, Instant)>>, // (score, disconnect_time)
}

pub fn garbage_collect_peer_states(&self, connected_peers: HashSet<PeerNetworkId>) {
    // Store scores before removing
    self.peer_to_state.iter().for_each(|entry| {
        if !connected_peers.contains(entry.key()) {
            let score = entry.value().get_score();
            if score < STARTING_SCORE {
                self.disconnected_peer_history.insert(
                    *entry.key(),
                    (score, Instant::now())
                );
            }
        }
    });
    
    self.peer_to_state.retain(|peer_network_id, _| connected_peers.contains(peer_network_id));
    
    // Clean up old history (e.g., after 24 hours)
    self.disconnected_peer_history.retain(|_, (_, time)| {
        time.elapsed() < Duration::from_secs(86400)
    });
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_score_reset_via_reconnection() {
    use crate::{
        client::AptosDataClient,
        interface::{AptosDataClientInterface, ResponseError},
        peer_states::ErrorType,
        priority::PeerPriority,
        tests::{mock::MockNetwork, utils},
    };
    use aptos_config::config::AptosDataClientConfig;

    // Create base config and data client
    let base_config = utils::create_validator_base_config();
    let data_client_config = AptosDataClientConfig {
        ignore_low_score_peers: true,
        ..Default::default()
    };
    let (mut mock_network, _, client, _) =
        MockNetwork::new(Some(base_config), Some(data_client_config), None);

    // Add a malicious peer
    let (malicious_peer, _) = utils::add_peer_to_network(PeerPriority::HighPriority, &mut mock_network);

    // Update peer summary to make it serviceable
    client.update_peer_storage_summary(malicious_peer, utils::create_storage_summary(1000));
    client.update_global_summary_cache().unwrap();

    // Get initial score (should be STARTING_SCORE = 50.0)
    let peer_states = client.get_peer_states();
    let initial_score = peer_states.get_peer_to_states().get(&malicious_peer).unwrap().get_score();
    assert_eq!(initial_score, 50.0, "Initial score should be 50.0");

    // Simulate malicious behavior: repeatedly mark peer as malicious
    for _ in 0..20 {
        peer_states.update_score_error(malicious_peer, ErrorType::Malicious);
    }

    // Verify score dropped significantly (MALICIOUS_MULTIPLIER = 0.8, so 50 * 0.8^20 â‰ˆ 0.58)
    let low_score = peer_states.get_peer_to_states().get(&malicious_peer).unwrap().get_score();
    assert!(low_score < 25.0, "Score should drop below ignore threshold after malicious behavior");
    println!("Score after malicious behavior: {}", low_score);

    // Peer should now be ignored
    client.update_global_summary_cache().unwrap();
    let global_summary = client.get_global_data_summary();
    assert!(global_summary.advertised_data.transactions.is_empty(), 
            "Malicious peer should be ignored");

    // === EXPLOIT: Disconnect and reconnect ===
    mock_network.disconnect_peer(malicious_peer);
    client.update_global_summary_cache().unwrap();

    // Verify peer state was removed by GC
    assert!(!peer_states.get_peer_to_states().contains_key(&malicious_peer),
            "Peer state should be removed after disconnect");

    // Reconnect the peer
    mock_network.reconnect_peer(malicious_peer);
    client.update_peer_storage_summary(malicious_peer, utils::create_storage_summary(1000));
    client.update_global_summary_cache().unwrap();

    // === VULNERABILITY: Score reset to STARTING_SCORE ===
    let reset_score = peer_states.get_peer_to_states().get(&malicious_peer).unwrap().get_score();
    assert_eq!(reset_score, 50.0, "Score should reset to STARTING_SCORE after reconnection");
    println!("Score after reconnection: {} (reset from {})", reset_score, low_score);

    // Peer is now serviceable again despite previous malicious behavior
    let global_summary = client.get_global_data_summary();
    assert!(!global_summary.advertised_data.transactions.is_empty(),
            "Malicious peer is serviceable again after score reset");

    println!("VULNERABILITY CONFIRMED: Malicious peer evaded reputation system via reconnection");
}
```

## Notes

This vulnerability fundamentally breaks the reputation system's security guarantee that malicious peers will be increasingly penalized over time. The lack of score persistence across connections means the system has no long-term memory of peer behavior, allowing sophisticated attackers to maintain disruption indefinitely through automated reconnection cycles. The issue is particularly concerning because the 100ms GC interval makes the exploit trivial to automate and highly reliable.

### Citations

**File:** state-sync/aptos-data-client/src/client.rs (L234-243)
```rust
    fn garbage_collect_peer_states(&self) -> crate::error::Result<(), Error> {
        // Get all connected peers
        let all_connected_peers = self.get_all_connected_peers()?;

        // Garbage collect the disconnected peers
        self.peer_states
            .garbage_collect_peer_states(all_connected_peers);

        Ok(())
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L85-93)
```rust
    pub fn new(data_client_config: Arc<AptosDataClientConfig>) -> Self {
        Self {
            data_client_config,
            received_responses_by_type: Arc::new(DashMap::new()),
            sent_requests_by_type: Arc::new(DashMap::new()),
            storage_summary: None,
            score: STARTING_SCORE,
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L325-330)
```rust
    pub fn update_summary(&self, peer: PeerNetworkId, storage_summary: StorageServerSummary) {
        self.peer_to_state
            .entry(peer)
            .or_insert(PeerState::new(self.data_client_config.clone()))
            .update_storage_summary(storage_summary);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L333-336)
```rust
    pub fn garbage_collect_peer_states(&self, connected_peers: HashSet<PeerNetworkId>) {
        self.peer_to_state
            .retain(|peer_network_id, _| connected_peers.contains(peer_network_id));
    }
```

**File:** config/src/config/state_sync_config.rs (L355-355)
```rust
            poll_loop_interval_ms: 100,
```

**File:** state-sync/aptos-data-client/src/tests/peers.rs (L470-523)
```rust
async fn disconnected_peers_garbage_collection() {
    // Ensure the properties hold for all peer priorities
    for peer_priority in PeerPriority::get_all_ordered_priorities() {
        // Create a base config for a validator
        let base_config = utils::create_validator_base_config();

        // Create the mock network, client and poller
        let data_client_config = AptosDataClientConfig::default();
        let (mut mock_network, _, client, poller) =
            MockNetwork::new(Some(base_config), Some(data_client_config), None);

        // Connect several peers
        let peer_1 = mock_network.add_peer(peer_priority);
        let peer_2 = mock_network.add_peer(peer_priority);
        let peer_3 = mock_network.add_peer(peer_priority);

        // Poll all of the peers to initialize the peer states
        let all_peers = hashset![peer_1, peer_2, peer_3];
        poll_peers(&mut mock_network, &poller, peer_priority, all_peers.clone()).await;

        // Verify we have peer states for all peers
        verify_peer_states(&client, all_peers.clone());

        // Disconnect peer 1 and update the global data summary
        mock_network.disconnect_peer(peer_1);
        client.update_global_summary_cache().unwrap();

        // Verify we have peer states for only peer 2 and 3
        verify_peer_states(&client, hashset![peer_2, peer_3]);

        // Disconnect peer 2 and update the global data summary
        mock_network.disconnect_peer(peer_2);
        client.update_global_summary_cache().unwrap();

        // Verify we have peer states for only peer 3
        verify_peer_states(&client, hashset![peer_3]);

        // Reconnect peer 1, poll it and update the global data summary
        mock_network.reconnect_peer(peer_1);
        poll_peers(&mut mock_network, &poller, peer_priority, hashset![peer_1]).await;
        client.update_global_summary_cache().unwrap();

        // Verify we have peer states for peers 1 and 3
        verify_peer_states(&client, hashset![peer_1, peer_3]);

        // Reconnect peer 2, poll it and update the global data summary
        mock_network.reconnect_peer(peer_2);
        poll_peers(&mut mock_network, &poller, peer_priority, hashset![peer_2]).await;
        client.update_global_summary_cache().unwrap();

        // Verify we have peer states for all peers
        verify_peer_states(&client, all_peers);
    }
}
```
