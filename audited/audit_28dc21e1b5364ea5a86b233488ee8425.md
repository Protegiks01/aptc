# Audit Report

## Title
Stale Peer Monitoring State Persists After Rapid Reconnection Leading to Incorrect Peer Assessment

## Summary
The peer monitoring service client fails to reset peer state when a peer disconnects and reconnects rapidly (within the 1-second monitoring loop interval). This causes stale latency measurements and network metadata to persist, leading to incorrect peer selection decisions in critical network operations including connectivity management, state synchronization, and mempool transaction forwarding.

## Finding Description

The peer monitoring service maintains state for each connected peer, including latency measurements (`recorded_latency_ping_durations_secs`), consecutive failure counts, and network distance information. This state is used throughout the network stack to make peer selection decisions. [1](#0-0) 

The monitoring loop performs garbage collection to remove disconnected peers, but this only runs once per `peer_monitor_interval_usec` (default 1 second): [2](#0-1) 

The garbage collection function only removes states for peers NOT in the connected list: [3](#0-2) 

When creating states for new peers, the function checks if a state already exists and doesn't create a new one: [4](#0-3) 

**Attack Scenario:**

1. Malicious peer connects to the network and establishes good latency metrics (e.g., 10ms average ping)
2. Peer disconnects from the network
3. **Within 1 second**, peer reconnects (before next monitoring loop iteration)
4. The old `PeerState` persists because:
   - Garbage collection hasn't run yet
   - Peer is in the connected list when it does run
   - State creation skips peers with existing state
5. The stale latency data continues to be used for peer selection decisions

This stale state is consumed by critical network components:

**Connectivity Manager** uses stale latency for weighted peer selection: [5](#0-4) 

**State Sync** uses stale latency and distance metadata for data request routing: [6](#0-5) 

**Mempool** uses stale latency for transaction forwarding prioritization: [7](#0-6) 

The latency info state stores historical measurements that persist across reconnections: [8](#0-7) 

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per Aptos bug bounty criteria due to:

1. **State Inconsistency**: The peer monitoring metadata does not reflect the current network conditions of reconnected peers, violating the assumption that peer selection is based on accurate, current information.

2. **Availability Impact**: An attacker controlling multiple peers can:
   - Build good reputation with low latency
   - Rapidly reconnect to maintain stale good metrics
   - Provide degraded service (high latency, dropped requests)
   - Force the network to preferentially select malicious peers over legitimate ones
   - Cause performance degradation across consensus, state sync, and mempool

3. **No Direct Consensus/Fund Impact**: This does not directly break consensus safety or cause fund loss, preventing Critical severity classification.

4. **Requires Intervention**: Operators may need to manually disconnect problematic peers or restart nodes to clear stale state, requiring operational intervention.

The default monitoring interval of 1 second provides a realistic attack window: [9](#0-8) 

## Likelihood Explanation

**High Likelihood** due to:

1. **Low Attack Complexity**: Attacker only needs to:
   - Control network peer(s) with valid PeerId
   - Establish initial connection
   - Disconnect and reconnect within 1 second
   - No special permissions or validator access required

2. **Wide Attack Window**: The default 1-second monitoring interval provides ample time for rapid reconnection, especially on low-latency networks.

3. **No Existing Defenses**: The peer monitoring service does not:
   - Listen to connection notification events
   - Track connection IDs or session identifiers
   - Validate staleness of monitoring data
   - Reset state on reconnection

4. **Realistic Network Scenario**: Legitimate network instability (brief disconnections) could accidentally trigger this issue, though sustained exploitation requires intentional rapid reconnection patterns.

## Recommendation

**Fix 1: Event-Driven State Cleanup**

Subscribe to connection notification events and immediately clear peer state on disconnection:

```rust
// In peer-monitoring-service/client/src/lib.rs
// Add connection event listener that immediately removes peer state when LostPeer is received

pub fn handle_connection_notification(
    peer_monitor_state: &PeerMonitorState,
    notification: ConnectionNotification,
) {
    match notification {
        ConnectionNotification::LostPeer(metadata, network_id, peer_id) => {
            let peer_network_id = PeerNetworkId::new(network_id, peer_id);
            peer_monitor_state.peer_states.write().remove(&peer_network_id);
            info!("Removed peer state for disconnected peer: {:?}", peer_network_id);
        },
        _ => {}, // Handle other notifications as needed
    }
}
```

**Fix 2: Connection Session Tracking**

Add connection ID or session tracking to prevent state reuse across different connections:

```rust
// In PeerState
pub struct PeerState {
    connection_id: ConnectionId,  // Track specific connection
    state_entries: Arc<RwLock<HashMap<PeerStateKey, Arc<RwLock<PeerStateValue>>>>>,
}

// Validate connection ID matches before using state
// Create new state if connection ID changed
```

**Fix 3: State Staleness Validation**

Add timestamp validation to detect and discard stale state:

```rust
// In PeerState
pub struct PeerState {
    last_connection_time: Instant,
    state_entries: ...,
}

// In create_states_for_new_peers, check if existing state is stale:
if let Some(existing_state) = peer_monitor_state.peer_states.read().get(peer_network_id) {
    if existing_state.last_connection_time.elapsed() > Duration::from_secs(60) {
        // Remove stale state
        peer_monitor_state.peer_states.write().remove(peer_network_id);
    }
}
```

**Recommended Approach**: Implement Fix 1 (event-driven cleanup) as the primary solution, as it provides immediate response to disconnections and aligns with the existing connection notification infrastructure used by other network components.

## Proof of Concept

```rust
// Test demonstrating stale state persistence after rapid reconnection
// Add to peer-monitoring-service/client/src/tests/

#[tokio::test]
async fn test_rapid_reconnection_stale_state() {
    use crate::{PeerMonitorState, start_peer_monitor_with_state};
    use aptos_config::config::NodeConfig;
    use aptos_network::application::storage::PeersAndMetadata;
    use aptos_time_service::TimeService;
    use std::sync::Arc;

    // Setup
    let node_config = NodeConfig::default();
    let time_service = TimeService::mock();
    let peer_monitor_state = PeerMonitorState::new();
    let peers_and_metadata = Arc::new(PeersAndMetadata::new());
    
    // Create test peer
    let peer_network_id = PeerNetworkId::random_validator();
    
    // 1. Connect peer and build good latency state
    connect_peer(&peers_and_metadata, peer_network_id);
    simulate_good_latency_pings(&peer_monitor_state, peer_network_id, 10); // 10ms latency
    
    // Verify good latency is recorded
    let initial_state = peer_monitor_state.get_peer_state(&peer_network_id).unwrap();
    let initial_latency = initial_state.get_latency_info_state().unwrap()
        .get_average_latency_ping_secs().unwrap();
    assert!(initial_latency < 0.015); // Less than 15ms
    
    // 2. Disconnect peer
    disconnect_peer(&peers_and_metadata, peer_network_id);
    
    // 3. Reconnect IMMEDIATELY (within monitoring loop interval)
    connect_peer(&peers_and_metadata, peer_network_id);
    
    // 4. Verify old state persists (BUG!)
    let reconnected_state = peer_monitor_state.get_peer_state(&peer_network_id).unwrap();
    let reconnected_latency = reconnected_state.get_latency_info_state().unwrap()
        .get_average_latency_ping_secs().unwrap();
    
    // BUG: Old latency data persists despite reconnection
    assert_eq!(initial_latency, reconnected_latency);
    
    // 5. Demonstrate impact: peer is selected despite now having poor performance
    simulate_poor_latency_pings(&peer_monitor_state, peer_network_id, 1000); // Now 1000ms latency
    
    // But selection still uses stale good latency for weighting
    let selection_latency = peers_and_metadata
        .get_peer_monitoring_metadata(peer_network_id)
        .unwrap()
        .average_ping_latency_secs
        .unwrap();
    assert!(selection_latency < 0.015); // Still using stale good latency!
    
    // Expected behavior: State should be reset on reconnection
    // Fix: Implement event-driven state cleanup on LostPeer notification
}

// Helper functions
fn connect_peer(peers_and_metadata: &Arc<PeersAndMetadata>, peer: PeerNetworkId) {
    // Add peer to connected list with metadata
}

fn disconnect_peer(peers_and_metadata: &Arc<PeersAndMetadata>, peer: PeerNetworkId) {
    // Remove peer from connected list
}

fn simulate_good_latency_pings(state: &PeerMonitorState, peer: PeerNetworkId, latency_ms: u64) {
    // Record multiple successful low-latency pings
}

fn simulate_poor_latency_pings(state: &PeerMonitorState, peer: PeerNetworkId, latency_ms: u64) {
    // Simulate high latency but state already populated
}
```

## Notes

This vulnerability demonstrates a classic time-of-check-time-of-use (TOCTOU) issue where the peer monitoring loop's periodic execution creates a race condition window. The lack of event-driven state management allows stale data to persist across connection boundaries, violating the principle that peer assessment should reflect current network conditions.

The impact extends beyond simple performance degradation - in adversarial scenarios, this enables sustained targeting of specific network paths or peers, potentially contributing to eclipse attacks or network partitioning attempts when combined with other attack vectors.

### Citations

**File:** peer-monitoring-service/client/src/lib.rs (L114-156)
```rust
    loop {
        // Wait for the next round before pinging peers
        peer_monitor_ticker.next().await;

        // Get all connected peers
        let connected_peers_and_metadata =
            match peers_and_metadata.get_connected_peers_and_metadata() {
                Ok(connected_peers_and_metadata) => connected_peers_and_metadata,
                Err(error) => {
                    warn!(LogSchema::new(LogEntry::PeerMonitorLoop)
                        .event(LogEvent::UnexpectedErrorEncountered)
                        .error(&error.into())
                        .message("Failed to get connected peers and metadata!"));
                    continue; // Move to the next loop iteration
                },
            };

        // Garbage collect the peer states (to remove disconnected peers)
        garbage_collect_peer_states(&peer_monitor_state, &connected_peers_and_metadata);

        // Ensure all peers have a state (and create one for newly connected peers)
        create_states_for_new_peers(
            &node_config,
            &peer_monitor_state,
            &time_service,
            &connected_peers_and_metadata,
        );

        // Refresh the peer states
        if let Err(error) = peer_states::refresh_peer_states(
            &monitoring_service_config,
            peer_monitor_state.clone(),
            peer_monitoring_client.clone(),
            connected_peers_and_metadata,
            time_service.clone(),
            runtime.clone(),
        ) {
            warn!(LogSchema::new(LogEntry::PeerMonitorLoop)
                .event(LogEvent::UnexpectedErrorEncountered)
                .error(&error)
                .message("Failed to refresh peer states!"));
        }
    }
```

**File:** peer-monitoring-service/client/src/lib.rs (L159-178)
```rust
/// Creates a new peer state for peers that don't yet have one
fn create_states_for_new_peers(
    node_config: &NodeConfig,
    peer_monitor_state: &PeerMonitorState,
    time_service: &TimeService,
    connected_peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) {
    for peer_network_id in connected_peers_and_metadata.keys() {
        let state_exists = peer_monitor_state
            .peer_states
            .read()
            .contains_key(peer_network_id);
        if !state_exists {
            peer_monitor_state.peer_states.write().insert(
                *peer_network_id,
                PeerState::new(node_config.clone(), time_service.clone()),
            );
        }
    }
}
```

**File:** peer-monitoring-service/client/src/lib.rs (L180-202)
```rust
/// Garbage collects peer states for peers that are no longer connected
fn garbage_collect_peer_states(
    peer_monitor_state: &PeerMonitorState,
    connected_peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) {
    // Get the set of peers with existing states
    let peers_with_existing_states: Vec<PeerNetworkId> = peer_monitor_state
        .peer_states
        .read()
        .keys()
        .cloned()
        .collect();

    // Remove the states for disconnected peers
    for peer_network_id in peers_with_existing_states {
        if !connected_peers_and_metadata.contains_key(&peer_network_id) {
            peer_monitor_state
                .peer_states
                .write()
                .remove(&peer_network_id);
        }
    }
}
```

**File:** network/framework/src/connectivity_manager/selection.rs (L103-146)
```rust
fn choose_peers_by_ping_latency(
    network_context: &NetworkContext,
    peer_ids: &HashSet<PeerId>,
    num_peers_to_choose: usize,
    discovered_peers: Arc<RwLock<DiscoveredPeerSet>>,
) -> HashSet<PeerId> {
    // If no peers can be chosen, return an empty list
    if num_peers_to_choose == 0 || peer_ids.is_empty() {
        return hashset![];
    }

    // Gather the latency weights for all peers
    let mut peer_ids_and_latency_weights = vec![];
    for peer_id in peer_ids {
        if let Some(ping_latency_secs) = discovered_peers.read().get_ping_latency_secs(peer_id) {
            let latency_weight = convert_latency_to_weight(ping_latency_secs);
            peer_ids_and_latency_weights.push((peer_id, OrderedFloat(latency_weight)));
        }
    }

    // Get the random peers by weight
    let weighted_selected_peers = peer_ids_and_latency_weights
        .choose_multiple_weighted(
            &mut ::rand_latest::thread_rng(),
            num_peers_to_choose,
            |peer| peer.1,
        )
        .map(|peers| peers.into_iter().map(|peer| *peer.0).collect::<Vec<_>>());

    // Return the random peers by weight
    weighted_selected_peers
        .unwrap_or_else(|error| {
            // We failed to select any peers
            error!(
                NetworkSchema::new(network_context),
                "Failed to choose peers by latency for network context: {:?}. Error: {:?}",
                network_context,
                error
            );
            vec![]
        })
        .into_iter()
        .collect::<HashSet<_>>()
}
```

**File:** state-sync/aptos-data-client/src/utils.rs (L73-121)
```rust
pub fn choose_peers_by_latency(
    data_client_config: Arc<AptosDataClientConfig>,
    num_peers_to_choose: u64,
    potential_peers: HashSet<PeerNetworkId>,
    peers_and_metadata: Arc<PeersAndMetadata>,
    ignore_high_latency_peers: bool,
) -> HashSet<PeerNetworkId> {
    // If no peers can be chosen, return an empty set
    if num_peers_to_choose == 0 || potential_peers.is_empty() {
        return hashset![];
    }

    // Gather the latency weights for all potential peers
    let mut potential_peers_and_latency_weights = vec![];
    for peer in potential_peers {
        if let Some(latency) = get_latency_for_peer(&peers_and_metadata, peer) {
            let latency_weight = convert_latency_to_weight(latency);
            potential_peers_and_latency_weights.push((peer, OrderedFloat(latency_weight)));
        }
    }

    // Determine the number of peers to consider. If high latency peers can be
    // ignored, we only want to consider a subset of peers with the lowest
    // latencies. However, this can only be done if we have a large total
    // number of peers, and there are enough potential peers for each request.
    let mut num_peers_to_consider = potential_peers_and_latency_weights.len() as u64;
    if ignore_high_latency_peers {
        let latency_filtering_config = &data_client_config.latency_filtering_config;
        let peer_ratio_per_request = num_peers_to_consider / num_peers_to_choose;
        if num_peers_to_consider >= latency_filtering_config.min_peers_for_latency_filtering
            && peer_ratio_per_request
                >= latency_filtering_config.min_peer_ratio_for_latency_filtering
        {
            // Consider a subset of peers with the lowest latencies
            num_peers_to_consider /= latency_filtering_config.latency_filtering_reduction_factor
        }
    }

    // Sort the peers by latency weights and take the number of peers to consider
    potential_peers_and_latency_weights.sort_by_key(|(_, latency_weight)| *latency_weight);
    let potential_peers_and_latency_weights = potential_peers_and_latency_weights
        .into_iter()
        .take(num_peers_to_consider as usize)
        .map(|(peer, latency_weight)| (peer, latency_weight.into_inner()))
        .collect::<Vec<_>>();

    // Select the peers by latency weights
    choose_random_peers_by_weight(num_peers_to_choose, potential_peers_and_latency_weights)
}
```

**File:** mempool/src/shared_mempool/priority.rs (L111-115)
```rust
        // Otherwise, compare by peer ping latency (the lower the better)
        let latency_ordering = compare_ping_latency(monitoring_metadata_a, monitoring_metadata_b);
        if !latency_ordering.is_eq() {
            return latency_ordering; // Only return if it's not equal
        }
```

**File:** peer-monitoring-service/client/src/peer_states/latency_info.rs (L25-50)
```rust
/// A simple container that holds a peer's latency info
#[derive(Clone, Debug)]
pub struct LatencyInfoState {
    latency_monitoring_config: LatencyMonitoringConfig, // The config for latency monitoring
    latency_ping_counter: u64, // The monotonically increasing counter for each ping
    recorded_latency_ping_durations_secs: BTreeMap<u64, f64>, // Successful ping durations by counter (secs)
    request_tracker: Arc<RwLock<RequestTracker>>, // The request tracker for latency ping requests
}

impl LatencyInfoState {
    pub fn new(
        latency_monitoring_config: LatencyMonitoringConfig,
        time_service: TimeService,
    ) -> Self {
        let request_tracker = RequestTracker::new(
            latency_monitoring_config.latency_ping_interval_ms,
            time_service,
        );

        Self {
            latency_monitoring_config,
            latency_ping_counter: 0,
            recorded_latency_ping_durations_secs: BTreeMap::new(),
            request_tracker: Arc::new(RwLock::new(request_tracker)),
        }
    }
```

**File:** config/src/config/peer_monitoring_config.rs (L33-33)
```rust
            peer_monitor_interval_usec: 1_000_000, // 1 second
```
