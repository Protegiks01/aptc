# Audit Report

## Title
Lack of Key Size Validation in RawTableItemRequest Enables API Resource Exhaustion Attack

## Summary
The `RawTableItemRequest` struct accepts arbitrarily large hex-encoded keys (up to ~4 MB) without validation, allowing attackers to cause API performance degradation and memory exhaustion through state lookup operations with unreasonably large keys that would never exist in actual table storage.

## Finding Description
The `/tables/{table_handle}/raw_item` API endpoint accepts `RawTableItemRequest` with a `key` field of type `HexEncodedBytes` that has no size validation. [1](#0-0) 

The `HexEncodedBytes` type is a simple wrapper around `Vec<u8>` with no inherent size limits. [2](#0-1) 

When processing requests, the API has a general content length limit of 8 MB, [3](#0-2)  which allows hex-encoded keys up to approximately 4 MB after decoding.

The `raw_table_item` handler directly uses the provided key bytes without any size validation to construct a `StateKey` for database lookup. [4](#0-3) 

During `StateKey::table_item` construction, the key is stored in a global registry which creates an Entry containing: (1) the deserialized `StateKeyInner` with the full key bytes, (2) an encoded `Bytes` representation including the full key, and (3) a computed hash. [5](#0-4) 

The encoding process writes the entire key directly without compression or limits. [6](#0-5) 

**Attack Path:**
1. Attacker sends POST requests to `/tables/{any_table_handle}/raw_item` with ~4 MB hex-encoded keys (within 8 MB request limit)
2. Each request causes memory amplification: 4 MB key â†’ ~12-16 MB total allocation (hex string, decoded bytes, StateKeyInner storage, encoded Bytes, plus overhead)
3. CPU-intensive operations: encoding and hashing multi-megabyte keys
4. With concurrent requests (API supports multiple worker threads), an attacker can accumulate significant memory usage and CPU load
5. Database seek operations with 4+ MB keys are unnecessarily slow

**Invariant Violation:**
This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The API should validate that table lookup keys are within reasonable bounds. Real table keys in Move are constrained (e.g., PropertyMap limits keys to 128 bytes, BigOrderedMap enforces dynamic size limits based on node structure).

## Impact Explanation
This qualifies as **High Severity** under Aptos bug bounty criteria for "Validator node slowdowns" and "API crashes."

An attacker can exploit this to:
- Cause API performance degradation by tying up worker threads with slow encode/hash/lookup operations
- Accumulate memory pressure through concurrent requests with different large keys
- Reduce API availability and responsiveness for legitimate users
- Potentially trigger out-of-memory conditions or API instability on resource-constrained nodes

While the existing 8 MB request limit and 100 req/min rate limiting provide some mitigation, they don't prevent concurrent requests from causing resource exhaustion. An attacker can send multiple concurrent requests within the rate limit to amplify the impact.

## Likelihood Explanation
This attack is **highly likely** to succeed because:
- The endpoint is publicly accessible without authentication
- No validation exists on key size
- Attack requires only HTTP POST requests with large hex strings
- Default configuration allows exploitation within existing limits
- No logging or alerting for abnormally large keys

The attack is trivial to execute and requires no special privileges, insider access, or cryptographic capabilities.

## Recommendation
Add explicit validation on the `RawTableItemRequest.key` size before processing. Since real table keys are constrained by Move framework limits (128 bytes to ~5 KB depending on data structure), a reasonable maximum would be 4-8 KB for the raw key bytes.

Implement validation in the `raw_table_item` method:

```rust
pub fn raw_table_item(
    &self,
    accept_type: &AcceptType,
    table_handle: Address,
    table_item_request: RawTableItemRequest,
    ledger_version: Option<U64>,
) -> BasicResultWith404<MoveValue> {
    const MAX_TABLE_KEY_SIZE: usize = 8192; // 8 KB limit
    
    if table_item_request.key.0.len() > MAX_TABLE_KEY_SIZE {
        return Err(BasicErrorWith404::bad_request_with_code(
            format!("Table key size {} exceeds maximum allowed size {}", 
                    table_item_request.key.0.len(), MAX_TABLE_KEY_SIZE),
            AptosErrorCode::InvalidInput,
            &LedgerInfo::default(), // or fetch current ledger info
        ));
    }
    
    // ... existing code ...
}
```

Additionally, consider adding similar validation to `TableItemRequest` to prevent the same issue through the regular table item endpoint.

## Proof of Concept

```rust
// Proof of Concept - HTTP request that causes resource exhaustion
// This can be executed using curl or any HTTP client

use hex;

fn main() {
    // Create a 4 MB key (8 MB hex string)
    let large_key = vec![0xAA; 4 * 1024 * 1024]; // 4 MB of 0xAA bytes
    let hex_key = format!("0x{}", hex::encode(&large_key));
    
    // JSON payload
    let payload = format!(r#"{{"key":"{}"}}"#, hex_key);
    
    println!("Payload size: {} bytes", payload.len());
    println!("Key size after decode: {} bytes", large_key.len());
    
    // Send POST request to API endpoint:
    // POST http://localhost:8080/v1/tables/0x1/raw_item
    // Content-Type: application/json
    // Accept: application/x-bcs
    // Body: {"key":"0xAAAAAA..."} (with 4MB of AA bytes hex-encoded)
    
    // Expected behavior: API accepts and processes the request,
    // causing memory allocation of ~12-16 MB and CPU-intensive encoding/hashing
    
    // Multiple concurrent requests will amplify resource consumption:
    // 10 concurrent requests = ~120-160 MB memory usage + significant CPU load
}
```

To reproduce:
1. Start an Aptos node with default API configuration
2. Send concurrent POST requests to `/v1/tables/{any_table_handle}/raw_item` with 4 MB hex-encoded keys
3. Monitor API response times and memory usage - observe degradation
4. Legitimate API requests experience slowdowns or timeouts

## Notes
This vulnerability demonstrates a failure to validate untrusted user input at the API boundary. While Move framework enforces reasonable limits on table keys during write operations, the read API blindly accepts arbitrarily large keys for lookup. Defense-in-depth principles require validating input sizes at all entry points, especially for resource-intensive operations like state lookups.

### Citations

**File:** api/types/src/table.rs (L25-29)
```rust
/// Table Item request for the GetTableItemRaw API
#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, Object)]
pub struct RawTableItemRequest {
    pub key: HexEncodedBytes,
}
```

**File:** api/types/src/move_types.rs (L146-147)
```rust
#[derive(Clone, Debug, PartialEq, Eq)]
pub struct HexEncodedBytes(pub Vec<u8>);
```

**File:** config/src/config/api_config.rs (L97-97)
```rust
const DEFAULT_REQUEST_CONTENT_LENGTH_LIMIT: u64 = 8 * 1024 * 1024; // 8 MB
```

**File:** api/src/state.rs (L484-485)
```rust
        let state_key =
            StateKey::table_item(&TableHandle(table_handle.into()), &table_item_request.key.0);
```

**File:** types/src/state_store/state_key/mod.rs (L190-202)
```rust
    pub fn table_item(handle: &TableHandle, key: &[u8]) -> Self {
        Self(
            REGISTRY
                .table_item(handle, key)
                .get_or_add(handle, key, || {
                    Ok(StateKeyInner::TableItem {
                        handle: *handle,
                        key: key.to_vec(),
                    })
                })
                .expect("only possible error is resource path serialization"),
        )
    }
```

**File:** types/src/state_store/state_key/inner.rs (L71-74)
```rust
            StateKeyInner::TableItem { handle, key } => {
                writer.write_all(&[StateKeyTag::TableItem as u8])?;
                bcs::serialize_into(&mut writer, &handle)?;
                writer.write_all(key)?;
```
