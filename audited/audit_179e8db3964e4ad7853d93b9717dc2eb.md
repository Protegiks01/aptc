# Audit Report

## Title
Unbounded Task Spawning and Connection Creation in Indexer gRPC Gateway Leads to Resource Exhaustion

## Summary
The indexer-grpc-gateway lacks task spawning limits in its tokio runtime and creates a new gRPC connection for every incoming request without connection pooling, enabling attackers to cause resource exhaustion through concurrent request floods.

## Finding Description

The indexer-grpc-gateway initializes its tokio runtime using the default `#[tokio::main]` macro without any configured limits. [1](#0-0) 

The gateway's request handling middleware creates a **new gRPC connection** to the grpc_manager service for every incoming request, with no connection pooling or reuse: [2](#0-1) 

The Axum server is configured without any concurrency limits or rate limiting middleware: [3](#0-2) 

**Attack Flow:**
1. Attacker sends a flood of concurrent HTTP requests to the gateway endpoint
2. Axum accepts all connections (no limit configured)
3. For each request, a new async task is spawned (tokio has no spawning limits)
4. Each task calls `GrpcManagerClient::connect()` which establishes a **new TCP connection** to the grpc_manager
5. Each task then makes gRPC calls and proxies HTTP requests to data services

This violates the **Resource Limits invariant** (#9): "All operations must respect gas, storage, and computational limits."

**Contrast with Proper Implementation:**

Other indexer services correctly implement connection pooling. The indexer-grpc-data-service-v2 uses a `ConnectionManager` with a `DashMap` to pool and reuse connections: [4](#0-3) 

It retrieves clients from the pool instead of creating new connections: [5](#0-4) 

And uses lazy connection establishment with proper configuration: [6](#0-5) 

## Impact Explanation

This qualifies as **HIGH severity** per the Aptos bug bounty criteria:
- **API crashes**: The gateway can become unresponsive or crash under load
- **Validator node slowdowns**: If the grpc_manager service is shared infrastructure, it can affect other services

**Specific impacts:**
- **Memory exhaustion**: Unbounded task spawning consumes memory linearly with concurrent requests
- **File descriptor exhaustion**: Each new connection consumes a file descriptor; OS limits (~65k) can be reached
- **CPU saturation**: Processing excessive concurrent tasks degrades performance
- **Cascade failure**: The grpc_manager service receives unbounded connection attempts, potentially affecting all connected services
- **Service unavailability**: The gateway becomes unresponsive to legitimate users

## Likelihood Explanation

**HIGH likelihood** - This attack is:
- **Trivially exploitable**: Requires only sending HTTP requests (e.g., curl, python requests)
- **No authentication required**: The gateway is publicly accessible
- **Low attacker cost**: Simple scripting, no special infrastructure needed
- **Immediate impact**: Resource exhaustion occurs within seconds to minutes
- **Difficult to detect**: Appears as legitimate traffic without proper monitoring

## Recommendation

Implement the following mitigations:

1. **Add connection pooling** similar to indexer-grpc-data-service-v2:
```rust
// In config.rs
pub(crate) static GRPC_MANAGER_CLIENT: OnceCell<GrpcManagerClient<Channel>> = OnceCell::const_new();

// In gateway.rs - initialize once at startup
fn create_grpc_manager_client(address: &str) -> GrpcManagerClient<Channel> {
    let channel = Channel::from_shared(address.to_string())
        .expect("Bad address.")
        .connect_lazy();
    GrpcManagerClient::new(channel)
        .send_compressed(CompressionEncoding::Zstd)
        .accept_compressed(CompressionEncoding::Zstd)
        .max_decoding_message_size(MAX_MESSAGE_SIZE)
        .max_encoding_message_size(MAX_MESSAGE_SIZE)
}

// Reuse the client instead of reconnecting
let client = GRPC_MANAGER_CLIENT.get_or_init(|| async {
    create_grpc_manager_client(&config.grpc_manager_address)
}).await.clone();
```

2. **Add concurrency limiting middleware**:
```rust
use tower::limit::ConcurrencyLimitLayer;

let app = Router::new()
    .route("/*path", any(proxy).with_state(self.config.clone()))
    .layer(from_fn_with_state(self.config.clone(), get_data_service_url))
    .layer(ConcurrencyLimitLayer::new(1000)); // Limit concurrent requests
```

3. **Configure tokio runtime limits** in main.rs:
```rust
#[tokio::main(worker_threads = 8)]
async fn main() -> Result<()> {
    // Or use a custom runtime builder with explicit limits
}
```

4. **Add rate limiting** per client IP using tower-governor or similar middleware

## Proof of Concept

```rust
// Rust reproduction test
#[tokio::test]
async fn test_unbounded_connection_exhaustion() {
    // Start the gateway
    let gateway_addr = "http://localhost:8080";
    
    // Spawn 10,000 concurrent requests
    let mut handles = vec![];
    for i in 0..10_000 {
        let addr = gateway_addr.to_string();
        let handle = tokio::spawn(async move {
            let client = reqwest::Client::new();
            client
                .post(&format!("{}/aptos.indexer.v1.RawData/GetTransactions", addr))
                .body(vec![0u8; 100]) // Minimal request body
                .send()
                .await
        });
        handles.push(handle);
    }
    
    // Observe:
    // - Memory usage grows unbounded
    // - File descriptor count increases linearly
    // - Gateway becomes unresponsive
    // - grpc_manager receives 10,000 connection attempts
    
    for handle in handles {
        let _ = handle.await;
    }
}
```

Alternative bash PoC:
```bash
#!/bin/bash
# Send 1000 concurrent requests to trigger resource exhaustion
for i in {1..1000}; do
    curl -X POST http://gateway:8080/aptos.indexer.v1.RawData/GetTransactions \
         -H "Content-Type: application/grpc" \
         -d "" &
done
wait

# Monitor with:
# - lsof -p $(pgrep indexer-grpc-gateway) | wc -l  # File descriptors
# - ps aux | grep indexer-grpc-gateway  # Memory usage
# - netstat -an | grep grpc_manager_port | wc -l  # Connections to manager
```

## Notes

This vulnerability specifically affects the indexer-grpc-gateway service's availability and can cause cascade failures to the grpc_manager service. While the indexer infrastructure is separate from consensus operations, the gateway is a critical API component for querying blockchain data, and its unavailability impacts user-facing applications and services that depend on indexer data.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-gateway/src/main.rs (L13-17)
```rust
#[tokio::main]
async fn main() -> Result<()> {
    let args = ServerArgs::parse();
    args.run::<IndexerGrpcGatewayConfig>().await
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-gateway/src/gateway.rs (L43-62)
```rust
    pub(crate) async fn start(&self) -> anyhow::Result<()> {
        let app = Router::new()
            .route("/*path", any(proxy).with_state(self.config.clone()))
            .layer(from_fn_with_state(
                self.config.clone(),
                get_data_service_url,
            ));

        info!(
            "gRPC Gateway listening on {}:{}",
            LISTEN_ADDRESS, self.config.port
        );
        let listener = tokio::net::TcpListener::bind((LISTEN_ADDRESS, self.config.port))
            .await
            .expect("Failed to bind TCP listener");

        axum::serve(listener, app)
            .await
            .context("Failed to serve gRPC Gateway")
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-gateway/src/gateway.rs (L138-140)
```rust
    let mut client = GrpcManagerClient::connect(config.grpc_manager_address.to_string())
        .await
        .map_err(|e| (StatusCode::INTERNAL_SERVER_ERROR, e.to_string()))?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L101-108)
```rust
pub(crate) struct ConnectionManager {
    chain_id: u64,
    grpc_manager_connections: DashMap<String, GrpcManagerClient<Channel>>,
    self_advertised_address: String,
    known_latest_version: AtomicU64,
    active_streams: DashMap<String, (ActiveStream, StreamProgressSamples)>,
    is_live_data_service: bool,
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L172-179)
```rust
    pub(crate) fn get_grpc_manager_client_for_request(&self) -> GrpcManagerClient<Channel> {
        let mut rng = thread_rng();
        self.grpc_manager_connections
            .iter()
            .choose(&mut rng)
            .map(|kv| kv.value().clone())
            .unwrap()
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L303-313)
```rust
    fn create_client_from_address(address: &str) -> GrpcManagerClient<Channel> {
        info!("Creating GrpcManagerClient for {address}.");
        let channel = Channel::from_shared(address.to_string())
            .expect("Bad address.")
            .connect_lazy();
        GrpcManagerClient::new(channel)
            .send_compressed(CompressionEncoding::Zstd)
            .accept_compressed(CompressionEncoding::Zstd)
            .max_decoding_message_size(MAX_MESSAGE_SIZE)
            .max_encoding_message_size(MAX_MESSAGE_SIZE)
    }
```
