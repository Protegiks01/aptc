# Audit Report

## Title
Partial Ledger Pruning State Inconsistency During OOM in Parallel Sub-Pruner Execution

## Summary
An out-of-memory (OOM) condition during the parallel execution of sub-pruners in `LedgerPruner::prune()` can cause some sub-pruners to complete their atomic write operations while others fail, resulting in a temporarily inconsistent database state where different ledger components have inconsistent pruning progress markers and partially deleted data.

## Finding Description

The `LedgerPruner::prune()` function executes multiple sub-pruners in parallel using Rayon's `par_iter()` [1](#0-0) . The pruning sequence is:

1. **Sequential pruning**: `LedgerMetadataPruner` prunes first and updates `LedgerPrunerProgress` in the database [2](#0-1) 

2. **Parallel pruning**: Seven sub-pruners execute concurrently (EventStorePruner, TransactionPruner, TransactionInfoPruner, WriteSetPruner, etc.) [3](#0-2) 

3. **Progress update**: Only if ALL operations succeed, the overall progress is recorded [4](#0-3) 

**The Vulnerability:**

Each sub-pruner performs atomic batch writes to RocksDB, updating both data and its individual progress marker in a single `SchemaBatch` [5](#0-4) . If an OOM occurs during parallel execution:

- Sub-pruners that completed before OOM have already committed their batches (e.g., `EventPrunerProgress = 2000`, `TransactionPrunerProgress = 2000`)
- Sub-pruners that failed or didn't start still have old progress markers (e.g., `TransactionInfoPrunerProgress = 1000`, `WriteSetPrunerProgress = 1000`)
- The error propagates via the `?` operator, preventing `self.record_progress()` from executing
- **Result**: Database enters a partially pruned state where events and transactions for versions [1000, 2000) are deleted, but transaction info and write sets still exist

**Broken Invariant:**

This violates **Critical Invariant #4: State Consistency** - "State transitions must be atomic and verifiable via Merkle proofs." The pruning operation should be all-or-nothing across all ledger components, but the parallel execution without cross-pruner coordination allows partial completion.

**Query Impact:**

During this inconsistent window, if a query attempts to retrieve a transaction in the partially pruned range via `get_transaction_with_proof()` [6](#0-5) , the following occurs:

1. `error_if_ledger_pruned()` check may pass (depending on `min_readable_version` state)
2. `get_transaction_info_with_proof()` succeeds (TransactionInfoPruner hasn't run)
3. `get_transaction()` fails with `AptosDbError::NotFound` because TransactionPruner already deleted the transaction [7](#0-6) 
4. API returns error for data that should either be fully available or properly marked as pruned

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:

- **API crashes**: Queries for partially pruned versions fail unexpectedly with `NotFound` errors rather than consistent "pruned" errors, causing API endpoint failures
- **Significant protocol violations**: Violates the atomicity guarantee that all ledger data for a version is pruned together
- **State inconsistency window**: Between OOM failure and successful retry, the node serves in an inconsistent state, potentially affecting state synchronization with other nodes

While the system has a recovery mechanism where sub-pruners catch up during initialization [8](#0-7) , the window of inconsistency can persist until the next successful prune attempt or node restart.

## Likelihood Explanation

**Medium-High Likelihood:**

- OOM conditions can occur naturally on memory-constrained validator nodes during heavy load
- The pruning operation processes large batches (configurable batch size) and iterates through multiple database schemas simultaneously
- Memory pressure is common during state synchronization, large transaction batches, or concurrent operations
- Seven parallel sub-pruners executing simultaneously increases memory consumption during the critical window
- No explicit memory reservation or checking before starting the expensive parallel operation

## Recommendation

Implement one of the following solutions:

**Option 1: Two-Phase Pruning with Prepare-Commit**
```rust
fn prune(&self, max_versions: usize) -> Result<Version> {
    // ... existing code ...
    
    // Phase 1: Prepare all batches (without writing)
    let batches: Vec<SchemaBatch> = THREAD_MANAGER.get_background_pool().install(|| {
        self.sub_pruners.par_iter().map(|sub_pruner| {
            sub_pruner.prepare_prune_batch(progress, current_batch_target_version)
        }).collect::<Result<Vec<_>>>()
    })?;
    
    // Phase 2: Write all batches sequentially (fast, already prepared)
    for (sub_pruner, batch) in self.sub_pruners.iter().zip(batches.iter()) {
        sub_pruner.commit_batch(batch)?;
    }
    
    progress = current_batch_target_version;
    self.record_progress(progress);
}
```

**Option 2: Check Sub-Pruner Progress Before Calling**
```rust
fn prune(&self, max_versions: usize) -> Result<Version> {
    // ... existing code ...
    
    THREAD_MANAGER.get_background_pool().install(|| {
        self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
            // Read current sub-pruner progress from DB
            let sub_progress = sub_pruner.get_progress()?;
            
            // Only prune if sub-pruner is behind
            if sub_progress < current_batch_target_version {
                sub_pruner.prune(sub_progress, current_batch_target_version)
                    .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
            } else {
                Ok(())
            }
        })
    })?;
}
```

**Option 3: Centralized Transaction Coordinator**
Use a single RocksDB `WriteBatch` that aggregates all sub-pruner operations before committing, ensuring atomicity across all sub-pruners.

## Proof of Concept

```rust
#[test]
fn test_partial_pruning_on_oom() {
    // Setup: Create LedgerPruner with mocked sub-pruners
    let ledger_db = create_test_ledger_db();
    let pruner = LedgerPruner::new(ledger_db, None).unwrap();
    
    // Commit transactions up to version 3000
    commit_test_transactions(&ledger_db, 0, 3000);
    
    // Set pruner target to version 2000 (prune [0, 2000))
    pruner.set_target_version(2000);
    
    // Simulate OOM by injecting a failing sub-pruner in the middle of parallel execution
    // This would require test instrumentation to inject failures during par_iter()
    
    // Step 1: First prune attempt with simulated OOM during par_iter
    // Expected: Some sub-pruners complete, others fail
    let result = pruner.prune(1000);
    assert!(result.is_err(), "Prune should fail due to simulated OOM");
    
    // Step 2: Verify inconsistent state
    let event_progress = get_sub_pruner_progress(&ledger_db, "EventPrunerProgress");
    let txn_info_progress = get_sub_pruner_progress(&ledger_db, "TransactionInfoPrunerProgress");
    
    // Events pruned successfully
    assert_eq!(event_progress, 2000, "EventStorePruner completed");
    // Transaction info not pruned due to failure after OOM
    assert_eq!(txn_info_progress, 0, "TransactionInfoPruner failed");
    
    // Step 3: Attempt to query a version in the partially pruned range
    let query_result = ledger_db.get_transaction_by_version(1500, 3000, true);
    
    // This will fail inconsistently - transaction info exists but transaction data deleted
    match query_result {
        Err(e) => assert!(e.to_string().contains("NotFound"), 
            "Should fail with NotFound for deleted transaction"),
        Ok(_) => panic!("Should not succeed in inconsistent state"),
    }
    
    // Step 4: Retry prune operation (recovery)
    let retry_result = pruner.prune(1000);
    assert!(retry_result.is_ok(), "Retry should succeed");
    
    // Verify all sub-pruners now consistent
    assert_eq!(get_sub_pruner_progress(&ledger_db, "EventPrunerProgress"), 2000);
    assert_eq!(get_sub_pruner_progress(&ledger_db, "TransactionInfoPrunerProgress"), 2000);
}
```

## Notes

While the system includes recovery mechanisms through sub-pruner catch-up during initialization and idempotent prune operations, the window of database inconsistency violates the atomic state transition guarantee. The lack of coordination between parallel sub-pruners means that partial completion is architecturally possible, not just a rare edge case. This issue is exacerbated during periods of memory pressure when validators are under heavy loadâ€”precisely when database consistency is most critical for maintaining network health.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L75-76)
```rust
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L86-87)
```rust
            progress = current_batch_target_version;
            self.record_progress(progress);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L176-184)
```rust
            sub_pruners: vec![
                event_store_pruner,
                persisted_auxiliary_info_pruner,
                transaction_accumulator_pruner,
                transaction_auxiliary_data_pruner,
                transaction_info_pruner,
                transaction_pruner,
                write_set_pruner,
            ],
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L38-73)
```rust
        let mut batch = SchemaBatch::new();
        let candidate_transactions =
            self.get_pruning_candidate_transactions(current_progress, target_version)?;
        self.ledger_db
            .transaction_db()
            .prune_transaction_by_hash_indices(
                candidate_transactions.iter().map(|(_, txn)| txn.hash()),
                &mut batch,
            )?;
        self.ledger_db.transaction_db().prune_transactions(
            current_progress,
            target_version,
            &mut batch,
        )?;
        self.transaction_store
            .prune_transaction_summaries_by_account(&candidate_transactions, &mut batch)?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        if let Some(indexer_db) = self.internal_indexer_db.as_ref() {
            if indexer_db.transaction_enabled() {
                let mut index_batch = SchemaBatch::new();
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut index_batch)?;
                index_batch.put::<InternalIndexerMetadataSchema>(
                    &IndexerMetadataKey::TransactionPrunerProgress,
                    &IndexerMetadataValue::Version(target_version),
                )?;
                indexer_db.get_inner_db_ref().write_schemas(index_batch)?;
            } else {
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut batch)?;
            }
        }
        self.ledger_db.transaction_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L1068-1099)
```rust
    pub(super) fn get_transaction_with_proof(
        &self,
        version: Version,
        ledger_version: Version,
        fetch_events: bool,
    ) -> Result<TransactionWithProof> {
        self.error_if_ledger_pruned("Transaction", version)?;

        let proof = self
            .ledger_db
            .transaction_info_db()
            .get_transaction_info_with_proof(
                version,
                ledger_version,
                self.ledger_db.transaction_accumulator_db(),
            )?;

        let transaction = self.ledger_db.transaction_db().get_transaction(version)?;

        // If events were requested, also fetch those.
        let events = if fetch_events {
            Some(self.ledger_db.event_db().get_events_by_version(version)?)
        } else {
            None
        };

        Ok(TransactionWithProof {
            version,
            transaction,
            events,
            proof,
        })
```

**File:** storage/aptosdb/src/ledger_db/transaction_db.rs (L56-60)
```rust
    pub(crate) fn get_transaction(&self, version: Version) -> Result<Transaction> {
        self.db
            .get::<TransactionSchema>(&version)?
            .ok_or_else(|| AptosDbError::NotFound(format!("Txn {version}")))
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_info_pruner.rs (L41-54)
```rust
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_info_db_raw(),
            &DbMetadataKey::TransactionInfoPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionInfoPruner { ledger_db };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionInfoPruner."
        );
        myself.prune(progress, metadata_progress)?;
```
