# Audit Report

## Title
SSRF Vulnerability in NFT Metadata Crawler Enables Port Scanning of Internal Infrastructure

## Summary
The NFT metadata crawler's `JSONParser::parse()` function makes HTTP requests to user-controlled URIs from on-chain token metadata without validating against private IP addresses or internal hostnames. This Server-Side Request Forgery (SSRF) vulnerability allows attackers to use the crawler as a port scanner to map internal network topology and enumerate open ports through timing differences and error message analysis.

## Finding Description

The vulnerability exists in the NFT metadata crawler's URI processing pipeline. When a token is created on-chain, users can specify arbitrary URIs in the token metadata. The crawler automatically processes these URIs without proper validation:

**Attack Flow:**

1. **User-Controlled Input**: Token creators specify URIs in token metadata stored on-chain [1](#0-0) 

2. **URI Extraction**: The indexer extracts these URIs and stores them in the database [2](#0-1) 

3. **Automatic Processing**: The NFT metadata crawler Worker processes these URIs [3](#0-2) 

4. **Insufficient Validation**: The only validation is basic URI format checking and a configurable blacklist - no protection against private IP addresses [4](#0-3) 

5. **SSRF Execution**: `JSONParser::parse()` sends HEAD and GET requests to the attacker-controlled URI without IP validation [5](#0-4) 

6. **Information Leakage via get_uri_metadata**: The HEAD request reveals port states through timing and errors [6](#0-5) 

**Port Scanning Technique:**

Attackers can infer port states through:

- **Timing Differences**: 
  - Closed ports: Immediate connection refused (~milliseconds)
  - Open ports with HTTP: Quick response (~seconds) 
  - Filtered ports: Connection timeout (15 seconds for HEAD request) [7](#0-6) 

- **Error Messages**: Different `reqwest` errors reveal connection status:
  - "Connection refused" = port closed
  - "Connection timeout" = port filtered/no response
  - HTTP errors = port open with HTTP service
  - Protocol errors = port open with non-HTTP service

**Exploitable Targets:**

- Private IP ranges: `10.0.0.0/8`, `172.16.0.0/12`, `192.168.0.0/16`
- Localhost: `127.0.0.1`, `::1`
- Link-local addresses: `169.254.0.0/16`
- Cloud metadata endpoints: `http://169.254.169.254/latest/meta-data/`
- Internal service discovery via hostname resolution

## Impact Explanation

This vulnerability meets **High severity** criteria per the Aptos bug bounty program for the following reasons:

1. **Infrastructure Reconnaissance**: Enables complete mapping of internal network topology and service discovery, violating the security boundary between public blockchain data and private infrastructure.

2. **Precursor to Further Attacks**: Port scanning is typically the first step in a sophisticated attack chain. Knowledge of internal services, their ports, and response characteristics enables targeted exploitation of those services.

3. **Cloud Metadata Access**: Access to cloud provider metadata endpoints (AWS, GCP, Azure) could leak:
   - IAM credentials and tokens
   - Instance configuration
   - Security group information
   - Internal DNS entries

4. **Automatic Processing**: The crawler automatically processes token URIs without human review, making this a fully automated attack vector requiring no social engineering.

5. **Production Infrastructure**: The NFT metadata crawler is production infrastructure supporting the Aptos ecosystem's NFT functionality.

While this doesn't directly affect blockchain consensus or validator operations, it represents a **significant protocol violation** in the infrastructure security layer supporting the Aptos ecosystem.

## Likelihood Explanation

**Likelihood: High**

1. **No Privileges Required**: Any user can create a token on-chain with arbitrary URIs - no special permissions, stake, or validator access required.

2. **Trivial Exploitation**: The attack requires only:
   - Creating a token with URIs like `http://10.0.0.1:22`, `http://127.0.0.1:8080`, etc.
   - Waiting for automatic crawler processing
   - Observing timing differences or error messages

3. **Automatic Triggering**: The crawler automatically processes new token URIs via PubSub messages, requiring no additional interaction from the attacker.

4. **No Detection Barriers**: There are no rate limits, anomaly detection, or IP validation to prevent or detect the attack.

5. **High Value Target**: Internal infrastructure reconnaissance is a high-value capability for attackers targeting the Aptos ecosystem.

## Recommendation

Implement comprehensive SSRF protection by validating and filtering URIs before making HTTP requests:

```rust
// Add to ecosystem/nft-metadata-crawler/src/utils/uri_validator.rs
use std::net::{IpAddr, ToSocketAddrs};
use url::Url;

pub struct URIValidator;

impl URIValidator {
    /// Validates URI is safe for external HTTP requests
    pub fn validate_uri(uri: &str) -> anyhow::Result<()> {
        let parsed = Url::parse(uri)?;
        
        // Only allow HTTP/HTTPS schemes
        match parsed.scheme() {
            "http" | "https" => {},
            _ => return Err(anyhow::anyhow!("Invalid URI scheme")),
        }
        
        // Resolve hostname to IP addresses
        let host = parsed.host_str()
            .ok_or_else(|| anyhow::anyhow!("Missing host"))?;
        
        // Block localhost hostnames
        if ["localhost", "127.0.0.1", "::1"].contains(&host) {
            return Err(anyhow::anyhow!("Localhost access forbidden"));
        }
        
        // Resolve and validate IP addresses
        let socket_addr = format!("{}:80", host);
        for addr in socket_addr.to_socket_addrs()? {
            let ip = addr.ip();
            
            if Self::is_private_ip(&ip) {
                return Err(anyhow::anyhow!("Private IP address forbidden"));
            }
        }
        
        Ok(())
    }
    
    fn is_private_ip(ip: &IpAddr) -> bool {
        match ip {
            IpAddr::V4(ipv4) => {
                ipv4.is_private() || 
                ipv4.is_loopback() ||
                ipv4.is_link_local() ||
                ipv4.is_documentation() ||
                ipv4.is_broadcast() ||
                // Cloud metadata
                ipv4.octets() == [169, 254, 169, 254]
            },
            IpAddr::V6(ipv6) => {
                ipv6.is_loopback() ||
                ipv6.is_unspecified() ||
                ipv6.is_multicast()
            }
        }
    }
}
```

Then modify `JSONParser::parse()` and `ImageOptimizer::optimize()` to validate URIs:

```rust
// In json_parser.rs
pub async fn parse(
    uri: String,
    max_file_size_bytes: u32,
) -> anyhow::Result<(Option<String>, Option<String>, Value)> {
    PARSE_JSON_INVOCATION_COUNT.inc();
    
    // Validate URI before making requests
    URIValidator::validate_uri(&uri)?;
    
    let (mime, size) = get_uri_metadata(&uri).await?;
    // ... rest of implementation
}
```

Additionally:
- Implement rate limiting per asset owner to prevent mass scanning
- Log SSRF validation failures for monitoring
- Consider using a dedicated egress proxy with strict allowlisting
- Add monitoring for suspicious URI patterns in token metadata

## Proof of Concept

**Step 1: Create Token with Malicious URI (Move)**

```move
// Create a token with internal network URI for port scanning
script {
    use aptos_framework::aptos_token;
    
    fun create_scanner_token(creator: &signer) {
        // Create collection
        aptos_token::create_collection(
            creator,
            b"Scanner Collection",
            b"Test",
            b"https://example.com",
            1000,
            vector[false, false, false]
        );
        
        // Create token with internal IP URI for port 22 (SSH)
        aptos_token::create_token(
            creator,
            b"Scanner Collection",
            b"Port Scanner Token",
            b"Scanning port 22",
            1,
            b"http://10.0.0.1:22/metadata.json", // Internal IP - port 22
            creator_addr,
            0,
            0,
            vector[false, false, false, false, false],
            vector[],
            vector[],
            vector[]
        );
    }
}
```

**Step 2: Observe Crawler Behavior**

The crawler will automatically:
1. Send HEAD request to `http://10.0.0.1:22`
2. Send GET request to `http://10.0.0.1:22/metadata.json`

**Port State Detection:**

- If port 22 is **closed**: Connection refused error in ~10ms
- If port 22 is **open** (SSH): Connection succeeds but returns invalid HTTP â†’ protocol error
- If port 22 is **filtered**: Connection timeout after 15 seconds

By creating tokens with URIs targeting different internal IPs and ports (`:22`, `:80`, `:443`, `:3000`, `:5432`, `:6379`, `:8080`, etc.), an attacker can systematically map the internal network infrastructure.

**Step 3: Automate Scanning**

Create 100+ tokens with different internal IP/port combinations and analyze timing patterns and error messages to build a complete map of internal services.

---

**Notes:**

This vulnerability is a classic Server-Side Request Forgery (SSRF) issue that affects the NFT metadata crawler infrastructure component of the Aptos ecosystem. While it doesn't directly impact blockchain consensus or validator operations, it represents a serious infrastructure security vulnerability that could enable attackers to:

- Map internal network topology
- Discover internal services and their ports
- Access cloud provider metadata endpoints
- Use the crawler as a proxy for attacking internal services

The automatic processing of user-controlled URIs from on-chain data without proper validation creates an exploitable attack surface. The fix requires comprehensive URI validation including IP address filtering, scheme restrictions, and hostname validation before making any HTTP requests.

### Citations

**File:** crates/indexer/src/models/token_models/v2_token_utils.rs (L183-189)
```rust
pub struct TokenV2 {
    collection: ResourceReference,
    pub description: String,
    // These are set to private because we should never get name or uri directly
    name: String,
    uri: String,
}
```

**File:** crates/indexer/src/models/token_models/v2_token_datas.rs (L113-113)
```rust
            let token_uri = inner.get_uri_trunc();
```

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L103-108)
```rust
        if Url::parse(&self.asset_uri).is_err() {
            self.log_info("URI is invalid, skipping parse, marking as do_not_parse");
            self.model.set_do_not_parse(true);
            SKIP_URI_COUNT.with_label_values(&["invalid"]).inc();
            return Ok(());
        }
```

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L113-122)
```rust
            let json_uri = URIParser::parse(
                &self.parser_config.ipfs_prefix,
                &self.model.get_asset_uri(),
                self.parser_config.ipfs_auth_key.as_deref(),
            )
            .unwrap_or_else(|_| {
                self.log_warn("Failed to parse asset_uri", None);
                PARSE_URI_TYPE_COUNT.with_label_values(&["other"]).inc();
                self.model.get_asset_uri()
            });
```

**File:** ecosystem/nft-metadata-crawler/src/utils/json_parser.rs (L27-64)
```rust
    pub async fn parse(
        uri: String,
        max_file_size_bytes: u32,
    ) -> anyhow::Result<(Option<String>, Option<String>, Value)> {
        PARSE_JSON_INVOCATION_COUNT.inc();
        let (mime, size) = get_uri_metadata(&uri).await?;
        if ImageFormat::from_mime_type(&mime).is_some() {
            FAILED_TO_PARSE_JSON_COUNT
                .with_label_values(&["found image instead"])
                .inc();
            return Err(anyhow::anyhow!(format!(
                "JSON parser received image file: {}, skipping",
                mime
            )));
        } else if size > max_file_size_bytes {
            FAILED_TO_PARSE_JSON_COUNT
                .with_label_values(&["json file too large"])
                .inc();
            return Err(anyhow::anyhow!(format!(
                "JSON parser received file too large: {} bytes, skipping",
                size
            )));
        }

        let op = || {
            async {
                info!(asset_uri = uri, "Sending request for asset_uri");

                let client = Client::builder()
                    .timeout(Duration::from_secs(MAX_JSON_REQUEST_RETRY_SECONDS))
                    .build()
                    .context("Failed to build reqwest client")?;

                let response = client
                    .get(uri.trim())
                    .send()
                    .await
                    .context("Failed to get JSON")?;
```

**File:** ecosystem/nft-metadata-crawler/src/lib.rs (L17-38)
```rust
pub async fn get_uri_metadata(url: &str) -> anyhow::Result<(String, u32)> {
    let client = Client::builder()
        .timeout(Duration::from_secs(MAX_HEAD_REQUEST_RETRY_SECONDS))
        .build()
        .context("Failed to build reqwest client")?;
    let request = client.head(url.trim());
    let response = request.send().await?;
    let headers = response.headers();

    let mime_type = headers
        .get(header::CONTENT_TYPE)
        .map(|value| value.to_str().unwrap_or("text/plain"))
        .unwrap_or("text/plain")
        .to_string();
    let size = headers
        .get(header::CONTENT_LENGTH)
        .and_then(|value| value.to_str().ok())
        .and_then(|s| s.parse::<u32>().ok())
        .unwrap_or(0);

    Ok((mime_type, size))
}
```

**File:** ecosystem/nft-metadata-crawler/src/utils/constants.rs (L7-8)
```rust
/// Allocate 15 seconds for a HEAD request
pub const MAX_HEAD_REQUEST_RETRY_SECONDS: u64 = 15;
```
