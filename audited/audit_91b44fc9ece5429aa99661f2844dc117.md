# Audit Report

## Title
Logical Time Update Before State Sync Verification Causes Validator Inconsistency

## Summary
The `sync_to_target` method in `ExecutionProxy` unconditionally updates the logical time tracker before verifying state sync success, while `sync_for_duration` correctly checks for success first. This allows validators to incorrectly believe they've synced to a target after a partial sync failure, causing them to skip future sync attempts and remain in an inconsistent state compared to validators that successfully synced.

## Finding Description

In [1](#0-0) , the `sync_to_target` method updates `latest_logical_time` unconditionally on line 222, even when the state sync operation fails. This is inconsistent with `sync_for_duration` shown in [2](#0-1) , which only updates the logical time when `result` is `Ok`.

The vulnerability manifests through the following sequence:

1. Consensus calls `sync_to_target` with a target ledger info at version V
2. State sync begins processing chunks through the pipeline shown in [3](#0-2) 
3. Some chunks successfully commit to storage (e.g., versions up to V-100)
4. An error occurs during chunk commit as shown in [4](#0-3)  (e.g., storage error, network timeout, executor failure)
5. The error propagates back to consensus through [5](#0-4) 
6. Despite the error, `latest_logical_time` was already updated to the target on line 222
7. The next time consensus calls `sync_to_target` with the same target, the check in [6](#0-5)  returns early with success
8. The validator remains stuck at version V-100 while believing it's at version V

This breaks the **Deterministic Execution** invariant: validators that successfully synced to version V will have different state than validators that experienced partial sync failures, yet both believe they're at the same logical time.

## Impact Explanation

This vulnerability qualifies as **Critical Severity** under Aptos bug bounty criteria for multiple reasons:

**Consensus Safety Violation**: The core issue violates consensus safety guarantees. When validators have different committed states but identical logical times, they will produce different state roots for subsequent blocks. This can cause consensus to fail or result in a chain split requiring manual intervention or a hard fork to resolve.

**Non-Recoverable Network Partition**: As shown in [7](#0-6) , epoch changes expect state sync to succeed and will panic otherwise. However, affected validators will skip the sync due to their incorrect logical time, causing them to operate on stale state during the new epoch while other validators operate on fresh state. This creates an irreconcilable divergence.

**State Consistency Violation**: The bug breaks the atomic state transition guarantee. Partial commits should either be rolled back or retried, but the incorrect logical time update prevents retry logic from functioning, leaving validators in a permanently inconsistent intermediate state.

## Likelihood Explanation

This vulnerability has **Medium-to-High likelihood** of occurring in production:

**Triggering Conditions**: State sync can fail for numerous realistic reasons without requiring attacker involvement:
- Network interruptions during chunk retrieval from peers
- Storage I/O errors during commit operations  
- Resource exhaustion (disk space, memory)
- Timeout conditions during long sync operations
- Executor errors during chunk execution shown in [8](#0-7) 

**Frequency**: State sync operations occur during:
- Fast-forward sync in [9](#0-8) 
- Epoch transitions as shown above
- Consensus observer fallback scenarios
- Validators catching up after downtime

**Detection Difficulty**: The bug is silent - affected validators don't crash or log errors indicating their inconsistent state. They appear to function normally until consensus divergence manifests in block voting disagreements.

## Recommendation

Fix the logical time update to only occur on successful state sync, matching the pattern used in `sync_for_duration`:

```rust
async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
    let mut latest_logical_time = self.write_mutex.lock().await;
    let target_logical_time =
        LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

    self.executor.finish();

    if *latest_logical_time >= target_logical_time {
        warn!(
            "State sync target {:?} is lower than already committed logical time {:?}",
            target_logical_time, *latest_logical_time
        );
        return Ok(());
    }

    if let Some(inner) = self.state.read().as_ref() {
        let block_timestamp = target.commit_info().timestamp_usecs();
        inner
            .payload_manager
            .notify_commit(block_timestamp, Vec::new());
    }

    fail_point!("consensus::sync_to_target", |_| {
        Err(anyhow::anyhow!("Injected error in sync_to_target").into())
    });

    let result = monitor!(
        "sync_to_target",
        self.state_sync_notifier.sync_to_target(target).await
    );

    // FIXED: Only update logical time on success
    if result.is_ok() {
        *latest_logical_time = target_logical_time;
    }

    self.executor.reset()?;

    result.map_err(|error| {
        let anyhow_error: anyhow::Error = error.into();
        anyhow_error.into()
    })
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_sync_to_target_logical_time_inconsistency() {
    use consensus::state_computer::ExecutionProxy;
    use aptos_executor_types::BlockExecutorTrait;
    use aptos_consensus_notifications::ConsensusNotificationSender;
    use std::sync::Arc;
    
    // Setup: Create ExecutionProxy with mocked dependencies
    let mock_executor = Arc::new(MockBlockExecutor::new());
    let mock_notifier = Arc::new(MockConsensusNotifier::new());
    let execution_proxy = ExecutionProxy::new(
        mock_executor.clone(),
        Arc::new(MockTxnNotifier),
        mock_notifier.clone(),
        Default::default(),
        false,
        None,
    );
    
    // Setup: Configure mock to fail state sync after partial commits
    mock_notifier.configure_partial_sync_failure(
        target_version: 1000,
        partial_commit_version: 900, // Commits up to version 900
        then_fail: true,
    );
    
    // Create target ledger info at version 1000
    let target = create_test_ledger_info_with_sigs(epoch: 1, round: 10, version: 1000);
    
    // Step 1: Call sync_to_target - it will partially sync then fail
    let result = execution_proxy.sync_to_target(target.clone()).await;
    
    // Verify: sync_to_target returned an error
    assert!(result.is_err(), "State sync should have failed");
    
    // Verify: Storage only has data up to version 900
    let actual_version = mock_executor.get_committed_version();
    assert_eq!(actual_version, 900, "Only partial data should be committed");
    
    // Step 2: Call sync_to_target again with same target
    // BUG: This will skip syncing because logical time was incorrectly updated
    let result2 = execution_proxy.sync_to_target(target.clone()).await;
    
    // Verify: Second call returns success (incorrectly)
    assert!(result2.is_ok(), "Second sync incorrectly succeeds due to logical time check");
    
    // Verify: Storage still only has data up to version 900
    let actual_version_after = mock_executor.get_committed_version();
    assert_eq!(actual_version_after, 900, "Data should still be at partial version");
    
    // Impact: This validator now believes it's at version 1000 but only has version 900
    // It will produce different state roots than validators that successfully synced
    // This breaks consensus safety guarantees
}
```

## Notes

The bug exists because `sync_for_duration` and `sync_to_target` have inconsistent error handling patterns. The former correctly updates logical time only on success, while the latter updates unconditionally. This inconsistency suggests the bug was introduced during refactoring or exists from the initial implementation.

The vulnerability affects all consensus operations that rely on `sync_to_target`, including fast-forward sync, epoch transitions, and consensus observer fallback mechanisms. Any validator experiencing transient failures during these operations can become permanently inconsistent with the network.

### Citations

**File:** consensus/src/state_computer.rs (L158-163)
```rust
        // Update the latest logical time
        if let Ok(latest_synced_ledger_info) = &result {
            let ledger_info = latest_synced_ledger_info.ledger_info();
            let synced_logical_time = LogicalTime::new(ledger_info.epoch(), ledger_info.round());
            *latest_logical_time = synced_logical_time;
        }
```

**File:** consensus/src/state_computer.rs (L177-233)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // The pipeline phase already committed beyond the target block timestamp, just return.
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }

        // This is to update QuorumStore with the latest known commit in the system,
        // so it can set batches expiration accordingly.
        // Might be none if called in the recovery path, or between epoch stop and start.
        if let Some(inner) = self.state.read().as_ref() {
            let block_timestamp = target.commit_info().timestamp_usecs();
            inner
                .payload_manager
                .notify_commit(block_timestamp, Vec::new());
        }

        // Inject an error for fail point testing
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Invoke state sync to synchronize to the specified target. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );

        // Update the latest logical time
        *latest_logical_time = target_logical_time;

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L232-262)
```rust
        // Spawn the executor that executes/applies storage data chunks
        let runtime = runtime.map(|runtime| runtime.handle().clone());
        let executor_handle = spawn_executor(
            chunk_executor.clone(),
            error_notification_sender.clone(),
            executor_listener,
            ledger_updater_notifier,
            pending_data_chunks.clone(),
            runtime.clone(),
        );

        // Spawn the ledger updater that updates the ledger in storage
        let ledger_updater_handle = spawn_ledger_updater(
            chunk_executor.clone(),
            error_notification_sender.clone(),
            ledger_updater_listener,
            committer_notifier,
            pending_data_chunks.clone(),
            runtime.clone(),
        );

        // Spawn the committer that commits executed (but pending) chunks
        let committer_handle = spawn_committer(
            chunk_executor.clone(),
            error_notification_sender.clone(),
            committer_listener,
            commit_post_processor_notifier,
            pending_data_chunks.clone(),
            runtime.clone(),
            storage.reader.clone(),
        );
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L764-774)
```rust
                Err(error) => {
                    // Send an error notification to the driver (we failed to commit the chunk)
                    let error = format!("Failed to commit executed chunk! Error: {:?}", error);
                    handle_storage_synchronizer_error(
                        notification_metadata,
                        error,
                        &error_notification_sender,
                        &pending_data_chunks,
                    )
                    .await;
                },
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L199-206)
```rust
        // Process the response
        match callback_receiver.await {
            Ok(response) => response.get_result(),
            Err(error) => Err(Error::UnexpectedErrorEncountered(format!(
                "Sync to target failure: {:?}",
                error
            ))),
        }
```

**File:** consensus/src/epoch_manager.rs (L558-565)
```rust
        self.execution_client
            .sync_to_target(ledger_info.clone())
            .await
            .context(format!(
                "[EpochManager] State sync to new epoch {}",
                ledger_info
            ))
            .expect("Failed to sync to new epoch");
```

**File:** execution/executor/src/chunk_executor/mod.rs (L261-288)
```rust
    fn commit_chunk_impl(&self) -> Result<ExecutedChunk> {
        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__total"]);
        let chunk = {
            let _timer =
                CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__next_chunk_to_commit"]);
            self.commit_queue.lock().next_chunk_to_commit()?
        };

        let output = chunk.output.expect_complete_result();
        let num_txns = output.num_transactions_to_commit();
        if chunk.ledger_info_opt.is_some() || num_txns != 0 {
            let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__save_txns"]);
            // TODO(aldenhu): remove since there's no practical strategy to recover from this error.
            fail_point!("executor::commit_chunk", |_| {
                Err(anyhow::anyhow!("Injected error in commit_chunk"))
            });
            self.db.writer.save_transactions(
                output.as_chunk_to_commit(),
                chunk.ledger_info_opt.as_ref(),
                false, // sync_commit
            )?;
        }

        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__dequeue_and_return"]);
        self.commit_queue.lock().dequeue_committed()?;

        Ok(chunk)
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L512-514)
```rust
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```
