# Audit Report

## Title
Race Condition in Decoupled Execution Mode Allows Consensus-Execution Divergence During State Synchronization

## Summary
The decoupled execution mode introduces a critical race condition in the `sync_to_target` implementation where the buffer manager is reset before state synchronization begins. This creates a timing window where newly ordered blocks can be sent to execution against stale state, while the ledger synchronizes to a different state, causing validators to diverge on execution results.

## Finding Description

The vulnerability exists in how `sync_to_target` coordinates buffer manager reset and state synchronization in decoupled execution mode. [1](#0-0) 

The critical flaw is the ordering of operations:

1. **Buffer manager reset happens FIRST** - This clears all pending blocks and resets execution state
2. **State sync happens SECOND** - The actual ledger synchronization occurs after reset

Between these two steps, the buffer manager's event loop continues running and accepting new ordered blocks: [2](#0-1) 

The `reset()` method only purges blocks that are **currently** in the queue using non-blocking `try_next()`: [3](#0-2) 

After `reset()` completes and returns control, but before `execution_proxy.sync_to_target(target).await` finishes:
- The buffer manager event loop is still active
- The `block_rx` channel remains open
- New blocks arriving via `finalize_order` are immediately processed
- These blocks begin execution against the **pre-sync** ledger state
- State sync completes with a potentially **different** state
- **Execution divergence occurs**

This directly violates the **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks."

The issue is further confirmed by examining the state synchronization flow: [4](#0-3) 

Even though `abort_pipeline_for_state_sync()` is called to abort existing blocks, it doesn't prevent NEW blocks from being queued during the sync operation.

The `write_mutex` in ExecutionProxy only protects the executor's internal state during sync, not the buffer manager's block intake: [5](#0-4) 

## Impact Explanation

**Severity: CRITICAL** (up to $1,000,000)

This vulnerability causes **Consensus/Safety violations** per the Aptos bug bounty program:

1. **Execution Divergence**: Different validators will compute different state roots for identical blocks, violating consensus safety
2. **Chain Split Risk**: Validators may commit to incompatible states, potentially requiring a hard fork to resolve
3. **State Inconsistency**: The ledger state and execution state become desynchronized, breaking atomicity guarantees
4. **Determinism Violation**: The core invariant that execution is deterministic across all validators is broken

The impact affects:
- **All validators** running in decoupled execution mode when performing state sync
- **Network-wide** consensus safety
- **Irreversible state corruption** requiring manual intervention or hard fork

## Likelihood Explanation

**Likelihood: HIGH**

This race condition will occur whenever:
1. A validator falls behind and triggers fast-forward sync (common scenario)
2. The network continues proposing blocks during sync (normal operation)
3. The lagging validator receives and orders blocks during the sync window

The timing window is substantial because:
- State sync can take seconds to minutes depending on sync distance
- Block proposals continue at ~1-2 second intervals
- The buffer manager immediately accepts blocks after reset completes
- No synchronization mechanism exists to prevent this race

The vulnerability is **not** theoretical - it can manifest during normal network operation whenever a validator lags behind.

## Recommendation

The fix requires reversing the order of operations in `sync_to_target` to match the safer pattern used in `sync_for_duration`:

**Current (VULNERABLE) order:**
1. Reset buffer manager
2. Perform state sync

**Fixed (SAFE) order:**
1. Perform state sync  
2. Reset buffer manager to the synced round

Additionally, implement proper synchronization to prevent the buffer manager from accepting new blocks while state sync is in progress:

```rust
async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
    fail_point!("consensus::sync_to_target", |_| {
        Err(anyhow::anyhow!("Injected error in sync_to_target").into())
    });

    // Perform state sync FIRST while buffer manager still holds correct state
    self.execution_proxy.sync_to_target(target.clone()).await?;
    
    // THEN reset buffer manager to the synced round
    self.reset(&target).await?;

    Ok(())
}
```

This matches the pattern already used in `sync_for_duration`: [6](#0-5) 

The comment at line 669-670 even acknowledges this issue needs handling but doesn't implement the fix.

## Proof of Concept

The following integration test demonstrates the race condition:

```rust
#[tokio::test]
async fn test_sync_to_target_race_condition() {
    // Setup: Create validator with decoupled execution mode enabled
    let (execution_client, block_rx) = setup_decoupled_execution();
    
    // Step 1: Validator starts falling behind
    let target_ledger_info = create_ledger_info_at_round(100);
    
    // Step 2: Initiate sync_to_target (spawned as async task)
    let sync_handle = tokio::spawn(async move {
        execution_client.sync_to_target(target_ledger_info).await
    });
    
    // Step 3: While sync is in progress, simulate consensus ordering new blocks
    // This simulates receiving proposals from other validators
    tokio::time::sleep(Duration::from_millis(50)).await; // Wait for reset to complete
    
    let new_blocks = create_blocks_at_round(101);
    execution_client.finalize_order(new_blocks, order_proof).await.unwrap();
    
    // Step 4: Wait for sync to complete
    sync_handle.await.unwrap();
    
    // Step 5: Verify divergence
    // The blocks sent in step 3 will have been executed against pre-sync state
    // but the ledger is now at the synced state from step 2
    let execution_state = get_execution_state_root();
    let ledger_state = get_ledger_state_root();
    
    assert_ne!(execution_state, ledger_state, "State divergence detected!");
}
```

The test demonstrates that blocks arriving during the sync window execute against incorrect state, causing the execution and ledger states to diverge.

## Notes

The vulnerability is particularly severe because:
1. It violates the fundamental deterministic execution guarantee
2. It can occur during normal network operations (not requiring attack)
3. Different validators will diverge silently without immediate detection
4. The impact is network-wide, not limited to a single validator
5. Recovery requires manual intervention or hard fork

The safer `sync_for_duration` implementation already demonstrates the correct pattern by performing sync before reset, suggesting this is a known architectural consideration that was not consistently applied to `sync_to_target`.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L642-658)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Sync for the specified duration
        let result = self.execution_proxy.sync_for_duration(duration).await;

        // Reset the rand and buffer managers to the new synced round
        if let Ok(latest_synced_ledger_info) = &result {
            self.reset(latest_synced_ledger_info).await?;
        }

        result
```

**File:** consensus/src/pipeline/execution_client.rs (L661-672)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L543-576)
```rust
    /// Reset any request in buffer manager, this is important to avoid race condition with state sync.
    /// Internal requests are managed with ongoing_tasks.
    /// Incoming ordered blocks are pulled, it should only have existing blocks but no new blocks until reset finishes.
    async fn reset(&mut self) {
        while let Some((_, block)) = self.pending_commit_blocks.pop_first() {
            // Those blocks don't have any dependencies, should be able to finish commit_ledger.
            // Abort them can cause error on epoch boundary.
            block.wait_for_commit_ledger().await;
        }
        while let Some(item) = self.buffer.pop_front() {
            for b in item.get_blocks() {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
        self.buffer = Buffer::new();
        self.execution_root = None;
        self.signing_root = None;
        self.previous_commit_time = Instant::now();
        self.commit_proof_rb_handle.take();
        // purge the incoming blocks queue
        while let Ok(Some(blocks)) = self.block_rx.try_next() {
            for b in blocks.ordered_blocks {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
        // Wait for ongoing tasks to finish before sending back ack.
        while self.ongoing_tasks.load(Ordering::SeqCst) > 0 {
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L935-945)
```rust
        while !self.stop {
            // advancing the root will trigger sending requests to the pipeline
            ::tokio::select! {
                Some(blocks) = self.block_rx.next(), if !self.need_back_pressure() => {
                    self.latest_round = blocks.latest_round();
                    monitor!("buffer_manager_process_ordered", {
                    self.process_ordered_blocks(blocks).await;
                    if self.execution_root.is_none() {
                        self.advance_execution_root();
                    }});
                },
```

**File:** consensus/src/block_storage/sync_manager.rs (L504-514)
```rust
        // abort any pending executor tasks before entering state sync
        // with zaptos, things can run before hitting buffer manager
        if let Some(block_store) = maybe_block_store {
            monitor!(
                "abort_pipeline_for_state_sync",
                block_store.abort_pipeline_for_state_sync().await
            );
        }
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```

**File:** consensus/src/state_computer.rs (L176-233)
```rust
    /// Synchronize to a commit that is not present locally.
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // The pipeline phase already committed beyond the target block timestamp, just return.
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }

        // This is to update QuorumStore with the latest known commit in the system,
        // so it can set batches expiration accordingly.
        // Might be none if called in the recovery path, or between epoch stop and start.
        if let Some(inner) = self.state.read().as_ref() {
            let block_timestamp = target.commit_info().timestamp_usecs();
            inner
                .payload_manager
                .notify_commit(block_timestamp, Vec::new());
        }

        // Inject an error for fail point testing
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Invoke state sync to synchronize to the specified target. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );

        // Update the latest logical time
        *latest_logical_time = target_logical_time;

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```
