# Audit Report

## Title
Hash Collision Vulnerability in Token Data ID Generation Enables Indexer Data Corruption

## Summary
The `token_data_id_hash` computation in the Aptos indexer is vulnerable to delimiter collision attacks. The hash is generated by concatenating creator address, collection name, and token name with "::" as a delimiter, but since collection and token names can contain "::" characters (only length-validated on-chain), distinct tokens can produce identical hashes, causing data corruption in the indexer's `all_current_token_datas` HashMap and database.

## Finding Description
The vulnerability exists in the token data ID hash generation mechanism used throughout the indexer: [1](#0-0) 

The hash is computed from the string representation of `TokenDataIdType`: [2](#0-1) 

This creates the format: `<standardized_creator>::<collection>::<name>`, which is then SHA256-hashed: [3](#0-2) 

**The Problem:** The on-chain Move framework only validates the LENGTH of collection and token names, not their content: [4](#0-3) 

This allows tokens with "::" in their names, creating hash collisions:

**Example Collision:**
- Token A: creator=`0x1`, collection=`"Art"`, name=`"Work::2"` → hash(`"0x0000...0001::Art::Work::2"`)
- Token B: creator=`0x1`, collection=`"Art::Work"`, name=`"2"` → hash(`"0x0000...0001::Art::Work::2"`)

Both produce **identical hashes**.

**Impact on Indexer Processing:**

In the transaction processor, the HashMap uses `token_data_id_hash` as the key for deduplication: [5](#0-4) 

When processing transactions: [6](#0-5) 

If two distinct tokens with colliding hashes appear in the same batch, one overwrites the other in the HashMap. The database insert operation uses `token_data_id_hash` as the primary key: [7](#0-6) 

Result: One token's metadata is permanently lost or corrupted in the indexer database, making it invisible or incorrectly displayed to applications querying the indexer.

## Impact Explanation
This is a **Medium Severity** vulnerability under the Aptos bug bounty program: "State inconsistencies requiring intervention."

**Concrete Impact:**
1. **Data Corruption**: Tokens with colliding hashes cannot coexist in the indexer database
2. **Loss of Token Visibility**: One of the colliding tokens becomes invisible to applications querying the indexer
3. **Marketplace Disruption**: NFT marketplaces relying on indexer data show incorrect token counts, wrong metadata, or missing tokens
4. **Manual Intervention Required**: Database administrators must manually identify and fix corrupted records
5. **Ecosystem-Wide Effect**: All applications using the Aptos indexer API are affected

While this does not affect on-chain state or consensus, the indexer is critical infrastructure for the Aptos ecosystem. Most applications (wallets, marketplaces, explorers) rely on it for token queries.

## Likelihood Explanation
**HIGH Likelihood**

**Attacker Requirements:**
- Ability to create tokens (standard user permission)
- Knowledge of existing token names to craft collisions
- No special privileges required

**Exploitation Complexity:**
- Trivial - simply create a token with "::" in the collection or name field
- Deterministic - collisions are guaranteed by design
- No timing requirements or race conditions

**Real-World Scenario:**
1. Attacker identifies a popular NFT collection (e.g., creator=`0xabc`, collection=`"Aptos Monkeys"`, name=`"#1234"`)
2. Attacker creates their own collection with name=`"Aptos Monkeys::#1234"` and a token with name=`""` (empty string)
3. This produces the same hash as the original token
4. The indexer now shows corrupted data for one of these tokens

## Recommendation
**Immediate Fix:** Use a proper encoding scheme that eliminates delimiter ambiguity.

**Option 1 - BCS Serialization (Recommended):**
```rust
pub fn to_hash(&self) -> String {
    // Serialize the struct using BCS, then hash
    let serialized = bcs::to_bytes(&(
        standardize_address(self.creator.as_str()),
        &self.collection,
        &self.name
    )).expect("BCS serialization failed");
    hex::encode(sha2::Sha256::digest(&serialized))
}
```

**Option 2 - Length-Prefixed Encoding:**
```rust
pub fn to_hash(&self) -> String {
    let creator = standardize_address(self.creator.as_str());
    let input = format!("{}:{}:{}:{}:{}:{}",
        creator.len(), creator,
        self.collection.len(), self.collection,
        self.name.len(), self.name
    );
    hash_str(&input)
}
```

**Option 3 - Escape Delimiters:**
```rust
fn escape_delimiter(s: &str) -> String {
    s.replace("\\", "\\\\").replace("::", "\\::")
}

impl fmt::Display for TokenDataIdType {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        write!(f, "{}::{}::{}",
            standardize_address(self.creator.as_str()),
            escape_delimiter(&self.collection),
            escape_delimiter(&self.name)
        )
    }
}
```

**Migration Consideration:** Changing the hash function requires re-indexing all existing token data. The team should plan a coordinated migration.

## Proof of Concept

**Move Test demonstrating collision:**

```move
#[test_only]
module test_addr::hash_collision_poc {
    use aptos_token::token;
    use std::string;
    use std::signer;
    
    #[test(creator = @0x123)]
    public fun test_token_hash_collision(creator: &signer) {
        // Initialize token store
        token::initialize_token_store(creator);
        token::opt_in_direct_transfer(creator, true);
        
        // Create collection "Art"
        token::create_collection(
            creator,
            string::utf8(b"Art"),
            string::utf8(b"Art Collection"),
            string::utf8(b"https://art.com"),
            1000,
            vector[false, false, false]
        );
        
        // Create Token A with name containing "::"
        let token_a_id = token::create_tokendata(
            creator,
            string::utf8(b"Art"),
            string::utf8(b"Work::2"),  // Name contains "::"
            string::utf8(b"Token A"),
            1,
            string::utf8(b"https://token-a.com"),
            @0x123,
            100, 100,
            token::create_token_mutability_config(&vector[false, false, false, false, false]),
            vector[], vector[], vector[]
        );
        
        // Create collection "Art::Work"
        token::create_collection(
            creator,
            string::utf8(b"Art::Work"),  // Collection contains "::"
            string::utf8(b"Art Work Collection"),
            string::utf8(b"https://artwork.com"),
            1000,
            vector[false, false, false]
        );
        
        // Create Token B
        let token_b_id = token::create_tokendata(
            creator,
            string::utf8(b"Art::Work"),
            string::utf8(b"2"),
            string::utf8(b"Token B"),
            1,
            string::utf8(b"https://token-b.com"),
            @0x123,
            100, 100,
            token::create_token_mutability_config(&vector[false, false, false, false, false]),
            vector[], vector[], vector[]
        );
        
        // Both tokens are valid and distinct on-chain
        // But in the indexer, they produce the same hash:
        // 0x0000...0123::Art::Work::2
        // This PoC demonstrates the on-chain validity of colliding tokens
    }
}
```

**Indexer Verification (Rust):**
```rust
#[test]
fn test_hash_collision() {
    use crate::models::token_models::token_utils::TokenDataIdType;
    
    let token_a = TokenDataIdType {
        creator: "0x123".to_string(),
        collection: "Art".to_string(),
        name: "Work::2".to_string(),
    };
    
    let token_b = TokenDataIdType {
        creator: "0x123".to_string(),
        collection: "Art::Work".to_string(),
        name: "2".to_string(),
    };
    
    let hash_a = token_a.to_hash();
    let hash_b = token_b.to_hash();
    
    // This assertion will PASS, demonstrating the collision
    assert_eq!(hash_a, hash_b, "Hash collision detected!");
    println!("Token A hash: {}", hash_a);
    println!("Token B hash: {}", hash_b);
    println!("Both distinct tokens produce identical hash!");
}
```

## Notes
- This vulnerability affects Token V1 (0x3::token). Token V2 may have similar issues and should be audited separately
- The blockchain consensus is NOT affected - this is purely an indexer data corruption issue
- Applications that bypass the indexer and query on-chain storage directly are unaffected
- The collision is deterministic and can be weaponized to target specific popular tokens
- A malicious actor could systematically create collisions for high-value NFTs to disrupt marketplaces

### Citations

**File:** crates/indexer/src/models/token_models/token_utils.rs (L46-48)
```rust
    pub fn to_hash(&self) -> String {
        hash_str(&self.to_string())
    }
```

**File:** crates/indexer/src/models/token_models/token_utils.rs (L67-77)
```rust
impl fmt::Display for TokenDataIdType {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        write!(
            f,
            "{}::{}::{}",
            standardize_address(self.creator.as_str()),
            self.collection,
            self.name
        )
    }
}
```

**File:** crates/indexer/src/util.rs (L19-21)
```rust
pub fn hash_str(val: &str) -> String {
    hex::encode(sha2::Sha256::digest(val.as_bytes()))
}
```

**File:** aptos-move/framework/aptos-token/sources/token.move (L1543-1545)
```text
        assert!(collection.length() <= MAX_COLLECTION_NAME_LENGTH, error::invalid_argument(ECOLLECTION_NAME_TOO_LONG));
        assert!(name.length() <= MAX_NFT_NAME_LENGTH, error::invalid_argument(ENFT_NAME_TOO_LONG));
        TokenDataId { creator, collection, name }
```

**File:** crates/indexer/src/processors/token_processor.rs (L877-878)
```rust
        let mut all_current_token_datas: HashMap<TokenDataIdHash, CurrentTokenData> =
            HashMap::new();
```

**File:** crates/indexer/src/processors/token_processor.rs (L908-908)
```rust
            all_current_token_datas.extend(current_token_datas);
```

**File:** crates/indexer/src/models/token_models/token_datas.rs (L46-47)
```rust
#[diesel(primary_key(token_data_id_hash))]
#[diesel(table_name = current_token_datas)]
```
