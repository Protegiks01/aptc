# Audit Report

## Title
Memory Exhaustion DoS in Backup Restore via Unbounded Record Size Allocation

## Summary
The `read_record_bytes()` function in the backup restore system reads a 4-byte size field from backup files and immediately allocates that amount of memory without validation. An attacker who can modify backup files can set record sizes to 0x7FFFFFFF (2GB) or larger, causing memory exhaustion that prevents legitimate backup restoration and could block network recovery during emergency scenarios.

## Finding Description

The backup restoration process is critical infrastructure for recovering Aptos nodes from persistent storage. When restoring state snapshots or transactions, the system reads records from backup files using the `read_record_bytes()` function. [1](#0-0) 

The vulnerability occurs at line 54 where a 4-byte size field is read from the backup file and converted to a `usize`, and at line 60 where `BytesMut::with_capacity(record_size)` immediately allocates memory based on this untrusted value. There is no validation that `record_size` is reasonable or within expected bounds.

The attack flow:

1. **Attacker Prerequisites**: Attacker gains write access to backup storage through compromised cloud credentials, supply chain attack, MITM during backup creation/transfer, or insider threat.

2. **Malicious Backup Creation**: Attacker modifies a backup blob file to contain records with size fields set to 0x7FFFFFFF (2,147,483,647 bytes = ~2GB) or 0xFFFFFFFF (4,294,967,295 bytes = ~4GB). A single blob file can contain thousands of such records.

3. **Memory Allocation Before Verification**: During restore, the state snapshot restoration process calls `read_state_value()`: [2](#0-1) 

This function reads ALL records from a blob file (line 261) and accumulates them in a vector BEFORE any cryptographic verification occurs. The verification only happens later: [3](#0-2) 

At line 191, `read_state_value()` reads all records with unbounded allocations. Only at line 213 does `add_chunk()` verify the cryptographic proofs.

4. **Memory Exhaustion**: For each malicious record, the system attempts to allocate 2-4GB. With thousands of records per file and multiple files processed concurrently, memory is exhausted before verification can reject the invalid data. The node crashes with OOM, preventing backup restoration.

The same vulnerability affects transaction and epoch ending restore processes: [4](#0-3) [5](#0-4) 

**Invariant Violation**: This breaks Critical Invariant #9: "Resource Limits: All operations must respect gas, storage, and computational limits." The restore process allocates unbounded memory based on untrusted external input without validation.

## Impact Explanation

This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria:

- **Validator node slowdowns/crashes**: Attempting to restore from compromised backups causes immediate OOM crashes
- **Availability impact**: Prevents legitimate backup restoration, which is critical for network recovery
- **Emergency scenario amplification**: During disasters when multiple validators need to restore simultaneously, a compromised backup repository could prevent entire network recovery

While not reaching "Total loss of liveness" (Critical), it significantly impacts the ability to recover from catastrophic failures, which is a key resilience mechanism for blockchain networks.

The impact is severe in scenarios where:
- Network-wide state corruption requires coordinated restore from backup
- Validators bootstrap new nodes from backup during rapid scaling
- Disaster recovery procedures depend on backup restoration
- Supply chain attacks compromise backup data before distribution

## Likelihood Explanation

**Likelihood: Medium**

Prerequisites for exploitation:
1. Write access to backup storage (cloud credentials, MITM, supply chain, insider)
2. Operators attempting to restore from the compromised backup

While this requires elevated access compared to network-level attacks, several realistic scenarios exist:

- **Compromised cloud storage**: Stolen AWS/GCS credentials could allow backup modification
- **Supply chain attacks**: Malicious backups distributed during disaster recovery coordination
- **Insider threats**: Malicious operator access to backup infrastructure  
- **MITM attacks**: If backup transfers don't use integrity verification during download
- **Emergency scenarios**: Operators downloading backups from untrusted/compromised sources during crisis

The lack of size validation violates defense-in-depth principles. Even if backup storage is "trusted," input validation is a fundamental security control that should never be omitted for external data.

## Recommendation

Implement maximum record size validation before memory allocation:

```rust
const MAX_RECORD_SIZE: usize = 128 * 1024 * 1024; // 128MB, matching max_chunk_size default

async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
    let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
    
    // read record size
    let mut size_buf = BytesMut::with_capacity(4);
    self.read_full_buf_or_none(&mut size_buf).await?;
    if size_buf.is_empty() {
        return Ok(None);
    }

    // empty record
    let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
    
    // VALIDATION: Enforce maximum record size
    if record_size > MAX_RECORD_SIZE {
        bail!(
            "Record size {} exceeds maximum allowed size {}",
            record_size,
            MAX_RECORD_SIZE
        );
    }
    
    if record_size == 0 {
        return Ok(Some(Bytes::new()));
    }

    // read record
    let mut record_buf = BytesMut::with_capacity(record_size);
    self.read_full_buf_or_none(&mut record_buf).await?;
    if record_buf.is_empty() {
        bail!("Hit EOF when reading record.")
    }

    Ok(Some(record_buf.freeze()))
}
```

The 128MB limit aligns with the existing `max_chunk_size` default configuration and prevents unbounded allocation while still supporting legitimate large state values.

## Proof of Concept

```rust
// Create malicious backup file with oversized record
use std::fs::File;
use std::io::Write;

fn create_malicious_backup() -> std::io::Result<()> {
    let mut file = File::create("malicious_backup.blob")?;
    
    // Write 1000 records, each claiming to be 2GB
    for _ in 0..1000 {
        // Record size: 0x7FFFFFFF (2GB)
        let size: u32 = 0x7FFFFFFF;
        file.write_all(&size.to_be_bytes())?;
        
        // Write minimal actual data (will fail verification, but memory exhausted first)
        file.write_all(&[0u8; 100])?;
    }
    
    Ok(())
}

// Attempt to restore - will OOM before cryptographic verification
#[tokio::test]
async fn test_memory_exhaustion() {
    use backup_cli::utils::read_record_bytes::ReadRecordBytes;
    use tokio::fs::File;
    
    create_malicious_backup().unwrap();
    let mut file = File::open("malicious_backup.blob").await.unwrap();
    
    // This will attempt to allocate 2GB per record
    // With 1000 records, requests 2TB of memory
    // Node will OOM long before verification happens
    let mut count = 0;
    while let Ok(Some(_record)) = file.read_record_bytes().await {
        count += 1;
        println!("Allocated record {} (2GB)", count);
        if count > 10 {
            break; // Would run out of memory before this on typical nodes
        }
    }
}
```

This PoC demonstrates that the restore process will attempt unbounded memory allocation based on the size field, exhausting resources before cryptographic verification can reject the malicious data.

### Citations

**File:** storage/backup/backup-cli/src/utils/read_record_bytes.rs (L44-67)
```rust
    async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
        let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
        // read record size
        let mut size_buf = BytesMut::with_capacity(4);
        self.read_full_buf_or_none(&mut size_buf).await?;
        if size_buf.is_empty() {
            return Ok(None);
        }

        // empty record
        let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
        if record_size == 0 {
            return Ok(Some(Bytes::new()));
        }

        // read record
        let mut record_buf = BytesMut::with_capacity(record_size);
        self.read_full_buf_or_none(&mut record_buf).await?;
        if record_buf.is_empty() {
            bail!("Hit EOF when reading record.")
        }

        Ok(Some(record_buf.freeze()))
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L186-215)
```rust
        let storage = self.storage.clone();
        let futs_iter = chunks.into_iter().enumerate().map(|(chunk_idx, chunk)| {
            let storage = storage.clone();
            async move {
                tokio::spawn(async move {
                    let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
                    Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
                })
                .await?
            }
        });
        let con = self.concurrent_downloads;
        let mut futs_stream = stream::iter(futs_iter).buffered_x(con * 2, con);
        let mut start = None;
        while let Some((chunk_idx, chunk, mut blobs, proof)) = futs_stream.try_next().await? {
            start = start.or_else(|| Some(Instant::now()));
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["add_state_chunk"]);
            let receiver = receiver.clone();
            if self.validate_modules {
                blobs = tokio::task::spawn_blocking(move || {
                    Self::validate_modules(&blobs);
                    blobs
                })
                .await?;
            }
            tokio::task::spawn_blocking(move || {
                receiver.lock().as_mut().unwrap().add_chunk(blobs, proof)
            })
            .await??;
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L253-266)
```rust
    async fn read_state_value(
        storage: &Arc<dyn BackupStorage>,
        file_handle: FileHandle,
    ) -> Result<Vec<(StateKey, StateValue)>> {
        let mut file = storage.open_for_read(&file_handle).await?;

        let mut chunk = vec![];

        while let Some(record_bytes) = file.read_record_bytes().await? {
            chunk.push(bcs::from_bytes(&record_bytes)?);
        }

        Ok(chunk)
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L100-137)
```rust
    async fn load(
        manifest: TransactionChunk,
        storage: &Arc<dyn BackupStorage>,
        epoch_history: Option<&Arc<EpochHistory>>,
    ) -> Result<Self> {
        let mut file = BufReader::new(storage.open_for_read(&manifest.transactions).await?);
        let mut txns = Vec::new();
        let mut persisted_aux_info = Vec::new();
        let mut txn_infos = Vec::new();
        let mut event_vecs = Vec::new();
        let mut write_sets = Vec::new();

        while let Some(record_bytes) = file.read_record_bytes().await? {
            let (txn, aux_info, txn_info, events, write_set): (
                _,
                PersistedAuxiliaryInfo,
                _,
                _,
                WriteSet,
            ) = match manifest.format {
                TransactionChunkFormat::V0 => {
                    let (txn, txn_info, events, write_set) = bcs::from_bytes(&record_bytes)?;
                    (
                        txn,
                        PersistedAuxiliaryInfo::None,
                        txn_info,
                        events,
                        write_set,
                    )
                },
                TransactionChunkFormat::V1 => bcs::from_bytes(&record_bytes)?,
            };
            txns.push(txn);
            persisted_aux_info.push(aux_info);
            txn_infos.push(txn_info);
            event_vecs.push(events);
            write_sets.push(write_set);
        }
```

**File:** storage/backup/backup-cli/src/backup_types/epoch_ending/restore.rs (L160-172)
```rust
    async fn read_chunk(
        &self,
        file_handle: &FileHandleRef,
    ) -> Result<Vec<LedgerInfoWithSignatures>> {
        let mut file = self.storage.open_for_read(file_handle).await?;
        let mut chunk = vec![];

        while let Some(record_bytes) = file.read_record_bytes().await? {
            chunk.push(bcs::from_bytes(&record_bytes)?);
        }

        Ok(chunk)
    }
```
