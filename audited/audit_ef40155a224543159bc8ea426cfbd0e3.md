# Audit Report

## Title
Stale Epoch State Persistence Across RandManager Reset Causing Liveness Failure During Cross-Epoch Recovery

## Summary
When `RandManager::process_reset()` is called during cross-epoch state synchronization, the `epoch_state` field is never updated or validated. This allows stale epoch state from epoch N to persist while the manager operates at rounds from epoch N+1, causing all incoming randomness messages from the new epoch to be rejected and preventing the node from participating in consensus.

## Finding Description
The vulnerability exists in the `process_reset()` function which handles reset requests but fails to update the critical `epoch_state` field: [1](#0-0) 

When a validator node falls behind and performs `fast_forward_sync` to catch up across an epoch boundary, the following sequence occurs:

1. The node calls `BlockStore::fast_forward_sync()` with a `highest_commit_cert` from epoch N+1
2. This calls `execution_client.sync_to_target(ledger_info)` where the ledger info is from epoch N+1: [2](#0-1) 

3. The `sync_to_target()` method calls `reset(&target)` before syncing: [3](#0-2) 

4. The `reset()` method extracts only the **round** from the target, not the epoch: [4](#0-3) 

5. RandManager's `process_reset()` receives `ResetSignal::TargetRound` but never updates `self.epoch_state`

The RandManager now operates with:
- `epoch_state.epoch = N` (stale)
- `rand_store` reset to round from epoch N+1
- `block_queue` cleared

All incoming randomness messages are verified with an epoch check: [5](#0-4) 

Messages from epoch N+1 are rejected because `message.epoch() != epoch_state.epoch`. Additionally, the verification task spawned at startup continues using the stale epoch state: [6](#0-5) 

The RandManager also uses stale validator addresses from the old epoch when broadcasting messages: [7](#0-6) 

## Impact Explanation
**High Severity** - This issue causes validator node liveness failure:

1. **Randomness Generation Blocked**: The node cannot process any randomness shares from epoch N+1, preventing randomness generation for new blocks
2. **Consensus Participation Prevented**: Without functional randomness generation, the node cannot participate in consensus
3. **Validator Set Mismatch**: Broadcasts go to the wrong validator set from epoch N instead of N+1

This qualifies as **High Severity** per the Aptos bug bounty criteria: "Validator node slowdowns" and "Significant protocol violations". While the node eventually recovers when the epoch change completes properly, the vulnerability window can last for seconds during which the validator is non-functional.

Note: This is **not** a Critical safety violation because the broken state prevents message acceptance rather than causing acceptance of invalid messages.

## Likelihood Explanation
**Medium-High Likelihood** - This occurs naturally whenever:
1. A validator node falls behind during normal operation
2. An epoch change occurs while the node is catching up
3. The node performs `fast_forward_sync` to recover across the epoch boundary

This is a common scenario in distributed systems where network partitions, brief outages, or heavy load cause nodes to fall behind. The bug triggers automatically without requiring attacker interaction, though network-level DoS (which is out of scope) could increase the frequency.

## Recommendation
The `process_reset()` function should validate that the reset request is for the current epoch, or the `ResetRequest` should include epoch information:

```rust
fn process_reset(&mut self, request: ResetRequest) {
    let ResetRequest { tx, signal } = request;
    let target_round = match signal {
        ResetSignal::Stop => 0,
        ResetSignal::TargetRound(round) => round,
    };
    
    // Add epoch validation - if reset crosses epoch boundary,
    // log error and do not reset (rely on proper epoch change flow)
    // Alternatively, modify ResetSignal to include epoch and validate:
    // ensure!(target_epoch == self.epoch_state.epoch, 
    //         "Reset epoch mismatch: cannot reset across epochs");
    
    self.block_queue = BlockQueue::new();
    self.rand_store.lock().reset(target_round);
    self.stop = matches!(signal, ResetSignal::Stop);
    let _ = tx.send(ResetAck::default());
}
```

Better solution: Ensure `end_epoch()` is always called before any cross-epoch `sync_to_target()`, preventing the reset from being sent to a manager with stale epoch state.

## Proof of Concept
```rust
// Integration test scenario:
// 1. Start validator node in epoch N with RandManager
// 2. Simulate node falling behind (pause message processing)
// 3. Advance network to epoch N+1
// 4. Trigger fast_forward_sync with target from epoch N+1
// 5. Observe that RandManager rejects all epoch N+1 messages
// 6. Verify validator cannot participate in randomness generation
// 7. Confirm node recovers only after epoch change completes

#[tokio::test]
async fn test_stale_epoch_state_across_reset() {
    // Setup: Create RandManager for epoch 5
    let epoch_state_5 = create_epoch_state(5, validator_set_5);
    let rand_manager = RandManager::new(
        author,
        Arc::new(epoch_state_5),
        // ... other params
    );
    
    // Simulate reset to round from epoch 6
    let ledger_info_epoch_6 = create_ledger_info(6, 100);
    let reset_request = ResetRequest {
        tx: oneshot::channel().0,
        signal: ResetSignal::TargetRound(100), // Round from epoch 6
    };
    rand_manager.process_reset(reset_request);
    
    // Verify: Messages from epoch 6 are rejected
    let message_epoch_6 = create_rand_share_message(6, 100);
    assert!(rand_manager.verify_message(&message_epoch_6).is_err());
    // Expected error: epoch mismatch (6 != 5)
}
```

**Notes:**
- This is a **HIGH severity** liveness bug, not a **CRITICAL** consensus safety violation
- The node does not accept invalid messages; it fails to accept valid ones
- Impact is temporary until proper epoch change completes
- The vulnerability window depends on network conditions and epoch change timing
- A full PoC would require complex integration testing with multiple validators and epoch transitions

### Citations

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L184-194)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.rand_store.lock().reset(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L221-261)
```rust
    async fn verification_task(
        epoch_state: Arc<EpochState>,
        mut incoming_rpc_request: aptos_channel::Receiver<Author, IncomingRandGenRequest>,
        verified_msg_tx: UnboundedSender<RpcRequest<S, D>>,
        rand_config: RandConfig,
        fast_rand_config: Option<RandConfig>,
        bounded_executor: BoundedExecutor,
    ) {
        while let Some(rand_gen_msg) = incoming_rpc_request.next().await {
            let tx = verified_msg_tx.clone();
            let epoch_state_clone = epoch_state.clone();
            let config_clone = rand_config.clone();
            let fast_config_clone = fast_rand_config.clone();
            bounded_executor
                .spawn(async move {
                    match bcs::from_bytes::<RandMessage<S, D>>(rand_gen_msg.req.data()) {
                        Ok(msg) => {
                            if msg
                                .verify(
                                    &epoch_state_clone,
                                    &config_clone,
                                    &fast_config_clone,
                                    rand_gen_msg.sender,
                                )
                                .is_ok()
                            {
                                let _ = tx.unbounded_send(RpcRequest {
                                    req: msg,
                                    protocol: rand_gen_msg.protocol,
                                    response_sender: rand_gen_msg.response_sender,
                                });
                            }
                        },
                        Err(e) => {
                            warn!("Invalid rand gen message: {}", e);
                        },
                    }
                })
                .await;
        }
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L512-514)
```rust
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```

**File:** consensus/src/pipeline/execution_client.rs (L661-672)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L683-693)
```rust
        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }
```

**File:** consensus/src/rand/rand_gen/network_messages.rs (L36-60)
```rust
    pub fn verify(
        &self,
        epoch_state: &EpochState,
        rand_config: &RandConfig,
        fast_rand_config: &Option<RandConfig>,
        sender: Author,
    ) -> anyhow::Result<()> {
        ensure!(self.epoch() == epoch_state.epoch);
        match self {
            RandMessage::RequestShare(_) => Ok(()),
            RandMessage::Share(share) => share.verify(rand_config),
            RandMessage::AugData(aug_data) => {
                aug_data.verify(rand_config, fast_rand_config, sender)
            },
            RandMessage::CertifiedAugData(certified_aug_data) => {
                certified_aug_data.verify(&epoch_state.verifier)
            },
            RandMessage::FastShare(share) => {
                share.share.verify(fast_rand_config.as_ref().ok_or_else(|| {
                    anyhow::anyhow!("[RandMessage] rand config for fast path not found")
                })?)
            },
            _ => bail!("[RandMessage] unexpected message type"),
        }
    }
```

**File:** consensus/src/network.rs (L387-405)
```rust
    pub fn broadcast_without_self(&self, msg: ConsensusMsg) {
        fail_point!("consensus::send::any", |_| ());

        let self_author = self.author;
        let mut other_validators: Vec<_> = self
            .validators
            .get_ordered_account_addresses_iter()
            .filter(|author| author != &self_author)
            .collect();
        self.sort_peers_by_latency(&mut other_validators);

        counters::CONSENSUS_SENT_MSGS
            .with_label_values(&[msg.name()])
            .inc_by(other_validators.len() as u64);
        // Broadcast message over direct-send to all other validators.
        if let Err(err) = self
            .consensus_network_client
            .send_to_many(other_validators, msg)
        {
```
