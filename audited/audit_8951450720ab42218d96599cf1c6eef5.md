# Audit Report

## Title
Unhandled Panic Propagation from Block Execution to Consensus Layer Causing Validator Crashes

## Summary
The Aptos blockchain lacks panic catching mechanisms between the VM block execution layer and consensus layer. Any panic occurring during `execute_block()` will propagate uncaught through the execution stack, crash the `spawn_blocking` task, and bring down the consensus pipeline, causing validator liveness failures.

## Finding Description

The consensus layer calls block execution inside a `tokio::task::spawn_blocking` without any panic recovery mechanism: [1](#0-0) 

When the spawned task panics, the `.expect("spawn blocking failed")` call will panic, crashing the consensus execution pipeline. The panic propagation path flows through:

1. **VM Execution Layer** - Multiple panic sources exist in transaction execution:
   - Assertion checking gas invariants: [2](#0-1) 
   - Unwrap on checked arithmetic: [3](#0-2) 
   - Explicit panics on fallback failure: [4](#0-3) 

2. **Worker Thread Execution** - Panics in Rayon worker threads during parallel execution are caught by Rayon's scope but re-panicked when the scope exits: [5](#0-4) 

3. **Block Executor Wrapper** - No panic catching in the execution pipeline: [6](#0-5) 

4. **Executor Layer** - The `DoGetExecutionOutput` module uses `?` operator for Result errors but provides no panic protection: [7](#0-6) 

5. **BlockExecutor** - Delegates to inner executor without panic handling: [8](#0-7) 

The entire call chain from consensus to VM execution lacks any `catch_unwind` or panic recovery mechanisms, violating the fault isolation principle critical for validator stability.

### Invariant Violated
**Validator Liveness**: Validators must remain operational and able to participate in consensus even when encountering execution errors. Panics should be converted to errors and handled gracefully rather than crashing the node.

## Impact Explanation

**Severity: High** (per Aptos Bug Bounty criteria for "Validator node slowdowns" and "Significant protocol violations")

This vulnerability enables:

1. **Validator Crashes**: Any panic during block execution immediately terminates the consensus task, requiring node restart
2. **Liveness Degradation**: If multiple validators crash simultaneously (e.g., from executing the same buggy transaction), network liveness could be compromised
3. **Deterministic Failures**: Panics triggered by on-chain state or transaction content will consistently crash all validators attempting to execute that block

While this requires a panic-inducing condition (bug, assertion failure, or misconfiguration), the impact on validator availability is direct and severe. The vulnerability becomes critical when combined with:
- Bugs in gas metering logic (triggering the assertion at line 2027)
- Misconfiguration of `allow_fallback` parameter
- Future code changes introducing new panic sources in the execution path

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability is present in production code, but exploitation requires:

1. **Panic Trigger**: A condition that causes a panic during execution, such as:
   - Gas metering bug violating the invariant checked by `assert_eq!(initial_gas, gas_meter.balance())`
   - Configuration setting `allow_fallback = false` (non-default)
   - Arithmetic underflow in gas calculations (protected by condition but theoretically possible)
   - Future code changes introducing new panics

2. **Execution Context**: The panic must occur during block execution, not validation

While default configurations include some protections (e.g., `allow_fallback = true`), the complete absence of panic catching means ANY panic source in the execution path will crash validators. The assertion at line 2027 is particularly concerning as it checks an invariant that should hold but could be violated by subtle bugs in gas metering.

## Recommendation

Implement defense-in-depth panic catching at the boundary between consensus and execution:

```rust
// In consensus/src/pipeline/pipeline_builder.rs
use std::panic::{catch_unwind, AssertUnwindSafe};

let start = Instant::now();
tokio::task::spawn_blocking(move || {
    // Catch any panics from block execution
    let result = catch_unwind(AssertUnwindSafe(|| {
        executor.execute_and_update_state(
            (block.id(), txns, auxiliary_info).into(),
            block.parent_id(),
            onchain_execution_config,
        )
    }));
    
    match result {
        Ok(exec_result) => exec_result.map_err(anyhow::Error::from),
        Err(panic_err) => {
            error!("Block execution panicked: {:?}", panic_err);
            Err(anyhow::anyhow!("Block execution panicked"))
        }
    }
})
.await
.expect("spawn blocking failed")?;
```

Additionally:
1. Replace `assert_eq!` at line 2027 with proper error handling that returns a VMStatus instead of panicking
2. Replace `panic!()` calls at lines 2582 and 2615 with error returns
3. Replace `.unwrap()` at line 2024 with proper error handling using `ok_or_else()`
4. Add validator metrics to track caught panics for debugging

## Proof of Concept

```rust
// Minimal reproduction demonstrating panic propagation
// This would be added as a test in consensus/src/pipeline/pipeline_builder.rs

#[tokio::test]
async fn test_panic_propagation_crashes_consensus() {
    // Simulate a BlockExecutor that panics during execution
    struct PanickingExecutor;
    
    impl BlockExecutorTrait for PanickingExecutor {
        fn execute_and_update_state(
            &self,
            _block: ExecutableBlock,
            _parent_block_id: HashValue,
            _onchain_config: BlockExecutorConfigFromOnchain,
        ) -> ExecutorResult<()> {
            panic!("Simulated execution panic");
        }
        // ... other trait methods ...
    }
    
    let executor = Arc::new(PanickingExecutor);
    
    // Attempt to execute block - this will panic
    let result = tokio::task::spawn_blocking(move || {
        executor.execute_and_update_state(
            test_block(),
            HashValue::zero(),
            BlockExecutorConfigFromOnchain::default(),
        )
    })
    .await;
    
    // Without panic catching, expect() here will panic and crash
    // With fix, this returns an error instead
    assert!(result.is_err());
}
```

To trigger in production:
1. Deploy a gas parameter update that violates the invariant `initial_gas == gas_meter.balance()` after validation when account abstraction is disabled
2. Submit a transaction that triggers this condition
3. All validators executing this transaction will panic and crash

## Notes

The vulnerability's severity is mitigated by:
- Default configuration includes `allow_fallback = true` preventing explicit panic calls
- Gas metering code is generally well-tested, reducing likelihood of assertion failures
- Parallel execution errors are mostly converted to Result types rather than panics

However, the complete absence of panic catching violates defense-in-depth principles and creates a single point of failure. Any future code change introducing a panic in the execution path will immediately become a validator crash vulnerability.

### Citations

**File:** consensus/src/pipeline/pipeline_builder.rs (L857-868)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .execute_and_update_state(
                    (block.id(), txns, auxiliary_info).into(),
                    block.parent_id(),
                    onchain_execution_config,
                )
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(start.elapsed())
```

**File:** aptos-move/aptos-vm/src/aptos_vm.rs (L2023-2024)
```rust
                unwrap_or_discard!(gas_meter
                    .inject_balance(txn_data.max_gas_amount().checked_sub(max_aa_gas).unwrap()));
```

**File:** aptos-move/aptos-vm/src/aptos_vm.rs (L2027-2027)
```rust
            assert_eq!(initial_gas, gas_meter.balance());
```

**File:** aptos-move/block-executor/src/executor.rs (L1922-1961)
```rust
        self.executor_thread_pool.scope(|s| {
            for worker_id in &worker_ids {
                s.spawn(|_| {
                    let environment = module_cache_manager_guard.environment();
                    let executor = {
                        let _init_timer = VM_INIT_SECONDS.start_timer();
                        E::init(
                            &environment.clone(),
                            base_view,
                            async_runtime_checks_enabled,
                        )
                    };

                    if let Err(err) = self.worker_loop(
                        &executor,
                        environment,
                        signature_verified_block,
                        &scheduler,
                        &skip_module_reads_validation,
                        &shared_sync_params,
                        num_workers,
                    ) {
                        // If there are multiple errors, they all get logged:
                        // ModulePathReadWriteError and FatalVMError variant is logged at construction,
                        // and below we log CodeInvariantErrors.
                        if let PanicOr::CodeInvariantError(err_msg) = err {
                            alert!("[BlockSTM] worker loop: CodeInvariantError({:?})", err_msg);
                        }
                        shared_maybe_error.store(true, Ordering::SeqCst);

                        // Make sure to halt the scheduler if it hasn't already been halted.
                        scheduler.halt();
                    }

                    if *worker_id == 0 {
                        maybe_executor.acquire().replace(executor);
                    }
                });
            }
        });
```

**File:** aptos-move/block-executor/src/executor.rs (L2581-2583)
```rust
            if !self.config.local.allow_fallback {
                panic!("Parallel execution failed and fallback is not allowed");
            }
```

**File:** aptos-move/aptos-vm/src/block_executor/mod.rs (L515-610)
```rust
    pub fn execute_block_on_thread_pool<
        S: StateView + Sync,
        L: TransactionCommitHook,
        TP: TxnProvider<SignatureVerifiedTransaction, AuxiliaryInfo> + Sync,
    >(
        executor_thread_pool: Arc<rayon::ThreadPool>,
        signature_verified_block: &TP,
        state_view: &S,
        module_cache_manager: &AptosModuleCacheManager,
        config: BlockExecutorConfig,
        transaction_slice_metadata: TransactionSliceMetadata,
        transaction_commit_listener: Option<L>,
    ) -> Result<BlockOutput<SignatureVerifiedTransaction, TransactionOutput>, VMStatus> {
        let _timer = BLOCK_EXECUTOR_EXECUTE_BLOCK_SECONDS.start_timer();

        let num_txns = signature_verified_block.num_txns();
        if state_view.id() != StateViewId::Miscellaneous {
            // Speculation is disabled in Miscellaneous context, which is used by testing and
            // can even lead to concurrent execute_block invocations, leading to errors on flush.
            init_speculative_logs(num_txns);
        }

        BLOCK_EXECUTOR_CONCURRENCY.set(config.local.concurrency_level as i64);

        let mut module_cache_manager_guard = module_cache_manager.try_lock(
            &state_view,
            &config.local.module_cache_config,
            transaction_slice_metadata,
        )?;

        let executor =
            BlockExecutor::<SignatureVerifiedTransaction, E, S, L, TP, AuxiliaryInfo>::new(
                config,
                executor_thread_pool,
                transaction_commit_listener,
            );

        let ret = executor.execute_block(
            signature_verified_block,
            state_view,
            &transaction_slice_metadata,
            &mut module_cache_manager_guard,
        );
        match ret {
            Ok(block_output) => {
                let (transaction_outputs, block_epilogue_txn) = block_output.into_inner();
                let output_vec: Vec<_> = transaction_outputs
                    .into_iter()
                    .map(|output| output.take_output())
                    .collect();

                // Flush the speculative logs of the committed transactions.
                let pos = output_vec.partition_point(|o| !o.status().is_retry());

                if state_view.id() != StateViewId::Miscellaneous {
                    // Speculation is disabled in Miscellaneous context, which is used by testing and
                    // can even lead to concurrent execute_block invocations, leading to errors on flush.
                    flush_speculative_logs(pos);
                }

                Ok(BlockOutput::new(output_vec, block_epilogue_txn))
            },
            Err(BlockExecutionError::FatalBlockExecutorError(PanicError::CodeInvariantError(
                err_msg,
            ))) => Err(VMStatus::Error {
                status_code: StatusCode::DELAYED_FIELD_OR_BLOCKSTM_CODE_INVARIANT_ERROR,
                sub_status: None,
                message: Some(err_msg),
            }),
            Err(BlockExecutionError::FatalVMError(err)) => Err(err),
        }
    }

    /// Uses shared thread pool to execute blocks.
    pub(crate) fn execute_block<
        S: StateView + Sync,
        L: TransactionCommitHook,
        TP: TxnProvider<SignatureVerifiedTransaction, AuxiliaryInfo> + Sync,
    >(
        signature_verified_block: &TP,
        state_view: &S,
        module_cache_manager: &AptosModuleCacheManager,
        config: BlockExecutorConfig,
        transaction_slice_metadata: TransactionSliceMetadata,
        transaction_commit_listener: Option<L>,
    ) -> Result<BlockOutput<SignatureVerifiedTransaction, TransactionOutput>, VMStatus> {
        Self::execute_block_on_thread_pool::<S, L, TP>(
            Arc::clone(&RAYON_EXEC_POOL),
            signature_verified_block,
            state_view,
            module_cache_manager,
            config,
            transaction_slice_metadata,
            transaction_commit_listener,
        )
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L288-295)
```rust
        let _timer = OTHER_TIMERS.timer_with(&["vm_execute_block"]);
        Ok(executor.execute_block(
            txn_provider,
            state_view,
            onchain_config,
            transaction_slice_metadata,
        )?)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L97-113)
```rust
    fn execute_and_update_state(
        &self,
        block: ExecutableBlock,
        parent_block_id: HashValue,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "execute_and_state_checkpoint"]);

        self.maybe_initialize()?;
        // guarantee only one block being executed at a time
        let _guard = self.execution_lock.lock();
        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .execute_and_update_state(block, parent_block_id, onchain_config)
    }
```
