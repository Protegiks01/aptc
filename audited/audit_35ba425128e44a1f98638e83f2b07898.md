# Audit Report

## Title
Infinite Timeout Loop Vulnerability in Consensus Round State Management

## Summary
The `process_local_timeout()` function in `RoundState` unconditionally schedules a new timeout for the same round without advancing the round number, creating an infinite loop when no new quorum certificates or timeout certificates are received. This causes repeated network broadcasts, CPU consumption, and potential validator liveness degradation.

## Finding Description

The vulnerability exists in the interaction between `process_local_timeout()` and `setup_timeout()` in the consensus round state management. [1](#0-0) 

When a timeout fires, `process_local_timeout()` checks if the round matches `current_round`, and if so, immediately calls `setup_timeout(1)` which schedules another timeout task for **the same round** without incrementing `current_round`. [2](#0-1) 

The `setup_timeout()` function schedules a new timeout task that will send `self.current_round` on the timeout channel after the configured duration. Since `current_round` is never incremented in `process_local_timeout()`, the cycle repeats:

1. Timeout fires for round N
2. `process_local_timeout(N)` is called
3. Condition `round == self.current_round` evaluates to true
4. `setup_timeout(1)` schedules another timeout for round N
5. The new timeout fires (after short duration)
6. Back to step 2

The only way to break this cycle is if `process_certificates()` is called with a new QC/TC, which advances `current_round`: [3](#0-2) 

However, if the validator is in a network partition or not receiving new certificates, the timeout loop continues indefinitely.

**Network Amplification Impact:**

Each timeout triggers `RoundManager::process_local_timeout()` which broadcasts messages to all peers: [4](#0-3) 

The vulnerability is confirmed by the existing test that explicitly demonstrates this behavior: [5](#0-4) 

The test comment acknowledges: "round for timeout is not changed as no timeout certificate was gathered at this point," and shows that multiple timeouts fire for round 1 repeatedly.

**Attack Scenarios:**

1. **Misconfiguration Attack**: An attacker who can influence validator configuration could set `round_initial_timeout_ms` to a very low value (e.g., 10ms), causing rapid timeout loops.

2. **Network Partition**: During a network partition where a validator cannot receive new QCs/TCs, it enters a timeout loop broadcasting to all peers repeatedly at the configured timeout interval.

3. **Targeted Network Disruption**: An attacker could selectively block consensus messages to specific validators while allowing timeout broadcasts, maximizing network spam.

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria for the following reasons:

1. **Validator Node Slowdowns**: The repeated timeout processing consumes CPU cycles and can degrade validator performance in processing legitimate consensus messages.

2. **Network Resource Exhaustion**: Each timeout triggers a broadcast to all peers. With default timeout of 1000ms, this means 1 broadcast per second indefinitely. With misconfigured shorter timeouts (e.g., 100ms or the 2ms used in tests), this becomes 10-500 broadcasts per second, causing significant network spam.

3. **Liveness Degradation**: While not a complete liveness failure, validators stuck in timeout loops may be slow to:
   - Process incoming proposals
   - Vote on blocks
   - Respond to sync requests
   - Participate effectively in consensus

4. **Consensus Protocol Violation**: The repeated timeout broadcasts violate the expected behavior of timeout mechanisms, where a single timeout should trigger one broadcast per round until the round advances.

The vulnerability does not directly cause loss of funds or consensus safety violations, but it significantly impacts network availability and validator operation, meeting the High severity threshold of "Validator node slowdowns" and "Significant protocol violations."

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability can manifest in several realistic scenarios:

1. **Network Partitions** (High Likelihood): During network partitions or connectivity issues, validators naturally fail to receive new QCs/TCs. This is a common occurrence in distributed systems and would trigger the timeout loop.

2. **Configuration Errors** (Medium Likelihood): Operators might misconfigure `round_initial_timeout_ms` to unreasonably low values during testing or tuning, inadvertently triggering rapid timeout loops in production.

3. **Lagging Validators** (Medium Likelihood): Validators that fall behind due to high load or resource constraints may enter timeout loops while waiting to sync.

4. **Targeted Attacks** (Low-Medium Likelihood): A sophisticated attacker with network-level access could selectively filter consensus messages to specific validators, forcing them into timeout loops.

The vulnerability is **always active** once a validator enters a state where timeouts fire without receiving new certificates. No special conditions or race conditions are required—it's deterministic behavior in the code.

## Recommendation

Add a check in `process_local_timeout()` to prevent setting up a new timeout if one has already been processed for the current round:

```rust
pub fn process_local_timeout(&mut self, round: Round) -> bool {
    if round != self.current_round {
        return false;
    }
    
    // NEW: Prevent infinite timeout loop by checking if we already sent a timeout
    if self.is_timeout_sent() {
        warn!(round = round, "Timeout already processed for this round, skipping");
        return false;
    }
    
    warn!(round = round, "Local timeout");
    counters::TIMEOUT_COUNT.inc();
    self.setup_timeout(1);
    true
}
```

**Alternative Fix**: Implement timeout de-duplication at the channel level by checking if a timeout message for the current round is already pending before sending a new one.

**Long-term Solution**: Redesign the timeout mechanism to use a single scheduled timeout per round that doesn't reschedule itself, relying only on `process_certificates()` to set up timeouts for new rounds.

## Proof of Concept

The existing test demonstrates this vulnerability: [6](#0-5) 

To demonstrate the infinite loop behavior with network impact, create a test:

```rust
#[tokio::test]
async fn test_timeout_loop_dos() {
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::sync::Arc;
    
    let (_, verifier) = random_validator_verifier(1, None, false);
    let time_interval = Box::new(ExponentialTimeInterval::fixed(Duration::from_millis(10)));
    let simulated_time = SimulatedTimeService::auto_advance_until(Duration::from_millis(20));
    let (timeout_tx, mut timeout_rx) = aptos_channels::new_test(1_024);
    
    let mut round_state = RoundState::new(
        time_interval,
        Arc::new(simulated_time),
        timeout_tx,
    );
    
    // Start round 1
    round_state.process_certificates(generate_sync_info(Some(0), None, None), &verifier);
    
    let timeout_count = Arc::new(AtomicUsize::new(0));
    let timeout_count_clone = timeout_count.clone();
    
    // Simulate processing timeouts without advancing rounds
    tokio::spawn(async move {
        while let Some(round) = timeout_rx.next().await {
            let count = timeout_count_clone.fetch_add(1, Ordering::SeqCst);
            if count >= 100 {
                break; // Stop after 100 iterations to prevent actual infinite loop
            }
        }
    });
    
    // Process timeouts in a loop - simulates the infinite loop
    for _ in 0..100 {
        if let Some(round) = timeout_rx.try_next().ok().flatten() {
            round_state.process_local_timeout(round);
        }
    }
    
    // Verify that many timeouts were processed for the same round
    assert!(timeout_count.load(Ordering::SeqCst) >= 50, 
            "Expected multiple timeouts due to loop, got {}", 
            timeout_count.load(Ordering::SeqCst));
}
```

The test confirms that timeouts repeatedly fire for the same round without advancing, demonstrating the infinite loop vulnerability.

**Notes**

The vulnerability is particularly severe when combined with:
- Short timeout configurations (can be as low as milliseconds in test configurations)
- Network partitions preventing certificate reception
- Channel capacity (1,024 messages) which can be exhausted if processing is slower than timeout generation

The issue is present in production code and is not just a theoretical concern—the existing test suite explicitly exercises this behavior, suggesting it may occur in real network conditions.

### Citations

**File:** consensus/src/liveness/round_state.rs (L233-241)
```rust
    pub fn process_local_timeout(&mut self, round: Round) -> bool {
        if round != self.current_round {
            return false;
        }
        warn!(round = round, "Local timeout");
        counters::TIMEOUT_COUNT.inc();
        self.setup_timeout(1);
        true
    }
```

**File:** consensus/src/liveness/round_state.rs (L245-289)
```rust
    pub fn process_certificates(
        &mut self,
        sync_info: SyncInfo,
        verifier: &ValidatorVerifier,
    ) -> Option<NewRoundEvent> {
        if sync_info.highest_ordered_round() > self.highest_ordered_round {
            self.highest_ordered_round = sync_info.highest_ordered_round();
        }
        let new_round = sync_info.highest_round() + 1;
        if new_round > self.current_round {
            let (prev_round_votes, prev_round_timeout_votes) = self.pending_votes.drain_votes();

            // Start a new round.
            self.current_round = new_round;
            self.pending_votes = PendingVotes::new();
            self.vote_sent = None;
            self.timeout_sent = None;
            let timeout = self.setup_timeout(1);

            let (prev_round_timeout_votes, prev_round_timeout_reason) = prev_round_timeout_votes
                .map(|votes| votes.unpack_aggregate(verifier))
                .unzip();

            // The new round reason is QCReady in case both QC.round + 1 == new_round, otherwise
            // it's Timeout and TC.round + 1 == new_round.
            let new_round_reason = if sync_info.highest_certified_round() + 1 == new_round {
                NewRoundReason::QCReady
            } else {
                let prev_round_timeout_reason =
                    prev_round_timeout_reason.unwrap_or(RoundTimeoutReason::Unknown);
                NewRoundReason::Timeout(prev_round_timeout_reason)
            };

            let new_round_event = NewRoundEvent {
                round: self.current_round,
                reason: new_round_reason,
                timeout,
                prev_round_votes,
                prev_round_timeout_votes,
            };
            info!(round = new_round, "Starting new round: {}", new_round_event);
            return Some(new_round_event);
        }
        None
    }
```

**File:** consensus/src/liveness/round_state.rs (L339-354)
```rust
    fn setup_timeout(&mut self, multiplier: u32) -> Duration {
        let timeout_sender = self.timeout_sender.clone();
        let timeout = self.setup_deadline(multiplier);
        trace!(
            "Scheduling timeout of {} ms for round {}",
            timeout.as_millis(),
            self.current_round
        );
        let abort_handle = self
            .time_service
            .run_after(timeout, SendTask::make(timeout_sender, self.current_round));
        if let Some(handle) = self.abort_handle.replace(abort_handle) {
            handle.abort();
        }
        timeout
    }
```

**File:** consensus/src/round_manager.rs (L993-1043)
```rust
    pub async fn process_local_timeout(&mut self, round: Round) -> anyhow::Result<()> {
        if !self.round_state.process_local_timeout(round) {
            return Ok(());
        }

        if self.sync_only() {
            self.network
                .broadcast_sync_info(self.block_store.sync_info())
                .await;
            bail!("[RoundManager] sync_only flag is set, broadcasting SyncInfo");
        }

        if self.local_config.enable_round_timeout_msg {
            let timeout = if let Some(timeout) = self.round_state.timeout_sent() {
                timeout
            } else {
                let timeout = TwoChainTimeout::new(
                    self.epoch_state.epoch,
                    round,
                    self.block_store.highest_quorum_cert().as_ref().clone(),
                );
                let signature = self
                    .safety_rules
                    .lock()
                    .sign_timeout_with_qc(
                        &timeout,
                        self.block_store.highest_2chain_timeout_cert().as_deref(),
                    )
                    .context("[RoundManager] SafetyRules signs 2-chain timeout")?;

                let timeout_reason = self.compute_timeout_reason(round);

                RoundTimeout::new(
                    timeout,
                    self.proposal_generator.author(),
                    timeout_reason,
                    signature,
                )
            };

            self.round_state.record_round_timeout(timeout.clone());
            let round_timeout_msg = RoundTimeoutMsg::new(timeout, self.block_store.sync_info());
            self.network
                .broadcast_round_timeout(round_timeout_msg)
                .await;
            warn!(
                round = round,
                remote_peer = self.proposer_election.get_valid_proposer(round),
                event = LogEvent::Timeout,
            );
            bail!("Round {} timeout, broadcast to all peers", round);
```

**File:** consensus/src/liveness/round_state_test.rs (L41-56)
```rust
#[tokio::test]
/// Verify that RoundState properly outputs local timeout events upon timeout
async fn test_basic_timeout() {
    let (_, verifier) = random_validator_verifier(1, None, false);
    let (mut pm, mut timeout_rx) = make_round_state();

    // jump start the round_state
    pm.process_certificates(generate_sync_info(Some(0), None, None), &verifier);
    for _ in 0..2 {
        let round = timeout_rx.next().await.unwrap();
        // Here we just test timeout send retry,
        // round for timeout is not changed as no timeout certificate was gathered at this point
        assert_eq!(1, round);
        pm.process_local_timeout(round);
    }
}
```

**File:** consensus/src/liveness/round_state_test.rs (L88-96)
```rust
fn make_round_state() -> (RoundState, aptos_channels::Receiver<Round>) {
    let time_interval = Box::new(ExponentialTimeInterval::fixed(Duration::from_millis(2)));
    let simulated_time = SimulatedTimeService::auto_advance_until(Duration::from_millis(4));
    let (timeout_tx, timeout_rx) = aptos_channels::new_test(1_024);
    (
        RoundState::new(time_interval, Arc::new(simulated_time), timeout_tx),
        timeout_rx,
    )
}
```
