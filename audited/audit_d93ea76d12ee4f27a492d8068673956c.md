# Audit Report

## Title
Indexer gRPC Stream Synchronization Gap Allows Premature Wait Completion

## Summary
The `wait_for_version()` function in `indexer_grpc_waiter.rs` only validates that the gRPC stream has reached the target version (`stream_version >= target_version`), but does not verify that the `TableInfoService` has finished processing and persisting table info data to the `indexer_async_v2` database. This creates a synchronization gap where the waiter can return while table info indexing is still incomplete.

## Finding Description

The indexer synchronization system consists of two independent asynchronous components:

1. **gRPC Stream Consumer**: Reads transactions from `FullnodeDataService` (which queries the main AptosDB) and updates `stream_version` [1](#0-0) 

2. **TableInfoService**: Independently processes transactions, parses table info, and writes to the `indexer_async_v2` database, updating its own `current_version` [2](#0-1) 

The `wait_for_version()` function retrieves both versions but only checks `stream_version` for the exit condition: [3](#0-2) 

The gRPC stream can process transactions significantly faster than the `TableInfoService` can parse and persist table info, since:
- The stream simply reads and forwards transaction data from the main database
- The `TableInfoService` must parse write sets, process table info in parallel batches, handle pending items, and write to RocksDB [4](#0-3) 

## Impact Explanation

This is a **Medium Severity** state inconsistency issue. While this code is in the benchmark/executor-benchmark path, it affects the correctness of indexer synchronization testing and could lead to:

1. **Incomplete Indexer State**: Any operations that query or depend on table info data after the waiter returns may encounter incomplete data, as `table_info_version` can lag significantly behind `stream_version`.

2. **Benchmark Inaccuracy**: The benchmark may report completion before the indexer has truly caught up, leading to incorrect performance measurements.

3. **Race Conditions**: The 2-second sleep hack after the waiter indicates awareness of synchronization issues: [5](#0-4) 

## Likelihood Explanation

**Likelihood: High** - The race condition occurs whenever:
- Table info processing is slower than stream consumption (common with complex transactions)
- Multiple async tasks are processing data in parallel
- The parser batch size and task count create processing delays

The vulnerability is deterministic and happens in normal operation, not requiring specific attack conditions.

## Recommendation

The `wait_for_version()` function should verify that **both** `stream_version` AND `table_info_version` have reached the target:

```rust
pub async fn wait_for_version(&self, target_version: Version, abort_on_finish: bool) {
    info!(
        "Waiting for indexer_grpc to reach target version: {}",
        target_version
    );

    let start_time = Instant::now();
    let mut last_log_time = Instant::now();

    loop {
        let table_info_version = self.table_info_service.next_version().saturating_sub(1);
        let stream_version = self.stream_version.load(Ordering::SeqCst);
        
        // FIXED: Check both versions
        if stream_version >= target_version && table_info_version >= target_version {
            info!(
                "Indexer fully synchronized. Stream: {}, TableInfo: {}, Target: {}, elapsed: {:.2}s",
                stream_version,
                table_info_version,
                target_version,
                start_time.elapsed().as_secs_f64()
            );
            if abort_on_finish {
                self.table_info_service.abort();
            }
            break;
        }

        // Log status every 1 second
        if last_log_time.elapsed().as_secs() >= STATUS_LOG_INTERVAL_SECS {
            let stream_behind = target_version.saturating_sub(stream_version);
            let table_info_behind = target_version.saturating_sub(table_info_version);
            let elapsed_secs = start_time.elapsed().as_secs_f64();
            info!(
                "Indexer_grpc progress: target={}, stream={} (behind={}), table_info={} (behind={}), elapsed={:.2}s",
                target_version, stream_version, stream_behind, table_info_version, table_info_behind, elapsed_secs
            );
            last_log_time = Instant::now();
        }

        tokio::time::sleep(Duration::from_millis(INDEXER_GRPC_POLL_INTERVAL_MS)).await;
    }
}
```

## Proof of Concept

Create a benchmark test that demonstrates the synchronization gap:

```rust
#[tokio::test]
async fn test_indexer_synchronization_gap() {
    // Setup: Create indexer components
    let table_info_service = Arc::new(/* ... */);
    let stream_version = Arc::new(AtomicU64::new(0));
    let waiter = IndexerGrpcWaiter::new(table_info_service.clone(), stream_version.clone());
    
    // Simulate fast stream consumption
    let stream_handle = tokio::spawn({
        let stream_version = stream_version.clone();
        async move {
            for v in 0..1000 {
                stream_version.store(v, Ordering::SeqCst);
                tokio::time::sleep(Duration::from_micros(100)).await;
            }
        }
    });
    
    // Simulate slow table info processing
    let table_info_handle = tokio::spawn({
        let service = table_info_service.clone();
        async move {
            for v in 0..1000 {
                // Simulate slow processing
                tokio::time::sleep(Duration::from_millis(10)).await;
                // Update service version
            }
        }
    });
    
    // Wait for version 999
    waiter.wait_for_version(999, false).await;
    
    // BUG: Waiter returns but table info service may still be processing
    let final_stream = stream_version.load(Ordering::SeqCst);
    let final_table_info = table_info_service.next_version().saturating_sub(1);
    
    // This assertion may fail, demonstrating the gap
    assert_eq!(final_stream, final_table_info, 
        "Synchronization gap detected: stream={}, table_info={}", 
        final_stream, final_table_info);
}
```

**Notes**

This vulnerability exists in the executor benchmark tooling for the Aptos indexer gRPC subsystem. While not directly affecting consensus or validator operations, it represents a state consistency issue where dependent systems may observe incomplete indexer state. The fix ensures both the gRPC stream and table info processing have completed before signaling synchronization completion.

### Citations

**File:** execution/executor-benchmark/src/lib.rs (L196-222)
```rust
    indexer_runtime.spawn(async move {
        let grpc_service = FullnodeDataService {
            service_context,
            abort_handle,
        };
        println!("Starting grpc stream at version {start_version}.");
        let request = GetTransactionsFromNodeRequest {
            starting_version: Some(start_version),
            transactions_count: None,
        };
        let mut response = grpc_service
            .get_transactions_from_node(request.into_request())
            .await
            .unwrap()
            .into_inner();
        while let Some(item) = response.next().await {
            if let Ok(r) = item {
                if let Some(response) = r.response {
                    if let Response::Data(data) = response {
                        if let Some(txn) = data.transactions.last().as_ref() {
                            grpc_version_clone.store(txn.version, Ordering::SeqCst);
                        }
                    }
                }
            }
        }
    });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L106-193)
```rust
        loop {
            let start_time = std::time::Instant::now();
            let ledger_version = self.get_highest_known_version().await.unwrap_or_default();
            if self.aborted.load(Ordering::SeqCst) {
                info!("table info service aborted");
                break;
            }
            let batches = self.get_batches(ledger_version).await;
            let transactions = self.fetch_batches(batches, ledger_version).await.unwrap();
            let num_transactions = transactions.len();
            let last_version = transactions
                .last()
                .map(|txn| txn.version)
                .unwrap_or_default();
            let (transactions_in_previous_epoch, transactions_in_current_epoch, epoch) =
                transactions_in_epochs(&self.context, current_epoch, transactions);

            // At the end of the epoch, snapshot the database.
            if !transactions_in_previous_epoch.is_empty() {
                self.process_transactions_in_parallel(
                    self.indexer_async_v2.clone(),
                    transactions_in_previous_epoch,
                )
                .await;
                let previous_epoch = epoch - 1;
                if backup_is_enabled {
                    aptos_logger::info!(
                        epoch = previous_epoch,
                        "[Table Info] Snapshot taken at the end of the epoch"
                    );
                    Self::snapshot_indexer_async_v2(
                        self.context.clone(),
                        self.indexer_async_v2.clone(),
                        previous_epoch,
                    )
                    .await
                    .expect("Failed to snapshot indexer async v2");
                }
            } else {
                // If there are no transactions in the previous epoch, it means we have caught up to the latest epoch.
                // We still need to figure out if we're at the start of the epoch or in the middle of the epoch.
                if let Some(current_epoch) = current_epoch {
                    if current_epoch != epoch {
                        // We're at the start of the epoch.
                        // We need to snapshot the database.
                        if backup_is_enabled {
                            aptos_logger::info!(
                                epoch = current_epoch,
                                "[Table Info] Snapshot taken at the start of the epoch"
                            );
                            Self::snapshot_indexer_async_v2(
                                self.context.clone(),
                                self.indexer_async_v2.clone(),
                                current_epoch,
                            )
                            .await
                            .expect("Failed to snapshot indexer async v2");
                        }
                    }
                }
            }

            self.process_transactions_in_parallel(
                self.indexer_async_v2.clone(),
                transactions_in_current_epoch,
            )
            .await;

            let versions_processed = num_transactions as i64;
            let start_version = self.current_version.load(Ordering::SeqCst);
            log_grpc_step(
                SERVICE_TYPE,
                IndexerGrpcStep::TableInfoProcessed,
                Some(start_version as i64),
                Some(last_version as i64),
                None,
                None,
                Some(start_time.elapsed().as_secs_f64()),
                None,
                Some(versions_processed),
                None,
            );

            self.current_version
                .store(last_version + 1, Ordering::SeqCst);
            current_epoch = Some(epoch);
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L239-313)
```rust
    /// Fans out a bunch of threads and processes write sets from transactions in parallel.
    /// Pushes results in parallel to the stream, but only return that the batch is
    /// fully completed if every job in the batch is successful and no pending on items
    /// Processing transactions in 2 stages:
    /// 1. Fetch transactions from ledger db
    /// 2. Get write sets from transactions and parse write sets to get handle -> key,value type mapping, write the mapping to the rocksdb
    async fn process_transactions_in_parallel(
        &self,
        indexer_async_v2: Arc<IndexerAsyncV2>,
        transactions: Vec<TransactionOnChainData>,
    ) -> Vec<EndVersion> {
        let mut tasks = vec![];
        let context = self.context.clone();
        let last_version = transactions
            .last()
            .map(|txn| txn.version)
            .unwrap_or_default();

        let transactions = Arc::new(transactions);
        for (chunk_idx, batch_size) in transactions
            .chunks(self.parser_batch_size as usize)
            .enumerate()
            .map(|(idx, chunk)| (idx, chunk.len()))
        {
            let start = chunk_idx * self.parser_batch_size as usize;
            let end = start + batch_size;

            let transactions = transactions.clone();
            let context = context.clone();
            let indexer_async_v2 = indexer_async_v2.clone();
            let task = tokio::spawn(async move {
                Self::process_transactions(context, indexer_async_v2, &transactions[start..end])
                    .await
            });
            tasks.push(task);
        }

        match futures::future::try_join_all(tasks).await {
            Ok(res) => {
                let end_version = last_version;

                // If pending on items are not empty, meaning the current loop hasn't fully parsed all table infos
                // due to the nature of multithreading where instructions used to parse table info might come later,
                // retry sequentially to ensure parsing is complete
                //
                // Risk of this sequential approach is that it could be slow when the txns to process contain extremely
                // nested table items, but the risk is bounded by the configuration of the number of txns to process and number of threads
                if !self.indexer_async_v2.is_indexer_async_v2_pending_on_empty() {
                    self.indexer_async_v2.clear_pending_on();
                    Self::process_transactions(
                        context.clone(),
                        indexer_async_v2.clone(),
                        &transactions,
                    )
                    .await;
                }

                assert!(
                    self.indexer_async_v2.is_indexer_async_v2_pending_on_empty(),
                    "Missing data in table info parsing after sequential retry"
                );

                // Update rocksdb's to be processed next version after verifying all txns are successfully parsed
                self.indexer_async_v2
                    .update_next_version(end_version + 1)
                    .unwrap();

                res
            },
            Err(err) => panic!(
                "[Table Info] Error processing table info batches: {:?}",
                err
            ),
        }
    }
```

**File:** execution/executor-benchmark/src/indexer_grpc_waiter.rs (L37-75)
```rust
    pub async fn wait_for_version(&self, target_version: Version, abort_on_finish: bool) {
        info!(
            "Waiting for indexer_grpc to reach target version: {}",
            target_version
        );

        let start_time = Instant::now();
        let mut last_log_time = Instant::now();

        loop {
            let table_info_version = self.table_info_service.next_version().saturating_sub(1);
            let stream_version = self.stream_version.load(Ordering::SeqCst);
            if stream_version >= target_version {
                info!(
                    "Indexer stream reached target version. Current: {}, Target: {}, elapsed: {:.2}s",
                    stream_version,
                    target_version,
                    start_time.elapsed().as_secs_f64()
                );
                if abort_on_finish {
                    self.table_info_service.abort();
                }
                break;
            }

            // Log status every 1 second
            if last_log_time.elapsed().as_secs() >= STATUS_LOG_INTERVAL_SECS {
                let versions_behind = target_version.saturating_sub(stream_version);
                let elapsed_secs = start_time.elapsed().as_secs_f64();
                info!(
                    "Indexer_grpc progress: target={}, table_info_current={}, stream_version={}, behind={}, elapsed={:.2}s",
                    target_version, table_info_version, stream_version, versions_behind, elapsed_secs
                );
                last_log_time = Instant::now();
            }

            tokio::time::sleep(Duration::from_millis(INDEXER_GRPC_POLL_INTERVAL_MS)).await;
        }
    }
```

**File:** execution/executor-benchmark/src/pipeline.rs (L338-344)
```rust
                            waiter.wait_for_version(target_ver - 1, /*abort_on_finish=*/ true),
                        );
                        indexer_wrapper.2.store(true, Ordering::SeqCst);
                        info!("Indexer_grpc waiter finished");
                        // This is a HACK. Just sleep here to wait the DB lock to drop.
                        std::thread::sleep(std::time::Duration::from_secs(2));
                        0
```
