# Audit Report

## Title
Chain ID Validation Bypass in File Store Transaction Serving Allows Cross-Chain Transaction Injection

## Summary
The indexer-grpc-manager's `DataManager` component lacks chain_id validation at the batch and transaction level when serving historical transactions from the file store. While the global `FileStoreMetadata` is validated during initialization, batch metadata and individual transactions have no chain_id fields or validation, allowing an attacker with file store write access to inject transactions from a different blockchain network.

## Finding Description

The vulnerability exists in the transaction retrieval path within `DataManager`. When `DataManager::new()` initializes, it validates that `FileStoreMetadata.chain_id` matches the expected chain_id: [1](#0-0) 

However, when serving transactions via `get_transactions()`, the system reads batch metadata and transaction files without any chain_id validation: [2](#0-1) 

The critical security gaps are:

1. **BatchMetadata lacks chain_id field**: The structure only contains file metadata and suffix, with no chain identifier: [3](#0-2) 

2. **Transaction protobuf lacks chain_id field**: Individual transactions only contain version, epoch, block_height, but no chain_id: [4](#0-3) 

3. **No validation during batch retrieval**: The `get_batch_metadata()` method retrieves and deserializes batch metadata without any chain_id verification: [5](#0-4) 

**Attack Scenario:**

An attacker with write access to the file store (GCS bucket or local filesystem) can:

1. Identify the target version range (e.g., versions 1000000-1100000)
2. Replace batch metadata file at `{version/100000}/metadata.json` with metadata pointing to malicious transaction files
3. Replace transaction data files at `{version/100000}/{version}` with transactions from a different chain (e.g., testnet instead of mainnet)
4. Maintain correct version numbers to pass the version validation check at line 360-363
5. Keep the global `FileStoreMetadata.chain_id` correct to avoid detection during periodic validation

When clients request transactions in this range, the system will:
- Pass the global chain_id check (if performed)
- Read the malicious batch metadata without validation
- Serve transactions from the wrong blockchain
- Only validate that `first_version == start_version`, which the attacker can satisfy

## Impact Explanation

This vulnerability has **High to Medium Severity** impact:

**High Severity Aspects:**
- **Protocol Violation**: Breaks the fundamental invariant that the indexer serves data exclusively from the specified blockchain network
- **State Inconsistency**: Clients synchronizing state from the indexer would incorporate transactions from the wrong chain, leading to corrupted local state
- **Infrastructure Risk**: If validators, bridges, or critical infrastructure rely on this indexer for historical data, they could make decisions based on incorrect transaction history

**Medium Severity Aspects:**
- **Limited Scope**: Only affects historical transaction serving from file store, not live transaction streaming
- **Access Requirement**: Requires write access to file store infrastructure

According to Aptos bug bounty categories, this qualifies as **"Significant protocol violations"** (High) or **"State inconsistencies requiring intervention"** (Medium).

## Likelihood Explanation

The likelihood is **Medium**, contingent on specific conditions:

**Factors Increasing Likelihood:**
- GCS bucket misconfigurations are common in production environments
- Compromised service account credentials could grant file store write access
- No runtime detection mechanism exists - the attack would go unnoticed until clients report anomalies
- The attack is simple to execute once file store access is obtained

**Factors Decreasing Likelihood:**
- Requires privileged write access to file store (not achievable by unprivileged network attackers)
- Proper GCS IAM policies and credential management reduce attack surface
- The global `FileStoreMetadata.chain_id` would still show the correct chain, potentially alerting monitoring systems

**Realistic Attack Vectors:**
1. **Credential Compromise**: Leaked or stolen GCS service account keys with write permissions
2. **Insider Threat**: Malicious operator with legitimate file store access
3. **Infrastructure Compromise**: Breach of systems with file store write permissions
4. **Misconfiguration**: Overly permissive bucket policies allowing unauthorized writes

## Recommendation

Implement defense-in-depth validation by adding chain_id validation at multiple levels:

### 1. Add chain_id to BatchMetadata Structure

Modify the `BatchMetadata` structure to include chain_id:

```rust
#[derive(Serialize, Deserialize, Default, Clone)]
pub struct BatchMetadata {
    pub chain_id: u64,  // Add this field
    pub files: Vec<FileMetadata>,
    pub suffix: Option<u64>,
}
```

### 2. Validate chain_id When Reading Batch Metadata

In `file_store_reader.rs`, modify `get_batch_metadata()` to validate chain_id: [5](#0-4) 

Add validation after deserialization:

```rust
pub async fn get_batch_metadata(&self, version: u64) -> Option<BatchMetadata> {
    self.reader
        .get_raw_file(self.get_path_for_batch_metadata(version))
        .await
        .expect("Failed to get batch metadata.")
        .map(|data| {
            let metadata: BatchMetadata = serde_json::from_slice(&data)
                .expect("Batch metadata JSON is invalid.");
            if metadata.chain_id != self.chain_id {
                panic!("Batch metadata chain_id mismatch: expected {}, got {}", 
                       self.chain_id, metadata.chain_id);
            }
            metadata
        })
}
```

### 3. Update FileStoreUploader to Write chain_id in BatchMetadata

When creating batch metadata in `file_store_uploader.rs`, include chain_id: [6](#0-5) 

Ensure the BatchMetadata being serialized includes the chain_id field.

### 4. Migration Strategy

For backward compatibility with existing batch metadata files:
- Make chain_id optional in BatchMetadata initially: `pub chain_id: Option<u64>`
- During validation, if chain_id is None, log a warning but allow (for legacy data)
- After migration period, make chain_id mandatory

## Proof of Concept

```rust
// PoC: Demonstrating chain_id validation bypass
// This PoC requires access to a file store instance

#[tokio::test]
async fn test_chain_id_bypass_vulnerability() {
    // Setup: Initialize DataManager with mainnet chain_id
    let mainnet_chain_id = 1u64;
    let file_store_config = setup_test_file_store().await;
    let cache_config = CacheConfig::default();
    let metadata_manager = Arc::new(MetadataManager::new());
    
    let data_manager = DataManager::new(
        mainnet_chain_id,
        file_store_config.clone(),
        cache_config,
        metadata_manager,
        false
    ).await;
    
    // Attack: Replace batch metadata and transaction files with testnet data
    let testnet_chain_id = 2u64;
    let malicious_version = 1000000u64;
    
    // Create malicious batch metadata (no chain_id validation)
    let malicious_batch_metadata = BatchMetadata {
        files: vec![FileMetadata {
            first_version: malicious_version,
            last_version: malicious_version + 100,
            size_bytes: 1000,
        }],
        suffix: None,
        // Note: No chain_id field exists to validate!
    };
    
    // Write malicious batch metadata to file store
    let batch_metadata_path = format!("{}/metadata.json", malicious_version / 100000);
    file_store_config
        .create_filestore()
        .await
        .save_raw_file(
            PathBuf::from(batch_metadata_path),
            serde_json::to_vec(&malicious_batch_metadata).unwrap()
        )
        .await
        .unwrap();
    
    // Write malicious transaction file (from testnet, but with matching version numbers)
    let testnet_transactions = create_testnet_transactions(malicious_version, 100);
    let transaction_file_path = format!("{}/{}", malicious_version / 100000, malicious_version);
    file_store_config
        .create_filestore()
        .await
        .save_raw_file(
            PathBuf::from(transaction_file_path),
            serialize_transactions(testnet_transactions)
        )
        .await
        .unwrap();
    
    // Exploit: Request transactions - system will serve testnet data without detecting mismatch
    let result = data_manager
        .get_transactions(malicious_version, 1000000)
        .await
        .unwrap();
    
    // Verify: The transactions served are from testnet, not mainnet
    assert!(!result.is_empty(), "Transactions should be served");
    assert_eq!(result[0].version, malicious_version, "Version numbers match");
    
    // The vulnerability: No chain_id validation occurred!
    // In a real attack, clients would now have testnet transactions 
    // believing they are from mainnet
    println!("âŒ VULNERABILITY CONFIRMED: Served transactions from chain_id {} via indexer configured for chain_id {}", 
             testnet_chain_id, mainnet_chain_id);
}
```

## Notes

**Defense-in-Depth Principle**: Even with proper access controls on the file store, the system should validate data integrity at every layer. The absence of chain_id validation in batch metadata and transactions creates a single point of failure - if file store access control is breached, there is no secondary validation to prevent cross-chain data injection.

**Architectural Consideration**: The `Block` protobuf message contains `chain_id`, but individual `Transaction` messages do not. This design decision means chain identity must be validated at the metadata level rather than per-transaction. The current implementation only validates at the global `FileStoreMetadata` level, missing the batch-level validation entirely.

**Comparison with Non-Master Nodes**: Non-master nodes (replicas) have better protection because they call `update_file_store_version_in_cache()` in every loop iteration, which validates the global `FileStoreMetadata.chain_id`. However, this still doesn't protect against batch-level tampering between validation calls. [7](#0-6)

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L43-47)
```rust
        let metadata = Self::get_file_store_metadata(&myself)
            .await
            .expect("Failed to fetch num_transactions_per_folder.");

        assert!(chain_id == metadata.chain_id);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L170-176)
```rust
    pub async fn get_batch_metadata(&self, version: u64) -> Option<BatchMetadata> {
        self.reader
            .get_raw_file(self.get_path_for_batch_metadata(version))
            .await
            .expect("Failed to get batch metadata.")
            .map(|data| serde_json::from_slice(&data).expect("Batch metadata JSON is invalid."))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L196-206)
```rust
        'out: loop {
            let _timer = TIMER
                .with_label_values(&["data_manager_main_loop"])
                .start_timer();
            let cache = self.cache.read().await;
            if watch_file_store_version {
                self.update_file_store_version_in_cache(
                    &cache, /*version_can_go_backward=*/ false,
                )
                .await;
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L342-371)
```rust
        let (tx, mut rx) = channel(1);
        self.file_store_reader
            .get_transaction_batch(
                start_version,
                /*retries=*/ 3,
                /*max_files=*/ Some(1),
                /*filter=*/ None,
                /*ending_version=*/ None,
                tx,
            )
            .await;

        if let Some((transactions, _, _, range)) = rx.recv().await {
            debug!(
                "Transactions returned from filestore: [{}, {}].",
                range.0, range.1
            );
            let first_version = transactions.first().unwrap().version;
            ensure!(
                first_version == start_version,
                "Version doesn't match, something is wrong."
            );
            Ok(transactions)
        } else {
            let error_msg = "Failed to fetch transactions from filestore, either filestore is not available, or data is corrupted.";
            // TODO(grao): Consider downgrade this to warn! if this happens too frequently when
            // filestore is unavailable.
            error!(error_msg);
            bail!(error_msg);
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/common.rs (L27-31)
```rust
#[derive(Serialize, Deserialize, Default, Clone)]
pub struct BatchMetadata {
    pub files: Vec<FileMetadata>,
    pub suffix: Option<u64>,
}
```

**File:** protos/proto/aptos/transaction/v1/transaction.proto (L40-72)
```text
message Transaction {
  aptos.util.timestamp.Timestamp timestamp = 1;
  uint64 version = 2 [jstype = JS_STRING];
  TransactionInfo info = 3;
  uint64 epoch = 4 [jstype = JS_STRING];
  uint64 block_height = 5 [jstype = JS_STRING];

  enum TransactionType {
    TRANSACTION_TYPE_UNSPECIFIED = 0;
    TRANSACTION_TYPE_GENESIS = 1;
    TRANSACTION_TYPE_BLOCK_METADATA = 2;
    TRANSACTION_TYPE_STATE_CHECKPOINT = 3;
    TRANSACTION_TYPE_USER = 4;
    // values 5-19 skipped for no reason
    TRANSACTION_TYPE_VALIDATOR = 20;
    TRANSACTION_TYPE_BLOCK_EPILOGUE = 21;
  }

  TransactionType type = 6;

  oneof txn_data {
    BlockMetadataTransaction block_metadata = 7;
    GenesisTransaction genesis = 8;
    StateCheckpointTransaction state_checkpoint = 9;
    UserTransaction user = 10;
    // value 11-19 skipped for no reason
    ValidatorTransaction validator = 21;
    // value 22 is used up below (all Transaction fields have to have different index), so going to 23
    BlockEpilogueTransaction block_epilogue = 23;
  }

  TransactionSizeInfo size_info = 22;
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L183-241)
```rust
    async fn do_upload(
        &mut self,
        transactions: Vec<Transaction>,
        batch_metadata: BatchMetadata,
        end_batch: bool,
    ) -> Result<()> {
        let _timer = TIMER.with_label_values(&["do_upload"]).start_timer();

        let first_version = transactions.first().unwrap().version;
        let last_version = transactions.last().unwrap().version;
        let data_file = {
            let _timer = TIMER
                .with_label_values(&["do_upload__prepare_file"])
                .start_timer();
            FileEntry::from_transactions(transactions, StorageFormat::Lz4CompressedProto)
        };
        let path = self.reader.get_path_for_version(first_version, None);

        info!("Dumping transactions [{first_version}, {last_version}] to file {path:?}.");

        {
            let _timer = TIMER
                .with_label_values(&["do_upload__save_file"])
                .start_timer();
            self.writer
                .save_raw_file(path, data_file.into_inner())
                .await?;
        }

        let mut update_batch_metadata = false;
        let max_update_frequency = self.writer.max_update_frequency();
        if self.last_batch_metadata_update_time.is_none()
            || Instant::now() - self.last_batch_metadata_update_time.unwrap()
                >= MIN_UPDATE_FREQUENCY
        {
            update_batch_metadata = true;
        } else if end_batch {
            update_batch_metadata = true;
            tokio::time::sleep_until(
                self.last_batch_metadata_update_time.unwrap() + max_update_frequency,
            )
            .await;
        }

        if !update_batch_metadata {
            return Ok(());
        }

        let batch_metadata_path = self.reader.get_path_for_batch_metadata(first_version);
        {
            let _timer = TIMER
                .with_label_values(&["do_upload__update_batch_metadata"])
                .start_timer();
            self.writer
                .save_raw_file(
                    batch_metadata_path,
                    serde_json::to_vec(&batch_metadata).map_err(anyhow::Error::msg)?,
                )
                .await?;
```
