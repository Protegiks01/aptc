# Audit Report

## Title
Silent Failures in Peer Monitoring Service Spawned Tasks Lead to Undetected Operational Issues

## Summary
The peer monitoring service spawns async tasks without monitoring their execution status. The returned `JoinHandle<()>` values are immediately dropped, allowing tasks to fail silently without detection, logging, or alerting. This affects both the peer metadata updater task and individual peer state refresh tasks.

## Finding Description

The peer monitoring service has two critical spawning points where `JoinHandle<()>` values are dropped:

**1. Metadata Updater Task (Unmonitored Background Service)** [1](#0-0) 

The `spawn_peer_metadata_updater` function returns a `JoinHandle<()>`, but when called from `start_peer_monitor`, this handle is completely ignored. The spawned task runs an infinite loop updating peer monitoring metadata: [2](#0-1) 

**2. Peer State Refresh Tasks (Dropped After Spawning)** [3](#0-2) 

The `refresh_peer_state_key` method spawns async tasks and returns `Result<JoinHandle<()>, Error>`. The `?` operator only checks the `Result` wrapper for initial setup errors, but the `JoinHandle` itself is immediately dropped: [4](#0-3) 

**Comparison with Proper Task Supervision**

Other components in the Aptos codebase properly supervise spawned tasks using `tokio::select!`: [5](#0-4) 

This pattern monitors spawned tasks and exits the process if they fail unexpectedly, providing clear error visibility.

## Impact Explanation

This issue qualifies as **High severity** under the category of "Significant protocol violations" for the following reasons:

1. **Validator Node Operational Degradation**: Silent failures in peer monitoring can lead to suboptimal peer selection decisions, causing validators to maintain connections with unhealthy or malicious peers while missing better alternatives.

2. **No Operational Visibility**: Node operators have no metrics, logs, or alerts indicating that peer monitoring has failed. The main peer monitor loop continues running, creating a false sense that monitoring is operational.

3. **Cascading Failures**: If individual peer state refresh tasks panic and their request trackers remain marked as "in-flight," new requests cannot be spawned for those peers, leading to permanently stale peer information.

4. **State Sync and Consensus Network Impact**: Other subsystems (state sync, consensus networking) rely on accurate peer metadata from the monitoring service to make intelligent peer selection decisions. Stale metadata can cause these components to prefer unhealthy peers.

## Likelihood Explanation

**Medium to High likelihood** for the following reasons:

1. **Future Code Changes**: As the codebase evolves, new code paths in the spawned tasks could introduce panics or unhandled error conditions.

2. **Dependency Updates**: External dependencies used within these tasks could introduce panics in future versions.

3. **Resource Constraints**: While not directly attacker-controlled, resource exhaustion scenarios (memory pressure, thread pool saturation) could cause task failures.

4. **No Test Coverage for Task Failures**: The test suite doesn't verify that task supervision is in place, making this type of regression easy to introduce.

5. **Existing Pattern Inconsistency**: The codebase already shows proper task supervision in other components, indicating awareness of this pattern. The peer monitoring service's deviation from this pattern suggests it was overlooked rather than intentionally designed.

## Recommendation

Implement proper task supervision following the patterns used in other Aptos components:

```rust
pub async fn start_peer_monitor(
    node_config: NodeConfig,
    network_client: NetworkClient<PeerMonitoringServiceMessage>,
    runtime: Option<Handle>,
) {
    let peer_monitoring_client = PeerMonitoringServiceClient::new(network_client);
    let peer_monitor_state = PeerMonitorState::new();
    let time_service = TimeService::real();
    
    // Spawn the peer metadata updater and store the handle
    let metadata_updater_handle = spawn_peer_metadata_updater(
        node_config.peer_monitoring_service,
        peer_monitor_state.clone(),
        peer_monitoring_client.get_peers_and_metadata(),
        time_service.clone(),
        runtime.clone(),
    );
    
    // Start the main peer monitor with supervision
    let main_monitor_handle = tokio::spawn(start_peer_monitor_with_state(
        node_config,
        peer_monitoring_client,
        peer_monitor_state,
        time_service,
        runtime,
    ));
    
    // Monitor both tasks and exit if either fails
    tokio::select! {
        res = metadata_updater_handle => {
            if let Err(e) = res {
                error!("Peer metadata updater task panicked: {:?}", e);
                process::exit(1);
            } else {
                panic!("Peer metadata updater exited unexpectedly");
            }
        },
        res = main_monitor_handle => {
            if let Err(e) = res {
                error!("Main peer monitor task panicked: {:?}", e);
                process::exit(1);
            } else {
                panic!("Main peer monitor exited unexpectedly");
            }
        },
    }
}
```

For the individual peer state refresh tasks, use a `JoinSet` to track and monitor them:

```rust
use tokio::task::JoinSet;

pub fn refresh_peer_states(
    // ... existing parameters ...
    join_set: &mut JoinSet<()>,
) -> Result<(), Error> {
    for peer_state_key in PeerStateKey::get_all_keys() {
        for (peer_network_id, peer_metadata) in &connected_peers_and_metadata {
            let peer_state = get_peer_state(&peer_monitor_state, peer_network_id)?;
            let request_tracker = peer_state.get_request_tracker(&peer_state_key)?;
            
            if request_tracker.read().new_request_required() {
                let handle = peer_state.refresh_peer_state_key(/* ... */)?;
                join_set.spawn(handle);
            }
        }
    }
    
    // Periodically check for completed tasks
    while let Some(result) = join_set.try_join_next() {
        if let Err(e) = result {
            warn!("Peer state refresh task failed: {:?}", e);
        }
    }
    
    Ok(())
}
```

Additionally, add metrics to track task health:
- Time since last successful metadata update
- Number of peer state refresh tasks that have panicked
- Heartbeat metrics for the metadata updater loop

## Proof of Concept

```rust
#[tokio::test]
async fn test_silent_task_failure_detection() {
    use std::sync::atomic::{AtomicBool, Ordering};
    use std::sync::Arc;
    
    // Create a flag to simulate a panic condition
    let should_panic = Arc::new(AtomicBool::new(false));
    let should_panic_clone = should_panic.clone();
    
    // Spawn a task similar to the metadata updater
    let handle = tokio::spawn(async move {
        loop {
            tokio::time::sleep(Duration::from_millis(100)).await;
            
            if should_panic_clone.load(Ordering::Relaxed) {
                panic!("Simulated task failure");
            }
            
            // Simulate metadata update work
        }
    });
    
    // Drop the handle (simulating current behavior)
    drop(handle);
    
    // Wait a bit
    tokio::time::sleep(Duration::from_millis(200)).await;
    
    // Trigger the panic
    should_panic.store(true, Ordering::Relaxed);
    
    // Wait - the panic will happen silently
    tokio::time::sleep(Duration::from_millis(300)).await;
    
    // The test continues - no indication the task failed!
    // This demonstrates the vulnerability: silent failures go undetected
}

#[tokio::test]
async fn test_proper_task_supervision() {
    use std::sync::atomic::{AtomicBool, Ordering};
    use std::sync::Arc;
    
    let should_panic = Arc::new(AtomicBool::new(false));
    let should_panic_clone = should_panic.clone();
    
    // Spawn and MONITOR the task
    let handle = tokio::spawn(async move {
        loop {
            tokio::time::sleep(Duration::from_millis(100)).await;
            
            if should_panic_clone.load(Ordering::Relaxed) {
                panic!("Simulated task failure");
            }
        }
    });
    
    // Wait a bit
    tokio::time::sleep(Duration::from_millis(200)).await;
    
    // Trigger the panic
    should_panic.store(true, Ordering::Relaxed);
    
    // Monitor the task - this will detect the failure
    let result = handle.await;
    
    // Assert that we detected the panic
    assert!(result.is_err());
    println!("Successfully detected task failure: {:?}", result.unwrap_err());
}
```

This PoC demonstrates that:
1. Without monitoring (first test), task failures go undetected
2. With proper monitoring (second test), failures are caught and can be handled

## Notes

While this vulnerability represents a significant operational and reliability concern, it's important to note that:

1. **Indirect Impact**: The security impact is indirect - poor peer selection due to stale monitoring data could lead to connecting with malicious peers, but the consensus protocol is designed to tolerate Byzantine behavior.

2. **No Direct Exploitation**: An external attacker cannot directly trigger these task failures; they would occur due to internal bugs or resource constraints.

3. **Defensive Design**: The Aptos consensus and networking layers have multiple layers of defense beyond peer monitoring, so this issue alone would not compromise network security.

However, the lack of task supervision is inconsistent with best practices demonstrated elsewhere in the Aptos codebase and should be addressed to ensure operational reliability and maintain defense-in-depth.

### Citations

**File:** peer-monitoring-service/client/src/lib.rs (L70-76)
```rust
    spawn_peer_metadata_updater(
        node_config.peer_monitoring_service,
        peer_monitor_state.clone(),
        peer_monitoring_client.get_peers_and_metadata(),
        time_service.clone(),
        runtime.clone(),
    );
```

**File:** peer-monitoring-service/client/src/lib.rs (L206-270)
```rust
pub(crate) fn spawn_peer_metadata_updater(
    peer_monitoring_config: PeerMonitoringServiceConfig,
    peer_monitor_state: PeerMonitorState,
    peers_and_metadata: Arc<PeersAndMetadata>,
    time_service: TimeService,
    runtime: Option<Handle>,
) -> JoinHandle<()> {
    // Create the updater task for the peers and metadata struct
    let metadata_updater = async move {
        // Create an interval ticker for the updater loop
        let metadata_update_loop_duration =
            Duration::from_millis(peer_monitoring_config.metadata_update_interval_ms);
        let metadata_update_loop_ticker = time_service.interval(metadata_update_loop_duration);
        futures::pin_mut!(metadata_update_loop_ticker);

        // Start the updater loop
        info!(LogSchema::new(LogEntry::MetadataUpdateLoop)
            .event(LogEvent::StartedMetadataUpdaterLoop)
            .message("Starting the peers and metadata updater!"));
        loop {
            // Wait for the next round before updating peers and metadata
            metadata_update_loop_ticker.next().await;

            // Get all peers
            let all_peers = peers_and_metadata.get_all_peers();

            // Update the latest peer monitoring metadata
            for peer_network_id in all_peers {
                let peer_monitoring_metadata =
                    match peer_monitor_state.peer_states.read().get(&peer_network_id) {
                        Some(peer_state) => {
                            peer_state
                                .extract_peer_monitoring_metadata()
                                .unwrap_or_else(|error| {
                                    // Log the error and return the default
                                    warn!(LogSchema::new(LogEntry::MetadataUpdateLoop)
                                        .event(LogEvent::UnexpectedErrorEncountered)
                                        .peer(&peer_network_id)
                                        .error(&error));
                                    PeerMonitoringMetadata::default()
                                })
                        },
                        None => PeerMonitoringMetadata::default(), // Use the default
                    };

                // Insert the latest peer monitoring metadata into peers and metadata
                if let Err(error) = peers_and_metadata
                    .update_peer_monitoring_metadata(peer_network_id, peer_monitoring_metadata)
                {
                    warn!(LogSchema::new(LogEntry::MetadataUpdateLoop)
                        .event(LogEvent::UnexpectedErrorEncountered)
                        .peer(&peer_network_id)
                        .error(&error.into()));
                }
            }
        }
    };

    // Spawn the peer metadata updater task
    if let Some(runtime) = runtime {
        runtime.spawn(metadata_updater)
    } else {
        tokio::spawn(metadata_updater)
    }
}
```

**File:** peer-monitoring-service/client/src/peer_states/mod.rs (L58-67)
```rust
                peer_state.refresh_peer_state_key(
                    monitoring_service_config,
                    &peer_state_key,
                    peer_monitoring_client.clone(),
                    *peer_network_id,
                    peer_metadata.clone(),
                    peer_monitor_state.request_id_generator.clone(),
                    time_service.clone(),
                    runtime.clone(),
                )?;
```

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L162-169)
```rust
        // Spawn the request task
        let join_handle = if let Some(runtime) = runtime {
            runtime.spawn(request_task)
        } else {
            tokio::spawn(request_task)
        };

        Ok(join_handle)
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L53-76)
```rust
    let task_handler = tokio::spawn(async move {
        register_probes_and_metrics_handler(config_clone, health_port).await;
        anyhow::Ok(())
    });
    let main_task_handler =
        tokio::spawn(async move { config.run().await.expect("task should exit with Ok.") });
    tokio::select! {
        res = task_handler => {
            if let Err(e) = res {
                error!("Probes and metrics handler panicked or was shutdown: {:?}", e);
                process::exit(1);
            } else {
                panic!("Probes and metrics handler exited unexpectedly");
            }
        },
        res = main_task_handler => {
            if let Err(e) = res {
                error!("Main task panicked or was shutdown: {:?}", e);
                process::exit(1);
            } else {
                panic!("Main task exited unexpectedly");
            }
        },
    }
```
