# Audit Report

## Title
Missing Configuration Validation Allows Mainnet Validators to Disable Critical Storage Pruners Leading to Unbounded Growth and Node Failure

## Summary
The AptosDB storage configuration system allows validators to disable all storage pruners (ledger, state merkle, and epoch snapshot pruners) without any validation checks for production mainnet deployments. When pruners are disabled, transaction history and associated data grow unbounded until available storage is exhausted, causing validator node crashes and potential network liveness degradation.

## Finding Description

The storage pruner system in AptosDB consists of three independent pruners that prevent unbounded data growth: [1](#0-0) 

Each pruner has an `enable` boolean flag that controls whether pruning occurs. The `LedgerPrunerManager` checks this flag during initialization: [2](#0-1) 

When `enable: false`, the `pruner_worker` is set to `None`, and `is_pruner_enabled()` returns false, permanently disabling all pruning for that pruner type: [3](#0-2) 

Transaction data is stored in multiple RocksDB schemas keyed by monotonically increasing version numbers: [4](#0-3) 

Without pruning, these schemas accumulate data indefinitely as each transaction commits. The `ConfigSanitizer` for `StorageConfig` validates prune window sizes but **does not enforce that pruners must be enabled** for mainnet validators: [5](#0-4) 

This contrasts with `ExecutionConfig`, which **does enforce** critical safety settings for mainnet: [6](#0-5) 

The monitoring system includes alerts for low disk space but cannot prevent the misconfiguration: [7](#0-6) 

**Attack Scenario:**
1. Node operator (or attacker via social engineering) sets `storage.storage_pruner_config.ledger_pruner_config.enable = false` in validator configuration
2. Validator starts and accepts the configuration without validation error
3. Transactions accumulate at mainnet throughput (~1000-10000 TPS)
4. Storage grows unbounded: ~1-10GB per day depending on transaction size and throughput
5. After weeks/months (depending on disk size), storage exhausts
6. Validator crashes due to inability to write new transactions
7. If multiple validators are affected, network liveness degrades

## Impact Explanation

**High Severity** per Aptos bug bounty criteria: "Validator node slowdowns" and "API crashes"

The impact includes:
- **Validator Unavailability**: Individual validators crash when storage fills, removing them from the validator set temporarily
- **Network Liveness Risk**: If multiple validators are misconfigured (>1/3), network could experience liveness issues
- **Data Availability**: Even if <1/3 validators are affected, the network loses historical data availability from those nodes
- **Operational Burden**: Recovery requires manual intervention, disk expansion, or database restoration

The default configuration has pruners enabled, but nothing prevents operators from disabling them. The alert runbook explicitly mentions "shorten the pruner windows" as a mitigation, acknowledging this is a real operational concern.

## Likelihood Explanation

**Medium-Low Likelihood** due to:

**Increasing factors:**
- Simple configuration change (single boolean flag)
- No validation prevents this dangerous configuration
- Affects all node types (validators, VFNs, fullnodes) if misconfigured
- Social engineering could trick operators into disabling pruners
- Documentation exists but operators may not understand the severity

**Decreasing factors:**
- Default configuration is safe (pruners enabled)
- Requires node operator action, not external attacker exploit
- Monitoring alerts warn before total exhaustion
- Most operators use recommended configurations

However, the **lack of defensive validation** means this is a latent risk that violates the security principle of defense-in-depth. Critical infrastructure should not allow obviously dangerous configurations.

## Recommendation

Add mainnet validation to `StorageConfig::sanitize()` following the pattern established in `ExecutionConfig::sanitize()`:

```rust
impl ConfigSanitizer for StorageConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.storage;

        // NEW: Enforce pruners must be enabled for mainnet/testnet
        if let Some(chain_id) = chain_id {
            if chain_id.is_mainnet() || chain_id.is_testnet() {
                if !config.storage_pruner_config.ledger_pruner_config.enable {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "ledger_pruner_config.enable must be true for mainnet/testnet nodes to prevent storage exhaustion!".into(),
                    ));
                }
                if !config.storage_pruner_config.state_merkle_pruner_config.enable {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "state_merkle_pruner_config.enable must be true for mainnet/testnet nodes!".into(),
                    ));
                }
                if !config.storage_pruner_config.epoch_snapshot_pruner_config.enable {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "epoch_snapshot_pruner_config.enable must be true for mainnet/testnet nodes!".into(),
                    ));
                }
            }
        }

        // Existing validation code...
        let ledger_prune_window = config
            .storage_pruner_config
            .ledger_pruner_config
            .prune_window;
        // ... rest of existing checks
    }
}
```

Additionally, consider:
1. Adding similar validation for minimum prune windows on mainnet (not just warnings)
2. Adding metrics for pruner enablement status
3. Documenting the risks of disabling pruners more prominently

## Proof of Concept

Create a test node configuration with pruners disabled and observe unbounded growth:

```rust
// In storage/aptosdb/tests/pruner_test.rs (new test file)
#[test]
fn test_unbounded_growth_without_pruner() {
    use aptos_config::config::{LedgerPrunerConfig, PrunerConfig, NO_OP_STORAGE_PRUNER_CONFIG};
    use aptos_temppath::TempPath;
    use aptos_storage_interface::DbWriter;
    
    let tmpdir = TempPath::new();
    
    // Create DB with pruner disabled
    let db = AptosDB::open(
        StorageDirPaths::from_path(&tmpdir),
        false, // not readonly
        NO_OP_STORAGE_PRUNER_CONFIG, // All pruners disabled!
        RocksdbConfigs::default(),
        false, // no indexer
        BUFFERED_STATE_TARGET_ITEMS_FOR_TEST,
        DEFAULT_MAX_NUM_NODES_PER_LRU_CACHE_SHARD,
        None,
        HotStateConfig::default(),
    ).unwrap();
    
    // Commit many transactions
    for i in 0..100000 {
        let txns_to_commit = create_test_transaction_batch(i);
        db.save_transactions_for_test(&txns_to_commit, i, None, true).unwrap();
    }
    
    // Verify all transactions still exist (no pruning occurred)
    assert!(db.get_transaction(0).is_ok());
    assert!(db.get_transaction(99999).is_ok());
    
    // Check disk usage keeps growing
    let initial_size = get_db_size(&tmpdir);
    
    for i in 100000..200000 {
        let txns_to_commit = create_test_transaction_batch(i);
        db.save_transactions_for_test(&txns_to_commit, i, None, true).unwrap();
    }
    
    let final_size = get_db_size(&tmpdir);
    assert!(final_size > initial_size * 2, "Storage should grow unbounded");
}

// Test that mainnet config validation catches disabled pruners
#[test]
fn test_mainnet_rejects_disabled_pruners() {
    let mut node_config = NodeConfig::default();
    node_config.storage.storage_pruner_config.ledger_pruner_config.enable = false;
    
    let result = StorageConfig::sanitize(
        &node_config,
        NodeType::Validator,
        Some(ChainId::mainnet()),
    );
    
    assert!(result.is_err(), "Mainnet should reject disabled ledger pruner");
    assert!(result.unwrap_err().to_string().contains("ledger_pruner_config.enable must be true"));
}
```

**Notes:**
- The vulnerability exists in production code paths ( [8](#0-7) )
- The test file reference in the question ( [9](#0-8) ) demonstrates the NO_OP_STORAGE_PRUNER_CONFIG usage pattern
- Real-world impact depends on operator adoption of unsafe configurations
- Defense-in-depth principle suggests this validation should exist even if likelihood is low

### Citations

**File:** config/src/config/storage_config.rs (L327-341)
```rust
pub struct LedgerPrunerConfig {
    /// Boolean to enable/disable the ledger pruner. The ledger pruner is responsible for pruning
    /// everything else except for states (e.g. transactions, events etc.)
    pub enable: bool,
    /// This is the default pruning window for any other store except for state store. State store
    /// being big in size, we might want to configure a smaller window for state store vs other
    /// store.
    pub prune_window: u64,
    /// Batch size of the versions to be sent to the ledger pruner - this is to avoid slowdown due to
    /// issuing too many DB calls and batch prune instead. For ledger pruner, this means the number
    /// of versions to prune a time.
    pub batch_size: usize,
    /// The offset for user pruning window to adjust
    pub user_pruning_window_offset: u64,
}
```

**File:** config/src/config/storage_config.rs (L682-728)
```rust
impl ConfigSanitizer for StorageConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.storage;

        let ledger_prune_window = config
            .storage_pruner_config
            .ledger_pruner_config
            .prune_window;
        let state_merkle_prune_window = config
            .storage_pruner_config
            .state_merkle_pruner_config
            .prune_window;
        let epoch_snapshot_prune_window = config
            .storage_pruner_config
            .epoch_snapshot_pruner_config
            .prune_window;
        let user_pruning_window_offset = config
            .storage_pruner_config
            .ledger_pruner_config
            .user_pruning_window_offset;

        if ledger_prune_window < 50_000_000 {
            warn!("Ledger prune_window is too small, harming network data availability.");
        }
        if state_merkle_prune_window < 100_000 {
            warn!("State Merkle prune_window is too small, node might stop functioning.");
        }
        if epoch_snapshot_prune_window < 50_000_000 {
            warn!("Epoch snapshot prune_window is too small, harming network data availability.");
        }
        if user_pruning_window_offset > 1_000_000 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset too large, so big a buffer is unlikely necessary. Set something < 1 million.".to_string(),
            ));
        }
        if user_pruning_window_offset > ledger_prune_window {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset is larger than the ledger prune window, the API will refuse to return any data.".to_string(),
            ));
        }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L40-42)
```rust
    fn is_pruner_enabled(&self) -> bool {
        self.pruner_worker.is_some()
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L113-121)
```rust
        let pruner_worker = if ledger_pruner_config.enable {
            Some(Self::init_pruner(
                Arc::clone(&ledger_db),
                ledger_pruner_config,
                internal_indexer_db,
            ))
        } else {
            None
        };
```

**File:** storage/aptosdb/src/schema/transaction/mod.rs (L4-13)
```rust
//! This module defines physical storage schema for signed transactions.
//!
//! Serialized signed transaction bytes identified by version.
//! ```text
//! |<--key-->|<--value-->|
//! | version | txn bytes |
//! ```
//!
//! `Version` is serialized in big endian so that records in RocksDB will be in order of it's
//! numeric value.
```

**File:** config/src/config/execution_config.rs (L157-186)
```rust
impl ConfigSanitizer for ExecutionConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let execution_config = &node_config.execution;

        // If this is a mainnet node, ensure that additional verifiers are enabled
        if let Some(chain_id) = chain_id {
            if chain_id.is_mainnet() {
                if !execution_config.paranoid_hot_potato_verification {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "paranoid_hot_potato_verification must be enabled for mainnet nodes!"
                            .into(),
                    ));
                }
                if !execution_config.paranoid_type_verification {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "paranoid_type_verification must be enabled for mainnet nodes!".into(),
                    ));
                }
            }
        }

        Ok(())
    }
```

**File:** terraform/helm/monitoring/files/rules/alerts.yml (L110-125)
```yaml
  - alert: Validator Very Low Disk Space (critical)
    expr: (kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~".*(validator|fullnode)-e.*"} - kubelet_volume_stats_used_bytes) / 1024 / 1024 / 1024 < 50
    for: 5m
    labels:
      severity: critical
      summary: "Less than 50 GB of free space on Aptos Node."
    annotations:
      description: "A validator or fullnode pod has less than 50 GB of disk space -- that's dangerously low. \
        1. A warning level alert of disk space less than 200GB should've fired a few days ago at least, search on slack and understand why it's not dealt with.
        2. Search in the code for the runbook of the warning alert, quickly go through that too determine if it's a bug. Involve the storage team and other team accordingly.
      If no useful information is found, evaluate the trend of disk usage increasing, how long can we run further? If it can't last the night, you have these options to mitigate this:
        1. Expand the disk if it's a cloud volume.
        2. Shorten the pruner windows. Before that, find the latest version of these https://github.com/aptos-labs/aptos-core/blob/48cc64df8a64f2d13012c10d8bd5bf25d94f19dc/config/src/config/storage_config.rs#L166-L218 \
          and read carefully the comments on the prune window config entries -- set safe values.
        3. If you believe this is happening on nodes that are not run by us, involve the PE / Community / Ecosystem teams to coordinate efforts needed on those nodes.
      "
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L112-192)
```rust
    pub(super) fn open_internal(
        db_paths: &StorageDirPaths,
        readonly: bool,
        pruner_config: PrunerConfig,
        rocksdb_configs: RocksdbConfigs,
        enable_indexer: bool,
        buffered_state_target_items: usize,
        max_num_nodes_per_lru_cache_shard: usize,
        empty_buffered_state_for_restore: bool,
        internal_indexer_db: Option<InternalIndexerDB>,
        hot_state_config: HotStateConfig,
    ) -> Result<Self> {
        ensure!(
            pruner_config.eq(&NO_OP_STORAGE_PRUNER_CONFIG) || !readonly,
            "Do not set prune_window when opening readonly.",
        );

        let mut env =
            Env::new().map_err(|err| AptosDbError::OtherRocksDbError(err.into_string()))?;
        env.set_high_priority_background_threads(rocksdb_configs.high_priority_background_threads);
        env.set_low_priority_background_threads(rocksdb_configs.low_priority_background_threads);
        let block_cache = Cache::new_hyper_clock_cache(
            rocksdb_configs.shared_block_cache_size,
            /* estimated_entry_charge = */ 0,
        );

        let (ledger_db, hot_state_merkle_db, state_merkle_db, state_kv_db) = Self::open_dbs(
            db_paths,
            rocksdb_configs,
            Some(&env),
            Some(&block_cache),
            readonly,
            max_num_nodes_per_lru_cache_shard,
            hot_state_config.delete_on_restart,
        )?;

        let mut myself = Self::new_with_dbs(
            ledger_db,
            hot_state_merkle_db,
            state_merkle_db,
            state_kv_db,
            pruner_config,
            buffered_state_target_items,
            readonly,
            empty_buffered_state_for_restore,
            rocksdb_configs.enable_storage_sharding,
            internal_indexer_db,
            hot_state_config,
        );

        if !readonly {
            if let Some(version) = myself.get_synced_version()? {
                myself
                    .ledger_pruner
                    .maybe_set_pruner_target_db_version(version);
                myself
                    .state_store
                    .state_kv_pruner
                    .maybe_set_pruner_target_db_version(version);
            }
            if let Some(version) = myself.get_latest_state_checkpoint_version()? {
                myself
                    .state_store
                    .state_merkle_pruner
                    .maybe_set_pruner_target_db_version(version);
                myself
                    .state_store
                    .epoch_snapshot_pruner
                    .maybe_set_pruner_target_db_version(version);
            }
        }

        if !readonly && enable_indexer {
            myself.open_indexer(
                db_paths.default_root_path(),
                rocksdb_configs.index_db_config,
            )?;
        }

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_testonly.rs (L29-40)
```rust
impl AptosDB {
    /// This opens db in non-readonly mode, without the pruner.
    pub fn new_for_test<P: AsRef<Path> + Clone>(db_root_path: P) -> Self {
        Self::new_without_pruner(
            db_root_path,
            false,
            BUFFERED_STATE_TARGET_ITEMS_FOR_TEST,
            DEFAULT_MAX_NUM_NODES_PER_LRU_CACHE_SHARD,
            false, /* indexer */
            false,
        )
    }
```
