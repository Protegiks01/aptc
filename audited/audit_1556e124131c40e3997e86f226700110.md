# Audit Report

## Title
Unbounded HashMap Growth in State Restore Causing OOM via Malicious State Chunks

## Summary
The `add_chunk` method in state restoration unconditionally allocates a HashMap for all entries in a received chunk without validating the chunk size. A malicious peer can send state chunks with hundreds of thousands to millions of small entries that fit within the 40 MiB network limit but consume excessive memory when converted to a HashMap, causing out-of-memory crashes on victim nodes during state synchronization.

## Finding Description
The vulnerability exists in the state restoration module where incoming state value chunks are processed. At lines 117-120 of `state_restore/mod.rs`, the code converts the entire chunk vector into a HashMap without any size validation: [1](#0-0) 

The chunk parameter comes from `StateValueChunkWithProof.raw_values`, which is received from network peers during state synchronization. While the total serialized message size is limited to 40 MiB for v2 protocol: [2](#0-1) 

There is NO validation on the number of entries within that 40 MiB. An attacker can craft state chunks containing hundreds of thousands of very small StateKey-StateValue pairs (each serializing to ~40-50 bytes) that fit within the network size limit.

**Attack Flow:**
1. Victim node initiates state sync (fast sync mode)
2. Malicious peer responds to state value requests
3. Attacker crafts StateValueChunkWithProof with ~800,000-1,000,000 tiny entries
4. Serialized size: 40 bytes × 1M entries = 40 MB (within 40 MiB limit)
5. Victim node deserializes the chunk successfully (BCS validates total size only)
6. `add_chunk()` is called with the chunk containing 1M entries
7. HashMap allocation: ~200 bytes per entry × 1M = ~200 MB-1 GB memory
8. Multiple concurrent chunks or repeated attacks exhaust node memory
9. Node crashes with OOM, breaking availability

The configuration-based protection is insufficient. While `max_state_chunk_size` defaults to 4000: [3](#0-2) 

This limit is only enforced when the storage service **creates** chunks to send: [4](#0-3) 

When **receiving** chunks from peers, there is NO equivalent validation. The bootstrapper processes state values without checking entry count: [5](#0-4) 

The received chunk is passed directly to the state snapshot receiver without size validation, eventually reaching the vulnerable `add_chunk` code.

This breaks **Invariant #9 (Resource Limits)**: "All operations must respect gas, storage, and computational limits." The unbounded memory allocation violates resource constraints.

## Impact Explanation
**Medium Severity** - This vulnerability enables a Denial of Service attack against nodes performing state synchronization:

- **Affected Nodes**: Any validator or fullnode in fast sync mode or state snapshot restoration
- **Attack Vector**: Malicious peer sending oversized state chunks during state sync
- **Damage**: Node crash via OOM, requiring manual restart and potential sync restart
- **Scope**: Does NOT affect consensus safety or cause fund loss
- **Recovery**: Node can restart and retry sync (though vulnerable to repeated attacks)

Per Aptos Bug Bounty criteria, this qualifies as **Medium Severity** ($10,000): "State inconsistencies requiring intervention" - the OOM crash creates an inconsistent state requiring node operator intervention (restart, potential resync, possibly blacklisting malicious peers).

This does NOT reach High/Critical severity because:
- No consensus safety violation (crashed nodes simply fall behind)
- No fund loss or theft
- No permanent network partition
- Nodes can recover through restart

## Likelihood Explanation
**Moderate Likelihood:**

**Attacker Requirements:**
- Must be a peer connected to victim nodes
- Victim must be in state sync mode (common during initial sync or catch-up)
- No validator privileges required

**Attack Complexity:**
- Low - Simply craft StateValueChunkWithProof with many small entries
- Network reachability is the main barrier
- Can be automated

**Realistic Scenarios:**
1. New validator joining the network (state syncing from peers)
2. Fullnode catching up after downtime
3. Eclipse attack where attacker controls victim's peer connections

## Recommendation
Add explicit entry count validation before HashMap allocation:

```rust
pub fn add_chunk(&mut self, mut chunk: Vec<(K, V)>) -> Result<()> {
    // ADD: Validate chunk size before processing
    const MAX_CHUNK_ENTRIES: usize = 10_000; // Conservative limit
    if chunk.len() > MAX_CHUNK_ENTRIES {
        return Err(anyhow::anyhow!(
            "State chunk too large: {} entries exceeds maximum of {}",
            chunk.len(),
            MAX_CHUNK_ENTRIES
        ));
    }
    
    // load progress
    let progress_opt = self.db.get_progress(self.version)?;
    
    // ... rest of existing code
```

**Additional Hardening:**
1. Add validation in `StateSnapshotReceiver::add_chunk` before calling inner `add_chunk`
2. Add client-side validation when receiving `StateValueChunkWithProof` from network
3. Log warnings when chunks approach size limits to detect potential attacks
4. Consider rate-limiting state chunk processing per peer

## Proof of Concept
```rust
// PoC demonstrating memory exhaustion via large chunk
#[test]
fn test_state_restore_oom_via_large_chunk() {
    use aptos_types::state_store::{state_key::StateKey, state_value::StateValue};
    use bytes::Bytes;
    
    // Create a mock state restore instance
    let version = 1000;
    let db = Arc::new(MockStateValueWriter::new());
    let mut state_restore = StateValueRestore::new(db, version);
    
    // Craft malicious chunk with 1 million tiny entries
    // Each entry: 32-byte key + 10-byte value = ~42 bytes serialized
    // Total: ~42 MB serialized (within 40 MiB limit)
    // But HashMap overhead: ~200 bytes per entry = ~200 MB in memory
    let mut malicious_chunk = Vec::new();
    for i in 0..1_000_000 {
        let key = StateKey::raw(format!("key{}", i).as_bytes());
        let value = StateValue::new_legacy(Bytes::from(vec![0u8; 10]));
        malicious_chunk.push((key, value));
    }
    
    // This should fail with OOM or take excessive memory
    // Without the fix, this allocates ~200+ MB for the HashMap
    let result = state_restore.add_chunk(malicious_chunk);
    
    // With the fix, this should return an error
    assert!(result.is_err());
    assert!(result.unwrap_err().to_string().contains("too large"));
}
```

**Notes:**
- This PoC requires extending the test to actually measure memory consumption
- In practice, the attack would use multiple chunks sent concurrently to amplify impact
- The exact OOM threshold depends on available system memory and concurrent chunk processing

### Citations

**File:** storage/aptosdb/src/state_restore/mod.rs (L117-120)
```rust
        let kv_batch: StateValueBatch<K, Option<V>> = chunk
            .into_iter()
            .map(|(k, v)| ((k, self.version), Some(v)))
            .collect();
```

**File:** config/src/config/state_sync_config.rs (L20-21)
```rust
const CLIENT_MAX_MESSAGE_SIZE_V2: usize = 20 * 1024 * 1024; // 20 MiB (used for v2 data requests)
const SERVER_MAX_MESSAGE_SIZE_V2: usize = 40 * 1024 * 1024; // 40 MiB (used for v2 data requests)
```

**File:** config/src/config/state_sync_config.rs (L25-25)
```rust
const MAX_STATE_CHUNK_SIZE: u64 = 4000;
```

**File:** state-sync/storage-service/server/src/storage.rs (L910-911)
```rust
        let max_num_state_values = self.config.max_state_chunk_size;
        let num_state_values_to_fetch = min(expected_num_state_values, max_num_state_values);
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L872-881)
```rust
                StorageDataChunk::States(notification_id, states_with_proof) => {
                    // Commit the state value chunk
                    let all_states_synced = states_with_proof.is_last_chunk();
                    let last_committed_state_index = states_with_proof.last_index;
                    let num_state_values = states_with_proof.raw_values.len();

                    let result = state_snapshot_receiver.add_chunk(
                        states_with_proof.raw_values,
                        states_with_proof.proof.clone(),
                    );
```
