# Audit Report

## Title
Quota Desynchronization in Batch Cleanup Leading to Unbounded Database Growth

## Summary
The `update_certified_timestamp()` function in the quorum store batch management system contains a critical Time-Of-Check-Time-Of-Use (TOCTOU) vulnerability. When expired batches are cleaned up, their quota is freed from in-memory accounting **before** the actual database deletion is attempted. If the database deletion fails (e.g., disk full, I/O errors, corruption), the error is only logged while quota remains freed. This allows new batches to be continuously accepted while old batches accumulate in the database, causing unbounded database growth and eventual validator node failure.

## Finding Description

The vulnerability exists in the batch cleanup flow within the quorum store's batch management system. The `QuotaManager` tracks resource usage across three dimensions: batch count, database bytes, and memory bytes to prevent resource exhaustion. [1](#0-0) 

The critical flaw occurs in `update_certified_timestamp()` where the order of operations creates a TOCTOU vulnerability:

**Step 1 - Quota Freed First**: `clear_expired_payload()` removes expired batches from the in-memory cache and immediately frees their quota via `free_quota()`. [2](#0-1) 

The `free_quota()` method increases `batch_balance`, `db_balance`, and `memory_balance` immediately upon removing batches from the cache. [3](#0-2) 

**Step 2 - Database Deletion Attempted After**: Only after quota is freed does `update_certified_timestamp()` attempt to delete batches from persistent storage. [4](#0-3) 

**Step 3 - Silent Failure**: If `db.delete_batches()` fails, the error is only logged at debug level with no quota rollback or retry mechanism. [5](#0-4) 

The database deletion implementation uses RocksDB operations that can fail due to disk full, I/O errors, filesystem permissions, or database corruption. [6](#0-5) 

**Execution Flow:**
The vulnerable code path is triggered on every block commit through `QuorumStorePayloadManager::notify_commit()` which calls `update_certified_timestamp()`. [7](#0-6) 

**Attack Scenario:**
1. Validator operates normally with disk space becoming constrained
2. `update_certified_timestamp()` is called (happens every block commit)
3. `clear_expired_payload()` removes batches from memory and frees quota
4. `db.delete_batches()` fails due to disk full or I/O error
5. Error is logged but quota remains freed
6. New batches are accepted because quota accounting shows available space (checked via `update_quota()` which verifies `batch_balance` and `db_balance`) [8](#0-7) 
7. Cycle repeats: old batches accumulate in database while new batches are added
8. Database grows unboundedly until disk is completely full
9. Validator node crashes or becomes unresponsive

**Code Inconsistency**: Notably, the bootstrap/GC code treats deletion failures as fatal using `.expect("Deletion of expired keys should not fail")`, [9](#0-8)  but the runtime code in `update_certified_timestamp()` only logs failures, indicating inconsistent error handling philosophy.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty program criteria:

**Validator Node Slowdowns & Crashes:**
- Unbounded database growth eventually fills the disk
- RocksDB operations become extremely slow or fail when disk is full
- Node becomes unresponsive to consensus messages
- Requires manual intervention and disk expansion

**Liveness Impact:**
- If multiple validators experience this issue simultaneously during disk space constraints, network liveness can be affected
- Validators may fall behind in consensus rounds
- Could trigger epoch change complications

**Operational Disruption:**
- Requires operator intervention to clean up database manually
- Potential data loss if emergency cleanup is performed incorrectly
- Extended downtime during recovery

**No Monitoring/Detection:**
While quota exceeded events are tracked via metrics, [10](#0-9)  there are no corresponding metrics for `delete_batches` failures, leaving operators blind to the accumulating problem until catastrophic failure.

## Likelihood Explanation

This vulnerability has **HIGH** likelihood of occurrence:

**Frequent Execution Path:** The vulnerable code executes on every block commit (hundreds to thousands of times per day on active validators) as `notify_commit()` is called in the consensus pipeline for each committed block.

**Common Failure Conditions:**
- Disk full is a common operational issue in production systems
- I/O errors occur naturally with hardware aging
- Filesystem issues can arise from configuration or OS updates

**No Special Privileges Required:**
- Can be triggered naturally through disk space exhaustion
- No validator compromise or special access needed
- No cryptographic or consensus protocol attacks required

**Progressive Failure:**
Once the first deletion failure occurs, the problem compounds with each subsequent failure making the situation worse. No self-healing mechanism exists to recover from this state.

## Recommendation

Implement transactional quota management by deferring quota freeing until after successful database deletion:

```rust
pub fn update_certified_timestamp(&self, certified_time: u64) {
    trace!("QS: batch reader updating time {:?}", certified_time);
    self.last_certified_time
        .fetch_max(certified_time, Ordering::SeqCst);

    let expired_batches = self.clear_expired_payload_without_freeing_quota(certified_time);
    
    match self.db.delete_batches(expired_batches.keys()) {
        Ok(_) => {
            // Only free quota after successful deletion
            for batch in expired_batches {
                self.free_quota(batch);
            }
        },
        Err(e) => {
            // Log error and increment metric
            error!("Critical: Failed to delete batches: {:?}", e);
            counters::BATCH_DELETION_FAILED_COUNT.inc();
            // Rollback: re-insert batches into cache
            for (digest, batch) in expired_batches {
                self.db_cache.insert(digest, batch);
            }
        }
    }
}
```

Additionally:
1. Add metrics tracking for `delete_batches` failures
2. Implement quota reconciliation on validator restart
3. Add alerts when deletion failures are detected
4. Consider making deletion failures fatal (matching bootstrap behavior) or implementing retry logic

## Proof of Concept

The vulnerability can be demonstrated through the following scenario:

1. Set up a validator node with limited disk space
2. Monitor the quorum store database size and quota balances
3. Fill disk space to near capacity (e.g., 95% full)
4. Observe block commits triggering `update_certified_timestamp()`
5. Force RocksDB write failures by restricting remaining disk space
6. Monitor logs for "Error deleting batches" debug messages
7. Observe quota balances increasing (quota freed) while database size does not decrease
8. Observe new batches being accepted based on incorrect quota accounting
9. Observe unbounded database growth as old batches accumulate
10. Eventually observe validator node slowdown/crash as disk fills completely

The proof lies in the code structure itself where quota is freed (line 467) before deletion is attempted (line 536), with no recovery mechanism when deletion fails (line 537 only logs).

**Notes**

This vulnerability represents a classic TOCTOU race condition in distributed systems where resource accounting is updated before the actual resource operation is confirmed. The inconsistency between bootstrap code (which panics on deletion failure) and runtime code (which ignores deletion failure) strongly suggests this is an unintended bug rather than a design decision. The lack of metrics for deletion failures and the absence of any recovery mechanism further supports this being a genuine security issue rather than expected behavior.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L41-49)
```rust
pub(crate) struct QuotaManager {
    memory_balance: usize,
    db_balance: usize,
    batch_balance: usize,
    // Recording the provided quotas for asserts.
    memory_quota: usize,
    db_quota: usize,
    batch_quota: usize,
}
```

**File:** consensus/src/quorum_store/batch_store.rs (L64-84)
```rust
    pub(crate) fn update_quota(&mut self, num_bytes: usize) -> anyhow::Result<StorageMode> {
        if self.batch_balance == 0 {
            counters::EXCEEDED_BATCH_QUOTA_COUNT.inc();
            bail!("Batch quota exceeded ");
        }

        if self.db_balance >= num_bytes {
            self.batch_balance -= 1;
            self.db_balance -= num_bytes;

            if self.memory_balance >= num_bytes {
                self.memory_balance -= num_bytes;
                Ok(StorageMode::MemoryAndPersisted)
            } else {
                Ok(StorageMode::PersistedOnly)
            }
        } else {
            counters::EXCEEDED_STORAGE_QUOTA_COUNT.inc();
            bail!("Storage quota exceeded ");
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L97-108)
```rust
    pub(crate) fn free_quota(&mut self, num_bytes: usize, storage_mode: StorageMode) {
        Self::assert_quota(self.batch_balance, 1, self.batch_quota, "Batch");
        self.batch_balance += 1;

        Self::assert_quota(self.db_balance, num_bytes, self.db_quota, "DB");
        self.db_balance += num_bytes;

        if matches!(storage_mode, StorageMode::MemoryAndPersisted) {
            Self::assert_quota(self.memory_balance, num_bytes, self.memory_quota, "Memory");
            self.memory_balance += num_bytes;
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L208-209)
```rust
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
```

**File:** consensus/src/quorum_store/batch_store.rs (L443-472)
```rust
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L93-100)
```rust
    fn delete_batches(&self, digests: Vec<HashValue>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        for digest in digests.iter() {
            trace!("QS: db delete digest {}", digest);
            batch.delete::<BatchSchema>(digest)?;
        }
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L168-170)
```rust
    fn notify_commit(&self, block_timestamp: u64, payloads: Vec<Payload>) {
        self.batch_reader
            .update_certified_timestamp(block_timestamp);
```

**File:** consensus/src/quorum_store/counters.rs (L744-759)
```rust
pub static EXCEEDED_STORAGE_QUOTA_COUNT: Lazy<IntCounter> = Lazy::new(|| {
    register_int_counter!(
        "quorum_store_exceeded_storage_quota_count",
        "Count of the exceeded storage quota."
    )
    .unwrap()
});

/// Count of the exceeded batch quota.
pub static EXCEEDED_BATCH_QUOTA_COUNT: Lazy<IntCounter> = Lazy::new(|| {
    register_int_counter!(
        "quorum_store_exceeded_batch_quota_count",
        "Count of the exceeded batch quota."
    )
    .unwrap()
});
```
