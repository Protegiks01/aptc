# Audit Report

## Title
Validator Transaction Replay Attack After Block Pruning Due to Insufficient Deduplication

## Summary
Validator transactions (DKG transcripts, JWK updates) can be replayed after their containing blocks are pruned from consensus storage. The exclude filter only checks blocks from commit root to parent, missing pruned blocks. While execution-level checks prevent state corruption, replayed transactions waste validator resources and can be used for targeted denial-of-service attacks.

## Finding Description

The vulnerability exists in the validator transaction pool's `pull()` mechanism and the exclude filter construction in proposal generation.

**Root Cause:**

The validator transaction pool keeps transactions until their `TxnGuard` is dropped, which only happens on epoch changes or on-chain state updates. [1](#0-0) 

When pulling transactions for a new proposal, the exclude filter is built from pending blocks: [2](#0-1) 

However, `path_from_commit_root()` only returns blocks between the commit root and parent: [3](#0-2) 

When blocks are committed and the window advances, old blocks are pruned: [4](#0-3) 

**Attack Scenario:**

1. Validator transaction T1 (e.g., JWK update version 5) is added to pool at round 100
2. Block B101 includes T1 and commits, updating on-chain state to version 5
3. T1 remains in pool (TxnGuard held by JWK manager in `ConsensusState::Finished`)
4. Many blocks commit, advancing window root past B101
5. B101 is pruned from block tree
6. At round 500, a new proposal is generated with parent at round 499
7. `pending_blocks` = path from commit_root (round 450) to 499, **does not include B101**
8. T1's hash is **not in the exclude filter**
9. `pool.pull()` returns T1 again
10. T1 is included in the new block proposal

**Execution-Level Protection:**

The replayed transaction fails validation: [5](#0-4) 

Or for DKG transcripts: [6](#0-5) 

Failed transactions are discarded but the block still commits: [7](#0-6) 

## Impact Explanation

This qualifies as **High Severity** under Aptos bug bounty criteria for:

1. **Validator node slowdowns**: Replayed transactions waste CPU executing validation checks and VM processing before being discarded
2. **Significant protocol violations**: Validator transactions should only be included once per epoch

**Resource Waste:**
- Network bandwidth: Replayed transactions in block proposals and gossip
- CPU cycles: Full VM execution including cryptographic verification before discard
- Storage I/O: Reading on-chain state for validation checks

**DoS Potential:**
- An attacker controlling block proposals can include many replayed validator transactions
- Each discarded transaction delays block processing
- Targeted attacks against specific validators by replaying their historical transactions
- Can be sustained throughout an epoch as TxnGuards persist

**Not Critical because:**
- No state corruption (transactions discarded)
- No consensus safety violation (blocks commit successfully)
- No fund loss or theft

## Likelihood Explanation

**Likelihood: High**

This is highly likely to occur naturally or through exploitation:

1. **Natural occurrence**: After sufficient block commits (window advancement), pruning removes historical blocks containing validator transactions
2. **No special privileges required**: Any validator selected as proposer can exploit this
3. **Simple exploitation**: Just wait for pruning, then propose a block - the pool automatically returns replayed transactions
4. **Sustained attack**: Works throughout entire epoch as TxnGuards persist

**Attacker Requirements:**
- Be selected as block proposer (rotates among validators)
- Wait for window advancement to prune target blocks (~few hundred blocks)
- No Byzantine behavior or collusion needed

## Recommendation

**Solution 1: Track executed validator transactions**

Add a per-epoch set tracking validator transaction hashes that have been executed:

```rust
pub struct VTxnPoolState {
    inner: Arc<Mutex<PoolStateInner>>,
    executed_txns: Arc<Mutex<HashSet<HashValue>>>, // Add this
}

impl VTxnPoolState {
    pub fn pull(&self, ..., filter: TransactionFilter) -> Vec<ValidatorTransaction> {
        let executed = self.executed_txns.lock();
        self.inner.lock().pull(deadline, max_items, max_bytes, filter)
            .into_iter()
            .filter(|txn| !executed.contains(&txn.hash()))
            .collect()
    }
    
    pub fn mark_executed(&self, hash: HashValue) {
        self.executed_txns.lock().insert(hash);
    }
}
```

Call `mark_executed()` after successful transaction execution in the executor.

**Solution 2: Enhance exclude filter**

Maintain a persistent cache of validator transaction hashes seen in the current epoch:

```rust
pub struct ProposalGenerator {
    // existing fields...
    seen_validator_txns: Mutex<HashSet<HashValue>>,
}

async fn generate_proposal_inner(&self, ...) {
    // existing code...
    let mut seen = self.seen_validator_txns.lock();
    
    // Add all validator txns from blocks we've seen
    for block in &pending_blocks {
        if let Some(vtxns) = block.validator_txns() {
            seen.extend(vtxns.iter().map(|t| t.hash()));
        }
    }
    
    let validator_txn_filter = vtxn_pool::TransactionFilter::PendingTxnHashSet(seen.clone());
    // ...
}
```

## Proof of Concept

```rust
// Test demonstrating the replay vulnerability
#[tokio::test]
async fn test_validator_transaction_replay_after_pruning() {
    // Setup: Create validator transaction pool and block storage
    let pool = VTxnPoolState::default();
    let vtxn = Arc::new(ValidatorTransaction::dummy(vec![1, 2, 3]));
    
    // Step 1: Add validator transaction to pool
    let guard = pool.put(Topic::DKG, vtxn.clone(), None);
    
    // Step 2: Simulate first pull - transaction returned
    let filter1 = TransactionFilter::empty();
    let pulled1 = pool.pull(
        Instant::now() + Duration::from_secs(1),
        10,
        1000,
        filter1
    );
    assert_eq!(pulled1.len(), 1);
    assert_eq!(pulled1[0].hash(), vtxn.hash());
    
    // Step 3: Simulate block commit and pruning
    // (In real system, the block containing vtxn would be committed,
    //  then pruned when window advances)
    
    // Step 4: Pull again with empty filter (simulating after pruning)
    // The transaction is NOT in exclude filter since containing block was pruned
    let filter2 = TransactionFilter::empty();
    let pulled2 = pool.pull(
        Instant::now() + Duration::from_secs(1),
        10,
        1000,
        filter2
    );
    
    // BUG: Transaction is returned again!
    assert_eq!(pulled2.len(), 1);
    assert_eq!(pulled2[0].hash(), vtxn.hash());
    
    // The guard is still held, so transaction remains in pool
    drop(guard); // Only when dropped is it removed
}
```

**Notes:**
- This PoC demonstrates the core issue: transactions remain in pool and are returned multiple times
- In production, the exclude filter would only include non-pruned blocks, allowing replay
- Execution-level checks would catch this, but only after wasting resources

### Citations

**File:** crates/validator-transaction-pool/src/lib.rs (L56-82)
```rust
    /// Append a txn to the pool.
    /// Return a txn guard that allows you to later delete the txn from the pool.
    pub fn put(
        &self,
        topic: Topic,
        txn: Arc<ValidatorTransaction>,
        pull_notification_tx: Option<aptos_channel::Sender<(), Arc<ValidatorTransaction>>>,
    ) -> TxnGuard {
        let mut pool = self.inner.lock();
        let seq_num = pool.next_seq_num;
        pool.next_seq_num += 1;

        pool.txn_queue.insert(seq_num, PoolItem {
            topic: topic.clone(),
            txn,
            pull_notification_tx,
        });

        if let Some(old_seq_num) = pool.seq_nums_by_topic.insert(topic.clone(), seq_num) {
            pool.txn_queue.remove(&old_seq_num);
        }

        TxnGuard {
            pool: self.inner.clone(),
            seq_num,
        }
    }
```

**File:** consensus/src/liveness/proposal_generator.rs (L643-650)
```rust
        let pending_validator_txn_hashes: HashSet<HashValue> = pending_blocks
            .iter()
            .filter_map(|block| block.validator_txns())
            .flatten()
            .map(ValidatorTransaction::hash)
            .collect();
        let validator_txn_filter =
            vtxn_pool::TransactionFilter::PendingTxnHashSet(pending_validator_txn_hashes);
```

**File:** consensus/src/block_storage/block_tree.rs (L405-434)
```rust
    pub(super) fn find_blocks_to_prune(
        &self,
        next_window_root_id: HashValue,
    ) -> VecDeque<HashValue> {
        // Nothing to do if this is the window root
        if next_window_root_id == self.window_root_id {
            return VecDeque::new();
        }

        let mut blocks_pruned = VecDeque::new();
        let mut blocks_to_be_pruned = vec![self.linkable_window_root()];

        while let Some(block_to_remove) = blocks_to_be_pruned.pop() {
            block_to_remove.executed_block().abort_pipeline();
            // Add the children to the blocks to be pruned (if any), but stop when it reaches the
            // new root
            for child_id in block_to_remove.children() {
                if next_window_root_id == *child_id {
                    continue;
                }
                blocks_to_be_pruned.push(
                    self.get_linkable_block(child_id)
                        .expect("Child must exist in the tree"),
                );
            }
            // Track all the block ids removed
            blocks_pruned.push_back(block_to_remove.id());
        }
        blocks_pruned
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L519-546)
```rust
    pub(super) fn path_from_root_to_block(
        &self,
        block_id: HashValue,
        root_id: HashValue,
        root_round: u64,
    ) -> Option<Vec<Arc<PipelinedBlock>>> {
        let mut res = vec![];
        let mut cur_block_id = block_id;
        loop {
            match self.get_block(&cur_block_id) {
                Some(ref block) if block.round() <= root_round => {
                    break;
                },
                Some(block) => {
                    cur_block_id = block.parent_id();
                    res.push(block);
                },
                None => return None,
            }
        }
        // At this point cur_block.round() <= self.root.round()
        if cur_block_id != root_id {
            return None;
        }
        // Called `.reverse()` to get the chronically increased order.
        res.reverse();
        Some(res)
    }
```

**File:** aptos-move/aptos-vm/src/validator_txns/jwk.rs (L127-130)
```rust
        // Check version.
        if on_chain.version + 1 != observed.version {
            return Err(Expected(IncorrectVersion));
        }
```

**File:** aptos-move/aptos-vm/src/validator_txns/dkg.rs (L99-102)
```rust
        // Check epoch number.
        if dkg_node.metadata.epoch != config_resource.epoch() {
            return Err(Expected(EpochNotCurrent));
        }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L487-497)
```rust
                TransactionStatus::Discard(_) => to_discard.push(
                    transactions[idx].clone(),
                    transaction_outputs[idx].clone(),
                    persisted_auxiliary_infos[idx],
                ),
            }
        }

        transactions.truncate(num_keep_txns);
        transaction_outputs.truncate(num_keep_txns);
        persisted_auxiliary_infos.truncate(num_keep_txns);
```
