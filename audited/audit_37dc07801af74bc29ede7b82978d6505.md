# Audit Report

## Title
Mempool State Divergence Due to Race Condition Between Storage Updates and Transaction Validation

## Summary
Mempool transaction validation relies on querying account sequence numbers from storage's `current_state`, which is updated immediately upon block commit but before mempool notifications are processed. Different validators commit blocks at slightly different times in BFT consensus, creating a race condition window where validators query different sequence numbers for the same account. This causes validators to make inconsistent accept/reject decisions for identical transactions, resulting in divergent mempool states across the network.

## Finding Description

The vulnerability exists at the storage-mempool integration boundary. When validators receive incoming transactions, mempool validates them by checking sequence numbers against storage: [1](#0-0) 

This validation queries `latest_state_checkpoint_view()` which returns the version from storage's in-memory `current_state`: [2](#0-1) [3](#0-2) 

The `current_state` is updated immediately when consensus commits a block via `pre_commit_ledger`: [4](#0-3) 

Specifically at the storage update: [5](#0-4) 

However, mempool is notified of committed transactions **asynchronously and later** through state sync, creating a timing gap.

**Attack Scenario:**

1. **T₀**: Account Alice has sequence number 100 across all validators
2. **T₁**: Validator A commits Block N containing Alice's transaction (seq 100)
   - A's `current_state` updated: Alice seq → 101 (via `pre_commit_ledger`)
   - A's mempool not yet notified
3. **T₁ + δ**: Validator B has not committed Block N yet
   - B's `current_state`: Alice seq = 100
4. **T₁ + δ**: Attacker broadcasts duplicate transaction (Alice, seq 100)
   - **Validator A**: Queries storage → seq 101, checks `100 >= 101` → **REJECTS** (SEQUENCE_NUMBER_TOO_OLD)
   - **Validator B**: Queries storage → seq 100, checks `100 >= 100` → **ACCEPTS** [6](#0-5) 

5. **Result**: Validator B adds transaction to mempool and broadcasts it; Validator A rejects it. Mempool states diverge.

This breaks the implicit invariant that validators should have consistent mempool contents for network efficiency and predictable block proposals.

## Impact Explanation

**Severity: High** (per Aptos bug bounty: validator node slowdowns, significant protocol violations)

**Immediate Impacts:**
1. **Bandwidth Waste**: Invalid transactions are broadcast across the network, consuming bandwidth as validators repeatedly exchange transactions that some have already rejected
2. **Resource Exhaustion**: Validators with stale state accept and store invalid transactions in mempool, wasting memory and CPU
3. **Inconsistent Block Proposals**: Validators with divergent mempools will propose different transaction sets, causing consensus inefficiency
4. **DoS Attack Vector**: Attackers can repeatedly exploit the race window to flood mempools with transactions that should be rejected

**Why Not Critical:**
- Consensus re-validates all transactions before execution, preventing invalid state transitions
- No funds can be stolen or permanently lost
- Network eventually reconciles through consensus

**Why High:**
- Exploitable by any transaction sender without special privileges
- Degrades validator performance through wasted resources
- Violates protocol assumptions about mempool consistency
- Can be weaponized for targeted DoS during high-load periods

## Likelihood Explanation

**Likelihood: Medium-High**

**Normal Operation:**
- In healthy BFT networks, validators commit within milliseconds of each other
- Race window is small but **always present** (microseconds to milliseconds)
- High transaction volume increases exploitation opportunities

**Adverse Conditions:**
- Network partitions extend the window significantly
- Validator performance differences (slow I/O, CPU load) widen the gap
- The security question specifically mentions "caching or replication lag," suggesting scenarios beyond normal timing variances
- If future architectures introduce read replicas or caching layers, this becomes much worse

**Exploitation:**
- Requires no special access or coordination
- Attacker simply monitors blockchain for committed transactions
- Immediately broadcasts duplicate transactions with old sequence numbers
- Success rate depends on network timing but is non-zero in all conditions

## Recommendation

**Immediate Mitigation:**

Add synchronization between storage updates and mempool validation to ensure consistency:

```rust
// Option 1: In process_incoming_transactions, validate against COMMITTED version only
// Get the last COMMITTED version from ledger, not the pre-committed checkpoint
let committed_version = smp.db.get_latest_ledger_info()?.ledger_info().version();
let state_view = smp.db.state_view_at_version(Some(committed_version))?;

// Option 2: Lock mempool validation during commits
// Use the existing pre_commit_lock to prevent validation during state updates
let _guard = smp.db.pre_commit_lock.lock();
let state_view = smp.db.latest_state_checkpoint_view()?;
```

**Long-term Solution:**

1. **Separate validation state from speculative state**: Maintain two version pointers:
   - `speculative_version`: Used for consensus execution (updated in pre_commit)
   - `validated_version`: Used for mempool validation (updated only after mempool notification)

2. **Add mempool notification to commit atomic operation**: Ensure storage update and mempool notification happen atomically or with proper ordering guarantees

3. **Implement version-based validation**: Tag incoming transactions with the validator's current committed version, reject if significantly stale

**Code Fix Example:** [7](#0-6) 

Add mempool validation version update in `commit_ledger` BEFORE releasing the commit lock, ensuring validators only validate against fully committed state.

## Proof of Concept

```rust
// PoC demonstrating the race condition
// This would be added to mempool/src/tests/shared_mempool_test.rs

#[tokio::test]
async fn test_mempool_divergence_on_commit_race() {
    // Setup two validator nodes
    let (mut validator_a, mut validator_b) = setup_two_validators();
    
    // Initial state: Alice has seq 100
    let alice = AccountAddress::random();
    setup_account(&mut validator_a.db, alice, 100);
    setup_account(&mut validator_b.db, alice, 100);
    
    // Transaction with seq 100
    let txn_seq_100 = create_signed_txn(alice, 100);
    
    // Step 1: Validator A commits block with Alice's txn
    let block = vec![txn_seq_100.clone()];
    validator_a.commit_block(block).await;
    // A's storage now has Alice seq = 101
    
    // Step 2: Before B commits, submit duplicate txn with seq 100
    let duplicate_txn = create_signed_txn(alice, 100);
    
    // Step 3: Both validators try to accept the duplicate
    let status_a = validator_a.mempool.submit_transaction(duplicate_txn.clone()).await;
    let status_b = validator_b.mempool.submit_transaction(duplicate_txn.clone()).await;
    
    // Assert divergence
    assert_eq!(status_a.code, MempoolStatusCode::VmError); // A rejects (seq too old)
    assert_eq!(status_b.code, MempoolStatusCode::Accepted); // B accepts
    
    // Verify mempool state differs
    assert!(!validator_a.mempool.contains(&duplicate_txn.hash()));
    assert!(validator_b.mempool.contains(&duplicate_txn.hash()));
    
    println!("Mempool divergence confirmed: A={:?}, B={:?}", 
             validator_a.mempool.len(), 
             validator_b.mempool.len());
}
```

**Notes:**
- This vulnerability is inherent to the asynchronous nature of distributed consensus
- The race window is always present, though typically brief
- Impact is mitigated by consensus re-validation but still causes resource waste and network inefficiency
- Recommended to implement validation against committed-only state or add proper synchronization between storage updates and mempool validation

### Citations

**File:** mempool/src/shared_mempool/tasks.rs (L328-342)
```rust
    let start_storage_read = Instant::now();
    let state_view = smp
        .db
        .latest_state_checkpoint_view()
        .expect("Failed to get latest state checkpoint view.");

    // Track latency: fetching seq number
    let account_seq_numbers = IO_POOL.install(|| {
        transactions
            .par_iter()
            .map(|(t, _, _)| match t.replay_protector() {
                ReplayProtector::Nonce(_) => Ok(None),
                ReplayProtector::SequenceNumber(_) => {
                    get_account_sequence_number(&state_view, t.sender())
                        .map(Some)
```

**File:** mempool/src/shared_mempool/tasks.rs (L358-393)
```rust
    let transactions: Vec<_> = transactions
        .into_iter()
        .enumerate()
        .filter_map(|(idx, (t, ready_time_at_sender, priority))| {
            if let Ok(account_sequence_num) = account_seq_numbers[idx] {
                match account_sequence_num {
                    Some(sequence_num) => {
                        if t.sequence_number() >= sequence_num {
                            return Some((t, Some(sequence_num), ready_time_at_sender, priority));
                        } else {
                            statuses.push((
                                t,
                                (
                                    MempoolStatus::new(MempoolStatusCode::VmError),
                                    Some(DiscardedVMStatus::SEQUENCE_NUMBER_TOO_OLD),
                                ),
                            ));
                        }
                    },
                    None => {
                        return Some((t, None, ready_time_at_sender, priority));
                    },
                }
            } else {
                // Failed to get account's onchain sequence number
                statuses.push((
                    t,
                    (
                        MempoolStatus::new(MempoolStatusCode::VmError),
                        Some(DiscardedVMStatus::RESOURCE_DOES_NOT_EXIST),
                    ),
                ));
            }
            None
        })
        .collect();
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L812-820)
```rust
    fn get_latest_state_checkpoint_version(&self) -> Result<Option<Version>> {
        gauged_api("get_latest_state_checkpoint_version", || {
            Ok(self
                .state_store
                .current_state_locked()
                .last_checkpoint()
                .version())
        })
    }
```

**File:** storage/storage-interface/src/state_store/state_view/db_state_view.rs (L81-90)
```rust
impl LatestDbStateCheckpointView for Arc<dyn DbReader> {
    fn latest_state_checkpoint_view(&self) -> StateViewResult<DbStateView> {
        Ok(DbStateView {
            db: self.clone(),
            version: self
                .get_latest_state_checkpoint_version()
                .map_err(Into::<StateViewError>::into)?,
            maybe_verify_against_state_root_hash: None,
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L44-75)
```rust
    fn pre_commit_ledger(&self, chunk: ChunkToCommit, sync_commit: bool) -> Result<()> {
        gauged_api("pre_commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .pre_commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["pre_commit_ledger"]);

            chunk
                .state_summary
                .latest()
                .global_state_summary
                .log_generation("db_save");

            self.pre_commit_validation(&chunk)?;
            let _new_root_hash =
                self.calculate_and_commit_ledger_and_state_kv(&chunk, self.skip_index_and_usage)?;

            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions__others"]);

            self.state_store.buffered_state().lock().update(
                chunk.result_ledger_state_with_summary(),
                chunk.estimated_total_state_updates(),
                sync_commit || chunk.is_reconfig,
            )?;

            Ok(())
        })
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L78-112)
```rust
    fn commit_ledger(
        &self,
        version: Version,
        ledger_info_with_sigs: Option<&LedgerInfoWithSignatures>,
        chunk_opt: Option<ChunkToCommit>,
    ) -> Result<()> {
        gauged_api("commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_ledger"]);

            let old_committed_ver = self.get_and_check_commit_range(version)?;

            let mut ledger_batch = SchemaBatch::new();
            // Write down LedgerInfo if provided.
            if let Some(li) = ledger_info_with_sigs {
                self.check_and_put_ledger_info(version, li, &mut ledger_batch)?;
            }
            // Write down commit progress
            ledger_batch.put::<DbMetadataSchema>(
                &DbMetadataKey::OverallCommitProgress,
                &DbMetadataValue::Version(version),
            )?;
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;

            // Notify the pruners, invoke the indexer, and update in-memory ledger info.
            self.post_commit(old_committed_ver, version, ledger_info_with_sigs, chunk_opt)
        })
    }
```

**File:** storage/aptosdb/src/state_store/buffered_state.rs (L155-179)
```rust
    /// This method updates the buffered state with new data.
    pub fn update(
        &mut self,
        new_state: LedgerStateWithSummary,
        estimated_new_items: usize,
        sync_commit: bool,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["buffered_state___update"]);

        let old_state = self.current_state_locked().clone();
        assert!(new_state.is_descendant_of(&old_state));

        self.estimated_items += estimated_new_items;
        let version = new_state.last_checkpoint().version();

        let last_checkpoint = new_state.last_checkpoint().clone();
        // Commit state only if there is a new checkpoint, eases testing and make estimated
        // buffer size a tad more realistic.
        let checkpoint_to_commit_opt =
            (old_state.next_version() < last_checkpoint.next_version()).then_some(last_checkpoint);
        *self.current_state_locked() = new_state;
        self.maybe_commit(checkpoint_to_commit_opt, sync_commit);
        Self::report_last_checkpoint_version(version);
        Ok(())
    }
```
