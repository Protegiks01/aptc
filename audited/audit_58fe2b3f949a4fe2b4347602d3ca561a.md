# Audit Report

## Title
Resource Leak in Indexer gRPC Service Due to Unset Abort Handle on Connection Drops

## Summary
The indexer gRPC fullnode service contains a resource leak vulnerability where connection handler tasks can hang indefinitely when clients disconnect while waiting for future transactions. The abort mechanism exists but is never activated when connections drop, leading to accumulation of orphaned tasks that hold `Arc<Context>` references and other resources.

## Finding Description

The `FullnodeDataService` implements an abort mechanism through `abort_handle: Arc<AtomicBool>` [1](#0-0) , which is passed to the `IndexerStreamCoordinator` [2](#0-1)  and checked in critical wait loops [3](#0-2) .

However, this abort handle is **never set to true** anywhere in the codebase when connections are dropped. When a gRPC connection is established, a tokio task is spawned [4](#0-3)  that processes transactions and sends them through a channel back to the client.

**The vulnerability occurs when:**

1. A client requests transactions starting from a version beyond the current ledger version (future transactions)
2. The spawned task enters `ensure_highest_known_version()` which waits in a loop for new transactions to arrive [5](#0-4) 
3. The client disconnects, closing the channel receiver
4. The task continues waiting because it only checks the abort handle (which is never set) and doesn't actively try to send to detect channel closure
5. The task holds `Arc<Context>` [6](#0-5)  and wakes every 100ms to check for transactions [7](#0-6) 

The `Context` structure contains heavy resources including `Arc<dyn DbReader>`, mempool sender, node config, and various caches [8](#0-7) .

**Attack scenario:**
```
1. Attacker opens N connections to the gRPC indexer endpoint
2. Each requests transactions from version (current_ledger_version + 1000000)
3. Immediately closes all connections
4. N tasks spawn and wait in ensure_highest_known_version()
5. Tasks never detect disconnection (abort_handle never set)
6. Each task holds Arc<Context> + timer resources
7. Memory and task pool exhaustion occurs
```

## Impact Explanation

This qualifies as **Medium severity** under the Aptos bug bounty program for the following reasons:

1. **Resource Exhaustion**: The vulnerability allows an unprivileged attacker to cause memory and tokio task pool exhaustion on fullnodes running the indexer gRPC service
2. **Service Degradation**: Accumulated orphaned tasks can degrade or crash the indexer service, falling under "API crashes" (High severity) or requiring manual intervention (Medium severity)
3. **Limited Scope**: Does NOT affect consensus, validator operations, or blockchain correctness - only impacts the auxiliary indexer service

While HTTP/2 keepalive timeouts provide some mitigation (detecting disconnections after ~65 seconds) [9](#0-8) , tasks can still hang for extended periods until transactions arrive, and the keepalive mechanism doesn't immediately signal the waiting task.

## Likelihood Explanation

**High likelihood** - The attack is trivial to execute:
- No authentication required to connect to public indexer endpoints
- Simple gRPC client can request future transactions and disconnect
- Triggering condition requires only specifying a `starting_version` beyond current ledger [10](#0-9) 
- Can be automated to continuously spawn hanging tasks

The bug will trigger in any deployment where:
- The indexer gRPC service is enabled [11](#0-10) 
- Clients can request arbitrary starting versions
- No rate limiting on connection establishment

## Recommendation

**Fix the abort handle mechanism to detect channel closure:**

```rust
// In fullnode_data_service.rs, spawn a monitoring task
let abort_handle = self.abort_handle.clone();
let tx_monitor = tx.clone();
tokio::spawn(async move {
    tx_monitor.closed().await;
    abort_handle.store(true, Ordering::SeqCst);
});
```

This spawns a lightweight task that waits for the channel receiver to be dropped, then sets the abort handle to signal all waiting loops to exit.

**Alternative approach - Check channel closure in the wait loop:**

```rust
// In stream_coordinator.rs ensure_highest_known_version
while self.highest_known_version == 0 || self.current_version > self.highest_known_version {
    // Check if channel is closed
    if self.transactions_sender.is_closed() {
        return false;
    }
    // ... rest of logic
}
```

**Additional hardening:**
- Add maximum wait time in `ensure_highest_known_version` (e.g., 5 minutes)
- Add metrics to track orphaned tasks
- Consider requiring authentication for indexer gRPC endpoints

## Proof of Concept

```rust
// PoC demonstrating the vulnerability
// This would be a Rust integration test

#[tokio::test]
async fn test_connection_drop_resource_leak() {
    // Setup: Start indexer gRPC service
    let (service, context) = setup_test_service();
    let current_version = context.get_latest_ledger_info().unwrap().version();
    
    // Attack: Connect and request future transactions
    let future_version = current_version + 1_000_000;
    let mut client = connect_grpc_client().await;
    
    // Request transactions from far future
    let request = GetTransactionsFromNodeRequest {
        starting_version: Some(future_version),
        transactions_count: None, // Will set ending_version to u64::MAX
    };
    
    let mut stream = client.get_transactions_from_node(request).await.unwrap();
    
    // Immediately drop the stream (disconnect)
    drop(stream);
    drop(client);
    
    // Verify: Task is still waiting (holding Arc<Context>)
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // Check that Arc reference count is still elevated
    // (This demonstrates the resource leak)
    let initial_count = Arc::strong_count(&service.service_context.context);
    
    // Repeat attack N times
    for _ in 0..10 {
        let mut client = connect_grpc_client().await;
        let request = GetTransactionsFromNodeRequest {
            starting_version: Some(future_version),
            transactions_count: None,
        };
        let stream = client.get_transactions_from_node(request).await.unwrap();
        drop(stream);
        drop(client);
    }
    
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // Arc count should have increased by 10 (one per leaked task)
    let leaked_count = Arc::strong_count(&service.service_context.context);
    assert!(leaked_count > initial_count + 5, 
        "Resource leak detected: {} tasks still holding Arc<Context>", 
        leaked_count - initial_count);
}
```

**Notes:**

The vulnerability is confirmed by examining the code flow where the abort handle is created but never set to true when channels close. The `ServiceContext` itself is not cloned per connection (it's wrapped in an `Arc` by tonic [12](#0-11) ), but the `Arc<Context>` inside it is cloned for each spawned task [13](#0-12) , leading to the resource leak when tasks hang.

There are no circular references in the `Context` structure itself, so the original question's concern about "Arc<Context> reference cycles" is unfounded - the issue is purely a resource leak from hanging tasks, not reference cycles.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L38-38)
```rust
    pub abort_handle: Arc<AtomicBool>,
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L73-77)
```rust
        let starting_version = match r.starting_version {
            Some(version) => version,
            // Live mode unavailable for FullnodeDataService
            // Enable use_data_service_interface in config to use LocalnetDataService instead
            None => return Err(Status::invalid_argument("Starting version must be set")),
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L90-90)
```rust
        let context = self.service_context.context.clone();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L101-117)
```rust
        tokio::spawn(async move {
            // Initialize the coordinator that tracks starting version and processes transactions
            let mut coordinator = IndexerStreamCoordinator::new(
                context,
                starting_version,
                ending_version,
                processor_task_count,
                processor_batch_size,
                output_batch_size,
                tx.clone(),
                // For now the request for this interface doesn't include a txn filter
                // because it is only used for the txn stream filestore worker, which
                // needs every transaction. Later we may add support for txn filtering
                // to this interface too.
                None,
                Some(abort_handle.clone()),
            );
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L52-52)
```rust
    pub context: Arc<Context>,
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L55-55)
```rust
    abort_handle: Option<Arc<AtomicBool>>,
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L550-579)
```rust
    async fn ensure_highest_known_version(&mut self) -> bool {
        let mut empty_loops = 0;
        while self.highest_known_version == 0 || self.current_version > self.highest_known_version {
            if let Some(abort_handle) = self.abort_handle.as_ref() {
                if abort_handle.load(Ordering::SeqCst) {
                    return false;
                }
            }
            if empty_loops > 0 {
                tokio::time::sleep(Duration::from_millis(RETRY_TIME_MILLIS)).await;
            }
            empty_loops += 1;
            if let Err(err) = self.set_highest_known_version() {
                error!(
                    error = format!("{:?}", err),
                    "[Indexer Fullnode] Failed to set highest known version"
                );
                continue;
            } else {
                sample!(
                    SampleRate::Frequency(10),
                    info!(
                        highest_known_version = self.highest_known_version,
                        "[Indexer Fullnode] Found new highest known version",
                    )
                );
            }
        }
        true
    }
```

**File:** api/src/context.rs (L72-85)
```rust
#[derive(Clone)]
pub struct Context {
    chain_id: ChainId,
    pub db: Arc<dyn DbReader>,
    mp_sender: MempoolClientSender,
    pub node_config: Arc<NodeConfig>,
    gas_schedule_cache: Arc<RwLock<GasScheduleCache>>,
    gas_estimation_cache: Arc<RwLock<GasEstimationCache>>,
    gas_limit_cache: Arc<RwLock<GasLimitCache>>,
    view_function_stats: Arc<FunctionStats>,
    simulate_txn_stats: Arc<FunctionStats>,
    pub indexer_reader: Option<Arc<dyn IndexerReader>>,
    pub wait_for_hash_active_connections: Arc<AtomicUsize>,
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/runtime.rs (L44-46)
```rust
    if !config.indexer_grpc.enabled {
        return None;
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/runtime.rs (L102-103)
```rust
            .http2_keepalive_interval(Some(std::time::Duration::from_secs(60)))
            .http2_keepalive_timeout(Some(std::time::Duration::from_secs(5)))
```

**File:** protos/rust/src/pb/aptos.internal.fullnode.v1.tonic.rs (L189-190)
```rust
    pub struct FullnodeDataServer<T: FullnodeData> {
        inner: Arc<T>,
```
