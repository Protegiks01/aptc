# Audit Report

## Title
Missing Abort Handle Cleanup in RoundState Drop Leading to Stale Timeout Tasks and Potential Consensus Liveness Issues

## Summary
The `RoundState` struct lacks a `Drop` implementation to properly abort pending timeout tasks when it is destroyed during epoch transitions. This allows scheduled timeout tasks from previous epochs to continue executing, potentially sending spurious timeout messages that could disrupt consensus liveness if round numbers coincide between epochs.

## Finding Description

The `RoundState` struct in the consensus layer manages round timeouts by scheduling asynchronous tasks that fire after a delay. [1](#0-0)  The `abort_handle` field stores an `AbortHandle` that can cancel the scheduled timeout task.

When transitioning between rounds within the same epoch, the code correctly aborts old timeout tasks: [2](#0-1)  However, when an epoch change occurs and the `RoundState` is dropped as part of shutting down the old `RoundManager`, there is no `Drop` implementation to abort the pending timeout task. [3](#0-2) 

According to Rust's `futures` library semantics, dropping an `AbortHandle` without calling `.abort()` does **not** abort the associated task—the task continues to run. The scheduled timeout task will eventually fire and execute its `SendTask::run()` method: [4](#0-3) 

This causes the task to send a round number through the shared `timeout_sender` channel, which persists across epochs: [5](#0-4) 

The `EpochManager`'s main event loop receives these timeout messages and forwards them to the current `RoundManager`: [6](#0-5) [7](#0-6) 

**Critical Race Condition**: Round numbers reset to 0 at the start of each new epoch. If the old epoch had a pending timeout for round N, and the new epoch advances to round N before the stale timeout fires, the `RoundManager` will incorrectly process it as a legitimate timeout: [8](#0-7) 

The `round_state.process_local_timeout(round)` check will succeed if the round numbers match: [9](#0-8) 

This breaks the **Resource Limits** invariant (proper cleanup of scheduled tasks) and can trigger the **Consensus Liveness** issues if validators prematurely timeout in the new epoch.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty criteria:

1. **Resource Leak**: Every epoch transition leaves orphaned timeout tasks that consume memory and CPU until they fire. With frequent epoch changes or long timeout durations, this accumulates system resources.

2. **State Inconsistencies**: Stale timeout messages can cause validators to process incorrect timeout events, potentially leading to premature round advancement or spurious timeout broadcasts that require operator intervention to diagnose.

3. **Consensus Liveness Impact**: If multiple validators experience the race condition simultaneously (matching round numbers between epochs), it could cause coordinated spurious timeouts that disrupt consensus progress, though this would not cause safety violations or permanent liveness loss.

The issue does not reach **High** or **Critical** severity because:
- It does not cause consensus safety violations (no double-spending or chain splits)
- It does not cause permanent liveness loss (validators can recover)
- It does not allow theft or minting of funds
- The race condition requires specific timing (round number collision)

## Likelihood Explanation

This issue has **Medium-to-High likelihood** of occurring in production:

**Guaranteed to occur:**
- Resource leaks happen on every epoch transition when a pending timeout exists
- Stale timeout messages are sent with certainty after epoch changes

**Race condition likelihood:**
- Epoch changes occur regularly (validator set updates, governance changes)
- Round numbers reset to 0 each epoch, making collisions probable
- Timeout tasks are typically scheduled seconds to minutes in advance
- If an epoch ends at round 100 with a timeout scheduled, and the new epoch reaches round 100 before the old timeout fires (within the timeout window), the race triggers
- This is especially likely during quick epoch transitions or network disruptions

The vulnerability requires no attacker interaction—it occurs naturally during normal network operations.

## Recommendation

Implement a `Drop` trait for `RoundState` that explicitly aborts any pending timeout task:

```rust
impl Drop for RoundState {
    fn drop(&mut self) {
        if let Some(handle) = self.abort_handle.take() {
            handle.abort();
        }
    }
}
```

Add this implementation in `consensus/src/liveness/round_state.rs` after the struct definition. This ensures that when a `RoundState` is dropped (during epoch transitions or any other cleanup), the pending timeout task is properly cancelled.

**Alternative consideration**: If there are concerns about explicit Drop implementations affecting performance or behavior, consider adding explicit cleanup in the `EpochManager::shutdown_current_processor()` method to call a cleanup function on the RoundState before dropping the RoundManager.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use crate::util::time_service::ClockTimeService;
    use std::sync::{Arc, atomic::{AtomicBool, Ordering}};
    use std::time::Duration;
    use tokio::runtime::Runtime;

    #[test]
    fn test_abort_handle_not_aborted_on_drop() {
        let rt = Runtime::new().unwrap();
        rt.block_on(async {
            let time_service = Arc::new(ClockTimeService::new(
                tokio::runtime::Handle::current()
            ));
            let (timeout_sender, mut timeout_receiver) = 
                aptos_channels::new(10, &counters::PENDING_ROUND_TIMEOUTS);
            
            let timeout_fired = Arc::new(AtomicBool::new(false));
            let timeout_fired_clone = timeout_fired.clone();
            
            // Create a RoundState with a timeout
            let mut round_state = RoundState::new(
                Box::new(ExponentialTimeInterval::fixed(Duration::from_millis(100))),
                time_service.clone(),
                timeout_sender,
            );
            
            // Process certificates to start round 1 and schedule timeout
            let sync_info = create_sync_info_with_qc(0); // Helper function
            let verifier = create_test_verifier(); // Helper function
            round_state.process_certificates(sync_info, &verifier);
            
            // Spawn a task to check if timeout fires
            tokio::spawn(async move {
                if let Some(_round) = timeout_receiver.select_next_some().await {
                    timeout_fired_clone.store(true, Ordering::SeqCst);
                }
            });
            
            // Drop the RoundState (simulating epoch change)
            drop(round_state);
            
            // Wait longer than the timeout duration
            tokio::time::sleep(Duration::from_millis(200)).await;
            
            // BUG: The timeout task should have been aborted, but it fires anyway
            assert!(timeout_fired.load(Ordering::SeqCst), 
                "Timeout task should NOT have fired after RoundState was dropped, \
                 but it did because abort_handle was not explicitly aborted");
        });
    }
    
    #[test]
    fn test_round_number_collision_across_epochs() {
        let rt = Runtime::new().unwrap();
        rt.block_on(async {
            let time_service = Arc::new(ClockTimeService::new(
                tokio::runtime::Handle::current()
            ));
            let (timeout_sender, mut timeout_receiver) = 
                aptos_channels::new(10, &counters::PENDING_ROUND_TIMEOUTS);
            
            // Epoch 1: Create RoundState and advance to round 50
            let mut round_state_epoch1 = RoundState::new(
                Box::new(ExponentialTimeInterval::fixed(Duration::from_millis(100))),
                time_service.clone(),
                timeout_sender.clone(),
            );
            
            // Advance to round 50 and schedule timeout
            for i in 0..50 {
                let sync_info = create_sync_info_with_qc(i);
                let verifier = create_test_verifier();
                round_state_epoch1.process_certificates(sync_info, &verifier);
            }
            
            // Simulate epoch change: drop epoch 1 RoundState
            drop(round_state_epoch1);
            
            // Epoch 2: Create new RoundState (rounds reset to 0)
            let mut round_state_epoch2 = RoundState::new(
                Box::new(ExponentialTimeInterval::fixed(Duration::from_millis(100))),
                time_service.clone(),
                timeout_sender.clone(),
            );
            
            // Quickly advance to round 50 in new epoch
            for i in 0..50 {
                let sync_info = create_sync_info_with_qc(i);
                let verifier = create_test_verifier();
                round_state_epoch2.process_certificates(sync_info, &verifier);
            }
            
            // Wait for old timeout to fire
            tokio::time::sleep(Duration::from_millis(150)).await;
            
            // BUG: The old timeout for round 50 fires and could be processed
            // by the new epoch's RoundManager if it's at round 50
            if let Ok(Some(round)) = 
                tokio::time::timeout(Duration::from_millis(10), 
                    timeout_receiver.select_next_some()).await {
                panic!("Received stale timeout for round {} from previous epoch! \
                       This could disrupt consensus in the new epoch.", round);
            }
        });
    }
}
```

**Notes**

The vulnerability is confirmed by examining the code flow:
1. During normal epoch transitions via `shutdown_current_processor()`, the `RoundManager` is dropped when it exits its event loop [10](#0-9) [11](#0-10) 

2. The `RoundManager` owns the `RoundState` as a field [12](#0-11) , so dropping `RoundManager` drops `RoundState`

3. The timeout task is created via `time_service.run_after()` which spawns an `Abortable` future [13](#0-12) 

4. The shared `timeout_sender` channel persists across epochs in `EpochManager` and is reused when creating new `RoundState` instances [14](#0-13) 

This represents a clear violation of proper resource management and can lead to observable consensus liveness issues in production deployments.

### Citations

**File:** consensus/src/liveness/round_state.rs (L140-166)
```rust
pub struct RoundState {
    // Determines the time interval for a round given the number of non-ordered rounds since
    // last ordering.
    time_interval: Box<dyn RoundTimeInterval>,
    // Highest known ordered round as reported by the caller. The caller might choose not to
    // inform the RoundState about certain ordered rounds (e.g., NIL blocks): in this case the
    // ordered round in RoundState might lag behind the ordered round of a block tree.
    highest_ordered_round: Round,
    // Current round is max{highest_qc, highest_tc} + 1.
    current_round: Round,
    // The deadline for the next local timeout event. It is reset every time a new round start, or
    // a previous deadline expires.
    // Represents as Duration since UNIX_EPOCH.
    current_round_deadline: Duration,
    // Service for timer
    time_service: Arc<dyn TimeService>,
    // To send local timeout events to the subscriber (e.g., SMR)
    timeout_sender: aptos_channels::Sender<Round>,
    // Votes received for the current round.
    pending_votes: PendingVotes,
    // Vote sent locally for the current round.
    vote_sent: Option<Vote>,
    // Timeout sent locally for the current round.
    timeout_sent: Option<RoundTimeout>,
    // The handle to cancel previous timeout task when moving to next round.
    abort_handle: Option<AbortHandle>,
}
```

**File:** consensus/src/liveness/round_state.rs (L233-241)
```rust
    pub fn process_local_timeout(&mut self, round: Round) -> bool {
        if round != self.current_round {
            return false;
        }
        warn!(round = round, "Local timeout");
        counters::TIMEOUT_COUNT.inc();
        self.setup_timeout(1);
        true
    }
```

**File:** consensus/src/liveness/round_state.rs (L347-352)
```rust
        let abort_handle = self
            .time_service
            .run_after(timeout, SendTask::make(timeout_sender, self.current_round));
        if let Some(handle) = self.abort_handle.replace(abort_handle) {
            handle.abort();
        }
```

**File:** consensus/src/util/time_service.rs (L81-96)
```rust
    fn run(&mut self) -> Pin<Box<dyn Future<Output = ()> + Send>> {
        let mut sender = self
            .sender
            .take()
            .expect("Expect to be able to take sender");
        let message = self
            .message
            .take()
            .expect("Expect to be able to take message");
        let r = async move {
            if let Err(e) = sender.send(message).await {
                error!("Error on send: {:?}", e);
            };
        };
        r.boxed()
    }
```

**File:** consensus/src/util/time_service.rs (L114-124)
```rust
    fn run_after(&self, timeout: Duration, mut t: Box<dyn ScheduledTask>) -> AbortHandle {
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        let task = Abortable::new(
            async move {
                sleep(timeout).await;
                t.run().await;
            },
            abort_registration,
        );
        self.executor.spawn(task);
        abort_handle
```

**File:** consensus/src/epoch_manager.rs (L140-140)
```rust
    timeout_sender: aptos_channels::Sender<Round>,
```

**File:** consensus/src/epoch_manager.rs (L273-284)
```rust
    fn create_round_state(
        &self,
        time_service: Arc<dyn TimeService>,
        timeout_sender: aptos_channels::Sender<Round>,
    ) -> RoundState {
        let time_interval = Box::new(ExponentialTimeInterval::new(
            Duration::from_millis(self.config.round_initial_timeout_ms),
            self.config.round_timeout_backoff_exponent_base,
            self.config.round_timeout_backoff_max_exponent,
        ));
        RoundState::new(time_interval, time_service, timeout_sender)
    }
```

**File:** consensus/src/epoch_manager.rs (L637-647)
```rust
    async fn shutdown_current_processor(&mut self) {
        if let Some(close_tx) = self.round_manager_close_tx.take() {
            // Release the previous RoundManager, especially the SafetyRule client
            let (ack_tx, ack_rx) = oneshot::channel();
            close_tx
                .send(ack_tx)
                .expect("[EpochManager] Fail to drop round manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop round manager");
        }
```

**File:** consensus/src/epoch_manager.rs (L1896-1909)
```rust
    fn process_local_timeout(&mut self, round: u64) {
        let Some(sender) = self.round_manager_tx.as_mut() else {
            warn!(
                "Received local timeout for round {} without Round Manager",
                round
            );
            return;
        };

        let peer_id = self.author;
        let event = VerifiedEvent::LocalTimeout(round);
        if let Err(e) = sender.push((peer_id, discriminant(&event)), (peer_id, event)) {
            error!("Failed to send event to round manager {:?}", e);
        }
```

**File:** consensus/src/epoch_manager.rs (L1949-1951)
```rust
                round = round_timeout_sender_rx.select_next_some() => {
                    monitor!("epoch_manager_process_round_timeout",
                    self.process_local_timeout(round));
```

**File:** consensus/src/round_manager.rs (L306-306)
```rust
    round_state: RoundState,
```

**File:** consensus/src/round_manager.rs (L993-996)
```rust
    pub async fn process_local_timeout(&mut self, round: Round) -> anyhow::Result<()> {
        if !self.round_state.process_local_timeout(round) {
            return Ok(());
        }
```

**File:** consensus/src/round_manager.rs (L2076-2080)
```rust
                close_req = close_rx.select_next_some() => {
                    if let Ok(ack_sender) = close_req {
                        ack_sender.send(()).expect("[RoundManager] Fail to ack shutdown");
                    }
                    break;
```
