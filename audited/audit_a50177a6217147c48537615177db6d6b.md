Based on my thorough validation of the codebase, this vulnerability claim is **VALID**. Here is the complete audit report:

# Audit Report

## Title
Silent Message Delivery Failures in NetworkMessageService Hide Execution Errors from Remote Executor Coordinators

## Summary
The `NetworkMessageService` trait implementation in `GRPCNetworkMessageServiceServerWrapper` always returns success (`Ok`) even when execution commands are not delivered to registered handlers, causing coordinators to indefinitely wait for results that will never arrive. This error suppression violates the contract between coordinators and executor shards, leading to liveness failures in sharded block execution.

## Finding Description

The `NetworkMessageService::simple_msg_exchange` implementation has critical error handling flaws that hide execution failures from coordinators. [1](#0-0) 

**Issue 1: Silent Message Drops**

When no handler is registered for a message type, the implementation logs an error but returns `Ok(Response::new(Empty {}))` on line 114. The coordinator receives a success response indicating the message was delivered, when in fact it was silently dropped. [2](#0-1) 

**Issue 2: Panic on Channel Failures**

The `.unwrap()` call on line 107 will panic if the channel's receiver is disconnected, crashing the entire gRPC server. [3](#0-2) 

**Exploitation Path:**

1. Coordinator sends `ExecuteBlockCommand` to executor shard via the remote executor client: [4](#0-3) 

2. The message is sent through the network controller's outbound handler: [5](#0-4) 

3. The gRPC client calls `simple_msg_exchange`: [6](#0-5) 

4. The server's `simple_msg_exchange` receives the message but either silently drops it (no handler) or panics (disconnected channel).

5. Coordinator waits for results that never arrive: [7](#0-6) 

The coordinator blocks indefinitely on `rx.recv().unwrap()` at line 167, causing a complete liveness failure for that shard's block execution.

## Impact Explanation

This vulnerability meets **Medium Severity** criteria per the Aptos bug bounty program:

- **State inconsistencies requiring intervention**: The coordinator believes commands were delivered successfully while shards never received them, creating inconsistent execution states that require manual intervention to recover.

- **Liveness failures**: Block execution on affected shards hangs indefinitely when commands are silently dropped, requiring process restart.

While this primarily affects the remote executor feature (used for sharded execution), it violates the critical invariant that execution commands must be reliably delivered between coordinators and executor shards. Silent failures undermine the entire remote execution architecture.

The vulnerability can occur naturally during:
- Shutdown sequences (handlers dropped before server stops) [8](#0-7) 
- Message type mismatches between sender and receiver
- Channel disconnections during handler cleanup

## Likelihood Explanation

**Moderate Likelihood** in production deployments using remote executor:

1. **Shutdown Race Conditions**: The network controller's shutdown is explicitly noted as "not very clean" with handlers potentially dropped before the server fully stops accepting messages. [9](#0-8) 

2. **Message Type Mismatches**: Any mismatch in message type strings between sender and receiver causes silent drops with success responses.

3. **Channel Disconnections**: Handler channels can be disconnected during cleanup operations, causing panics.

The remote executor feature is optional and controlled by configuration: [10](#0-9) 

While external exploitation requires network access to the gRPC endpoints (which may be internal-only in production), the vulnerability can trigger naturally during normal operations, making it a reliability issue even without malicious actors.

## Recommendation

1. **Return errors instead of success for unhandled messages**:
```rust
if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
    handler.send(msg).map_err(|e| Status::internal(format!("Failed to send message: {}", e)))?;
} else {
    return Err(Status::not_found(format!("No handler registered for message type: {:?}", message_type)));
}
Ok(Response::new(Empty {}))
```

2. **Handle channel send failures gracefully**:
```rust
handler.send(msg).map_err(|e| Status::internal(format!("Channel disconnected: {}", e)))?;
```

3. **Implement proper shutdown synchronization** to ensure the gRPC server stops accepting messages before handlers are dropped.

4. **Add timeout mechanisms** on the coordinator side to detect hung operations and implement retry logic.

## Proof of Concept

A proof of concept would involve:
1. Starting a remote executor service
2. Initiating shutdown of the executor service while messages are in flight
3. Observing the coordinator blocking indefinitely on `rx.recv().unwrap()`

The exact PoC requires a multi-process test setup with the remote executor feature enabled, which is beyond inline demonstration but can be triggered in the execution-benchmark tool with the `--remote-executor-addresses` flag during shutdown operations.

### Citations

**File:** secure/net/src/grpc_network_service/mod.rs (L93-115)
```rust
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L140-160)
```rust
    pub async fn send_message(
        &mut self,
        sender_addr: SocketAddr,
        message: Message,
        mt: &MessageType,
    ) {
        let request = tonic::Request::new(NetworkMessage {
            message: message.data,
            message_type: mt.get_type(),
        });
        // TODO: Retry with exponential backoff on failures
        match self.remote_channel.simple_msg_exchange(request).await {
            Ok(_) => {},
            Err(e) => {
                panic!(
                    "Error '{}' sending message to {} on node {:?}",
                    e, self.remote_addr, sender_addr
                );
            },
        }
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L180-212)
```rust
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        trace!("RemoteExecutorClient Sending block to shards");
        self.state_view_service.set_state_view(state_view);
        let (sub_blocks, global_txns) = transactions.into();
        if !global_txns.is_empty() {
            panic!("Global transactions are not supported yet");
        }
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }

        let execution_results = self.get_output_from_shards()?;

        self.state_view_service.drop_state_view();
        Ok(ShardedExecutionOutput::new(execution_results, vec![]))
    }
```

**File:** secure/net/src/network_controller/outbound_handler.rs (L155-160)
```rust
                grpc_clients
                    .get_mut(remote_addr)
                    .unwrap()
                    .send_message(*socket_addr, msg, message_type)
                    .await;
            }
```

**File:** secure/net/src/network_controller/mod.rs (L152-166)
```rust
    // TODO: This is still not a very clean shutdown. We don't wait for the full shutdown after
    //       sending the signal. May not matter much for now because we shutdown before exiting the
    //       process. Ideally, we want to fix this.
    pub fn shutdown(&mut self) {
        info!("Shutting down network controller at {}", self.listen_addr);
        if let Some(shutdown_signal) = self.inbound_server_shutdown_tx.take() {
            shutdown_signal.send(()).unwrap();
        }

        if let Some(shutdown_signal) = self.outbound_task_shutdown_tx.take() {
            shutdown_signal.send(Message::new(vec![])).unwrap_or_else(|_| {
                warn!("Failed to send shutdown signal to outbound task; probably already shutdown");
            })
        }
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L261-275)
```rust
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
```
