# Audit Report

## Title
Validator Node Panic on Missing Consensus Key During Epoch Transitions

## Summary
A critical error handling flaw in the consensus epoch manager causes validator nodes to panic and crash when consensus keys are missing from secure storage during epoch transitions. This occurs due to an unconditional panic when `load_consensus_key()` fails, which can happen during consensus key rotation if the on-chain key update and local storage update are not properly synchronized. [1](#0-0) 

## Finding Description

The vulnerability exists in the epoch transition flow where the `EpochManager` loads consensus keys from secure storage. The critical code path is:

1. **Storage Layer**: The `OnDiskStorage::get()` method returns `Error::KeyNotSet` when a requested key doesn't exist in storage. [2](#0-1) 

2. **Error Conversion**: This error propagates through `PersistentSafetyStorage::consensus_sk_by_pk()`, which attempts to load both an explicit key (for rotated keys) and the default consensus key. If both are missing or don't match the expected public key, it returns `Error::ValidatorKeyNotFound`. [3](#0-2) 

3. **Panic Trigger**: The `EpochManager::load_consensus_key()` method converts these errors to `anyhow::Result`, which is then handled by `start_new_epoch()` with an unconditional panic. [4](#0-3) 

The `start_new_epoch()` method is called automatically during epoch transitions via the reconfiguration notification listener: [5](#0-4) 

**Attack Scenario - Consensus Key Rotation Timing Issue:**

1. Validator operator calls `rotate_consensus_key()` on-chain to update their consensus public key: [6](#0-5) 

2. The on-chain key rotation takes effect at the next epoch boundary (automatic)

3. **Critical Window**: If the operator has not updated their local node configuration with the new private key and restarted the node, the storage will not contain the new key

4. When the epoch changes, `start_new_epoch()` is triggered, which calls `load_consensus_key()` with the new validator verifier containing the updated public key

5. The key lookup fails because:
   - The explicit key `consensus_{new_pk_hex}` doesn't exist in storage
   - The default consensus key exists but doesn't match the new expected public key
   - `consensus_sk_by_pk()` returns `Error::SecureStorageMissingDataError` [7](#0-6) 

6. The error is converted to `anyhow::Error` and the node **panics**, crashing the entire validator

The error conversion confirms this propagation path: [8](#0-7) 

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:

- **Validator node crashes**: Complete validator node unavailability requiring manual intervention
- **API crashes**: Node is completely down, all APIs unavailable
- **Significant protocol violations**: Breaks validator availability invariant

**Specific Impacts:**
1. **Consensus Participation Loss**: Affected validator cannot participate in consensus, reducing network fault tolerance
2. **Network Liveness Risk**: If multiple validators encounter this issue simultaneously, network liveness could be threatened
3. **Manual Recovery Required**: Operator must manually diagnose, update configuration, and restart node
4. **Epoch Boundary Vulnerability**: Attack window occurs at every epoch transition (regular intervals)

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability is likely to occur because:

1. **Operator Error**: Validators rotating consensus keys may update on-chain configuration before local storage, or forget to restart nodes after config updates
2. **Race Condition**: Even with correct procedures, network delays or epoch timing could cause the epoch to change before node restart completes
3. **Configuration Complexity**: The test suite shows the correct procedure requires stopping the node, updating config with overriding identity blobs, and restarting before on-chain rotation: [9](#0-8) 

4. **No Graceful Degradation**: The code provides no warning, retry mechanism, or fallback - it immediately panics

**Exploitation Requirements:**
- Validator operator performing key rotation (legitimate operation)
- Timing mismatch between on-chain update and local storage update
- No special privileges or attack knowledge required

## Recommendation

Replace the panic with graceful error handling that allows the node to continue operating in a degraded state:

```rust
let loaded_consensus_key = match self.load_consensus_key(&epoch_state.verifier) {
    Ok(k) => Arc::new(k),
    Err(e) => {
        error!(
            epoch = epoch_state.epoch,
            author = ?self.author,
            "Failed to load consensus key for new epoch: {e}. \
             Validator will not participate in consensus until key is available. \
             Please update node configuration with the correct consensus key and restart."
        );
        
        // Set validator_signer to None to indicate we cannot participate
        // This allows the node to continue syncing and monitoring without crashing
        // The operator can fix the configuration and restart
        
        // For now, use a placeholder or skip consensus participation
        // rather than crashing the entire node
        return; // or continue with limited functionality
    },
};
```

**Better approach**: Add validation checks before epoch transitions to verify consensus keys are available:

1. Add a pre-epoch-change health check that validates the consensus key exists for the upcoming validator set
2. Emit clear error logs/metrics when key mismatch is detected  
3. Allow the node to continue operating in read-only/observer mode rather than crashing
4. Implement automatic retry mechanism with exponential backoff

## Proof of Concept

**Reproduction Steps:**

1. Start a validator node with an initial consensus key
2. Submit an on-chain `rotate_consensus_key` transaction with a new public key
3. Do NOT update the local node configuration or restart the node
4. Wait for the next epoch boundary
5. Observe the validator node panic with: `"load_consensus_key failed: could not find sk by pk: ..."`

**Code path to panic:**
```
ReconfigNotificationListener receives NewEpochEvent
→ await_reconfig_notification() [epoch_manager.rs:1912]
→ start_new_epoch() [epoch_manager.rs:1918] 
→ load_consensus_key() [epoch_manager.rs:1228]
→ consensus_sk_by_pk() returns Error [persistent_safety_storage.rs:122]
→ Converted to anyhow::Error [epoch_manager.rs:1976]
→ panic!() [epoch_manager.rs:1231]
```

The vulnerability is confirmed by the single panic point found via grep search: [10](#0-9) 

## Notes

This vulnerability represents a critical operational risk for validators during consensus key rotation, which is a recommended security practice. The lack of graceful error handling contradicts standard best practices for production systems and creates unnecessary availability risks during routine maintenance operations.

The issue could be particularly severe during coordinated key rotations (e.g., multiple validators updating keys for security compliance) where simultaneous crashes could threaten network liveness.

### Citations

**File:** consensus/src/epoch_manager.rs (L1228-1233)
```rust
        let loaded_consensus_key = match self.load_consensus_key(&epoch_state.verifier) {
            Ok(k) => Arc::new(k),
            Err(e) => {
                panic!("load_consensus_key failed: {e}");
            },
        };
```

**File:** consensus/src/epoch_manager.rs (L1912-1920)
```rust
    async fn await_reconfig_notification(&mut self) {
        let reconfig_notification = self
            .reconfig_events
            .next()
            .await
            .expect("Reconfig sender dropped, unable to start new epoch");
        self.start_new_epoch(reconfig_notification.on_chain_configs)
            .await;
    }
```

**File:** consensus/src/epoch_manager.rs (L1971-1984)
```rust
    fn load_consensus_key(&self, vv: &ValidatorVerifier) -> anyhow::Result<PrivateKey> {
        match vv.get_public_key(&self.author) {
            Some(pk) => self
                .key_storage
                .consensus_sk_by_pk(pk)
                .map_err(|e| anyhow!("could not find sk by pk: {:?}", e)),
            None => {
                warn!("could not find my pk in validator set, loading default sk!");
                self.key_storage
                    .default_consensus_sk()
                    .map_err(|e| anyhow!("could not load default sk: {e}"))
            },
        }
    }
```

**File:** secure/storage/src/on_disk.rs (L78-83)
```rust
    fn get<V: DeserializeOwned>(&self, key: &str) -> Result<GetResponse<V>, Error> {
        let mut data = self.read()?;
        data.remove(key)
            .ok_or_else(|| Error::KeyNotSet(key.to_string()))
            .and_then(|value| serde_json::from_value(value).map_err(|e| e.into()))
    }
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L106-132)
```rust
    pub fn consensus_sk_by_pk(
        &self,
        pk: bls12381::PublicKey,
    ) -> Result<bls12381::PrivateKey, Error> {
        let _timer = counters::start_timer("get", CONSENSUS_KEY);
        let pk_hex = hex::encode(pk.to_bytes());
        let explicit_storage_key = format!("{}_{}", CONSENSUS_KEY, pk_hex);
        let explicit_sk = self
            .internal_store
            .get::<bls12381::PrivateKey>(explicit_storage_key.as_str())
            .map(|v| v.value);
        let default_sk = self.default_consensus_sk();
        let key = match (explicit_sk, default_sk) {
            (Ok(sk_0), _) => sk_0,
            (Err(_), Ok(sk_1)) => sk_1,
            (Err(_), Err(_)) => {
                return Err(Error::ValidatorKeyNotFound("not found!".to_string()));
            },
        };
        if key.public_key() != pk {
            return Err(Error::SecureStorageMissingDataError(format!(
                "Incorrect sk saved for {:?} the expected pk",
                pk
            )));
        }
        Ok(key)
    }
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L910-932)
```text
    public entry fun rotate_consensus_key(
        operator: &signer,
        pool_address: address,
        new_consensus_pubkey: vector<u8>,
        proof_of_possession: vector<u8>,
    ) acquires StakePool, ValidatorConfig {
        check_stake_permission(operator);
        assert_reconfig_not_in_progress();
        assert_stake_pool_exists(pool_address);

        let stake_pool = borrow_global_mut<StakePool>(pool_address);
        assert!(signer::address_of(operator) == stake_pool.operator_address, error::unauthenticated(ENOT_OPERATOR));

        assert!(exists<ValidatorConfig>(pool_address), error::not_found(EVALIDATOR_CONFIG));
        let validator_info = borrow_global_mut<ValidatorConfig>(pool_address);
        let old_consensus_pubkey = validator_info.consensus_pubkey;
        // Checks the public key has a valid proof-of-possession to prevent rogue-key attacks.
        let pubkey_from_pop = &bls12381::public_key_from_bytes_with_pop(
            new_consensus_pubkey,
            &proof_of_possession_from_bytes(proof_of_possession)
        );
        assert!(option::is_some(pubkey_from_pop), error::invalid_argument(EINVALID_PUBLIC_KEY));
        validator_info.consensus_pubkey = new_consensus_pubkey;
```

**File:** consensus/safety-rules/src/error.rs (L92-95)
```rust
            aptos_secure_storage::Error::KeyVersionNotFound(_, _)
            | aptos_secure_storage::Error::KeyNotSet(_) => {
                Self::SecureStorageMissingDataError(error.to_string())
            },
```

**File:** testsuite/smoke-test/src/consensus_key_rotation.rs (L62-116)
```rust
            info!("Stopping the last node.");

            validator.stop();
            tokio::time::sleep(Duration::from_secs(5)).await;

            let new_identity_path = PathBuf::from(
                format!(
                    "/tmp/{}-new-validator-identity.yaml",
                    thread_rng().r#gen::<u64>()
                )
                .as_str(),
            );
            info!(
                "Generating and writing new validator identity to {:?}.",
                new_identity_path
            );
            let new_sk = bls12381::PrivateKey::generate(&mut thread_rng());
            let pop = bls12381::ProofOfPossession::create(&new_sk);
            let new_pk = bls12381::PublicKey::from(&new_sk);
            let mut validator_identity_blob = validator
                .config()
                .consensus
                .safety_rules
                .initial_safety_rules_config
                .identity_blob()
                .unwrap();
            validator_identity_blob.consensus_private_key = Some(new_sk);
            let operator_addr = validator_identity_blob.account_address.unwrap();

            Write::write_all(
                &mut File::create(&new_identity_path).unwrap(),
                serde_yaml::to_string(&validator_identity_blob)
                    .unwrap()
                    .as_bytes(),
            )
            .unwrap();

            info!("Updating the node config accordingly.");
            let config_path = validator.config_path();
            let mut validator_override_config =
                OverrideNodeConfig::load_config(config_path.clone()).unwrap();
            validator_override_config
                .override_config_mut()
                .consensus
                .safety_rules
                .initial_safety_rules_config
                .overriding_identity_blob_paths_mut()
                .push(new_identity_path);
            validator_override_config.save_config(config_path).unwrap();

            info!("Restarting the node.");
            validator.start().unwrap();
            info!("Let it bake for 5 secs.");
            tokio::time::sleep(Duration::from_secs(5)).await;
            (operator_addr, new_pk, pop, operator_idx)
```
