# Audit Report

## Title
Thundering Herd on Storage Layer Due to Lack of Exponential Backoff in Indexer GRPC Retry Logic

## Summary
The `fetch_raw_txns_with_retries()` function in the indexer-grpc-fullnode service uses a fixed 300ms retry delay without exponential backoff or jitter. When multiple concurrent tasks experience transient storage failures simultaneously, they all retry at the same instant, creating a thundering herd that overwhelms the shared storage layer and prevents recovery. This can degrade performance for all services sharing the AptosDB storage, including validator consensus operations and API endpoints.

## Finding Description

The indexer-grpc-fullnode service spawns multiple concurrent tasks (default: 20) to fetch transaction batches from storage. Each task uses the `fetch_raw_txns_with_retries()` function which implements a naive retry mechanism with a hardcoded 300ms fixed delay. [1](#0-0) 

The retry logic exhibits these problematic characteristics:

1. **Fixed delay**: All retries wait exactly 300ms regardless of retry attempt number
2. **No jitter**: All concurrent tasks that fail will retry at precisely the same time
3. **No exponential backoff**: The system doesn't gradually back off to allow recovery
4. **Multiple concurrent tasks**: Up to 20 tasks (configurable via `processor_task_count`) execute in parallel [2](#0-1) 

Each task fetches up to 1000 transactions (default `processor_batch_size`), and each transaction requires approximately 7 database read operations from the shared AptosDB storage layer: [3](#0-2) 

**Attack Scenario:**

When the storage layer experiences transient issues (I/O contention, RocksDB compaction, memory pressure, lock contention), all concurrent tasks fail simultaneously. Due to the synchronized fixed retry delay:

1. At T=0: Storage experiences transient issue, 20 tasks * 1000 transactions * 7 reads = ~140,000 operations fail
2. At T=300ms: All 20 tasks retry simultaneously, hitting storage with another ~140,000 operations
3. The thundering herd prevents storage recovery, causing more failures
4. At T=600ms: Another synchronized retry wave
5. At T=900ms: Final retry attempt before panic

This pattern is particularly severe when multiple GRPC clients connect concurrently. The gRPC service allows unlimited concurrent connections (subject only to network-layer limits), with each spawning its own set of tasks: [4](#0-3) 

The storage layer error types that can trigger transient failures include: [5](#0-4) 

These errors (`RocksDbIncompleteResult`, `OtherRocksDbError`, `IoError`) are often transient and recoverable with proper backoff, but the current retry logic amplifies the problem.

## Impact Explanation

This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria for the following reasons:

1. **Validator Node Slowdowns**: The storage layer (AptosDB) is shared across all node services. When the indexer-grpc service creates a thundering herd on storage, it causes I/O contention and lock contention that degrades performance for consensus operations, state synchronization, and transaction execution. On validator nodes running the indexer-grpc service, this can directly impact consensus performance.

2. **API Crashes**: When retry exhaustion occurs after 3 attempts, the tasks panic, potentially crashing the indexer API endpoint. Additionally, storage overwhelm can cause API timeouts and unresponsiveness.

3. **Cascading Failures**: The thundering herd prevents natural recovery of transient storage issues, converting brief hiccups into prolonged outages.

While this vulnerability does not directly affect consensus safety or cause fund loss, it impacts the **availability** and **liveness** of critical blockchain infrastructure, meeting the High severity threshold of "Validator node slowdowns" and "API crashes."

## Likelihood Explanation

This vulnerability has **MEDIUM to HIGH likelihood** of occurring in production:

**Natural Triggers:**
- RocksDB compaction operations during high transaction volume
- I/O contention from concurrent state sync operations  
- Memory pressure causing swapping and slower storage responses
- Multiple concurrent indexer clients connecting simultaneously
- Disk performance degradation or temporary I/O delays

**Amplification Factors:**
- Default configuration spawns 20 concurrent tasks per request
- Multiple GRPC clients can connect without rate limiting
- Each transaction requires 7 separate database operations
- No circuit breaker or backpressure mechanism exists

The issue is particularly likely during:
- Network upgrades when multiple indexers reconnect simultaneously
- High transaction volume periods causing storage stress
- Node restarts when indexers reconnect and request historical data

## Recommendation

Implement exponential backoff with jitter in the retry logic:

```rust
pub async fn fetch_raw_txns_with_retries(
    context: Arc<Context>,
    ledger_version: u64,
    batch: TransactionBatchInfo,
) -> Vec<TransactionOnChainData> {
    let mut retries = 0;
    loop {
        match context.get_transactions(
            batch.start_version,
            batch.num_transactions_to_fetch,
            ledger_version,
        ) {
            Ok(raw_txns) => return raw_txns,
            Err(err) => {
                UNABLE_TO_FETCH_TRANSACTION.inc();
                retries += 1;

                if retries >= DEFAULT_NUM_RETRIES {
                    error!(
                        starting_version = batch.start_version,
                        num_transactions = batch.num_transactions_to_fetch,
                        error = format!("{:?}", err),
                        "Could not fetch transactions: retries exhausted",
                    );
                    panic!(
                        "Could not fetch {} transactions after {} retries, starting at {}: {:?}",
                        batch.num_transactions_to_fetch, retries, batch.start_version, err
                    );
                } else {
                    error!(
                        starting_version = batch.start_version,
                        num_transactions = batch.num_transactions_to_fetch,
                        error = format!("{:?}", err),
                        "Could not fetch transactions: will retry",
                    );
                }
                
                // Exponential backoff: 300ms * 2^(retries-1) with jitter
                let base_delay_ms = 300u64;
                let exponential_delay_ms = base_delay_ms * (1 << (retries - 1));
                // Add random jitter (Â±25%)
                let jitter_range = exponential_delay_ms / 4;
                let jitter = rand::thread_rng().gen_range(0..=jitter_range * 2);
                let final_delay_ms = exponential_delay_ms - jitter_range + jitter;
                
                tokio::time::sleep(Duration::from_millis(final_delay_ms)).await;
            },
        }
    }
}
```

**Additional Recommendations:**
1. Add connection-level rate limiting to prevent excessive concurrent GRPC streams
2. Implement a circuit breaker pattern to detect storage issues early
3. Add monitoring/alerting for storage contention metrics
4. Consider task-level backpressure based on storage health

## Proof of Concept

```rust
#[tokio::test]
async fn test_thundering_herd_on_storage() {
    use std::sync::atomic::{AtomicU64, Ordering};
    use std::sync::Arc;
    use tokio::time::Instant;
    
    // Simulate storage with request counter
    let storage_requests = Arc::new(AtomicU64::new(0));
    let peak_concurrent_requests = Arc::new(AtomicU64::new(0));
    let current_requests = Arc::new(AtomicU64::new(0));
    
    // Simulate 20 concurrent tasks
    let mut handles = vec![];
    let start_time = Instant::now();
    
    for task_id in 0..20 {
        let storage_requests = storage_requests.clone();
        let peak_concurrent = peak_concurrent_requests.clone();
        let current = current_requests.clone();
        
        let handle = tokio::spawn(async move {
            for retry in 0..3 {
                // Simulate storage request
                current.fetch_add(1, Ordering::SeqCst);
                storage_requests.fetch_add(1, Ordering::SeqCst);
                
                // Track peak concurrency
                let curr = current.load(Ordering::SeqCst);
                let mut peak = peak_concurrent.load(Ordering::SeqCst);
                while curr > peak {
                    match peak_concurrent.compare_exchange(
                        peak, curr, Ordering::SeqCst, Ordering::SeqCst
                    ) {
                        Ok(_) => break,
                        Err(x) => peak = x,
                    }
                }
                
                // Simulate transient failure
                tokio::time::sleep(Duration::from_millis(50)).await;
                current.fetch_sub(1, Ordering::SeqCst);
                
                // Fixed 300ms retry delay (current vulnerable implementation)
                if retry < 2 {
                    tokio::time::sleep(Duration::from_millis(300)).await;
                }
            }
        });
        handles.push(handle);
    }
    
    // Wait for all tasks
    for handle in handles {
        handle.await.unwrap();
    }
    
    let total_requests = storage_requests.load(Ordering::SeqCst);
    let peak = peak_concurrent_requests.load(Ordering::SeqCst);
    
    println!("Total storage requests: {}", total_requests);
    println!("Peak concurrent requests: {}", peak);
    println!("Expected: 60 total (20 tasks * 3 retries)");
    
    // Demonstrate thundering herd: all 20 tasks hit storage simultaneously
    assert_eq!(total_requests, 60); // 20 tasks * 3 attempts each
    assert!(peak >= 15); // At least 15 concurrent requests (thundering herd)
    
    // With proper exponential backoff + jitter, peak should be much lower (< 5)
}
```

**Notes**

This vulnerability exists in auxiliary indexer infrastructure but affects the core shared storage layer used by all node services. The lack of exponential backoff in retry logic is a well-known anti-pattern that causes thundering herd problems. While the indexer-grpc service itself is not consensus-critical, its ability to overwhelm shared storage creates availability and performance impacts for validator operations and API services. The fix is straightforward and follows industry best practices for retry logic design.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L320-360)
```rust
    pub async fn fetch_raw_txns_with_retries(
        context: Arc<Context>,
        ledger_version: u64,
        batch: TransactionBatchInfo,
    ) -> Vec<TransactionOnChainData> {
        let mut retries = 0;
        loop {
            match context.get_transactions(
                batch.start_version,
                batch.num_transactions_to_fetch,
                ledger_version,
            ) {
                Ok(raw_txns) => return raw_txns,
                Err(err) => {
                    UNABLE_TO_FETCH_TRANSACTION.inc();
                    retries += 1;

                    if retries >= DEFAULT_NUM_RETRIES {
                        error!(
                            starting_version = batch.start_version,
                            num_transactions = batch.num_transactions_to_fetch,
                            error = format!("{:?}", err),
                            "Could not fetch transactions: retries exhausted",
                        );
                        panic!(
                            "Could not fetch {} transactions after {} retries, starting at {}: {:?}",
                            batch.num_transactions_to_fetch, retries, batch.start_version, err
                        );
                    } else {
                        error!(
                            starting_version = batch.start_version,
                            num_transactions = batch.num_transactions_to_fetch,
                            error = format!("{:?}", err),
                            "Could not fetch transactions: will retry",
                        );
                    }
                    tokio::time::sleep(Duration::from_millis(300)).await;
                },
            }
        }
    }
```

**File:** config/src/config/indexer_grpc_config.rs (L23-29)
```rust
pub fn get_default_processor_task_count(use_data_service_interface: bool) -> u16 {
    if use_data_service_interface {
        1
    } else {
        20
    }
}
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L391-422)
```rust
            let (txn_infos, txns_and_outputs, persisted_aux_info) = (start_version
                ..start_version + limit)
                .map(|version| {
                    let txn_info = self
                        .ledger_db
                        .transaction_info_db()
                        .get_transaction_info(version)?;
                    let events = self.ledger_db.event_db().get_events_by_version(version)?;
                    let write_set = self.ledger_db.write_set_db().get_write_set(version)?;
                    let txn = self.ledger_db.transaction_db().get_transaction(version)?;
                    let auxiliary_data = self
                        .ledger_db
                        .transaction_auxiliary_data_db()
                        .get_transaction_auxiliary_data(version)?
                        .unwrap_or_default();
                    let txn_output = TransactionOutput::new(
                        write_set,
                        events,
                        txn_info.gas_used(),
                        txn_info.status().clone().into(),
                        auxiliary_data,
                    );
                    let persisted_aux_info = self
                        .ledger_db
                        .persisted_auxiliary_info_db()
                        .get_persisted_auxiliary_info(version)?
                        .unwrap_or(PersistedAuxiliaryInfo::None);
                    Ok((txn_info, (txn, txn_output), persisted_aux_info))
                })
                .collect::<Result<Vec<_>>>()?
                .into_iter()
                .multiunzip();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L67-117)
```rust
    async fn get_transactions_from_node(
        &self,
        req: Request<GetTransactionsFromNodeRequest>,
    ) -> Result<Response<Self::GetTransactionsFromNodeStream>, Status> {
        // Gets configs for the stream, partly from the request and partly from the node config
        let r = req.into_inner();
        let starting_version = match r.starting_version {
            Some(version) => version,
            // Live mode unavailable for FullnodeDataService
            // Enable use_data_service_interface in config to use LocalnetDataService instead
            None => return Err(Status::invalid_argument("Starting version must be set")),
        };
        let processor_task_count = self.service_context.processor_task_count;
        let processor_batch_size = self.service_context.processor_batch_size;
        let output_batch_size = self.service_context.output_batch_size;
        let transaction_channel_size = self.service_context.transaction_channel_size;
        let ending_version = if let Some(count) = r.transactions_count {
            starting_version.saturating_add(count)
        } else {
            u64::MAX
        };

        // Some node metadata
        let context = self.service_context.context.clone();
        let ledger_chain_id = context.chain_id().id();

        // Creates a channel to send the stream to the client.
        let (tx, rx) = mpsc::channel(transaction_channel_size);

        // Creates a moving average to track tps
        let mut ma = MovingAverage::new(10_000);

        let abort_handle = self.abort_handle.clone();
        // This is the main thread handling pushing to the stream
        tokio::spawn(async move {
            // Initialize the coordinator that tracks starting version and processes transactions
            let mut coordinator = IndexerStreamCoordinator::new(
                context,
                starting_version,
                ending_version,
                processor_task_count,
                processor_batch_size,
                output_batch_size,
                tx.clone(),
                // For now the request for this interface doesn't include a txn filter
                // because it is only used for the txn stream filestore worker, which
                // needs every transaction. Later we may add support for txn filtering
                // to this interface too.
                None,
                Some(abort_handle.clone()),
            );
```

**File:** storage/storage-interface/src/errors.rs (L23-32)
```rust
    #[error("AptosDB RocksDb Error: {0}")]
    RocksDbIncompleteResult(String),
    #[error("AptosDB RocksDB Error: {0}")]
    OtherRocksDbError(String),
    #[error("AptosDB bcs Error: {0}")]
    BcsError(String),
    #[error("AptosDB IO Error: {0}")]
    IoError(String),
    #[error("AptosDB Recv Error: {0}")]
    RecvError(String),
```
