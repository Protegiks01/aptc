# Audit Report

## Title
Missing Shard ID Validation in Remote Executor Command Processing Enables Consensus Violation

## Summary
The sharded block executor fails to validate that incoming `ExecuteSubBlocks` commands contain the correct `shard_id` matching the receiving shard's identity. This allows misrouted execution commands to cause result aggregation corruption and consensus failures in distributed executor deployments.

## Finding Description

The `ShardedExecutorService` receives `ExecuteSubBlocks` commands containing `SubBlocksForShard` objects with embedded `shard_id` fields, but never validates that these IDs match the shard's own identity. [1](#0-0) 

The `SubBlocksForShard` struct contains a `shard_id` field that should identify which shard the work belongs to: [2](#0-1) 

In remote executor deployments, commands arrive over the network without authentication, are deserialized, and passed directly to execution: [3](#0-2) 

The remote executor client sends commands over unauthenticated network channels: [4](#0-3) 

**Attack Flow:**
1. In distributed executor setup, coordinator sends work to remote shards via `NetworkController`
2. Network messages use simple GRPC without authentication in the `NetworkController`
3. A compromised coordinator, network attacker, or misconfigured system could send shard 0's work to shard 1
4. Shard 1 executes the work without validating `transactions.shard_id == 1`
5. Coordinator aggregates results expecting `[shard_0_output, shard_1_output]` but receives `[shard_0_output_from_shard_1, shard_1_output]`
6. Block execution produces incorrect state, violating deterministic execution

Additionally, cross-shard dependencies contain `ShardedTxnIndex` with `shard_id` and `round_id` that are never validated: [5](#0-4) 

## Impact Explanation

**Severity: HIGH** (potentially CRITICAL in production configurations)

This vulnerability violates the **Deterministic Execution** invariant: validators must produce identical state roots for identical blocks. If different shards execute mismatched work, aggregated results will be incorrect, causing:

1. **Consensus Safety Violation**: Different validators could produce different state roots
2. **State Corruption**: Wrong transactions executed in wrong order
3. **Liveness Failure**: Cross-shard messages routed to wrong destinations could cause hangs

However, this requires the remote executor feature to be enabled (opt-in, primarily for benchmarking): [6](#0-5) 

The feature is disabled by default (`get_remote_addresses()` returns empty), limiting production impact.

## Likelihood Explanation

**Likelihood: LOW** in typical production deployments

The vulnerability requires:
1. Remote executor feature explicitly enabled (opt-in configuration)
2. Network access to executor shard endpoints
3. Ability to send crafted BCS-serialized messages
4. Either network compromise, coordinator compromise, or misconfiguration

The remote executor appears designed for benchmarking/testing scenarios based on file locations and configuration patterns. Production validators likely use local in-process executors which are not vulnerable (commands sent via in-memory channels controlled by trusted code).

## Recommendation

Add validation at command reception:

```rust
// In ShardedExecutorService::start()
ExecutorShardCommand::ExecuteSubBlocks(
    state_view,
    transactions,
    concurrency_level_per_shard,
    onchain_config,
) => {
    // SECURITY: Validate shard_id matches our identity
    assert_eq!(
        transactions.shard_id, 
        self.shard_id,
        "Received ExecuteSubBlocks command with shard_id {} but this is shard {}",
        transactions.shard_id,
        self.shard_id
    );
    
    // Validate round numbers in cross-shard dependencies
    for sub_block in transactions.sub_block_iter() {
        for txn in sub_block.iter() {
            for (dep_idx, _) in txn.cross_shard_dependencies().required_edges_iter() {
                assert!(
                    dep_idx.round_id < MAX_ALLOWED_PARTITIONING_ROUNDS,
                    "Invalid round_id {} in cross-shard dependency",
                    dep_idx.round_id
                );
            }
        }
    }
    
    // Continue with execution...
}
```

Additionally, implement authentication for remote executor communication using the main network framework's Noise protocol handshake mechanisms.

## Proof of Concept

```rust
// Conceptual PoC - demonstrates the missing validation
use aptos_types::block_executor::partitioner::{SubBlocksForShard, SubBlock};

// Attacker crafts command with wrong shard_id
let malicious_sub_blocks = SubBlocksForShard::new(
    0, // shard_id = 0 (intended for shard 0)
    vec![SubBlock::new(0, vec![/* transactions */])],
);

// Send to shard 1 instead
let command = ExecuteBlockCommand {
    sub_blocks: malicious_sub_blocks, // shard_id=0 but going to shard 1
    concurrency_level: 4,
    onchain_config: /* ... */,
};

// Shard 1 receives and executes without validation
// Result: Shard 1 executes shard 0's work
// Coordinator expects results in order, gets corrupted aggregation
```

---

**Notes:**
- This vulnerability exists in the codebase but has **limited practical exploitability** in standard production deployments
- The remote executor is an opt-in feature primarily for benchmarking/testing
- Production impact depends on deployment configuration and network exposure
- Defense-in-depth: validation should be added regardless of current deployment patterns

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-260)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
```

**File:** types/src/block_executor/partitioner.rs (L302-323)
```rust
// A set of sub blocks assigned to a shard.
#[derive(Default, Clone, Debug, Serialize, Deserialize, Eq, PartialEq)]
pub struct SubBlocksForShard<T> {
    pub shard_id: ShardId,
    pub sub_blocks: Vec<SubBlock<T>>,
}

impl<T: Clone> SubBlocksForShard<T> {
    pub fn new(shard_id: ShardId, sub_blocks: Vec<SubBlock<T>>) -> Self {
        Self {
            shard_id,
            sub_blocks,
        }
    }

    pub fn empty(shard_id: ShardId) -> Self {
        Self {
            shard_id,
            sub_blocks: Vec::new(),
        }
    }

```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L79-113)
```rust
impl CoordinatorClient<RemoteStateViewClient> for RemoteCoordinatorClient {
    fn receive_execute_command(&self) -> ExecutorShardCommand<RemoteStateViewClient> {
        match self.command_rx.recv() {
            Ok(message) => {
                let _rx_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx"])
                    .start_timer();
                let bcs_deser_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx_bcs_deser"])
                    .start_timer();
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
                drop(bcs_deser_timer);

                match request {
                    RemoteExecutionRequest::ExecuteBlock(command) => {
                        let init_prefetch_timer = REMOTE_EXECUTOR_TIMER
                            .with_label_values(&[&self.shard_id.to_string(), "init_prefetch"])
                            .start_timer();
                        let state_keys = Self::extract_state_keys(&command);
                        self.state_view_client.init_for_block(state_keys);
                        drop(init_prefetch_timer);

                        let (sub_blocks, concurrency, onchain_config) = command.into();
                        ExecutorShardCommand::ExecuteSubBlocks(
                            self.state_view_client.clone(),
                            sub_blocks,
                            concurrency,
                            onchain_config,
                        )
                    },
                }
            },
            Err(_) => ExecutorShardCommand::Stop,
        }
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L180-212)
```rust
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        trace!("RemoteExecutorClient Sending block to shards");
        self.state_view_service.set_state_view(state_view);
        let (sub_blocks, global_txns) = transactions.into();
        if !global_txns.is_empty() {
            panic!("Global transactions are not supported yet");
        }
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }

        let execution_results = self.get_output_from_shards()?;

        self.state_view_service.drop_state_view();
        Ok(ShardedExecutionOutput::new(execution_results, vec![]))
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L61-101)
```rust
impl CrossShardCommitSender {
    pub fn new(
        shard_id: ShardId,
        cross_shard_client: Arc<dyn CrossShardClient>,
        sub_block: &SubBlock<AnalyzedTransaction>,
    ) -> Self {
        let mut dependent_edges = HashMap::new();
        let mut num_dependent_edges = 0;
        for (txn_idx, txn_with_deps) in sub_block.txn_with_index_iter() {
            let mut storage_locations_to_target = HashMap::new();
            for (txn_id_with_shard, storage_locations) in txn_with_deps
                .cross_shard_dependencies
                .dependent_edges()
                .iter()
            {
                for storage_location in storage_locations {
                    storage_locations_to_target
                        .entry(storage_location.clone().into_state_key())
                        .or_insert_with(HashSet::new)
                        .insert((txn_id_with_shard.shard_id, txn_id_with_shard.round_id));
                    num_dependent_edges += 1;
                }
            }
            if !storage_locations_to_target.is_empty() {
                dependent_edges.insert(txn_idx as TxnIndex, storage_locations_to_target);
            }
        }

        trace!(
            "CrossShardCommitSender::new: shard_id: {:?}, num_dependent_edges: {:?}",
            shard_id,
            num_dependent_edges
        );

        Self {
            shard_id,
            cross_shard_client,
            dependent_edges,
            index_offset: sub_block.start_index as TxnIndex,
        }
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```
