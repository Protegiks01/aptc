# Audit Report

## Title
Atomicity Violation in StateKvShardPruner: Progress Update Commits Before Pruning Work Completes

## Summary
The `StateKvShardPruner::new()` function contains a critical atomicity violation where pruner progress is written to the database in a separate transaction from the actual pruning operations. If deserialization fails during iteration in `prune()`, the progress metadata remains committed while no stale values are deleted, creating a permanent inconsistent database state that prevents pruner initialization and causes unbounded storage growth.

## Finding Description
The vulnerability exists in the initialization sequence of `StateKvShardPruner`. The code performs two separate database transactions that should be atomic: [1](#0-0) 

This calls `get_or_initialize_subpruner_progress()`, which performs a **direct write** to the database if progress doesn't exist: [2](#0-1) 

The `sub_db.put()` at lines 53-56 is a direct database write that commits immediately: [3](#0-2) 

After the progress is committed, the code attempts to perform the actual pruning work: [4](#0-3) 

If the iterator encounters corrupted data that fails deserialization at line 59: [5](#0-4) 

The deserialization failure in the schema's `decode_key()` function: [6](#0-5) 

Will propagate via the `?` operator, causing `prune()` to return early. The `SchemaBatch` containing the actual pruning deletes and progress update is never written: [7](#0-6) 

**Result:** The database contains progress metadata indicating "pruning complete up to version X" while stale state values at version X remain un-deleted, violating the State Consistency invariant.

**Exploitation Path:**
1. Database contains corrupted data at version 100 that fails deserialization
2. On first initialization: `get_or_initialize_subpruner_progress()` writes progress=100, then `prune(100, 100)` fails at corrupted data, batch never written
3. Database state: progress=100 (committed), but stale values at version 100 still exist (not deleted)
4. On subsequent initializations: `get_or_initialize_subpruner_progress()` returns existing progress=100, `prune()` seeks to 100, immediately hits corrupted data, fails
5. Pruner permanently stuck, database initialization fails repeatedly

## Impact Explanation
**Severity: High** (per Aptos Bug Bounty criteria: "Validator node slowdowns, API crashes, Significant protocol violations")

This vulnerability causes:

1. **Permanent Denial of Service**: Database cannot initialize because pruner initialization fails with `.expect("Failed to create state kv pruner.")`: [8](#0-7) 

2. **Unbounded Storage Growth**: Once the pruner is stuck, stale state values accumulate indefinitely as new versions are created but old versions cannot be deleted

3. **State Consistency Violation**: Database metadata claims pruning is complete when it's not, breaking the critical invariant: "State transitions must be atomic and verifiable"

4. **Non-Recoverable Failure**: The only recovery is manual database repair or restoration from backup, as the corrupted data prevents the validator from starting

## Likelihood Explanation
**Likelihood: Medium**

While this requires pre-existing database corruption, several realistic scenarios can trigger it:

1. **Disk Corruption**: Hardware failures, power loss, or filesystem bugs can corrupt RocksDB data
2. **Software Bugs**: Bugs in other storage components could write malformed schema data
3. **Upgrade/Migration Issues**: Database schema changes during upgrades could leave incompatible data
4. **State Sync Corruption**: Corrupted data received during state synchronization

The vulnerability is **deterministic** once corrupted data exists - every attempt to initialize the pruner will fail, and the inconsistent state persists permanently.

## Recommendation
Make the progress initialization atomic with the pruning work by eliminating the separate progress write. Instead, initialize progress pessimistically and only update it within the pruning batch:

```rust
pub(in crate::pruner) fn new(
    shard_id: usize,
    db_shard: Arc<DB>,
    metadata_progress: Version,
) -> Result<Self> {
    // Don't write progress yet - just read existing value or use 0
    let progress = db_shard
        .get::<DbMetadataSchema>(&DbMetadataKey::StateKvShardPrunerProgress(shard_id))?
        .map(|v| v.expect_version())
        .unwrap_or(0);  // Start from 0 if never initialized
    
    let myself = Self { shard_id, db_shard };

    info!(
        progress = progress,
        metadata_progress = metadata_progress,
        "Catching up state kv shard {shard_id}."
    );
    
    // This will atomically update progress in the same batch as pruning work
    myself.prune(progress, metadata_progress)?;

    Ok(myself)
}
```

This ensures progress is only updated when pruning actually succeeds, maintaining atomicity.

Alternatively, wrap both operations in a transaction or add retry logic with corrupted data skipping.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use crate::schema::stale_state_value_index_by_key_hash::StaleStateValueIndexByKeyHashSchema;
    use aptos_schemadb::DB;
    use aptos_temppath::TempPath;
    
    #[test]
    fn test_corrupted_data_leaves_inconsistent_progress() {
        let tmpdir = TempPath::new();
        let db = DB::open(
            tmpdir.path(),
            "test_db",
            vec![
                StaleStateValueIndexByKeyHashSchema::COLUMN_FAMILY_NAME,
                DbMetadataSchema::COLUMN_FAMILY_NAME,
            ],
        ).unwrap();
        
        // Write corrupted data that will fail deserialization
        let corrupted_key = vec![0, 0, 0, 0, 0, 0, 0, 100]; // Missing hash bytes
        db.inner.put_cf(
            db.get_cf_handle(StaleStateValueIndexByKeyHashSchema::COLUMN_FAMILY_NAME).unwrap(),
            &corrupted_key,
            b"",
        ).unwrap();
        
        // Attempt to initialize pruner - should fail
        let result = StateKvShardPruner::new(0, Arc::new(db.clone()), 100);
        assert!(result.is_err(), "Pruner initialization should fail on corrupted data");
        
        // Verify progress was written despite failure
        let progress = db.get::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(0)
        ).unwrap();
        assert_eq!(progress.unwrap().expect_version(), 100, "Progress incorrectly set");
        
        // Verify corrupted data still exists (was not deleted)
        let iter = db.iter::<StaleStateValueIndexByKeyHashSchema>().unwrap();
        assert!(iter.seek_to_first().is_ok(), "Corrupted data still in database");
        
        // Second attempt also fails - pruner permanently stuck
        let result2 = StateKvShardPruner::new(0, Arc::new(db), 100);
        assert!(result2.is_err(), "Pruner remains stuck on retry");
    }
}
```

**Notes:**
- This vulnerability specifically violates the atomicity requirement for state transitions
- The batch write at line 71 is correctly atomic via RocksDB WriteBatch semantics
- However, the separate progress write in `get_or_initialize_subpruner_progress()` breaks end-to-end atomicity
- The fix requires ensuring progress updates only commit when the corresponding work succeeds

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L30-34)
```rust
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L42-42)
```rust
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L58-59)
```rust
        for item in iter {
            let (index, _) = item?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L66-71)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/pruner_utils.rs (L44-59)
```rust
pub(crate) fn get_or_initialize_subpruner_progress(
    sub_db: &DB,
    progress_key: &DbMetadataKey,
    metadata_progress: Version,
) -> Result<Version> {
    Ok(
        if let Some(v) = sub_db.get::<DbMetadataSchema>(progress_key)? {
            v.expect_version()
        } else {
            sub_db.put::<DbMetadataSchema>(
                progress_key,
                &DbMetadataValue::Version(metadata_progress),
            )?;
            metadata_progress
        },
    )
```

**File:** storage/schemadb/src/lib.rs (L239-244)
```rust
    pub fn put<S: Schema>(&self, key: &S::Key, value: &S::Value) -> DbResult<()> {
        // Not necessary to use a batch, but we'd like a central place to bump counters.
        let mut batch = self.new_native_batch();
        batch.put::<S>(key, value)?;
        self.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/schema/stale_state_value_index_by_key_hash/mod.rs (L49-62)
```rust
    fn decode_key(data: &[u8]) -> Result<Self> {
        const VERSION_SIZE: usize = size_of::<Version>();

        ensure_slice_len_eq(data, 2 * VERSION_SIZE + HashValue::LENGTH)?;
        let stale_since_version = (&data[..VERSION_SIZE]).read_u64::<BigEndian>()?;
        let version = (&data[VERSION_SIZE..2 * VERSION_SIZE]).read_u64::<BigEndian>()?;
        let state_key_hash = HashValue::from_slice(&data[2 * VERSION_SIZE..])?;

        Ok(Self {
            stale_since_version,
            version,
            state_key_hash,
        })
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L127-132)
```rust
            for shard_id in 0..num_shards {
                shard_pruners.push(StateKvShardPruner::new(
                    shard_id,
                    state_kv_db.db_shard_arc(shard_id),
                    metadata_progress,
                )?);
```
