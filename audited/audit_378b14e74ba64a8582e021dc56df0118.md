# Audit Report

## Title
Metadata Cache Poisoning Allows Replay Verification Against Wrong Transaction Data, Masking Backup Corruption

## Summary
The metadata cache synchronization mechanism in `replay_verify.rs` lacks content integrity verification for cached files. A corrupted metadata cache can cause the replay verification tool to verify transactions against incorrect reference data, allowing backup corruption to go undetected.

## Finding Description

The replay verification tool uses a metadata cache to avoid re-downloading metadata files on subsequent runs. However, the cache synchronization logic has a critical flaw that breaks the **State Consistency** invariant. [1](#0-0) 

The `sync_and_load` function compares local cached files with remote files using only filename-based hashing (derived from FileHandle URIs, not file content). Files present in both sets are considered "up_to_date_local_hashes" and are **not** re-downloaded or integrity-checked. [2](#0-1) 

The `FileHandleHash` trait computes a hash of the FileHandle string itself (the URI path), not the actual file content. This means a locally corrupted metadata file with the same filename will be trusted without verification. [3](#0-2) 

These "up to date" files are loaded directly from cache without any checksum, signature, or content integrity verification. [4](#0-3) 

The metadata loading simply deserializes JSON with no integrity checks.

**Attack Propagation:**

1. The corrupted metadata is used to construct a `MetadataView`: [5](#0-4) 

2. The coordinator selects transaction backups based on corrupted metadata: [6](#0-5) 

3. If corrupted `TransactionBackupMeta` contains wrong version ranges or wrong manifest FileHandles, the verification process will:
   - Load and replay transactions from the **wrong** manifest files
   - Compare replayed outputs against the **wrong** expected data from those manifests [7](#0-6) 

4. Verification compares against `txn_info`, `write_set`, and `events` all loaded from the corrupted metadata-selected manifest, not the actual backup being verified.

**Example Attack Scenario:**
- Real backup contains transactions 1000-2000 with specific outputs
- Metadata cache is corrupted (disk error, malicious modification) to point to a manifest for transactions 3000-4000
- Replay verification loads manifest for 3000-4000 instead of 1000-2000
- Replays transactions 3000-4000 and verifies them against their own expected outputs (which match)
- Verification **PASSES** while the actual backup corruption for transactions 1000-2000 remains **UNDETECTED**

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program:

1. **Significant Protocol Violation**: The backup verification mechanism is designed to detect corruption in backups. This vulnerability allows corruption to go undetected, violating the integrity guarantees of the backup/restore system.

2. **Disaster Recovery Failure**: In a disaster recovery scenario, operators may rely on corrupted backups thinking they are valid (because verification passed), leading to inability to restore the blockchain state correctly.

3. **State Inconsistencies Requiring Intervention**: If a corrupted backup is used for restoration, it could lead to state inconsistencies between nodes requiring manual intervention.

While this doesn't directly cause loss of funds or consensus violations in production, it undermines the critical backup verification infrastructure that ensures disaster recovery capability - a fundamental requirement for blockchain operation.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can manifest through multiple realistic scenarios:

1. **Disk Corruption**: Storage media errors can corrupt cached metadata files - a common occurrence in long-running systems.

2. **Malicious Modification**: An attacker with filesystem access to the machine running replay verification (e.g., compromised backup operator machine, insider threat) could intentionally corrupt the cache.

3. **Cache Directory Reuse**: The CLI allows specifying a persistent cache directory that's shared across runs. If this directory is shared across different environments (devnet, testnet, mainnet) or contains stale data, metadata corruption is likely.

The vulnerability requires:
- Access to the metadata cache directory (filesystem-level)
- Knowledge of the metadata format
- No privileged validator access needed

The impact is **guaranteed** once the cache is corrupted - there's no fallback integrity check.

## Recommendation

Implement content integrity verification for cached metadata files:

```rust
// Add to MetadataCacheOpt or sync_and_load
pub async fn sync_and_load(
    opt: &MetadataCacheOpt,
    storage: Arc<dyn BackupStorage>,
    concurrent_downloads: usize,
) -> Result<MetadataView> {
    // ... existing code ...
    
    // For "up to date" files, verify integrity before trusting
    for h in up_to_date_local_hashes.clone() {
        let cached_file = cache_dir.join(h);
        let file_handle = remote_file_handle_by_hash.get(h).expect("In map.");
        
        // Re-download and compare hash of content, not just filename
        let remote_content = storage.load_raw_bytes(file_handle).await?;
        let local_content = tokio::fs::read(&cached_file).await?;
        
        let remote_hash = aptos_crypto::HashValue::sha3_256_of(&remote_content);
        let local_hash = aptos_crypto::HashValue::sha3_256_of(&local_content);
        
        if remote_hash != local_hash {
            warn!(
                file_handle = file_handle,
                "Cached metadata file corrupted, re-downloading."
            );
            // Remove corrupted file and add to new_remote_hashes for re-download
            remove_file(&cached_file).await.err_notes(&cached_file)?;
            new_remote_hashes.push(h);
        }
    }
    
    // ... rest of existing code ...
}
```

**Alternative approach**: Store content checksums in a separate index file and verify against it before loading cached metadata.

## Proof of Concept

```rust
// Rust test demonstrating the vulnerability
#[tokio::test]
async fn test_metadata_cache_poisoning() {
    use tempfile::TempDir;
    use std::fs;
    
    // Setup: Create a temporary cache directory
    let cache_dir = TempDir::new().unwrap();
    let cache_opt = MetadataCacheOpt::new(Some(cache_dir.path()));
    
    // Step 1: Initial sync - downloads legitimate metadata
    let storage = Arc::new(MockBackupStorage::new());
    let metadata_view_1 = sync_and_load(&cache_opt, storage.clone(), 1).await.unwrap();
    
    // Step 2: Corrupt the cached metadata file
    let cache_path = cache_opt.cache_dir();
    let cached_files: Vec<_> = fs::read_dir(&cache_path)
        .unwrap()
        .filter_map(|e| e.ok())
        .collect();
    
    assert!(!cached_files.is_empty());
    
    // Corrupt the first metadata file by modifying version ranges
    let corrupted_file = cached_files[0].path();
    let mut metadata_json: serde_json::Value = 
        serde_json::from_str(&fs::read_to_string(&corrupted_file).unwrap()).unwrap();
    
    // Change the version range in TransactionBackupMeta
    if let Some(first_version) = metadata_json.get_mut("first_version") {
        *first_version = serde_json::json!(99999); // Wrong version
    }
    
    fs::write(&corrupted_file, metadata_json.to_string()).unwrap();
    
    // Step 3: Re-sync - corrupted file is loaded without verification
    let metadata_view_2 = sync_and_load(&cache_opt, storage.clone(), 1).await.unwrap();
    
    // Step 4: Verify that corrupted metadata was used
    let txn_backups = metadata_view_2.select_transaction_backups(0, u64::MAX).unwrap();
    
    // The corrupted metadata will be present
    assert!(txn_backups.iter().any(|b| b.first_version == 99999));
    
    // This demonstrates that replay verification would now verify against
    // wrong transaction data based on corrupted metadata
}
```

## Notes

This vulnerability specifically affects the backup verification infrastructure and does not directly impact consensus or live transaction processing. However, it severely undermines disaster recovery capabilities, which are critical for blockchain resilience. The fix should be prioritized as it affects the integrity of the backup/restore system that all node operators rely on.

### Citations

**File:** storage/backup/backup-cli/src/metadata/cache.rs (L99-135)
```rust
    // List cached metadata files.
    let dir = read_dir(&cache_dir).await.err_notes(&cache_dir)?;
    let local_hashes_vec: Vec<String> = ReadDirStream::new(dir)
        .filter_map(|entry| match entry {
            Ok(e) => {
                let path = e.path();
                let file_name = path.file_name()?.to_str()?;
                Some(file_name.to_string())
            },
            Err(_) => None,
        })
        .collect()
        .await;
    let local_hashes: HashSet<_> = local_hashes_vec.into_iter().collect();
    // List remote metadata files.
    let mut remote_file_handles = storage.list_metadata_files().await?;
    if remote_file_handles.is_empty() {
        initialize_identity(&storage).await.context(
            "\
            Backup storage appears empty and failed to put in identity metadata, \
            no point to go on. If you believe there is content in the backup, check authentication.\
            ",
        )?;
        remote_file_handles = storage.list_metadata_files().await?;
    }
    let remote_file_handle_by_hash: HashMap<_, _> = remote_file_handles
        .iter()
        .map(|file_handle| (file_handle.file_handle_hash(), file_handle))
        .collect();
    let remote_hashes: HashSet<_> = remote_file_handle_by_hash.keys().cloned().collect();
    info!("Metadata files listed.");
    NUM_META_FILES.set(remote_hashes.len() as i64);

    // Sync local cache with remote metadata files.
    let stale_local_hashes = local_hashes.difference(&remote_hashes);
    let new_remote_hashes = remote_hashes.difference(&local_hashes).collect::<Vec<_>>();
    let up_to_date_local_hashes = local_hashes.intersection(&remote_hashes);
```

**File:** storage/backup/backup-cli/src/metadata/cache.rs (L192-207)
```rust
    // Load metadata from synced cache files.
    let mut metadata_vec = Vec::new();
    for h in new_remote_hashes.into_iter().chain(up_to_date_local_hashes) {
        let cached_file = cache_dir.join(h);
        metadata_vec.extend(
            OpenOptions::new()
                .read(true)
                .open(&cached_file)
                .await
                .err_notes(&cached_file)?
                .load_metadata_lines()
                .await
                .err_notes(&cached_file)?
                .into_iter(),
        )
    }
```

**File:** storage/backup/backup-cli/src/metadata/cache.rs (L216-228)
```rust
trait FileHandleHash {
    fn file_handle_hash(&self) -> String;
}

impl FileHandleHash for FileHandle {
    fn file_handle_hash(&self) -> String {
        use std::hash::{Hash, Hasher};

        let mut hasher = std::collections::hash_map::DefaultHasher::new();
        self.hash(&mut hasher);
        format!("{:x}", hasher.finish())
    }
}
```

**File:** storage/backup/backup-cli/src/metadata/cache.rs (L236-246)
```rust
impl<R: AsyncRead + Send + Unpin> LoadMetadataLines for R {
    async fn load_metadata_lines(&mut self) -> Result<Vec<Metadata>> {
        let mut buf = String::new();
        self.read_to_string(&mut buf)
            .await
            .err_notes((file!(), line!(), &buf))?;
        Ok(buf
            .lines()
            .map(serde_json::from_str::<Metadata>)
            .collect::<Result<_, serde_json::error::Error>>()?)
    }
```

**File:** storage/backup/backup-cli/src/coordinators/replay_verify.rs (L104-109)
```rust
        let metadata_view = metadata::cache::sync_and_load(
            &self.metadata_cache_opt,
            Arc::clone(&self.storage),
            self.concurrent_downloads,
        )
        .await?;
```

**File:** storage/backup/backup-cli/src/coordinators/replay_verify.rs (L159-164)
```rust
        let transactions = metadata_view.select_transaction_backups(
            // transaction info at the snapshot must be restored otherwise the db will be confused
            // about the latest version after snapshot is restored.
            next_txn_version.saturating_sub(1),
            self.end_version,
        )?;
```

**File:** execution/executor/src/chunk_executor/mod.rs (L629-649)
```rust
        for (version, txn_out, txn_info, write_set, events) in multizip((
            begin_version..end_version,
            &execution_output.to_commit.transaction_outputs,
            transaction_infos.iter(),
            write_sets.iter(),
            event_vecs.iter(),
        )) {
            if let Err(err) = txn_out.ensure_match_transaction_info(
                version,
                txn_info,
                Some(write_set),
                Some(events),
            ) {
                return if verify_execution_mode.is_lazy_quit() {
                    error!("(Not quitting right away.) {}", err);
                    verify_execution_mode.mark_seen_error();
                    Ok(version + 1)
                } else {
                    Err(err)
                };
            }
```
