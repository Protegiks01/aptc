# Audit Report

## Title
Race Condition in Secret Share Aggregation Leading to Consensus Pipeline Liveness Failure

## Summary
When `block_queue.item_mut()` returns None in `process_aggregated_key()`, it silently drops the aggregated secret key. While this is expected behavior during epoch transitions, a race condition exists during normal operation where secret share aggregation can complete and send a key before the corresponding block is added to the processing queue, causing permanent pipeline stalls.

## Finding Description

The vulnerability exists in the order of operations when processing incoming blocks in the SecretShareManager: [1](#0-0) 

The critical flow creates a race window:

1. **Block Processing Starts**: `process_incoming_blocks` loops through blocks, calling `process_incoming_block` for each [2](#0-1) 

2. **Aggregation Triggered Early**: Within `process_incoming_block`, the self share is added to the store, which immediately calls `try_aggregate`: [3](#0-2) 

3. **Async Aggregation**: If enough shares exist (threshold reached), aggregation occurs in `spawn_blocking` on a separate thread pool: [4](#0-3) 

4. **Queue Addition Delayed**: The block is only added to `block_queue` AFTER all blocks are processed: [5](#0-4) 

5. **Key Dropped**: The aggregated key arrives before the block is in the queue, and `item_mut()` returns None: [6](#0-5) 

**Critical Invariant Violation**: The `pending_secret_key_rounds` HashSet for that block is never cleared because `set_secret_shared_key` is never called: [7](#0-6) 

This breaks the **Consensus Liveness** invariant - blocks cannot proceed through the pipeline, causing a permanent stall of the secret sharing manager on that validator node. The `dequeue_ready_prefix` function only dequeues blocks where `is_fully_secret_shared()` returns true: [8](#0-7) 

## Impact Explanation

**Medium Severity** per Aptos bug bounty criteria: "State inconsistencies requiring intervention"

- **Liveness Impact**: The affected validator node's secret sharing pipeline becomes permanently stuck, unable to process current and future blocks
- **Node Recovery**: Requires manual intervention (node restart) or waiting for epoch transition 
- **Network Impact**: Single node affected; other validators continue normally, but the stuck node falls behind and may need state sync
- **No Safety Violation**: Does not cause consensus safety issues (chain splits, double-spending) as the affected node simply stops participating
- **Deterministic Failure**: Once triggered, the node remains stuck until manual intervention

## Likelihood Explanation

**Moderate to High Likelihood** under specific conditions:

**Triggering Factors**:
- Processing batches of multiple blocks (increases processing time window)
- Fast secret share aggregation (BLS operations on modern hardware can be very fast, sub-millisecond)
- Shares from other validators already present in the store (common in normal operation as validators broadcast shares immediately)
- High CPU availability for `spawn_blocking` thread pool

**Real-World Scenario**:
When a validator processes a batch of ordered blocks, other validators have already processed those same blocks and broadcasted their shares. By the time this validator adds its own share (which triggers aggregation), enough shares from others already exist to meet the threshold. The `spawn_blocking` aggregation completes on a separate thread pool while the main event loop is still processing remaining blocks in the batch. The aggregated key enters the `decision_rx` channel buffer but is processed by the event loop only after blocks are added to the queue - except the `spawn_blocking` task can complete before `process_incoming_blocks` finishes, and the channel already has the key ready when `tokio::select!` checks arms again. [9](#0-8) 

## Recommendation

**Fix 1: Defer Aggregation Until Block is Queued**

Add the block to the queue BEFORE calling operations that could trigger aggregation. Modify `process_incoming_blocks`:

```rust
async fn process_incoming_blocks(&mut self, blocks: OrderedBlocks) {
    let rounds: Vec<u64> = blocks.ordered_blocks.iter().map(|b| b.round()).collect();
    info!(rounds = rounds, "Processing incoming blocks.");

    // First, derive shares without aggregating
    let mut self_shares = Vec::new();
    for block in blocks.ordered_blocks.iter() {
        let futures = block.pipeline_futs().expect("pipeline must exist");
        let self_secret_share = futures
            .secret_sharing_derive_self_fut
            .await
            .expect("Decryption share computation is expected to succeed")
            .expect("Must not be None");
        self_shares.push(self_secret_share);
    }

    // Add to queue FIRST
    let mut pending_secret_key_rounds = HashSet::new();
    for block in blocks.ordered_blocks.iter() {
        pending_secret_key_rounds.insert(block.round());
    }
    let queue_item = QueueItem::new(blocks.clone(), None, pending_secret_key_rounds);
    self.block_queue.push_back(queue_item);

    // THEN add shares and trigger aggregation + spawn requesters
    let mut share_requester_handles = Vec::new();
    for (block, self_share) in blocks.ordered_blocks.iter().zip(self_shares.into_iter()) {
        let handle = self.broadcast_and_spawn_requester(block, self_share).await;
        share_requester_handles.push(handle);
    }
    
    // Update queue item with handles
    if let Some(item) = self.block_queue.item_mut(blocks.ordered_blocks[0].round()) {
        item.share_requester_handles = Some(share_requester_handles);
    }
}
```

**Fix 2: Add Defensive Logging and Monitoring**

Add logging when None is encountered to detect anomalies:

```rust
fn process_aggregated_key(&mut self, secret_share_key: SecretSharedKey) {
    if let Some(item) = self.block_queue.item_mut(secret_share_key.metadata.round) {
        item.set_secret_shared_key(secret_share_key.metadata.round, secret_share_key);
    } else {
        warn!(
            epoch = self.epoch_state.epoch,
            round = secret_share_key.metadata.round,
            stop_flag = self.stop,
            "[SecretShareManager] Received aggregated key for round not in queue"
        );
        counters::SECRET_SHARE_KEY_DROPPED.inc();
    }
}
```

## Proof of Concept

The following demonstrates the race condition timing:

```rust
// consensus/src/rand/secret_sharing/tests/race_test.rs

use super::*;
use std::sync::atomic::{AtomicBool, Ordering};
use std::sync::Arc;

#[tokio::test]
async fn test_aggregation_before_queue_race() {
    // Setup: Create manager with fast aggregation config
    let (decision_tx, mut decision_rx) = unbounded();
    let race_detected = Arc::new(AtomicBool::new(false));
    let race_flag = race_detected.clone();
    
    // Simulate fast aggregation by sending key immediately
    tokio::spawn(async move {
        // Simulate aggregation completing during block processing
        tokio::time::sleep(Duration::from_micros(100)).await;
        let key = create_test_secret_shared_key(100);
        decision_tx.unbounded_send(key).unwrap();
    });
    
    // Simulate slow block processing (processing batch of blocks)
    tokio::time::sleep(Duration::from_millis(200)).await;
    
    // Add block to queue AFTER key was sent
    let mut block_queue = BlockQueue::new();
    let queue_item = create_test_queue_item(100);
    // Key should arrive before this line
    
    // Process key - this simulates the event loop receiving the key
    if let Some(key) = decision_rx.next().await {
        // Try to find block in queue
        if block_queue.item_mut(key.metadata.round).is_none() {
            // RACE DETECTED: Key arrived before block was queued
            race_flag.store(true, Ordering::SeqCst);
        }
    }
    
    // Now add block (too late!)
    block_queue.push_back(queue_item);
    
    assert!(race_detected.load(Ordering::SeqCst), 
        "Race condition detected: aggregated key arrived before block was queued");
}
```

This PoC demonstrates that with fast aggregation and delayed queue insertion, the key can arrive before the block is in the queue, causing permanent pipeline stall.

## Notes

While reviewing this issue, I found that the race window is real and can occur under the following conditions:

1. **Expected Behavior**: During epoch transitions with `ResetSignal::Stop`, the queue is cleared and pending keys are correctly dropped: [10](#0-9) 

2. **Unexpected Behavior**: During normal operation, if aggregation completes before blocks are queued due to async timing, keys are silently dropped with no logging, monitoring, or recovery mechanism.

3. **State Sync Behavior**: During state sync resets with `ResetSignal::TargetRound`, only rand_manager and buffer_manager are reset, NOT secret_share_manager: [11](#0-10) 

The lack of reset signals to secret_share_manager during state sync is a separate concern but doesn't directly cause this race condition.

### Citations

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L112-130)
```rust
    async fn process_incoming_blocks(&mut self, blocks: OrderedBlocks) {
        let rounds: Vec<u64> = blocks.ordered_blocks.iter().map(|b| b.round()).collect();
        info!(rounds = rounds, "Processing incoming blocks.");

        let mut share_requester_handles = Vec::new();
        let mut pending_secret_key_rounds = HashSet::new();
        for block in blocks.ordered_blocks.iter() {
            let handle = self.process_incoming_block(block).await;
            share_requester_handles.push(handle);
            pending_secret_key_rounds.insert(block.round());
        }

        let queue_item = QueueItem::new(
            blocks,
            Some(share_requester_handles),
            pending_secret_key_rounds,
        );
        self.block_queue.push_back(queue_item);
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L143-148)
```rust
            let mut secret_share_store = self.secret_share_store.lock();
            secret_share_store.update_highest_known_round(block.round());
            secret_share_store
                .add_self_share(self_secret_share.clone())
                .expect("Add self dec share should succeed");
        }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L186-190)
```rust
    fn process_aggregated_key(&mut self, secret_share_key: SecretSharedKey) {
        if let Some(item) = self.block_queue.item_mut(secret_share_key.metadata.round) {
            item.set_secret_shared_key(secret_share_key.metadata.round, secret_share_key);
        }
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L55-70)
```rust
        tokio::task::spawn_blocking(move || {
            let maybe_key = SecretShare::aggregate(self.shares.values(), &dec_config);
            match maybe_key {
                Ok(key) => {
                    let dec_key = SecretSharedKey::new(metadata, key);
                    let _ = decision_tx.unbounded_send(dec_key);
                },
                Err(e) => {
                    warn!(
                        epoch = metadata.epoch,
                        round = metadata.round,
                        "Aggregation error: {e}"
                    );
                },
            }
        });
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L64-77)
```rust
    pub fn set_secret_shared_key(&mut self, round: Round, key: SecretSharedKey) {
        let offset = self.offset(round);
        if self.pending_secret_key_rounds.contains(&round) {
            observe_block(
                self.blocks()[offset].timestamp_usecs(),
                BlockStage::SECRET_SHARING_ADD_DECISION,
            );
            let block = &self.blocks_mut()[offset];
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
                tx.secret_shared_key_tx.take().map(|tx| tx.send(Some(key)));
            }
            self.pending_secret_key_rounds.remove(&round);
        }
    }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L112-127)
```rust
    pub fn dequeue_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.is_fully_secret_shared() {
                let (_, item) = self.queue.pop_first().expect("First key must exist");
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::SECRET_SHARING_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        ready_prefix
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L333-360)
```rust
                let entry = select! {
                    Some(ordered_blocks) = ordered_block_rx.next() => {
                        let _ = rand_manager_input_tx.send(ordered_blocks.clone()).await;
                        let _ = secret_share_manager_input_tx.send(ordered_blocks.clone()).await;
                        let first_block_id = ordered_blocks.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.insert(first_block_id, (ordered_blocks, false, false));
                        inflight_block_tracker.entry(first_block_id)
                    },
                    Some(rand_ready_block) = rand_ready_block_rx.next() => {
                        let first_block_id = rand_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.1 = true;
                        })
                    },
                    Some(secret_ready_block) = secret_ready_block_rx.next() => {
                        let first_block_id = secret_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.2 = true;
                        })
                    },
                };
                let Entry::Occupied(o) = entry else {
                    unreachable!("Entry must exist");
                };
                if o.get().1 && o.get().2 {
                    let (_, (ordered_blocks, _, _)) = o.remove_entry();
                    let _ = ready_block_tx.send(ordered_blocks).await;
                }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```
