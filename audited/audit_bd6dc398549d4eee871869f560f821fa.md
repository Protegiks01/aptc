# Audit Report

## Title
State Desynchronization in LeaderReputation Causes Non-Deterministic Proposer Election and Consensus Liveness Failure

## Summary
The `LeaderReputation` proposer election mechanism in `ProposerAndVoterV2` mode uses database-derived root hashes as part of the seed for proposer selection. When validators have divergent database states due to state sync lag, database pruning, or incomplete historical data, they compute different root hashes and elect different proposers for the same round. This breaks consensus determinism and can cause total liveness failure when sufficient validators disagree on the valid proposer.

## Finding Description

The vulnerability exists in how the `state` parameter for proposer election is constructed and synchronized across validators in the `LeaderReputation` implementation. [1](#0-0) 

When `use_root_hash` is true (the default for `ProposerAndVoterV2`), the state includes a `root_hash` obtained from the local database: [2](#0-1) 

The `get_block_metadata` function retrieves this root hash from the validator's local AptosDB: [3](#0-2) 

**Critical Issue #1**: When a validator's database lacks the required block events (due to being behind in state sync, database pruning, or never backfilling historical data), the function returns `HashValue::zero()`: [4](#0-3) [5](#0-4) 

**Critical Issue #2**: The code explicitly warns about this problem: [6](#0-5) 

The warning states: **"Elected proposers are unlikely to match!!"** - confirming the developers knew this could cause proposer disagreement.

**Attack Scenario**:

1. Default consensus configuration uses `ProposerAndVoterV2`: [7](#0-6) 

2. An attacker causes multiple validators to fall behind in state sync through network disruption, DDoS, or resource exhaustion attacks.

3. These validators query their local databases at different sync levels:
   - Validator A (synced): `state = [actual_root_hash, epoch, round]`
   - Validator B (behind): `state = [zero_hash, epoch, round]`
   - Validator C (partially synced): `state = [different_root_hash, epoch, round]`

4. Different `state` values are fed into `choose_index`: [8](#0-7) 

5. Validators compute different proposers (e.g., Validator A elected, but Validator B and C expect different proposers).

6. When Validator A proposes, validators B and C reject the proposal because their proposer election check fails: [9](#0-8) 

7. Without sufficient votes (need 2f+1), the proposal cannot form a quorum certificate.

8. Consensus stalls - **total liveness failure**.

**Why sync_up doesn't prevent this**: The `sync_up` mechanism syncs to the `highest_commit_cert` in the received sync_info, but `LeaderReputation` queries for historical data at `round - exclude_round` (default exclude_round = 40). If the historical window isn't fully synced or has been pruned, validators will still have divergent database states for the reputation computation.

## Impact Explanation

This is a **Critical Severity** vulnerability under Aptos Bug Bounty criteria:

1. **Total loss of liveness/network availability**: When multiple validators (>f) have divergent database states, they disagree on valid proposers, preventing quorum formation and stalling consensus indefinitely.

2. **Consensus Safety Violation**: Breaks the fundamental consensus invariant that all validators must deterministically agree on the same leader for a given round. This violates "Deterministic Execution: All validators must produce identical state roots for identical blocks."

3. **Requires only network-level attacks**: Attacker doesn't need validator compromise - only the ability to disrupt state sync for multiple validators through:
   - Network partitioning
   - DDoS attacks on validator nodes
   - Resource exhaustion attacks
   - Timing attacks around database pruning windows

4. **Affects default configuration**: The vulnerable `ProposerAndVoterV2` mode is the default, meaning all Aptos networks using default settings are affected.

## Likelihood Explanation

**High Likelihood** in adverse conditions:

1. **Network instability**: Any significant network disruption that causes state sync delays will trigger this vulnerability.

2. **New validators joining**: New validators backfilling historical data will temporarily have incomplete database states.

3. **Database pruning**: When validators prune old data at different times, they'll have different historical availability.

4. **Epoch transitions**: During epoch changes, validators may sync at different rates.

5. **Explicit warning in code**: The warning message confirms this condition occurs in practice ("unlikely scenario that we have many failed consecutive rounds").

The vulnerability doesn't require sophisticated attacks - normal network variability and operational differences can trigger it. An adversary can deliberately amplify these conditions through targeted attacks.

## Recommendation

**Immediate Mitigation**: Switch back to `ProposerAndVoter` (V1) mode which uses only `epoch` and `round` for the seed (no root hash), making proposer election deterministic regardless of database state: [10](#0-9) 

**Long-term Fix**: Modify `LeaderReputation` to ensure all validators use a synchronized, consensus-derived state value instead of locally-queried database state:

```rust
// Instead of querying local database:
let (sliding_window, root_hash) = self.backend.get_block_metadata(self.epoch, target_round);

// Use the root hash from the highest committed QC in sync_info:
let root_hash = sync_info.highest_commit_cert().ledger_info()
    .transaction_accumulator_hash();
```

This ensures all validators use the same committed state for proposer election.

**Additional Safeguards**:
1. Add explicit validation that all validators have sufficient historical data before participating in proposer election
2. Implement fallback to deterministic round-robin if historical data is unavailable
3. Add consensus-level check that validates all validators computed the same proposer before accepting proposals
4. Change error handling to fail-safe mode (halt) rather than proceeding with `HashValue::zero()`

## Proof of Concept

```rust
// Reproduction steps for Rust integration test

#[tokio::test]
async fn test_proposer_election_desync() {
    // Setup: 4 validators with ProposerAndVoterV2
    let validators = setup_validators(4, ProposerElectionType::LeaderReputation(
        LeaderReputationType::ProposerAndVoterV2(default_config())
    )).await;
    
    // Step 1: All validators sync to round 100
    advance_all_to_round(&validators, 100).await;
    let round = 101;
    
    // Step 2: Simulate validator[1] and validator[2] falling behind
    // by clearing their database historical data
    validators[1].clear_history_before_round(80).await;
    validators[2].clear_history_before_round(85).await;
    
    // Step 3: Each validator computes proposer for round 101
    // (which looks back to round 101 - 40 = 61 for historical data)
    let proposer_0 = validators[0].get_valid_proposer(round);
    let proposer_1 = validators[1].get_valid_proposer(round);
    let proposer_2 = validators[2].get_valid_proposer(round);
    let proposer_3 = validators[3].get_valid_proposer(round);
    
    // Expected: Validators compute DIFFERENT proposers
    // because they have different historical database states
    assert_ne!(proposer_0, proposer_1); // validator[1] lacks history
    assert_ne!(proposer_0, proposer_2); // validator[2] has partial history
    assert_eq!(proposer_0, proposer_3); // only these two agree
    
    // Step 4: proposer_0 sends a proposal
    let proposal = validators[proposer_0].create_proposal(round).await;
    
    // Step 5: Process proposal at all validators
    let results = vec![
        validators[0].process_proposal(&proposal).await,
        validators[1].process_proposal(&proposal).await,
        validators[2].process_proposal(&proposal).await,
        validators[3].process_proposal(&proposal).await,
    ];
    
    // Expected: validators[1] and validators[2] REJECT the proposal
    // because their proposer election computed different proposers
    assert!(results[0].is_ok()); // validator 0 accepts
    assert!(results[1].is_err()); // validator 1 rejects: "Not a valid proposer"
    assert!(results[2].is_err()); // validator 2 rejects: "Not a valid proposer"
    assert!(results[3].is_ok()); // validator 3 accepts
    
    // Result: Only 2 votes (< 2f+1 = 3 needed), no QC formed
    // CONSENSUS LIVENESS FAILURE
    assert_eq!(count_votes(&validators, round), 2);
    assert!(validators[0].get_qc(round).is_none());
}
```

**Notes**

This vulnerability is **active in the default Aptos mainnet configuration**. The `ProposerAndVoterV2` mode was introduced to make proposer seeds "unpredictable" (preventing proposer prediction attacks), but this came at the cost of determinism across validators with divergent database states. The explicit warning in the code confirms this is a known implementation concern, but the security implications appear underestimated. Any network conditions causing state sync delays—natural or attacker-induced—can trigger consensus stalls affecting all network participants.

### Citations

**File:** consensus/src/liveness/leader_reputation.rs (L119-122)
```rust
                warn!(
                    "Local history is too old, asking for {} epoch and {} round, and latest from db is {} epoch and {} round! Elected proposers are unlikely to match!!",
                    target_epoch, target_round, events.first().map_or(0, |e| e.event.epoch()), events.first().map_or(0, |e| e.event.round()))
            }
```

**File:** consensus/src/liveness/leader_reputation.rs (L168-214)
```rust
impl MetadataBackend for AptosDBBackend {
    // assume the target_round only increases
    fn get_block_metadata(
        &self,
        target_epoch: u64,
        target_round: Round,
    ) -> (Vec<NewBlockEvent>, HashValue) {
        let mut locked = self.db_result.lock();
        let latest_db_version = self.aptos_db.get_latest_ledger_info_version().unwrap_or(0);
        // lazy init db_result
        if locked.is_none() {
            if let Err(e) = self.refresh_db_result(&mut locked, latest_db_version) {
                warn!(
                    error = ?e, "[leader reputation] Fail to initialize db result",
                );
                return (vec![], HashValue::zero());
            }
        }
        let (events, version, hit_end) = {
            // locked is somenthing
            #[allow(clippy::unwrap_used)]
            let result = locked.as_ref().unwrap();
            (&result.0, result.1, result.2)
        };

        let has_larger = events
            .first()
            .is_some_and(|e| (e.event.epoch(), e.event.round()) >= (target_epoch, target_round));
        // check if fresher data has potential to give us different result
        if !has_larger && version < latest_db_version {
            let fresh_db_result = self.refresh_db_result(&mut locked, latest_db_version);
            match fresh_db_result {
                Ok((events, _version, hit_end)) => {
                    self.get_from_db_result(target_epoch, target_round, &events, hit_end)
                },
                Err(e) => {
                    // fails if requested events were pruned / or we never backfil them.
                    warn!(
                        error = ?e, "[leader reputation] Fail to refresh window",
                    );
                    (vec![], HashValue::zero())
                },
            }
        } else {
            self.get_from_db_result(target_epoch, target_round, events, hit_end)
        }
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L701-701)
```rust
        let (sliding_window, root_hash) = self.backend.get_block_metadata(self.epoch, target_round);
```

**File:** consensus/src/liveness/leader_reputation.rs (L717-730)
```rust
        let state = if self.use_root_hash {
            [
                root_hash.to_vec(),
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        } else {
            [
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        };
```

**File:** types/src/on_chain_config/consensus_config.rs (L489-489)
```rust
                LeaderReputationType::ProposerAndVoterV2(ProposerAndVoterConfig {
```

**File:** types/src/on_chain_config/consensus_config.rs (L541-544)
```rust
    pub fn use_root_hash_for_seed(&self) -> bool {
        // all versions after V1 should use root hash
        !matches!(self, Self::ProposerAndVoter(_))
    }
```

**File:** consensus/src/liveness/proposer_election.rs (L49-69)
```rust
pub(crate) fn choose_index(mut weights: Vec<u128>, state: Vec<u8>) -> usize {
    let mut total_weight = 0;
    // Create cumulative weights vector
    // Since we own the vector, we can safely modify it in place
    for w in &mut weights {
        total_weight = total_weight
            .checked_add(w)
            .expect("Total stake shouldn't exceed u128::MAX");
        *w = total_weight;
    }
    let chosen_weight = next_in_range(state, total_weight);
    weights
        .binary_search_by(|w| {
            if *w <= chosen_weight {
                Ordering::Less
            } else {
                Ordering::Greater
            }
        })
        .expect_err("Comparison never returns equals, so it's always guaranteed to be error")
}
```

**File:** consensus/src/round_manager.rs (L825-831)
```rust
            ensure!(
                self.proposer_election
                    .is_valid_proposer(proposal_msg.proposer(), proposal_msg.round()),
                "[OptProposal] Not a valid proposer for round {}: {}",
                proposal_msg.round(),
                proposal_msg.proposer()
            );
```
