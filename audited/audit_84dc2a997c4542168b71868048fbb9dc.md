# Audit Report

## Title
Consensus State Desynchronization via Mid-Epoch CommitHistory Resource Replacement

## Summary
A governance proposal can replace the `CommitHistory` resource mid-epoch without forcing reconfiguration, causing a critical split between validators' in-memory reputation state and on-chain commit history. Validators that restart mid-epoch will re-bootstrap from the new table while running validators retain state from the old table, leading to divergent anchor election decisions and potential consensus liveness failure.

## Finding Description

The vulnerability exists in the DAG consensus bootstrap mechanism and its interaction with the on-chain `CommitHistory` resource:

**1. Bootstrap Reads Commit History Once Per Epoch**

During epoch initialization, `DagBootstrapper::build_anchor_election()` calls `get_latest_k_committed_events()` to fetch historical commit events: [1](#0-0) 

This reads the `CommitHistoryResource` and extracts the table handle: [2](#0-1) 

The method fetches the resource at the latest version and reads table items using the obtained handle: [3](#0-2) 

These historical events initialize the `MetadataBackendAdapter` sliding window: [4](#0-3) 

**2. Block Prologues Write to Current Resource**

When blocks execute, `emit_new_block_event()` writes `NewBlockEvent` entries to the on-chain `CommitHistory` using `borrow_global_mut`: [5](#0-4) 

This always accesses the CURRENT state of the resource at `@aptos_framework`.

**3. The Vulnerability: Mid-Epoch Resource Replacement**

If a governance proposal uses `WriteSetPayload::Direct` to replace the `CommitHistory` resource mid-epoch:

- The proposal executes `move_from<CommitHistory>` to remove the old resource (table handle H1)
- Then executes `move_to<CommitHistory>` to create a new resource (table handle H2)
- This can occur WITHOUT triggering `aptos_governance::reconfigure()`, allowing it to happen within the same epoch

**4. The Split Occurs**

After replacement at version V100:
- Running validators continue using their in-memory `MetadataBackendAdapter` state initialized from H1 at V0
- All subsequent blocks (V100+) write `NewBlockEvent` entries to H2 via `borrow_global_mut`
- The in-memory sliding window and on-chain table are now desynchronized

**5. Validator Restart Triggers Divergence**

When a validator crashes and restarts mid-epoch at version V200: [6](#0-5) 

The restarted validator:
- Calls `full_bootstrap()` which invokes `get_latest_k_committed_events()`
- Reads from the NEW table H2 at version V200
- Initializes its reputation state with potentially different/incomplete history

Other validators retain:
- Reputation state from original table H1
- Plus local updates via `update_reputation()` during the epoch [7](#0-6) 

**Invariant Broken:**

Critical Invariant #1: "**Deterministic Execution**: All validators must produce identical state roots for identical blocks" is violated because validators have divergent views of commit history, leading to non-deterministic anchor election.

## Impact Explanation

**Severity: HIGH** (per Aptos Bug Bounty criteria)

This vulnerability causes:

1. **Consensus Liveness Failure**: Different validators elect different anchors based on divergent reputation states, preventing quorum formation on proposed blocks

2. **Split-Brain Scenario**: The validator set effectively operates with incompatible leadership views, causing rounds to time out repeatedly

3. **State Inconsistency**: The on-chain `CommitHistory` (H2) diverges from consensus's operational history (H1-based), creating auditing and recovery challenges

4. **Cascading Failures**: As more validators restart, the network fragments into groups with different historical views, potentially requiring manual intervention

This qualifies as **High Severity** under "Significant protocol violations" and "Validator node slowdowns" categories. While not immediately causing fund loss, it severely degrades network availability and consensus safety.

## Likelihood Explanation

**Likelihood: MEDIUM**

Required conditions:
1. **Governance Compromise**: Attacker must pass a malicious governance proposal (requires significant voting power or exploit)
2. **Avoid Reconfiguration**: Proposal must avoid calling `reconfigure()` to stay within the same epoch
3. **Validator Restart**: At least one validator must crash/restart mid-epoch for divergence to manifest

Mitigating factors:
- Governance proposals are publicly visible and require voting period
- Most standard governance patterns include reconfiguration
- Validator restarts are relatively common (maintenance, crashes, upgrades)

Amplifying factors:
- Once one validator diverges, consensus stalls affect all validators
- The issue persists until epoch boundary, potentially lasting hours
- No automatic recovery mechanism exists

## Recommendation

**Immediate Fix**: Enforce epoch boundary on CommitHistory modifications

Add a check in the `block.move` module to prevent `CommitHistory` replacement without reconfiguration:

```move
// In block.move, add a friend-only function:
public(friend) fun require_commit_history_immutable_in_epoch() acquires CommitHistory {
    // This function should be called by any operation attempting to 
    // replace CommitHistory to ensure it only happens at epoch boundaries
    assert!(
        reconfiguration::is_in_reconfiguration() || !exists<CommitHistory>(@aptos_framework),
        error::invalid_state(ECOMMIT_HISTORY_MODIFICATION_REQUIRES_RECONFIGURATION)
    );
}
```

**Long-term Fix**: Implement version tracking for critical resources

Modify `CommitHistoryResource` to include an epoch version field:

```rust
// In types/src/on_chain_config/commit_history.rs
pub struct CommitHistoryResource {
    max_capacity: u32,
    next_idx: u32,
    table: TableWithLength,
    epoch_version: u64,  // NEW: Track which epoch this resource belongs to
}
```

And verify epoch consistency in `StorageAdapter::get_commit_history_resource()`:

```rust
// In consensus/src/dag/adapter.rs
fn get_commit_history_resource(&self, latest_version: u64) -> anyhow::Result<CommitHistoryResource> {
    let resource = bcs::from_bytes(/* ... */)?;
    ensure!(
        resource.epoch_version() == self.epoch,
        "CommitHistory epoch mismatch: expected {}, got {}",
        self.epoch,
        resource.epoch_version()
    );
    Ok(resource)
}
```

**Governance Policy**: Document that any proposal modifying `CommitHistory` MUST include `aptos_governance::reconfigure()` call.

## Proof of Concept

**Conceptual PoC** (Move + Rust integration test):

```move
// malicious_proposal.move
script {
    use aptos_framework::block;
    use std::signer;
    
    fun execute(governance_signer: &signer) {
        // Step 1: Extract old CommitHistory (requires VM privileges via writeset)
        // This would be done via WriteSetPayload::Direct in practice
        
        // Step 2: Create new empty CommitHistory with different capacity
        block::initialize_commit_history(governance_signer, 1000); // Different from default 2000
        
        // Step 3: Do NOT call aptos_governance::reconfigure()
        // Epoch remains the same, but resource is replaced
    }
}
```

**Rust validation test**:

```rust
// In consensus/src/dag/tests/integration_tests.rs
#[tokio::test]
async fn test_commit_history_replacement_causes_divergence() {
    // Setup: Create 4-node DAG consensus at epoch 1
    let mut nodes = create_dag_nodes(4).await;
    
    // Advance 100 blocks normally
    advance_blocks(&mut nodes, 100).await;
    
    // Simulate governance attack: Replace CommitHistory via writeset
    let malicious_writeset = create_commit_history_replacement_writeset();
    execute_writeset(&mut nodes, malicious_writeset).await;
    
    // Advance 50 more blocks (new events go to new table)
    advance_blocks(&mut nodes, 50).await;
    
    // Trigger: Restart node 0
    nodes[0].restart().await;
    
    // Validation: Node 0 now has different reputation state
    let node0_reputation = nodes[0].get_reputation_state();
    let node1_reputation = nodes[1].get_reputation_state();
    
    // These should be different (vulnerability)
    assert_ne!(node0_reputation, node1_reputation);
    
    // Impact: Consensus fails to make progress
    let result = try_advance_blocks(&mut nodes, 10).await;
    assert!(result.is_err()); // Liveness failure
}
```

**Notes**

This vulnerability fundamentally stems from the assumption that consensus's in-memory state remains synchronized with on-chain state throughout an epoch. The design does not account for malicious or erroneous mid-epoch modifications to critical system resources. The `table_handle()` method itself is not the issueâ€”rather, it's the broader architectural assumption that `CommitHistory` is immutable within an epoch.

The attack requires governance-level privileges but does not require validator collusion, making it a realistic threat model given Aptos's on-chain governance system where proposals can be submitted by any participant meeting minimum stake requirements.

### Citations

**File:** consensus/src/dag/bootstrap.rs (L467-479)
```rust
            AnchorElectionMode::LeaderReputation(reputation_type) => {
                let (commit_events, leader_reputation) = match reputation_type {
                    ProposerAndVoterV2(config) => {
                        let commit_events = self
                            .storage
                            .get_latest_k_committed_events(
                                std::cmp::max(
                                    config.proposer_window_num_validators_multiplier,
                                    config.voter_window_num_validators_multiplier,
                                ) as u64
                                    * self.epoch_state.verifier.len() as u64,
                            )
                            .expect("Failed to read commit events from storage");
```

**File:** consensus/src/dag/bootstrap.rs (L697-707)
```rust
    pub async fn start(
        self,
        mut dag_rpc_rx: Receiver<Author, IncomingDAGRequest>,
        mut shutdown_rx: oneshot::Receiver<oneshot::Sender<()>>,
    ) {
        info!(
            LogSchema::new(LogEvent::EpochStart),
            epoch = self.epoch_state.epoch,
        );

        let (base_state, handler, fetch_service) = self.full_bootstrap();
```

**File:** consensus/src/dag/adapter.rs (L326-339)
```rust
    fn get_commit_history_resource(
        &self,
        latest_version: u64,
    ) -> anyhow::Result<CommitHistoryResource> {
        Ok(bcs::from_bytes(
            self.aptos_db
                .get_state_value_by_version(
                    &StateKey::on_chain_config::<CommitHistoryResource>()?,
                    latest_version,
                )?
                .ok_or_else(|| format_err!("Resource doesn't exist"))?
                .bytes(),
        )?)
    }
```

**File:** consensus/src/dag/adapter.rs (L381-410)
```rust
    fn get_latest_k_committed_events(&self, k: u64) -> anyhow::Result<Vec<CommitEvent>> {
        let timer = counters::FETCH_COMMIT_HISTORY_DURATION.start_timer();
        let version = self.aptos_db.get_latest_ledger_info_version()?;
        let resource = self.get_commit_history_resource(version)?;
        let handle = resource.table_handle();
        let mut commit_events = vec![];
        for i in 1..=std::cmp::min(k, resource.length()) {
            let idx = (resource.next_idx() + resource.max_capacity() - i as u32)
                % resource.max_capacity();
            // idx is an u32, so it's not possible to fail to convert it to bytes
            let idx_bytes = bcs::to_bytes(&idx)
                .map_err(|e| anyhow::anyhow!("Failed to serialize index: {:?}", e))?;
            let state_value = self
                .aptos_db
                .get_state_value_by_version(&StateKey::table_item(handle, &idx_bytes), version)?
                .ok_or_else(|| anyhow::anyhow!("Table item doesn't exist"))?;
            let new_block_event = bcs::from_bytes::<NewBlockEvent>(state_value.bytes())
                .map_err(|e| anyhow::anyhow!("Failed to deserialize NewBlockEvent: {:?}", e))?;
            if self
                .epoch_to_validators
                .contains_key(&new_block_event.epoch())
            {
                commit_events.push(self.convert(new_block_event)?);
            }
        }
        let duration = timer.stop_and_record();
        info!("[DAG] fetch commit history duration: {} sec", duration);
        commit_events.reverse();
        Ok(commit_events)
    }
```

**File:** consensus/src/dag/anchor_election/leader_reputation_adapter.rs (L25-41)
```rust
pub struct MetadataBackendAdapter {
    epoch_to_validators: HashMap<u64, HashMap<Author, usize>>,
    window_size: usize,
    sliding_window: Mutex<BoundedVecDeque<CommitEvent>>,
}

impl MetadataBackendAdapter {
    pub fn new(
        window_size: usize,
        epoch_to_validators: HashMap<u64, HashMap<Author, usize>>,
    ) -> Self {
        Self {
            epoch_to_validators,
            window_size,
            sliding_window: Mutex::new(BoundedVecDeque::new(window_size)),
        }
    }
```

**File:** aptos-move/framework/aptos-framework/sources/block.move (L264-280)
```text
    fun emit_new_block_event(
        vm: &signer,
        event_handle: &mut EventHandle<NewBlockEvent>,
        new_block_event: NewBlockEvent,
    ) acquires CommitHistory {
        if (exists<CommitHistory>(@aptos_framework)) {
            let commit_history_ref = borrow_global_mut<CommitHistory>(@aptos_framework);
            let idx = commit_history_ref.next_idx;
            if (table_with_length::contains(&commit_history_ref.table, idx)) {
                table_with_length::remove(&mut commit_history_ref.table, idx);
            };
            table_with_length::add(&mut commit_history_ref.table, idx, copy new_block_event);
            spec {
                assume idx + 1 <= MAX_U32;
            };
            commit_history_ref.next_idx = (idx + 1) % commit_history_ref.max_capacity;
        };
```

**File:** consensus/src/dag/order_rule.rs (L186-194)
```rust
        let event = CommitEvent::new(
            anchor.id(),
            parents,
            failed_authors_and_rounds
                .iter()
                .map(|(_, author)| *author)
                .collect(),
        );
        self.anchor_election.update_reputation(event);
```
