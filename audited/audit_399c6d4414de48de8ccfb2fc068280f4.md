# Audit Report

## Title
Atomic Checkpoint Creation Failure in AptosDB Leads to Unrecoverable Checkpoint Corruption

## Summary
The `AptosDB::create_checkpoint()` function lacks atomicity when creating checkpoints across multiple database components (LedgerDb, StateKvDb, StateMerkleDb). If the checkpoint operation fails after successfully creating some but not all component checkpoints, a partial checkpoint remains on disk that cannot be used for node recovery, causing permanent availability loss.

## Finding Description

The `create_checkpoint()` function creates checkpoints for multiple database components sequentially without any transaction mechanism or cleanup-on-failure logic. [1](#0-0) 

The execution sequence is:
1. LedgerDb checkpoint creation (line 181)
2. StateKvDb checkpoint creation (line 183, if sharding enabled)
3. StateMerkleDb hot checkpoint creation (lines 184-189, if sharding enabled)
4. StateMerkleDb non-hot checkpoint creation (lines 191-196)

Each component's `create_checkpoint()` only removes its own subdirectory before creation: [2](#0-1) [3](#0-2) [4](#0-3) 

**Vulnerability**: If any checkpoint operation fails (e.g., StateKvDb fails due to disk full, I/O error, or system crash), earlier successful checkpoints remain on disk. This creates a **partial checkpoint** with inconsistent state.

When attempting to restore from this partial checkpoint, the node tries to open all database components. Missing components cause panics during shard opening: [5](#0-4) 

The checkpoint creation is called with `.expect()` which panics on failure but doesn't clean up partial state: [6](#0-5) 

**Broken Invariant**: This violates State Consistency Invariant #4 - checkpoint operations must be atomic. A checkpoint should either fully succeed or fully fail, never leaving partial state.

## Impact Explanation

This is **Critical Severity** under Aptos bug bounty criteria for the following reasons:

1. **Total loss of liveness/network availability**: A node cannot start from a corrupted checkpoint. The restoration attempt panics during database open, preventing node startup completely.

2. **Non-recoverable failure in disaster scenarios**: During disaster recovery, operators rely on checkpoints as the last resort. A corrupted checkpoint appearing valid but being unusable is catastrophic - it's worse than having no checkpoint at all because it provides false confidence in backup availability.

3. **No automatic recovery mechanism**: There is no built-in detection or cleanup of partial checkpoints. Manual intervention is required to identify and remove corrupted checkpoints.

4. **Silent corruption**: The checkpoint directory exists and appears normal until restoration is attempted. Operators may not discover the corruption until it's too late (during actual disaster recovery).

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability can be triggered by common operational scenarios:

1. **Disk space exhaustion**: Blockchain nodes frequently run low on disk space. If space runs out during checkpoint creation after LedgerDb succeeds but before StateKvDb/StateMerkleDb complete, partial checkpoint remains.

2. **I/O errors**: Hardware failures, network storage issues, or filesystem errors during checkpoint creation.

3. **System crashes**: Power failures, OOM kills, or operator interventions (SIGKILL) during checkpoint operations.

4. **Maintenance operations**: Checkpoint operations are I/O intensive and long-running, increasing the window for interruption.

The probability increases with:
- Number of database components (4 separate checkpoints must all succeed)
- Checkpoint size (larger checkpoints = longer operation = more failure opportunities)
- System resource constraints

## Recommendation

Implement atomic checkpoint creation with proper rollback on failure:

```rust
pub fn create_checkpoint(
    db_path: impl AsRef<Path>,
    cp_path: impl AsRef<Path>,
    sharding: bool,
) -> Result<()> {
    let start = Instant::now();
    info!(sharding = sharding, "Creating checkpoint for AptosDB.");
    
    // Create checkpoint in a temporary directory first
    let temp_cp_path = cp_path.as_ref().with_extension("tmp");
    std::fs::remove_dir_all(&temp_cp_path).unwrap_or(());
    std::fs::create_dir_all(&temp_cp_path)?;
    
    // Attempt all checkpoint operations to temp location
    let result = (|| -> Result<()> {
        LedgerDb::create_checkpoint(db_path.as_ref(), &temp_cp_path, sharding)?;
        if sharding {
            StateKvDb::create_checkpoint(db_path.as_ref(), &temp_cp_path)?;
            StateMerkleDb::create_checkpoint(
                db_path.as_ref(),
                &temp_cp_path,
                sharding,
                /* is_hot = */ true,
            )?;
        }
        StateMerkleDb::create_checkpoint(
            db_path.as_ref(),
            &temp_cp_path,
            sharding,
            /* is_hot = */ false,
        )?;
        Ok(())
    })();
    
    // Only move to final location if all operations succeeded
    match result {
        Ok(()) => {
            // Atomic rename from temp to final location
            std::fs::remove_dir_all(cp_path.as_ref()).unwrap_or(());
            std::fs::rename(&temp_cp_path, cp_path.as_ref())?;
            info!(
                db_path = db_path.as_ref(),
                cp_path = cp_path.as_ref(),
                time_ms = %start.elapsed().as_millis(),
                "Made AptosDB checkpoint."
            );
            Ok(())
        },
        Err(e) => {
            // Clean up partial checkpoint on failure
            std::fs::remove_dir_all(&temp_cp_path).unwrap_or(());
            Err(e)
        }
    }
}
```

## Proof of Concept

```rust
// Reproduction steps:
// 1. Setup a test AptosDB with sharding enabled
// 2. Simulate failure during StateKvDb checkpoint creation
// 3. Verify partial checkpoint remains on disk
// 4. Attempt to open from partial checkpoint
// 5. Observe panic

#[test]
fn test_partial_checkpoint_corruption() {
    use std::fs;
    use tempfile::tempdir;
    
    // Setup source database
    let source_dir = tempdir().unwrap();
    let checkpoint_dir = tempdir().unwrap();
    
    // Create a functional AptosDB
    let db = AptosDB::new_for_test(source_dir.path());
    // ... commit some transactions ...
    
    // Simulate partial checkpoint by:
    // 1. Creating LedgerDb checkpoint manually
    LedgerDb::create_checkpoint(
        source_dir.path(), 
        checkpoint_dir.path(), 
        true
    ).unwrap();
    
    // 2. NOT creating StateKvDb/StateMerkleDb checkpoints
    // (simulates failure after LedgerDb succeeds)
    
    // 3. Verify LedgerDb checkpoint exists
    assert!(checkpoint_dir.path().join("ledger_db").exists());
    
    // 4. Verify StateKvDb checkpoint does NOT exist
    assert!(!checkpoint_dir.path().join("state_kv_db").exists());
    
    // 5. Attempt to open from partial checkpoint - should panic
    let result = std::panic::catch_unwind(|| {
        AptosDB::open(
            StorageDirPaths::from_path(checkpoint_dir.path()),
            false,
            PrunerConfig::default(),
            RocksdbConfigs {
                enable_storage_sharding: true,
                ..Default::default()
            },
            false,
            1000,
            1000,
            None,
            HotStateConfig::default(),
        )
    });
    
    // Node panics when trying to open partial checkpoint
    assert!(result.is_err(), "Expected panic when opening partial checkpoint");
}
```

**Notes**

This vulnerability represents a critical failure in checkpoint atomicity that can cause permanent node unavailability. The issue is exacerbated by the fact that partial checkpoints appear valid (directory exists) until restoration is attempted. The recommended fix uses a two-phase commit approach: create all checkpoints in a temporary location, then atomically rename to the final location only if all operations succeed. This ensures checkpoints are either fully valid or don't exist at all.

### Citations

**File:** storage/aptosdb/src/db/mod.rs (L172-205)
```rust
    pub fn create_checkpoint(
        db_path: impl AsRef<Path>,
        cp_path: impl AsRef<Path>,
        sharding: bool,
    ) -> Result<()> {
        let start = Instant::now();

        info!(sharding = sharding, "Creating checkpoint for AptosDB.");

        LedgerDb::create_checkpoint(db_path.as_ref(), cp_path.as_ref(), sharding)?;
        if sharding {
            StateKvDb::create_checkpoint(db_path.as_ref(), cp_path.as_ref())?;
            StateMerkleDb::create_checkpoint(
                db_path.as_ref(),
                cp_path.as_ref(),
                sharding,
                /* is_hot = */ true,
            )?;
        }
        StateMerkleDb::create_checkpoint(
            db_path.as_ref(),
            cp_path.as_ref(),
            sharding,
            /* is_hot = */ false,
        )?;

        info!(
            db_path = db_path.as_ref(),
            cp_path = cp_path.as_ref(),
            time_ms = %start.elapsed().as_millis(),
            "Made AptosDB checkpoint."
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L336-343)
```rust
        std::fs::remove_dir_all(&cp_ledger_db_folder).unwrap_or(());
        if sharding {
            std::fs::create_dir_all(&cp_ledger_db_folder).unwrap_or(());
        }

        ledger_db
            .metadata_db()
            .create_checkpoint(Self::metadata_db_path(cp_root_path.as_ref(), sharding))?;
```

**File:** storage/aptosdb/src/state_kv_db.rs (L107-125)
```rust
        let state_kv_db_shards = (0..NUM_STATE_SHARDS)
            .into_par_iter()
            .map(|shard_id| {
                let shard_root_path = db_paths.state_kv_db_shard_root_path(shard_id);
                let db = Self::open_shard(
                    shard_root_path,
                    shard_id,
                    &state_kv_db_config,
                    env,
                    block_cache,
                    readonly,
                    /* is_hot = */ false,
                )
                .unwrap_or_else(|e| panic!("Failed to open state kv db shard {shard_id}: {e:?}."));
                Arc::new(db)
            })
            .collect::<Vec<_>>()
            .try_into()
            .unwrap();
```

**File:** storage/aptosdb/src/state_kv_db.rs (L240-245)
```rust
        std::fs::remove_dir_all(&cp_state_kv_db_path).unwrap_or(());
        std::fs::create_dir_all(&cp_state_kv_db_path).unwrap_or(());

        state_kv_db
            .metadata_db()
            .create_checkpoint(Self::metadata_db_path(cp_root_path.as_ref()))?;
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L217-228)
```rust
        std::fs::remove_dir_all(&cp_state_merkle_db_path).unwrap_or(());
        if sharding {
            std::fs::create_dir_all(&cp_state_merkle_db_path).unwrap_or(());
        }

        state_merkle_db
            .metadata_db()
            .create_checkpoint(Self::metadata_db_path(
                cp_root_path.as_ref(),
                sharding,
                is_hot,
            ))?;
```

**File:** aptos-node/src/storage.rs (L150-155)
```rust
    AptosDB::create_checkpoint(
        &source_dir,
        &checkpoint_dir,
        node_config.storage.rocksdb_configs.enable_storage_sharding,
    )
    .expect("AptosDB checkpoint creation failed.");
```
