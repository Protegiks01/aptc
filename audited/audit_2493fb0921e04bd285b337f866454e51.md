# Audit Report

## Title
Unhandled gRPC Connection Failure in Remote Executor Causes Permanent Validator Hang

## Summary
The `GRPCNetworkMessageServiceClientWrapper` uses lazy connection establishment via `connect_lazy()` without error handling, and `send_message()` panics on any connection failure. When used in the remote executor pipeline, this panic kills the outbound handler task, causing the validator to hang indefinitely waiting for responses that never arrive, resulting in total loss of validator liveness.

## Finding Description

The vulnerability exists in the gRPC network service layer used by the remote sharded block executor. The connection is established lazily, meaning the actual network connection is not attempted until the first message is sent. [1](#0-0) 

When the first message is sent and the connection fails (due to network issues, remote shard unavailability, or misconfiguration), the error is not handled gracefully: [2](#0-1) 

The panic occurs within the outbound handler's async task: [3](#0-2) 

When this task panics, it is caught by the Tokio runtime and the task is terminated, but the main process continues. However, the executor is left waiting indefinitely for responses: [4](#0-3) 

The `rx.recv().unwrap()` call blocks forever because the outbound handler (which would send responses back) is dead. This blocks the entire executor workflow: [5](#0-4) 

**Exploitation Path:**
1. Validator configured with remote executor shards
2. Network failure, DNS issue, or remote shard unavailability occurs
3. First block execution attempt triggers connection via `send_message()`
4. Connection fails, panic occurs in outbound handler task
5. Task dies but validator process continues
6. `get_output_from_shards()` blocks indefinitely on `rx.recv()`
7. Validator cannot process any more blocks - complete loss of liveness

The codebase itself acknowledges this issue via TODO comments indicating missing retry logic and the need for proper error handling. [6](#0-5) 

## Impact Explanation

**Severity: HIGH**

This vulnerability causes **total loss of validator liveness** without crashing the process. According to the Aptos bug bounty criteria, this qualifies as HIGH severity under "Validator node slowdowns" and approaches CRITICAL under "Total loss of liveness/network availability."

**Specific Impact:**
- Affected validator becomes permanently unresponsive and cannot process blocks
- No automatic recovery mechanism (process doesn't crash, so no restart)
- Monitoring systems may not immediately detect the hang (process still alive)
- Requires manual intervention to diagnose and restart
- If multiple validators are affected, network liveness is severely impacted
- Violates the consensus liveness invariant requiring validators to participate in block production

This is worse than a crash because there is no automatic recovery, and the validator appears alive while being completely non-functional.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability has a high likelihood of occurrence because:

1. **Common Triggers:**
   - Transient network failures (packet loss, routing issues)
   - DNS resolution failures
   - Remote shard crashes or restarts
   - Firewall rule changes
   - Kubernetes pod rescheduling in cloud deployments
   - Configuration errors during deployment

2. **Race Condition at Startup:**
   - Remote executor shards may not start simultaneously
   - First block execution could attempt connection before shards are ready
   - The test code explicitly requires a sleep to avoid this race condition

3. **No Defense Mechanisms:**
   - No connection timeout configured
   - No retry logic implemented
   - No circuit breaker pattern
   - No health checks before sending

4. **Production Deployment Reality:**
   - Distributed systems commonly experience transient network issues
   - Multi-region or multi-datacenter deployments increase network failure probability
   - Cloud infrastructure updates can cause temporary connectivity loss

The TODO comments in the codebase confirm this is a known gap that has not been addressed.

## Recommendation

Implement comprehensive error handling with retries and timeouts:

1. **Add connection timeout and retry logic in `send_message()`:**
```rust
pub async fn send_message(
    &mut self,
    sender_addr: SocketAddr,
    message: Message,
    mt: &MessageType,
) -> Result<(), tonic::Status> {
    let request = tonic::Request::new(NetworkMessage {
        message: message.data,
        message_type: mt.get_type(),
    });
    
    // Retry with exponential backoff
    let mut retries = 0;
    const MAX_RETRIES: u32 = 3;
    const BASE_DELAY_MS: u64 = 100;
    
    loop {
        match tokio::time::timeout(
            std::time::Duration::from_secs(5),
            self.remote_channel.simple_msg_exchange(request.clone())
        ).await {
            Ok(Ok(_)) => return Ok(()),
            Ok(Err(e)) | Err(_) if retries < MAX_RETRIES => {
                retries += 1;
                let delay = BASE_DELAY_MS * 2_u64.pow(retries - 1);
                error!(
                    "Error sending message to {} (attempt {}/{}): {}. Retrying in {}ms",
                    self.remote_addr, retries, MAX_RETRIES, e, delay
                );
                tokio::time::sleep(std::time::Duration::from_millis(delay)).await;
            },
            Ok(Err(e)) | Err(_) => {
                error!(
                    "Failed to send message to {} after {} retries",
                    self.remote_addr, MAX_RETRIES
                );
                return Err(e);
            }
        }
    }
}
```

2. **Add timeout to receive operations in `get_output_from_shards()`:**
```rust
fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
    trace!("RemoteExecutorClient Waiting for results");
    let mut results = vec![];
    for rx in self.result_rxs.iter() {
        let received_bytes = rx.recv_timeout(std::time::Duration::from_secs(30))
            .map_err(|e| VMStatus::Error(StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR))?
            .to_bytes();
        let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes)
            .map_err(|_| VMStatus::Error(StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR))?;
        results.push(result.inner?);
    }
    Ok(results)
}
```

3. **Handle errors gracefully in `outbound_handler` instead of panicking:** [7](#0-6) 

Replace the unwrap with proper error logging and continue operation.

4. **Implement health checks before attempting to send messages**

5. **Add connection pre-warming during initialization instead of relying on lazy connections**

## Proof of Concept

```rust
#[test]
fn test_connection_failure_causes_hang() {
    use aptos_config::utils;
    use std::{
        net::{IpAddr, Ipv4Addr, SocketAddr},
        time::Duration,
    };

    // Create a client pointing to a non-existent server
    let nonexistent_addr = SocketAddr::new(
        IpAddr::V4(Ipv4Addr::new(192, 0, 2, 1)), // TEST-NET-1 (unreachable)
        9999
    );

    let rt = Runtime::new().unwrap();
    let mut grpc_client = GRPCNetworkMessageServiceClientWrapper::new(&rt, nonexistent_addr);

    let client_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), utils::get_available_port());
    let test_message = Message::new("test".as_bytes().to_vec());
    let message_type = MessageType::new("test_type".to_string());

    // This will panic when the connection fails, killing the task
    // In production, this would hang the validator
    let send_future = async {
        grpc_client.send_message(client_addr, test_message, &message_type).await;
    };

    // Attempt to send with timeout to demonstrate the hang/panic
    let result = rt.block_on(async {
        tokio::time::timeout(Duration::from_secs(2), send_future).await
    });

    // This demonstrates the panic would occur (in real scenario)
    // or timeout if the panic handler catches it
    assert!(result.is_err(), "Should timeout or panic on connection failure");
}
```

**Notes:**
- The vulnerability is confirmed by TODO comments in the codebase acknowledging missing retry logic
- The existing test requires a sleep workaround to avoid this exact issue
- Production deployments using remote executors are at risk
- The issue affects validator liveness, a critical invariant for blockchain operation
- No privilege escalation required - environmental conditions trigger the bug

### Citations

**File:** secure/net/src/grpc_network_service/mod.rs (L132-138)
```rust
    async fn get_channel(remote_addr: String) -> NetworkMessageServiceClient<Channel> {
        info!("Trying to connect to remote server at {:?}", remote_addr);
        let conn = tonic::transport::Endpoint::new(remote_addr)
            .unwrap()
            .connect_lazy();
        NetworkMessageServiceClient::new(conn).max_decoding_message_size(MAX_MESSAGE_SIZE)
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L150-160)
```rust
        // TODO: Retry with exponential backoff on failures
        match self.remote_channel.simple_msg_exchange(request).await {
            Ok(_) => {},
            Err(e) => {
                panic!(
                    "Error '{}' sending message to {} on node {:?}",
                    e, self.remote_addr, sender_addr
                );
            },
        }
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L198-201)
```rust
    // wait for the server to be ready before sending messages
    // TODO: We need to implement retry on send_message failures such that we can pass this test
    //       without this sleep
    thread::sleep(std::time::Duration::from_millis(10));
```

**File:** secure/net/src/network_controller/outbound_handler.rs (L89-99)
```rust
        rt.spawn(async move {
            info!("Starting outbound handler at {}", address.to_string());
            Self::process_one_outgoing_message(
                outbound_handlers,
                &address,
                inbound_handler.clone(),
                &mut grpc_clients,
            )
            .await;
            info!("Stopping outbound handler at {}", address.to_string());
        });
```

**File:** secure/net/src/network_controller/outbound_handler.rs (L155-160)
```rust
                grpc_clients
                    .get_mut(remote_addr)
                    .unwrap()
                    .send_message(*socket_addr, msg, message_type)
                    .await;
            }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```
