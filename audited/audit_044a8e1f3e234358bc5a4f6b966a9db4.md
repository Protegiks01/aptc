# Audit Report

## Title
Non-Atomic Progress Recording in StateKvPruner Causes State Inconsistency on Partial Batch Failure

## Summary
The `StateKvPruner::prune()` function updates progress in the metadata database before completing shard-level pruning operations. When shard pruning fails after metadata progress has been committed, the system enters an inconsistent state where the global progress marker indicates data has been pruned that still exists in some shards. This violates atomicity guarantees and can cause node startup failures or incorrect query rejections.

## Finding Description

The vulnerability exists in the pruning workflow for sharded state key-value storage. The `StateKvPruner::prune()` function performs a multi-step operation: [1](#0-0) 

Within each batch iteration, the function:
1. Calls `metadata_pruner.prune()` which atomically updates `DbMetadataKey::StateKvPrunerProgress` in the metadata database
2. Then calls shard pruners in parallel, each updating their own `DbMetadataKey::StateKvShardPrunerProgress(shard_id)`
3. Only if all steps succeed does it record progress in-memory at line 81

The critical flaw is in the `StateKvMetadataPruner::prune()` implementation: [2](#0-1) 

This function commits `StateKvPrunerProgress = target_version` to the metadata database (lines 67-72) as part of its atomic batch. However, if subsequent shard pruning fails, there's **no rollback mechanism** for this metadata database write.

Each shard pruner similarly commits its progress atomically: [3](#0-2) 

**Attack Scenario:**
1. Pruner attempts to prune versions 1000-1500
2. `metadata_pruner.prune(1000, 1500)` succeeds → writes `StateKvPrunerProgress = 1500`
3. Shard 0 pruning succeeds → writes `StateKvShardPrunerProgress[0] = 1500`
4. Shard 1 pruning fails (disk error, bug, resource exhaustion) → `StateKvShardPrunerProgress[1]` remains at 1000
5. Error propagates, line 81 never executes → in-memory `progress` remains at 1000

**Result:** The metadata database claims pruning reached version 1500, but Shard 1 still contains stale data from versions 1000-1500.

This inconsistency has cascading effects:
- The `min_readable_version` is derived from `StateKvPrunerProgress` during initialization: [4](#0-3) 

- Read operations check against this version: [5](#0-4) 

This means queries for versions 1000-1499 will be rejected as "pruned" even though the data exists in Shard 1.

On restart, the system attempts recovery: [6](#0-5) 

If catch-up pruning fails again (persistent disk issue), the node cannot start.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per the Aptos bug bounty criteria: "State inconsistencies requiring intervention."

**Specific Impacts:**
1. **State Consistency Violation**: Breaks the invariant that "State transitions must be atomic and verifiable via Merkle proofs" - the pruning operation is not atomic across shards
2. **Availability Issues**: Queries for unpruned data may be incorrectly rejected with "data has been pruned" errors
3. **Node Liveness Failure**: If catch-up pruning fails on restart, the node cannot initialize, requiring manual intervention
4. **Operational Risk**: Nodes in different states may have different views of data availability during the inconsistency window

While this does not directly lead to consensus violations or fund loss (no attacker can directly exploit it for financial gain), it creates operational hazards that can affect network reliability and require manual intervention to resolve.

## Likelihood Explanation

**Likelihood: Medium to High**

This issue can occur naturally without malicious intent:
- **Transient disk errors** during shard writes (hardware failures, I/O timeouts)
- **Resource exhaustion** (disk full, memory pressure causing write failures)
- **Software bugs** in pruning logic or database layer
- **Concurrent operations** causing locks or conflicts

The likelihood increases with:
- Number of shards (more shards = more failure points)
- System load and I/O pressure
- Storage infrastructure reliability
- Long-running validator nodes experiencing hardware degradation

Evidence from the codebase shows awareness of failure modes (error handling with `?` operators), but the lack of transactional semantics across the metadata update and shard operations creates a window for inconsistency.

## Recommendation

**Implement Two-Phase Commit for Progress Updates:**

The progress marker in the metadata database should only be updated AFTER all shard pruning operations complete successfully. Modify the `prune()` function:

```rust
// In StateKvPruner::prune()
pub fn prune(&self, max_versions: usize) -> Result<Version> {
    let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_pruner__prune"]);
    
    let mut progress = self.progress();
    let target_version = self.target_version();
    
    while progress < target_version {
        let current_batch_target_version =
            min(progress + max_versions as Version, target_version);
        
        info!(
            progress = progress,
            target_version = current_batch_target_version,
            "Pruning state kv data."
        );
        
        // Phase 1: Prune data from all shards WITHOUT updating metadata progress
        self.metadata_pruner
            .prune_data_only(progress, current_batch_target_version)?;
        
        THREAD_MANAGER.get_background_pool().install(|| {
            self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                shard_pruner
                    .prune(progress, current_batch_target_version)
                    .map_err(|err| {
                        anyhow!(
                            "Failed to prune state kv shard {}: {err}",
                            shard_pruner.shard_id(),
                        )
                    })
            })
        })?;
        
        // Phase 2: Only update metadata progress after all shards succeed
        self.metadata_pruner
            .update_progress_only(current_batch_target_version)?;
        
        progress = current_batch_target_version;
        self.record_progress(progress);
        info!(progress = progress, "Pruning state kv data is done.");
    }
    
    Ok(target_version)
}
```

Split `StateKvMetadataPruner::prune()` into two methods:
- `prune_data_only()`: Performs data deletion without updating progress
- `update_progress_only()`: Updates progress marker only after all operations succeed

This ensures atomicity: either all operations complete and progress advances, or none do and progress remains unchanged.

**Alternative: Use Database Transactions**

Wrap the entire batch operation (metadata + all shards) in a distributed transaction if the database layer supports it, or implement compensating transactions to rollback metadata progress on shard failure.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_temppath::TempPath;
    use aptos_schemadb::DB;
    use std::sync::Arc;

    #[test]
    fn test_pruner_inconsistent_state_on_shard_failure() {
        // Setup: Create StateKvDb with sharding enabled
        let tmpdir = TempPath::new();
        let db_config = RocksdbConfig::default();
        let state_kv_db = StateKvDb::new(...); // Initialize with sharding
        
        // Create pruner
        let pruner = StateKvPruner::new(Arc::new(state_kv_db)).unwrap();
        
        // Initial state: progress = 100
        assert_eq!(pruner.progress(), 100);
        
        // Inject failure in shard 1 by making it read-only or corrupting it
        let shard1_path = tmpdir.path().join("state_kv_db/1");
        std::fs::set_permissions(&shard1_path, 
            std::fs::Permissions::from_mode(0o444)).unwrap();
        
        // Set target and attempt pruning
        pruner.set_target_version(200);
        let result = pruner.prune(100);
        
        // Pruning should fail
        assert!(result.is_err());
        
        // BUG: Check inconsistent state
        // In-memory progress should still be 100 (line 81 not reached)
        assert_eq!(pruner.progress(), 100);
        
        // But metadata DB shows progress = 200 (metadata_pruner succeeded)
        let metadata_progress = pruner_utils::get_state_kv_pruner_progress(
            &state_kv_db
        ).unwrap();
        assert_eq!(metadata_progress, 200); // INCONSISTENT!
        
        // Shard 0 should show progress = 200 (succeeded)
        let shard0_progress = get_or_initialize_subpruner_progress(
            state_kv_db.db_shard(0),
            &DbMetadataKey::StateKvShardPrunerProgress(0),
            0
        ).unwrap();
        assert_eq!(shard0_progress, 200);
        
        // Shard 1 should show progress = 100 (failed)
        let shard1_progress = get_or_initialize_subpruner_progress(
            state_kv_db.db_shard(1),
            &DbMetadataKey::StateKvShardPrunerProgress(1),
            0
        ).unwrap();
        assert_eq!(shard1_progress, 100); // INCONSISTENT!
        
        // Demonstrate impact: restart simulation
        drop(pruner);
        
        // On restart, if shard 1 catch-up fails, initialization fails
        let result = StateKvPruner::new(Arc::new(state_kv_db));
        assert!(result.is_err()); // Node cannot start!
    }
}
```

This test demonstrates the inconsistency: the metadata database records progress at 200, but shard 1 remains at 100, and the in-memory state shows 100. On restart, the catch-up mechanism fails, preventing node initialization.

## Notes

This vulnerability is particularly concerning for production deployments because:

1. **Silent Inconsistency**: The error is logged but the inconsistent state persists in the database until restart or successful retry
2. **Cascade Failures**: Once one shard fails, subsequent pruning attempts may also fail if the underlying issue persists
3. **Recovery Complexity**: Manual intervention may be required to restore consistency if automated catch-up fails
4. **Monitoring Blind Spot**: The metrics show different progress values (metadata vs. in-memory) but this discrepancy may not trigger alerts

The fix requires careful consideration of the atomicity boundaries and proper two-phase commit semantics to ensure the metadata progress marker accurately reflects the actual pruned state across all shards.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L49-86)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_pruner__prune"]);

        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning state kv data."
            );
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning state kv data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L28-73)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        if self.state_kv_db.enabled_sharding() {
            let num_shards = self.state_kv_db.num_shards();
            // NOTE: This can be done in parallel if it becomes the bottleneck.
            for shard_id in 0..num_shards {
                let mut iter = self
                    .state_kv_db
                    .db_shard(shard_id)
                    .iter::<StaleStateValueIndexByKeyHashSchema>()?;
                iter.seek(&current_progress)?;
                for item in iter {
                    let (index, _) = item?;
                    if index.stale_since_version > target_version {
                        break;
                    }
                }
            }
        } else {
            let mut iter = self
                .state_kv_db
                .metadata_db()
                .iter::<StaleStateValueIndexSchema>()?;
            iter.seek(&current_progress)?;
            for item in iter {
                let (index, _) = item?;
                if index.stale_since_version > target_version {
                    break;
                }
                batch.delete::<StaleStateValueIndexSchema>(&index)?;
                batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
            }
        }

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        self.state_kv_db.metadata_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L25-45)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L94-95)
```rust
        let min_readable_version =
            pruner_utils::get_state_kv_pruner_progress(&state_kv_db).expect("Must succeed.");
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L305-315)
```rust
    pub(super) fn error_if_state_kv_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.state_store.state_kv_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```
