# Audit Report

## Title
Cache Failure State Vulnerability Enables Provider Execution Amplification in Node-Checker

## Summary
The `OutputCache::get()` function in the node-checker's provider caching system fails to update cache metadata when provider operations fail, allowing attackers controlling target nodes to force repeated expensive provider re-executions within the cache TTL window, bypassing cache protection and causing severe performance degradation.

## Finding Description

The `OutputCache` implementation contains a critical flaw in its failure handling that enables cache bypass attacks. The vulnerability exists in the interaction between cache initialization, failure handling, and subsequent cache lookups. [1](#0-0) 

When an `OutputCache` is created, `last_run` is initialized to `Instant::now()` but `last_output` is set to `None`. This creates a vulnerable initial state. [2](#0-1) 

The `get()` function implements a two-stage cache check:
1. Lines 40-44: Check if cache is valid (elapsed < TTL) AND has a value
2. Lines 48-52: If check fails, acquire write locks and execute provider

**The Critical Flaw:** When the provider function fails at line 50, the `?` operator causes an early return. Lines 51-52 never execute, meaning:
- `last_output` remains `None` (or its previous value)
- `last_run` is NOT updated to the current time

On subsequent calls within the TTL window:
- Line 40 check passes: `last_run.elapsed() < cache_ttl` is `true`
- Line 41 check fails: `last_output` is still `None`
- Provider executes again despite being within the cache TTL period

**Attack Vector:**

An attacker controlling the target node being health-checked can deliberately cause provider failures by:
- Returning HTTP errors (4xx, 5xx status codes)
- Timing out requests
- Sending malformed responses
- Being slow to respond [3](#0-2) 

Each provider makes network requests to the target node. When these fail, they return `ProviderError::RetryableEndpointError`, which propagates through the cache without updating its state.

**Amplification via Retry Logic:** [4](#0-3) 

The node-checker's retry mechanism (default: 0 retries, configurable up to 255) multiplies the impact. With `num_retries=3` and `retry_delay_secs=2`:
1. Initial attempt: Provider executes, fails (0-4 seconds)
2. Wait 2 seconds, retry attempt 1: Cache lookup fails, provider executes again, fails
3. Wait 2 seconds, retry attempt 2: Same
4. Wait 2 seconds, retry attempt 3: Same

A single checker failure triggers 4 provider executions instead of 1, with 6+ seconds of delays.

**Concurrent Request Amplification:** [5](#0-4) 

Each `/check` request creates fresh provider instances for the target node. Multiple concurrent requests checking the same malicious node each create independent caches, all experiencing the same bypass, resulting in O(requests Ã— retries) provider executions. [6](#0-5) 

With the default cache TTL of 1000ms, an attacker can sustain this attack for the entire duration of each health check request.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos Bug Bounty program criteria:

1. **Validator Node Slowdowns**: The node-checker is used to evaluate validator and fullnode health. Performance degradation of this critical infrastructure component affects the ability to monitor and maintain network quality.

2. **API Crashes**: Under sufficient load with concurrent requests and retry amplification, the node-checker service can experience resource exhaustion (thread pool saturation, memory pressure, network socket exhaustion).

3. **Service Degradation**: Health check responses become unreliable or unavailable, impacting:
   - Validator qualification processes (AIT programs)
   - Node monitoring infrastructure
   - Operational decision-making for node operators

The impact is amplified by:
- Each failed check can trigger 4+ provider executions (with default retry config)
- Multiple concurrent requests multiply the effect
- All checkers within a request that share the same provider are affected
- The default 1-second cache TTL provides a sustained attack window

## Likelihood Explanation

**Likelihood: HIGH**

The attack is trivially exploitable:
1. **No special privileges required**: Any node operator can control their own node's responses
2. **Trivial to execute**: Simply configure the target node to return errors or timeout
3. **No detection complexity**: The attacker's malicious node is the intended target of the health check
4. **Natural occurrence**: Legitimate node misconfigurations can trigger this behavior unintentionally

**Attacker Requirements:**
- Control over a node being health-checked (their own node)
- Basic ability to configure HTTP responses or cause request failures
- No cryptographic operations or complex exploitation needed

**Real-World Scenarios:**
- Malicious actors during validator incentivized testnet programs
- Nodes under attack trying to hide their degraded state
- Accidental triggering by misconfigured nodes
- Deliberate DoS against node-checker infrastructure

## Recommendation

Implement proper cache state management on provider failures with a double-checked locking pattern:

```rust
pub async fn get(
    &self,
    func: impl Future<Output = Result<T, ProviderError>>,
) -> Result<T, ProviderError> {
    // First check with read locks
    if self.last_run.read().await.elapsed() < self.cache_ttl {
        if let Some(last_output) = &*self.last_output.read().await {
            return Ok(last_output.clone());
        }
    }

    // Acquire write locks
    let mut last_output = self.last_output.write().await;
    let mut last_run = self.last_run.write().await;
    
    // Double-check after acquiring locks (prevents cache stampede)
    if last_run.elapsed() < self.cache_ttl {
        if let Some(cached) = &*last_output {
            return Ok(cached.clone());
        }
    }

    // Execute provider
    let result = func.await;
    
    // CRITICAL: Update last_run regardless of success/failure
    *last_run = Instant::now();
    
    // Only update last_output on success
    match result {
        Ok(new_output) => {
            *last_output = Some(new_output.clone());
            Ok(new_output)
        }
        Err(e) => Err(e),
    }
}
```

**Key Changes:**
1. Always update `last_run` after provider execution, even on failure
2. Add double-check after acquiring write locks to prevent cache stampede
3. Only update `last_output` on successful provider execution
4. This ensures failed providers are rate-limited by the cache TTL

**Additional Hardening:**
Consider implementing exponential backoff for consecutive failures to further mitigate amplification attacks.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic::{AtomicU32, Ordering};
    use std::sync::Arc;

    #[tokio::test]
    async fn test_cache_bypass_on_provider_failure() {
        let cache = OutputCache::<String>::new(Duration::from_secs(5));
        let execution_count = Arc::new(AtomicU32::new(0));
        
        // Simulate a provider that always fails
        let failing_provider = || {
            let counter = execution_count.clone();
            async move {
                counter.fetch_add(1, Ordering::SeqCst);
                Err(ProviderError::RetryableEndpointError(
                    "/test",
                    anyhow::anyhow!("Simulated failure"),
                ))
            }
        };

        // First call - should execute provider
        let _ = cache.get(failing_provider()).await;
        assert_eq!(execution_count.load(Ordering::SeqCst), 1);

        // Second call within TTL - SHOULD use cache but DOESN'T due to bug
        let _ = cache.get(failing_provider()).await;
        assert_eq!(execution_count.load(Ordering::SeqCst), 2); // BUG: Should be 1

        // Third call within TTL - same issue
        let _ = cache.get(failing_provider()).await;
        assert_eq!(execution_count.load(Ordering::SeqCst), 3); // BUG: Should be 1

        // With 3 calls in quick succession, provider executed 3 times
        // Expected with proper caching: 1 execution + 2 cache hits
        // Actual behavior: 3 executions (cache completely bypassed)
        println!("Provider executed {} times (expected 1 with caching)", 
                 execution_count.load(Ordering::SeqCst));
    }

    #[tokio::test]
    async fn test_concurrent_request_amplification() {
        let cache = Arc::new(OutputCache::<String>::new(Duration::from_secs(5)));
        let execution_count = Arc::new(AtomicU32::new(0));
        
        let mut handles = vec![];
        
        // Simulate 10 concurrent requests
        for _ in 0..10 {
            let cache_clone = cache.clone();
            let counter_clone = execution_count.clone();
            
            let handle = tokio::spawn(async move {
                let _ = cache_clone.get(async move {
                    counter_clone.fetch_add(1, Ordering::SeqCst);
                    tokio::time::sleep(Duration::from_millis(100)).await;
                    Err(ProviderError::RetryableEndpointError(
                        "/test",
                        anyhow::anyhow!("Simulated failure"),
                    ))
                }).await;
            });
            
            handles.push(handle);
        }
        
        for handle in handles {
            handle.await.unwrap();
        }
        
        // With cache stampede: 10 threads all execute provider
        // Expected with double-check locking: 1 execution
        println!("Concurrent executions: {} (expected 1 with proper locking)", 
                 execution_count.load(Ordering::SeqCst));
        assert!(execution_count.load(Ordering::SeqCst) > 1); // Demonstrates stampede
    }
}
```

## Notes

This vulnerability demonstrates two related cache design flaws:

1. **Failure State Management**: The cache doesn't distinguish between "no data yet" and "data fetch failed", treating both as cache misses. This violates the principle that caches should rate-limit expensive operations regardless of success/failure.

2. **Cache Stampede (TOCTOU)**: The lack of double-checked locking allows multiple threads to simultaneously execute the provider when the cache expires, defeating the purpose of the cache under concurrent load.

While the node-checker is not a consensus-critical component, it is essential infrastructure for network health monitoring and validator qualification processes. Performance degradation or unavailability of this service can impact the broader ecosystem's ability to maintain network quality and onboard new validators safely.

### Citations

**File:** ecosystem/node-checker/src/provider/cache.rs (L25-31)
```rust
    pub fn new(cache_ttl: Duration) -> Self {
        Self {
            cache_ttl,
            last_run: RwLock::new(Instant::now()),
            last_output: RwLock::new(None),
        }
    }
```

**File:** ecosystem/node-checker/src/provider/cache.rs (L35-54)
```rust
    pub async fn get(
        &self,
        func: impl Future<Output = Result<T, ProviderError>>,
    ) -> Result<T, ProviderError> {
        // If the cache isn't too old and there is a value, return it.
        if self.last_run.read().await.elapsed() < self.cache_ttl {
            if let Some(last_output) = &*self.last_output.read().await {
                return Ok(last_output.clone());
            }
        }

        // Otherwise fetch the value and update the cache. We take the locks while
        // fetching the new value so we don't waste effort fetching it multiple times.
        let mut last_output = self.last_output.write().await;
        let mut last_run = self.last_run.write().await;
        let new_output = func.await?;
        *last_output = Some(new_output.clone());
        *last_run = Instant::now();
        Ok(new_output)
    }
```

**File:** ecosystem/node-checker/src/provider/api_index.rs (L55-64)
```rust
    async fn provide(&self) -> Result<Self::Output, ProviderError> {
        self.output_cache
            .get(
                self.client
                    .get_index()
                    .map_ok(|r| r.into_inner())
                    .map_err(|e| ProviderError::RetryableEndpointError("/", e.into())),
            )
            .await
    }
```

**File:** ecosystem/node-checker/src/runner/sync_runner.rs (L103-119)
```rust
        // Build the MetricsProvider for the target node.
        if let Ok(metrics_client) = target_node_address.get_metrics_client(Duration::from_secs(4)) {
            let metrics_client = Arc::new(metrics_client);
            provider_collection.target_metrics_provider = Some(MetricsProvider::new(
                self.provider_configs.metrics.clone(),
                metrics_client.clone(),
                target_node_address.url.clone(),
                target_node_address.get_metrics_port().unwrap(),
            ));
            provider_collection.target_system_information_provider =
                Some(SystemInformationProvider::new(
                    self.provider_configs.system_information.clone(),
                    metrics_client,
                    target_node_address.url.clone(),
                    target_node_address.get_metrics_port().unwrap(),
                ));
        }
```

**File:** ecosystem/node-checker/src/runner/sync_runner.rs (L182-213)
```rust
    async fn call_check(
        &self,
        checker: &Box<dyn Checker>,
        provider_collection: &ProviderCollection,
    ) -> Result<Vec<CheckResult>, CheckerError> {
        let mut num_attempts = 0;
        let check_result = loop {
            match checker.check(provider_collection).await {
                Ok(check_result) => break check_result,
                Err(err) => {
                    if num_attempts < self.config.num_retries {
                        num_attempts += 1;
                        warn!(
                            "Checker failed with a retryable error: {:#}. Retrying in {} seconds.",
                            err, self.config.retry_delay_secs
                        );
                        tokio::time::sleep(Duration::from_secs(
                            self.config.retry_delay_secs.into(),
                        ))
                        .await;
                    } else {
                        error!(
                            "Checker failed with a retryable error too many times ({}): {:#}.",
                            self.config.num_retries, err
                        );
                        return Err(err);
                    }
                },
            }
        };
        Ok(check_result)
    }
```

**File:** ecosystem/node-checker/src/provider/mod.rs (L60-62)
```rust
    fn default_cache_ttl_ms() -> u64 {
        1000
    }
```
