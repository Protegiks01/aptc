# Audit Report

## Title
CPU Exhaustion via Unthrottled BCS Deserialization in Network Message Stream

## Summary
The `MultiplexMessageStream::poll_next()` function performs BCS deserialization on all received network frames without any rate limiting or resource control applied beforehand. An attacker can send a continuous stream of valid large messages (up to 4 MiB each) to force CPU-intensive deserialization operations, degrading validator node performance and consensus processing.

## Finding Description

The vulnerability exists in the network message processing pipeline where BCS deserialization occurs **before** any application-level rate limiting or throttling mechanisms can take effect.

**Vulnerable Code Path:** [1](#0-0) 

The `poll_next()` function immediately calls `bcs::from_bytes(&frame)` on every received frame without pre-validation. The frame can be up to `max_frame_size` bytes: [2](#0-1) 

**Missing Rate Limiting:**

While `RateLimitConfig` is defined in the configuration, it is never applied to network connections: [3](#0-2) 

The `AsyncRateLimiter` utility exists but is not integrated into the network framework - it's only used in test code: [4](#0-3) 

**Attack Flow:**

1. Attacker establishes connections to a validator node (up to 100 for unknown peers, unlimited for malicious validators)
2. Peer connections are accepted and passed directly to `Peer::start()`: [5](#0-4) 

3. The connection is wrapped in `MultiplexMessageStream` without rate limiting: [6](#0-5) 

4. Each large message forces BCS deserialization of the entire frame, including allocation and parsing of multi-megabyte `Vec<u8>` buffers for message payloads: [7](#0-6) 

5. This CPU work accumulates across all active connections, potentially saturating validator CPU resources and degrading consensus performance.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty program category: "Validator node slowdowns."

**Resource Limits Invariant Violation:** The documented invariant #9 states "All operations must respect gas, storage, and computational limits." This vulnerability violates this invariant by allowing unbounded CPU consumption through deserialization before any computational limits are enforced.

**Consensus Performance Impact:** Validator nodes performing consensus operations (block proposal, voting, certificate aggregation) share CPU resources with the network message handling tasks. Sustained CPU exhaustion from deserialization can delay consensus messages, increasing round times or causing timeout failures in the AptosBFT protocol.

**Attack Scale:**
- Unknown attackers: 100 concurrent connections (per `MAX_INBOUND_CONNECTIONS`)
- Malicious validators: Unlimited connections (no limit for known peers)
- Each connection: Up to 4 MiB messages sent continuously
- Aggregate CPU load: 100+ simultaneous deserializations of multi-megabyte payloads

## Likelihood Explanation

**HIGH likelihood** - The attack is straightforward to execute:

1. **No special privileges required:** Any network peer can connect to validator nodes
2. **Simple attack vector:** Send valid but large network messages continuously  
3. **No detection mechanisms:** The messages are structurally valid, making them indistinguishable from legitimate traffic until CPU saturation occurs
4. **Low cost for attacker:** Network bandwidth is the only resource requirement (400 MB/s for 100 connections sending 4 MiB messages)
5. **Persistent attack:** Even deserialization errors don't close the connection, allowing sustained attacks

The connection limit for unknown peers (100) provides some mitigation but is insufficient. A single compromised validator node has no connection limits and can launch this attack with full effectiveness.

## Recommendation

**Immediate Fix:** Apply rate limiting at the socket layer **before** BCS deserialization occurs.

**Implementation Steps:**

1. **Enable AsyncRateLimiter integration:**
   - Wrap connection sockets with `AsyncRateLimiter` before passing to `Peer::new()`
   - Use the configured `inbound_rate_limit_config` from `NetworkConfig`

2. **Modify connection establishment in PeerManager:**

```rust
fn add_peer(&mut self, connection: Connection<TSocket>) -> Result<(), Error> {
    // ... existing code ...
    
    // Wrap socket with rate limiter if configured
    let rate_limited_connection = if let Some(rate_config) = &self.inbound_rate_limit_config {
        if rate_config.enabled {
            let bucket = create_rate_limit_bucket(
                connection.metadata.remote_peer_id,
                rate_config
            );
            Connection {
                socket: AsyncRateLimiter::new(connection.socket, Some(bucket)),
                metadata: connection.metadata,
            }
        } else {
            connection
        }
    } else {
        connection
    };
    
    let peer = Peer::new(/* ... use rate_limited_connection ... */);
    // ... rest of function ...
}
```

3. **Additional hardening:**
   - Implement message size validation before deserialization
   - Add per-connection message rate limits (messages/second)
   - Monitor deserialization CPU time and disconnect abusive peers

## Proof of Concept

**Rust Network Test (pseudocode framework):**

```rust
#[tokio::test]
async fn test_cpu_exhaustion_via_large_messages() {
    // Setup validator node
    let validator = spawn_test_validator().await;
    let validator_addr = validator.listen_address();
    
    // Create 100 attacker connections
    let mut connections = Vec::new();
    for _ in 0..100 {
        let conn = establish_peer_connection(validator_addr).await.unwrap();
        connections.push(conn);
    }
    
    // Measure baseline CPU usage
    let baseline_cpu = measure_validator_cpu(&validator).await;
    
    // Attack: Send continuous large messages from all connections
    let attack_duration = Duration::from_secs(30);
    let attack_handle = tokio::spawn(async move {
        let start = Instant::now();
        while start.elapsed() < attack_duration {
            for conn in &mut connections {
                // Create maximum size DirectSendMsg
                let large_payload = vec![0u8; 4 * 1024 * 1024 - 1024]; // ~4 MiB minus headers
                let msg = NetworkMessage::DirectSendMsg(DirectSendMsg {
                    protocol_id: ProtocolId::NetbenchDirectSend,
                    priority: 0,
                    raw_msg: large_payload,
                });
                
                send_message(conn, &msg).await.ok();
            }
        }
    });
    
    attack_handle.await.unwrap();
    
    // Measure CPU during attack
    let attack_cpu = measure_validator_cpu(&validator).await;
    
    // Measure consensus performance degradation
    let consensus_latency = measure_consensus_round_time(&validator).await;
    
    // Assertions
    assert!(attack_cpu > baseline_cpu * 2.0, "CPU usage should double");
    assert!(consensus_latency > normal_latency * 1.5, "Consensus should slow down");
}
```

**Attack parameters:**
- Connections: 100 (maximum for unknown peers)
- Message size: 4 MiB per message (maximum frame size)
- Send rate: Limited only by network bandwidth
- Duration: Sustained until validator performance degrades observably

**Expected result:** Validator CPU usage increases significantly, consensus round times increase by 50%+ indicating performance degradation.

**Notes**

This vulnerability demonstrates a critical gap between designed rate limiting capabilities (`RateLimitConfig`, `AsyncRateLimiter`) and their actual deployment in the production code path. The infrastructure exists to mitigate this attack but remains unused, leaving validators vulnerable to CPU exhaustion through unthrottled deserialization of large network messages.

The attack is particularly concerning for consensus validators where CPU availability directly impacts consensus liveness and performance. While not causing safety violations or fund loss, sustained attacks could degrade network performance below acceptable thresholds or force validators to disconnect legitimate peers to preserve resources.

### Citations

**File:** network/framework/src/protocols/wire/messaging/v1/mod.rs (L116-128)
```rust
#[derive(Clone, Debug, PartialEq, Eq, Deserialize, Serialize)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct RpcRequest {
    /// `protocol_id` is a variant of the ProtocolId enum.
    pub protocol_id: ProtocolId,
    /// RequestId for the RPC Request.
    pub request_id: RequestId,
    /// Request priority in the range 0..=255.
    pub priority: Priority,
    /// Request payload. This will be parsed by the application-level handler.
    #[serde(with = "serde_bytes")]
    pub raw_request: Vec<u8>,
}
```

**File:** network/framework/src/protocols/wire/messaging/v1/mod.rs (L225-241)
```rust
    fn poll_next(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        match self.project().framed_read.poll_next(cx) {
            Poll::Ready(Some(Ok(frame))) => {
                let frame = frame.freeze();

                match bcs::from_bytes(&frame) {
                    Ok(message) => Poll::Ready(Some(Ok(message))),
                    // Failed to deserialize the NetworkMessage
                    Err(err) => {
                        let mut frame = frame;
                        let frame_len = frame.len();
                        // Keep a few bytes from the frame for debugging
                        frame.truncate(8);
                        let err = ReadError::DeserializeError(err, frame_len, frame);
                        Poll::Ready(Some(Err(err)))
                    },
                }
```

**File:** config/src/config/network_config.rs (L49-50)
```rust
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** config/src/config/network_config.rs (L117-120)
```rust
    pub inbound_rate_limit_config: Option<RateLimitConfig>,
    /// Outbound rate limiting configuration, if not specified, no rate limiting
    pub outbound_rate_limit_config: Option<RateLimitConfig>,
    /// The maximum size of an inbound or outbound message (it may be divided into multiple frame)
```

**File:** crates/aptos-rate-limiter/src/async_lib.rs (L82-99)
```rust
/// A rate limiter for `AsyncRead` or `AsyncWrite` interfaces to rate limit read/write bytes
///
/// This will pause and wait to send any future bytes until it's permitted to in the future
#[pin_project]
pub struct AsyncRateLimiter<T> {
    #[pin]
    inner: T,
    rate_limiter: PollRateLimiter,
}

impl<T> AsyncRateLimiter<T> {
    pub fn new(inner: T, bucket: Option<SharedBucket>) -> Self {
        Self {
            inner,
            rate_limiter: PollRateLimiter::new(bucket),
        }
    }
}
```

**File:** network/framework/src/peer_manager/mod.rs (L665-679)
```rust
        let peer = Peer::new(
            self.network_context,
            self.executor.clone(),
            self.time_service.clone(),
            connection,
            self.transport_notifs_tx.clone(),
            peer_reqs_rx,
            self.upstream_handlers.clone(),
            Duration::from_millis(constants::INBOUND_RPC_TIMEOUT_MS),
            constants::MAX_CONCURRENT_INBOUND_RPCS,
            constants::MAX_CONCURRENT_OUTBOUND_RPCS,
            self.max_frame_size,
            self.max_message_size,
        );
        self.executor.spawn(peer.start());
```

**File:** network/framework/src/peer/mod.rs (L213-218)
```rust
        let (read_socket, write_socket) =
            tokio::io::split(self.connection.take().unwrap().compat());

        let mut reader =
            MultiplexMessageStream::new(read_socket.compat(), self.max_frame_size).fuse();
        let writer = MultiplexMessageSink::new(write_socket.compat_write(), self.max_frame_size);
```
