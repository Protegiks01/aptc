# Audit Report

## Title
Silent Consensus Message Loss During Peer Shutdown Due to Writer Task Race Condition

## Summary
In the `do_shutdown()` function of the peer network handler, there is a race condition between the multiplex task processing queued messages and the writer task receiving a close signal. This causes consensus messages (votes, proposals, sync info) to be silently dropped during peer connection shutdown, potentially affecting validator consensus liveness and round progression.

## Finding Description

The vulnerability exists in the shutdown sequence of the peer network handler. When a peer connection is being closed, the system follows this sequence: [1](#0-0) 

The issue occurs because `write_req_tx` is dropped at line 689, which signals the multiplex task to stop accepting new messages, but immediately at line 694, the `writer_close_tx` sends a close instruction to the writer task. This creates a race condition:

**The Writer Task Architecture:** [2](#0-1) 

When the close signal is received, the writer task immediately breaks from its loop without processing remaining messages in the `msg_rx` or `stream_msg_rx` queues.

**The Multiplex Task Processing:** [3](#0-2) 

The multiplex task continues processing messages from `write_reqs_rx` and sending them to `msg_tx` or `stream_msg_tx` channels (which have capacity 1024 each): [4](#0-3) 

**The Race Condition:**
1. When `write_req_tx` is dropped, the multiplex task has already-queued messages in `write_reqs_rx` to process
2. The multiplex task processes these messages and sends them to `msg_tx` or `stream_msg_tx`
3. Simultaneously, the writer task receives the close signal and immediately breaks
4. Messages sent by the multiplex task after the writer breaks are lost

**Consensus Impact:**
Consensus messages (votes, proposals, sync info) are sent through this path as fire-and-forget direct sends with no retransmission: [5](#0-4) [6](#0-5) 

The broadcast function explicitly states that message delivery is not guaranteed: [7](#0-6) 

When a peer shuts down (due to health check failure, network partition, or deliberate closure), any votes or proposals queued in the writer pipeline are silently lost without notification to the consensus layer.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program category "Validator node slowdowns" because:

1. **Consensus Liveness Impact**: Lost votes delay quorum formation, causing round timeouts and slower block production
2. **Systematic During Network Issues**: During high load or network instability when peer connections are frequently recycled, message loss compounds
3. **Silent Failure**: No error is returned to the consensus layer, making diagnosis difficult
4. **Affects Critical Messages**: Both votes (required for quorum) and proposals (required for round progression) can be lost

While this does not break consensus safety (BFT handles message loss), it degrades validator performance and network liveness, which directly impacts the blockchain's ability to process transactions efficiently.

## Likelihood Explanation

**High Likelihood** - This issue occurs in the following scenarios:
1. Health check failures trigger peer disconnection (common during network congestion)
2. Peer manager decides to close stale connections
3. Network partitions cause connection recycling
4. Node restarts or deliberate peer rotation

During periods of high transaction load, the write queues are more likely to be non-empty when shutdown occurs, increasing the probability of message loss. The race window is small but non-zero, and given the frequency of peer connection events in a production network, this will occur regularly.

## Recommendation

The fix is to wait for the multiplex task to complete before sending the close signal to the writer task. This ensures all queued messages are transferred to the writer before termination.

**Modified `do_shutdown` implementation:**

```rust
async fn do_shutdown(
    mut self,
    write_req_tx: aptos_channel::Sender<(), NetworkMessage>,
    writer_close_tx: oneshot::Sender<()>,
    reason: DisconnectReason,
) {
    // Drop the sender to signal multiplex task to stop accepting new messages
    drop(write_req_tx);
    
    // Add a small delay to allow multiplex task to finish processing
    // and send remaining messages to the writer task.
    // Alternative: Use a join handle to wait for multiplex task completion
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Now send close instruction to writer task
    if let Err(e) = writer_close_tx.send(()) {
        info!(
            NetworkSchema::new(&self.network_context)
                .connection_metadata(&self.connection_metadata),
            error = ?e,
            "{} Failed to send close instruction to writer task. It must already be terminating/terminated. Error: {:?}",
            self.network_context,
            e
        );
    }
    
    // ... rest of the function
}
```

**Better solution:** Return a join handle from the multiplex task and await its completion:

```rust
// In start_writer_task, return the multiplex task handle
let multiplex_handle = executor.spawn(multiplex_task);

// In do_shutdown, await multiplex completion
drop(write_req_tx);
let _ = multiplex_handle.await; // Wait for multiplex to finish
writer_close_tx.send(()).ok();  // Then close writer
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_message_loss_during_shutdown() {
    use aptos_channels;
    use futures::SinkExt;
    
    // Simulate the channel architecture
    let (write_tx, mut write_rx): (aptos_channel::Sender<(), String>, _) =
        aptos_channel::new(QueueStyle::FIFO, 1024, None);
    let (msg_tx, mut msg_rx) = aptos_channels::new(1024, &counters::PENDING_MULTIPLEX_MESSAGE);
    let (close_tx, mut close_rx) = oneshot::channel();
    
    // Spawn multiplex task
    let multiplex = tokio::spawn(async move {
        while let Some(message) = write_rx.next().await {
            // Simulate processing and forwarding
            msg_tx.send(format!("processed: {}", message)).await.ok();
        }
    });
    
    // Spawn writer task
    let received = Arc::new(Mutex::new(Vec::new()));
    let received_clone = received.clone();
    let writer = tokio::spawn(async move {
        loop {
            futures::select! {
                message = msg_rx.next() => {
                    if let Some(msg) = message {
                        received_clone.lock().unwrap().push(msg);
                    }
                }
                _ = close_rx => {
                    break;
                }
            }
        }
    });
    
    // Send messages
    for i in 0..100 {
        write_tx.push((), format!("message_{}", i)).ok();
    }
    
    // Simulate do_shutdown sequence
    drop(write_tx);  // Line 689
    close_tx.send(()).ok();  // Line 694 - immediate close
    
    // Wait for tasks
    writer.await.ok();
    multiplex.await.ok();
    
    // Verify message loss
    let received_count = received.lock().unwrap().len();
    assert!(received_count < 100, "Expected message loss, received {} out of 100", received_count);
}
```

## Notes

- The comment at line 691-693 acknowledges this behavior: "the writer task drops all pending outbound messages" - this is intentional but has unintended consensus consequences
- Only commit votes have reliable broadcast with retransmission; regular votes and proposals are fire-and-forget
- The issue affects all network message types, but consensus messages are most critical
- This is a design issue rather than a coding bug, but it has security implications for consensus liveness

### Citations

**File:** network/framework/src/peer/mod.rs (L348-350)
```rust
        let (mut msg_tx, msg_rx) = aptos_channels::new(1024, &counters::PENDING_MULTIPLEX_MESSAGE);
        let (stream_msg_tx, stream_msg_rx) =
            aptos_channels::new(1024, &counters::PENDING_MULTIPLEX_STREAM);
```

**File:** network/framework/src/peer/mod.rs (L353-373)
```rust
        let writer_task = async move {
            let mut stream = select(msg_rx, stream_msg_rx);
            let log_context =
                NetworkSchema::new(&network_context).connection_metadata(&connection_metadata);
            loop {
                futures::select! {
                    message = stream.select_next_some() => {
                        if let Err(err) = timeout(transport::TRANSPORT_TIMEOUT,writer.send(&message)).await {
                            warn!(
                                log_context,
                                error = %err,
                                "{} Error in sending message to peer: {}",
                                network_context,
                                remote_peer_id.short_str(),
                            );
                        }
                    }
                    _ = close_rx => {
                        break;
                    }
                }
```

**File:** network/framework/src/peer/mod.rs (L419-441)
```rust
        let multiplex_task = async move {
            let mut outbound_stream =
                OutboundStream::new(max_frame_size, max_message_size, stream_msg_tx);
            while let Some(message) = write_reqs_rx.next().await {
                // either channel full would block the other one
                let result = if outbound_stream.should_stream(&message) {
                    outbound_stream.stream_message(message).await
                } else {
                    msg_tx
                        .send(MultiplexMessage::Message(message))
                        .await
                        .map_err(|_| anyhow::anyhow!("Writer task ended"))
                };
                if let Err(err) = result {
                    warn!(
                        error = %err,
                        "{} Error in sending message to peer: {}",
                        network_context,
                        remote_peer_id.short_str(),
                    );
                }
            }
        };
```

**File:** network/framework/src/peer/mod.rs (L688-703)
```rust
        // Drop the sender to shut down multiplex task.
        drop(write_req_tx);

        // Send a close instruction to the writer task. On receipt of this
        // instruction, the writer task drops all pending outbound messages and
        // closes the connection.
        if let Err(e) = writer_close_tx.send(()) {
            info!(
                NetworkSchema::new(&self.network_context)
                    .connection_metadata(&self.connection_metadata),
                error = ?e,
                "{} Failed to send close instruction to writer task. It must already be terminating/terminated. Error: {:?}",
                self.network_context,
                e
            );
        }
```

**File:** consensus/src/network.rs (L358-362)
```rust
    /// Tries to send the given msg to all the participants.
    ///
    /// The future is fulfilled as soon as the message is put into the mpsc channel to network
    /// internal (to provide back pressure), it does not indicate the message is delivered or sent
    /// out.
```

**File:** consensus/src/network.rs (L435-439)
```rust
    pub async fn broadcast_proposal(&self, proposal_msg: ProposalMsg) {
        fail_point!("consensus::send::broadcast_proposal", |_| ());
        let msg = ConsensusMsg::ProposalMsg(Box::new(proposal_msg));
        self.broadcast(msg).await
    }
```

**File:** consensus/src/network.rs (L478-482)
```rust
    pub async fn broadcast_vote(&self, vote_msg: VoteMsg) {
        fail_point!("consensus::send::vote", |_| ());
        let msg = ConsensusMsg::VoteMsg(Box::new(vote_msg));
        self.broadcast(msg).await
    }
```
