# Audit Report

## Title
Missing Transaction-to-Payload Digest Validation in Consensus Execution Pipeline Allows State Divergence Under Storage Corruption

## Summary
The `TPayloadManager` trait does not specify Byzantine-fault-tolerance requirements in its contract, and the consensus execution pipeline lacks validation to verify that transactions returned by `get_transactions()` match the block's committed payload digests. While remote batch fetching includes digest validation, local storage retrievals bypass this check, creating a potential attack surface for storage-layer exploits or corruption scenarios. [1](#0-0) 

## Finding Description

The vulnerability exists in the interaction between the payload manager abstraction and the consensus execution pipeline:

**1. Trait Contract Weakness:**
The `TPayloadManager` trait documentation states only that implementations are "responsible for resolving the transactions in a block's payload" without specifying Byzantine-fault-tolerance requirements or guarantees about transaction-to-digest matching. [2](#0-1) 

**2. Missing Post-Validation:**
After `get_transactions()` returns transactions, the pipeline proceeds through filtering, deduplication, signature verification, and execution without validating that returned transactions match the block's committed payload digest. [3](#0-2) 

**3. Local Storage Bypass:**
In `QuorumStorePayloadManager`, when batches are retrieved from local storage via `get_batch_from_local()`, there is no re-validation of the digest against the actual transaction payload: [4](#0-3) 

The local retrieval path directly returns cached payload without calling `verify()` or `verify_with_digest()`: [5](#0-4) 

**4. Contrast with Remote Validation:**
Remote batch fetching DOES include validation via `Batch::verify_with_digest()` which checks payload hash matches digest: [6](#0-5) 

**5. Consensus Observer Has Proper Validation:**
The consensus observer path includes `verify_payload_digests()` which reconstructs batches and validates their digests: [7](#0-6) 

However, this validation is NOT used in the regular consensus execution path.

**Attack Propagation:**
1. Block with `Payload::InQuorumStore(ProofWithData)` receives QC (consensus commits to specific batch digests)
2. During execution, `get_transactions()` is called
3. For QuorumStore payloads, `BatchReader.get_batch()` is invoked
4. If batch exists locally, `get_batch_from_local()` returns stored transactions WITHOUT digest validation
5. If storage is corrupted (bug, bit flip, race condition), wrong transactions are returned
6. Validator executes wrong transactions and computes different state root
7. Validator diverges from honest validators

## Impact Explanation

**Severity Assessment: Medium**

This does NOT meet Critical severity because realistic exploitation requires either:
- Compromised validator binaries (can inject malicious `TPayloadManager`)
- Storage-layer corruption affecting >2/3 validators simultaneously
- Both scenarios fall outside standard protocol vulnerability scope

However, it represents a **defense-in-depth failure** with potential for:

**Single Validator Impact:**
- Storage corruption causes wrong transaction execution
- Validator computes incorrect state root
- Validator falls out of consensus, causing liveness degradation
- Recovery requires state sync or node restart

**Multiple Validator Impact (Low Probability):**
- If >1/3 validators experience correlated storage corruption, network liveness failure
- If >2/3 validators affected, potential consensus safety violation (wrong transactions executed)

**Why Not Critical:**
Per Aptos bug bounty exclusions: "51% attacks or stake majority attacks" are out of scope. The >2/3 scenario is effectively this. Storage bugs affecting multiple validators simultaneously is unrealistic without a systematic exploit.

## Likelihood Explanation

**Likelihood: Low**

The vulnerability requires:

1. **Storage-layer corruption** - Could occur from:
   - Database bugs in QuorumStoreDB
   - Race conditions in batch persistence
   - Bit flips in memory/disk
   - Malicious modification of database files (requires local access)

2. **No re-validation trigger** - Currently, local storage is trusted implicitly

3. **Scale requirement** - Single validator impact is reliability issue; security impact requires multiple validators

**Mitigating Factors:**
- `TPayloadManager` implementations are part of core codebase (not external plugins)
- Remote batch fetching IS validated
- Storage layer has other integrity mechanisms
- Validators independently compute state; single corruption doesn't compromise network

**However**, defense-in-depth principle suggests this validation SHOULD exist.

## Recommendation

**Add post-retrieval digest validation for all batch sources:**

```rust
// In consensus/src/quorum_store/batch_store.rs
// Modify get_batch_from_local to validate digests

pub(crate) fn get_batch_from_local(
    &self,
    digest: &HashValue,
) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
    if let Some(value) = self.db_cache.get(digest) {
        if value.payload_storage_mode() == StorageMode::PersistedOnly {
            // Retrieve from DB and validate
            let persisted_value = self.get_batch_from_db(digest, value.batch_info().is_v2())?;
            
            // ADDED: Validate digest matches payload
            if let Some(payload) = persisted_value.payload() {
                let computed_digest = payload.hash();
                if computed_digest != *digest {
                    return Err(ExecutorError::InternalError {
                        error: format!(
                            "Batch digest mismatch in local storage! Expected: {:?}, Computed: {:?}",
                            digest, computed_digest
                        )
                    });
                }
            }
            
            Ok(persisted_value)
        } else {
            // ADDED: Validate in-memory batch as well
            if let Some(payload) = value.payload() {
                let computed_digest = payload.hash();
                if computed_digest != *digest {
                    return Err(ExecutorError::InternalError {
                        error: format!(
                            "Batch digest mismatch in memory! Expected: {:?}, Computed: {:?}",
                            digest, computed_digest
                        )
                    });
                }
            }
            Ok(value.clone())
        }
    } else {
        Err(ExecutorError::CouldNotGetData)
    }
}
```

**Alternative: Add validation layer in pipeline:**

Add validation in `pipeline_builder.rs` after `materialize_block` to verify transactions match payload digests before execution. This could reuse the consensus observer's `verify_payload_digests()` logic.

## Proof of Concept

Due to the nature of this vulnerability (requiring storage corruption), a traditional PoC is not straightforward. However, here's a test scenario demonstrating the missing validation:

```rust
// In consensus/src/quorum_store/tests/batch_store_test.rs

#[tokio::test]
async fn test_corrupted_local_storage_bypass() {
    // Setup: Create batch store with valid batch
    let (batch_store, _db) = create_batch_store();
    let batch_info = create_test_batch_info();
    let correct_payload = create_test_transactions(10);
    let correct_digest = BatchPayload::new(batch_info.author(), correct_payload.clone()).hash();
    
    // Store batch with correct digest
    let value = PersistedValue::new(
        batch_info.clone(),
        Some(correct_payload.clone())
    );
    batch_store.persist(vec![value]);
    
    // Simulate storage corruption: directly modify database to have wrong transactions
    let wrong_payload = create_test_transactions(5); // Different transactions!
    let corrupted_value = PersistedValue::new(
        batch_info.clone(),
        Some(wrong_payload.clone())
    );
    // Directly insert into cache, bypassing validation
    batch_store.db_cache.insert(correct_digest, corrupted_value);
    
    // Retrieve batch - should fail validation but currently doesn't
    let retrieved = batch_store.get_batch_from_local(&correct_digest).unwrap();
    let retrieved_payload = retrieved.take_payload().unwrap();
    
    // VULNERABILITY: This assertion passes (wrong payload retrieved)
    assert_eq!(retrieved_payload.len(), 5);
    // EXPECTED: Should have failed with digest mismatch error
    
    // Verify the digest mismatch
    let computed_digest = BatchPayload::new(batch_info.author(), retrieved_payload).hash();
    assert_ne!(computed_digest, correct_digest); // Digests don't match!
    
    // This demonstrates that corrupted storage can provide wrong transactions
    // without detection in the current implementation
}
```

**Notes:**
This PoC demonstrates that the current implementation does not validate local storage retrievals. In production, storage corruption leading to this scenario would cause validators to compute different state roots and fall out of consensus, but the corruption itself would not be detected at the payload manager level.

---

**Additional Context:**

The lack of explicit Byzantine-fault-tolerance requirements in the trait contract combined with missing post-retrieval validation creates a gap in defense-in-depth. While the current design assumes `TPayloadManager` implementations are part of the trusted codebase (reasonable for Aptos Core), adding validation would:

1. Protect against storage-layer bugs
2. Detect corruption earlier in the pipeline
3. Align with the consensus observer's more defensive approach
4. Provide better error diagnostics for validator operators

The fix is straightforward and aligns with security best practices: verify all data, even from local sources.

### Citations

**File:** consensus/src/payload_manager/mod.rs (L24-56)
```rust
/// A trait that defines the interface for a payload manager. The payload manager is responsible for
/// resolving the transactions in a block's payload.
#[async_trait]
pub trait TPayloadManager: Send + Sync {
    /// Notify the payload manager that a block has been committed. This indicates that the
    /// transactions in the block's payload are no longer required for consensus.
    fn notify_commit(&self, block_timestamp: u64, payloads: Vec<Payload>);

    /// Prefetch the data for a payload. This is used to ensure that the data for a payload is
    /// available when block is executed.
    fn prefetch_payload_data(&self, payload: &Payload, author: Author, timestamp: u64);

    /// Check if the block contains any inline transactions that need
    /// to be denied (e.g., due to block transaction filtering).
    /// This is only used when processing block proposals.
    fn check_denied_inline_transactions(
        &self,
        block: &Block,
        block_txn_filter_config: &BlockTransactionFilterConfig,
    ) -> anyhow::Result<()>;

    /// Check if the transactions corresponding are available. This is specific to payload
    /// manager implementations. For optimistic quorum store, we only check if optimistic
    /// batches are available locally.
    fn check_payload_availability(&self, block: &Block) -> Result<(), BitVec>;

    /// Get the transactions in a block's payload. This function returns a vector of transactions.
    async fn get_transactions(
        &self,
        block: &Block,
        block_voters: Option<BitVec>,
    ) -> ExecutorResult<(Vec<SignedTransaction>, Option<u64>, Option<u64>)>;
}
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L615-681)
```rust
    async fn materialize(
        preparer: Arc<BlockPreparer>,
        block: Arc<Block>,
        qc_rx: oneshot::Receiver<Arc<QuorumCert>>,
    ) -> TaskResult<MaterializeResult> {
        let mut tracker = Tracker::start_waiting("materialize", &block);
        tracker.start_working();

        let qc_rx = async {
            match qc_rx.await {
                Ok(qc) => Some(qc),
                Err(_) => {
                    warn!("[BlockPreparer] qc tx cancelled for block {}", block.id());
                    None
                },
            }
        }
        .shared();
        // the loop can only be abort by the caller
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
        Ok(result)
    }

    async fn prepare(
        decryption_fut: TaskFuture<DecryptionResult>,
        preparer: Arc<BlockPreparer>,
        block: Arc<Block>,
    ) -> TaskResult<PrepareResult> {
        let mut tracker = Tracker::start_waiting("prepare", &block);
        let (input_txns, max_txns_from_block_to_execute, block_gas_limit) = decryption_fut.await?;

        tracker.start_working();

        let (input_txns, block_gas_limit) = preparer
            .prepare_block(
                &block,
                input_txns,
                max_txns_from_block_to_execute,
                block_gas_limit,
            )
            .await;

        let sig_verification_start = Instant::now();
        let sig_verified_txns: Vec<SignatureVerifiedTransaction> = SIG_VERIFY_POOL.install(|| {
            let num_txns = input_txns.len();
            input_txns
                .into_par_iter()
                .with_min_len(optimal_min_len(num_txns, 32))
                .map(|t| Transaction::UserTransaction(t).into())
                .collect::<Vec<_>>()
        });
        counters::PREPARE_BLOCK_SIG_VERIFICATION_TIME
            .observe_duration(sig_verification_start.elapsed());
        Ok((Arc::new(sig_verified_txns), block_gas_limit))
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L571-585)
```rust
    pub(crate) fn get_batch_from_local(
        &self,
        digest: &HashValue,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        if let Some(value) = self.db_cache.get(digest) {
            if value.payload_storage_mode() == StorageMode::PersistedOnly {
                self.get_batch_from_db(digest, value.batch_info().is_v2())
            } else {
                // Available in memory.
                Ok(value.clone())
            }
        } else {
            Err(ExecutorError::CouldNotGetData)
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L684-709)
```rust
                let fut = async move {
                    let batch_digest = *batch_info.digest();
                    defer!({
                        inflight_requests_clone.lock().remove(&batch_digest);
                    });
                    // TODO(ibalajiarun): Support V2 batch
                    if let Ok(mut value) = batch_store.get_batch_from_local(&batch_digest) {
                        Ok(value.take_payload().expect("Must have payload"))
                    } else {
                        // Quorum store metrics
                        counters::MISSED_BATCHES_COUNT.inc();
                        let subscriber_rx = batch_store.subscribe(*batch_info.digest());
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
                            .await?;
                        batch_store.persist(vec![PersistedValue::new(
                            batch_info.into(),
                            Some(payload.clone()),
                        )]);
                        Ok(payload)
                    }
```

**File:** consensus/src/quorum_store/types.rs (L262-300)
```rust
    pub fn verify(&self) -> anyhow::Result<()> {
        ensure!(
            self.payload.author() == self.author(),
            "Payload author doesn't match the info"
        );
        ensure!(
            self.payload.hash() == *self.digest(),
            "Payload hash doesn't match the digest"
        );
        ensure!(
            self.payload.num_txns() as u64 == self.num_txns(),
            "Payload num txns doesn't match batch info"
        );
        ensure!(
            self.payload.num_bytes() as u64 == self.num_bytes(),
            "Payload num bytes doesn't match batch info"
        );
        for txn in self.payload.txns() {
            ensure!(
                txn.gas_unit_price() >= self.gas_bucket_start(),
                "Payload gas unit price doesn't match batch info"
            );
            ensure!(
                !txn.payload().is_encrypted_variant(),
                "Encrypted transaction is not supported yet"
            );
        }
        Ok(())
    }

    /// Verify the batch, and that it matches the requested digest
    pub fn verify_with_digest(&self, requested_digest: HashValue) -> anyhow::Result<()> {
        ensure!(
            requested_digest == *self.digest(),
            "Response digest doesn't match the request"
        );
        self.verify()?;
        Ok(())
    }
```

**File:** consensus/src/consensus_observer/network/observer_message.rs (L1019-1038)
```rust
fn verify_batch(
    expected_batch_info: &BatchInfo,
    batch_transactions: Vec<SignedTransaction>,
) -> Result<(), Error> {
    // Calculate the batch digest
    let batch_payload = BatchPayload::new(expected_batch_info.author(), batch_transactions);
    let batch_digest = batch_payload.hash();

    // Verify the reconstructed digest against the expected digest
    let expected_digest = expected_batch_info.digest();
    if batch_digest != *expected_digest {
        return Err(Error::InvalidMessageError(format!(
            "The reconstructed batch digest does not match the expected digest! \
             Batch: {:?}, Expected digest: {:?}, Reconstructed digest: {:?}",
            expected_batch_info, expected_digest, batch_digest
        )));
    }

    Ok(())
}
```
