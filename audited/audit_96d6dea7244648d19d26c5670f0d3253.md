# Audit Report

## Title
Late Bootstrap Subscription Causes Node Panic via Premature Chunk Executor Termination

## Summary
When a component subscribes to bootstrap notifications after the node has already completed bootstrapping, the bootstrapper immediately notifies the subscriber and unconditionally calls `finish_chunk_executor()`. This prematurely terminates the shared chunk executor that may be actively in use by the continuous syncer or consensus, causing the node to panic with "not reset" when any subsequent chunk executor operation is attempted.

## Finding Description

The vulnerability exists in the bootstrap notification mechanism's handling of late subscribers. When a component calls `notify_once_bootstrapped()` after bootstrapping has completed, the following sequence occurs: [1](#0-0) 

The driver receives this notification and calls `subscribe_to_bootstrap_notifications()`: [2](#0-1) 

The bootstrapper's `subscribe_to_bootstrap_notifications()` immediately invokes `notify_listeners_if_bootstrapped()`: [3](#0-2) 

When the node is already bootstrapped, `notify_listeners_if_bootstrapped()` unconditionally calls `finish_chunk_executor()`: [4](#0-3) 

The `finish_chunk_executor()` method sets the chunk executor's inner state to `None`: [5](#0-4) 

The critical issue is that the bootstrapper and continuous syncer share the same `StorageSynchronizer` instance (created via `.clone()` in the driver constructor), which contains an `Arc` reference to the same `ChunkExecutor`: [6](#0-5) [7](#0-6) 

When any component subsequently attempts to use the chunk executor (via `execute_transactions()`, `apply_transaction_outputs()`, etc.), it calls the `with_inner()` method which panics if inner is `None`: [8](#0-7) 

The driver's async event loop allows this race condition because `handle_client_notification()` can be selected and executed while `drive_progress()` is awaiting, causing cleanup operations to execute while the continuous syncer or consensus may be actively using the chunk executor: [9](#0-8) 

## Impact Explanation

This is a **Medium Severity** vulnerability per Aptos bug bounty criteria:

1. **Node Availability Impact**: The panic causes the affected validator or fullnode to crash, requiring a restart. This results in temporary loss of availability but does not cause consensus violations or permanent state corruption.

2. **No Consensus Safety Violation**: While the node crashes, it does not produce invalid blocks or cause chain splits. Other validators continue operating normally.

3. **No Loss of Funds**: The crash does not result in theft, minting, or permanent freezing of funds.

4. **State Inconsistencies**: While the node must restart, state remains consistent as the panic occurs before any corrupted data is committed.

The impact aligns with Medium severity: "State inconsistencies requiring intervention" - the node requires restart/intervention due to the panic.

## Likelihood Explanation

**Likelihood: Medium**

This vulnerability can be triggered in several realistic scenarios:

1. **Component Initialization Ordering**: Services or components that start after the node has bootstrapped may legitimately call `notify_once_bootstrapped()` to wait for bootstrap completion, unaware that bootstrapping already finished.

2. **Error Recovery Paths**: Components recovering from errors or restarting may re-subscribe to bootstrap notifications as part of their initialization.

3. **Dynamic Module Loading**: New modules or plugins loaded during runtime may subscribe to bootstrap notifications.

4. **No Special Privileges Required**: Any internal component with access to the `DriverClient` can trigger this vulnerability - no validator or consensus privileges are needed.

While not an everyday occurrence, late subscription is a legitimate use case that the API should handle safely. The fact that the code provides a public API for subscription implies it should work correctly regardless of when it's called.

## Recommendation

Add a flag to track whether resource cleanup has already been performed, preventing duplicate calls to `finish_chunk_executor()` and `reset_active_stream()`:

```rust
pub struct Bootstrapper<MetadataStorage, StorageSyncer, StreamingClient> {
    // ... existing fields ...
    
    // Flag to track if cleanup has been performed
    cleanup_performed: bool,
}

impl<...> Bootstrapper<...> {
    pub fn new(...) -> Self {
        Self {
            // ... existing initialization ...
            cleanup_performed: false,
        }
    }
    
    async fn notify_listeners_if_bootstrapped(&mut self) -> Result<(), Error> {
        if self.is_bootstrapped() {
            if let Some(notifier_channel) = self.bootstrap_notifier_channel.take() {
                if let Err(error) = notifier_channel.send(Ok(())) {
                    return Err(Error::CallbackSendFailed(format!(
                        "Bootstrap notification error: {:?}",
                        error
                    )));
                }
            }
            
            // Only perform cleanup once
            if !self.cleanup_performed {
                self.reset_active_stream(None).await?;
                self.storage_synchronizer.finish_chunk_executor();
                self.cleanup_performed = true;
            }
        }
        Ok(())
    }
}
```

Alternatively, move the cleanup operations to only occur in `bootstrapping_complete()` rather than in every call to `notify_listeners_if_bootstrapped()`.

## Proof of Concept

The following test demonstrates the vulnerability:

```rust
#[tokio::test]
async fn test_late_subscription_panic() {
    // Setup: Create a bootstrapped node with shared storage synchronizer
    let (driver, storage_sync, client) = setup_test_driver().await;
    
    // Step 1: Complete bootstrapping
    driver.bootstrapper.bootstrapping_complete().await.unwrap();
    assert!(driver.bootstrapper.is_bootstrapped());
    
    // Step 2: Continuous syncer initializes and uses chunk executor
    driver.continuous_syncer.drive_progress(Arc::new(Mutex::new(None))).await.unwrap();
    
    // Step 3: Late subscriber arrives (simulating a component that started late)
    let late_subscription_future = client.notify_once_bootstrapped();
    
    // Step 4: Process the late subscription
    tokio::spawn(late_subscription_future);
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Step 5: Continuous syncer tries to process more data
    // This will panic with "not reset" because finish_chunk_executor() was called
    let result = driver.continuous_syncer.drive_progress(Arc::new(Mutex::new(None))).await;
    
    // Expected: Panic occurs when trying to use chunk executor with inner = None
    // Actual behavior: Node crashes with "thread panicked at 'not reset'"
}
```

To reproduce manually:
1. Start an Aptos validator or fullnode
2. Wait for bootstrapping to complete
3. Have any component call `DriverClient::notify_once_bootstrapped()` after bootstrapping
4. Observe that the continuous syncer (or consensus if validator) crashes when attempting to execute/apply the next transaction batch

The panic occurs at the `expect("not reset")` call in `ChunkExecutor::with_inner()`, which is invoked by any chunk execution operation after the premature `finish_chunk_executor()` call.

## Notes

This vulnerability demonstrates a lifecycle management issue where shared resources (the chunk executor) are not properly protected against premature cleanup. The API design implicitly allows late subscription but doesn't handle it safely. The fix should ensure idempotent cleanup or restrict when cleanup can occur to prevent interference with active operations.

### Citations

**File:** state-sync/state-sync-driver/src/driver_client.rs (L34-44)
```rust
    pub fn notify_once_bootstrapped(&self) -> impl Future<Output = Result<(), Error>> + use<> {
        let mut notification_sender = self.notification_sender.clone();
        let (callback_sender, callback_receiver) = oneshot::channel();

        async move {
            notification_sender
                .send(DriverNotification::NotifyOnceBootstrapped(callback_sender))
                .await?;
            callback_receiver.await?
        }
    }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L174-190)
```rust
        let output_fallback_handler =
            OutputFallbackHandler::new(driver_configuration.clone(), time_service.clone());
        let bootstrapper = Bootstrapper::new(
            driver_configuration.clone(),
            metadata_storage,
            output_fallback_handler.clone(),
            streaming_client.clone(),
            storage.clone(),
            storage_synchronizer.clone(),
        );
        let continuous_syncer = ContinuousSyncer::new(
            driver_configuration.clone(),
            streaming_client,
            output_fallback_handler,
            storage.clone(),
            storage_synchronizer.clone(),
        );
```

**File:** state-sync/state-sync-driver/src/driver.rs (L221-239)
```rust
        loop {
            ::futures::select! {
                notification = self.client_notification_listener.select_next_some() => {
                    self.handle_client_notification(notification).await;
                },
                notification = self.commit_notification_listener.select_next_some() => {
                    self.handle_snapshot_commit_notification(notification).await;
                }
                notification = self.consensus_notification_handler.select_next_some() => {
                    self.handle_consensus_or_observer_notification(notification).await;
                }
                notification = self.error_notification_listener.select_next_some() => {
                    self.handle_error_notification(notification).await;
                }
                _ = progress_check_interval.select_next_some() => {
                    self.drive_progress().await;
                }
            }
        }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L445-467)
```rust
    async fn handle_client_notification(&mut self, notification: DriverNotification) {
        debug!(LogSchema::new(LogEntry::ClientNotification)
            .message("Received a notify bootstrap notification from the client!"));
        metrics::increment_counter(
            &metrics::DRIVER_COUNTERS,
            metrics::DRIVER_CLIENT_NOTIFICATION,
        );

        // TODO(joshlind): refactor this if the client only supports one notification type!
        // Extract the bootstrap notifier channel
        let DriverNotification::NotifyOnceBootstrapped(notifier_channel) = notification;

        // Subscribe the bootstrap notifier channel
        if let Err(error) = self
            .bootstrapper
            .subscribe_to_bootstrap_notifications(notifier_channel)
            .await
        {
            warn!(LogSchema::new(LogEntry::ClientNotification)
                .error(&error)
                .message("Failed to subscribe to bootstrap notifications!"));
        }
    }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L380-393)
```rust
    /// Subscribes the specified channel to bootstrap completion notifications
    pub async fn subscribe_to_bootstrap_notifications(
        &mut self,
        bootstrap_notifier_channel: oneshot::Sender<Result<(), Error>>,
    ) -> Result<(), Error> {
        if self.bootstrap_notifier_channel.is_some() {
            return Err(Error::UnexpectedError(
                "Only one boostrap subscriber is supported at a time!".into(),
            ));
        }

        self.bootstrap_notifier_channel = Some(bootstrap_notifier_channel);
        self.notify_listeners_if_bootstrapped().await
    }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L395-411)
```rust
    /// Notifies any listeners if we've now bootstrapped
    async fn notify_listeners_if_bootstrapped(&mut self) -> Result<(), Error> {
        if self.is_bootstrapped() {
            if let Some(notifier_channel) = self.bootstrap_notifier_channel.take() {
                if let Err(error) = notifier_channel.send(Ok(())) {
                    return Err(Error::CallbackSendFailed(format!(
                        "Bootstrap notification error: {:?}",
                        error
                    )));
                }
            }
            self.reset_active_stream(None).await?;
            self.storage_synchronizer.finish_chunk_executor(); // The bootstrapper is now complete
        }

        Ok(())
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L89-94)
```rust
    fn with_inner<F, T>(&self, f: F) -> Result<T>
    where
        F: FnOnce(&ChunkExecutorInner<V>) -> Result<T>,
    {
        let locked = self.inner.read();
        let inner = locked.as_ref().expect("not reset");
```

**File:** execution/executor/src/chunk_executor/mod.rs (L221-225)
```rust
    fn finish(&self) {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["chunk", "finish"]);

        *self.inner.write() = None;
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L136-150)
```rust
pub struct StorageSynchronizer<ChunkExecutor, MetadataStorage> {
    // The executor for transaction and transaction output chunks
    chunk_executor: Arc<ChunkExecutor>,

    // A channel through which to notify the driver of committed data
    commit_notification_sender: mpsc::UnboundedSender<CommitNotification>,

    // The configuration of the state sync driver
    driver_config: StateSyncDriverConfig,

    // A channel through which to notify the driver of storage errors
    error_notification_sender: mpsc::UnboundedSender<ErrorNotification>,

    // A channel through which to notify the executor of new data chunks
    executor_notifier: mpsc::Sender<StorageDataChunk>,
```
