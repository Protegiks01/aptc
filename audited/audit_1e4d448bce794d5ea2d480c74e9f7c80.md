# Audit Report

## Title
Database Leak During V1→V2 Batch Migration: V2 Batches Never Deleted, Causing Unbounded Storage Growth

## Summary
The QuorumStore database contains two critical bugs in batch deletion logic that prevent V2 batches from ever being removed from persistent storage. When the `enable_batch_v2` configuration flag is enabled, expired V2 batches accumulate indefinitely in the database, leading to unbounded storage growth, resource exhaustion, and potential validator failure.

## Finding Description
The QuorumStore system maintains two separate database schemas for batch storage: `BatchSchema` (V1) stored in the "batch" column family and `BatchV2Schema` (V2) stored in the "batch_v2" column family. [1](#0-0) 

During migration controlled by the `enable_batch_v2` configuration flag, validators can create V2 batches that are stored in the separate V2 schema. [2](#0-1) 

**Bug #1: Epoch Transition GC Uses Wrong Deletion Method**

The function `gc_previous_epoch_batches_from_db_v2` is responsible for garbage collecting batches from previous epochs. It correctly reads V2 batches using `get_all_batches_v2()` but incorrectly calls `delete_batches()` instead of `delete_batches_v2()` to delete them. [3](#0-2) 

This is a copy-paste error where the V1 deletion method is used. Since `delete_batches()` operates on BatchSchema while the batches are stored in BatchV2Schema, the V2 batches are never actually deleted from persistent storage. [4](#0-3) [5](#0-4) 

**Bug #2: Periodic Expiration Only Deletes V1 Batches**

The `update_certified_timestamp` method periodically removes expired batches. It calls `clear_expired_payload` to remove entries from the in-memory cache (which can contain both V1 and V2 batches), but then only calls `delete_batches()` to remove from the database, never calling `delete_batches_v2()`. [6](#0-5) 

This means V2 batches are removed from the cache but remain in persistent storage indefinitely, accumulating over time.

**Storage Mode and Persistence**

When batches are persisted, the code correctly branches based on `is_v2()` to save to the appropriate schema. [7](#0-6) 

However, the deletion logic fails to implement the same branching, creating an asymmetry where V2 batches can be written but never deleted.

This breaks the Resource Limits invariant: storage operations should clean up expired data to prevent unbounded growth.

## Impact Explanation
This is a **High Severity** vulnerability under the Aptos bug bounty program, specifically matching the "Validator node slowdowns" category through resource exhaustion.

The impact escalates progressively:
- **Short term (days 1-7)**: Gradual database growth with minimal impact
- **Medium term (weeks 2-4)**: Noticeable performance degradation as database size reaches gigabytes, affecting read/write latency
- **Long term (months 2-3)**: Severe slowdowns with increased I/O wait times, impacting consensus participation
- **Critical (months 3+)**: Risk of disk exhaustion leading to validator crashes and potential consensus disruption

If multiple validators enable `enable_batch_v2` simultaneously during a coordinated migration, widespread validator performance degradation could threaten network liveness. A high-throughput validator creating 1000 batches/day at 100KB each would accumulate approximately 100MB/day or 3GB/month of unreclaimed storage.

The vulnerability also creates state inconsistencies requiring manual intervention, as the database contains expired batches that should have been deleted but persist indefinitely.

## Likelihood Explanation
**Likelihood: CERTAIN when enable_batch_v2 is enabled**

This is a logic vulnerability that will trigger automatically on every validator that:
1. Enables the `enable_batch_v2` configuration flag [8](#0-7) 
2. Creates V2 batches during normal operation
3. Experiences batch expiration through normal consensus progression

No attacker action is required. The bug manifests through normal protocol operation. While the flag defaults to `false`, this represents a latent security flaw in the V2 migration code path that will inevitably trigger when validators enable V2 batches for the intended upgrade.

The framework explicitly states that logic vulnerabilities are valid even if not currently triggered, and this is a clear logic error where the wrong deletion method is called.

## Recommendation
Fix both bugs by calling the correct deletion method for V2 batches:

**Fix #1:** In `gc_previous_epoch_batches_from_db_v2`, change line 241 from:
```rust
db.delete_batches(expired_keys)
```
to:
```rust
db.delete_batches_v2(expired_keys)
```

**Fix #2:** In `update_certified_timestamp`, modify the deletion logic to handle both V1 and V2 batches. The cache stores `PersistedValue<BatchInfoExt>` which contains version information, so the expired keys should be separated by version and deleted using the appropriate method.

Consider adding a check to determine which schema each batch belongs to and call the corresponding deletion method, similar to how `persist_inner` branches based on `is_v2()`.

## Proof of Concept
This is a logic vulnerability in the code structure. The bug can be verified through code inspection:

1. Compare `gc_previous_epoch_batches_from_db_v1` (lines 181-210) with `gc_previous_epoch_batches_from_db_v2` (lines 212-243) in `batch_store.rs`
2. Observe that V1 correctly uses `get_all_batches()` → `delete_batches()`
3. Observe that V2 incorrectly uses `get_all_batches_v2()` → `delete_batches()` instead of `delete_batches_v2()`
4. Verify that `delete_batches()` and `delete_batches_v2()` operate on different column families in `quorum_store_db.rs`

To reproduce at runtime, enable `enable_batch_v2 = true` in the validator configuration, wait for batches to expire, and verify that entries remain in the "batch_v2" column family after garbage collection runs.

## Notes
This vulnerability is classified as HIGH severity because it causes validator node slowdowns through unbounded resource consumption, directly matching the Aptos bug bounty criteria. While the `enable_batch_v2` flag defaults to false, this is a legitimate security flaw in production code that will impact validators during the V2 migration period. The framework explicitly recognizes logic vulnerabilities as valid even when not currently triggered in the default configuration.

### Citations

**File:** consensus/src/quorum_store/schema.rs (L14-55)
```rust
pub(crate) const BATCH_CF_NAME: ColumnFamilyName = "batch";
pub(crate) const BATCH_ID_CF_NAME: ColumnFamilyName = "batch_ID";
pub(crate) const BATCH_V2_CF_NAME: ColumnFamilyName = "batch_v2";

#[derive(Debug)]
pub(crate) struct BatchSchema;

impl Schema for BatchSchema {
    type Key = HashValue;
    type Value = PersistedValue<BatchInfo>;

    const COLUMN_FAMILY_NAME: aptos_schemadb::ColumnFamilyName = BATCH_CF_NAME;
}

impl KeyCodec<BatchSchema> for HashValue {
    fn encode_key(&self) -> Result<Vec<u8>> {
        Ok(self.to_vec())
    }

    fn decode_key(data: &[u8]) -> Result<Self> {
        Ok(HashValue::from_slice(data)?)
    }
}

impl ValueCodec<BatchSchema> for PersistedValue<BatchInfo> {
    fn encode_value(&self) -> Result<Vec<u8>> {
        Ok(bcs::to_bytes(&self)?)
    }

    fn decode_value(data: &[u8]) -> Result<Self> {
        Ok(bcs::from_bytes(data)?)
    }
}

#[derive(Debug)]
pub(crate) struct BatchV2Schema;

impl Schema for BatchV2Schema {
    type Key = HashValue;
    type Value = PersistedValue<BatchInfoExt>;

    const COLUMN_FAMILY_NAME: aptos_schemadb::ColumnFamilyName = BATCH_V2_CF_NAME;
```

**File:** config/src/config/quorum_store_config.rs (L102-102)
```rust
    pub enable_batch_v2: bool,
```

**File:** config/src/config/quorum_store_config.rs (L144-144)
```rust
            enable_batch_v2: false,
```

**File:** consensus/src/quorum_store/batch_store.rs (L212-243)
```rust
    fn gc_previous_epoch_batches_from_db_v2(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L488-528)
```rust
    fn persist_inner(
        &self,
        batch_info: BatchInfoExt,
        persist_request: PersistedValue<BatchInfoExt>,
    ) -> Option<SignedBatchInfo<BatchInfoExt>> {
        assert!(
            &batch_info == persist_request.batch_info(),
            "Provided batch info doesn't match persist request batch info"
        );
        match self.save(&persist_request) {
            Ok(needs_db) => {
                trace!("QS: sign digest {}", persist_request.digest());
                if needs_db {
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
                }
                if !batch_info.is_v2() {
                    self.generate_signed_batch_info(batch_info.info().clone())
                        .ok()
                        .map(|inner| inner.into())
                } else {
                    self.generate_signed_batch_info(batch_info).ok()
                }
            },
            Err(e) => {
                debug!("QS: failed to store to cache {:?}", e);
                None
            },
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L93-101)
```rust
    fn delete_batches(&self, digests: Vec<HashValue>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        for digest in digests.iter() {
            trace!("QS: db delete digest {}", digest);
            batch.delete::<BatchSchema>(digest)?;
        }
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L123-131)
```rust
    fn delete_batches_v2(&self, digests: Vec<HashValue>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        for digest in digests.iter() {
            trace!("QS: db delete digest {}", digest);
            batch.delete::<BatchV2Schema>(digest)?;
        }
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```
