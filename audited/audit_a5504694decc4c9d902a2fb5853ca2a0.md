# Audit Report

## Title
Byzantine Peers Can Exhaust Block Retrieval Retries Causing Node Liveness Failure

## Summary
The `retrieve_block_chunk()` function implements a fixed retry limit that caps the maximum number of peers contacted at 13, regardless of the total number of available peers in the quorum certificate signer list. Byzantine or unresponsive peers can exhaust this limit by responding slowly, incorrectly, or timing out, preventing legitimate block retrieval even when honest peers remain available but uncontacted. This causes affected nodes to fall behind in consensus and fail to participate in validation.

## Finding Description
The block retrieval mechanism in the consensus layer has a fundamental design flaw in its retry logic that can be exploited to cause node liveness failures. [1](#0-0) 

The retry constants define:
- `NUM_RETRIES = 5`: Maximum 5 retry attempts
- `NUM_PEERS_PER_RETRY = 3`: 3 peers contacted per retry (after first attempt)
- Total peers contacted: 1 (preferred peer) + 3×4 (retries 1-4) = **13 peers maximum** [2](#0-1) 

The critical flaw occurs in the retry logic. The function increments `cur_retry` on each interval tick (every 500ms) and stops selecting new peers once `cur_retry >= NUM_RETRIES` (line 742). The termination condition only triggers when both no more peers are being selected AND all pending requests have completed: [3](#0-2) 

**Attack Path:**

1. A node needs to sync and calls `insert_quorum_cert()`, which triggers block retrieval [4](#0-3) 

2. The QC signer list contains, for example, 50+ validators with ≤33% Byzantine
3. The `retrieve_block_chunk()` function selects peers:
   - t=0ms: Preferred peer (if Byzantine: responds with invalid data or times out)
   - t=500ms: 3 random peers
   - t=1000ms: 3 random peers  
   - t=1500ms: 3 random peers
   - t=2000ms: 3 random peers
   - t=2500ms: Stops selecting (cur_retry=5)

4. Byzantine peers can:
   - Respond with blocks that fail signature verification [5](#0-4) 
   - Respond with malformed blocks
   - Timeout after 5000ms
   - Respond slowly (just under timeout)

5. When responses fail, they increment `failed_attempt` but don't trigger additional peer selection: [6](#0-5) 

6. If all 13 selected peers fail (Byzantine, slow, or network issues), and there are 37+ honest peers remaining that were never contacted, the function still bails with "Couldn't fetch block"

7. This failure propagates up through `retrieve_blocks()`, causing `fetch_quorum_cert()` to fail [7](#0-6) 

8. The node cannot insert the QC, fails in `new_qc_aggregated()`, and cannot participate in consensus: [8](#0-7) 

**Scenarios where this is exploitable:**

- **Small validator sets**: 15-20 validators with 5-7 Byzantine can more easily exhaust 13 attempts
- **Network partitions**: Honest peers unreachable while Byzantine peers responsive
- **Coordinated attack**: Byzantine validators deliberately respond incorrectly/slowly as preferred peer or early selections
- **Natural network degradation**: Legitimate network issues affecting multiple honest peers combined with a few Byzantine peers

## Impact Explanation
This is a **Medium severity** vulnerability per Aptos bug bounty criteria:

**Consensus Liveness Impact:**
- Affected nodes cannot retrieve blocks required for QC insertion
- Nodes fall behind in consensus rounds
- Cannot participate in voting or validation
- Must wait for alternative sync mechanisms or manual intervention

**Not Critical/High because:**
- Does NOT break consensus safety (no double-spend, no chain splits)
- Does NOT cause direct fund loss
- Does NOT affect all nodes simultaneously (attack targets specific nodes)
- Network can continue with remaining validators

**Why Medium:**
- Causes "state inconsistencies requiring intervention" 
- Individual node availability degradation
- Reduced network participation if multiple nodes affected
- Requires Byzantine coordination or adverse network conditions, limiting scope

The impact is amplified if:
- Multiple nodes targeted simultaneously
- Small validator sets (higher probability of exhausting 13 peers)
- During critical epochs (network upgrades, high transaction volume)

## Likelihood Explanation
**Overall Likelihood: MEDIUM**

**Factors increasing likelihood:**
- **Small validator sets (MEDIUM-HIGH)**: With 10-20 validators and 3-7 Byzantine, selecting 13 faulty peers is statistically plausible
- **Network instability (MEDIUM)**: Legitimate network issues can cause timeouts, which combined with Byzantine peers exhausts retries
- **Byzantine preferred peer (MEDIUM)**: If the block proposer is Byzantine, the first attempt always fails

**Factors decreasing likelihood:**
- **Large validator sets (LOW)**: With 100+ validators and ≤33% Byzantine, randomly selecting 13 Byzantine/faulty peers from 67+ QC signers is statistically very unlikely
- **Peer selection randomization**: Random peer selection makes targeted attacks harder
- **Signature verification**: Byzantine peers cannot send validly-signed blocks from honest validators

**Realistic attack requirements:**
- Byzantine validators (up to 1/3 of validator set) - already assumed in BFT threat model
- Ability to influence peer selection (low) or wait for random selection to favor Byzantine peers
- OR natural network conditions causing honest peer timeouts

The vulnerability is most concerning in:
1. Testnets/devnets with small validator sets
2. Network degradation scenarios
3. Epoch transitions where sync is critical

## Recommendation
**Immediate Fix: Remove the fixed retry limit and implement adaptive retry logic**

```rust
// In consensus/consensus-types/src/block_retrieval.rs
// Replace fixed limits with adaptive constants
pub const NUM_RETRIES: usize = usize::MAX; // Remove artificial limit
pub const MAX_FAILED_PEERS: usize = 20;     // Stop after 20 consecutive failures
pub const MIN_SUCCESS_RATE: f64 = 0.1;      // Require 10% success rate
```

**Primary fix in retrieve_block_chunk():**

```rust
async fn retrieve_block_chunk(
    &mut self,
    block_id: HashValue,
    target_block_retrieval_payload: TargetBlockRetrieval,
    retrieve_batch_size: u64,
    mut peers: Vec<AccountAddress>,
) -> anyhow::Result<BlockRetrievalResponse> {
    let mut failed_attempt = 0_u32;
    let mut total_attempts = 0_u32;
    
    // Continue retrying while we have peers AND haven't failed too many times
    let max_consecutive_failures = 20;
    let retry_interval = Duration::from_millis(RETRY_INTERVAL_MSEC);
    let rpc_timeout = Duration::from_millis(RPC_TIMEOUT_MSEC);
    
    monitor!("retrieve_block_for_id_chunk", {
        let mut interval = time::interval(retry_interval);
        let mut futures = FuturesUnordered::new();
        
        // ... [existing self-retrieval logic] ...
        
        loop {
            tokio::select! {
                biased;
                Some((peer, response)) = futures.next() => {
                    total_attempts += 1;
                    match response {
                        Ok(result) => {
                            // Success - reset failure counter
                            return Ok(result)
                        },
                        e => {
                            warn!(
                                remote_peer = peer,
                                block_id = block_id,
                                "Failed to fetch block: {:?}",
                                e,
                            );
                            failed_attempt += 1;
                            
                            // Only bail if we've exhausted all peers
                            if failed_attempt >= max_consecutive_failures && 
                               peers.is_empty() && 
                               futures.is_empty() {
                                bail!("Exhausted all available peers after {} failures", failed_attempt)
                            }
                        },
                    }
                },
                _ = interval.tick() => {
                    // Continue picking peers while we have any available
                    let next_peers = if !peers.is_empty() {
                        self.pick_peers(false, &mut peers, NUM_PEERS_PER_RETRY)
                    } else {
                        Vec::new()
                    };
                    
                    // Only bail if no peers left AND no pending requests
                    if next_peers.is_empty() && futures.is_empty() {
                        bail!("No more peers available and all requests completed")
                    }
                    
                    for peer in next_peers {
                        // ... [existing request sending logic] ...
                    }
                }
            }
        }
    })
}
```

**Key improvements:**
1. Remove fixed `NUM_RETRIES` limit - continue while peers available
2. Add `max_consecutive_failures` to prevent infinite loops
3. Only terminate when ALL peers exhausted AND all requests complete
4. Track success rates for future adaptive strategies

**Alternative: Exponential backoff with peer pool expansion**
Consider fetching additional peers from the broader validator set if initial QC signers are unresponsive.

## Proof of Concept

```rust
#[cfg(test)]
mod test_retry_exhaustion {
    use super::*;
    use std::sync::Arc;
    use tokio::sync::Mutex;
    
    #[tokio::test]
    async fn test_byzantine_peers_exhaust_retries() {
        // Setup: Create a BlockRetriever with simulated network
        let mut retriever = create_test_retriever();
        
        // Simulate 50 validators in QC, 15 are Byzantine
        let mut peers: Vec<AccountAddress> = (0..50)
            .map(|i| AccountAddress::from_hex_literal(&format!("0x{:x}", i)).unwrap())
            .collect();
        
        // Mark first 15 as Byzantine (will fail requests)
        let byzantine_peers: HashSet<_> = peers[0..15].iter().cloned().collect();
        
        // Configure network to fail for Byzantine peers
        retriever.network = Arc::new(MockNetworkSender::new(move |peer, _, _| {
            if byzantine_peers.contains(&peer) {
                // Byzantine peer: return error after delay
                async move {
                    tokio::time::sleep(Duration::from_millis(100)).await;
                    Err(anyhow!("Byzantine peer invalid response"))
                }.boxed()
            } else {
                // Honest peer: return valid response
                async move {
                    Ok(BlockRetrievalResponse::new(
                        BlockRetrievalStatus::SucceededWithTarget,
                        vec![create_test_block()]
                    ))
                }.boxed()
            }
        }));
        
        // Set preferred peer to Byzantine
        retriever.preferred_peer = peers[0];
        
        // Shuffle so Byzantine peers are selected early
        peers.shuffle(&mut thread_rng());
        peers.rotate_left(13); // Ensure first 13 include many Byzantine
        
        // Attempt retrieval - should fail despite 35 honest peers available
        let result = retriever.retrieve_block_chunk(
            HashValue::random(),
            TargetBlockRetrieval::TargetBlockId(HashValue::random()),
            1,
            peers,
        ).await;
        
        // Verify: Retrieval fails even though honest peers were available
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("Couldn't fetch block"));
        
        // This demonstrates the vulnerability: 13 Byzantine/faulty peers 
        // exhausted the retry limit, and 37 honest peers were never contacted
    }
    
    #[tokio::test] 
    async fn test_network_degradation_exhausts_retries() {
        let mut retriever = create_test_retriever();
        let peers: Vec<AccountAddress> = (0..50)
            .map(|i| AccountAddress::from_hex_literal(&format!("0x{:x}", i)).unwrap())
            .collect();
        
        // Simulate network where first 13 peers timeout (not Byzantine, just slow)
        retriever.network = Arc::new(MockNetworkSender::new(|peer_idx, _, _| {
            async move {
                if peer_idx < 13 {
                    // Simulate timeout
                    tokio::time::sleep(Duration::from_millis(5500)).await;
                    Err(anyhow!("Timeout"))
                } else {
                    // Fast honest response
                    Ok(BlockRetrievalResponse::new(
                        BlockRetrievalStatus::SucceededWithTarget,
                        vec![create_test_block()]
                    ))
                }
            }.boxed()
        }));
        
        let result = retriever.retrieve_block_chunk(
            HashValue::random(),
            TargetBlockRetrieval::TargetBlockId(HashValue::random()),
            1,
            peers,
        ).await;
        
        // Network degradation + retry limit = failure
        assert!(result.is_err());
    }
}
```

**Notes:**
- Full PoC requires mocking the network layer and consensus infrastructure
- The test demonstrates that with Byzantine/slow peers selected within the first 13, retrieval fails despite available honest peers
- This can be reproduced in integration tests with simulated Byzantine validators

### Citations

**File:** consensus/consensus-types/src/block_retrieval.rs (L12-15)
```rust
pub const NUM_RETRIES: usize = 5;
pub const NUM_PEERS_PER_RETRY: usize = 3;
pub const RETRY_INTERVAL_MSEC: u64 = 500;
pub const RPC_TIMEOUT_MSEC: u64 = 5000;
```

**File:** consensus/consensus-types/src/block_retrieval.rs (L260-281)
```rust
    pub fn verify(
        &self,
        retrieval_request: BlockRetrievalRequest,
        sig_verifier: &ValidatorVerifier,
    ) -> anyhow::Result<()> {
        self.verify_inner(&retrieval_request)?;

        self.blocks
            .iter()
            .try_fold(retrieval_request.block_id(), |expected_id, block| {
                block.validate_signature(sig_verifier)?;
                block.verify_well_formed()?;
                ensure!(
                    block.id() == expected_id,
                    "blocks doesn't form a chain: expect {}, get {}",
                    expected_id,
                    block.id()
                );
                Ok(block.parent_id())
            })
            .map(|_| ())
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L175-201)
```rust
    pub async fn insert_quorum_cert(
        &self,
        qc: &QuorumCert,
        retriever: &mut BlockRetriever,
    ) -> anyhow::Result<()> {
        match self.need_fetch_for_quorum_cert(qc) {
            NeedFetchResult::NeedFetch => self.fetch_quorum_cert(qc.clone(), retriever).await?,
            NeedFetchResult::QCBlockExist => self.insert_single_quorum_cert(qc.clone())?,
            NeedFetchResult::QCAlreadyExist => return Ok(()),
            _ => (),
        }
        if self.ordered_root().round() < qc.commit_info().round() {
            SUCCESSFUL_EXECUTED_WITH_REGULAR_QC.inc();
            self.send_for_execution(qc.into_wrapped_ledger_info())
                .await?;
            if qc.ends_epoch() {
                retriever
                    .network
                    .broadcast_epoch_change(EpochChangeProof::new(
                        vec![qc.ledger_info().clone()],
                        /* more = */ false,
                    ))
                    .await;
            }
        }
        Ok(())
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L233-270)
```rust
    async fn fetch_quorum_cert(
        &self,
        qc: QuorumCert,
        retriever: &mut BlockRetriever,
    ) -> anyhow::Result<()> {
        let mut pending = vec![];
        let mut retrieve_qc = qc.clone();
        loop {
            if self.block_exists(retrieve_qc.certified_block().id()) {
                break;
            }
            BLOCKS_FETCHED_FROM_NETWORK_WHILE_INSERTING_QUORUM_CERT.inc_by(1);
            let target_block_retrieval_payload = match &self.window_size {
                None => TargetBlockRetrieval::TargetBlockId(retrieve_qc.certified_block().id()),
                Some(_) => TargetBlockRetrieval::TargetRound(retrieve_qc.certified_block().round()),
            };
            let mut blocks = retriever
                .retrieve_blocks_in_range(
                    retrieve_qc.certified_block().id(),
                    1,
                    target_block_retrieval_payload,
                    qc.ledger_info()
                        .get_voters(&retriever.validator_addresses()),
                )
                .await?;
            // retrieve_blocks_in_range guarantees that blocks has exactly 1 element
            let block = blocks.remove(0);
            retrieve_qc = block.quorum_cert().clone();
            pending.push(block);
        }
        // insert the qc <- block pair
        while let Some(block) = pending.pop() {
            let block_qc = block.quorum_cert().clone();
            self.insert_single_quorum_cert(block_qc)?;
            self.insert_block(block).await?;
        }
        self.insert_single_quorum_cert(qc)
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L667-779)
```rust
    async fn retrieve_block_chunk(
        &mut self,
        block_id: HashValue,
        target_block_retrieval_payload: TargetBlockRetrieval,
        retrieve_batch_size: u64,
        mut peers: Vec<AccountAddress>,
    ) -> anyhow::Result<BlockRetrievalResponse> {
        let mut failed_attempt = 0_u32;
        let mut cur_retry = 0;

        let num_retries = NUM_RETRIES;
        let request_num_peers = NUM_PEERS_PER_RETRY;
        let retry_interval = Duration::from_millis(RETRY_INTERVAL_MSEC);
        let rpc_timeout = Duration::from_millis(RPC_TIMEOUT_MSEC);

        monitor!("retrieve_block_for_id_chunk", {
            let mut interval = time::interval(retry_interval);
            let mut futures = FuturesUnordered::new();
            if retrieve_batch_size == 1 {
                let (tx, rx) = oneshot::channel();
                self.pending_blocks
                    .lock()
                    .insert_request(target_block_retrieval_payload, tx);
                let author = self.network.author();
                futures.push(
                    async move {
                        let response = match timeout(rpc_timeout, rx).await {
                            Ok(Ok(block)) => Ok(BlockRetrievalResponse::new(
                                BlockRetrievalStatus::SucceededWithTarget,
                                vec![block],
                            )),
                            Ok(Err(_)) => Err(anyhow!("self retrieval cancelled")),
                            Err(_) => Err(anyhow!("self retrieval timeout")),
                        };
                        (author, response)
                    }
                    .boxed(),
                )
            }
            let request = match target_block_retrieval_payload {
                TargetBlockRetrieval::TargetBlockId(target_block_id) => {
                    BlockRetrievalRequest::V1(BlockRetrievalRequestV1::new_with_target_block_id(
                        block_id,
                        retrieve_batch_size,
                        target_block_id,
                    ))
                },
                TargetBlockRetrieval::TargetRound(target_round) => {
                    BlockRetrievalRequest::V2(BlockRetrievalRequestV2::new_with_target_round(
                        block_id,
                        retrieve_batch_size,
                        target_round,
                    ))
                },
            };

            loop {
                tokio::select! {
                    biased;
                    Some((peer, response)) = futures.next() => {
                        match response {
                            Ok(result) => return Ok(result),
                            e => {
                                warn!(
                                    remote_peer = peer,
                                    block_id = block_id,
                                    "{:?}, Failed to fetch block",
                                    e,
                                );
                                failed_attempt += 1;
                            },
                        }
                    },
                    _ = interval.tick() => {
                        // send batch request to a set of peers of size request_num_peers (or 1 for the first time)
                        let next_peers = if cur_retry < num_retries {
                            let first_attempt = cur_retry == 0;
                            cur_retry += 1;
                            self.pick_peers(
                                first_attempt,
                                &mut peers,
                                if first_attempt { 1 } else {request_num_peers}
                            )
                        } else {
                            Vec::new()
                        };

                        if next_peers.is_empty() && futures.is_empty() {
                            bail!("Couldn't fetch block")
                        }

                        for peer in next_peers {
                            debug!(
                                LogSchema::new(LogEvent::RetrieveBlock).remote_peer(peer),
                                block_id = block_id,
                                "Fetching {} blocks, retry {}, failed attempts {}",
                                retrieve_batch_size,
                                cur_retry,
                                failed_attempt
                            );
                            let remote_peer = peer;
                            let future = self.network.request_block(
                                request.clone(),
                                peer,
                                rpc_timeout,
                            );
                            futures.push(async move { (remote_peer, future.await) }.boxed());
                        }
                    }
                }
            }
        })
    }
```

**File:** consensus/src/round_manager.rs (L1925-1936)
```rust
    async fn new_qc_aggregated(
        &mut self,
        qc: Arc<QuorumCert>,
        preferred_peer: Author,
    ) -> anyhow::Result<()> {
        let result = self
            .block_store
            .insert_quorum_cert(&qc, &mut self.create_block_retriever(preferred_peer))
            .await
            .context("[RoundManager] Failed to process a newly aggregated QC");
        self.process_certificates().await?;
        result
```
