# Audit Report

## Title
Pruner Worker Thread Join Can Block Node Shutdown Indefinitely Due to Database Operation Hang

## Summary
The `PrunerWorker::drop()` implementation performs an unconditional blocking `join()` on the worker thread. If the worker thread is blocked inside a synchronous RocksDB write operation due to database errors (disk I/O hang, lock contention, corruption), the `join()` call will block indefinitely, preventing graceful node shutdown. [1](#0-0) 

## Finding Description

The vulnerability exists in the shutdown sequence of the pruner worker:

1. **Worker Thread Loop**: The background worker thread continuously calls `self.pruner.prune(self.batch_size)` which performs synchronous database writes. [2](#0-1) 

2. **Database Write Operations**: The pruner implementations (LedgerPruner, StateKvPruner) perform synchronous writes via `write_schemas()` which uses `sync_write_option()` with `set_sync(true)`. [3](#0-2) [4](#0-3) [5](#0-4) 

3. **Blocking RocksDB Write**: The synchronous write to RocksDB (`write_opt`) can block indefinitely in scenarios such as:
   - Disk I/O stalls or hardware failures
   - RocksDB internal lock contention with stuck operations
   - Database corruption triggering long recovery attempts
   - Write stalls due to compaction backlog [6](#0-5) 

4. **Drop Hang**: When the node attempts to shut down and `PrunerWorker` is dropped:
   - `stop_pruning()` sets the `quit_worker` flag
   - But the worker thread is already blocked inside the `prune()` call's database write
   - The thread cannot check the quit flag because it's blocked
   - The `join()` call waits indefinitely for the thread to complete
   - Node shutdown hangs

The `quit_worker` flag is only checked at the start of the work loop iteration, not during the blocking database operation itself.

## Impact Explanation

**Severity: Medium** (up to $10,000 per bug bounty criteria)

This qualifies as a **state inconsistency requiring intervention** under the Medium severity category. Specifically:

- **Prevents Graceful Shutdown**: Nodes cannot shut down cleanly when database issues occur, requiring forceful termination (SIGKILL)
- **Operational Impact**: During maintenance windows, upgrades, or configuration changes, nodes may hang during restart
- **Resource Cleanup Failure**: Other resources may not be properly cleaned up if Drop panics or hangs
- **Cascading Effects**: If multiple nodes in a validator set experience database issues simultaneously, coordinated maintenance becomes difficult

While this doesn't directly affect consensus safety or cause fund loss, it creates operational reliability issues that can indirectly impact network availability during critical maintenance operations.

## Likelihood Explanation

**Likelihood: Medium**

This issue can occur in production scenarios:

1. **Disk Hardware Degradation**: As validator nodes age, disk I/O performance can degrade, leading to occasional hangs
2. **High Load Conditions**: Under heavy write load, RocksDB compaction can cause write stalls
3. **Database Corruption**: Power failures or kernel crashes can corrupt RocksDB databases, causing recovery operations to hang
4. **Network Storage Issues**: Nodes using network-attached storage (NAS/SAN) may experience I/O hangs during network issues

The likelihood increases with:
- Node uptime (longer running nodes accumulate more database state)
- Hardware age and quality
- Storage backend characteristics
- Operational load patterns

## Recommendation

Implement a timeout-based shutdown mechanism with graceful degradation:

```rust
impl Drop for PrunerWorker {
    fn drop(&mut self) {
        self.inner.stop_pruning();
        
        if let Some(handle) = self.worker_thread.take() {
            // Use a timeout for the join operation
            let timeout = Duration::from_secs(30); // Configurable timeout
            let start = std::time::Instant::now();
            
            // Poll the thread with timeout
            loop {
                if handle.is_finished() {
                    // Thread completed, join it
                    handle.join().unwrap_or_else(|e| {
                        error!(
                            "Pruner worker ({}) thread panicked during shutdown: {:?}",
                            self.worker_name, e
                        );
                    });
                    break;
                }
                
                if start.elapsed() >= timeout {
                    error!(
                        "Pruner worker ({}) thread did not complete within {}s timeout. \
                         Proceeding with shutdown (thread will be detached).",
                        self.worker_name,
                        timeout.as_secs()
                    );
                    // Don't join - let the thread be detached
                    // This allows shutdown to proceed but logs the issue
                    break;
                }
                
                std::thread::sleep(Duration::from_millis(100));
            }
        }
    }
}
```

Additional recommendations:
1. Add timeout configuration option for pruner shutdown
2. Implement health checking in the pruner loop to detect hung operations
3. Use async/await patterns with timeout support for database operations
4. Add monitoring/metrics for pruner thread health
5. Consider implementing a separate watchdog thread that can forcefully terminate stuck operations

## Proof of Concept

```rust
// Reproduction steps (requires running Aptos node environment):

#[test]
fn test_pruner_worker_hang_on_drop() {
    use std::sync::{Arc, Mutex};
    use std::sync::atomic::{AtomicBool, Ordering};
    use std::time::Duration;
    
    // Create a mock pruner that blocks indefinitely
    struct HangingPruner {
        should_hang: Arc<AtomicBool>,
    }
    
    impl DBPruner for HangingPruner {
        fn name(&self) -> &'static str {
            "hanging_pruner"
        }
        
        fn prune(&self, _batch_size: usize) -> Result<Version> {
            if self.should_hang.load(Ordering::SeqCst) {
                // Simulate a hung database operation
                std::thread::park(); // Block indefinitely
            }
            Ok(0)
        }
        
        fn progress(&self) -> Version { 0 }
        fn set_target_version(&self, _target: Version) {}
        fn target_version(&self) -> Version { 0 }
        fn record_progress(&self, _progress: Version) {}
    }
    
    let should_hang = Arc::new(AtomicBool::new(false));
    let pruner = Arc::new(HangingPruner {
        should_hang: should_hang.clone(),
    });
    
    // Create the pruner worker
    let worker = PrunerWorker::new(pruner, 100, "test");
    
    // Give the worker thread time to start
    std::thread::sleep(Duration::from_millis(100));
    
    // Now trigger the hang
    should_hang.store(true, Ordering::SeqCst);
    
    // Wait for worker to enter the hung prune() call
    std::thread::sleep(Duration::from_millis(500));
    
    // Drop the worker - this will hang indefinitely
    // In a real scenario, this would prevent node shutdown
    let start = std::time::Instant::now();
    
    // Spawn drop in a separate thread to demonstrate the hang
    let drop_thread = std::thread::spawn(move || {
        drop(worker); // This will hang
    });
    
    // Wait a reasonable time
    std::thread::sleep(Duration::from_secs(5));
    
    // The drop thread should still be running (hung)
    assert!(!drop_thread.is_finished(), 
            "Drop thread should be hung waiting for worker thread");
    
    println!("âœ“ Demonstrated: PrunerWorker drop hangs when worker thread is blocked");
}
```

To reproduce in a live environment:
1. Start an Aptos node with pruning enabled
2. Simulate disk I/O issues (e.g., use `tc` to add latency, or use `cgroup` blkio limits)
3. Trigger database write stalls by filling disk or causing compaction backlog
4. Attempt to gracefully shutdown the node (SIGTERM)
5. Observe that the node hangs during shutdown
6. SIGKILL is required to terminate the process

## Notes

This vulnerability affects all three pruner implementations in the codebase:
- LedgerPruner (ledger data pruning)
- StateKvPruner (state key-value pruning)  
- StateMerklePruner (state merkle tree pruning) [7](#0-6) [8](#0-7) 

A similar pattern exists in `BufferedState` which also performs blocking operations during Drop: [9](#0-8) [10](#0-9) 

All of these components should be reviewed for timeout-based shutdown mechanisms to ensure graceful degradation under failure conditions.

### Citations

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L105-118)
```rust
impl Drop for PrunerWorker {
    fn drop(&mut self) {
        self.inner.stop_pruning();
        self.worker_thread
            .take()
            .unwrap_or_else(|| panic!("Pruner worker ({}) thread must exist.", self.worker_name))
            .join()
            .unwrap_or_else(|e| {
                panic!(
                    "Pruner worker ({}) thread should join peacefully: {e:?}",
                    self.worker_name
                )
            });
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_metadata_pruner.rs (L42-56)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();
        for version in current_progress..target_version {
            batch.delete::<VersionDataSchema>(&version)?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::LedgerPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_metadata_db.write_schemas(batch)
    }
```

**File:** storage/schemadb/src/lib.rs (L289-309)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
    }

    /// Writes a group of records wrapped in a [`SchemaBatch`].
    pub fn write_schemas(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &sync_write_option())
    }
```

**File:** storage/schemadb/src/lib.rs (L374-378)
```rust
fn sync_write_option() -> rocksdb::WriteOptions {
    let mut opts = rocksdb::WriteOptions::default();
    opts.set_sync(true);
    opts
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L159-159)
```rust
        PrunerWorker::new(pruner, ledger_pruner_config.batch_size, "ledger")
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L125-125)
```rust
        PrunerWorker::new(pruner, state_kv_pruner_config.batch_size, "state_kv")
```

**File:** storage/aptosdb/src/state_store/buffered_state.rs (L181-189)
```rust
    pub(crate) fn quit(&mut self) {
        if let Some(handle) = self.join_handle.take() {
            self.sync_commit();
            self.state_commit_sender.send(CommitMessage::Exit).unwrap();
            handle
                .join()
                .expect("snapshot commit thread should join peacefully.");
        }
    }
```

**File:** storage/aptosdb/src/state_store/buffered_state.rs (L197-200)
```rust
impl Drop for BufferedState {
    fn drop(&mut self) {
        self.quit()
    }
```
