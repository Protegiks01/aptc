# Audit Report

## Title
Byzantine Shard Result Withholding Causes Indefinite Coordinator Blocking and Validator DoS

## Summary
A Byzantine shard in the remote sharded execution architecture can compute transaction results correctly but withhold them from the coordinator, causing the coordinator to block indefinitely while waiting for results. This results in a complete denial-of-service of the validator node, halting block execution with no timeout or recovery mechanism.

## Finding Description

The sharded block execution system allows splitting transaction execution across multiple remote shards for parallelization. The coordinator dispatches work to shards and waits for their results to aggregate the final block output.

**Attack Flow**:

1. The coordinator sends execution commands to all shards via `RemoteExecutorClient::execute_block()`. [1](#0-0) 

2. Each shard receives the command through `RemoteCoordinatorClient::receive_execute_command()` and executes the assigned transactions in `ShardedExecutorService::start()`. [2](#0-1) 

3. After execution, each shard should send results back via `coordinator_client.send_execution_result(ret)`. [3](#0-2) 

4. The coordinator waits for all shard results in `get_output_from_shards()` using blocking channel receives: [4](#0-3) 

**The Vulnerability**: A Byzantine shard can execute the transactions (consuming CPU, memory, and other resources) but simply **not call** `send_execution_result()` or delay it indefinitely. The coordinator will block forever on the `rx.recv().unwrap()` call, as crossbeam channels' `recv()` method blocks indefinitely until a message arrives or the channel is disconnected.

There is **no timeout mechanism** anywhere in this flow:
- No timeout on the channel receive operation
- No timeout wrapper around `execute_block()` calls
- No fallback or retry logic
- The 5000ms timeout in NetworkController is for RPC timeouts, not channel operations [5](#0-4) 

This breaks the **Resource Limits** invariant (operation timeouts) and causes a **Total loss of liveness** for the affected validator.

## Impact Explanation

**Severity: High** (per Aptos Bug Bounty criteria - "Validator node slowdowns" / "Significant protocol violations")

This vulnerability enables:

1. **Validator Node DoS**: The coordinator becomes permanently blocked, unable to execute any subsequent blocks. The validator effectively goes offline.

2. **Resource Consumption**: Both the Byzantine shard and coordinator have already consumed resources (CPU, memory, network) with no recovery path.

3. **Network Liveness Impact**: If multiple validators in the network use sharded execution and are targeted, this could impact overall network liveness, though not causing complete network failure (requires > 1/3 Byzantine nodes).

4. **No Recovery**: Unlike temporary network issues, there is no automatic recovery. The validator must be manually restarted, losing synchronization and requiring state sync.

While this is categorized as "High" severity, it borders on "Critical" due to the **Total loss of liveness** for the affected validator, which is a Critical severity criterion. However, since it affects individual validators rather than the entire network (unless > 1/3 validators are affected), High severity is appropriate.

## Likelihood Explanation

**Likelihood: High**

1. **Attack Complexity**: Extremely low. A Byzantine shard simply needs to not call `send_execution_result()` after receiving an execution command. This requires minimal code modification.

2. **Attacker Requirements**: Control of a single remote executor shard. While sharded execution is an advanced feature, if deployed in production with remote shards, any compromised shard can exploit this.

3. **Detection Difficulty**: The coordinator would appear to hang during block execution with no clear indication that a specific shard is Byzantine, making debugging difficult.

4. **Reproducibility**: 100% reproducible once the attack is initiated.

5. **Current Deployment**: The likelihood depends on whether remote sharded execution is enabled in production. The code path exists and is used when `get_remote_addresses()` returns non-empty addresses. [6](#0-5) 

## Recommendation

Implement timeout mechanisms at multiple levels:

1. **Immediate Fix**: Add a timeout to the channel receive operation in `get_output_from_shards()`:

```rust
fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
    trace!("RemoteExecutorClient Waiting for results");
    let mut results = vec![];
    let timeout_duration = std::time::Duration::from_secs(30); // configurable
    
    for (shard_id, rx) in self.result_rxs.iter().enumerate() {
        let received_bytes = rx
            .recv_timeout(timeout_duration)
            .map_err(|_| {
                VMStatus::error(StatusCode::UNEXPECTED_ERROR_FROM_KNOWN_MOVE_FUNCTION, 
                    Some(format!("Shard {} failed to respond within timeout", shard_id)))
            })?
            .to_bytes();
        let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
        results.push(result.inner?);
    }
    Ok(results)
}
```

2. **Additional Safeguards**:
   - Add health check / heartbeat mechanism between coordinator and shards
   - Implement retry logic with exponential backoff
   - Add monitoring/alerting for shard response times
   - Consider fallback to local execution if remote shard fails
   - Add circuit breaker pattern to disable problematic shards

3. **Configuration**: Make timeout configurable via `BlockExecutorConfig` to allow tuning based on block complexity and network conditions.

## Proof of Concept

The following Rust test demonstrates the vulnerability:

```rust
#[cfg(test)]
mod byzantine_shard_test {
    use super::*;
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    use aptos_secure_net::network_controller::NetworkController;
    use aptos_types::block_executor::partitioner::PartitionedTransactions;
    
    #[test]
    #[ignore] // Run with --ignored flag as this will hang
    fn test_byzantine_shard_withholding_results() {
        // Setup: Create a coordinator and one Byzantine shard
        let coordinator_addr = "127.0.0.1:50000".parse().unwrap();
        let shard_addr = "127.0.0.1:50001".parse().unwrap();
        
        // Start Byzantine shard that receives commands but never sends results
        thread::spawn(move || {
            let mut byzantine_shard = ExecutorService::new(
                0, // shard_id
                1, // num_shards
                4, // num_threads
                shard_addr,
                coordinator_addr,
                vec![],
            );
            byzantine_shard.start();
            
            // Override the executor service to not send results
            // (In real attack, the shard would modify send_execution_result() to be a no-op)
            loop {
                thread::sleep(Duration::from_secs(1));
                // Byzantine shard keeps running but never sends results
            }
        });
        
        thread::sleep(Duration::from_millis(100));
        
        // Coordinator attempts to execute block
        let coordinator = RemoteExecutorClient::<CachedStateView>::new(
            vec![shard_addr],
            NetworkController::new("test-coordinator".to_string(), coordinator_addr, 5000),
            Some(4),
        );
        
        let state_view = Arc::new(CachedStateView::new_dummy());
        let transactions = PartitionedTransactions::empty(1);
        let config = BlockExecutorConfigFromOnchain::default();
        
        // This call will BLOCK INDEFINITELY waiting for shard results
        // Demonstrating the DoS vulnerability
        let start = std::time::Instant::now();
        let result = coordinator.execute_block(
            state_view,
            transactions,
            4,
            config,
        );
        
        // This line will never execute because recv() blocks forever
        println!("Execution completed in {:?}", start.elapsed());
        assert!(result.is_ok());
    }
}
```

**Expected Behavior**: The test hangs indefinitely on the `execute_block()` call, demonstrating that the coordinator has no recovery mechanism when a shard withholds results.

**Notes**:
- The Byzantine shard executes transactions normally but omits the `send_execution_result()` call
- No error is returned to the coordinator; it simply blocks forever
- Manual intervention (process kill) is required to recover
- In production, this would render the validator node unresponsive until restart

### Citations

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L180-212)
```rust
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        trace!("RemoteExecutorClient Sending block to shards");
        self.state_view_service.set_state_view(state_view);
        let (sub_blocks, global_txns) = transactions.into();
        if !global_txns.is_empty() {
            panic!("Global transactions are not supported yet");
        }
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }

        let execution_results = self.get_output_from_shards()?;

        self.state_view_service.drop_state_view();
        Ok(ShardedExecutionOutput::new(execution_results, vec![]))
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-260)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
```

**File:** secure/net/src/network_controller/mod.rs (L95-113)
```rust
    pub fn new(service: String, listen_addr: SocketAddr, timeout_ms: u64) -> Self {
        let inbound_handler = Arc::new(Mutex::new(InboundHandler::new(
            service.clone(),
            listen_addr,
            timeout_ms,
        )));
        let outbound_handler = OutboundHandler::new(service, listen_addr, inbound_handler.clone());
        info!("Network controller created for node {}", listen_addr);
        Self {
            inbound_handler,
            outbound_handler,
            inbound_rpc_runtime: Runtime::new().unwrap(),
            outbound_rpc_runtime: Runtime::new().unwrap(),
            // we initialize the shutdown handles when we start the network controller
            inbound_server_shutdown_tx: None,
            outbound_task_shutdown_tx: None,
            listen_addr,
        }
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```
