# Audit Report

## Title
Duplicate Sequence Number Assignment in Event V2 Translation on Reprocessing

## Summary
The `translate_event_v2_to_v1()` function lacks idempotency protection, causing the same event to receive different sequence numbers when processed multiple times across different sessions. This violates the uniqueness constraint that each sequence number should map to exactly one event per event key, leading to state inconsistency in the internal indexer database.

## Finding Description

When `DBIndexer::translate_event_v2_to_v1()` translates Event V2 to Event V1, it assigns sequence numbers based on cached or database-persisted values. The sequence number assignment logic in `EventV2TranslationEngine::get_next_sequence_number()` operates as follows: [1](#0-0) 

The vulnerability occurs when the same version range is processed multiple times (due to node restart, crash recovery, or manual reindexing):

**First Processing of Version 100:**
1. Event V2 at (version=100, index=0) is translated
2. `get_next_sequence_number(event_key, default)` returns sequence number N
3. EventByKeySchema entry created: `(event_key, N) -> (100, 0)`
4. EventSequenceNumberSchema entry created: `event_key -> N`
5. Sequence number N is cached in memory

**Reprocessing After Restart:**
1. In-memory cache is empty (new process)
2. Same Event V2 at (version=100, index=0) is translated again
3. `get_next_sequence_number(event_key, default)` reads N from database
4. Returns N + 1 (database value incremented)
5. EventByKeySchema entry created: `(event_key, N+1) -> (100, 0)` (NEW ENTRY)
6. EventSequenceNumberSchema entry updated: `event_key -> N+1` (OVERWRITES)

The code that writes to EventByKeySchema uses `(event_key, sequence_number)` as a compound key: [2](#0-1) 

Since sequence numbers differ (N vs N+1), these create distinct database entries. The result is two EventByKeySchema entries pointing to the same event at version 100 with different sequence numbers, violating the uniqueness invariant.

The EventSequenceNumberSchema is updated at batch completion: [3](#0-2) 

This overwrites the previous value, so future events for this event_key will use the incremented sequence number base, creating a permanent gap.

## Impact Explanation

**Medium Severity - State Inconsistencies Requiring Intervention:**

1. **Index Corruption**: EventByKeySchema contains duplicate entries for the same event with different sequence numbers, breaking the fundamental invariant that sequence numbers uniquely identify events per event key.

2. **Query Result Duplication**: Applications querying events by sequence number ranges will receive the same event multiple times, causing incorrect data aggregation and analytics.

3. **Sequence Number Gaps**: Future events skip sequence numbers, breaking continuity expectations in event-dependent applications.

4. **Data Integrity Violations**: The indexer database no longer represents a consistent view of on-chain events, requiring manual intervention to repair.

This meets the Medium severity criteria: "State inconsistencies requiring intervention" as the internal indexer database becomes corrupted and needs manual cleanup or rebuilding to restore consistency.

## Likelihood Explanation

**Likelihood: Medium-High**

This issue occurs in several realistic operational scenarios:

1. **Node Crash During Indexing**: If a validator node crashes after the internal indexer DB commits a batch but before the processing completes, reprocessing will trigger the bug.

2. **Database Desynchronization**: If the internal indexer DB and main DB become desynchronized (partial backup restore, database recovery, etc.), reprocessing is necessary.

3. **Manual Reindexing**: Operators may need to reindex data for bug fixes or data corrections, triggering duplicate sequence number assignments.

4. **Fast Sync Recovery**: During state sync or fast sync recovery, events may be reprocessed if the internal indexer starts from a checkpoint.

The internal indexer service determines the start version from persisted metadata: [4](#0-3) 

However, there is no validation that versions haven't already been partially processed with different sequence number assignments, and `process_a_batch` contains no duplicate detection logic.

## Recommendation

Implement idempotent sequence number assignment by checking if an event has already been translated before assigning a new sequence number:

**Option 1: Check TranslatedV1EventSchema First**
Before translating, check if the event at (version, index) already exists in TranslatedV1EventSchema and reuse its sequence number:

```rust
pub fn translate_event_v2_to_v1(
    &self,
    v2: &ContractEventV2,
    version: Version,
    index: u64,
) -> Result<Option<ContractEventV1>> {
    // Check if already translated
    if let Ok(existing_v1) = self.indexer_db.get_translated_v1_event_by_version_and_index(version, index) {
        return Ok(Some(existing_v1));
    }
    
    // Proceed with translation using existing logic...
}
```

**Option 2: Add Version-Based Sequence Number Tracking**
Store sequence numbers indexed by (event_key, version, index) instead of just event_key to detect duplicates:

```rust
// New schema: EventTranslationMappingSchema
// Key: (Version, Index), Value: (EventKey, SeqNum)

pub fn get_next_sequence_number(
    &self, 
    event_key: &EventKey, 
    version: Version,
    index: u64,
    default: u64
) -> Result<u64> {
    // Check if this specific event was already assigned a sequence number
    if let Some((stored_key, seq)) = self.internal_indexer_db
        .get::<EventTranslationMappingSchema>(&(version, index))? 
    {
        if stored_key == *event_key {
            return Ok(seq); // Reuse existing sequence number
        }
    }
    
    // Assign new sequence number using existing logic...
}
```

**Option 3: Delete Old Entries on Reprocessing**
Before writing new EventByKeySchema entries, delete any existing entries for the same (version, index):

```rust
// In process_a_batch, before putting new entries:
if let Ok(existing_v1) = self.indexer_db.get_translated_v1_event_by_version_and_index(version, idx) {
    let old_key = *existing_v1.key();
    let old_seq = existing_v1.sequence_number();
    batch.delete::<EventByKeySchema>(&(old_key, old_seq))?;
    batch.delete::<EventByVersionSchema>(&(old_key, version, old_seq))?;
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_temppath::TempPath;
    use aptos_types::contract_event::ContractEventV2;
    
    #[test]
    fn test_duplicate_sequence_number_on_reprocessing() {
        // Setup: Create internal indexer DB and main DB reader
        let tmpdir = TempPath::new();
        let db = open_internal_indexer_db(tmpdir.path(), &default_config()).unwrap();
        let config = InternalIndexerDBConfig::new(true, true, true, 0, true, 10_000);
        let internal_indexer = InternalIndexerDB::new(Arc::new(db), config);
        
        // Mock DB reader with test data
        let db_reader = Arc::new(MockDbReader::new());
        let indexer = DBIndexer::new(internal_indexer.clone(), db_reader.clone());
        
        // First processing: Process version 100 with an Event V2
        let version = 100;
        indexer.process_a_batch(version, version + 1).unwrap();
        
        // Verify first sequence number assignment
        let seq_num_1 = internal_indexer
            .db
            .get::<EventSequenceNumberSchema>(&test_event_key())
            .unwrap()
            .unwrap();
        
        let entries_1: Vec<_> = internal_indexer
            .db
            .iter::<EventByKeySchema>()
            .unwrap()
            .collect();
        assert_eq!(entries_1.len(), 1);
        
        // Simulate restart: Clear cache by creating new indexer instance
        let indexer_after_restart = DBIndexer::new(
            internal_indexer.clone(), 
            db_reader.clone()
        );
        
        // Reprocess same version 100
        indexer_after_restart.process_a_batch(version, version + 1).unwrap();
        
        // Verify duplicate sequence number assignment
        let seq_num_2 = internal_indexer
            .db
            .get::<EventSequenceNumberSchema>(&test_event_key())
            .unwrap()
            .unwrap();
        
        // Bug: seq_num_2 = seq_num_1 + 1 (incremented)
        assert_eq!(seq_num_2, seq_num_1 + 1, "Sequence number incremented on reprocessing");
        
        let entries_2: Vec<_> = internal_indexer
            .db
            .iter::<EventByKeySchema>()
            .unwrap()
            .collect();
        
        // Bug: Now we have TWO entries for the same event
        assert_eq!(entries_2.len(), 2, "Duplicate EventByKeySchema entries created");
        
        // Both entries point to the same (version, index) but with different sequence numbers
        let (key1, seq1) = entries_2[0].as_ref().unwrap().0;
        let (ver1, idx1) = entries_2[0].as_ref().unwrap().1;
        let (key2, seq2) = entries_2[1].as_ref().unwrap().0;
        let (ver2, idx2) = entries_2[1].as_ref().unwrap().1;
        
        assert_eq!(key1, key2, "Same event key");
        assert_eq!(ver1, ver2, "Same version");
        assert_eq!(idx1, idx2, "Same index");
        assert_ne!(seq1, seq2, "Different sequence numbers - VIOLATION!");
    }
}
```

## Notes

This vulnerability specifically affects the internal indexer database used for event querying APIs. It does not directly impact consensus or on-chain state, but creates data inconsistency that can affect:

1. **Event-based Applications**: DApps relying on event queries may receive duplicate or missing events
2. **Indexer Services**: External indexers consuming event data may build inconsistent indexes
3. **Analytics Systems**: Event analytics will produce incorrect results due to duplicate event counts

The issue is particularly problematic because EventByKeySchema is used by the `lookup_events_by_key` function to serve event queries, meaning the corrupted data is exposed through public APIs. [5](#0-4)

### Citations

**File:** storage/indexer/src/event_v2_translator.rs (L190-200)
```rust
    pub fn get_next_sequence_number(&self, event_key: &EventKey, default: u64) -> Result<u64> {
        if let Some(seq) = self.get_cached_sequence_number(event_key) {
            Ok(seq + 1)
        } else {
            let seq = self
                .internal_indexer_db
                .get::<EventSequenceNumberSchema>(event_key)?
                .map_or(default, |seq| seq + 1);
            Ok(seq)
        }
    }
```

**File:** storage/indexer/src/db_indexer.rs (L209-245)
```rust
    pub fn lookup_events_by_key(
        &self,
        event_key: &EventKey,
        start_seq_num: u64,
        limit: u64,
        ledger_version: u64,
    ) -> Result<
        Vec<(
            u64,     // sequence number
            Version, // transaction version it belongs to
            u64,     // index among events for the same transaction
        )>,
    > {
        let mut iter = self.db.iter::<EventByKeySchema>()?;
        iter.seek(&(*event_key, start_seq_num))?;

        let mut result = Vec::new();
        let mut cur_seq = start_seq_num;
        for res in iter.take(limit as usize) {
            let ((path, seq), (ver, idx)) = res?;
            if path != *event_key || ver > ledger_version {
                break;
            }
            if seq != cur_seq {
                let msg = if cur_seq == start_seq_num {
                    "First requested event is probably pruned."
                } else {
                    "DB corruption: Sequence number not continuous."
                };
                bail!("{} expected: {}, actual: {}", msg, cur_seq, seq);
            }
            result.push((seq, ver, idx));
            cur_seq += 1;
        }

        Ok(result)
    }
```

**File:** storage/indexer/src/db_indexer.rs (L464-469)
```rust
                                batch
                                    .put::<EventByKeySchema>(
                                        &(key, sequence_number),
                                        &(version, idx as u64),
                                    )
                                    .expect("Failed to put events by key to a batch");
```

**File:** storage/indexer/src/db_indexer.rs (L511-521)
```rust
            for event_key in event_keys {
                batch
                    .put::<EventSequenceNumberSchema>(
                        &event_key,
                        &self
                            .event_v2_translation_engine
                            .get_cached_sequence_number(&event_key)
                            .unwrap_or(0),
                    )
                    .expect("Failed to put events by key to a batch");
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L102-106)
```rust
        let start_version = self
            .db_indexer
            .indexer_db
            .get_persisted_version()?
            .map_or(0, |v| v + 1);
```
