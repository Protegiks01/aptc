# Audit Report

## Title
Certified Timestamp Divergence Causes Premature Batch Expiration and Block Execution Failures

## Summary
Network partitions and sync lag cause validators to have divergent `certified_timestamp` values. When a validator with a lower certified_timestamp requests a batch, validators with higher certified_timestamps incorrectly signal that the batch has globally expired, causing the requester to short-circuit and fail block execution even when the batch is valid for the block's timestamp.

## Finding Description

The vulnerability exists in the batch request-response flow between validators in the Quorum Store system.

**Core Issue:**

When a validator requests a batch that it doesn't have locally, responders return `BatchResponse::NotFound(ledger_info)` containing their current ledger info from the database. [1](#0-0) 

The requester then checks if the responder's ledger_info timestamp exceeds the batch expiration, and if so, **immediately short-circuits** the entire request without retrying. [2](#0-1) 

**Why This Is Wrong:**

The batch expiration should be evaluated relative to the **block's timestamp** being processed, not the responder's current certified_timestamp. The payload manager correctly checks `block_timestamp <= batch_info.expiration()` before requesting batches, [3](#0-2)  but this context is lost when the batch request is made.

The `request_batch` function signature does not include the block timestamp parameter, [4](#0-3)  and the batch retrieval call passes only the batch expiration without block context. [5](#0-4) 

**Attack Scenario:**

1. Network partition or sync lag occurs
2. Validator A: `certified_time = 150` (ahead)
3. Validator B: `certified_time = 50` (behind)
4. Batch exists with `expiration = 100`
5. Validator A deletes the batch via `clear_expired_payload` because `150 > 100`. [6](#0-5) 
6. Validator B receives block proposal with `timestamp = 90` referencing this batch
7. Validator B checks expiration: `90 ≤ 100` (valid) and attempts to fetch the batch
8. Validator B requests from Validator A
9. Validator A responds: `NotFound(ledger_info)` where `ledger_info.timestamp = 150`
10. Validator B checks: `150 > 100` (TRUE) → **immediately returns error**
11. Validator B **fails to execute the block** despite the batch being valid

The certified_timestamp is updated atomically when blocks are committed through the payload manager's `notify_commit` method, [7](#0-6)  which calls `update_certified_timestamp` to trigger batch expiration. [8](#0-7) 

The batch store also checks expiration when saving batches to reject expired ones. [9](#0-8) 

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:

1. **Validator node slowdowns**: Validators with lower certified_timestamps cannot execute blocks, getting stuck in the consensus process. This matches the "Validator Node Slowdowns (High)" category in the bug bounty program.

2. **Significant protocol violations**: Breaks the invariant that all honest validators should be able to execute the same valid blocks deterministically. Different validators will succeed or fail at executing identical blocks based solely on their certified_timestamp divergence.

3. **Liveness Impact**: If enough validators are behind during network partition or high latency, consensus cannot proceed efficiently as multiple validators cannot execute the proposed blocks, leading to temporary liveness degradation.

4. **Asymmetric Execution**: Different validators succeed/fail at executing identical blocks based on their certified_timestamp, violating deterministic execution guarantees.

This does not constitute a Critical severity issue because:
- No direct fund loss or theft
- No permanent network partition (validators will eventually catch up)
- No state corruption (execution fails rather than producing wrong state)
- Temporary rather than permanent liveness impact

## Likelihood Explanation

**High Likelihood:**

- Network partitions occur naturally in distributed systems, especially in geographically distributed validator sets
- Validators routinely operate at different sync states during normal operation, particularly after brief disconnections or during high network latency
- No malicious behavior required - happens through natural network conditions
- The vulnerability triggers whenever:
  - A block references a batch that some validators have already deleted due to higher certified_timestamp
  - The requesting validator queries a validator with higher certified_timestamp
  - This is a common occurrence during network latency spikes, partition recovery, or when validators restart

The test suite confirms this behavior is intentional for the "globally expired" case, [10](#0-9)  but fails to account for scenarios where the batch is still valid for the block being executed despite being expired relative to some validators' local state.

## Recommendation

**Fix Option 1: Pass block_timestamp to request_batch**

Modify the `request_batch` signature to include the block timestamp:
```rust
pub(crate) async fn request_batch(
    &self,
    digest: HashValue,
    expiration: u64,
    block_timestamp: u64,  // Add this parameter
    responders: Arc<Mutex<BTreeSet<PeerId>>>,
    mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
) -> ExecutorResult<Vec<SignedTransaction>>
```

Then change the expiration check to:
```rust
if ledger_info.commit_info().epoch() == epoch
    && ledger_info.commit_info().timestamp_usecs() > block_timestamp  // Compare to block_timestamp
    && ledger_info.verify_signatures(&validator_verifier).is_ok()
{
    // Short-circuit only if the responder's chain has moved past the block timestamp
}
```

**Fix Option 2: Remove the short-circuit entirely**

Remove the short-circuit logic and rely solely on the block-level expiration check in the payload manager, allowing retries to other validators who may still have the batch.

## Proof of Concept

While no compiled PoC is provided, the vulnerability can be reproduced by:

1. Setting up a network with multiple validators
2. Creating a network partition such that Validator A advances to timestamp 150
3. Keeping Validator B at timestamp 50
4. Creating a block with timestamp 90 that references a batch with expiration 100
5. Observing that Validator B passes the block-level expiration check but fails when requesting from Validator A

The test file `batch_requester_test.rs` already demonstrates the short-circuit behavior but doesn't test the scenario where the batch is valid for the block timestamp despite being expired relative to the responder.

## Notes

This vulnerability represents a legitimate protocol logic bug in the Quorum Store batch retrieval mechanism. The root cause is using the responder's local certified_timestamp as a proxy for global batch expiration, when in fact batch validity should be determined relative to the specific block being executed. During network partitions or sync lag—both common in distributed systems—this causes validators to incorrectly reject valid batches, leading to execution failures and temporary liveness degradation.

### Citations

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L417-418)
```rust
                    match aptos_db_clone.get_latest_ledger_info() {
                        Ok(ledger_info) => BatchResponse::NotFound(ledger_info),
```

**File:** consensus/src/quorum_store/batch_requester.rs (L101-107)
```rust
    pub(crate) async fn request_batch(
        &self,
        digest: HashValue,
        expiration: u64,
        responders: Arc<Mutex<BTreeSet<PeerId>>>,
        mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
    ) -> ExecutorResult<Vec<SignedTransaction>> {
```

**File:** consensus/src/quorum_store/batch_requester.rs (L142-151)
```rust
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L102-106)
```rust
            if block_timestamp <= batch_info.expiration() {
                futures.push(batch_reader.get_batch(batch_info, responders.clone()));
            } else {
                debug!("QSE: skipped expired batch {}", batch_info.digest());
            }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L168-170)
```rust
    fn notify_commit(&self, block_timestamp: u64, payloads: Vec<Payload>) {
        self.batch_reader
            .update_certified_timestamp(block_timestamp);
```

**File:** consensus/src/quorum_store/batch_store.rs (L419-438)
```rust
    pub(crate) fn save(&self, value: &PersistedValue<BatchInfoExt>) -> anyhow::Result<bool> {
        let last_certified_time = self.last_certified_time();
        if value.expiration() > last_certified_time {
            fail_point!("quorum_store::save", |_| {
                // Skip caching and storing value to the db
                Ok(false)
            });
            counters::GAP_BETWEEN_BATCH_EXPIRATION_AND_CURRENT_TIME_WHEN_SAVE.observe(
                Duration::from_micros(value.expiration() - last_certified_time).as_secs_f64(),
            );

            return self.insert_to_cache(value);
        }
        counters::NUM_BATCH_EXPIRED_WHEN_SAVE.inc();
        bail!(
            "Incorrect expiration {} in epoch {}, last committed timestamp {}",
            value.expiration(),
            self.epoch(),
            last_certified_time,
        );
```

**File:** consensus/src/quorum_store/batch_store.rs (L443-472)
```rust
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-538)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L696-702)
```rust
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
```

**File:** consensus/src/quorum_store/tests/batch_requester_test.rs (L234-277)
```rust
#[tokio::test]
async fn test_batch_request_not_exists_expired() {
    let retry_interval_ms = 1_000;
    let expiration = 10_000;

    // Batch has expired according to the ledger info that will be returned
    let (ledger_info_with_signatures, validator_verifier) =
        create_ledger_info_with_timestamp(expiration + 1);

    let batch = Batch::new(
        BatchId::new_for_test(1),
        vec![],
        1,
        expiration,
        AccountAddress::random(),
        0,
    );
    let batch_response = BatchResponse::NotFound(ledger_info_with_signatures);
    let batch_requester = BatchRequester::new(
        1,
        AccountAddress::random(),
        1,
        2,
        retry_interval_ms,
        1_000,
        MockBatchRequester::new(batch_response),
        validator_verifier.into(),
    );

    let request_start = Instant::now();
    let (_, subscriber_rx) = oneshot::channel();
    let result = batch_requester
        .request_batch(
            *batch.digest(),
            batch.expiration(),
            Arc::new(Mutex::new(btreeset![AccountAddress::random()])),
            subscriber_rx,
        )
        .await;
    let request_duration = request_start.elapsed();
    assert_err!(result);
    // No retry because of short-circuiting of expired batch
    assert!(request_duration < Duration::from_millis(retry_interval_ms as u64));
}
```
