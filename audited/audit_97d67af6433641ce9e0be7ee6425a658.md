# Audit Report

## Title
TOCTOU Race Condition in Data Stream Availability Check Causes State Sync Failures

## Summary
A Time-Of-Check-Time-Of-Use (TOCTOU) race condition exists in `process_new_stream_request()` where data availability is verified using a snapshot of `advertised_data` at stream creation time, but actual stream initialization uses a different, potentially stale global data summary. This allows streams to pass the availability check but fail during initialization, causing state synchronization to stall indefinitely.

## Finding Description

The vulnerability occurs in the data streaming service's stream creation flow. When a new stream is requested, the system performs the following operations: [1](#0-0) 

At line 273, `advertised_data` is cloned from the current global data summary. This snapshot is used to create the DataStream (lines 274-284) and verify data availability (line 287). However, a background task continuously refreshes the global data summary every 50ms: [2](#0-1) 

When the stream's progress is checked (which happens every 50ms or when notified), it retrieves a fresh global data summary: [3](#0-2) 

The critical issue is at line 345 where a NEW `global_data_summary` is obtained, and line 369 where this potentially different summary is used to initialize requests. If the advertised data has changed between the availability check and stream initialization, the stream can fail with `DataIsUnavailable` error.

This is particularly problematic for `ContinuousTransactionStreamEngine` where `select_target_ledger_info()` is called during request creation: [4](#0-3) 

If no `highest_synced_ledger_info` exists in the new advertised data (line 530-534), an error is returned, causing stream initialization to fail despite passing the initial availability check.

**Attack Scenario:**
1. Network conditions change (peers go offline, data gets pruned, or malicious peer stops advertising)
2. Client requests a stream at time T0 with advertised data snapshot A showing data availability
3. `ensure_data_is_available()` passes using snapshot A
4. Stream is created and stored
5. 50-100ms later, stream progress check occurs at time T1
6. Global data summary has refreshed to snapshot B (different advertised data)
7. `initialize_data_requests()` uses snapshot B
8. `select_target_ledger_info()` fails because required data is no longer advertised
9. Stream initialization fails with `DataIsUnavailable` error
10. Stream remains stuck in uninitialized state (`sent_data_requests = None`)
11. Every subsequent progress check fails with the same error
12. Stream is never automatically terminated
13. State sync stalls indefinitely

The error handling only logs the failure but doesn't terminate the stuck stream: [5](#0-4) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria for "Significant protocol violations" because:

1. **State Sync Liveness Impact**: If this is the only active stream for a syncing node, state synchronization will stall completely. The node cannot catch up with the network, rendering it unable to participate in consensus or validate transactions.

2. **Validator Degradation**: Validator nodes experiencing this issue will fall behind the network, potentially missing their turn to propose blocks and earning reduced rewards.

3. **No Automatic Recovery**: The stream remains stuck indefinitely with repeated error logging. There is no automatic termination or retry mechanism - manual intervention would be required to terminate and recreate the stream.

4. **Resource Exhaustion**: Stuck streams consume memory and continuously generate error logs and metrics, potentially degrading node performance over time.

5. **Naturally Exploitable**: This can occur without malicious intent through normal network dynamics (peers going offline, data pruning, network partitions), making it a realistic threat to network reliability.

## Likelihood Explanation

The likelihood is **HIGH** because:

1. **Short Time Window**: The TOCTOU window is only 50ms (the default `progress_check_interval_ms` and `global_summary_refresh_interval_ms`), meaning advertised data can change very quickly.

2. **Common Network Events**: Normal network operations cause advertised data changes:
   - Peers going offline
   - Peers pruning old state
   - Network topology changes
   - Peer discovery churn

3. **No Synchronization**: There is no locking or versioning mechanism to ensure the same advertised data snapshot is used for both the check and the actual usage.

4. **Wide Impact Surface**: All stream types are affected - transactions, transaction outputs, states, and epoch ending ledger infos.

5. **Production Likelihood**: In a production network with dynamic peer sets and continuous data pruning, this race condition will occur with measurable frequency.

## Recommendation

Implement one of the following fixes:

**Option 1 (Preferred): Re-validate availability during initialization**

Add a re-check of data availability when initializing data requests, using the same global data summary:

```rust
pub fn initialize_data_requests(
    &mut self,
    global_data_summary: GlobalDataSummary,
) -> Result<(), Error> {
    // Re-validate data availability with the current global data summary
    self.ensure_data_is_available(&global_data_summary.advertised_data)?;
    
    // Initialize the data client requests queue
    self.sent_data_requests = Some(VecDeque::new());

    // Create and send the data client requests to the network
    self.create_and_send_client_requests(&global_data_summary)
}
```

**Option 2: Store and reuse the advertised data snapshot**

Store the `advertised_data` snapshot in the `DataStream` structure and use the same snapshot for both validation and initialization:

```rust
pub struct DataStream<T> {
    // ... existing fields ...
    
    // The advertised data snapshot used for availability validation
    advertised_data_snapshot: AdvertisedData,
}
```

Then use this snapshot consistently in `initialize_data_requests()` instead of the potentially different global data summary.

**Option 3: Graceful degradation**

If data becomes unavailable, automatically terminate the stream and allow the client to request a new stream with updated parameters:

```rust
// In update_progress_of_data_stream, after catching initialization error:
if matches!(error, Error::DataIsUnavailable(_)) {
    warn!("Data became unavailable for stream {}, terminating stream", data_stream_id);
    self.data_streams.remove(data_stream_id);
    return Ok(()); // Stream terminated gracefully
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_toctou_stream_availability_race() {
    use aptos_config::config::DataStreamingServiceConfig;
    use aptos_data_client::global_summary::{AdvertisedData, GlobalDataSummary};
    use std::sync::Arc;
    use arc_swap::ArcSwap;
    
    // Create a streaming service with real configuration
    let mut streaming_service = create_test_streaming_service();
    
    // Step 1: Set up initial advertised data with transactions at version 100-200
    let initial_advertised_data = create_advertised_data_with_transactions(100, 200);
    streaming_service.global_data_summary.store(Arc::new(
        GlobalDataSummary::new(initial_advertised_data)
    ));
    
    // Step 2: Client requests a stream for transactions starting at version 100
    let (stream_request, response_receiver) = create_transaction_stream_request(100);
    streaming_service.handle_stream_request_message(
        stream_request,
        streaming_service.stream_update_notifier.clone()
    );
    
    // Step 3: Verify stream was created successfully
    let stream_listener = response_receiver.await.unwrap().unwrap();
    let stream_id = stream_listener.data_stream_id;
    assert!(streaming_service.data_streams.contains_key(&stream_id));
    
    // Step 4: SIMULATE TOCTOU - Change advertised data before stream initialization
    // (This simulates network peers going offline or data becoming unavailable)
    let changed_advertised_data = create_advertised_data_with_transactions(500, 600);
    streaming_service.global_data_summary.store(Arc::new(
        GlobalDataSummary::new(changed_advertised_data)
    ));
    
    // Step 5: Trigger stream progress check (normally happens every 50ms)
    streaming_service.check_progress_of_all_data_streams().await;
    
    // Step 6: Verify the stream failed to initialize
    let stream = streaming_service.data_streams.get(&stream_id).unwrap();
    assert!(stream.sent_data_requests.is_none(), 
        "Stream should remain uninitialized due to TOCTOU race");
    
    // Step 7: Verify repeated progress checks continue to fail
    for _ in 0..10 {
        streaming_service.check_progress_of_all_data_streams().await;
        let stream = streaming_service.data_streams.get(&stream_id).unwrap();
        assert!(stream.sent_data_requests.is_none(), 
            "Stream should remain stuck in uninitialized state");
    }
    
    // Step 8: Verify stream is never automatically terminated
    assert!(streaming_service.data_streams.contains_key(&stream_id),
        "Stuck stream should not be auto-terminated, causing indefinite stall");
}
```

## Notes

This TOCTOU vulnerability breaks the **State Consistency** invariant (#4) because it allows the state synchronization mechanism to enter an inconsistent state where a stream has passed validation but cannot make progress. It also impacts the overall liveness of the state sync subsystem, which is critical for maintaining network health and validator participation.

The vulnerability is exacerbated by the lack of automatic recovery mechanisms - once a stream enters this stuck state, it requires manual intervention to resolve, making it particularly problematic for production deployments.

### Citations

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L265-287)
```rust
        // Refresh the cached global data summary
        refresh_global_data_summary(
            self.aptos_data_client.clone(),
            self.global_data_summary.clone(),
        );

        // Create a new data stream
        let stream_id = self.stream_id_generator.next();
        let advertised_data = self.get_global_data_summary().advertised_data.clone();
        let (data_stream, stream_listener) = DataStream::new(
            self.data_client_config,
            self.streaming_service_config,
            stream_id,
            &request_message.stream_request,
            stream_update_notifier,
            self.aptos_data_client.clone(),
            self.notification_id_generator.clone(),
            &advertised_data,
            self.time_service.clone(),
        )?;

        // Verify the data stream can be fulfilled using the currently advertised data
        data_stream.ensure_data_is_available(&advertised_data)?;
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L308-337)
```rust
    /// Ensures that all existing data streams are making progress
    async fn check_progress_of_all_data_streams(&mut self) {
        // Drive the progress of each stream
        let data_stream_ids = self.get_all_data_stream_ids();
        for data_stream_id in &data_stream_ids {
            if let Err(error) = self.update_progress_of_data_stream(data_stream_id).await {
                if matches!(error, Error::NoDataToFetch(_)) {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(NO_DATA_TO_FETCH_LOG_FREQ_SECS)),
                        info!(LogSchema::new(LogEntry::CheckStreamProgress)
                            .stream_id(*data_stream_id)
                            .event(LogEvent::Pending)
                            .error(&error))
                    );
                } else {
                    metrics::increment_counter(
                        &metrics::CHECK_STREAM_PROGRESS_ERROR,
                        error.get_label(),
                    );
                    warn!(LogSchema::new(LogEntry::CheckStreamProgress)
                        .stream_id(*data_stream_id)
                        .event(LogEvent::Error)
                        .error(&error));
                }
            }
        }

        // Update the metrics
        metrics::set_active_data_streams(data_stream_ids.len());
    }
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L339-384)
```rust
    /// Ensures that a data stream has in-flight data requests and handles
    /// any new responses that have arrived since we last checked.
    async fn update_progress_of_data_stream(
        &mut self,
        data_stream_id: &DataStreamId,
    ) -> Result<(), Error> {
        let global_data_summary = self.get_global_data_summary();

        // If there was a send failure, terminate the stream
        let data_stream = self.get_data_stream(data_stream_id)?;
        if data_stream.send_failure() {
            info!(
                (LogSchema::new(LogEntry::TerminateStream)
                    .stream_id(*data_stream_id)
                    .event(LogEvent::Success)
                    .message("There was a send failure, terminating the stream."))
            );
            metrics::DATA_STREAM_SEND_FAILURE.inc();
            if self.data_streams.remove(data_stream_id).is_none() {
                return Err(Error::UnexpectedErrorEncountered(format!(
                    "Failed to terminate stream id {:?} for send failure! Stream not found.",
                    data_stream_id
                )));
            }
            return Ok(());
        }

        // Drive data stream progress
        if !data_stream.data_requests_initialized() {
            // Initialize the request batch by sending out data client requests
            data_stream.initialize_data_requests(global_data_summary)?;
            info!(
                (LogSchema::new(LogEntry::InitializeStream)
                    .stream_id(*data_stream_id)
                    .event(LogEvent::Success)
                    .message("Data stream initialized."))
            );
        } else {
            // Process any data client requests that have received responses
            data_stream
                .process_data_responses(global_data_summary)
                .await?;
        }

        Ok(())
    }
```

**File:** config/src/config/state_sync_config.rs (L265-282)
```rust
impl Default for DataStreamingServiceConfig {
    fn default() -> Self {
        Self {
            dynamic_prefetching: DynamicPrefetchingConfig::default(),
            enable_subscription_streaming: false,
            global_summary_refresh_interval_ms: 50,
            max_concurrent_requests: MAX_CONCURRENT_REQUESTS,
            max_concurrent_state_requests: MAX_CONCURRENT_STATE_REQUESTS,
            max_data_stream_channel_sizes: 50,
            max_notification_id_mappings: 300,
            max_num_consecutive_subscriptions: 45, // At ~3 blocks per second, this should last ~15 seconds
            max_pending_requests: 50,
            max_request_retry: 5,
            max_subscription_stream_lag_secs: 10, // 10 seconds
            progress_check_interval_ms: 50,
        }
    }
}
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L498-535)
```rust
    fn select_target_ledger_info(
        &self,
        advertised_data: &AdvertisedData,
    ) -> Result<Option<LedgerInfoWithSignatures>, Error> {
        // Check if the stream has a final target ledger info
        match &self.request {
            StreamRequest::ContinuouslyStreamTransactions(request) => {
                if let Some(target) = &request.target {
                    return Ok(Some(target.clone()));
                }
            },
            StreamRequest::ContinuouslyStreamTransactionOutputs(request) => {
                if let Some(target) = &request.target {
                    return Ok(Some(target.clone()));
                }
            },
            StreamRequest::ContinuouslyStreamTransactionsOrOutputs(request) => {
                if let Some(target) = &request.target {
                    return Ok(Some(target.clone()));
                }
            },
            request => invalid_stream_request!(request),
        };

        // We don't have a final target, select the highest to make progress
        if let Some(highest_synced_ledger_info) = advertised_data.highest_synced_ledger_info() {
            let (next_request_version, _) = self.next_request_version_and_epoch;
            if next_request_version > highest_synced_ledger_info.ledger_info().version() {
                Ok(None) // We're already at the highest synced ledger info. There's no known target.
            } else {
                Ok(Some(highest_synced_ledger_info))
            }
        } else {
            Err(Error::DataIsUnavailable(
                "Unable to find the highest synced ledger info!".into(),
            ))
        }
    }
```
