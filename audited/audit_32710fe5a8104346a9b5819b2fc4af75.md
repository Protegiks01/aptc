# Audit Report

## Title
Indefinite Initialization Loop in Indexer-gRPC Data Service Blocks Service Startup

## Summary
The `ConnectionManager::new()` initialization loop waits indefinitely for `known_latest_version` to become non-zero, with no timeout mechanism. If all heartbeat calls persistently fail or if the GrpcManager's `known_latest_version` remains at its initial value of zero, the data service startup is permanently blocked, causing deployment failures. [1](#0-0) 

## Finding Description
The vulnerability exists in the initialization logic that waits for blockchain version synchronization. The initialization loop continuously attempts heartbeat calls until it receives a non-zero `known_latest_version` value. However, this creates two critical failure scenarios:

**Scenario 1: GrpcManager Unavailability**
When the GrpcManager service is unreachable due to network issues, misconfiguration, or the service being down, all heartbeat attempts fail. The loop continues indefinitely with no timeout or retry limit. [2](#0-1) 

**Scenario 2: GrpcManager Zero Version State**
The GrpcManager's `known_latest_version` is initialized to zero and only updates when it receives version information from fullnodes. If the data service starts before any fullnodes connect to the GrpcManager, or if fullnodes haven't processed transactions yet, the GrpcManager responds with `known_latest_version = 0`. The data service receives successful heartbeat responses but the version remains zero, causing the loop to continue indefinitely. [3](#0-2) [4](#0-3) [5](#0-4) 

The initialization is called during service startup without any timeout wrapper: [6](#0-5) [7](#0-6) 

## Impact Explanation
This qualifies as **High Severity** under the Aptos bug bounty criteria as an "API crash" - the indexer-grpc data service fails to start, resulting in complete service unavailability. The impact includes:

- **Complete Service Unavailability**: Both live and historical data services cannot start
- **Deployment Failures**: Automated deployments hang indefinitely  
- **No Self-Recovery**: Once hung, manual intervention is required to restart
- **Cascading Failures**: Applications depending on the indexer cannot access blockchain data
- **Operational Impact**: Monitoring systems may not detect the hung initialization state vs a slow startup

## Likelihood Explanation
This issue has **HIGH likelihood** of occurring in production environments:

1. **Deployment Ordering Dependencies**: If the data service container starts before GrpcManager or fullnodes are ready, the initialization hangs
2. **Network Transient Issues**: Temporary network partitions during startup can trigger the infinite loop
3. **Configuration Errors**: Invalid GrpcManager addresses lead to permanent heartbeat failures
4. **Cold Start Scenarios**: Fresh deployments where no fullnode has connected to GrpcManager yet

The bug requires no attacker action - it's triggered by normal operational conditions or misconfigurations.

## Recommendation
Implement a timeout mechanism with graceful failure handling:

```rust
pub(crate) async fn new(
    chain_id: u64,
    grpc_manager_addresses: Vec<String>,
    self_advertised_address: String,
    is_live_data_service: bool,
) -> Result<Self, anyhow::Error> {
    let grpc_manager_connections = DashMap::new();
    grpc_manager_addresses.into_iter().for_each(|address| {
        grpc_manager_connections
            .insert(address.clone(), Self::create_client_from_address(&address));
    });
    let res = Self {
        chain_id,
        grpc_manager_connections,
        self_advertised_address,
        known_latest_version: AtomicU64::new(0),
        active_streams: DashMap::new(),
        is_live_data_service,
    };

    // Keep fetching latest version until it is available, with timeout
    const MAX_INITIALIZATION_ATTEMPTS: usize = 60;
    const INITIALIZATION_TIMEOUT: Duration = Duration::from_secs(60);
    
    let start = tokio::time::Instant::now();
    let mut attempts = 0;
    
    while res.known_latest_version.load(Ordering::SeqCst) == 0 {
        if start.elapsed() > INITIALIZATION_TIMEOUT {
            return Err(anyhow::anyhow!(
                "Failed to initialize: timeout after {} seconds waiting for known_latest_version",
                INITIALIZATION_TIMEOUT.as_secs()
            ));
        }
        
        if attempts >= MAX_INITIALIZATION_ATTEMPTS {
            return Err(anyhow::anyhow!(
                "Failed to initialize: exceeded maximum attempts ({}) waiting for known_latest_version",
                MAX_INITIALIZATION_ATTEMPTS
            ));
        }
        
        for entry in res.grpc_manager_connections.iter() {
            let address = entry.key();
            if let Err(e) = res.heartbeat(address).await {
                warn!("Error during initialization heartbeat to {}: {}", address, e);
            }
        }
        
        attempts += 1;
        tokio::time::sleep(Duration::from_secs(1)).await;
    }

    Ok(res)
}
```

Additionally, update the calling code to handle the error:

```rust
let connection_manager = Arc::new(
    ConnectionManager::new(
        self.chain_id,
        self.grpc_manager_addresses.clone(),
        self.self_advertised_address.clone(),
        /*is_live_data_service=*/ true,
    )
    .await
    .context("Failed to initialize ConnectionManager")?
);
```

## Proof of Concept

**Setup**:
1. Deploy indexer-grpc-data-service-v2 with valid configuration
2. Ensure GrpcManager is either unreachable OR has not received any fullnode connections yet

**Reproduction Steps**:
```bash
# Start only the data service without GrpcManager or fullnodes
docker run -it aptos-indexer-grpc-data-service-v2 \
  --grpc-manager-addresses "http://nonexistent-grpc-manager:50051" \
  --self-advertised-address "http://data-service:50052" \
  --chain-id 1

# Observe: Service hangs indefinitely during initialization
# Log shows repeated: "Error during heartbeat: ..."
# Service never completes startup
# Health checks time out
# No error is returned - process just hangs
```

**Alternative PoC - Timing Issue**:
```bash
# Start data service before GrpcManager is ready
docker-compose up data-service &
sleep 2
docker-compose up grpc-manager
docker-compose up fullnode

# Result: data-service hangs waiting for version, even though
# GrpcManager and fullnode start successfully afterwards
```

**Verification**:
Monitor the logs - you'll see continuous heartbeat errors or warnings about missing `known_latest_version`, but the service never fails or succeeds in starting. The initialization loop runs forever.

## Notes

This vulnerability specifically affects the indexer-grpc infrastructure, which is an ecosystem service for serving blockchain data to applications, not a core consensus component. However, it represents a significant operational reliability issue that prevents the indexer service from starting under common deployment scenarios. The lack of timeout or circuit breaker patterns makes this a deterministic failure mode rather than a transient issue.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L132-140)
```rust
        while res.known_latest_version.load(Ordering::SeqCst) == 0 {
            for entry in res.grpc_manager_connections.iter() {
                let address = entry.key();
                if let Err(e) = res.heartbeat(address).await {
                    warn!("Error during heartbeat: {e}.");
                }
            }
            tokio::time::sleep(Duration::from_secs(1)).await;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L249-301)
```rust
    async fn heartbeat(&self, address: &str) -> Result<(), tonic::Status> {
        info!("Sending heartbeat to GrpcManager {address}.");
        let timestamp = Some(timestamp_now_proto());
        let known_latest_version = Some(self.known_latest_version());
        let stream_info = Some(StreamInfo {
            active_streams: self.get_active_streams(),
        });

        let info = if self.is_live_data_service {
            let min_servable_version = match LIVE_DATA_SERVICE.get() {
                Some(svc) => Some(svc.get_min_servable_version().await),
                None => None,
            };
            Some(Info::LiveDataServiceInfo(LiveDataServiceInfo {
                chain_id: self.chain_id,
                timestamp,
                known_latest_version,
                stream_info,
                min_servable_version,
            }))
        } else {
            Some(Info::HistoricalDataServiceInfo(HistoricalDataServiceInfo {
                chain_id: self.chain_id,
                timestamp,
                known_latest_version,
                stream_info,
            }))
        };
        let service_info = ServiceInfo {
            address: Some(self.self_advertised_address.clone()),
            info,
        };
        let request = HeartbeatRequest {
            service_info: Some(service_info),
        };
        let response = self
            .grpc_manager_connections
            .get(address)
            // TODO(grao): Consider to not use unwrap here.
            .unwrap()
            .clone()
            .heartbeat(request)
            .await?
            .into_inner();
        if let Some(known_latest_version) = response.known_latest_version {
            info!("Received known_latest_version ({known_latest_version}) from GrpcManager {address}.");
            self.update_known_latest_version(known_latest_version);
        } else {
            warn!("HeartbeatResponse doesn't contain known_latest_version, GrpcManager address: {address}");
        }

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L162-162)
```rust
            known_latest_version: AtomicU64::new(0),
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L401-403)
```rust
    pub(crate) fn get_known_latest_version(&self) -> u64 {
        self.known_latest_version.load(Ordering::SeqCst)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/service.rs (L42-44)
```rust
        Ok(Response::new(HeartbeatResponse {
            known_latest_version: Some(self.metadata_manager.get_known_latest_version()),
        }))
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L115-121)
```rust
            ConnectionManager::new(
                self.chain_id,
                self.grpc_manager_addresses.clone(),
                self.self_advertised_address.clone(),
                /*is_live_data_service=*/ true,
            )
            .await,
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L165-171)
```rust
            ConnectionManager::new(
                self.chain_id,
                self.grpc_manager_addresses.clone(),
                self.self_advertised_address.clone(),
                /*is_live_data_service=*/ false,
            )
            .await,
```
