# Audit Report

## Title
State Inconsistency in VM Validator Due to Race Between Pre-Commit and Commit Phases

## Summary
The VMValidator in mempool can read uncommitted state during the window between `pre_commit_ledger` updating the checkpoint version and `commit_ledger` finalizing the commit. If a node crashes during this window, the uncommitted state is rolled back, but mempool has already made validation decisions based on it, leading to cross-node mempool inconsistencies.

## Finding Description

The vulnerability exists in the two-phase commit architecture where state checkpoint version visibility is decoupled from commit finalization.

**The Race Condition:** [1](#0-0) 

VMValidator creates a state view by calling `latest_state_checkpoint_view()`, which reads: [2](#0-1) 

This reads from: [3](#0-2) 

The checkpoint version is updated during `pre_commit_ledger`: [4](#0-3) 

Specifically, at line 68-72, `buffered_state.update()` is called, which updates the shared `current_state`: [5](#0-4) 

However, the commit is only finalized later in `commit_ledger`: [6](#0-5) 

**The Window of Vulnerability:**

Between when `pre_commit_ledger` completes (updating `current_state`) and when `commit_ledger` writes `OverallCommitProgress`, concurrent mempool operations can read the pre-committed checkpoint version. This is used for transaction validation: [7](#0-6) 

**Crash Recovery Exposes the Issue:**

If the node crashes before `commit_ledger` completes, recovery truncates to the last committed version: [8](#0-7) 

This removes the pre-committed state that mempool already used for validation decisions.

**Attack Scenario:**

1. Node A executes consensus block, calls `pre_commit_ledger(version=100)`
2. State values written to RocksDB, `current_state.last_checkpoint` updated to version 100
3. Mempool thread validates incoming transaction using checkpoint version 100
4. Transaction accepted/rejected based on version 100 state
5. **Node crashes** before `commit_ledger` writes `OverallCommitProgress`
6. On restart, `sync_commit_progress` reads `OverallCommitProgress=99`, truncates version 100
7. Mempool validation decisions based on version 100 are now inconsistent with actual state
8. Other nodes that didn't crash may have different mempool contents
9. Network-wide mempool divergence occurs

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria: "State inconsistencies requiring intervention."

**Concrete Impacts:**
- **Mempool Divergence**: Different nodes have different transaction acceptance/rejection decisions
- **Sequence Number Mismatches**: Account sequence numbers used for validation may be inconsistent
- **Transaction Loss**: Valid transactions may be incorrectly rejected and dropped from network
- **DoS Potential**: Critical transactions (governance, emergency actions) could be incorrectly filtered
- **Network Inconsistency**: P2P mempool broadcast ACKs become unreliable

While this doesn't directly break consensus safety (block execution remains deterministic), it violates the protocol invariant that all honest nodes should maintain consistent mempool state when following the protocol correctly.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability triggers whenever:
1. A node processes a consensus block (frequent)
2. Mempool validates transactions between `pre_commit` and `commit` (timing window is small but real)
3. Node crashes before `commit_ledger` completes (can be triggered by various conditions: OOM, power loss, kill signal, panic in other threads)

The window is narrow (milliseconds to seconds depending on system load), but given the frequency of block commits and the possibility of crashes in production environments, this scenario will occur naturally over time across a network of validators.

## Recommendation

**Solution: Delay checkpoint version visibility until after commit completes**

Modify the state management to not expose the new checkpoint version to readers until `OverallCommitProgress` is written. This ensures atomicity of the commit operation from external observers' perspective.

**Implementation approach:**

1. Keep `current_state` update in `pre_commit_ledger` for internal executor state tracking
2. Add a separate "committed_checkpoint_version" field that is only updated in `commit_ledger`
3. Modify `get_latest_state_checkpoint_version()` to read from the committed version instead of current_state

**Alternative approach:**

Add a "visibility barrier" that prevents VMValidator from seeing checkpoint versions that haven't been committed:

```rust
// In get_latest_state_checkpoint_version
pub fn get_latest_state_checkpoint_version(&self) -> Result<Option<Version>> {
    let current_checkpoint = self.state_store.current_state_locked().last_checkpoint().version();
    let committed_version = self.ledger_db.metadata_db().get_synced_version()?;
    
    // Only return checkpoint if it's been committed
    Ok(current_checkpoint.filter(|&v| {
        committed_version.map_or(false, |committed| v <= committed)
    }))
}
```

This ensures reads never see uncommitted checkpoints while preserving the two-phase commit optimization for the executor.

## Proof of Concept

The following scenario demonstrates the vulnerability:

```rust
// Pseudo-code reproduction steps:

// Thread 1: Consensus/Executor
async fn execute_block() {
    // Execute block, version advances to 100
    let result = executor.execute_block(block_100).await;
    
    // Pre-commit writes state and updates current_state to version 100
    db.pre_commit_ledger(result).await;
    // <- CRASH POINT: current_state.last_checkpoint = 100, but OverallCommitProgress = 99
    
    // This never executes due to crash:
    // db.commit_ledger(100).await;
}

// Thread 2: Mempool (concurrent)
async fn validate_transaction() {
    // Reads checkpoint version - gets 100 (pre-committed but not committed)
    let state_view = db.latest_state_checkpoint_view();
    
    // Makes validation decision based on version 100
    let account_seq = get_account_sequence_number(&state_view, sender);
    
    // Transaction accepted/rejected based on uncommitted state
    if txn.sequence_number() >= account_seq {
        mempool.add(txn);  // Accepted based on version 100
    }
}

// Thread 3: Crash Recovery (after restart)
fn recover() {
    // Reads OverallCommitProgress = 99
    let committed_version = metadata_db.get_synced_version(); // Returns 99
    
    // Truncates databases back to version 99
    sync_commit_progress();  // Removes all version 100 state
    
    // Now mempool has transaction accepted based on version 100,
    // but database state is at version 99
    // -> Inconsistency!
}
```

To reproduce in a test environment:
1. Set up node with mempool and executor
2. Inject a delay between `pre_commit_ledger` and `commit_ledger` 
3. Trigger mempool validation during the delay window
4. Crash the node (SIGKILL) before `commit_ledger` completes
5. Restart and observe that mempool state is inconsistent with database state
6. Compare with another node that didn't crash - mempool contents will differ

## Notes

The vulnerability stems from an architectural choice to optimize performance by separating pre-commit (write data) from commit (mark as durable). While this two-phase commit improves throughput, it inadvertently exposes uncommitted state to concurrent readers in the mempool validation path.

The issue is exacerbated by the fact that `pre_commit_lock` and `commit_lock` are separate, and read operations don't acquire either lock, allowing truly concurrent access to partially-committed state.

This is a subtle consistency violation that won't cause immediate consensus failures but can lead to gradual network-wide divergence in mempool state, particularly in scenarios with node restarts or crashes.

### Citations

**File:** vm-validator/src/vm_validator.rs (L54-62)
```rust
    fn new(db_reader: Arc<dyn DbReader>) -> Self {
        let db_state_view = db_reader
            .latest_state_checkpoint_view()
            .expect("Get db view cannot fail");
        VMValidator {
            db_reader,
            state: CachedModuleView::new(db_state_view.into()),
        }
    }
```

**File:** storage/storage-interface/src/state_store/state_view/db_state_view.rs (L82-90)
```rust
    fn latest_state_checkpoint_view(&self) -> StateViewResult<DbStateView> {
        Ok(DbStateView {
            db: self.clone(),
            version: self
                .get_latest_state_checkpoint_version()
                .map_err(Into::<StateViewError>::into)?,
            maybe_verify_against_state_root_hash: None,
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L812-819)
```rust
    fn get_latest_state_checkpoint_version(&self) -> Result<Option<Version>> {
        gauged_api("get_latest_state_checkpoint_version", || {
            Ok(self
                .state_store
                .current_state_locked()
                .last_checkpoint()
                .version())
        })
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L44-76)
```rust
    fn pre_commit_ledger(&self, chunk: ChunkToCommit, sync_commit: bool) -> Result<()> {
        gauged_api("pre_commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .pre_commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["pre_commit_ledger"]);

            chunk
                .state_summary
                .latest()
                .global_state_summary
                .log_generation("db_save");

            self.pre_commit_validation(&chunk)?;
            let _new_root_hash =
                self.calculate_and_commit_ledger_and_state_kv(&chunk, self.skip_index_and_usage)?;

            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions__others"]);

            self.state_store.buffered_state().lock().update(
                chunk.result_ledger_state_with_summary(),
                chunk.estimated_total_state_updates(),
                sync_commit || chunk.is_reconfig,
            )?;

            Ok(())
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L78-112)
```rust
    fn commit_ledger(
        &self,
        version: Version,
        ledger_info_with_sigs: Option<&LedgerInfoWithSignatures>,
        chunk_opt: Option<ChunkToCommit>,
    ) -> Result<()> {
        gauged_api("commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_ledger"]);

            let old_committed_ver = self.get_and_check_commit_range(version)?;

            let mut ledger_batch = SchemaBatch::new();
            // Write down LedgerInfo if provided.
            if let Some(li) = ledger_info_with_sigs {
                self.check_and_put_ledger_info(version, li, &mut ledger_batch)?;
            }
            // Write down commit progress
            ledger_batch.put::<DbMetadataSchema>(
                &DbMetadataKey::OverallCommitProgress,
                &DbMetadataValue::Version(version),
            )?;
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;

            // Notify the pruners, invoke the indexer, and update in-memory ledger info.
            self.post_commit(old_committed_ver, version, ledger_info_with_sigs, chunk_opt)
        })
    }
```

**File:** storage/aptosdb/src/state_store/buffered_state.rs (L156-179)
```rust
    pub fn update(
        &mut self,
        new_state: LedgerStateWithSummary,
        estimated_new_items: usize,
        sync_commit: bool,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["buffered_state___update"]);

        let old_state = self.current_state_locked().clone();
        assert!(new_state.is_descendant_of(&old_state));

        self.estimated_items += estimated_new_items;
        let version = new_state.last_checkpoint().version();

        let last_checkpoint = new_state.last_checkpoint().clone();
        // Commit state only if there is a new checkpoint, eases testing and make estimated
        // buffer size a tad more realistic.
        let checkpoint_to_commit_opt =
            (old_state.next_version() < last_checkpoint.next_version()).then_some(last_checkpoint);
        *self.current_state_locked() = new_state;
        self.maybe_commit(checkpoint_to_commit_opt, sync_commit);
        Self::report_last_checkpoint_version(version);
        Ok(())
    }
```

**File:** mempool/src/shared_mempool/tasks.rs (L328-333)
```rust
    let start_storage_read = Instant::now();
    let state_view = smp
        .db
        .latest_state_checkpoint_view()
        .expect("Failed to get latest state checkpoint view.");

```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```
