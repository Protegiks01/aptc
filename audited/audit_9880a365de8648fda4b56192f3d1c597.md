# Audit Report

## Title
Shutdown Race Condition in Outbound Handler Causes Permanent Node Hang During Sharded Block Execution

## Summary
The `process_one_outgoing_message()` function in the network controller's outbound handler contains a critical shutdown race condition that causes message loss of execution results during sharded block execution. When shutdown occurs while execution results are queued, the coordinator blocks indefinitely waiting for lost messages, causing permanent validator node hang and consensus liveness failure.

## Finding Description

**Important Note on Scope**: The security question asks about "consensus votes" but the `NetworkController` in `secure/net/` is **not** used for consensus vote messages. Consensus uses the separate `aptos-network` framework. However, this `NetworkController` **is** used for remote sharded block execution, which is consensus-critical infrastructure. Loss of execution results prevents validators from completing block execution, computing state roots, and participating in consensus. [1](#0-0) 

The vulnerability exists in the shutdown handling logic. When a shutdown signal arrives, the function immediately returns without draining pending messages from unbounded channels: [2](#0-1) 

Messages are sent to unbounded channels created here: [3](#0-2) 

The shutdown implementation is acknowledged as incomplete: [4](#0-3) 

**How the Vulnerability Manifests:**

1. **Sharded Execution Setup**: Remote executor shards use `NetworkController` for communication: [5](#0-4) 

2. **Execution Result Sending**: When a shard completes execution, it sends results through the outbound channel: [6](#0-5) 

3. **Coordinator Blocking Wait**: The coordinator blocks waiting for results from ALL shards: [7](#0-6) 

4. **The Race**: If `NetworkController::shutdown()` is called while execution results are queued:
   - The outbound handler receives the stop_task signal
   - Returns immediately at line 144
   - Execution result messages still in channel buffers are **never sent**
   - The coordinator's `rx.recv().unwrap()` blocks **forever** (no timeout implemented)
   - The validator node becomes permanently hung

## Impact Explanation

**Critical Severity** - This meets multiple Critical impact categories:

1. **Total Loss of Liveness**: The validator node becomes permanently hung, unable to complete block execution. The `recv().unwrap()` call has no timeout mechanism, causing indefinite blocking.

2. **Consensus Participation Failure**: Without completing execution, the validator cannot:
   - Generate transaction outputs
   - Compute state roots  
   - Vote on blocks
   - Commit to the ledger

3. **Network Availability Impact**: If multiple validators are affected (e.g., during coordinated shutdown scenarios or operational issues), the network suffers liveness degradation.

4. **Invariant Violations**:
   - **Deterministic Execution** invariant broken: Some validators complete execution, others hang
   - **Consensus Liveness** invariant broken: Hung validators cannot participate in consensus

The sharded executor integration into consensus is shown here: [8](#0-7) 

## Likelihood Explanation

**High Likelihood** during operational scenarios:

1. **Normal Shutdown Operations**: Node restarts, upgrades, or maintenance during active block execution
2. **Error-Triggered Shutdowns**: Other errors causing shutdown while execution is in-flight  
3. **Race Window**: The window is significant - any message queued in unbounded channels when shutdown occurs is lost
4. **No Recovery Mechanism**: Once hung, the node requires manual restart

The vulnerability is not easily exploitable by external attackers (shutdown requires node operator privileges), but it's a critical **operational vulnerability** that causes deterministic node failure during legitimate operations.

## Recommendation

Implement graceful shutdown with message draining:

```rust
async fn process_one_outgoing_message(
    outbound_handlers: Vec<(Receiver<Message>, SocketAddr, MessageType)>,
    socket_addr: &SocketAddr,
    inbound_handler: Arc<Mutex<InboundHandler>>,
    grpc_clients: &mut HashMap<SocketAddr, GRPCNetworkMessageServiceClientWrapper>,
) {
    let mut shutdown_requested = false;
    
    loop {
        let mut select = Select::new();
        for (receiver, _, _) in outbound_handlers.iter() {
            select.recv(receiver);
        }

        let index;
        let msg;
        let _timer;
        {
            let oper = select.select();
            _timer = NETWORK_HANDLER_TIMER
                .with_label_values(&[&socket_addr.to_string(), "outbound_msgs"])
                .start_timer();
            index = oper.index();
            match oper.recv(&outbound_handlers[index].0) {
                Ok(m) => {
                    msg = m;
                },
                Err(e) => {
                    warn!(
                        "{:?} for outbound handler on {:?}. This can happen in shutdown,\
                         but should not happen otherwise",
                        e.to_string(),
                        socket_addr
                    );
                    return;
                },
            }
        }

        let remote_addr = &outbound_handlers[index].1;
        let message_type = &outbound_handlers[index].2;

        if message_type.get_type() == "stop_task" {
            shutdown_requested = true;
            // Don't return immediately - set flag to drain remaining messages
        }

        // Only skip message processing if already draining non-stop messages
        if shutdown_requested && message_type.get_type() == "stop_task" {
            continue; // Skip additional stop signals
        }

        if remote_addr == socket_addr {
            inbound_handler
                .lock()
                .unwrap()
                .send_incoming_message_to_handler(message_type, msg);
        } else {
            grpc_clients
                .get_mut(remote_addr)
                .unwrap()
                .send_message(*socket_addr, msg, message_type)
                .await;
        }

        // If shutdown requested, try to drain remaining messages with timeout
        if shutdown_requested {
            let mut all_empty = true;
            for (receiver, _, message_type) in outbound_handlers.iter() {
                if message_type.get_type() != "stop_task" {
                    if let Ok(msg) = receiver.try_recv() {
                        all_empty = false;
                        // Process this message before exiting
                        let remote_addr = &outbound_handlers
                            .iter()
                            .find(|(r, _, mt)| std::ptr::eq(r, receiver))
                            .map(|(_, addr, _)| addr)
                            .unwrap();
                        
                        grpc_clients
                            .get_mut(remote_addr)
                            .unwrap()
                            .send_message(*socket_addr, msg, message_type)
                            .await;
                    }
                }
            }
            if all_empty {
                info!("All messages drained, shutting down outbound handler");
                return;
            }
        }
    }
}
```

Additionally, add timeout mechanism on the receiving side:

```rust
fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
    trace!("RemoteExecutorClient Waiting for results");
    let mut results = vec![];
    for rx in self.result_rxs.iter() {
        let timeout = Duration::from_secs(300); // 5 minute timeout
        let received_bytes = rx
            .recv_timeout(timeout)
            .map_err(|_| VMStatus::Error(StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR, None))?
            .to_bytes();
        let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
        results.push(result.inner?);
    }
    Ok(results)
}
```

## Proof of Concept

```rust
// Test demonstrating the hang scenario
#[test]
fn test_shutdown_race_causes_hang() {
    use std::time::Duration;
    use std::thread;
    
    // Setup: Create coordinator and remote shard with NetworkController
    let coordinator_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 52200);
    let shard_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 52201);
    
    let mut coordinator_controller = NetworkController::new(
        "coordinator".to_string(),
        coordinator_addr,
        5000,
    );
    
    let mut shard_controller = NetworkController::new(
        "shard".to_string(),
        shard_addr,
        5000,
    );
    
    // Create channels for execution results
    let result_tx = shard_controller.create_outbound_channel(
        coordinator_addr,
        "execute_result_0".to_string(),
    );
    let result_rx = coordinator_controller.create_inbound_channel(
        "execute_result_0".to_string(),
    );
    
    coordinator_controller.start();
    shard_controller.start();
    
    // Simulate execution: shard sends result
    let execution_result = vec![1, 2, 3, 4]; // Mock result
    result_tx.send(Message::new(execution_result.clone())).unwrap();
    
    // Race: Shutdown shard BEFORE coordinator receives the message
    thread::sleep(Duration::from_millis(10)); // Small delay to ensure message is queued
    shard_controller.shutdown(); // This triggers the race!
    
    // Coordinator tries to receive - THIS WILL HANG FOREVER
    let start = std::time::Instant::now();
    let received = result_rx.recv_timeout(Duration::from_secs(5));
    
    // Expected: timeout because message was lost
    assert!(received.is_err(), "Should timeout because message was lost during shutdown");
    assert!(start.elapsed() >= Duration::from_secs(5), "Should have waited for timeout");
    
    coordinator_controller.shutdown();
}
```

**Notes:**
- While the security question mentions "consensus votes," the actual vulnerability affects **execution result messages** in the sharded block executor
- This indirectly impacts consensus by preventing validators from completing block execution
- The vulnerability requires node operator actions (shutdown) rather than external attacker exploitation, making it an **operational security issue**
- The impact is Critical because it causes permanent node hang requiring manual intervention
- The existing TODO comment acknowledges the shutdown implementation is incomplete

### Citations

**File:** secure/net/src/network_controller/outbound_handler.rs (L103-163)
```rust
    async fn process_one_outgoing_message(
        outbound_handlers: Vec<(Receiver<Message>, SocketAddr, MessageType)>,
        socket_addr: &SocketAddr,
        inbound_handler: Arc<Mutex<InboundHandler>>,
        grpc_clients: &mut HashMap<SocketAddr, GRPCNetworkMessageServiceClientWrapper>,
    ) {
        loop {
            let mut select = Select::new();
            for (receiver, _, _) in outbound_handlers.iter() {
                select.recv(receiver);
            }

            let index;
            let msg;
            let _timer;
            {
                let oper = select.select();
                _timer = NETWORK_HANDLER_TIMER
                    .with_label_values(&[&socket_addr.to_string(), "outbound_msgs"])
                    .start_timer();
                index = oper.index();
                match oper.recv(&outbound_handlers[index].0) {
                    Ok(m) => {
                        msg = m;
                    },
                    Err(e) => {
                        warn!(
                            "{:?} for outbound handler on {:?}. This can happen in shutdown,\
                             but should not happen otherwise",
                            e.to_string(),
                            socket_addr
                        );
                        return;
                    },
                }
            }

            let remote_addr = &outbound_handlers[index].1;
            let message_type = &outbound_handlers[index].2;

            if message_type.get_type() == "stop_task" {
                return;
            }

            if remote_addr == socket_addr {
                // If the remote address is the same as the local address, then we are sending a message to ourselves
                // so we should just pass it to the inbound handler
                inbound_handler
                    .lock()
                    .unwrap()
                    .send_incoming_message_to_handler(message_type, msg);
            } else {
                grpc_clients
                    .get_mut(remote_addr)
                    .unwrap()
                    .send_message(*socket_addr, msg, message_type)
                    .await;
            }
        }
    }
}
```

**File:** secure/net/src/network_controller/mod.rs (L115-126)
```rust
    pub fn create_outbound_channel(
        &mut self,
        remote_peer_addr: SocketAddr,
        message_type: String,
    ) -> Sender<Message> {
        let (outbound_sender, outbound_receiver) = unbounded();

        self.outbound_handler
            .register_handler(message_type, remote_peer_addr, outbound_receiver);

        outbound_sender
    }
```

**File:** secure/net/src/network_controller/mod.rs (L152-166)
```rust
    // TODO: This is still not a very clean shutdown. We don't wait for the full shutdown after
    //       sending the signal. May not matter much for now because we shutdown before exiting the
    //       process. Ideally, we want to fix this.
    pub fn shutdown(&mut self) {
        info!("Shutting down network controller at {}", self.listen_addr);
        if let Some(shutdown_signal) = self.inbound_server_shutdown_tx.take() {
            shutdown_signal.send(()).unwrap();
        }

        if let Some(shutdown_signal) = self.outbound_task_shutdown_tx.take() {
            shutdown_signal.send(Message::new(vec![])).unwrap_or_else(|_| {
                warn!("Failed to send shutdown signal to outbound task; probably already shutdown");
            })
        }
    }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L27-47)
```rust
    pub fn new(
        shard_id: ShardId,
        controller: &mut NetworkController,
        coordinator_address: SocketAddr,
    ) -> Self {
        let execute_command_type = format!("execute_command_{}", shard_id);
        let execute_result_type = format!("execute_result_{}", shard_id);
        let command_rx = controller.create_inbound_channel(execute_command_type);
        let result_tx =
            controller.create_outbound_channel(coordinator_address, execute_result_type);

        let state_view_client =
            RemoteStateViewClient::new(shard_id, controller, coordinator_address);

        Self {
            state_view_client: Arc::new(state_view_client),
            command_rx,
            result_tx,
            shard_id,
        }
    }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L115-119)
```rust
    fn send_execution_result(&self, result: Result<Vec<Vec<TransactionOutput>>, VMStatus>) {
        let remote_execution_result = RemoteExecutionResult::new(result);
        let output_message = bcs::to_bytes(&remote_execution_result).unwrap();
        self.result_tx.send(Message::new(output_message)).unwrap();
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L70-116)
```rust
    pub fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>, VMStatus> {
        let _timer = SHARDED_BLOCK_EXECUTION_SECONDS.start_timer();
        let num_executor_shards = self.executor_client.num_shards();
        NUM_EXECUTOR_SHARDS.set(num_executor_shards as i64);
        assert_eq!(
            num_executor_shards,
            transactions.num_shards(),
            "Block must be partitioned into {} sub-blocks",
            num_executor_shards
        );
        let (sharded_output, global_output) = self
            .executor_client
            .execute_block(
                state_view,
                transactions,
                concurrency_level_per_shard,
                onchain_config,
            )?
            .into_inner();
        // wait for all remote executors to send the result back and append them in order by shard id
        info!("ShardedBlockExecutor Received all results");
        let _aggregation_timer = SHARDED_EXECUTION_RESULT_AGGREGATION_SECONDS.start_timer();
        let num_rounds = sharded_output[0].len();
        let mut aggregated_results = vec![];
        let mut ordered_results = vec![vec![]; num_executor_shards * num_rounds];
        // Append the output from individual shards in the round order
        for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
            for (round, result) in results_from_shard.into_iter().enumerate() {
                ordered_results[round * num_executor_shards + shard_id] = result;
            }
        }

        for result in ordered_results.into_iter() {
            aggregated_results.extend(result);
        }

        // Lastly append the global output
        aggregated_results.extend(global_output);

        Ok(aggregated_results)
    }
```
