# Audit Report

## Title
Reset Protocol Lacks Timeout and Cancellation Support, Creating Liveness vs. Correctness Dilemma

## Summary
The `reset()` function in `execution_client.rs` awaits acknowledgments from buffer_manager and rand_manager without any timeout mechanism. This creates a design vulnerability where the system must choose between indefinite hangs (liveness failure) or implementing timeouts that could incorrectly trigger `ResetDropped`/`RandResetDropped` errors while resets actually succeed (correctness failure).

## Finding Description

The reset protocol has a critical design flaw in how it coordinates acknowledgments: [1](#0-0) 

The `ack_rx.await` calls at lines 692 and 705 wait indefinitely for acknowledgments. However, the buffer_manager's reset operation can take unbounded time: [2](#0-1) 

The buffer_manager waits without timeout for:
1. Pending commit blocks to finish at line 550
2. Pipeline futures to complete at lines 555, 568
3. Ongoing tasks counter to reach zero at lines 573-575

When the manager finally completes the reset, it ignores send failures: [3](#0-2) 

This creates a Catch-22:
- **Without timeout**: If reset operations hang on stuck tasks or slow commits, `ack_rx.await` hangs indefinitely, causing consensus liveness failure
- **With timeout**: If a timeout is added and expires before legitimate slow reset completes, `ResetDropped` error is triggered even though the reset succeeds afterward, causing state inconsistency between consensus layer (thinks reset failed) and execution pipeline (actually reset)

The managers cannot be notified of timeout/cancellation and have no mechanism to abort in-progress resets.

## Impact Explanation

**Current State (Medium Severity):**
- Indefinite hangs violate liveness guarantees
- Meets "State inconsistencies requiring intervention" criterion
- Can cause consensus to stall during sync operations

**If Timeout Added Incorrectly (Medium to High Severity):**
- False positive errors while reset succeeds
- State divergence between consensus and execution layers
- Could trigger incorrect recovery logic
- Violates State Consistency invariant [4](#0-3) 

The `sync_for_duration` function calls `reset()` and propagates errors, which could cause cascading failures if timeouts produce false positives.

## Likelihood Explanation

**High likelihood of manifestation:**
- Reset operations regularly wait for pending execution tasks
- Under high load, tasks can legitimately take seconds to complete
- Network issues or slow storage can extend wait times
- Any operator attempting to add defensive timeouts would trigger this issue
- No current mitigation exists

## Recommendation

Implement proper timeout and cancellation support for the reset protocol:

1. **Add cancellation tokens to ResetRequest:**
```rust
pub struct ResetRequest {
    pub tx: oneshot::Sender<ResetAck>,
    pub signal: ResetSignal,
    pub cancel_token: CancellationToken, // New field
}
```

2. **Implement cooperative cancellation in buffer_manager.reset():**
    - Check cancellation token periodically in wait loops
    - Abort pending operations when cancelled
    - Send cancellation-specific response

3. **Add reasonable timeout with retry logic:**
```rust
const RESET_TIMEOUT: Duration = Duration::from_secs(30);
const MAX_RETRIES: u32 = 3;

match tokio::time::timeout(RESET_TIMEOUT, ack_rx).await {
    Ok(Ok(_)) => Ok(()),
    Ok(Err(_)) => Err(Error::ResetDropped),
    Err(_) => {
        // Timeout - retry or escalate
        if retry_count < MAX_RETRIES {
            // Retry logic
        } else {
            // Force reset through alternative mechanism
        }
    }
}
```

4. **Add monitoring for hung resets:**
    - Track reset duration metrics
    - Alert on abnormally long resets
    - Implement circuit breaker pattern

## Proof of Concept

The vulnerability manifests in realistic scenarios but requires specific conditions. Here's a reproduction scenario:

```rust
// Scenario: Slow commit_ledger causes reset timeout

// 1. Submit blocks that trigger slow commit operations
// 2. Initiate sync_to_target which calls reset()
// 3. If timeout were added (e.g., 10 seconds)
// 4. commit_ledger takes 15 seconds due to slow storage
// 5. Timeout fires at 10 seconds, triggering ResetDropped error
// 6. At 15 seconds, reset actually completes successfully
// 7. System state: consensus thinks reset failed, managers are reset
// 8. Result: State inconsistency and potential consensus stall

// Current behavior (no timeout):
// - Same scenario causes indefinite hang at step 4
// - No error returned, just permanent stall
// - Requires node restart to recover
```

To reproduce the hang scenario:
1. Deploy consensus node
2. Create high transaction load to fill buffer_manager
3. Trigger epoch transition or sync operation
4. Inject delays in block execution (via fail points or slow disk I/O)
5. Observe reset() call hangs indefinitely in `ack_rx.await`
6. Node becomes unresponsive, requires manual intervention

**Notes**

The vulnerability exists in the architectural design rather than a specific code bug. The reset protocol was designed assuming fast, reliable completion but lacks defensive mechanisms for real-world failure scenarios. This affects all validators during sync and epoch transitions, making it a systemic reliability issue rather than an exploitable attack vector.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L642-659)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Sync for the specified duration
        let result = self.execution_proxy.sync_for_duration(duration).await;

        // Reset the rand and buffer managers to the new synced round
        if let Ok(latest_synced_ledger_info) = &result {
            self.reset(latest_synced_ledger_info).await?;
        }

        result
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L546-576)
```rust
    async fn reset(&mut self) {
        while let Some((_, block)) = self.pending_commit_blocks.pop_first() {
            // Those blocks don't have any dependencies, should be able to finish commit_ledger.
            // Abort them can cause error on epoch boundary.
            block.wait_for_commit_ledger().await;
        }
        while let Some(item) = self.buffer.pop_front() {
            for b in item.get_blocks() {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
        self.buffer = Buffer::new();
        self.execution_root = None;
        self.signing_root = None;
        self.previous_commit_time = Instant::now();
        self.commit_proof_rb_handle.take();
        // purge the incoming blocks queue
        while let Ok(Some(blocks)) = self.block_rx.try_next() {
            for b in blocks.ordered_blocks {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
        // Wait for ongoing tasks to finish before sending back ack.
        while self.ongoing_tasks.load(Ordering::SeqCst) > 0 {
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L594-594)
```rust
        let _ = tx.send(ResetAck::default());
```
