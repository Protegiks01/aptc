# Audit Report

## Title
Optimistic File Store Version Update Causes False Negative in IS_FILE_STORE_LAGGING Metric Leading to Silent Indexer Data Loss

## Summary
The `IS_FILE_STORE_LAGGING` metric exhibits a false negative condition on master instances where it remains at 0 even when the file store is actually lagging. This occurs because the `file_store_version` is optimistically updated in cache before transactions are successfully uploaded to the file store, allowing garbage collection to proceed and potentially causing data loss if uploads subsequently fail.

## Finding Description
On master instances of the indexer-grpc-manager, the `file_store_version` tracking variable in the cache is updated **before** transactions are actually persisted to the file store, creating a dangerous race condition.

The vulnerability flow:

1. **Optimistic Version Update**: When the FileStoreUploader requests transactions from the cache, the `file_store_version` is immediately incremented [1](#0-0) .

2. **Asynchronous Upload**: Transactions are then buffered and sent to an asynchronous upload task [2](#0-1) , which may fail or be delayed.

3. **Premature Garbage Collection**: The cache's `maybe_gc()` function uses the optimistically-updated `file_store_version` to determine which transactions can be safely removed [3](#0-2) . This allows GC to remove transactions that haven't actually been uploaded yet.

4. **False Negative Metric**: When GC succeeds based on the optimistic version, `IS_FILE_STORE_LAGGING` is set to 0 [4](#0-3) , even though uploads may be failing in the background.

5. **Data Loss on Failure**: If the upload task fails after GC has removed transactions from cache, and the process crashes [5](#0-4) , those transactions are lost from cache but never made it to the file store.

6. **No Recovery Path**: On restart, the system recovers from the actual file store version. If fullnodes have already pruned the missing transactions, they are permanently lost from the indexer.

Master instances are specifically vulnerable because they don't call `update_file_store_version_in_cache()` in their main loop [6](#0-5) , relying solely on the optimistic updates from the FileStoreUploader.

## Impact Explanation
This issue is classified as **Medium Severity** according to Aptos bug bounty criteria because it causes **state inconsistencies requiring intervention** in the indexer service:

- **Data Availability Impact**: Historical transaction data can be permanently lost if fullnodes prune before successful file store persistence
- **Monitoring Blind Spot**: The false negative in `IS_FILE_STORE_LAGGING` prevents operators from detecting and responding to upload failures
- **Silent Failure Mode**: Data loss can occur without triggering alerts, making the issue difficult to detect and debug

While this doesn't directly affect blockchain consensus or fund security (the core blockchain state remains intact), it breaks the data durability guarantee of the indexer service, which is critical for ecosystem participants that rely on historical transaction data (exchanges, explorers, analytics platforms).

## Likelihood Explanation
**Likelihood: Medium to High**

This vulnerability can be triggered by common operational scenarios:
- **Network transient failures** during file store uploads (cloud storage timeouts, connection drops)
- **File store service degradation** or temporary unavailability
- **Rate limiting** from cloud storage providers
- **Disk space exhaustion** on file store backends

These conditions are realistic in production environments and do not require any attacker action. The optimistic version update pattern makes the race condition highly probable during any upload disruption.

## Recommendation
Implement a **pessimistic file store version update** pattern where `file_store_version` is only incremented after successful persistence to the file store:

1. Remove the optimistic update from `get_transactions_from_cache()` 
2. Have the upload task confirm successful uploads back to the DataManager
3. Only then update `file_store_version` in cache
4. Implement explicit error handling in the upload task to retry failed uploads instead of panicking

**Alternative Approach** (if optimistic update must be retained):
- Implement a separate "pending uploads" tracking structure
- Don't allow GC of transactions in the pending upload queue
- Only mark transactions as GC-eligible after upload confirmation
- Add a timeout to detect stuck uploads and set `IS_FILE_STORE_LAGGING` appropriately

The fix should ensure that `file_store_version` accurately reflects what has been **persisted** to the file store, not what has been **sent** for upload.

## Proof of Concept

```rust
// Simulation of the vulnerability:
// 
// 1. Set up master instance with cache containing transactions 1000-2000
// 2. file_store_version in cache = 1000
// 3. Inject file store upload failure after transactions are retrieved
// 4. Observe that:
//    - file_store_version in cache is updated to 2000 (optimistically)
//    - GC removes transactions 1000-2000 from cache
//    - IS_FILE_STORE_LAGGING remains 0
//    - Upload fails and process panics
//    - On restart, transactions 1000-2000 are not in cache or file store
//    - If fullnodes have pruned, data is permanently lost
//
// Steps to reproduce:
// 1. Deploy master indexer-grpc-manager instance
// 2. Configure file store with artificially high failure rate
// 3. Monitor IS_FILE_STORE_LAGGING metric
// 4. Observe cache growth and GC behavior
// 5. Trigger upload failures
// 6. Verify metric remains 0 while uploads are failing
// 7. Kill process and restart
// 8. Check for missing transaction ranges
```

**Notes**
This finding is specific to the indexer-grpc-manager service, which is an auxiliary component that provides historical transaction data to clients. While this represents a real data durability bug that can cause permanent loss of indexer data, it does not affect the core blockchain's consensus, state integrity, or fund security. The blockchain itself continues to operate correctly even if the indexer loses data.

The severity assessment is based on the potential for data loss in a critical ecosystem service, though operators should note that this is fundamentally an indexer reliability issue rather than a core protocol vulnerability.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L68-74)
```rust
        while self.start_version < self.file_store_version.load(Ordering::SeqCst)
            && self.cache_size > self.target_cache_size
        {
            let transaction = self.transactions.pop_front().unwrap();
            self.cache_size -= transaction.encoded_len();
            self.start_version += 1;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L127-135)
```rust
        if update_file_store_version {
            if !transactions.is_empty() {
                let old_version = self
                    .file_store_version
                    .fetch_add(transactions.len() as u64, Ordering::SeqCst);
                let new_version = old_version + transactions.len() as u64;
                FILE_STORE_VERSION_IN_CACHE.set(new_version as i64);
                info!("Updated file_store_version in cache to {new_version}.");
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L179-206)
```rust
        let watch_file_store_version = !is_master;

        if is_master {
            // For master, we need to wait for the FileStoreUploader to finish the recover to get
            // the true file_store_version.
            info!("Waiting for FileStoreUploader recovering.");
            match file_store_uploader_recover_rx.await {
                Ok(_) => {},
                Err(_) => panic!("Should not happen!"),
            };
            let cache = self.cache.read().await;
            self.update_file_store_version_in_cache(&cache, /*version_can_go_backward=*/ true)
                .await;
        }

        info!("Starting DataManager loop.");

        'out: loop {
            let _timer = TIMER
                .with_label_values(&["data_manager_main_loop"])
                .start_timer();
            let cache = self.cache.read().await;
            if watch_file_store_version {
                self.update_file_store_version_in_cache(
                    &cache, /*version_can_go_backward=*/ false,
                )
                .await;
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L237-239)
```rust
                    if self.cache.write().await.maybe_gc() {
                        IS_FILE_STORE_LAGGING.set(0);
                        trace!("GC is done, file store is not lagging.");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L141-143)
```rust
                    self.do_upload(transactions, batch_metadata, end_batch)
                        .await
                        .unwrap();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L166-171)
```rust
                    for transaction in transactions {
                        file_store_operator
                            .buffer_and_maybe_dump_transactions_to_file(transaction, tx.clone())
                            .await
                            .unwrap();
                    }
```
