# Audit Report

## Title
Execution Failure Prevents Commit Vote Generation Leading to Consensus Stall

## Summary
ExecutionWaitPhase returns error responses that prevent buffer items from advancing to Executed state, blocking commit vote generation. When execution fails, there is no retry mechanism, causing blocks to remain stuck in Ordered state indefinitely. If execution errors are deterministic and affect all validators, consensus cannot proceed as no commit votes are generated.

## Finding Description

The vulnerability exists in the consensus pipeline's handling of execution failures. The flow is:

1. **ExecutionWaitPhase returns error**: When `wait_for_compute_result()` fails (e.g., executor errors, pipeline aborted), ExecutionWaitPhase wraps the error in `ExecutionResponse.inner` [1](#0-0) 

2. **BufferManager fails to advance item**: Upon receiving an error response, `process_execution_response()` logs the error and returns early without advancing the buffer item to Executed state [2](#0-1) 

3. **Signing phase never triggered**: The `advance_signing_root()` method only selects buffer items in Executed state [3](#0-2) , meaning blocks stuck in Ordered state never reach the signing phase.

4. **No retry mechanism exists**: While `advance_execution_root()` returns `Some(block_id)` with a comment "Schedule retry" when execution root hasn't advanced [4](#0-3) , this return value is never captured or used in the main event loop [5](#0-4) . This contrasts sharply with the signing phase, which implements actual retry logic [6](#0-5) .

5. **Execution errors propagate from executor**: The `wait_for_compute_result()` method can fail when `ledger_update_fut` returns errors from the executor [7](#0-6) , which calls `executor.ledger_update()` [8](#0-7) . Various `ExecutorError` types can occur [9](#0-8) .

**Attack Scenario**: If an attacker discovers transactions or block patterns that trigger deterministic executor bugs (e.g., edge cases in Move VM execution, state management errors, or resource limit violations), they can submit these transactions. When included in blocks, all validators would experience identical execution failures, preventing any validator from generating commit votes, thereby stalling consensus.

## Impact Explanation

This qualifies as **High Severity** per the Aptos bug bounty program because it can cause "Significant protocol violations" and "Validator node slowdowns" leading to liveness issues. 

While the impact depends on finding exploitable executor bugs, the systemic lack of error recovery for a critical consensus path represents a significant vulnerability. If execution fails deterministically:
- No commit votes are generated by any validator
- Consensus cannot proceed beyond the failed block
- The network becomes stuck until manual intervention (reset/recovery)

This breaks the **Consensus Safety** invariant (liveness component) and the **Deterministic Execution** invariant (error handling consistency).

## Likelihood Explanation

**Moderate to High likelihood** if executor bugs exist:
- Execution path is exercised for every block
- No retry or recovery mechanism exists
- Complex executor codebase (Move VM, state management, storage) increases bug surface area
- Historical precedent shows execution engines in blockchain systems have contained edge-case bugs

The likelihood is reduced by:
- Requiring discovery of specific executor bugs
- Deterministic failures needed to affect all validators
- Extensive testing of the executor components

However, the complete absence of retry logic for a critical consensus path is concerning.

## Recommendation

**Immediate Fix**: Implement retry logic for execution failures similar to the signing phase:

```rust
fn advance_execution_root(&mut self) -> Option<HashValue> {
    let cursor = self.execution_root;
    self.execution_root = self
        .buffer
        .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
            item.is_ordered()
        });
    if self.execution_root.is_some() && cursor == self.execution_root {
        // Return block_id to trigger retry
        self.execution_root
    } else {
        None
    }
}
```

Then capture and use the return value:

```rust
Some(response) = self.execution_wait_phase_rx.next() => {
    monitor!("buffer_manager_process_execution_wait_response", {
        self.process_execution_response(response).await;
        if let Some(retry_block_id) = self.advance_execution_root() {
            // Implement retry logic: re-send execution request
            // Or trigger reset/recovery after N retries
            warn!("Execution retry needed for block {}", retry_block_id);
        }
        if self.signing_root.is_none() {
            self.advance_signing_root().await;
        }
    });
},
```

**Additional Mitigations**:
1. Add retry counter with exponential backoff
2. Implement circuit breaker after N consecutive failures
3. Add alerting for execution errors
4. Consider allowing commit decisions from peers to advance stuck blocks in degraded mode

## Proof of Concept

```rust
// Test demonstrating execution failure blocking commit votes
#[tokio::test]
async fn test_execution_failure_blocks_commit_votes() {
    // Setup: Create BufferManager with mocked executor that returns errors
    let (buffer_manager, mut signing_phase_rx, execution_wait_tx) = setup_buffer_manager();
    
    // 1. Send ordered blocks to buffer manager
    let ordered_blocks = create_test_blocks(1);
    buffer_manager.process_ordered_blocks(ordered_blocks).await;
    
    // 2. Simulate execution failure by sending error response
    let error_response = ExecutionResponse {
        block_id: test_block_id,
        inner: Err(ExecutorError::InternalError { 
            error: "Simulated executor failure".to_string() 
        }),
    };
    execution_wait_tx.send(error_response).await.unwrap();
    
    // 3. Verify buffer item remains in Ordered state
    let item = buffer_manager.buffer.get_by_id(test_block_id);
    assert!(item.is_ordered(), "Item should remain Ordered after execution failure");
    
    // 4. Verify no signing request is sent (timeout proves this)
    tokio::time::timeout(Duration::from_secs(1), signing_phase_rx.next())
        .await
        .expect_err("No signing request should be sent for failed execution");
    
    // 5. Verify commit vote is never generated
    // This demonstrates the vulnerability: execution failure prevents voting
}
```

**Notes**

The vulnerability is conditional on the existence of exploitable executor bugs. However, the systemic lack of error recovery in a critical consensus path represents a significant defensive programming failure. The contrast between signing phase retry logic and execution phase's complete lack of retry handling suggests this was an oversight rather than a design decision.

The severity assessment assumes that execution errors, while hopefully rare, are possible in complex systems and should be handled gracefully rather than causing consensus to permanently stall.

### Citations

**File:** consensus/src/pipeline/execution_wait_phase.rs (L49-56)
```rust
    async fn process(&self, req: ExecutionWaitRequest) -> ExecutionResponse {
        let ExecutionWaitRequest { block_id, fut } = req;

        ExecutionResponse {
            block_id,
            inner: fut.await,
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L436-438)
```rust
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
```

**File:** consensus/src/pipeline/buffer_manager.rs (L456-462)
```rust
    async fn advance_signing_root(&mut self) {
        let cursor = self.signing_root;
        self.signing_root = self
            .buffer
            .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
                item.is_executed()
            });
```

**File:** consensus/src/pipeline/buffer_manager.rs (L478-480)
```rust
            if cursor == self.signing_root {
                let sender = self.signing_phase_tx.clone();
                Self::spawn_retry_request(sender, request, Duration::from_millis(100));
```

**File:** consensus/src/pipeline/buffer_manager.rs (L617-627)
```rust
        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
        };
```

**File:** consensus/src/pipeline/buffer_manager.rs (L954-960)
```rust
                Some(response) = self.execution_wait_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_wait_response", {
                    self.process_execution_response(response).await;
                    self.advance_execution_root();
                    if self.signing_root.is_none() {
                        self.advance_signing_root().await;
                    }});
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L549-560)
```rust
    pub async fn wait_for_compute_result(&self) -> ExecutorResult<(StateComputeResult, Duration)> {
        self.pipeline_futs()
            .ok_or(ExecutorError::InternalError {
                error: "Pipeline aborted".to_string(),
            })?
            .ledger_update_fut
            .await
            .map(|(compute_result, execution_time, _)| (compute_result, execution_time))
            .map_err(|e| ExecutorError::InternalError {
                error: e.to_string(),
            })
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L887-893)
```rust
        let result = tokio::task::spawn_blocking(move || {
            executor
                .ledger_update(block_clone.id(), block_clone.parent_id())
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```

**File:** execution/executor-types/src/error.rs (L11-43)
```rust
#[derive(Debug, Deserialize, Error, PartialEq, Eq, Serialize, Clone)]
/// Different reasons for proposal rejection
pub enum ExecutorError {
    #[error("Cannot find speculation result for block id {0}")]
    BlockNotFound(HashValue),

    #[error("Cannot get data for batch id {0}")]
    DataNotFound(HashValue),

    #[error(
        "Bad num_txns_to_commit. first version {}, num to commit: {}, target version: {}",
        first_version,
        to_commit,
        target_version
    )]
    BadNumTxnsToCommit {
        first_version: Version,
        to_commit: usize,
        target_version: Version,
    },

    #[error("Internal error: {:?}", error)]
    InternalError { error: String },

    #[error("Serialization error: {0}")]
    SerializationError(String),

    #[error("Received Empty Blocks")]
    EmptyBlocks,

    #[error("request timeout")]
    CouldNotGetData,
}
```
