# Audit Report

## Title
Time-of-Check-Time-of-Use Race Condition in Global Layout Cache During Concurrent Module Publishing

## Summary
A race condition exists in the global layout cache implementation when modules are published during parallel transaction execution. Stale struct layouts computed from old module definitions can be inserted into the cache after `flush_layout_cache()` clears it, causing subsequent transactions to deserialize struct values using incorrect layouts. This breaks deterministic execution and can lead to consensus violations.

## Finding Description

The vulnerability occurs in the interaction between layout computation, cache invalidation, and concurrent module publishing in BlockSTM parallel execution.

**Architecture Context:**
The layout cache is a global shared structure `DashMap<StructKey, LayoutCacheEntry>` that caches Move struct type layouts for performance. [1](#0-0) 

When a transaction needs a struct layout, it checks the cache and computes the layout if missing. [2](#0-1) 

When modules are published, the system marks the old module as overridden and flushes the entire layout cache to ensure layouts are recomputed with the new module definitions. [3](#0-2) 

**The Race Condition:**

1. **Thread A (Transaction T1, index 10)**: Computing layout for `Foo::Bar`
   - Checks cache at line 109: MISS
   - Loads module `Foo` from global cache (not yet overridden)
   - Begins layout computation using OLD module definition

2. **Thread B (Transaction T2, index 5)**: Publishes new version of module `Foo`
   - Marks old module as overridden [4](#0-3) 
   - Flushes layout cache (clears all cached layouts) [5](#0-4) 

3. **Thread A (continuing)**: Finishes computing layout with OLD module
   - Attempts to store layout in cache
   - DashMap Vacant entry check passes (cache was just cleared) [6](#0-5) 
   - Inserts STALE layout into cache

4. **Thread C (Transaction T3, index 15)**: Needs layout for `Foo::Bar`
   - Cache HIT: retrieves STALE layout computed from old module
   - Re-reads modules for gas charging (gets NEW module from per-block cache)
   - Module reads are validated correctly
   - But uses STALE layout that doesn't match NEW module structure

**Critical Flaw:**
The `load_layout_from_cache` function re-reads modules only for gas charging and read-set tracking, NOT for validating that the cached layout matches the current module versions. [7](#0-6) 

The cache key `StructKey` contains only struct name index and type arguments, with no version information to detect stale layouts. [8](#0-7) 

**Consensus Impact:**
Different validators executing the same block at slightly different times may:
- Validator V1: Executes T3 before stale layout is cached → cache miss → computes correct layout
- Validator V2: Executes T3 after stale layout is cached → cache hit → uses incorrect layout

If T3 deserializes a struct value created by T2 (with the new module), the two validators will produce different execution results, violating **Invariant #1: Deterministic Execution**.

## Impact Explanation

**Severity: Critical** (Consensus/Safety Violation)

This vulnerability breaks the fundamental deterministic execution invariant of the Aptos blockchain. All validators must produce identical state roots for identical blocks to maintain consensus safety.

The race condition can cause:

1. **Non-deterministic Execution**: Different validators may execute transactions with different cached layouts, producing different outputs for the same transaction sequence.

2. **State Root Divergence**: Validators computing state roots with different layouts will produce different Merkle tree roots, causing consensus failure.

3. **Transaction Execution Errors**: Using an incorrect layout to deserialize struct values can cause:
   - Wrong field values (if field order changed)
   - Type mismatches (if field types changed)  
   - Crashes or panics in the Move VM
   - Silent data corruption

4. **Network Partition Risk**: If sufficient validators diverge due to inconsistent cache states, the network could partition, requiring manual intervention or a hard fork to recover.

This meets the **Critical Severity** criteria: "Consensus/Safety violations" and potentially "Non-recoverable network partition (requires hardfork)."

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability is triggered whenever:
1. Parallel execution is enabled (default with BlockSTM)
2. Layout caching is enabled (default: `layout_caches_enabled: true`) [9](#0-8) 
3. A transaction publishes a module that modifies struct definitions
4. Another transaction concurrently computes a layout for a struct in that module
5. The timing window aligns such that layout computation spans the cache flush

**Factors increasing likelihood:**
- Module upgrades are common in Move smart contract development
- High transaction throughput increases parallel execution overlap
- Layout computation is non-trivial (can take significant time for complex nested structs)
- No synchronization between layout computation and cache invalidation

**Factors decreasing likelihood:**
- Requires specific timing alignment
- Only affects struct layouts from modules being upgraded
- Validators typically execute deterministically similar code paths, so all may hit the same race condition (leading to consensus, not divergence)

However, even low-probability consensus violations are **critical** because a single occurrence can halt the network.

## Recommendation

**Primary Fix: Add versioning or module hash to layout cache keys**

Modify `StructKey` to include module version/hash information:

```rust
pub struct StructKey {
    pub idx: StructNameIndex,
    pub ty_args_id: TypeVecId,
    pub module_version: ModuleVersion, // NEW: Track module version
}
```

Where `ModuleVersion` could be:
- Module hash (from `AptosModuleExtension`)
- Transaction index where module was published (for per-block cache)
- Epoch number (for cross-block cache)

This ensures cached layouts are only used when computed from the same module version.

**Alternative Fix: Validate layouts against modules on cache hit**

In `load_layout_from_cache`, after re-reading modules, verify the layout matches the current module definitions:

```rust
fn load_layout_from_cache(...) -> Option<PartialVMResult<LayoutWithDelayedFields>> {
    let entry = self.module_storage.get_struct_layout(key)?;
    let (layout, modules) = entry.unpack();
    
    // Re-read modules and validate they haven't changed
    for module_id in modules.iter() {
        if let Err(err) = self.charge_module(gas_meter, traversal_context, module_id) {
            return Some(Err(err));
        }
        
        // NEW: Check if module was overridden or changed
        if module_was_republished(module_id) {
            return None; // Cache miss, force recomputation
        }
    }
    
    Some(Ok(layout))
}
```

**Immediate Mitigation:**

Use proper locking around layout cache operations:
- Acquire a write lock before `flush_layout_cache()`
- Hold lock during module publication and cache flush
- Prevent concurrent layout insertions during flush

Or disable layout caching (`layout_caches_enabled: false`) until the fix is deployed.

## Proof of Concept

```rust
// Rust test demonstrating the race condition
// File: aptos-move/block-executor/src/code_cache_global_test.rs

#[test]
fn test_layout_cache_race_condition() {
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    let global_cache = Arc::new(GlobalModuleCache::empty());
    let barrier = Arc::new(Barrier::new(2));
    
    // Thread 1: Compute and cache layout
    let cache1 = global_cache.clone();
    let barrier1 = barrier.clone();
    let handle1 = thread::spawn(move || {
        // Simulate layout computation from OLD module
        barrier1.wait(); // Sync start
        thread::sleep(Duration::from_millis(10)); // Simulate computation time
        
        let layout_entry = /* compute layout from old module */;
        cache1.store_struct_layout_entry(&key, layout_entry).unwrap();
    });
    
    // Thread 2: Publish module and flush cache
    let cache2 = global_cache.clone();
    let barrier2 = barrier.clone();
    let handle2 = thread::spawn(move || {
        barrier2.wait(); // Sync start
        thread::sleep(Duration::from_millis(5)); // Start flush mid-computation
        
        // Mark module as overridden and flush layout cache
        cache2.mark_overridden(&module_id);
        cache2.flush_layout_cache();
    });
    
    handle1.join().unwrap();
    handle2.join().unwrap();
    
    // Verify: Stale layout is in cache after flush
    let cached_layout = global_cache.get_struct_layout_entry(&key);
    assert!(cached_layout.is_some()); // BUG: Stale entry present after flush!
}
```

This test demonstrates that layouts computed before the flush can be inserted after the flush, creating stale cache entries.

---

**Notes:**
- The vulnerability exists in the interaction between concurrent layout computation (`ty_layout_converter.rs`), cache storage (`code_cache_global.rs`), and module publishing (`txn_last_input_output.rs`)
- The root cause is the lack of atomic cache invalidation and the absence of version tracking in cache keys
- This affects the default configuration where both parallel execution and layout caching are enabled
- The fix requires careful consideration of cache key design to maintain performance while ensuring correctness

### Citations

**File:** aptos-move/block-executor/src/code_cache_global.rs (L96-96)
```rust
    struct_layouts: DashMap<StructKey, LayoutCacheEntry>,
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L186-188)
```rust
        if let dashmap::Entry::Vacant(e) = self.struct_layouts.entry(*key) {
            e.insert(entry);
        }
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L317-317)
```rust
    global_module_cache.mark_overridden(write.module_id());
```

**File:** third_party/move/move-vm/runtime/src/storage/ty_layout_converter.rs (L108-130)
```rust
            if let Some(key) = key {
                if let Some(result) = self.struct_definition_loader.load_layout_from_cache(
                    gas_meter,
                    traversal_context,
                    &key,
                ) {
                    return result;
                }

                // Otherwise a cache miss, compute the result and store it.
                let mut modules = DefiningModules::new();
                let layout = self.type_to_type_layout_with_delayed_fields_impl::<false>(
                    gas_meter,
                    traversal_context,
                    &mut modules,
                    ty,
                    check_option_type,
                )?;
                let cache_entry = LayoutCacheEntry::new(layout.clone(), modules);
                self.struct_definition_loader
                    .store_layout_to_cache(&key, cache_entry)?;
                return Ok(layout);
            }
```

**File:** aptos-move/block-executor/src/txn_last_input_output.rs (L572-575)
```rust
        if published {
            // Record validation requirements after the modules are published.
            global_module_cache.flush_layout_cache();
            scheduler.record_validation_requirements(txn_idx, module_ids_for_v2)?;
```

**File:** third_party/move/move-vm/runtime/src/storage/loader/lazy.rs (L203-221)
```rust
    fn load_layout_from_cache(
        &self,
        gas_meter: &mut impl DependencyGasMeter,
        traversal_context: &mut TraversalContext,
        key: &StructKey,
    ) -> Option<PartialVMResult<LayoutWithDelayedFields>> {
        let entry = self.module_storage.get_struct_layout(key)?;
        let (layout, modules) = entry.unpack();
        for module_id in modules.iter() {
            // Re-read all modules for this layout, so that transaction gets invalidated
            // on module publish. Also, we re-read them in exactly the same way as they
            // were traversed during layout construction, so gas charging should be exactly
            // the same as on the cache miss.
            if let Err(err) = self.charge_module(gas_meter, traversal_context, module_id) {
                return Some(Err(err));
            }
        }
        Some(Ok(layout))
    }
```

**File:** third_party/move/move-vm/runtime/src/storage/layout_cache.rs (L79-83)
```rust
#[derive(Debug, Copy, Clone, Eq, PartialEq, Hash)]
pub struct StructKey {
    pub idx: StructNameIndex,
    pub ty_args_id: TypeVecId,
}
```

**File:** config/src/config/execution_config.rs (L92-92)
```rust
            layout_caches_enabled: true,
```
