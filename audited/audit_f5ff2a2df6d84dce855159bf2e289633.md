# Audit Report

## Title
Byzantine Validators Can Create Targeted Network Partitions Through Selective Health Check Response

## Summary
Byzantine validators can selectively respond to health check pings from different validators, creating asymmetric network topology where they remain connected to some validators while being disconnected from others. This violates the full mesh connectivity invariant and enables targeted message delivery failures that can degrade consensus performance and potentially enable network partition attacks.

## Finding Description

The health checker protocol in Aptos enforces peer liveness by periodically sending ping requests to all connected peers. When a peer fails to respond to consecutive pings exceeding `ping_failures_tolerated`, the health checker disconnects from that peer. [1](#0-0) 

The critical vulnerability lies in the fact that a Byzantine validator can selectively choose which ping requests to respond to. The `handle_ping_response()` function processes ping responses, but there's no mechanism to detect or prevent selective response behavior: [2](#0-1) 

When pings fail, the failure counter is incremented, and after exceeding the threshold, the peer is disconnected: [3](#0-2) 

**Attack Execution Path:**

1. Byzantine validator B is part of the validator set
2. B receives ping requests from all validators (V1, V2, V3, V4)
3. B selectively responds to pings from V1, V2 but intentionally drops pings from V3, V4
4. V3, V4 increment failure counters for B and eventually disconnect
5. V1, V2 maintain connections to B (receiving pong responses)
6. ConnectivityManager on V3, V4 attempts reconnection with exponential backoff
7. B repeats selective response pattern to maintain asymmetric topology

This breaks the critical full mesh connectivity invariant documented in the ConnectivityManager: [4](#0-3) 

**Consensus Message Impact:**

Aptos consensus uses direct-send messaging to broadcast to all validators: [5](#0-4) 

When validators are disconnected, messages to those peers are **silently dropped** with only a warning logged: [6](#0-5) 

This means during disconnection periods:
- Byzantine validator B's proposals won't reach V3, V4
- Byzantine validator B's votes won't reach V3, V4
- B can send different messages to connected vs. disconnected validators
- Honest validators V3, V4 have incomplete view of consensus messages

## Impact Explanation

This vulnerability enables **High Severity** protocol violations per Aptos bug bounty criteria:

1. **Significant Protocol Violation**: Breaks the full mesh connectivity assumption that consensus relies upon. The system explicitly states validators must maintain connectivity with all peers and cannot use gossip or relay protocols.

2. **Validator Node Performance Degradation**: Validators experiencing intermittent connectivity due to selective ping responses will suffer:
   - Missed consensus messages during disconnection periods
   - Reconnection overhead with exponential backoff
   - Potential timeout and round delays

3. **Coordinated Network Partition Risk**: Multiple Byzantine validators (up to f validators in a 3f+1 system) could coordinate to selectively partition honest validators into groups, where:
   - Group A receives messages from Byzantine validators
   - Group B is disconnected from Byzantine validators
   - Different validator subsets have inconsistent views of network state
   - This could potentially degrade consensus liveness significantly

4. **Targeted Validator Exclusion**: Byzantine validators can strategically exclude specific honest validators from receiving their messages, potentially targeting validators that would otherwise detect malicious behavior.

While not reaching Critical severity (the network will eventually reconnect and is not permanently partitioned), this represents a significant weakening of the consensus protocol's security guarantees.

## Likelihood Explanation

**Likelihood: HIGH**

The attack is trivial to execute:
- Byzantine validator simply needs to filter incoming health check ping requests
- Drop pong responses to targeted validators
- No special timing, cryptographic operations, or complex state manipulation required
- Can be implemented with a simple conditional in the ping request handler

The attack is highly feasible because:
- Health check pings use RPC protocol that Byzantine validators fully control
- No cryptographic proof or external verification of ping response behavior
- Each validator independently maintains its own health check state
- No global coordination or peer attestation of health status
- Reconnection attempts by ConnectivityManager can be exploited repeatedly

## Recommendation

Implement a bidirectional health verification mechanism that prevents selective response attacks:

**Solution 1: Mutual Health Attestation**
- Track both outbound ping successes AND inbound ping receipts
- If a peer receives pings from validator V but V reports connectivity failures, investigate the asymmetry
- Require validators to periodically attest to their connectivity view
- Cross-validate health status across multiple validators

**Solution 2: Challenge-Response with Proof**
- Include a cryptographically signed challenge in ping requests
- Require pong responses to include proof of received challenges
- Broadcast health check results to other validators for verification
- Detect validators that report different connectivity to different peers

**Solution 3: Gossip-Based Health Monitoring**
- Have validators gossip their connectivity status to peers
- If validator V reports B as connected but B reports V as disconnected, flag the asymmetry
- Use quorum-based health decisions rather than bilateral checks
- Implement reputation system for validators with inconsistent health reporting

**Recommended Code Fix (Partial):**

Add asymmetry detection to health checker interface by tracking both directions:

```rust
// In health_checker/interface.rs
pub struct HealthCheckData {
    pub round: u64,
    pub failures: u64,
    pub last_outbound_success: Option<u64>, // Last round we sent successful ping
    pub last_inbound_ping: Option<u64>,     // Last round we received ping from peer
}

// Detect asymmetry in handle_ping_request
pub fn handle_ping_request(&mut self, peer_id: PeerId, round: u64) {
    let data = self.health_check_data.read().get(&peer_id);
    if let Some(health_data) = data {
        // If we're receiving pings but our outbound pings are failing,
        // this indicates potential selective response attack
        if health_data.failures > ASYMMETRY_THRESHOLD 
            && health_data.last_inbound_ping.is_some() {
            warn!("Detected health check asymmetry with peer {}", peer_id);
            // Report to monitoring/governance system
        }
    }
}
```

## Proof of Concept

```rust
// Rust test demonstrating selective health check response attack
// File: network/framework/src/protocols/health_checker/test.rs

#[tokio::test]
async fn test_selective_ping_response_creates_partition() {
    // Setup 4 validators: V1 (Byzantine), V2, V3, V4 (honest)
    let mut validators = setup_test_validators(4).await;
    let byzantine = &mut validators[0];
    let v2 = &validators[1];
    let v3 = &validators[2];
    let v4 = &validators[3];
    
    // Configure Byzantine validator to only respond to V2, drop V3/V4 pings
    byzantine.set_ping_response_filter(|peer_id| {
        peer_id == v2.peer_id() // Only respond to V2
    });
    
    // Run health checks for multiple rounds exceeding ping_failures_tolerated
    for round in 0..10 {
        run_health_check_round(&mut validators).await;
    }
    
    // Verify asymmetric topology was created:
    // V2 should still be connected to V1 (Byzantine)
    assert!(v2.is_connected_to(byzantine.peer_id()));
    
    // V3 and V4 should have disconnected from V1
    assert!(!v3.is_connected_to(byzantine.peer_id()));
    assert!(!v4.is_connected_to(byzantine.peer_id()));
    
    // Verify consensus messages from Byzantine validator are dropped for V3/V4
    byzantine.broadcast_test_message().await;
    
    assert!(v2.received_message()); // V2 receives (connected)
    assert!(!v3.received_message()); // V3 drops (disconnected)
    assert!(!v4.received_message()); // V4 drops (disconnected)
    
    // This demonstrates successful network partition attack
    println!("Successfully created asymmetric network partition!");
}
```

## Notes

This vulnerability exploits a fundamental design assumption in the health checker: that peers will behave honestly in responding to health checks. The current implementation lacks mechanisms to detect or prevent selective response behavior, enabling Byzantine validators to strategically control their network connectivity topology.

The issue is particularly concerning because:
1. It requires no complex attack setupâ€”just selective message dropping
2. It operates at the network layer, below consensus protocol defenses
3. Multiple Byzantine validators can coordinate for amplified effect
4. The attack is difficult to detect without cross-validator health attestation

While BFT consensus is designed to tolerate Byzantine validators, it assumes full mesh network connectivity. This vulnerability allows Byzantine validators to violate that assumption, potentially creating attack vectors not contemplated by the consensus protocol's security model.

### Citations

**File:** network/framework/src/protocols/health_checker/mod.rs (L243-263)
```rust
                    for peer_id in connected {
                        let nonce = self.rng.r#gen::<u32>();
                        trace!(
                            NetworkSchema::new(&self.network_context),
                            round = self.round,
                            "{} Will ping: {} for round: {} nonce: {}",
                            self.network_context,
                            peer_id.short_str(),
                            self.round,
                            nonce
                        );

                        tick_handlers.push(Self::ping_peer(
                            self.network_context,
                            self.network_interface.network_client(),
                            peer_id,
                            self.round,
                            nonce,
                            self.ping_timeout,
                        ));
                    }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L308-395)
```rust
    async fn handle_ping_response(
        &mut self,
        peer_id: PeerId,
        round: u64,
        req_nonce: u32,
        ping_result: Result<Pong, RpcError>,
    ) {
        match ping_result {
            Ok(pong) => {
                if pong.0 == req_nonce {
                    trace!(
                        NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                        rount = round,
                        "{} Ping successful for peer: {} round: {}",
                        self.network_context,
                        peer_id.short_str(),
                        round
                    );
                    // Update last successful ping to current round.
                    // If it's not in storage, don't bother updating it
                    self.network_interface
                        .reset_peer_round_state(peer_id, round);
                } else {
                    warn!(
                        SecurityEvent::InvalidHealthCheckerMsg,
                        NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                        "{} Pong nonce doesn't match Ping nonce. Round: {}, Pong: {}, Ping: {}",
                        self.network_context,
                        round,
                        pong.0,
                        req_nonce
                    );
                    debug_assert!(false, "Pong nonce doesn't match our challenge Ping nonce");
                }
            },
            Err(err) => {
                warn!(
                    NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                    round = round,
                    "{} Ping failed for peer: {} round: {} with error: {:#}",
                    self.network_context,
                    peer_id.short_str(),
                    round,
                    err
                );
                self.network_interface
                    .increment_peer_round_failure(peer_id, round);

                // If the ping failures are now more than
                // `self.ping_failures_tolerated`, we disconnect from the node.
                // The HealthChecker only performs the disconnect. It relies on
                // ConnectivityManager or the remote peer to re-establish the connection.
                let failures = self
                    .network_interface
                    .get_peer_failures(peer_id)
                    .unwrap_or(0);
                if failures > self.ping_failures_tolerated {
                    info!(
                        NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                        "{} Disconnecting from peer: {}",
                        self.network_context,
                        peer_id.short_str()
                    );
                    let peer_network_id =
                        PeerNetworkId::new(self.network_context.network_id(), peer_id);
                    if let Err(err) = timeout(
                        Duration::from_millis(50),
                        self.network_interface.disconnect_peer(
                            peer_network_id,
                            DisconnectReason::NetworkHealthCheckFailure,
                        ),
                    )
                    .await
                    {
                        warn!(
                            NetworkSchema::new(&self.network_context)
                                .remote_peer(&peer_id),
                            error = ?err,
                            "{} Failed to disconnect from peer: {} with error: {:?}",
                            self.network_context,
                            peer_id.short_str(),
                            err
                        );
                    }
                }
            },
        }
    }
```

**File:** network/framework/src/connectivity_manager/mod.rs (L25-27)
```rust
//! absolutely important that we maintain connectivity with all peers and heal
//! any partitions asap, as we aren't currently gossiping consensus messages or
//! using a relay protocol.
```

**File:** consensus/src/network.rs (L387-408)
```rust
    pub fn broadcast_without_self(&self, msg: ConsensusMsg) {
        fail_point!("consensus::send::any", |_| ());

        let self_author = self.author;
        let mut other_validators: Vec<_> = self
            .validators
            .get_ordered_account_addresses_iter()
            .filter(|author| author != &self_author)
            .collect();
        self.sort_peers_by_latency(&mut other_validators);

        counters::CONSENSUS_SENT_MSGS
            .with_label_values(&[msg.name()])
            .inc_by(other_validators.len() as u64);
        // Broadcast message over direct-send to all other validators.
        if let Err(err) = self
            .consensus_network_client
            .send_to_many(other_validators, msg)
        {
            warn!(error = ?err, "Error broadcasting message");
        }
    }
```

**File:** network/framework/src/peer_manager/mod.rs (L538-546)
```rust
        } else {
            warn!(
                NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                protocol_id = %protocol_id,
                "{} Can't send message to peer.  Peer {} is currently not connected",
                self.network_context,
                peer_id.short_str()
            );
        }
```
