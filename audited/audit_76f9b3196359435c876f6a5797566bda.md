# Audit Report

## Title
Lock Poisoning in Pipeline Execution Phase Causes Validator Node Crash

## Summary
The `aptos_infallible::Mutex` wrapper used throughout the consensus pipeline does not handle lock poisoning gracefully. If any thread panics while holding a lock on a `PipelinedBlock`'s `pipeline_tx` mutex, subsequent attempts to acquire that lock will panic due to the `.expect()` call in the lock implementation. This triggers the global panic handler which exits the entire validator node process, causing complete loss of availability. [1](#0-0) 

## Finding Description

The vulnerability stems from a design decision in the `aptos_infallible::Mutex` implementation. When acquiring a lock, if the underlying `std::sync::Mutex` is poisoned (due to a previous panic while the lock was held), the wrapper calls `.expect("Cannot currently handle a poisoned lock")` which panics instead of handling the error gracefully. [2](#0-1) 

The `pipeline_tx` field in `PipelinedBlock` uses this mutex type, and is accessed from multiple locations across the consensus pipeline:

1. **ExecutionSchedulePhase** - sends randomness data [3](#0-2) 

2. **ExecutionClient** - sends order proofs [4](#0-3) 

3. **PersistingPhase** - sends commit proofs [5](#0-4) 

4. **BlockQueue** - sends secret shared keys [6](#0-5) 

5. **RoundManager** - sends order vote notifications [7](#0-6) 

6. **PipelinedBlock::set_qc** - sends quorum certificates [8](#0-7) 

**Attack Scenario:**

1. Due to an edge case, resource exhaustion, or bug, a panic occurs while any of these code paths holds the lock on a block's `pipeline_tx` mutex
2. The mutex becomes permanently poisoned for that `PipelinedBlock` instance
3. When `ExecutionSchedulePhase::process()` subsequently attempts to lock that poisoned mutex, the `.expect()` call panics
4. The global panic handler intercepts the panic and terminates the validator node [9](#0-8) [10](#0-9) 

The panic handler is installed during validator node startup: [11](#0-10) 

The pipeline phases are spawned as tokio tasks, and while tokio catches panics, the global panic handler executes first and terminates the process: [12](#0-11) 

## Impact Explanation

**Severity: HIGH to CRITICAL**

This vulnerability causes **complete validator node crash** requiring operator intervention to restart. The impact exceeds the original question's scope of "permanently breaking execution for all future blocks" - it crashes the entire node process.

According to Aptos bug bounty criteria:
- **Critical Severity**: "Total loss of liveness/network availability" - if multiple validators are affected simultaneously
- **High Severity**: "Validator node slowdowns" and "API crashes" - single validator crash

If an attacker can trigger this condition across multiple validators (e.g., through carefully crafted transactions causing resource exhaustion or edge case panics), the network could lose liveness.

The vulnerability violates the **deterministic execution** and **availability** invariants - validators should not crash due to transient error conditions that could be handled gracefully.

## Likelihood Explanation

**Likelihood: LOW to MEDIUM**

Under normal operation with well-tested code paths, the likelihood is LOW because:
- The operations in critical sections are simple and safe
- Normal Rust panic sources (unwrap, expect, array bounds) are not present
- Channel send operations return `Result` and don't panic

However, likelihood increases due to:
- **Resource exhaustion**: Out-of-memory during `Vec` cloning could panic allocations
- **Future code changes**: Developers may add panic-inducing code in critical sections without realizing the consequence
- **Edge cases**: Unexpected conditions in dependencies or untested code paths
- **Multiple attack vectors**: 6 different lock acquisition sites create multiple potential trigger points

The defensive comment "Cannot currently handle a poisoned lock" indicates the developers are aware this is unhandled but chose to punt on proper error handling.

## Recommendation

**Immediate Fix:** Replace `aptos_infallible::Mutex` with proper error handling that recovers from poison errors.

**Option 1 - Recover from poisoned locks:**
```rust
pub fn lock(&self) -> MutexGuard<'_, T> {
    match self.0.lock() {
        Ok(guard) => guard,
        Err(poisoned) => {
            warn!("Recovering from poisoned lock");
            poisoned.into_inner()
        }
    }
}
```

**Option 2 - Graceful degradation:**
```rust
pub fn try_lock(&self) -> Option<MutexGuard<'_, T>> {
    match self.0.lock() {
        Ok(guard) => Some(guard),
        Err(e) => {
            error!("Lock poisoned, cannot acquire: {}", e);
            None
        }
    }
}
```

Then update all lock acquisition sites to handle `None`:
```rust
if let Some(guard) = b.pipeline_tx().try_lock() {
    if let Some(tx) = guard.as_mut() {
        tx.rand_tx.take().map(|tx| tx.send(b.randomness().cloned()));
    }
}
```

**Long-term Fix:** Audit all critical sections to ensure panic-free execution, add panic guards, and implement proper error propagation instead of using infallible wrappers.

## Proof of Concept

The following Rust test demonstrates the vulnerability (would need to be added to the appropriate test file):

```rust
#[test]
#[should_panic(expected = "Cannot currently handle a poisoned lock")]
fn test_lock_poisoning_crashes() {
    use std::sync::Arc;
    use std::thread;
    use aptos_infallible::Mutex;
    
    let mutex = Arc::new(Mutex::new(42));
    let mutex_clone = mutex.clone();
    
    // Thread 1: Panic while holding lock
    let handle = thread::spawn(move || {
        let _guard = mutex_clone.lock();
        panic!("Simulated panic in critical section");
    });
    
    // Wait for thread to panic
    let _ = handle.join();
    
    // Thread 2: Try to acquire poisoned lock - will panic
    let _guard = mutex.lock(); // This panics with "Cannot currently handle a poisoned lock"
}
```

To demonstrate in the actual consensus pipeline, one would need to:
1. Inject a panic in one of the 6 lock acquisition sites during block processing
2. Observe that the validator node crashes with exit code 12
3. Verify that no graceful recovery occurs

**Notes**

The vulnerability is confirmed through code analysis. While the exact trigger mechanism for an unprivileged attacker is unclear, the systemic weakness is real: the consensus pipeline has no graceful handling for lock poisoning errors, and any panic in a critical section will cascade into a node crash. This represents a robustness failure that violates availability guarantees and could potentially be exploited through resource exhaustion, malformed inputs, or edge cases in future code changes.

### Citations

**File:** crates/aptos-infallible/src/mutex.rs (L19-23)
```rust
    pub fn lock(&self) -> MutexGuard<'_, T> {
        self.0
            .lock()
            .expect("Cannot currently handle a poisoned lock")
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L214-214)
```rust
    pipeline_tx: Mutex<Option<PipelineInputTx>>,
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L342-344)
```rust
        if let Some(tx) = self.pipeline_tx().lock().as_mut() {
            tx.qc_tx.take().map(|tx| tx.send(qc));
        }
```

**File:** consensus/src/pipeline/execution_schedule_phase.rs (L65-67)
```rust
            if let Some(tx) = b.pipeline_tx().lock().as_mut() {
                tx.rand_tx.take().map(|tx| tx.send(b.randomness().cloned()));
            }
```

**File:** consensus/src/pipeline/execution_client.rs (L512-516)
```rust
        tokio::spawn(execution_schedule_phase.start());
        tokio::spawn(execution_wait_phase.start());
        tokio::spawn(signing_phase.start());
        tokio::spawn(persisting_phase.start());
        tokio::spawn(buffer_manager.start());
```

**File:** consensus/src/pipeline/execution_client.rs (L606-610)
```rust
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
                tx.order_proof_tx
                    .take()
                    .map(|tx| tx.send(ordered_proof.clone()));
            }
```

**File:** consensus/src/pipeline/persisting_phase.rs (L66-70)
```rust
            if let Some(tx) = b.pipeline_tx().lock().as_mut() {
                tx.commit_proof_tx
                    .take()
                    .map(|tx| tx.send(commit_ledger_info.clone()));
            }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L72-74)
```rust
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
                tx.secret_shared_key_tx.take().map(|tx| tx.send(Some(key)));
            }
```

**File:** consensus/src/round_manager.rs (L1682-1684)
```rust
                if let Some(tx) = proposed_block.pipeline_tx().lock().as_mut() {
                    let _ = tx.order_vote_tx.take().map(|tx| tx.send(()));
                }
```

**File:** crates/crash-handler/src/lib.rs (L26-30)
```rust
pub fn setup_panic_handler() {
    panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}
```

**File:** crates/crash-handler/src/lib.rs (L56-57)
```rust
    // Kill the process
    process::exit(12);
```

**File:** aptos-node/src/lib.rs (L234-234)
```rust
    aptos_crash_handler::setup_panic_handler();
```
