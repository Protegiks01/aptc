# Audit Report

## Title
Node-Checker Monitors Wrong Consensus Timeout Metric Leading to Health Monitoring Blindspot

## Summary
The node-checker's `ConsensusTimeoutsChecker` monitors `aptos_consensus_timeout_count` (local timeouts) instead of `aptos_consensus_timeout_rounds_count` (network-wide timeout rounds), creating a monitoring blindspot where validators can appear healthy while the consensus network experiences frequent timeouts. Additionally, implementation changes could make the metric meaningless without compile-time detection.

## Finding Description

The node-checker component is designed to assess validator health by monitoring consensus timeout behavior. However, there is a critical misalignment between what the checker monitors and what it should monitor for consensus health. [1](#0-0) 

The checker monitors the metric `aptos_consensus_timeout_count`, which is defined as: [2](#0-1) 

This metric is incremented ONLY when a specific node experiences a local timeout: [3](#0-2) 

However, there exists a DIFFERENT metric that tracks actual consensus timeout behavior: [4](#0-3) 

This second metric is incremented when rounds end with timeout certificates, regardless of whether a specific node timed out locally: [5](#0-4) 

**The Critical Gap:**

When a validator receives a Timeout Certificate (TC) from other validators and advances to the next round, its local `timeout_count` is NOT incremented, but the `timeout_rounds_count` IS incremented. This means:

1. A well-connected, fast validator might rarely experience local timeouts (low `timeout_count`)
2. But the network could be experiencing frequent timeout rounds (high `timeout_rounds_count`)
3. The node-checker would incorrectly report the validator as healthy
4. Consensus liveness issues would go undetected until critical

Furthermore, the echo timeout mechanism can inflate local timeout counts: [6](#0-5) 

When a node receives f+1 timeout votes, it calls `process_local_timeout` even though it didn't experience an actual local timeout, incrementing the metric artificially.

**Implementation Change Vulnerability:**

The TODO comment acknowledges there's no compile-time checking: [7](#0-6) 

This means:
- If consensus timeout logic is refactored to bypass `process_local_timeout`, the metric won't be incremented
- If new timeout mechanisms are added, they might not increment the metric
- If the metric is renamed or removed, the checker fails silently at runtime

## Impact Explanation

This qualifies as **High Severity** under "Significant protocol violations" because:

1. **Consensus Health Masking**: Validators experiencing consensus liveness issues (frequent timeout rounds) could appear healthy if they have fast network connectivity and rarely time out locally

2. **Delayed Incident Response**: Node operators monitoring the node-checker would not detect consensus degradation until it becomes severe enough to affect their specific node

3. **Validator Performance Misrepresentation**: Validators could claim good performance based on low local timeout counts while the network suffers from their poor performance triggering timeout rounds

4. **Silent Monitoring Failure**: Future refactoring could completely break the metric without any compile-time warning, leading to undetected consensus issues

While this doesn't directly cause funds loss or consensus safety violations, it represents a significant protocol-level monitoring failure that could mask serious consensus health issues and delay critical incident response.

## Likelihood Explanation

**Likelihood: HIGH**

This issue is occurring RIGHT NOW in production:
1. The node-checker is actively deployed and monitoring validators
2. The metric mismatch exists in the current codebase
3. Well-connected validators in networks experiencing timeouts would exhibit this behavior
4. The TODO comment indicates developers are already aware of the compile-time checking gap

The divergence between `timeout_count` and `timeout_rounds_count` naturally occurs in heterogeneous networks where some validators are faster/better-connected than others.

## Recommendation

**Immediate Fix:**

1. **Change the monitored metric** to `aptos_consensus_timeout_rounds_count`:

```rust
// ecosystem/node-checker/src/checker/consensus_timeouts.rs
const METRIC: &str = "aptos_consensus_timeout_rounds_count";
```

2. **Create a unified metrics constants crate** as suggested by the TODO:

```rust
// crates/aptos-metrics-names/src/consensus.rs
pub const TIMEOUT_COUNT: &str = "aptos_consensus_timeout_count";
pub const TIMEOUT_ROUNDS_COUNT: &str = "aptos_consensus_timeout_rounds_count";

// Update node-checker to use:
use aptos_metrics_names::consensus::TIMEOUT_ROUNDS_COUNT;
const METRIC: &str = TIMEOUT_ROUNDS_COUNT;
```

3. **Update checker description** to clarify it monitors network-wide timeout rounds:

```rust
/// The amount by which timeout ROUNDS are allowed to increase between each
/// round of metrics collection. This tracks network-wide consensus timeouts,
/// not just local node timeouts.
#[serde(default)]
pub allowed_consensus_timeout_rounds: u64,
```

**Long-term Fix:**

Implement compile-time metric name validation by generating metric constants from a single source of truth and sharing them between the consensus module and node-checker.

## Proof of Concept

**Scenario Setup:**
- 4-validator network (f=1)
- Validators A (fast), B, C, D (slower)

**Round N Timeline:**

1. Validators B, C, D timeout locally after 2 seconds
   - Their `aptos_consensus_timeout_count` increments to 1
   - Their `aptos_consensus_timeout_rounds_count` = 0 (no TC yet)

2. They broadcast timeout messages and form a TC (2f+1 = 3 validators)

3. Validator A receives the TC at 1.5 seconds (before its local timeout)
   - Advances to round N+1 via TC without local timeout
   - Its `aptos_consensus_timeout_count` stays at 0
   - Its `aptos_consensus_timeout_rounds_count` increments to 1

4. Node-checker scrapes Validator A's metrics:
   - Sees `aptos_consensus_timeout_count` = 0
   - Reports: "Consensus timeouts metric okay" with score 100
   - **FALSE NEGATIVE**: Network experienced a timeout round

**Verification Steps:**

```bash
# Deploy 4 validators with varying network latencies
# Monitor metrics on fast validator (A):
curl http://validator-a:9101/metrics | grep aptos_consensus_timeout

# Expected output showing the divergence:
# aptos_consensus_timeout_count 0
# aptos_consensus_timeout_rounds_count 15  # Network has timeout issues

# But node-checker reports:
# {"Consensus timeouts metric okay": {"score": 100}}
```

This demonstrates that the node-checker gives a false positive health score while the consensus network is experiencing significant timeout issues.

**Notes**

The vulnerability has two distinct but related aspects:

1. **Semantic Mismatch** (Active Now): The checker monitors local timeouts (`timeout_count`) when it should monitor network timeout rounds (`timeout_rounds_count`). This creates an immediate monitoring blindspot where fast validators appear healthy in unhealthy networks.

2. **No Compile-Time Safety** (Future Risk): The hardcoded string metric name has no compile-time validation, acknowledged by the TODO comment. Future refactoring could rename, remove, or bypass the metric without breaking compilation, silently breaking the checker.

Both issues stem from the fundamental problem that the node-checker is decoupled from the consensus metrics definition, with no type-safe binding between them. The correct metric to monitor for consensus health is `aptos_consensus_timeout_rounds_count`, which accurately reflects network-wide consensus timeout behavior regardless of individual validator connectivity.

### Citations

**File:** ecosystem/node-checker/src/checker/consensus_timeouts.rs (L22-25)
```rust
// TODO: When we have it, switch to using a crate that unifies metric names.
// As it is now, this metric name could change and we'd never catch it here
// at compile time.
const METRIC: &str = "aptos_consensus_timeout_count";
```

**File:** consensus/src/counters.rs (L637-644)
```rust
/// Count of the timeout rounds since last restart (close to 0 in happy path).
pub static TIMEOUT_ROUNDS_COUNT: Lazy<IntCounter> = Lazy::new(|| {
    register_int_counter!(
        "aptos_consensus_timeout_rounds_count",
        "Count of the timeout rounds since last restart (close to 0 in happy path)."
    )
    .unwrap()
});
```

**File:** consensus/src/counters.rs (L666-672)
```rust
/// Count the number of timeouts a node experienced since last restart (close to 0 in happy path).
/// This count is different from `TIMEOUT_ROUNDS_COUNT`, because not every time a node has
/// a timeout there is an ultimate decision to move to the next round (it might take multiple
/// timeouts to get the timeout certificate).
pub static TIMEOUT_COUNT: Lazy<IntCounter> = Lazy::new(|| {
    register_int_counter!("aptos_consensus_timeout_count", "Count the number of timeouts a node experienced since last restart (close to 0 in happy path).").unwrap()
});
```

**File:** consensus/src/liveness/round_state.rs (L233-241)
```rust
    pub fn process_local_timeout(&mut self, round: Round) -> bool {
        if round != self.current_round {
            return false;
        }
        warn!(round = round, "Local timeout");
        counters::TIMEOUT_COUNT.inc();
        self.setup_timeout(1);
        true
    }
```

**File:** consensus/src/round_manager.rs (L434-439)
```rust
        match new_round_event.reason {
            NewRoundReason::QCReady => {
                counters::QC_ROUNDS_COUNT.inc();
            },
            NewRoundReason::Timeout(ref reason) => {
                counters::TIMEOUT_ROUNDS_COUNT.inc();
```

**File:** consensus/src/round_manager.rs (L1821-1823)
```rust
            VoteReceptionResult::EchoTimeout(_) if !self.round_state.is_timeout_sent() => {
                self.process_local_timeout(round).await
            },
```
