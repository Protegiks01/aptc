# Audit Report

## Title
Memory Exhaustion Vulnerability in State Merkle Pruner During Shard Catch-Up

## Summary
The state merkle pruner's shard initialization logic can cause out-of-memory (OOM) crashes when catching up large version ranges. When a shard pruner initializes with a significant gap between its progress and the metadata pruner's progress, it attempts to load all stale node indices from the entire range into memory at once using an unbounded limit (`usize::MAX`), potentially allocating hundreds of gigabytes and crashing the validator.

## Finding Description
The vulnerability exists in two locations, with the shard pruner initialization being the critical issue:

**Critical Path: Shard Pruner Initialization** [1](#0-0) 

During shard pruner initialization, the catch-up call at line 53 uses `usize::MAX` as the pruning limit. This propagates to: [2](#0-1) 

The `prune()` method then invokes `get_stale_node_indices` with the full version range: [3](#0-2) 

The `get_stale_node_indices` function iterates through the database and accumulates ALL stale node indices from `start_version` to `target_version` into a Vec when the limit is `usize::MAX` (line 205).

**Attack Scenario:**

1. A validator runs for an extended period with state merkle pruning enabled and sharding
2. Millions of versions accumulate (e.g., 10,000,000 versions over weeks/months)
3. A shard pruner's progress metadata gets reset to 0 due to:
   - Database corruption recovery
   - Migration from non-sharded to sharded storage
   - Manual intervention or configuration changes
4. On validator restart, `StateMerkleShardPruner::new()` is called for each shard
5. Each shard attempts to catch up from progress=0 to metadata_progress=10,000,000
6. The call `get_stale_node_indices(&db, 0, 10,000,000, usize::MAX)` loads ALL stale nodes

**Memory Calculation:**
- Average stale nodes per version: ~500-1000 (varies with transaction complexity)
- Total stale nodes for 10M versions: 5-10 billion
- Memory per `StaleNodeIndex`: ~80 bytes (two u64 versions + NibblePath with Vec overhead)
- Total memory required: 5,000,000,000 Ã— 80 = **400 GB**

This exceeds typical validator memory (64-128 GB) and causes OOM crash.

**Less Critical Path: Metadata Pruner** [4](#0-3) 

While the metadata pruner also uses `usize::MAX` at line 57, the incremental versioning logic at lines 45-50 means `target_version_for_this_round` typically equals `current_progress`, limiting each call to process approximately one version. This makes it less exploitable but still represents poor defensive programming.

**Invariant Violated:**
- **Resource Limits** (Invariant #9): The system must respect computational and memory limits to maintain validator availability
- **State Consistency** (Invariant #4): Validator crashes during pruning can leave the system in an inconsistent state

## Impact Explanation
**High Severity** per Aptos Bug Bounty criteria:

- **Validator node crashes**: OOM condition causes immediate validator termination
- **Loss of availability**: Affected validators cannot participate in consensus
- **Repeated failure**: Validator crashes on every restart attempt until the issue is manually resolved
- **Network impact**: If multiple validators upgrade simultaneously or face the same condition, network liveness could be compromised

The vulnerability does NOT reach Critical severity because:
- It does not cause permanent network partition
- It does not result in fund loss or theft
- Recovery is possible (though requires manual intervention or code patch)
- It's configuration/state-dependent, not universally triggerable

## Likelihood Explanation
**Medium to High Likelihood:**

The vulnerability triggers when:
1. **Sharding is enabled** (increasingly common for scalability)
2. **Large version gap exists** between shard progress and metadata progress
3. **Validator restarts** or shard pruner reinitializes

Common triggering scenarios:
- **Database recovery**: After corruption or backup restoration
- **Configuration migration**: Enabling sharding on existing validator
- **Software upgrades**: That reset pruner metadata
- **Extended downtime**: Validator offline for extended period, falls behind

The issue is NOT easily exploitable by external attackers, as it requires validator operator actions or system events. However, it represents a serious operational risk that can affect validator availability during routine maintenance or recovery procedures.

## Recommendation

Implement bounded batch processing with a reasonable limit instead of loading all stale nodes at once:

```rust
// In state_merkle_shard_pruner.rs, modify the initialization catch-up:
pub(in crate::pruner) fn new(
    shard_id: usize,
    db_shard: Arc<DB>,
    metadata_progress: Version,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        &db_shard,
        &S::progress_metadata_key(Some(shard_id)),
        metadata_progress,
    )?;
    let myself = Self {
        shard_id,
        db_shard,
        _phantom: PhantomData,
    };

    info!(
        progress = progress,
        metadata_progress = metadata_progress,
        "Catching up {} shard {shard_id}.",
        S::name(),
    );
    
    // Use bounded batch size for catch-up instead of usize::MAX
    const CATCHUP_BATCH_SIZE: usize = 10_000; // ~800KB per batch at 80 bytes/node
    myself.prune(progress, metadata_progress, CATCHUP_BATCH_SIZE)?;

    Ok(myself)
}
```

Additionally, for the metadata pruner, replace `usize::MAX` with a reasonable constant:

```rust
// In state_merkle_metadata_pruner.rs:
const MAX_NODES_PER_VERSION: usize = 100_000; // Allow up to 100K nodes per version batch

let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
    &self.metadata_db,
    current_progress,
    target_version_for_this_round,
    MAX_NODES_PER_VERSION, // Instead of usize::MAX
)?;
```

**Alternative approach**: Implement streaming deletion without accumulating all indices in memory:

```rust
// Process and delete nodes incrementally without building large Vec
pub fn prune_streaming(
    state_merkle_db_shard: &DB,
    start_version: Version,
    target_version: Version,
    batch_size: usize,
) -> Result<Option<Version>> {
    let mut batch = SchemaBatch::new();
    let mut count = 0;
    let mut iter = state_merkle_db_shard.iter::<S>()?;
    iter.seek(&StaleNodeIndex {
        stale_since_version: start_version,
        node_key: NodeKey::new_empty_path(0),
    })?;

    while let Some((index, _)) = iter.next().transpose()? {
        if index.stale_since_version > target_version {
            return Ok(Some(index.stale_since_version));
        }
        
        batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
        batch.delete::<S>(&index)?;
        count += 1;
        
        if count >= batch_size {
            state_merkle_db_shard.write_schemas(batch)?;
            batch = SchemaBatch::new();
            count = 0;
        }
    }
    
    if count > 0 {
        state_merkle_db_shard.write_schemas(batch)?;
    }
    Ok(None)
}
```

## Proof of Concept

**Reproduction Steps:**

1. Set up a local Aptos validator with sharding enabled
2. Run the validator and generate significant transaction load (1M+ versions)
3. Stop the validator
4. Manually corrupt or delete the shard pruner progress metadata in RocksDB:
   ```bash
   # Delete shard progress keys from metadata DB
   # This simulates a reset scenario
   ```
5. Restart the validator while monitoring memory usage
6. Observe exponential memory growth as shard pruner attempts to load millions of stale nodes
7. Validator OOMs and crashes

**Rust Test Demonstration:**

```rust
#[test]
#[should_panic(expected = "out of memory")]
fn test_shard_catchup_oom() {
    // Setup: Create database with 1M versions of stale nodes
    let tmpdir = TempPath::new();
    let db = AptosDB::new_for_test(&tmpdir);
    
    // Simulate 1M versions with 1000 stale nodes each
    for version in 0..1_000_000 {
        let mut batch = SchemaBatch::new();
        for i in 0..1000 {
            let index = StaleNodeIndex {
                stale_since_version: version,
                node_key: NodeKey::new(version, NibblePath::new_even(vec![i as u8])),
            };
            batch.put::<StaleNodeIndexSchema>(&index, &())?;
        }
        db.commit_batch(batch)?;
    }
    
    // Trigger: Initialize shard pruner with progress=0, metadata_progress=1M
    // This will attempt to allocate ~80GB of memory for 1 billion stale nodes
    let pruner = StateMerkleShardPruner::<StaleNodeIndexSchema>::new(
        0,
        db.state_merkle_db.db_shard_arc(0),
        1_000_000, // Large gap triggers massive allocation
    );
    
    // Should panic with OOM before completing
}
```

## Notes

This vulnerability represents a critical operational risk for validator operators, particularly during:
- Database migrations and upgrades
- Disaster recovery scenarios  
- Configuration changes enabling sharding
- Extended downtime followed by restart

While not directly exploitable by external attackers, it can cause validator unavailability at critical moments, impacting network health and consensus participation. The fix is straightforward (bounded batch processing) and should be implemented with high priority to ensure validator robustness.

### Citations

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L31-56)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &S::progress_metadata_key(Some(shard_id)),
            metadata_progress,
        )?;
        let myself = Self {
            shard_id,
            db_shard,
            _phantom: PhantomData,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up {} shard {shard_id}.",
            S::name(),
        );
        myself.prune(progress, metadata_progress, usize::MAX)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L58-100)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
        max_nodes_to_prune: usize,
    ) -> Result<()> {
        loop {
            let mut batch = SchemaBatch::new();
            let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
                &self.db_shard,
                current_progress,
                target_version,
                max_nodes_to_prune,
            )?;

            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;

            let mut done = true;
            if let Some(next_version) = next_version {
                if next_version <= target_version {
                    done = false;
                }
            }

            if done {
                batch.put::<DbMetadataSchema>(
                    &S::progress_metadata_key(Some(self.shard_id)),
                    &DbMetadataValue::Version(target_version),
                )?;
            }

            self.db_shard.write_schemas(batch)?;

            if done {
                break;
            }
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs (L40-79)
```rust
    pub(in crate::pruner) fn maybe_prune_single_version(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<Option<Version>> {
        let next_version = self.next_version.load(Ordering::SeqCst);
        // This max here is only to handle the case when next version is not initialized.
        let target_version_for_this_round = max(next_version, current_progress);
        if target_version_for_this_round > target_version {
            return Ok(None);
        }

        // When next_version is not initialized, this call is used to initialize it.
        let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
            &self.metadata_db,
            current_progress,
            target_version_for_this_round,
            usize::MAX,
        )?;

        let mut batch = SchemaBatch::new();
        indices.into_iter().try_for_each(|index| {
            batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
            batch.delete::<S>(&index)
        })?;

        batch.put::<DbMetadataSchema>(
            &S::progress_metadata_key(None),
            &DbMetadataValue::Version(target_version_for_this_round),
        )?;

        self.metadata_db.write_schemas(batch)?;

        self.next_version
            // If next_version is None, meaning we've already reached the end of stale index.
            // Updating it to the target_version to make sure it's still making progress.
            .store(next_version.unwrap_or(target_version), Ordering::SeqCst);

        Ok(Some(target_version_for_this_round))
    }
```
