# Audit Report

## Title
Race Condition in Indexer Backup Metadata Updates Allows Epoch Regression

## Summary
The indexer table-info backup system contains a race condition where concurrent backup operations from multiple instances can cause the metadata epoch value to regress. The `update_metadata()` function performs unconditional writes without verifying monotonicity, allowing a slower backup of epoch N to overwrite metadata after a faster backup of epoch N+1 has completed.

## Finding Description

The vulnerability exists in the GCS backup metadata update mechanism. When multiple indexer instances are configured to backup to the same GCS bucket, they can process and backup different epochs concurrently. The race condition occurs between two critical operations:

1. **Metadata Check Phase**: [1](#0-0) 

The code reads the current metadata and checks if the epoch to backup is newer than what's already backed up. However, this check is not atomic with the subsequent update.

2. **Unconditional Write Phase**: [2](#0-1) 

The `update_metadata()` function performs a simple PUT operation to GCS without any preconditions or monotonicity checks. It blindly overwrites the existing metadata with whatever epoch value is provided.

**Attack Scenario:**
- Instance A begins backing up epoch 100 (slow backup, large dataset)
- Instance B begins backing up epoch 101 (faster backup, smaller dataset)
- Both pass the metadata check at T0 (current epoch in metadata = 99)
- Instance B completes first, updates metadata to epoch 101 at T1
- Instance A completes later, updates metadata to epoch 100 at T2
- **Result**: Metadata epoch regresses from 101 â†’ 100

The code explicitly acknowledges this is unhandled: [3](#0-2) 

And: [4](#0-3) 

The backup metadata structure itself has no built-in validation: [5](#0-4) 

## Impact Explanation

**Severity: Medium** (not Critical as suggested in the question)

This vulnerability affects the **indexer backup/restore infrastructure**, which is off-chain data infrastructure, not the consensus protocol or on-chain state. According to Aptos bug bounty criteria:

- **NOT Critical**: This does not cause loss of funds, consensus violations, network partition, liveness loss, or permanent freezing of funds. The indexer is read-only infrastructure that queries blockchain data but does not participate in consensus or affect on-chain state.

- **Qualifies as Medium**: "State inconsistencies requiring intervention" - When epoch regresses in metadata, subsequent indexer restores will fetch stale backup data (epoch 100 instead of 101), causing indexers to serve outdated table information to API clients. This requires manual operator intervention to correct the metadata.

**Actual Impact:**
- Indexers restoring from backup retrieve outdated table info snapshots
- API queries return stale/incorrect table metadata to applications
- Data inconsistency across indexer replicas
- Requires manual metadata correction by operators
- Loss of up-to-N-epochs of indexed table information requiring re-indexing

## Likelihood Explanation

**Likelihood: Medium-High** in realistic deployment scenarios:

1. **Multiple instances are feasible**: Organizations commonly run multiple fullnode replicas for redundancy and load balancing, potentially configured to backup to a shared GCS bucket for cost efficiency or centralized recovery.

2. **Large race window**: Backup operations take minutes to hours depending on database size, providing a significant window for interleaving.

3. **Different processing speeds**: Nodes with different hardware, network conditions, or starting at different sync points will naturally process epochs at different rates.

4. **No synchronization**: The code has zero distributed locking, coordination, or atomic operations to prevent this race.

The code comments confirm this is a known concern awaiting implementation.

## Recommendation

Implement atomic compare-and-swap semantics using GCS preconditions:

```rust
pub async fn update_metadata(&self, chain_id: u64, epoch: u64) -> anyhow::Result<()> {
    // First, get current metadata and its generation number
    let current = self.gcs_client.get_object(&GetObjectRequest {
        bucket: self.bucket_name.clone(),
        object: METADATA_FILE_NAME.to_string(),
        ..Default::default()
    }).await;
    
    // Check monotonicity
    if let Ok(existing) = current {
        let existing_metadata: BackupRestoreMetadata = 
            serde_json::from_slice(&existing.metadata)?;
        if existing_metadata.epoch >= epoch {
            info!("Metadata already at epoch {}, skipping update to {}", 
                  existing_metadata.epoch, epoch);
            return Ok(());
        }
        let generation = existing.generation;
        
        // Use if_generation_match precondition for atomic update
        let upload_req = UploadObjectRequest {
            bucket: self.bucket_name.clone(),
            if_generation_match: Some(generation), // Critical: atomic CAS
            ..Default::default()
        };
        
        // Attempt conditional upload
        match self.gcs_client.upload_object(
            &upload_req,
            serde_json::to_vec(&BackupRestoreMetadata::new(chain_id, epoch))?,
            &UploadType::Simple(Media {
                name: Borrowed(METADATA_FILE_NAME),
                content_type: Borrowed(JSON_FILE_TYPE),
                content_length: None,
            }),
        ).await {
            Ok(_) => return Ok(()),
            Err(Error::Response(err)) if err.code == 412 => {
                // Precondition failed - another instance updated first
                info!("Concurrent update detected, retrying validation");
                // Retry the whole operation
            }
            Err(e) => return Err(anyhow::Error::from(e)),
        }
    }
    // ... handle first-time creation case
}
```

Additionally, add validation in `BackupRestoreMetadata` to enforce monotonicity.

## Proof of Concept

```rust
// Reproduction steps:
// 1. Configure two fullnode instances with same GCS bucket
// 2. Start both instances with different sync states (one ahead)
// 3. Run concurrent backup operations

#[tokio::test]
async fn test_concurrent_epoch_regression() {
    let bucket = "test-bucket";
    let operator1 = GcsBackupRestoreOperator::new(bucket.to_string()).await;
    let operator2 = GcsBackupRestoreOperator::new(bucket.to_string()).await;
    
    // Initialize with epoch 99
    operator1.update_metadata(1, 99).await.unwrap();
    
    // Simulate concurrent backups
    let handle1 = tokio::spawn(async move {
        tokio::time::sleep(Duration::from_secs(2)).await; // Slow backup
        operator1.update_metadata(1, 100).await.unwrap();
    });
    
    let handle2 = tokio::spawn(async move {
        tokio::time::sleep(Duration::from_millis(100)).await; // Fast backup
        operator2.update_metadata(1, 101).await.unwrap();
    });
    
    let _ = tokio::join!(handle1, handle2);
    
    // Verify final state - will show epoch 100 instead of expected 101
    let final_metadata = operator1.get_metadata().await.unwrap();
    assert_eq!(final_metadata.epoch, 100); // BUG: Should be 101
}
```

## Notes

This is a **real vulnerability** affecting the indexer backup system, confirmed by explicit code comments acknowledging the lack of thread safety and concurrent backup handling. However, the severity classification in the original question as "Critical" is **incorrect** - this is properly classified as **Medium Severity** per Aptos bug bounty criteria, as it affects off-chain indexer infrastructure state consistency, not consensus, funds, or network availability.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L441-441)
```rust
    /// Not thread safe.
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L544-592)
```rust
    let backup_metadata = backup_restore_operator.get_metadata().await;
    if let Some(metadata) = backup_metadata {
        if metadata.chain_id != (ledger_chain_id as u64) {
            panic!(
                "Table Info backup chain id does not match with current network. Expected: {}, found in backup: {}",
                context.chain_id().id(),
                metadata.chain_id
            );
        }
    } else {
        aptos_logger::warn!(
            epoch = epoch,
            snapshot_folder_name = snapshot_folder_name,
            "[Table Info] No backup metadata found. Skipping the backup."
        );
    }

    let start_time = std::time::Instant::now();
    // temporary path to store the snapshot
    let snapshot_dir = context
        .node_config
        .get_data_dir()
        .join(snapshot_folder_name.clone());
    // If the backup is for old epoch, clean up and return.
    if let Some(metadata) = backup_metadata {
        aptos_logger::info!(
            epoch = epoch,
            metadata_epoch = metadata.epoch,
            snapshot_folder_name = snapshot_folder_name,
            snapshot_dir = snapshot_dir.to_str(),
            "[Table Info] Checking the metadata before backup."
        );
        if metadata.epoch >= epoch {
            aptos_logger::info!(
                epoch = epoch,
                snapshot_folder_name = snapshot_folder_name,
                "[Table Info] Snapshot already backed up. Skipping the backup."
            );
            // Remove the snapshot directory.
            std::fs::remove_dir_all(snapshot_dir).unwrap();
            return;
        }
    } else {
        aptos_logger::warn!(
            epoch = epoch,
            snapshot_folder_name = snapshot_folder_name,
            "[Table Info] No backup metadata found."
        );
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L599-599)
```rust
    // TODO: add checks to handle concurrent backup jobs.
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L124-162)
```rust
    pub async fn update_metadata(&self, chain_id: u64, epoch: u64) -> anyhow::Result<()> {
        let metadata = BackupRestoreMetadata::new(chain_id, epoch);
        loop {
            match self
                .gcs_client
                .upload_object(
                    &UploadObjectRequest {
                        bucket: self.bucket_name.clone(),
                        ..Default::default()
                    },
                    serde_json::to_vec(&metadata).unwrap(),
                    &UploadType::Simple(Media {
                        name: Borrowed(METADATA_FILE_NAME),
                        content_type: Borrowed(JSON_FILE_TYPE),
                        content_length: None,
                    }),
                )
                .await
            {
                Ok(_) => {
                    aptos_logger::info!(
                        "[Table Info] Successfully updated metadata to GCS bucket: {}",
                        METADATA_FILE_NAME
                    );
                    return Ok(());
                },
                // https://cloud.google.com/storage/quotas
                // add retry logic due to: "Maximum rate of writes to the same object name: One write per second"
                Err(Error::Response(err)) if (err.is_retriable() && err.code == 429) => {
                    info!("Retried with rateLimitExceeded on gcs single object at epoch {} when updating the metadata", epoch);
                    tokio::time::sleep(Duration::from_millis(500)).await;
                    continue;
                },
                Err(err) => {
                    anyhow::bail!("Failed to update metadata: {}", err);
                },
            }
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/mod.rs (L22-32)
```rust
#[derive(Serialize, Deserialize, Copy, Clone, Debug)]
pub struct BackupRestoreMetadata {
    pub chain_id: u64,
    pub epoch: u64,
}

impl BackupRestoreMetadata {
    pub fn new(chain_id: u64, epoch: u64) -> Self {
        Self { chain_id, epoch }
    }
}
```
