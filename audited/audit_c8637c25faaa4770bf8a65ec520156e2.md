# Audit Report

## Title
Infinite Loop in Pruner Implementations Due to Zero Batch Size Configuration

## Summary
The `StateKvPruner` and `LedgerPruner` implementations contain an infinite loop vulnerability when configured with `batch_size=0` while pruning is enabled. This causes indefinite CPU consumption in the pruner thread, degrading node performance.

## Finding Description

The vulnerability exists in the `prune()` method of both `StateKvPruner` and `LedgerPruner`. The loop advances `progress` by adding `max_versions` (derived from `batch_size` configuration) to the current progress value. When `batch_size` is 0, progress never advances, creating an infinite loop. [1](#0-0) 

The vulnerable pattern exists identically in the ledger pruner: [2](#0-1) 

**Attack Vector:**

The configuration system allows `batch_size=0` when `enable=true`. While the `NO_OP_STORAGE_PRUNER_CONFIG` sets `batch_size=0` with `enable=false` (safe), there is no validation preventing the dangerous combination of `enable=true` with `batch_size=0`: [3](#0-2) 

The `ConfigSanitizer` implementation validates prune windows and other parameters but completely omits `batch_size` validation: [4](#0-3) 

**Exploitation Path:**

1. Node operator configures pruner with `enable: true` and `batch_size: 0` in storage configuration
2. `StateKvPrunerManager` or `LedgerPrunerManager` creates a `PrunerWorker` with `batch_size=0`
3. `PrunerWorker.work()` continuously calls `pruner.prune(0)`
4. In the pruner's loop: `current_batch_target_version = min(progress + 0, target_version) = progress`
5. Progress never advances beyond its initial value
6. Loop condition `progress < target_version` remains perpetually true
7. CPU spins at 100% in the pruner thread indefinitely
8. Continuous log spam with "Pruning state kv data" or "Pruning ledger data" messages

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

**Medium Severity** - This vulnerability causes resource exhaustion but with limited scope:

- **CPU Exhaustion**: The pruner thread consumes 100% CPU indefinitely, impacting node performance
- **Log Spam**: Continuous logging degrades disk I/O and fills storage
- **Pruner Unavailability**: Storage pruning functionality becomes non-functional
- **Node Degradation**: Overall node performance suffers, but consensus and execution continue
- **No Consensus Impact**: The main validator threads (consensus, execution, state sync) remain operational
- **No Funds at Risk**: Does not affect transaction processing or fund security
- **Requires Configuration Error**: Needs misconfiguration, not exploitable via transaction

The impact aligns with **Medium Severity** per Aptos bug bounty: "State inconsistencies requiring intervention" and validator node performance degradation (though not qualifying for High as it doesn't cause crashes).

## Likelihood Explanation

**Low to Medium Likelihood**:

- **Requires Misconfiguration**: Node operator must explicitly set `batch_size=0` with `enable=true`
- **No Default Risk**: Default configurations use safe values (5000 for ledger pruner, 1000 for state merkle)
- **Operational Error**: Could occur during troubleshooting, testing, or copy-paste configuration errors
- **No External Trigger**: Cannot be triggered by external attackers or malicious transactions
- **Detection**: Would be noticed quickly due to CPU spikes and log spam
- **Recovery**: Simple restart with corrected configuration resolves the issue

The lack of validation makes this an operational hazard rather than a direct attack vector.

## Recommendation

Implement defense-in-depth validation at both configuration and runtime levels:

**1. Configuration Validation** - Add to `ConfigSanitizer::sanitize()`:

```rust
// In storage/config_config.rs ConfigSanitizer implementation
if config.storage_pruner_config.ledger_pruner_config.enable
    && config.storage_pruner_config.ledger_pruner_config.batch_size == 0
{
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "ledger_pruner_config.batch_size must be greater than 0 when pruning is enabled.".to_string(),
    ));
}

if config.storage_pruner_config.state_merkle_pruner_config.enable
    && config.storage_pruner_config.state_merkle_pruner_config.batch_size == 0
{
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "state_merkle_pruner_config.batch_size must be greater than 0 when pruning is enabled.".to_string(),
    ));
}
```

**2. Runtime Guard** - Add early return in `prune()` methods:

```rust
fn prune(&self, max_versions: usize) -> Result<Version> {
    if max_versions == 0 {
        return Ok(self.progress());
    }
    
    let mut progress = self.progress();
    let target_version = self.target_version();
    // ... rest of implementation
}
```

**3. Minimum Value Enforcement** - Set reasonable minimums in defaults:

```rust
pub const MIN_PRUNER_BATCH_SIZE: usize = 1;
```

This three-layer approach ensures the vulnerability cannot be triggered through configuration error, runtime edge cases, or future code changes.

## Proof of Concept

```rust
#[test]
fn test_infinite_loop_with_zero_batch_size() {
    use std::sync::Arc;
    use std::time::Duration;
    use aptos_temppath::TempPath;
    use crate::pruner::state_kv_pruner::StateKvPruner;
    use crate::state_kv_db::StateKvDb;
    
    // Setup test database
    let tmpdir = TempPath::new();
    let state_kv_db = Arc::new(StateKvDb::new(&tmpdir, false, false, 1).unwrap());
    
    // Create pruner
    let pruner = StateKvPruner::new(state_kv_db).unwrap();
    
    // Set target ahead of progress to trigger loop
    pruner.set_target_version(100);
    assert_eq!(pruner.progress(), 0);
    assert_eq!(pruner.target_version(), 100);
    
    // Call prune with batch_size=0 in a timeout wrapper
    let result = std::thread::spawn(move || {
        pruner.prune(0) // This will hang indefinitely
    });
    
    // Verify it doesn't complete within reasonable time
    let timeout_result = result.join_timeout(Duration::from_secs(2));
    assert!(timeout_result.is_err(), "prune(0) should hang indefinitely but completed");
    
    // Expected: Test times out or fails assertion
    // Actual: Infinite loop consumes CPU until timeout
}
```

To reproduce manually:
1. Edit node configuration file (e.g., `fullnode.yaml`)
2. Set: `storage.storage_pruner_config.ledger_pruner_config.batch_size: 0` with `enable: true`
3. Start node: observe immediate 100% CPU usage in pruner thread
4. Check logs: continuous "Pruning ledger data" messages
5. Monitor: `top` shows high CPU, progress never advances

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L49-86)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_pruner__prune"]);

        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning state kv data."
            );
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning state kv data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** config/src/config/storage_config.rs (L306-323)
```rust
pub const NO_OP_STORAGE_PRUNER_CONFIG: PrunerConfig = PrunerConfig {
    ledger_pruner_config: LedgerPrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
        user_pruning_window_offset: 0,
    },
    state_merkle_pruner_config: StateMerklePrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
    },
    epoch_snapshot_pruner_config: EpochSnapshotPrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
    },
};
```

**File:** config/src/config/storage_config.rs (L682-728)
```rust
impl ConfigSanitizer for StorageConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.storage;

        let ledger_prune_window = config
            .storage_pruner_config
            .ledger_pruner_config
            .prune_window;
        let state_merkle_prune_window = config
            .storage_pruner_config
            .state_merkle_pruner_config
            .prune_window;
        let epoch_snapshot_prune_window = config
            .storage_pruner_config
            .epoch_snapshot_pruner_config
            .prune_window;
        let user_pruning_window_offset = config
            .storage_pruner_config
            .ledger_pruner_config
            .user_pruning_window_offset;

        if ledger_prune_window < 50_000_000 {
            warn!("Ledger prune_window is too small, harming network data availability.");
        }
        if state_merkle_prune_window < 100_000 {
            warn!("State Merkle prune_window is too small, node might stop functioning.");
        }
        if epoch_snapshot_prune_window < 50_000_000 {
            warn!("Epoch snapshot prune_window is too small, harming network data availability.");
        }
        if user_pruning_window_offset > 1_000_000 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset too large, so big a buffer is unlikely necessary. Set something < 1 million.".to_string(),
            ));
        }
        if user_pruning_window_offset > ledger_prune_window {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset is larger than the ledger prune window, the API will refuse to return any data.".to_string(),
            ));
        }
```
