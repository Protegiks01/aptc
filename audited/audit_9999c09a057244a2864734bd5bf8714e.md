# Audit Report

## Title
Indexer Cache Worker Fast-Fail Loop Without Backoff Leading to Resource Exhaustion

## Summary
The `Worker::run()` function in the indexer-grpc-cache-worker contains an error handling flaw where streaming errors cause `process_streaming_response` to return `Ok(())` instead of `Err()`, resulting in immediate retry without backoff. This creates a fast-fail loop that consumes excessive CPU, generates log spam, and creates unnecessary network traffic when persistent error conditions occur.

## Finding Description
The vulnerability exists in the interaction between `Worker::run()` and `process_streaming_response()`. When the gRPC stream from the fullnode encounters errors, the code breaks from the inner processing loop but returns `Ok(())`, signaling success to the outer loop. [1](#0-0) 

The outer reconnection loop at line 111 contains no backoff mechanism. When `process_streaming_response` is called at line 167-173, it uses the `?` operator which only propagates errors. If the function returns `Ok(())`, the loop immediately continues to reconnect.

Within `process_streaming_response`, multiple error conditions break the inner loop but return `Ok(())`: [2](#0-1) [3](#0-2) [4](#0-3) [5](#0-4) 

All these error cases exit the loop and fall through to return `Ok(())` at line 504. While `create_grpc_client` has exponential backoff for connection failures, this doesn't help when the connection succeeds but the stream produces errors. [6](#0-5) 

**Attack Scenario:**
1. A malicious or malfunctioning fullnode establishes a gRPC connection successfully
2. The fullnode sends malformed data or invalid chain IDs during streaming
3. `process_streaming_response` catches the error, breaks the loop, and returns `Ok(())`
4. `Worker::run()` immediately reconnects and retries
5. If the fullnode condition persists, this creates a tight loop (limited only by connection establishment time)
6. The cache worker consumes high CPU processing rapid connection attempts
7. Logs fill with error messages at high frequency
8. Network bandwidth is wasted on repeated connection attempts

## Impact Explanation
**This issue does NOT meet the stated Medium severity criteria** for the Aptos bug bounty program. The described Medium severity requires "Limited funds loss or manipulation" or "State inconsistencies requiring intervention."

This vulnerability:
- **Does NOT affect blockchain consensus** - The indexer is an auxiliary off-chain service
- **Does NOT affect state consistency** - It only impacts the Redis cache population
- **Does NOT cause fund loss** - No blockchain assets are at risk
- **Does NOT break any of the 10 critical invariants** listed in the audit scope

The actual impact is limited to:
- High CPU usage on the cache worker process
- Log spam consuming disk space
- Network bandwidth waste
- Potential Redis connection exhaustion
- Degraded indexer query performance (stale cache data)

While these are operational concerns, they do not constitute a blockchain security vulnerability under the stated criteria. The core blockchain operation remains unaffected even if the entire indexer infrastructure fails.

## Likelihood Explanation
The likelihood is **MODERATE** in production environments:
- Network instability can naturally trigger this condition
- A misconfigured or buggy fullnode can produce malformed data
- Chain ID mismatches during upgrades could trigger it
- The condition persists until manual intervention or the underlying issue resolves

However, the severity of impact is low because it only affects an auxiliary service.

## Recommendation
Implement exponential backoff in the outer reconnection loop:

```rust
pub async fn run(&mut self) -> Result<()> {
    let mut backoff = backoff::ExponentialBackoff::default();
    
    loop {
        let conn = self.redis_client.get_tokio_connection_manager().await
            .context("Get redis connection failed.")?;
        let mut rpc_client = create_grpc_client(self.fullnode_grpc_address.clone()).await;
        
        // ... existing setup code ...
        
        match process_streaming_response(
            conn,
            self.cache_storage_format,
            file_store_metadata,
            response.into_inner(),
        ).await {
            Ok(_) => {
                // Normal stream end - reset backoff
                backoff.reset();
                info!(service_type = SERVICE_TYPE, "[Indexer Cache] Streaming RPC ended normally.");
            },
            Err(e) => {
                // Error occurred - apply backoff before retry
                error!(service_type = SERVICE_TYPE, "[Indexer Cache] Streaming error: {}", e);
                if let Some(duration) = backoff.next_backoff() {
                    tokio::time::sleep(duration).await;
                } else {
                    bail!("Max retries exceeded");
                }
            }
        }
    }
}
```

Additionally, change `process_streaming_response` to return `Err()` for error conditions instead of silently returning `Ok(())` after breaking.

## Proof of Concept
This issue cannot be demonstrated with a standard Move test as it involves gRPC streaming infrastructure. A reproduction would require:

1. Deploy the indexer-grpc-cache-worker
2. Configure it to connect to a test fullnode
3. Modify the fullnode to send invalid data or simulate network errors
4. Observe rapid retry attempts without backoff in logs and CPU metrics
5. Measure CPU usage increasing to near 100% on a single core
6. Observe log files growing rapidly with error messages

**Note:** This is an operational/reliability issue in auxiliary infrastructure rather than a core blockchain security vulnerability. While the technical finding is valid, it does not meet the severity threshold defined in the Aptos bug bounty program criteria focusing on consensus, execution, state management, and fund security.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L109-180)
```rust
    pub async fn run(&mut self) -> Result<()> {
        // Re-connect if lost.
        loop {
            let conn = self
                .redis_client
                .get_tokio_connection_manager()
                .await
                .context("Get redis connection failed.")?;
            let mut rpc_client = create_grpc_client(self.fullnode_grpc_address.clone()).await;

            // 1. Fetch metadata.
            let file_store_operator: Box<dyn FileStoreOperator> = self.file_store.create();
            // TODO: move chain id check somewhere around here
            // This ensures that metadata is created before we start the cache worker
            let mut starting_version = file_store_operator.get_latest_version().await;
            while starting_version.is_none() {
                starting_version = file_store_operator.get_latest_version().await;
                tracing::warn!(
                    "[Indexer Cache] File store metadata not found. Waiting for {} ms.",
                    FILE_STORE_METADATA_WAIT_MS
                );
                tokio::time::sleep(std::time::Duration::from_millis(
                    FILE_STORE_METADATA_WAIT_MS,
                ))
                .await;
            }

            // There's a guarantee at this point that starting_version is not null
            let starting_version = starting_version.unwrap();

            let file_store_metadata = file_store_operator.get_file_store_metadata().await.unwrap();

            tracing::info!(
                service_type = SERVICE_TYPE,
                "[Indexer Cache] Starting cache worker with version {}",
                starting_version
            );

            // 2. Start streaming RPC.
            let request = tonic::Request::new(GetTransactionsFromNodeRequest {
                starting_version: Some(starting_version),
                ..Default::default()
            });

            let response = rpc_client
                .get_transactions_from_node(request)
                .await
                .with_context(|| {
                    format!(
                        "Failed to get transactions from node at starting version {}",
                        starting_version
                    )
                })?;
            info!(
                service_type = SERVICE_TYPE,
                "[Indexer Cache] Streaming RPC started."
            );
            // 3&4. Infinite streaming until error happens. Either stream ends or worker crashes.
            process_streaming_response(
                conn,
                self.cache_storage_format,
                file_store_metadata,
                response.into_inner(),
            )
            .await?;

            info!(
                service_type = SERVICE_TYPE,
                "[Indexer Cache] Streaming RPC ended."
            );
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L356-380)
```rust
    loop {
        let download_start_time = std::time::Instant::now();
        let received = match resp_stream.next().await {
            Some(r) => r,
            _ => {
                error!(
                    service_type = SERVICE_TYPE,
                    "[Indexer Cache] Streaming error: no response."
                );
                ERROR_COUNT.with_label_values(&["streaming_error"]).inc();
                break;
            },
        };
        // 10 batches doewnload + slowest processing& uploading task
        let received: TransactionsFromNodeResponse = match received {
            Ok(r) => r,
            Err(err) => {
                error!(
                    service_type = SERVICE_TYPE,
                    "[Indexer Cache] Streaming error: {}", err
                );
                ERROR_COUNT.with_label_values(&["streaming_error"]).inc();
                break;
            },
        };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L405-443)
```rust
                GrpcDataStatus::StreamInit(new_version) => {
                    error!(
                        current_version = new_version,
                        "[Indexer Cache] Init signal received twice."
                    );
                    ERROR_COUNT.with_label_values(&["data_init_twice"]).inc();
                    break;
                },
                GrpcDataStatus::BatchEnd {
                    start_version,
                    num_of_transactions,
                } => {
                    // Handle the data multithreading.
                    let result = join_all(tasks_to_run).await;
                    if result
                        .iter()
                        .any(|r| r.is_err() || r.as_ref().unwrap().is_err())
                    {
                        error!(
                            start_version = start_version,
                            num_of_transactions = num_of_transactions,
                            "[Indexer Cache] Process transactions from fullnode failed."
                        );
                        ERROR_COUNT.with_label_values(&["response_error"]).inc();
                        panic!("Error happens when processing transactions from fullnode.");
                    }
                    // Cleanup.
                    tasks_to_run = vec![];
                    if current_version != start_version + num_of_transactions {
                        error!(
                            current_version = current_version,
                            actual_current_version = start_version + num_of_transactions,
                            "[Indexer Cache] End signal received with wrong version."
                        );
                        ERROR_COUNT
                            .with_label_values(&["data_end_wrong_version"])
                            .inc();
                        break;
                    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L465-475)
```rust
            Err(e) => {
                error!(
                    start_version = current_version,
                    chain_id = fullnode_chain_id,
                    service_type = SERVICE_TYPE,
                    "[Indexer Cache] Process transactions from fullnode failed: {}",
                    e
                );
                ERROR_COUNT.with_label_values(&["response_error"]).inc();
                break;
            },
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L500-505)
```rust
    }

    // It is expected that we get to this point, the upstream server disconnects
    // clients after 5 minutes.
    Ok(())
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/lib.rs (L35-63)
```rust
/// Create a gRPC client with exponential backoff.
pub async fn create_grpc_client(address: Url) -> GrpcClientType {
    backoff::future::retry(backoff::ExponentialBackoff::default(), || async {
        match FullnodeDataClient::connect(address.to_string()).await {
            Ok(client) => {
                tracing::info!(
                    address = address.to_string(),
                    "[Indexer Cache] Connected to indexer gRPC server."
                );
                Ok(client
                    .max_decoding_message_size(usize::MAX)
                    .max_encoding_message_size(usize::MAX)
                    .send_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Gzip)
                    .accept_compressed(CompressionEncoding::Zstd))
            },
            Err(e) => {
                tracing::error!(
                    address = address.to_string(),
                    "[Indexer Cache] Failed to connect to indexer gRPC server: {}",
                    e
                );
                Err(backoff::Error::transient(e))
            },
        }
    })
    .await
    .unwrap()
}
```
