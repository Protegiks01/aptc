# Audit Report

After thorough analysis of the consensus persistent storage layer and related components, I have identified a critical error handling issue that matches the security question's scope.

## Title
Silent Database Pruning Failures in Consensus Commit Callback Leading to State Divergence

## Summary
The `commit_callback` method in the consensus block tree silently ignores database errors when pruning old blocks, allowing in-memory consensus state to diverge from persistent storage. This violates the fundamental invariant that persistent storage should be the authoritative source of truth for consensus state. [1](#0-0) 

## Finding Description
During normal consensus operation, when blocks are finalized and committed, the `commit_callback` method is invoked to update consensus state and prune old blocks from persistent storage. The pruning operation calls `storage.prune_tree()`, which can fail due to database errors (disk full, I/O errors, corruption, permissions issues).

The vulnerability lies in the error handling pattern at the cited location. When `storage.prune_tree()` returns an error, the code:
1. Catches the error with `if let Err(e)`
2. Only logs it with `warn!` 
3. Continues execution to update in-memory state via `process_pruned_blocks()`, `update_window_root()`, and `update_highest_commit_cert()`

This creates a divergence where:
- The in-memory block tree believes blocks have been pruned and updates its root accordingly
- The persistent database still contains the blocks that should have been deleted
- The node continues operating with inconsistent state between memory and disk

The issue propagates through the commit pipeline, which is invoked from block insertion: [2](#0-1) 

In contrast, all other storage operations properly propagate errors: [3](#0-2) [4](#0-3) [5](#0-4) 

## Impact Explanation
This qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Protocol Violation**: Violates the fundamental "State Consistency" invariant that state transitions must be atomic and verifiable. The in-memory and persistent states become desynchronized.

2. **Node Availability Impact**: When the node restarts, the recovery process attempts to prune the dangling blocks with strict error handling: [6](#0-5) 

If the database error persists (disk corruption, permissions), the restart will panic, causing validator node unavailability.

3. **Cascading Failures**: If the root cause is disk space exhaustion, the accumulated unpruned blocks exacerbate the problem, and other critical operations (`save_tree`, `save_vote`) will eventually fail, halting consensus.

4. **Validator Network Degradation**: Multiple validators experiencing similar database issues could lead to reduced network capacity or liveness problems affecting the entire network.

## Likelihood Explanation
**High Likelihood** in production environments:

1. **Common Trigger Conditions**: Database errors are realistic in production:
   - Disk space exhaustion from log accumulation or state growth
   - I/O errors from hardware degradation
   - Filesystem permission issues during deployment
   - Database corruption from crashes or power failures

2. **Persistent Errors**: Many database error conditions are persistent (won't self-resolve), meaning every commit will fail to prune, accumulating the divergence over time.

3. **Silent Failure Mode**: The `warn!` log may be missed in normal operations, allowing the issue to accumulate undetected until a restart triggers failure.

## Recommendation

Replace the silent error handling with proper error propagation. Change the function signature to return `Result<()>` and propagate the error:

```rust
pub fn commit_callback(
    &mut self,
    storage: Arc<dyn PersistentLivenessStorage>,
    block_id: HashValue,
    block_round: Round,
    finality_proof: WrappedLedgerInfo,
    commit_decision: LedgerInfoWithSignatures,
    window_size: Option<u64>,
) -> anyhow::Result<()> {
    let current_round = self.commit_root().round();
    let committed_round = block_round;
    let commit_proof = finality_proof
        .create_merged_with_executed_state(commit_decision)
        .expect("Inconsistent commit proof and evaluation decision, cannot commit block");

    debug!(
        LogSchema::new(LogEvent::CommitViaBlock).round(current_round),
        committed_round = committed_round,
        block_id = block_id,
    );

    let window_root_id = self.find_window_root(block_id, window_size);
    let ids_to_remove = self.find_blocks_to_prune(window_root_id);

    // Propagate pruning errors - do not update in-memory state if persistence fails
    storage.prune_tree(ids_to_remove.clone().into_iter().collect())
        .context("Failed to prune blocks from persistent storage")?;
    
    self.process_pruned_blocks(ids_to_remove);
    self.update_window_root(window_root_id);
    self.update_highest_commit_cert(commit_proof);
    Ok(())
}
```

Update the callback invocation site to handle the error appropriately (e.g., halt consensus or retry with backoff).

## Proof of Concept

```rust
#[cfg(test)]
mod test_silent_prune_failure {
    use super::*;
    use std::sync::atomic::{AtomicBool, Ordering};
    
    struct FailingStorage {
        fail_prune: AtomicBool,
        prune_called: AtomicBool,
    }
    
    impl PersistentLivenessStorage for FailingStorage {
        fn prune_tree(&self, _block_ids: Vec<HashValue>) -> Result<()> {
            self.prune_called.store(true, Ordering::SeqCst);
            if self.fail_prune.load(Ordering::SeqCst) {
                Err(anyhow::anyhow!("Database error: disk full"))
            } else {
                Ok(())
            }
        }
        // ... other required trait methods ...
    }
    
    #[test]
    fn test_prune_failure_updates_memory_state() {
        let storage = Arc::new(FailingStorage {
            fail_prune: AtomicBool::new(true),
            prune_called: AtomicBool::new(false),
        });
        
        let mut block_tree = /* initialize block tree */;
        let initial_window_root = block_tree.window_root_id;
        
        // Trigger commit callback with failing storage
        block_tree.commit_callback(
            storage.clone(),
            new_block_id,
            new_round,
            finality_proof,
            commit_decision,
            window_size,
        );
        
        // Verify prune was attempted
        assert!(storage.prune_called.load(Ordering::SeqCst));
        
        // VULNERABILITY: In-memory state was updated despite prune failure
        assert_ne!(block_tree.window_root_id, initial_window_root);
        // But the database still contains the old blocks!
    }
}
```

## Notes

The vulnerability is particularly insidious because:
1. The inline comment suggests this behavior is intentional ("it's fine to fail here")
2. However, this violates consensus safety principles - persistent storage operations must succeed before updating consensus state
3. The error handling is inconsistent with other storage operations in the same codebase
4. The "recovery on restart" assumption fails if the database error is persistent

### Citations

**File:** consensus/src/block_storage/block_tree.rs (L591-596)
```rust
        if let Err(e) = storage.prune_tree(ids_to_remove.clone().into_iter().collect()) {
            // it's fine to fail here, as long as the commit succeeds, the next restart will clean
            // up dangling blocks, and we need to prune the tree to keep the root consistent with
            // executor.
            warn!(error = ?e, "fail to delete block");
        }
```

**File:** consensus/src/block_storage/block_store.rs (L475-489)
```rust
            let callback = Box::new(
                move |finality_proof: WrappedLedgerInfo,
                      commit_decision: LedgerInfoWithSignatures| {
                    if let Some(tree) = block_tree.upgrade() {
                        tree.write().commit_callback(
                            storage,
                            id,
                            round,
                            finality_proof,
                            commit_decision,
                            window_size,
                        );
                    }
                },
            );
```

**File:** consensus/src/block_storage/block_store.rs (L512-514)
```rust
        self.storage
            .save_tree(vec![pipelined_block.block().clone()], vec![])
            .context("Insert block failed when saving block")?;
```

**File:** consensus/src/block_storage/block_store.rs (L552-554)
```rust
        self.storage
            .save_tree(vec![], vec![qc.clone()])
            .context("Insert block failed when saving quorum")?;
```

**File:** consensus/src/block_storage/block_store.rs (L570-572)
```rust
        self.storage
            .save_highest_2chain_timeout_cert(tc.as_ref())
            .context("Timeout certificate insert failed when persisting to DB")?;
```

**File:** consensus/src/persistent_liveness_storage.rs (L570-572)
```rust
                (self as &dyn PersistentLivenessStorage)
                    .prune_tree(initial_data.take_blocks_to_prune())
                    .expect("unable to prune dangling blocks during restart");
```
