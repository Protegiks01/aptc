# Audit Report

## Title
HashMap Capacity Growth Without Shrinkage in PerKeyQueue Causes Permanent Memory Bloat Under Peer Churn

## Summary
The `remove_empty_queues()` function in `message_queues.rs` uses `HashMap::retain()` to remove empty per-key queues, but this does not shrink the HashMap's internal capacity. Under sustained peer connection churn on public-facing nodes, the HashMap capacity can grow to accommodate peak key counts and never reclaim that memory, leading to permanent memory bloat of tens to hundreds of megabytes across multiple channel instances.

## Finding Description
The `PerKeyQueue` structure maintains a `HashMap<K, VecDeque<T>>` to store per-key message queues. [1](#0-0) 

When queues become empty, the garbage collection mechanism calls `remove_empty_queues()`: [2](#0-1) 

The `HashMap::retain()` method removes entries from the map but does **not** shrink the HashMap's internal capacity. In Rust, a HashMap allocates capacity in powers of 2, and this capacity persists even after entries are removed.

**Attack Scenario:**

1. **Public-facing VFN nodes** accept connections from unknown peers up to `MAX_INBOUND_CONNECTIONS` (default 100) [3](#0-2) 

2. Channels use keys of type `(PeerId, ProtocolId)`, where `PeerId` is derived from the peer's x25519 public key and there are 29 protocol types. [4](#0-3) 

3. An attacker can:
   - Generate unique x25519 keypairs (cheap, <1ms each)
   - Connect with unique PeerIds (limited to 100 concurrent)
   - Send messages across multiple protocols, creating `(PeerId, ProtocolId)` keys
   - Disconnect and repeat with new PeerIds

4. **If message processing is slower than connection churn** (e.g., under system load), messages accumulate in queues and keys persist until queues are drained.

5. Over many cycles, the HashMap observes thousands of unique keys, growing capacity to accommodate peak load (e.g., 131,072 capacity for ~100K keys seen).

6. When queues empty and GC runs every 50 pops, entries are removed but capacity remains: [5](#0-4) 

7. **Memory overhead**: For capacity C, empty HashMap consumes ~8 bytes per slot. A capacity of 131,072 = ~1 MB per HashMap instance.

8. **Multiple affected channels**: The codebase creates numerous `aptos_channel` instances for peer manager requests, connection requests, upstream handlers, consensus, state sync, and mempool. [6](#0-5) 

9. **Cumulative impact**: With 10-50+ affected channel instances, total unreclaimable capacity overhead can reach 50-200+ MB.

The developers were aware of the underlying transient peer memory leak issue and added GC to address it: [7](#0-6) 

However, the GC implementation does not address capacity shrinkage, leaving a residual memory bloat vulnerability.

## Impact Explanation
This qualifies as **Medium Severity** under the Aptos bug bounty program because it represents a resource exhaustion vulnerability that:

1. **Causes gradual memory bloat** that accumulates over time and is never reclaimed
2. **Affects node availability** by consuming memory resources that could impact node performance under sustained attack
3. **Requires operational intervention** to remediate (node restart to clear bloated HashMaps)
4. **Exploitable without privileged access** by any external peer connecting to public-facing nodes

While not causing immediate critical failure, sustained exploitation over weeks/months could:
- Accumulate hundreds of MB of wasted memory
- Reduce available memory for legitimate operations
- Contribute to node performance degradation
- Force operators to restart nodes to reclaim memory

This falls under "state inconsistencies requiring intervention" per Medium severity criteria, as the memory state becomes inefficient and requires node restart to reset.

## Likelihood Explanation
**Likelihood: Medium-High**

The attack is feasible because:

1. **Low attacker cost**: Generating unique x25519 keypairs and establishing connections is computationally cheap
2. **No authentication required**: Public-facing VFN nodes accept unknown peers up to connection limits
3. **Natural trigger conditions**: The vulnerability is exacerbated during periods of legitimate high load when message processing slows
4. **Persistent impact**: Once HashMap capacity grows, it never shrinks, so even temporary attack periods have permanent effects
5. **Multiple attack targets**: All public-facing nodes (VFNs serving public clients) are vulnerable

However, the attack requires:
- Sustained connection churn over extended periods
- System conditions where message processing lags behind connection rate
- Access to multiple source IPs to bypass rate limiting

## Recommendation

Add periodic HashMap capacity shrinking to the garbage collection mechanism:

```rust
fn remove_empty_queues(&mut self) {
    self.per_key_queue.retain(|_key, queue| !queue.is_empty());
    
    // Shrink HashMap capacity if it's significantly larger than needed
    // Only shrink if capacity is at least 4x larger than current size
    // to avoid frequent reallocations
    if self.per_key_queue.capacity() > self.per_key_queue.len() * 4 
        && self.per_key_queue.capacity() > 1024 {
        self.per_key_queue.shrink_to_fit();
    }
}
```

Alternatively, use a more aggressive shrinking strategy:

```rust
fn remove_empty_queues(&mut self) {
    self.per_key_queue.retain(|_key, queue| !queue.is_empty());
    
    // Shrink to a reasonable capacity based on current size
    let target_capacity = (self.per_key_queue.len() * 2).max(128);
    if self.per_key_queue.capacity() > target_capacity {
        self.per_key_queue.shrink_to(target_capacity);
    }
}
```

The heuristic should balance between:
- Avoiding frequent reallocations (keep some headroom)
- Reclaiming memory from past peak loads
- Considering the typical peer set size for the node type

## Proof of Concept

```rust
#[cfg(test)]
mod capacity_bloat_test {
    use super::*;
    use std::num::NonZeroUsize;
    
    #[test]
    fn test_hashmap_capacity_not_shrunk() {
        let mut queue = PerKeyQueue::<u64, String>::new(
            QueueStyle::FIFO,
            NonZeroUsize::new(10).unwrap(),
            None,
        );
        
        // Simulate high peer churn: add 10,000 unique keys
        for i in 0..10000 {
            queue.push(i, format!("message_{}", i));
        }
        
        // Pop all messages to empty queues
        while queue.pop().is_some() {
            // Process messages
        }
        
        // Record capacity after GC
        let capacity_after_gc = queue.per_key_queue.capacity();
        
        // Capacity should be large (power of 2 >= 10000)
        assert!(capacity_after_gc >= 16384); // 2^14 = 16384
        
        // Add just 10 new keys (normal operation)
        for i in 20000..20010 {
            queue.push(i, format!("message_{}", i));
        }
        
        // Capacity remains inflated, not shrunk to fit 10 keys
        assert_eq!(queue.per_key_queue.capacity(), capacity_after_gc);
        
        // Memory bloat: HashMap maintains capacity for 16K+ entries
        // but only holds 10 active keys
        println!("Capacity: {}, Active keys: {}", 
                 capacity_after_gc, 
                 queue.per_key_queue.len());
    }
}
```

**Notes**

This vulnerability represents a **resource management flaw** rather than a direct protocol violation. While the immediate per-instance impact is moderate (1-8 MB), the cumulative effect across multiple channel instances in a long-running public-facing node can accumulate to significant memory waste. The issue is exacerbated by the fact that validator and VFN nodes are expected to run continuously without restarts, making any permanent memory bloat particularly concerning for operational stability.

The existing GC mechanism successfully prevents the original OOM issue documented in GitHub #5543, but the incomplete fix leaves residual memory bloat that, while not catastrophic, violates resource efficiency expectations and could contribute to degraded performance under sustained peer churn patterns common in public blockchain networks.

### Citations

**File:** crates/channel/src/message_queues.rs (L45-63)
```rust
pub(crate) struct PerKeyQueue<K: Eq + Hash + Clone, T> {
    /// QueueStyle for the messages stored per key
    queue_style: QueueStyle,
    /// per_key_queue maintains a map from a Key to a queue
    /// of all the messages from that Key. A Key is usually
    /// represented by AccountAddress
    per_key_queue: HashMap<K, VecDeque<T>>,
    /// This is a (round-robin)queue of Keys which have pending messages
    /// This queue will be used for performing round robin among
    /// Keys for choosing the next message
    round_robin_queue: VecDeque<K>,
    /// Maximum number of messages to store per key
    max_queue_size: NonZeroUsize,
    /// Number of messages dequeued since last GC
    num_popped_since_gc: u32,
    /// Optional counters for recording # enqueued, # dequeued, and # dropped
    /// messages
    counters: Option<&'static IntCounterVec>,
}
```

**File:** crates/channel/src/message_queues.rs (L177-192)
```rust
            // aptos-channel never removes keys from its PerKeyQueue (without
            // this logic). This works fine for the validator network, where we
            // have a bounded set of peers that almost never changes; however,
            // this does not work for servicing public clients, where we can have
            // large and frequent connection churn.
            //
            // Periodically removing these empty queues prevents us from causing
            // an effective memory leak when we have lots of transient peers in
            // e.g. the public-facing vfn use-case.
            //
            // This GC strategy could probably be more sophisticated, though it
            // seems to work well in some basic stress tests / micro benches.
            //
            // See: common/channel/src/bin/many_keys_stress_test.rs
            //
            // For more context, see: https://github.com/aptos-labs/aptos-core/issues/5543
```

**File:** crates/channel/src/message_queues.rs (L193-197)
```rust
            self.num_popped_since_gc += 1;
            if self.num_popped_since_gc >= POPS_PER_GC {
                self.num_popped_since_gc = 0;
                self.remove_empty_queues();
            }
```

**File:** crates/channel/src/message_queues.rs (L203-206)
```rust
    /// Garbage collect any empty per-key-queues.
    fn remove_empty_queues(&mut self) {
        self.per_key_queue.retain(|_key, queue| !queue.is_empty());
    }
```

**File:** config/src/config/network_config.rs (L44-44)
```rust
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
```

**File:** network/framework/src/peer_manager/builder.rs (L65-68)
```rust
    pm_reqs_tx: aptos_channel::Sender<(PeerId, ProtocolId), PeerManagerRequest>,
    pm_reqs_rx: aptos_channel::Receiver<(PeerId, ProtocolId), PeerManagerRequest>,
    connection_reqs_tx: aptos_channel::Sender<PeerId, ConnectionRequest>,
    connection_reqs_rx: aptos_channel::Receiver<PeerId, ConnectionRequest>,
```

**File:** network/framework/src/peer_manager/builder.rs (L177-184)
```rust
        let (pm_reqs_tx, pm_reqs_rx) = aptos_channel::new(
            QueueStyle::FIFO,
            channel_size,
            Some(&counters::PENDING_PEER_MANAGER_REQUESTS),
        );
        // Setup channel to send connection requests to peer manager.
        let (connection_reqs_tx, connection_reqs_rx) =
            aptos_channel::new(QueueStyle::FIFO, channel_size, None);
```
