# Audit Report

## Title
Unbounded Memory Consumption in NFT Metadata Crawler API Status Response Serialization

## Summary
The NFT metadata crawler's asset uploader API contains multiple unbounded vectors that can be exploited to cause excessive memory consumption when querying upload status. An attacker can submit batch upload requests with many URLs and trigger the accumulation of unbounded error messages, which are then loaded entirely into memory and serialized without pagination when the status endpoint is queried.

## Finding Description

The vulnerability exists across three components of the asset uploader service:

**1. Unbounded Batch Upload Size**

The `BatchUploadRequest` structure accepts an unbounded vector of URLs with no size validation: [1](#0-0) 

**2. Unbounded Error Messages from External API**

When uploads fail, the system stores error messages directly from the Cloudflare API response without any size validation or truncation: [2](#0-1) 

The `CloudflareImageUploadResponse.errors` field is an unbounded vector: [3](#0-2) 

**3. Unbounded Status Query Response**

The `get_status` function queries all rows matching an idempotency tuple without pagination or limits: [4](#0-3) 

The query has no LIMIT clause: [5](#0-4) 

**4. Database Schema Lacks Constraints**

The PostgreSQL schema defines `error_messages` as an unbounded array type: [6](#0-5) 

**Attack Scenario:**

1. Attacker sends POST request to `/upload` endpoint with a batch containing thousands of URLs (limited only by HTTP body size constraints)
2. Each URL is crafted to fail upload (invalid format, unreachable host, etc.)
3. The throttler processes each failed upload and stores Cloudflare error responses in the database
4. Attacker sends GET request to `/status/:application_id/:idempotency_key`
5. The API loads ALL rows for that batch into memory with ALL error messages
6. The API serializes the entire dataset into JSON without pagination
7. Memory consumption = (number of URLs) × (average errors per URL) × (average error message size) + JSON overhead

With no explicit body size limits in the nft-metadata-crawler service, an attacker could potentially submit hundreds or thousands of URLs in a single batch request. Even with Axum's default 2MB body limit, this allows for substantial attack payloads.

## Impact Explanation

This vulnerability meets **High Severity** criteria under the Aptos bug bounty program as it can cause **API crashes** through resource exhaustion. While the nft-metadata-crawler is an ecosystem service rather than core blockchain infrastructure, it is:

1. Part of the official Aptos Core repository
2. Potentially deployed as public-facing infrastructure
3. Vulnerable to trivial exploitation causing service disruption
4. Lacking any defensive mechanisms (rate limiting, pagination, size limits)

The vulnerability breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." While this specific service doesn't use gas metering, it should still enforce resource constraints on memory consumption.

## Likelihood Explanation

**Likelihood: HIGH**

- Requires only HTTP access to the API endpoint (no authentication shown in code)
- Trivial to exploit with basic HTTP clients
- No rate limiting observed in the codebase
- Attack can be automated and repeated with different idempotency keys
- No monitoring or alerting mechanisms visible to detect abuse

The attacker needs minimal resources and no special privileges to execute this attack.

## Recommendation

Implement multiple defensive layers:

**1. Add Request Size Validation**
```rust
const MAX_URLS_PER_BATCH: usize = 100;

#[derive(Debug, Deserialize)]
struct BatchUploadRequest {
    #[serde(flatten)]
    idempotency_tuple: IdempotencyTuple,
    #[serde(deserialize_with = "validate_urls_length")]
    urls: Vec<Url>,
}

fn validate_urls_length<'de, D>(deserializer: D) -> Result<Vec<Url>, D::Error>
where
    D: Deserializer<'de>,
{
    let urls: Vec<Url> = Deserialize::deserialize(deserializer)?;
    if urls.len() > MAX_URLS_PER_BATCH {
        return Err(serde::de::Error::custom(format!(
            "Too many URLs. Maximum {} allowed", MAX_URLS_PER_BATCH
        )));
    }
    Ok(urls)
}
```

**2. Truncate Error Messages**
```rust
const MAX_ERRORS_PER_ASSET: usize = 10;
const MAX_ERROR_MESSAGE_LENGTH: usize = 500;

asset.error_messages = Some(
    body.errors
        .iter()
        .take(MAX_ERRORS_PER_ASSET)
        .map(|err| {
            let msg = err.to_string();
            Some(if msg.len() > MAX_ERROR_MESSAGE_LENGTH {
                format!("{}...", &msg[..MAX_ERROR_MESSAGE_LENGTH])
            } else {
                msg
            })
        })
        .collect::<Vec<_>>(),
);
```

**3. Add Pagination to Status Query**
```rust
const MAX_STATUS_RESULTS: i64 = 1000;

fn query_status(
    conn: &mut PooledConnection<ConnectionManager<PgConnection>>,
    idempotency_tuple: &IdempotencyTuple,
    limit: Option<i64>,
) -> anyhow::Result<Vec<AssetUploaderRequestStatusesQuery>> {
    use schema::nft_metadata_crawler::asset_uploader_request_statuses::dsl::*;

    let query = asset_uploader_request_statuses
        .filter(
            idempotency_key
                .eq(&idempotency_tuple.idempotency_key)
                .and(application_id.eq(&idempotency_tuple.application_id)),
        )
        .limit(limit.unwrap_or(MAX_STATUS_RESULTS).min(MAX_STATUS_RESULTS));

    let rows = query.load(conn)?;
    Ok(rows)
}
```

**4. Add Request Body Size Limit Middleware**
```rust
use axum::extract::DefaultBodyLimit;

fn build_router(&self) -> axum::Router {
    let self_arc = Arc::new(self.clone());
    axum::Router::new()
        .route("/upload", post(Self::handle_upload_batch))
        .route("/status/:application_id/:idempotency_key", get(Self::handle_get_status))
        .layer(DefaultBodyLimit::max(2 * 1024 * 1024)) // 2MB limit
        .layer(Extension(self_arc.clone()))
}
```

## Proof of Concept

```rust
// PoC demonstrating the vulnerability
// This would be run as a Rust integration test

#[tokio::test]
async fn test_unbounded_error_messages_memory_exhaustion() {
    use reqwest::Client;
    use serde_json::json;
    
    let client = Client::new();
    let api_base = "http://localhost:8080"; // NFT crawler API endpoint
    
    // Step 1: Create a batch with many URLs designed to fail
    let mut urls = Vec::new();
    for i in 0..10000 {
        urls.push(format!("https://invalid-domain-{}.example.com/image.png", i));
    }
    
    let batch_request = json!({
        "idempotency_key": "attack_key_123",
        "application_id": "attacker_app",
        "urls": urls
    });
    
    // Step 2: Submit batch upload
    let upload_response = client
        .post(&format!("{}/upload", api_base))
        .json(&batch_request)
        .send()
        .await
        .expect("Failed to send upload request");
    
    assert_eq!(upload_response.status(), 200);
    
    // Step 3: Wait for processing (uploads will fail and store errors)
    tokio::time::sleep(tokio::time::Duration::from_secs(30)).await;
    
    // Step 4: Query status - this will load all 10,000 failed assets with error messages into memory
    let status_response = client
        .get(&format!("{}/status/attacker_app/attack_key_123", api_base))
        .send()
        .await
        .expect("Failed to get status");
    
    // The response will attempt to serialize all error messages
    // With 10,000 URLs × ~5 errors each × ~100 bytes per error = ~5MB of error data
    // Plus JSON overhead, this can cause significant memory pressure
    
    let response_body = status_response.text().await.expect("Failed to read response");
    println!("Response size: {} bytes", response_body.len());
    
    // In a real attack, an attacker would:
    // 1. Create multiple batches with different idempotency keys
    // 2. Query all of them simultaneously
    // 3. Cause the API server to run out of memory
}
```

**Notes:**
- This vulnerability is in the NFT metadata crawler ecosystem service, not core blockchain consensus or Move VM components
- However, it represents a valid resource exhaustion attack on Aptos infrastructure
- The lack of any bounds checking violates defensive programming principles
- Multiple layers of defense are missing: input validation, output pagination, error message truncation, and request size limits

### Citations

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/api/mod.rs (L37-42)
```rust
#[derive(Debug, Deserialize)]
struct BatchUploadRequest {
    #[serde(flatten)]
    idempotency_tuple: IdempotencyTuple,
    urls: Vec<Url>,
}
```

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/throttler/mod.rs (L62-65)
```rust
struct CloudflareImageUploadResponse {
    errors: Vec<CloudflareImageUploadResponseError>,
    result: Option<CloudflareImageUploadResponseResult>,
}
```

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/throttler/mod.rs (L143-148)
```rust
            asset.error_messages = Some(
                body.errors
                    .iter()
                    .map(|err| Some(err.to_string()))
                    .collect::<Vec<_>>(),
            );
```

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/api/get_status.rs (L17-39)
```rust
pub fn get_status(
    pool: Pool<ConnectionManager<PgConnection>>,
    idempotency_tuple: &IdempotencyTuple,
) -> anyhow::Result<AHashMap<String, GetStatusResponseSuccess>> {
    let mut conn = pool.get()?;
    let mut status_response = AHashMap::new();
    let rows = query_status(&mut conn, idempotency_tuple)?;
    for row in rows {
        if row.status_code == StatusCode::OK.as_u16() as i64 {
            status_response.insert(row.asset_uri, GetStatusResponseSuccess::Success {
                status_code: StatusCode::OK.as_u16(),
                cdn_image_uri: row.cdn_image_uri.unwrap_or_default(),
            });
        } else {
            status_response.insert(row.asset_uri, GetStatusResponseSuccess::Error {
                status_code: row.status_code as u16,
                error_message: row.error_messages,
            });
        };
    }

    Ok(status_response)
}
```

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/api/get_status.rs (L41-57)
```rust
fn query_status(
    conn: &mut PooledConnection<ConnectionManager<PgConnection>>,
    idempotency_tuple: &IdempotencyTuple,
) -> anyhow::Result<Vec<AssetUploaderRequestStatusesQuery>> {
    use schema::nft_metadata_crawler::asset_uploader_request_statuses::dsl::*;

    let query = asset_uploader_request_statuses.filter(
        idempotency_key
            .eq(&idempotency_tuple.idempotency_key)
            .and(application_id.eq(&idempotency_tuple.application_id)),
    );

    let debug_query = diesel::debug_query::<diesel::pg::Pg, _>(&query).to_string();
    debug!("Executing Query: {}", debug_query);
    let rows = query.load(conn)?;
    Ok(rows)
}
```

**File:** ecosystem/nft-metadata-crawler/src/schema.rs (L13-13)
```rust
            error_messages -> Nullable<Array<Nullable<Text>>>,
```
