# Audit Report

## Title
Indexer Token Processor Panic on Parse Failures Causes Permanent Data Loss and Infinite Crash Loop

## Summary
The token indexer processor uses `.unwrap()` on fallible parsing operations, causing the entire indexer to panic and crash when encountering malformed token table items. This leads to permanent data loss and creates an infinite crash loop, as the indexer repeatedly attempts to process the same problematic transaction.

## Finding Description

The vulnerability exists in the token processing pipeline where V1 token data is parsed from blockchain table items. The parsing function `TokenWriteSet::from_table_item_type()` can fail when deserializing JSON data that doesn't match expected struct formats. [1](#0-0) 

When this function returns an error, it propagates through `get_v1_from_write_table_item()` using the `?` operator. [2](#0-1) 

The critical error handling failure occurs in `parse_v2_token()` where the result is unwrapped without proper error handling. [3](#0-2) 

This pattern is repeated for multiple token types (Collections, TokenData, TokenOwnership), all using `.unwrap()` that will panic on parse failures. [4](#0-3) 

When a panic occurs in the spawned processing task, the main indexer loop catches it and panics the entire process. [5](#0-4) 

**Attack Scenario:**
1. Attacker deploys a Move contract that writes token data with unexpected structure to table items
2. The blockchain validators accept and execute this transaction (it's valid on-chain)
3. The indexer fetches this transaction and attempts to parse it
4. `TokenWriteSet::from_table_item_type()` fails to deserialize the malformed structure
5. The `.unwrap()` panics, crashing the indexer task
6. The panic propagates to the main loop, crashing the entire indexer process
7. Upon restart, the indexer attempts to process the same transaction batch again
8. **Infinite crash loop ensues** - the indexer can never progress past this transaction
9. **All transactions in the affected batch are never indexed** - permanent data loss

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program criteria:

- **Data Loss**: Transactions containing problematic token data are permanently lost from the indexer database, creating gaps in historical data
- **State Inconsistencies**: The indexer database becomes incomplete and inconsistent with the actual blockchain state, requiring manual intervention
- **Availability Impact**: The indexer enters an infinite crash loop, effectively becoming unavailable until the issue is manually resolved
- **Cascading Failures**: Applications and services relying on the indexer for token data will malfunction or provide incorrect information

While this doesn't directly affect consensus or validator operations, it severely impacts the ecosystem's ability to query and interact with token data, which is critical for dApps, wallets, and exchanges.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability can be triggered by:
- **Malicious actors**: Intentionally creating malformed token data to DoS the indexer
- **Contract bugs**: Poorly written Move contracts that create non-standard token structures
- **Protocol upgrades**: Changes to token standards that don't match indexer expectations
- **Edge cases**: Legitimate but unusual token configurations that violate implicit parsing assumptions

The attack requires no special privileges - any user can submit transactions that create token table items. The barrier to exploitation is low, requiring only:
1. Knowledge of Move programming
2. Ability to deploy a contract or transaction that writes to token tables
3. Understanding of which data structures will cause parse failures

Given the complexity of token data structures with nested types, custom deserializers, and multiple required fields, parse failures are reasonably likely even without malicious intent.

## Recommendation

Replace all `.unwrap()` calls with proper error handling that logs the error and continues processing. Parse failures should be treated as warnings, not fatal errors.

**Recommended fix for `parse_v2_token()` function:**

```rust
// Instead of:
if let Some((token_data, current_token_data)) =
    TokenDataV2::get_v1_from_write_table_item(
        table_item,
        txn_version,
        wsc_index,
        txn_timestamp,
    )
    .unwrap()  // DANGEROUS
{
    // process...
}

// Use:
match TokenDataV2::get_v1_from_write_table_item(
    table_item,
    txn_version,
    wsc_index,
    txn_timestamp,
) {
    Ok(Some((token_data, current_token_data))) => {
        token_datas_v2.push(token_data);
        current_token_datas_v2.insert(
            current_token_data.token_data_id.clone(),
            current_token_data,
        );
    },
    Ok(None) => {
        // No token data found, continue
    },
    Err(e) => {
        // Log error but continue processing
        aptos_logger::warn!(
            transaction_version = txn_version,
            write_set_change_index = wsc_index,
            error = ?e,
            "Failed to parse V1 token data from table item, skipping"
        );
    }
}
```

Apply this pattern to all similar calls in the function. Additionally, consider adding metrics to track parse failures for monitoring and debugging.

## Proof of Concept

```rust
#[cfg(test)]
mod test_parse_failure_vulnerability {
    use super::*;
    use serde_json::json;
    
    #[test]
    #[should_panic(expected = "failed to parse")]
    fn test_token_data_parse_failure_causes_panic() {
        // Create malformed TokenData JSON missing required fields
        let malformed_token_data = json!({
            "description": "Test",
            // Missing: largest_property_version, maximum, mutability_config,
            // name, royalty, supply, uri
        });
        
        // This will fail to deserialize
        let result = TokenWriteSet::from_table_item_type(
            "0x3::token::TokenData",
            &malformed_token_data,
            12345,
        );
        
        // Verify it returns an error
        assert!(result.is_err());
        
        // Simulating what the current code does - unwrap will panic
        let _ = result.unwrap();
    }
    
    #[test]
    fn test_token_processor_crash_on_malformed_data() {
        use aptos_api_types::{WriteTableItem, WriteSetChange};
        
        // Create a WriteTableItem with malformed data
        let malformed_table_item = WriteTableItem {
            state_key_hash: "0x...".to_string(),
            handle: "0x...".to_string(),
            key: "0x...".to_string(),
            value: "0x...".to_string(),
            data: Some(WriteTableChangeData {
                key: json!({"creator": "0x1", "collection": "test", "name": "token"}),
                key_type: "0x3::token::TokenDataId".to_string(),
                value: json!({"description": "incomplete"}), // Malformed!
                value_type: "0x3::token::TokenData".to_string(),
            }),
        };
        
        // This call will panic due to .unwrap() on parse error
        let result = std::panic::catch_unwind(|| {
            TokenDataV2::get_v1_from_write_table_item(
                &malformed_table_item,
                12345,
                0,
                chrono::NaiveDateTime::from_timestamp(1000000, 0),
            )
            .unwrap() // This is what the actual code does - PANIC!
        });
        
        assert!(result.is_err(), "Expected panic on malformed data");
    }
}
```

## Notes

This vulnerability is pervasive throughout the token processor, with similar `.unwrap()` patterns used for:
- Collection V2 parsing [4](#0-3) 
- Token ownership parsing [6](#0-5) 
- Delete table item parsing [7](#0-6) 
- Resource parsing operations [8](#0-7) 

All of these locations require the same fix to prevent indexer crashes and data loss.

### Citations

**File:** crates/indexer/src/models/token_models/token_utils.rs (L322-347)
```rust
    pub fn from_table_item_type(
        data_type: &str,
        data: &serde_json::Value,
        txn_version: i64,
    ) -> Result<Option<TokenWriteSet>> {
        match data_type {
            "0x3::token::TokenDataId" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::TokenDataId(inner))),
            "0x3::token::TokenId" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::TokenId(inner))),
            "0x3::token::TokenData" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::TokenData(inner))),
            "0x3::token::Token" => {
                serde_json::from_value(data.clone()).map(|inner| Some(TokenWriteSet::Token(inner)))
            },
            "0x3::token::CollectionData" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::CollectionData(inner))),
            "0x3::token_transfers::TokenOfferId" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::TokenOfferId(inner))),
            _ => Ok(None),
        }
        .context(format!(
            "version {} failed! failed to parse type {}, data {:?}",
            txn_version, data_type, data
        ))
    }
```

**File:** crates/indexer/src/models/token_models/v2_token_datas.rs (L163-170)
```rust
        let maybe_token_data = match TokenWriteSet::from_table_item_type(
            table_item_data.value_type.as_str(),
            &table_item_data.value,
            txn_version,
        )? {
            Some(TokenWriteSet::TokenData(inner)) => Some(inner),
            _ => None,
        };
```

**File:** crates/indexer/src/processors/token_processor.rs (L1096-1096)
```rust
                        ObjectWithMetadata::from_write_resource(wr, txn_version).unwrap()
```

**File:** crates/indexer/src/processors/token_processor.rs (L1231-1240)
```rust
                        if let Some((collection, current_collection)) =
                            CollectionV2::get_v1_from_write_table_item(
                                table_item,
                                txn_version,
                                wsc_index,
                                txn_timestamp,
                                table_handle_to_owner,
                                conn,
                            )
                            .unwrap()
```

**File:** crates/indexer/src/processors/token_processor.rs (L1248-1256)
```rust
                        if let Some((token_data, current_token_data)) =
                            TokenDataV2::get_v1_from_write_table_item(
                                table_item,
                                txn_version,
                                wsc_index,
                                txn_timestamp,
                            )
                            .unwrap()
                        {
```

**File:** crates/indexer/src/processors/token_processor.rs (L1263-1272)
```rust
                        if let Some((token_ownership, current_token_ownership)) =
                            TokenOwnershipV2::get_v1_from_write_table_item(
                                table_item,
                                txn_version,
                                wsc_index,
                                txn_timestamp,
                                table_handle_to_owner,
                            )
                            .unwrap()
                        {
```

**File:** crates/indexer/src/processors/token_processor.rs (L1296-1305)
```rust
                        if let Some((token_ownership, current_token_ownership)) =
                            TokenOwnershipV2::get_v1_from_delete_table_item(
                                table_item,
                                txn_version,
                                wsc_index,
                                txn_timestamp,
                                table_handle_to_owner,
                            )
                            .unwrap()
                        {
```

**File:** crates/indexer/src/runtime.rs (L216-243)
```rust
        let batches = match futures::future::try_join_all(tasks).await {
            Ok(res) => res,
            Err(err) => panic!("Error processing transaction batches: {:?}", err),
        };

        let mut batch_start_version = u64::MAX;
        let mut batch_end_version = 0;
        let mut num_res = 0;

        for (num_txn, res) in batches {
            let processed_result: ProcessingResult = match res {
                // When the batch is empty b/c we're caught up, continue to next batch
                None => continue,
                Some(Ok(res)) => res,
                Some(Err(tpe)) => {
                    let (err, start_version, end_version, _) = tpe.inner();
                    error!(
                        processor_name = processor_name,
                        start_version = start_version,
                        end_version = end_version,
                        error =? err,
                        "Error processing batch!"
                    );
                    panic!(
                        "Error in '{}' while processing batch: {:?}",
                        processor_name, err
                    );
                },
```
