# Audit Report

## Title
Stale Node Misclassification Due to Incorrect Version Check in Cross-Epoch Schema Selection

## Summary
The `create_jmt_commit_batch_for_shard` function incorrectly classifies stale Jellyfish Merkle tree nodes by checking the node's creation version instead of when it became stale, causing nodes to be assigned to the wrong pruning schema and retained up to 80x longer than necessary.

## Finding Description

In the state merkle database commit logic, stale nodes must be classified into one of two schemas:
- `StaleNodeIndexSchema`: Regular stale nodes pruned after 1,000,000 versions
- `StaleNodeIndexCrossEpochSchema`: Epoch snapshot nodes retained for 80,000,000 versions

The classification logic incorrectly checks `row.node_key.version()` (the version when the node was created) instead of `row.stale_since_version` (when the node became stale): [1](#0-0) 

The correct classification should be: nodes that became stale AT OR BEFORE the epoch boundary should go to the cross-epoch schema (as they represent historical epoch states), while nodes that became stale AFTER should go to the regular schema.

**Attack Scenario:**
1. Epoch N ends at version 1,000,000
2. Node X was created at version 500,000 and remains live through the epoch boundary  
3. At version 1,500,000 (in Epoch N+1), Node X is updated and becomes stale
4. Classification logic checks: `500,000 <= 1,000,000` â†’ TRUE
5. Node X goes to `StaleNodeIndexCrossEpochSchema` (80M version retention)
6. Should go to `StaleNodeIndexSchema` (1M version retention) since it became stale AFTER the epoch boundary

**Why This Matters:**
The `stale_since_version` field is what pruners use to determine when to delete nodes: [2](#0-1) 

Nodes from early blockchain history that are still being updated will accumulate in the wrong schema with 80x longer retention. [3](#0-2) [4](#0-3) 

## Impact Explanation

This bug causes **storage inefficiency** leading to gradual database bloat, but does **NOT** corrupt epoch snapshots or break consensus safety. Classification as **Medium Severity** under "State inconsistencies requiring intervention":

- Nodes are retained 80x longer than necessary (80M vs 1M versions)
- At 5,000 TPS with ~300k stale nodes/second, misclassified nodes accumulate significantly
- Over time, this causes measurable storage overhead and query performance degradation
- However, no nodes are pruned too early (reverse case is impossible since `stale_since_version >= node_key.version()`)
- All validators make the same mistake, maintaining state root consistency
- No consensus safety violation or fund loss

The question asks if this can "corrupt epoch snapshots" - **it cannot**, because nodes needed for epoch snapshots are never pruned too early, only retained longer than optimal.

## Likelihood Explanation

This bug triggers automatically during normal operation:
- Every time a node created before the most recent epoch boundary is updated
- Affects all validator nodes equally
- Accumulates over the blockchain's lifetime
- Impact grows as the chain ages and more historical nodes are updated

However, the security impact remains limited to storage/performance, not safety/liveness.

## Recommendation

Change the classification logic to check `stale_since_version` instead of `node_key.version()`:

```rust
stale_node_index_batch.iter().try_for_each(|row| {
    ensure!(row.node_key.get_shard_id() == shard_id, "shard_id mismatch");
    if previous_epoch_ending_version.is_some()
        && row.stale_since_version <= previous_epoch_ending_version.unwrap()  // Fixed: was row.node_key.version()
    {
        batch.put::<StaleNodeIndexCrossEpochSchema>(row, &())
    } else {
        batch.put::<StaleNodeIndexSchema>(row, &())
    }
})?;
```

## Proof of Concept

```rust
#[test]
fn test_stale_node_classification_at_epoch_boundary() {
    // Setup: Create state merkle DB
    let tmpdir = aptos_temppath::TempPath::new();
    let db = StateMerkleDb::new(...);
    
    // Scenario: Node created before epoch boundary, becomes stale after
    let node_creation_version = 500_000;
    let epoch_ending_version = 1_000_000;  
    let stale_at_version = 1_500_000;
    
    // Create a node at version 500k
    let node_key = NodeKey::new_empty_path(node_creation_version);
    let node = Node::new_leaf(...);
    
    // Simulate it becoming stale at version 1.5M with epoch boundary at 1M
    let stale_index = StaleNodeIndex {
        stale_since_version: stale_at_version,
        node_key: node_key.clone(),
    };
    
    let mut tree_update_batch = TreeUpdateBatch::new();
    tree_update_batch.put_stale_node(node_key, stale_at_version);
    
    // Create commit batch with previous epoch = 1M
    let batch = db.create_jmt_commit_batch_for_shard(
        stale_at_version,
        Some(0),
        &tree_update_batch,
        Some(epoch_ending_version)
    )?;
    
    // BUG: This node will go to StaleNodeIndexCrossEpochSchema
    // because node_key.version() (500k) <= epoch_ending (1M)
    // But it SHOULD go to StaleNodeIndexSchema  
    // because stale_since_version (1.5M) > epoch_ending (1M)
    
    // Verify misclassification by checking which schema contains the entry
    assert!(db.db_shard(0).get::<StaleNodeIndexCrossEpochSchema>(&stale_index).unwrap().is_some());
    assert!(db.db_shard(0).get::<StaleNodeIndexSchema>(&stale_index).unwrap().is_none());
}
```

## Notes

While this is a confirmed logic bug causing storage inefficiency, it does **not** meet the threshold for a critical security vulnerability because:

1. **No snapshot corruption**: Nodes are retained longer, never pruned too early
2. **No consensus impact**: All nodes classify identically, maintaining state root consistency  
3. **No fund loss or availability disruption**: Impact is limited to gradual storage growth
4. **Deterministic behavior**: Not exploitable by attackers, affects all nodes equally

This should be fixed to optimize storage efficiency, but it does not constitute a critical security flaw per the strict validation criteria requiring demonstrable harm to funds, consensus, or availability.

### Citations

**File:** storage/aptosdb/src/state_merkle_db.rs (L376-386)
```rust
        stale_node_index_batch.iter().try_for_each(|row| {
            ensure!(row.node_key.get_shard_id() == shard_id, "shard_id mismatch");
            if previous_epoch_ending_version.is_some()
                && row.node_key.version() <= previous_epoch_ending_version.unwrap()
            {
                batch.put::<StaleNodeIndexCrossEpochSchema>(row, &())
            } else {
                // These are processed by the state merkle pruner.
                batch.put::<StaleNodeIndexSchema>(row, &())
            }
        })?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L198-214)
```rust
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }
```

**File:** config/src/config/storage_config.rs (L398-412)
```rust
impl Default for StateMerklePrunerConfig {
    fn default() -> Self {
        StateMerklePrunerConfig {
            enable: true,
            // This allows a block / chunk being executed to have access to a non-latest state tree.
            // It needs to be greater than the number of versions the state committing thread is
            // able to commit during the execution of the block / chunk. If the bad case indeed
            // happens due to this being too small, a node restart should recover it.
            // Still, defaulting to 1M to be super safe.
            prune_window: 1_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
```

**File:** config/src/config/storage_config.rs (L415-429)
```rust
impl Default for EpochSnapshotPrunerConfig {
    fn default() -> Self {
        Self {
            enable: true,
            // This is based on ~5K TPS * 2h/epoch * 2 epochs. -- epoch ending snapshots are used
            // by state sync in fast sync mode.
            // The setting is in versions, not epochs, because this makes it behave more like other
            // pruners: a slower network will have longer history in db with the same pruner
            // settings, but the disk space take will be similar.
            // settings.
            prune_window: 80_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
```
