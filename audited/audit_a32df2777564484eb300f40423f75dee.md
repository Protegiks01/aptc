# Audit Report

## Title
Indexer Batch Processing Denial of Service via Unhandled Deserialization Error on Malformed Token Resources

## Summary
The indexer's token processor contains an unsafe `.unwrap()` call when processing token resource metadata during batch processing. If a transaction contains a malformed token resource with invalid or missing table handle data, the unwrap will panic, crashing the indexer thread and halting all transaction indexing. [1](#0-0) 

## Finding Description
During batch processing of transactions, the token processor builds a mapping of table handles to owner metadata by iterating through all `WriteResource` changes in the batch. For each resource matching a token type (`0x3::token::Collections`, `0x3::token::TokenStore`, or `0x3::token_transfers::PendingClaims`), it calls `get_table_handle_to_owner()` which attempts to deserialize the resource JSON data into strongly-typed Rust structs. [2](#0-1) 

The deserialization process in `TokenResource::from_resource()` uses `serde_json::from_value()` to parse the resource data. This can fail if:
- The `handle` field is missing from a Table struct
- The `handle` field has incorrect type
- Required fields are absent or malformed
- The JSON structure doesn't match expected schema [3](#0-2) 

When deserialization fails, the error propagates via the `?` operator, but line 364 unwraps this `Result`, causing a panic. This panic is caught by the tokio task spawner and returned as an error, which then triggers another panic in the main indexer runtime loop, crashing the entire indexer process. [4](#0-3) 

## Impact Explanation
This constitutes a **Medium Severity** vulnerability under the Aptos bug bounty program ("State inconsistencies requiring intervention"). While the indexer is not consensus-critical, it provides essential data access for the ecosystem:

1. **Indexer Unavailability**: The indexer crashes and cannot process new transactions
2. **Persistent Failure**: Upon restart, the indexer will repeatedly crash when attempting to process the same malformed transaction
3. **Ecosystem Disruption**: Applications relying on indexed data (NFT marketplaces, explorers, wallets) lose real-time blockchain visibility
4. **Manual Intervention Required**: Operators must identify and skip the problematic transaction or patch the indexer code

The vulnerability does not affect consensus, chain liveness, or fund security, but requires manual intervention to restore indexer functionality.

## Likelihood Explanation
While Move VM's type system should prevent malformed BCS data from being written to state under normal operation, several scenarios could produce malformed JSON at the indexer layer:

1. **API Layer Bugs**: Errors in BCS-to-JSON conversion logic could produce structurally invalid JSON
2. **Schema Evolution**: Contract upgrades changing resource structure without corresponding indexer updates
3. **Storage Corruption**: Underlying database corruption returning invalid BCS bytes
4. **Network Issues**: Partial data transmission or corruption during API communication

The likelihood is **Low-to-Medium** as it requires auxiliary failures, but the impact is certain once triggered due to the unconditional panic.

## Recommendation
Replace the `.unwrap()` with proper error handling that logs the error and continues processing:

```rust
// In get_table_handle_to_owner_from_transactions (line 360-364)
if let APIWriteSetChange::WriteResource(write_resource) = wsc {
    match TableMetadataForToken::get_table_handle_to_owner(
        write_resource,
        txn_version,
    ) {
        Ok(maybe_map) => {
            if let Some(map) = maybe_map {
                table_handle_to_owner.extend(map);
            }
        }
        Err(e) => {
            aptos_logger::warn!(
                txn_version = txn_version,
                error = ?e,
                "Failed to parse table handle metadata, skipping resource"
            );
            // Continue processing other resources
        }
    }
}
```

This allows the indexer to gracefully skip malformed resources while continuing to process the rest of the batch, preventing complete service disruption.

## Proof of Concept

The vulnerability can be demonstrated by simulating a malformed resource write reaching the indexer:

```rust
#[test]
fn test_malformed_table_handle_causes_panic() {
    use aptos_api_types::{WriteResource, MoveStructTag, Address};
    use serde_json::json;
    
    // Simulate a WriteResource with malformed Table structure (missing 'handle' field)
    let malformed_data = json!({
        "collection_data": {
            // Missing 'handle' field - should have {"handle": "0x..."}
            "invalid_field": "some_value"
        },
        "token_data": {
            "handle": "0x1"
        }
    });
    
    let write_resource = WriteResource {
        address: Address::from_hex_literal("0x1").unwrap(),
        state_key_hash: "0xabc".to_string(),
        data: MoveResourceData {
            typ: MoveStructTag {
                address: Address::from_hex_literal("0x3").unwrap(),
                module: Identifier::new("token").unwrap(),
                name: Identifier::new("Collections").unwrap(),
                generic_type_params: vec![],
            },
            data: malformed_data,
        },
    };
    
    // This will panic due to unwrap on line 364
    let result = std::panic::catch_unwind(|| {
        TableMetadataForToken::get_table_handle_to_owner(&write_resource, 100)
            .unwrap() // This is line 364
    });
    
    assert!(result.is_err(), "Should panic on malformed data");
}
```

When this malformed resource is encountered during batch processing, the indexer will crash, requiring manual intervention to resume operation.

## Notes

This vulnerability demonstrates a critical defensive programming failure. While the Move VM's type system provides strong guarantees about on-chain state integrity, off-chain components like the indexer must handle data robustly, accounting for potential bugs in API layers, schema evolution, or infrastructure failures. The use of `.unwrap()` in production batch processing code violates this principle and creates a single point of failure for the entire indexing service.

### Citations

**File:** crates/indexer/src/models/token_models/tokens.rs (L360-364)
```rust
                        let maybe_map = TableMetadataForToken::get_table_handle_to_owner(
                            write_resource,
                            txn_version,
                        )
                        .unwrap();
```

**File:** crates/indexer/src/models/token_models/tokens.rs (L376-412)
```rust
    fn get_table_handle_to_owner(
        write_resource: &APIWriteResource,
        txn_version: i64,
    ) -> anyhow::Result<Option<TableHandleToOwner>> {
        let type_str = format!(
            "{}::{}::{}",
            write_resource.data.typ.address,
            write_resource.data.typ.module,
            write_resource.data.typ.name
        );
        if !TokenResource::is_resource_supported(type_str.as_str()) {
            return Ok(None);
        }
        let resource = MoveResource::from_write_resource(
            write_resource,
            0, // Placeholder, this isn't used anyway
            txn_version,
            0, // Placeholder, this isn't used anyway
        );

        let value = TableMetadataForToken {
            owner_address: standardize_address(&resource.address),
            table_type: write_resource.data.typ.to_string(),
        };
        let table_handle: TableHandle = match TokenResource::from_resource(
            &type_str,
            resource.data.as_ref().unwrap(),
            txn_version,
        )? {
            TokenResource::CollectionResource(collection_resource) => {
                collection_resource.collection_data.get_handle()
            },
            TokenResource::TokenStoreResource(inner) => inner.tokens.get_handle(),
            TokenResource::PendingClaimsResource(inner) => inner.pending_claims.get_handle(),
        };
        Ok(Some(HashMap::from([(table_handle, value)])))
    }
```

**File:** crates/indexer/src/models/token_models/token_utils.rs (L411-433)
```rust
    pub fn from_resource(
        data_type: &str,
        data: &serde_json::Value,
        txn_version: i64,
    ) -> Result<TokenResource> {
        match data_type {
            "0x3::token::Collections" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenResource::CollectionResource(inner))),
            "0x3::token::TokenStore" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenResource::TokenStoreResource(inner))),
            "0x3::token_transfers::PendingClaims" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenResource::PendingClaimsResource(inner))),
            _ => Ok(None),
        }
        .context(format!(
            "version {} failed! failed to parse type {}, data {:?}",
            txn_version, data_type, data
        ))?
        .context(format!(
            "Resource unsupported! Call is_resource_supported first. version {} type {}",
            txn_version, data_type
        ))
    }
```

**File:** crates/indexer/src/runtime.rs (L216-243)
```rust
        let batches = match futures::future::try_join_all(tasks).await {
            Ok(res) => res,
            Err(err) => panic!("Error processing transaction batches: {:?}", err),
        };

        let mut batch_start_version = u64::MAX;
        let mut batch_end_version = 0;
        let mut num_res = 0;

        for (num_txn, res) in batches {
            let processed_result: ProcessingResult = match res {
                // When the batch is empty b/c we're caught up, continue to next batch
                None => continue,
                Some(Ok(res)) => res,
                Some(Err(tpe)) => {
                    let (err, start_version, end_version, _) = tpe.inner();
                    error!(
                        processor_name = processor_name,
                        start_version = start_version,
                        end_version = end_version,
                        error =? err,
                        "Error processing batch!"
                    );
                    panic!(
                        "Error in '{}' while processing batch: {:?}",
                        processor_name, err
                    );
                },
```
