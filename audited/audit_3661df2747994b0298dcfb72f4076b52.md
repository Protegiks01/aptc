# Audit Report

## Title
Executor Service Deadlock via Zero Thread Pool Configuration Causing Total Loss of Liveness

## Summary
The `num_executor_threads` command-line argument lacks minimum value validation, allowing operators to set it to 0. This creates a thread pool with only 2 threads, where one thread permanently blocks in the cross-shard message receiver and the other attempts parallel execution with nested worker tasks that can block on condition variables waiting for dependencies. This configuration causes an unrecoverable deadlock, resulting in total loss of validator liveness.

## Finding Description

The executor service creates a Rayon thread pool with `num_threads + 2` threads, where the "+2" accounts for the cross-shard commit receiver and execution coordination thread. [1](#0-0) 

The `num_executor_threads` parameter has no validation preventing it from being set to 0: [2](#0-1) 

When `num_executor_threads = 0`, the following deadlock occurs:

**Phase 1: Outer Scope Task Spawning**

The executor spawns two tasks in the outer scope: [3](#0-2) 

**Phase 2: Thread 1 Permanent Blocking**

Thread 1 executes `CrossShardCommitReceiver::start()` which contains an infinite loop with a blocking channel receive call: [4](#0-3) 

This thread remains blocked waiting for messages and cannot participate in work-stealing.

**Phase 3: Thread 2 Nested Scope Creation**

Thread 2 executes the block, calling `execute_block_on_thread_pool()` which eventually invokes `execute_transactions_parallel()`: [5](#0-4) 

The nested scope spawns at least 2 workers (calculated at line 1876: `num_workers >= 2`).

**Phase 4: Worker Dependency Blocking**

Workers in the parallel execution can encounter read dependencies and block on condition variables: [6](#0-5) 

**Phase 5: Deadlock Condition**

With only 2 threads total:
- Thread 1: Permanently blocked in `receive_cross_shard_msg()` 
- Thread 2: Executing Worker A, which blocks on `cvar.wait()` waiting for Worker B to resolve a dependency
- Worker B needs a thread to execute, but both threads are blocked
- The `StopMsg` that would unblock Thread 1 can only be sent after block execution completes, but block execution is deadlocked

This violates the **Resource Limits** invariant (all operations must respect computational limits) and the **Liveness** guarantee (the node must be able to process blocks).

## Impact Explanation

**Severity: Critical** - Total loss of liveness/network availability

This vulnerability causes the executor service to permanently deadlock during block execution. The affected validator node cannot:
- Execute any transactions
- Commit blocks
- Participate in consensus
- Synchronize state
- Process new transactions

The node becomes completely non-functional and will be removed from the active validator set due to non-responsiveness. This requires a node restart to recover, and the misconfiguration would cause the deadlock to recur immediately.

This meets the **Critical Severity** criteria: "Total loss of liveness/network availability" as defined in the Aptos bug bounty program.

## Likelihood Explanation

**Likelihood: Medium to High**

Operators may misconfigure this parameter through:

1. **Accidental misconfiguration**: Setting `--num_executor_threads 0` due to configuration errors or copy-paste mistakes
2. **Misunderstanding the parameter**: Attempting to "disable" parallel execution by setting it to 0, not realizing it controls the thread pool size
3. **Automated deployment**: Scripts with incorrect default values propagating to production
4. **Resource constraints**: Operators trying to minimize resource usage on low-spec hardware

The lack of validation or documentation warnings makes this error easy to introduce. The default value of 8 protects against this in normal deployments, but operator override can trigger the vulnerability immediately upon service startup.

## Recommendation

Add input validation to enforce a minimum value for `num_executor_threads`. The minimum should be based on the architectural requirements:

**Recommended Fix:**

```rust
#[derive(Debug, Parser)]
struct Args {
    #[clap(long, default_value_t = 8, value_parser = clap::value_parser!(usize).range(4..))]
    pub num_executor_threads: usize,
    // ... rest of fields
}
```

The minimum value of 4 ensures:
- 2 threads for cross-shard receiver and coordination (the "+2")
- At least 2 additional threads for actual parallel worker execution

Additionally, add runtime assertion:

```rust
fn main() {
    let args = Args::parse();
    
    // Validate minimum thread count
    assert!(
        args.num_executor_threads >= 4,
        "num_executor_threads must be at least 4 (found {})", 
        args.num_executor_threads
    );
    
    aptos_logger::Logger::new().init();
    // ... rest of main
}
```

Add documentation explaining the minimum and why it exists.

## Proof of Concept

**Reproduction Steps:**

1. Build the executor service binary
2. Launch with zero threads:
```bash
./executor-service \
  --num_executor_threads 0 \
  --shard_id 0 \
  --num_shards 1 \
  --coordinator_address 127.0.0.1:8000 \
  --remote_executor_addresses 127.0.0.1:8001
```

3. Send a block execution command with transactions that have read dependencies
4. Observe the service deadlock:
   - Thread 1 blocked in `CrossShardCommitReceiver::start()` at `receive_cross_shard_msg()`
   - Thread 2 blocked in worker execution at `cvar.wait()`
   - No progress possible, service completely hung

**Expected Behavior:** Service should reject the configuration with a validation error

**Actual Behavior:** Service starts but deadlocks on first block execution with inter-transaction dependencies

**Alternative PoC (Rust unit test):**

```rust
#[test]
#[should_panic(expected = "deadlock")]
fn test_zero_threads_causes_deadlock() {
    // Create thread pool with 0 + 2 = 2 threads
    let pool = rayon::ThreadPoolBuilder::new()
        .num_threads(2)
        .build()
        .unwrap();
    
    // Simulate the executor service pattern
    pool.scope(|s| {
        // Spawn blocking receiver (Thread 1)
        s.spawn(|_| {
            let (tx, rx) = crossbeam_channel::bounded(0);
            rx.recv().unwrap(); // Blocks forever
        });
        
        // Spawn execution task (Thread 2)
        s.spawn(|_| {
            // Create nested scope with workers that have dependencies
            pool.scope(|s2| {
                s2.spawn(|_| {
                    // Worker A - blocks waiting for Worker B
                    let condvar = Arc::new((Mutex::new(false), Condvar::new()));
                    let (lock, cvar) = &*condvar;
                    let mut resolved = lock.lock().unwrap();
                    while !*resolved {
                        resolved = cvar.wait(resolved).unwrap();
                    }
                });
                s2.spawn(|_| {
                    // Worker B - never executes due to thread exhaustion
                    // Would resolve Worker A's dependency
                });
            });
        });
    });
}
```

## Notes

This vulnerability demonstrates a critical flaw in the resource configuration validation. The comment in the code explicitly states "We need two extra threads for the cross-shard commit receiver and the thread that is blocked on waiting for execute block to finish," yet no enforcement exists to ensure `num_threads` provides sufficient additional threads for actual parallel execution.

The vulnerability affects any deployment where operators have the ability to override the default thread count, including:
- Manual configuration files
- Command-line overrides
- Automated deployment systems
- Testing/development environments that might use minimal thread counts

The fix is straightforward (input validation) but the impact is severe (total node failure), making this a critical security issue requiring immediate remediation.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L58-66)
```rust
        let executor_thread_pool = Arc::new(
            rayon::ThreadPoolBuilder::new()
                // We need two extra threads for the cross-shard commit receiver and the thread
                // that is blocked on waiting for execute block to finish.
                .thread_name(move |i| format!("sharded-executor-shard-{}-{}", shard_id, i))
                .num_threads(num_threads + 2)
                .build()
                .unwrap(),
        );
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L134-180)
```rust
        executor_thread_pool.clone().scope(|s| {
            s.spawn(move |_| {
                CrossShardCommitReceiver::start(
                    cross_shard_state_view_clone,
                    cross_shard_client,
                    round,
                );
            });
            s.spawn(move |_| {
                let txn_provider =
                    DefaultTxnProvider::new_without_info(signature_verified_transactions);
                let ret = AptosVMBlockExecutorWrapper::execute_block_on_thread_pool(
                    executor_thread_pool,
                    &txn_provider,
                    aggr_overridden_state_view.as_ref(),
                    // Since we execute blocks in parallel, we cannot share module caches, so each
                    // thread has its own caches.
                    &AptosModuleCacheManager::new(),
                    config,
                    TransactionSliceMetadata::unknown(),
                    cross_shard_commit_sender,
                )
                .map(BlockOutput::into_transaction_outputs_forced);
                if let Some(shard_id) = shard_id {
                    trace!(
                        "executed sub block for shard {} and round {}",
                        shard_id,
                        round
                    );
                    // Send a self message to stop the cross-shard commit receiver.
                    cross_shard_client_clone.send_cross_shard_msg(
                        shard_id,
                        round,
                        CrossShardMsg::StopMsg,
                    );
                } else {
                    trace!("executed block for global shard and round {}", round);
                    // Send a self message to stop the cross-shard commit receiver.
                    cross_shard_client_clone.send_global_msg(CrossShardMsg::StopMsg);
                }
                callback.send(ret).unwrap();
                executor_thread_pool_clone.spawn(move || {
                    // Explicit async drop
                    drop(txn_provider);
                });
            });
        });
```

**File:** execution/executor-service/src/main.rs (L10-13)
```rust
struct Args {
    #[clap(long, default_value_t = 8)]
    pub num_executor_threads: usize,

```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L26-45)
```rust
    pub fn start<S: StateView + Sync + Send>(
        cross_shard_state_view: Arc<CrossShardStateView<S>>,
        cross_shard_client: Arc<dyn CrossShardClient>,
        round: RoundId,
    ) {
        loop {
            let msg = cross_shard_client.receive_cross_shard_msg(round);
            match msg {
                RemoteTxnWriteMsg(txn_commit_msg) => {
                    let (state_key, write_op) = txn_commit_msg.take();
                    cross_shard_state_view
                        .set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
                },
                CrossShardMsg::StopMsg => {
                    trace!("Cross shard commit receiver stopped for round {}", round);
                    break;
                },
            }
        }
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L1922-1961)
```rust
        self.executor_thread_pool.scope(|s| {
            for worker_id in &worker_ids {
                s.spawn(|_| {
                    let environment = module_cache_manager_guard.environment();
                    let executor = {
                        let _init_timer = VM_INIT_SECONDS.start_timer();
                        E::init(
                            &environment.clone(),
                            base_view,
                            async_runtime_checks_enabled,
                        )
                    };

                    if let Err(err) = self.worker_loop(
                        &executor,
                        environment,
                        signature_verified_block,
                        &scheduler,
                        &skip_module_reads_validation,
                        &shared_sync_params,
                        num_workers,
                    ) {
                        // If there are multiple errors, they all get logged:
                        // ModulePathReadWriteError and FatalVMError variant is logged at construction,
                        // and below we log CodeInvariantErrors.
                        if let PanicOr::CodeInvariantError(err_msg) = err {
                            alert!("[BlockSTM] worker loop: CodeInvariantError({:?})", err_msg);
                        }
                        shared_maybe_error.store(true, Ordering::SeqCst);

                        // Make sure to halt the scheduler if it hasn't already been halted.
                        scheduler.halt();
                    }

                    if *worker_id == 0 {
                        maybe_executor.acquire().replace(executor);
                    }
                });
            }
        });
```

**File:** aptos-move/block-executor/src/view.rs (L510-513)
```rust
            let mut dep_resolved = lock.lock();
            while matches!(*dep_resolved, DependencyStatus::Unresolved) {
                dep_resolved = cvar.wait(dep_resolved).unwrap();
            }
```
