# Audit Report

## Title
DKG Transcript Deserialization DoS via Oversized Arrays Before Size Validation

## Summary
The `add()` function in the DKG transcript aggregation module deserializes peer transcripts before validating their size, allowing attackers to craft transcripts with oversized elliptic curve point arrays (up to ~62 MiB) that consume excessive CPU during BCS deserialization and cryptographic point validation, while passing the lightweight `verify_transcript_extra()` check quickly before being rejected by `verify_transcript()`.

## Finding Description
The vulnerability exists in the transcript processing flow where size validation occurs too late: [1](#0-0) 

The attack flow:

1. **Deserialization occurs first** - At line 88, `bcs::from_bytes()` deserializes the entire transcript including all elliptic curve point arrays (R, R_hat, V, V_hat, C) without size limits.

2. **Fast verification passes** - At line 96, `verify_transcript_extra()` only validates the dealer set, which is O(d) where d is the number of dealers: [2](#0-1) 

3. **Expensive verification eventually rejects** - At line 99, `verify_transcript()` calls the underlying PVSS verification which checks array sizes: [3](#0-2) 

The Transcript struct contains vectors of elliptic curve points: [4](#0-3) 

Each point deserialization involves expensive cryptographic validation: [5](#0-4) 

The comment confirms point validation occurs during deserialization. The network allows messages up to 64 MiB: [6](#0-5) 

**Attack scenario**: A malicious validator crafts a DKGTranscript with:
- Valid metadata (correct epoch, valid dealer)
- Small dealer set (e.g., 1 dealer) to pass `verify_transcript_extra()` quickly
- Massive arrays: V, V_hat, R, R_hat, C filled with ~1.35 million G1 points or ~677k G2 points (up to ~62 MiB)

When processed:
- Lines 74-87: Metadata checks pass quickly
- Line 88: BCS deserializes ~1 million elliptic curve points, each requiring curve membership and subgroup checks
- Line 96: `verify_transcript_extra()` passes (only checks dealer set)
- Line 99: `verify_transcript()` â†’ `check_sizes()` finally rejects the oversized arrays

The computational asymmetry: deserializing and validating 1 million curve points takes significant CPU time, while the eventual rejection is instantaneous.

## Impact Explanation
This is a **Medium to High severity** computational DoS vulnerability:

- **Validator Node Slowdowns**: Each malicious transcript can consume seconds of CPU time deserializing and validating hundreds of thousands of elliptic curve points
- **DKG Disruption**: During epoch transitions, validators must complete DKG. CPU exhaustion could delay or prevent timely DKG completion
- **Resource Exhaustion**: Multiple validators could send malicious transcripts, multiplying the impact
- **Limited by epoch**: Each validator can only send one transcript per epoch due to the contributor check at line 92-94 [7](#0-6) 

Per Aptos bug bounty: **High Severity** includes "Validator node slowdowns" (up to $50,000).

## Likelihood Explanation
**Likelihood: Medium-High**

- **Attacker requirements**: Must be a validator in the current epoch with access to validator network
- **Complexity**: Low - simply craft a transcript with oversized arrays and send via normal DKG protocol
- **Detection**: The attack would be logged as "trx verification failure" but only after CPU consumption
- **Mitigation**: None currently - no size limits before deserialization

## Recommendation
Implement early size validation before deserialization:

**Option 1**: Validate `transcript_bytes` size against expected bounds:
```rust
// Before line 88 in transcript_aggregation/mod.rs
const MAX_TRANSCRIPT_SIZE: usize = 1024 * 1024; // 1 MiB reasonable limit
ensure!(
    transcript_bytes.len() <= MAX_TRANSCRIPT_SIZE,
    "[DKG] transcript_bytes exceeds maximum size"
);
```

**Option 2**: Use bounded BCS deserialization:
```rust
// Replace line 88
let transcript = bcs::from_bytes_with_limit(
    transcript_bytes.as_slice(),
    MAX_TRANSCRIPT_SIZE
).map_err(|e| {
    anyhow!("[DKG] adding peer transcript failed with trx deserialization error: {e}")
})?;
```

**Option 3**: Validate array sizes early in `verify_transcript_extra()`:
```rust
// In verify_transcript_extra, add size checks before other validations
let dealers_count = trx.main.get_dealers().len();
let expected_weight = /* calculate from dealers_count and config */;
ensure!(
    trx.main.V.len() <= expected_weight + 1,
    "transcript V array oversized"
);
// Similar checks for other arrays
```

The recommended approach is Option 1 or 2, as they prevent deserialization entirely.

## Proof of Concept

```rust
#[test]
fn test_oversized_transcript_dos() {
    use aptos_types::dkg::{DKGTranscript, DKGTranscriptMetadata};
    use blstrs::{G1Projective, G2Projective};
    use group::Group;
    
    // Create a transcript with oversized arrays
    let mut malicious_transcript = Transcript {
        soks: vec![],
        R: vec![G1Projective::generator(); 500_000], // 500k points
        R_hat: vec![G2Projective::generator(); 500_000],
        V: vec![G1Projective::generator(); 500_001],
        V_hat: vec![G2Projective::generator(); 500_001],
        C: vec![G1Projective::generator(); 500_000],
    };
    
    let transcript_bytes = bcs::to_bytes(&Transcripts {
        main: malicious_transcript,
        fast: None,
    }).unwrap();
    
    println!("Malicious transcript size: {} bytes", transcript_bytes.len());
    
    let dkg_transcript = DKGTranscript {
        metadata: DKGTranscriptMetadata {
            epoch: 1,
            author: AccountAddress::ZERO,
        },
        transcript_bytes,
    };
    
    // This will consume significant CPU during deserialization
    let start = std::time::Instant::now();
    let result = bcs::from_bytes::<Transcripts>(&dkg_transcript.transcript_bytes);
    let elapsed = start.elapsed();
    
    println!("Deserialization took: {:?}", elapsed);
    // Expect: several seconds for 500k points
    
    // The transcript will eventually be rejected by verify_transcript,
    // but only after expensive deserialization
}
```

**Notes**

The vulnerability exploits the gap between lightweight preliminary validation (`verify_transcript_extra`) and comprehensive cryptographic validation (`verify_transcript`), with expensive deserialization happening in between. The fix requires moving size validation before deserialization to prevent resource exhaustion attacks during DKG transcript aggregation.

### Citations

**File:** dkg/src/transcript_aggregation/mod.rs (L88-101)
```rust
        let transcript = bcs::from_bytes(transcript_bytes.as_slice()).map_err(|e| {
            anyhow!("[DKG] adding peer transcript failed with trx deserialization error: {e}")
        })?;
        let mut trx_aggregator = self.trx_aggregator.lock();
        if trx_aggregator.contributors.contains(&metadata.author) {
            return Ok(None);
        }

        S::verify_transcript_extra(&transcript, &self.epoch_state.verifier, false, Some(sender))
            .context("extra verification failed")?;

        S::verify_transcript(&self.dkg_pub_params, &transcript).map_err(|e| {
            anyhow!("[DKG] adding peer transcript failed with trx verification failure: {e}")
        })?;
```

**File:** types/src/dkg/real_dkg/mod.rs (L295-329)
```rust
    fn verify_transcript_extra(
        trx: &Self::Transcript,
        verifier: &ValidatorVerifier,
        checks_voting_power: bool,
        ensures_single_dealer: Option<AccountAddress>,
    ) -> anyhow::Result<()> {
        let all_validator_addrs = verifier.get_ordered_account_addresses();
        let main_trx_dealers = trx.main.get_dealers();
        let mut dealer_set = HashSet::with_capacity(main_trx_dealers.len());
        for dealer in main_trx_dealers.iter() {
            if let Some(dealer_addr) = all_validator_addrs.get(dealer.id) {
                dealer_set.insert(*dealer_addr);
            } else {
                bail!("invalid dealer idx");
            }
        }
        ensure!(main_trx_dealers.len() == dealer_set.len());
        if ensures_single_dealer.is_some() {
            let expected_dealer_set: HashSet<AccountAddress> =
                ensures_single_dealer.into_iter().collect();
            ensure!(expected_dealer_set == dealer_set);
        }

        if checks_voting_power {
            verifier
                .check_voting_power(dealer_set.iter(), true)
                .context("not enough power")?;
        }

        if let Some(fast_trx) = &trx.fast {
            ensure!(fast_trx.get_dealers() == main_trx_dealers);
            ensure!(trx.main.get_dealt_public_key() == fast_trx.get_dealt_public_key());
        }
        Ok(())
    }
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L48-72)
```rust
#[derive(Serialize, Deserialize, Clone, Debug, PartialEq, Eq, BCSCryptoHash, CryptoHasher)]
#[allow(non_snake_case)]
pub struct Transcript {
    /// Proofs-of-knowledge (PoKs) for the dealt secret committed in $c = g_2^{p(0)}$.
    /// Since the transcript could have been aggregated from other transcripts with their own
    /// committed secrets in $c_i = g_2^{p_i(0)}$, this is a vector of PoKs for all these $c_i$'s
    /// such that $\prod_i c_i = c$.
    ///
    /// Also contains BLS signatures from each player $i$ on that player's contribution $c_i$, the
    /// player ID $i$ and auxiliary information `aux[i]` provided during dealing.
    soks: Vec<SoK<G1Projective>>,
    /// Commitment to encryption randomness $g_1^{r_j} \in G_1, \forall j \in [W]$
    R: Vec<G1Projective>,
    /// Same as $R$ except uses $g_2$.
    R_hat: Vec<G2Projective>,
    /// First $W$ elements are commitments to the evaluations of $p(X)$: $g_1^{p(\omega^i)}$,
    /// where $i \in [W]$. Last element is $g_1^{p(0)}$ (i.e., the dealt public key).
    V: Vec<G1Projective>,
    /// Same as $V$ except uses $g_2$.
    V_hat: Vec<G2Projective>,
    /// ElGamal encryption of the $j$th share of player $i$:
    /// i.e., $C[s_i+j-1] = h_1^{p(\omega^{s_i + j - 1})} ek_i^{r_j}, \forall i \in [n], j \in [w_i]$.
    /// We sometimes denote $C[s_i+j-1]$ by C_{i, j}.
    C: Vec<G1Projective>,
}
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L82-90)
```rust
impl TryFrom<&[u8]> for Transcript {
    type Error = CryptoMaterialError;

    fn try_from(bytes: &[u8]) -> Result<Self, Self::Error> {
        // NOTE: The `serde` implementation in `blstrs` already performs the necessary point validation
        // by ultimately calling `GroupEncoding::from_bytes`.
        bcs::from_bytes::<Transcript>(bytes).map_err(|_| CryptoMaterialError::DeserializationError)
    }
}
```

**File:** crates/aptos-dkg/src/pvss/das/weighted_protocol.rs (L415-455)
```rust
    fn check_sizes(&self, sc: &WeightedConfigBlstrs) -> anyhow::Result<()> {
        let W = sc.get_total_weight();

        if self.V.len() != W + 1 {
            bail!(
                "Expected {} G_2 (polynomial) commitment elements, but got {}",
                W + 1,
                self.V.len()
            );
        }

        if self.V_hat.len() != W + 1 {
            bail!(
                "Expected {} G_2 (polynomial) commitment elements, but got {}",
                W + 1,
                self.V_hat.len()
            );
        }

        if self.R.len() != W {
            bail!(
                "Expected {} G_1 commitment(s) to ElGamal randomness, but got {}",
                W,
                self.R.len()
            );
        }

        if self.R_hat.len() != W {
            bail!(
                "Expected {} G_2 commitment(s) to ElGamal randomness, but got {}",
                W,
                self.R_hat.len()
            );
        }

        if self.C.len() != W {
            bail!("Expected C of length {}, but got {}", W, self.C.len());
        }

        Ok(())
    }
```

**File:** config/src/config/network_config.rs (L50-50)
```rust
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```
