# Audit Report

## Title
Double-Counting of PROCESSOR_SUCCESSES Metrics During Transaction Reprocessing in Aptos Indexer

## Summary
The Aptos indexer's Prometheus metrics counters (`PROCESSOR_SUCCESSES`, `PROCESSOR_INVOCATIONS`, `PROCESSOR_ERRORS`) are unconditionally incremented during transaction processing, without checking if a transaction version has already been processed. When transactions are reprocessed after manual rollback or via the `starting_version` configuration override, these counters are incremented again for the same transaction versions, causing metric inflation and incorrect success rate calculations.

## Finding Description

The indexer uses Prometheus `IntCounter` metrics to track processing statistics. [1](#0-0) 

When a transaction batch is processed successfully, the flow is:
1. `process_transactions_with_status()` is called [2](#0-1) 
2. This unconditionally increments `PROCESSOR_INVOCATIONS` [3](#0-2) 
3. On success, `update_status_success()` is called, which unconditionally increments `PROCESSOR_SUCCESSES` [4](#0-3) 
4. The database is updated via upsert on the `(name, version)` primary key [5](#0-4) 

The `processor_statuses` table has a composite primary key on `(name, version)`, ensuring database idempotency. [6](#0-5) 

However, operators can manually override the starting version through configuration, forcing reprocessing of already-processed transactions: [7](#0-6) 

When `starting_version` is set to an earlier value than the database's tracked progress, the indexer will re-fetch and re-process those transactions: [8](#0-7) 

The critical issue is that the Prometheus counters are **monotonically increasing** - they can only go up, never down. When the same transaction version is reprocessed, the counter increments again even though the database correctly handles the duplicate via upsert. This creates a permanent divergence between:
- Counter values (which accumulate duplicates)
- Actual unique processed versions (tracked correctly in the database)

## Impact Explanation

This is a **Medium Severity** issue for the following reasons:

1. **Metric Corruption**: Success rate calculations (e.g., `PROCESSOR_SUCCESSES / (PROCESSOR_SUCCESSES + PROCESSOR_ERRORS)`) become inflated and unreliable after any reprocessing event.

2. **Operational Impact**: Operators rely on these metrics for:
   - Capacity planning and resource allocation
   - SLA monitoring and alerting
   - Performance analysis and troubleshooting
   - Incident response and root cause analysis

3. **Cascading Effects**: Incorrect metrics can lead to:
   - False-positive alerts triggering unnecessary interventions
   - Misleading dashboards causing incorrect operational decisions
   - Difficulty diagnosing real issues when baseline metrics are corrupted
   - Wasted engineering time investigating phantom problems

4. **State Inconsistencies**: This represents a state inconsistency between the monitoring layer (Prometheus) and the data layer (PostgreSQL), requiring manual intervention to reconcile.

Per the Aptos bug bounty criteria, this qualifies as **Medium Severity** - "State inconsistencies requiring intervention" and impacts the observability infrastructure critical for production operations.

## Likelihood Explanation

**High Likelihood** - This will occur in any of the following common scenarios:

1. **Manual Rollback Recovery**: When recovering from data corruption or migration issues, operators commonly set `starting_version` to reprocess transactions, triggering double-counting immediately.

2. **Configuration Errors**: Accidental misconfiguration of `STARTING_VERSION` environment variable in deployment scripts will cause reprocessing.

3. **Disaster Recovery**: During disaster recovery procedures, operators may need to restore from an earlier database snapshot and reprocess recent transactions.

4. **Gap Backfilling**: When gaps are detected in processing, operators may manually reprocess ranges of transactions.

The test suite even demonstrates idempotent reprocessing is expected: [9](#0-8) 

However, the test only validates database idempotency, not metric correctness.

## Recommendation

Implement version-based deduplication before incrementing counters. Add a check to determine if a version has already been successfully processed:

**Solution 1: Query Database Before Incrementing**
```rust
fn update_status_success(&self, processing_result: &ProcessingResult) {
    // Check if versions are already successfully processed
    let already_processed = self.check_versions_already_processed(
        processing_result.start_version,
        processing_result.end_version
    );
    
    if !already_processed {
        PROCESSOR_SUCCESSES.with_label_values(&[self.name()]).inc();
    }
    
    LATEST_PROCESSED_VERSION
        .with_label_values(&[self.name()])
        .set(processing_result.end_version as i64);
    // ... rest of implementation
}
```

**Solution 2: Use Gauge Instead of Counter**
Replace the counter with a gauge that tracks the current highest successfully processed version: [1](#0-0) 

Change from `IntCounterVec` to use only `LATEST_PROCESSED_VERSION` gauge for success tracking, and derive counts from version ranges rather than event counts.

**Solution 3: Add Reprocessing Flag**
Add a configuration flag that indicates reprocessing mode and prevents counter increments: [7](#0-6) 

Add a `reprocessing_mode` boolean that, when true, skips counter increments while still updating the database.

## Proof of Concept

```rust
// Reproduction steps:
// 1. Start indexer and process transactions 0-1000
// 2. Observe PROCESSOR_SUCCESSES = 1 (one batch of 1000 txns)
// 3. Set STARTING_VERSION=0 and restart indexer
// 4. Wait for reprocessing of transactions 0-1000
// 5. Observe PROCESSOR_SUCCESSES = 2 (double-counted)
// 6. Query processor_statuses table shows only 1 record per version
// 7. Metric divergence confirmed

use crate::counters::PROCESSOR_SUCCESSES;

#[test]
fn test_double_counting_on_reprocess() {
    // Setup: Process transactions once
    let processor = setup_test_processor();
    let txns = generate_test_transactions(0, 100);
    
    processor.process_transactions_with_status(txns.clone()).await.unwrap();
    let count_first = PROCESSOR_SUCCESSES.with_label_values(&["test"]).get();
    assert_eq!(count_first, 1); // One batch processed
    
    // Reprocess same transactions (simulating starting_version override)
    processor.process_transactions_with_status(txns).await.unwrap();
    let count_second = PROCESSOR_SUCCESSES.with_label_values(&["test"]).get();
    assert_eq!(count_second, 2); // BUG: Counter incremented again!
    
    // Database shows correct idempotency
    let db_count = query_processor_statuses_count(&processor.name(), 0, 100);
    assert_eq!(db_count, 101); // 101 unique versions (0-100 inclusive)
    
    // Metric divergence confirmed
    assert_ne!(count_second as usize, db_count);
}
```

**Notes**

This vulnerability specifically affects the PostgreSQL-based indexer in `crates/indexer/`, not the storage-layer indexer in `storage/indexer/`. The issue is isolated to the metrics layer and does not affect actual transaction data integrity, state commitment, or consensus operations. However, it significantly degrades the reliability of operational monitoring, which is critical for maintaining production systems. The fix should be implemented before any production deployment that relies on these metrics for alerting or capacity planning.

### Citations

**File:** crates/indexer/src/counters.rs (L31-38)
```rust
pub static PROCESSOR_SUCCESSES: Lazy<IntCounterVec> = Lazy::new(|| {
    register_int_counter_vec!(
        "indexer_processor_success_count",
        "Number of times a given processor has completed successfully",
        &["processor_name"]
    )
    .unwrap()
});
```

**File:** crates/indexer/src/indexer/transaction_processor.rs (L66-91)
```rust
    async fn process_transactions_with_status(
        &self,
        txns: Vec<Transaction>,
    ) -> Result<ProcessingResult, TransactionProcessingError> {
        assert!(
            !txns.is_empty(),
            "Must provide at least one transaction to this function"
        );
        PROCESSOR_INVOCATIONS
            .with_label_values(&[self.name()])
            .inc();

        let start_version = txns.first().unwrap().version().unwrap();
        let end_version = txns.last().unwrap().version().unwrap();

        self.mark_versions_started(start_version, end_version);
        let res = self
            .process_transactions(txns, start_version, end_version)
            .await;
        // Handle block success/failure
        match res.as_ref() {
            Ok(processing_result) => self.update_status_success(processing_result),
            Err(tpe) => self.update_status_err(tpe),
        };
        res
    }
```

**File:** crates/indexer/src/indexer/transaction_processor.rs (L112-131)
```rust
    fn update_status_success(&self, processing_result: &ProcessingResult) {
        aptos_logger::debug!(
            "[{}] Marking processing version OK from versions {} to {}",
            self.name(),
            processing_result.start_version,
            processing_result.end_version
        );
        PROCESSOR_SUCCESSES.with_label_values(&[self.name()]).inc();
        LATEST_PROCESSED_VERSION
            .with_label_values(&[self.name()])
            .set(processing_result.end_version as i64);
        let psms = ProcessorStatusModel::from_versions(
            self.name(),
            processing_result.start_version,
            processing_result.end_version,
            true,
            None,
        );
        self.apply_processor_status(&psms);
    }
```

**File:** crates/indexer/src/indexer/transaction_processor.rs (L146-165)
```rust
    fn apply_processor_status(&self, psms: &[ProcessorStatusModel]) {
        let mut conn = self.get_conn();
        let chunks = get_chunks(psms.len(), ProcessorStatusModel::field_count());
        for (start_ind, end_ind) in chunks {
            execute_with_better_error(
                &mut conn,
                diesel::insert_into(processor_statuses::table)
                    .values(&psms[start_ind..end_ind])
                    .on_conflict((dsl::name, dsl::version))
                    .do_update()
                    .set((
                        dsl::success.eq(excluded(dsl::success)),
                        dsl::details.eq(excluded(dsl::details)),
                        dsl::last_updated.eq(excluded(dsl::last_updated)),
                    )),
                None,
            )
            .expect("Error updating Processor Status!");
        }
    }
```

**File:** crates/indexer/migrations/2022-08-08-043603_core_tables/up.sql (L306-314)
```sql
CREATE TABLE processor_statuses (
  name VARCHAR(50) NOT NULL,
  version BIGINT NOT NULL,
  success BOOLEAN NOT NULL,
  details TEXT,
  last_updated TIMESTAMP NOT NULL DEFAULT NOW(),
  -- Constraints
  PRIMARY KEY (name, version)
);
```

**File:** config/src/config/indexer_config.rs (L43-47)
```rust
    /// If set, will ignore database contents and start processing from the specified version.
    /// This will not delete any database contents, just transactions as it reprocesses them.
    /// Alternatively can set the `STARTING_VERSION` env var
    #[serde(default, skip_serializing_if = "Option::is_none")]
    pub starting_version: Option<u64>,
```

**File:** crates/indexer/src/runtime.rs (L173-176)
```rust
    let start_version = match config.starting_version {
        None => starting_version_from_db_short,
        Some(version) => version,
    };
```

**File:** crates/indexer/src/indexer/tailer.rs (L815-825)
```rust
        // We run it twice to ensure we don't explode. Idempotency!
        tailer
            .processor
            .process_transactions_with_status(vec![user_txn.clone()])
            .await
            .unwrap();
        tailer
            .processor
            .process_transactions_with_status(vec![user_txn.clone()])
            .await
            .unwrap();
```
