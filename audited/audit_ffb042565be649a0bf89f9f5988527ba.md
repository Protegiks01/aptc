# Audit Report

## Title
Mempool Failover Bucket Assignment Silent Failure Causes Transaction Propagation Loss

## Summary
The `update_sender_bucket_for_peers()` function in `mempool/src/shared_mempool/priority.rs` contains a logic bug in the failover bucket assignment loop that causes failover buckets to remain unassigned when all available peers already have a bucket. This silently breaks the mempool's redundancy mechanism, potentially halting transaction propagation when primary peers become unavailable.

## Finding Description

The vulnerability exists in the nested loop structure for failover bucket assignment: [1](#0-0) 

The inner loop (line 417) iterates up to `num_prioritized_peers` times attempting to find a peer that doesn't already have the current bucket assigned. However, if all peers already have that bucket (either as Primary from the first assignment phase, or as Failover from a previous iteration), the loop exits **without assigning the failover bucket** and **without logging any error or warning**.

**Attack Scenario - Single Peer:**

Configuration (default for PFNs):
- `num_sender_buckets = 4` [2](#0-1) 

- `default_failovers = 1` [3](#0-2) 

With only 1 upstream peer:

**Step 1 - Primary Assignment** (lines 401-409): Peer 0 receives buckets [0, 1, 2, 3] all with Primary priority

**Step 2 - Failover Assignment** (lines 414-430):
- For bucket 0: Check peer 0 (already has it as Primary) → skip → inner loop completes after 1 iteration → **bucket 0 failover NOT assigned**
- For bucket 1: Same issue → **bucket 1 failover NOT assigned**
- For bucket 2: Same issue → **bucket 2 failover NOT assigned**  
- For bucket 3: Same issue → **bucket 3 failover NOT assigned**

**Result**: Zero failover buckets assigned despite `default_failovers = 1`.

When the broadcast logic tries to send transactions: [4](#0-3) 

If peer 0 becomes unreachable and no other peer has been assigned sender buckets, the broadcast fails with `BroadcastError::PeerNotPrioritized`, completely halting transaction propagation for this node.

**Attack Scenario - Multiple Failovers:**

With 2 peers and `default_failovers = 2`:

**After Primary**: Peer 0 has [0, 2] Primary, Peer 1 has [1, 3] Primary  
**After 1st Failover**: Peer 0 has [0, 1, 2, 3], Peer 1 has [0, 1, 2, 3] (all buckets covered)  
**During 2nd Failover**: All peers already have all buckets → **NO 2nd failover buckets assigned**

This breaks multi-level redundancy configurations.

## Impact Explanation

**High Severity** - This vulnerability qualifies as High Severity under the Aptos bug bounty program due to:

1. **Significant Protocol Violation**: The mempool is designed with redundancy through failover peers. The test suite expects failover buckets to exist for all sender buckets: [5](#0-4) 

However, the bug silently breaks this invariant in low-connectivity scenarios.

2. **Network Liveness Impact**: When failover assignment fails and primary peers become unavailable, affected nodes cannot propagate transactions, degrading network liveness and transaction throughput. This is especially critical during network partitions or peer churn.

3. **Silent Failure**: No error logging or validation occurs when failover assignment fails, making the issue extremely difficult to detect and debug in production environments.

4. **Validator Node Impact**: While validators are optimized to use `num_sender_buckets = 1` and `default_failovers = 0`, VFNs and PFNs are vulnerable. VFN transaction propagation issues can cascade to validators, causing "validator node slowdowns" (High severity criterion).

## Likelihood Explanation

**High Likelihood** - This bug manifests under common operational conditions:

1. **Low Connectivity Scenarios**: Nodes with poor network connectivity, during bootstrap, or in network partitions often have 1-2 upstream peers
2. **VFN Configuration**: VFNs are optimized to `default_failovers = 0`, which suggests awareness of connectivity constraints, but PFNs maintain `default_failovers = 1`
3. **Load Balancing Thresholds**: The system attempts to load balance across multiple peers based on traffic, but the bug prevents proper failover assignment even when intended: [6](#0-5) 

4. **No Guards**: The code has assertions checking peer count matches: [7](#0-6) 

But no validation that failover assignment succeeded.

## Recommendation

Add validation after the failover assignment loop to detect and handle assignment failures:

```rust
// After the failover assignment loops (after line 430)
// Validate that all expected failover assignments succeeded
for failover_level in 0..self.mempool_config.default_failovers {
    for bucket_index in 0..self.mempool_config.num_sender_buckets {
        let peers_with_bucket = self.peer_to_sender_buckets.iter()
            .filter(|(_, buckets)| buckets.contains_key(&bucket_index))
            .count();
        
        let expected_peers = min(
            1 + failover_level + 1, // Primary + failover levels
            num_prioritized_peers
        );
        
        if peers_with_bucket < expected_peers {
            warn!(
                "Failed to assign failover level {} for bucket {}. \
                 Expected {} peers but only {} assigned. \
                 Available peers: {}, Configuration: default_failovers={}, num_sender_buckets={}",
                failover_level,
                bucket_index,
                expected_peers,
                peers_with_bucket,
                num_prioritized_peers,
                self.mempool_config.default_failovers,
                self.mempool_config.num_sender_buckets
            );
        }
    }
}
```

Alternatively, modify the inner loop to skip assignment attempts when impossible:

```rust
// Before starting failover assignments
let max_achievable_failovers = min(
    self.mempool_config.default_failovers,
    num_prioritized_peers.saturating_sub(1) // At least 1 peer needed beyond primary
);

for failover_idx in 0..max_achievable_failovers {
    // ... rest of failover logic
}
```

## Proof of Concept

```rust
#[test]
fn test_failover_assignment_failure_with_single_peer() {
    use aptos_config::config::{MempoolConfig, NodeType};
    use aptos_peer_monitoring_service_types::PeerMonitoringMetadata;
    use aptos_time_service::TimeService;
    use crate::shared_mempool::priority::PrioritizedPeersState;
    
    // Create config with default failover settings
    let mempool_config = MempoolConfig {
        num_sender_buckets: 4,
        default_failovers: 1,
        ..MempoolConfig::default()
    };
    
    let mut prioritized_peers_state = PrioritizedPeersState::new(
        mempool_config.clone(),
        NodeType::PublicFullnode,
        TimeService::mock(),
    );
    
    // Create only a single peer
    let peer_metadata = create_metadata_with_distance_and_latency(1, 0.5);
    let peer = (create_public_peer(), Some(&peer_metadata));
    
    // Update with single peer
    prioritized_peers_state.update_prioritized_peers(
        vec![peer],
        1000,
        1000
    );
    
    // Check failover bucket assignment
    let peer_buckets = prioritized_peers_state
        .get_sender_buckets_for_peer(&peer.0)
        .expect("Peer should have sender buckets");
    
    // Count failover vs primary buckets
    let (primary_count, failover_count) = peer_buckets.iter()
        .fold((0, 0), |(p, f), (_, priority)| {
            match priority {
                BroadcastPeerPriority::Primary => (p + 1, f),
                BroadcastPeerPriority::Failover => (p, f + 1),
            }
        });
    
    println!("Primary buckets: {}, Failover buckets: {}", primary_count, failover_count);
    
    // BUG: With only 1 peer, failover_count will be 0 instead of expected 4
    // Expected: Each of 4 buckets should have 1 Primary + 1 Failover = 4 failover total
    // Actual: Only Primary assignments succeed, failover_count = 0
    assert_eq!(primary_count, 4, "Should have 4 primary buckets");
    assert_eq!(failover_count, 4, "BUG: Should have 4 failover buckets but has 0!");
}
```

This test will **fail** due to the bug, demonstrating that failover buckets are not assigned when only a single peer is available, despite `default_failovers = 1` configuration requiring them.

### Citations

**File:** mempool/src/shared_mempool/priority.rs (L280-280)
```rust
        assert!(self.prioritized_peers.read().len() == peer_monitoring_data.len());
```

**File:** mempool/src/shared_mempool/priority.rs (L304-317)
```rust
        let threshold_config = self
            .mempool_config
            .load_balancing_thresholds
            .clone()
            .into_iter()
            .rev()
            .find(|threshold_config| {
                threshold_config.avg_mempool_traffic_threshold_in_tps
                    <= max(
                        average_mempool_traffic_observed as u64,
                        average_committed_traffic_observed as u64,
                    )
            })
            .unwrap_or_default();
```

**File:** mempool/src/shared_mempool/priority.rs (L411-430)
```rust
            // Assign sender buckets with Failover priority. Use Round Robin.
            peer_index = 0;
            let num_prioritized_peers = self.prioritized_peers.read().len();
            for _ in 0..self.mempool_config.default_failovers {
                for bucket_index in 0..self.mempool_config.num_sender_buckets {
                    // Find the first peer that already doesn't have the sender bucket, and add the bucket
                    for _ in 0..num_prioritized_peers {
                        let peer = self.prioritized_peers.read()[peer_index];
                        let sender_bucket_list =
                            self.peer_to_sender_buckets.entry(peer).or_default();
                        if let std::collections::hash_map::Entry::Vacant(e) =
                            sender_bucket_list.entry(bucket_index)
                        {
                            e.insert(BroadcastPeerPriority::Failover);
                            break;
                        }
                        peer_index = (peer_index + 1) % num_prioritized_peers;
                    }
                }
            }
```

**File:** mempool/src/shared_mempool/priority.rs (L977-985)
```rust
        // There is exists a peer with failover priority for each bucket
        for bucket in 0..num_sender_buckets {
            assert!(prioritized_peers_state.peer_to_sender_buckets.iter().any(
                |(_, sender_buckets)| {
                    sender_buckets.contains_key(&bucket)
                        && sender_buckets.get(&bucket).unwrap() == &BroadcastPeerPriority::Failover
                }
            ));
        }
```

**File:** config/src/config/mempool_config.rs (L124-124)
```rust
            default_failovers: 1,
```

**File:** config/src/config/mempool_config.rs (L137-137)
```rust
            num_sender_buckets: 4,
```

**File:** mempool/src/shared_mempool/network.rs (L502-509)
```rust
                            self.prioritized_peers_state
                                .get_sender_buckets_for_peer(&peer)
                                .ok_or_else(|| {
                                    BroadcastError::PeerNotPrioritized(
                                        peer,
                                        self.prioritized_peers_state.get_peer_priority(&peer),
                                    )
                                })?
```
