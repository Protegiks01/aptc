# Audit Report

## Title
Non-Atomic Batch Writes in Event Pruner Cause API Failures and False Corruption Errors

## Summary
The EventStorePruner performs two separate database write operations to different RocksDB instances without transaction isolation, creating a race condition window where concurrent API queries can observe partial pruning state, resulting in false "DB corruption" errors and API service disruption.

## Finding Description

The vulnerability exists in the EventStorePruner's `prune()` method, which executes two non-atomic batch writes when the internal indexer database is enabled. [1](#0-0) 

These operations write to two separate RocksDB instances:

1. **First write (line 76-78)**: Writes to the internal indexer database, deleting event index entries from `EventByKeySchema` and `EventByVersionSchema`. The `prune_event_indices` method populates the indexer_batch with deletion operations. [2](#0-1) 

2. **Second write (line 80)**: Writes to the event database, deleting actual event data from `EventSchema`. [3](#0-2) 

**Critical Configuration Requirement**: The internal indexer REQUIRES database sharding to be enabled, as enforced by the config sanitizer. [4](#0-3) 

**API Routing**: When database sharding is enabled, the API routes event queries to the indexer reader instead of the main database. [5](#0-4) 

**Race Condition Window**: Between the two write operations, there exists a critical race condition window where:
- Event indices have been deleted from the indexer DB (committed at line 78)
- Event data still exists in the event DB (not yet committed at line 80)

During this window, API queries read from the indexer database and encounter missing index entries. The `lookup_events_by_key` method has a sequence discontinuity check that throws an error when it detects gaps in sequence numbers. [6](#0-5) 

**Attack Path**:
1. EventStorePruner begins pruning events containing sequence numbers 50-54
2. Line 76-78 executes: Index entries for seq 50-54 are deleted from indexer DB and committed
3. **RACE WINDOW**: Before line 80 executes
4. User calls `/accounts/:address/events/:creation_number?start=48&limit=10` [7](#0-6) 
5. API invokes `get_events` which routes to indexer reader [8](#0-7) 
6. IndexerDB's `lookup_events_by_key` seeks indices for seq 48, 49 (found), then seq 50 (missing - deleted)
7. Iterator jumps to seq 55 (next available after pruned range)
8. Sequence discontinuity detected at line 232: expected 50, found 55
9. Error returned: "DB corruption: Sequence number not continuous."

**No Protection Mechanism**: Unlike other ledger read operations that check `error_if_ledger_pruned` before accessing data, the `get_events_by_event_key` method does NOT perform this check. [9](#0-8) 

For comparison, other operations like `get_transaction_iterator` properly check for pruned data. [10](#0-9) 

**Parallel Execution**: The pruner system executes sub-pruners in parallel without synchronization with readers. [11](#0-10) 

## Impact Explanation

**Severity: High** - This qualifies as "API crashes" under the High severity category (up to $50,000 per the Aptos bug bounty program).

**Impact:**
- **Service Disruption**: Public event query APIs return 500 Internal Server Error during pruning operations due to the `bail!` macro throwing unrecoverable errors
- **False Corruption Alerts**: Error messages incorrectly indicate "DB corruption: Sequence number not continuous." when the database is functioning normally - this is simply a race condition, not actual corruption
- **Application Failures**: Client applications dependent on event queries will crash or enter error states when receiving these errors
- **Operational Confusion**: Node operators may perform unnecessary database recovery procedures based on false corruption errors, wasting resources and potentially causing additional downtime
- **Availability Impact**: During active pruning (which occurs regularly based on configured prune windows), event querying becomes unreliable

The vulnerability affects all nodes with:
1. Pruning enabled (standard for production nodes to manage disk space)
2. Internal indexer enabled (required for API nodes serving event queries with DB sharding)
3. DB sharding enabled (enforced requirement for internal indexer)

## Likelihood Explanation

**Likelihood: High**

This vulnerability occurs naturally during normal operations:

1. **Frequent Trigger**: Pruning runs automatically based on the configured prune window, which for production nodes is typically continuous or daily to manage disk usage
2. **Wide Race Window**: The window between the two database writes depends on RocksDB write latency, which can be milliseconds to seconds under load - sufficient time for multiple API requests to land in the window
3. **No Special Access Required**: Any external user calling the public `/accounts/:address/events/:creation_number` API endpoint can observe the error
4. **High Traffic Scenarios**: Production API nodes serving many concurrent requests have high probability of queries landing in the race window during each pruning cycle
5. **No Synchronization**: There are no locks, reader-writer coordination, or transaction isolation mechanisms protecting readers from observing partial state during the two-phase write

The pruner manager tracks minimum readable versions but the event query path does not consult this value before reading, unlike other ledger operations.

## Recommendation

Implement atomic pruning operations by either:

1. **Option A - Atomic Transaction**: Extend RocksDB transaction support to span both databases, ensuring both writes commit atomically or neither commits
2. **Option B - Reader Protection**: Add `error_if_ledger_pruned` check in `get_events_by_event_key` before calling `lookup_events_by_key`, similar to other ledger read operations:

```rust
pub(super) fn get_events_by_event_key(
    &self,
    event_key: &EventKey,
    start_seq_num: u64,
    order: Order,
    limit: u64,
    ledger_version: Version,
) -> Result<Vec<EventWithVersion>> {
    ensure!(
        !self.state_kv_db.enabled_sharding(),
        "This API is deprecated for sharded DB"
    );
    error_if_too_many_requested(limit, MAX_REQUEST_LIMIT)?;
    
    // Add pruning check before querying indices
    self.error_if_ledger_pruned("Event", start_seq_num)?;
    
    // ... rest of implementation
}
```

3. **Option C - Write Order Reversal**: Write to the event database first, then the indexer database, so queries encounter "event not found" errors (expected) rather than "DB corruption" errors (misleading)

Option B is recommended as it provides immediate protection without architectural changes.

## Proof of Concept

```rust
// Simulated race condition test demonstrating the vulnerability
// This would need to be integrated into the existing test suite

#[tokio::test]
async fn test_event_pruner_race_condition() {
    let (db, indexer_db) = setup_test_dbs_with_events();
    let pruner = EventStorePruner::new(db.clone(), 0, Some(indexer_db.clone())).unwrap();
    
    // Start pruning in background
    let pruner_handle = tokio::spawn(async move {
        pruner.prune(0, 100).unwrap();
    });
    
    // Simulate API query landing in race window
    tokio::time::sleep(Duration::from_millis(1)).await;
    
    let api_result = db.get_events_by_event_key(
        &test_event_key(),
        48,
        Order::Ascending,
        10,
        latest_version
    );
    
    // Expected: Error "DB corruption: Sequence number not continuous."
    assert!(api_result.is_err());
    assert!(api_result.unwrap_err().to_string().contains("not continuous"));
    
    pruner_handle.await.unwrap();
}
```

## Notes

This vulnerability represents a classic TOCTOU (Time-Of-Check-Time-Of-Use) race condition in distributed systems where multi-phase operations lack atomicity guarantees. The impact is amplified because the error message misleads operators into believing there is database corruption when it's actually a synchronization issue.

The root cause is architectural: the decision to split event indices and event data across two separate RocksDB instances for performance reasons introduced a consistency boundary that is not properly protected during pruning operations.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L43-81)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let mut indexer_batch = None;

        let indices_batch = if let Some(indexer_db) = self.indexer_db() {
            if indexer_db.event_enabled() {
                indexer_batch = Some(SchemaBatch::new());
            }
            indexer_batch.as_mut()
        } else {
            Some(&mut batch)
        };
        let num_events_per_version = self.ledger_db.event_db().prune_event_indices(
            current_progress,
            target_version,
            indices_batch,
        )?;
        self.ledger_db.event_db().prune_events(
            num_events_per_version,
            current_progress,
            target_version,
            &mut batch,
        )?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        if let Some(mut indexer_batch) = indexer_batch {
            indexer_batch.put::<InternalIndexerMetadataSchema>(
                &IndexerMetadataKey::EventPrunerProgress,
                &IndexerMetadataValue::Version(target_version),
            )?;
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L192-222)
```rust
    pub(crate) fn prune_event_indices(
        &self,
        start: Version,
        end: Version,
        mut indices_batch: Option<&mut SchemaBatch>,
    ) -> Result<Vec<usize>> {
        let mut ret = Vec::new();

        let mut current_version = start;

        for events in self.get_events_by_version_iter(start, (end - start) as usize)? {
            let events = events?;
            ret.push(events.len());

            if let Some(ref mut batch) = indices_batch {
                for event in events {
                    if let ContractEvent::V1(v1) = event {
                        batch.delete::<EventByKeySchema>(&(*v1.key(), v1.sequence_number()))?;
                        batch.delete::<EventByVersionSchema>(&(
                            *v1.key(),
                            current_version,
                            v1.sequence_number(),
                        ))?;
                    }
                }
            }
            current_version += 1;
        }

        Ok(ret)
    }
```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L225-243)
```rust
    pub(crate) fn prune_events(
        &self,
        num_events_per_version: Vec<usize>,
        start: Version,
        end: Version,
        db_batch: &mut SchemaBatch,
    ) -> Result<()> {
        let mut current_version = start;

        for num_events in num_events_per_version {
            for idx in 0..num_events {
                db_batch.delete::<EventSchema>(&(current_version, idx as u64))?;
            }
            current_version += 1;
        }
        self.event_store
            .prune_event_accumulator(start, end, db_batch)?;
        Ok(())
    }
```

**File:** config/src/config/internal_indexer_db_config.rs (L82-103)
```rust
impl ConfigSanitizer for InternalIndexerDBConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = node_config.indexer_db_config;

        // Shouldn't turn on internal indexer for db without sharding
        if !node_config.storage.rocksdb_configs.enable_storage_sharding
            && config.is_internal_indexer_db_enabled()
        {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "Don't turn on internal indexer db if DB sharding is off".into(),
            ));
        }

        Ok(())
    }
}
```

**File:** api/src/context.rs (L1084-1110)
```rust
    pub fn get_events(
        &self,
        event_key: &EventKey,
        start: Option<u64>,
        limit: u16,
        ledger_version: u64,
    ) -> Result<Vec<EventWithVersion>> {
        let (start, order) = if let Some(start) = start {
            (start, Order::Ascending)
        } else {
            (u64::MAX, Order::Descending)
        };
        let mut res = if !db_sharding_enabled(&self.node_config) {
            self.db
                .get_events(event_key, start, order, limit as u64, ledger_version)?
        } else {
            self.indexer_reader
                .as_ref()
                .ok_or_else(|| anyhow!("Internal indexer reader doesn't exist"))?
                .get_events(event_key, start, order, limit as u64, ledger_version)?
        };
        if order == Order::Descending {
            res.reverse();
            Ok(res)
        } else {
            Ok(res)
        }
```

**File:** storage/indexer/src/db_indexer.rs (L209-245)
```rust
    pub fn lookup_events_by_key(
        &self,
        event_key: &EventKey,
        start_seq_num: u64,
        limit: u64,
        ledger_version: u64,
    ) -> Result<
        Vec<(
            u64,     // sequence number
            Version, // transaction version it belongs to
            u64,     // index among events for the same transaction
        )>,
    > {
        let mut iter = self.db.iter::<EventByKeySchema>()?;
        iter.seek(&(*event_key, start_seq_num))?;

        let mut result = Vec::new();
        let mut cur_seq = start_seq_num;
        for res in iter.take(limit as usize) {
            let ((path, seq), (ver, idx)) = res?;
            if path != *event_key || ver > ledger_version {
                break;
            }
            if seq != cur_seq {
                let msg = if cur_seq == start_seq_num {
                    "First requested event is probably pruned."
                } else {
                    "DB corruption: Sequence number not continuous."
                };
                bail!("{} expected: {}, actual: {}", msg, cur_seq, seq);
            }
            result.push((seq, ver, idx));
            cur_seq += 1;
        }

        Ok(result)
    }
```

**File:** api/src/events.rs (L41-88)
```rust
    #[oai(
        path = "/accounts/:address/events/:creation_number",
        method = "get",
        operation_id = "get_events_by_creation_number",
        tag = "ApiTags::Events"
    )]
    async fn get_events_by_creation_number(
        &self,
        accept_type: AcceptType,
        /// Hex-encoded 32 byte Aptos account, with or without a `0x` prefix, for
        /// which events are queried. This refers to the account that events were
        /// emitted to, not the account hosting the move module that emits that
        /// event type.
        address: Path<Address>,
        /// Creation number corresponding to the event stream originating
        /// from the given account.
        creation_number: Path<U64>,
        /// Starting sequence number of events.
        ///
        /// If unspecified, by default will retrieve the most recent events
        start: Query<Option<U64>>,
        /// Max number of events to retrieve.
        ///
        /// If unspecified, defaults to default page size
        limit: Query<Option<u16>>,
    ) -> BasicResultWith404<Vec<VersionedEvent>> {
        fail_point_poem("endpoint_get_events_by_event_key")?;
        self.context
            .check_api_output_enabled("Get events by event key", &accept_type)?;
        let page = Page::new(
            start.0.map(|v| v.0),
            limit.0,
            self.context.max_events_page_size(),
        );

        // Ensure that account exists
        let api = self.clone();
        api_spawn_blocking(move || {
            let account = Account::new(api.context.clone(), address.0, None, None, None)?;
            api.list(
                account.latest_ledger_info,
                accept_type,
                page,
                EventKey::new(creation_number.0 .0, address.0.into()),
            )
        })
        .await
    }
```

**File:** storage/indexer/src/indexer_reader.rs (L68-90)
```rust
    fn get_events(
        &self,
        event_key: &EventKey,
        start: u64,
        order: Order,
        limit: u64,
        ledger_version: Version,
    ) -> anyhow::Result<Vec<EventWithVersion>> {
        if let Some(db_indexer_reader) = &self.db_indexer_reader {
            if db_indexer_reader.indexer_db.event_enabled() {
                return Ok(db_indexer_reader.get_events(
                    event_key,
                    start,
                    order,
                    limit,
                    ledger_version,
                )?);
            } else {
                anyhow::bail!("Internal event index is not enabled")
            }
        }
        anyhow::bail!("DB Indexer reader is not available")
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L477-492)
```rust
    fn get_transaction_iterator(
        &self,
        start_version: Version,
        limit: u64,
    ) -> Result<Box<dyn Iterator<Item = Result<Transaction>> + '_>> {
        gauged_api("get_transaction_iterator", || {
            error_if_too_many_requested(limit, MAX_REQUEST_LIMIT)?;
            self.error_if_ledger_pruned("Transaction", start_version)?;

            let iter = self
                .ledger_db
                .transaction_db()
                .get_transaction_iter(start_version, limit as usize)?;
            Ok(Box::new(iter) as Box<dyn Iterator<Item = Result<Transaction>> + '_>)
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L1103-1138)
```rust
    pub(super) fn get_events_by_event_key(
        &self,
        event_key: &EventKey,
        start_seq_num: u64,
        order: Order,
        limit: u64,
        ledger_version: Version,
    ) -> Result<Vec<EventWithVersion>> {
        ensure!(
            !self.state_kv_db.enabled_sharding(),
            "This API is deprecated for sharded DB"
        );
        error_if_too_many_requested(limit, MAX_REQUEST_LIMIT)?;
        let get_latest = order == Order::Descending && start_seq_num == u64::MAX;

        let cursor = if get_latest {
            // Caller wants the latest, figure out the latest seq_num.
            // In the case of no events on that path, use 0 and expect empty result below.
            self.event_store
                .get_latest_sequence_number(ledger_version, event_key)?
                .unwrap_or(0)
        } else {
            start_seq_num
        };

        // Convert requested range and order to a range in ascending order.
        let (first_seq, real_limit) = get_first_seq_num_and_limit(order, cursor, limit)?;

        // Query the index.
        let mut event_indices = self.event_store.lookup_events_by_key(
            event_key,
            first_seq,
            real_limit,
            ledger_version,
        )?;

```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```
