# Audit Report

## Title
Time-Based Validation Inconsistency in Storage Service Request Moderation Causing Unreliable Invalid Request Counts

## Summary
The `can_service()` method in `StorageServerSummary` returns different validation results for identical requests at different times due to time progression and storage state changes. This causes legitimate peers to accumulate invalid request counts for reasons beyond their control, undermining the request moderation security mechanism and potentially causing incorrect peer ignoring during network stress.

## Finding Description

The storage service request moderator validates incoming requests using `StorageServerSummary.can_service()`, which performs both static validation (request format, data availability) and dynamic validation (storage freshness based on current time). [1](#0-0) 

For optimistic fetch and subscription requests, the validation depends on checking if the server's synced ledger info timestamp is within an acceptable lag window from the current time. [2](#0-1) 

The critical issue occurs at this comparison: the validation checks if `ledger_info_timestamp_usecs + max_version_lag_usecs > current_timestamp_usecs`. As time progresses (current timestamp increases), this comparison can transition from `true` to `false` even when the request and cached summary remain unchanged.

**Validation Inconsistency Scenario:**

1. At time T=0s: Peer sends an optimistic fetch request. The server's `synced_ledger_info` has timestamp 0s. Validation: `0 + 20_000_000 > 0` = `true`. Request is valid.

2. At time T=25s: Same peer sends the identical request. The server's `synced_ledger_info` still has timestamp 0s (no new commits during network delay). Validation: `0 + 20_000_000 > 25_000_000` = `false`. Request is now invalid.

3. The invalid request count is incremented for the peer. [3](#0-2) 

**Root Cause:**

The validation mixes two fundamentally different types of checks:
- **Static checks**: Request format correctness, data range availability (peer's responsibility)
- **Dynamic checks**: Server storage freshness (server's responsibility, time-dependent)

Both types of failures increment the same `invalid_request_count`, but dynamic check failures are not the peer's fault. The cached storage summary refreshes periodically but the ledger info timestamp may not update during sync delays. [4](#0-3) 

**Security Impact:**

After accumulating 500 invalid requests, peers on public networks are ignored for 5 minutes with exponential backoff. [5](#0-4) [6](#0-5) [7](#0-6) 

During network partitions or slow sync periods, legitimate peers sending normal requests at regular intervals will repeatedly cross the staleness threshold, accumulating invalid counts through no fault of their own. This undermines the request moderation mechanism's ability to identify truly malicious peers and can cause cascading availability issues.

## Impact Explanation

This vulnerability meets **Medium severity** criteria per the Aptos bug bounty program under "State inconsistencies requiring intervention":

1. **Security Metric Unreliability**: The invalid request count mechanism, designed to identify malicious peers, becomes unreliable when it increments for server-side staleness rather than peer-side misbehavior.

2. **Legitimate Peer Ignoring**: During network stress or sync delays (which are common in distributed systems), well-behaved peers can be incorrectly flagged and ignored, reducing network connectivity when it's most needed.

3. **Operational Impact**: Operators may need to manually intervene to restore peer connectivity or adjust thresholds during extended sync delays, indicating a state inconsistency requiring intervention.

4. **Amplification Effect**: During network partitions, this issue amplifies problems by causing nodes to ignore each other, potentially fragmenting the network further.

While this doesn't directly cause fund loss or consensus violations (Critical severity), it degrades the reliability of a security mechanism and can cause availability issues during network stress (Medium severity).

## Likelihood Explanation

**Likelihood: High**

This issue will occur regularly in production under normal operating conditions:

1. **Time Window**: With `max_optimistic_fetch_lag_secs = 20` seconds, any storage service that doesn't receive new commits for 20+ seconds will start rejecting optimistic/subscription requests. [8](#0-7) 

2. **Network Variability**: Network delays, temporary partitions, or slow sync periods naturally cause gaps in commit notifications, making the 20-second window easy to exceed.

3. **Request Patterns**: Legitimate peers send requests at regular intervals (e.g., every 5 seconds for optimistic fetches). During a 60-second sync delay, a peer could accumulate 8-12 invalid counts from repeated attempts.

4. **No Special Conditions**: This requires no attacker action or special craftingâ€”it happens naturally during normal operation when sync is delayed.

5. **Widespread Effect**: All storage service instances are susceptible, and during network stress, many nodes could simultaneously experience this issue.

## Recommendation

**Solution: Separate Time-Based Validation from Request Validation**

The fix should distinguish between request invalidity (peer's fault) and service unavailability (server's fault):

```rust
// In moderator.rs validate_request():
pub fn validate_request(
    &self,
    peer_network_id: &PeerNetworkId,
    request: &StorageServiceRequest,
) -> Result<(), Error> {
    let validate_request = || {
        // If the peer is being ignored, return an error
        if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
            if peer_state.is_ignored() {
                return Err(Error::TooManyInvalidRequests(...));
            }
        }

        let storage_server_summary = self.cached_storage_server_summary.load();
        
        // NEW: Separate check for service availability (time-based)
        if !storage_server_summary.is_service_available(
            &self.aptos_data_client_config,
            self.time_service.clone(),
            request,
        ) {
            // Service unavailable - don't increment invalid count
            return Err(Error::ServiceTemporarilyUnavailable(format!(
                "Storage is not fresh enough to service request: {:?}", request
            )));
        }
        
        // NEW: Only static validation - increments invalid count on failure
        if !storage_server_summary.can_service_static(request) {
            let mut unhealthy_peer_state = ...;
            unhealthy_peer_state.increment_invalid_request_count(peer_network_id);
            
            return Err(Error::InvalidRequest(...));
        }

        Ok(())
    };
    // ...
}
```

In `responses.rs`, split the validation:

```rust
impl StorageServerSummary {
    // Only time-based checks - doesn't affect peer reputation
    pub fn is_service_available(&self, ...) -> bool {
        // Check time-based staleness for optimistic/subscription requests
        // Return false if storage is stale, but don't penalize peers
    }
    
    // Only static checks - affects peer reputation
    pub fn can_service_static(&self, request: &StorageServiceRequest) -> bool {
        // Check data ranges, request format validity
        // Return false only if peer sent a truly invalid request
    }
}
```

**Key Benefits:**
1. Invalid request counts only increment for actual peer misbehavior
2. Time-based unavailability returns a different error that doesn't penalize peers
3. The security metric remains reliable for identifying malicious actors
4. During network stress, peers aren't incorrectly ignored

## Proof of Concept

```rust
#[tokio::test]
async fn test_time_based_validation_inconsistency() {
    use aptos_config::config::{StorageServiceConfig, AptosDataClientConfig};
    use aptos_storage_service_types::requests::{
        DataRequest, StorageServiceRequest, NewTransactionsWithProofRequest
    };
    use aptos_time_service::TimeService;
    use std::time::Duration;

    // Setup: Create moderator with time service
    let time_service = TimeService::mock();
    let storage_service_config = StorageServiceConfig::default();
    let aptos_data_client_config = AptosDataClientConfig::default();
    
    // Create a storage summary with stale ledger info (timestamp = 0)
    let mut storage_summary = StorageServerSummary::default();
    storage_summary.data_summary.synced_ledger_info = Some(
        create_test_ledger_info_with_timestamp(0)
    );
    
    let cached_summary = Arc::new(ArcSwap::new(Arc::new(storage_summary)));
    
    let moderator = RequestModerator::new(
        aptos_data_client_config.clone(),
        cached_summary,
        peers_and_metadata,
        storage_service_config.clone(),
        time_service.clone(),
    );
    
    // Create an optimistic fetch request
    let request = StorageServiceRequest::new(
        DataRequest::GetNewTransactionsWithProof(
            NewTransactionsWithProofRequest {
                known_version: 0,
                known_epoch: 0,
                include_events: false,
            }
        ),
        false,
    );
    
    let peer = PeerNetworkId::new(NetworkId::Public, PeerId::random());
    
    // T=0: Request should be valid (within 20s lag window)
    let result_t0 = moderator.validate_request(&peer, &request);
    assert!(result_t0.is_ok(), "Request should be valid at T=0");
    
    // Verify no invalid count
    let peer_state = moderator.get_unhealthy_peer_states().get(&peer);
    assert!(peer_state.is_none() || peer_state.unwrap().invalid_request_count == 0);
    
    // Advance time by 25 seconds (beyond the 20s max_optimistic_fetch_lag_secs)
    time_service.into_mock().advance(Duration::from_secs(25));
    
    // T=25: SAME request should now be INVALID due to time progression
    let result_t25 = moderator.validate_request(&peer, &request);
    assert!(result_t25.is_err(), "Same request should be invalid at T=25");
    
    // VULNERABILITY: Invalid count incremented even though peer did nothing wrong
    let peer_state = moderator.get_unhealthy_peer_states().get(&peer);
    assert!(peer_state.is_some());
    assert_eq!(peer_state.unwrap().invalid_request_count, 1,
        "Invalid count incremented for server staleness, not peer misbehavior");
    
    // Demonstrate accumulation: If peer retries 500 times during a long sync delay,
    // they get ignored even though they sent valid requests
    for _ in 0..499 {
        let _ = moderator.validate_request(&peer, &request);
    }
    
    let peer_state = moderator.get_unhealthy_peer_states().get(&peer).unwrap();
    assert_eq!(peer_state.invalid_request_count, 500);
    assert!(peer_state.is_ignored(), "Legitimate peer is now ignored!");
}
```

## Notes

This vulnerability represents a design flaw in how request validation conflates two distinct failure modes: peer misbehavior (which should affect reputation) and server unavailability (which should not). The issue is particularly problematic during network stress when sync delays are common, as it causes the peer reputation system to degrade precisely when reliable peer connectivity is most critical. The recommended fix maintains security while improving operational reliability by distinguishing between these failure modes.

### Citations

**File:** state-sync/storage-service/server/src/moderator.rs (L54-69)
```rust
        // If the peer is a PFN and has sent too many invalid requests, start ignoring it
        if self.ignore_start_time.is_none()
            && peer_network_id.network_id().is_public_network()
            && self.invalid_request_count >= self.max_invalid_requests
        {
            // TODO: at some point we'll want to terminate the connection entirely

            // Start ignoring the peer
            self.ignore_start_time = Some(self.time_service.now());

            // Log the fact that we're now ignoring the peer
            warn!(LogSchema::new(LogEntry::RequestModeratorIgnoredPeer)
                .peer_network_id(peer_network_id)
                .message("Ignoring peer due to too many invalid requests!"));
        }
    }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L155-159)
```rust
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
```

**File:** state-sync/storage-service/server/src/moderator.rs (L161-178)
```rust
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);
```

**File:** state-sync/storage-service/types/src/responses.rs (L916-933)
```rust
fn check_synced_ledger_lag(
    synced_ledger_info: Option<&LedgerInfoWithSignatures>,
    time_service: TimeService,
    max_lag_secs: u64,
) -> bool {
    if let Some(synced_ledger_info) = synced_ledger_info {
        // Get the ledger info timestamp (in microseconds)
        let ledger_info_timestamp_usecs = synced_ledger_info.ledger_info().timestamp_usecs();

        // Get the current timestamp and max version lag (in microseconds)
        let current_timestamp_usecs = time_service.now_unix_time().as_micros() as u64;
        let max_version_lag_usecs = max_lag_secs * NUM_MICROSECONDS_IN_SECOND;

        // Return true iff the synced ledger info timestamp is within the max version lag
        ledger_info_timestamp_usecs + max_version_lag_usecs > current_timestamp_usecs
    } else {
        false // No synced ledger info was found!
    }
```

**File:** state-sync/storage-service/server/src/lib.rs (L188-213)
```rust
            loop {
                futures::select! {
                    _ = ticker.select_next_some() => {
                        // Refresh the cache periodically
                        refresh_cached_storage_summary(
                            cached_storage_server_summary.clone(),
                            storage.clone(),
                            config,
                            cache_update_notifiers.clone(),
                        )
                    },
                    notification = storage_service_listener.select_next_some() => {
                        trace!(LogSchema::new(LogEntry::ReceivedCommitNotification)
                            .message(&format!(
                                "Received commit notification for highest synced version: {:?}.",
                                notification.highest_synced_version
                            ))
                        );

                        // Refresh the cache because of a commit notification
                        refresh_cached_storage_summary(
                            cached_storage_server_summary.clone(),
                            storage.clone(),
                            config,
                            cache_update_notifiers.clone(),
                        )
```

**File:** config/src/config/state_sync_config.rs (L201-201)
```rust
            max_invalid_requests_per_peer: 500,
```

**File:** config/src/config/state_sync_config.rs (L213-213)
```rust
            min_time_to_ignore_peers_secs: 300, // 5 minutes
```

**File:** config/src/config/state_sync_config.rs (L471-471)
```rust
            max_optimistic_fetch_lag_secs: 20, // 20 seconds
```
