# Audit Report

## Title
Unbounded Memory Allocation in `get_all_batches()` Enables Validator OOM Attacks and Consensus Halt

## Summary
The `get_all_batches()` function loads all persisted batches from the database into memory without bounds checking, creating a critical denial-of-service vulnerability. With per-peer batch quotas and multiple validators, the total database size can grow to multiple gigabytes. During mid-epoch restarts, this unbounded memory allocation can cause validator out-of-memory crashes, leading to consensus liveness failures.

## Finding Description

The vulnerability exists in the quorum store database recovery mechanism. The `get_all_batches()` function unconditionally loads all database entries into a HashMap: [1](#0-0) 

This function is called during validator initialization when recovering from a mid-epoch restart: [2](#0-1) 

The critical issue is that batch quotas are enforced **per-peer**, not globally: [3](#0-2) 

With default configuration, each validator can store up to 300,000 batches and 300MB of data: [4](#0-3) 

**The Attack Path:**

1. Over time, N validators create batches within their per-peer quotas (300k batches each, 300MB each)
2. Total database accumulates: N × 300,000 batches and N × 300MB across all peers
3. A validator restarts mid-epoch (`is_new_epoch = false` at initialization)
4. The `populate_cache_and_gc_expired_batches_v1/v2()` functions call `get_all_batches()`
5. ALL batches from ALL peers are loaded into memory simultaneously in a HashMap
6. With 100 validators: ~30 million batches (~4-30GB depending on payload inclusion)
7. Memory exhaustion causes OOM kill, validator crashes
8. If sufficient validators crash simultaneously, consensus cannot reach quorum, halting the network

The vulnerability is triggered during the initialization code path: [5](#0-4) 

When `is_new_epoch = false`, the code loads all batches before filtering by expiration, violating the resource limits invariant.

## Impact Explanation

**Critical Severity - Total Loss of Liveness/Network Availability**

This vulnerability enables a complete consensus halt through coordinated validator crashes:

1. **Memory Exhaustion**: With 100 validators each at maximum quota (300k batches × 300MB), the total database contains 30 million batches and 30GB of data. Loading this into memory exceeds typical validator RAM.

2. **Cascading Failures**: When multiple validators restart (routine operations, upgrades, crashes), they simultaneously attempt to load the entire database, triggering OOM crashes.

3. **Consensus Halt**: If >1/3 of validators crash due to OOM, the network cannot form quorum certificates, permanently halting consensus until manual intervention (database cleanup).

4. **Non-Recoverable**: Crashed validators cannot restart because they immediately hit the same OOM condition. Recovery requires manual database pruning or hardfork.

This meets the **Critical Severity** criteria: "Total loss of liveness/network availability" and "Non-recoverable network partition (requires hardfork)."

## Likelihood Explanation

**High Likelihood - Natural Accumulation Without Active Attack**

This vulnerability is highly likely to occur even without malicious actors:

1. **Natural Accumulation**: Honest validators naturally create batches during normal operation. Over weeks/months of operation, each validator approaches quota limits.

2. **Routine Restarts**: Validators restart for legitimate reasons (software upgrades, configuration changes, hardware maintenance). Each restart mid-epoch triggers the vulnerability.

3. **No Coordination Required**: Unlike attacks requiring collusion, this happens naturally as the system operates at scale.

4. **Amplified by Byzantine Validators**: Even a small fraction of Byzantine validators (<1/3) can intentionally maximize their batch quotas to accelerate the timeline to exploitation.

5. **Increasing Severity Over Time**: The longer the network runs, the more batches accumulate, increasing OOM probability with each restart.

## Recommendation

Implement streaming iteration with memory budgets instead of loading all batches at once:

**Fix for `get_all_batches()` and callers:**

1. Replace `collect()` with streaming iteration
2. Add global memory budget checks during recovery
3. Implement progressive loading with backpressure
4. Add database size monitoring and alerts

**Immediate Mitigation:**
```rust
// In populate_cache_and_gc_expired_batches_v1():
fn populate_cache_and_gc_expired_batches_v1(
    db: Arc<dyn QuorumStoreStorage>,
    current_epoch: u64,
    last_certified_time: u64,
    expiration_buffer_usecs: u64,
    batch_store: &BatchStore,
) {
    const MAX_BATCHES_TO_LOAD: usize = 1_000_000; // Global limit
    let mut loaded_count = 0;
    let mut expired_keys = Vec::new();
    let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
    
    let mut iter = db.iter::<BatchSchema>().expect("failed to create iterator");
    iter.seek_to_first();
    
    while let Some(Ok((digest, value))) = iter.next() {
        if loaded_count >= MAX_BATCHES_TO_LOAD {
            warn!("Hit batch loading limit, some batches may not be loaded");
            break;
        }
        loaded_count += 1;
        
        if value.expiration() < gc_timestamp {
            expired_keys.push(digest);
        } else {
            if let Err(e) = batch_store.insert_to_cache(&value.into()) {
                warn!("Failed to insert batch during recovery: {:?}", e);
            }
        }
    }
    
    // Delete expired in batches to avoid memory spikes
    const DELETE_BATCH_SIZE: usize = 10000;
    for chunk in expired_keys.chunks(DELETE_BATCH_SIZE) {
        db.delete_batches(chunk.to_vec())
            .expect("Deletion of expired keys should not fail");
    }
}
```

**Long-term Fix:**
- Add global quota limits across all peers
- Implement database size limits with automatic pruning
- Add pre-flight checks before loading batches
- Monitor and alert on approaching memory limits

## Proof of Concept

**Rust Reproduction Steps:**

1. Setup test environment with 100 mock validators
2. Populate database with 300k batches per validator (within per-peer quota)
3. Restart BatchStore with `is_new_epoch = false`
4. Monitor memory usage - observe unbounded growth during `get_all_batches()`
5. Verify OOM crash or excessive memory consumption

```rust
#[test]
fn test_oom_vulnerability_on_recovery() {
    // Setup database with maximum batches from 100 validators
    let db = QuorumStoreDB::new(test_db_path);
    let num_validators = 100;
    let batches_per_validator = 300_000;
    
    // Populate database (within per-peer quotas)
    for validator_id in 0..num_validators {
        for batch_num in 0..batches_per_validator {
            let batch_info = create_test_batch(validator_id, batch_num);
            db.save_batch(PersistedValue::new(batch_info, None)).unwrap();
        }
    }
    
    // Trigger recovery with is_new_epoch = false
    // This calls populate_cache_and_gc_expired_batches_v1()
    let batch_store = BatchStore::new(
        epoch,
        false, // is_new_epoch = false triggers vulnerability
        current_time,
        Arc::new(db),
        memory_quota,
        db_quota,
        batch_quota,
        signer,
        expiration_buffer,
    );
    
    // Observe: get_all_batches() loads 30M batches into HashMap
    // Memory usage: 4-30GB depending on payload inclusion
    // Result: OOM crash on systems with <64GB RAM
}
```

**Expected Behavior:** Process crashes with OOM error when attempting to allocate HashMap for 30 million entries.

**Notes**

This vulnerability demonstrates a fundamental design flaw: **per-peer resource limits without global bounds** combined with **unbounded batch loading during recovery**. The issue affects both v1 and v2 batch schemas as both use the same unbounded loading pattern. The vulnerability is particularly insidious because it worsens over time as the network operates normally, eventually becoming guaranteed to trigger on any mid-epoch restart.

### Citations

**File:** consensus/src/quorum_store/quorum_store_db.rs (L103-108)
```rust
    fn get_all_batches(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfo>>> {
        let mut iter = self.db.iter::<BatchSchema>()?;
        iter.seek_to_first();
        iter.map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<HashValue, PersistedValue<BatchInfo>>>>()
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L161-176)
```rust
        } else {
            Self::populate_cache_and_gc_expired_batches_v1(
                db_clone.clone(),
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
            Self::populate_cache_and_gc_expired_batches_v2(
                db_clone,
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L252-254)
```rust
        let db_content = db
            .get_all_batches()
            .expect("failed to read v1 data from db");
```

**File:** consensus/src/quorum_store/batch_store.rs (L384-390)
```rust
                .peer_quota
                .entry(author)
                .or_insert(QuotaManager::new(
                    self.db_quota,
                    self.memory_quota,
                    self.batch_quota,
                ))
```

**File:** config/src/config/quorum_store_config.rs (L133-135)
```rust
            memory_quota: 120_000_000,
            db_quota: 300_000_000,
            batch_quota: 300_000,
```
