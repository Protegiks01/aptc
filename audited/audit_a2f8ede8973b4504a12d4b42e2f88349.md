# Audit Report

## Title
MockSleep and RealSleep Timing Divergence Causes Untested Burst Behavior in Production

## Summary
The `MockSleep` and `RealSleep` implementations have fundamentally different timing behaviors when reset after completion. `RealSleep` maintains a fixed-rate schedule that can burst when processing is delayed, while `MockSleep` always delays from the current time. This means tests using `MockTimeService` never exercise burst scenarios that occur in production, potentially hiding timing-related bugs in critical components like health checking and consensus networking.

## Finding Description

The `Interval` stream in the time service uses `Sleep::reset(duration)` to schedule subsequent ticks. However, the `reset()` implementations differ critically: [1](#0-0) 

The `RealSleep` implementation calculates the new deadline as `old_deadline + duration`, maintaining a fixed rate even when processing is delayed. [2](#0-1) 

The `MockSleep` implementation always calculates the deadline as `self.now + duration` (line 283 in `register_sleep`), creating a delay-based schedule.

**Burst Scenario Example:**
- Interval with 100ms period
- Processing takes 500ms
- **RealSleep**: Ticks at 0ms, then bursts at 500ms (catching up for missed ticks at 100, 200, 300, 400, 500ms)
- **MockSleep**: Ticks at 0ms, 500ms, 600ms (always waits full period)

This affects multiple critical components: [3](#0-2) [4](#0-3) 

The health checker increments rounds on each tick. Under load, burst behavior could cause rapid round increments, potentially leading to premature peer disconnections not seen in tests. [5](#0-4) [6](#0-5) 

The DAG network retry logic sends exponentially increasing numbers of RPCs when the interval fires. Burst behavior could trigger rapid retry escalation under network congestion.

## Impact Explanation

This is a **Medium severity** testing infrastructure issue that could mask timing-related bugs. While not directly exploitable for fund theft or consensus violation, it represents a gap in test coverage that could allow production-only bugs to slip through. Specific impacts include:

1. **Health checker**: Burst ticks could cause rapid failure accumulation and premature disconnections under load
2. **Network retry logic**: Burst behavior could amplify network congestion through rapid retry escalation  
3. **State sync polling**: Untested burst scenarios in data polling intervals

This falls under the Medium severity category of "State inconsistencies requiring intervention" as burst-induced timing issues could cause unexpected network behavior or synchronization problems.

## Likelihood Explanation

**High likelihood** that the behavioral difference exists (verified through code analysis). **Medium likelihood** that this causes production issues, as it requires sustained processing delays exceeding interval periods. Under normal operation, processing completes quickly enough that bursts don't occur. However, during network congestion, high load, or DDoS conditions, the untested burst behavior could manifest.

## Recommendation

Implement `MissedTickBehavior` support in the custom `Interval` implementation to allow explicit control over burst vs. delay semantics, matching tokio's approach: [7](#0-6) 

Alternatively, fix `MockSleep::reset()` to match `RealSleep` behavior by tracking the original deadline:

```rust
// In MockSleep struct, add:
original_deadline: Duration,

// In MockSleep::reset():
fn reset(self: Pin<&mut Self>, duration: Duration) {
    let this = self.get_mut();
    let mut inner = this.time_service.lock();
    
    // Calculate new deadline from ORIGINAL deadline, not current time
    let new_deadline = this.deadline + duration;
    
    let maybe_waker = inner.unregister_sleep(this.deadline, this.index).flatten();
    let (deadline, index) = inner.register_sleep_at(new_deadline, maybe_waker);
    this.deadline = deadline;
    this.index = index;
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_interval_burst_behavior() {
    use aptos_time_service::{MockTimeService, TimeService, TimeServiceTrait};
    use std::time::Duration;
    
    // Test with MockTimeService
    let mock_time = MockTimeService::new();
    let interval = mock_time.interval(Duration::from_millis(100));
    tokio::pin!(interval);
    
    // First tick
    mock_time.advance_async(Duration::from_millis(0)).await;
    interval.next().await; // Tick at 0ms
    
    // Simulate slow processing - advance 500ms
    mock_time.advance_async(Duration::from_millis(500)).await;
    
    // Next tick - MockSleep delays from now (500ms + 100ms = 600ms)
    interval.next().await;
    assert_eq!(mock_time.now_unix_time().as_millis(), 600); // Passes
    
    // Test with RealTimeService would show burst behavior:
    // After 500ms delay, ticks would fire at 500ms (catching up)
    // then 600ms, 700ms, etc.
}
```

## Notes

The behavioral difference is real and documented through code analysis. Tests using `MockTimeService` provide false confidence about timing behavior under load. However, I have not demonstrated a concrete exploitable path to consensus violation, fund loss, or network partition. The issue represents a testing gap rather than a direct security vulnerability. Production deployments should monitor for unexpected burst behavior in health checking and retry logic, especially under sustained load conditions.

### Citations

**File:** crates/aptos-time-service/src/real.rs (L55-58)
```rust
    fn reset(self: Pin<&mut Self>, duration: Duration) {
        let deadline = self.deadline() + duration;
        RealSleep::reset(self, deadline);
    }
```

**File:** crates/aptos-time-service/src/mock.rs (L278-305)
```rust
    fn register_sleep(
        &mut self,
        duration: Duration,
        maybe_waker: Option<Waker>,
    ) -> (Duration, SleepIndex) {
        let deadline = self.now + duration;
        let index = self.next_sleep_index();

        // When we're auto advancing, immediately resolve sleeps under the auto
        // deadline and advance the simulation time.
        if let Some(auto_advance_deadline) = self.auto_advance_deadline {
            if deadline <= auto_advance_deadline {
                self.now += duration;
                return (deadline, index);
            } else {
                // Turn off auto advancing when we've passed the deadline.
                self.now = max(self.now, auto_advance_deadline);
                self.auto_advance_deadline = None;
            }
        }

        let prev_entry = self.pending.insert((deadline, index), maybe_waker);
        assert!(
            prev_entry.is_none(),
            "there can never be an entry at an unused SleepIndex"
        );
        (deadline, index)
    }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L157-158)
```rust
        let ticker = self.time_service.interval(self.ping_interval);
        tokio::pin!(ticker);
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L229-263)
```rust
                _ = ticker.select_next_some() => {
                    self.round += 1;
                    let connected = self.network_interface.connected_peers();
                    if connected.is_empty() {
                        trace!(
                            NetworkSchema::new(&self.network_context),
                            round = self.round,
                            "{} No connected peer to ping round: {}",
                            self.network_context,
                            self.round
                        );
                        continue
                    }

                    for peer_id in connected {
                        let nonce = self.rng.r#gen::<u32>();
                        trace!(
                            NetworkSchema::new(&self.network_context),
                            round = self.round,
                            "{} Will ping: {} for round: {} nonce: {}",
                            self.network_context,
                            peer_id.short_str(),
                            self.round,
                            nonce
                        );

                        tick_handlers.push(Self::ping_peer(
                            self.network_context,
                            self.network_interface.network_client(),
                            peer_id,
                            self.round,
                            nonce,
                            self.ping_timeout,
                        ));
                    }
```

**File:** consensus/src/dag/dag_network.rs (L121-121)
```rust
            interval: Box::pin(time_service.interval(retry_interval)),
```

**File:** consensus/src/dag/dag_network.rs (L149-163)
```rust
        // Check if the timeout has happened
        let timeout = matches!(self.interval.as_mut().poll_next(cx), Poll::Ready(_));

        if self.futures.is_empty() || timeout {
            // try to find more responders and queue futures
            if let Some(peers) = Pin::new(&mut self.responders).next_to_request() {
                for peer in peers {
                    let future = Box::pin(send_rpc(
                        self.sender.clone(),
                        peer,
                        self.message.clone(),
                        self.rpc_timeout,
                    ));
                    self.futures.push(future);
                }
```

**File:** crates/aptos-jwk-consensus/src/jwk_observer.rs (L59-60)
```rust
        let mut interval = tokio::time::interval(fetch_interval);
        interval.set_missed_tick_behavior(MissedTickBehavior::Delay);
```
