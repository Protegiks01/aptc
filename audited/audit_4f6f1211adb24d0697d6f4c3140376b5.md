# Audit Report

## Title
Timestamp Update and Payload Clearing Race Condition in BatchStore

## Summary
The `BatchStore::update_certified_timestamp()` function performs two non-atomic operations: updating the certified timestamp via `fetch_max()` and clearing expired payloads via `clear_expired_payload()`. When multiple threads call this function concurrently with different timestamp values, the clearing operations can execute out of order relative to the timestamp updates, creating a window where expired batches remain accessible despite the timestamp indicating they should be cleared.

## Finding Description

The vulnerability exists in the `update_certified_timestamp()` method which performs two separate operations: [1](#0-0) 

The atomic timestamp update uses `fetch_max()` which correctly preserves the maximum timestamp value. However, the subsequent `clear_expired_payload()` call uses the **parameter value** rather than reading back the actual stored atomic value. This creates a race condition when concurrent calls arrive with different timestamps.

**Race Scenario:**
1. Thread A calls `update_certified_timestamp(100)` at time t1
2. Thread B calls `update_certified_timestamp(200)` at time t2, where 200 > 100
3. Thread B's `fetch_max(200)` executes first, setting atomic to 200
4. Thread A's `fetch_max(100)` executes, atomic stays at 200 (100 < 200)
5. Thread A's `clear_expired_payload(100)` acquires lock first due to timing
6. Thread A clears batches with expiration â‰¤ (100 - buffer)
7. **Race Window:** Batches with expiration between (100-buffer) and (200-buffer) remain accessible
8. During this window, `get_batch_from_local()` can retrieve expired batches
9. Thread B's `clear_expired_payload(200)` eventually runs and clears them

The `get_batch_from_local()` method does not verify expiration before returning cached batches: [2](#0-1) 

During the race window (steps 7-9), the atomic timestamp indicates 200 but batches that should be expired (with expiration < 200-buffer but > 100-buffer) are still in cache and retrievable. This violates the invariant that expired batches should not be accessible once the certified timestamp advances past their expiration.

The `TimeExpirations::expire()` method acquires a lock and pops items from the heap, but this locking only serializes the expiration logic itself, not the relationship between the atomic timestamp update and clearing: [3](#0-2) 

This function is called from the payload manager when blocks are committed: [4](#0-3) 

## Impact Explanation

This qualifies as **Medium severity** per the Aptos bug bounty program: "State inconsistencies requiring intervention."

The vulnerability causes temporary state inconsistency where:
- The certified timestamp atomic value is correct (maximum seen)
- But expired batches remain accessible during the race window
- These batches can be served to peers via the batch retrieval service or used in internal operations

While additional expiration checks exist in the batch pulling logic, the fundamental invariant is violated that the batch store's state should be consistent with its certified timestamp. This could cause:
- Inconsistent views across nodes of which batches are available
- Wasted resources fetching/serving batches that should be expired
- Potential for including stale data in consensus operations

The impact is limited because:
- The race window is typically small
- Additional expiration filtering exists in critical paths
- No direct consensus safety violation or fund loss occurs

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability can manifest when:
- Multiple block commits occur in quick succession
- Commit notifications arrive out of order or are processed concurrently
- Network delays cause timestamp updates to be processed non-sequentially

While not directly controllable by an external attacker, the race can occur naturally under normal network conditions, especially during:
- High transaction throughput periods
- Network partition recovery
- Validator catching up after temporary downtime
- Epoch transitions with multiple rapid commits

The delegation pattern at line 742 makes this more likely because `BatchReaderImpl` simply forwards calls without coordination: [5](#0-4) 

## Recommendation

**Fix:** Ensure the clearing operation uses the actual stored timestamp value, not the parameter value, and document that these operations are not guaranteed to be atomic together.

```rust
pub fn update_certified_timestamp(&self, certified_time: u64) {
    trace!("QS: batch reader updating time {:?}", certified_time);
    
    // Update the timestamp atomically
    self.last_certified_time
        .fetch_max(certified_time, Ordering::SeqCst);
    
    // Read back the actual stored value to ensure consistency
    let actual_certified_time = self.last_certified_time.load(Ordering::SeqCst);
    
    // Clear based on the actual stored maximum, not the parameter
    let expired_keys = self.clear_expired_payload(actual_certified_time);
    if let Err(e) = self.db.delete_batches(expired_keys) {
        debug!("Error deleting batches: {:?}", e)
    }
}
```

**Alternative Fix:** Add validation in `get_batch_from_local()` to check expiration before returning:

```rust
pub(crate) fn get_batch_from_local(
    &self,
    digest: &HashValue,
) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
    if let Some(value) = self.db_cache.get(digest) {
        // Check if batch is expired
        let last_certified = self.last_certified_time();
        let expiration_threshold = last_certified.saturating_sub(self.expiration_buffer_usecs);
        if value.expiration() <= expiration_threshold {
            return Err(ExecutorError::CouldNotGetData);
        }
        
        if value.payload_storage_mode() == StorageMode::PersistedOnly {
            self.get_batch_from_db(digest, value.batch_info().is_v2())
        } else {
            Ok(value.clone())
        }
    } else {
        Err(ExecutorError::CouldNotGetData)
    }
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_timestamp_update_race() {
    use std::sync::Arc;
    use std::time::Duration;
    use tokio::time::sleep;
    
    // Setup batch store with initial timestamp 100
    let batch_store = Arc::new(create_batch_store(100));
    
    // Add a batch with expiration 150
    let batch = create_test_batch_with_expiration(150);
    batch_store.insert_to_cache(&batch).unwrap();
    
    let store_clone1 = batch_store.clone();
    let store_clone2 = batch_store.clone();
    let digest = *batch.digest();
    
    // Thread A: Update timestamp to 100 (no-op but will trigger clear)
    let handle_a = tokio::spawn(async move {
        sleep(Duration::from_millis(5)).await; // Delay to ensure B's fetch_max runs first
        store_clone1.update_certified_timestamp(100);
    });
    
    // Thread B: Update timestamp to 200
    let handle_b = tokio::spawn(async move {
        store_clone2.update_certified_timestamp(200);
    });
    
    // Wait a bit for race to occur
    sleep(Duration::from_millis(3)).await;
    
    // During race window: atomic is 200 but clearing for 100 may have run first
    // Try to retrieve batch with expiration 150
    // With buffer of 60, this should be expired (150 < 200-60=140) but may still be accessible
    let result = batch_store.get_batch_from_local(&digest);
    
    handle_a.await.unwrap();
    handle_b.await.unwrap();
    
    // After both complete, batch should definitely be gone
    sleep(Duration::from_millis(10)).await;
    assert!(batch_store.get_batch_from_local(&digest).is_err());
}
```

## Notes

The timestamp updates themselves are not "lost" (the atomic correctly maintains the maximum value via `fetch_max`), but the **effects** of timestamp updates (clearing expired payloads) can execute out of order, creating temporary state inconsistencies. This breaks the expected invariant that once the certified timestamp advances, all batches expired relative to that timestamp should be immediately inaccessible.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L571-585)
```rust
    pub(crate) fn get_batch_from_local(
        &self,
        digest: &HashValue,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        if let Some(value) = self.db_cache.get(digest) {
            if value.payload_storage_mode() == StorageMode::PersistedOnly {
                self.get_batch_from_db(digest, value.batch_info().is_v2())
            } else {
                // Available in memory.
                Ok(value.clone())
            }
        } else {
            Err(ExecutorError::CouldNotGetData)
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L742-744)
```rust
    fn update_certified_timestamp(&self, certified_time: u64) {
        self.batch_store.update_certified_timestamp(certified_time);
    }
```

**File:** consensus/src/quorum_store/utils.rs (L78-89)
```rust
    pub(crate) fn expire(&mut self, certified_time: u64) -> HashSet<I> {
        let mut ret = HashSet::new();
        while let Some((Reverse(t), _)) = self.expiries.peek() {
            if *t <= certified_time {
                let (_, item) = self.expiries.pop().unwrap();
                ret.insert(item);
            } else {
                break;
            }
        }
        ret
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L168-171)
```rust
    fn notify_commit(&self, block_timestamp: u64, payloads: Vec<Payload>) {
        self.batch_reader
            .update_certified_timestamp(block_timestamp);

```
