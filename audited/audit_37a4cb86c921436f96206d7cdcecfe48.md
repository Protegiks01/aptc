# Audit Report

## Title
Quota Desynchronization in Batch Cleanup Leading to Unbounded Database Growth

## Summary
The `update_certified_timestamp()` function in the quorum store batch management system contains a critical Time-Of-Check-Time-Of-Use (TOCTOU) vulnerability. When expired batches are cleaned up, their quota is freed from in-memory accounting **before** the actual database deletion is attempted. If the database deletion fails (e.g., disk full, I/O errors, corruption), the error is only logged while quota remains freed. This allows new batches to be continuously accepted while old batches accumulate in the database, causing unbounded database growth and eventual validator node failure.

## Finding Description

The vulnerability exists in the batch cleanup flow within the quorum store's batch management system. The quorum store uses a `QuotaManager` to track resource usage across three dimensions: batch count, database bytes, and memory bytes. This quota system is designed to prevent resource exhaustion by rejecting new batches when limits are reached. [1](#0-0) 

The critical flaw occurs in the order of operations:

1. **Step 1 - Quota Freed**: `clear_expired_payload()` removes expired batches from the in-memory cache and immediately frees their quota through `free_quota()`, which increases `batch_balance`, `db_balance`, and `memory_balance`. [2](#0-1) 

2. **Step 2 - Deletion Attempted**: After quota is freed, `db.delete_batches()` attempts to delete the batch data from persistent storage via RocksDB.

3. **Step 3 - Silent Failure**: If deletion fails, the error is only logged at debug level with no remediation or quota rollback.

The database deletion can fail for multiple realistic reasons: [3](#0-2) 

Database write operations can fail due to:
- Disk full (most common in production)
- I/O errors from hardware failures
- Filesystem permission issues  
- Database corruption
- RocksDB internal errors [4](#0-3) 

**Attack Scenario:**

1. Validator node operates normally, accepting and persisting batches
2. Disk space becomes constrained (naturally or through external filling)
3. When `update_certified_timestamp()` is called (happens every block commit), `clear_expired_payload()` removes expired batches from memory and frees their quota
4. `db.delete_batches()` fails due to disk full or I/O error
5. Error is logged but quota remains freed
6. New batches are accepted because quota accounting shows available space
7. Cycle repeats: quota freed → deletion fails → new batches added → database grows
8. Database size increases unboundedly until disk is completely full
9. Validator node crashes or becomes unresponsive

The quota management system becomes permanently desynchronized from actual database state: [5](#0-4) 

Once quota is freed, there is no mechanism to detect or recover from deletion failures. The validator will continue accepting new batches based on the incorrect quota accounting while old data accumulates.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty program criteria for the following reasons:

**Validator Node Slowdowns & Crashes:**
- Unbounded database growth eventually fills the disk
- Once disk is full, RocksDB operations become extremely slow or fail
- Node becomes unresponsive to consensus messages
- May require manual intervention and disk expansion

**Liveness Impact:**
- If multiple validators experience this issue simultaneously (e.g., during disk space constraints), network liveness can be affected
- Validators may fall behind in consensus rounds
- Could trigger epoch change complications

**Operational Disruption:**
- Requires operator intervention to clean up database manually
- Potential data loss if emergency cleanup is performed incorrectly
- Extended downtime during recovery

**No Monitoring/Detection:**
There are no metrics tracking `delete_batches` failures, making this issue difficult to detect until catastrophic failure occurs. [6](#0-5) 

While quota exceeded events are tracked, there is no corresponding metric for deletion failures, leaving operators blind to the accumulating problem.

## Likelihood Explanation

This vulnerability has **HIGH** likelihood of occurrence in production environments:

**Frequent Execution Path:**
The vulnerable code path executes on every block commit via `QuorumStorePayloadManager::notify_commit()`: [7](#0-6) 

This means the vulnerability is triggered hundreds or thousands of times per day on active validators.

**Common Failure Conditions:**
- Disk full is a common operational issue in production systems
- I/O errors occur naturally with hardware aging
- Filesystem issues can arise from configuration or OS updates

**No Special Privileges Required:**
- Attacker can trigger by filling disk space through other means
- Or simply wait for natural disk growth
- No validator access or special permissions needed

**Progressive Failure:**
Once the first deletion failure occurs, the problem compounds:
- Each subsequent failure makes the problem worse
- No self-healing mechanism exists
- Continues until complete disk exhaustion

## Recommendation

Implement proper transactional cleanup with rollback semantics. The quota should only be freed **after** successful database deletion, or rolled back if deletion fails:

```rust
pub fn update_certified_timestamp(&self, certified_time: u64) {
    trace!("QS: batch reader updating time {:?}", certified_time);
    self.last_certified_time
        .fetch_max(certified_time, Ordering::SeqCst);

    let expired_entries = self.collect_expired_entries(certified_time);
    
    // Attempt database deletion FIRST
    let expired_keys: Vec<_> = expired_entries.iter().map(|(k, _)| *k).collect();
    match self.db.delete_batches(expired_keys) {
        Ok(()) => {
            // Only free quota AFTER successful deletion
            for (_, value) in expired_entries {
                self.free_quota(value);
            }
        },
        Err(e) => {
            // Log error and do NOT free quota
            error!("CRITICAL: Failed to delete batches from database: {:?}", e);
            counters::BATCH_DELETION_FAILED_COUNT.inc();
            // Consider: trigger alert, retry mechanism, or graceful degradation
        }
    }
}

// New helper method that removes from cache but doesn't free quota yet
fn collect_expired_entries(&self, certified_time: u64) -> Vec<(HashValue, PersistedValue<BatchInfoExt>)> {
    let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
    let expired_digests = self.expirations.lock().expire(expiration_time);
    let mut entries = Vec::new();
    
    for h in expired_digests {
        if let Some(value) = self.db_cache.remove(&h) {
            if value.1.expiration() <= expiration_time {
                self.persist_subscribers.remove(value.1.digest());
                entries.push((h, value.1));
            } else {
                // Re-insert if not actually expired
                self.db_cache.insert(h, value.1);
            }
        }
    }
    entries
}
```

**Additional Improvements:**

1. **Add Metrics**: Track deletion failures with a dedicated counter
2. **Retry Logic**: Implement exponential backoff retry for transient failures
3. **Alerting**: Emit critical alerts when deletions fail repeatedly
4. **Quota Verification**: Periodically verify quota state matches database state
5. **Upgrade Error Level**: Change from `debug!` to `error!` logging

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic::{AtomicBool, Ordering};
    
    // Mock database that can simulate failures
    struct FailingQuorumStoreDB {
        should_fail: AtomicBool,
        batches: Mutex<HashMap<HashValue, PersistedValue<BatchInfo>>>,
    }
    
    impl FailingQuorumStoreDB {
        fn new() -> Self {
            Self {
                should_fail: AtomicBool::new(false),
                batches: Mutex::new(HashMap::new()),
            }
        }
        
        fn set_deletion_failure(&self, fail: bool) {
            self.should_fail.store(fail, Ordering::SeqCst);
        }
    }
    
    impl QuorumStoreStorage for FailingQuorumStoreDB {
        fn delete_batches(&self, digests: Vec<HashValue>) -> Result<(), DbError> {
            if self.should_fail.load(Ordering::SeqCst) {
                // Simulate disk full error
                return Err(DbError::from(AptosDbError::IoError("No space left on device".to_string())));
            }
            
            let mut batches = self.batches.lock().unwrap();
            for digest in digests {
                batches.remove(&digest);
            }
            Ok(())
        }
        
        fn save_batch(&self, batch: PersistedValue<BatchInfo>) -> Result<(), DbError> {
            self.batches.lock().unwrap().insert(*batch.digest(), batch);
            Ok(())
        }
        
        // ... other trait methods
    }
    
    #[test]
    fn test_quota_desync_on_deletion_failure() {
        // Setup: Create batch store with 1000 byte quota
        let failing_db = Arc::new(FailingQuorumStoreDB::new());
        let batch_store = BatchStore::new(
            1, // epoch
            false, // is_new_epoch
            0, // last_certified_time
            failing_db.clone(),
            1000, // memory_quota
            1000, // db_quota  
            10,   // batch_quota
            validator_signer,
            60_000_000, // expiration_buffer_usecs
        );
        
        // Step 1: Add batches consuming 800 bytes total
        for i in 0..8 {
            let batch = create_test_batch(100); // 100 bytes each
            batch_store.save(&batch).unwrap();
        }
        
        // Verify initial state: 200 bytes quota remaining
        assert_eq!(get_available_quota(&batch_store), 200);
        assert_eq!(count_batches_in_db(&failing_db), 8);
        
        // Step 2: Simulate time passing so batches expire
        let future_time = 120_000_000; // 2 minutes later
        
        // Step 3: Enable deletion failures
        failing_db.set_deletion_failure(true);
        
        // Step 4: Trigger cleanup - this will free quota but fail to delete
        batch_store.update_certified_timestamp(future_time);
        
        // BUG: Quota is freed but batches remain in database
        assert_eq!(get_available_quota(&batch_store), 1000); // QUOTA FREED
        assert_eq!(count_batches_in_db(&failing_db), 8);     // BATCHES STILL PRESENT
        
        // Step 5: Add more batches - they are accepted because quota shows availability
        for i in 0..8 {
            let batch = create_test_batch(100);
            batch_store.save(&batch).unwrap(); // SUCCEEDS due to freed quota
        }
        
        // Result: Database has 16 batches (1600 bytes) but quota accounting shows only 200 bytes used
        assert_eq!(count_batches_in_db(&failing_db), 16); // ACTUAL: 1600 bytes
        assert_eq!(get_available_quota(&batch_store), 200); // ACCOUNTING: 800 bytes "used"
        
        // This desynchronization persists and compounds with each cleanup cycle
        // Eventually leads to disk exhaustion
    }
}
```

**Notes**

This vulnerability represents a critical failure in the resource management layer of the quorum store consensus subsystem. The root cause is a violation of atomicity principles: quota accounting and database operations must be atomic, but the current implementation separates them with error-prone ordering.

The issue is particularly insidious because:
1. It degrades gradually rather than failing immediately
2. No metrics or alerts exist to detect the problem
3. Once triggered, it compounds with each cleanup cycle
4. Recovery requires manual operator intervention

This matches the **High Severity** classification under Aptos Bug Bounty criteria for "Validator node slowdowns" and represents a significant operational risk to validator infrastructure.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L97-108)
```rust
    pub(crate) fn free_quota(&mut self, num_bytes: usize, storage_mode: StorageMode) {
        Self::assert_quota(self.batch_balance, 1, self.batch_quota, "Batch");
        self.batch_balance += 1;

        Self::assert_quota(self.db_balance, num_bytes, self.db_quota, "DB");
        self.db_balance += num_bytes;

        if matches!(storage_mode, StorageMode::MemoryAndPersisted) {
            Self::assert_quota(self.memory_balance, num_bytes, self.memory_quota, "Memory");
            self.memory_balance += num_bytes;
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L443-472)
```rust
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L93-101)
```rust
    fn delete_batches(&self, digests: Vec<HashValue>) -> Result<(), DbError> {
        let mut batch = SchemaBatch::new();
        for digest in digests.iter() {
            trace!("QS: db delete digest {}", digest);
            batch.delete::<BatchSchema>(digest)?;
        }
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** storage/storage-interface/src/errors.rs (L9-37)
```rust
/// This enum defines errors commonly used among `AptosDB` APIs.
#[derive(Clone, Debug, Error)]
pub enum AptosDbError {
    /// A requested item is not found.
    #[error("{0} not found.")]
    NotFound(String),
    /// Requested too many items.
    #[error("Too many items requested: at least {0} requested, max is {1}")]
    TooManyRequested(u64, u64),
    #[error("Missing state root node at version {0}, probably pruned.")]
    MissingRootError(u64),
    /// Other non-classified error.
    #[error("AptosDB Other Error: {0}")]
    Other(String),
    #[error("AptosDB RocksDb Error: {0}")]
    RocksDbIncompleteResult(String),
    #[error("AptosDB RocksDB Error: {0}")]
    OtherRocksDbError(String),
    #[error("AptosDB bcs Error: {0}")]
    BcsError(String),
    #[error("AptosDB IO Error: {0}")]
    IoError(String),
    #[error("AptosDB Recv Error: {0}")]
    RecvError(String),
    #[error("AptosDB ParseInt Error: {0}")]
    ParseIntError(String),
    #[error("Hot state not configured properly")]
    HotStateError,
}
```

**File:** consensus/src/quorum_store/counters.rs (L743-760)
```rust
/// Count of the exceeded storage quota.
pub static EXCEEDED_STORAGE_QUOTA_COUNT: Lazy<IntCounter> = Lazy::new(|| {
    register_int_counter!(
        "quorum_store_exceeded_storage_quota_count",
        "Count of the exceeded storage quota."
    )
    .unwrap()
});

/// Count of the exceeded batch quota.
pub static EXCEEDED_BATCH_QUOTA_COUNT: Lazy<IntCounter> = Lazy::new(|| {
    register_int_counter!(
        "quorum_store_exceeded_batch_quota_count",
        "Count of the exceeded batch quota."
    )
    .unwrap()
});

```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L168-171)
```rust
    fn notify_commit(&self, block_timestamp: u64, payloads: Vec<Payload>) {
        self.batch_reader
            .update_certified_timestamp(block_timestamp);

```
