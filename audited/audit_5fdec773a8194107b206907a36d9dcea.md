# Audit Report

## Title
Priority Inversion in Consensus Observer Pending Block Store Causes Permanent Loss of Ready Blocks

## Summary
The `remove_ready_block()` function in the pending block store suffers from a critical priority inversion bug. When a payload arrives for a high-round block, all lower-round pending blocks are permanently dropped regardless of whether they are ready for processing. This causes consensus observer nodes to lose ready blocks and fail to make progress, resulting in a liveness failure.

## Finding Description

The vulnerability exists in the `remove_ready_block()` function [1](#0-0) .

When a block payload arrives, the consensus observer calls `order_ready_pending_block()` [2](#0-1)  which attempts to retrieve and process any pending block that has now become ready. The function calls `remove_ready_pending_block()` [3](#0-2) , which delegates to the pending block store's `remove_ready_block()`.

The flawed logic in `remove_ready_block()` works as follows:

1. It splits the pending blocks BTreeMap at `received_payload_round + 1`, separating blocks into those with rounds ≤ received round and those with higher rounds
2. It pops **only the last (highest-round)** block from the lower-rounds set
3. It checks if this single block is ready
4. **All remaining lower-round blocks are permanently deleted**, even if they were ready for processing

The critical bug occurs at these lines [4](#0-3) . After checking only the highest-round block, the function clears all blocks and repopulates the store with only the higher-round blocks, permanently losing any lower-round ready blocks.

**Attack Scenario:**
1. Consensus observer receives OrderedBlock A (rounds 100-104) - all payloads already present (READY)
2. Consensus observer receives OrderedBlock B (rounds 200-204) - payload for round 200 exists, but missing payloads for rounds 201-204
3. When payload for round 200 arrives, `remove_ready_block(epoch, 200)` is called
4. Both blocks A and B have first_block.round ≤ 200, so they remain after the split
5. Block B (round 200) is popped as the highest round
6. Block B is NOT ready (missing payloads 201-204)
7. Block B's last_pending_block_round (204) > received_payload_round (200), so it gets re-inserted to higher rounds
8. **Block A is permanently dropped from the pending store even though it was ready**
9. The observer node never processes block A, causing the node to fall behind

This violates the consensus liveness invariant - ready blocks must eventually be processed.

## Impact Explanation

This is a **Medium Severity** vulnerability based on Aptos bug bounty criteria:

- **Liveness Failure**: Observer nodes fail to process ready blocks, causing them to fall behind the network
- **State Inconsistencies**: Observer nodes maintain inconsistent state compared to active validators
- **Requires Manual Intervention**: Node operators must restart their observers to resync

The impact is limited to consensus observer nodes, not active validators. However, observers are critical infrastructure for:
- Fullnode operators monitoring the chain
- Indexing services tracking blockchain state
- Public RPC endpoints serving applications

A malicious peer can deliberately trigger this condition by sending ordered blocks with strategically missing payloads, causing observer nodes to permanently lose ready blocks and requiring manual intervention to recover.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability can be triggered in two scenarios:

1. **Malicious Attack**: An adversarial peer can intentionally send ordered blocks with missing payloads at strategic rounds to trigger block starvation. Since consensus observers accept messages from any network peer [5](#0-4) , no special privileges are required.

2. **Normal Network Conditions**: Even without malicious intent, network delays and out-of-order message delivery can cause this bug to trigger. If payloads arrive out of order (high-round payload before low-round payload), ready low-round blocks will be dropped.

The bug is deterministic - once the conditions are met, blocks are always lost. The only requirement is that pending blocks exist at different rounds with varying payload availability.

## Recommendation

Modify the `remove_ready_block()` function to check **all** pending blocks with rounds ≤ received_payload_round for readiness, not just the highest-round one. Process all ready blocks in round order:

```rust
pub fn remove_ready_block(
    &mut self,
    received_payload_epoch: u64,
    received_payload_round: Round,
    block_payload_store: &mut BlockPayloadStore,
) -> Option<Arc<PendingBlockWithMetadata>> {
    let split_round = received_payload_round.saturating_add(1);
    let mut blocks_at_higher_rounds = self
        .blocks_without_payloads
        .split_off(&(received_payload_epoch, split_round));

    // Check ALL blocks in round order for readiness
    let mut ready_block = None;
    let mut blocks_to_keep = BTreeMap::new();
    
    for (epoch_and_round, pending_block) in self.blocks_without_payloads.iter() {
        if block_payload_store.all_payloads_exist(pending_block.ordered_block().blocks()) {
            // Return the first ready block in round order
            if ready_block.is_none() {
                ready_block = Some(pending_block.clone());
            }
            // Don't keep this block, it's ready
        } else {
            // Check if we should keep this not-ready block
            let last_pending_block_round = pending_block.ordered_block().last_block().round();
            if last_pending_block_round > received_payload_round {
                blocks_to_keep.insert(*epoch_and_round, pending_block.clone());
            }
        }
    }

    // Merge blocks to keep with higher round blocks
    blocks_at_higher_rounds.append(&mut blocks_to_keep);
    
    // Clear and repopulate
    self.clear_missing_blocks();
    self.blocks_without_payloads = blocks_at_higher_rounds;
    for pending_block in self.blocks_without_payloads.values() {
        let first_block = pending_block.ordered_block().first_block();
        self.blocks_without_payloads_by_hash
            .insert(first_block.id(), pending_block.clone());
    }

    ready_block
}
```

Alternatively, maintain a separate priority queue of ready blocks to ensure they are never lost.

## Proof of Concept

```rust
#[test]
fn test_priority_inversion_drops_ready_blocks() {
    use aptos_config::config::ConsensusObserverConfig;
    use aptos_infallible::Mutex;
    use std::sync::Arc;
    
    // Create pending block store
    let consensus_observer_config = ConsensusObserverConfig::default();
    let pending_block_store = Arc::new(Mutex::new(PendingBlockStore::new(
        consensus_observer_config,
    )));
    
    let epoch = 10;
    
    // Create and insert OrderedBlock A at round 100 (single block)
    let block_a = create_ordered_block(epoch, 100, 1, 0);
    let observed_block_a = ObservedOrderedBlock::new_for_testing(block_a.clone());
    let pending_a = PendingBlockWithMetadata::new_with_arc(
        PeerNetworkId::random(),
        Instant::now(),
        observed_block_a,
    );
    pending_block_store.lock().insert_pending_block(pending_a);
    
    // Create and insert OrderedBlock B at rounds 200-204 (5 blocks)
    let block_b = create_ordered_block(epoch, 200, 5, 1);
    let observed_block_b = ObservedOrderedBlock::new_for_testing(block_b.clone());
    let pending_b = PendingBlockWithMetadata::new_with_arc(
        PeerNetworkId::random(),
        Instant::now(),
        observed_block_b,
    );
    pending_block_store.lock().insert_pending_block(pending_b);
    
    // Create payload store and insert ALL payloads for block A (making it READY)
    let mut block_payload_store = BlockPayloadStore::new(consensus_observer_config);
    insert_payloads_for_ordered_block(&mut block_payload_store, &block_a);
    
    // Insert ONLY the first payload for block B (round 200)
    let first_block_b = block_b.blocks()[0].clone();
    let payload_b = BlockPayload::new(
        first_block_b.block_info(),
        BlockTransactionPayload::empty(),
    );
    block_payload_store.insert_block_payload(payload_b, true);
    
    // Verify both blocks are in pending store
    assert!(pending_block_store.lock().existing_pending_block(&block_a));
    assert!(pending_block_store.lock().existing_pending_block(&block_b));
    
    // Trigger the bug: payload for round 200 arrives
    let ready_block = pending_block_store.lock().remove_ready_block(
        epoch,
        200,
        &mut block_payload_store,
    );
    
    // BUG: No block is returned because block B is not ready
    assert!(ready_block.is_none());
    
    // CRITICAL BUG: Block A (which was READY) is now GONE
    assert!(!pending_block_store.lock().existing_pending_block(&block_a));
    
    // Block A will never be processed - permanent data loss!
}
```

**Notes**

The vulnerability is exacerbated by the comment at line 214-215 [6](#0-5)  which states "this should be the only ready block" - this assumption is incorrect and leads to the bug. Multiple blocks at different rounds can simultaneously be ready, and the code must handle this correctly.

The issue is a textbook priority inversion: high-round blocks (even when not ready) cause low-round ready blocks to be permanently starved from processing. This breaks the fundamental invariant that ready consensus data must eventually be processed to maintain liveness.

### Citations

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L200-256)
```rust
    pub fn remove_ready_block(
        &mut self,
        received_payload_epoch: u64,
        received_payload_round: Round,
        block_payload_store: &mut BlockPayloadStore,
    ) -> Option<Arc<PendingBlockWithMetadata>> {
        // Calculate the round at which to split the blocks
        let split_round = received_payload_round.saturating_add(1);

        // Split the blocks at the epoch and round
        let mut blocks_at_higher_rounds = self
            .blocks_without_payloads
            .split_off(&(received_payload_epoch, split_round));

        // Check if the last block is ready (this should be the only ready block).
        // Any earlier blocks are considered out-of-date and will be dropped.
        let mut ready_block = None;
        if let Some((epoch_and_round, pending_block)) = self.blocks_without_payloads.pop_last() {
            // If all payloads exist for the block, then the block is ready
            if block_payload_store.all_payloads_exist(pending_block.ordered_block().blocks()) {
                ready_block = Some(pending_block);
            } else {
                // Otherwise, check if we're still waiting for higher payloads for the block
                let last_pending_block_round = pending_block.ordered_block().last_block().round();
                if last_pending_block_round > received_payload_round {
                    blocks_at_higher_rounds.insert(epoch_and_round, pending_block);
                }
            }
        }

        // Check if any out-of-date blocks are going to be dropped
        if !self.blocks_without_payloads.is_empty() {
            info!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Dropped {:?} out-of-date pending blocks before epoch and round: {:?}",
                    self.blocks_without_payloads.len(),
                    (received_payload_epoch, received_payload_round)
                ))
            );
        }

        // TODO: optimize this flow!

        // Clear all blocks from the pending block stores
        self.clear_missing_blocks();

        // Update the pending block stores to only include the blocks at higher rounds
        self.blocks_without_payloads = blocks_at_higher_rounds;
        for pending_block in self.blocks_without_payloads.values() {
            let first_block = pending_block.ordered_block().first_block();
            self.blocks_without_payloads_by_hash
                .insert(first_block.id(), pending_block.clone());
        }

        // Return the ready block (if one exists)
        ready_block
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L341-353)
```rust
    async fn order_ready_pending_block(&mut self, block_epoch: u64, block_round: Round) {
        // Remove any ready pending block
        let pending_block_with_metadata = self
            .observer_block_data
            .lock()
            .remove_ready_pending_block(block_epoch, block_round);

        // Process the ready ordered block (if it exists)
        if let Some(pending_block_with_metadata) = pending_block_with_metadata {
            self.process_ordered_block(pending_block_with_metadata)
                .await;
        }
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L355-439)
```rust
    /// Processes the block payload message
    async fn process_block_payload_message(
        &mut self,
        peer_network_id: PeerNetworkId,
        message_received_time: Instant,
        block_payload: BlockPayload,
    ) {
        // Get the epoch and round for the block
        let block_epoch = block_payload.epoch();
        let block_round = block_payload.round();

        // Determine if the payload is behind the last ordered block, or if it already exists
        let last_ordered_block = self.observer_block_data.lock().get_last_ordered_block();
        let payload_out_of_date =
            (block_epoch, block_round) <= (last_ordered_block.epoch(), last_ordered_block.round());
        let payload_exists = self
            .observer_block_data
            .lock()
            .existing_payload_entry(&block_payload);

        // If the payload is out of date or already exists, ignore it
        if payload_out_of_date || payload_exists {
            // Update the metrics for the dropped block payload
            update_metrics_for_dropped_block_payload_message(peer_network_id, &block_payload);
            return;
        }

        // Update the metrics for the received block payload
        update_metrics_for_block_payload_message(peer_network_id, &block_payload);

        // Verify the block payload digests
        if let Err(error) = block_payload.verify_payload_digests() {
            // Log the error and update the invalid message counter
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to verify block payload digests! Ignoring block: {:?}, from peer: {:?}. Error: {:?}",
                    block_payload.block(), peer_network_id,
                    error
                ))
            );
            increment_invalid_message_counter(&peer_network_id, metrics::BLOCK_PAYLOAD_LABEL);
            return;
        }

        // If the payload is for the current epoch, verify the proof signatures
        let epoch_state = self.get_epoch_state();
        let verified_payload = if block_epoch == epoch_state.epoch {
            // Verify the block proof signatures
            if let Err(error) = block_payload.verify_payload_signatures(&epoch_state) {
                // Log the error and update the invalid message counter
                error!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Failed to verify block payload signatures! Ignoring block: {:?}, from peer: {:?}. Error: {:?}",
                        block_payload.block(), peer_network_id, error
                    ))
                );
                increment_invalid_message_counter(&peer_network_id, metrics::BLOCK_PAYLOAD_LABEL);
                return;
            }

            true // We have successfully verified the signatures
        } else {
            false // We can't verify the signatures yet
        };

        // Update the latency metrics for block payload processing
        update_message_processing_latency_metrics(
            message_received_time,
            &peer_network_id,
            metrics::BLOCK_PAYLOAD_LABEL,
        );

        // Update the payload store with the payload
        self.observer_block_data
            .lock()
            .insert_block_payload(block_payload, verified_payload);

        // Check if there are blocks that were missing payloads but are
        // now ready because of the new payload. Note: this should only
        // be done if the payload has been verified correctly.
        if verified_payload {
            self.order_ready_pending_block(block_epoch, block_round)
                .await;
        }
    }
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L244-254)
```rust
    pub fn remove_ready_pending_block(
        &mut self,
        received_payload_epoch: u64,
        received_payload_round: Round,
    ) -> Option<Arc<PendingBlockWithMetadata>> {
        self.pending_block_store.remove_ready_block(
            received_payload_epoch,
            received_payload_round,
            &mut self.block_payload_store,
        )
    }
```
