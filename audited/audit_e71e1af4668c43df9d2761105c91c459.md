# Audit Report

## Title
Partial Shard Pruner Failure Causes Inconsistent State KV Database State

## Summary
The StateKvPruner parallel shard pruning implementation commits metadata progress before verifying all shard pruners complete successfully. When one or more shard pruners fail during `par_iter().try_for_each()`, the metadata progress remains committed while only some shards have pruned their data, leading to permanently inconsistent database state where historical state queries return different results depending on which shard a key maps to.

## Finding Description

The StateKvPruner coordinates pruning across a metadata pruner and multiple shard pruners. The critical flaw occurs in the pruning sequence: [1](#0-0) 

The metadata pruner executes first and commits `DbMetadataKey::StateKvPrunerProgress` to disk before shard pruning begins. When shards prune in parallel using `par_iter().try_for_each()`, each shard independently commits its deletions and progress: [2](#0-1) 

Each shard's `write_schemas` call commits atomically to its individual database, but there is no distributed transaction across shards. If shard 2 fails (e.g., disk full, I/O error), shards 0-1 have already committed their deletions.

The metadata pruner when sharding is enabled only iterates through shards without performing actual deletions, then commits its progress: [3](#0-2) 

On node restart or initialization, `min_readable_version` is derived from the metadata progress, not individual shard progress: [4](#0-3) 

This creates a critical inconsistency: the system believes versions up to the metadata progress are still pruned uniformly, but some shards have deleted data while others retain it. When reading state at a partially-pruned version: [5](#0-4) 

Keys mapping to successfully-pruned shards will return stale or missing data, while keys in failed shards return correct historical data.

**Exploitation Scenario:**
1. Node begins pruning versions 100â†’200
2. Metadata pruner commits progress=200
3. Shard 0: prunes successfully, commits progress=200, deletes versions 100-200
4. Shard 1: prunes successfully, commits progress=200, deletes versions 100-200  
5. Shard 2: fails (disk full), progress remains 100, versions 100-200 retained
6. Error propagates, global progress stays 100
7. Node restarts, reads `StateKvPrunerProgress`=200, sets `min_readable_version`=200
8. State query at version 150: keys in shards 0-1 return wrong data, keys in shard 2 return correct data

This breaks the **State Consistency** invariant: state transitions must be atomic and verifiable. Different keys at the same version produce inconsistent results.

## Impact Explanation

This is a **High Severity** issue per Aptos bug bounty criteria for the following reasons:

1. **Significant Protocol Violations**: Violates the State Consistency invariant (#4 from critical invariants). The database enters a state where the same version query produces different results based on key-to-shard mapping.

2. **State Inconsistencies Requiring Intervention**: Once the partial failure occurs, the database remains permanently inconsistent. The pruner will not automatically recover because:
   - Metadata progress indicates versions are pruned
   - Successfully-pruned shards have no data to re-verify
   - Failed shards retain data but their progress is out of sync
   - No consistency check validates shard progress matches metadata progress

3. **Potential Consensus Impact**: While not directly causing consensus divergence, if different validator nodes experience different partial failures:
   - State sync requests for historical versions will return inconsistent data
   - Snapshot creation at partially-pruned versions will produce invalid snapshots
   - This could cascade into validator disagreements about state availability

4. **Affects Core Storage Layer**: This is not a peripheral feature but the primary state pruning mechanism for the sharded StateKV database, affecting all historical state queries.

The issue does not reach Critical severity because:
- It doesn't directly cause loss of funds or consensus safety violations
- It requires intervention but doesn't require a hardfork
- It affects availability of historical state, not current state correctness

## Likelihood Explanation

**High Likelihood** - This vulnerability can be triggered through natural operational failures:

1. **Common Failure Scenarios**:
   - Disk space exhaustion during pruning (very common in production)
   - I/O errors on specific shard databases
   - File system corruption on individual shard paths
   - Process termination mid-pruning (OOM killer, crash)

2. **No Special Conditions Required**:
   - Sharding must be enabled (standard configuration for production)
   - Normal pruning operations automatically trigger the vulnerable code path
   - No attacker action required - natural system failures suffice

3. **Difficult to Detect**:
   - No error logs persist after restart (error occurred but was handled)
   - State queries still "work" but return incorrect results
   - Metrics show metadata progress but not per-shard consistency
   - Silent data corruption of historical state

4. **Permanent Impact**:
   - Once occurred, manual intervention required to detect and remediate
   - No automatic recovery mechanism exists
   - Each subsequent pruning attempt may worsen inconsistency

## Recommendation

Implement atomic shard pruning with two-phase commit protocol:

**Phase 1: Prepare** - All shards validate and prepare deletions without committing:
```rust
// Collect all shard batches first
let shard_batches: Vec<_> = THREAD_MANAGER.get_background_pool().install(|| {
    self.shard_pruners.par_iter().map(|shard_pruner| {
        shard_pruner.prepare_prune(progress, current_batch_target_version)
    }).collect::<Result<Vec<_>>>()
})?;
```

**Phase 2: Commit** - Only after all shards succeed, commit metadata then shards:
```rust
// Metadata pruner can now commit since all shards prepared successfully
self.metadata_pruner.prune(progress, current_batch_target_version)?;

// Commit all shard batches sequentially (quick operation)
for (shard_pruner, batch) in self.shard_pruners.iter().zip(shard_batches) {
    shard_pruner.commit_batch(batch)?;
}
```

**Alternative: Consistency Validation on Startup**

Add validation that checks all shard progress matches metadata progress:

```rust
pub fn new(state_kv_db: Arc<StateKvDb>) -> Result<Self> {
    let metadata_pruner = StateKvMetadataPruner::new(Arc::clone(&state_kv_db));
    let metadata_progress = metadata_pruner.progress()?;
    
    let shard_pruners = if state_kv_db.enabled_sharding() {
        let num_shards = state_kv_db.num_shards();
        let mut shard_pruners = Vec::with_capacity(num_shards);
        
        for shard_id in 0..num_shards {
            let shard_pruner = StateKvShardPruner::new(
                shard_id,
                state_kv_db.db_shard_arc(shard_id),
                metadata_progress,
            )?;
            
            // Validate shard progress matches metadata progress
            let shard_progress = get_or_initialize_subpruner_progress(
                &state_kv_db.db_shard(shard_id),
                &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
                metadata_progress,
            )?;
            
            if shard_progress != metadata_progress {
                bail!(
                    "Shard {} progress ({}) inconsistent with metadata progress ({})",
                    shard_id, shard_progress, metadata_progress
                );
            }
            
            shard_pruners.push(shard_pruner);
        }
        shard_pruners
    } else {
        Vec::new()
    };
    
    Ok(StateKvPruner { /* ... */ })
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_temppath::TempPath;
    use std::sync::Arc;
    
    #[test]
    fn test_partial_shard_failure_inconsistency() {
        // Setup: Create sharded StateKvDb
        let tmpdir = TempPath::new();
        let mut config = RocksdbConfigs::default();
        config.enable_storage_sharding = true;
        
        let state_kv_db = Arc::new(StateKvDb::new(
            &StorageDirPaths::from_path(&tmpdir),
            config,
            None,
            None,
            false,
            ledger_db,
        ).unwrap());
        
        // Commit state at versions 100-200 across all shards
        for version in 100..=200 {
            // Write test state keys distributed across shards
            let mut batch = ShardedStateKvSchemaBatch::new();
            for shard_id in 0..NUM_STATE_SHARDS {
                let key = create_test_key_for_shard(shard_id);
                batch.put_state_value(shard_id, key, version, test_value());
            }
            state_kv_db.commit(batch).unwrap();
        }
        
        // Simulate partial failure: manually corrupt shard 2 before pruning
        let shard_2_path = state_kv_db.db_shard_path(2);
        std::fs::set_permissions(shard_2_path, Permissions::from_mode(0o000)).unwrap();
        
        // Attempt pruning 100->200 (will partially fail)
        let pruner = StateKvPruner::new(state_kv_db.clone()).unwrap();
        let result = pruner.prune(100); // Will fail due to shard 2
        assert!(result.is_err());
        
        // Restore permissions and restart pruner (simulating node restart)
        std::fs::set_permissions(shard_2_path, Permissions::from_mode(0o755)).unwrap();
        let pruner_after_restart = StateKvPruner::new(state_kv_db.clone()).unwrap();
        
        // Verify inconsistency: query version 150 across shards
        for shard_id in 0..NUM_STATE_SHARDS {
            let key = create_test_key_for_shard(shard_id);
            let result = state_kv_db.get_state_value_with_version_by_version(&key, 150);
            
            if shard_id < 2 {
                // Shards 0-1 pruned successfully - data is missing!
                assert!(result.unwrap().is_none(), 
                    "Shard {} should have pruned version 150", shard_id);
            } else {
                // Shard 2 failed - data still exists
                assert!(result.unwrap().is_some(),
                    "Shard {} should still have version 150", shard_id);
            }
        }
        
        // This demonstrates inconsistent state: same version query returns
        // different results based on which shard the key maps to
        println!("VULNERABILITY CONFIRMED: Shards have inconsistent pruning state");
    }
}
```

**Notes:**
- This vulnerability affects all Aptos nodes running with sharded StateKV storage enabled
- The issue is latent - nodes may be running with inconsistent shard states without awareness
- Historical state queries and state sync operations will produce unpredictable results at partially-pruned version ranges
- Detection requires explicit per-shard progress verification, which is not currently implemented
- Manual recovery requires identifying the minimum shard progress and re-running pruning from that point, or restoring from a consistent snapshot

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L64-78)
```rust
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L52-72)
```rust
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_metadata_pruner.rs (L35-72)
```rust
        if self.state_kv_db.enabled_sharding() {
            let num_shards = self.state_kv_db.num_shards();
            // NOTE: This can be done in parallel if it becomes the bottleneck.
            for shard_id in 0..num_shards {
                let mut iter = self
                    .state_kv_db
                    .db_shard(shard_id)
                    .iter::<StaleStateValueIndexByKeyHashSchema>()?;
                iter.seek(&current_progress)?;
                for item in iter {
                    let (index, _) = item?;
                    if index.stale_since_version > target_version {
                        break;
                    }
                }
            }
        } else {
            let mut iter = self
                .state_kv_db
                .metadata_db()
                .iter::<StaleStateValueIndexSchema>()?;
            iter.seek(&current_progress)?;
            for item in iter {
                let (index, _) = item?;
                if index.stale_since_version > target_version {
                    break;
                }
                batch.delete::<StaleStateValueIndexSchema>(&index)?;
                batch.delete::<StateValueSchema>(&(index.state_key, index.version))?;
            }
        }

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        self.state_kv_db.metadata_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L93-99)
```rust

        let min_readable_version =
            pruner_utils::get_state_kv_pruner_progress(&state_kv_db).expect("Must succeed.");

        PRUNER_VERSIONS
            .with_label_values(&["state_kv_pruner", "min_readable"])
            .set(min_readable_version as i64);
```

**File:** storage/aptosdb/src/state_kv_db.rs (L374-401)
```rust
    pub(crate) fn get_state_value_with_version_by_version(
        &self,
        state_key: &StateKey,
        version: Version,
    ) -> Result<Option<(Version, StateValue)>> {
        let mut read_opts = ReadOptions::default();

        // We want `None` if the state_key changes in iteration.
        read_opts.set_prefix_same_as_start(true);
        if !self.enabled_sharding() {
            let mut iter = self
                .db_shard(state_key.get_shard_id())
                .iter_with_opts::<StateValueSchema>(read_opts)?;
            iter.seek(&(state_key.clone(), version))?;
            Ok(iter
                .next()
                .transpose()?
                .and_then(|((_, version), value_opt)| value_opt.map(|value| (version, value))))
        } else {
            let mut iter = self
                .db_shard(state_key.get_shard_id())
                .iter_with_opts::<StateValueByKeyHashSchema>(read_opts)?;
            iter.seek(&(state_key.hash(), version))?;
            Ok(iter
                .next()
                .transpose()?
                .and_then(|((_, version), value_opt)| value_opt.map(|value| (version, value))))
        }
```
