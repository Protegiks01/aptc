# Audit Report

## Title
Memory Ordering Vulnerability in Block-STM Estimate Flag Causes Consensus Safety Violation

## Summary
The `mark_estimate()` and `is_estimate()` functions in `versioned_data.rs` use `Ordering::Relaxed` for atomic operations on the estimate flag, providing insufficient memory ordering guarantees. This allows threads to read stale estimate flag values even after `mark_estimate()` returns, leading to missed dependencies in parallel transaction execution and potential consensus splits between validators.

## Finding Description

Block-STM uses an "estimate" flag to signal when a transaction's write-set is being re-executed after validation failure. When transaction T1 at index I1 aborts, the system calls `mark_estimate()` to mark all its written entries as estimates. [1](#0-0) 

The critical issue is in the implementation of the estimate flag synchronization: [2](#0-1) 

Both operations use `Ordering::Relaxed`, which provides **no synchronization guarantees** between threads. This creates a race condition:

**Race Scenario:**
1. **Thread A (Validator Node 1):** Transaction T1 validation fails, calls `update_transaction_on_abort()` which invokes `mark_estimate()` on key K with `Ordering::Relaxed` store
2. **Thread B (Validator Node 1):** Concurrently executing transaction T2 (which depends on T1), calls `read()` on key K
3. The `read()` function checks `is_estimate()` with `Ordering::Relaxed` load [3](#0-2) 
4. **Due to weak memory ordering**, Thread B may observe the **stale** (non-estimate) flag value even though Thread A's store has completed
5. Thread B returns the value instead of `MVDataError::Dependency`, proceeds with execution using potentially stale data
6. During validation, the **same race occurs** - validation calls `fetch_data_no_record()` which internally uses the same `is_estimate()` check [4](#0-3) 
7. Validation incorrectly passes, transaction commits with non-deterministic results

**Evidence of Incorrect Pattern:**

The codebase itself demonstrates the correct pattern in a similar context. The `Entry` struct in `code_cache_global.rs` uses proper memory ordering for an analogous invalidation flag: [5](#0-4) 

This confirms that:
1. The developers understand proper memory ordering
2. The pattern in `versioned_data.rs` is incorrect by comparison
3. `Release/Acquire` ordering is necessary for cross-thread flag synchronization

**Why This Breaks Deterministic Execution:**

With `Ordering::Relaxed`:
- CPU can reorder stores in the store buffer
- No guarantee when writes become visible to other cores
- Different cores can observe writes in different orders
- More pronounced on weakly-ordered architectures (ARM)

This means **different validator nodes** executing the same block can:
- Have different timing of when they observe the estimate flag
- Make different decisions about whether to wait for dependencies
- Execute transactions with different read-sets
- Produce **different state roots** for identical input blocks

This violates Invariant #1: **Deterministic Execution**.

## Impact Explanation

**Critical Severity - Consensus Safety Violation**

This vulnerability meets the **Critical Severity** criteria ($1,000,000) in the Aptos Bug Bounty program:

1. **Consensus/Safety violations**: Different validators can produce different state roots for the same block, breaking Byzantine Fault Tolerance assumptions. This is the most severe class of blockchain vulnerabilities.

2. **Non-recoverable network partition**: When validators diverge on state roots:
   - They cannot reach consensus on subsequent blocks
   - Network splits into incompatible forks
   - Requires hard fork to recover and manual intervention to choose canonical chain
   - All post-divergence transactions become invalid on one fork

3. **State Consistency violation**: Violates the fundamental invariant that all honest validators must agree on state transitions.

**Specific Attack Vector:**

An attacker doesn't need to actively exploit this - the vulnerability manifests naturally under high parallel execution load:
- Multiple transactions writing to shared keys
- High validation failure rate triggers frequent `mark_estimate()` calls
- Concurrent execution increases probability of race condition
- ARM-based validator nodes (more weakly-ordered than x86) are especially vulnerable

The bug is **consensus-breaking** because it makes execution non-deterministic based on CPU timing and memory architecture rather than transaction inputs.

## Likelihood Explanation

**High Likelihood** of occurring in production:

1. **Parallel execution is default**: Block-STM runs with multiple worker threads by default, creating continuous opportunity for the race condition

2. **Natural trigger conditions**: 
   - Validation failures are normal during optimistic execution
   - Each failure triggers `mark_estimate()` 
   - Concurrent transactions naturally read recently-written keys
   - No special inputs required - happens during normal operation

3. **Architecture dependence**:
   - x86 Total Store Order (TSO) may mask the bug in testing, making it harder to detect
   - ARM validators (used by some operators) have weaker ordering, making bug more likely
   - Different CPU architectures → different manifestation rates → **non-deterministic network behavior**

4. **Validation amplification**: The bug can occur **twice** per transaction:
   - During initial read (execution phase)
   - During validation phase (re-read)
   - Both use the same vulnerable code path

5. **No error detection**: When the race occurs, there's no runtime error - execution appears normal but produces wrong results

## Recommendation

**Fix: Use Acquire/Release memory ordering**

Replace `Ordering::Relaxed` with proper synchronization:

```rust
impl<V> Entry<V> {
    pub(crate) fn is_estimate(&self) -> bool {
        // Use Acquire to ensure we see all writes that happened-before
        // the Release store in mark_estimate()
        self.flag.load(Ordering::Acquire) == FLAG_ESTIMATE
    }

    pub(crate) fn mark_estimate(&self) {
        // Use Release to ensure this write is visible to subsequent
        // Acquire loads in is_estimate()
        self.flag.store(FLAG_ESTIMATE, Ordering::Release);
    }
}
```

**Justification:**
- `Ordering::Release` on store establishes a happens-before relationship
- `Ordering::Acquire` on load synchronizes with the Release store
- Guarantees that when a thread observes `FLAG_ESTIMATE`, it also sees all memory operations that happened before `mark_estimate()` was called
- Minimal performance impact (typically 1-2 CPU cycles on modern processors)
- Matches the pattern already used correctly in `code_cache_global.rs`

**Alternative (more conservative):** Use `Ordering::SeqCst` for both operations if absolute strongest guarantees are desired, though `Release/Acquire` is sufficient and more performant.

## Proof of Concept

**Conceptual PoC (Rust pseudocode demonstrating the race):**

```rust
// Thread 1: Abort transaction
fn abort_transaction(entry: &Entry<V>) {
    // Mark as estimate with Relaxed ordering
    entry.mark_estimate(); // Relaxed store
    // Continue with abort cleanup...
}

// Thread 2: Read during execution (concurrent with Thread 1)
fn execute_transaction(entry: &Entry<V>) -> Result<Value, Dependency> {
    // Check estimate flag with Relaxed ordering
    if entry.is_estimate() { // Relaxed load - MAY SEE STALE VALUE
        return Err(Dependency(...));
    }
    // Proceeds with execution using potentially stale data
    Ok(entry.value)
}

// On x86 (strong TSO): Race rarely manifests
// On ARM (weak ordering): Race occurs frequently
// Different CPU architectures → different behaviors → consensus split
```

**Reproduction Strategy:**

1. Set up two validator nodes with different CPU architectures (x86 and ARM)
2. Submit a block with high parallelism:
   - 100 transactions, many writing to shared keys
   - Designed to trigger validation failures (conflicting writes)
3. Execute block on both nodes simultaneously
4. Compare state roots

**Expected Result:** Under sufficient load and validation failures, ARM node and x86 node will eventually produce different state roots for the same block due to different manifestation rates of the memory ordering bug.

**Practical Testing:**

Run Block-STM under ThreadSanitizer or with explicit memory barriers disabled on ARM to trigger the race more reliably. Insert artificial delays between `mark_estimate()` return and `is_estimate()` check to widen the race window.

---

**Notes:**

This is a subtle but critical bug that violates the fundamental blockchain invariant of deterministic execution. The fix is straightforward (change two `Ordering` parameters), but the impact of leaving it unfixed is severe - potential network partition and consensus failure. The existence of the correct pattern in `code_cache_global.rs` strongly suggests this is an oversight rather than an intentional design choice.

### Citations

**File:** aptos-move/block-executor/src/executor_utilities.rs (L321-325)
```rust
    // Not valid and successfully aborted, mark the latest write/delta sets as estimates.
    if let Some(keys) = last_input_output.modified_resource_keys(txn_idx) {
        for (k, _) in keys {
            versioned_cache.data().mark_estimate(&k, txn_idx);
        }
```

**File:** aptos-move/mvhashmap/src/versioned_data.rs (L99-105)
```rust
    pub(crate) fn is_estimate(&self) -> bool {
        self.flag.load(Ordering::Relaxed) == FLAG_ESTIMATE
    }

    pub(crate) fn mark_estimate(&self) {
        self.flag.store(FLAG_ESTIMATE, Ordering::Relaxed);
    }
```

**File:** aptos-move/mvhashmap/src/versioned_data.rs (L258-268)
```rust
        while let Some((idx, entry)) = iter.next_back() {
            if entry.is_estimate() {
                debug_assert!(
                    maybe_reader_incarnation.is_none(),
                    "Entry must not be marked as estimate for BlockSTMv2"
                );
                // Found a dependency.
                return Err(Dependency(
                    idx.idx().expect("May not depend on storage version"),
                ));
            }
```

**File:** aptos-move/block-executor/src/captured_reads.rs (L920-943)
```rust
        for (key, read) in iter {
            // We use fetch_data even with BlockSTMv2, because we don't want to record reads.
            if !match data_map.fetch_data_no_record(key, idx_to_validate) {
                Ok(Versioned(version, value)) => {
                    matches!(
                        self.data_read_comparator.compare_data_reads(
                            &DataRead::from_value_with_layout(version, value),
                            read
                        ),
                        DataReadComparison::Contains
                    )
                },
                Ok(Resolved(value)) => matches!(
                    self.data_read_comparator
                        .compare_data_reads(&DataRead::Resolved(value), read),
                    DataReadComparison::Contains
                ),
                // Dependency implies a validation failure, and if the original read were to
                // observe an unresolved delta, it would set the aggregator base value in the
                // multi-versioned data-structure, resolve, and record the resolved value.
                Err(Dependency(_))
                | Err(Unresolved(_))
                | Err(DeltaApplicationFailure)
                | Err(Uninitialized) => false,
```

**File:** aptos-move/block-executor/src/code_cache_global.rs (L55-63)
```rust
    /// Marks the module as overridden.
    fn mark_overridden(&self) {
        self.overridden.store(true, Ordering::Release)
    }

    /// Returns true if the module is not overridden.
    fn is_not_overridden(&self) -> bool {
        !self.overridden.load(Ordering::Acquire)
    }
```
