# Audit Report

## Title
Database Recovery Failure Due to Decode Errors in Truncation Process

## Summary
The automatic database recovery mechanism can fail when corrupted data cannot be decoded during truncation, leaving the node unable to start and requiring manual intervention. This violates the system's design goal of automatic recovery from partial writes during crashes or disk-full conditions.

## Finding Description

The AptosDB uses a two-phase commit mechanism with automatic recovery via truncation. When the system detects inconsistent state on restart (e.g., after a crash during commit), it attempts to truncate databases back to the last known consistent version. However, this truncation process requires **decoding database entries** to determine what to delete. [1](#0-0) 

The recovery flow works as follows:

1. On startup, `sync_commit_progress` reads `OverallCommitProgress` and compares it with individual database progress markers
2. If databases have progressed beyond `OverallCommitProgress`, it calls `truncate_state_kv_db` to roll back partial writes [2](#0-1) 

3. During truncation, the system iterates through stale index entries and **decodes them** to identify what data to delete: [3](#0-2) 

4. The decode operations use schema-defined decoders that validate data integrity: [4](#0-3) [5](#0-4) 

**The Vulnerability:**

If database entries become corrupted (due to partial writes during crashes, disk I/O errors, memory corruption, or disk-full conditions during the parallel shard commits), the decode operations will fail with errors like "Unexpected data len". This causes the truncation to fail, and since truncation uses `.expect()`, the node panics on startup. [6](#0-5) 

On subsequent restart attempts, the same corrupted data causes the same decode failure, creating an infinite restart loop.

**Failure Scenario:**

1. During transaction commit, 16 shards are written in parallel
2. System runs out of disk space or encounters I/O errors after some shards succeed
3. RocksDB may have partially flushed data, leaving corrupted index entries
4. Process crashes with `.unwrap()` panic
5. On restart, `sync_commit_progress` attempts truncation
6. Truncation tries to decode corrupted stale index entries
7. Decode fails with "Unexpected data len" error
8. Truncation fails and panics: "Failed to truncate state K/V db."
9. Node cannot start without manual database repair or restoration from backup

## Impact Explanation

This qualifies as **Medium Severity** per the Aptos bug bounty criteria: "State inconsistencies requiring intervention."

**Impact:**
- Validator node cannot restart automatically after certain crash scenarios
- Requires manual intervention (database repair tools, backup restoration, or direct RocksDB manipulation)
- Validator downtime leads to loss of staking rewards
- In coordinated failure scenarios (e.g., network-wide disk space exhaustion), multiple validators could be affected simultaneously, potentially impacting consensus liveness
- Does not directly cause fund loss or consensus safety violations, but impacts availability

The issue breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable." While the atomicity is maintained, the automatic recovery mechanism (which is part of ensuring consistency) can fail.

## Likelihood Explanation

**Likelihood: Medium**

This can occur in several realistic scenarios:
- **Disk full conditions**: Validators running low on storage during high transaction throughput
- **Hardware failures**: Disk I/O errors, memory corruption from bad RAM
- **Crash during critical window**: Process killed (SIGKILL, OOM killer, power loss) during the parallel shard commit phase
- **Cascading failures**: Multiple validators experiencing similar resource exhaustion simultaneously

While not a direct attack vector, it represents a critical operational failure mode that the system should handle gracefully. The parallel commit of 16 shards increases the window of vulnerability where partial writes can occur.

## Recommendation

Implement defensive error handling in the truncation process:

1. **Add corruption-tolerant truncation**: When decode fails during truncation, use raw key iteration to delete entries without decoding:

```rust
fn delete_state_value_and_index_robust(
    state_kv_db_shard: &DB,
    start_version: Version,
    batch: &mut SchemaBatch,
    enable_sharding: bool,
) -> Result<()> {
    // Try normal decoding first
    match delete_state_value_and_index(state_kv_db_shard, start_version, batch, enable_sharding) {
        Ok(()) => Ok(()),
        Err(e) => {
            // If decode fails, fall back to raw key deletion
            warn!("Decode failed during truncation, using raw key deletion: {}", e);
            delete_by_raw_keys(state_kv_db_shard, start_version, batch, enable_sharding)
        }
    }
}

fn delete_by_raw_keys(
    state_kv_db_shard: &DB,
    start_version: Version,
    batch: &mut SchemaBatch,
    enable_sharding: bool,
) -> Result<()> {
    // Use raw iteration without decoding to handle corrupted entries
    let cf_name = if enable_sharding {
        STALE_STATE_VALUE_INDEX_BY_KEY_HASH_CF_NAME
    } else {
        STALE_STATE_VALUE_INDEX_CF_NAME
    };
    
    let mut iter = state_kv_db_shard.raw_iterator_cf(cf_name)?;
    iter.seek(&start_version.to_be_bytes());
    
    while iter.valid() {
        if let Some(key) = iter.key() {
            batch.raw_delete(cf_name, key.to_vec())?;
        }
        iter.next();
    }
    Ok(())
}
```

2. **Add checksum verification**: Include checksums in encoded data to detect corruption early

3. **Implement progressive truncation**: Delete data in smaller batches with progress checkpoints to avoid getting stuck on single corrupted entries

4. **Add database repair tool**: Provide a CLI tool for operators to manually clean corrupted entries when automatic recovery fails

## Proof of Concept

```rust
// This reproduces the vulnerability by simulating corrupted database entries
#[test]
fn test_truncation_fails_on_corrupted_index() {
    use tempfile::tempdir;
    use aptos_types::state_store::state_value::StaleStateValueByKeyHashIndex;
    
    // Setup test database
    let tmpdir = tempdir().unwrap();
    let db = DB::open_cf(...);
    
    // Write valid index entry at version 100
    let index = StaleStateValueByKeyHashIndex {
        stale_since_version: 100,
        version: 100,
        state_key_hash: HashValue::random(),
    };
    let mut batch = SchemaBatch::new();
    batch.put::<StaleStateValueIndexByKeyHashSchema>(&index, &()).unwrap();
    db.write_schemas(batch).unwrap();
    
    // Simulate corruption by writing malformed data directly
    let corrupted_key = vec![0u8, 0u8, 0u8, 100u8]; // Wrong size, missing fields
    db.put_cf_raw(
        STALE_STATE_VALUE_INDEX_BY_KEY_HASH_CF_NAME,
        &corrupted_key,
        &[],
    ).unwrap();
    
    // Attempt truncation - this should fail with decode error
    let result = truncate_state_kv_db_single_shard(&state_kv_db, 0, 99);
    
    // Verify it fails on decode
    assert!(result.is_err());
    assert!(result.unwrap_err().to_string().contains("Unexpected data len"));
    
    // On actual startup, this would cause:
    // sync_commit_progress -> truncate_state_kv_db -> .expect() -> PANIC
    // Node cannot start, requires manual intervention
}
```

## Notes

This vulnerability represents a gap between the intended design (automatic recovery from crashes) and the actual implementation (recovery requires valid, decodable data). While not a direct security exploit, it's a critical operational issue that can cause extended validator downtime in scenarios the system should handle gracefully. The issue is particularly concerning given the parallel nature of shard commits, which increases the probability of partial writes during failures.

### Citations

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L551-581)
```rust
fn delete_state_value_and_index(
    state_kv_db_shard: &DB,
    start_version: Version,
    batch: &mut SchemaBatch,
    enable_sharding: bool,
) -> Result<()> {
    if enable_sharding {
        let mut iter = state_kv_db_shard.iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&start_version)?;

        for item in iter {
            let (index, _) = item?;
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(
                index.state_key_hash,
                index.stale_since_version,
            ))?;
        }
    } else {
        let mut iter = state_kv_db_shard.iter::<StaleStateValueIndexSchema>()?;
        iter.seek(&start_version)?;

        for item in iter {
            let (index, _) = item?;
            batch.delete::<StaleStateValueIndexSchema>(&index)?;
            batch.delete::<StateValueSchema>(&(index.state_key, index.stale_since_version))?;
        }
    }

    Ok(())
}
```

**File:** storage/aptosdb/src/schema/stale_state_value_index_by_key_hash/mod.rs (L49-62)
```rust
    fn decode_key(data: &[u8]) -> Result<Self> {
        const VERSION_SIZE: usize = size_of::<Version>();

        ensure_slice_len_eq(data, 2 * VERSION_SIZE + HashValue::LENGTH)?;
        let stale_since_version = (&data[..VERSION_SIZE]).read_u64::<BigEndian>()?;
        let version = (&data[VERSION_SIZE..2 * VERSION_SIZE]).read_u64::<BigEndian>()?;
        let state_key_hash = HashValue::from_slice(&data[2 * VERSION_SIZE..])?;

        Ok(Self {
            stale_since_version,
            version,
            state_key_hash,
        })
    }
```

**File:** storage/aptosdb/src/schema/mod.rs (L71-89)
```rust
fn ensure_slice_len_eq(data: &[u8], len: usize) -> Result<()> {
    ensure!(
        data.len() == len,
        "Unexpected data len {}, expected {}.",
        data.len(),
        len,
    );
    Ok(())
}

fn ensure_slice_len_gt(data: &[u8], len: usize) -> Result<()> {
    ensure!(
        data.len() > len,
        "Unexpected data len {}, expected to be greater than {}.",
        data.len(),
        len,
    );
    Ok(())
}
```

**File:** storage/aptosdb/src/state_kv_db.rs (L177-208)
```rust
    pub(crate) fn commit(
        &self,
        version: Version,
        state_kv_metadata_batch: Option<SchemaBatch>,
        sharded_state_kv_batches: ShardedStateKvSchemaBatch,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit"]);
        {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_shards"]);
            THREAD_MANAGER.get_io_pool().scope(|s| {
                let mut batches = sharded_state_kv_batches.into_iter();
                for shard_id in 0..NUM_STATE_SHARDS {
                    let state_kv_batch = batches
                        .next()
                        .expect("Not sufficient number of sharded state kv batches");
                    s.spawn(move |_| {
                        // TODO(grao): Consider propagating the error instead of panic, if necessary.
                        self.commit_single_shard(version, shard_id, state_kv_batch)
                            .unwrap_or_else(|err| {
                                panic!("Failed to commit shard {shard_id}: {err}.")
                            });
                    });
                }
            });
        }
        if let Some(batch) = state_kv_metadata_batch {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_metadata"]);
            self.state_kv_metadata_db.write_schemas(batch)?;
        }

        self.write_progress(version)
    }
```
