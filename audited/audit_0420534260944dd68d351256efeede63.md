# Audit Report

## Title
Missing Input Validation in Backup Service Allows Resource Exhaustion Through Long-Lived Database Iterator Snapshots

## Summary
The backup service's `get_transaction_iter()` endpoint accepts arbitrary `num_transactions` values without validation, bypassing the `MAX_REQUEST_LIMIT` protection enforced in the public database API. While the iterators themselves are lazy and don't pre-allocate unbounded memory, requesting extremely large values creates long-lived RocksDB snapshots across 5 database iterators that prevent compaction, leading to gradual memory accumulation, potential OOM, and validator node crashes. [1](#0-0) 

## Finding Description

The vulnerability exists in the backup handler's transaction iterator endpoint, which directly calls internal database methods without input validation.

The backup service HTTP endpoint accepts `num_transactions` as a usize parameter from the URL path: [2](#0-1) 

This value is passed directly to `BackupHandler::get_transaction_iter()`, which creates 5 separate database iterators without any limit checking: [3](#0-2) 

In contrast, the public database API enforces a `MAX_REQUEST_LIMIT` of 20,000 transactions: [4](#0-3) [5](#0-4) 

The overflow check in `expect_continuous_versions()` only prevents arithmetic overflow, not large values: [6](#0-5) 

When `start_version=0` and `num_transactions=u64::MAX-1`, the check `0.checked_add(u64::MAX-1)` succeeds, creating iterators with `end_version=u64::MAX-1`.

**Attack Mechanism:**

1. Attacker sends: `GET /transactions/0/{large_number}` where large_number >> 20,000
2. Backup handler creates 5 RocksDB iterators (txn, txn_info, events, write_set, aux_info)
3. Each iterator holds a RocksDB snapshot, pinning data at that point in time
4. The iteration consumes transactions one-by-one until the database is exhausted
5. While the operation runs (potentially hours/days for billions of transactions):
   - New transactions continue being committed to the database
   - Old data versions cannot be compacted because they're pinned by the snapshots
   - RocksDB memory usage grows unboundedly as old versions accumulate
   - Disk space increases as compaction is blocked
6. Eventually, the node experiences OOM or disk full errors, causing a crash

The backup service runs embedded in the validator node process: [7](#0-6) 

## Impact Explanation

**Severity: High** (up to $50,000 per Aptos bug bounty)

This vulnerability enables:
- **Validator node slowdowns**: RocksDB contention degrades performance for all operations
- **Validator node crash**: OOM or disk full errors can crash the node
- **Consensus participation failure**: Crashed nodes cannot participate in consensus, affecting network liveness if multiple nodes are attacked

The attack breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

While the backup service defaults to localhost binding, production configurations may expose it externally for backup operations: [8](#0-7) 

## Likelihood Explanation

**Likelihood: Low to Medium**

**Factors increasing likelihood:**
- No authentication or rate limiting on backup service endpoints
- Simple HTTP GET request is easy to craft
- Affects any node with externally accessible backup service
- Legitimate backup misconfigurations could trigger this unintentionally

**Factors decreasing likelihood:**
- Default configuration binds to localhost only (127.0.0.1:6186)
- Requires backup service to be exposed externally (configuration dependent)
- Attack requires sustained network access over hours/days for maximum impact

## Recommendation

Add input validation to the backup handler's `get_transaction_iter()` method to enforce `MAX_REQUEST_LIMIT`:

```rust
pub fn get_transaction_iter(
    &self,
    start_version: Version,
    num_transactions: usize,
) -> Result<
    impl Iterator<
        Item = Result<(
            Transaction,
            PersistedAuxiliaryInfo,
            TransactionInfo,
            Vec<ContractEvent>,
            WriteSet,
        )>,
    > + '_,
> {
    // Add validation
    error_if_too_many_requested(num_transactions as u64, MAX_REQUEST_LIMIT)?;
    
    let txn_iter = self
        .ledger_db
        .transaction_db()
        .get_transaction_iter(start_version, num_transactions)?;
    // ... rest of implementation
}
```

Additionally, consider:
1. Adding authentication/authorization to the backup service
2. Implementing per-client rate limiting
3. Adding monitoring for long-running backup operations
4. Documenting the security implications of exposing the backup service

## Proof of Concept

```rust
#[test]
fn test_backup_handler_resource_exhaustion() {
    use aptos_temppath::TempPath;
    use std::sync::Arc;
    
    // Setup test database
    let tmpdir = TempPath::new();
    let db = Arc::new(AptosDB::new_for_test(&tmpdir));
    
    // Populate with some test transactions
    // (omitted for brevity - use standard test setup)
    
    let backup_handler = db.get_backup_handler();
    
    // Attempt to request more than MAX_REQUEST_LIMIT
    let large_request = 1_000_000_000_usize; // 1 billion transactions
    
    // This should fail but currently succeeds if start_version + limit doesn't overflow
    let result = backup_handler.get_transaction_iter(0, large_request);
    
    // Currently, this will create iterators without error
    assert!(result.is_ok()); // Bug: should be Err(TooManyRequested)
    
    // The iterators will hold RocksDB snapshots indefinitely
    // while trying to iterate through billions of transactions
}
```

To demonstrate the actual HTTP attack:

```bash
# If backup service is exposed on 0.0.0.0:6186
curl "http://validator-node:6186/transactions/0/1000000000"
# This will tie up the backup service for an extremely long time
# and cause RocksDB snapshot retention issues
```

## Notes

While the security question's premise about "unbounded memory allocation across 5 separate iterators" is technically imprecise (the iterators are lazy and don't pre-allocate memory), there is a legitimate resource exhaustion vulnerability. The actual mechanism is RocksDB snapshot retention preventing compaction, causing gradual memory accumulation over time rather than immediate unbounded allocation. The impact remains severe: validator node crashes and consensus participation failures.

### Citations

**File:** storage/aptosdb/src/backup/backup_handler.rs (L41-76)
```rust
    pub fn get_transaction_iter(
        &self,
        start_version: Version,
        num_transactions: usize,
    ) -> Result<
        impl Iterator<
                Item = Result<(
                    Transaction,
                    PersistedAuxiliaryInfo,
                    TransactionInfo,
                    Vec<ContractEvent>,
                    WriteSet,
                )>,
            > + '_,
    > {
        let txn_iter = self
            .ledger_db
            .transaction_db()
            .get_transaction_iter(start_version, num_transactions)?;
        let mut txn_info_iter = self
            .ledger_db
            .transaction_info_db()
            .get_transaction_info_iter(start_version, num_transactions)?;
        let mut event_vec_iter = self
            .ledger_db
            .event_db()
            .get_events_by_version_iter(start_version, num_transactions)?;
        let mut write_set_iter = self
            .ledger_db
            .write_set_db()
            .get_write_set_iter(start_version, num_transactions)?;
        let mut persisted_aux_info_iter = self
            .ledger_db
            .persisted_auxiliary_info_db()
            .get_persisted_auxiliary_info_iter(start_version, num_transactions)?;

```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L101-110)
```rust
    // GET transactions/<start_version>/<num_transactions>
    let bh = backup_handler.clone();
    let transactions = warp::path!(Version / usize)
        .map(move |start_version, num_transactions| {
            reply_with_bytes_sender(&bh, TRANSACTIONS, move |bh, sender| {
                bh.get_transaction_iter(start_version, num_transactions)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```

**File:** storage/storage-interface/src/lib.rs (L56-58)
```rust
// This is last line of defense against large queries slipping through external facing interfaces,
// like the API and State Sync, etc.
pub const MAX_REQUEST_LIMIT: u64 = 20_000;
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L477-491)
```rust
    fn get_transaction_iterator(
        &self,
        start_version: Version,
        limit: u64,
    ) -> Result<Box<dyn Iterator<Item = Result<Transaction>> + '_>> {
        gauged_api("get_transaction_iterator", || {
            error_if_too_many_requested(limit, MAX_REQUEST_LIMIT)?;
            self.error_if_ledger_pruned("Transaction", start_version)?;

            let iter = self
                .ledger_db
                .transaction_db()
                .get_transaction_iter(start_version, limit as usize)?;
            Ok(Box::new(iter) as Box<dyn Iterator<Item = Result<Transaction>> + '_>)
        })
```

**File:** storage/aptosdb/src/utils/iterators.rs (L88-102)
```rust
    fn expect_continuous_versions(
        self,
        first_version: Version,
        limit: usize,
    ) -> Result<ContinuousVersionIter<Self, T>> {
        Ok(ContinuousVersionIter {
            inner: self,
            first_version,
            expected_next_version: first_version,
            end_version: first_version
                .checked_add(limit as u64)
                .ok_or(AptosDbError::TooManyRequested(first_version, limit as u64))?,
            _phantom: Default::default(),
        })
    }
```

**File:** aptos-node/src/storage.rs (L63-73)
```rust
    let (aptos_db_reader, db_rw, backup_service) = match FastSyncStorageWrapper::initialize_dbs(
        node_config,
        internal_indexer_db.clone(),
        update_sender,
    )? {
        Either::Left(db) => {
            let (db_arc, db_rw) = DbReaderWriter::wrap(db);
            let db_backup_service =
                start_backup_service(node_config.storage.backup_service_address, db_arc.clone());
            maybe_apply_genesis(&db_rw, node_config)?;
            (db_arc as Arc<dyn DbReader>, db_rw, Some(db_backup_service))
```

**File:** config/src/config/storage_config.rs (L434-436)
```rust
    fn default() -> StorageConfig {
        StorageConfig {
            backup_service_address: SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 6186),
```
