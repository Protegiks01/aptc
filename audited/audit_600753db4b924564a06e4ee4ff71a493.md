# Audit Report

## Title
Exponential Window Failure Tracker Hysteresis Vulnerability Allows Permanent OptQS Disablement

## Summary
The `ExponentialWindowFailureTracker` in `consensus/src/liveness/proposal_status_tracker.rs` contains a critical asymmetry in its window growth and reset logic that creates a hysteresis trap. The failure window grows exponentially with each `PayloadUnavailable` timeout but only resets when ALL stored history (up to 100 rounds) consists of consecutive successes. This design flaw can permanently disable Optimistic Quorum Store (OptQS), forcing the network into a degraded performance state that is extremely difficult to recover from in production environments.

## Finding Description

The vulnerability exists in the window management logic within `compute_failure_window()`. [1](#0-0) 

The asymmetry manifests in two ways:

**Window Growth (Easy):**
When `last_consecutive_success_count == 0` (indicating a `PayloadUnavailable` failure just occurred), the window doubles until reaching `max_window`. [2](#0-1)  This means only 6-7 consecutive failures are needed to reach the maximum window size of 100.

**Window Reset (Extremely Difficult):**
The window only resets to 2 when `last_consecutive_success_count == past_round_statuses.len()`, requiring ALL stored history to be consecutive successes. [3](#0-2)  Since `past_round_statuses` is bounded by `max_window` (hardcoded to 100 in the instantiation), [4](#0-3)  this requires 100 consecutive successful rounds with ZERO `PayloadUnavailable` failures.

**OptQS Disablement Logic:**
OptQS is disabled when `last_consecutive_success_count < window`. [5](#0-4)  Once the window reaches 100, the system needs 100 consecutive perfect rounds to re-enable OptQS.

**How the Trap Works:**
1. Transient network issues (partitions, slowdowns, batch propagation delays) naturally cause `PayloadUnavailable` failures
2. After 6-7 such failures, window grows to 100
3. OptQS becomes disabled until 100 consecutive successes occur
4. Any single `PayloadUnavailable` failure in those 100 rounds resets `last_consecutive_success_count` to 0, restarting the recovery process
5. In a real distributed system with occasional network hiccups, achieving 100 consecutive perfect rounds is extremely unlikely

The system does track and exclude problematic authors, [6](#0-5)  but this exclusion mechanism doesn't reduce the window size. The window remains elevated, keeping OptQS disabled even after problematic validators are excluded.

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria under "Validator Node Slowdowns":

**Performance Degradation:**
OptQS (Optimistic Quorum Store) provides significant performance benefits by enabling optimistic proposals where validators can propose for round r+1 immediately after voting on round r, rather than waiting for QuorumCert formation. When OptQS is disabled, the network falls back to standard QuorumStore mode, which:
- Increases consensus latency per round
- Reduces overall blockchain throughput
- Degrades transaction finalization times

**Protocol Design Violation:**
The failure tracker is designed to temporarily disable OptQS during network issues and automatically re-enable it when conditions improve. The hysteresis flaw violates this design intent by creating a state where recovery becomes practically impossible, transforming temporary issues into permanent degradation.

**Network-Wide Impact:**
Unlike single-node issues, this affects the entire consensus network's performance. All validators suffer from the degraded OptQS availability, impacting the overall blockchain's ability to process transactions efficiently.

## Likelihood Explanation

**High Likelihood - Natural Occurrence:**

This vulnerability can be triggered without malicious actors through:
- Transient network partitions during validator maintenance or deployments
- Temporary validator node slowdowns due to resource constraints
- Network congestion causing batch propagation delays between validators
- Cloud infrastructure issues affecting connectivity between validator nodes
- Geographic network latency spikes

In production networks, occasional `PayloadUnavailable` events are expected. Once 6-7 such events occur (which could happen over days or weeks), the window reaches 100. From that point, the network needs 100 consecutive perfect roundsâ€”a condition extremely difficult to maintain in any real distributed system operating at scale.

**Recovery Difficulty:**
The 100-consecutive-success requirement creates a practical impossibility. Even if the underlying network issues are resolved and problematic validators are excluded, any single legitimate hiccup in batch propagation (which are inevitable in distributed systems) resets progress to zero, keeping OptQS permanently disabled.

## Recommendation

Implement a more balanced window decay mechanism:

```rust
fn compute_failure_window(&mut self) {
    self.last_consecutive_success_count = self.last_consecutive_statuses_matching(|reason| {
        !matches!(
            reason,
            NewRoundReason::Timeout(RoundTimeoutReason::PayloadUnavailable { .. })
        )
    });
    
    if self.last_consecutive_success_count == 0 {
        // Still double on failure
        self.window *= 2;
        self.window = self.window.min(self.max_window);
    } else if self.last_consecutive_success_count == self.past_round_statuses.len() {
        // Full reset as before
        self.window = 2;
    } else if self.last_consecutive_success_count >= self.window {
        // NEW: Gradually reduce window when consecutive successes exceed current window
        // This prevents hysteresis trap while maintaining conservative behavior
        self.window = (self.window / 2).max(2);
    }
}
```

This change maintains the exponential backoff on failures while adding gradual recovery: once consecutive successes exceed the current window threshold, the window halves (preventing it from staying at 100 indefinitely). This preserves the conservative approach while enabling practical recovery.

Alternative approach: Use a sliding window that counts failures within the last N rounds rather than requiring consecutive successes from the end, which better matches the documented intent in the comments. [7](#0-6) 

## Proof of Concept

```rust
#[test]
fn test_hysteresis_trap() {
    let (_signers, verifier) = random_validator_verifier(4, None, false);
    let mut tracker = ExponentialWindowFailureTracker::new(100, verifier.get_ordered_account_addresses());
    
    // Simulate 7 failures to reach max window
    for _ in 0..7 {
        tracker.push(NewRoundReason::Timeout(
            RoundTimeoutReason::PayloadUnavailable {
                missing_authors: BitVec::with_num_bits(4),
            },
        ));
    }
    assert_eq!(tracker.window, 100);
    
    // Now need 100 consecutive successes to reset
    // Simulate 99 successes followed by 1 failure
    for _ in 0..99 {
        tracker.push(NewRoundReason::QCReady);
    }
    assert_eq!(tracker.last_consecutive_success_count, 99);
    // OptQS still disabled: 99 < 100
    
    // Single failure resets all progress
    tracker.push(NewRoundReason::Timeout(
        RoundTimeoutReason::PayloadUnavailable {
            missing_authors: BitVec::with_num_bits(4),
        },
    ));
    assert_eq!(tracker.last_consecutive_success_count, 0);
    assert_eq!(tracker.window, 100); // Window stays at max
    
    // This demonstrates the hysteresis trap: any single failure in 100 rounds
    // prevents recovery, creating permanent OptQS disablement
}
```

## Notes

While the report's malicious attack vector is theoretically possible, the more significant concern is the natural occurrence scenario. In production environments, transient network issues are inevitable, and the current design makes recovery from even temporary problems extremely difficult. The behavior appears to be intentionally implemented and tested, but the design creates unintended consequences that constitute a practical denial-of-service against the OptQS performance optimization. This falls squarely within the "Validator Node Slowdowns" category of the bug bounty program.

### Citations

**File:** consensus/src/liveness/proposal_status_tracker.rs (L23-29)
```rust
/// A exponential window based algorithm to decide whether to go optimistic or not, based on
/// configurable number of past proposal statuses
///
/// Initialize the window at 2.
/// - For each proposal failure, double the window up to a MAX size
/// - If there are no failures within the window, then propose optimistic batch
/// - If there are no failures up to MAX proposals, reset the window to 2.
```

**File:** consensus/src/liveness/proposal_status_tracker.rs (L65-78)
```rust
    fn compute_failure_window(&mut self) {
        self.last_consecutive_success_count = self.last_consecutive_statuses_matching(|reason| {
            !matches!(
                reason,
                NewRoundReason::Timeout(RoundTimeoutReason::PayloadUnavailable { .. })
            )
        });
        if self.last_consecutive_success_count == 0 {
            self.window *= 2;
            self.window = self.window.min(self.max_window);
        } else if self.last_consecutive_success_count == self.past_round_statuses.len() {
            self.window = 2;
        }
    }
```

**File:** consensus/src/liveness/proposal_status_tracker.rs (L80-98)
```rust
    fn get_exclude_authors(&self) -> HashSet<Author> {
        let mut exclude_authors = HashSet::new();

        let limit = self.window;
        for round_reason in self.past_round_statuses.iter().rev().take(limit) {
            if let NewRoundReason::Timeout(RoundTimeoutReason::PayloadUnavailable {
                missing_authors,
            }) = round_reason
            {
                for author_idx in missing_authors.iter_ones() {
                    if let Some(author) = self.ordered_authors.get(author_idx) {
                        exclude_authors.insert(*author);
                    }
                }
            }
        }

        exclude_authors
    }
```

**File:** consensus/src/liveness/proposal_status_tracker.rs (L137-143)
```rust
        if tracker.last_consecutive_success_count < tracker.window {
            warn!(
                "Skipping OptQS: (last_consecutive_successes) {} < {} (window)",
                tracker.last_consecutive_success_count, tracker.window
            );
            return None;
        }
```

**File:** consensus/src/epoch_manager.rs (L901-904)
```rust
        let failures_tracker = Arc::new(Mutex::new(ExponentialWindowFailureTracker::new(
            100,
            epoch_state.verifier.get_ordered_account_addresses(),
        )));
```
