# Audit Report

## Title
Transaction Censorship via Selective Non-Broadcast in Quorum Store Mode

## Summary
When Quorum Store or DAG consensus is enabled, client-submitted transactions directly to validators are marked as `NonQualified` and isolated to a single validator's mempool without being broadcast to other validators. This creates a single point of failure where validator downtime, bugs, or malicious behavior can result in transaction censorship with no recovery mechanism, violating fundamental liveness guarantees.

## Finding Description

The vulnerability exists in the transaction submission and broadcast logic when operating in Quorum Store or DAG consensus mode (the current production configuration).

**Core Protocol Design Issue:**

When a client submits a transaction directly to a validator via REST API, the `process_client_transaction_submission()` function checks whether the node is a validator and whether `broadcast_within_validator_network()` is enabled to determine the transaction's timeline state. [1](#0-0) 

When Quorum Store or DAG consensus is enabled, the on-chain configuration update sets `broadcast_within_validator_network` to false: [2](#0-1) 

This causes client-submitted transactions to validators to be marked as `TimelineState::NonQualified`. The `TimelineState` enum explicitly documents this behavior: [3](#0-2) 

**Critical Flow Breakdown:**

Transactions marked as `NonQualified` are inserted into the `priority_index` but explicitly NOT added to the `timeline_index`: [4](#0-3) 

The broadcast mechanism only retrieves transactions from the `timeline_index`: [5](#0-4) 

Meanwhile, consensus batch creation pulls from the `priority_index`: [6](#0-5) [7](#0-6) 

**Single Point of Failure:**

Because the transaction is only in ONE validator's `priority_index` and is never broadcast:
1. Only that validator can create Quorum Store batches containing the transaction
2. If that validator experiences downtime, bugs, or refuses to propose the transaction, it is effectively censored
3. No other validator has visibility into the transaction
4. No recovery or re-broadcast mechanism exists

**Validators Accept Client Submissions by Default:**

The API configuration enables transaction submission by default: [8](#0-7) 

And the submit endpoint enforces this check: [9](#0-8) 

There is no configuration sanitization that prevents validators from accepting client submissions or warns about this behavior.

## Impact Explanation

**Severity: High** ("Significant protocol violations" - up to $50,000)

This vulnerability breaks the fundamental liveness guarantee that valid transactions submitted to the network will eventually be included in blocks. Specific impacts:

1. **Single Point of Failure for Liveness**: A single validator's downtime, bugs, or overload can cause transaction censorship, violating BFT's core principle of resilience to < 1/3 failures.

2. **Time-Sensitive Transaction Loss**: For time-critical operations (liquidations, oracle updates, governance votes, auction bids), delays can result in financial harm to users.

3. **Protocol Liveness Violation**: The design comment suggests transactions should be available via "at least one of mempool broadcast or quorum store batch," but `NonQualified` transactions fail this guarantee - they're only available from a single validator's batches. [10](#0-9) 

4. **No Recovery Mechanism**: Unlike the traditional mempool broadcast system which provides redundancy, this design creates an unrecoverable failure mode.

This does not reach Critical severity because it doesn't directly enable fund theft or consensus safety violations, but it represents a significant protocol design flaw affecting liveness.

## Likelihood Explanation

**Likelihood: Medium**

**Preconditions:**
- Quorum Store or DAG consensus enabled (âœ“ production configuration)
- Client submits to validator rather than full node
- Validator experiences issues or behaves improperly

**Increasing Factors:**
- Validators expose public APIs accepting transactions (default configuration)
- No documentation or system warnings prevent direct validator submission
- Wallet/SDK implementations may not distinguish validator vs. full node endpoints
- Attack is undetectable (no on-chain evidence)

**Decreasing Factors:**
- Best practice is submitting to full nodes (VFNs/PFNs)
- Most sophisticated clients submit to multiple endpoints for redundancy
- Requires the specific validator to have issues or act improperly

**Realistic Scenarios:**
1. Validator downtime during client transaction submission
2. Validator experiencing bugs or resource exhaustion
3. Load balancers directing traffic to validators
4. Client retry logic repeatedly hitting the same validator

## Recommendation

**Option 1: Force Broadcast for Client Submissions (Preferred)**
Modify `process_client_transaction_submission` to always use `TimelineState::NotReady` for client submissions, even when Quorum Store is enabled:

```rust
let timeline_state = if client_submitted {
    TimelineState::NotReady
} else if ineligible_for_broadcast {
    TimelineState::NonQualified  
} else {
    TimelineState::NotReady
};
```

**Option 2: Disable Validator Transaction Submission**
Configure validators to reject client transaction submissions by default:
- Set `transaction_submission_enabled = false` for validator node type
- Add config sanitizer enforcement
- Document that clients must submit to full nodes

**Option 3: Implement Fallback Broadcast**
Add a timeout mechanism that converts `NonQualified` transactions to broadcast-eligible if not included within N seconds.

## Proof of Concept

```rust
// Test demonstrating isolated transaction
#[test]
fn test_quorum_store_client_submission_isolation() {
    // Setup validator with Quorum Store enabled
    let mut config = NodeConfig::default();
    config.consensus.quorum_store_enabled = true;
    
    // Simulate client submission to validator
    let txn = create_signed_transaction();
    
    // Submit via REST API to validator
    let result = validator.submit_transaction(txn.clone());
    assert!(result.is_ok());
    
    // Verify transaction is in validator's mempool
    let mempool_txn = validator.mempool.get_transaction(&txn.hash());
    assert!(mempool_txn.is_some());
    assert_eq!(mempool_txn.unwrap().timeline_state, TimelineState::NonQualified);
    
    // Verify transaction is NOT in other validators' mempools
    for other_validator in other_validators {
        assert!(other_validator.mempool.get_transaction(&txn.hash()).is_none());
    }
    
    // Verify only the receiving validator can propose it
    // If that validator is offline, transaction is censored indefinitely
}
```

## Notes

This vulnerability represents a protocol design flaw rather than an implementation bug. The behavior is intentional (preventing duplicate transaction propagation in Quorum Store mode), but creates an unintended consequence: single point of failure for liveness. The system should maintain BFT's core property of resilience to individual node failures, which this design violates.

The vulnerability is particularly concerning because:
1. Validators accept client submissions by default with no warnings
2. Clients may not be aware they're submitting to validators vs. full nodes
3. No recovery mechanism exists once a transaction is isolated
4. The failure mode is silent - no error is returned to the client

While "malicious validator behavior" is one attack vector, the more realistic concern is validator downtime, bugs, or resource exhaustion creating an unintentional censorship effect.

### Citations

**File:** mempool/src/shared_mempool/tasks.rs (L140-146)
```rust
    let ineligible_for_broadcast =
        smp.network_interface.is_validator() && !smp.broadcast_within_validator_network();
    let timeline_state = if ineligible_for_broadcast {
        TimelineState::NonQualified
    } else {
        TimelineState::NotReady
    };
```

**File:** mempool/src/shared_mempool/tasks.rs (L783-784)
```rust
            *broadcast_within_validator_network.write() =
                !consensus_config.quorum_store_enabled() && !consensus_config.is_dag_enabled()
```

**File:** mempool/src/core_mempool/transaction.rs (L76-85)
```rust
pub enum TimelineState {
    // The transaction is ready for broadcast.
    // Associated integer represents it's position in the log of such transactions.
    Ready(u64),
    // Transaction is not yet ready for broadcast, but it might change in a future.
    NotReady,
    // Transaction will never be qualified for broadcasting.
    // Currently we don't broadcast transactions originated on other peers.
    NonQualified,
}
```

**File:** mempool/src/core_mempool/transaction_store.rs (L559-567)
```rust
                // If timeline_state is `NonQualified`, then the transaction is never added to the timeline_index,
                // and never broadcasted to the shared mempool.
                let ready_for_mempool_broadcast = txn.timeline_state == TimelineState::NotReady;
                if ready_for_mempool_broadcast {
                    self.timeline_index
                        .get_mut(&sender_bucket)
                        .unwrap()
                        .insert(txn);
                }
```

**File:** mempool/src/core_mempool/transaction_store.rs (L774-790)
```rust
    pub(crate) fn read_timeline(
        &self,
        sender_bucket: MempoolSenderBucket,
        timeline_id: &MultiBucketTimelineIndexIds,
        count: usize,
        before: Option<Instant>,
        // The priority of the receipient of the transactions
        priority_of_receiver: BroadcastPeerPriority,
    ) -> (Vec<(SignedTransaction, u64)>, MultiBucketTimelineIndexIds) {
        let mut batch = vec![];
        let mut batch_total_bytes: u64 = 0;
        let mut last_timeline_id = timeline_id.id_per_bucket.clone();

        // Add as many transactions to the batch as possible
        for (i, bucket) in self
            .timeline_index
            .get(&sender_bucket)
```

**File:** mempool/src/core_mempool/transaction_store.rs (L1008-1010)
```rust
    pub(crate) fn iter_queue(&self) -> PriorityQueueIter<'_> {
        self.priority_index.iter()
    }
```

**File:** mempool/src/core_mempool/mempool.rs (L449-449)
```rust
        'main: for txn in self.transactions.iter_queue() {
```

**File:** config/src/config/api_config.rs (L47-49)
```rust
    /// Enables transaction submission APIs
    #[serde(default = "default_enabled")]
    pub transaction_submission_enabled: bool,
```

**File:** api/src/transactions.rs (L490-492)
```rust
        if !self.context.node_config.api.transaction_submission_enabled {
            return Err(api_disabled("Submit transaction"));
        }
```

**File:** mempool/src/shared_mempool/types.rs (L95-103)
```rust
    pub fn broadcast_within_validator_network(&self) -> bool {
        // This value will be changed true -> false via onchain config when quorum store is enabled.
        // On the transition from true -> false, all transactions in mempool will be eligible for
        // at least one of mempool broadcast or quorum store batch.
        // A transition from false -> true is unexpected -- it would only be triggered if quorum
        // store needs an emergency rollback. In this case, some transactions may not be propagated,
        // they will neither go through a mempool broadcast or quorum store batch.
        *self.broadcast_within_validator_network.read()
    }
```
