# Audit Report

## Title
Secret Share Manager Not Reset During Consensus Sync Operations Leading to Consensus Liveness Failure

## Summary
The `reset()` method in `ExecutionProxyClient` fails to reset the `SecretShareManager` during state synchronization operations (`sync_to_target()` and `sync_for_duration()`), while correctly resetting the `RandManager` and `BufferManager`. This causes the `SecretShareManager` to maintain stale state after a validator syncs, leading to rejection of valid secret shares, coordinator deadlocks, and potential consensus liveness failures.

## Finding Description

When a validator falls behind and needs to synchronize state, the consensus layer calls either `sync_to_target()` or `sync_for_duration()` to catch up. Both methods internally call the `reset()` method to reset internal consensus components to the new synchronized state. [1](#0-0) 

The `reset()` method retrieves and resets only two of the three consensus managers:
1. **RandManager** - Reset correctly (if present)
2. **BufferManager** - Reset correctly (if present)  
3. **SecretShareManager** - **NOT reset** (missing from the method entirely)

In contrast, the `end_epoch()` method correctly retrieves and resets all three managers: [2](#0-1) 

The `SecretShareManager` maintains critical state that must be synchronized:
- `block_queue`: Queue of blocks awaiting secret share aggregation
- `secret_share_store.highest_known_round`: Validation boundary for accepting shares
- Protocol state for threshold decryption [3](#0-2) 

When secret shares arrive after sync, they are validated against `highest_known_round`: [4](#0-3) 

The validation at line 263-266 rejects shares from rounds beyond `highest_known_round + FUTURE_ROUNDS_TO_ACCEPT` (200 rounds). If a validator syncs forward by more than 200 rounds without resetting the secret share manager, all incoming secret shares will be rejected as "from future round". [5](#0-4) 

**Attack Scenario:**

1. Secret sharing is enabled (when `secret_sharing_config` is `Some` during epoch start)
2. Validator is processing blocks at round N with `highest_known_round = N`
3. Validator falls behind due to network partition or temporary downtime
4. Validator needs to sync to catch up to round N+300
5. Consensus calls `sync_to_target()` which invokes `reset()`
6. RandManager and BufferManager are reset to round N+300
7. **SecretShareManager is NOT reset and maintains `highest_known_round = N`**
8. Validator resumes consensus at round N+300
9. New blocks arrive requiring secret sharing for decryption
10. Incoming secret shares for round N+300 are rejected (N+300 > N+200)
11. The coordinator waits indefinitely for secret shares that will never arrive [6](#0-5) 

The coordinator requires both `rand_ready` and `secret_ready` flags before forwarding blocks to execution. Without valid secret shares, blocks remain stuck in the coordinator indefinitely, causing consensus liveness failure.

For encrypted transactions, the decryption pipeline expects secret shares to be available: [7](#0-6) 

The code expects the decryption key (line 119), which won't be available if secret shares are rejected, causing a panic or incorrect decryption.

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

1. **Validator Node Slowdowns/Liveness Failure**: Validators that sync while secret sharing is enabled will be unable to process blocks requiring secret shares, causing them to fall out of consensus. This degrades network performance and can lead to validator set disruption.

2. **Consensus Liveness Issues**: If enough validators experience this issue simultaneously (e.g., after a network partition), the network could experience significant liveness degradation or temporary halt until validators are manually restarted or the epoch changes.

3. **Deterministic Execution Violation**: Validators with properly reset state versus stale state may process encrypted transactions differently, potentially leading to consensus divergence.

This does not constitute a Critical severity issue because:
- It does not directly cause loss of funds
- It does not enable permanent chain splits (recoverable via epoch change or restart)
- It requires specific conditions (sync + secret sharing enabled)

However, it represents a significant protocol violation affecting validator operations and network availability, clearly falling into the High Severity category (up to $50,000).

## Likelihood Explanation

**Likelihood: High**

This vulnerability will trigger whenever:
1. Secret sharing is enabled in production (encrypted transactions feature)
2. A validator falls behind by more than 200 rounds (approximately 20 seconds at 10 rounds/second)
3. The validator calls `sync_to_target()` or `sync_for_duration()` to catch up

These conditions are common in production environments:
- Validators regularly experience temporary network issues
- Validators restart and need to sync
- Consensus observers use `sync_for_duration()` for fallback synchronization

The bug is deterministic and will occur every time these conditions are met. The only mitigation is that epoch changes trigger `end_epoch()` which correctly resets all managers, providing temporary recovery until the next sync operation.

## Recommendation

Add `reset_tx_to_secret_share_manager` to the `reset()` method to match the behavior of `end_epoch()`:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager, reset_tx_to_secret_share_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
            handle.reset_tx_to_secret_share_manager.clone(), // ADD THIS LINE
        )
    };

    if let Some(mut reset_tx) = reset_tx_to_rand_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::RandResetDropped)?;
        ack_rx.await.map_err(|_| Error::RandResetDropped)?;
    }

    // ADD THIS BLOCK
    if let Some(mut reset_tx) = reset_tx_to_secret_share_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::ResetDropped)?; // Consider adding specific error
        ack_rx.await.map_err(|_| Error::ResetDropped)?;
    }

    if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
        // ... existing code ...
    }

    Ok(())
}
```

Additionally, consider adding a specific error variant to `consensus/src/pipeline/errors.rs`:

```rust
#[error("Secret Share Reset host dropped")]
SecretShareResetDropped,
```

## Proof of Concept

**Reproduction Steps:**

1. Enable secret sharing by providing `secret_sharing_config` during epoch initialization
2. Start a validator node and let it process blocks up to round N
3. Simulate the validator falling behind by 250 rounds (beyond the 200-round acceptance window)
4. Trigger `sync_to_target()` with a target LedgerInfo at round N+250
5. Observe that:
   - RandManager's `highest_known_round` is updated to N+250
   - BufferManager is reset to round N+250
   - SecretShareManager's `highest_known_round` remains at N
6. Send a block at round N+250 that requires secret sharing
7. Attempt to add secret shares for round N+250
8. Observe rejection with "Share from future round" error (N+250 > N+200)
9. Observe coordinator deadlock as it waits indefinitely for `secret_ready` flag

**Test Implementation (Rust):**

```rust
#[tokio::test]
async fn test_secret_share_manager_not_reset_on_sync() {
    // Setup execution client with secret sharing enabled
    let execution_client = create_test_execution_client_with_secret_sharing();
    
    // Process blocks up to round 100
    for round in 1..=100 {
        let block = create_test_block(round);
        execution_client.finalize_order(vec![block], create_ordered_proof(round)).await.unwrap();
    }
    
    // Verify SecretShareManager has highest_known_round = 100
    assert_eq!(get_secret_share_highest_known_round(&execution_client), 100);
    
    // Simulate falling behind and syncing to round 350 (250 rounds ahead)
    let target_ledger_info = create_ledger_info_with_round(350);
    execution_client.sync_to_target(target_ledger_info).await.unwrap();
    
    // BUG: SecretShareManager still has highest_known_round = 100 (not reset)
    assert_eq!(get_secret_share_highest_known_round(&execution_client), 100); // FAILS - shows bug
    
    // Attempt to process secret share for round 350
    let secret_share = create_test_secret_share(350);
    let result = add_secret_share_to_manager(&execution_client, secret_share);
    
    // Share is rejected because 350 > 100 + 200
    assert!(result.is_err());
    assert!(result.unwrap_err().to_string().contains("future round"));
    
    // Coordinator will be stuck waiting for secret shares that never arrive
}
```

**Notes:**

This vulnerability is particularly dangerous because:
1. The error is silent - no immediate crash or obvious failure
2. It manifests as a liveness issue that may be mistaken for network problems
3. It affects all validators equally during network-wide sync events
4. The only recovery is epoch change or manual restart

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L311-365)
```rust
    fn make_coordinator(
        mut rand_manager_input_tx: UnboundedSender<OrderedBlocks>,
        mut rand_ready_block_rx: UnboundedReceiver<OrderedBlocks>,
        mut secret_share_manager_input_tx: UnboundedSender<OrderedBlocks>,
        mut secret_ready_block_rx: UnboundedReceiver<OrderedBlocks>,
    ) -> (
        UnboundedSender<OrderedBlocks>,
        futures_channel::mpsc::UnboundedReceiver<OrderedBlocks>,
    ) {
        let (ordered_block_tx, mut ordered_block_rx) = unbounded::<OrderedBlocks>();
        let (mut ready_block_tx, ready_block_rx) = unbounded::<OrderedBlocks>();

        tokio::spawn(async move {
            let mut inflight_block_tracker: HashMap<
                HashValue,
                (
                    OrderedBlocks,
                    /* rand_ready */ bool,
                    /* secret ready */ bool,
                ),
            > = HashMap::new();
            loop {
                let entry = select! {
                    Some(ordered_blocks) = ordered_block_rx.next() => {
                        let _ = rand_manager_input_tx.send(ordered_blocks.clone()).await;
                        let _ = secret_share_manager_input_tx.send(ordered_blocks.clone()).await;
                        let first_block_id = ordered_blocks.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.insert(first_block_id, (ordered_blocks, false, false));
                        inflight_block_tracker.entry(first_block_id)
                    },
                    Some(rand_ready_block) = rand_ready_block_rx.next() => {
                        let first_block_id = rand_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.1 = true;
                        })
                    },
                    Some(secret_ready_block) = secret_ready_block_rx.next() => {
                        let first_block_id = secret_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.2 = true;
                        })
                    },
                };
                let Entry::Occupied(o) = entry else {
                    unreachable!("Entry must exist");
                };
                if o.get().1 && o.get().2 {
                    let (_, (ordered_blocks, _, _)) = o.remove_entry();
                    let _ = ready_block_tx.send(ordered_blocks).await;
                }
            }
        });

        (ordered_block_tx, ready_block_rx)
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L711-760)
```rust
    async fn end_epoch(&self) {
        let (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        ) = {
            let mut handle = self.handle.write();
            handle.reset()
        };

        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }

        if let Some(mut tx) = reset_tx_to_buffer_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop buffer manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop buffer manager");
        }
        self.execution_proxy.end_epoch();
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L259-275)
```rust
    pub fn add_share(&mut self, share: SecretShare) -> anyhow::Result<bool> {
        let weight = self.secret_share_config.get_peer_weight(share.author());
        let metadata = share.metadata();
        ensure!(metadata.epoch == self.epoch, "Share from different epoch");
        ensure!(
            metadata.round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT,
            "Share from future round"
        );

        let item = self
            .secret_share_map
            .entry(metadata.round)
            .or_insert_with(|| SecretShareItem::new(self.self_author));
        item.add_share(share, weight)?;
        item.try_aggregate(&self.secret_share_config, self.decision_tx.clone());
        Ok(item.has_decision())
    }
```

**File:** consensus/src/rand/secret_sharing/types.rs (L16-16)
```rust
pub const FUTURE_ROUNDS_TO_ACCEPT: u64 = 200;
```

**File:** consensus/src/pipeline/decryption_pipeline_builder.rs (L115-120)
```rust
        let maybe_decryption_key = secret_shared_key_rx
            .await
            .expect("decryption key should be available");
        // TODO(ibalajiarun): account for the case where decryption key is not available
        let decryption_key = maybe_decryption_key.expect("decryption key should be available");

```
