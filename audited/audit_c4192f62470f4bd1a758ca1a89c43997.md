# Audit Report

## Title
JWK Consensus Manager Silently Drops On-Chain State Updates Due to Insufficient Channel Capacity

## Summary
The `TConsensusManager::run()` method accepts `jwk_updated_rx` and `rpc_req_rx` channels without validating their queue depth or policies. The `jwk_updated_rx` channel is configured with capacity 1 and KLAST queue style, causing silent message loss when multiple `ObservedJWKsUpdated` events arrive in a single batch from the blockchain. This leads to state inconsistency between the on-chain JWK state and the consensus manager's view.

## Finding Description

The `TConsensusManager` trait defines a `run()` method that receives two critical channels: [1](#0-0) 

These channels are created in `EpochManager::start_new_epoch()` with hardcoded configurations: [2](#0-1) 

The `jwk_updated_rx` channel has **capacity 1** with **KLAST** (Keep Last) queue style. According to the queue implementation, KLAST drops the oldest message when the queue is full: [3](#0-2) 

When the `EpochManager` receives on-chain JWK update events, it processes them in a loop and forwards each to the consensus manager: [4](#0-3) 

**The Critical Issue:** A single `EventNotification` can contain multiple `ObservedJWKsUpdated` events (when multiple validator transactions emit events in the same block). The event subscription service batches all events at a version into one notification: [5](#0-4) 

**Attack Scenario:**
1. Block N contains 2+ validator transactions that update JWKs (e.g., updates for different issuers or different keys in per-key mode)
2. Each transaction calls `upsert_into_observed_jwks()` which emits an `ObservedJWKsUpdated` event: [6](#0-5) 

3. These events are batched into a single `EventNotification` with multiple entries in `subscribed_events`
4. `EpochManager::process_onchain_event()` loops through all events (line 112)
5. For each event, it calls `tx.push((), jwk_event)` on the capacity-1 channel
6. The second push drops the first event (KLAST behavior)
7. Push results are silently ignored (line 115: `let _ = ...`)
8. The consensus manager only receives the last event, missing all previous ones
9. The consensus manager's state becomes inconsistent with actual on-chain state

The `push()` method returns `Ok(())` even when dropping messages - drops are only reported via an optional status channel that is not used here: [7](#0-6) 

Multiple validator transactions can legitimately appear in a single block (up to 100 by default): [8](#0-7) 

## Impact Explanation

This vulnerability causes **state inconsistency** between the on-chain JWK state and the consensus manager's internal state. According to the Aptos bug bounty criteria, this qualifies as **Medium Severity**: "State inconsistencies requiring intervention."

**Specific Impacts:**
- Consensus manager operates with stale/incomplete on-chain JWK state
- May propose validator transactions with incorrect version numbers
- Could fail to achieve quorum for subsequent updates
- Breaks the **State Consistency** invariant: validators must maintain consistent views of on-chain state
- Potential liveness issues if the consensus manager cannot synchronize properly

This does not directly cause fund loss or consensus safety violations, but degrades the JWK consensus subsystem's ability to function correctly.

## Likelihood Explanation

**Likelihood: Medium to High**

This occurs naturally without attacker involvement when:
1. Multiple issuers update their JWKs in the same block (common in per-issuer mode)
2. Per-key mode with a single issuer updating multiple keys simultaneously
3. High validator activity where multiple validators submit updates

The validator transaction pool accepts up to 100 validator transactions per block by default, and different validators may submit updates for different issuers/keys concurrently. During coordinated JWK rotations (e.g., security incident response), multiple updates are expected.

No special privileges or attack coordination is required - this is a design flaw that manifests under normal operational conditions.

## Recommendation

**Fix 1: Increase Channel Capacity**
Change the `jwk_updated_rx` channel capacity from 1 to a reasonable value (e.g., 100) to match the potential number of validator transactions per block:

```rust
let (jwk_event_tx, jwk_event_rx) = aptos_channel::new(QueueStyle::KLAST, 100, None);
```

**Fix 2: Add Validation in TConsensusManager::run()**
Add channel validation to detect misconfiguration:

```rust
trait TConsensusManager: Send + Sync {
    async fn run(
        self: Box<Self>,
        oidc_providers: Option<SupportedOIDCProviders>,
        observed_jwks: Option<ObservedJWKs>,
        mut jwk_updated_rx: aptos_channel::Receiver<(), ObservedJWKsUpdated>,
        mut rpc_req_rx: aptos_channel::Receiver<AccountAddress, (AccountAddress, IncomingRpcRequest)>,
        close_rx: oneshot::Receiver<oneshot::Sender<()>>,
    ) {
        // Validate channel configurations
        assert!(jwk_updated_rx.capacity() >= MIN_JWK_CHANNEL_CAPACITY, 
            "jwk_updated_rx capacity too low");
        assert!(rpc_req_rx.capacity() >= MIN_RPC_CHANNEL_CAPACITY,
            "rpc_req_rx capacity too low");
        // ... rest of implementation
    }
}
```

**Fix 3: Handle Push Errors**
Use feedback channels to detect drops and log warnings:

```rust
for event in subscribed_events {
    if let Ok(jwk_event) = ObservedJWKsUpdated::try_from(&event) {
        if let Some(tx) = self.jwk_updated_event_txs.as_ref() {
            if let Err(e) = tx.push((), jwk_event) {
                error!("Failed to forward JWK event: {:?}", e);
            }
        }
    }
}
```

**Recommended Solution:** Implement all three fixes - increase capacity, add validation, and improve error handling.

## Proof of Concept

```rust
#[cfg(test)]
mod test_jwk_event_loss {
    use super::*;
    use aptos_channels::{aptos_channel, message_queues::QueueStyle};
    use aptos_types::jwks::{ObservedJWKsUpdated, AllProvidersJWKs};
    
    #[tokio::test]
    async fn test_multiple_events_cause_message_loss() {
        // Create channel with capacity 1 (as in production)
        let (tx, mut rx) = aptos_channel::new(QueueStyle::KLAST, 1, None);
        
        // Simulate multiple events arriving in same batch
        let event1 = ObservedJWKsUpdated {
            epoch: 1,
            jwks: AllProvidersJWKs { entries: vec![] },
        };
        let event2 = ObservedJWKsUpdated {
            epoch: 1,
            jwks: AllProvidersJWKs { entries: vec![] },
        };
        
        // Push both events (simulating the loop in process_onchain_event)
        let _ = tx.push((), event1.clone());
        let _ = tx.push((), event2.clone());
        
        // Try to receive - only event2 will be available
        let received = rx.next().await.unwrap();
        assert_eq!(received.epoch, event2.epoch);
        
        // event1 was silently dropped - no second message available
        assert!(rx.try_recv().is_none());
        
        println!("VULNERABILITY CONFIRMED: First event was dropped!");
    }
}
```

## Notes

The `TConsensusManager::run()` method accepts channels as parameters without any validation of their configuration. The trait definition provides no documentation about required channel capacities or queue policies. This allows misconfiguration that causes silent data loss, violating the state consistency invariant required for correct JWK consensus operation.

### Citations

**File:** crates/aptos-jwk-consensus/src/lib.rs (L64-77)
```rust
#[async_trait::async_trait]
trait TConsensusManager: Send + Sync {
    async fn run(
        self: Box<Self>,
        oidc_providers: Option<SupportedOIDCProviders>,
        observed_jwks: Option<ObservedJWKs>,
        mut jwk_updated_rx: aptos_channel::Receiver<(), ObservedJWKsUpdated>,
        mut rpc_req_rx: aptos_channel::Receiver<
            AccountAddress,
            (AccountAddress, IncomingRpcRequest),
        >,
        close_rx: oneshot::Receiver<oneshot::Sender<()>>,
    );
}
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L107-120)
```rust
    /// On a on-chain JWK updated events, forward to JWK consensus manager if it is alive.
    fn process_onchain_event(&mut self, notification: EventNotification) -> Result<()> {
        let EventNotification {
            subscribed_events, ..
        } = notification;
        for event in subscribed_events {
            if let Ok(jwk_event) = ObservedJWKsUpdated::try_from(&event) {
                if let Some(tx) = self.jwk_updated_event_txs.as_ref() {
                    let _ = tx.push((), jwk_event);
                }
            }
        }
        Ok(())
    }
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L220-222)
```rust
            let (jwk_event_tx, jwk_event_rx) = aptos_channel::new(QueueStyle::KLAST, 1, None);
            self.jwk_updated_event_txs = Some(jwk_event_tx);
            let (jwk_rpc_msg_tx, jwk_rpc_msg_rx) = aptos_channel::new(QueueStyle::FIFO, 100, None);
```

**File:** crates/channel/src/message_queues.rs (L138-147)
```rust
            match self.queue_style {
                // Drop the newest message for FIFO
                QueueStyle::FIFO => Some(message),
                // Drop the oldest message for LIFO
                QueueStyle::LIFO | QueueStyle::KLAST => {
                    let oldest = key_message_queue.pop_front();
                    key_message_queue.push_back(message);
                    oldest
                },
            }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L345-358)
```rust
    fn buffer_event(&mut self, event: ContractEvent) {
        self.event_buffer.push(event)
    }

    fn notify_subscriber_of_events(&mut self, version: Version) -> Result<(), Error> {
        let event_notification = EventNotification {
            subscribed_events: self.event_buffer.drain(..).collect(),
            version,
        };

        self.notification_sender
            .push((), event_notification)
            .map_err(|error| Error::UnexpectedErrorEncountered(format!("{:?}", error)))
    }
```

**File:** aptos-move/framework/aptos-framework/sources/jwks.move (L502-504)
```text
        let epoch = reconfiguration::current_epoch();
        emit(ObservedJWKsUpdated { epoch, jwks: observed_jwks.jwks });
        regenerate_patched_jwks();
```

**File:** crates/channel/src/aptos_channel.rs (L96-112)
```rust
    ) -> Result<()> {
        let mut shared_state = self.shared_state.lock();
        ensure!(!shared_state.receiver_dropped, "Channel is closed");
        debug_assert!(shared_state.num_senders > 0);

        let dropped = shared_state.internal_queue.push(key, (message, status_ch));
        // If this or an existing message had to be dropped because of the queue being full, we
        // notify the corresponding status channel if it was registered.
        if let Some((dropped_val, Some(dropped_status_ch))) = dropped {
            // Ignore errors.
            let _err = dropped_status_ch.send(ElementStatus::Dropped(dropped_val));
        }
        if let Some(w) = shared_state.waker.take() {
            w.wake();
        }
        Ok(())
    }
```

**File:** consensus/src/payload_client/mixed.rs (L90-95)
```rust
        debug!("num_validator_txns={}", validator_txns.len());
        // Update constraints with validator txn pull results.
        let mut user_txn_pull_params = params;
        user_txn_pull_params.max_txns -= vtxn_size;
        user_txn_pull_params.max_txns_after_filtering -= validator_txns.len() as u64;
        user_txn_pull_params.soft_max_txns_after_filtering -= validator_txns.len() as u64;
```
