# Audit Report

## Title
Race Condition in Notification Feedback Handling Allows Malicious Peers to Evade Reputation Penalties

## Summary
A race condition exists in the data streaming service where notification IDs can be garbage collected before client feedback arrives, causing legitimate feedback about malicious peer behavior to be silently dropped. This allows bad peers to avoid reputation penalties and continue serving invalid data.

## Finding Description

The data streaming service maintains a mapping between notification IDs and response contexts in `notifications_to_responses` to enable feedback processing. However, this mapping is subject to garbage collection when it exceeds `max_notification_id_mappings` (default: 300 entries). [1](#0-0) 

When a new notification is sent, its ID is inserted into the map and garbage collection is triggered: [2](#0-1) 

The garbage collection removes the oldest notification IDs when the map size exceeds the limit: [3](#0-2) 

When feedback is later provided via `handle_notification_feedback()`, it looks up the notification ID in the map. If the ID was garbage collected, an error is returned: [4](#0-3) 

The configuration sets the default limit to 300 mappings: [5](#0-4) 

**Attack Scenario:**

1. A malicious peer serves data to a state sync client
2. The peer sends 301+ valid responses, triggering garbage collection of early notification IDs
3. The peer then provides invalid data in one of the early responses (e.g., notification ID #1)
4. By the time the client processes notification #1 and attempts to report it via `terminate_stream_with_feedback()`, the notification ID has been garbage collected
5. The feedback lookup fails and returns an error, preventing the bad peer from being reported [6](#0-5) 

The critical issue is that when feedback is lost, `notify_bad_response()` is never called, so the peer's reputation score is not updated: [7](#0-6) 

This prevents the peer scoring system from functioning correctly. The scoring system is designed to penalize bad peers by reducing their score, and eventually ignoring them when the score drops below the threshold: [8](#0-7) [9](#0-8) 

## Impact Explanation

This vulnerability meets **Medium Severity** criteria according to Aptos bug bounty guidelines:

- **State Inconsistencies**: The peer reputation system becomes inconsistent, with malicious peers avoiding penalties they should receive
- **Not Direct Fund Loss**: Does not directly cause loss of funds or consensus violations
- **Protocol Violation**: Breaks the intended behavior of the peer reputation and feedback system
- **Security Degradation**: Allows malicious peers to continue operating without proper penalties, potentially serving invalid data repeatedly

The impact is limited because:
- It doesn't affect consensus safety directly
- It doesn't cause fund loss
- State sync can still make progress (albeit with degraded quality)
- However, it undermines a critical security mechanism (peer reputation) designed to protect the network

## Likelihood Explanation

**High Likelihood** - This issue can occur in several realistic scenarios:

1. **Natural Occurrence**: On a busy network with fast streams, 300 notifications can be sent quickly. If a client is under load or processing slowly, legitimate feedback on early notifications will fail.

2. **Malicious Exploitation**: An attacker controlling a peer can deliberately:
   - Send >300 valid responses to trigger garbage collection
   - Inject invalid data in earlier responses
   - Evade reputation penalties when the client reports the bad data

3. **Low Attack Complexity**: Requires only network peer access, no privileged validator position needed

4. **No Rate Limiting**: There's no mechanism to prevent rapid notification generation that triggers garbage collection

## Recommendation

**Solution 1: Track Feedback Before Garbage Collection**

Modify the garbage collection to check if a notification is pending feedback before removing it. However, this is complex as we'd need to track which notifications haven't been acknowledged.

**Solution 2: Increase the Window Size or Make it Configurable Per Stream Type**

For continuous subscription streams that may accumulate many notifications, increase `max_notification_id_mappings` or make it proportional to the stream type and expected processing rate.

**Solution 3: Use a Time-Based Expiry Instead of Count-Based**

Replace the count-based garbage collection with time-based expiry. Keep notification mappings for a minimum time period (e.g., 60 seconds) regardless of map size, ensuring clients have reasonable time to provide feedback.

**Recommended Fix (Solution 3):**

```rust
fn garbage_collect_notification_response_map(&mut self) -> Result<(), Error> {
    let max_notification_id_mappings =
        self.streaming_service_config.max_notification_id_mappings;
    let min_notification_age_ms = 
        self.streaming_service_config.min_notification_age_before_gc_ms; // e.g., 60000
    let map_length = self.notifications_to_responses.len() as u64;
    
    if map_length > max_notification_id_mappings {
        let current_time = self.time_service.now();
        
        // Only remove notifications older than min_notification_age_ms
        let keys_to_remove: Vec<NotificationId> = self
            .notifications_to_responses
            .iter()
            .filter(|(_, ctx)| {
                current_time.duration_since(ctx.creation_time).as_millis() as u64 
                    > min_notification_age_ms
            })
            .map(|(id, _)| *id)
            .take((map_length - max_notification_id_mappings) as usize)
            .collect();

        for key in keys_to_remove {
            self.notifications_to_responses.remove(&key);
        }
    }
    
    Ok(())
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_notification_feedback_race_condition() {
    use crate::tests::utils::{
        create_data_stream_for_test, create_mock_data_client,
        MockDataClient,
    };
    use crate::data_notification::NotificationId;
    use crate::streaming_client::NotificationFeedback;
    
    // Create a data stream with default config (max 300 notification mappings)
    let (mut data_stream, _listener) = create_data_stream_for_test();
    
    // Simulate sending 301 notifications
    let mut notification_ids = Vec::new();
    for i in 0..301 {
        let notification_id = i as NotificationId;
        let response_context = create_mock_response_context(notification_id);
        
        // Insert the notification mapping
        data_stream
            .insert_notification_response_mapping(notification_id, response_context)
            .unwrap();
            
        notification_ids.push(notification_id);
    }
    
    // At this point, notification ID 0 should have been garbage collected
    // Try to provide feedback for notification ID 0 (the first notification)
    let feedback = NotificationFeedback::InvalidPayloadData;
    let result = data_stream.handle_notification_feedback(&notification_ids[0], &feedback);
    
    // This should fail with "Response context missing" error, even though
    // notification ID 0 was legitimately sent
    assert!(result.is_err());
    assert!(result.unwrap_err().to_string().contains("Response context missing"));
    
    // However, feedback for notification ID 250 (still in the map) should succeed
    let result = data_stream.handle_notification_feedback(&notification_ids[250], &feedback);
    assert!(result.is_ok());
}
```

## Notes

This vulnerability specifically affects the state synchronization's peer reputation mechanism rather than consensus safety. While it doesn't directly compromise blockchain integrity, it significantly degrades the network's ability to identify and exclude malicious peers, which is a critical defense mechanism. The issue is particularly concerning for continuous subscription streams where notification counts can grow rapidly, making garbage collection frequent. The fix should balance memory constraints with ensuring sufficient time for clients to process notifications and provide feedback.

### Citations

**File:** state-sync/data-streaming-service/src/data_stream.rs (L96-96)
```rust
    notifications_to_responses: BTreeMap<NotificationId, ResponseContext>,
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L234-263)
```rust
    pub fn handle_notification_feedback(
        &self,
        notification_id: &NotificationId,
        notification_feedback: &NotificationFeedback,
    ) -> Result<(), Error> {
        if self.stream_end_notification_id == Some(*notification_id) {
            return if matches!(notification_feedback, NotificationFeedback::EndOfStream) {
                Ok(())
            } else {
                Err(Error::UnexpectedErrorEncountered(format!(
                    "Invalid feedback given for stream end: {:?}",
                    notification_feedback
                )))
            };
        }

        let response_context = self
            .notifications_to_responses
            .get(notification_id)
            .ok_or_else(|| {
                Error::UnexpectedErrorEncountered(format!(
                    "Response context missing for notification ID: {:?}",
                    notification_id
                ))
            })?;
        let response_error = extract_response_error(notification_feedback)?;
        self.notify_bad_response(response_context, response_error);

        Ok(())
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L746-764)
```rust
    /// Notifies the Aptos data client of a bad client response
    fn notify_bad_response(
        &self,
        response_context: &ResponseContext,
        response_error: ResponseError,
    ) {
        let response_id = response_context.id;
        info!(LogSchema::new(LogEntry::ReceivedDataResponse)
            .stream_id(self.data_stream_id)
            .event(LogEvent::Error)
            .message(&format!(
                "Notifying the data client of a bad response. Response id: {:?}, error: {:?}",
                response_id, response_error
            )));

        response_context
            .response_callback
            .notify_bad_response(response_error);
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L813-831)
```rust
    fn insert_notification_response_mapping(
        &mut self,
        notification_id: NotificationId,
        response_context: ResponseContext,
    ) -> Result<(), Error> {
        if let Some(response_context) = self
            .notifications_to_responses
            .insert(notification_id, response_context)
        {
            Err(Error::UnexpectedErrorEncountered(format!(
                "Duplicate sent notification ID found! \
                 Notification ID: {:?}, \
                 previous Response context: {:?}",
                notification_id, response_context
            )))
        } else {
            self.garbage_collect_notification_response_map()
        }
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L833-862)
```rust
    fn garbage_collect_notification_response_map(&mut self) -> Result<(), Error> {
        let max_notification_id_mappings =
            self.streaming_service_config.max_notification_id_mappings;
        let map_length = self.notifications_to_responses.len() as u64;
        if map_length > max_notification_id_mappings {
            let num_entries_to_remove = map_length
                .checked_sub(max_notification_id_mappings)
                .ok_or_else(|| {
                    Error::IntegerOverflow("Number of entries to remove has overflown!".into())
                })?;

            // Collect all the keys that need to removed. Note: BTreeMap keys
            // are sorted, so we'll remove the lowest notification IDs. These
            // will be the oldest notifications.
            let mut all_keys = self.notifications_to_responses.keys();
            let mut keys_to_remove = vec![];
            for _ in 0..num_entries_to_remove {
                if let Some(key_to_remove) = all_keys.next() {
                    keys_to_remove.push(*key_to_remove);
                }
            }

            // Remove the keys
            for key_to_remove in &keys_to_remove {
                self.notifications_to_responses.remove(key_to_remove);
            }
        }

        Ok(())
    }
```

**File:** config/src/config/state_sync_config.rs (L241-274)
```rust
    /// Maximum number of notification ID to response context mappings held in
    /// memory. Once the number grows beyond this value, garbage collection occurs.
    pub max_notification_id_mappings: u64,

    /// Maximum number of consecutive subscriptions that can be made before
    /// the subscription stream is terminated and a new stream must be created.
    pub max_num_consecutive_subscriptions: u64,

    /// Maximum number of pending requests per data stream. This includes the
    /// requests that have already succeeded but have not yet been consumed
    /// because they're head-of-line blocked by other requests.
    pub max_pending_requests: u64,

    /// Maximum number of retries for a single client request before a data
    /// stream will terminate.
    pub max_request_retry: u64,

    /// Maximum lag (in seconds) we'll tolerate when sending subscription requests
    pub max_subscription_stream_lag_secs: u64,

    /// The interval (milliseconds) at which to check the progress of each stream.
    pub progress_check_interval_ms: u64,
}

impl Default for DataStreamingServiceConfig {
    fn default() -> Self {
        Self {
            dynamic_prefetching: DynamicPrefetchingConfig::default(),
            enable_subscription_streaming: false,
            global_summary_refresh_interval_ms: 50,
            max_concurrent_requests: MAX_CONCURRENT_REQUESTS,
            max_concurrent_state_requests: MAX_CONCURRENT_STATE_REQUESTS,
            max_data_stream_channel_sizes: 50,
            max_notification_id_mappings: 300,
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L199-251)
```rust
    /// Processes a request for terminating a data stream.
    /// TODO(joshlind): once this is exposed to the wild, we'll need automatic
    /// garbage collection for misbehaving clients.
    fn process_terminate_stream_request(
        &mut self,
        terminate_request: &TerminateStreamRequest,
    ) -> Result<(), Error> {
        // Grab the stream id and feedback
        let data_stream_id = &terminate_request.data_stream_id;
        let notification_and_feedback = &terminate_request.notification_and_feedback;

        // Increment the stream termination counter
        let feedback_label = match notification_and_feedback {
            Some(notification_and_feedback) => {
                notification_and_feedback.notification_feedback.get_label()
            },
            None => TERMINATE_NO_FEEDBACK,
        };
        metrics::increment_counter(&metrics::TERMINATE_DATA_STREAM, feedback_label);

        // Remove the data stream
        if let Some(data_stream) = self.data_streams.remove(data_stream_id) {
            info!(LogSchema::new(LogEntry::HandleTerminateRequest)
                .stream_id(*data_stream_id)
                .event(LogEvent::Success)
                .message(&format!(
                    "Terminating the data stream with ID: {:?}. Notification and feedback: {:?}",
                    data_stream_id, notification_and_feedback,
                )));

            // Handle any notification feedback
            if let Some(notification_and_feedback) = notification_and_feedback {
                let notification_id = &notification_and_feedback.notification_id;
                let feedback = &notification_and_feedback.notification_feedback;
                if data_stream.sent_notification(notification_id) {
                    data_stream.handle_notification_feedback(notification_id, feedback)?;
                    Ok(())
                } else {
                    Err(Error::UnexpectedErrorEncountered(format!(
                        "Data stream ID: {:?} did not appear to send notification ID: {:?}",
                        data_stream_id, notification_id,
                    )))
                }
            } else {
                Ok(())
            }
        } else {
            Err(Error::UnexpectedErrorEncountered(format!(
                "Unable to find data stream with ID: {:?}. Notification and feedback: {:?}",
                data_stream_id, notification_and_feedback,
            )))
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L32-43)
```rust
/// Scores for peer rankings based on preferences and behavior.
const MAX_SCORE: f64 = 100.0;
const MIN_SCORE: f64 = 0.0;
const STARTING_SCORE: f64 = 50.0;
/// Add this score on a successful response.
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
/// Not necessarily a malicious response, but not super useful.
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
/// Likely to be a malicious response.
const MALICIOUS_MULTIPLIER: f64 = 0.8;
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L303-320)
```rust
    pub fn update_score_error(&self, peer: PeerNetworkId, error: ErrorType) {
        if let Some(mut entry) = self.peer_to_state.get_mut(&peer) {
            // Get the peer's old score
            let old_score = entry.score;

            // Update the peer's score with an error
            entry.update_score_error(error);

            // Log if the peer is now ignored
            let new_score = entry.score;
            if old_score > IGNORE_PEER_THRESHOLD && new_score <= IGNORE_PEER_THRESHOLD {
                info!(
                    (LogSchema::new(LogEntry::PeerStates)
                        .event(LogEvent::PeerIgnored)
                        .message("Peer will be ignored")
                        .peer(&peer))
                );
            }
```
