# Audit Report

## Title
Configuration-Based Denial of Service via Zero Timeout Values in MempoolConfig

## Summary
The `MempoolConfig` struct lacks validation for timeout values, allowing all timeout fields to be set to 0. This causes immediate transaction expiration, continuous garbage collection loops, and excessive broadcast retries, resulting in complete node dysfunction and potential network-wide degradation.

## Finding Description

The `MempoolConfig::sanitize()` method is a no-op that performs no validation on timeout values: [1](#0-0) 

When timeout values are set to 0, multiple critical failures occur:

**1. Immediate Transaction Expiration (`system_transaction_timeout_secs = 0`):**

When transactions are inserted into mempool, their expiration time is calculated as: [2](#0-1) 

With timeout = 0, `expiration_time = current_time`, making all transactions immediately eligible for garbage collection. The test suite explicitly demonstrates this behavior: [3](#0-2) 

**2. Continuous GC Loop (`system_transaction_gc_interval_ms = 0`):**

The GC coordinator runs on an interval that can be set to 0: [4](#0-3) 

Combined with immediate expiration, this creates a tight loop continuously locking and scanning the mempool.

**3. Broadcast Spam (`shared_mempool_tick_interval_ms = 0`):**

Broadcasts are scheduled with zero delay: [5](#0-4) 

This causes broadcasts to fire continuously without delay, saturating network bandwidth and CPU.

**4. Immediate ACK Timeout (`shared_mempool_ack_timeout_ms = 0`):**

All un-ACKed broadcasts are immediately considered expired: [6](#0-5) 

This triggers constant rebroadcasting, amplifying network traffic.

The vulnerability breaks the **Resource Limits** invariant: operations must respect computational limits. With zero timeouts, the node enters pathological states with unbounded CPU consumption and network traffic.

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:
- **Validator node slowdowns**: CPU pegged at 100% from tight GC/broadcast loops
- **Significant protocol violations**: Mempool cannot retain transactions, breaking transaction propagation
- **Loss of liveness**: Affected validators cannot propose blocks or process transactions

If multiple validators are misconfigured (e.g., via centralized configuration management), this could escalate to **Critical Severity**:
- **Total loss of network availability**: Network cannot process transactions if sufficient validators are affected
- **Non-recoverable without intervention**: Requires manual configuration fix and node restart

The impact is severe because:
1. Node becomes completely non-functional for transaction processing
2. Excessive network traffic affects peer nodes
3. Memory and CPU exhaustion can crash the node
4. No runtime protection or recovery mechanism exists

## Likelihood Explanation

**Medium-High Likelihood:**

This vulnerability can manifest through:

1. **Accidental Misconfiguration**: Operators may set timeouts to 0 thinking it means "no timeout" or during testing/debugging
2. **Configuration Template Errors**: Zero values in configuration templates propagated across multiple nodes
3. **Centralized Configuration Systems**: If nodes pull config from a central source, a single misconfiguration affects many nodes
4. **Automated Configuration Tools**: Infrastructure-as-code tools may default to 0 or allow unvalidated inputs

The TODO comment indicates developers recognized the need for validation but never implemented it: [7](#0-6) 

Other config modules implement proper validation (e.g., `ConsensusConfig`, `QuorumStoreConfig`), demonstrating this is a known pattern that was missed for MempoolConfig: [8](#0-7) 

## Recommendation

Implement validation in `MempoolConfig::sanitize()` to enforce minimum timeout values:

```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        
        // Validate system transaction timeout
        if _node_config.mempool.system_transaction_timeout_secs == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "system_transaction_timeout_secs must be greater than 0".to_string(),
            ));
        }
        
        // Validate GC interval
        if _node_config.mempool.system_transaction_gc_interval_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "system_transaction_gc_interval_ms must be greater than 0".to_string(),
            ));
        }
        
        // Validate broadcast tick interval
        if _node_config.mempool.shared_mempool_tick_interval_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "shared_mempool_tick_interval_ms must be greater than 0".to_string(),
            ));
        }
        
        // Validate ACK timeout
        if _node_config.mempool.shared_mempool_ack_timeout_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "shared_mempool_ack_timeout_ms must be greater than 0".to_string(),
            ));
        }
        
        // Validate backoff interval
        if _node_config.mempool.shared_mempool_backoff_interval_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "shared_mempool_backoff_interval_ms must be greater than 0".to_string(),
            ));
        }
        
        Ok(())
    }
}
```

Consider enforcing reasonable minimum values (e.g., tick_interval >= 1ms, timeout >= 10s) to prevent near-zero values that could still cause performance issues.

## Proof of Concept

The existing test demonstrates immediate expiration with zero timeout: [3](#0-2) 

To reproduce the full DoS scenario:

1. Create a node config with zero timeouts:
```rust
let mut config = NodeConfig::generate_random_config();
config.mempool.system_transaction_timeout_secs = 0;
config.mempool.system_transaction_gc_interval_ms = 0;
config.mempool.shared_mempool_tick_interval_ms = 0;
config.mempool.shared_mempool_ack_timeout_ms = 0;
```

2. Start a node with this configuration
3. Submit transactions - they are immediately expired and removed
4. Observe CPU usage at 100% from GC and broadcast loops
5. Monitor network traffic showing continuous rebroadcast attempts

The node becomes unable to retain or propagate transactions, effectively rendering it non-functional.

## Notes

This is an operational security vulnerability that requires configuration file access to exploit. However, it represents a critical gap in input validation that violates security best practices. The severity is high because:

1. The lack of validation is explicitly noted as a TODO in the code
2. Similar validation exists for other critical config modules
3. The impact on node operation is complete and immediate
4. Multiple deployment scenarios make accidental exploitation likely
5. It affects network-wide stability if multiple nodes are misconfigured

The fix is straightforward and follows established patterns in the codebase.

### Citations

**File:** config/src/config/mempool_config.rs (L176-184)
```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        Ok(()) // TODO: add reasonable verifications
    }
}
```

**File:** mempool/src/core_mempool/mempool.rs (L332-334)
```rust
        let now = SystemTime::now();
        let expiration_time =
            aptos_infallible::duration_since_epoch_at(&now) + self.system_transaction_timeout;
```

**File:** mempool/src/tests/core_mempool_test.rs (L520-544)
```rust
#[test]
fn test_system_ttl() {
    // Created mempool with system_transaction_timeout = 0.
    // All transactions are supposed to be evicted on next gc run.
    let mut config = NodeConfig::generate_random_config();
    config.mempool.system_transaction_timeout_secs = 0;
    let mut mempool = CoreMempool::new(&config);

    add_txn(
        &mut mempool,
        TestTransaction::new(0, ReplayProtector::SequenceNumber(0), 10),
    )
    .unwrap();

    // Reset system ttl timeout.
    mempool.system_transaction_timeout = Duration::from_secs(10);
    // Add new transaction. Should be valid for 10 seconds.
    let transaction = TestTransaction::new(1, ReplayProtector::SequenceNumber(0), 1);
    add_txn(&mut mempool, transaction.clone()).unwrap();

    // GC routine should clear transaction from first insert but keep last one.
    mempool.gc();
    let batch = mempool.get_batch(1, 1024, true, btreemap![]);
    assert_eq!(vec![transaction.make_signed_transaction()], batch);
}
```

**File:** mempool/src/shared_mempool/coordinator.rs (L445-454)
```rust
pub(crate) async fn gc_coordinator(mempool: Arc<Mutex<CoreMempool>>, gc_interval_ms: u64) {
    debug!(LogSchema::event_log(LogEntry::GCRuntime, LogEvent::Start));
    let mut interval = IntervalStream::new(interval(Duration::from_millis(gc_interval_ms)));
    while let Some(_interval) = interval.next().await {
        sample!(
            SampleRate::Duration(Duration::from_secs(60)),
            debug!(LogSchema::event_log(LogEntry::GCRuntime, LogEvent::Live))
        );
        mempool.lock().gc();
    }
```

**File:** mempool/src/shared_mempool/tasks.rs (L110-122)
```rust
    let interval_ms = if schedule_backoff {
        smp.config.shared_mempool_backoff_interval_ms
    } else {
        smp.config.shared_mempool_tick_interval_ms
    };

    scheduled_broadcasts.push(ScheduledBroadcast::new(
        Instant::now() + Duration::from_millis(interval_ms),
        peer,
        schedule_backoff,
        executor,
    ))
}
```

**File:** mempool/src/shared_mempool/network.rs (L432-439)
```rust
            let deadline = sent_time.add(Duration::from_millis(
                self.mempool_config.shared_mempool_ack_timeout_ms,
            ));
            if SystemTime::now().duration_since(deadline).is_ok() {
                expired_message_id = Some(message);
            } else {
                pending_broadcasts += 1;
            }
```

**File:** config/src/config/consensus_config.rs (L503-532)
```rust
impl ConfigSanitizer for ConsensusConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        // Verify that the safety rules and quorum store configs are valid
        SafetyRulesConfig::sanitize(node_config, node_type, chain_id)?;
        QuorumStoreConfig::sanitize(node_config, node_type, chain_id)?;

        // Verify that the consensus-only feature is not enabled in mainnet
        if let Some(chain_id) = chain_id {
            if chain_id.is_mainnet() && is_consensus_only_perf_test_enabled() {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "consensus-only-perf-test should not be enabled in mainnet!".to_string(),
                ));
            }
        }

        // Sender block limits must be <= receiver block limits
        Self::sanitize_send_recv_block_limits(&sanitizer_name, &node_config.consensus)?;

        // Quorum store batches must be <= consensus blocks
        Self::sanitize_batch_block_limits(&sanitizer_name, &node_config.consensus)?;

        Ok(())
    }
```
