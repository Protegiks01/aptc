# Audit Report

## Title
Remote Validator Node Crash via Admin Service OptQuorumStore Batch Retrieval Panic

## Summary
The `extract_txns_from_block()` function in the admin service contains two `.unwrap()` calls when processing `OptQuorumStore` payloads that cause validator node crashes when batches have been garbage collected. An attacker with access to the admin service (enabled by default on testnet/devnet) can remotely crash validators by querying blocks from previous epochs whose batches have been cleaned up.

## Finding Description

The vulnerability exists in the admin service's block dumping functionality. When processing `OptQuorumStore` payload types (V1 and V2), the code retrieves transactions from two separate batch sources using `.unwrap()`: [1](#0-0) [2](#0-1) 

The `extract_txns_from_quorum_store()` helper function returns an error when batches are missing from the database: [3](#0-2) 

The quorum store database performs aggressive garbage collection of batches from previous epochs: [4](#0-3) [5](#0-4) 

When the `.unwrap()` panics, Aptos's global panic handler terminates the entire validator process: [6](#0-5) [7](#0-6) 

The admin service exposes this vulnerable code path through HTTP endpoints: [8](#0-7) [9](#0-8) 

The admin service is enabled by default on non-mainnet networks and can operate without authentication: [10](#0-9) [11](#0-10) [12](#0-11) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria:

1. **Validator Node Crash**: The panic causes `process::exit(12)`, terminating the entire validator process, not just the admin service thread.

2. **Remote Exploitation**: No privileged access required—only HTTP access to the admin service endpoint, which is enabled by default on testnet/devnet.

3. **Network Liveness Risk**: If exploited against multiple validators simultaneously on testnet/devnet, this could cause network-wide liveness issues when < 2/3 validators remain operational.

4. **Trivial Exploitation**: Attacker only needs to identify block IDs from previous epochs (publicly available on-chain data) and send HTTP requests to the `/debug/consensus/block?block_id=<hash>` endpoint.

## Likelihood Explanation

**High Likelihood**:

1. **Natural Occurrence**: Old blocks with `OptQuorumStore` payloads persist in consensus DB while their batches are routinely garbage collected during epoch transitions.

2. **Default Configuration**: Admin service is enabled by default on all non-mainnet deployments, with optional authentication that may not be configured.

3. **Simple Attack**: No special timing, race conditions, or complex state manipulation required—just query old block IDs.

4. **Wide Exposure**: All testnet/devnet validators with admin service enabled are vulnerable.

## Recommendation

Replace `.unwrap()` with proper error handling that returns errors instead of panicking:

```rust
Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
    let mut all_txns = extract_txns_from_quorum_store(
        p.proof_with_data().iter().map(|proof| *proof.digest()),
        all_batches,
    )?;  // Return error instead of panic
    all_txns.extend(
        extract_txns_from_quorum_store(
            p.opt_batches().iter().map(|info| *info.digest()),
            all_batches,
        )?,  // Return error instead of panic
    );
    Ok(all_txns)
},
```

Apply the same fix to V2 and all other payload types using `.unwrap()` in this function.

Additionally, consider:
1. Enforcing authentication on admin service for all deployments
2. Adding graceful degradation when batches are missing (return partial data with warnings)
3. Implementing rate limiting on admin endpoints

## Proof of Concept

1. **Setup**: Deploy an Aptos testnet/devnet validator with admin service enabled (default configuration)

2. **Wait for Epoch Transition**: Allow the node to transition to a new epoch, triggering garbage collection of old batches

3. **Identify Target Block**: Query the consensus DB to find a block ID from the previous epoch with `OptQuorumStore` payload

4. **Trigger Crash**: Send HTTP request:
   ```bash
   curl "http://validator-ip:9102/debug/consensus/block?block_id=<old_block_hash>&bcs=true"
   ```

5. **Observe Result**: Validator process terminates with exit code 12, logs show panic in `extract_txns_from_block`

**Expected Behavior**: Admin endpoint returns error response without crashing the node

**Actual Behavior**: Entire validator process terminates via `process::exit(12)`

---

**Notes**

This vulnerability demonstrates a critical design flaw where debugging/tooling code shares the same process and panic handler as the consensus-critical validator components. The `.unwrap()` usage is explicitly allowed in this file (`#![allow(clippy::unwrap_used)]` at line 4), but this exception is inappropriate given the global panic handler's process termination behavior.

### Citations

**File:** consensus/src/util/db_tool.rs (L69-86)
```rust
fn extract_txns_from_quorum_store(
    digests: impl Iterator<Item = HashValue>,
    all_batches: &HashMap<HashValue, PersistedValue<BatchInfo>>,
) -> anyhow::Result<Vec<&SignedTransaction>> {
    let mut block_txns = Vec::new();
    for digest in digests {
        if let Some(batch) = all_batches.get(&digest) {
            if let Some(txns) = batch.payload() {
                block_txns.extend(txns);
            } else {
                bail!("Payload is not found for batch ({digest}).");
            }
        } else {
            bail!("Batch ({digest}) is not found.");
        }
    }
    Ok(block_txns)
}
```

**File:** consensus/src/util/db_tool.rs (L121-135)
```rust
            Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
                let mut all_txns = extract_txns_from_quorum_store(
                    p.proof_with_data().iter().map(|proof| *proof.digest()),
                    all_batches,
                )
                .unwrap();
                all_txns.extend(
                    extract_txns_from_quorum_store(
                        p.opt_batches().iter().map(|info| *info.digest()),
                        all_batches,
                    )
                    .unwrap(),
                );
                Ok(all_txns)
            },
```

**File:** consensus/src/util/db_tool.rs (L136-150)
```rust
            Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => {
                let mut all_txns = extract_txns_from_quorum_store(
                    p.proof_with_data().iter().map(|proof| *proof.digest()),
                    all_batches,
                )
                .unwrap();
                all_txns.extend(
                    extract_txns_from_quorum_store(
                        p.opt_batches().iter().map(|info| *info.digest()),
                        all_batches,
                    )
                    .unwrap(),
                );
                Ok(all_txns)
            },
```

**File:** consensus/src/quorum_store/batch_store.rs (L181-210)
```rust
    fn gc_previous_epoch_batches_from_db_v1(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db.get_all_batches().expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L212-243)
```rust
    fn gc_previous_epoch_batches_from_db_v2(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** crates/crash-handler/src/lib.rs (L21-58)
```rust
/// Invoke to ensure process exits on a thread panic.
///
/// Tokio's default behavior is to catch panics and ignore them.  Invoking this function will
/// ensure that all subsequent thread panics (even Tokio threads) will report the
/// details/backtrace and then exit.
pub fn setup_panic_handler() {
    panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}

// Formats and logs panic information
fn handle_panic(panic_info: &PanicHookInfo<'_>) {
    // The Display formatter for a PanicHookInfo contains the message, payload and location.
    let details = format!("{}", panic_info);
    let backtrace = format!("{:#?}", Backtrace::new());

    let info = CrashInfo { details, backtrace };
    let crash_info = toml::to_string_pretty(&info).unwrap();
    error!("{}", crash_info);
    // TODO / HACK ALARM: Write crash info synchronously via eprintln! to ensure it is written before the process exits which error! doesn't guarantee.
    // This is a workaround until https://github.com/aptos-labs/aptos-core/issues/2038 is resolved.
    eprintln!("{}", crash_info);

    // Wait till the logs have been flushed
    aptos_logger::flush();

    // Do not kill the process if the panics happened at move-bytecode-verifier.
    // This is safe because the `state::get_state()` uses a thread_local for storing states. Thus the state can only be mutated to VERIFIER by the thread that's running the bytecode verifier.
    //
    // TODO: once `can_unwind` is stable, we should assert it. See https://github.com/rust-lang/rust/issues/92988.
    if state::get_state() == VMState::VERIFIER || state::get_state() == VMState::DESERIALIZER {
        return;
    }

    // Kill the process
    process::exit(12);
}
```

**File:** aptos-node/src/lib.rs (L233-234)
```rust
    // Setup panic handler
    aptos_crash_handler::setup_panic_handler();
```

**File:** crates/aptos-admin-service/src/server/mod.rs (L154-181)
```rust
        let mut authenticated = false;
        if context.config.authentication_configs.is_empty() {
            authenticated = true;
        } else {
            for authentication_config in &context.config.authentication_configs {
                match authentication_config {
                    AuthenticationConfig::PasscodeSha256(passcode_sha256) => {
                        let query = req.uri().query().unwrap_or("");
                        let query_pairs: HashMap<_, _> =
                            url::form_urlencoded::parse(query.as_bytes()).collect();
                        let passcode: Option<String> =
                            query_pairs.get("passcode").map(|p| p.to_string());
                        if let Some(passcode) = passcode {
                            if sha256::digest(passcode) == *passcode_sha256 {
                                authenticated = true;
                            }
                        }
                    },
                }
            }
        };

        if !authenticated {
            return Ok(reply_with_status(
                StatusCode::NETWORK_AUTHENTICATION_REQUIRED,
                format!("{} endpoint requires authentication.", req.uri().path()),
            ));
        }
```

**File:** crates/aptos-admin-service/src/server/mod.rs (L216-228)
```rust
            (hyper::Method::GET, "/debug/consensus/block") => {
                let consensus_db = context.consensus_db.read().clone();
                let quorum_store_db = context.quorum_store_db.read().clone();
                if let Some(consensus_db) = consensus_db
                    && let Some(quorum_store_db) = quorum_store_db
                {
                    consensus::handle_dump_block_request(req, consensus_db, quorum_store_db).await
                } else {
                    Ok(reply_with_status(
                        StatusCode::NOT_FOUND,
                        "Consensus db and/or quorum store db is not available.",
                    ))
                }
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L217-240)
```rust
fn dump_blocks_bcs(
    consensus_db: &dyn PersistentLivenessStorage,
    quorum_store_db: &dyn QuorumStoreStorage,
    block_id: Option<HashValue>,
) -> anyhow::Result<Vec<u8>> {
    let all_batches = quorum_store_db.get_all_batches()?;

    let (_, _, blocks, _) = consensus_db.consensus_db().get_data()?;

    let mut all_txns = Vec::new();
    for block in blocks {
        let id = block.id();
        if block_id.is_none() || id == block_id.unwrap() {
            match extract_txns_from_block(&block, &all_batches) {
                Ok(txns) => {
                    all_txns.extend(txns.into_iter().cloned().map(Transaction::UserTransaction));
                },
                Err(e) => bail!("Failed to extract txns from block ({id:?}): {e:?}."),
            };
        }
    }

    bcs::to_bytes(&all_txns).map_err(Error::msg)
}
```

**File:** config/src/config/admin_service_config.rs (L15-24)
```rust
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct AdminServiceConfig {
    pub enabled: Option<bool>,
    pub address: String,
    pub port: u16,
    // If empty, will allow all requests without authentication. (Not allowed on mainnet.)
    pub authentication_configs: Vec<AuthenticationConfig>,
    pub malloc_stats_max_len: usize,
}
```

**File:** config/src/config/admin_service_config.rs (L93-103)
```rust
        if node_config.admin_service.enabled.is_none() {
            // Only enable the admin service if the chain is not mainnet
            let admin_service_enabled = if let Some(chain_id) = chain_id {
                !chain_id.is_mainnet()
            } else {
                false // We cannot determine the chain ID, so we disable the admin service
            };
            node_config.admin_service.enabled = Some(admin_service_enabled);

            modified_config = true; // The config was modified
        }
```
