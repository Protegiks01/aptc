# Audit Report

## Title
Consensus Break Due to Non-Deterministic Remote Executor Shard Address Ordering

## Summary
The sharded block executor's remote execution mode assumes that the `remote_executor_addresses` vector is ordered by `shard_id`, but this ordering is not validated or enforced. If validators configure their remote executor addresses in different orders, they will dispatch transactions to wrong shards and aggregate results with mismatched shard IDs, producing different final transaction orderings and breaking consensus.

## Finding Description

The sharded block executor relies on a critical but unvalidated assumption about the ordering of remote executor addresses. The vulnerability spans multiple components:

**1. Assumption in Process Executor Service**

The remote executor service process expects that `remote_executor_addresses[shard_id]` corresponds to its own address: [1](#0-0) 

This creates an implicit requirement that the `remote_executor_addresses` vector must be ordered by `shard_id`.

**2. Partitioner Output is Deterministic**

The partitioner creates `PartitionedTransactions` with shards always ordered by `shard_id` (0, 1, 2, ...): [2](#0-1) 

**3. Transaction Dispatch Uses Vector Index**

When dispatching transactions to remote executors, the code iterates with `enumerate()` and sends `sub_blocks[i]` to `command_txs[i]`, which maps to `remote_shard_addresses[i]`: [3](#0-2) 

**4. Result Collection Uses Same Index**

Results are collected by iterating over channels in order: [4](#0-3) 

**5. Aggregation Uses Enumerate Index as Shard ID**

The aggregation assumes the vector position corresponds to `shard_id`: [5](#0-4) 

**Attack Scenario:**

Validator A configures: `remote_executor_addresses = [addr_shard_0, addr_shard_1, addr_shard_2]`
Validator B configures: `remote_executor_addresses = [addr_shard_1, addr_shard_0, addr_shard_2]`

Execution flow:
1. Both validators receive the same block with `PartitionedTransactions` where `sharded_txns[0]` contains transactions for `shard_id=0`
2. Validator A sends `sharded_txns[0]` to `addr_shard_0` (correct)
3. Validator B sends `sharded_txns[0]` to `addr_shard_1` (WRONG - shard 1 executes shard 0's transactions!)
4. Results come back in different orders
5. Aggregation places results at positions using formula `ordered_results[round * num_shards + enumerate_index]`
6. Validator A places shard 0's results at position `round * 3 + 0`
7. Validator B places shard 0's results at position `round * 3 + 1` (different position!)
8. Final transaction orderings differ → Different state roots → **CONSENSUS BREAK**

**Root Cause:**

The system has no validation that `remote_executor_addresses` is correctly ordered across all validators. Each validator independently configures this via CLI arguments: [6](#0-5) 

And the coordinator checks if remote addresses exist to decide which executor to use: [7](#0-6) 

## Impact Explanation

**Critical Severity** - This vulnerability directly violates the **Deterministic Execution** invariant, which states: "All validators must produce identical state roots for identical blocks."

If validators have different `remote_executor_addresses` orderings:
- They execute transactions in different orders
- They compute different state roots for the same block
- Consensus cannot be achieved
- Network halts or forks (requires hardfork to recover)

This meets the Critical Severity criteria:
- **Consensus/Safety violation**: Different validators produce different state roots
- **Non-recoverable network partition**: Requires manual intervention or hardfork
- **Total loss of liveness**: Network cannot make progress when validators disagree on state

## Likelihood Explanation

**HIGH Likelihood** - This vulnerability can occur through:

1. **Operator Misconfiguration**: Validator operators manually configure `remote_executor_addresses` via CLI or config files. Simple human error (copy-paste in wrong order, DNS resolution differences, etc.) will trigger this bug.

2. **No Validation**: The system provides no validation to detect misconfiguration before consensus breaks. The bug only manifests when blocks are executed and compared.

3. **No On-Chain Coordination**: Unlike other critical parameters (validator set, staking, etc.), remote executor addresses are purely off-chain configuration with no consensus mechanism.

4. **Deployment Complexity**: When deploying sharded execution across multiple machines, operators must ensure address ordering matches across ALL validators - a complex operational requirement with no tooling support.

5. **Configuration Drift**: Over time, as validator sets change or configurations are updated, the likelihood of ordering mismatches increases.

## Recommendation

**Immediate Fix**: Add validation to ensure `remote_executor_addresses` ordering is deterministic and consistent across all validators.

**Option 1: Embed Shard ID in Protocol**
Modify the remote executor protocol to include explicit shard IDs in messages, and validate that received results match expected shard IDs:

```rust
// In RemoteExecutorClient::get_output_from_shards()
fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
    let mut results = vec![vec![]; self.num_shards()];
    for rx in self.result_rxs.iter() {
        let received_bytes = rx.recv().unwrap().to_bytes();
        let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
        let shard_id = result.shard_id; // Add shard_id to RemoteExecutionResult
        ensure!(shard_id < self.num_shards(), "Invalid shard_id");
        results[shard_id] = result.inner?;
    }
    Ok(results)
}
```

**Option 2: Configuration Validation**
Add startup validation that queries each remote shard for its `shard_id` and verifies the ordering:

```rust
// In RemoteExecutorClient::new()
pub fn new(remote_shard_addresses: Vec<SocketAddr>, ...) -> Self {
    // ... existing setup ...
    
    // Validate shard ordering
    for (expected_shard_id, address) in remote_shard_addresses.iter().enumerate() {
        let actual_shard_id = query_shard_id(address)?;
        ensure!(
            actual_shard_id == expected_shard_id,
            "Shard ordering mismatch: address {} has shard_id {} but is at position {}",
            address, actual_shard_id, expected_shard_id
        );
    }
    
    Self { ... }
}
```

**Option 3: On-Chain Coordination**
Store canonical shard address mappings on-chain as part of validator configuration, ensuring all validators use the same ordering.

## Proof of Concept

**Setup:**

1. Deploy 2 remote executor shard processes:
   - Shard 0 at `10.0.0.1:5000`
   - Shard 1 at `10.0.0.2:5000`

2. Configure Validator A:
   ```bash
   --remote-executor-addresses 10.0.0.1:5000 10.0.0.2:5000
   ```

3. Configure Validator B (swapped order):
   ```bash
   --remote-executor-addresses 10.0.0.2:5000 10.0.0.1:5000
   ```

**Execution:**

1. Both validators receive the same block with transactions partitioned into 2 shards
2. Validator A sends:
   - Shard 0 transactions → 10.0.0.1:5000 ✓
   - Shard 1 transactions → 10.0.0.2:5000 ✓
3. Validator B sends:
   - Shard 0 transactions → 10.0.0.2:5000 ✗ (should be shard 1!)
   - Shard 1 transactions → 10.0.0.1:5000 ✗ (should be shard 0!)

**Result:**

Validator A aggregates: `[shard_0_results, shard_1_results]`
Validator B aggregates: `[shard_1_results, shard_0_results]`

Both validators compute different state roots for the same block → Consensus failure.

**Notes**

This vulnerability demonstrates a critical gap in the sharded execution architecture: the lack of validation for a configuration parameter that is essential for consensus correctness. While the local executor mode is safe (channels are created in deterministic order within the same process), the remote executor mode introduces an unvalidated external configuration dependency that can break consensus.

The issue is particularly insidious because:
1. It only manifests in production deployments with remote executors
2. No error is raised until state roots are compared
3. The misconfiguration is silent and difficult to diagnose
4. Recovery requires manual intervention or hardfork

This represents a violation of the defense-in-depth principle: critical consensus invariants should be validated programmatically, not left to operational procedures.

### Citations

**File:** execution/executor-service/src/process_executor_service.rs (L24-24)
```rust
        let self_address = remote_shard_addresses[shard_id];
```

**File:** execution/block-partitioner/src/v2/build_edge.rs (L73-88)
```rust
        let sharded_txns = (0..state.num_executor_shards)
            .map(|shard_id| {
                let sub_blocks: Vec<SubBlock<AnalyzedTransaction>> = (0..final_num_rounds)
                    .map(|round_id| {
                        state.sub_block_matrix[round_id][shard_id]
                            .lock()
                            .unwrap()
                            .take()
                            .unwrap()
                    })
                    .collect();
                SubBlocksForShard::new(shard_id, sub_blocks)
            })
            .collect();

        PartitionedTransactions::new(sharded_txns, global_txns)
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L193-206)
```rust
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L102-105)
```rust
        for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
            for (round, result) in results_from_shard.into_iter().enumerate() {
                ordered_results[round * num_executor_shards + shard_id] = result;
            }
```

**File:** execution/executor-service/src/main.rs (L20-21)
```rust
    #[clap(long, num_args = 1..)]
    pub remote_executor_addresses: Vec<SocketAddr>,
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L261-267)
```rust
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
```
