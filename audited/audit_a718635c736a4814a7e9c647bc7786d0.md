# Audit Report

## Title
StateKvDb Checkpoint Race Condition Allows Inconsistent Cross-Shard Snapshots

## Summary
The `StateKvDb::create_checkpoint()` function creates checkpoints of 16 database shards sequentially without any synchronization mechanism to prevent concurrent writes. This allows ongoing commit operations to modify shards while checkpointing is in progress, resulting in inconsistent snapshots where different shards represent different blockchain versions.

## Finding Description

The vulnerability exists in the checkpoint creation mechanism for the sharded State Key-Value database. [1](#0-0) 

The `create_checkpoint()` function sequentially creates checkpoints for the metadata database and all 16 shards in a for loop. Meanwhile, the `commit()` function writes to all shards in parallel using a thread pool: [2](#0-1) 

**Critical Issue**: `AptosDB::create_checkpoint()` is a static method with no access to instance-level locks: [3](#0-2) 

While the running AptosDB instance uses `commit_lock` and `pre_commit_lock` mutexes to prevent concurrent commits, the checkpoint creation process opens its own database instances and has no coordination with these locks: [4](#0-3) 

**Race Condition Scenario**:
1. Node is at version N
2. `create_checkpoint()` begins execution
3. Metadata DB checkpoint created (captures `StateKvCommitProgress = N`)
4. Shards 0-5 checkpointed (version N state)
5. Node commits version N+1 via `commit()`, writing to all 16 shards in parallel
6. Shards 6-15 checkpointed (version N+1 state)

**Result**: The checkpoint contains mixed state where:
- Metadata indicates `StateKvCommitProgress = N`
- Shards 0-5 contain data at version N
- Shards 6-15 contain data at version N+1
- Per-shard progress markers `StateKvShardCommitProgress(shard_id)` are inconsistent

This violates the **State Consistency** invariant that requires state transitions to be atomic and verifiable.

While recovery mechanisms exist to detect and fix inconsistencies on startup via truncation: [5](#0-4) 

The checkpoint itself remains fundamentally inconsistent, containing partial state from multiple blockchain versions.

## Impact Explanation

This qualifies as **HIGH severity** per Aptos bug bounty criteria due to:

1. **Significant Protocol Violation**: Breaks the atomic state consistency invariant. Different shards in the checkpoint represent different blockchain versions, violating the fundamental requirement that all state at a given version must be consistent.

2. **State Inconsistencies Requiring Intervention**: When restored, the inconsistent checkpoint requires truncation back to the lower version, causing data loss. The state from version N+1 in shards 6-15 must be deleted.

3. **Consensus Risk**: If multiple validators create checkpoints at different times and restore from them for disaster recovery, they could restore to slightly different states, potentially causing consensus divergence or state root mismatches.

4. **Operational Impact**: Compromises backup integrity and disaster recovery procedures. Checkpoints cannot be trusted as consistent point-in-time snapshots.

## Likelihood Explanation

**High Likelihood** - This occurs naturally during normal operation:

- Checkpoints are created for disaster recovery, node cloning, and backup purposes
- Nodes continuously commit new blocks during normal operation
- No warnings or safeguards prevent checkpoint creation during active operation
- The sequential checkpoint creation of 16 shards takes non-trivial time, increasing the window for concurrent commits
- The vulnerability requires no malicious actor - it's a race condition in normal operations

## Recommendation

Implement atomic checkpoint creation across all shards by acquiring the commit lock before starting checkpoint operations:

```rust
// In AptosDB
pub fn create_checkpoint_with_instance(
    &self,
    cp_path: impl AsRef<Path>,
    sharding: bool,
) -> Result<()> {
    // Acquire commit lock to prevent concurrent writes
    let _lock = self.commit_lock.lock().expect("Failed to acquire commit lock");
    
    let start = Instant::now();
    info!(sharding = sharding, "Creating checkpoint for AptosDB with lock acquired.");
    
    // Now create checkpoints - no concurrent writes possible
    LedgerDb::create_checkpoint(self.db_path(), cp_path.as_ref(), sharding)?;
    if sharding {
        StateKvDb::create_checkpoint(self.db_path(), cp_path.as_ref())?;
        StateMerkleDb::create_checkpoint(
            self.db_path(),
            cp_path.as_ref(),
            sharding,
            /* is_hot = */ true,
        )?;
    }
    StateMerkleDb::create_checkpoint(
        self.db_path(),
        cp_path.as_ref(),
        sharding,
        /* is_hot = */ false,
    )?;
    
    info!(time_ms = %start.elapsed().as_millis(), "Checkpoint complete with consistency guarantee.");
    Ok(())
}
```

Alternative solution: Use RocksDB's `GetSnapshot()` API to create a consistent snapshot handle, then create checkpoints from that snapshot.

## Proof of Concept

```rust
#[test]
fn test_checkpoint_race_condition() {
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    // Setup: Create a StateKvDb with test data at version 1000
    let tmpdir = TempPath::new();
    let db_paths = StorageDirPaths::from_path(&tmpdir);
    let state_kv_db = Arc::new(
        StateKvDb::open_sharded(&db_paths, RocksdbConfig::default(), None, None, false).unwrap()
    );
    
    // Commit initial state at version 1000
    commit_version(&state_kv_db, 1000);
    
    let barrier = Arc::new(Barrier::new(2));
    let db_clone = Arc::clone(&state_kv_db);
    let barrier_clone = Arc::clone(&barrier);
    let checkpoint_dir = tmpdir.path().join("checkpoint");
    
    // Thread 1: Start checkpoint creation
    let checkpoint_thread = thread::spawn(move || {
        barrier_clone.wait();
        StateKvDb::create_checkpoint(
            tmpdir.path(),
            checkpoint_dir.as_path(),
        ).unwrap();
    });
    
    // Thread 2: Commit version 1001 while checkpoint is in progress
    let commit_thread = thread::spawn(move || {
        barrier.wait();
        thread::sleep(Duration::from_millis(10)); // Let checkpoint start
        commit_version(&db_clone, 1001);
    });
    
    checkpoint_thread.join().unwrap();
    commit_thread.join().unwrap();
    
    // Verify: Check if checkpoint has inconsistent shard versions
    let checkpoint_db = StateKvDb::open_sharded(
        &StorageDirPaths::from_path(&checkpoint_dir),
        RocksdbConfig::default(),
        None,
        None,
        true, // readonly
    ).unwrap();
    
    let mut shard_versions = Vec::new();
    for shard_id in 0..NUM_STATE_SHARDS {
        let version = checkpoint_db.db_shard(shard_id)
            .get::<DbMetadataSchema>(&DbMetadataKey::StateKvShardCommitProgress(shard_id))
            .unwrap()
            .unwrap()
            .expect_version();
        shard_versions.push(version);
    }
    
    // Assertion: If race condition occurred, shard versions will differ
    let min_version = *shard_versions.iter().min().unwrap();
    let max_version = *shard_versions.iter().max().unwrap();
    assert!(max_version > min_version, 
        "Race condition detected: checkpoint has inconsistent shard versions {} to {}", 
        min_version, max_version);
}
```

## Notes

This vulnerability affects all checkpoint-based recovery mechanisms and backup systems. The inconsistency is automatically fixed on node restart through the truncation mechanism in `sync_commit_progress()`, but this requires data loss and doesn't address the fundamental problem that checkpoints cannot be trusted as consistent snapshots. Additionally, the recovery mechanism assumes single-node scenarios - if multiple validators restore from checkpoints created at different times with different inconsistencies, they may diverge in their recovered state.

### Citations

**File:** storage/aptosdb/src/state_kv_db.rs (L177-208)
```rust
    pub(crate) fn commit(
        &self,
        version: Version,
        state_kv_metadata_batch: Option<SchemaBatch>,
        sharded_state_kv_batches: ShardedStateKvSchemaBatch,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit"]);
        {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_shards"]);
            THREAD_MANAGER.get_io_pool().scope(|s| {
                let mut batches = sharded_state_kv_batches.into_iter();
                for shard_id in 0..NUM_STATE_SHARDS {
                    let state_kv_batch = batches
                        .next()
                        .expect("Not sufficient number of sharded state kv batches");
                    s.spawn(move |_| {
                        // TODO(grao): Consider propagating the error instead of panic, if necessary.
                        self.commit_single_shard(version, shard_id, state_kv_batch)
                            .unwrap_or_else(|err| {
                                panic!("Failed to commit shard {shard_id}: {err}.")
                            });
                    });
                }
            });
        }
        if let Some(batch) = state_kv_metadata_batch {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_metadata"]);
            self.state_kv_metadata_db.write_schemas(batch)?;
        }

        self.write_progress(version)
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L224-259)
```rust
    pub(crate) fn create_checkpoint(
        db_root_path: impl AsRef<Path>,
        cp_root_path: impl AsRef<Path>,
    ) -> Result<()> {
        // TODO(grao): Support path override here.
        let state_kv_db = Self::open_sharded(
            &StorageDirPaths::from_path(db_root_path),
            RocksdbConfig::default(),
            None,
            None,
            false,
        )?;
        let cp_state_kv_db_path = cp_root_path.as_ref().join(STATE_KV_DB_FOLDER_NAME);

        info!("Creating state_kv_db checkpoint at: {cp_state_kv_db_path:?}");

        std::fs::remove_dir_all(&cp_state_kv_db_path).unwrap_or(());
        std::fs::create_dir_all(&cp_state_kv_db_path).unwrap_or(());

        state_kv_db
            .metadata_db()
            .create_checkpoint(Self::metadata_db_path(cp_root_path.as_ref()))?;

        // TODO(HotState): should handle hot state as well.
        for shard_id in 0..NUM_STATE_SHARDS {
            state_kv_db
                .db_shard(shard_id)
                .create_checkpoint(Self::db_shard_path(
                    cp_root_path.as_ref(),
                    shard_id,
                    /* is_hot = */ false,
                ))?;
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/db/mod.rs (L34-41)
```rust
    /// This is just to detect concurrent calls to `pre_commit_ledger()`
    pre_commit_lock: std::sync::Mutex<()>,
    /// This is just to detect concurrent calls to `commit_ledger()`
    commit_lock: std::sync::Mutex<()>,
    indexer: Option<Indexer>,
    skip_index_and_usage: bool,
    update_subscriber: Option<Sender<(Instant, Version)>>,
}
```

**File:** storage/aptosdb/src/db/mod.rs (L172-205)
```rust
    pub fn create_checkpoint(
        db_path: impl AsRef<Path>,
        cp_path: impl AsRef<Path>,
        sharding: bool,
    ) -> Result<()> {
        let start = Instant::now();

        info!(sharding = sharding, "Creating checkpoint for AptosDB.");

        LedgerDb::create_checkpoint(db_path.as_ref(), cp_path.as_ref(), sharding)?;
        if sharding {
            StateKvDb::create_checkpoint(db_path.as_ref(), cp_path.as_ref())?;
            StateMerkleDb::create_checkpoint(
                db_path.as_ref(),
                cp_path.as_ref(),
                sharding,
                /* is_hot = */ true,
            )?;
        }
        StateMerkleDb::create_checkpoint(
            db_path.as_ref(),
            cp_path.as_ref(),
            sharding,
            /* is_hot = */ false,
        )?;

        info!(
            db_path = db_path.as_ref(),
            cp_path = cp_path.as_ref(),
            time_ms = %start.elapsed().as_millis(),
            "Made AptosDB checkpoint."
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-467)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");
```
