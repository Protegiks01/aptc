# Audit Report

## Title
RPC Response Deserialization CPU Exhaustion After Timeout Expiry

## Summary
The `NetworkSender::send_rpc_raw()` function performs expensive deserialization (including decompression) outside the scope of the RPC timeout, allowing attackers to cause CPU exhaustion on validator nodes by sending late responses with large compressed payloads that decompress after the timeout period has elapsed.

## Finding Description

The vulnerability exists in the timeout scoping for outbound RPC operations. The RPC timeout only covers the network round-trip (waiting for response bytes to arrive), but does not cover the subsequent deserialization step which can be computationally expensive. [1](#0-0) 

The timeout is applied in the RPC protocol layer when waiting for the response: [2](#0-1) 

**Attack Flow:**

1. Attacker sends an RPC request to a victim validator node using a compressed protocol (e.g., `ConsensusRpcCompressed`, `DKGRpcCompressed`, `JWKConsensusRpcCompressed`)
2. The victim node initiates a timeout (e.g., 10 seconds)
3. Just before the timeout expires (e.g., at 9.999 seconds), the attacker sends a crafted response containing:
   - Small compressed size (fits within network frame limits)
   - Decompresses to maximum size (~61.875 MiB per `MAX_APPLICATION_MESSAGE_SIZE`)
   - Content designed to maximize decompression CPU cost
4. The network layer receives the response before timeout expiry, and `send_rpc()` returns successfully with the compressed bytes
5. The `spawn_blocking` task begins deserialization/decompression AFTER the timeout period has conceptually elapsed
6. Decompression of up to 61 MiB of data consumes significant CPU time on the blocking thread pool [3](#0-2) 

The decompression operation is CPU-intensive: [4](#0-3) 

The maximum decompressed size is defined as: [5](#0-4) 

While there is a `max_parallel_deserialization_tasks` configuration, it only applies to inbound messages via `NetworkEvents`, not to outbound RPC response deserialization: [6](#0-5) 

**Security Invariant Violated:** The "Resource Limits" invariant is broken - the timeout is intended as a resource limit on RPC operations, but it doesn't enforce a limit on the total CPU time consumed, allowing unbounded CPU usage after the timeout expires.

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria ("Validator node slowdowns"). The attack enables:

1. **CPU Exhaustion**: Multiple concurrent late responses trigger expensive decompression operations that consume CPU resources after their respective timeouts
2. **Blocking Thread Pool Exhaustion**: Tokio's blocking thread pool (default: number of CPU cores) becomes saturated with decompression tasks
3. **Validator Performance Degradation**: Consensus operations, state sync, and other critical protocols using compressed RPC are affected
4. **Amplification**: Attack scales multiplicatively - each malicious peer can send multiple concurrent RPC requests, and multiple malicious peers amplify the effect
5. **Resource Exhaustion After Timeout**: Applications believe RPCs have timed out and failed, but the node continues consuming CPU processing responses

The attack is particularly dangerous for consensus-critical protocols like `ConsensusRpcCompressed` which are used in the BFT protocol.

## Likelihood Explanation

**Likelihood: High**

- **Attack Complexity**: Low - attacker only needs to:
  - Send standard RPC requests using existing protocols
  - Time responses to arrive just before timeout
  - Craft payloads with high compression ratio (common compression techniques)
- **Attacker Requirements**: Any network peer can exploit this (no privileged access needed)
- **Detection Difficulty**: Hard to distinguish from legitimate slow responses
- **Exploit Cost**: Minimal - uses standard protocol messages
- **Attack Surface**: Affects all compressed RPC protocols used in production

## Recommendation

**Solution: Extend timeout coverage to include deserialization**

Move the deserialization step inside the timeout scope by performing it before the timeout future completes:

```rust
pub async fn send_rpc_raw(
    &self,
    recipient: PeerId,
    protocol: ProtocolId,
    req_msg: Bytes,
    timeout: Duration,
) -> Result<TMessage, RpcError> {
    // Send the request and wait for the response
    let res_data = self
        .peer_mgr_reqs_tx
        .send_rpc(recipient, protocol, req_msg, timeout)
        .await?;

    // Apply a separate timeout to the deserialization step
    let deserialization_timeout = Duration::from_secs(5); // Configure appropriately
    let res_msg = tokio::time::timeout(
        deserialization_timeout,
        tokio::task::spawn_blocking(move || protocol.from_bytes(&res_data))
    )
    .await
    .map_err(|_| RpcError::TimedOut)?? // Timeout on deserialization
    ?; // Handle spawn_blocking join error

    Ok(res_msg)
}
```

**Additional Mitigations:**

1. Apply `max_parallel_deserialization_tasks` limit to outbound RPC response deserialization
2. Add metrics/monitoring for deserialization duration
3. Consider reducing `MAX_APPLICATION_MESSAGE_SIZE` for network protocols
4. Implement per-peer rate limiting on RPC responses

## Proof of Concept

```rust
#[tokio::test]
async fn test_rpc_deserialization_timeout_bypass() {
    use std::time::{Duration, Instant};
    use tokio::time::sleep;
    
    // Simulate the attack scenario
    let rpc_timeout = Duration::from_secs(10);
    let start = Instant::now();
    
    // Simulate network delay until just before timeout
    sleep(Duration::from_millis(9999)).await;
    
    // Response arrives just before timeout - send_rpc() succeeds
    let compressed_response = create_large_compressed_payload(); // ~61 MiB decompressed
    let network_duration = start.elapsed();
    assert!(network_duration < rpc_timeout); // Timeout NOT exceeded
    
    // Now deserialization happens AFTER timeout conceptually expired
    let deser_start = Instant::now();
    let _ = tokio::task::spawn_blocking(move || {
        // Simulate expensive decompression
        decompress_large_payload(compressed_response) // Takes additional seconds
    }).await;
    
    let total_duration = start.elapsed();
    // Total time exceeds intended timeout!
    assert!(total_duration > rpc_timeout); // Demonstrates timeout bypass
    assert!(deser_start.elapsed() > Duration::from_secs(1)); // Significant CPU time after timeout
}

fn create_large_compressed_payload() -> Vec<u8> {
    // Create payload that compresses well but decompresses slowly
    let data = vec![0u8; 61 * 1024 * 1024]; // 61 MiB of zeros
    aptos_compression::compress(
        data,
        aptos_compression::client::CompressionClient::Consensus,
        64 * 1024 * 1024
    ).unwrap()
}

fn decompress_large_payload(compressed: Vec<u8>) -> Vec<u8> {
    aptos_compression::decompress(
        &compressed,
        aptos_compression::client::CompressionClient::Consensus,
        64 * 1024 * 1024
    ).unwrap()
}
```

**Real-world exploitation:**
1. Attacker connects to validator node as peer
2. Sends 100 concurrent `ConsensusRpcCompressed` requests  
3. For each request, waits 9.99 seconds then sends 61 MiB compressed response
4. Validator processes responses, triggering 100 concurrent decompression tasks
5. Blocking thread pool saturates, CPU spikes to 100%
6. Consensus performance degrades, potentially causing missed rounds

### Citations

**File:** network/framework/src/protocols/network/mod.rs (L208-243)
```rust
impl<TMessage: Message + Send + Sync + 'static> NewNetworkEvents for NetworkEvents<TMessage> {
    fn new(
        peer_mgr_notifs_rx: aptos_channel::Receiver<(PeerId, ProtocolId), ReceivedMessage>,
        max_parallel_deserialization_tasks: Option<usize>,
        allow_out_of_order_delivery: bool,
    ) -> Self {
        // Determine the number of parallel deserialization tasks to use
        let max_parallel_deserialization_tasks = max_parallel_deserialization_tasks.unwrap_or(1);

        let data_event_stream = peer_mgr_notifs_rx.map(|notification| {
            tokio::task::spawn_blocking(move || received_message_to_event(notification))
        });

        let data_event_stream: Pin<
            Box<dyn Stream<Item = Event<TMessage>> + Send + Sync + 'static>,
        > = if allow_out_of_order_delivery {
            Box::pin(
                data_event_stream
                    .buffer_unordered(max_parallel_deserialization_tasks)
                    .filter_map(|res| future::ready(res.expect("JoinError from spawn blocking"))),
            )
        } else {
            Box::pin(
                data_event_stream
                    .buffered(max_parallel_deserialization_tasks)
                    .filter_map(|res| future::ready(res.expect("JoinError from spawn blocking"))),
            )
        };

        Self {
            event_stream: data_event_stream,
            done: false,
            _marker: PhantomData,
        }
    }
}
```

**File:** network/framework/src/protocols/network/mod.rs (L455-471)
```rust
    pub async fn send_rpc_raw(
        &self,
        recipient: PeerId,
        protocol: ProtocolId,
        req_msg: Bytes,
        timeout: Duration,
    ) -> Result<TMessage, RpcError> {
        // Send the request and wait for the response
        let res_data = self
            .peer_mgr_reqs_tx
            .send_rpc(recipient, protocol, req_msg, timeout)
            .await?;

        // Deserialize the response using a blocking task
        let res_msg = tokio::task::spawn_blocking(move || protocol.from_bytes(&res_data)).await??;
        Ok(res_msg)
    }
```

**File:** network/framework/src/protocols/rpc/mod.rs (L515-525)
```rust
        let wait_for_response = self
            .time_service
            .timeout(timeout, response_rx)
            .map(|result| {
                // Flatten errors.
                match result {
                    Ok(Ok(response)) => Ok(Bytes::from(response.raw_response)),
                    Ok(Err(oneshot::Canceled)) => Err(RpcError::UnexpectedResponseChannelCancel),
                    Err(timeout::Elapsed) => Err(RpcError::TimedOut),
                }
            });
```

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L226-252)
```rust
    pub fn from_bytes<T: DeserializeOwned>(&self, bytes: &[u8]) -> anyhow::Result<T> {
        // Start the deserialization timer
        let deserialization_timer = start_serialization_timer(*self, DESERIALIZATION_LABEL);

        // Deserialize the message
        let result = match self.encoding() {
            Encoding::Bcs(limit) => self.bcs_decode(bytes, limit),
            Encoding::CompressedBcs(limit) => {
                let compression_client = self.get_compression_client();
                let raw_bytes = aptos_compression::decompress(
                    &bytes.to_vec(),
                    compression_client,
                    MAX_APPLICATION_MESSAGE_SIZE,
                )
                .map_err(|e| anyhow! {"{:?}", e})?;
                self.bcs_decode(&raw_bytes, limit)
            },
            Encoding::Json => serde_json::from_slice(bytes).map_err(|e| anyhow!("{:?}", e)),
        };

        // Only record the duration if deserialization was successful
        if result.is_ok() {
            deserialization_timer.observe_duration();
        }

        result
    }
```

**File:** crates/aptos-compression/src/lib.rs (L92-120)
```rust
pub fn decompress(
    compressed_data: &CompressedData,
    client: CompressionClient,
    max_size: usize,
) -> Result<Vec<u8>, Error> {
    // Start the decompression timer
    let start_time = Instant::now();

    // Check size of the data and initialize raw_data
    let decompressed_size = match get_decompressed_size(compressed_data, max_size) {
        Ok(size) => size,
        Err(error) => {
            let error_string = format!("Failed to get decompressed size: {}", error);
            return create_decompression_error(&client, error_string);
        },
    };
    let mut raw_data = vec![0u8; decompressed_size];

    // Decompress the data
    if let Err(error) = lz4::block::decompress_to_buffer(compressed_data, None, &mut raw_data) {
        let error_string = format!("Failed to decompress the data: {}", error);
        return create_decompression_error(&client, error_string);
    };

    // Stop the timer and update the metrics
    metrics::observe_decompression_operation_time(&client, start_time);
    metrics::update_decompression_metrics(&client, compressed_data, &raw_data);

    Ok(raw_data)
```

**File:** config/src/config/network_config.rs (L45-50)
```rust
pub const MAX_MESSAGE_METADATA_SIZE: usize = 128 * 1024; /* 128 KiB: a buffer for metadata that might be added to messages by networking */
pub const MESSAGE_PADDING_SIZE: usize = 2 * 1024 * 1024; /* 2 MiB: a safety buffer to allow messages to get larger during serialization */
pub const MAX_APPLICATION_MESSAGE_SIZE: usize =
    (MAX_MESSAGE_SIZE - MAX_MESSAGE_METADATA_SIZE) - MESSAGE_PADDING_SIZE; /* The message size that applications should check against */
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```
