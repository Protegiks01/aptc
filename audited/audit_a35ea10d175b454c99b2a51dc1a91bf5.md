# Audit Report

## Title
Synchronous JSON Serialization in Consensus Vote Processing Enables Byzantine Validators to Introduce Latency Through Repeated Equivocation Logging

## Summary
Byzantine validators can repeatedly send equivocating votes to trigger expensive synchronous JSON serialization in the consensus hot path. The error logging for equivocating votes performs unbounded serialization of two full `Vote` objects on the consensus message processing thread, and equivocating votes are not stored in the tracking map, allowing the same Byzantine validator to trigger the expensive log multiple times per round.

## Finding Description

When the consensus system detects an equivocating vote (same author voting for different proposals in the same round), it logs an error with both votes but returns early without updating the `author_to_vote` tracking map. [1](#0-0) 

The map update only occurs when votes are successfully added. [2](#0-1) 

This means subsequent different votes from the same Byzantine validator will always be compared against the first vote stored in the map, triggering the expensive error log repeatedly.

The error logging performs synchronous JSON serialization of both `Vote` objects through the structured logging system. The `Vote` struct implements `Serialize`. [3](#0-2) 

The logging infrastructure's `record()` method synchronously creates a `LogEntry` on the calling thread. [4](#0-3) 

Inside `LogEntry::new()`, the `JsonVisitor` performs synchronous JSON serialization via `serde_json::to_value()` for all structured log data. [5](#0-4) 

This serialization happens on the consensus message processing thread before any async operations, creating a direct performance impact on the consensus hot path.

**Attack Scenario:**
1. Byzantine validator V sends vote A for round R → stored in `author_to_vote` at first occurrence
2. V sends vote B (different ledger_info) → equivocation detected vs A, error logged, returns without updating map
3. V sends vote C (different from both A and B) → equivocation detected vs A again, error logged again
4. V repeats indefinitely, triggering expensive JSON serialization each time

With f Byzantine validators (f < n/3) each sending k different equivocating votes, this generates f×(k-1) synchronous JSON serializations processed by every honest validator.

## Impact Explanation

This qualifies as **High Severity** per the Aptos bug bounty criteria: "Validator node slowdowns." The synchronous JSON serialization accumulates latency on the consensus message processing thread. Each `Vote` object contains `VoteData`, `LedgerInfo`, `Author`, cryptographic signatures, and potentially `TwoChainTimeout` data - all requiring non-trivial serialization work.

The attack affects **all honest validators** simultaneously since equivocating votes are broadcast network-wide. With sub-second consensus round times, accumulated serialization overhead from multiple Byzantine validators can cause validators to miss round deadlines, degrading consensus liveness.

This is NOT a network-layer DoS attack (which is out of scope) but rather exploitation of a consensus protocol logic bug combined with expensive synchronous operations on the hot path.

## Likelihood Explanation

**Likelihood: High**

Byzantine validators within the f < n/3 threshold are part of the expected threat model. The attack is:
- **Easy to execute**: Simply broadcast multiple different votes for the same round
- **No rate limiting**: The equivocation detection does not prevent subsequent equivocating votes from the same author
- **No map update on equivocation**: Bug allows repeated triggering [6](#0-5) 
- **Persistent**: Can be repeated every round indefinitely
- **Amplified**: Multiple Byzantine validators can coordinate

## Recommendation

Update the `insert_vote` function to store equivocating votes in the `author_to_vote` map to prevent repeated expensive logging:

```rust
// After logging equivocation at lines 300-305
self.author_to_vote
    .insert(vote.author(), (vote.clone(), li_digest));
return VoteReceptionResult::EquivocateVote;
```

Additionally, consider:
1. Rate limiting equivocation logs per author per round
2. Moving expensive serialization off the consensus hot path
3. Using lighter-weight logging for repeated equivocations

## Proof of Concept

The vulnerability can be demonstrated by examining the code flow:
1. `process_vote_msg` receives votes [7](#0-6) 
2. Calls `process_vote` which invokes `insert_vote` [8](#0-7) 
3. Equivocation detection triggers error log but returns early [1](#0-0) 
4. Map never updated, allowing repeated triggers

A Byzantine validator can send multiple different votes per round, each triggering synchronous `serde_json::to_value()` serialization [9](#0-8)  on the consensus thread before the result is handled [10](#0-9) .

## Notes

- The vulnerability exists even with async logging enabled because `LogEntry::new()` executes synchronously before the async channel send
- Network-level rate limiting does not prevent this as votes are valid consensus messages that pass signature verification
- The core issue is the logic bug (map not updated) combined with expensive synchronous operations, not the logging itself

### Citations

**File:** consensus/src/pending_votes.rs (L298-309)
```rust
            } else {
                // we have seen a different vote for the same round
                error!(
                    SecurityEvent::ConsensusEquivocatingVote,
                    remote_peer = vote.author(),
                    vote = vote,
                    previous_vote = previously_seen_vote
                );

                return VoteReceptionResult::EquivocateVote;
            }
        }
```

**File:** consensus/src/pending_votes.rs (L315-316)
```rust
        self.author_to_vote
            .insert(vote.author(), (vote.clone(), li_digest));
```

**File:** consensus/consensus-types/src/vote.rs (L22-34)
```rust
#[derive(Deserialize, Serialize, Clone, PartialEq, Eq)]
pub struct Vote {
    /// The data of the vote.
    vote_data: VoteData,
    /// The identity of the voter.
    author: Author,
    /// LedgerInfo of a block that is going to be committed in case this vote gathers QC.
    ledger_info: LedgerInfo,
    /// Signature on the LedgerInfo along with a status on whether the signature is verified.
    signature: SignatureWithStatus,
    /// The 2-chain timeout and corresponding signature.
    two_chain_timeout: Option<(TwoChainTimeout, bls12381::Signature)>,
}
```

**File:** crates/aptos-logger/src/aptos_logger.rs (L167-188)
```rust
        impl Visitor for JsonVisitor<'_> {
            fn visit_pair(&mut self, key: Key, value: Value<'_>) {
                let v = match value {
                    Value::Debug(d) => serde_json::Value::String(
                        TruncatedLogString::from(format!("{:?}", d)).into(),
                    ),
                    Value::Display(d) => {
                        serde_json::Value::String(TruncatedLogString::from(d.to_string()).into())
                    },
                    Value::Serde(s) => match serde_json::to_value(s) {
                        Ok(value) => value,
                        Err(e) => {
                            // Log and skip the value that can't be serialized
                            eprintln!("error serializing structured log: {} for key {:?}", e, key);
                            return;
                        },
                    },
                };

                self.0.insert(key, v);
            }
        }
```

**File:** crates/aptos-logger/src/aptos_logger.rs (L572-579)
```rust
    fn record(&self, event: &Event) {
        let entry = LogEntry::new(
            event,
            ::std::thread::current().name(),
            self.enable_backtrace,
        );

        self.send_entry(entry)
```

**File:** consensus/src/round_manager.rs (L1697-1716)
```rust
    pub async fn process_vote_msg(&mut self, vote_msg: VoteMsg) -> anyhow::Result<()> {
        fail_point!("consensus::process_vote_msg", |_| {
            Err(anyhow::anyhow!("Injected error in process_vote_msg"))
        });
        // Check whether this validator is a valid recipient of the vote.
        if self
            .ensure_round_and_sync_up(
                vote_msg.vote().vote_data().proposed().round(),
                vote_msg.sync_info(),
                vote_msg.vote().author(),
            )
            .await
            .context("[RoundManager] Stop processing vote")?
        {
            self.process_vote(vote_msg.vote())
                .await
                .context("[RoundManager] Add a new vote")?;
        }
        Ok(())
    }
```

**File:** consensus/src/round_manager.rs (L1767-1769)
```rust
        let vote_reception_result = self
            .round_state
            .insert_vote(vote, &self.epoch_state.verifier);
```

**File:** consensus/src/round_manager.rs (L1774-1830)
```rust
    async fn process_vote_reception_result(
        &mut self,
        vote: &Vote,
        result: VoteReceptionResult,
    ) -> anyhow::Result<()> {
        let round = vote.vote_data().proposed().round();
        match result {
            VoteReceptionResult::NewQuorumCertificate(qc) => {
                if !vote.is_timeout() {
                    observe_block(
                        qc.certified_block().timestamp_usecs(),
                        BlockStage::QC_AGGREGATED,
                    );
                }
                QC_AGGREGATED_FROM_VOTES.inc();
                self.new_qc_aggregated(qc.clone(), vote.author())
                    .await
                    .context(format!(
                        "[RoundManager] Unable to process the created QC {:?}",
                        qc
                    ))?;
                if self.onchain_config.order_vote_enabled() {
                    // This check is already done in safety rules. As printing the "failed to broadcast order vote"
                    // in humio logs could sometimes look scary, we are doing the same check again here.
                    if let Some(last_sent_vote) = self.round_state.vote_sent() {
                        if let Some((two_chain_timeout, _)) = last_sent_vote.two_chain_timeout() {
                            if round <= two_chain_timeout.round() {
                                return Ok(());
                            }
                        }
                    }
                    // Broadcast order vote if the QC is successfully aggregated
                    // Even if broadcast order vote fails, the function will return Ok
                    if let Err(e) = self.broadcast_order_vote(vote, qc.clone()).await {
                        warn!(
                            "Failed to broadcast order vote for QC {:?}. Error: {:?}",
                            qc, e
                        );
                    } else {
                        self.broadcast_fast_shares(qc.certified_block()).await;
                    }
                }
                Ok(())
            },
            VoteReceptionResult::New2ChainTimeoutCertificate(tc) => {
                self.new_2chain_tc_aggregated(tc).await
            },
            VoteReceptionResult::EchoTimeout(_) if !self.round_state.is_timeout_sent() => {
                self.process_local_timeout(round).await
            },
            VoteReceptionResult::VoteAdded(_) => {
                PROPOSAL_VOTE_ADDED.inc();
                Ok(())
            },
            VoteReceptionResult::EchoTimeout(_) | VoteReceptionResult::DuplicateVote => Ok(()),
            e => Err(anyhow::anyhow!("{:?}", e)),
        }
```
