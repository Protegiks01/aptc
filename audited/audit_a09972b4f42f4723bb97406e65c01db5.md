# Audit Report

## Title
Silent Failure of Database Commit Operations Causing State Divergence Across Validators

## Summary
ExecutorError instances from critical database commit operations are silently dropped in the consensus pipeline, allowing validators to continue participating in consensus despite failing to persist blocks to disk. This breaks the fundamental invariant that all validators must maintain consistent state and can lead to consensus safety violations.

## Finding Description

The Aptos consensus pipeline contains a critical error handling flaw where database write failures during block commitment are silently ignored, allowing consensus to proceed as if the commit succeeded when it actually failed.

**The Error Flow:**

1. When blocks are committed, the executor calls the database writer's `commit_ledger` method which performs critical RocksDB write operations. [1](#0-0) 

2. Database write failures (disk full, I/O errors, corruption) propagate as `ExecutorResult<()>` errors through the executor. [2](#0-1) 

3. These errors flow through the pipeline builder's `commit_ledger` async function, which properly propagates them. [3](#0-2) 

4. **Critical Flaw**: In `PipelinedBlock::wait_for_commit_ledger()`, the result of the commit future is explicitly discarded with `let _ = fut.commit_ledger_fut.await;` - the error is completely dropped. [4](#0-3) 

5. The persisting phase calls this method and always returns success regardless of whether the commit actually succeeded. [5](#0-4) 

6. The buffer manager receives the success signal and updates `highest_committed_round`, believing the block was persisted when it was not. [6](#0-5) 

**Contrast with Execution Error Handling:**

Execution phase errors are properly handled with logging and early returns. [7](#0-6) 

However, commit errors receive no such treatment - they are silently dropped with no logging, no retry mechanism, and no halt signal.

**Broken Invariants:**

- **State Consistency**: "State transitions must be atomic and verifiable via Merkle proofs" - A validator claims to have committed a block but hasn't persisted it
- **Deterministic Execution**: "All validators must produce identical state roots for identical blocks" - Validators diverge on what is committed
- **Consensus Safety**: Validators voting on future blocks based on inconsistent local state

## Impact Explanation

This vulnerability represents a **HIGH severity** issue per Aptos bug bounty criteria for "significant protocol violations."

**Realistic Attack Scenario:**

1. A validator experiences disk write failure (disk full, I/O error, filesystem corruption)
2. The `commit_ledger` call fails but error is silently dropped
3. Validator continues consensus participation, voting on subsequent blocks
4. Other validators successfully commit and advance
5. The affected validator has divergent state but continues voting
6. Upon restart or state query, the validator is missing blocks it claimed to commit
7. With multiple affected validators, the network cannot reach consensus on the actual committed state

**Impact Severity:**
- **State Divergence**: Validators maintain different committed states, breaking core consensus assumptions
- **Consensus Liveness Risk**: If enough validators have divergent state, consensus can stall
- **Data Loss**: Committed blocks are lost on validator restart despite acknowledgment
- **Silent Failure**: No alerts, monitoring signals, or automatic recovery
- **State Sync Failures**: Divergent validators cannot properly sync with network

This does not require attacker action - hardware failures, disk space exhaustion, or filesystem issues naturally trigger this vulnerability in production environments.

## Likelihood Explanation

**High Likelihood** - This vulnerability can be triggered by common operational failures:

- **Disk Space Exhaustion**: Production validators running out of disk space during high transaction volumes
- **Hardware Failures**: Disk I/O errors, controller failures, storage system issues
- **Filesystem Corruption**: Power failures, kernel bugs, storage driver issues
- **Resource Limits**: OS-level file descriptor limits, inode exhaustion

These are routine operational issues in distributed systems, not rare edge cases. The validator continues functioning normally from the consensus perspective, making the issue difficult to detect until state queries reveal inconsistencies.

**Detection Difficulty**: The silent nature means operators have no immediate indication of the failure. Standard consensus monitoring shows normal operation while the validator state silently diverges.

## Recommendation

Implement proper error handling for commit operations to halt consensus and alert operators when persistence fails:

**Fix 1**: Propagate commit errors instead of dropping them in `wait_for_commit_ledger`:

```rust
// In consensus/consensus-types/src/pipelined_block.rs
pub async fn wait_for_commit_ledger(&self) -> Result<(), anyhow::Error> {
    if let Some(fut) = self.pipeline_futs() {
        fut.commit_ledger_fut.await??;  // Propagate the error
    }
    Ok(())
}
```

**Fix 2**: Handle commit errors in the persisting phase:

```rust
// In consensus/src/pipeline/persisting_phase.rs
async fn process(&self, req: PersistingRequest) -> PersistingResponse {
    let PersistingRequest { blocks, commit_ledger_info } = req;
    
    for b in &blocks {
        if let Some(tx) = b.pipeline_tx().lock().as_mut() {
            tx.commit_proof_tx.take().map(|tx| tx.send(commit_ledger_info.clone()));
        }
        // Propagate commit errors
        b.wait_for_commit_ledger().await.map_err(|e| {
            error!("Commit ledger failed: {:?}", e);
            ExecutorError::InternalError { 
                error: format!("Commit failed: {}", e) 
            }
        })?;
    }
    
    Ok(blocks.last().expect("Blocks can't be empty").round())
}
```

**Fix 3**: Handle errors in buffer manager and halt on commit failure:

```rust
// In consensus/src/pipeline/buffer_manager.rs
Some(result) = self.persisting_phase_rx.next() => {
    match result {
        Ok(round) => {
            self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
            self.highest_committed_round = round;
            self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
        }
        Err(e) => {
            error!("CRITICAL: Persisting phase failed: {:?}", e);
            counters::CONSENSUS_COMMIT_FAILURE_COUNT.inc();
            // Halt consensus participation until issue is resolved
            self.stop = true;
            // Trigger alert/monitoring
        }
    }
}
```

## Proof of Concept

**Rust Integration Test** demonstrating the vulnerability:

```rust
// Test file: consensus/src/pipeline/test_commit_error_handling.rs
use crate::pipeline::buffer_manager::BufferManager;
use aptos_executor_types::{ExecutorError, ExecutorResult};
use std::sync::Arc;

#[tokio::test]
async fn test_commit_failure_silently_ignored() {
    // Setup: Create a mock executor that fails on commit_ledger
    let mock_executor = Arc::new(FailingExecutor::new());
    
    // Create buffer manager with normal configuration
    let buffer_manager = BufferManager::new(/* params */);
    
    // Order blocks through consensus
    let blocks = create_test_blocks(3);
    buffer_manager.process_ordered_blocks(blocks).await;
    
    // Execute blocks (succeeds)
    advance_to_execution().await;
    
    // Sign blocks (succeeds)  
    advance_to_signing().await;
    
    // Persist blocks - commit_ledger FAILS but error is dropped
    advance_to_persisting().await;
    
    // VULNERABILITY: Buffer manager shows blocks as committed
    assert_eq!(buffer_manager.highest_committed_round, 3);
    
    // But executor shows blocks NOT actually committed
    let actual_committed = mock_executor.get_committed_version();
    assert_eq!(actual_committed, 0); // No blocks committed to storage!
    
    // Validator continues in divergent state
    // State queries return inconsistent data
    // Consensus continues despite failed persistence
}

struct FailingExecutor;
impl BlockExecutorTrait for FailingExecutor {
    fn commit_ledger(&self, _: LedgerInfoWithSignatures) -> ExecutorResult<()> {
        // Simulate disk write failure
        Err(ExecutorError::InternalError {
            error: "Disk write failed: No space left on device".to_string()
        })
    }
}
```

**Manual Reproduction Steps:**

1. Set up a validator node with limited disk space
2. Fill disk to near capacity during block execution
3. Monitor consensus continuing normally via RPC queries showing increasing committed rounds
4. Observe database files not growing (commits failing)
5. Restart validator node
6. Query ledger version - shows lower version than what consensus claimed
7. Node cannot sync properly due to missing blocks it claimed to have

**Notes**

This vulnerability demonstrates a critical gap between consensus-layer error handling (which properly handles execution errors) and storage-layer error handling (which silently drops commit errors). The asymmetry creates a dangerous silent failure mode where validators diverge without detection. The fix requires propagating storage errors with the same rigor as execution errors, treating commit failures as consensus-halting events that require operator intervention.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L78-112)
```rust
    fn commit_ledger(
        &self,
        version: Version,
        ledger_info_with_sigs: Option<&LedgerInfoWithSignatures>,
        chunk_opt: Option<ChunkToCommit>,
    ) -> Result<()> {
        gauged_api("commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_ledger"]);

            let old_committed_ver = self.get_and_check_commit_range(version)?;

            let mut ledger_batch = SchemaBatch::new();
            // Write down LedgerInfo if provided.
            if let Some(li) = ledger_info_with_sigs {
                self.check_and_put_ledger_info(version, li, &mut ledger_batch)?;
            }
            // Write down commit progress
            ledger_batch.put::<DbMetadataSchema>(
                &DbMetadataKey::OverallCommitProgress,
                &DbMetadataValue::Version(version),
            )?;
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;

            // Notify the pruners, invoke the indexer, and update in-memory ledger info.
            self.post_commit(old_committed_ver, version, ledger_info_with_sigs, chunk_opt)
        })
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L362-394)
```rust
    fn commit_ledger(&self, ledger_info_with_sigs: LedgerInfoWithSignatures) -> ExecutorResult<()> {
        let _timer = OTHER_TIMERS.timer_with(&["commit_ledger"]);

        let block_id = ledger_info_with_sigs.ledger_info().consensus_block_id();
        info!(
            LogSchema::new(LogEntry::BlockExecutor).block_id(block_id),
            "commit_ledger"
        );

        // Check for any potential retries
        // TODO: do we still have such retries?
        let committed_block = self.block_tree.root_block();
        if committed_block.num_persisted_transactions()?
            == ledger_info_with_sigs.ledger_info().version() + 1
        {
            return Ok(());
        }

        // Confirm the block to be committed is tracked in the tree.
        self.block_tree.get_block(block_id)?;

        fail_point!("executor::commit_blocks", |_| {
            Err(anyhow::anyhow!("Injected error in commit_blocks.").into())
        });

        let target_version = ledger_info_with_sigs.ledger_info().version();
        self.db
            .writer
            .commit_ledger(target_version, Some(&ledger_info_with_sigs), None)?;

        self.block_tree.prune(ledger_info_with_sigs.ledger_info())?;

        Ok(())
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1079-1106)
```rust
    async fn commit_ledger(
        pre_commit_fut: TaskFuture<PreCommitResult>,
        commit_proof_fut: TaskFuture<LedgerInfoWithSignatures>,
        parent_block_commit_fut: TaskFuture<CommitLedgerResult>,
        executor: Arc<dyn BlockExecutorTrait>,
        block: Arc<Block>,
    ) -> TaskResult<CommitLedgerResult> {
        let mut tracker = Tracker::start_waiting("commit_ledger", &block);
        parent_block_commit_fut.await?;
        pre_commit_fut.await?;
        let ledger_info_with_sigs = commit_proof_fut.await?;

        // it's committed as prefix
        if ledger_info_with_sigs.commit_info().id() != block.id() {
            return Ok(None);
        }

        tracker.start_working();
        let ledger_info_with_sigs_clone = ledger_info_with_sigs.clone();
        tokio::task::spawn_blocking(move || {
            executor
                .commit_ledger(ledger_info_with_sigs_clone)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(Some(ledger_info_with_sigs))
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L562-568)
```rust
    pub async fn wait_for_commit_ledger(&self) {
        // may be aborted (e.g. by reset)
        if let Some(fut) = self.pipeline_futs() {
            // this may be cancelled
            let _ = fut.commit_ledger_fut.await;
        }
    }
```

**File:** consensus/src/pipeline/persisting_phase.rs (L59-81)
```rust
    async fn process(&self, req: PersistingRequest) -> PersistingResponse {
        let PersistingRequest {
            blocks,
            commit_ledger_info,
        } = req;

        for b in &blocks {
            if let Some(tx) = b.pipeline_tx().lock().as_mut() {
                tx.commit_proof_tx
                    .take()
                    .map(|tx| tx.send(commit_ledger_info.clone()));
            }
            b.wait_for_commit_ledger().await;
        }

        let response = Ok(blocks.last().expect("Blocks can't be empty").round());
        if commit_ledger_info.ledger_info().ends_epoch() {
            self.commit_msg_tx
                .send_epoch_change(EpochChangeProof::new(vec![commit_ledger_info], false))
                .await;
        }
        response
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L617-626)
```rust
        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
```

**File:** consensus/src/pipeline/buffer_manager.rs (L968-973)
```rust
                Some(Ok(round)) = self.persisting_phase_rx.next() => {
                    // see where `need_backpressure()` is called.
                    self.pending_commit_votes = self.pending_commit_votes.split_off(&(round + 1));
                    self.highest_committed_round = round;
                    self.pending_commit_blocks = self.pending_commit_blocks.split_off(&(round + 1));
                },
```
