# Audit Report

## Title
Indexer Service Crashes on Corrupted Files Due to Missing Integrity Validation and Panic-Based Error Handling

## Summary
The indexer-grpc service reads transaction and metadata files from storage without any integrity validation (checksums, hashes). When files are corrupted, the code uses `.expect()` statements that cause panics, leading to service crashes or infinite retry loops. Critical metadata file corruption prevents the entire service from starting.

## Finding Description

The indexer-grpc system has three critical file reading paths without integrity checks:

**Path 1: Metadata File Reading** - The `get_raw_file()` function in `LocalFileStoreOperator` reads files without validation. [1](#0-0) 

The bytes are passed to `FileStoreMetadata::from_bytes()` which deserializes with `.expect()`. [2](#0-1) 

**Path 2: FileStoreReader Initialization Crash** - During service initialization, `FileStoreReader::new()` calls `get_file_store_metadata()` with `.expect()`. [3](#0-2) 

The metadata reading itself contains two `.expect()` calls for file reading and JSON deserialization. [4](#0-3) 

**Path 3: Transaction File Decompression** - Transaction files are decompressed and deserialized using `.expect()` statements. [5](#0-4) 

When spawned in a blocking task, panics are caught but cause infinite retry loops. [6](#0-5) 

The data fetcher enters an infinite loop on errors. [7](#0-6) 

**Attack Scenario:**
1. An attacker with filesystem write access (or natural disk corruption) corrupts `metadata.json`
2. On service startup, `FileStoreReader::new()` attempts to deserialize the corrupted JSON
3. The `.expect("Metadata JSON is invalid.")` triggers a panic
4. The entire indexer service crashes and cannot restart
5. All downstream consumers (wallets, explorers, dApps) lose access to historical transaction data

## Impact Explanation

This qualifies as **HIGH severity** per Aptos bug bounty criteria for "API crashes" - the indexer gRPC API becomes completely unavailable. The impact varies by corruption type:

- **Metadata file corruption**: Complete service crash on startup (cannot serve any requests)
- **Batch metadata corruption**: Service crash when attempting to serve specific version ranges
- **Transaction file corruption**: Infinite retry loop causing resource exhaustion and DoS for affected versions

The indexer is critical infrastructure for the Aptos ecosystem - wallets, block explorers, analytics platforms, and dApps depend on it for transaction history queries. A crash breaks availability guarantees.

## Likelihood Explanation

**Moderate to High Likelihood:**

1. **Natural Corruption** (High probability over time):
   - Disk hardware failures
   - Network transmission errors for cloud storage (GCS)
   - Power failures during file writes
   - Filesystem bugs or corruption

2. **Malicious Corruption** (Low probability, high impact):
   - Requires compromising the file storage backend
   - However, if achieved, causes immediate and persistent DoS

The lack of any integrity validation (checksums, hashes) means the system cannot detect corruption before attempting to use the data. This is a defense-in-depth failure - systems handling persistent data should always validate integrity.

## Recommendation

Implement integrity validation for all file operations:

```rust
// Add checksums to file metadata
pub struct FileWithChecksum {
    data: Vec<u8>,
    checksum: [u8; 32], // SHA-256 hash
}

// Validate on write
async fn write_file_with_checksum(path: PathBuf, data: Vec<u8>) -> Result<()> {
    let checksum = sha256(&data);
    let wrapper = FileWithChecksum { data, checksum };
    tokio::fs::write(path, serde_json::to_vec(&wrapper)?).await?;
    Ok(())
}

// Validate on read - use Result instead of expect()
async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
    let file_entry_key = FileEntry::build_key(version, self.storage_format).to_string();
    let file_path = self.path.join(file_entry_key);
    
    let file = tokio::fs::read(&file_path).await.context("Failed to read file")?;
    
    let wrapper: FileWithChecksum = serde_json::from_slice(&file)
        .context("Failed to deserialize file wrapper")?;
    
    let computed_checksum = sha256(&wrapper.data);
    if computed_checksum != wrapper.checksum {
        anyhow::bail!("Checksum mismatch for file {:?}", file_path);
    }
    
    Ok(wrapper.data)
}

// Replace all .expect() with proper error handling
pub async fn get_file_store_metadata(&self) -> Result<Option<FileStoreMetadata>> {
    match self.reader.get_raw_file(PathBuf::from(METADATA_FILE_NAME)).await {
        Ok(Some(data)) => {
            serde_json::from_slice(&data)
                .context("Failed to deserialize metadata")
                .map(Some)
        },
        Ok(None) => Ok(None),
        Err(e) => Err(e),
    }
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_corrupted_metadata_causes_crash() {
    use std::path::PathBuf;
    use tempfile::TempDir;
    
    // Create temporary directory with corrupted metadata
    let temp_dir = TempDir::new().unwrap();
    let metadata_path = temp_dir.path().join("metadata.json");
    
    // Write corrupted JSON
    tokio::fs::write(&metadata_path, b"{ corrupted json {{{").await.unwrap();
    
    // Attempt to create LocalFileStoreOperator
    let operator = LocalFileStoreOperator::new(
        PathBuf::from(temp_dir.path()),
        true
    );
    
    // This will panic with "Metadata JSON is invalid."
    let result = std::panic::catch_unwind(|| {
        futures::executor::block_on(async {
            operator.get_file_store_metadata().await
        })
    });
    
    assert!(result.is_err(), "Expected panic due to corrupted metadata");
}

#[tokio::test]
async fn test_corrupted_transaction_file_infinite_loop() {
    // Create corrupted LZ4 file
    let corrupted_data = vec![0xFF; 1000]; // Invalid LZ4 data
    
    // Write to storage location
    let file_path = "compressed_files/lz4/test_0.bin";
    tokio::fs::write(file_path, corrupted_data).await.unwrap();
    
    // Attempt to read - will enter infinite retry loop
    // This test would need timeout to prevent hanging
    tokio::time::timeout(
        Duration::from_secs(5),
        operator.get_transactions(0, 3)
    ).await.expect_err("Should timeout due to infinite retry loop");
}
```

**Notes:**
- The vulnerability affects indexer availability but does not impact core blockchain consensus or validator operations
- While exploitation requires filesystem write access, natural corruption scenarios are realistic
- The absence of integrity checks violates defense-in-depth principles for production systems handling persistent data

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L58-73)
```rust
    async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
        let file_entry_key = FileEntry::build_key(version, self.storage_format).to_string();
        let file_path = self.path.join(file_entry_key);
        match tokio::fs::read(file_path).await {
            Ok(file) => Ok(file),
            Err(err) => {
                if err.kind() == std::io::ErrorKind::NotFound {
                    anyhow::bail!("[Indexer File] Transactions file not found. Gap might happen between cache and file store. {}", err)
                } else {
                    anyhow::bail!(
                        "[Indexer File] Error happens when transaction file. {}",
                        err
                    );
                }
            },
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L58-61)
```rust
    pub fn from_bytes(bytes: Vec<u8>) -> Self {
        serde_json::from_slice(bytes.as_slice())
            .expect("FileStoreMetadata json deserialization failed.")
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L262-292)
```rust
    pub fn into_transactions_in_storage(self) -> TransactionsInStorage {
        match self {
            FileEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                TransactionsInStorage::decode(decompressed.as_slice())
                    .expect("proto deserialization failed.")
            },
            FileEntry::JsonBase64UncompressedProto(bytes) => {
                let file: TransactionsLegacyFile =
                    serde_json::from_slice(bytes.as_slice()).expect("json deserialization failed.");
                let transactions = file
                    .transactions_in_base64
                    .into_iter()
                    .map(|base64| {
                        let bytes: Vec<u8> =
                            base64::decode(base64).expect("base64 decoding failed.");
                        Transaction::decode(bytes.as_slice())
                            .expect("proto deserialization failed.")
                    })
                    .collect::<Vec<Transaction>>();
                TransactionsInStorage {
                    starting_version: Some(file.starting_version),
                    transactions,
                }
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L43-45)
```rust
        let metadata = Self::get_file_store_metadata(&myself)
            .await
            .expect("Failed to fetch num_transactions_per_folder.");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L160-166)
```rust
    pub async fn get_file_store_metadata(&self) -> Option<FileStoreMetadata> {
        self.reader
            .get_raw_file(PathBuf::from(METADATA_FILE_NAME))
            .await
            .expect("Failed to get file store metadata.")
            .map(|data| serde_json::from_slice(&data).expect("Metadata JSON is invalid."))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/mod.rs (L70-74)
```rust
        let transactions_in_storage = tokio::task::spawn_blocking(move || {
            FileEntry::new(bytes, storage_format).into_transactions_in_storage()
        })
        .await
        .context("Converting storage bytes to FileEntry transactions thread panicked")?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L468-487)
```rust
    loop {
        // 1. Fetch data from cache and file store.
        let transaction_data = match get_data_with_tasks(
            current_version,
            transactions_count,
            chain_id,
            &mut cache_operator,
            file_store_operator.clone(),
            request_metadata.clone(),
            cache_storage_format,
            in_memory_cache.clone(),
        )
        .await
        {
            DataFetchSubTaskResult::BatchSuccess(txns) => txns,
            DataFetchSubTaskResult::Success(_) => {
                unreachable!("Fetching from multiple tasks will never return a single vector")
            },
            DataFetchSubTaskResult::NoResults => continue,
        };
```
