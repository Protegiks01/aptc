# Audit Report

## Title
Task Cancellation Causes Permanent Counter Corruption Leading to Node Liveness Failure

## Summary
The `handle_committed_transactions` async function in `utils.rs` is not cancellation-safe. When the spawned `commit_post_processor` task is cancelled during notification handling, or when storage read operations fail, the `pending_data_chunks` counter is never decremented. This leaves the counter permanently inflated, causing the driver to enter an infinite busy-wait loop that prevents consensus sync completion and requires a full node restart.

## Finding Description
The state-sync driver uses a 4-stage pipeline to process transaction chunks with a shared atomic counter `pending_data_chunks` to track in-flight work. The counter is incremented when chunks enter the pipeline and must be decremented when processing completes. [1](#0-0) 

The vulnerability occurs in the `spawn_commit_post_processor` function where `handle_committed_transactions` is called: [2](#0-1) 

The decrement occurs at line 818, **after** the async call completes. However, `handle_committed_transactions` has multiple cancellation points and early-return paths: [3](#0-2) 

**Critical Issues:**

1. **Early Returns on Storage Errors (lines 344, 351)**: If `fetch_pre_committed_version` or `fetch_latest_synced_ledger_info` fail, the function returns without any error handling. Since the decrement happens in the caller after the await, these early returns cause the counter to never be decremented.

2. **Task Cancellation During Await (line 356-365)**: If the spawned task is cancelled while awaiting `handle_transaction_notification` (e.g., during node shutdown via `JoinHandle::abort()`), the decrement at line 818 never executes.

3. **Partial Notification Delivery**: The notification handler has three sequential await points: [4](#0-3) 

If cancelled between these, some subsystems receive notifications while others don't, creating state inconsistency.

**Infinite Loop Consequence:**

The inflated counter causes `pending_storage_data()` to return true forever: [5](#0-4) 

This triggers an infinite busy-wait loop in the driver's sync request completion check: [6](#0-5) 

Once in this state, the node cannot complete consensus sync requests and becomes permanently stuck until restarted.

## Impact Explanation
This is a **High Severity** vulnerability per the Aptos bug bounty criteria:

1. **Validator Node Slowdowns/Liveness Loss**: The infinite loop prevents the node from transitioning from state sync back to consensus, effectively causing a liveness failure for that validator.

2. **Requires Node Restart**: There is no recovery mechanism. The only solution is to restart the node, which causes:
   - Temporary validator unavailability
   - Potential consensus degradation if multiple validators are affected
   - Operational overhead and potential SLA violations

3. **State Inconsistency**: Partial notification delivery leaves mempool, event subscription service, and storage service in inconsistent states where they have different views of committed transactions.

4. **Realistic Trigger Conditions**:
   - Normal node shutdown (task cancellation)
   - Storage read failures (disk I/O errors, corruption)
   - Task aborts during system stress

## Likelihood Explanation
**High Likelihood** - This vulnerability will occur in production:

1. **Node Shutdown**: Every graceful or forced node shutdown cancels spawned tasks. If shutdown occurs while the commit_post_processor is handling notifications, the counter corruption occurs.

2. **Storage Errors**: The early returns on storage read failures (lines 344, 351) are triggered by any transient storage issue, database lock contention, or I/O error.

3. **No Error Detection**: The system has no mechanism to detect or recover from counter corruption. The test suite even explicitly aborts tasks: [7](#0-6) 

However, the test expects error handling that doesn't exist for the specific cancellation scenario.

## Recommendation

**Solution 1: Use RAII Guard Pattern for Counter Management**

Wrap the counter decrement in a Drop guard that executes even on cancellation:

```rust
struct PendingDataGuard {
    pending_data_chunks: Arc<AtomicU64>,
}

impl Drop for PendingDataGuard {
    fn drop(&mut self) {
        decrement_pending_data_chunks(self.pending_data_chunks.clone());
    }
}

// In spawn_commit_post_processor:
let commit_post_processor = async move {
    while let Some(notification) = commit_post_processor_listener.next().await {
        let _guard = PendingDataGuard { 
            pending_data_chunks: pending_data_chunks.clone() 
        };
        
        // ... existing processing ...
        utils::handle_committed_transactions(/* ... */).await;
        // Guard automatically decrements on drop, even if cancelled
    }
};
```

**Solution 2: Fix Early Returns in `handle_committed_transactions`**

Remove early returns and always complete notification handling:

```rust
pub async fn handle_committed_transactions<...>(
    committed_transactions: CommittedTransactions,
    storage: Arc<dyn DbReader>,
    mempool_notification_handler: MempoolNotificationHandler<M>,
    event_subscription_service: Arc<Mutex<EventSubscriptionService>>,
    storage_service_notification_handler: StorageServiceNotificationHandler<S>,
) {
    // Fetch storage data - log errors but don't return early
    let storage_result = fetch_pre_committed_version(storage.clone())
        .and_then(|v| fetch_latest_synced_ledger_info(storage.clone()).map(|l| (v, l)));
    
    let (latest_synced_version, latest_synced_ledger_info) = match storage_result {
        Ok(result) => result,
        Err(error) => {
            error!(LogSchema::new(LogEntry::SynchronizerNotification)
                .error(&error)
                .message("Failed to fetch storage state - cannot notify subsystems!"));
            return; // This is still problematic - need guard pattern
        }
    };
    
    // Always attempt notifications
    if let Err(error) = CommitNotification::handle_transaction_notification(
        committed_transactions.events,
        committed_transactions.transactions,
        latest_synced_version,
        latest_synced_ledger_info,
        mempool_notification_handler,
        event_subscription_service,
        storage_service_notification_handler,
    ).await {
        error!(LogSchema::new(LogEntry::SynchronizerNotification)
            .error(&error)
            .message("Failed to handle transaction commit notification!"));
    }
}
```

**Solution 3: Add Counter Validation and Recovery**

Implement periodic validation to detect and recover from counter corruption:

```rust
// Add to DriverConfiguration
pub fn validate_pending_data_counter(&self) -> bool {
    let pending = self.storage_synchronizer.pending_storage_data();
    let actual_pending = /* check actual pending work */;
    if pending && !actual_pending {
        warn!("Detected pending_data_chunks corruption - resetting!");
        // Reset counter or trigger recovery
        return false;
    }
    true
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_commit_post_processor_cancellation_counter_corruption() {
    // Setup mock executor and storage
    let mut chunk_executor = create_mock_executor();
    chunk_executor.expect_enqueue_chunk_by_execution()
        .returning(|_, _, _| Ok(()));
    chunk_executor.expect_update_ledger().returning(|| Ok(()));
    chunk_executor.expect_commit_chunk().returning(|| {
        Ok(ChunkCommitNotification {
            subscribable_events: vec![],
            committed_transactions: vec![create_transaction()],
            reconfiguration_occurred: false,
        })
    });
    
    let mock_storage = create_mock_reader_writer_with_version(None, None, 100);
    
    // Create storage synchronizer
    let (_, _, _, _, _, mut storage_synchronizer, handles) = 
        create_storage_synchronizer(chunk_executor, mock_storage);
    
    // Submit a chunk for processing
    storage_synchronizer.execute_transactions(
        NotificationMetadata::new_for_test(1),
        create_transaction_list_with_proof(),
        create_epoch_ending_ledger_info(),
        None,
    ).await.unwrap();
    
    // Wait for chunk to reach commit_post_processor
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Verify pending data exists
    assert!(storage_synchronizer.pending_storage_data());
    
    // SIMULATE NODE SHUTDOWN: Abort commit_post_processor task
    handles.commit_post_processor.abort();
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // BUG: pending_storage_data() still returns true!
    // Counter was never decremented due to task cancellation
    assert!(storage_synchronizer.pending_storage_data());  // STUCK FOREVER
    
    // Driver would now enter infinite loop waiting for pending data to drain
    let start = Instant::now();
    let timeout = Duration::from_secs(5);
    while storage_synchronizer.pending_storage_data() {
        if start.elapsed() > timeout {
            panic!("VULNERABILITY CONFIRMED: Infinite loop - counter never decremented!");
        }
        tokio::task::yield_now().await;
    }
}
```

**Notes**

This vulnerability affects the core state-sync subsystem and will impact all Aptos nodes during shutdown scenarios or storage errors. The lack of cancellation-safe design in the async notification pipeline violates Rust's best practices for async code and creates a persistent liveness failure requiring manual intervention. The recommended fix using RAII guards ensures the counter is always decremented regardless of cancellation or early returns, providing robust recovery from all failure modes.

### Citations

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L304-321)
```rust
    /// Notifies the executor of new data chunks
    async fn notify_executor(&mut self, storage_data_chunk: StorageDataChunk) -> Result<(), Error> {
        if let Err(error) = send_and_monitor_backpressure(
            &mut self.executor_notifier,
            metrics::STORAGE_SYNCHRONIZER_EXECUTOR,
            storage_data_chunk,
        )
        .await
        {
            Err(Error::UnexpectedError(format!(
                "Failed to send storage data chunk to executor: {:?}",
                error
            )))
        } else {
            increment_pending_data_chunks(self.pending_data_chunks.clone());
            Ok(())
        }
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L408-410)
```rust
    fn pending_storage_data(&self) -> bool {
        load_pending_data_chunks(self.pending_data_chunks.clone()) > 0
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L797-824)
```rust
    let commit_post_processor = async move {
        while let Some(notification) = commit_post_processor_listener.next().await {
            // Start the commit post-process timer
            let _timer = metrics::start_timer(
                &metrics::STORAGE_SYNCHRONIZER_LATENCIES,
                metrics::STORAGE_SYNCHRONIZER_COMMIT_POST_PROCESS,
            );

            // Handle the committed transaction notification (e.g., notify mempool)
            let committed_transactions = CommittedTransactions {
                events: notification.subscribable_events,
                transactions: notification.committed_transactions,
            };
            utils::handle_committed_transactions(
                committed_transactions,
                storage.clone(),
                mempool_notification_handler.clone(),
                event_subscription_service.clone(),
                storage_service_notification_handler.clone(),
            )
            .await;
            decrement_pending_data_chunks(pending_data_chunks.clone());
        }
    };

    // Spawn the commit post-processor
    spawn(runtime, commit_post_processor)
}
```

**File:** state-sync/state-sync-driver/src/utils.rs (L325-371)
```rust
pub async fn handle_committed_transactions<
    M: MempoolNotificationSender,
    S: StorageServiceNotificationSender,
>(
    committed_transactions: CommittedTransactions,
    storage: Arc<dyn DbReader>,
    mempool_notification_handler: MempoolNotificationHandler<M>,
    event_subscription_service: Arc<Mutex<EventSubscriptionService>>,
    storage_service_notification_handler: StorageServiceNotificationHandler<S>,
) {
    // Fetch the latest synced version and ledger info from storage
    let (latest_synced_version, latest_synced_ledger_info) =
        match fetch_pre_committed_version(storage.clone()) {
            Ok(latest_synced_version) => match fetch_latest_synced_ledger_info(storage.clone()) {
                Ok(latest_synced_ledger_info) => (latest_synced_version, latest_synced_ledger_info),
                Err(error) => {
                    error!(LogSchema::new(LogEntry::SynchronizerNotification)
                        .error(&error)
                        .message("Failed to fetch latest synced ledger info!"));
                    return;
                },
            },
            Err(error) => {
                error!(LogSchema::new(LogEntry::SynchronizerNotification)
                    .error(&error)
                    .message("Failed to fetch latest synced version!"));
                return;
            },
        };

    // Handle the commit notification
    if let Err(error) = CommitNotification::handle_transaction_notification(
        committed_transactions.events,
        committed_transactions.transactions,
        latest_synced_version,
        latest_synced_ledger_info,
        mempool_notification_handler,
        event_subscription_service,
        storage_service_notification_handler,
    )
    .await
    {
        error!(LogSchema::new(LogEntry::SynchronizerNotification)
            .error(&error)
            .message("Failed to handle a transaction commit notification!"));
    }
}
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L96-109)
```rust
        // Notify the storage service of the committed transactions
        storage_service_notification_handler
            .notify_storage_service_of_committed_transactions(latest_synced_version)
            .await?;

        // Notify mempool of the committed transactions
        mempool_notification_handler
            .notify_mempool_of_committed_transactions(transactions, blockchain_timestamp_usecs)
            .await?;

        // Notify the event subscription service of the events
        event_subscription_service
            .lock()
            .notify_events(latest_synced_version, events)?;
```

**File:** state-sync/state-sync-driver/src/driver.rs (L554-564)
```rust
        // The sync request has been satisfied. Wait for the storage synchronizer
        // to drain. This prevents notifying consensus prematurely.
        while self.storage_synchronizer.pending_storage_data() {
            sample!(
                SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                info!("Waiting for the storage synchronizer to handle pending data!")
            );

            // Yield to avoid starving the storage synchronizer threads.
            yield_now().await;
        }
```

**File:** state-sync/state-sync-driver/src/tests/storage_synchronizer.rs (L604-607)
```rust
    // Explicitly drop the commit post processor to cause a send error for the ledger updater
    let commit_post_processor = storage_synchronizer_handles.commit_post_processor;
    commit_post_processor.abort();

```
