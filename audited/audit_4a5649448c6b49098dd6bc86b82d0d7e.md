# Audit Report

## Title
Indexer-GRPC Cache Poisoning via Unvalidated Redis Data Leading to Service Denial

## Summary
The `CacheEntry::into_transaction()` function in the indexer-grpc service deserializes transaction data from Redis cache without validation, using `.expect()` calls that panic on malformed input. If Redis is network-accessible without authentication (as configured in the reference docker-compose setup), external actors can poison the cache with malformed data, causing the indexer data service to crash. [1](#0-0) 

## Finding Description

The indexer-grpc-data-service reads cached transaction data from Redis and deserializes it without validation. The deserialization flow exhibits three failure points that trigger panics:

1. **LZ4 decompression failure** - `Decoder::new(&bytes[..]).expect("Lz4 decompression failed.")`
2. **Base64 decoding failure** - `base64::decode(bytes).expect("base64 decoding failed.")`  
3. **Protobuf deserialization failure** - `Transaction::decode(...).expect("proto deserialization failed.")` [2](#0-1) 

The data service retrieves cached transactions via `deserialize_cached_transactions()` which directly calls the vulnerable `into_transaction()` method: [3](#0-2) 

**Attack Path:**

1. The reference Docker Compose configuration exposes Redis on port 6379 without authentication: [4](#0-3) 

2. An attacker with network access to Redis writes malformed data to cache keys (format: `"l4:{version}"` or `"{version}"` depending on storage format) [5](#0-4) 

3. When the data service attempts to serve this version to a downstream indexer, the panic occurs in the deserialization task, crashing the service worker thread

4. No transaction validation occurs after deserialization - the service checks sequential version numbers but not transaction integrity: [6](#0-5) 

**Important Scope Limitation:** This vulnerability affects the **indexer-grpc ecosystem service only**, NOT the core Aptos blockchain validators or consensus layer. The indexer-grpc serves historical transaction data to external indexers and does not participate in consensus, transaction validation, or state commitment.

## Impact Explanation

**Severity: High (API Crashes)**

This qualifies as **High Severity** under Aptos bug bounty criteria specifically for "API crashes". The vulnerability enables:

1. **Denial of Service**: Repeated crashes of the indexer-grpc-data-service API, preventing downstream indexers from consuming blockchain data
2. **Data Integrity Violation**: If malformed but parseable data is injected, downstream indexers may process fabricated transaction history

**Critical Limitations:**
- Does NOT affect blockchain consensus or validator operations
- Does NOT enable fund theft or blockchain state manipulation  
- Does NOT break any of the 10 critical Aptos invariants (consensus safety, deterministic execution, etc.)
- Impact limited to off-chain indexing infrastructure

The reference deployment documentation confirms Redis requires "at least 75 GB of memory" but does not mandate authentication: [7](#0-6) 

## Likelihood Explanation

**Likelihood: Medium**

The exploitability depends heavily on deployment configuration:

**Factors Increasing Likelihood:**
- Reference docker-compose exposes Redis publicly without authentication
- No code-level validation of cached data (violates defense-in-depth)
- Cache key format is predictable and documented

**Factors Decreasing Likelihood:**  
- Production deployments may use Kubernetes network policies restricting Redis access
- Redis URL configuration supports authentication credentials via standard redis:// URL format [8](#0-7) 
- Attack requires network proximity to Redis instance

The vulnerability is **exploitable if Redis is misconfigured** (no authentication + network exposure), which is the default in the reference setup but may not reflect production deployments.

## Recommendation

**Immediate Fix: Add Defensive Validation**

Replace panic-inducing `.expect()` calls with proper error handling that returns `Result`:

```rust
pub fn into_transaction(self) -> Result<Transaction, anyhow::Error> {
    match self {
        CacheEntry::Lz4CompressionProto(bytes) => {
            let mut decompressor = Decoder::new(&bytes[..])
                .context("Failed to create LZ4 decoder")?;
            let mut decompressed = Vec::new();
            decompressor.read_to_end(&mut decompressed)
                .context("LZ4 decompression failed")?;
            Transaction::decode(decompressed.as_slice())
                .context("Protobuf deserialization failed")
        },
        CacheEntry::Base64UncompressedProto(bytes) => {
            let bytes: Vec<u8> = base64::decode(bytes)
                .context("Base64 decoding failed")?;
            Transaction::decode(bytes.as_slice())
                .context("Protobuf deserialization failed")
        },
    }
}
```

Update callers to handle errors gracefully rather than panicking.

**Additional Mitigations:**

1. **Mandatory Redis Authentication**: Enforce `requirepass` in Redis configuration and reject connections without credentials
2. **Network Isolation**: Deploy Redis with Kubernetes ClusterIP (not LoadBalancer/NodePort) and implement network policies
3. **Transaction Integrity Checks**: Add cryptographic verification (signature validation, hash checks) for cached transactions
4. **Rate Limiting**: Implement per-key write rate limits in Redis to slow cache poisoning attempts

## Proof of Concept

```rust
use redis::Commands;

// Connect to unprotected Redis instance
let client = redis::Client::open("redis://127.0.0.1:6379/").unwrap();
let mut con = client.get_connection().unwrap();

// Poison cache with malformed LZ4 data for version 1000
let malformed_data = vec![0xFF, 0xFF, 0xFF, 0xFF]; // Invalid LZ4 stream
let cache_key = "l4:1000"; // Key format for Lz4CompressedProto

// Write poisoned data
let _: () = con.set(cache_key, malformed_data).unwrap();

// When indexer-grpc-data-service attempts to serve version 1000,
// it will panic in CacheEntry::into_transaction() at the Decoder::new() call
// Logs will show: "thread 'tokio-runtime-worker' panicked at 'Lz4 decompression failed.'"
```

**Expected Result**: The data service worker thread panics and the service becomes unavailable for serving transaction version 1000 and subsequent requests may fail depending on thread pool behavior.

## Notes

This vulnerability represents a **defense-in-depth failure** rather than a core protocol vulnerability. While the impact is real (service crashes), it is confined to the off-chain indexing infrastructure and does not threaten blockchain consensus, validator operations, or fund security. 

Production deployments should treat Redis as an untrusted data source and implement both network-level security (authentication, isolation) and application-level validation (error handling, integrity checks). The current code assumes Redis is a trusted component within a secured perimeter, which may not hold in all deployment scenarios.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L127-140)
```rust
    pub fn build_key(version: u64, storage_format: StorageFormat) -> String {
        match storage_format {
            StorageFormat::Lz4CompressedProto => {
                format!("l4:{}", version)
            },
            StorageFormat::Base64UncompressedProto => {
                format!("{}", version)
            },
            StorageFormat::JsonBase64UncompressedProto => {
                // This is fatal to see that we are using legacy file format in cache side.
                panic!("JsonBase64UncompressedProto is not supported in cache.")
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L142-157)
```rust
    pub fn into_transaction(self) -> Transaction {
        match self {
            CacheEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                Transaction::decode(decompressed.as_slice()).expect("proto deserialization failed.")
            },
            CacheEntry::Base64UncompressedProto(bytes) => {
                let bytes: Vec<u8> = base64::decode(bytes).expect("base64 decoding failed.");
                Transaction::decode(bytes.as_slice()).expect("proto deserialization failed.")
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L589-668)
```rust
/// Takes in multiple batches of transactions, and:
/// 1. De-dupes in the case of overlap (but log to prom metric)
/// 2. Panics in cases of gaps
fn ensure_sequential_transactions(mut batches: Vec<Vec<Transaction>>) -> Vec<Transaction> {
    // If there's only one, no sorting required
    if batches.len() == 1 {
        return batches.pop().unwrap();
    }

    // Sort by the first version per batch, ascending
    batches.sort_by(|a, b| a.first().unwrap().version.cmp(&b.first().unwrap().version));
    let first_version = batches.first().unwrap().first().unwrap().version;
    let last_version = batches.last().unwrap().last().unwrap().version;
    let mut transactions: Vec<Transaction> = vec![];

    let mut prev_start = None;
    let mut prev_end = None;
    for mut batch in batches {
        let mut start_version = batch.first().unwrap().version;
        let end_version = batch.last().unwrap().version;
        if let Some(prev_start) = prev_start {
            let prev_end = prev_end.unwrap();
            // If this batch is fully contained within the previous batch, skip it
            if prev_start <= start_version && prev_end >= end_version {
                NUM_MULTI_FETCH_OVERLAPPED_VERSIONS
                    .with_label_values(&[SERVICE_TYPE, "full"])
                    .inc_by(end_version - start_version);
                continue;
            }
            // If this batch overlaps with the previous batch, combine them
            if prev_end >= start_version {
                NUM_MULTI_FETCH_OVERLAPPED_VERSIONS
                    .with_label_values(&[SERVICE_TYPE, "partial"])
                    .inc_by(prev_end - start_version + 1);
                tracing::debug!(
                    batch_first_version = first_version,
                    batch_last_version = last_version,
                    start_version = start_version,
                    end_version = end_version,
                    prev_start = ?prev_start,
                    prev_end = prev_end,
                    "[Filestore] Overlapping version data"
                );
                batch.drain(0..(prev_end - start_version + 1) as usize);
                start_version = batch.first().unwrap().version;
            }

            // Otherwise there is a gap
            if prev_end + 1 != start_version {
                NUM_MULTI_FETCH_OVERLAPPED_VERSIONS
                    .with_label_values(&[SERVICE_TYPE, "gap"])
                    .inc_by(prev_end - start_version + 1);

                tracing::error!(
                    batch_first_version = first_version,
                    batch_last_version = last_version,
                    start_version = start_version,
                    end_version = end_version,
                    prev_start = ?prev_start,
                    prev_end = prev_end,
                    "[Filestore] Gaps or dupes in processing version data"
                );
                panic!("[Filestore] Gaps in processing data batch_first_version: {}, batch_last_version: {}, start_version: {}, end_version: {}, prev_start: {:?}, prev_end: {:?}",
                       first_version,
                       last_version,
                       start_version,
                       end_version,
                       prev_start,
                       prev_end,
                );
            }
        }

        prev_start = Some(start_version);
        prev_end = Some(end_version);
        transactions.extend(batch);
    }

    transactions
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L694-709)
```rust
async fn deserialize_cached_transactions(
    transactions: Vec<Vec<u8>>,
    storage_format: StorageFormat,
) -> anyhow::Result<Vec<Transaction>> {
    let task = tokio::task::spawn_blocking(move || {
        transactions
            .into_iter()
            .map(|transaction| {
                let cache_entry = CacheEntry::new(transaction, storage_format);
                cache_entry.into_transaction()
            })
            .collect::<Vec<Transaction>>()
    })
    .await;
    task.context("Transaction bytes to CacheEntry deserialization task failed")
}
```

**File:** docker/compose/indexer-grpc/docker-compose.yaml (L16-26)
```yaml
  redis:
    image: ${REDIS_IMAGE_REPO:-redis}:7.2
    command: redis-server --appendonly yes
    networks:
      shared:
        ipv4_address:  172.16.1.12
    restart: unless-stopped
    expose:
      - 6379
    ports:
      - 6379:6379
```

**File:** ecosystem/indexer-grpc/README.md (L10-15)
```markdown
### Requirements

* Redis with at least 75 GB of memory.
* If using the GCS File Operator, you also need Google Cloud Storage with one bucket and one service account JSON, which should be assigned as `Object Owner` and `Bucket Owner`.
  * `Object Owner` is to raed and write to each file.
  * `Bucket Owner` is to verify the bucket existence.
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/types.rs (L12-26)
```rust
/// A URL that only allows the redis:// scheme.
#[derive(Clone, Debug, Eq, PartialEq, Serialize)]
pub struct RedisUrl(pub Url);

impl FromStr for RedisUrl {
    type Err = anyhow::Error;

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        let url = Url::parse(s)?;
        if url.scheme() != "redis" {
            return Err(anyhow::anyhow!("Invalid scheme: {}", url.scheme()));
        }
        Ok(RedisUrl(url))
    }
}
```
