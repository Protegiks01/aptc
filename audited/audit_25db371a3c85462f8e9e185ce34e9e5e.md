# Audit Report

## Title
Unauthenticated Tokio-Console Debug Interface Exposes Real-Time Validator Runtime State Enabling Timing Attacks

## Summary
When the `tokio-console` feature is compiled and configuration lacks explicit `tokio_console_port` setting, the `ConfigOptimizer` automatically enables an unauthenticated debug interface on port 6669 bound to all network interfaces (0.0.0.0). This exposes real-time consensus task scheduling, block processing timing, and validator behavior patterns that attackers can monitor to time attacks or understand validator operations. [1](#0-0) 

## Finding Description

The vulnerability exists across three components:

**1. Automatic Enabling via ConfigOptimizer:**
The `ConfigOptimizer::optimize()` method automatically sets `tokio_console_port` to 6669 when the feature is enabled and the config YAML doesn't explicitly specify this field: [2](#0-1) 

**2. Unauthenticated Binding to All Interfaces:**
When enabled, tokio-console binds to `0.0.0.0` (all network interfaces) with no authentication mechanism: [3](#0-2) 

**3. Task Name Exposure:**
The `spawn_named` macro exposes detailed task names throughout the consensus layer when the feature is enabled: [4](#0-3) 

**Attack Scenario:**

1. Developer builds validator binary with `--features tokio-console` for debugging
2. Deploys to production with standard config that doesn't specify `tokio_console_port`
3. ConfigOptimizer automatically enables it on port 6669 during config loading
4. ConfigSanitizer passes because both feature flag and port are set (mismatch is prevented, but automatic enabling is not)
5. Attacker connects to `validator_ip:6669` without authentication
6. Tokio-console exposes:
   - Consensus task names: "DirectMempoolQuorumStore", "quorum_store_coordinator", "batch_generator", "proof_coordinator", "proof_manager", "network_listener"
   - Real-time task states (polling, idle, blocked)
   - Poll durations revealing block processing times
   - Task scheduling patterns showing consensus round timing [5](#0-4) 

**Exploitable Information:**
- Consensus round timing patterns (when validators are proposing/voting)
- Block execution durations
- Mempool transaction processing timing
- Validator task scheduling under load
- Retry patterns revealing failure conditions

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:

This qualifies as **"Validator node slowdowns"** because the exposed information enables attackers to:

1. **Timing Attacks:** Monitor real-time consensus task activity to identify when validators are processing blocks, then send malicious transactions or network messages timed to maximize resource consumption during busy periods

2. **Consensus Behavior Profiling:** Understand validator behavior patterns to predict block proposal timing, identify performance bottlenecks, and exploit consensus timing windows

3. **Targeted DoS Amplification:** Observe when validators are under load and coordinate attacks during those periods to amplify impact

4. **No Authentication Barrier:** Any network attacker with access to port 6669 can connect - no credentials, API keys, or validator access required

5. **Significant Information Disclosure:** Exposes internal consensus operations including task names, execution timing, and scheduling patterns - far beyond "minor information leaks"

The vulnerability doesn't directly cause slowdowns, but it provides the operational intelligence necessary to execute sophisticated timing-based attacks that would cause validator performance degradation.

## Likelihood Explanation

**Likelihood: Medium to High**

This is realistic in production scenarios:

1. **Development Builds in Production:** Developers commonly build with debug features enabled for troubleshooting, then accidentally deploy those binaries to production environments

2. **Automatic Enabling:** The ConfigOptimizer automatically enables tokio-console without explicit configuration, creating a "silent" activation that operators may not notice

3. **Default Config Lacks Explicit Setting:** Standard validator configs may not include `tokio_console_port` field, triggering the automatic enabling behavior

4. **Port Not in Standard Firewall Rules:** Port 6669 is not a standard validator port (consensus uses 6180, VFN uses 6181, metrics uses 9101), so it may not be explicitly blocked in firewall rules

5. **Testnet/Staging Exposure:** Highly likely on testnet validators and staging environments where firewall rules are more permissive

## Recommendation

**Immediate Fix:**

1. **Never Auto-Enable in Production:** Remove the automatic enabling logic from ConfigOptimizer. Require explicit opt-in for tokio-console:

```rust
impl ConfigOptimizer for LoggerConfig {
    fn optimize(
        node_config: &mut NodeConfig,
        local_config_yaml: &Value,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<bool, Error> {
        let logger_config = &mut node_config.logger;
        let local_logger_config_yaml = &local_config_yaml["logger"];

        // Set the tokio console port
        let mut modified_config = false;
        if local_logger_config_yaml["tokio_console_port"].is_null() {
            // FIXED: Never auto-enable tokio-console, always disable unless explicitly set
            logger_config.tokio_console_port = None;
            modified_config = true;
        }

        Ok(modified_config)
    }
}
```

2. **Bind to Localhost Only:** When enabled, bind to `127.0.0.1` instead of `0.0.0.0`:

```rust
#[cfg(feature = "tokio-console")]
{
    if let Some(tokio_console_port) = tokio_console_port {
        let console_layer = console_subscriber::ConsoleLayer::builder()
            .server_addr(([127, 0, 0, 1], tokio_console_port))  // Changed from [0, 0, 0, 0]
            .spawn();

        tracing_subscriber::registry().with(console_layer).init();
        return;
    }
}
```

3. **Add Authentication:** Implement token-based authentication for tokio-console connections (requires upstream changes to console-subscriber crate)

4. **Warn on Production Use:** Add explicit warnings when tokio-console is enabled:

```rust
if tokio_console_port.is_some() {
    error!("⚠️  WARNING: tokio-console is enabled! This exposes internal runtime state and should NEVER be enabled in production!");
}
```

## Proof of Concept

**Step 1: Build validator with tokio-console feature**
```bash
cd aptos-node
cargo build --release --features tokio-console
```

**Step 2: Create validator config WITHOUT tokio_console_port setting**
```yaml
# validator.yaml (minimal config)
base:
  role: "validator"
  
execution:
  genesis_file_location: "./genesis.blob"
  
# Note: logger.tokio_console_port is NOT specified
logger:
  level: "INFO"
```

**Step 3: Start validator**
```bash
./target/release/aptos-node -f validator.yaml
```

**Expected behavior:** 
- ConfigOptimizer automatically sets `tokio_console_port = Some(6669)`
- Tokio-console binds to `0.0.0.0:6669`
- No authentication required

**Step 4: Connect as attacker from external machine**
```bash
# Install tokio-console client
cargo install tokio-console

# Connect to validator
tokio-console http://validator_ip:6669
```

**Observable Information:**
- Real-time list of all async tasks with names like:
  - "DirectMempoolQuorumStore"
  - "quorum_store_coordinator"  
  - "batch_generator"
  - "proof_coordinator"
  - "proof_manager"
  - "consensus" runtime
  - "retry request"
- Task states (Running, Idle, Blocking)
- Poll counts and durations
- Task spawn locations (file:line)
- Resource usage metrics

**Attack Exploitation:**
```python
# Pseudocode for timing attack
import tokio_console_client

console = tokio_console_client.connect("validator_ip:6669")

while True:
    tasks = console.get_tasks()
    consensus_tasks = [t for t in tasks if "consensus" in t.name.lower()]
    
    # Wait for consensus tasks to be busy (high poll count)
    if any(t.state == "Running" and t.poll_count > threshold for t in consensus_tasks):
        # Validator is busy processing block - send malicious transaction now
        send_resource_intensive_transaction()
        time.sleep(attack_interval)
```

**Notes:**

The ConfigOptimizer runs BEFORE ConfigSanitizer in the config loading flow: [6](#0-5) 

This means the automatic enabling happens first, then the sanitizer only validates that the feature flag matches the config - it doesn't prevent the automatic enabling itself.

### Citations

**File:** config/src/config/utils.rs (L59-67)
```rust
pub fn is_tokio_console_enabled() -> bool {
    cfg_if! {
        if #[cfg(feature = "tokio-console")] {
            true
        } else {
            false
        }
    }
}
```

**File:** config/src/config/logger_config.rs (L97-122)
```rust
impl ConfigOptimizer for LoggerConfig {
    fn optimize(
        node_config: &mut NodeConfig,
        local_config_yaml: &Value,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<bool, Error> {
        let logger_config = &mut node_config.logger;
        let local_logger_config_yaml = &local_config_yaml["logger"];

        // Set the tokio console port
        let mut modified_config = false;
        if local_logger_config_yaml["tokio_console_port"].is_null() {
            // If the tokio-console feature is enabled, set the default port.
            // Otherwise, disable the tokio console port.
            if is_tokio_console_enabled() {
                logger_config.tokio_console_port = Some(DEFAULT_TOKIO_CONSOLE_PORT);
            } else {
                logger_config.tokio_console_port = None;
            }
            modified_config = true;
        }

        Ok(modified_config)
    }
}
```

**File:** crates/aptos-logger/src/logger.rs (L54-64)
```rust
    #[cfg(feature = "tokio-console")]
    {
        if let Some(tokio_console_port) = tokio_console_port {
            let console_layer = console_subscriber::ConsoleLayer::builder()
                .server_addr(([0, 0, 0, 0], tokio_console_port))
                .spawn();

            tracing_subscriber::registry().with(console_layer).init();
            return;
        }
    }
```

**File:** crates/aptos-logger/src/macros.rs (L16-48)
```rust
#[cfg(feature = "tokio-console")]
#[macro_export]
macro_rules! spawn_named {
      ($name:expr, $func:expr) => { tokio::task::Builder::new()
                                          .name($name)
                                          .spawn($func)
                                          .unwrap(); };
      ($name:expr, $handle:expr, $func:expr) => { tokio::task::Builder::new()
                                                      .name($name)
                                                      .spawn_on($func, $handle)
                                                      .unwrap(); };

      ($name:expr, $async:ident = async; $clojure:block) => { tokio::task::Builder::new()
                                                                      .name($name)
                                                                      .spawn(async $clojure)
                                                                      .unwrap(); };

      ($name:expr, $async:ident = async; $move:ident = move; $clojure:block) => { tokio::task::Builder::new()
                                                                      .name($name)
                                                                      .spawn(async move $clojure)
                                                                      .unwrap(); };

      ($name:expr, $handler:expr, $async:ident = async; $clojure:block) => { tokio::task::Builder::new()
                                                                              .name($name)
                                                                              .spawn_on(async $clojure, $handler)
                                                                              .unwrap(); };

      ($name:expr, $handler:expr, $async:ident = async; $move:ident = move; $clojure:block) => { tokio::task::Builder::new()
                                                                                                  .name($name)
                                                                                                  .spawn_on(async move $clojure, $handler)
                                                                                                  .unwrap(); };

}
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L295-393)
```rust
        spawn_named!(
            "quorum_store_coordinator",
            quorum_store_coordinator.start(coordinator_rx)
        );

        let batch_generator_cmd_rx = self.batch_generator_cmd_rx.take().unwrap();
        let back_pressure_rx = self.back_pressure_rx.take().unwrap();
        let batch_generator = BatchGenerator::new(
            self.epoch,
            self.author,
            self.config.clone(),
            self.quorum_store_storage.clone(),
            self.batch_store.clone().unwrap(),
            self.quorum_store_to_mempool_sender,
            self.mempool_txn_pull_timeout_ms,
        );
        spawn_named!(
            "batch_generator",
            batch_generator.start(
                self.network_sender.clone(),
                batch_generator_cmd_rx,
                back_pressure_rx,
                interval
            )
        );

        for (i, remote_batch_coordinator_cmd_rx) in
            self.remote_batch_coordinator_cmd_rx.into_iter().enumerate()
        {
            let batch_coordinator = BatchCoordinator::new(
                self.author,
                self.network_sender.clone(),
                self.proof_manager_cmd_tx.clone(),
                self.batch_generator_cmd_tx.clone(),
                self.batch_store.clone().unwrap(),
                self.config.receiver_max_batch_txns as u64,
                self.config.receiver_max_batch_bytes as u64,
                self.config.receiver_max_total_txns as u64,
                self.config.receiver_max_total_bytes as u64,
                self.config.batch_expiry_gap_when_init_usecs,
                self.transaction_filter_config.clone(),
            );
            #[allow(unused_variables)]
            let name = format!("batch_coordinator-{}", i);
            spawn_named!(
                name.as_str(),
                batch_coordinator.start(remote_batch_coordinator_cmd_rx)
            );
        }

        let proof_coordinator_cmd_rx = self.proof_coordinator_cmd_rx.take().unwrap();
        let proof_coordinator = ProofCoordinator::new(
            self.config.proof_timeout_ms,
            self.author,
            self.batch_reader.clone().unwrap(),
            self.batch_generator_cmd_tx.clone(),
            self.proof_cache,
            self.broadcast_proofs,
            self.config.batch_expiry_gap_when_init_usecs,
        );
        spawn_named!(
            "proof_coordinator",
            proof_coordinator.start(
                proof_coordinator_cmd_rx,
                self.network_sender.clone(),
                self.verifier.clone(),
            )
        );

        let proof_manager_cmd_rx = self.proof_manager_cmd_rx.take().unwrap();
        let proof_manager = ProofManager::new(
            self.author,
            self.config.back_pressure.backlog_txn_limit_count,
            self.config
                .back_pressure
                .backlog_per_validator_batch_limit_count
                * self.num_validators,
            self.batch_store.clone().unwrap(),
            self.config.allow_batches_without_pos_in_proposal,
            self.config.enable_payload_v2,
            self.config.batch_expiry_gap_when_init_usecs,
        );
        spawn_named!(
            "proof_manager",
            proof_manager.start(
                self.back_pressure_tx.clone(),
                self.consensus_to_quorum_store_receiver,
                proof_manager_cmd_rx,
            )
        );

        let network_msg_rx = self.quorum_store_msg_rx.take().unwrap();
        let net = NetworkListener::new(
            network_msg_rx,
            self.proof_coordinator_cmd_tx.clone(),
            self.remote_batch_coordinator_cmd_tx.clone(),
            self.proof_manager_cmd_tx.clone(),
        );
        spawn_named!("network_listener", net.start());
```

**File:** config/src/config/node_config_loader.rs (L126-145)
```rust
/// Optimize and sanitize the node config for the current environment
fn optimize_and_sanitize_node_config(
    node_config: &mut NodeConfig,
    local_config_yaml: Value,
) -> Result<(), Error> {
    // Extract the node type and chain ID from the node config
    let (node_type, chain_id) = extract_node_type_and_chain_id(node_config);

    // Print the extracted node type and chain ID
    println!(
        "Identified node type ({:?}) and chain ID ({:?}) from node config!",
        node_type, chain_id
    );

    // Optimize the node config
    NodeConfig::optimize(node_config, &local_config_yaml, node_type, chain_id)?;

    // Sanitize the node config
    NodeConfig::sanitize(node_config, node_type, chain_id)
}
```
