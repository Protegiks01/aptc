# Audit Report

## Title
Zombie Process Accumulation in CommandAdapter Backup Storage Leading to File Descriptor Exhaustion

## Summary
The backup system's `CommandAdapter` implementation spawns child processes for each file write operation but lacks proper cleanup mechanisms when errors occur. This causes zombie processes to accumulate during backup operations, eventually exhausting system file descriptors and rendering the backup system inoperable.

## Finding Description

The backup system supports multiple storage backends, including `CommandAdapter` which is used in production for cloud storage (S3, GCS, Azure). When `CommandAdapter.create_for_write()` is called, it spawns a bash child process to handle the actual file upload. [1](#0-0) 

This returns a `ChildStdinAsDataSink` wrapper that implements `AsyncWrite`: [2](#0-1) 

The critical issue is that `ChildStdinAsDataSink` **has no `Drop` implementation** to clean up the child process. When write operations fail, the backup code returns early with the `?` operator, dropping the `ChildStdinAsDataSink` without calling `shutdown()`: [3](#0-2) 

Notice the error paths:
- Line 162: If `create_for_write` fails after spawning the process
- Line 167: If `get_transaction_range_proof` fails, `proof_file` is never shutdown
- Line 170: If `tokio::io::copy` fails, `proof_file` is never shutdown  
- Line 177: If `write_all` fails, `chunk_file` is never shutdown

The same pattern exists in `write_manifest()`: [4](#0-3) 

And in state snapshot backups: [5](#0-4) 

When `ChildStdinAsDataSink` is dropped without `shutdown()`, the underlying `tokio::process::Child` is also dropped without being waited on. **Tokio's Child does NOT automatically reap processes on drop** - the codebase itself demonstrates awareness of this issue by implementing a custom `Process` wrapper in test code: [6](#0-5) 

This `Process` wrapper properly kills and waits for child processes in its `Drop` implementation. However, the production `CommandAdapter` code lacks this protection.

The problem is exacerbated by concurrent backup operations. The `BackupCoordinator` runs three backup streams concurrently: [7](#0-6) 

Additionally, state snapshot backups write chunks concurrently with up to 4 concurrent operations: [8](#0-7) 

**Attack Scenario:**
1. Node uses `CommandAdapter` for backup storage (standard production configuration)
2. Backup operations run continuously with concurrent streams
3. Transient failures occur (network issues, rate limiting, disk pressure)
4. Each failure leaves 1-2 zombie child processes (proof file + chunk file)
5. With 4 concurrent state snapshot writes, failures can leave 8 zombies rapidly
6. Zombie processes hold file descriptors (stdin, stdout, stderr = 3 FDs minimum each)
7. Over hours/days of operation, zombies accumulate to hundreds or thousands
8. System hits per-process FD limit (typically 1024-4096)
9. Backup system fails completely; node operation may be affected if limit is system-wide

## Impact Explanation

This qualifies as **Medium Severity** under Aptos bug bounty criteria:
- **"State inconsistencies requiring intervention"**: Loss of backup capability represents a critical operational failure requiring manual restart to resolve
- The backup system is essential for disaster recovery and network resilience
- Complete backup failure leaves nodes vulnerable to data loss
- Zombie process accumulation consumes system resources (FD table, process slots)
- Recovery requires manual intervention (process restart or reboot)

While this doesn't directly affect consensus or fund security, it compromises the availability and operational integrity of validator nodes by exhausting critical system resources.

## Likelihood Explanation

**High likelihood** in production environments:
- `CommandAdapter` is the standard backend for cloud storage in production
- Backups run continuously 24/7
- Transient failures are common: network blips, cloud provider rate limiting, temporary disk pressure
- Even a 1% failure rate over days of operation accumulates significant zombies
- The concurrent execution model (3 streams + 4 concurrent chunks) amplifies the accumulation rate
- No cleanup mechanism exists to reap zombie processes
- System FD limits are finite and will eventually be exhausted

## Recommendation

Implement a `Drop` trait for `ChildStdinAsDataSink` and `SpawnedCommand` that properly cleans up child processes, following the pattern used in the test code's `Process` wrapper:

```rust
impl Drop for SpawnedCommand {
    fn drop(&mut self) {
        // Attempt to reap the child process
        match self.child.try_wait() {
            Ok(Some(_)) => {
                // Process already terminated
            },
            Ok(None) => {
                // Process still running, kill it
                let _ = self.child.start_kill();
                // Note: Can't block in Drop, but start_kill() initiates termination
            },
            Err(_) => {
                // Error checking status, attempt kill anyway
                let _ = self.child.start_kill();
            },
        }
    }
}
```

For a more complete solution, use a background task to periodically reap zombie processes, or restructure the code to use RAII guards that guarantee `shutdown()` is called even on error paths (e.g., using `scopeguard` crate or manual defer patterns).

Additionally, consider adding resource limits and monitoring:
- Track active child process count and log warnings
- Implement maximum concurrent operation limits
- Add metrics for zombie process detection
- Consider using a process pool with lifecycle management

## Proof of Concept

```rust
// Integration test demonstrating zombie process accumulation
#[tokio::test]
async fn test_zombie_process_accumulation() {
    use std::process::Command;
    
    // Create a CommandAdapter configured to fail intermittently
    let config = CommandAdapterConfig {
        commands: Commands {
            create_for_write: "echo 'handle'; sleep 10".to_string(), // Long-running process
            // ... other commands
        },
        env_vars: vec![],
    };
    
    let adapter = CommandAdapter::new(config);
    let backup_handle = adapter.create_backup(&"test_backup".try_into().unwrap()).await.unwrap();
    
    // Count initial processes
    let initial_count = count_child_processes();
    
    // Simulate rapid write_chunk calls with failures
    for i in 0..50 {
        let result = async {
            let (_handle, mut file) = adapter
                .create_for_write(&backup_handle, &format!("chunk_{}", i).try_into().unwrap())
                .await?;
            
            // Simulate failure before shutdown
            if i % 2 == 0 {
                return Err(anyhow::anyhow!("Simulated write failure"));
            }
            
            file.write_all(b"data").await?;
            file.shutdown().await?;
            Ok(())
        }.await;
        
        // Failures leave zombie processes
    }
    
    // Count processes after failures
    tokio::time::sleep(Duration::from_secs(1)).await;
    let final_count = count_child_processes();
    
    // Verify zombie accumulation
    assert!(final_count > initial_count + 20, 
        "Zombie processes accumulated: initial={}, final={}", 
        initial_count, final_count);
}

fn count_child_processes() -> usize {
    // Count bash child processes
    let output = Command::new("pgrep")
        .arg("-P")
        .arg(std::process::id().to_string())
        .output()
        .unwrap();
    
    String::from_utf8_lossy(&output.stdout)
        .lines()
        .count()
}
```

## Notes

This vulnerability specifically affects production deployments using `CommandAdapter` with cloud storage backends (S3, GCS, Azure). The `LocalFs` backend is not affected as it uses `tokio::fs::File` which properly closes file descriptors on drop. The issue represents a violation of the documented invariant that "all operations must respect gas, storage, and computational limits" - in this case, system resource limits (file descriptors and process slots) are not properly managed.

### Citations

**File:** storage/backup/backup-cli/src/storage/command_adapter/mod.rs (L93-112)
```rust
    async fn create_for_write(
        &self,
        backup_handle: &BackupHandleRef,
        name: &ShellSafeName,
    ) -> Result<(FileHandle, Box<dyn AsyncWrite + Send + Unpin>)> {
        let mut child = self
            .cmd(&self.config.commands.create_for_write, vec![
                EnvVar::backup_handle(backup_handle.to_string()),
                EnvVar::file_name(name.as_ref()),
            ])
            .spawn()?;
        let mut file_handle = FileHandle::new();
        child
            .stdout()
            .read_to_string(&mut file_handle)
            .await
            .err_notes(backup_handle)?;
        file_handle.truncate(file_handle.trim_end().len());
        Ok((file_handle, Box::new(child.into_data_sink())))
    }
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/command.rs (L167-179)
```rust
pub(super) struct ChildStdinAsDataSink<'a> {
    child: Option<SpawnedCommand>,
    join_fut: Option<BoxFuture<'a, Result<()>>>,
}

impl ChildStdinAsDataSink<'_> {
    fn new(child: SpawnedCommand) -> Self {
        Self {
            child: Some(child),
            join_fut: None,
        }
    }
}
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/backup.rs (L149-187)
```rust
    async fn write_chunk(
        &self,
        backup_handle: &BackupHandleRef,
        chunk_bytes: &[u8],
        first_version: u64,
        last_version: u64,
    ) -> Result<TransactionChunk> {
        let (proof_handle, mut proof_file) = self
            .storage
            .create_for_write(
                backup_handle,
                &Self::chunk_proof_name(first_version, last_version),
            )
            .await?;
        tokio::io::copy(
            &mut self
                .client
                .get_transaction_range_proof(first_version, last_version)
                .await?,
            &mut proof_file,
        )
        .await?;
        proof_file.shutdown().await?;

        let (chunk_handle, mut chunk_file) = self
            .storage
            .create_for_write(backup_handle, &Self::chunk_name(first_version))
            .await?;
        chunk_file.write_all(chunk_bytes).await?;
        chunk_file.shutdown().await?;

        Ok(TransactionChunk {
            first_version,
            last_version,
            transactions: chunk_handle,
            proof: proof_handle,
            format: TransactionChunkFormat::V1,
        })
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/backup.rs (L189-218)
```rust
    async fn write_manifest(
        &self,
        backup_handle: &BackupHandleRef,
        first_version: Version,
        last_version: Version,
        chunks: Vec<TransactionChunk>,
    ) -> Result<FileHandle> {
        let manifest = TransactionBackup {
            first_version,
            last_version,
            chunks,
        };
        let (manifest_handle, mut manifest_file) = self
            .storage
            .create_for_write(backup_handle, Self::manifest_name())
            .await?;
        manifest_file
            .write_all(&serde_json::to_vec(&manifest)?)
            .await?;
        manifest_file.shutdown().await?;

        let metadata =
            Metadata::new_transaction_backup(first_version, last_version, manifest_handle.clone());
        self.storage
            .save_metadata_line(&metadata.name(), &metadata.to_text_line()?)
            .await?;

        Ok(manifest_handle)
    }
}
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L253-254)
```rust
        let chunks: Vec<_> = chunk_manifest_fut_stream
            .try_buffered_x(8, 4) // 4 concurrently, at most 8 results in buffer.
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L404-447)
```rust
    async fn write_chunk(
        &self,
        backup_handle: &BackupHandleRef,
        chunk: Chunk,
    ) -> Result<StateSnapshotChunk> {
        let _timer = BACKUP_TIMER.timer_with(&["state_snapshot_write_chunk"]);

        let Chunk {
            bytes,
            first_idx,
            last_idx,
            first_key,
            last_key,
        } = chunk;

        let (chunk_handle, mut chunk_file) = self
            .storage
            .create_for_write(backup_handle, &Self::chunk_name(first_idx))
            .await?;
        chunk_file.write_all(&bytes).await?;
        chunk_file.shutdown().await?;
        let (proof_handle, mut proof_file) = self
            .storage
            .create_for_write(backup_handle, &Self::chunk_proof_name(first_idx, last_idx))
            .await?;
        tokio::io::copy(
            &mut self
                .client
                .get_account_range_proof(last_key, self.version())
                .await?,
            &mut proof_file,
        )
        .await?;
        proof_file.shutdown().await?;

        Ok(StateSnapshotChunk {
            first_idx,
            last_idx,
            first_key,
            last_key,
            blobs: chunk_handle,
            proof: proof_handle,
        })
    }
```

**File:** testsuite/forge/src/backend/local/node.rs (L29-46)
```rust
#[derive(Debug)]
struct Process(Child);

impl Drop for Process {
    // When the Process struct goes out of scope we need to kill the child process
    fn drop(&mut self) {
        // check if the process has already been terminated
        match self.0.try_wait() {
            // The child process has already terminated, perhaps due to a crash
            Ok(Some(_)) => {},

            // The process is still running so we need to attempt to kill it
            _ => {
                self.0.kill().expect("Process wasn't running");
                self.0.wait().unwrap();
            },
        }
    }
```

**File:** storage/backup/backup-cli/src/coordinators/backup.rs (L161-167)
```rust
        info!("Backup coordinator started.");
        let mut all_work = stream::select_all(vec![
            watch_db_state,
            backup_epoch_endings,
            backup_state_snapshots,
            backup_transactions,
        ]);
```
