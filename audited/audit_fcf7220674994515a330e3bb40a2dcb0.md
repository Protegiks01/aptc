# Audit Report

## Title
Database Truncation Silently Fails with Mismatched Sharding Configuration Leading to State Inconsistency

## Summary
The database truncation tool (`db_debugger/truncate`) does not validate that the sharding configuration used to open the database matches the configuration used when the database was created. This allows truncation to silently skip shards when opened with incorrect configuration, leaving data that should be deleted and causing state inconsistencies.

## Finding Description

The database truncation functionality in [1](#0-0)  uses a user-provided sharding configuration to open the database. However, there is no validation that this configuration matches the actual sharding configuration used when the database was originally created.

The core issue occurs in the truncation flow:

1. The `run()` function opens databases using `self.sharding_config.enable_storage_sharding` from CLI parameters [2](#0-1) 

2. When `StateKvDb::new()` is called with `enable_storage_sharding=false` on a database created with sharding enabled, it returns early and points all 16 shard references to the `ledger_db` [3](#0-2) 

3. The truncation helpers use `hack_num_real_shards()` to determine how many shards to process [4](#0-3) 

4. When sharding is disabled, `hack_num_real_shards()` returns 1 instead of 16 [5](#0-4) 

5. As a result, only the `ledger_db` is truncated while the 16 actual physical shard directories containing the real data are never opened or processed

The test at line 356 does not catch this vulnerability because it always uses the same sharding configuration for both database creation and truncation [6](#0-5) 

There is no metadata stored or checked to validate the original sharding configuration [7](#0-6) 

**Attack Scenario:**

1. Node operator creates database with `--enable-storage-sharding=true`
2. 16 separate shard directories are created with state data distributed across them
3. Database accumulates data from version 0 to 1000
4. Operator decides to truncate to version 500
5. Operator runs: `aptos-db-tool truncate --db-dir /path/to/db --target-version 500 --opt-out-backup-checkpoint`
6. Operator forgets to specify `--enable-storage-sharding=true` flag
7. Truncation opens database with sharding=false
8. Only `ledger_db` is truncated (which contains minimal state data in a sharded setup)
9. All 16 shard directories still contain data for versions 501-1000
10. Command completes successfully with no error
11. Database is now in inconsistent state with old data remaining

## Impact Explanation

This vulnerability constitutes a **High Severity** issue per Aptos bug bounty criteria for the following reasons:

1. **Significant Protocol Violation**: The truncation operation's core guarantee—that all data after `target_version` is removed—is violated. This breaks the "State Consistency" invariant that state operations must be atomic.

2. **State Inconsistencies Requiring Intervention**: If the database is later opened with the correct sharding configuration, the old data becomes visible again, creating an inconsistent state that requires manual intervention to fix.

3. **Silent Failure**: The operation completes successfully without any error or warning, making the issue difficult to detect until the inconsistency manifests.

4. **Potential Consensus Impact**: If this occurs on a validator node, the inconsistent state could lead to divergent behavior from other validators, potentially causing consensus issues or serving incorrect state proofs to light clients.

5. **Data Exposure**: Data that should have been deleted for compliance or operational reasons remains accessible, potentially exposing sensitive information.

## Likelihood Explanation

This vulnerability has **HIGH** likelihood of occurrence:

1. **Easy to Trigger**: Requires only a missing CLI flag, which is a common operator error
2. **No Documentation Warning**: The CLI help does not warn about matching sharding configuration
3. **No Runtime Validation**: The system performs no checks to detect the mismatch
4. **Production Usage**: The truncate command is a legitimate operational tool used for database maintenance
5. **Default Behavior**: If the flag is omitted, it defaults to the unsafe value (false) rather than detecting the database's actual configuration

The combination of ease of triggering and silent failure makes this highly likely to occur in production environments.

## Recommendation

Implement sharding configuration validation by:

1. **Store sharding metadata**: When a database is created, store the `enable_storage_sharding` configuration in `DbMetadataSchema`:

```rust
// In db_metadata/mod.rs
pub enum DbMetadataKey {
    // ... existing keys ...
    ShardingConfiguration,
}

pub enum DbMetadataValue {
    // ... existing variants ...
    ShardingEnabled(bool),
}
```

2. **Validate on open**: In `StateKvDb::new()` and `StateMerkleDb::new()`, check that the provided configuration matches the stored configuration:

```rust
// In state_kv_db.rs, modify StateKvDb::new()
pub(crate) fn new(
    db_paths: &StorageDirPaths,
    rocksdb_configs: RocksdbConfigs,
    env: Option<&Env>,
    block_cache: Option<&Cache>,
    readonly: bool,
    ledger_db: Arc<DB>,
) -> Result<Self> {
    let requested_sharding = rocksdb_configs.enable_storage_sharding;
    
    // Read stored sharding configuration
    let stored_sharding = ledger_db
        .get::<DbMetadataSchema>(&DbMetadataKey::ShardingConfiguration)?
        .map(|v| v.expect_sharding_enabled());
    
    if let Some(stored) = stored_sharding {
        ensure!(
            stored == requested_sharding,
            "Sharding configuration mismatch: database was created with sharding={}, but opened with sharding={}",
            stored,
            requested_sharding
        );
    }
    
    // Rest of existing code...
}
```

3. **Store configuration on first write**: When creating a new database, store the sharding configuration in metadata.

This ensures that any mismatch between the database's actual configuration and the requested configuration is immediately detected and reported as an error, preventing silent data corruption.

## Proof of Concept

```rust
#[test]
fn test_truncation_with_sharding_mismatch() {
    use aptos_config::config::DEFAULT_MAX_NUM_NODES_PER_LRU_CACHE_SHARD;
    use aptos_temppath::TempPath;
    
    let tmp_dir = TempPath::new();
    
    // Create database WITH sharding
    let db = AptosDB::new_for_test_with_sharding(&tmp_dir, DEFAULT_MAX_NUM_NODES_PER_LRU_CACHE_SHARD);
    
    // Write transactions to version 100
    for version in 0..100 {
        let txns_to_commit = vec![/* test transactions */];
        db.save_transactions_for_test(&txns_to_commit, version, None, true).unwrap();
    }
    
    drop(db);
    
    // Run truncation WITHOUT sharding (wrong configuration)
    let cmd = Cmd {
        db_dir: tmp_dir.path().to_path_buf(),
        target_version: 50,
        ledger_db_batch_size: 15,
        opt_out_backup_checkpoint: true,
        backup_checkpoint_dir: None,
        sharding_config: ShardingConfig { enable_storage_sharding: false }, // WRONG!
    };
    
    cmd.run().unwrap(); // Silently succeeds
    
    // Re-open database WITH sharding (correct configuration)
    let db = AptosDB::new_for_test_with_sharding(&tmp_dir, DEFAULT_MAX_NUM_NODES_PER_LRU_CACHE_SHARD);
    
    // Verify that old data still exists in shards (BUG!)
    for shard_id in 0..NUM_STATE_SHARDS {
        let shard_db = db.state_kv_db().db_shard(shard_id);
        let mut iter = shard_db.iter::<StateValueByKeyHashSchema>().unwrap();
        iter.seek_to_first();
        
        for item in iter {
            let ((_, version), _) = item.unwrap();
            // This assertion should fail because versions > 50 still exist
            assert!(version <= 50, "Found version {} in shard {} after truncation to 50", version, shard_id);
        }
    }
}
```

This test demonstrates that when truncation is run with the wrong sharding configuration, data after the target version remains in the physical shards, violating the truncation guarantee.

## Notes

This vulnerability affects both directions of mismatch:
- Database created WITH sharding, opened WITHOUT sharding: Skips all 16 shards
- Database created WITHOUT sharding, opened WITH sharding: Processes 16 empty shards, skips actual data in ledger_db

The issue is particularly insidious because the `hack_num_real_shards()` function name suggests it's a temporary workaround ("hack" prefix), yet it's used in production-critical truncation code where correctness is essential.

### Citations

**File:** storage/aptosdb/src/db_debugger/truncate/mod.rs (L48-163)
```rust
    pub fn run(self) -> Result<()> {
        if !self.opt_out_backup_checkpoint {
            let backup_checkpoint_dir = self.backup_checkpoint_dir.unwrap();
            ensure!(
                !backup_checkpoint_dir.exists(),
                "Backup dir already exists."
            );
            println!("Creating backup at: {:?}", &backup_checkpoint_dir);
            fs::create_dir_all(&backup_checkpoint_dir)?;
            AptosDB::create_checkpoint(
                &self.db_dir,
                backup_checkpoint_dir,
                self.sharding_config.enable_storage_sharding,
            )?;
            println!("Done!");
        } else {
            println!("Opted out backup creation!.");
        }

        let rocksdb_config = RocksdbConfigs {
            enable_storage_sharding: self.sharding_config.enable_storage_sharding,
            ..Default::default()
        };
        let env = None;
        let block_cache = None;
        // TODO(HotState): handle hot state merkle db.
        let (ledger_db, hot_state_merkle_db, state_merkle_db, state_kv_db) = AptosDB::open_dbs(
            &StorageDirPaths::from_path(&self.db_dir),
            rocksdb_config,
            env,
            block_cache,
            /*readonly=*/ false,
            /*max_num_nodes_per_lru_cache_shard=*/ 0,
            /*reset_hot_state=*/ true,
        )?;

        let ledger_db = Arc::new(ledger_db);
        let hot_state_merkle_db = hot_state_merkle_db.map(Arc::new);
        let state_merkle_db = Arc::new(state_merkle_db);
        let state_kv_db = Arc::new(state_kv_db);
        let overall_version = ledger_db
            .metadata_db()
            .get_synced_version()
            .expect("DB read failed.")
            .expect("Overall commit progress must exist.");
        let ledger_db_version = ledger_db
            .metadata_db()
            .get_ledger_commit_progress()
            .expect("Current version of ledger db must exist.");
        let state_kv_db_version = get_state_kv_commit_progress(&state_kv_db)?
            .expect("Current version of state kv db must exist.");
        let state_merkle_db_version = get_current_version_in_state_merkle_db(&state_merkle_db)?
            .expect("Current version of state merkle db must exist.");

        let mut target_version = self.target_version;

        assert_le!(overall_version, ledger_db_version);
        assert_le!(overall_version, state_kv_db_version);
        assert_le!(state_merkle_db_version, overall_version);
        assert_le!(target_version, overall_version);

        println!(
            "overall_version: {}, ledger_db_version: {}, state_kv_db_version: {}, state_merkle_db_version: {}, target_version: {}",
            overall_version, ledger_db_version, state_kv_db_version, state_merkle_db_version, target_version,
        );

        if ledger_db.metadata_db().get_usage(target_version).is_err() {
            println!(
                "Unable to truncate to version {}, since there is no VersionData on that version.",
                target_version
            );
            println!(
                "Trying to fallback to the largest valid version before version {}.",
                target_version,
            );
            target_version = ledger_db
                .metadata_db()
                .get_usage_before_or_at(target_version)?
                .0;
        }

        println!("Starting db truncation...");
        let mut batch = SchemaBatch::new();
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::OverallCommitProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        ledger_db.metadata_db().write_schemas(batch)?;

        StateStore::sync_commit_progress(
            Arc::clone(&ledger_db),
            Arc::clone(&state_kv_db),
            Arc::clone(&state_merkle_db),
            /*crash_if_difference_is_too_large=*/ false,
        );
        println!("Done!");

        if let Some(state_merkle_db_version) =
            get_current_version_in_state_merkle_db(&state_merkle_db)?
        {
            if state_merkle_db_version < target_version {
                println!(
                    "Trying to catch up state merkle db, by replaying write set in ledger db."
                );
                let version = StateStore::catch_up_state_merkle_db(
                    Arc::clone(&ledger_db),
                    hot_state_merkle_db,
                    Arc::clone(&state_merkle_db),
                    Arc::clone(&state_kv_db),
                )?;
                println!("Done! current_version: {:?}", version);
            }
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/db_debugger/truncate/mod.rs (L354-391)
```rust
            if sharding_config.enable_storage_sharding {
                let state_merkle_db = Arc::new(state_merkle_db);
                for i in 0..NUM_STATE_SHARDS {
                    let mut kv_shard_iter = state_kv_db.db_shard(i).iter::<StateValueByKeyHashSchema>().unwrap();
                    kv_shard_iter.seek_to_first();
                    for item in kv_shard_iter {
                        let ((_, version), _) = item.unwrap();
                        prop_assert!(version <= target_version);
                    }

                    let value_index_shard_iter = state_kv_db.db_shard(i).iter::<StaleStateValueIndexByKeyHashSchema>().unwrap();
                    for item in value_index_shard_iter {
                        let version = item.unwrap().0.stale_since_version;
                        prop_assert!(version <= target_version);
                    }

                    let mut stale_node_ind_iter = state_merkle_db.db_shard(i).iter::<StaleNodeIndexSchema>().unwrap();
                    stale_node_ind_iter.seek_to_first();
                    for item in stale_node_ind_iter {
                        let version = item.unwrap().0.stale_since_version;
                        prop_assert!(version <= target_version);
                    }

                    let mut jelly_iter = state_merkle_db.db_shard(i).iter::<JellyfishMerkleNodeSchema>().unwrap();
                    jelly_iter.seek_to_first();
                    for item in jelly_iter {
                        let version = item.unwrap().0.version();
                        prop_assert!(version <= target_version);
                    }

                    let mut cross_iter = state_merkle_db.db_shard(i).iter::<StaleNodeIndexCrossEpochSchema>().unwrap();
                    cross_iter.seek_to_first();
                    for item in cross_iter {
                        let version = item.unwrap().0.stale_since_version;
                        prop_assert!(version <= target_version);
                    }
                }
            }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L63-70)
```rust
        if !sharding {
            info!("State K/V DB is not enabled!");
            return Ok(Self {
                state_kv_metadata_db: Arc::clone(&ledger_db),
                state_kv_db_shards: arr![Arc::clone(&ledger_db); 16],
                hot_state_kv_db_shards: None,
                enabled_sharding: false,
            });
```

**File:** storage/aptosdb/src/state_kv_db.rs (L285-291)
```rust
    pub(crate) fn hack_num_real_shards(&self) -> usize {
        if self.enabled_sharding {
            NUM_STATE_SHARDS
        } else {
            1
        }
    }
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L122-127)
```rust
    (0..state_kv_db.hack_num_real_shards())
        .into_par_iter()
        .try_for_each(|shard_id| {
            truncate_state_kv_db_single_shard(state_kv_db, shard_id, target_version)
        })
}
```

**File:** storage/aptosdb/src/schema/db_metadata/mod.rs (L49-72)
```rust
pub enum DbMetadataKey {
    LedgerPrunerProgress,
    StateMerklePrunerProgress,
    EpochEndingStateMerklePrunerProgress,
    StateKvPrunerProgress,
    StateSnapshotKvRestoreProgress(Version),
    LedgerCommitProgress,
    StateKvCommitProgress,
    OverallCommitProgress,
    StateKvShardCommitProgress(ShardId),
    StateMerkleCommitProgress,
    StateMerkleShardCommitProgress(ShardId),
    EventPrunerProgress,
    TransactionAccumulatorPrunerProgress,
    TransactionInfoPrunerProgress,
    TransactionPrunerProgress,
    WriteSetPrunerProgress,
    StateMerkleShardPrunerProgress(ShardId),
    EpochEndingStateMerkleShardPrunerProgress(ShardId),
    StateKvShardPrunerProgress(ShardId),
    StateMerkleShardRestoreProgress(ShardId, Version),
    TransactionAuxiliaryDataPrunerProgress,
    PersistedAuxiliaryInfoPrunerProgress,
}
```
