# Audit Report

## Title
Cache Initialization Race Condition Causes Permanent Data Gap in Indexer File Store During Master Mode Recovery

## Summary
A race condition during master mode initialization can cause the cache's `start_version` to be permanently higher than the FileStoreUploader's recovered version, creating an unrecoverable data gap where transactions are never uploaded to the file store. This occurs when file store metadata is ahead of actual batch data during system restart.

## Finding Description

The vulnerability exists in the initialization sequence when the indexer-grpc-manager starts in master mode. The issue stems from three critical timing problems:

**1. Cache Initialization Before Recovery** [1](#0-0) 

The DataManager reads the file store version and initializes the cache BEFORE FileStoreUploader performs recovery. The `start_version` is permanently set at this point.

**2. Recovery May Find Different Version** [2](#0-1) 

FileStoreUploader's recover() function walks through batch metadata and may discover that the actual completed work is at a different version than what the metadata claims. It then updates the file store metadata to reflect the true state.

**3. fetch_max Prevents Downward Updates** [3](#0-2) 

When `update_file_store_version_in_cache` is called after recovery with `version_can_go_backward=true`, it uses `fetch_max` which atomically updates to the MAXIMUM of current and new values. If the cache was initialized with a higher version (155,000) but recovery found the true version is lower (150,000), `fetch_max` won't update the cache downward.

**Attack Scenario:**

If file store metadata becomes inconsistent (corruption, premature metadata update before batch completion, or timing issues):

1. File store metadata shows version 155,000 (next version to process)
2. Actual complete batches only reach version 150,000
3. System restarts in master mode
4. Cache initializes with `start_version = 155,000`
5. FileStoreUploader recovers and finds true version is 150,000
6. Cache's `file_store_version` stays at 155,000 (fetch_max prevents downward update)
7. DataManager fetches transactions from fullnode starting at 155,000
8. FileStoreUploader requests transactions from cache starting at 150,000
9. Cache returns empty (150,000 < start_version 155,000)
10. **Versions 150,000-154,999 are never uploaded - permanent data gap!**

**Invariant Broken:** Data completeness and availability in the indexer file store. The indexer's guarantee that all transaction data is preserved and accessible is violated.

## Impact Explanation

**High Severity** - This meets the "Significant protocol violations" category because:

1. **Permanent Data Loss**: Transactions in the gap range are never uploaded to file store, making them permanently unavailable to clients querying historical data
2. **Indexer Liveness Failure**: FileStoreUploader deadlocks waiting for versions that will never appear in cache
3. **Service Degradation**: Clients relying on file store for data queries experience gaps in transaction history
4. **Requires Manual Intervention**: Recovery requires operators to manually identify the gap, re-fetch missing transactions, and repair the file store

While this doesn't directly affect consensus or validator operations (indexer is separate), it severely impacts the indexer infrastructure which is critical for ecosystem participants querying blockchain history.

## Likelihood Explanation

**Medium Likelihood** - This can occur through several realistic scenarios:

1. **Crash During Metadata Update**: System crashes after updating file store metadata but before completing batch upload (line 254 in file_store_uploader.rs updates metadata periodically, creating windows where metadata is ahead of data)
2. **File Store Corruption**: Storage backend corruption causes metadata and batch metadata to become inconsistent
3. **Operational Errors**: Manual intervention or backup restoration where metadata is restored from a different point than batch data
4. **Timing-Based Race**: In high-load scenarios, timing between metadata updates and batch completions could create windows for inconsistency

The issue is exacerbated because `version_can_go_backward=true` in master mode masks the problem by not panicking, allowing the system to continue in a broken state.

## Recommendation

**Fix 1: Update cache.start_version after recovery**

After FileStoreUploader completes recovery and signals DataManager, update BOTH `cache.file_store_version` AND `cache.start_version` to match the recovered version:

```rust
// In DataManager::start(), after receiving recovery signal
let cache = self.cache.write().await;  // Need write lock
let recovered_version = self.get_file_store_version().await;
cache.start_version = recovered_version;
cache.file_store_version.store(recovered_version, Ordering::SeqCst);
self.update_file_store_version_in_cache(&cache, true).await;
```

**Fix 2: Use compare_exchange instead of fetch_max**

Replace `fetch_max` with proper compare-and-exchange that allows downward updates when `version_can_go_backward=true`:

```rust
async fn update_file_store_version_in_cache(...) {
    if let Some(file_store_version) = file_store_version {
        let mut current = cache.file_store_version.load(Ordering::SeqCst);
        
        if version_can_go_backward || file_store_version >= current {
            // Allow update in either direction if version_can_go_backward
            cache.file_store_version.store(file_store_version, Ordering::SeqCst);
        } else if file_store_version < current {
            panic!("File store version is going backward...");
        }
        
        FILE_STORE_VERSION_IN_CACHE.set(file_store_version as i64);
    }
}
```

**Fix 3: Add consistency check**

Add validation after recovery to detect and fail fast on version mismatches:

```rust
// After recovery completes
let cache_version = cache.start_version;
let recovered_version = file_store_operator.version();
if cache_version != recovered_version {
    panic!(
        "Cache initialization version mismatch: cache={}, recovered={}. \
         File store metadata may be corrupted.",
        cache_version, recovered_version
    );
}
```

## Proof of Concept

```rust
// Reproduction steps (manual test scenario):

// 1. Start indexer in master mode, let it upload to version 150,000
// 2. Manually edit file store metadata to set version = 155,000
// 3. Restart indexer in master mode
// 4. Observe:
//    - Cache initializes with start_version = 155,000
//    - FileStoreUploader recovers to version = 150,000 (or panics if batch walking fails)
//    - If no panic, FileStoreUploader deadlocks requesting version 150,000 from cache
//    - DataManager fetches from fullnode starting at 155,000
//    - Gap of 5,000 versions never uploaded
// 5. Query file store for versions 150,000-154,999: data not found
// 6. Check metrics: FILE_STORE_VERSION shows 150,000, but cache metrics show 155,000

// To verify in code:
// Add logging in DataManager::start() after recovery:
info!("Cache state after recovery: start_version={}, file_store_version={}, recovered_version={}", 
      cache.start_version, 
      cache.file_store_version.load(Ordering::SeqCst),
      self.get_file_store_version().await.unwrap());

// If these three values differ, the bug is triggered.
```

## Notes

This vulnerability is specific to the indexer-grpc-manager component and does not affect core blockchain consensus or validator operations. However, it represents a significant availability and data integrity issue for the indexer infrastructure that many ecosystem participants depend on for querying historical blockchain data. The issue is masked by the `version_can_go_backward=true` flag which prevents panics but allows silent data loss.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L156-172)
```rust
    pub(crate) async fn new(
        chain_id: u64,
        file_store_config: IndexerGrpcFileStoreConfig,
        cache_config: CacheConfig,
        metadata_manager: Arc<MetadataManager>,
        allow_fn_fallback: bool,
    ) -> Self {
        let file_store = file_store_config.create_filestore().await;
        let file_store_reader = FileStoreReader::new(chain_id, file_store).await;
        let file_store_version = file_store_reader.get_latest_version().await.unwrap();
        Self {
            cache: RwLock::new(Cache::new(cache_config, file_store_version)),
            file_store_reader,
            metadata_manager,
            allow_fn_fallback,
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L403-419)
```rust
    async fn update_file_store_version_in_cache(
        &self,
        cache: &RwLockReadGuard<'_, Cache>,
        version_can_go_backward: bool,
    ) {
        let file_store_version = self.file_store_reader.get_latest_version().await;
        if let Some(file_store_version) = file_store_version {
            let file_store_version_before_update = cache
                .file_store_version
                .fetch_max(file_store_version, Ordering::SeqCst);
            FILE_STORE_VERSION_IN_CACHE.set(file_store_version as i64);
            info!("Updated file_store_version in cache to {file_store_version}.");
            if !version_can_go_backward && file_store_version_before_update > file_store_version {
                panic!("File store version is going backward, data might be corrupted. {file_store_version_before_update} v.s. {file_store_version}");
            };
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L87-118)
```rust
    async fn recover(&self) -> Result<(u64, BatchMetadata)> {
        let _timer = TIMER.with_label_values(&["recover"]).start_timer();

        let mut version = self
            .reader
            .get_latest_version()
            .await
            .expect("Latest version must exist.");
        info!("Starting recovering process, current version in storage: {version}.");
        let mut num_folders_checked = 0;
        let mut buffered_batch_metadata_to_recover = BatchMetadata::default();
        while let Some(batch_metadata) = self.reader.get_batch_metadata(version).await {
            let batch_last_version = batch_metadata.files.last().unwrap().last_version;
            version = batch_last_version;
            if version % NUM_TXNS_PER_FOLDER != 0 {
                buffered_batch_metadata_to_recover = batch_metadata;
                break;
            }
            num_folders_checked += 1;
            if num_folders_checked >= MAX_NUM_FOLDERS_TO_CHECK_FOR_RECOVERY {
                panic!(
                    "File store metadata is way behind batch metadata, data might be corrupted."
                );
            }
        }

        self.update_file_store_metadata(version).await?;

        info!("Finished recovering process, recovered at version: {version}.");

        Ok((version, buffered_batch_metadata_to_recover))
    }
```
