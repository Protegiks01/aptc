# Audit Report

## Title
Silent Indexer gRPC Service Failure Due to Unchecked Port Binding in Production Nodes

## Summary
The indexer gRPC service in Aptos nodes performs port binding with `.unwrap()` inside a spawned async task, causing silent failures when the configured port is already in use. The node process continues running and appears healthy while the transaction stream service remains unavailable, with only a panic message logged that operators may overlook.

## Finding Description

The vulnerability exists in the indexer gRPC bootstrap flow where port binding occurs asynchronously without proper error propagation to the main node process.

**Execution Flow:**

1. Production nodes start via `AptosNodeArgs::run()` which calls `start()` [1](#0-0) 

2. The `start()` function calls `start_and_report_ports()` with `None` for port notification channels [2](#0-1) 

3. Eventually `bootstrap_api_and_indexer()` is called, which invokes `bootstrap_indexer_grpc()` [3](#0-2) 

4. The `bootstrap()` function creates a tokio runtime and **immediately spawns an async task**, then returns `Some(runtime)` without waiting for the task to complete [4](#0-3) 

5. **The Critical Failure Point:** Inside the spawned task, `TcpListener::bind(address).await.unwrap()` is called [5](#0-4) 

6. If the port is already in use, the bind fails and `.unwrap()` panics. However, since this occurs in a spawned tokio task, **the panic is not propagated** to the parent thread

7. The main thread continues executing and enters an infinite park loop [6](#0-5) 

**Result:** The node process remains running with consensus, API, and mempool functioning normally, but the indexer gRPC transaction stream service never starts. There is no health check for this service in production node startup, making the failure silent from an operational perspective.

**Local Testnet Mitigation:** The local testnet implementation includes health checkers that would detect this failure [7](#0-6) , but production nodes have no such validation.

## Impact Explanation

This qualifies as **Medium Severity** under Aptos bug bounty criteria for the following reasons:

1. **Service Availability Disruption:** The indexer gRPC transaction stream becomes completely unavailable, affecting any indexers, monitoring systems, or data pipelines depending on this node for transaction data

2. **State Inconsistencies Requiring Intervention:** Indexers relying on this node will have incomplete data and gaps in their transaction history, requiring manual intervention to identify the gap period and backfill from alternative sources

3. **Silent Failure Characteristics:** The node appears healthy (API responds, consensus participates normally), making the issue difficult to detect without specific monitoring of the indexer gRPC endpoint

4. **Operational Impact:** Operators may deploy nodes thinking they're fully functional, only discovering the indexer service is unavailable when downstream systems fail

The issue does not reach High severity because it doesn't affect core blockchain functions (consensus, transaction execution, state commitment) and doesn't cause validator node slowdowns. It also doesn't reach Critical severity as there is no funds loss or consensus violation.

## Likelihood Explanation

**Likelihood: Medium to High** in specific deployment scenarios:

**Common Scenarios:**
- **Port Misconfiguration:** Operators configuring multiple services to use the same port (e.g., two nodes on the same machine, or another service using the default port 50051)
- **Zombie Processes:** Previous node instances not properly cleaned up still holding the port
- **Container Environments:** Port conflicts in containerized deployments with improper port mapping

**Malicious Scenarios:**
- **Local DoS Attack:** An attacker with local system access could bind to the indexer gRPC port before node startup, preventing the service from starting
- **Targeted Service Disruption:** In shared hosting environments, malicious actors could intentionally cause port collisions to disrupt specific node services

The likelihood increases in:
- Automated deployment pipelines without proper port validation
- Development/staging environments with multiple nodes on the same machine  
- Cloud environments with dynamic port allocation

## Recommendation

Implement proper error handling for port binding with graceful failure propagation:

**Fix 1: Return Result instead of panicking**

Modify the `bootstrap()` function to return `Result<Option<Runtime>>` and handle binding errors properly. Replace the `.unwrap()` calls with proper error propagation that can be checked by the calling function.

**Fix 2: Pre-flight port validation**

Before spawning the async task, perform a synchronous port availability check:

```rust
// Check if port is available before spawning task
let test_bind = std::net::TcpListener::bind(address)
    .context("Indexer gRPC port is already in use")?;
drop(test_bind);
```

**Fix 3: Add production health checking**

Similar to the local testnet implementation, add health checkers for critical services in production node startup and fail fast if essential services cannot start.

**Fix 4: Better logging**

Add explicit logging before the bind attempt:
```rust
info!("Attempting to bind indexer gRPC service to {}", address);
let listener = TcpListener::bind(address).await
    .context(format!("Failed to bind indexer gRPC to {}. Port may be in use.", address))?;
info!("Successfully bound indexer gRPC service to {}", address);
```

## Proof of Concept

**Reproduction Steps:**

```rust
// Terminal 1: Bind to the default indexer gRPC port before starting the node
use tokio::net::TcpListener;

#[tokio::main]
async fn main() {
    let listener = TcpListener::bind("127.0.0.1:50051").await.unwrap();
    println!("Bound to port 50051, node indexer gRPC will fail silently");
    
    // Keep the port bound
    loop {
        tokio::time::sleep(std::time::Duration::from_secs(60)).await;
    }
}
```

```bash
# Terminal 2: Start a production Aptos node with default indexer gRPC config
cargo run -p aptos-node -- -f <config_path>

# Observe:
# 1. Node starts successfully
# 2. API is accessible
# 3. Only a panic message appears in logs (easily missed)
# 4. Indexer gRPC service is unavailable at port 50051
# 5. Node process continues running indefinitely
```

**Verification:**

```bash
# Node API responds (healthy)
curl http://127.0.0.1:8080/v1

# Indexer gRPC does not respond (silent failure)
grpcurl -plaintext 127.0.0.1:50051 list
# Error: connection refused or timeout
```

## Notes

This vulnerability is specific to how the `bootstrap()` function spawns an async task without waiting for port binding completion. The local testnet implementation has proper health checking that would catch this issue, but production node startup lacks this validation. The fix should ensure that critical service failures are detected during node initialization rather than silently failing in background tasks.

### Citations

**File:** aptos-node/src/lib.rs (L186-186)
```rust
            start(config, None, true).expect("Node should start correctly");
```

**File:** aptos-node/src/lib.rs (L222-222)
```rust
    start_and_report_ports(config, log_file, create_global_rayon_pool, None, None)
```

**File:** aptos-node/src/lib.rs (L284-288)
```rust
    while !term.load(Ordering::Acquire) {
        thread::park();
    }

    Ok(())
```

**File:** aptos-node/src/services.rs (L114-121)
```rust
    let indexer_grpc = bootstrap_indexer_grpc(
        node_config,
        chain_id,
        db_rw.reader.clone(),
        mempool_client_sender.clone(),
        indexer_reader,
        indexer_grpc_port_tx,
    );
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/runtime.rs (L64-134)
```rust
    runtime.spawn(async move {
        let context = Arc::new(Context::new(
            chain_id,
            db,
            mp_sender,
            node_config,
            indexer_reader,
        ));
        let service_context = ServiceContext {
            context: context.clone(),
            processor_task_count,
            processor_batch_size,
            output_batch_size,
            transaction_channel_size,
            max_transaction_filter_size_bytes,
        };
        // If we are here, we know indexer grpc is enabled.
        let server = FullnodeDataService {
            service_context: service_context.clone(),
            abort_handle: Arc::new(AtomicBool::new(false)),
        };
        let localnet_data_server = LocalnetDataService { service_context };

        let reflection_service = tonic_reflection::server::Builder::configure()
            // Note: It is critical that the file descriptor set is registered for every
            // file that the top level API proto depends on recursively. If you don't,
            // compilation will still succeed but reflection will fail at runtime.
            //
            // TODO: Add a test for this / something in build.rs, this is a big footgun.
            .register_encoded_file_descriptor_set(INDEXER_V1_FILE_DESCRIPTOR_SET)
            .register_encoded_file_descriptor_set(TRANSACTION_V1_TESTING_FILE_DESCRIPTOR_SET)
            .register_encoded_file_descriptor_set(UTIL_TIMESTAMP_FILE_DESCRIPTOR_SET)
            .build_v1()
            .expect("Failed to build reflection service");

        let reflection_service_clone = reflection_service.clone();

        let tonic_server = Server::builder()
            .http2_keepalive_interval(Some(std::time::Duration::from_secs(60)))
            .http2_keepalive_timeout(Some(std::time::Duration::from_secs(5)))
            .add_service(reflection_service_clone);

        let router = match use_data_service_interface {
            false => {
                let svc = FullnodeDataServer::new(server)
                    .send_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Gzip);
                tonic_server.add_service(svc)
            },
            true => {
                let svc = RawDataServer::new(localnet_data_server)
                    .send_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Gzip);
                tonic_server.add_service(svc)
            },
        };

        let listener = TcpListener::bind(address).await.unwrap();
        if let Some(port_tx) = port_tx {
            port_tx.send(listener.local_addr().unwrap().port()).unwrap();
        }
        let incoming = TcpIncoming::from_listener(listener, false, None).unwrap();

        // Make port into a config
        router.serve_with_incoming(incoming).await.unwrap();

        info!(address = address, "[indexer-grpc] Started GRPC server");
    });
    Some(runtime)
```

**File:** crates/aptos/src/node/local_testnet/node.rs (L206-215)
```rust
    fn get_health_checkers(&self) -> HashSet<HealthChecker> {
        let node_api_url = self.get_node_api_url();
        let mut checkers = HashSet::new();
        checkers.insert(HealthChecker::NodeApi(node_api_url));
        if self.config.indexer_grpc.enabled {
            let data_service_url = self.get_data_service_url();
            checkers.insert(HealthChecker::DataServiceGrpc(data_service_url));
        }
        checkers
    }
```
