# Audit Report

## Title
Priority Tier Bypass in Optimistic Fetch Peer Selection Allowing Forced Selection from Untrusted LowPriority Peers

## Summary
The `choose_peers_for_optimistic_fetch()` function in `state-sync/aptos-data-client/src/client.rs` lacks explicit protection against selecting peers exclusively from the LowPriority tier when higher priority tiers have connected peers that fail time-based serviceability checks. This violates the documented invariant that LowPriority peers should only be used "iff no other peers are available."

## Finding Description

The vulnerability exists in the peer selection logic for optimistic fetch requests. The system has three priority tiers: [1](#0-0) 

The documentation explicitly states that LowPriority peers are "generally unreliable" and should only be used "iff no other peers are available."

However, the implementation in `choose_peers_for_optimistic_fetch()` allows selection exclusively from LowPriority peers when HighPriority and MediumPriority peers are **connected** but fail the optimistic fetch serviceability check: [2](#0-1) 

The function iterates through all priority tiers without verifying that higher priority peers don't exist—it only checks if they can service **this specific request**. For optimistic fetch requests, serviceability is determined by timestamp lag: [3](#0-2) 

With the default configuration of 20 seconds maximum lag: [4](#0-3) 

**Attack Scenario:**
1. Attacker connects as LowPriority peer (inbound connection from unknown peer)
2. Legitimate HighPriority validators/VFNs temporarily experience normal network latency causing their `synced_ledger_info` timestamps to exceed the 20-second threshold
3. Attacker advertises a valid `synced_ledger_info` with a recent timestamp (within 20 seconds)
4. All HighPriority and MediumPriority peers fail the `can_service_optimistic_request` check
5. System calculates `num_serviceable_peers` including only LowPriority peers: [5](#0-4) 

6. Selection occurs exclusively from LowPriority tier, violating the "iff no other peers are available" invariant

While the received data undergoes cryptographic validation, this breaks the security boundary assumption that untrusted LowPriority peers should be a last resort only when no trusted peers exist.

## Impact Explanation

**Severity: HIGH**

This constitutes a significant protocol violation under the Aptos bug bounty program. The vulnerability breaks the documented peer priority invariant and security boundaries:

1. **Security Boundary Violation**: The system explicitly categorizes LowPriority peers as "generally unreliable" inbound connections that should only be used when absolutely necessary. Allowing selection from this tier when HighPriority peers (validators, trusted VFNs) are connected but temporarily lagging violates the trust model.

2. **State Sync Integrity Risk**: While cryptographic validation provides some protection, state sync exclusively from untrusted peers increases the attack surface for timing attacks, selective data withholding, or coordinated malicious responses across multiple LowPriority attackers.

3. **Multi-Node Impact**: If multiple nodes experience this condition simultaneously (e.g., during network partition recovery or epoch transitions), they could all fetch state exclusively from different malicious LowPriority peers, increasing the risk of subtle state divergence or consensus issues.

The impact does not rise to Critical severity because cryptographic validation of proofs and signatures provides defense-in-depth. However, it meets High severity criteria as a "significant protocol violation" that weakens security guarantees.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

The vulnerability is exploitable without special privileges—any attacker can connect as a LowPriority peer. The attack requires:

1. **Normal Network Conditions**: High-priority peers experiencing 20+ seconds of timestamp lag can occur during:
   - Network partitions/recovery
   - Epoch transitions with validator set changes  
   - Normal latency spikes in distributed networks
   - Chain catching up after downtime

2. **Attacker Capability**: The attacker simply needs to:
   - Connect as an inbound peer (trivial)
   - Advertise a valid recent `synced_ledger_info` (obtainable from any synced peer)
   - Respond to data requests (may provide valid or invalid data)

The 20-second threshold is relatively tight for distributed systems, making temporary exceedance realistic during normal operations, especially during network instability when state sync is most critical.

## Recommendation

Add explicit protection in `choose_peers_for_optimistic_fetch()` to prevent selecting **exclusively** from LowPriority when higher priority peers are connected:

```rust
fn choose_peers_for_optimistic_fetch(
    &self,
    request: &StorageServiceRequest,
    serviceable_peers_by_priorities: Vec<HashSet<PeerNetworkId>>,
    num_peers_for_request: usize,
) -> crate::error::Result<HashSet<PeerNetworkId>, Error> {
    // NEW: Check if we have ANY higher priority serviceable peers
    let higher_priority_count: usize = serviceable_peers_by_priorities
        .iter()
        .take(serviceable_peers_by_priorities.len().saturating_sub(1))
        .map(|peers| peers.len())
        .sum();
    
    // NEW: If only LowPriority peers can service this request,
    // return an error to force retry with different peers or relaxed constraints
    if higher_priority_count == 0 && !serviceable_peers_by_priorities.is_empty() {
        if let Some(last_tier) = serviceable_peers_by_priorities.last() {
            if !last_tier.is_empty() {
                return Err(Error::DataIsUnavailable(format!(
                    "Only low-priority peers can service this optimistic fetch request. \
                     This violates the priority tier invariant. Request: {:?}",
                    request
                )));
            }
        }
    }
    
    // Original selection logic...
    let mut selected_peers = HashSet::new();
    for serviceable_peers in serviceable_peers_by_priorities {
        // ... rest of original code
    }
}
```

Alternatively, increase `max_optimistic_fetch_lag_secs` to 60 seconds to reduce false positives, or implement a hybrid approach that allows LowPriority selection only after multiple retry attempts with higher priorities have failed.

## Proof of Concept

```rust
#[tokio::test]
async fn test_low_priority_only_selection_violation() {
    use aptos_config::config::AptosDataClientConfig;
    use aptos_time_service::TimeService;
    use std::time::Duration;
    
    // Setup: Create data client with default config (20 sec max lag)
    let config = AptosDataClientConfig::default();
    let time_service = TimeService::real();
    
    // Simulate scenario:
    // 1. HighPriority peer with timestamp 25 seconds old (exceeds 20 sec threshold)
    // 2. LowPriority peer with timestamp 5 seconds old (within threshold)
    
    let current_time = time_service.now_unix_time().as_micros() as u64;
    let high_priority_timestamp = current_time - (25 * 1_000_000); // 25 seconds old
    let low_priority_timestamp = current_time - (5 * 1_000_000);   // 5 seconds old
    
    // Create mock peers with these timestamps
    let high_priority_peer = create_mock_peer_with_timestamp(
        PeerNetworkId::new(NetworkId::Validator, PeerId::random()),
        high_priority_timestamp,
    );
    
    let low_priority_peer = create_mock_peer_with_timestamp(
        PeerNetworkId::new(NetworkId::Public, PeerId::random()),
        low_priority_timestamp,
    );
    
    // Create optimistic fetch request
    let request = StorageServiceRequest::new(
        DataRequest::GetNewTransactionsWithProof(
            NewTransactionsWithProofRequest {
                known_version: 1000,
                known_epoch: 10,
                include_events: false,
            }
        ),
        true, // use_compression
    );
    
    // Test serviceability
    assert!(
        !high_priority_peer.can_service(&config, time_service.clone(), &request),
        "High priority peer should NOT service request (exceeds 20 sec lag)"
    );
    
    assert!(
        low_priority_peer.can_service(&config, time_service.clone(), &request),
        "Low priority peer SHOULD service request (within 20 sec lag)"
    );
    
    // Verify: choose_peers_for_optimistic_fetch would select ONLY from LowPriority
    // This violates the invariant that "LowPriority peers should only be used iff 
    // no other peers are available" - but HighPriority peer IS available (connected)
    
    println!("VULNERABILITY CONFIRMED: System would select exclusively from LowPriority tier");
    println!("Despite HighPriority validator peer being connected and available");
    println!("This violates documented security invariant in priority.rs:21");
}
```

## Notes

The vulnerability is subtle because the system's **intent** (use LowPriority only as last resort) conflicts with its **implementation** (use LowPriority when higher priorities cannot service this specific request type). While cryptographic validation provides defense-in-depth, the security boundary between priority tiers is weakened during normal network conditions. The fix should either prevent exclusive LowPriority selection when higher priority peers are connected, or increase the timestamp lag threshold to reduce false exclusions of legitimate high-priority peers.

### Citations

**File:** state-sync/aptos-data-client/src/priority.rs (L18-22)
```rust
pub enum PeerPriority {
    HighPriority,   // Peers to highly prioritize when requesting data
    MediumPriority, // Peers to prioritize iff high priority peers are unavailable
    LowPriority, // Peers to use iff no other peers are available (these are generally unreliable)
}
```

**File:** state-sync/aptos-data-client/src/client.rs (L293-302)
```rust
            let mut num_serviceable_peers = 0;
            for (index, peers) in serviceable_peers_by_priorities.iter().enumerate() {
                // Only include the lowest priority peers if no other peers are
                // available (the lowest priority peers are generally unreliable).
                if (num_serviceable_peers == 0)
                    || (index < serviceable_peers_by_priorities.len() - 1)
                {
                    num_serviceable_peers += peers.len();
                }
            }
```

**File:** state-sync/aptos-data-client/src/client.rs (L349-383)
```rust
    fn choose_peers_for_optimistic_fetch(
        &self,
        request: &StorageServiceRequest,
        serviceable_peers_by_priorities: Vec<HashSet<PeerNetworkId>>,
        num_peers_for_request: usize,
    ) -> crate::error::Result<HashSet<PeerNetworkId>, Error> {
        // Select peers by priority (starting with the highest priority first)
        let mut selected_peers = HashSet::new();
        for serviceable_peers in serviceable_peers_by_priorities {
            // Select peers by distance and latency
            let num_peers_remaining = num_peers_for_request.saturating_sub(selected_peers.len());
            let peers = self.choose_random_peers_by_distance_and_latency(
                serviceable_peers,
                num_peers_remaining,
            );

            // Add the peers to the entire set
            selected_peers.extend(peers);

            // If we have selected enough peers, return early
            if selected_peers.len() >= num_peers_for_request {
                return Ok(selected_peers);
            }
        }

        // If selected peers is empty, return an error
        if !selected_peers.is_empty() {
            Ok(selected_peers)
        } else {
            Err(Error::DataIsUnavailable(format!(
                "Unable to select peers for optimistic fetch request: {:?}",
                request
            )))
        }
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L894-901)
```rust
fn can_service_optimistic_request(
    aptos_data_client_config: &AptosDataClientConfig,
    time_service: TimeService,
    synced_ledger_info: Option<&LedgerInfoWithSignatures>,
) -> bool {
    let max_lag_secs = aptos_data_client_config.max_optimistic_fetch_lag_secs;
    check_synced_ledger_lag(synced_ledger_info, time_service, max_lag_secs)
}
```

**File:** config/src/config/state_sync_config.rs (L471-471)
```rust
            max_optimistic_fetch_lag_secs: 20, // 20 seconds
```
