# Audit Report

## Title
Missing WriteSet Verification in Backup Restoration Allows State Corruption

## Summary
The transaction backup restoration system fails to cryptographically verify WriteSet data against the `state_change_hash` field in `TransactionInfo` during both verification and restoration phases. This allows corrupted or malicious WriteSets to be persisted to the database, violating the "State Consistency" and "Deterministic Execution" invariants and potentially causing chain splits.

## Finding Description

The backup restoration system has a critical gap in its verification logic. When loading transaction backup chunks, the system deserializes five components: `Transaction`, `PersistedAuxiliaryInfo`, `TransactionInfo`, `Vec<ContractEvent>`, and `WriteSet`. However, during verification, only the transaction hashes and event root hashes are cryptographically validated—WriteSet integrity is never checked.

**The vulnerability exists in the verification flow:**

1. `LoadedChunk::load()` deserializes all components including WriteSets from backup chunks [1](#0-0) 

2. A `TransactionListWithProof` is constructed containing transactions, events, and transaction infos, but **explicitly excluding WriteSets** [2](#0-1) 

3. The verification only checks transaction hashes match `txn_info.transaction_hash()` and event root hashes match `txn_info.event_root_hash()` [3](#0-2) 

4. The unverified WriteSets are stored in the `LoadedChunk` and later persisted to the database [4](#0-3) 

**The proper verification exists but is never invoked:**

The codebase contains `TransactionOutput::ensure_match_transaction_info()` which properly verifies WriteSets by computing their hash and comparing to `state_change_hash` in `TransactionInfo` [5](#0-4) 

However, this verification is only called during transaction replay in `ChunkExecutor` [6](#0-5) , not during the backup load/verification phase or when transactions are saved without replay.

**Attack scenario:**

1. An attacker gains access to backup storage (e.g., misconfigured cloud storage, compromised backup service)
2. Attacker modifies WriteSets in backup chunk files while keeping transactions and TransactionInfo intact
3. Verification passes because `LoadedChunk::load()` doesn't validate WriteSets against `state_change_hash`
4. During restoration without replay (versions before `replay_from_version`), corrupted WriteSets are saved directly via `save_transactions_impl()` [7](#0-6)  without any hash verification
5. Database state becomes inconsistent with what transaction execution would produce

**Why the analysis CSV doesn't prevent this:**

The `add_transaction()` function generates CSV metadata from the already-loaded (potentially corrupted) WriteSets [8](#0-7) . The CSV is output-only and not used for verification, so it will contain metadata about corrupted data without detecting the corruption.

## Impact Explanation

**Critical Severity** - This vulnerability breaks fundamental blockchain invariants:

1. **State Consistency Violation**: Different nodes restoring from corrupted backups will have different state values for the same transaction versions, violating the requirement that "State transitions must be atomic and verifiable via Merkle proofs."

2. **Deterministic Execution Violation**: Nodes that execute transactions normally will produce different state than nodes that restore from corrupted backups, breaking the invariant that "All validators must produce identical state roots for identical blocks."

3. **Potential Chain Split**: If a significant portion of validators restore from corrupted backups, they will compute different state roots, preventing consensus on subsequent blocks and potentially requiring a hard fork to resolve.

4. **Silent State Corruption**: The corruption is not detected during verification, making it particularly dangerous as operators believe their backups are valid when they are not.

This meets the **Critical Severity** criteria: "Consensus/Safety violations" and potentially "Non-recoverable network partition (requires hardfork)."

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability is exploitable when:
- Backup storage has weak access controls (common in cloud deployments)
- Restoration is performed without transaction replay for performance
- Verification is run without detecting the issue

This is realistic because:
1. Many deployments use cloud storage (S3, GCS) where misconfigurations or compromised credentials are common
2. Restoration without replay is the default path for transactions before the replay version threshold
3. Operators routinely verify backups, which would give false confidence
4. The attack requires no validator privileges or protocol-level access

The complexity is low—an attacker only needs to modify BCS-serialized WriteSet data in backup chunks.

## Recommendation

**Add WriteSet verification to the backup restoration flow:**

In `LoadedChunk::load()`, after the existing verification, add WriteSet hash verification:

```rust
// After line 167 in restore.rs, add:
// Verify WriteSets match state_change_hash in TransactionInfo
for (write_set, txn_info) in write_sets.iter().zip(txn_infos.iter()) {
    let write_set_hash = aptos_crypto::hash::CryptoHash::hash(write_set);
    anyhow::ensure!(
        write_set_hash == txn_info.state_change_hash(),
        "WriteSet hash mismatch at version {}. Calculated: {:x}, Expected: {:x}",
        manifest.first_version + (write_sets.iter().position(|ws| ws == write_set).unwrap() as u64),
        write_set_hash,
        txn_info.state_change_hash()
    );
}
```

This ensures WriteSets are cryptographically verified before being used, consistent with how events are verified via `verify_events_against_root_hash()`.

## Proof of Concept

**Rust test demonstrating the vulnerability:**

```rust
#[tokio::test]
async fn test_corrupted_writeset_passes_verification() {
    // 1. Create a valid backup chunk with transaction, txn_info, events, write_set
    let txn = /* create valid transaction */;
    let write_set = /* correct write_set from execution */;
    let txn_info = TransactionInfo::new(
        txn.hash(),
        write_set.hash(), // state_change_hash
        event_root_hash,
        /* other fields */
    );
    
    // 2. Serialize to chunk file
    let chunk_data = bcs::to_bytes(&(txn, aux_info, txn_info, events, write_set))?;
    
    // 3. Modify the write_set in the serialized data (simulating attacker)
    let corrupted_write_set = WriteSet::default(); // different write_set
    let corrupted_data = bcs::to_bytes(&(txn, aux_info, txn_info, events, corrupted_write_set))?;
    
    // 4. Attempt to load the corrupted chunk
    let loaded_chunk = LoadedChunk::load(manifest, &storage, None).await;
    
    // BUG: This succeeds even though write_set hash doesn't match txn_info.state_change_hash()
    assert!(loaded_chunk.is_ok()); // Currently passes - this is the bug!
    
    // 5. Verify the corrupted write_set was loaded
    let chunk = loaded_chunk.unwrap();
    assert_ne!(
        CryptoHash::hash(&chunk.write_sets[0]),
        chunk.txn_infos[0].state_change_hash()
    ); // Hashes don't match but verification passed
}
```

This demonstrates that corrupted WriteSets bypass verification and would be persisted to the database during restoration.

## Notes

The vulnerability is particularly subtle because:
1. The proper verification function exists (`ensure_match_transaction_info`) but is only used during replay
2. Events have similar verification logic (`verify_events_against_root_hash`) that IS called, creating false security
3. The TransactionInfo contains the correct `state_change_hash` for verification, but it's never checked during backup load
4. The analysis CSV generates metadata from corrupted data, providing no protection

The fix is straightforward: apply the same hash verification pattern used for events to WriteSets during the backup load phase.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L112-137)
```rust
        while let Some(record_bytes) = file.read_record_bytes().await? {
            let (txn, aux_info, txn_info, events, write_set): (
                _,
                PersistedAuxiliaryInfo,
                _,
                _,
                WriteSet,
            ) = match manifest.format {
                TransactionChunkFormat::V0 => {
                    let (txn, txn_info, events, write_set) = bcs::from_bytes(&record_bytes)?;
                    (
                        txn,
                        PersistedAuxiliaryInfo::None,
                        txn_info,
                        events,
                        write_set,
                    )
                },
                TransactionChunkFormat::V1 => bcs::from_bytes(&record_bytes)?,
            };
            txns.push(txn);
            persisted_aux_info.push(aux_info);
            txn_infos.push(txn_info);
            event_vecs.push(events);
            write_sets.push(write_set);
        }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L157-167)
```rust
        let txn_list_with_proof =
            TransactionListWithProofV2::new(TransactionListWithAuxiliaryInfos::new(
                TransactionListWithProof::new(
                    txns,
                    Some(event_vecs),
                    Some(manifest.first_version),
                    TransactionInfoListWithProof::new(range_proof, txn_infos),
                ),
                persisted_aux_info,
            ));
        txn_list_with_proof.verify(ledger_info.ledger_info(), Some(manifest.first_version))?;
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L177-185)
```rust
        Ok(Self {
            manifest,
            txns,
            persisted_aux_info,
            txn_infos,
            event_vecs,
            range_proof,
            write_sets,
        })
```

**File:** types/src/transaction/mod.rs (L1898-1908)
```rust
        let write_set_hash = CryptoHash::hash(self.write_set());
        ensure!(
            write_set_hash == txn_info.state_change_hash(),
            "{}: version:{}, write_set_hash:{:?}, expected:{:?}, write_set: {:?}, expected(if known): {:?}",
            ERR_MSG,
            version,
            write_set_hash,
            txn_info.state_change_hash(),
            self.write_set,
            expected_write_set,
        );
```

**File:** types/src/transaction/mod.rs (L2317-2350)
```rust
        // Verify the transaction hashes match those of the transaction infos
        self.transactions
            .par_iter()
            .zip_eq(self.proof.transaction_infos.par_iter())
            .map(|(txn, txn_info)| {
                let txn_hash = CryptoHash::hash(txn);
                ensure!(
                    txn_hash == txn_info.transaction_hash(),
                    "The hash of transaction does not match the transaction info in proof. \
                     Transaction hash: {:x}. Transaction hash in txn_info: {:x}.",
                    txn_hash,
                    txn_info.transaction_hash(),
                );
                Ok(())
            })
            .collect::<Result<Vec<_>>>()?;

        // Verify the transaction infos are proven by the ledger info.
        self.proof
            .verify(ledger_info, self.get_first_transaction_version())?;

        // Verify the events if they exist.
        if let Some(event_lists) = &self.events {
            ensure!(
                event_lists.len() == self.get_num_transactions(),
                "The length of event_lists ({}) does not match the number of transactions ({}).",
                event_lists.len(),
                self.get_num_transactions(),
            );
            event_lists
                .into_par_iter()
                .zip_eq(self.proof.transaction_infos.par_iter())
                .map(|(events, txn_info)| verify_events_against_root_hash(events, txn_info))
                .collect::<Result<Vec<_>>>()?;
```

**File:** execution/executor/src/chunk_executor/mod.rs (L636-641)
```rust
            if let Err(err) = txn_out.ensure_match_transaction_info(
                version,
                txn_info,
                Some(write_set),
                Some(events),
            ) {
```

**File:** storage/aptosdb/src/backup/restore_utils.rs (L261-267)
```rust
    for (idx, ws) in write_sets.iter().enumerate() {
        WriteSetDb::put_write_set(
            first_version + idx as Version,
            ws,
            &mut ledger_db_batch.write_set_db_batches,
        )?;
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/analysis.rs (L88-98)
```rust
        let mut write_set_size = 0;
        for (index, (key, op)) in write_set.write_op_iter().enumerate() {
            let write_op_size = key.size() + op.as_state_value().map_or(0, |value| value.size());
            write_set_size += write_op_size;

            self.write_op_writer.serialize(WriteOpRow {
                version,
                index,
                write_op_size,
            })?;
        }
```
