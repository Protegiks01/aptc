# Audit Report

## Title
File Descriptor Leak in NetworkController Due to Improper Async Task Shutdown

## Summary
The `NetworkController::shutdown()` method sends shutdown signals to async tasks but does not wait for them to complete cleanup, leading to potential file descriptor leaks when gRPC client connections are not properly closed. This resource leak can accumulate over time if `NetworkController` instances are repeatedly created and destroyed, eventually exhausting the system's file descriptor limit and causing node availability failures.

## Finding Description

The vulnerability exists in the shutdown sequence of the `NetworkController` in the secure networking module. When `shutdown()` is called, it signals the async tasks to stop but returns immediately without waiting for cleanup to complete. [1](#0-0) 

The code explicitly acknowledges this issue with a TODO comment stating that the shutdown is not clean and doesn't wait for full cleanup. [2](#0-1) 

The documentation also explicitly warns that async tasks must be properly shut down to prevent resource leaks. [3](#0-2) 

**Attack Scenario:**

1. The `OutboundHandler` creates gRPC clients for all remote peers at startup and stores them in a HashMap. [4](#0-3) 

2. Each client holds a `NetworkMessageServiceClient<Channel>` which wraps a Tonic gRPC channel created with `connect_lazy()`. [5](#0-4) 

3. When messages are sent, these channels establish TCP connections (file descriptors) to remote peers. [6](#0-5) 

4. The gRPC clients are owned by the async task spawned in `OutboundHandler::start()`. [7](#0-6) 

5. When `NetworkController::shutdown()` is called, it sends a stop signal but returns immediately without waiting for the async task to finish and drop the gRPC clients.

6. When services like `ExecutorService` are dropped, they call `shutdown()` in their Drop implementation. [8](#0-7) 

7. The `NetworkController` and its tokio `Runtime` instances are then dropped while the async task may still be running, leading to incomplete cleanup of gRPC channels and their underlying file descriptors.

**Resource Leak Path:**
If `ExecutorService` instances (or other components using `NetworkController`) are created and destroyed during:
- Service reconfigurations
- Error recovery scenarios  
- Node restarts that don't fully exit the process

Each cycle leaves file descriptors from old gRPC connections not properly closed, accumulating until the OS file descriptor limit is reached.

## Impact Explanation

This vulnerability represents a **Medium Severity** issue per the Aptos bug bounty criteria:

**Availability Impact:** When file descriptors are exhausted:
- The node cannot establish new network connections
- The node cannot accept incoming RPC requests
- The node becomes unable to participate in consensus
- The node effectively goes offline, reducing network availability

**Why Medium, not High:**
- Does not directly compromise consensus safety or cause funds loss
- Requires specific usage patterns (repeated service lifecycle operations)
- Impact is limited to individual node availability rather than network-wide failure
- Recovery is possible by restarting the affected process

**Violated Invariant:** This breaks the **Resource Limits** invariant - all operations must respect system resource limits. File descriptors are a critical system resource that must be properly managed.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability requires specific conditions:
1. Components using `NetworkController` must be created and destroyed multiple times without process exit
2. Each instance must create connections to remote peers
3. The accumulation must happen faster than the OS can reclaim abandoned file descriptors

**Realistic Scenarios:**
- Long-running validator nodes performing internal service restarts
- Executor service reconfigurations during shard topology changes
- Error recovery loops that recreate network services
- Development/testing environments with frequent service cycling

**Mitigating Factors:**
- Most production deployments restart the entire process rather than individual services
- The TODO comment suggests the issue is known and "may not matter" for typical usage
- The OS will eventually reclaim file descriptors from terminated processes

However, for a long-running blockchain validator node, any resource leak is concerning as uptime is critical.

## Recommendation

Implement proper async task shutdown with completion acknowledgment:

1. **Modify `OutboundHandler::start()`** to return a `JoinHandle` for the spawned task
2. **Modify `NetworkController::shutdown()`** to await task completion before returning
3. **Add timeout-based cleanup** to prevent indefinite blocking

Proposed fix for `NetworkController::shutdown()`:

```rust
pub fn shutdown(&mut self) {
    info!("Shutting down network controller at {}", self.listen_addr);
    
    // Send shutdown signals
    if let Some(shutdown_signal) = self.inbound_server_shutdown_tx.take() {
        shutdown_signal.send(()).unwrap();
    }
    
    if let Some(shutdown_signal) = self.outbound_task_shutdown_tx.take() {
        shutdown_signal.send(Message::new(vec![])).unwrap_or_else(|_| {
            warn!("Failed to send shutdown signal to outbound task; probably already shutdown");
        });
    }
    
    // Wait for tasks to complete with timeout
    let shutdown_timeout = std::time::Duration::from_secs(5);
    self.inbound_rpc_runtime.shutdown_timeout(shutdown_timeout);
    self.outbound_rpc_runtime.shutdown_timeout(shutdown_timeout);
    
    info!("Network controller shutdown complete at {}", self.listen_addr);
}
```

This ensures that:
- All async tasks receive shutdown signals
- Runtimes wait for tasks to complete (with timeout)
- gRPC clients are properly dropped
- File descriptors are released before the function returns

## Proof of Concept

```rust
// Reproduction test demonstrating file descriptor leak
#[test]
fn test_network_controller_fd_leak() {
    use aptos_config::utils;
    use std::net::{IpAddr, Ipv4Addr, SocketAddr};
    use std::thread;
    
    // Get initial FD count (platform-specific, use /proc/self/fd on Linux)
    let initial_fd_count = count_open_fds();
    
    // Create and destroy NetworkControllers multiple times
    for iteration in 0..10 {
        let listen_port = utils::get_available_port();
        let listen_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), listen_port);
        
        let remote_port = utils::get_available_port();
        let remote_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), remote_port);
        
        let mut controller = NetworkController::new(
            format!("test-controller-{}", iteration),
            listen_addr,
            1000,
        );
        
        // Register handler to remote peer (creates gRPC client)
        let _sender = controller.create_outbound_channel(remote_addr, "test".to_string());
        
        // Start the controller (spawns async tasks, creates connections)
        controller.start();
        
        // Give time for connections to establish
        thread::sleep(std::time::Duration::from_millis(100));
        
        // Shutdown and drop
        controller.shutdown(); // Sends signal but doesn't wait
        drop(controller);      // Drops runtime while tasks may still be running
        
        // File descriptors should be released here, but they may not be
    }
    
    // Check FD count after cycles
    thread::sleep(std::time::Duration::from_millis(500));
    let final_fd_count = count_open_fds();
    
    // If proper cleanup, FD count should return to baseline
    // If leak exists, FD count will have increased
    assert!(
        final_fd_count <= initial_fd_count + 5,
        "File descriptor leak detected: initial={}, final={}",
        initial_fd_count,
        final_fd_count
    );
}

#[cfg(target_os = "linux")]
fn count_open_fds() -> usize {
    std::fs::read_dir("/proc/self/fd")
        .expect("Failed to read /proc/self/fd")
        .count()
}

#[cfg(not(target_os = "linux"))]
fn count_open_fds() -> usize {
    // Platform-specific implementation needed
    0
}
```

**Notes**

The existing codebase shows awareness of this issue through the TODO comment, and the comment suggests "it may not matter much for now because we shutdown before exiting the process." However, for a production blockchain validator that must maintain high availability and may perform internal service reconfigurations without full process restarts, this resource leak represents a real availability risk that should be properly addressed.

### Citations

**File:** secure/net/src/network_controller/mod.rs (L81-82)
```rust
/// 4. We need to shutdown all the async tasks spawned by the NetworkController runtimes, otherwise
///    the program will hang, or have resource leaks.
```

**File:** secure/net/src/network_controller/mod.rs (L152-166)
```rust
    // TODO: This is still not a very clean shutdown. We don't wait for the full shutdown after
    //       sending the signal. May not matter much for now because we shutdown before exiting the
    //       process. Ideally, we want to fix this.
    pub fn shutdown(&mut self) {
        info!("Shutting down network controller at {}", self.listen_addr);
        if let Some(shutdown_signal) = self.inbound_server_shutdown_tx.take() {
            shutdown_signal.send(()).unwrap();
        }

        if let Some(shutdown_signal) = self.outbound_task_shutdown_tx.take() {
            shutdown_signal.send(Message::new(vec![])).unwrap_or_else(|_| {
                warn!("Failed to send shutdown signal to outbound task; probably already shutdown");
            })
        }
    }
```

**File:** secure/net/src/network_controller/outbound_handler.rs (L68-76)
```rust
        // Create a grpc client for each remote address
        let mut grpc_clients: HashMap<SocketAddr, GRPCNetworkMessageServiceClientWrapper> =
            HashMap::new();
        self.remote_addresses.iter().for_each(|remote_addr| {
            grpc_clients.insert(
                *remote_addr,
                GRPCNetworkMessageServiceClientWrapper::new(rt, *remote_addr),
            );
        });
```

**File:** secure/net/src/network_controller/outbound_handler.rs (L89-99)
```rust
        rt.spawn(async move {
            info!("Starting outbound handler at {}", address.to_string());
            Self::process_one_outgoing_message(
                outbound_handlers,
                &address,
                inbound_handler.clone(),
                &mut grpc_clients,
            )
            .await;
            info!("Stopping outbound handler at {}", address.to_string());
        });
```

**File:** secure/net/src/grpc_network_service/mod.rs (L118-138)
```rust
pub struct GRPCNetworkMessageServiceClientWrapper {
    remote_addr: String,
    remote_channel: NetworkMessageServiceClient<Channel>,
}

impl GRPCNetworkMessageServiceClientWrapper {
    pub fn new(rt: &Runtime, remote_addr: SocketAddr) -> Self {
        Self {
            remote_addr: remote_addr.to_string(),
            remote_channel: rt
                .block_on(async { Self::get_channel(format!("http://{}", remote_addr)).await }),
        }
    }

    async fn get_channel(remote_addr: String) -> NetworkMessageServiceClient<Channel> {
        info!("Trying to connect to remote server at {:?}", remote_addr);
        let conn = tonic::transport::Endpoint::new(remote_addr)
            .unwrap()
            .connect_lazy();
        NetworkMessageServiceClient::new(conn).max_decoding_message_size(MAX_MESSAGE_SIZE)
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L140-160)
```rust
    pub async fn send_message(
        &mut self,
        sender_addr: SocketAddr,
        message: Message,
        mt: &MessageType,
    ) {
        let request = tonic::Request::new(NetworkMessage {
            message: message.data,
            message_type: mt.get_type(),
        });
        // TODO: Retry with exponential backoff on failures
        match self.remote_channel.simple_msg_exchange(request).await {
            Ok(_) => {},
            Err(e) => {
                panic!(
                    "Error '{}' sending message to {} on node {:?}",
                    e, self.remote_addr, sender_addr
                );
            },
        }
    }
```

**File:** execution/executor-service/src/process_executor_service.rs (L52-56)
```rust
impl Drop for ProcessExecutorService {
    fn drop(&mut self) {
        self.shutdown();
    }
}
```
