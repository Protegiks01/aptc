# Audit Report

## Title
Resource Leak in Indexer Table Info Database Restore Leading to Disk Exhaustion

## Summary
The `restore_db_snapshot()` function in the indexer-grpc table-info component contains two resource leaks: (1) temporary files are not cleaned up when errors occur during snapshot restoration, and (2) temporary directories created during unpacking are not cleaned up on failure. Over time with repeated restore failures, these resources accumulate and can exhaust disk space on indexer nodes, causing service unavailability.

## Finding Description

The restore process creates temporary resources that are only cleaned up in the success path, leaving them behind when errors occur.

**Resource Leak #1 - Temporary File:**
In the `restore_db_snapshot()` function, a temporary file `snapshot.tar.gz` is created to store the downloaded snapshot. [1](#0-0) 

Multiple error paths exist after file creation but before cleanup:
- Stream write failures [2](#0-1) 
- File sync failures [3](#0-2) 
- Task spawn or unpacking failures [4](#0-3) 

The cleanup only occurs in the success path: [5](#0-4) 

**Resource Leak #2 - Temporary Directory:**
The `unpack_tar_gz()` helper function creates a temporary directory with a `.tmp` extension. [6](#0-5) 

If errors occur during file opening, unpacking, or the final rename operation, the temporary directory containing partial or complete database data is never cleaned up: [7](#0-6) 

**Attack/Failure Scenarios:**
1. Network interruptions during GCS download streaming
2. Disk I/O errors during file writes or sync operations
3. Corrupted snapshot data causing unpacking failures
4. Insufficient permissions or disk space during atomic rename
5. Process crashes or forced termination during restore

Each failure leaves behind large temporary resources (database snapshots are typically gigabytes in size). With repeated restore attempts during node startup, recovery operations, or scheduled restores, these resources accumulate and eventually exhaust available disk space.

## Impact Explanation

This qualifies as **Medium severity** under the Aptos bug bounty program criteria for the following reasons:

1. **Availability Impact**: Disk exhaustion on indexer-grpc nodes leads to service unavailability. When disk space is consumed, the indexer cannot:
   - Process new blockchain data
   - Serve query requests from DApps and users
   - Perform database operations
   - Accept new snapshot backups

2. **State Inconsistencies Requiring Intervention**: The accumulation of temporary resources creates an inconsistent state that requires manual operator intervention to identify and clean up orphaned files and directories. This aligns with the Medium severity category "State inconsistencies requiring intervention."

3. **Operational Impact**: While this doesn't directly affect consensus or validator operations, the indexer-grpc infrastructure is critical for the Aptos ecosystem. Many DApps and services rely on indexer data for functionality. Service degradation or outage affects the user experience and developer ecosystem.

4. **Gradual Degradation**: The issue compounds over time, making it harder to detect initially but potentially catastrophic once disk space is exhausted, leading to cascading failures.

## Likelihood Explanation

The likelihood of this vulnerability manifesting is **HIGH** for the following reasons:

1. **Natural Operational Failures**: The failure conditions (network interruptions, I/O errors, corrupted data) occur naturally in production environments without requiring attacker involvement.

2. **Restore Operation Frequency**: Database restores happen during:
   - Initial node deployment
   - Node recovery after failures
   - Scheduled restore operations for data integrity
   - System upgrades or migrations

3. **Large Resource Sizes**: Database snapshots are large (typically gigabytes), meaning even a few failed restores can consume significant disk space.

4. **No Automatic Cleanup**: The absence of any background cleanup mechanism means resources accumulate indefinitely until manual intervention.

5. **Currently Inactive**: While the function is not currently called in the codebase (as indicated by a TODO comment about fixing restore logic), the vulnerability will become active once the restore functionality is enabled.

## Recommendation

Implement proper resource cleanup using RAII patterns or explicit cleanup in error paths:

**Solution 1 - Use a guard pattern for automatic cleanup:**
```rust
pub async fn restore_db_snapshot(
    &self,
    chain_id: u64,
    metadata: BackupRestoreMetadata,
    db_path: PathBuf,
    base_path: PathBuf,
) -> anyhow::Result<()> {
    // ... existing code ...
    
    let temp_file_path = base_path.join("snapshot.tar.gz");
    
    // Create a guard that ensures cleanup on drop
    struct TempFileGuard {
        path: PathBuf,
    }
    
    impl Drop for TempFileGuard {
        fn drop(&mut self) {
            let _ = std::fs::remove_file(&self.path);
        }
    }
    
    let _guard = TempFileGuard {
        path: temp_file_path.clone(),
    };
    
    // Existing download and processing logic
    // Guard automatically cleans up on any error or success
}
```

**Solution 2 - Add explicit cleanup in error paths:**
```rust
pub async fn restore_db_snapshot(...) -> anyhow::Result<()> {
    // ... existing setup ...
    
    let temp_file_path = base_path.join("snapshot.tar.gz");
    let mut temp_file = File::create(&temp_file_path).await?;
    
    let result = async {
        // All operations that can fail
        while let Some(chunk) = stream.next().await {
            match chunk {
                Ok(data) => temp_file.write_all(&data).await?,
                Err(e) => return Err(anyhow::Error::new(e)),
            }
        }
        temp_file.sync_all().await?;
        task::spawn_blocking(move || unpack_tar_gz(&temp_file_path_clone, &db_path))
            .await??;
        Ok(())
    }.await;
    
    // Always cleanup temp file, regardless of success or failure
    fs::remove_file(&temp_file_path).await.ok();
    
    result
}
```

**For unpack_tar_gz(), add cleanup on error:**
```rust
pub fn unpack_tar_gz(temp_file_path: &PathBuf, target_db_path: &PathBuf) -> anyhow::Result<()> {
    let temp_dir_path = target_db_path.with_extension("tmp");
    
    let result = (|| {
        fs::create_dir(&temp_dir_path)?;
        let file = File::open(temp_file_path)?;
        let gz_decoder = GzDecoder::new(file);
        let mut archive = Archive::new(gz_decoder);
        archive.unpack(&temp_dir_path)?;
        fs::remove_dir_all(target_db_path).unwrap_or(());
        fs::rename(&temp_dir_path, target_db_path)?;
        Ok(())
    })();
    
    // Cleanup temp directory on error
    if result.is_err() {
        fs::remove_dir_all(&temp_dir_path).ok();
    }
    
    result
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test_resource_leak {
    use super::*;
    use std::io::{self, Error, ErrorKind};
    use tempfile::tempdir;

    #[tokio::test]
    async fn test_temp_file_leak_on_sync_failure() {
        let temp_dir = tempdir().unwrap();
        let base_path = temp_dir.path().to_path_buf();
        let temp_file_path = base_path.join("snapshot.tar.gz");
        
        // Simulate the file creation
        let mut temp_file = File::create(&temp_file_path).await.unwrap();
        temp_file.write_all(b"test data").await.unwrap();
        
        // Simulate an error before cleanup (e.g., sync failure)
        // In real code, this would return early with ? operator
        let sync_result: io::Result<()> = Err(Error::new(ErrorKind::Other, "simulated failure"));
        
        if let Err(_) = sync_result {
            // Early return - temp file is NOT cleaned up
            assert!(temp_file_path.exists(), "Temp file should still exist after error");
            return;
        }
        
        // Cleanup would only happen here in success path
        fs::remove_file(&temp_file_path).await.unwrap();
    }

    #[test]
    fn test_temp_dir_leak_on_unpack_failure() {
        let temp_dir = tempdir().unwrap();
        let target_db_path = temp_dir.path().join("db");
        let temp_dir_path = target_db_path.with_extension("tmp");
        
        // Simulate temp directory creation
        std::fs::create_dir(&temp_dir_path).unwrap();
        std::fs::write(temp_dir_path.join("test_file"), b"data").unwrap();
        
        // Simulate unpack failure - directory is not cleaned up
        // The real unpack_tar_gz would return error here
        let unpack_result: Result<(), anyhow::Error> = 
            Err(anyhow::Error::msg("simulated unpack failure"));
        
        if let Err(_) = unpack_result {
            // Error path - temp directory remains
            assert!(temp_dir_path.exists(), "Temp directory should still exist after error");
            assert!(temp_dir_path.join("test_file").exists(), 
                   "Temp directory contents should still exist");
            return;
        }
        
        // Cleanup only in success path
        std::fs::remove_dir_all(&temp_dir_path).unwrap();
    }
}
```

## Notes

While the `restore_db_snapshot()` function is not currently invoked in the codebase (indicated by a TODO comment at line 82 in table_info_service.rs), this vulnerability exists in production code and will become exploitable once the restore functionality is enabled. The issue should be fixed proactively to prevent future operational problems when the feature is activated.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L289-293)
```rust
                // Create a temporary file and write the stream to it directly
                let temp_file_name = "snapshot.tar.gz";
                let temp_file_path = base_path.join(temp_file_name);
                let temp_file_path_clone = temp_file_path.clone();
                let mut temp_file = File::create(&temp_file_path_clone).await?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L294-299)
```rust
                while let Some(chunk) = stream.next().await {
                    match chunk {
                        Ok(data) => temp_file.write_all(&data).await?,
                        Err(e) => return Err(anyhow::Error::new(e)),
                    }
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L300-300)
```rust
                temp_file.sync_all().await?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L303-305)
```rust
                task::spawn_blocking(move || unpack_tar_gz(&temp_file_path_clone, &db_path))
                    .await?
                    .expect("Failed to unpack gzipped tar file");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L307-309)
```rust
                fs::remove_file(&temp_file_path)
                    .await
                    .context("Failed to remove temporary file after unpacking")?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/fs_ops.rs (L102-103)
```rust
    let temp_dir_path = target_db_path.with_extension("tmp");
    fs::create_dir(&temp_dir_path)?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/fs_ops.rs (L105-111)
```rust
    let file = File::open(temp_file_path)?;
    let gz_decoder = GzDecoder::new(file);
    let mut archive = Archive::new(gz_decoder);
    archive.unpack(&temp_dir_path)?;

    fs::remove_dir_all(target_db_path).unwrap_or(());
    fs::rename(&temp_dir_path, target_db_path)?; // Atomically replace the directory
```
