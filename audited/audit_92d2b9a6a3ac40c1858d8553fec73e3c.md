# Audit Report

## Title
URL Fragment and Query Parameter Handling Enables Cache Bypass and Duplicate Upload Resource Exhaustion in NFT Metadata Crawler

## Summary
The asset uploader API does not normalize URLs before processing, treating URLs that differ only in fragments or query parameters as distinct resources. This allows an attacker to bypass caching mechanisms and trigger duplicate uploads of the same resource to Cloudflare CDN, leading to storage bloat, API quota exhaustion, and database pollution.

## Finding Description

The `BatchUploadRequest` struct accepts a vector of URLs that are not normalized before processing. [1](#0-0) 

When URLs are processed, they are converted to strings using `as_str()` and used directly for database lookups and hashing. [2](#0-1) 

The worker generates a unique identifier by hashing the full URL string including fragments and query parameters. [3](#0-2) 

**The Core Issue:** Per HTTP/HTTPS specifications (RFC 7230), URL fragments (`#fragment`) are **never sent to origin servers** in HTTP requests - they are purely client-side directives. However, the system treats URLs differing only in fragments as completely distinct resources.

**Attack Scenario:**
An attacker submits multiple requests with the same base URL but different fragments:
- `https://nft.example.com/token/123.png`
- `https://nft.example.com/token/123.png#v1`
- `https://nft.example.com/token/123.png#v2`
- ... (thousands of variations)

Each URL:
1. Generates a different SHA256 hash
2. Bypasses the `parsed_asset_uris` cache (different `asset_uri` primary key)
3. Creates a separate entry in `asset_uploader_request_statuses` table
4. Triggers Cloudflare to fetch the SAME image (fragments stripped)
5. Results in duplicate CDN storage entries

The database uses `asset_uri` as a primary key without normalization. [4](#0-3) 

## Impact Explanation

This issue meets **Low severity** criteria as explicitly noted in the security question:

1. **Resource Exhaustion**: Cloudflare API rate limits can be exhausted
2. **Storage Bloat**: Multiple copies of identical images stored on CDN
3. **Database Pollution**: Unbounded growth of request status entries
4. **Cache Bypass**: Defeats the purpose of the existing asset cache

This is NOT a Critical/High/Medium severity issue because:
- No consensus violations occur
- No blockchain state is affected  
- No funds are at risk
- The NFT metadata crawler is an auxiliary indexing service, not a core blockchain component
- Does not break any of the 10 critical Aptos invariants listed in the audit guidelines

## Likelihood Explanation

**High likelihood** of exploitation:
- No authentication required (public API endpoint)
- Trivial to execute (simple HTTP POST with URL variations)
- Fragments are valid URL components, not filtered as malicious
- No rate limiting visible on URL distinctness

## Recommendation

Implement URL normalization before processing:

```rust
use url::Url;

fn normalize_url(url: &Url) -> String {
    let mut normalized = url.clone();
    // Remove fragment identifier (everything after #)
    normalized.set_fragment(None);
    // Optionally: normalize query parameters (sort, deduplicate)
    normalized.as_str().to_string()
}
```

Apply normalization at the earliest point:
1. In `BatchUploadRequest` deserialization or validation
2. Before generating SHA256 hashes
3. Before database lookups and insertions

Additionally, consider query parameter normalization based on origin server behavior.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use url::Url;
    
    #[test]
    fn test_url_fragment_hash_collision() {
        let url1 = Url::parse("https://example.com/image.png").unwrap();
        let url2 = Url::parse("https://example.com/image.png#v1").unwrap();
        let url3 = Url::parse("https://example.com/image.png#v2").unwrap();
        
        // All three URLs fetch the SAME resource (fragments not sent via HTTP)
        // But generate DIFFERENT hashes in the current implementation
        let hash1 = sha256::digest(url1.to_string());
        let hash2 = sha256::digest(url2.to_string());
        let hash3 = sha256::digest(url3.to_string());
        
        assert_ne!(hash1, hash2);
        assert_ne!(hash2, hash3);
        assert_ne!(hash1, hash3);
        // This demonstrates the vulnerability: same resource, different hashes
    }
    
    #[tokio::test]
    async fn test_duplicate_upload_attack() {
        // Attacker submits batch with fragment variations
        let request = BatchUploadRequest {
            idempotency_tuple: IdempotencyTuple {
                idempotency_key: "test".to_string(),
                application_id: "app".to_string(),
            },
            urls: vec![
                Url::parse("https://example.com/nft.png").unwrap(),
                Url::parse("https://example.com/nft.png#1").unwrap(),
                Url::parse("https://example.com/nft.png#2").unwrap(),
                // ... attacker could submit thousands more
            ],
        };
        
        // Each URL would be treated as distinct, triggering:
        // - 3 separate database entries
        // - 3 separate Cloudflare uploads
        // - 3x storage usage for the SAME image
    }
}
```

## Notes

While this is a valid Low severity vulnerability as described in the security question, it does **not** meet the validation checklist requirement of "Impact meets Critical, High, or Medium severity criteria per bounty program." The NFT metadata crawler is an auxiliary off-chain indexing service that does not affect consensus, Move VM execution, state management, governance, or staking - the core security invariants of the Aptos blockchain.

### Citations

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/api/mod.rs (L37-42)
```rust
#[derive(Debug, Deserialize)]
struct BatchUploadRequest {
    #[serde(flatten)]
    idempotency_tuple: IdempotencyTuple,
    urls: Vec<Url>,
}
```

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/api/upload_batch.rs (L27-39)
```rust
    for url in &request.urls {
        if let Some(cdn_image_uri) = existing_rows.get(url.as_str()) {
            request_statuses.push(AssetUploaderRequestStatuses::new_completed(
                &request.idempotency_tuple,
                url.as_str(),
                cdn_image_uri.as_deref().unwrap(), // Safe to unwrap because we checked for existence when querying
            ));
        } else {
            request_statuses.push(AssetUploaderRequestStatuses::new(
                &request.idempotency_tuple,
                url.as_str(),
            ));
        }
```

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/worker/mod.rs (L69-82)
```rust
    async fn upload_asset(&self, url: &Url) -> anyhow::Result<impl IntoResponse + use<>> {
        let hashed_url = sha256::digest(url.to_string());
        let client = Client::builder()
            .timeout(Duration::from_secs(MAX_ASSET_UPLOAD_RETRY_SECONDS))
            .build()
            .context("Error building reqwest client")?;
        let form = Form::new()
            .text("id", hashed_url.clone())
            .text(
                // Save the asset_uri in the upload metadata to enable retrieval by asset_uri later
                "metadata",
                format!("{{\"asset_uri\": \"{}\"}}", url),
            )
            .text("url", url.to_string());
```

**File:** ecosystem/nft-metadata-crawler/src/schema.rs (L28-29)
```rust
        nft_metadata_crawler.parsed_asset_uris (asset_uri) {
            asset_uri -> Varchar,
```
