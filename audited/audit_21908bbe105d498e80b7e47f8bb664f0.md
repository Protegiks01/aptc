# Audit Report

## Title
Consensus Liveness Failure Due to Missing State Cleanup in SecretShareManager After Mid-Epoch Sync

## Summary
The SecretShareManager does not properly clean up its internal state during mid-epoch synchronization operations, causing blocks to become permanently stuck in the processing queue when they re-enter after a `sync_to_target` operation. This results in consensus liveness failure.

## Finding Description
The vulnerability exists in the interaction between three components:

1. **Missing Reset Signal**: During `sync_to_target` operations, the `reset()` method sends `ResetSignal::TargetRound` to the RandManager and BufferManager, but **not** to the SecretShareManager. [1](#0-0) 

2. **Incomplete State Cleanup**: When SecretShareManager does receive a reset (only during `end_epoch`), the `process_reset()` method clears the block_queue but only updates the highest_known_round in the store without cleaning up the HashMap containing SecretShareItems. [2](#0-1) 

3. **Stale "Decided" State Blocks Re-processing**: SecretShareStore uses a `HashMap<Round, SecretShareItem>` that accumulates entries indefinitely. When a block was previously processed to "Decided" state and re-enters after sync, the `add_share_with_metadata()` method encounters the "Decided" item and returns early without processing. [3](#0-2) 

**Attack Scenario:**
1. Block at round R is processed normally, secret shares collected and aggregated, block dequeued
2. SecretShareItem for round R transitions to "Decided" state in the secret_share_map
3. Node falls behind and calls `sync_to_target` to synchronize
4. The `reset()` method clears BufferManager and RandManager state, but SecretShareManager's secret_share_map retains round R in "Decided" state
5. After sync completes, block R re-enters the processing pipeline
6. `process_incoming_block()` calls `add_self_share()` which returns `Ok()` without processing (line 178)
7. Other validators' shares arrive but `add_share()` also does nothing for "Decided" items (line 126)
8. No aggregation occurs, block never receives its secret_shared_key
9. Block R remains stuck in block_queue forever, blocking all subsequent blocks
10. **Consensus liveness failure**: No further blocks can be processed [4](#0-3) 

The RandManager properly handles this scenario with a `reset()` method that uses `split_off()` to remove stale entries: [5](#0-4) 

However, SecretShareStore lacks any such cleanup mechanism and uses HashMap instead of BTreeMap, making efficient range-based cleanup impossible. [6](#0-5) 

## Impact Explanation
This is a **Critical Severity** vulnerability causing **Total Loss of Liveness**:

- Affected nodes cannot process any blocks after the stuck block
- Consensus cannot make progress on affected nodes
- Requires manual intervention or node restart
- Can affect multiple validators if they sync to the same target during network partitions
- Violates the **Consensus Safety** invariant (liveness property)

Per the Aptos Bug Bounty Program, "Total loss of liveness/network availability" is Critical Severity (up to $1,000,000).

## Likelihood Explanation
**High Likelihood** - This occurs naturally during:
- State synchronization operations when nodes fall behind
- Network partitions followed by recovery
- Validator restarts that require sync_to_target
- Any scenario requiring mid-epoch sync to a specific ledger info

The vulnerability triggers automatically without attacker action when:
1. A block is processed before sync (common)
2. Node performs sync_to_target (routine operation)
3. Same block re-enters during recovery (guaranteed by sync protocol)

## Recommendation

**Fix 1: Send Reset Signal to SecretShareManager**

Modify `ExecutionProxyClient::reset()` to include the secret share manager: [1](#0-0) 

Change lines 675-681 to extract `reset_tx_to_secret_share_manager` and send it the reset signal similar to rand_manager.

**Fix 2: Implement Proper Cleanup in SecretShareStore**

Change `secret_share_map` from `HashMap` to `BTreeMap` and add a `reset()` method: [6](#0-5) 

```rust
pub struct SecretShareStore {
    epoch: u64,
    self_author: Author,
    secret_share_config: SecretShareConfig,
    secret_share_map: BTreeMap<Round, SecretShareItem>,  // Changed to BTreeMap
    highest_known_round: u64,
    decision_tx: Sender<SecretSharedKey>,
}

impl SecretShareStore {
    pub fn reset(&mut self, round: u64) {
        self.update_highest_known_round(round);
        // Remove future rounds items like RandStore does
        let _ = self.secret_share_map.split_off(&round);
    }
}
```

**Fix 3: Call reset() in SecretShareManager**

Modify `process_reset()` to call the new reset method: [2](#0-1) 

```rust
fn process_reset(&mut self, request: ResetRequest) {
    let ResetRequest { tx, signal } = request;
    let target_round = match signal {
        ResetSignal::Stop => 0,
        ResetSignal::TargetRound(round) => round,
    };
    self.block_queue = BlockQueue::new();
    self.secret_share_store.lock().reset(target_round);  // Changed from update_highest_known_round
    self.stop = matches!(signal, ResetSignal::Stop);
    let _ = tx.send(ResetAck::default());
}
```

## Proof of Concept

```rust
// Reproduction steps:
// 1. Start consensus with secret sharing enabled
// 2. Process block at round R normally until decided
// 3. Trigger sync_to_target with target round > R
// 4. Observe that SecretShareManager's secret_share_map still contains round R in Decided state
// 5. When block R re-enters during sync recovery, it gets stuck
// 6. Verify block_queue.dequeue_ready_prefix() never returns block R
// 7. All subsequent blocks are blocked, consensus halts

#[tokio::test]
async fn test_secret_share_manager_stuck_after_sync() {
    // Setup secret share manager with block at round 10
    // Process block 10 to Decided state
    // Call process_reset with TargetRound(15)
    // Re-add block 10 to queue
    // Verify it never completes due to Decided state in store
    // Assert: block remains in queue indefinitely
}
```

## Notes

While the original security question asks specifically about stale aggregated keys arriving after block dequeue (which are safely ignored via the None check in `process_aggregated_key()`), this investigation revealed a more severe related vulnerability: stale *state items* in SecretShareStore that persist across resets and prevent blocks from being re-processed. 

This is fundamentally a state management bug where the SecretShareManager, unlike the RandManager, lacks proper cleanup mechanisms during synchronization operations. The vulnerability is directly related to the question's concern about state consistency in the secret sharing pipeline.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L108-128)
```rust
    fn add_share(&mut self, share: SecretShare, share_weight: u64) -> anyhow::Result<()> {
        match self {
            SecretShareItem::PendingMetadata(aggr) => {
                aggr.add_share(share, share_weight);
                Ok(())
            },
            SecretShareItem::PendingDecision {
                metadata,
                share_aggregator,
            } => {
                ensure!(
                    metadata == &share.metadata,
                    "[SecretShareItem] SecretShare metadata from {} mismatch with block metadata!",
                    share.author,
                );
                share_aggregator.add_share(share, share_weight);
                Ok(())
            },
            SecretShareItem::Decided { .. } => Ok(()),
        }
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L156-182)
```rust
    fn add_share_with_metadata(
        &mut self,
        share: SecretShare,
        share_weights: &HashMap<Author, u64>,
    ) -> anyhow::Result<()> {
        let item = std::mem::replace(self, Self::new(Author::ONE));
        let share_weight = *share_weights
            .get(share.author())
            .expect("Author must exist in weights");
        let new_item = match item {
            SecretShareItem::PendingMetadata(mut share_aggregator) => {
                let metadata = share.metadata.clone();
                share_aggregator.retain(share.metadata(), share_weights);
                share_aggregator.add_share(share, share_weight);
                SecretShareItem::PendingDecision {
                    metadata,
                    share_aggregator,
                }
            },
            SecretShareItem::PendingDecision { .. } => {
                bail!("Cannot add self share in PendingDecision state");
            },
            SecretShareItem::Decided { .. } => return Ok(()),
        };
        let _ = std::mem::replace(self, new_item);
        Ok(())
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L207-214)
```rust
pub struct SecretShareStore {
    epoch: u64,
    self_author: Author,
    secret_share_config: SecretShareConfig,
    secret_share_map: HashMap<Round, SecretShareItem>,
    highest_known_round: u64,
    decision_tx: Sender<SecretSharedKey>,
}
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L253-259)
```rust
    pub fn reset(&mut self, round: u64) {
        self.update_highest_known_round(round);
        // remove future rounds items in case they're already decided
        // otherwise if the block re-enters the queue, it'll be stuck
        let _ = self.rand_map.split_off(&round);
        let _ = self.fast_rand_map.as_mut().map(|map| map.split_off(&round));
    }
```
