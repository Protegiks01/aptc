# Audit Report

## Title
Unbounded Memory Allocation in Backup Restore System Enables Resource Exhaustion DoS

## Summary
The `read_all()` function in the backup-cli storage extension loads entire files into memory without size limits or streaming mechanisms, allowing an attacker who can control backup storage or file handles to cause Out-of-Memory crashes during restore operations.

## Finding Description

The `read_all()` function implements no size validation before loading files into memory: [1](#0-0) 

This function is used to load manifest files (JSON) and proof files (BCS) during state snapshot, transaction, and epoch ending restore operations: [2](#0-1) [3](#0-2) 

The restore operations are invoked via command-line tools by node operators: [4](#0-3) 

**Attack Scenario:**

If an attacker gains access to backup storage (S3, Azure, local filesystem) through compromised credentials or MITM attacks, they can:

1. Replace legitimate proof files with arbitrarily large files (GB-sized)
2. Modify manifest JSON files to reference malicious file handles
3. When operators run `aptos-db-tool restore` commands, the `read_all()` function attempts to load entire malicious files into memory
4. This causes OOM (Out-of-Memory) crashes, preventing disaster recovery operations

While backup chunks have size limits (default 128MB): [5](#0-4) 

Proof files and manifest files have no such protections.

## Impact Explanation

**Severity: Medium**

This vulnerability breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

Impact:
- Prevents node operators from successfully completing restore operations during disaster recovery
- Affects validator node availability during bootstrapping or state recovery
- Could delay network recovery if multiple nodes attempt restore simultaneously

This does NOT reach High severity because:
- It does not directly affect consensus or running nodes
- It requires compromised backup infrastructure or social engineering
- It only affects the restore process, not normal node operations

## Likelihood Explanation

**Likelihood: Low-to-Medium**

The attack requires:
1. **Compromised backup storage** (cloud credentials theft) OR
2. **Social engineering** (tricking operators to use malicious backup sources) OR  
3. **MITM attacks** on backup retrieval

However, restore operations are common during:
- New validator node bootstrapping
- Disaster recovery scenarios
- Network-wide outages requiring state sync

The vulnerability becomes more likely during crisis scenarios when multiple operators may be rushing to restore from potentially compromised infrastructure.

## Recommendation

Implement size limits for file reads in the backup/restore system:

```rust
const MAX_MANIFEST_SIZE: usize = 100 * 1024 * 1024; // 100 MB
const MAX_PROOF_SIZE: usize = 10 * 1024 * 1024; // 10 MB

async fn read_all(&self, file_handle: &FileHandleRef) -> Result<Vec<u8>> {
    let mut file = self.open_for_read(file_handle).await?;
    let mut bytes = Vec::new();
    
    // Use take() to limit maximum bytes read
    let max_size = determine_max_size_for_file_type(file_handle);
    let mut limited_reader = file.take(max_size as u64);
    
    match limited_reader.read_to_end(&mut bytes).await {
        Ok(n) if n == max_size as usize => {
            bail!("File exceeds maximum allowed size of {} bytes", max_size)
        }
        Ok(_) => Ok(bytes),
        Err(e) => Err(e.into()),
    }
}
```

Additionally, consider implementing streaming deserialization for large files instead of loading them entirely into memory.

## Proof of Concept

```rust
#[cfg(test)]
mod resource_exhaustion_test {
    use super::*;
    use std::io::Cursor;
    use tokio::io::AsyncReadExt;

    #[tokio::test]
    async fn test_read_all_unbounded_memory() {
        // Simulate a malicious file that's extremely large
        let huge_size = 2 * 1024 * 1024 * 1024; // 2 GB
        let large_data = vec![0u8; huge_size];
        
        let mut cursor = Cursor::new(large_data);
        let mut bytes = Vec::new();
        
        // This will attempt to allocate 2GB of memory
        // In a real attack, this would cause OOM
        let result = cursor.read_to_end(&mut bytes).await;
        
        // Without size limits, this succeeds and allocates unbounded memory
        assert!(result.is_ok() || result.is_err()); // Either succeeds or OOMs
        
        // Expected: Should fail with a size limit error before attempting allocation
    }
}
```

**Notes:**

The vulnerability requires operational security failures (compromised backup storage, MITM attacks, or social engineering) rather than being directly exploitable by unprivileged attackers. While the code lacks defensive size validation, the attack surface is limited to scenarios where backup infrastructure is already compromised. This represents a defense-in-depth gap rather than a critical exploitable vulnerability.

### Citations

**File:** storage/backup/backup-cli/src/utils/storage_ext.rs (L24-29)
```rust
    async fn read_all(&self, file_handle: &FileHandleRef) -> Result<Vec<u8>> {
        let mut file = self.open_for_read(file_handle).await?;
        let mut bytes = Vec::new();
        file.read_to_end(&mut bytes).await?;
        Ok(bytes)
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-126)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L191-193)
```rust
                    let blobs = Self::read_state_value(&storage, chunk.blobs.clone()).await?;
                    let proof = storage.load_bcs_file(&chunk.proof).await?;
                    Result::<_>::Ok((chunk_idx, chunk, blobs, proof))
```

**File:** storage/db-tool/src/restore.rs (L83-96)
```rust
                    Oneoff::StateSnapshot {
                        storage,
                        opt,
                        global,
                    } => {
                        StateSnapshotRestoreController::new(
                            opt,
                            global.try_into()?,
                            storage.init_storage().await?,
                            None, /* epoch_history */
                        )
                        .run()
                        .await?;
                    },
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L98-100)
```rust
            // If buf + current_record exceeds max_chunk_size, dump current buf to a new chunk
            let chunk_cut_opt = should_cut_chunk(&self.buf, &record_bytes, self.max_chunk_size)
                .then(|| {
```
