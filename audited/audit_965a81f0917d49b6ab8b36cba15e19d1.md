# Audit Report

## Title
Iterator Invalidation During Concurrent Pruning Causes State Sync and API Failures

## Summary
The `get_transaction_iter()` function creates database iterators without synchronization against concurrent pruning operations. When transactions are pruned after the iterator is created but before all data is consumed, the iterator encounters missing versions and fails with a continuity check error, causing state sync failures, API crashes, and node degradation.

## Finding Description

The vulnerability exists in the transaction iterator implementation and its interaction with the pruning subsystem: [1](#0-0) 

The iterator is created without any lock or snapshot protection. Before creating the iterator, there is a check to ensure the start version hasn't been pruned: [2](#0-1) 

However, this check happens BEFORE the iterator is created, creating a TOCTOU (Time-of-Check-Time-of-Use) race condition. The iterator wraps a `ContinuousVersionIter` that enforces version continuity: [3](#0-2) 

When the pruner deletes transactions concurrently: [4](#0-3) 

The race condition unfolds as follows:

1. Thread A checks `error_if_ledger_pruned(start_version=100)` - passes because `min_readable_version=100`
2. Thread A creates iterator seeking to version 100
3. Thread A begins consuming iterator (reads versions 100, 101, 102...)
4. Thread B calls `prune_transactions(100, 600)` and deletes TransactionSchema entries
5. Thread A tries to read version 300 but RocksDB iterator skips deleted keys
6. `ContinuousVersionIter` expects version 300, gets version 600 instead
7. Iterator returns error: "Transaction iterator: first version 100, expecting version 300, got 600 from underlying iterator"

This failure propagates to critical components. State sync uses these iterators: [5](#0-4) 

When the iterator fails, the error propagates: [6](#0-5) 

The pruner runs asynchronously without coordination with active iterators: [7](#0-6) 

There is no read-write lock mechanism protecting iterators from concurrent pruning - only atomic version tracking for min_readable_version: [8](#0-7) 

## Impact Explanation

This issue qualifies as **High Severity** per Aptos bug bounty criteria:

- **API crashes**: Transaction iterators used by JSON-RPC APIs will fail with continuity errors, causing API endpoints to return 500 errors
- **Validator node slowdowns**: State sync failures force nodes to repeatedly retry synchronization, increasing CPU and I/O load
- **Significant protocol violations**: Breaks the State Consistency invariant that iterators must provide consistent snapshots of data

The impact affects multiple critical subsystems:
- State synchronization cannot fetch transaction batches
- Backup services fail during transaction export
- Block indexers cannot process historical data
- API queries for transaction ranges return errors

While this doesn't directly cause consensus failures or fund loss, it severely degrades node availability and reliability, which are core requirements for validator operations.

## Likelihood Explanation

**Likelihood: Medium to High**

The race condition occurs when:
- Pruning is enabled (common on production nodes to manage disk space)
- Iterator consumption spans multiple milliseconds (typical for large batches)
- Pruner processes versions that overlap with active iterator ranges

On a busy validator node with aggressive pruning settings (small prune_window, frequent pruning), this race can occur regularly. The window is largest for:
- Long-running state sync operations fetching 1000+ transactions
- Backup services iterating over large version ranges
- API clients requesting historical transaction ranges near pruning boundaries

The pruner runs asynchronously in a background thread without knowledge of active iterators, making the race unavoidable under normal operation.

## Recommendation

Implement snapshot-based iterators that hold references to the data throughout their lifetime. The fix requires two changes:

**Option 1: Reference-Counted Snapshots**
```rust
pub(crate) fn get_transaction_iter(
    &self,
    start_version: Version,
    num_transactions: usize,
) -> Result<impl Iterator<Item = Result<Transaction>> + '_> {
    // Create read options with explicit snapshot
    let mut read_opts = ReadOptions::default();
    let snapshot = self.db.snapshot();
    read_opts.set_snapshot(&snapshot);
    
    let mut iter = self.db.iter_with_opts::<TransactionSchema>(read_opts)?;
    iter.seek(&start_version)?;
    Ok(iter.expect_continuous_versions(start_version, num_transactions)?
        .snapshot_protected(snapshot)) // Keep snapshot alive
}
```

**Option 2: Read-Write Lock Protection**
```rust
// In LedgerPrunerManager
pub struct LedgerPrunerManager {
    // ... existing fields ...
    active_iterators: Arc<AtomicUsize>,
}

// Before creating iterator
pub(crate) fn get_transaction_iter(...) -> Result<...> {
    let guard = IteratorGuard::new(&self.ledger_pruner.active_iterators);
    // ... create iterator ...
    Ok(GuardedIterator::new(iter, guard))
}

// Pruner checks before pruning
fn prune(&self, ...) -> Result<Version> {
    while self.active_iterators.load(Ordering::SeqCst) > 0 {
        std::thread::sleep(Duration::from_millis(100));
    }
    // ... proceed with pruning ...
}
```

**Recommended Approach**: Option 1 (snapshot-based) is preferred as it leverages RocksDB's built-in snapshot isolation without requiring complex synchronization logic.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;

    #[test]
    fn test_iterator_invalidation_during_pruning() {
        // Setup: Create AptosDB with transactions 0-1000
        let tmpdir = tempfile::tempdir().unwrap();
        let db = Arc::new(AptosDB::new_for_test(&tmpdir));
        
        // Commit 1000 transactions
        for version in 0..1000 {
            let txn = create_test_transaction(version);
            db.save_transactions(&[txn], version, None).unwrap();
        }
        
        let db_clone = Arc::clone(&db);
        
        // Thread A: Create iterator and start consuming slowly
        let iterator_thread = thread::spawn(move || {
            let iter = db_clone
                .ledger_db
                .transaction_db()
                .get_transaction_iter(100, 500)
                .unwrap();
            
            let mut count = 0;
            for result in iter {
                count += 1;
                // Simulate slow consumption
                thread::sleep(Duration::from_millis(1));
                
                if result.is_err() {
                    // Expected: Iterator fails when pruning happens concurrently
                    println!("Iterator failed at count {}: {:?}", count, result);
                    assert!(result.unwrap_err().to_string().contains("expecting version"));
                    return true; // Vulnerability confirmed
                }
            }
            false // Did not fail (race didn't occur)
        });
        
        // Thread B: Prune transactions 100-400 after a short delay
        thread::sleep(Duration::from_millis(50));
        let mut batch = SchemaBatch::new();
        db.ledger_db
            .transaction_db()
            .prune_transactions(100, 400, &mut batch)
            .unwrap();
        db.ledger_db.transaction_db().write_schemas(batch).unwrap();
        
        // Verify iterator failed due to race condition
        let failed = iterator_thread.join().unwrap();
        assert!(failed, "Iterator should have failed during concurrent pruning");
    }
}
```

## Notes

This vulnerability demonstrates a fundamental design flaw in the storage layer's iterator implementation. While RocksDB provides snapshot isolation mechanisms, the current implementation doesn't utilize them, allowing concurrent modifications to invalidate active iterators. The fix requires architectural changes to ensure iterators maintain consistent views of the database throughout their lifetime, similar to MVCC (Multi-Version Concurrency Control) systems.

The issue is exacerbated by the asynchronous nature of the pruner, which operates independently without awareness of ongoing read operations. This makes the race condition non-deterministic but increasingly likely as pruning frequency increases or iterator consumption time grows.

### Citations

**File:** storage/aptosdb/src/ledger_db/transaction_db.rs (L63-71)
```rust
    pub(crate) fn get_transaction_iter(
        &self,
        start_version: Version,
        num_transactions: usize,
    ) -> Result<impl Iterator<Item = Result<Transaction>> + '_> {
        let mut iter = self.db.iter::<TransactionSchema>()?;
        iter.seek(&start_version)?;
        iter.expect_continuous_versions(start_version, num_transactions)
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_db.rs (L169-179)
```rust
    pub(crate) fn prune_transactions(
        &self,
        begin: Version,
        end: Version,
        db_batch: &mut SchemaBatch,
    ) -> Result<()> {
        for version in begin..end {
            db_batch.delete::<TransactionSchema>(&version)?;
        }
        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L261-271)
```rust
    pub(super) fn error_if_ledger_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.ledger_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/utils/iterators.rs (L40-62)
```rust
    fn next_impl(&mut self) -> Result<Option<T>> {
        if self.expected_next_version >= self.end_version {
            return Ok(None);
        }

        let ret = match self.inner.next().transpose()? {
            Some((version, transaction)) => {
                ensure!(
                    version == self.expected_next_version,
                    "{} iterator: first version {}, expecting version {}, got {} from underlying iterator.",
                    std::any::type_name::<T>(),
                    self.first_version,
                    self.expected_next_version,
                    version,
                );
                self.expected_next_version += 1;
                Some(transaction)
            },
            None => None,
        };

        Ok(ret)
    }
```

**File:** state-sync/storage-service/server/src/storage.rs (L374-401)
```rust
        let transaction_iterator = self
            .storage
            .get_transaction_iterator(start_version, num_transactions_to_fetch)?;
        let transaction_info_iterator = self
            .storage
            .get_transaction_info_iterator(start_version, num_transactions_to_fetch)?;
        let transaction_events_iterator = if include_events {
            self.storage
                .get_events_iterator(start_version, num_transactions_to_fetch)?
        } else {
            // If events are not included, create a fake iterator (they will be dropped anyway)
            Box::new(std::iter::repeat_n(
                Ok(vec![]),
                num_transactions_to_fetch as usize,
            ))
        };
        let persisted_auxiliary_info_iterator =
            self.storage.get_persisted_auxiliary_info_iterator(
                start_version,
                num_transactions_to_fetch as usize,
            )?;

        let mut multizip_iterator = itertools::multizip((
            transaction_iterator,
            transaction_info_iterator,
            transaction_events_iterator,
            persisted_auxiliary_info_iterator,
        ));
```

**File:** state-sync/storage-service/server/src/storage.rs (L451-456)
```rust
                Some((Err(error), _, _, _))
                | Some((_, Err(error), _, _))
                | Some((_, _, Err(error), _))
                | Some((_, _, _, Err(error))) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L48-50)
```rust
    fn get_min_readable_version(&self) -> Version {
        self.min_readable_version.load(Ordering::SeqCst)
    }
```
