# Audit Report

## Title
DKG Key Rotation Race Condition Causes Validator Decryption Failures

## Summary
A race condition exists in the Distributed Key Generation (DKG) system where validators can rotate their consensus keys after DKG captures the target validator set but before the new epoch begins. This causes the DKG transcript to contain secret shares encrypted with the validator's old key, while the validator attempts decryption with their new key, resulting in decryption failure and inability to participate in randomness generation.

## Finding Description

The vulnerability occurs due to improper synchronization between the DKG initialization process and the validator consensus key rotation mechanism.

**Key Rotation Mechanism:**
When a validator calls `rotate_consensus_key`, the `ValidatorConfig.consensus_pubkey` is immediately updated in on-chain storage, though the change only "takes effect" (used for consensus) in the next epoch. [1](#0-0) 

**DKG Initialization:**
When an epoch times out, the system calls `reconfiguration_with_dkg::try_start()` to initiate DKG for the next epoch: [2](#0-1) 

This function reads the target validator set by calling `stake::next_validator_consensus_infos()`: [3](#0-2) 

**The Race Window:**
The `next_validator_consensus_infos()` function reads the current on-chain state of `ValidatorConfig` to get each validator's consensus public key: [4](#0-3) [5](#0-4) 

These consensus public keys are then converted to DKG encryption keys: [6](#0-5) 

**The Vulnerability:**
1. DKG session starts, captures validator V's consensus key as `old_pk`
2. Validator V calls `rotate_consensus_key`, updating `ValidatorConfig.consensus_pubkey` to `new_pk`
3. DKG continues running, produces transcript with V's share encrypted using `old_pk`
4. Epoch transition occurs, V now uses `new_pk` for consensus
5. V attempts to decrypt their secret share using `new_pk`'s corresponding private key
6. Decryption fails because the share was encrypted with `old_pk` [7](#0-6) 

This breaks the **Cryptographic Correctness** invariant and can prevent validators from participating in randomness generation.

## Impact Explanation

**High Severity** - This qualifies as a "Significant protocol violation" per the Aptos bug bounty criteria.

**Direct Impacts:**
- Affected validators cannot decrypt their DKG secret shares
- These validators cannot participate in randomness generation for that epoch
- If enough validators rotate keys during DKG, the randomness threshold may not be met, causing total randomness failure

**Broader Consequences:**
- Randomness-dependent features (e.g., leader selection, random sampling) become unavailable
- Potential liveness impact if randomness is critical for epoch progression
- Validators may be unfairly penalized for non-participation despite following correct procedures

The severity is heightened because:
1. The race window is large (entire duration between DKG start and epoch transition)
2. Key rotation is a legitimate and necessary operational task
3. Validators have no way to detect if they're in the vulnerable window
4. The impact scales with the number of validators performing rotations

## Likelihood Explanation

**Likelihood: Medium to High**

This issue is likely to occur in production because:

1. **Common Operation:** Validators regularly rotate consensus keys for security best practices (key hygiene, suspected compromise, operational transfers)

2. **Long Race Window:** DKG sessions run for the duration between epoch timeout detection and epoch transition, typically multiple blocks or even minutes, providing ample time for the race condition to manifest

3. **No Protection Mechanism:** There is no check preventing `rotate_consensus_key` from being called during an active DKG session. The `assert_reconfig_not_in_progress()` check only prevents rotation during the final reconfiguration phase, not during DKG execution: [8](#0-7) 

4. **Coordinated Rotations:** During security incidents or planned maintenance windows, multiple validators might rotate keys simultaneously, dramatically increasing the probability of threshold failures

5. **No Warning:** Validators receive no indication that a DKG session is in progress when they initiate key rotation

## Recommendation

Implement a DKG-aware key rotation lock mechanism:

**Solution 1: Block key rotation during active DKG**
Add a check in `rotate_consensus_key` to prevent rotation when DKG is in progress:

```move
public entry fun rotate_consensus_key(
    operator: &signer,
    pool_address: address,
    new_consensus_pubkey: vector<u8>,
    proof_of_possession: vector<u8>,
) acquires StakePool, ValidatorConfig {
    check_stake_permission(operator);
    assert_reconfig_not_in_progress();
    
    // NEW: Check if DKG is in progress
    assert!(!dkg::is_in_progress(), error::invalid_state(EDKG_IN_PROGRESS));
    
    // ... rest of function
}
```

**Solution 2: Use pending key rotation queue**
Store pending key rotations in a queue that only gets applied after DKG completion:

```move
struct PendingKeyRotation has store {
    new_consensus_pubkey: vector<u8>,
    proof_of_possession: vector<u8>,
}

// Store pending rotations per validator
struct PendingRotations has key {
    rotations: SimpleMap<address, PendingKeyRotation>
}

// In rotate_consensus_key: if DKG active, queue the rotation
// In dkg::finish or reconfiguration_with_dkg::finish: apply pending rotations
```

**Solution 3: Re-read keys at DKG completion**
When finalizing the DKG transcript, re-read the current validator consensus keys and validate that they match what was used for encryption. If mismatches are detected, either abort the DKG or re-encrypt affected shares.

**Recommended Approach:** Solution 1 is simplest and safest. It prevents the race condition entirely by blocking the problematic operation during the critical window.

## Proof of Concept

```move
#[test]
fun test_key_rotation_during_dkg() {
    // Setup: Initialize framework and validator
    let (aptos_framework, validator_account) = setup_test();
    let validator_addr = signer::address_of(&validator_account);
    
    // Generate initial validator keys
    let (sk_old, pk_old, pop_old) = generate_bls_keys();
    initialize_validator(&validator_account, pk_old, pop_old);
    
    // Start epoch and join validator set
    stake::join_validator_set(&validator_account, validator_addr);
    advance_epoch();
    
    // Trigger DKG for next epoch (simulated)
    // In real scenario: wait for epoch interval timeout
    let cur_epoch = reconfiguration::current_epoch();
    reconfiguration_with_dkg::try_start(); // Captures validator set with pk_old
    
    // Get the DKG session metadata - it should have pk_old for our validator
    let dkg_session = dkg::incomplete_session();
    assert!(option::is_some(&dkg_session), 0);
    
    // NOW: Validator rotates key while DKG is in progress
    let (sk_new, pk_new, pop_new) = generate_bls_keys();
    stake::rotate_consensus_key(
        &validator_account,
        validator_addr,
        bls12381::public_key_to_bytes(&pk_new),
        bls12381::proof_of_possession_to_bytes(&pop_new)
    );
    
    // DKG completes (validators produce transcript with pk_old encryption)
    let transcript = simulate_dkg_transcript_generation(); // Would use pk_old
    dkg::finish(transcript);
    
    // Epoch transition happens
    reconfiguration_with_dkg::finish(&aptos_framework);
    advance_epoch();
    
    // Validator attempts to decrypt their share in new epoch
    // This should FAIL because:
    // - Transcript has share encrypted with pk_old
    // - Validator now has sk_new (private key for pk_new)
    // - Decryption: decrypt(sk_new, ciphertext_encrypted_with_pk_old) = FAILURE
    
    let result = try_decrypt_dkg_share(&validator_account);
    assert!(result.is_err(), 1); // Decryption should fail
}
```

**Notes:**
The issue affects validators immediately upon key rotation during active DKG sessions. The freeze mechanism used in confidential assets does not exist for DKG key management, leaving the system vulnerable to this race condition during normal operational activities.

### Citations

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L910-932)
```text
    public entry fun rotate_consensus_key(
        operator: &signer,
        pool_address: address,
        new_consensus_pubkey: vector<u8>,
        proof_of_possession: vector<u8>,
    ) acquires StakePool, ValidatorConfig {
        check_stake_permission(operator);
        assert_reconfig_not_in_progress();
        assert_stake_pool_exists(pool_address);

        let stake_pool = borrow_global_mut<StakePool>(pool_address);
        assert!(signer::address_of(operator) == stake_pool.operator_address, error::unauthenticated(ENOT_OPERATOR));

        assert!(exists<ValidatorConfig>(pool_address), error::not_found(EVALIDATOR_CONFIG));
        let validator_info = borrow_global_mut<ValidatorConfig>(pool_address);
        let old_consensus_pubkey = validator_info.consensus_pubkey;
        // Checks the public key has a valid proof-of-possession to prevent rogue-key attacks.
        let pubkey_from_pop = &bls12381::public_key_from_bytes_with_pop(
            new_consensus_pubkey,
            &proof_of_possession_from_bytes(proof_of_possession)
        );
        assert!(option::is_some(pubkey_from_pop), error::invalid_argument(EINVALID_PUBLIC_KEY));
        validator_info.consensus_pubkey = new_consensus_pubkey;
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L1540-1540)
```text
                let config = *borrow_global<ValidatorConfig>(candidate.addr);
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L1608-1612)
```text
            *vci = validator_consensus_info::new(
                vi.addr,
                vi.config.consensus_pubkey,
                vi.voting_power
            );
```

**File:** aptos-move/framework/aptos-framework/sources/block.move (L244-246)
```text
        if (timestamp - reconfiguration::last_reconfiguration_time() >= epoch_interval) {
            reconfiguration_with_dkg::try_start();
        };
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration_with_dkg.move (L34-39)
```text
        dkg::start(
            cur_epoch,
            randomness_config::current(),
            stake::cur_validator_consensus_infos(),
            stake::next_validator_consensus_infos(),
        );
```

**File:** types/src/dkg/real_dkg/mod.rs (L119-127)
```rust
    let validator_consensus_keys: Vec<bls12381::PublicKey> = next_validators
        .iter()
        .map(|vi| vi.public_key.clone())
        .collect();

    let consensus_keys: Vec<EncPK> = validator_consensus_keys
        .iter()
        .map(|k| k.to_bytes().as_slice().try_into().unwrap())
        .collect::<Vec<_>>();
```

**File:** consensus/src/epoch_manager.rs (L1054-1072)
```rust
        let dkg_decrypt_key = maybe_dk_from_bls_sk(consensus_key.as_ref())
            .map_err(NoRandomnessReason::ErrConvertingConsensusKeyToDecryptionKey)?;
        let transcript = bcs::from_bytes::<<DefaultDKG as DKGTrait>::Transcript>(
            dkg_session.transcript.as_slice(),
        )
        .map_err(NoRandomnessReason::TranscriptDeserializationError)?;

        let vuf_pp = WvufPP::from(&dkg_pub_params.pvss_config.pp);

        // No need to verify the transcript.

        // keys for randomness generation
        let (sk, pk) = DefaultDKG::decrypt_secret_share_from_transcript(
            &dkg_pub_params,
            &transcript,
            my_index as u64,
            &dkg_decrypt_key,
        )
        .map_err(NoRandomnessReason::SecretShareDecryptionFailed)?;
```
