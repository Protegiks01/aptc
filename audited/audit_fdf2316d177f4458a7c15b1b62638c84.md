# Audit Report

## Title
State Snapshot Restore Bypasses Merkle Proof Verification in KvOnly Mode, Allowing Database Corruption

## Summary
The state snapshot restore process fails to verify Merkle proofs when restoring in `KvOnly` mode, allowing an attacker who compromises backup storage to inject corrupted state into the database. This leads to state inconsistency where the Jellyfish Merkle tree and key-value store become desynchronized, potentially causing consensus splits across validator nodes.

## Finding Description

The vulnerability exists in the two-phase restore process used by the `RestoreCoordinator`. When restoring a database from backups, the system may restore a KV snapshot at an earlier version (V1) and a tree snapshot at a later version (V2). [1](#0-0) 

In Phase 1.a, the KV snapshot is restored using `StateSnapshotRestoreMode::KvOnly`. The critical flaw is in how `add_chunk` handles this mode: [2](#0-1) 

When `restore_mode` is `KvOnly`, only the `kv_fn()` is executed, which calls `StateValueRestore::add_chunk`. This function completely ignores the `proof` parameter: [3](#0-2) 

The proof is never passed to any verification function. In contrast, when `TreeOnly` or `Default` mode is used, `tree_fn()` calls `add_chunk_impl` which **does** verify the proof: [4](#0-3) 

The verification happens at line 391, ensuring the proof is valid before writing to storage. However, in `KvOnly` mode, this verification is completely bypassed.

**Attack Scenario:**

1. Attacker compromises backup storage and provides a malicious KV snapshot at version V1 with valid manifest but incomplete/forged Merkle proofs in chunk files

2. Victim runs restore with this backup. The manifest is verified (proving the expected root hash), but chunk proofs are not: [5](#0-4) 

3. Phase 1.a restores KV snapshot in `KvOnly` mode - **no proof verification occurs**, corrupted state is written to database

4. Phase 1.b replays transactions without execution verification: [6](#0-5) 

5. Phase 2.a restores tree snapshot in `TreeOnly` mode - proofs ARE verified for the tree, but this is independent of the already-corrupted KV

6. Result: Database contains verified tree structure but corrupted KV state derived from unverified data

The `SparseMerkleRangeProof::verify` implementation shows that proper verification would catch incomplete proofs: [7](#0-6) 

Missing siblings trigger errors at lines 801-803 and 809-811, but this verification is never called in `KvOnly` mode.

## Impact Explanation

This vulnerability meets **Critical Severity** criteria per the Aptos bug bounty program:

1. **Consensus/Safety violations**: If different validators restore from backups with varying levels of corruption, they will compute different state roots for the same transactions, causing consensus splits and potential chain forks.

2. **State Consistency**: Violates the fundamental invariant that "State transitions must be atomic and verifiable via Merkle proofs." The database ends up with state that was never cryptographically verified.

3. **Non-recoverable without intervention**: Once corrupted state enters the database, it persists through transaction replay. The only recovery is restoring from a different backup or rebuilding from genesis.

4. **Silent failure**: The restore completes successfully without detecting the corruption. The inconsistency is only discovered later when state is accessed and verified against the tree.

## Likelihood Explanation

**Moderate to High Likelihood:**

- **Attack Requirements**: Attacker must compromise backup storage or perform MITM on backup downloads. While this requires elevated access, backup systems are often less secured than live validator infrastructure.

- **Complexity**: Once backup access is achieved, the attack is straightforward - simply provide chunks with incomplete or malicious proofs. No complex cryptographic attacks needed.

- **Detection Difficulty**: The restore process provides no warning, and corruption may not be immediately apparent.

- **Real-world scenarios**: This is particularly concerning in disaster recovery scenarios where multiple validators simultaneously restore from backups that could have been tampered with.

## Recommendation

**Immediate Fix**: Always verify Merkle proofs regardless of restore mode. Modify `StateValueRestore::add_chunk` to accept and verify the proof:

```rust
// In storage/aptosdb/src/state_restore/mod.rs
impl<K: Key + CryptoHash + Eq + Hash, V: Value> StateValueRestore<K, V> {
    pub fn add_chunk(&mut self, mut chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
        // Load progress
        let progress_opt = self.db.get_progress(self.version)?;
        
        // Skip overlaps...
        // [existing skip logic]
        
        if chunk.is_empty() {
            return Ok(());
        }
        
        // **NEW: Verify the proof before writing**
        if let Some(last_entry) = chunk.last() {
            let last_key_hash = CryptoHash::hash(&last_entry.0);
            let last_value_hash = last_entry.1.hash();
            
            // Reconstruct left siblings from previously added state
            let left_siblings = self.compute_left_siblings(last_key_hash)?;
            
            proof.verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(last_key_hash, last_value_hash),
                left_siblings,
            )?;
        }
        
        // [existing save logic]
    }
}
```

**Alternative**: Remove `KvOnly` mode entirely and always use `Default` mode which verifies proofs through the tree restoration path. Update the restore coordinator to not separate KV and tree restoration phases.

**Long-term**: Implement a final consistency check that verifies the KV store matches the Merkle tree after restore completes.

## Proof of Concept

**Setup:**
1. Create a legitimate state snapshot backup at version V
2. Modify the backup by truncating a chunk's proof file (removing some right_siblings from the `SparseMerkleRangeProof`)
3. Keep the manifest intact (it will still have valid signatures)

**Execution:**
```bash
# Run restore with the corrupted backup
aptos-db-tool restore \
    --target-db-dir ./restored_db \
    --metadata-cache-dir ./metadata \
    --target-version V \
    backup_storage_location

# The restore will complete successfully despite the incomplete proof
# Verification:
# Query state at restored version and compute state root
# It will differ from the expected state root in the manifest
```

**Expected vs Actual Behavior:**

- **Expected**: Restore fails with "Missing right sibling" error during proof verification
- **Actual**: Restore succeeds, corrupted state is written to database, no error raised

The vulnerability can be confirmed by adding logging to `StateValueRestore::add_chunk` - the proof parameter is received but never used when in `KvOnly` mode.

---

**Notes:**

The vulnerability specifically affects the multi-phase restore workflow where `kv_snapshot.version < tree_snapshot.version`. In this scenario, the KV is restored without proof verification, making the database vulnerable to state corruption from compromised backups. This breaks the fundamental security guarantee that all state must be cryptographically verifiable via Merkle proofs.

### Citations

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L242-260)
```rust
            // phase 1.a: restore the kv snapshot
            if kv_snapshot.is_some() {
                let kv_snapshot = kv_snapshot.clone().unwrap();
                info!("Start restoring KV snapshot at {}", kv_snapshot.version);

                StateSnapshotRestoreController::new(
                    StateSnapshotRestoreOpt {
                        manifest_handle: kv_snapshot.manifest,
                        version: kv_snapshot.version,
                        validate_modules: false,
                        restore_mode: StateSnapshotRestoreMode::KvOnly,
                    },
                    self.global_opt.clone(),
                    Arc::clone(&self.storage),
                    epoch_history.clone(),
                )
                .run()
                .await?;
            }
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L289-300)
```rust
            TransactionRestoreBatchController::new(
                transaction_restore_opt,
                Arc::clone(&self.storage),
                txn_manifests,
                Some(db_next_version),
                Some((kv_replay_version, true /* only replay KV */)),
                epoch_history.clone(),
                VerifyExecutionMode::NoVerify,
                None,
            )
            .run()
            .await?;
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L88-127)
```rust
    pub fn add_chunk(&mut self, mut chunk: Vec<(K, V)>) -> Result<()> {
        // load progress
        let progress_opt = self.db.get_progress(self.version)?;

        // skip overlaps
        if let Some(progress) = progress_opt {
            let idx = chunk
                .iter()
                .position(|(k, _v)| CryptoHash::hash(k) > progress.key_hash)
                .unwrap_or(chunk.len());
            chunk = chunk.split_off(idx);
        }

        // quit if all skipped
        if chunk.is_empty() {
            return Ok(());
        }

        // save
        let mut usage = progress_opt.map_or(StateStorageUsage::zero(), |p| p.usage);
        let (last_key, _last_value) = chunk.last().unwrap();
        let last_key_hash = CryptoHash::hash(last_key);

        // In case of TreeOnly Restore, we only restore the usage of KV without actually writing KV into DB
        for (k, v) in chunk.iter() {
            usage.add_item(k.key_size() + v.value_size());
        }

        // prepare the sharded kv batch
        let kv_batch: StateValueBatch<K, Option<V>> = chunk
            .into_iter()
            .map(|(k, v)| ((k, self.version), Some(v)))
            .collect();

        self.db.write_kv_batch(
            self.version,
            &kv_batch,
            StateSnapshotProgress::new(last_key_hash, usage),
        )
    }
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L228-258)
```rust
    fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
        let kv_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_add_chunk"]);
            self.kv_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk(chunk.clone())
        };

        let tree_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["jmt_add_chunk"]);
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
        };
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => kv_fn()?,
            StateSnapshotRestoreMode::TreeOnly => tree_fn()?,
            StateSnapshotRestoreMode::Default => {
                // We run kv_fn with TreeOnly to restore the usage of DB
                let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
                r1?;
                r2?;
            },
        }

        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L339-412)
```rust
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }

        Ok(())
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-136)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
```

**File:** types/src/proof/definition.rs (L782-826)
```rust
    pub fn verify(
        &self,
        expected_root_hash: HashValue,
        rightmost_known_leaf: SparseMerkleLeafNode,
        left_siblings: Vec<HashValue>,
    ) -> Result<()> {
        let num_siblings = left_siblings.len() + self.right_siblings.len();
        let mut left_sibling_iter = left_siblings.iter();
        let mut right_sibling_iter = self.right_siblings().iter();

        let mut current_hash = rightmost_known_leaf.hash();
        for bit in rightmost_known_leaf
            .key()
            .iter_bits()
            .rev()
            .skip(HashValue::LENGTH_IN_BITS - num_siblings)
        {
            let (left_hash, right_hash) = if bit {
                (
                    *left_sibling_iter
                        .next()
                        .ok_or_else(|| format_err!("Missing left sibling."))?,
                    current_hash,
                )
            } else {
                (
                    current_hash,
                    *right_sibling_iter
                        .next()
                        .ok_or_else(|| format_err!("Missing right sibling."))?,
                )
            };
            current_hash = SparseMerkleInternalNode::new(left_hash, right_hash).hash();
        }

        ensure!(
            current_hash == expected_root_hash,
            "{}: Root hashes do not match. Actual root hash: {:x}. Expected root hash: {:x}.",
            type_name::<Self>(),
            current_hash,
            expected_root_hash,
        );

        Ok(())
    }
```
