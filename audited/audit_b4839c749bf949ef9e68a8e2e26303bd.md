# Audit Report

## Title
Network Layer Rate Limiting Not Implemented Despite Configuration - DirectSend Message Flooding Attack

## Summary
The Aptos network layer exposes configuration for `inbound_rate_limit_config` that is intended to rate-limit incoming network connections, but this rate limiting is never actually implemented. The `aptos-rate-limiter` crate exists with `AsyncRateLimiter` functionality, but the `aptos-network` framework does not import or use it. This allows attackers to flood validators with unlimited DirectSend messages, exhausting CPU resources through message deserialization and processing.

## Finding Description
The vulnerability exists at the integration layer between the rate limiting configuration and the actual network message processing pipeline.

**Configuration Layer**: [1](#0-0) 

The `NetworkConfig` struct defines `inbound_rate_limit_config: Option<RateLimitConfig>` which administrators can configure in production deployments: [2](#0-1) 

**Rate Limiter Implementation**: [3](#0-2) 

The `AsyncRateLimiter<T>` wrapper exists and can throttle AsyncRead/AsyncWrite operations using token bucket rate limiting.

**Critical Gap**: [4](#0-3) 

The `aptos-network` crate does NOT include `aptos-rate-limiter` as a dependency. There are zero imports or uses of the rate limiter in the network framework.

**Message Processing Path**: [5](#0-4) 

When DirectSend messages arrive, `handle_inbound_network_message` deserializes them from the wire format, extracts `protocol_id` via the accessor, and pushes to upstream handlers. This happens for EVERY message received, with no network-level rate limiting applied to the socket itself.

**Attack Vector**:
1. Attacker establishes connection(s) to validator nodes
2. Floods DirectSend messages over the wire
3. Each message undergoes BCS deserialization: [6](#0-5) 
4. CPU is consumed for deserialization, protocol_id() calls, and handler lookups
5. Even if the application-level channel queue fills up (max_capacity), the socket read and deserialization overhead continues to consume CPU

## Impact Explanation
This qualifies as **High Severity** per Aptos bug bounty criteria because it enables "Validator node slowdowns" through resource exhaustion. 

Validators processing consensus messages would experience:
- CPU exhaustion from deserializing flood of DirectSend messages
- Degraded performance affecting consensus participation
- Potential missed block proposals or vote timeouts
- Network bandwidth consumption without application-level throttling

While there is connection-level limiting (`max_inbound_connections`), once connections are established, no byte-rate limiting exists despite the configuration suggesting it should.

## Likelihood Explanation
**Likelihood: HIGH**

- Attack requires no special privileges - any network peer can send DirectSend messages
- Multiple protocol IDs exist (consensus, mempool, etc.) that accept DirectSend messages
- Configuration exists in production deployments, creating false sense of security
- Operators believe rate limiting is active when it is not
- Attack is trivial to execute with network tools
- No authentication required beyond initial connection handshake

## Recommendation
Integrate the `aptos-rate-limiter` crate into the network framework:

1. **Add dependency** to `network/framework/Cargo.toml`:
```toml
aptos-rate-limiter = { workspace = true }
```

2. **Wrap sockets with AsyncRateLimiter** when creating connections in transport upgrade functions. When `NetworkConfig.inbound_rate_limit_config` is set, create `SharedBucket` from config and wrap socket with `AsyncRateLimiter::new(socket, Some(bucket))` before passing to protocol handlers.

3. **Apply in connection upgrade path**: Modify `network/framework/src/transport/mod.rs` to check `inbound_rate_limit_config`, construct rate limiter, and wrap the negotiated socket before returning the `Connection<TSocket>`.

4. **Implement metrics**: Ensure rate limiting metrics are recorded via the existing counters: [7](#0-6) 

## Proof of Concept
```rust
// Reproduction steps:
// 1. Deploy validator with inbound_rate_limit_config set
// 2. Run this attack client:

use tokio::net::TcpStream;
use futures::SinkExt;
use aptos_network::protocols::wire::messaging::v1::{DirectSendMsg, NetworkMessage};

#[tokio::main]
async fn main() {
    let target = "validator.example.com:6180";
    let socket = TcpStream::connect(target).await.unwrap();
    
    // Complete handshake (omitted for brevity)
    // ...
    
    // Flood DirectSend messages
    for _ in 0..1_000_000 {
        let msg = NetworkMessage::DirectSendMsg(DirectSendMsg {
            protocol_id: ProtocolId::ConsensusDirectSendBcs,
            priority: 0,
            raw_msg: vec![0u8; 1024], // 1KB garbage
        });
        
        // Send without any rate limiting on attacker side
        sink.send(&MultiplexMessage::Message(msg)).await.unwrap();
    }
    
    // Observe: Validator CPU spikes, rate limiter config has no effect
}
```

**Validation**: Monitor validator CPU during attack. Despite `inbound_rate_limit_config` being set to 100KB/s, the validator will process messages at full socket bandwidth, consuming CPU for deserialization of every message until the application-level channel fills up (but socket processing continues).

## Notes
- Channel-level backpressure exists via `aptos_channel::Config` max_capacity but this only prevents memory exhaustion, not CPU exhaustion from deserialization
- The `RateLimitConfig` struct is properly defined with `ip_byte_bucket_rate` and `ip_byte_bucket_size` fields: [8](#0-7) 
- HAProxy configurations provide some external rate limiting but cannot protect against attacks from already-connected peers
- This is an integration vulnerability, not an implementation bug - all components exist but are not connected

### Citations

**File:** config/src/config/network_config.rs (L117-119)
```rust
    pub inbound_rate_limit_config: Option<RateLimitConfig>,
    /// Outbound rate limiting configuration, if not specified, no rate limiting
    pub outbound_rate_limit_config: Option<RateLimitConfig>,
```

**File:** config/src/config/network_config.rs (L368-388)
```rust
pub struct RateLimitConfig {
    /// Maximum number of bytes/s for an IP
    pub ip_byte_bucket_rate: usize,
    /// Maximum burst of bytes for an IP
    pub ip_byte_bucket_size: usize,
    /// Initial amount of tokens initially in the bucket
    pub initial_bucket_fill_percentage: u8,
    /// Allow for disabling the throttles
    pub enabled: bool,
}

impl Default for RateLimitConfig {
    fn default() -> Self {
        Self {
            ip_byte_bucket_rate: IP_BYTE_BUCKET_RATE,
            ip_byte_bucket_size: IP_BYTE_BUCKET_SIZE,
            initial_bucket_fill_percentage: 25,
            enabled: true,
        }
    }
}
```

**File:** terraform/helm/fullnode/files/fullnode-base.yaml (L36-44)
```yaml
  # be backwards compatible with the old inbound_rate_limit_config config
  {{- if $.Values.fullnode_inbound_rate_limit }}
  inbound_rate_limit_config:
    {{- $.Values.fullnode_inbound_rate_limit | toYaml | nindent 6 }}
  {{- else}}
  {{ with .inbound_rate_limit_config }}
  inbound_rate_limit_config: {{ . }}
  {{ end }}
  {{- end }}
```

**File:** crates/aptos-rate-limiter/src/async_lib.rs (L86-101)
```rust
pub struct AsyncRateLimiter<T> {
    #[pin]
    inner: T,
    rate_limiter: PollRateLimiter,
}

impl<T> AsyncRateLimiter<T> {
    pub fn new(inner: T, bucket: Option<SharedBucket>) -> Self {
        Self {
            inner,
            rate_limiter: PollRateLimiter::new(bucket),
        }
    }
}

impl<T: AsyncRead> AsyncRead for AsyncRateLimiter<T> {
```

**File:** network/framework/Cargo.toml (L15-61)
```text
[dependencies]
anyhow = { workspace = true }
aptos-bitvec = { workspace = true }
aptos-channels = { workspace = true }
aptos-compression = { workspace = true }
aptos-config = { workspace = true }
aptos-crypto = { workspace = true }
aptos-id-generator = { workspace = true }
aptos-infallible = { workspace = true }
aptos-logger = { workspace = true }
aptos-memsocket = { workspace = true, optional = true }
aptos-metrics-core = { workspace = true }
aptos-netcore = { workspace = true }
aptos-num-variants = { workspace = true }
aptos-peer-monitoring-service-types = { workspace = true }
aptos-proptest-helpers = { workspace = true, optional = true }
aptos-short-hex-str = { workspace = true }
aptos-time-service = { workspace = true }
aptos-types = { workspace = true }
arc-swap = { workspace = true }
async-trait = { workspace = true }
bcs = { workspace = true }
bytes = { workspace = true }
futures = { workspace = true }
futures-util = { workspace = true }
hex = { workspace = true }
itertools = { workspace = true }
maplit = { workspace = true }
once_cell = { workspace = true }
ordered-float = { workspace = true }
pin-project = { workspace = true }
proptest = { workspace = true, optional = true }
proptest-derive = { workspace = true, optional = true }
rand = { workspace = true, features = ["small_rng"] }
# Note: we cannot rely on the workspace version of rand. So we use this workaround. See:
# https://github.com/aptos-labs/aptos-core/blob/main/state-sync/aptos-data-client/Cargo.toml#L41.
# See also https://github.com/aptos-labs/aptos-core/issues/13031
rand_latest = { package = "rand", version = "0.8.5" }
serde = { workspace = true }
serde_bytes = { workspace = true }
serde_json = { workspace = true }
thiserror = { workspace = true }
tokio = { workspace = true }
tokio-retry = { workspace = true }
tokio-stream = { workspace = true }
tokio-util = { workspace = true }

```

**File:** network/framework/src/peer/mod.rs (L447-493)
```rust
    fn handle_inbound_network_message(
        &mut self,
        message: NetworkMessage,
    ) -> Result<(), PeerManagerError> {
        match &message {
            NetworkMessage::DirectSendMsg(direct) => {
                let data_len = direct.raw_msg.len();
                network_application_inbound_traffic(
                    self.network_context,
                    direct.protocol_id,
                    data_len as u64,
                );
                match self.upstream_handlers.get(&direct.protocol_id) {
                    None => {
                        counters::direct_send_messages(&self.network_context, UNKNOWN_LABEL).inc();
                        counters::direct_send_bytes(&self.network_context, UNKNOWN_LABEL)
                            .inc_by(data_len as u64);
                    },
                    Some(handler) => {
                        let key = (self.connection_metadata.remote_peer_id, direct.protocol_id);
                        let sender = self.connection_metadata.remote_peer_id;
                        let network_id = self.network_context.network_id();
                        let sender = PeerNetworkId::new(network_id, sender);
                        match handler.push(key, ReceivedMessage::new(message, sender)) {
                            Err(_err) => {
                                // NOTE: aptos_channel never returns other than Ok(()), but we might switch to tokio::sync::mpsc and then this would work
                                counters::direct_send_messages(
                                    &self.network_context,
                                    DECLINED_LABEL,
                                )
                                .inc();
                                counters::direct_send_bytes(&self.network_context, DECLINED_LABEL)
                                    .inc_by(data_len as u64);
                            },
                            Ok(_) => {
                                counters::direct_send_messages(
                                    &self.network_context,
                                    RECEIVED_LABEL,
                                )
                                .inc();
                                counters::direct_send_bytes(&self.network_context, RECEIVED_LABEL)
                                    .inc_by(data_len as u64);
                            },
                        }
                    },
                }
            },
```

**File:** network/framework/src/protocols/wire/messaging/v1/mod.rs (L225-241)
```rust
    fn poll_next(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        match self.project().framed_read.poll_next(cx) {
            Poll::Ready(Some(Ok(frame))) => {
                let frame = frame.freeze();

                match bcs::from_bytes(&frame) {
                    Ok(message) => Poll::Ready(Some(Ok(message))),
                    // Failed to deserialize the NetworkMessage
                    Err(err) => {
                        let mut frame = frame;
                        let frame_len = frame.len();
                        // Keep a few bytes from the frame for debugging
                        frame.truncate(8);
                        let err = ReadError::DeserializeError(err, frame_len, frame);
                        Poll::Ready(Some(Err(err)))
                    },
                }
```

**File:** network/framework/src/counters.rs (L543-550)
```rust
pub static NETWORK_RATE_LIMIT_METRICS: Lazy<HistogramVec> = Lazy::new(|| {
    register_histogram_vec!(
        "aptos_network_rate_limit",
        "Network Rate Limiting Metrics",
        &["direction", "metric"]
    )
    .unwrap()
});
```
