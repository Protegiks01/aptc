# Audit Report

## Title
Cache Stampede Race Condition in Node-Checker OutputCache Causes Redundant API Requests and Potential Target Node Overload

## Summary
The `OutputCache::get()` method in the node-checker's provider system contains a Time-of-Check-Time-of-Use (TOCTOU) race condition that allows multiple concurrent checkers to bypass the cache and execute redundant HTTP requests to the target node, potentially causing API overload and false health check failures.

## Finding Description

The node-checker runs multiple health checkers concurrently using `futures::future::try_join_all()`. [1](#0-0) 

All checkers share the same `ProviderCollection` containing providers with Arc-wrapped `OutputCache` instances. [2](#0-1) 

The `OutputCache::get()` method implements a cache to prevent redundant API calls, but contains a critical race condition: [3](#0-2) 

The vulnerability occurs because:

1. **Lines 40-44**: Multiple concurrent threads check if the cache is stale using read locks
2. All threads see the cache is stale and release their read locks
3. **Lines 48-52**: Each thread sequentially acquires write locks and executes `func.await` **without re-checking if another thread already updated the cache**

This violates the double-checked locking pattern. When 4+ checkers run concurrently (ConsensusProposalsChecker, ConsensusRoundChecker, ConsensusTimeoutsChecker, MinimumPeersChecker), each executes the expensive HTTP request independently: [4](#0-3) [5](#0-4) 

**Attack Scenario:**
1. Attacker sends health check request to node-checker service
2. SyncRunner spawns 4+ concurrent checkers, all sharing the same target_metrics_provider
3. All checkers call `provide()` simultaneously on cold cache
4. Race condition triggers 4+ HTTP GET requests to target node's `/metrics` endpoint instead of 1
5. Target node experiences 4-8x API load amplification
6. May trigger rate limiting, connection pool exhaustion, or API timeouts
7. Node-checker incorrectly marks healthy node as unhealthy

## Impact Explanation

**High Severity** - This vulnerability can cause:

1. **Validator Node Slowdowns**: If checking a validator node under load, 4-8x amplification of `/metrics` requests can degrade performance, fitting the "Validator node slowdowns" category from the bug bounty program.

2. **API Crashes**: Nodes without proper rate limiting may crash or become unresponsive when hit with amplified concurrent requests, fitting the "API crashes" category.

3. **False Health Check Failures**: The amplified load can cause legitimate timeouts, leading to incorrectly marking healthy nodes as unhealthy, affecting operational integrity.

## Likelihood Explanation

**Very High** - This occurs on every health check request when:
- Cache is cold or expired (default TTL from config)
- Multiple checkers run concurrently (always true in SyncRunner)
- No special attacker privileges required
- Triggered by normal node-checker operations

The race window is significant because HTTP requests to `/metrics` endpoints typically take 10-100ms, allowing multiple threads to enter the race condition simultaneously.

## Recommendation

Implement proper double-checked locking by re-validating cache state after acquiring write locks:

```rust
pub async fn get(
    &self,
    func: impl Future<Output = Result<T, ProviderError>>,
) -> Result<T, ProviderError> {
    // First check with read lock
    if self.last_run.read().await.elapsed() < self.cache_ttl {
        if let Some(last_output) = &*self.last_output.read().await {
            return Ok(last_output.clone());
        }
    }

    // Acquire write locks
    let mut last_output = self.last_output.write().await;
    let mut last_run = self.last_run.write().await;
    
    // CRITICAL FIX: Re-check if another thread updated the cache
    if last_run.elapsed() < self.cache_ttl {
        if let Some(cached) = &*last_output {
            return Ok(cached.clone());
        }
    }
    
    // Only fetch if still stale
    let new_output = func.await?;
    *last_output = Some(new_output.clone());
    *last_run = Instant::now();
    Ok(new_output)
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic::{AtomicU32, Ordering};
    use std::sync::Arc;
    use tokio::time::Duration;

    #[tokio::test]
    async fn test_cache_stampede_race_condition() {
        let fetch_count = Arc::new(AtomicU32::new(0));
        let cache = Arc::new(OutputCache::new(Duration::from_secs(10)));
        
        // Simulate 4 concurrent checkers
        let mut handles = vec![];
        for _ in 0..4 {
            let cache_clone = cache.clone();
            let count_clone = fetch_count.clone();
            
            handles.push(tokio::spawn(async move {
                cache_clone.get(async move {
                    // Simulate expensive HTTP request
                    count_clone.fetch_add(1, Ordering::SeqCst);
                    tokio::time::sleep(Duration::from_millis(50)).await;
                    Ok::<_, ProviderError>(42)
                }).await
            }));
        }
        
        // Wait for all checkers
        for handle in handles {
            handle.await.unwrap().unwrap();
        }
        
        // BUG: All 4 checkers execute the fetch function
        // Expected: 1, Actual: 4
        assert_eq!(fetch_count.load(Ordering::SeqCst), 4, 
            "Cache stampede: {} fetches instead of 1", 
            fetch_count.load(Ordering::SeqCst));
    }
}
```

**Notes:**
- This race condition specifically impacts the node-checker component, not core blockchain consensus
- While not directly compromising blockchain security, it affects operational reliability of node monitoring infrastructure
- The vulnerability is triggered automatically during normal operations, requiring no attacker action beyond sending health check requests
- The 4-8x amplification factor can overwhelm target nodes, especially validators under load
- Proper cache implementation would ensure only one HTTP request per TTL window regardless of concurrent checker count

### Citations

**File:** ecosystem/node-checker/src/runner/sync_runner.rs (L155-163)
```rust
        // Call each of the Checkers without awaiting them yet.
        let mut futures = Vec::new();
        for checker in &self.checkers {
            futures.push(self.call_check(checker, &provider_collection));
        }

        // Run all the Checkers concurrently and collect their results.
        let check_results: Vec<CheckResult> =
            try_join_all(futures).await?.into_iter().flatten().collect();
```

**File:** ecosystem/node-checker/src/provider/provider_collection.rs (L23-27)
```rust
///
/// You'll notice that some of these Providers are wrapped in an Arc. We do this
/// for Providers that could be used between requests, such Providers created for
/// querying the baseline node. Providers that are only used for a single request
/// are not wrapped in an Arc since they're only used once.
```

**File:** ecosystem/node-checker/src/provider/cache.rs (L35-54)
```rust
    pub async fn get(
        &self,
        func: impl Future<Output = Result<T, ProviderError>>,
    ) -> Result<T, ProviderError> {
        // If the cache isn't too old and there is a value, return it.
        if self.last_run.read().await.elapsed() < self.cache_ttl {
            if let Some(last_output) = &*self.last_output.read().await {
                return Ok(last_output.clone());
            }
        }

        // Otherwise fetch the value and update the cache. We take the locks while
        // fetching the new value so we don't waste effort fetching it multiple times.
        let mut last_output = self.last_output.write().await;
        let mut last_run = self.last_run.write().await;
        let new_output = func.await?;
        *last_output = Some(new_output.clone());
        *last_run = Instant::now();
        Ok(new_output)
    }
```

**File:** ecosystem/node-checker/src/checker/consensus_proposals.rs (L95-96)
```rust
        let first_scrape = match target_metrics_provider.provide().await {
            Ok(scrape) => scrape,
```

**File:** ecosystem/node-checker/src/provider/metrics.rs (L59-85)
```rust
    pub async fn get_scrape(&self) -> Result<Scrape, ProviderError> {
        let response = self
            .client
            .get(self.metrics_url.clone())
            .send()
            .await
            .with_context(|| format!("Failed to get data from {}", self.metrics_url))
            .map_err(|e| ProviderError::RetryableEndpointError("/metrics", e))?;
        let body = response
            .text()
            .await
            .with_context(|| {
                format!(
                    "Failed to process response body from {} as text",
                    self.metrics_url
                )
            })
            .map_err(|e| ProviderError::ParseError(anyhow!(e)))?;
        Scrape::parse(body.lines().map(|l| Ok(l.to_string())))
            .with_context(|| {
                format!(
                    "Failed to parse response text from {} as a Prometheus scrape",
                    self.metrics_url
                )
            })
            .map_err(|e| ProviderError::ParseError(anyhow!(e)))
    }
```
