# Audit Report

## Title
Unbounded Histogram Metric Cardinality via Peer ID Churn Causes CPU Exhaustion in Telemetry Collection

## Summary
The network telemetry system creates histogram metrics with `peer_id` labels that persist indefinitely without cleanup. An attacker can exploit this by rapidly cycling through connections with unique peer IDs, creating millions of histogram time series. The `sum_all_histogram_counts()` function must iterate over all these metrics every 60 seconds, causing progressive CPU exhaustion that degrades validator performance.

## Finding Description
The vulnerability exists in the telemetry system's network metric collection, specifically affecting the aggregation of histogram metrics.

**Vulnerable Code Path:**

The `sum_all_histogram_counts()` function performs O(n) iteration over all histogram metrics: [1](#0-0) 

This function is called during network metric collection: [2](#0-1) 

Network metrics are collected every 60 seconds: [3](#0-2) 

**Root Cause:**

The `NETWORK_APPLICATION_INBOUND_METRIC` histogram is defined with a `peer_id` label that has unbounded cardinality: [4](#0-3) 

Histogram metrics are created when inbound messages are received: [5](#0-4) 

**Attack Vector:**

1. Each unique `peer_id` creates a new histogram time series in Prometheus
2. The `peer_id` is cryptographically derived from the peer's public key, allowing arbitrary generation
3. Validators accept up to 100 concurrent unknown inbound connections: [6](#0-5) 

4. The connection limit only applies to **concurrent** connections - an attacker can disconnect and reconnect with new peer IDs
5. Once created, histogram metrics persist for the process lifetime with no cleanup mechanism
6. Each telemetry collection must iterate through ALL accumulated metrics

**Exploitation Steps:**

1. Attacker generates new Ed25519 key pair â†’ unique `peer_id`
2. Connects to validator node (consumes 1 of 100 connection slots)
3. Sends a DirectSend message to trigger metric creation with that `peer_id`
4. Disconnects
5. Repeats steps 1-4 thousands of times over hours/days
6. After sufficient iterations, millions of unique histogram time series exist
7. Every 60 seconds, `sum_all_histogram_counts()` must iterate all metrics, consuming increasing CPU time
8. CPU spikes during metric collection interfere with validator consensus participation

**Invariant Violation:**

This breaks the documented invariant: "**Resource Limits**: All operations must respect gas, storage, and computational limits." The metric aggregation operation has no computational limit and grows unboundedly with attacker-controlled input.

## Impact Explanation
This qualifies as **Medium Severity** per the Aptos Bug Bounty criteria:

- **Not Critical**: No funds loss, no consensus safety violation, no immediate network halt
- **Not High**: Requires sustained attack over extended period, impact is gradual
- **Yes Medium**: Causes "validator node slowdowns" through progressive CPU exhaustion during metric collection

The attack causes:
- Increasing CPU consumption every 60 seconds during network metric collection
- Potential interference with consensus participation during CPU spikes
- Memory pressure from storing millions of histogram time series
- Requires node restart to clear accumulated metrics

While telemetry runs in a separate runtime, the shared Prometheus registry involves locks during collection, and prolonged CPU consumption can still impact the overall node health.

## Likelihood Explanation
**HIGH likelihood** of exploitation:

- **Low barriers**: Attacker only needs network access to validator (publicly exposed P2P port)
- **Simple execution**: Generate key pairs (fast), connect, send message, disconnect, repeat
- **No authentication**: Unknown peers can connect up to the limit
- **No cleanup**: Metrics accumulate indefinitely
- **Gradual attack**: Can fly under radar, appears as normal peer churn
- **No IP rate limiting**: Connection attempt rate limiting was not found in the codebase

The connection limit of 100 concurrent unknowns is not a meaningful barrier since the attacker cycles through connections. Over 24 hours at 1 connection per second = 86,400 unique peer IDs created.

## Recommendation

**Immediate Mitigation:**

1. **Implement metric cardinality limits** - Use a bounded cache for peer_id labels:

```rust
// Add to network/framework/src/counters.rs
use std::collections::HashMap;
use std::sync::{Arc, Mutex};

lazy_static! {
    static ref PEER_METRIC_CACHE: Arc<Mutex<HashMap<PeerId, ()>>> = 
        Arc::new(Mutex::new(HashMap::new()));
    static ref MAX_PEER_METRICS: usize = 1000; // Configurable limit
}

pub fn network_application_inbound_traffic(
    network_context: NetworkContext,
    protocol_id: ProtocolId,
    size: u64,
) {
    let peer_id = network_context.peer_id();
    
    // Only create metrics for known peers or up to limit
    let mut cache = PEER_METRIC_CACHE.lock().unwrap();
    if cache.len() < *MAX_PEER_METRICS || cache.contains_key(&peer_id) {
        cache.insert(peer_id, ());
        
        NETWORK_APPLICATION_INBOUND_METRIC
            .with_label_values(&[
                network_context.role().as_str(),
                network_context.network_id().as_str(),
                network_context.peer_id().short_str().as_str(),
                protocol_id.as_str(),
                "size",
            ])
            .observe(size as f64);
    }
}
```

2. **Remove peer_id from high-cardinality metrics** - Aggregate at role/network level instead:

```rust
pub static NETWORK_APPLICATION_INBOUND_METRIC: Lazy<HistogramVec> = Lazy::new(|| {
    register_histogram_vec!(
        "aptos_network_app_inbound_traffic",
        "Network Inbound Traffic by application",
        &[
            "role_type",
            "network_id",
            // Remove: "peer_id"  
            "protocol_id",
            "metric"
        ]
    )
    .unwrap()
});
```

3. **Add periodic metric cleanup** - Implement garbage collection for disconnected peers
4. **Connection rate limiting** - Add per-IP connection attempt rate limiting

## Proof of Concept

```rust
// Test to demonstrate metric cardinality explosion
// Add to crates/aptos-telemetry/src/utils.rs

#[cfg(test)]
mod metric_bombing_test {
    use super::*;
    use aptos_crypto::{ed25519::Ed25519PrivateKey, Uniform};
    use prometheus::core::Collector;
    use std::time::Instant;

    #[test]
    fn test_histogram_aggregation_performance_degradation() {
        // Simulate attacker creating many unique peer_id metrics
        let metric = prometheus::register_histogram_vec!(
            "test_peer_metric",
            "Test metric with peer_id",
            &["peer_id"]
        ).unwrap();

        // Create metrics for many unique peer IDs
        let num_peers = 100_000; // Simulated attack over time
        println!("Creating {} unique histogram metrics...", num_peers);
        
        for i in 0..num_peers {
            let peer_id = format!("peer_{}", i);
            metric.with_label_values(&[&peer_id]).observe(1.0);
        }

        // Measure aggregation time
        let start = Instant::now();
        let families = metric.collect();
        let count = sum_all_histogram_counts(&families);
        let duration = start.elapsed();

        println!("Aggregated {} metrics in {:?}", count, duration);
        println!("Per-metric cost: {:?}", duration / num_peers);
        
        // With millions of metrics, this becomes CPU-intensive
        assert!(duration.as_millis() > 10, 
            "Aggregating {} metrics should take measurable time", num_peers);
    }
}
```

**Notes:**
- The vulnerability exploits the unbounded cardinality of the `peer_id` label in network histogram metrics
- Prometheus histograms do not support metric deletion, so accumulated metrics persist until node restart
- The attack is feasible because peer ID generation is cryptographically cheap and connection cycling bypasses the concurrent connection limit
- While telemetry runs in a separate runtime, the cumulative effect of millions of metrics still impacts validator performance through CPU consumption and memory pressure
- The issue affects both `NETWORK_APPLICATION_INBOUND_METRIC` and `NETWORK_APPLICATION_OUTBOUND_METRIC` with identical label structures

### Citations

**File:** crates/aptos-telemetry/src/utils.rs (L49-57)
```rust
pub fn sum_all_histogram_counts(metric_families: &Vec<MetricFamily>) -> f64 {
    let mut count_sum = 0.0;
    for metric_family in metric_families {
        for metric in metric_family.get_metric() {
            count_sum += metric.get_histogram().get_sample_count() as f64
        }
    }
    count_sum
}
```

**File:** crates/aptos-telemetry/src/network_metrics.rs (L81-82)
```rust
    let network_inbound_message_sum = utils::sum_all_histogram_counts(&inbound_metric_families);
    let network_inbound_traffic_sum = utils::sum_all_histogram_sums(&inbound_metric_families);
```

**File:** crates/aptos-telemetry/src/constants.rs (L37-37)
```rust
pub(crate) const NODE_NETWORK_METRICS_FREQ_SECS: u64 = 60; // 1 minute
```

**File:** network/framework/src/counters.rs (L552-565)
```rust
pub static NETWORK_APPLICATION_INBOUND_METRIC: Lazy<HistogramVec> = Lazy::new(|| {
    register_histogram_vec!(
        "aptos_network_app_inbound_traffic",
        "Network Inbound Traffic by application",
        &[
            "role_type",
            "network_id",
            "peer_id",
            "protocol_id",
            "metric"
        ]
    )
    .unwrap()
});
```

**File:** network/framework/src/peer/mod.rs (L452-458)
```rust
            NetworkMessage::DirectSendMsg(direct) => {
                let data_len = direct.raw_msg.len();
                network_application_inbound_traffic(
                    self.network_context,
                    direct.protocol_id,
                    data_len as u64,
                );
```

**File:** config/src/config/network_config.rs (L44-44)
```rust
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
```
