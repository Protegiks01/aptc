# Audit Report

## Title
Shared spawn_blocking Thread Pool Enables Cross-Protocol DoS Attack Against Consensus Deserialization

## Summary
Multiple network applications (consensus, mempool, storage service, peer monitoring, etc.) share a single spawn_blocking thread pool limited to 64 threads on the same network runtime. Malicious peers can send specially crafted, slow-to-deserialize messages to non-consensus protocols to exhaust this shared pool, preventing consensus messages from being deserialized and degrading validator performance.

## Finding Description

The vulnerability exists in how network message deserialization is handled across multiple protocols within the Aptos validator network stack.

**Architecture Overview:**

Network applications register with a shared NetworkBuilder and runtime. [1](#0-0) 

Each application uses `NetworkEvents::new()` which spawns blocking tasks for deserialization. [2](#0-1) 

The deserialization process calls `received_message_to_event()` which performs the actual BCS/compression deserialization. [3](#0-2) 

**Critical Flaw - Shared Thread Pool:**

All tokio runtimes created by `spawn_named_runtime()` share a spawn_blocking pool limited to exactly 64 threads. [4](#0-3) 

**Per-Application Configuration:**

Each application has `max_parallel_deserialization_tasks` defaulting to `num_cpus::get()` (typically 16-64). [5](#0-4) 

This value is passed through to the network builder. [6](#0-5) 

**Deserialization Complexity:**

Messages can be compressed up to MAX_APPLICATION_MESSAGE_SIZE (~62 MiB). [7](#0-6) 

Decompression is performed via LZ4. [8](#0-7) 

After decompression, BCS deserialization supports recursion up to RECURSION_LIMIT (64). [9](#0-8) 

**Attack Path:**

1. Attacker identifies a validator and establishes peer connections
2. Attacker crafts malicious messages with:
   - Maximum compression ratio (small wire size, ~62 MiB decompressed)
   - Deep BCS nesting up to recursion limit
   - Complex data structures requiring significant CPU time
3. Attacker simultaneously sends these messages to multiple protocols:
   - Mempool (MempoolDirectSend/MempoolRpc)
   - Storage Service (StorageServiceRpc)
   - Peer Monitoring (PeerMonitoringServiceRpc)
   - Other non-consensus protocols
4. Each protocol's `NetworkEvents` spawns blocking tasks for deserialization
5. With 8 protocols × 16 tasks = 128 potential concurrent tasks competing for 64 threads
6. Non-consensus protocols fill the 64-thread spawn_blocking pool
7. Consensus messages arrive but cannot spawn blocking tasks for deserialization
8. Consensus message processing is severely delayed
9. Validator appears unresponsive, misses votes/proposals, consensus participation degrades

**Invariant Violation:**

This breaks the Resource Limits invariant - deserialization resource usage is not properly isolated between critical (consensus) and non-critical (mempool, monitoring) protocols.

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty program criteria:

**"Validator node slowdowns"**: The attack directly causes validator nodes to experience significant slowdowns in processing consensus messages due to spawn_blocking thread exhaustion.

**Consensus Participation Impact**: While this doesn't cause a direct consensus safety violation, it degrades validator liveness and responsiveness:
- Delayed vote processing may cause validators to miss voting deadlines
- Delayed proposal processing may cause validators to fall behind in consensus rounds
- Persistent delays could trigger timeouts and leader rotations
- Multiple affected validators could degrade overall network performance

**Severity Justification**: This attack:
- Requires no special privileges (any peer can execute)
- Affects critical consensus operations
- Can be sustained continuously
- Impacts validator operational effectiveness
- Could amplify during network stress or high transaction volumes

## Likelihood Explanation

**High Likelihood** - This vulnerability is easily exploitable:

**Attacker Requirements:**
- Ability to connect as a network peer (standard networking capability)
- Knowledge of protocol message formats (publicly documented)
- Computational resources to craft compressed messages (minimal)

**Attack Complexity:**
- Low - Crafting maximally compressed messages is straightforward
- Sending to multiple protocols requires basic network programming
- No cryptographic operations needed
- No timing-sensitive coordination required

**Detection Difficulty:**
- Messages appear structurally valid
- Deserialization failures are logged but treated as normal peer misbehavior
- No immediate alerting mechanism for spawn_blocking exhaustion
- Slow consensus processing could be misattributed to network latency

**Practical Feasibility:**
- Attack can be launched from commodity hardware
- Can target specific validators or broadcast widely
- Sustainable over extended periods
- Multiple attackers could amplify the effect

## Recommendation

**Immediate Mitigation:**

1. **Isolate Critical Deserialization**: Create separate spawn_blocking thread pools or bounded executors for consensus vs. other protocols:

```rust
// In NetworkEvents::new()
// For consensus protocols, use a dedicated high-priority pool
let data_event_stream = if is_consensus_critical {
    // Use dedicated consensus runtime or bounded executor
    peer_mgr_notifs_rx.map(|notification| {
        consensus_blocking_executor.spawn(
            move || received_message_to_event(notification)
        )
    })
} else {
    // Non-critical protocols use shared pool
    peer_mgr_notifs_rx.map(|notification| {
        tokio::task::spawn_blocking(
            move || received_message_to_event(notification)
        )
    })
}
```

2. **Reduce MAX_APPLICATION_MESSAGE_SIZE**: Lower the decompression size limit for non-consensus protocols to reduce worst-case deserialization time.

3. **Add Deserialization Timeouts**: Implement per-message deserialization timeouts to prevent indefinite thread occupation:

```rust
// Wrap deserialization in timeout
tokio::time::timeout(
    Duration::from_millis(100),
    tokio::task::spawn_blocking(move || received_message_to_event(notification))
)
```

**Long-term Solution:**

1. Implement priority-based task scheduling for spawn_blocking
2. Add per-peer rate limiting on message complexity
3. Implement adaptive backpressure when deserialization queue depth exceeds threshold
4. Add metrics and alerting for spawn_blocking pool saturation
5. Consider using separate runtimes for critical vs. non-critical protocols

## Proof of Concept

```rust
// test_deserialization_dos.rs
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_cross_protocol_deserialization_dos() {
    use aptos_compression::compress;
    use aptos_config::config::MAX_APPLICATION_MESSAGE_SIZE;
    use network::protocols::wire::handshake::v1::ProtocolId;
    
    // Create a maximally compressed message
    fn create_malicious_message() -> Vec<u8> {
        // Create ~60 MiB of compressible data with deep nesting
        let large_data = vec![0u8; 60 * 1024 * 1024];
        
        // Compress it (will be much smaller on wire)
        compress(
            large_data,
            CompressionClient::Mempool,
            MAX_APPLICATION_MESSAGE_SIZE
        ).unwrap()
    }
    
    // Simulate attack scenario
    let malicious_msg = create_malicious_message();
    
    // Create multiple network applications sharing same runtime
    let runtime = aptos_runtimes::spawn_named_runtime("test-net".into(), Some(4));
    let _enter = runtime.enter();
    
    // Track consensus message processing time
    let start = Instant::now();
    
    // Flood non-consensus protocols with slow messages
    let mut tasks = vec![];
    for protocol in &[
        ProtocolId::MempoolDirectSend,
        ProtocolId::StorageServiceRpc,
        ProtocolId::PeerMonitoringServiceRpc,
    ] {
        for _ in 0..20 {
            let msg = malicious_msg.clone();
            let proto = *protocol;
            tasks.push(tokio::task::spawn_blocking(move || {
                // Simulate deserialization
                proto.from_bytes::<Vec<u8>>(&msg)
            }));
        }
    }
    
    // Try to process consensus message after flooding
    let consensus_msg = vec![0u8; 100]; // Small valid message
    let consensus_task = tokio::task::spawn_blocking(move || {
        ProtocolId::ConsensusDirectSendBcs.from_bytes::<Vec<u8>>(&consensus_msg)
    });
    
    // Measure delay
    let consensus_result = consensus_task.await;
    let delay = start.elapsed();
    
    // Assert: Consensus message processing is severely delayed
    // Expected: < 10ms, Actual: potentially seconds
    assert!(delay.as_millis() > 100, 
        "Consensus deserialization should be delayed by spawn_blocking exhaustion");
    
    // Cleanup
    for task in tasks {
        let _ = task.await;
    }
}
```

**Notes:**

The vulnerability stems from an architectural decision to share the spawn_blocking thread pool across all network protocols without isolation or prioritization. The 64-thread limit mentioned in the runtime configuration comment explicitly warns about "too many Rest API calls" but doesn't account for inter-protocol competition where malicious peers can deliberately target non-critical protocols to starve critical consensus operations.

This attack is particularly effective because:
- The attacker controls message timing and targeting
- Compression provides high amplification (small wire → large decompression)
- BCS recursion allows complex structures within limits
- No per-protocol thread pool isolation exists
- Consensus cannot preempt or prioritize its deserialization tasks

### Citations

**File:** aptos-node/src/network.rs (L275-400)
```rust
    for network_config in network_configs.into_iter() {
        // Create a network runtime for the config
        let runtime = create_network_runtime(&network_config);

        // Entering gives us a runtime to instantiate all the pieces of the builder
        let _enter = runtime.enter();

        // Create a new network builder
        let mut network_builder = NetworkBuilder::create(
            chain_id,
            node_config.base.role,
            &network_config,
            TimeService::real(),
            Some(event_subscription_service),
            peers_and_metadata.clone(),
        );

        // Register consensus (both client and server) with the network
        let network_id = network_config.network_id;
        if network_id.is_validator_network() {
            // A validator node must have only a single consensus network handle
            if consensus_network_handle.is_some() {
                panic!("There can be at most one validator network!");
            } else {
                let network_handle = register_client_and_service_with_network(
                    &mut network_builder,
                    network_id,
                    &network_config,
                    consensus_network_configuration(node_config),
                    true,
                );
                consensus_network_handle = Some(network_handle);
            }

            if dkg_network_handle.is_some() {
                panic!("There can be at most one validator network!");
            } else {
                let network_handle = register_client_and_service_with_network(
                    &mut network_builder,
                    network_id,
                    &network_config,
                    dkg_network_configuration(node_config),
                    true,
                );
                dkg_network_handle = Some(network_handle);
            }

            if jwk_consensus_network_handle.is_some() {
                panic!("There can be at most one validator network!");
            } else {
                let network_handle = register_client_and_service_with_network(
                    &mut network_builder,
                    network_id,
                    &network_config,
                    jwk_consensus_network_configuration(node_config),
                    true,
                );
                jwk_consensus_network_handle = Some(network_handle);
            }
        }

        // Register consensus observer (both client and server) with the network
        if node_config
            .consensus_observer
            .is_observer_or_publisher_enabled()
        {
            // Create the network handle for this network type
            let network_handle = register_client_and_service_with_network(
                &mut network_builder,
                network_id,
                &network_config,
                consensus_observer_network_configuration(node_config),
                false,
            );

            // Add the network handle to the set of handles
            if let Some(consensus_observer_network_handles) =
                &mut consensus_observer_network_handles
            {
                consensus_observer_network_handles.push(network_handle);
            } else {
                consensus_observer_network_handles = Some(vec![network_handle]);
            }
        }

        // Register mempool (both client and server) with the network
        let mempool_network_handle = register_client_and_service_with_network(
            &mut network_builder,
            network_id,
            &network_config,
            mempool_network_configuration(node_config),
            true,
        );
        mempool_network_handles.push(mempool_network_handle);

        // Register the peer monitoring service (both client and server) with the network
        let peer_monitoring_service_network_handle = register_client_and_service_with_network(
            &mut network_builder,
            network_id,
            &network_config,
            peer_monitoring_network_configuration(node_config),
            true,
        );
        peer_monitoring_service_network_handles.push(peer_monitoring_service_network_handle);

        // Register the storage service (both client and server) with the network
        let storage_service_network_handle = register_client_and_service_with_network(
            &mut network_builder,
            network_id,
            &network_config,
            storage_service_network_configuration(node_config),
            true,
        );
        storage_service_network_handles.push(storage_service_network_handle);

        // Register the network benchmark test service
        if let Some(app_config) = netbench_network_configuration(node_config) {
            let netbench_handle = register_client_and_service_with_network(
                &mut network_builder,
                network_id,
                &network_config,
                app_config,
                true,
            );
            netbench_handles.push(netbench_handle);
        }
```

**File:** aptos-node/src/network.rs (L480-485)
```rust
) -> ApplicationNetworkHandle<T> {
    let (network_sender, network_events) = network_builder.add_client_and_service(
        &application_config,
        network_config.max_parallel_deserialization_tasks,
        allow_out_of_order_delivery,
    );
```

**File:** network/framework/src/protocols/network/mod.rs (L208-243)
```rust
impl<TMessage: Message + Send + Sync + 'static> NewNetworkEvents for NetworkEvents<TMessage> {
    fn new(
        peer_mgr_notifs_rx: aptos_channel::Receiver<(PeerId, ProtocolId), ReceivedMessage>,
        max_parallel_deserialization_tasks: Option<usize>,
        allow_out_of_order_delivery: bool,
    ) -> Self {
        // Determine the number of parallel deserialization tasks to use
        let max_parallel_deserialization_tasks = max_parallel_deserialization_tasks.unwrap_or(1);

        let data_event_stream = peer_mgr_notifs_rx.map(|notification| {
            tokio::task::spawn_blocking(move || received_message_to_event(notification))
        });

        let data_event_stream: Pin<
            Box<dyn Stream<Item = Event<TMessage>> + Send + Sync + 'static>,
        > = if allow_out_of_order_delivery {
            Box::pin(
                data_event_stream
                    .buffer_unordered(max_parallel_deserialization_tasks)
                    .filter_map(|res| future::ready(res.expect("JoinError from spawn blocking"))),
            )
        } else {
            Box::pin(
                data_event_stream
                    .buffered(max_parallel_deserialization_tasks)
                    .filter_map(|res| future::ready(res.expect("JoinError from spawn blocking"))),
            )
        };

        Self {
            event_stream: data_event_stream,
            done: false,
            _marker: PhantomData,
        }
    }
}
```

**File:** network/framework/src/protocols/network/mod.rs (L274-300)
```rust
fn received_message_to_event<TMessage: Message>(
    message: ReceivedMessage,
) -> Option<Event<TMessage>> {
    let peer_id = message.sender.peer_id();
    let ReceivedMessage {
        message,
        sender: _sender,
        receive_timestamp_micros: rx_at,
        rpc_replier,
    } = message;
    let dequeue_at = unix_micros();
    let dt_micros = dequeue_at - rx_at;
    let dt_seconds = (dt_micros as f64) / 1000000.0;
    match message {
        NetworkMessage::RpcRequest(rpc_req) => {
            crate::counters::inbound_queue_delay_observe(rpc_req.protocol_id, dt_seconds);
            let rpc_replier = Arc::into_inner(rpc_replier.unwrap()).unwrap();
            request_to_network_event(peer_id, &rpc_req)
                .map(|msg| Event::RpcRequest(peer_id, msg, rpc_req.protocol_id, rpc_replier))
        },
        NetworkMessage::DirectSendMsg(request) => {
            crate::counters::inbound_queue_delay_observe(request.protocol_id, dt_seconds);
            request_to_network_event(peer_id, &request).map(|msg| Event::Message(peer_id, msg))
        },
        _ => None,
    }
}
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-51)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
        .enable_all();
```

**File:** config/src/config/network_config.rs (L47-50)
```rust
pub const MAX_APPLICATION_MESSAGE_SIZE: usize =
    (MAX_MESSAGE_SIZE - MAX_MESSAGE_METADATA_SIZE) - MESSAGE_PADDING_SIZE; /* The message size that applications should check against */
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** config/src/config/network_config.rs (L178-185)
```rust
    /// Configures the number of parallel deserialization tasks
    /// based on the number of CPU cores of the machine. This is
    /// only done if the config does not specify a value.
    fn configure_num_deserialization_tasks(&mut self) {
        if self.max_parallel_deserialization_tasks.is_none() {
            self.max_parallel_deserialization_tasks = Some(num_cpus::get());
        }
    }
```

**File:** crates/aptos-compression/src/lib.rs (L92-121)
```rust
pub fn decompress(
    compressed_data: &CompressedData,
    client: CompressionClient,
    max_size: usize,
) -> Result<Vec<u8>, Error> {
    // Start the decompression timer
    let start_time = Instant::now();

    // Check size of the data and initialize raw_data
    let decompressed_size = match get_decompressed_size(compressed_data, max_size) {
        Ok(size) => size,
        Err(error) => {
            let error_string = format!("Failed to get decompressed size: {}", error);
            return create_decompression_error(&client, error_string);
        },
    };
    let mut raw_data = vec![0u8; decompressed_size];

    // Decompress the data
    if let Err(error) = lz4::block::decompress_to_buffer(compressed_data, None, &mut raw_data) {
        let error_string = format!("Failed to decompress the data: {}", error);
        return create_decompression_error(&client, error_string);
    };

    // Stop the timer and update the metrics
    metrics::observe_decompression_operation_time(&client, start_time);
    metrics::update_decompression_metrics(&client, compressed_data, &raw_data);

    Ok(raw_data)
}
```

**File:** network/framework/src/protocols/wire/handshake/v1/mod.rs (L38-39)
```rust
pub const USER_INPUT_RECURSION_LIMIT: usize = 32;
pub const RECURSION_LIMIT: usize = 64;
```
