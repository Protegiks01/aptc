# Audit Report

## Title
Configuration-Dependent Non-Determinism in State Storage Usage Calculation Leading to Consensus Divergence

## Summary
The state storage usage calculation in AptosDB can produce different values across validators when they have different configurations for `enable_storage_sharding`. This creates a determinism break where validators with `skip_usage=true` will return `Untracked` usage (0,0) when usage metadata is missing, while validators with `skip_usage=false` will fail with an error. This configuration-dependent behavior violates the deterministic execution invariant and can cause consensus failures when the usage value is written to the on-chain `StateStorageUsage` resource.

## Finding Description

The vulnerability stems from a configuration parameter `enable_storage_sharding` that controls the `skip_usage` flag in StateDb, creating divergent behavior across validators: [1](#0-0) 

This configuration parameter is passed through to StateDb as `skip_usage`: [2](#0-1) 

When `skip_usage=true` and usage data is missing, the system returns `Untracked` usage instead of failing: [3](#0-2) 

This creates non-deterministic behavior because validators with different configurations will handle missing usage data differently. The issue becomes critical when the usage value is written to the on-chain `StateStorageUsage` resource: [4](#0-3) 

The native function that retrieves usage has an explicit warning about determinism: [5](#0-4) 

When `Untracked` usage is used, the system ignores cache misses during usage calculation: [6](#0-5) 

This causes incorrect usage calculations where old values are assumed to be `ColdVacant` (non-existent): [7](#0-6) 

## Impact Explanation

This is a **Critical** severity vulnerability that can cause:

1. **Consensus Failures**: Different validators calculate different usage values and write them to the `StateStorageUsage` resource, producing different state roots and breaking consensus safety.

2. **Network Partitioning**: Validators with different configurations will diverge when processing epoch boundaries, requiring a hard fork to recover.

3. **Validator Crashes**: The consistency check will fail when usage doesn't match JMT leaf count: [8](#0-7) 

This violates the **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks."

## Likelihood Explanation

**Medium Likelihood**: 
- The default configuration sets `enable_storage_sharding=true`, but validators can override this
- Scenarios where usage data is missing include: state sync, backup restoration, network upgrades
- Heterogeneous validator configurations are common in decentralized networks
- The issue manifests at epoch boundaries when `on_new_block` updates the `StateStorageUsage` resource

## Recommendation

**Solution 1 - Enforce Configuration Consistency**: Add consensus-level validation that all validators use the same `enable_storage_sharding` configuration. Store this in on-chain configuration and validate at startup.

**Solution 2 - Remove Configuration Dependency**: Eliminate the `skip_usage` flag entirely and always require usage data to be present. Ensure state sync and backup/restore processes include usage metadata.

**Solution 3 - Graceful Degradation**: If usage data is missing, all validators should handle it identically (either all fail or all use a deterministic fallback) rather than having configuration-dependent behavior.

**Recommended Fix**:
```rust
pub(crate) fn get_state_storage_usage(&self, version: Option<Version>) -> Result<StateStorageUsage> {
    version.map_or(Ok(StateStorageUsage::zero()), |version| {
        // Remove skip_usage check - always require usage data for determinism
        self.ledger_db.metadata_db().get_usage(version)
    })
}
```

## Proof of Concept

**Setup**: Deploy two validators with different configurations:
- Validator A: `enable_storage_sharding = true` (skip_usage = true)
- Validator B: `enable_storage_sharding = false` (skip_usage = false)

**Trigger**:
1. Perform state sync from epoch N-1 to epoch N where usage data is missing
2. Process the first block of epoch N+1 which calls `on_new_block()`
3. The native function `get_state_storage_usage_only_at_epoch_beginning()` is called

**Expected Result**:
- Validator A returns `Usage { items: 0, bytes: 0 }` (Untracked)
- Validator B crashes with error "VersionData missing for version X"
- If Validator B also has missing data tolerance, both write different values to `StateStorageUsage`
- State roots diverge, consensus fails

**Verification**: Monitor validator logs for "State item count inconsistent" errors and compare state roots across validators at epoch boundaries.

## Notes

While the default configuration (`enable_storage_sharding=true`) is consistent across validators in typical deployments, the code architecture allows this parameter to diverge, creating a consensus vulnerability. The issue is exacerbated during state sync and restoration scenarios where usage metadata may be incomplete or missing. The explicit warning in the native function implementation acknowledges the determinism concerns but doesn't enforce consistency at the configuration level.

### Citations

**File:** config/src/config/storage_config.rs (L203-203)
```rust
    pub enable_storage_sharding: bool,
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L148-160)
```rust
        let mut myself = Self::new_with_dbs(
            ledger_db,
            hot_state_merkle_db,
            state_merkle_db,
            state_kv_db,
            pruner_config,
            buffered_state_target_items,
            readonly,
            empty_buffered_state_for_restore,
            rocksdb_configs.enable_storage_sharding,
            internal_indexer_db,
            hot_state_config,
        );
```

**File:** storage/aptosdb/src/state_store/mod.rs (L238-248)
```rust
    fn get_state_storage_usage(&self, version: Option<Version>) -> Result<StateStorageUsage> {
        version.map_or(Ok(StateStorageUsage::zero()), |version| {
            Ok(match self.ledger_db.metadata_db().get_usage(version) {
                Ok(data) => data,
                _ => {
                    ensure!(self.skip_usage, "VersionData at {version} is missing.");
                    StateStorageUsage::new_untracked()
                },
            })
        })
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L871-877)
```rust
        Self::put_stale_state_value_index(
            state_update_refs,
            sharded_state_kv_batches,
            self.state_kv_db.enabled_sharding(),
            state_reads,
            latest_state.usage().is_untracked() || current_state.version().is_none(), // ignore_state_cache_miss
        );
```

**File:** storage/aptosdb/src/state_store/mod.rs (L954-968)
```rust
                let old_entry = cache
                    // TODO(HotState): Revisit: assuming every write op results in a hot slot
                    .insert(
                        (*key).clone(),
                        update_to_cold
                            .to_result_slot()
                            .expect("hot state ops should have been filtered out above"),
                    )
                    .unwrap_or_else(|| {
                        // n.b. all updated state items must be read and recorded in the state cache,
                        // otherwise we can't calculate the correct usage. The is_untracked() hack
                        // is to allow some db tests without real execution layer to pass.
                        assert!(ignore_state_cache_miss, "Must cache read.");
                        StateSlot::ColdVacant
                    });
```

**File:** aptos-move/framework/aptos-framework/sources/state_storage.move (L39-49)
```text
    public(friend) fun on_new_block(epoch: u64) acquires StateStorageUsage {
        assert!(
            exists<StateStorageUsage>(@aptos_framework),
            error::not_found(ESTATE_STORAGE_USAGE)
        );
        let usage = borrow_global_mut<StateStorageUsage>(@aptos_framework);
        if (epoch != usage.epoch) {
            usage.epoch = epoch;
            usage.usage = get_state_storage_usage_only_at_epoch_beginning();
        }
    }
```

**File:** aptos-move/framework/src/natives/state_storage.rs (L55-78)
```rust
/// Warning: the result returned is based on the base state view held by the
/// VM for the entire block or chunk of transactions, it's only deterministic
/// if called from the first transaction of the block because the execution layer
/// guarantees a fresh state view then.
fn native_get_usage(
    context: &mut SafeNativeContext,
    _ty_args: &[Type],
    _args: VecDeque<Value>,
) -> SafeNativeResult<SmallVec<[Value; 1]>> {
    assert!(_ty_args.is_empty());
    assert!(_args.is_empty());

    context.charge(STATE_STORAGE_GET_USAGE_BASE_COST)?;

    let ctx = context.extensions().get::<NativeStateStorageContext>();
    let usage = ctx.resolver.get_usage().map_err(|err| {
        PartialVMError::new(StatusCode::VM_EXTENSION_ERROR)
            .with_message(format!("Failed to get state storage usage: {}", err))
    })?;

    Ok(smallvec![Value::struct_(Struct::pack(vec![
        Value::u64(usage.items() as u64),
        Value::u64(usage.bytes() as u64),
    ]))])
```

**File:** storage/aptosdb/src/state_store/state_merkle_batch_committer.rs (L150-155)
```rust
        ensure!(
            usage_from_ledger_db.items() == leaf_count_from_jmt,
            "State item count inconsistent, {} from ledger db and {} from state tree.",
            usage_from_ledger_db.items(),
            leaf_count_from_jmt,
        );
```
