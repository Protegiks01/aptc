# Audit Report

## Title
Race Condition in Reconfig Subscription During Node Startup Allows Services to Miss Initial Epoch Configuration

## Summary
A race condition exists in the node startup sequence where reconfiguration subscriptions use KLAST queues with size 1, allowing newer epoch notifications to overwrite initial configurations when state sync processes blocks containing NewEpochEvents before dependent services consume their initial notifications. This causes critical services (mempool, consensus, DKG, JWK consensus) to start with incorrect epoch states.

## Finding Description

The vulnerability stems from the interaction between three key design decisions:

**1. KLAST Queue with Size 1:**
All reconfiguration subscriptions are created with `QueueStyle::KLAST` and capacity 1, explicitly designed to keep only the latest reconfiguration notification. [1](#0-0) [2](#0-1) 

**2. Node Startup Sequence:**
The startup sequence in `setup_environment_and_start_node()` creates reconfig subscriptions first, then starts state sync, followed by services. [3](#0-2) [4](#0-3) [5](#0-4) 

**3. Initial Config Notification:**
State sync sends initial configurations synchronously during driver factory initialization, before spawning the driver. [6](#0-5) 

However, the driver is immediately spawned asynchronously afterward, beginning block processing. [7](#0-6) 

**The Race Window:**

When state sync processes blocks during bootstrapping, committed transactions trigger the commit post-processor which calls `notify_events()`. [8](#0-7) [9](#0-8) 

The `notify_events()` method detects NewEpochEvents and triggers reconfiguration notifications to all subscribers. [10](#0-9) [11](#0-10) 

Because the channel uses KLAST with size 1, newer epoch notifications **overwrite** the initial notification.

**Victim Services:**

Mempool blocks on startup waiting for the initial reconfiguration notification before entering its main coordinator loop. [12](#0-11) 

Consensus EpochManager similarly blocks waiting for the initial reconfiguration before starting its main loop. [13](#0-12) [14](#0-13) 

If state sync processes multiple epochs between sending the initial config and these services consuming it, they receive only the latest epoch configuration, missing intermediate transitions.

## Impact Explanation

This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria:

1. **Consensus Safety Risk**: Services starting with incorrect epoch configurations may operate with wrong validator sets, consensus parameters, or quorum thresholds. This violates epoch transition assumptions critical to AptosBFT safety.

2. **Protocol Violation**: Different nodes observing different epoch transitions based on startup timing breaks deterministic execution invariants, potentially causing state divergence.

3. **Validator Operational Issues**: During network partitions or when validators restart and sync through multiple epochs, this race becomes highly likely, causing undefined behavior that could impact block production and validation.

4. **State Inconsistency**: Mempool, DKG, and JWK consensus missing intermediate epoch reconfigurations can lead to inconsistent state management across the network.

This does not reach Critical severity as it requires specific timing conditions (node startup coinciding with multi-epoch sync) and likely doesn't directly enable fund theft, but represents a significant consensus and liveness risk.

## Likelihood Explanation

**High Likelihood** in production scenarios:

1. **Validator Restarts During Epoch Transitions**: When validators restart and need to sync past epoch boundaries, state sync processes NewEpochEvents while services are initializing.

2. **Network Partitions**: Nodes rejoining after partitions often need to sync multiple epochs, making this race highly probable.

3. **New Node Bootstrap**: New validators or full nodes joining the network process many epochs during initial sync, making this race almost guaranteed if multiple epochs exist.

4. **Fast Sync Mode**: Nodes using fast sync catch up rapidly, processing epoch transitions during the critical startup window.

The race window exists from state sync driver spawn to service initialization - a substantial period where mempool, DKG, and JWK consensus are started but blocking on notification consumption while state sync actively processes blocks.

## Recommendation

Implement a synchronization mechanism to ensure services only start consuming reconfig notifications after state sync completes bootstrapping:

1. **Option 1**: Move service initialization (mempool, DKG, JWK consensus) to after `block_until_initialized()` completes, ensuring state sync finishes bootstrapping before services start.

2. **Option 2**: Use a separate "initial config" channel with size > 1 or a different queue style (FIFO) to preserve all epoch transitions during startup.

3. **Option 3**: Add explicit synchronization where `notify_initial_configs()` occurs only after all services have subscribed AND are ready to consume, preventing any intermediate notifications from being sent.

4. **Option 4**: Increase KLAST queue size during startup phase, then reduce to 1 after services have consumed initial configs.

The most robust solution is **Option 1**: reorganize the startup sequence to delay service initialization until after state sync bootstrapping completes.

## Proof of Concept

This vulnerability manifests naturally during node startup when multiple epochs need to be synced. To reproduce:

1. Start a network and allow it to progress through multiple epochs (e.g., 5+ epochs)
2. Start a new validator node or restart an existing one that needs to sync from genesis or early epoch
3. During bootstrapping, state sync will process blocks containing NewEpochEvents
4. Monitor mempool/consensus epoch state upon startup
5. Observe that services start in epoch N+k instead of epoch N, missing intermediate transitions

The race is timing-dependent but highly likely when syncing through multiple epochs, as the window between driver spawn and service notification consumption is substantial.

**Notes**

This is a concurrency vulnerability in the initialization sequence rather than a logic error in any single component. Each component behaves correctly in isolation, but their interaction during startup creates a race condition that violates the assumption that services receive all epoch transitions in order. The KLAST queue design is intentional for steady-state operation but problematic during startup when multiple rapid epoch transitions may occur before services are ready to consume them.

### Citations

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L40-40)
```rust
const RECONFIG_NOTIFICATION_CHANNEL_SIZE: usize = 1; // Note: this should be 1 to ensure only the latest reconfig is consumed
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L174-175)
```rust
        let (notification_sender, notification_receiver) =
            aptos_channel::new(QueueStyle::KLAST, RECONFIG_NOTIFICATION_CHANNEL_SIZE, None);
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L241-244)
```rust
            // Take note if a reconfiguration (new epoch) has occurred
            if event.is_new_epoch_event() {
                reconfig_event_found = true;
            }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L319-325)
```rust
        // If a reconfiguration event was found, also notify the reconfig subscribers
        // of the new configuration values.
        if reconfig_event_processed {
            self.notify_reconfiguration_subscribers(version)
        } else {
            Ok(())
        }
```

**File:** aptos-node/src/lib.rs (L726-734)
```rust
    // Create an event subscription service (and reconfig subscriptions for consensus and mempool)
    let (
        mut event_subscription_service,
        mempool_reconfig_subscription,
        consensus_observer_reconfig_subscription,
        consensus_reconfig_subscription,
        dkg_subscriptions,
        jwk_consensus_subscriptions,
    ) = state_sync::create_event_subscription_service(&node_config, &db_rw);
```

**File:** aptos-node/src/lib.rs (L762-769)
```rust
    let (aptos_data_client, state_sync_runtimes, mempool_listener, consensus_notifier) =
        state_sync::start_state_sync_and_get_notification_handles(
            &node_config,
            storage_service_network_interfaces,
            genesis_waypoint,
            event_subscription_service,
            db_rw.clone(),
        )?;
```

**File:** aptos-node/src/lib.rs (L801-822)
```rust
    let (mempool_runtime, consensus_to_mempool_sender) =
        services::start_mempool_runtime_and_get_consensus_sender(
            &mut node_config,
            &db_rw,
            mempool_reconfig_subscription,
            mempool_network_interfaces,
            mempool_listener,
            mempool_client_receiver,
            peers_and_metadata,
        );

    // Create the DKG runtime and get the VTxn pool
    let (vtxn_pool, dkg_runtime) =
        consensus::create_dkg_runtime(&mut node_config, dkg_subscriptions, dkg_network_interfaces);

    // Create the JWK consensus runtime
    let jwk_consensus_runtime = consensus::create_jwk_consensus_runtime(
        &mut node_config,
        jwk_consensus_subscriptions,
        jwk_consensus_network_interfaces,
        &vtxn_pool,
    );
```

**File:** state-sync/state-sync-driver/src/driver_factory.rs (L103-118)
```rust
        match storage.reader.get_latest_state_checkpoint_version() {
            Ok(Some(synced_version)) => {
                if let Err(error) =
                    event_subscription_service.notify_initial_configs(synced_version)
                {
                    panic!(
                        "Failed to notify subscribers of initial on-chain configs: {:?}",
                        error
                    )
                }
            },
            Ok(None) => {
                panic!("Latest state checkpoint version not found.")
            },
            Err(error) => panic!("Failed to fetch the initial synced version: {:?}", error),
        }
```

**File:** state-sync/state-sync-driver/src/driver_factory.rs (L184-189)
```rust
        // Spawn the driver
        if let Some(driver_runtime) = &driver_runtime {
            driver_runtime.spawn(state_sync_driver.start_driver());
        } else {
            tokio::spawn(state_sync_driver.start_driver());
        }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L810-817)
```rust
            utils::handle_committed_transactions(
                committed_transactions,
                storage.clone(),
                mempool_notification_handler.clone(),
                event_subscription_service.clone(),
                storage_service_notification_handler.clone(),
            )
            .await;
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L106-109)
```rust
        // Notify the event subscription service of the events
        event_subscription_service
            .lock()
            .notify_events(latest_synced_version, events)?;
```

**File:** mempool/src/shared_mempool/coordinator.rs (L95-98)
```rust
    let initial_reconfig = mempool_reconfig_events
        .next()
        .await
        .expect("Reconfig sender dropped, unable to start mempool");
```

**File:** consensus/src/epoch_manager.rs (L1912-1920)
```rust
    async fn await_reconfig_notification(&mut self) {
        let reconfig_notification = self
            .reconfig_events
            .next()
            .await
            .expect("Reconfig sender dropped, unable to start new epoch");
        self.start_new_epoch(reconfig_notification.on_chain_configs)
            .await;
    }
```

**File:** consensus/src/epoch_manager.rs (L1927-1928)
```rust
        // initial start of the processor
        self.await_reconfig_notification().await;
```
