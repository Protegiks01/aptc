# Audit Report

## Title
Unbounded Batch Size in State Merkle Pruner Initialization Causes Memory Exhaustion and Node Crashes

## Summary
The state merkle pruner uses `usize::MAX` as the batch size limit during initialization and catch-up operations, allowing unbounded collection of stale node indices into memory. This can cause memory exhaustion and node crashes when processing large backlogs, violating resource limit invariants and affecting validator availability.

## Finding Description

The vulnerability exists in two locations where `usize::MAX` is used as the batch size parameter:

**Location 1: During shard pruner initialization** [1](#0-0) 

When a `StateMerkleShardPruner` is created, it calls `prune()` with `usize::MAX` to catch up the shard to the metadata pruner's progress. If the shard's progress significantly lags behind the metadata progress (e.g., after node downtime), this creates a large version gap.

**Location 2: During metadata pruning** [2](#0-1) 

The metadata pruner also uses `usize::MAX` when retrieving stale node indices, though this is scoped to a single version.

**The Core Issue: Unbounded Collection**

The `get_stale_node_indices()` function collects indices until the limit is reached: [3](#0-2) 

When `limit` is `usize::MAX`, the function attempts to collect ALL stale node indices across the version range in a single call, with no size constraint. The configuration explicitly acknowledges significant stale node generation: [4](#0-3) 

Each collected index results in two delete operations: [5](#0-4) 

These operations are added to a `SchemaBatch` with no internal size limits: [6](#0-5) 

**Contrast with Normal Operation**

During normal pruning operations, the `PrunerWorker` uses the configured batch size (default 1,000): [7](#0-6) [8](#0-7) 

However, this protection is bypassed during initialization, where `usize::MAX` is hardcoded.

**Attack Scenario**

During node catch-up after extended downtime:
1. Node restarts after being offline (hardware maintenance, crash, upgrade)
2. Shard pruner progress lags behind metadata progress by millions of versions
3. Initialization calls `prune(shard_progress, metadata_progress, usize::MAX)`
4. `get_stale_node_indices()` attempts to collect all stale nodes across the version gap
5. If millions of stale nodes exist (realistic given 300k nodes per 10k transactions), the process:
   - Exhausts memory building the `Vec<StaleNodeIndex>`
   - Exhausts memory constructing the `SchemaBatch` with millions of delete operations
   - Crashes with OOM before write completion
6. Node enters crash loop if backlog remains too large upon restart

## Impact Explanation

This vulnerability meets **Medium to High Severity** criteria:

**Medium Severity Impact:**
- Validator node crashes during initialization or catch-up operations
- Memory exhaustion leading to out-of-memory kills by the OS
- Failed pruning operations can cause disk space exhaustion over time
- Degraded network availability if multiple validators experience this during coordinated upgrades

**Why Not Critical:**
- Does not directly compromise consensus safety (network can progress with remaining validators)
- Does not cause fund loss or theft
- Does not enable double-spending or state manipulation
- Node can eventually recover with increased memory or manual intervention

**Why Not Lower:**
- Violates Resource Limits invariants requiring bounded memory usage
- Directly impacts validator availability
- Can cause repeated crashes requiring manual intervention
- Realistic scenario in production environments

## Likelihood Explanation

**High likelihood** of occurrence:

1. **Natural Occurrence:** Happens during normal node operations after being offline for maintenance, crashes, or upgrades - no attacker needed

2. **Realistic Version Gaps:** With Aptos mainnet processing thousands of TPS, version gaps accumulate quickly during downtime. Even brief outages can create backlogs requiring millions of stale node pruning operations.

3. **Explicitly Acknowledged Scale:** The configuration itself acknowledges that a single 10k transaction block generates 300k JMT nodes, demonstrating this is a realistic production scenario.

4. **No Special Preconditions:** Only requires normal node initialization after downtime - a routine operational event for validators.

5. **Affects All Nodes:** Any validator experiencing downtime and attempting to rejoin will encounter this issue if sufficient stale nodes accumulated.

## Recommendation

Replace `usize::MAX` with the configured batch size during initialization to ensure consistent resource limits:

**Fix for Location 1:**
```rust
// In StateMerkleShardPruner::new()
let batch_size = /* pass from config, e.g., 10_000 for catch-up */;
myself.prune(progress, metadata_progress, batch_size)?;
```

**Alternative Approach:**
Implement incremental catch-up with proper batching:
```rust
const CATCHUP_BATCH_SIZE: usize = 100_000; // Larger than normal but bounded
myself.prune(progress, metadata_progress, CATCHUP_BATCH_SIZE)?;
```

**Additional Safeguards:**
- Add memory usage monitoring during pruning operations
- Implement batch size auto-tuning based on available memory
- Add progress logging for long-running catch-up operations
- Consider checkpoint-based recovery for very large backlogs

## Proof of Concept

The vulnerability can be demonstrated by simulating a node with a large version gap:

1. Initialize a node with state merkle database containing stale nodes across millions of versions
2. Set shard pruner progress to version 0
3. Set metadata pruner progress to version 10,000,000
4. Create a scenario where this gap contains ~30 million stale nodes (realistic given 300k per 10k transactions)
5. Call `StateMerkleShardPruner::new()` which triggers initialization with `usize::MAX`
6. Observe memory allocation growing unboundedly as `get_stale_node_indices` collects all indices
7. Observe OOM crash when memory exhaustion occurs

The root cause is confirmed by examining the initialization code path where the configured batch_size is explicitly bypassed in favor of `usize::MAX`.

---

**Notes:**
- This vulnerability specifically affects the **initialization phase**, not normal steady-state pruning operations
- The issue is exacerbated by Aptos's high transaction throughput which rapidly generates stale nodes
- Multiple validators experiencing simultaneous downtime (e.g., during coordinated upgrades) could face this issue concurrently, amplifying network impact
- The vulnerability demonstrates a disconnect between careful batching in normal operations versus unbounded operations during initialization

### Citations

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L53-53)
```rust
        myself.prune(progress, metadata_progress, usize::MAX)?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L73-76)
```rust
            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_metadata_pruner.rs (L53-58)
```rust
        let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
            &self.metadata_db,
            current_progress,
            target_version_for_this_round,
            usize::MAX,
        )?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** config/src/config/storage_config.rs (L408-410)
```rust
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
```

**File:** storage/schemadb/src/batch.rs (L129-133)
```rust
#[derive(Debug, Default)]
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-55)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L152-156)
```rust
        PrunerWorker::new(
            pruner,
            state_merkle_pruner_config.batch_size,
            "state_merkle",
        )
```
