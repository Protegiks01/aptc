# Audit Report

## Title
Panic Vulnerability in Cross-Shard State View Compromising Validator Liveness

## Summary
The `set_value()` function in `CrossShardStateView` contains an unsafe `unwrap()` call that can cause validator nodes to panic and crash if invoked with a state key not present in the `cross_shard_data` HashMap. While normal operation should prevent this scenario through consistent dependency tracking, any partitioner bug or edge case in cross-shard communication could trigger a validator crash, disrupting consensus liveness.

## Finding Description

The vulnerability exists in the `set_value()` method which is called when cross-shard messages are received: [1](#0-0) 

The function unconditionally calls `unwrap()` without validating that the `state_key` exists in `cross_shard_data`. This HashMap is initialized with keys from the transaction's `required_edges`: [2](#0-1) 

The vulnerability is triggered when cross-shard messages are received and processed: [3](#0-2) 

The partitioner is designed to maintain consistency between `dependent_edges` (what this shard sends) and `required_edges` (what other shards expect), verified only in tests: [4](#0-3) 

However, this validation is **test-only** and provides no runtime protection. If the partitioner has any bugs causing edge inconsistency, or if there are any race conditions or edge cases in the cross-shard communication protocol, the missing key will cause a panic.

**Attack Scenarios:**

1. **Partitioner Logic Bug**: Any bug in the complex partitioner logic that causes `dependent_edges` and `required_edges` to be inconsistent would trigger the panic when cross-shard messages are processed.

2. **Edge Case in Dependency Tracking**: The partitioner builds dependencies based on analyzed transaction hints, not actual execution results. Any mismatch between static analysis and runtime behavior could expose this vulnerability.

3. **Race Conditions**: In the remote execution mode with network communication, message ordering issues or race conditions could potentially deliver messages for unexpected keys.

This breaks the **Consensus Liveness** invariant - a single validator crash can disrupt block production and network progress.

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria:
- **Validator node crash**: Direct panic causes process termination
- **Significant protocol violation**: Disrupts consensus liveness
- **Network-wide impact**: If multiple validators encounter the same partitioner bug, simultaneous crashes could halt the network

The sharded block executor is used in production code paths: [5](#0-4) 

The vulnerability affects all validators running with sharded execution enabled, which is determined by: [6](#0-5) 

## Likelihood Explanation

**Likelihood: Medium**

While the partitioner has test coverage for edge consistency, several factors increase likelihood:

1. **Complexity**: The partitioner logic is highly complex with multiple rounds, shard assignments, and dependency tracking across storage keys.

2. **No Runtime Validation**: The consistency check only exists in tests, not production.

3. **Production Usage**: The code path is active when sharded execution is enabled (num_shards > 1).

4. **Defensive Programming Absence**: The code assumes perfect correctness with no error handling, making it fragile to edge cases.

5. **Remote Execution Mode**: In remote execution with network communication, additional attack surfaces exist for malformed or unexpected messages.

## Recommendation

Replace the unsafe `unwrap()` with defensive error handling:

```rust
pub fn set_value(&self, state_key: &StateKey, state_value: Option<StateValue>) {
    match self.cross_shard_data.get(state_key) {
        Some(remote_value) => {
            remote_value.set_value(state_value);
        },
        None => {
            // Log critical error with diagnostic information
            error!(
                "Cross-shard state key not found in expected keys. \
                This indicates a partitioner bug or message corruption. \
                StateKey: {:?}",
                state_key
            );
            // Option 1: Panic with detailed diagnostics (fail-fast)
            panic!("Cross-shard dependency violation: unexpected state key {:?}", state_key);
            
            // Option 2: Return error and handle gracefully (preferred for production)
            // This would require changing the function signature to return Result<(), Error>
        }
    }
}
```

Additionally, add runtime validation in `CrossShardCommitReceiver::start()` before calling `set_value()`:

```rust
RemoteTxnWriteMsg(txn_commit_msg) => {
    let (state_key, write_op) = txn_commit_msg.take();
    
    // Validate key exists before attempting to set value
    if !cross_shard_state_view.contains_key(&state_key) {
        error!("Received cross-shard message for unexpected key: {:?}", state_key);
        // Handle error appropriately - either panic with diagnostics or skip
        continue;
    }
    
    cross_shard_state_view.set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
}
```

This requires adding a `contains_key()` method to `CrossShardStateView`.

## Proof of Concept

```rust
#[test]
#[should_panic(expected = "called `Option::unwrap()` on a `None` value")]
fn test_panic_on_missing_key() {
    use aptos_types::state_store::{state_key::StateKey, state_value::StateValue};
    use std::collections::HashSet;
    
    struct EmptyView;
    impl TStateView for EmptyView {
        type Key = StateKey;
        fn get_state_value(&self, _: &StateKey) -> Result<Option<StateValue>, StateViewError> {
            Ok(None)
        }
        fn get_usage(&self) -> Result<StateStorageUsage, StateViewError> {
            Ok(StateStorageUsage::new_untracked())
        }
    }
    
    // Create CrossShardStateView with key1
    let mut keys = HashSet::new();
    let key1 = StateKey::raw(b"key1");
    keys.insert(key1);
    
    let view = CrossShardStateView::new(keys, &EmptyView);
    
    // Try to set value for key2 which was never registered
    let key2 = StateKey::raw(b"key2");
    let value = StateValue::from(b"value".to_vec());
    
    // This will panic with unwrap() on None
    view.set_value(&key2, Some(value));
}
```

**Notes**

This vulnerability represents a critical failure in defensive programming. While the partitioner's edge consistency is well-tested, production systems should never rely solely on other components being bug-free. The lack of error handling transforms any potential partitioner bug into an immediate consensus liveness failure, amplifying the impact of what might otherwise be a recoverable error. The fix is straightforward and essential for production resilience.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_state_view.rs (L49-56)
```rust
    pub fn set_value(&self, state_key: &StateKey, state_value: Option<StateValue>) {
        self.cross_shard_data
            .get(state_key)
            .unwrap()
            .set_value(state_value);
        // uncomment the following line to debug waiting count
        // trace!("waiting count for shard id {} is {}", self.shard_id, self.waiting_count());
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_state_view.rs (L58-71)
```rust
    pub fn create_cross_shard_state_view(
        base_view: &'a S,
        transactions: &[TransactionWithDependencies<AnalyzedTransaction>],
    ) -> CrossShardStateView<'a, S> {
        let mut cross_shard_state_key = HashSet::new();
        for txn in transactions {
            for (_, storage_locations) in txn.cross_shard_dependencies.required_edges_iter() {
                for storage_location in storage_locations {
                    cross_shard_state_key.insert(storage_location.clone().into_state_key());
                }
            }
        }
        CrossShardStateView::new(cross_shard_state_key, base_view)
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L26-45)
```rust
    pub fn start<S: StateView + Sync + Send>(
        cross_shard_state_view: Arc<CrossShardStateView<S>>,
        cross_shard_client: Arc<dyn CrossShardClient>,
        round: RoundId,
    ) {
        loop {
            let msg = cross_shard_client.receive_cross_shard_msg(round);
            match msg {
                RemoteTxnWriteMsg(txn_commit_msg) => {
                    let (state_key, write_op) = txn_commit_msg.take();
                    cross_shard_state_view
                        .set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
                },
                CrossShardMsg::StopMsg => {
                    trace!("Cross shard commit receiver stopped for round {}", round);
                    break;
                },
            }
        }
    }
```

**File:** execution/block-partitioner/src/test_utils.rs (L302-302)
```rust
    assert_eq!(edge_set_from_src_view, edge_set_from_dst_view);
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```

**File:** aptos-move/aptos-vm/src/aptos_vm.rs (L463-468)
```rust
    pub fn get_num_shards() -> usize {
        match NUM_EXECUTION_SHARD.get() {
            Some(num_shards) => *num_shards,
            None => 1,
        }
    }
```
