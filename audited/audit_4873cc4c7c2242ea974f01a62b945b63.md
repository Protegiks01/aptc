# Audit Report

## Title
Missing Empty Value Validation in Redis Cache Retrieval Causes Data Corruption and Service Crashes

## Summary
The `cache_operator.rs` module's `batch_get_encoded_proto_data_with_length` and `get_transactions_with_durations` functions fail to validate that Redis `mget` responses contain non-empty byte arrays before decoding. When Redis keys are missing due to TTL expiration or cache eviction, empty `Vec<u8>` entries are decoded into invalid transactions (Base64 format) or cause service panics (LZ4 format), breaking data integrity guarantees and causing API crashes.

## Finding Description

The vulnerability exists in the transaction retrieval logic where Redis `mget` operations return empty byte vectors for missing keys, but these are processed without validation. [1](#0-0) 

The vulnerable code retrieves multiple transactions from Redis but only validates the final count, not whether individual entries are empty. When Redis keys are missing:

1. **Base64UncompressedProto format**: Empty bytes successfully decode to default `Transaction` objects with `version=0` and other default field values, corrupting the data stream
2. **Lz4CompressedProto format**: Empty bytes cause `Decoder::new()` to panic with "Lz4 decompression failed", crashing the service [2](#0-1) 

The correct implementation pattern exists in the codebase but was not applied uniformly: [3](#0-2) 

**Root Cause**: Redis `mget` returns empty `Vec<u8>` for missing keys rather than filtering them out. The count-only validation (`ensure!(transactions.len() == transaction_count)`) passes because empty entries are still counted.

**How Keys Go Missing**:
- TTL-based expiration: Transactions have timestamp-based TTLs that cause older keys to expire [4](#0-3) 
- Active eviction: Keys older than 300,000 versions are actively deleted [5](#0-4) 
- TOCTOU race condition: Keys can expire between the latest version check and the actual `mget` call [6](#0-5) 

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:

1. **API Crashes** (Lz4 format): Service panics when attempting to decompress empty bytes, causing indexer API unavailability
2. **Data Corruption** (Base64 format): Clients receive invalid transaction objects with `version=0`, breaking indexer data integrity guarantees
3. **Service Disruption**: File store processor crashes prevent transaction archival, data service crashes prevent client queries

The vulnerability affects the indexer-grpc ecosystem service used by external clients to query transaction history. While it does not directly impact consensus or on-chain state, it meets the "API crashes" criterion explicitly listed as High Severity in the bug bounty program.

## Likelihood Explanation

**High Likelihood** - This occurs naturally under normal operating conditions:

1. **Guaranteed Occurrence**: TTL-based expiration ensures older transaction keys will eventually be missing from Redis
2. **Production Scenario**: High-load periods with aggressive eviction increase missing key probability
3. **No Special Access Required**: Vulnerability triggers through normal cache access patterns
4. **Active Exploitation Path**: File store processor continuously fetches batches from cache [7](#0-6) 

## Recommendation

Add empty value validation before decoding, matching the pattern used in `in_memory_cache.rs`:

```rust
pub async fn batch_get_encoded_proto_data_with_length(
    &mut self,
    start_version: u64,
    transaction_count: u64,
) -> anyhow::Result<(Vec<Transaction>, f64, f64)> {
    let start_time = std::time::Instant::now();
    let versions = (start_version..start_version + transaction_count)
        .map(|e| CacheEntry::build_key(e, self.storage_format).to_string())
        .collect::<Vec<String>>();
    let encoded_transactions: Vec<Vec<u8>> = self
        .conn
        .mget(versions)
        .await
        .context("Failed to mget from Redis")?;
    
    // ADD THIS CHECK:
    if encoded_transactions.iter().any(|v| v.is_empty()) {
        return Err(anyhow::anyhow!(format!(
            "Failed to fetch all keys from cache; some keys are missing. \
             Requested {} transactions starting at version {}",
            transaction_count, start_version
        )));
    }
    
    let io_duration = start_time.elapsed().as_secs_f64();
    // ... rest of function unchanged
}
```

Apply the same fix to `get_transactions_with_durations` at line 367-395.

## Proof of Concept

**Reproduction Steps**:

1. Start indexer-grpc file store processor with Redis cache
2. Set up Redis with aggressive TTL or manual key deletion
3. Wait for cache worker to populate keys with version range [1000, 2000]
4. Manually delete keys [1500, 1510] from Redis: `DEL 1500 1501 1502 ... 1510`
5. Trigger file store processor to fetch batch starting at version 1500

**Expected Behavior** (with fix): Returns error "Failed to fetch all keys from cache"

**Actual Behavior** (without fix):
- **Base64 format**: Returns 1000 transactions with versions [1500-1510] having default values (version=0)
- **LZ4 format**: Panics with "Lz4 decompression failed"

**Rust Test Case**:
```rust
#[tokio::test]
async fn test_missing_redis_keys_detected() {
    let mock_connection = MockRedisConnection::new(vec![
        MockCmd::new(
            redis::cmd("MGET")
                .arg("1000").arg("1001").arg("1002"),
            Ok(vec![
                encode_transaction(create_tx(1000)),
                vec![], // Empty entry for missing key
                encode_transaction(create_tx(1002)),
            ]),
        ),
    ]);
    
    let mut cache_operator = CacheOperator::new(
        mock_connection, 
        StorageFormat::Base64UncompressedProto
    );
    
    let result = cache_operator
        .batch_get_encoded_proto_data_with_length(1000, 3)
        .await;
    
    // Should fail with error about missing keys
    assert!(result.is_err());
    assert!(result.unwrap_err().to_string().contains("missing"));
}
```

---

## Notes

This vulnerability affects the indexer-grpc ecosystem service, not the core consensus or execution layer. While it does not impact blockchain state or validator operations, it meets High Severity criteria under "API crashes" and causes data integrity issues for external clients querying transaction history. The fix is straightforward and should be applied consistently across all Redis `mget` operations in the cache_operator module.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L91-97)
```rust
pub fn get_ttl_in_seconds(timestamp_in_seconds: u64) -> u64 {
    let current_time = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .unwrap()
        .as_secs();
    BASE_EXPIRATION_EPOCH_TIME_IN_SECONDS - (current_time - timestamp_in_seconds)
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L231-247)
```rust
        let encoded_transactions: Vec<Vec<u8>> = self
            .conn
            .mget(versions)
            .await
            .context("Failed to mget from Redis")?;
        let io_duration = start_time.elapsed().as_secs_f64();
        let start_time = std::time::Instant::now();
        let mut transactions = vec![];
        for encoded_transaction in encoded_transactions {
            let cache_entry: CacheEntry = CacheEntry::new(encoded_transaction, self.storage_format);
            let transaction = cache_entry.into_transaction();
            transactions.push(transaction);
        }
        ensure!(
            transactions.len() == transaction_count as usize,
            "Failed to get all transactions from cache."
        );
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L282-289)
```rust
            if version >= CACHE_SIZE_EVICTION_LOWER_BOUND {
                let key = CacheEntry::build_key(
                    version - CACHE_SIZE_EVICTION_LOWER_BOUND,
                    self.storage_format,
                )
                .to_string();
                redis_pipeline.cmd("DEL").arg(key).ignore();
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L142-157)
```rust
    pub fn into_transaction(self) -> Transaction {
        match self {
            CacheEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                Transaction::decode(decompressed.as_slice()).expect("proto deserialization failed.")
            },
            CacheEntry::Base64UncompressedProto(bytes) => {
                let bytes: Vec<u8> = base64::decode(bytes).expect("base64 decoding failed.");
                Transaction::decode(bytes.as_slice()).expect("proto deserialization failed.")
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/in_memory_cache.rs (L357-364)
```rust
            let values = conn.mget::<Vec<String>, Vec<Vec<u8>>>(keys).await?;
            // If any of the values are empty, we return an error.
            if values.iter().any(|v| v.is_empty()) {
                return Err(anyhow::anyhow!(format!(
                    "Failed to fetch all the keys; fetch size {}",
                    values.len()
                )));
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L128-165)
```rust
            let cache_worker_latest = self.cache_operator.get_latest_version().await?.unwrap();

            // batches tracks the start version of the batches to fetch. 1000 at the time
            let mut batches = vec![];
            let mut start_version = batch_start_version;
            while start_version + (FILE_ENTRY_TRANSACTION_COUNT) < cache_worker_latest {
                batches.push(start_version);
                start_version += FILE_ENTRY_TRANSACTION_COUNT;
                if batches.len() >= MAX_CONCURRENT_BATCHES {
                    break;
                }
            }
            // we're too close to the head
            if batches.is_empty() {
                debug!(
                    batch_start_version = batch_start_version,
                    cache_worker_latest = cache_worker_latest,
                    "[Filestore] No enough version yet, need 1000 versions at least"
                );
                tokio::time::sleep(Duration::from_millis(
                    AHEAD_OF_CACHE_SLEEP_DURATION_IN_MILLIS,
                ))
                .await;
                continue;
            }

            // Create thread and fetch transactions
            let mut tasks = vec![];

            for start_version in batches {
                let mut cache_operator_clone = self.cache_operator.clone();
                let mut file_store_operator_clone = self.file_store_operator.clone_box();
                let task = tokio::spawn(async move {
                    let fetch_start_time = std::time::Instant::now();
                    let transactions = cache_operator_clone
                        .get_transactions(start_version, FILE_ENTRY_TRANSACTION_COUNT)
                        .await
                        .unwrap();
```
