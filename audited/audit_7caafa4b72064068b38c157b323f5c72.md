# Audit Report

## Title
Race Condition in Table Info Backup Service Causes Concurrent Directory Deletion During Upload

## Summary
The `backup_the_snapshot_and_cleanup()` function contains a time-of-check-time-of-use (TOCTOU) race condition where concurrent backup iterations can cause one iteration to delete a snapshot directory while another iteration is still compressing or uploading it, leading to backup failures and potential service disruption.

## Finding Description

The vulnerability exists in the backup loop mechanism within the indexer-grpc table-info service. The backup service runs periodically every 5 seconds [1](#0-0) , scanning for snapshot directories and backing them up to GCS.

The race condition occurs due to these factors:

1. **Metadata Check at Function Start**: The `backup_the_snapshot_and_cleanup()` function reads the backup metadata once at the beginning to determine if an epoch has already been backed up [2](#0-1) .

2. **Early Directory Deletion**: If the metadata shows the epoch is already backed up (`metadata.epoch >= epoch`), the function immediately deletes the local snapshot directory [3](#0-2) .

3. **Long-Running Backup Operations**: The backup process involves CPU-intensive compression in a blocking task [4](#0-3)  followed by network I/O for uploading large files [5](#0-4) , which can easily exceed the 5-second loop interval.

4. **Acknowledged But Unhandled**: The code contains an explicit TODO comment acknowledging this issue is not handled [6](#0-5) .

**Attack Scenario:**
- T=0s: Iteration 1 starts backing up epoch 100, reads metadata (epoch=99), begins compression/upload
- T=5s: Iteration 2 starts while iteration 1 is still uploading
- T=5s: Iteration 2 finds epoch 100 directory still exists
- T=7s: Iteration 1 finishes upload, updates metadata to epoch 100, deletes directory [7](#0-6) 
- T=7s: Iteration 2 fails when trying to read files that were just deleted, or iteration 1 panics when cleanup fails

The vulnerability breaks the operational integrity guarantee that backup operations should complete atomically without interference.

## Impact Explanation

This qualifies as **High Severity** according to the Aptos bug bounty criteria for the following reasons:

1. **Service Disruption**: The race condition can cause panics with `.unwrap()` on failed directory operations [8](#0-7)  and [9](#0-8) , potentially crashing the backup service.

2. **Data Availability Risk**: Failed backups mean missing epoch snapshots, compromising disaster recovery capabilities. If the indexer database is lost, restoration will be incomplete.

3. **Repeated Failures**: Large snapshots (common in production) will consistently trigger this race, making the backup system unreliable.

While this doesn't directly affect consensus or on-chain execution, it qualifies as "API crashes" and "Significant protocol violations" under High Severity, as the indexer-grpc service is critical infrastructure for data availability and querying.

## Likelihood Explanation

**Likelihood: HIGH** for production deployments:

1. **Timing Window**: The 5-second loop interval [10](#0-9)  is shorter than typical backup operations for large databases. Multi-gigabyte snapshots routinely take >5 seconds to compress and upload.

2. **No Synchronization**: There are no locks, mutexes, or atomic operations protecting the snapshot directory from concurrent access across backup iterations.

3. **Natural Occurrence**: This requires no attacker action - it happens automatically in normal operation when backups are slow due to network conditions, large data size, or system load.

4. **Deterministic Trigger**: Any scenario where `(compression_time + upload_time) > 5 seconds` will trigger the race condition with high probability.

## Recommendation

Implement proper synchronization to prevent concurrent backup operations on the same epoch:

1. **Add Epoch Lock Tracking**: Use a `HashSet<u64>` wrapped in `Arc<Mutex<>>` to track epochs currently being backed up:

```rust
pub struct TableInfoService {
    // ... existing fields ...
    epochs_being_backed_up: Arc<Mutex<HashSet<u64>>>,
}
```

2. **Acquire Lock Before Backup**: In `backup_the_snapshot_and_cleanup()`, check and mark the epoch as in-progress:

```rust
async fn backup_the_snapshot_and_cleanup(
    context: Arc<ApiContext>,
    backup_restore_operator: Arc<GcsBackupRestoreOperator>,
    epoch: u64,
    epochs_in_progress: Arc<Mutex<HashSet<u64>>>,
) {
    // Try to acquire lock on this epoch
    {
        let mut in_progress = epochs_in_progress.lock().unwrap();
        if in_progress.contains(&epoch) {
            aptos_logger::info!(
                epoch = epoch,
                "[Table Info] Epoch already being backed up by another iteration. Skipping."
            );
            return;
        }
        in_progress.insert(epoch);
    }
    
    // ... rest of function ...
    
    // Release lock when done
    let mut in_progress = epochs_in_progress.lock().unwrap();
    in_progress.remove(&epoch);
}
```

3. **Use Temporary Marker Files**: Alternatively, create a `.in_progress` marker file alongside the snapshot directory before starting backup, and remove it when complete. Check for this marker before attempting backup.

4. **Increase Loop Interval**: Consider increasing `TABLE_INFO_SNAPSHOT_CHECK_INTERVAL_IN_SECS` to 30-60 seconds to reduce race likelihood, though this doesn't eliminate it.

## Proof of Concept

```rust
#[cfg(test)]
mod race_condition_test {
    use super::*;
    use std::sync::Arc;
    use std::time::Duration;
    use tokio::time::sleep;
    
    #[tokio::test]
    async fn test_concurrent_backup_race_condition() {
        // Setup: Create a snapshot directory and backup operator
        let temp_dir = tempfile::tempdir().unwrap();
        let snapshot_dir = temp_dir.path().join("snapshot_chain_1_epoch_100");
        std::fs::create_dir_all(&snapshot_dir).unwrap();
        
        // Create a large file to simulate slow compression/upload
        let large_file = snapshot_dir.join("large_data.db");
        let data = vec![0u8; 100_000_000]; // 100 MB
        std::fs::write(&large_file, data).unwrap();
        
        let context = Arc::new(create_test_context(temp_dir.path()));
        let backup_operator = Arc::new(create_test_backup_operator());
        
        // Simulate two concurrent backup iterations
        let context1 = context.clone();
        let backup_operator1 = backup_operator.clone();
        let snapshot_dir1 = snapshot_dir.clone();
        
        let task1 = tokio::spawn(async move {
            // First iteration starts backup
            backup_the_snapshot_and_cleanup(
                context1,
                backup_operator1,
                100,
            ).await;
        });
        
        // Wait 5 seconds (simulating the loop interval)
        sleep(Duration::from_secs(5)).await;
        
        let context2 = context.clone();
        let backup_operator2 = backup_operator.clone();
        
        let task2 = tokio::spawn(async move {
            // Second iteration starts while first is still running
            backup_the_snapshot_and_cleanup(
                context2,
                backup_operator2,
                100,
            ).await;
        });
        
        // Wait for both tasks
        let result1 = task1.await;
        let result2 = task2.await;
        
        // At least one task should fail due to directory deletion race
        // In practice, this will cause panics due to .unwrap() on failed operations
        assert!(result1.is_err() || result2.is_err(), 
            "Race condition should cause at least one task to fail");
        
        // Verify the snapshot directory may or may not exist depending on timing
        // This demonstrates the non-deterministic nature of the race
    }
}
```

**Notes:**

This vulnerability is a classic TOCTOU race condition in the backup service. While it doesn't directly compromise blockchain consensus or on-chain state, it severely affects the reliability of the indexer backup system, which is critical for operational resilience and disaster recovery. The explicit TODO comment indicates this was a known limitation that was never addressed. The use of `.unwrap()` on filesystem operations that can fail due to concurrent access transforms this race condition into a potential panic/crash condition, elevating its severity.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L28-28)
```rust
const TABLE_INFO_SNAPSHOT_CHECK_INTERVAL_IN_SECS: u64 = 5;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L94-96)
```rust
                        tokio::time::sleep(Duration::from_secs(
                            TABLE_INFO_SNAPSHOT_CHECK_INTERVAL_IN_SECS,
                        ))
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L544-544)
```rust
    let backup_metadata = backup_restore_operator.get_metadata().await;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L576-584)
```rust
        if metadata.epoch >= epoch {
            aptos_logger::info!(
                epoch = epoch,
                snapshot_folder_name = snapshot_folder_name,
                "[Table Info] Snapshot already backed up. Skipping the backup."
            );
            // Remove the snapshot directory.
            std::fs::remove_dir_all(snapshot_dir).unwrap();
            return;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L599-599)
```rust
    // TODO: add checks to handle concurrent backup jobs.
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L190-205)
```rust
        let tar_file = task::spawn_blocking(move || {
            aptos_logger::info!(
                snapshot_tar_file_name = snapshot_tar_file_name.as_str(),
                "[Table Info] Compressing the folder."
            );
            let result = create_tar_gz(snapshot_path_closure.clone(), &snapshot_tar_file_name);
            aptos_logger::info!(
                snapshot_tar_file_name = snapshot_tar_file_name.as_str(),
                result = result.is_ok(),
                "[Table Info] Compressed the folder."
            );
            result
        })
        .await
        .context("Failed to spawn task to create snapshot backup file.")?
        .context("Failed to create tar.gz file in blocking task")?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L223-237)
```rust
        match self
            .gcs_client
            .upload_streamed_object(
                &UploadObjectRequest {
                    bucket: self.bucket_name.clone(),
                    ..Default::default()
                },
                file_stream,
                &UploadType::Simple(Media {
                    name: filename.clone().into(),
                    content_type: Borrowed(TAR_FILE_TYPE),
                    content_length: None,
                }),
            )
            .await
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L240-246)
```rust
                self.update_metadata(chain_id, epoch).await?;
                let snapshot_path_clone = snapshot_path.clone();
                fs::remove_file(&tar_file)
                    .and_then(|_| fs::remove_dir_all(snapshot_path_clone))
                    .await
                    .expect("Failed to clean up after db snapshot upload");
                aptos_logger::info!(
```
