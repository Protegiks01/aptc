# Audit Report

## Title
DAG Broadcast Premature Abortion Causes Validator Crash and Potential Consensus Divergence

## Summary
The `rb_handles` queue in `DagDriver` uses a bounded deque that evicts old broadcast tasks when full. When evicted, the `DropGuard` immediately aborts the associated broadcast, which can interrupt certified node distribution mid-flight. This causes validators to have inconsistent DAG views, leading to validator crashes when computing strong links and potential consensus safety violations.

## Finding Description

The `DagDriver` maintains a `BoundedVecDeque` called `rb_handles` that stores `(DropGuard, u64)` tuples for managing broadcast tasks. When a validator advances through rounds quickly and exceeds the window size, the oldest entry is evicted. [1](#0-0) [2](#0-1) 

When `push_back` is called on a full queue, the oldest item is evicted from the front: [3](#0-2) 

The evicted `DropGuard` is immediately dropped when it goes out of scope, triggering the abort of the broadcast task: [4](#0-3) 

The broadcast task consists of two phases joined together:
1. Broadcasting the uncertified node and collecting signatures
2. Broadcasting the certified node once enough signatures are collected [5](#0-4) 

**Critical Issue**: When the abort occurs (potentially many rounds later when the queue fills up), the certified node broadcast may be interrupted mid-flight. This means:

1. Some validators receive the certified node and add it to their DAG
2. Some validators (including potentially the validator itself via self-send) do not receive it
3. Different validators have inconsistent DAG state

When any validator tries to advance to a subsequent round, it must compute strong links from the previous round: [6](#0-5) 

The strong links calculation checks if nodes in that round have sufficient voting power (2f+1): [7](#0-6) 

**Validator Crash**: If a validator is missing its own certified node from round R in its DAG, and this causes insufficient voting power in round R, then `get_strong_links_for_round` returns `None`. The code then executes `unwrap_or_else` with an assertion that only allows empty strong links for round 1. For any round > 1, this assertion fails and the validator panics.

**Consensus Divergence**: Different validators having different DAG views (some with the node, some without) can lead to:
- Different strong link selections
- Different anchor elections  
- Different block ordering
- Potential consensus safety violations

## Impact Explanation

This vulnerability has two severity levels depending on manifestation:

**High Severity** - Validator crashes occur when:
- A validator's own certified node is missing from its DAG due to aborted broadcast
- It attempts to compute strong links for the next round
- Insufficient voting power causes the assertion to fail
- The validator panics and becomes unavailable

This meets the High severity criteria: "Validator node slowdowns" and "Significant protocol violations"

**Critical Severity** - Consensus divergence when:
- Different validators have different subsets of certified nodes in their DAGs
- They compute different strong links and make different progress
- Different blocks get committed across the network
- Consensus safety invariant is violated

This meets the Critical severity criteria: "Consensus/Safety violations"

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability can trigger during normal operation without malicious intent:

1. **Network-based trigger**: If a validator experiences temporary network delays causing it to fall behind, then catches up rapidly by advancing many rounds quickly, this naturally fills the `rb_handles` queue and triggers evictions.

2. **Window size dependency**: The smaller the `window_size_config`, the more likely this occurs. Typical window sizes of 10-20 rounds mean advancing just 11-21 rounds triggers eviction.

3. **No attacker required**: This is a protocol-level bug, not an attack. Any validator can encounter this during normal operation, especially during:
   - Network partitions followed by recovery
   - Validator restarts with catch-up
   - Periods of high round advancement velocity

4. **Reliable broadcast timing**: The vulnerability depends on the broadcast not completing before eviction. Given network latency and asynchronous operations, there's a realistic window where broadcasts can be interrupted.

## Recommendation

**Immediate Fix**: Change eviction policy to only evict broadcasts that have fully completed both phases:

```rust
// In broadcast_node, track completion status
struct BroadcastHandle {
    drop_guard: DropGuard,
    timestamp: u64,
    completed: Arc<AtomicBool>,
}

// Mark as completed when both node and certified node broadcasts finish
let completed = Arc::new(AtomicBool::new(false));
let completed_clone = completed.clone();

let task = async move {
    core_task.await;
    completed_clone.store(true, Ordering::Release);
};

// Only allow eviction of completed broadcasts
// Or prevent eviction entirely - let old tasks run to completion
```

**Better Solution**: Remove premature eviction entirely. The `rb_handles` queue should track broadcasts for observability but not control their lifecycle. Broadcasts should run to completion regardless of round advancement. The window size should only limit DAG storage, not active broadcasts.

**Alternative**: Ensure certified nodes are added to the local DAG immediately after certification, before broadcasting, so the validator always has its own nodes regardless of broadcast completion.

## Proof of Concept

```rust
// Rust test to demonstrate the issue
#[tokio::test]
async fn test_premature_broadcast_abort() {
    // Setup: Create DagDriver with small window size (e.g., 5)
    let window_size = 5;
    let driver = create_test_dag_driver(window_size).await;
    
    // Step 1: Enter round 1 and broadcast node
    driver.enter_new_round(1).await;
    
    // Step 2: Quickly advance through rounds 2-7
    // This fills rb_handles queue (capacity 5)
    for round in 2..=7 {
        driver.enter_new_round(round).await;
    }
    
    // Step 3: Verify round 1's broadcast was aborted
    // by checking if the associated task was cancelled
    
    // Step 4: Check if driver's own certified node from round 1
    // is missing from its DAG
    let dag = driver.dag.read();
    let round_1_nodes = dag.get_round_iter(1);
    
    // Step 5: Try to enter round 8, which needs strong links from round 7
    // If round 7's node is missing, this should panic
    driver.enter_new_round(8).await; // Expected: panic
}
```

The PoC demonstrates that rapid round advancement causes old broadcasts to abort, leading to missing nodes in the DAG and subsequent validator panics when computing strong links.

## Notes

The root cause is the use of `DropGuard` for lifecycle management of async broadcasts combined with a bounded queue that evicts based on capacity rather than completion status. The `BoundedVecDeque` implementation correctly evicts FIFO (not out of order), but the eviction itself is premature - it aborts broadcasts that haven't completed, violating the assumption that all certified nodes get fully distributed before the validator moves on.

The vulnerability is exacerbated by the fact that validators rely on having consistent DAG views for consensus, and the strong links calculation has no fallback mechanism for missing nodes beyond an assertion that only allows empty links for round 1.

### Citations

**File:** consensus/src/dag/dag_driver.rs (L57-57)
```rust
    rb_handles: Mutex<BoundedVecDeque<(DropGuard, u64)>>,
```

**File:** consensus/src/dag/dag_driver.rs (L103-103)
```rust
            rb_handles: Mutex::new(BoundedVecDeque::new(window_size_config as usize)),
```

**File:** consensus/src/dag/dag_driver.rs (L214-219)
```rust
            let strong_links = dag_reader
                .get_strong_links_for_round(new_round - 1, &self.epoch_state.verifier)
                .unwrap_or_else(|| {
                    assert_eq!(new_round, 1, "Only expect empty strong links for round 1");
                    vec![]
                });
```

**File:** consensus/src/dag/dag_driver.rs (L333-370)
```rust
        let node_broadcast = async move {
            debug!(LogSchema::new(LogEvent::BroadcastNode), id = node.id());

            defer!( observe_round(timestamp, RoundStage::NodeBroadcasted); );
            rb.broadcast(node, signature_builder)
                .await
                .expect("Broadcast cannot fail")
        };
        let certified_broadcast = async move {
            let Ok(certificate) = rx.await else {
                error!("channel closed before receiving ceritifcate");
                return;
            };

            debug!(
                LogSchema::new(LogEvent::BroadcastCertifiedNode),
                id = node_clone.id()
            );

            defer!( observe_round(timestamp, RoundStage::CertifiedNodeBroadcasted); );
            let certified_node =
                CertifiedNode::new(node_clone, certificate.signatures().to_owned());
            let certified_node_msg = CertifiedNodeMessage::new(
                certified_node,
                latest_ledger_info.get_latest_ledger_info(),
            );
            rb2.broadcast(certified_node_msg, cert_ack_set)
                .await
                .expect("Broadcast cannot fail until cancelled")
        };
        let core_task = join(node_broadcast, certified_broadcast);
        let author = self.author;
        let task = async move {
            debug!("{} Start reliable broadcast for round {}", author, round);
            core_task.await;
            debug!("Finish reliable broadcast for round {}", round);
        };
        tokio::spawn(Abortable::new(task, abort_registration));
```

**File:** consensus/src/dag/dag_driver.rs (L373-380)
```rust
        if let Some((_handle, prev_round_timestamp)) = self
            .rb_handles
            .lock()
            .push_back((DropGuard::new(abort_handle), timestamp))
        {
            // TODO: this observation is inaccurate.
            observe_round(prev_round_timestamp, RoundStage::Finished);
        }
```

**File:** crates/aptos-collections/src/bounded_vec_deque.rs (L28-38)
```rust
    pub fn push_back(&mut self, item: T) -> Option<T> {
        let oldest = if self.is_full() {
            self.inner.pop_front()
        } else {
            None
        };

        self.inner.push_back(item);
        assert!(self.inner.len() <= self.capacity);
        oldest
    }
```

**File:** consensus/src/dag/dag_store.rs (L346-367)
```rust
    pub fn get_strong_links_for_round(
        &self,
        round: Round,
        validator_verifier: &ValidatorVerifier,
    ) -> Option<Vec<NodeCertificate>> {
        if validator_verifier
            .check_voting_power(
                self.get_round_iter(round)?
                    .map(|node_status| node_status.as_node().metadata().author()),
                true,
            )
            .is_ok()
        {
            Some(
                self.get_round_iter(round)?
                    .map(|node_status| node_status.as_node().certificate())
                    .collect(),
            )
        } else {
            None
        }
    }
```
