# Audit Report

## Title
Epoch Mismatch in BlockStore Rebuild Allows Stale Timeout Certificates to Persist

## Summary
The `BlockStore::rebuild()` function bypasses epoch filtering by rolling over the previous timeout certificate instead of using the epoch-filtered certificate from RecoveryData, causing consensus liveness failures when nodes perform fast-forward sync across epoch boundaries.

## Finding Description

This vulnerability exists in the interaction between `RecoveryData` epoch filtering and `BlockStore::rebuild()` during fast-forward synchronization.

**Step 1: RecoveryData Properly Filters by Epoch**

When RecoveryData is constructed during fast-forward sync, the timeout certificate is explicitly filtered to only retain certificates matching the current epoch: [1](#0-0) 

This epoch filtering uses the epoch from the recovery root block to ensure consistency.

**Step 2: RecoveryData.take() Discards the Filtered TC**

The `take()` method returns only `(root, root_metadata, blocks, quorum_certs)` but NOT the epoch-filtered timeout certificate: [2](#0-1) 

Despite the filtered TC being available via `highest_2chain_timeout_certificate()` method: [3](#0-2) 

**Step 3: rebuild() Rolls Over Stale TC**

During fast-forward sync, the flow is: `sync_to_highest_quorum_cert()` calls `fast_forward_sync()` which creates a properly filtered RecoveryData, then calls `.take()`, and passes the result to `rebuild()`: [4](#0-3) 

The `rebuild()` method retrieves the OLD timeout certificate from the current BlockStore without epoch validation: [5](#0-4) 

This effectively bypasses the epoch filtering that was done in RecoveryData.

**Step 4: Usage Path**

The stale timeout certificate is then used when the node attempts to participate in consensus. The RoundManager passes it to safety rules during timeout signing: [6](#0-5) [7](#0-6) 

**Step 5: Verification Failure**

SafetyRules verifies the timeout certificate against the current epoch's validator set: [8](#0-7) 

The `verify_tc()` method calls the certificate's verify function with the epoch state's validator verifier: [9](#0-8) 

The TwoChainTimeoutCertificate verification validates aggregate signatures against the provided validator verifier: [10](#0-9) 

Since the timeout certificate contains signatures from a previous epoch's validators but is being verified against the new epoch's validator set, verification fails if validator sets differ between epochs.

## Impact Explanation

This is a **High Severity** issue per Aptos bug bounty criteria:

1. **Validator Node Inability to Participate**: Affected nodes cannot construct votes or timeout messages because safety rules reject the stale timeout certificate, completely preventing consensus participation. This is more severe than mere slowdowns - the node becomes non-functional for consensus purposes.

2. **Network Security Impact**: If validator nodes are affected, they cannot fulfill their consensus duties, reducing the network's Byzantine fault tolerance margin and potentially affecting block production if multiple validators are impacted.

3. **Requires Manual Intervention**: The affected node remains stuck until manual restart or the stale timeout certificate is somehow cleared from memory. There is no automatic recovery mechanism.

This qualifies as High Severity under the Aptos bug bounty category of "Validator Node Slowdowns" - though the impact is actually complete inability to participate rather than just slowdowns.

## Likelihood Explanation

The likelihood is **Medium** because:

**Triggering Conditions:**
- Node falls significantly behind due to network issues, maintenance, or hardware problems (common in distributed systems)
- Node performs fast-forward sync via `sync_to_highest_quorum_cert()` (automatic when behind)
- Sync crosses an epoch boundary where validator sets have changed (regular occurrence in Aptos)
- The epoch change broadcast is processed asynchronously, creating a window where the node operates with mismatched data

**Why It's Realistic:**
- Fast-forward sync can cross epoch boundaries as evidenced by the epoch change broadcast logic: [11](#0-10) 

- The bug is definitively present in the code and violates the design intent where RecoveryData performs explicit epoch filtering
- Epoch transitions happen regularly in Aptos networks
- Validator nodes experiencing brief outages are not uncommon

**Mitigating Factors:**
- Clean epoch transitions via `initiate_new_epoch()` create fresh BlockStores that avoid this issue
- The vulnerability window may be limited in duration
- Not all fast-forward syncs cross epoch boundaries

## Recommendation

The fix should ensure that the epoch-filtered timeout certificate from RecoveryData is used during rebuild rather than rolling over the stale certificate. Modify the code path to:

1. Have `RecoveryData.take()` also return the `highest_2chain_timeout_certificate`, OR
2. Pass the RecoveryData's timeout certificate directly to `rebuild()`, OR  
3. Make `rebuild()` accept an optional timeout certificate parameter from the RecoveryData instead of automatically rolling over the old one

Recommended fix for `RecoveryData.take()`:

```rust
pub fn take(self) -> (RootInfo, RootMetadata, Vec<Block>, Vec<QuorumCert>, Option<TwoChainTimeoutCertificate>) {
    (
        self.root,
        self.root_metadata,
        self.blocks,
        self.quorum_certs,
        self.highest_2chain_timeout_certificate,
    )
}
```

And update the call sites accordingly to use the epoch-filtered TC.

## Proof of Concept

The vulnerability can be demonstrated through the following scenario:

1. Set up a validator node in epoch N with a timeout certificate from epoch N
2. Simulate the network transitioning to epoch N+1 with a different validator set
3. Take the validator node offline during the transition
4. Bring the node back online and trigger fast-forward sync
5. Observe that `fast_forward_sync()` creates RecoveryData with epoch N+1 data and TC filtered to None (since no TC exists for new epoch)
6. Observe that `.take()` discards this filtered value
7. Observe that `rebuild()` rolls over the TC from epoch N
8. Attempt to have the node sign a timeout via SafetyRules
9. Observe verification failure when the epoch N TC is verified against epoch N+1 validator set

The execution path is:
```
sync_to_highest_quorum_cert() 
  -> fast_forward_sync() [creates filtered RecoveryData]
  -> recovery_data.take() [discards filtered TC]
  -> rebuild(root, metadata, blocks, qcs) [uses stale TC]
  -> RoundManager.process_local_timeout()
  -> safety_rules.sign_timeout_with_qc(timeout, stale_tc)
  -> verify_tc(stale_tc) [fails due to epoch mismatch]
```

## Notes

The vulnerability is specific to fast-forward sync operations that span epoch boundaries. Normal epoch transitions via `initiate_new_epoch()` properly shut down and restart with fresh BlockStores, avoiding this issue. However, the asynchronous nature of epoch change processing during fast-forward sync creates a race condition window where the stale TC persists.

### Citations

**File:** consensus/src/persistent_liveness_storage.rs (L414-417)
```rust
            highest_2chain_timeout_certificate: match highest_2chain_timeout_cert {
                Some(tc) if tc.epoch() == epoch => Some(tc),
                _ => None,
            },
```

**File:** consensus/src/persistent_liveness_storage.rs (L429-436)
```rust
    pub fn take(self) -> (RootInfo, RootMetadata, Vec<Block>, Vec<QuorumCert>) {
        (
            self.root,
            self.root_metadata,
            self.blocks,
            self.quorum_certs,
        )
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L444-446)
```rust
    pub fn highest_2chain_timeout_certificate(&self) -> Option<TwoChainTimeoutCertificate> {
        self.highest_2chain_timeout_certificate.clone()
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L295-314)
```rust
        let (root, root_metadata, blocks, quorum_certs) = Self::fast_forward_sync(
            &highest_quorum_cert,
            &highest_commit_cert,
            retriever,
            self.storage.clone(),
            self.execution_client.clone(),
            self.payload_manager.clone(),
            self.order_vote_enabled,
            self.window_size,
            Some(self),
        )
        .await?
        .take();
        info!(
            LogSchema::new(LogEvent::CommitViaSync).round(self.ordered_root().round()),
            committed_round = root.commit_root_block.round(),
            block_id = root.commit_root_block.id(),
        );
        self.rebuild(root, root_metadata, blocks, quorum_certs)
            .await;
```

**File:** consensus/src/block_storage/sync_manager.rs (L316-324)
```rust
        if highest_commit_cert.ledger_info().ledger_info().ends_epoch() {
            retriever
                .network
                .send_epoch_change(EpochChangeProof::new(
                    vec![highest_quorum_cert.ledger_info().clone()],
                    /* more = */ false,
                ))
                .await;
        }
```

**File:** consensus/src/block_storage/block_store.rs (L370-379)
```rust
        // Rollover the previous highest TC from the old tree to the new one.
        let prev_2chain_htc = self
            .highest_2chain_timeout_cert()
            .map(|tc| tc.as_ref().clone());
        let _ = Self::build(
            root,
            root_metadata,
            blocks,
            quorum_certs,
            prev_2chain_htc,
```

**File:** consensus/src/round_manager.rs (L1014-1021)
```rust
                let signature = self
                    .safety_rules
                    .lock()
                    .sign_timeout_with_qc(
                        &timeout,
                        self.block_store.highest_2chain_timeout_cert().as_deref(),
                    )
                    .context("[RoundManager] SafetyRules signs 2-chain timeout")?;
```

**File:** consensus/src/round_manager.rs (L1068-1075)
```rust
                let signature = self
                    .safety_rules
                    .lock()
                    .sign_timeout_with_qc(
                        &timeout,
                        self.block_store.highest_2chain_timeout_cert().as_deref(),
                    )
                    .context("[RoundManager] SafetyRules signs 2-chain timeout")?;
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L32-34)
```rust
        if let Some(tc) = timeout_cert {
            self.verify_tc(tc)?;
        }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L180-188)
```rust
    fn verify_tc(&self, tc: &TwoChainTimeoutCertificate) -> Result<(), Error> {
        let epoch_state = self.epoch_state()?;

        if !self.skip_sig_verify {
            tc.verify(&epoch_state.verifier)
                .map_err(|e| Error::InvalidTimeoutCertificate(e.to_string()))?;
        }
        Ok(())
    }
```

**File:** consensus/consensus-types/src/timeout_2chain.rs (L141-169)
```rust
    pub fn verify(&self, validators: &ValidatorVerifier) -> anyhow::Result<()> {
        let hqc_round = self.timeout.hqc_round();
        // Verify the highest timeout validity.
        let (timeout_result, sig_result) = rayon::join(
            || self.timeout.verify(validators),
            || {
                let timeout_messages: Vec<_> = self
                    .signatures_with_rounds
                    .get_voters_and_rounds(
                        &validators
                            .get_ordered_account_addresses_iter()
                            .collect_vec(),
                    )
                    .into_iter()
                    .map(|(_, round)| TimeoutSigningRepr {
                        epoch: self.timeout.epoch(),
                        round: self.timeout.round(),
                        hqc_round: round,
                    })
                    .collect();
                let timeout_messages_ref: Vec<_> = timeout_messages.iter().collect();
                validators.verify_aggregate_signatures(
                    &timeout_messages_ref,
                    self.signatures_with_rounds.sig(),
                )
            },
        );
        timeout_result?;
        sig_result?;
```
