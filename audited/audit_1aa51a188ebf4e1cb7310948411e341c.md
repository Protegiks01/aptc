# Audit Report

## Title
Asynchronous State-Merkle Commit Causes Proof Generation Failures for Recently Committed Transactions

## Summary
State values and Jellyfish Merkle tree nodes are committed **asynchronously** in separate operations, creating a critical time window where committed transaction state values exist in `STATE_VALUE_CF_NAME` but their corresponding Merkle tree nodes are not yet available in `JELLYFISH_MERKLE_NODE_CF_NAME`. This causes `MissingRootError` when clients attempt to generate Merkle proofs for recently committed versions, violating the fundamental invariant that all committed state must be verifiable via Merkle proofs.

## Finding Description

The vulnerability stems from the architectural decision to separate state KV commits (synchronous) from Merkle tree commits (asynchronous via background threads).

**Commit Flow:**

1. `pre_commit_ledger` executes and calls `commit_state_kv_and_ledger_metadata`: [1](#0-0) 

2. State values are written synchronously to `STATE_VALUE_CF_NAME` and `StateKvCommitProgress` is updated: [2](#0-1) 

3. `buffered_state.update()` is called, which **asynchronously** queues Merkle tree node commits: [3](#0-2) 

4. The async commit message flows through `StateSnapshotCommitter` and eventually `StateMerkleBatchCommitter`: [4](#0-3) 

5. `commit_ledger` writes `OverallCommitProgress`, marking the version as "committed": [5](#0-4) 

**The Race Condition:**

Between steps 5 and when the async Merkle commit completes, clients can query `get_state_value_with_proof_by_version_ext` for the committed version. The query:

1. Passes the pruning check (version is within window): [6](#0-5) 

2. Attempts to read the Merkle root node at `NodeKey::new_empty_path(version)`: [7](#0-6) 

3. **Fails with `MissingRootError(version)`** because the async commit hasn't completed yet: [8](#0-7) 

This violates the critical invariant that **once a version is committed (OverallCommitProgress written), all data for that version must be accessible and verifiable via Merkle proofs**.

## Impact Explanation

**Severity: High to Critical**

This issue breaks the **State Consistency** invariant (#4: "State transitions must be atomic and verifiable via Merkle proofs"). The impacts include:

1. **Proof Generation Failures**: Clients attempting to retrieve state with proofs for recently committed transactions receive `MissingRootError`, breaking the fundamental guarantee that committed state is verifiable.

2. **State Sync Disruption**: State sync protocols relying on Merkle proofs for recently committed versions may fail or experience delays, potentially affecting network availability.

3. **Client Application Failures**: Applications expecting immediate proof availability after transaction commitment will encounter errors, leading to poor user experience or application failures.

4. **Consensus Verification Issues**: If validators attempt to verify state proofs for recently committed blocks during consensus or synchronization, they may encounter failures.

While the async commit eventually completes (typically within milliseconds to seconds), the window exists and can be exploited or occur naturally under heavy load when the background committer thread is backlogged.

## Likelihood Explanation

**Likelihood: Medium to High**

This race condition occurs **naturally during normal operation** and does not require attacker sophistication:

- **High Load Scenarios**: Under heavy transaction load, the async commit queue builds up, extending the vulnerability window from milliseconds to seconds or longer.

- **Immediate Queries**: Applications that commit transactions and immediately query for proofs (a common pattern) will reliably trigger this issue.

- **No Special Privileges Required**: Any client can trigger this by querying `get_state_value_with_proof_by_version` immediately after observing a transaction commit.

- **Observable via `get_synced_version`**: Clients can see when `OverallCommitProgress` advances and attempt queries, making the race condition easy to hit.

The issue is particularly likely in production environments with high throughput where the separation between "committed" (ledger) and "provable" (Merkle tree) is most pronounced.

## Recommendation

**Recommended Fix: Synchronize OverallCommitProgress with Merkle Commit Completion**

The `OverallCommitProgress` should only be written **after** the Merkle tree nodes are committed, ensuring atomicity between state visibility and proof availability.

**Implementation Approach:**

1. Modify `commit_ledger` to block until the async Merkle commit completes for the target version:

```rust
// In aptosdb_writer.rs commit_ledger()
fn commit_ledger(
    &self,
    version: Version,
    ledger_info_with_sigs: Option<&LedgerInfoWithSignatures>,
    chunk_opt: Option<ChunkToCommit>,
) -> Result<()> {
    // ... existing validation ...
    
    // NEW: Wait for Merkle commit to complete before marking as committed
    self.state_store.buffered_state().lock().sync_commit_up_to_version(version)?;
    
    let mut ledger_batch = SchemaBatch::new();
    if let Some(li) = ledger_info_with_sigs {
        self.check_and_put_ledger_info(version, li, &mut ledger_batch)?;
    }
    
    // Now safe to write OverallCommitProgress
    ledger_batch.put::<DbMetadataSchema>(
        &DbMetadataKey::OverallCommitProgress,
        &DbMetadataValue::Version(version),
    )?;
    self.ledger_db.metadata_db().write_schemas(ledger_batch)?;
    
    // ... existing post-commit ...
}
```

2. Add version-aware synchronization to `BufferedState`:

```rust
// In buffered_state.rs
impl BufferedState {
    pub(crate) fn sync_commit_up_to_version(&mut self, target_version: Version) -> Result<()> {
        // Wait until last_snapshot.version() >= target_version
        while self.last_snapshot.version().unwrap_or(0) < target_version {
            self.drain_commits();
            // Check if target is now committed
        }
        Ok(())
    }
}
```

**Alternative Approach (Less Preferred):**

Add explicit checks in `get_state_value_with_proof_by_version_ext` to return a specific error when Merkle nodes are not yet available, distinguishing this from pruning:

```rust
pub(super) fn error_if_state_merkle_not_ready(
    &self,
    data_type: &str,
    version: Version,
) -> Result<()> {
    // Check if root exists before claiming data is available
    if !self.state_store.state_merkle_db.root_exists_at_version(version)? {
        bail!("{} Merkle proof not yet available for version {}, try again shortly", data_type, version);
    }
    Ok(())
}
```

## Proof of Concept

```rust
#[test]
fn test_state_merkle_race_condition() {
    use aptos_types::transaction::Transaction;
    use std::thread;
    use std::time::Duration;
    
    // Setup test database
    let tmpdir = TempPath::new();
    let db = AptosDB::new_for_test(&tmpdir);
    
    // Commit a transaction chunk
    let chunk = create_test_chunk(/* ... */);
    db.pre_commit_ledger(chunk.clone(), false).unwrap();
    
    // Immediately commit ledger (this writes OverallCommitProgress)
    let version = chunk.expect_last_version();
    db.commit_ledger(version, None, Some(chunk)).unwrap();
    
    // Race condition: Try to get proof immediately
    // The state value exists but Merkle nodes may not
    let state_key = StateKey::arbitrary();
    let key_hash = CryptoHash::hash(&state_key);
    
    // This may fail with MissingRootError depending on async commit timing
    let result = db.get_state_value_with_proof_by_version_ext(
        &key_hash,
        version,
        0,
        false
    );
    
    match result {
        Err(AptosDbError::MissingRootError(v)) if v == version => {
            // Vulnerability confirmed: Committed version has no Merkle root
            println!("VULNERABILITY: MissingRootError for committed version {}", version);
            assert!(false, "Race condition confirmed");
        },
        Ok(_) => {
            // Async commit completed before query (timing dependent)
            println!("Async commit completed before query - retry test");
        },
        Err(e) => {
            panic!("Unexpected error: {:?}", e);
        }
    }
    
    // Wait and verify it eventually succeeds
    thread::sleep(Duration::from_millis(100));
    db.get_state_value_with_proof_by_version_ext(&key_hash, version, 0, false)
        .expect("Should succeed after async commit completes");
}
```

## Notes

The recovery mechanism `sync_commit_progress` handles crash scenarios by truncating inconsistent data during restart [9](#0-8) , so the issue is **transient during normal operation** rather than persistent after crashes. However, the transient window is sufficient to cause application failures and violates the atomicity guarantee expected from a committed transaction.

The architectural decision to use async Merkle commits was likely made for performance optimization (avoiding blocking on expensive Merkle tree calculations), but it introduces this consistency gap that must be addressed either through proper synchronization or explicit API contracts that distinguish "committed to ledger" from "Merkle proofs available."

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L103-107)
```rust
            ledger_batch.put::<DbMetadataSchema>(
                &DbMetadataKey::OverallCommitProgress,
                &DbMetadataValue::Version(version),
            )?;
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L324-384)
```rust
    fn commit_state_kv_and_ledger_metadata(
        &self,
        chunk: &ChunkToCommit,
        skip_index_and_usage: bool,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_state_kv_and_ledger_metadata"]);

        let mut ledger_metadata_batch = SchemaBatch::new();
        let mut sharded_state_kv_batches = self.state_kv_db.new_sharded_native_batches();

        self.state_store.put_state_updates(
            chunk.state,
            &chunk.state_update_refs.per_version,
            chunk.state_reads,
            &mut ledger_metadata_batch,
            &mut sharded_state_kv_batches,
        )?;

        // Write block index if event index is skipped.
        if skip_index_and_usage {
            for (i, txn_out) in chunk.transaction_outputs.iter().enumerate() {
                for event in txn_out.events() {
                    if let Some(event_key) = event.event_key() {
                        if *event_key == new_block_event_key() {
                            let version = chunk.first_version + i as Version;
                            LedgerMetadataDb::put_block_info(
                                version,
                                event,
                                &mut ledger_metadata_batch,
                            )?;
                        }
                    }
                }
            }
        }

        ledger_metadata_batch
            .put::<DbMetadataSchema>(
                &DbMetadataKey::LedgerCommitProgress,
                &DbMetadataValue::Version(chunk.expect_last_version()),
            )
            .unwrap();

        let _timer =
            OTHER_TIMERS_SECONDS.timer_with(&["commit_state_kv_and_ledger_metadata___commit"]);
        rayon::scope(|s| {
            s.spawn(|_| {
                self.ledger_db
                    .metadata_db()
                    .write_schemas(ledger_metadata_batch)
                    .unwrap();
            });
            s.spawn(|_| {
                self.state_kv_db
                    .commit(chunk.expect_last_version(), None, sharded_state_kv_batches)
                    .unwrap();
            });
        });

        Ok(())
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L177-208)
```rust
    pub(crate) fn commit(
        &self,
        version: Version,
        state_kv_metadata_batch: Option<SchemaBatch>,
        sharded_state_kv_batches: ShardedStateKvSchemaBatch,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit"]);
        {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_shards"]);
            THREAD_MANAGER.get_io_pool().scope(|s| {
                let mut batches = sharded_state_kv_batches.into_iter();
                for shard_id in 0..NUM_STATE_SHARDS {
                    let state_kv_batch = batches
                        .next()
                        .expect("Not sufficient number of sharded state kv batches");
                    s.spawn(move |_| {
                        // TODO(grao): Consider propagating the error instead of panic, if necessary.
                        self.commit_single_shard(version, shard_id, state_kv_batch)
                            .unwrap_or_else(|err| {
                                panic!("Failed to commit shard {shard_id}: {err}.")
                            });
                    });
                }
            });
        }
        if let Some(batch) = state_kv_metadata_batch {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_metadata"]);
            self.state_kv_metadata_db.write_schemas(batch)?;
        }

        self.write_progress(version)
    }
```

**File:** storage/aptosdb/src/state_store/buffered_state.rs (L156-179)
```rust
    pub fn update(
        &mut self,
        new_state: LedgerStateWithSummary,
        estimated_new_items: usize,
        sync_commit: bool,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["buffered_state___update"]);

        let old_state = self.current_state_locked().clone();
        assert!(new_state.is_descendant_of(&old_state));

        self.estimated_items += estimated_new_items;
        let version = new_state.last_checkpoint().version();

        let last_checkpoint = new_state.last_checkpoint().clone();
        // Commit state only if there is a new checkpoint, eases testing and make estimated
        // buffer size a tad more realistic.
        let checkpoint_to_commit_opt =
            (old_state.next_version() < last_checkpoint.next_version()).then_some(last_checkpoint);
        *self.current_state_locked() = new_state;
        self.maybe_commit(checkpoint_to_commit_opt, sync_commit);
        Self::report_last_checkpoint_version(version);
        Ok(())
    }
```

**File:** storage/aptosdb/src/state_store/state_merkle_batch_committer.rs (L52-115)
```rust
    pub fn run(self) {
        while let Ok(msg) = self.state_merkle_batch_receiver.recv() {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["batch_committer_work"]);
            match msg {
                CommitMessage::Data(StateMerkleCommit {
                    snapshot,
                    hot_batch,
                    cold_batch,
                }) => {
                    let base_version = self.persisted_state.get_state_summary().version();
                    let current_version = snapshot
                        .version()
                        .expect("Current version should not be None");

                    // commit jellyfish merkle nodes
                    let _timer =
                        OTHER_TIMERS_SECONDS.timer_with(&["commit_jellyfish_merkle_nodes"]);
                    if let Some(hot_state_merkle_batch) = hot_batch {
                        self.commit(
                            self.state_db
                                .hot_state_merkle_db
                                .as_ref()
                                .expect("Hot state merkle db must exist."),
                            current_version,
                            hot_state_merkle_batch,
                        )
                        .expect("Hot state merkle nodes commit failed.");
                    }
                    self.commit(&self.state_db.state_merkle_db, current_version, cold_batch)
                        .expect("State merkle nodes commit failed.");

                    info!(
                        version = current_version,
                        base_version = base_version,
                        root_hash = snapshot.summary().root_hash(),
                        hot_root_hash = snapshot.summary().hot_root_hash(),
                        "State snapshot committed."
                    );
                    LATEST_SNAPSHOT_VERSION.set(current_version as i64);
                    // TODO(HotState): no pruning for hot state right now, since we always reset it
                    // upon restart.
                    self.state_db
                        .state_merkle_pruner
                        .maybe_set_pruner_target_db_version(current_version);
                    self.state_db
                        .epoch_snapshot_pruner
                        .maybe_set_pruner_target_db_version(current_version);

                    self.check_usage_consistency(&snapshot).unwrap();

                    snapshot
                        .summary()
                        .global_state_summary
                        .log_generation("buffered_state_commit");
                    self.persisted_state.set(snapshot);
                },
                CommitMessage::Sync(finish_sender) => finish_sender.send(()).unwrap(),
                CommitMessage::Exit => {
                    break;
                },
            }
        }
        trace!("State merkle batch committing thread exit.")
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L273-303)
```rust
    pub(super) fn error_if_state_merkle_pruned(
        &self,
        data_type: &str,
        version: Version,
    ) -> Result<()> {
        let min_readable_version = self
            .state_store
            .state_db
            .state_merkle_pruner
            .get_min_readable_version();
        if version >= min_readable_version {
            return Ok(());
        }

        let min_readable_epoch_snapshot_version = self
            .state_store
            .state_db
            .epoch_snapshot_pruner
            .get_min_readable_version();
        if version >= min_readable_epoch_snapshot_version {
            self.ledger_db.metadata_db().ensure_epoch_ending(version)
        } else {
            bail!(
                "{} at version {} is pruned. snapshots are available at >= {}, epoch snapshots are available at >= {}",
                data_type,
                version,
                min_readable_version,
                min_readable_epoch_snapshot_version,
            )
        }
    }
```

**File:** storage/jellyfish-merkle/src/lib.rs (L717-741)
```rust
    pub fn get_with_proof_ext(
        &self,
        key: &HashValue,
        version: Version,
        target_root_depth: usize,
    ) -> Result<(Option<(HashValue, (K, Version))>, SparseMerkleProofExt)> {
        // Empty tree just returns proof with no sibling hash.
        let mut next_node_key = NodeKey::new_empty_path(version);
        let mut out_siblings = Vec::with_capacity(8); // reduces reallocation
        let nibble_path = NibblePath::new_even(key.to_vec());
        let mut nibble_iter = nibble_path.nibbles();

        // We limit the number of loops here deliberately to avoid potential cyclic graph bugs
        // in the tree structure.
        for nibble_depth in 0..=ROOT_NIBBLE_HEIGHT {
            let next_node = self
                .reader
                .get_node_with_tag(&next_node_key, "get_proof")
                .map_err(|err| {
                    if nibble_depth == 0 {
                        AptosDbError::MissingRootError(version)
                    } else {
                        err
                    }
                })?;
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```
