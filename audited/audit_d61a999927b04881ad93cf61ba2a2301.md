# Audit Report

## Title
Consensus Publisher Resource Exhaustion via Unlimited Subscriptions and Blocking Thread Pool Saturation

## Summary
The consensus publisher allows unlimited peer subscriptions without rate limiting, enabling an attacker to exhaust the global blocking thread pool through concurrent serialization of large consensus messages. This causes severe validator node slowdowns affecting consensus operations, state synchronization, and overall node responsiveness.

## Finding Description

The vulnerability exists in the consensus observer publisher's message serialization mechanism. The attack unfolds through the following chain:

**1. Unlimited Subscriptions:** The publisher accepts subscription requests from any peer without enforcing a limit. [1](#0-0) 

The `add_active_subscriber` function simply inserts peers into a HashSet with no capacity checks: [2](#0-1) 

**2. Broadcast to All Subscribers:** When a consensus message is published (e.g., blocks with transactions), it broadcasts to all active subscribers: [3](#0-2) 

**3. Serialization with spawn_blocking:** Each message is serialized in a blocking task to avoid blocking the async runtime: [4](#0-3) 

**4. Buffering Based on CPU Count:** The concurrent serialization tasks are limited by `max_parallel_serialization_tasks`, which defaults to the number of CPUs: [5](#0-4) 

**5. Global Thread Pool Limit:** All `spawn_blocking` calls across the entire node share a global pool of only 64 threads: [6](#0-5) 

**Attack Scenario:**
1. Attacker connects 100+ malicious peers (up to `MAX_INBOUND_CONNECTIONS` = 100): [7](#0-6) 

2. Each peer subscribes to consensus updates (no limit enforced)

3. When large consensus blocks are published (containing many transactions via `BlockTransactionPayload`): [8](#0-7) 

4. 100+ serialization messages are queued in the outbound channel: [9](#0-8) 

5. On a high-CPU server (e.g., 64 cores), up to 64 serialization tasks run concurrently via `.buffered()`: [10](#0-9) 

6. This exhausts all 64 blocking threads with CPU-intensive BCS serialization

7. Critical consensus operations that also depend on `spawn_blocking` are starved:
   - Consensus pipeline (execute, ledger_update, pre_commit, commit_ledger)
   - State sync chunk processing
   - Storage service request handling

The invariant broken is **Resource Limits** (#9): "All operations must respect gas, storage, and computational limits." The publisher fails to limit subscriptions and protect the shared blocking thread pool resource.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria for "Validator node slowdowns."

**Affected Systems:**
- **Consensus Pipeline:** Block execution and commitment operations use `spawn_blocking`, causing severe delays in block production and finalization
- **State Synchronization:** Chunk processing stalls, preventing nodes from catching up
- **Storage Service:** Request handling becomes unresponsive, affecting API and query operations
- **Network-wide Impact:** If multiple validators are attacked simultaneously, the entire network's consensus throughput degrades significantly

The attack causes operational disruption without requiring validator privileges or stake majority, making it accessible to any network participant.

## Likelihood Explanation

**Likelihood: HIGH**

**Attacker Requirements:**
- Ability to connect multiple network peers (up to 100 connections)
- Send subscription requests (simple RPC calls)
- No authentication or stake required
- No insider access needed

**Complexity: LOW**
- Attack is straightforward to execute
- No timing dependencies or race conditions
- Works reliably once sufficient subscribers are established
- Triggered automatically when consensus publishes blocks

**Exploitation Window:**
- Persistent: Attacker maintains subscriptions over extended periods
- Amplified during high transaction volume (larger blocks = slower serialization)
- Affects all validators running consensus publishers (enabled by default on validators)

## Recommendation

Implement subscription limits and protect the blocking thread pool:

```rust
// In ConsensusObserverConfig
pub struct ConsensusObserverConfig {
    // ... existing fields ...
    
    /// Maximum number of active publisher subscriptions
    pub max_publisher_subscriptions: usize,
}

impl Default for ConsensusObserverConfig {
    fn default() -> Self {
        Self {
            // ... existing defaults ...
            max_publisher_subscriptions: 10, // Reasonable limit
        }
    }
}

// In ConsensusPublisher
fn add_active_subscriber(&self, peer_network_id: PeerNetworkId) -> Result<(), Error> {
    let mut subscribers = self.active_subscribers.write();
    
    // Check subscription limit
    if subscribers.len() >= self.consensus_observer_config.max_publisher_subscriptions {
        return Err(Error::TooManySubscriptions(format!(
            "Maximum subscriptions reached: {}",
            self.consensus_observer_config.max_publisher_subscriptions
        )));
    }
    
    subscribers.insert(peer_network_id);
    Ok(())
}

// Update subscription handler to enforce limit
ConsensusObserverRequest::Subscribe => {
    match self.add_active_subscriber(peer_network_id) {
        Ok(_) => {
            info!(/* subscription success */);
            response_sender.send(ConsensusObserverResponse::SubscribeAck);
        },
        Err(error) => {
            warn!(/* subscription rejected */);
            // Send rejection response or close connection
        }
    }
}
```

Additionally, consider:
1. **Reduce `max_parallel_serialization_tasks`** to a lower value (e.g., 8) to prevent consuming too many blocking threads
2. **Use a dedicated thread pool** for serialization separate from the global blocking pool
3. **Implement per-peer rate limiting** for subscription requests
4. **Add monitoring** for blocking thread pool utilization

## Proof of Concept

```rust
#[tokio::test]
async fn test_subscription_resource_exhaustion() {
    use aptos_config::config::ConsensusObserverConfig;
    use aptos_config::network_id::{NetworkId, PeerNetworkId};
    use aptos_types::PeerId;
    use consensus::consensus_observer::publisher::consensus_publisher::ConsensusPublisher;
    use consensus::consensus_observer::network::observer_client::ConsensusObserverClient;
    use consensus::consensus_observer::network::observer_message::{
        BlockTransactionPayload, ConsensusObserverMessage,
    };
    use aptos_network::application::{storage::PeersAndMetadata, NetworkClient};
    use std::sync::Arc;
    use aptos_types::block_info::BlockInfo;
    use maplit::hashmap;

    // Create publisher with default config (unlimited subscriptions)
    let network_id = NetworkId::Validator;
    let peers_and_metadata = PeersAndMetadata::new(&[network_id]);
    let network_client = NetworkClient::new(vec![], vec![], hashmap![], peers_and_metadata);
    let observer_client = Arc::new(ConsensusObserverClient::new(network_client));
    let (publisher, mut receiver) = ConsensusPublisher::new(
        ConsensusObserverConfig::default(),
        observer_client,
    );

    // Subscribe many peers (simulating attack)
    let num_malicious_peers = 100;
    for i in 0..num_malicious_peers {
        let peer_id = PeerId::random();
        let peer_network_id = PeerNetworkId::new(network_id, peer_id);
        
        // Subscribe peer (no limit check - vulnerability)
        publisher.add_active_subscriber(peer_network_id);
    }

    // Verify all peers were subscribed
    assert_eq!(publisher.get_active_subscribers().len(), num_malicious_peers);

    // Publish large consensus message (block with transactions)
    let large_transactions = vec![/* create large transaction payload */];
    let block_payload = ConsensusObserverMessage::new_block_payload_message(
        BlockInfo::empty(),
        BlockTransactionPayload::new_quorum_store_inline_hybrid(
            large_transactions,
            vec![],
            Some(10000),
            Some(100000),
            vec![],
            true,
        ),
    );

    // This triggers serialization for all 100 subscribers
    publisher.publish_message(block_payload);

    // With max_parallel_serialization_tasks = 64 (on 64-core machine),
    // all 64 blocking threads would be consumed by serialization,
    // starving other critical operations
    
    // Verify many messages were queued
    let mut message_count = 0;
    while receiver.try_recv().is_ok() {
        message_count += 1;
    }
    assert_eq!(message_count, num_malicious_peers);
}
```

## Notes

This vulnerability demonstrates a classic resource exhaustion pattern where an unbounded operation (subscriptions) combined with expensive work (serialization) can saturate a shared, limited resource (blocking thread pool). The fix requires both limiting subscriptions and protecting the thread pool from monopolization by any single subsystem.

The attack is particularly effective because:
1. Serialization of large consensus blocks is genuinely CPU-intensive
2. The blocking thread pool is shared across critical node operations
3. No authentication or stake is required to subscribe
4. The default configuration enables the vulnerability on all validators

### Citations

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L56-59)
```rust
        // Create the outbound message sender and receiver
        let max_network_channel_size = consensus_observer_config.max_network_channel_size as usize;
        let (outbound_message_sender, outbound_message_receiver) =
            mpsc::channel(max_network_channel_size);
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L94-96)
```rust
    fn add_active_subscriber(&self, peer_network_id: PeerNetworkId) {
        self.active_subscribers.write().insert(peer_network_id);
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L181-192)
```rust
            ConsensusObserverRequest::Subscribe => {
                // Add the peer to the set of active subscribers
                self.add_active_subscriber(peer_network_id);
                info!(LogSchema::new(LogEntry::ConsensusPublisher)
                    .event(LogEvent::Subscription)
                    .message(&format!(
                        "New peer subscribed to consensus updates! Peer: {:?}",
                        peer_network_id
                    )));

                // Send a simple subscription ACK
                response_sender.send(ConsensusObserverResponse::SubscribeAck);
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L212-232)
```rust
    pub fn publish_message(&self, message: ConsensusObserverDirectSend) {
        // Get the active subscribers
        let active_subscribers = self.get_active_subscribers();

        // Send the message to all active subscribers
        for peer_network_id in &active_subscribers {
            // Send the message to the outbound receiver for publishing
            let mut outbound_message_sender = self.outbound_message_sender.clone();
            if let Err(error) =
                outbound_message_sender.try_send((*peer_network_id, message.clone()))
            {
                // The message send failed
                warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::SendDirectSendMessage)
                        .message(&format!(
                            "Failed to send outbound message to the receiver for peer {:?}! Error: {:?}",
                            peer_network_id, error
                    )));
            }
        }
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L286-299)
```rust
    tokio::spawn(async move {
        // Create the message serialization task
        let consensus_observer_client_clone = consensus_observer_client.clone();
        let serialization_task =
            outbound_message_receiver.map(move |(peer_network_id, message)| {
                // Spawn a new blocking task to serialize the message
                let consensus_observer_client_clone = consensus_observer_client_clone.clone();
                tokio::task::spawn_blocking(move || {
                    let message_label = message.get_label();
                    let serialized_message = consensus_observer_client_clone
                        .serialize_message_for_peer(&peer_network_id, message);
                    (peer_network_id, serialized_message, message_label)
                })
            });
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L303-304)
```rust
        serialization_task
            .buffered(consensus_observer_config.max_parallel_serialization_tasks)
```

**File:** config/src/config/consensus_observer_config.rs (L69-69)
```rust
            max_parallel_serialization_tasks: num_cpus::get(), // Default to the number of CPUs
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** config/src/config/network_config.rs (L44-44)
```rust
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
```

**File:** consensus/src/consensus_observer/network/observer_message.rs (L498-509)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub enum BlockTransactionPayload {
    // TODO: deprecate InQuorumStore* variants
    DeprecatedInQuorumStore(PayloadWithProof),
    DeprecatedInQuorumStoreWithLimit(PayloadWithProofAndLimit),
    QuorumStoreInlineHybrid(PayloadWithProofAndLimit, Vec<BatchInfo>),
    OptQuorumStore(
        TransactionsWithProof,
        /* OptQS and Inline Batches */ Vec<BatchInfo>,
    ),
    QuorumStoreInlineHybridV2(TransactionsWithProof, Vec<BatchInfo>),
}
```
