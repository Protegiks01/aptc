Based on my thorough validation of this security claim, I can confirm this is a **VALID vulnerability**. Let me provide the complete audit report with proper code citations:

---

# Audit Report

## Title
State Merkle Shard Pruner Progress Regression Due to Missing Version Validation

## Summary
The `prune()` function in `StateMerkleShardPruner` lacks validation to ensure `target_version >= current_progress` before entering the pruning loop. When `target_version < current_progress`, the function regresses the shard's pruner progress metadata, violating the invariant that pruner progress should be monotonically non-decreasing. This creates persistent state inconsistency in the database.

## Finding Description

The vulnerability exists in the initialization path of `StateMerkleShardPruner`. [1](#0-0) 

When `StateMerkleShardPruner::new()` is called, it reads the shard's existing progress via `get_or_initialize_subpruner_progress` and then immediately calls `prune(progress, metadata_progress, usize::MAX)` to catch up the shard. However, there is no validation that `metadata_progress >= progress`. [2](#0-1) 

The `prune()` function does not validate the relationship between `current_progress` and `target_version` before entering the loop. [3](#0-2)  When `target_version < current_progress`:

1. The call to `get_stale_node_indices()` seeks to `current_progress` but filters for indices with `stale_since_version <= target_version` [4](#0-3) 
2. Since all indices from `current_progress` onwards have `stale_since_version >= current_progress > target_version`, no indices are collected
3. The function sets `done = true` and writes `target_version` as the new progress metadata [5](#0-4) 
4. **This regresses the shard progress from `current_progress` to `target_version`**

The scenario occurs when database state becomes inconsistent, such as:
- Partial database restoration (metadata DB from older backup, shard DB from newer backup)
- Database corruption where shard progress > metadata progress
- Race conditions during crash recovery

In contrast, the main pruner properly validates this condition to prevent regression. [6](#0-5) 

The same vulnerability pattern also exists in `StateKvShardPruner`. [7](#0-6) 

## Impact Explanation

**Medium Severity** - State inconsistencies requiring intervention.

The vulnerability causes:
1. **Pruner metadata corruption**: Progress is regressed, creating false information about what data has been pruned
2. **Operational confusion**: Metrics and monitoring report incorrect pruner state  
3. **Potential state sync issues**: Nodes may incorrectly believe data is available when it has been pruned
4. **Persistent inconsistency**: The corrupted metadata persists across restarts until manually corrected

This does NOT cause:
- Consensus violations (consensus does not depend on pruner progress)
- Fund loss or theft
- Node crashes or liveness failures
- Direct security exploitation

The impact aligns with the Aptos Bug Bounty "Medium Severity" category for "State inconsistencies requiring intervention."

## Likelihood Explanation

**Low to Medium Likelihood**

The vulnerability requires database state inconsistency where `shard_progress > metadata_progress`. This can occur through:

1. **Partial database restoration** - Operator restores metadata DB from an older backup while shard DBs are from a newer backup
2. **Database corruption** - File system or storage corruption creates inconsistent progress values
3. **Concurrent pruner bugs** - Other bugs that cause metadata and shard progress to drift

While these scenarios are not common in normal operation, they are realistic operational edge cases that can occur in production environments during:
- Disaster recovery procedures
- Database maintenance operations
- Storage failures
- Software upgrade issues

The vulnerability is **NOT directly exploitable** by an unprivileged attacker, as it requires database state manipulation that only occurs through operational issues or system failures.

## Recommendation

Add validation at the beginning of the `prune()` function to prevent progress regression:

```rust
pub(in crate::pruner) fn prune(
    &self,
    current_progress: Version,
    target_version: Version,
    max_nodes_to_prune: usize,
) -> Result<()> {
    // Add this validation
    if current_progress >= target_version {
        return Ok(());
    }
    
    loop {
        // ... existing code
    }
}
```

This matches the validation pattern used in the main pruner. [6](#0-5) 

Apply the same fix to `StateKvShardPruner::prune()`. [8](#0-7) 

## Proof of Concept

A Rust test demonstrating the regression:

```rust
#[test]
fn test_shard_pruner_progress_regression() {
    // Setup: Create a shard DB with progress at version 1000
    let shard_db = setup_test_db();
    shard_db.put::<DbMetadataSchema>(
        &DbMetadataKey::StateMerkleShardPrunerProgress(0),
        &DbMetadataValue::Version(1000)
    ).unwrap();
    
    // Trigger: Initialize pruner with metadata_progress=500
    let pruner = StateMerkleShardPruner::new(
        0, 
        shard_db.clone(), 
        500  // metadata_progress < shard progress
    ).unwrap();
    
    // Verify: Progress has regressed from 1000 to 500
    let final_progress = shard_db
        .get::<DbMetadataSchema>(&DbMetadataKey::StateMerkleShardPrunerProgress(0))
        .unwrap()
        .unwrap()
        .expect_version();
    
    assert_eq!(final_progress, 500);  // Progress regressed!
}
```

## Notes

This vulnerability demonstrates a critical difference between the main pruner implementation and the shard pruner implementation. The main pruner includes proper validation to prevent progress regression, while the shard pruners do not. This inconsistency indicates that the missing validation is not intentional but rather an oversight in the shard pruner implementations.

The vulnerability affects both `StateMerkleShardPruner` and `StateKvShardPruner`, suggesting a systematic issue in how shard pruners handle initialization with potentially inconsistent database state.

### Citations

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L31-56)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &S::progress_metadata_key(Some(shard_id)),
            metadata_progress,
        )?;
        let myself = Self {
            shard_id,
            db_shard,
            _phantom: PhantomData,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up {} shard {shard_id}.",
            S::name(),
        );
        myself.prune(progress, metadata_progress, usize::MAX)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L58-100)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
        max_nodes_to_prune: usize,
    ) -> Result<()> {
        loop {
            let mut batch = SchemaBatch::new();
            let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
                &self.db_shard,
                current_progress,
                target_version,
                max_nodes_to_prune,
            )?;

            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;

            let mut done = true;
            if let Some(next_version) = next_version {
                if next_version <= target_version {
                    done = false;
                }
            }

            if done {
                batch.put::<DbMetadataSchema>(
                    &S::progress_metadata_key(Some(self.shard_id)),
                    &DbMetadataValue::Version(target_version),
                )?;
            }

            self.db_shard.write_schemas(batch)?;

            if done {
                break;
            }
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L65-67)
```rust
        if progress >= target_version {
            return Ok(progress);
        }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```
