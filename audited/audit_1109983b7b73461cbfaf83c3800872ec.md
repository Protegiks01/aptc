# Audit Report

## Title
Batch Encryption ID Prediction Enables Resource Waste and Capacity Manipulation Attacks

## Summary
Attackers can predict ciphertext IDs before creation and deliberately create multiple encrypted transactions with duplicate IDs. This exploits inefficiencies in the digest computation pipeline, causing validators to waste computational resources and potentially manipulate batch capacity limits.

## Finding Description

The batch encryption system allows ciphertext IDs to be predicted before creation because IDs are deterministically derived from Ed25519 verifying keys: [1](#0-0) 

During encryption, a random signing key is generated and its corresponding verifying key is hashed to produce the ID: [2](#0-1) 

An attacker can generate signing keys offline and compute their resulting IDs in advance. By reusing the same signing key across multiple encrypted transactions, the attacker creates multiple ciphertexts with identical IDs.

When validators process a batch of encrypted transactions, the `digest()` function creates an `IdSet` that includes all IDs, including duplicates: [3](#0-2) 

The `IdSet::add()` function does not check for duplicates: [4](#0-3) 

When evaluation proofs are computed, one proof is generated for each ID position (including duplicates), but only the last proof for each unique ID is retained in the HashMap: [5](#0-4) 

This causes two issues:

1. **Resource Waste**: Validators compute evaluation proofs for all duplicate IDs but discard all except the last one, wasting CPU cycles.

2. **Capacity Manipulation**: The batch capacity is computed as `ids.len().next_power_of_two()`, which includes duplicates. An attacker submitting many transactions with duplicate IDs artificially inflates the capacity calculation: [6](#0-5) 

If the inflated capacity exceeds the setup limit, the digest computation fails: [7](#0-6) 

## Impact Explanation

This vulnerability represents **Medium severity** according to Aptos bug bounty criteria:

- **Resource Exhaustion**: Forces validators to waste computational resources on redundant proof calculations
- **State Inconsistencies**: Can cause encrypted transaction batches to fail processing, requiring manual intervention
- **Limited DoS**: While currently mitigated by the 10-transaction truncation limit, this represents a design flaw that could enable broader attacks if limits are increased

The current implementation limits exploitation due to: [8](#0-7) 

However, this hardcoded limit is marked as "FIXME" indicating it may be removed, and the underlying vulnerability would persist.

## Likelihood Explanation

**Likelihood: Medium**

- **Attack Prerequisites**: Attacker only needs ability to submit encrypted transactions (no special privileges)
- **Complexity**: Low - attacker can easily generate signing keys offline and reuse them
- **Detectability**: Duplicate IDs are not validated during transaction submission
- **Current Mitigation**: Feature is gated and transaction count is limited, but fundamental issue remains

## Recommendation

Implement deduplication of IDs before digest computation:

1. **Add duplicate detection in `IdSet::from_slice()`**: Check for and remove duplicate IDs before adding to the set, or reject batches with duplicates.

2. **Add validation in transaction verification**: Check that each ciphertext in a batch has a unique ID and reject batches with duplicates:

```rust
// In FPTXWeighted::digest() or equivalent
fn digest(digest_key: &Self::DigestKey, cts: &[Self::Ciphertext], round: Self::Round) 
    -> anyhow::Result<(Self::Digest, Self::EvalProofsPromise)> {
    let ids: Vec<Id> = cts.iter().map(|ct| ct.id()).collect();
    
    // Check for duplicates
    let unique_ids: HashSet<&Id> = ids.iter().collect();
    if unique_ids.len() != ids.len() {
        return Err(anyhow!("Batch contains duplicate ciphertext IDs"));
    }
    
    let mut id_set = IdSet::from_slice(&ids).ok_or(anyhow!("Invalid ID set"))?;
    digest_key.digest(&mut id_set, round)
}
```

3. **Optimize eval proof computation**: Compute proofs only for unique IDs to prevent resource waste even if duplicates slip through.

## Proof of Concept

```rust
#[test]
fn test_duplicate_id_resource_waste() {
    use ark_std::rand::thread_rng;
    use aptos_crypto::arkworks::shamir::ShamirThresholdConfig;
    
    let mut rng = thread_rng();
    let tc = ShamirThresholdConfig::new(1, 1);
    let (ek, dk, _, _) = FPTX::setup_for_testing(rng.gen(), 16, 1, &tc).unwrap();
    
    // Create a signing key once
    let mut signing_key_bytes = [0u8; 32];
    rng.fill_bytes(&mut signing_key_bytes);
    let signing_key = SigningKey::from_bytes(&signing_key_bytes);
    let vk = signing_key.verifying_key();
    
    // Create multiple ciphertexts with the SAME ID (by reusing the signing key)
    let mut ciphertexts = Vec::new();
    for i in 0..10 {
        let plaintext = format!("message_{}", i);
        let associated_data = String::from("data");
        
        // Manually create ciphertext with reused verifying key
        let hashed_id = Id::from_verifying_key(&vk);
        let bibe_ct = ek.bibe_encrypt(&mut rng, &plaintext, hashed_id).unwrap();
        
        let ct = Ciphertext {
            vk,
            bibe_ct,
            associated_data_bytes: bcs::to_bytes(&associated_data).unwrap(),
            signature: signing_key.sign(&bcs::to_bytes(&(&bibe_ct, &bcs::to_bytes(&associated_data).unwrap())).unwrap()),
        };
        ciphertexts.push(ct);
    }
    
    // All ciphertexts have the same ID
    let id = ciphertexts[0].id();
    for ct in &ciphertexts {
        assert_eq!(ct.id(), id, "All IDs should be identical");
    }
    
    // Attempt to create digest - this will compute 10 eval proofs for the same ID
    let result = FPTX::digest(&dk, &ciphertexts, 0);
    
    // The capacity will be next_power_of_two(10) = 16
    // But only 1 unique ID exists
    // This demonstrates the resource waste
    assert!(result.is_ok(), "Digest should succeed but wastes resources");
}
```

This PoC demonstrates that multiple ciphertexts with identical IDs can be created, and the system will unnecessarily compute evaluation proofs for all of them despite only needing one per unique ID.

### Citations

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L36-41)
```rust
    pub fn from_verifying_key(vk: &VerifyingKey) -> Self {
        // using empty domain separator b/c this is a test implementation
        let field_hasher = <DefaultFieldHasher<Sha256> as HashToField<Fr>>::new(&[]);
        let field_element: [Fr; 1] = field_hasher.hash_to_field::<1>(&vk.to_bytes());
        Self::new(field_element[0])
    }
```

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L71-78)
```rust
    pub fn with_capacity(capacity: usize) -> Option<Self> {
        let capacity = capacity.next_power_of_two();
        Some(Self {
            poly_roots: Vec::new(),
            capacity,
            poly_coeffs: UncomputedCoeffs,
        })
    }
```

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L84-89)
```rust
    pub fn add(&mut self, id: &Id) {
        if self.poly_roots.len() >= self.capacity {
            panic!("Number of ids must be less than capacity");
        }
        self.poly_roots.push(id.root_x);
    }
```

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L124-146)
```rust
    pub fn compute_all_eval_proofs_with_setup(
        &self,
        setup: &crate::shared::digest::DigestKey,
        round: usize,
    ) -> HashMap<Id, G1Affine> {
        let pfs: Vec<G1Affine> = setup
            .fk_domain
            .eval_proofs_at_x_coords_naive_multi_point_eval(
                &self.poly_coeffs(),
                &self.poly_roots,
                round,
            )
            .iter()
            .map(|g| G1Affine::from(*g))
            .collect();

        HashMap::from_iter(
            self.as_vec()
                .into_iter()
                .zip(pfs)
                .collect::<Vec<(Id, G1Affine)>>(),
        )
    }
```

**File:** crates/aptos-batch-encryption/src/shared/ciphertext/mod.rs (L76-82)
```rust
        let mut signing_key_bytes: [u8; SECRET_KEY_LENGTH] = [0; SECRET_KEY_LENGTH];
        rng.fill_bytes(&mut signing_key_bytes);

        let signing_key: SigningKey = SigningKey::from_bytes(&signing_key_bytes);
        let vk = signing_key.verifying_key();
        let hashed_id = Id::from_verifying_key(&vk);
        let bibe_ct = self.bibe_encrypt(rng, plaintext, hashed_id)?;
```

**File:** crates/aptos-batch-encryption/src/schemes/fptx.rs (L100-110)
```rust
    fn digest(
        digest_key: &Self::DigestKey,
        cts: &[Self::Ciphertext],
        round: Self::Round,
    ) -> anyhow::Result<(Self::Digest, Self::EvalProofsPromise)> {
        let mut ids: IdSet<UncomputedCoeffs> =
            IdSet::from_slice(&cts.iter().map(|ct| ct.id()).collect::<Vec<Id>>())
                .ok_or(anyhow!(""))?;

        digest_key.digest(&mut ids, round)
    }
```

**File:** crates/aptos-batch-encryption/src/shared/digest.rs (L116-121)
```rust
        } else if ids.capacity() > self.tau_powers_g1[round].len() - 1 {
            Err(anyhow!(
                "Tried to compute a batch digest with size {}, where setup supports up to size {}",
                ids.capacity(),
                self.tau_powers_g1[round].len() - 1
            ))?
```

**File:** consensus/src/pipeline/decryption_pipeline_builder.rs (L68-76)
```rust
        // TODO(ibalajiarun): FIXME
        let len = 10;
        let encrypted_txns = if encrypted_txns.len() > len {
            let mut to_truncate = encrypted_txns;
            to_truncate.truncate(len);
            to_truncate
        } else {
            encrypted_txns
        };
```
