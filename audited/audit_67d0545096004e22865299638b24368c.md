# Audit Report

## Title
Race Condition in LocalFileStoreOperator Metadata Updates Due to Inconsistent Timing Constant Usage

## Summary
The `FILE_STORE_UPDATE_FREQUENCY_SECS` constant at line 17 is used inconsistently across file store implementations. The LocalFileStoreOperator uses 5-second intervals while GcsFileStoreOperator uses 200ms (`FILE_STORE_METADATA_TIMEOUT_MILLIS`). This inconsistency, combined with independent clone state and non-atomic file writes, creates a race condition where concurrent metadata updates can corrupt the metadata.json file, causing indexer crashes and requiring manual intervention.

## Finding Description

The vulnerability stems from three related issues in the indexer-grpc file store system:

**1. Inconsistent Timing Constants:** [1](#0-0) 

LocalFileStoreOperator uses this 5-second constant: [2](#0-1) 

While GcsFileStoreOperator uses a different 200ms constant: [3](#0-2) [4](#0-3) 

**2. Independent Clone State:**
The LocalFileStoreOperator struct uses `#[derive(Clone)]`, creating independent copies of the timing state: [5](#0-4) 

When the processor spawns parallel tasks, each gets an independent clone with its own `latest_metadata_update_timestamp`: [6](#0-5) 

**3. Non-Atomic Metadata Writes:**
The metadata update uses `tokio::fs::write()` directly without atomic write-then-rename pattern: [7](#0-6) 

Multiple clones can simultaneously pass the timing check and write different versions to metadata.json: [8](#0-7) 

**4. Panic on Corrupted Metadata:**
If concurrent writes corrupt the JSON, the indexer panics when reading: [9](#0-8) 

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program for the following reasons:

1. **State Inconsistencies Requiring Intervention**: Corrupted metadata.json requires manual file repair or deletion before the indexer can restart, meeting the Medium severity criterion of "State inconsistencies requiring intervention."

2. **API Crashes**: When metadata is corrupted, the indexer crashes on startup and cannot serve API requests, aligning with "API crashes" (High severity indicator), though the impact is limited to off-chain infrastructure.

3. **Limited Scope**: This affects the indexer-grpc infrastructure (off-chain component) rather than consensus or validator nodes. While critical for applications consuming indexer data, it does not directly impact blockchain state or consensus.

4. **Data Integrity Risk**: Applications relying on indexer metadata for transaction tracking may receive incorrect version information, potentially causing them to miss transactions or process incorrect data.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability is highly likely to manifest under normal operating conditions:

1. **Automatic Triggering**: The processor automatically spawns up to 50 concurrent tasks when processing transaction batches: [10](#0-9) 

2. **Timing Conditions**: After 5 seconds of operation, multiple concurrent tasks will simultaneously satisfy the timing check, causing race conditions.

3. **No External Attack Required**: This occurs during normal high-throughput operation without requiring attacker intervention.

4. **Production Deployments**: The backfiller also spawns concurrent tasks with cloned operators, exhibiting the same vulnerability: [11](#0-10) 

## Recommendation

**Fix 1: Use Atomic File Writes**

Implement the atomic write pattern used elsewhere in the codebase (write to temp file, then rename): [12](#0-11) 

Replace the direct write in `update_file_store_metadata_internal`:

```rust
async fn update_file_store_metadata_internal(
    &mut self,
    chain_id: u64,
    version: u64,
) -> anyhow::Result<()> {
    let metadata = FileStoreMetadata::new(chain_id, version, self.storage_format);
    let metadata_path = self.path.join(METADATA_FILE_NAME);
    
    // Atomic write: write to temp file then rename
    let temp_path = metadata_path.with_extension("tmp");
    tokio::fs::write(&temp_path, serde_json::to_vec(&metadata).unwrap()).await?;
    tokio::fs::rename(&temp_path, &metadata_path).await?;
    
    self.latest_metadata_update_timestamp = Some(std::time::Instant::now());
    Ok(())
}
```

**Fix 2: Centralize Metadata Updates**

Remove metadata updates from `upload_transaction_batch` in LocalFileStoreOperator to match GcsFileStoreOperator behavior, allowing only the main processor to update metadata via `update_file_store_metadata_with_timeout`.

**Fix 3: Unify Timing Constants**

Either use `FILE_STORE_UPDATE_FREQUENCY_SECS` consistently across both implementations, or document why different timing is required and ensure both use appropriate synchronization mechanisms.

## Proof of Concept

```rust
// Test demonstrating the race condition
#[tokio::test]
async fn test_concurrent_metadata_update_race() {
    use std::path::PathBuf;
    use tempfile::tempdir;
    
    // Setup local file store
    let temp_dir = tempdir().unwrap();
    let operator = LocalFileStoreOperator::new(temp_dir.path().to_path_buf(), false);
    
    // Create initial metadata
    let mut op1 = operator.clone();
    op1.update_file_store_metadata_internal(1, 1000).await.unwrap();
    
    // Simulate 50 concurrent tasks all trying to update metadata after 5+ seconds
    let mut handles = vec![];
    for i in 0..50 {
        let mut op_clone = operator.clone();
        let handle = tokio::spawn(async move {
            // Simulate passing the 5-second timing check
            // In real scenario, this would happen after 5 seconds
            op_clone.update_file_store_metadata_internal(1, 1000 + (i * 1000)).await
        });
        handles.push(handle);
    }
    
    // Wait for all tasks
    for handle in handles {
        let _ = handle.await;
    }
    
    // Try to read metadata - may be corrupted or have unexpected version
    let metadata = operator.get_file_store_metadata().await;
    match metadata {
        Some(m) => {
            // Version could be any of the 50 concurrent writes
            println!("Final version: {} (expected: 50000)", m.version);
            // In real scenario with corrupted JSON, this would panic
        },
        None => panic!("Metadata was lost"),
    }
}
```

**Notes:**
- This vulnerability specifically affects LocalFileStoreOperator, not GcsFileStoreOperator (which uses atomic GCS Object.create operations)
- The indexer is off-chain infrastructure, so this does not directly impact consensus or blockchain state
- However, applications depending on indexer data may experience service disruptions or incorrect transaction processing
- The fix should include comprehensive testing of concurrent metadata updates under high load

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/mod.rs (L17-17)
```rust
const FILE_STORE_UPDATE_FREQUENCY_SECS: u64 = 5;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L15-21)
```rust
#[derive(Clone)]
pub struct LocalFileStoreOperator {
    path: PathBuf,
    /// The timestamp of the latest metadata update; this is to avoid too frequent metadata update.
    latest_metadata_update_timestamp: Option<std::time::Instant>,
    storage_format: StorageFormat,
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L126-146)
```rust
    async fn update_file_store_metadata_internal(
        &mut self,
        chain_id: u64,
        version: u64,
    ) -> anyhow::Result<()> {
        let metadata = FileStoreMetadata::new(chain_id, version, self.storage_format);
        // If the metadata is not updated, the indexer will be restarted.
        let metadata_path = self.path.join(METADATA_FILE_NAME);
        info!(
            "Updating metadata file {} @ version {}",
            metadata_path.display(),
            version
        );
        match tokio::fs::write(metadata_path, serde_json::to_vec(&metadata).unwrap()).await {
            Ok(_) => {
                self.latest_metadata_update_timestamp = Some(std::time::Instant::now());
                Ok(())
            },
            Err(err) => Err(anyhow::Error::from(err)),
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L206-219)
```rust
        if let Some(ts) = self.latest_metadata_update_timestamp {
            // a periodic metadata update
            if (std::time::Instant::now() - ts).as_secs() > FILE_STORE_UPDATE_FREQUENCY_SECS {
                self.update_file_store_metadata_internal(
                    chain_id,
                    start_version + batch_size as u64,
                )
                .await?;
            }
        } else {
            // the first metadata update
            self.update_file_store_metadata_internal(chain_id, start_version + batch_size as u64)
                .await?;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/gcs.rs (L17-17)
```rust
const FILE_STORE_METADATA_TIMEOUT_MILLIS: u128 = 200;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/gcs.rs (L174-178)
```rust
        if self.file_store_metadata_last_updated.elapsed().as_millis()
            < FILE_STORE_METADATA_TIMEOUT_MILLIS
        {
            bail!("File store metadata is updated too frequently.")
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L21-21)
```rust
const MAX_CONCURRENT_BATCHES: usize = 50;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L157-159)
```rust
            for start_version in batches {
                let mut cache_operator_clone = self.cache_operator.clone();
                let mut file_store_operator_clone = self.file_store_operator.clone_box();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L58-61)
```rust
    pub fn from_bytes(bytes: Vec<u8>) -> Self {
        serde_json::from_slice(bytes.as_slice())
            .expect("FileStoreMetadata json deserialization failed.")
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L173-175)
```rust
        for _ in 0..self.backfill_processing_task_count {
            tracing::info!("Creating a new task");
            let mut current_file_store_operator = file_store_operator.clone_box();
```

**File:** secure/storage/src/on_disk.rs (L64-69)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
```
