# Audit Report

## Title
State Snapshot Progress Race Condition Prevents Node Synchronization

## Summary
A race condition in the parallel execution of KV and tree restoration during state snapshot synchronization allows the KV database to commit `StateSnapshotProgress` metadata before Merkle tree proof verification completes. When tree verification fails after KV commit, the node enters an inconsistent state where the progress metadata claims chunks were processed, but the Merkle tree database lacks those chunks, permanently preventing completion of state synchronization.

## Finding Description

The vulnerability exists in how `StateSnapshotRestore` handles parallel restoration of state snapshots. The system violates the **State Consistency** invariant (state transitions must be atomic and verifiable via Merkle proofs) by allowing non-atomic updates across the KV and tree databases. [1](#0-0) 

In `StateSnapshotRestore::add_chunk`, two operations execute in parallel via `IO_POOL.join()`:

1. **kv_fn**: Calls `StateValueRestore::add_chunk` which filters chunks based on existing progress, writes KV pairs to the database, and **commits `StateSnapshotProgress`** immediately
2. **tree_fn**: Calls `JellyfishMerkleRestore::add_chunk_impl` which processes the chunk, **verifies the Merkle proof**, then writes tree nodes [2](#0-1) 

The critical issue is in the `write_kv_batch` call which commits to the database immediately: [3](#0-2) 

The `StateSnapshotProgress` (containing `key_hash` and `usage`) is written to the database and committed before tree verification completes. If tree verification fails: [4](#0-3) 

The error from tree verification is returned, but the KV database commit **cannot be rolled back**. This creates an inconsistent state:
- **KV Progress**: Claims chunks up to key K are processed
- **Tree DB**: Lacks nodes for keys beyond key C
- **KV DB**: Contains state values up to key K

On subsequent synchronization attempts, `StateValueRestore::add_chunk` loads the progress and skips already-processed keys: [5](#0-4) 

Meanwhile, `JellyfishMerkleRestore` initializes with `previous_leaf` from the tree database (which stops at key C): [6](#0-5) 

When the next chunk arrives, the KV restorer skips it (already in progress), but the tree restorer expects earlier chunks that were never verified. Subsequent Merkle proofs fail because they assume a tree structure that doesn't exist, permanently blocking state synchronization.

## Impact Explanation

**High Severity** - This vulnerability prevents nodes from completing state synchronization, which has significant impact on network operations:

1. **New Validator Onboarding**: Nodes attempting to join the network via state sync cannot complete initialization, reducing network decentralization
2. **Node Recovery**: Validators recovering from failures or data loss cannot resynchronize, reducing network resilience  
3. **Persistent Failure**: No automatic recovery mechanism exists; manual intervention is required to clear corrupted progress metadata
4. **Attack Vector**: Malicious peers can deliberately send chunks with invalid proofs to trigger this condition

This meets the "Validator node slowdowns" and "Significant protocol violations" criteria under High Severity ($50,000 category), as it disrupts normal node operations and violates state synchronization guarantees.

## Likelihood Explanation

**Medium to High Likelihood**:

- **Trigger Condition**: Requires a state snapshot chunk with an invalid Merkle proof to arrive during synchronization
- **Attacker Capability**: Any malicious peer in the P2P network can send corrupted chunks; no privileged access required
- **Occurrence Rate**: State sync from untrusted peers is a normal operation; network instability or Byzantine peers make this realistic
- **No Defense**: The code lacks transactional guarantees or rollback mechanisms to prevent the inconsistent state

The vulnerability can manifest both from intentional attacks and from benign network issues (corrupted data in transit), making exploitation feasible.

## Recommendation

Implement atomic commit semantics for state snapshot restoration by ensuring tree verification completes successfully before committing KV progress:

**Option 1: Sequential Execution with Verification-First**
```rust
fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
    // Verify the Merkle proof FIRST
    self.tree_restore
        .lock()
        .as_mut()
        .unwrap()
        .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)?;
    
    // Only write KV and progress if verification succeeded
    self.kv_restore
        .lock()
        .as_mut()
        .unwrap()
        .add_chunk(chunk)?;
    
    Ok(())
}
```

**Option 2: Two-Phase Commit with Rollback**
```rust
fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
    // Phase 1: Prepare both databases
    let kv_batch = self.kv_restore.lock().as_mut().unwrap().prepare_chunk(chunk.clone())?;
    let tree_batch = self.tree_restore.lock().as_mut().unwrap().prepare_chunk_verified(
        chunk.iter().map(|(k, v)| (k, v.hash())).collect(), 
        proof
    )?;
    
    // Phase 2: Commit both atomically or rollback
    self.kv_restore.lock().as_mut().unwrap().commit_batch(kv_batch)?;
    self.tree_restore.lock().as_mut().unwrap().commit_batch(tree_batch)?;
    
    Ok(())
}
```

**Option 3: Add Progress Cleanup on Error**
```rust
fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
    let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
    
    // If tree verification fails, rollback KV progress
    if let Err(e) = r2 {
        self.kv_restore.lock().as_mut().unwrap().rollback_progress()?;
        return Err(e);
    }
    
    r1?;
    Ok(())
}
```

The recommended approach is **Option 1** (sequential execution) as it provides the strongest consistency guarantee with minimal complexity.

## Proof of Concept

```rust
// Reproduction scenario for the vulnerability
#[test]
fn test_state_snapshot_progress_inconsistency() {
    // Setup: Create a node with state snapshot restore in progress
    let db = setup_test_db();
    let version = 100;
    let expected_root_hash = HashValue::random();
    
    let mut restore = StateSnapshotRestore::new(
        &db.state_merkle_db,
        &db.state_store,
        version,
        expected_root_hash,
        false,
        StateSnapshotRestoreMode::Default,
    ).unwrap();
    
    // Step 1: Successfully process first chunk [keys A, B, C]
    let chunk1 = vec![
        (StateKey::raw(b"key_a"), StateValue::new_legacy(b"value_a".to_vec())),
        (StateKey::raw(b"key_b"), StateValue::new_legacy(b"value_b".to_vec())),
        (StateKey::raw(b"key_c"), StateValue::new_legacy(b"value_c".to_vec())),
    ];
    let valid_proof1 = generate_valid_proof(&chunk1, expected_root_hash);
    restore.add_chunk(chunk1.clone(), valid_proof1).unwrap();
    
    // Step 2: Process second chunk [keys D, E, F] with INVALID proof
    let chunk2 = vec![
        (StateKey::raw(b"key_d"), StateValue::new_legacy(b"value_d".to_vec())),
        (StateKey::raw(b"key_e"), StateValue::new_legacy(b"value_e".to_vec())),
        (StateKey::raw(b"key_f"), StateValue::new_legacy(b"value_f".to_vec())),
    ];
    let invalid_proof2 = SparseMerkleRangeProof::new(vec![]); // Invalid proof
    
    // This will fail tree verification, but KV progress is already committed
    let result = restore.add_chunk(chunk2.clone(), invalid_proof2);
    assert!(result.is_err(), "Tree verification should fail");
    
    // Step 3: Verify inconsistent state
    let kv_progress = db.state_store.get_progress(version).unwrap().unwrap();
    assert_eq!(kv_progress.key_hash, CryptoHash::hash(&StateKey::raw(b"key_f")));
    
    let tree_rightmost = db.state_merkle_db.get_rightmost_leaf(version).unwrap().unwrap();
    assert_eq!(tree_rightmost.1.account_key(), &CryptoHash::hash(&StateKey::raw(b"key_c")));
    
    // KV claims processed up to F, but tree only has up to C!
    assert_ne!(kv_progress.key_hash, *tree_rightmost.1.account_key());
    
    // Step 4: Attempt to process next chunk [keys G, H, I]
    let chunk3 = vec![
        (StateKey::raw(b"key_g"), StateValue::new_legacy(b"value_g".to_vec())),
        (StateKey::raw(b"key_h"), StateValue::new_legacy(b"value_h".to_vec())),
        (StateKey::raw(b"key_i"), StateValue::new_legacy(b"value_i".to_vec())),
    ];
    let proof3 = generate_proof_assuming_tree_has_up_to_f(&chunk3, expected_root_hash);
    
    // This will fail because proof assumes tree has keys up to F, but tree only has up to C
    let result = restore.add_chunk(chunk3, proof3);
    assert!(result.is_err(), "Proof verification fails due to incomplete tree");
    
    // State synchronization is now permanently stuck
}
```

**Note**: This test demonstrates the conceptual vulnerability. The actual compilation would require proper test infrastructure and mock implementations for proof generation.

### Citations

**File:** storage/aptosdb/src/state_restore/mod.rs (L88-127)
```rust
    pub fn add_chunk(&mut self, mut chunk: Vec<(K, V)>) -> Result<()> {
        // load progress
        let progress_opt = self.db.get_progress(self.version)?;

        // skip overlaps
        if let Some(progress) = progress_opt {
            let idx = chunk
                .iter()
                .position(|(k, _v)| CryptoHash::hash(k) > progress.key_hash)
                .unwrap_or(chunk.len());
            chunk = chunk.split_off(idx);
        }

        // quit if all skipped
        if chunk.is_empty() {
            return Ok(());
        }

        // save
        let mut usage = progress_opt.map_or(StateStorageUsage::zero(), |p| p.usage);
        let (last_key, _last_value) = chunk.last().unwrap();
        let last_key_hash = CryptoHash::hash(last_key);

        // In case of TreeOnly Restore, we only restore the usage of KV without actually writing KV into DB
        for (k, v) in chunk.iter() {
            usage.add_item(k.key_size() + v.value_size());
        }

        // prepare the sharded kv batch
        let kv_batch: StateValueBatch<K, Option<V>> = chunk
            .into_iter()
            .map(|(k, v)| ((k, self.version), Some(v)))
            .collect();

        self.db.write_kv_batch(
            self.version,
            &kv_batch,
            StateSnapshotProgress::new(last_key_hash, usage),
        )
    }
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L228-258)
```rust
    fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
        let kv_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_add_chunk"]);
            self.kv_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk(chunk.clone())
        };

        let tree_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["jmt_add_chunk"]);
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
        };
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => kv_fn()?,
            StateSnapshotRestoreMode::TreeOnly => tree_fn()?,
            StateSnapshotRestoreMode::Default => {
                // We run kv_fn with TreeOnly to restore the usage of DB
                let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
                r1?;
                r2?;
            },
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1244-1279)
```rust
    fn write_kv_batch(
        &self,
        version: Version,
        node_batch: &StateValueBatch,
        progress: StateSnapshotProgress,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_writer_write_chunk"]);
        let mut batch = SchemaBatch::new();
        let mut sharded_schema_batch = self.state_kv_db.new_sharded_native_batches();

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateSnapshotKvRestoreProgress(version),
            &DbMetadataValue::StateSnapshotProgress(progress),
        )?;

        if self.internal_indexer_db.is_some()
            && self
                .internal_indexer_db
                .as_ref()
                .unwrap()
                .statekeys_enabled()
        {
            let keys = node_batch.keys().map(|key| key.0.clone()).collect();
            self.internal_indexer_db
                .as_ref()
                .unwrap()
                .write_keys_to_indexer_db(&keys, version, progress)?;
        }
        self.shard_state_value_batch(
            &mut sharded_schema_batch,
            node_batch,
            self.state_kv_db.enabled_sharding(),
        )?;
        self.state_kv_db
            .commit(version, Some(batch), sharded_schema_batch)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L207-214)
```rust
        } else if let Some((node_key, leaf_node)) = tree_reader.get_rightmost_leaf(version)? {
            // If the system crashed in the middle of the previous restoration attempt, we need
            // to recover the partial nodes to the state right before the crash.
            (
                false,
                Self::recover_partial_nodes(tree_reader.as_ref(), version, node_key)?,
                Some(leaf_node),
            )
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L373-391)
```rust
        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;
```
