# Audit Report

## Title
Compression Bypass Vulnerability in Storage Service Optimistic Fetch Allows Unvalidated Oversized Responses

## Summary
The `notify_peer_of_new_data()` function in the storage service extracts the `use_compression` flag directly from client-controlled requests without validation. When an attacker sets `use_compression = false`, the server creates uncompressed responses that bypass size validation checks, potentially exceeding `MAX_APPLICATION_MESSAGE_SIZE` limits and causing resource exhaustion on validator nodes.

## Finding Description
The vulnerability exists in the state sync storage service's optimistic fetch and subscription handling mechanism. When a peer requests new data, the server honors the client-specified `use_compression` flag without enforcing any constraints. [1](#0-0) 

The extracted flag is then used directly when creating the response: [2](#0-1) 

The critical issue is in how `StorageServiceResponse::new` handles the compression flag differently:

**When compression is enabled:** The response is validated against `MAX_APPLICATION_MESSAGE_SIZE` (~61.875 MiB) before and after compression: [3](#0-2) [4](#0-3) 

**When compression is disabled:** The response is wrapped as `RawResponse` with **no size validation**: [5](#0-4) 

The comment in `notify_peer_of_new_data()` claims size checks aren't needed, but this comment is incomplete: [6](#0-5) 

When the response is sent, the entire unvalidated payload is serialized: [7](#0-6) 

**Attack Scenario:**
1. Attacker initiates an optimistic fetch request with `use_compression = false`
2. The request propagates through the optimistic fetch handler: [8](#0-7) 

3. Server fetches up to `max_transaction_chunk_size` (3000) transactions or outputs
4. With large transactions/events, the uncompressed response can easily exceed the intended `MAX_APPLICATION_MESSAGE_SIZE` limit
5. Server wastes resources serializing the oversized response
6. Network layer eventually rejects the message when it exceeds `MAX_MESSAGE_SIZE` (64 MiB), causing connection failures

## Impact Explanation
This vulnerability qualifies as **Medium Severity** per the Aptos bug bounty program criteria:

1. **Validator Node Slowdowns**: Processing and serializing oversized uncompressed responses wastes CPU and memory resources on validator nodes serving state sync requests.

2. **Protocol Violations**: The application layer is designed to enforce `MAX_APPLICATION_MESSAGE_SIZE` to account for network metadata and padding: [9](#0-8) 

Uncompressed responses bypass this check, violating the protocol's message sizing invariants.

3. **State Sync Disruptions**: Failed responses due to oversized messages disrupt state synchronization for honest nodes trying to catch up with the network.

4. **Resource Exhaustion**: Repeated exploitation can degrade validator performance and network stability.

## Likelihood Explanation
This vulnerability has **high likelihood** of occurrence:

- **Trivial to Exploit**: Any peer can set `use_compression = false` in optimistic fetch or subscription requests
- **No Special Access Required**: No validator privileges or cryptographic keys needed
- **Wide Attack Surface**: Affects both optimistic fetches and subscriptions (subscription code also calls `notify_peer_of_new_data`)
- **Realistic Data Sizes**: With 3000 transactions per chunk, even moderately-sized transactions with events can produce 30-40 MiB uncompressed responses

## Recommendation
Add size validation for uncompressed responses in `StorageServiceResponse::new`:

```rust
pub fn new(data_response: DataResponse, perform_compression: bool) -> Result<Self, Error> {
    if perform_compression {
        // Serialize and compress the raw data
        let raw_data = bcs::to_bytes(&data_response)
            .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
        let compressed_data = aptos_compression::compress(
            raw_data,
            CompressionClient::StateSync,
            MAX_APPLICATION_MESSAGE_SIZE,
        )?;

        // Create the compressed response
        let label = data_response.get_label().to_string() + COMPRESSION_SUFFIX_LABEL;
        Ok(StorageServiceResponse::CompressedResponse(
            label,
            compressed_data,
        ))
    } else {
        // ADD SIZE VALIDATION FOR RAW RESPONSES
        let raw_data = bcs::to_bytes(&data_response)
            .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
        
        // Ensure raw response doesn't exceed application message size limit
        if raw_data.len() > MAX_APPLICATION_MESSAGE_SIZE {
            return Err(Error::UnexpectedErrorEncountered(format!(
                "Raw response size {} exceeds max application message size {}",
                raw_data.len(),
                MAX_APPLICATION_MESSAGE_SIZE
            )));
        }
        
        Ok(StorageServiceResponse::RawResponse(data_response))
    }
}
```

Alternatively, enforce compression server-side for large responses regardless of client preference, or implement proper chunking with size awareness for uncompressed responses.

## Proof of Concept
```rust
#[tokio::test]
async fn test_compression_bypass_oversized_response() {
    // Setup mock storage with large transactions
    let mut storage = MockDatabaseReader::new();
    let large_transactions = create_large_transaction_list(3000); // 3000 txns with ~15KB each = ~45MB
    storage.expect_get_transactions_with_proof()
        .returning(move |_, _, _, _| {
            Ok(large_transactions.clone())
        });

    // Create optimistic fetch request with compression disabled
    let request = StorageServiceRequest::new(
        DataRequest::GetNewTransactionsWithProof(NewTransactionsWithProofRequest {
            known_version: 0,
            known_epoch: 0,
            include_events: true,
        }),
        false, // Disable compression to bypass size checks
    );

    // Process the request
    let handler = create_test_handler(storage);
    let response = handler.process_request(&peer_id, request, false).unwrap();

    // Verify response is uncompressed
    assert!(!response.is_compressed());

    // Serialize to measure actual size
    let serialized = bcs::to_bytes(&response).unwrap();
    
    // Response should exceed MAX_APPLICATION_MESSAGE_SIZE but not be caught
    assert!(serialized.len() > MAX_APPLICATION_MESSAGE_SIZE);
    
    // This demonstrates the bypass - compressed responses would fail here,
    // but uncompressed responses slip through without validation
}
```

### Citations

**File:** state-sync/storage-service/server/src/utils.rs (L87-88)
```rust
/// Note: we don't need to check the size of the response because:
/// (i) each sub-part should already be checked; and (ii) responses
```

**File:** state-sync/storage-service/server/src/utils.rs (L103-103)
```rust
    let use_compression = missing_data_request.use_compression;
```

**File:** state-sync/storage-service/server/src/utils.rs (L179-179)
```rust
        match StorageServiceResponse::new(transformed_data_response.clone(), use_compression) {
```

**File:** state-sync/storage-service/types/src/responses.rs (L75-83)
```rust
        if perform_compression {
            // Serialize and compress the raw data
            let raw_data = bcs::to_bytes(&data_response)
                .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
            let compressed_data = aptos_compression::compress(
                raw_data,
                CompressionClient::StateSync,
                MAX_APPLICATION_MESSAGE_SIZE,
            )?;
```

**File:** state-sync/storage-service/types/src/responses.rs (L91-93)
```rust
        } else {
            Ok(StorageServiceResponse::RawResponse(data_response))
        }
```

**File:** crates/aptos-compression/src/lib.rs (L52-60)
```rust
    // Ensure that the raw data size is not greater than the max bytes limit
    if raw_data.len() > max_bytes {
        let error_string = format!(
            "Raw data size greater than max bytes limit: {}, max: {}",
            raw_data.len(),
            max_bytes
        );
        return create_compression_error(&client, error_string);
    }
```

**File:** state-sync/storage-service/server/src/network.rs (L106-112)
```rust
    pub fn send(self, response: Result<StorageServiceResponse>) {
        let msg = StorageServiceMessage::Response(response);
        let result = bcs::to_bytes(&msg)
            .map(Bytes::from)
            .map_err(RpcError::BcsError);
        let _ = self.response_tx.send(result);
    }
```

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L139-141)
```rust
        let storage_request =
            StorageServiceRequest::new(data_request, self.request.use_compression);
        Ok(storage_request)
```

**File:** config/src/config/network_config.rs (L45-50)
```rust
pub const MAX_MESSAGE_METADATA_SIZE: usize = 128 * 1024; /* 128 KiB: a buffer for metadata that might be added to messages by networking */
pub const MESSAGE_PADDING_SIZE: usize = 2 * 1024 * 1024; /* 2 MiB: a safety buffer to allow messages to get larger during serialization */
pub const MAX_APPLICATION_MESSAGE_SIZE: usize =
    (MAX_MESSAGE_SIZE - MAX_MESSAGE_METADATA_SIZE) - MESSAGE_PADDING_SIZE; /* The message size that applications should check against */
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```
