# Audit Report

## Title
Node Checker Transaction Correctness Validation Gap - State Merkle Tree Divergence Not Detected

## Summary
The `TransactionCorrectnessChecker` in the node health monitoring system has a critical design flaw where it fails to validate state merkle tree consistency when checking non-checkpoint transactions, which represent the vast majority of transactions. This allows nodes with divergent state merkle trees to pass health checks as long as they execute transactions deterministically.

## Finding Description

The `TransactionCorrectnessChecker.check()` function compares nodes by selecting a transaction at the midpoint of the shared version range and comparing accumulator root hashes. [1](#0-0) 

The accumulator root hash is computed from the hash of `TransactionInfo` objects. [2](#0-1) 

Critically, `TransactionInfo` contains a `state_checkpoint_hash` field that represents the root hash of the state merkle tree, but this field is `Option<HashValue>` and is only populated at checkpoint transactions. [3](#0-2) 

Checkpoints occur only at block boundaries (typically the last transaction in each block) or at reconfiguration events, meaning most transactions have `state_checkpoint_hash = None`. [4](#0-3) 

The checkpoint determination logic confirms that only block-ending transactions or transactions with new epoch events become checkpoints. [5](#0-4) 

**Attack Scenario:**

1. A node (Node B) experiences state merkle tree corruption due to:
   - A bug in the sparse merkle tree update logic [6](#0-5) 
   - State sync data corruption
   - Implementation differences between node versions

2. Node B continues executing transactions correctly with deterministic execution (same write sets, same `state_change_hash`)

3. The `TransactionCorrectnessChecker` picks a transaction in the middle of the version range (line 166-167), which statistically is almost certainly NOT a checkpoint transaction

4. For non-checkpoint transactions, the `TransactionInfo` comparison includes:
   - `transaction_hash`: Same (deterministic) ✓
   - `state_change_hash`: Same (write set hash) ✓
   - `event_root_hash`: Same (deterministic) ✓
   - `state_checkpoint_hash`: **None for both nodes** ✓
   - `gas_used`: Same (deterministic) ✓
   - `status`: Same (deterministic) ✓

5. The accumulator root hashes match, Node B passes the health check [7](#0-6) 

6. Node B's state queries return incorrect results and its merkle proofs are invalid, but it's marked as healthy

This breaks **CRITICAL INVARIANT #4: State Consistency** - state transitions must be verifiable via Merkle proofs, but nodes with incorrect state trees are not detected.

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty criteria as a "Significant protocol violation":

- **State Consistency Violation**: Nodes with incorrect state merkle trees can operate undetected, serving invalid state data and merkle proofs
- **Consensus Risk**: If multiple nodes have divergent state trees, this could lead to consensus issues when proofs are validated
- **Query Integrity**: State queries from affected nodes return incorrect data
- **Proof Validity**: Merkle proofs generated by affected nodes will fail verification by other nodes

The issue could escalate to **Medium Severity** ("State inconsistencies requiring intervention") if it causes operational problems requiring manual intervention.

## Likelihood Explanation

**High Likelihood** of the monitoring gap being exploited:

1. **Statistical Certainty**: If blocks contain N transactions, the probability of checking a non-checkpoint transaction is (N-1)/N. For blocks with 10+ transactions, this is >90%

2. **Checkpoint Frequency**: State checkpoints only occur at block boundaries, so most transactions are non-checkpoints [8](#0-7) 

3. **Prerequisite Bugs**: While this requires an underlying bug in state merkle tree computation or state sync, such bugs are realistic:
   - Complex merkle tree update logic with parallel processing [9](#0-8) 
   - State sync restoration validation [10](#0-9) 

## Recommendation

**Fix: Always Check State Checkpoint Hash When Available**

Modify the `TransactionCorrectnessChecker` to explicitly select and compare checkpoint transactions:

```rust
// In transaction_correctness.rs, modify check() to:

// 1. Find the latest checkpoint version in the shared range
let checkpoint_version = find_latest_checkpoint_in_range(
    &target_api_index_provider.client,
    oldest_shared_version,
    latest_shared_version
).await?;

// 2. Compare both accumulator root hash AND state_checkpoint_hash
let baseline_txn = Self::get_transaction_by_version(
    &baseline_api_index_provider.client,
    checkpoint_version,
    "baseline"
).await?;

let target_txn = Self::get_transaction_by_version(
    &target_api_index_provider.client,
    checkpoint_version,
    "target"
).await?;

// 3. Extract and compare state_checkpoint_hash explicitly
if let (Some(baseline_state_hash), Some(target_state_hash)) = (
    baseline_txn.state_checkpoint_hash(),
    target_txn.state_checkpoint_hash()
) {
    if baseline_state_hash != target_state_hash {
        return Error("State merkle tree divergence detected");
    }
}
```

**Alternative Fix: Check Multiple Transactions**

Check both a checkpoint transaction AND several non-checkpoint transactions to ensure comprehensive validation.

## Proof of Concept

**Scenario Reproduction:**

```rust
// Pseudo-code demonstrating the vulnerability

// Setup: Create two nodes with identical execution but different state trees
let mut node_a = Node::new();
let mut node_b = Node::new_with_corrupted_state_tree();

// Execute a block with 10 transactions
for i in 0..10 {
    let txn = create_transaction(i);
    let output_a = node_a.execute(txn.clone());
    let output_b = node_b.execute(txn);
    
    // Execution is deterministic - same write sets
    assert_eq!(output_a.write_set().hash(), output_b.write_set().hash());
    
    // But state merkle trees diverge
    if i == 9 { // Checkpoint transaction
        assert_ne!(
            node_a.state_merkle_tree().root_hash(),
            node_b.state_merkle_tree().root_hash()
        );
    }
}

// Run transaction_correctness checker
let checker = TransactionCorrectnessChecker::new(config);

// Checker picks transaction at version 5 (non-checkpoint)
let middle_version = 5;

// Transaction 5 has state_checkpoint_hash = None
let txn_a = node_a.get_transaction(middle_version);
let txn_b = node_b.get_transaction(middle_version);

assert_eq!(txn_a.state_checkpoint_hash(), None);
assert_eq!(txn_b.state_checkpoint_hash(), None);

// Accumulator root hashes match (computed from TransactionInfo with None checkpoint)
assert_eq!(
    txn_a.accumulator_root_hash(),
    txn_b.accumulator_root_hash()
);

// CHECK PASSES despite different state trees!
assert!(checker.check().await.is_ok());

// But state trees are actually different
assert_ne!(
    node_a.state_merkle_tree().root_hash(),
    node_b.state_merkle_tree().root_hash()
);
```

**Notes**

This vulnerability is a monitoring/detection gap rather than a direct exploit. It requires an underlying bug (in merkle tree computation or state sync) to cause actual state divergence. However, the failure of the node checker to detect such divergence represents a significant weakness in the protocol's defense-in-depth strategy. The checker should explicitly validate state merkle tree consistency by checking checkpoint transactions, not just arbitrary transactions where state_checkpoint_hash is typically None.

The separation between `state_change_hash` (write set delta) and `state_checkpoint_hash` (full state snapshot) [11](#0-10)  creates this validation gap, as the checker only indirectly validates the former through accumulator comparison but rarely validates the latter.

### Citations

**File:** ecosystem/node-checker/src/checker/transaction_correctness.rs (L165-167)
```rust
        // Select a version in the middle of shared oldest and latest version.
        let middle_shared_version =
            (oldest_shared_version.saturating_add(latest_shared_version)) / 2;
```

**File:** ecosystem/node-checker/src/checker/transaction_correctness.rs (L192-204)
```rust
                        if middle_baseline_accumulator_root_hash
                            == middle_target_accumulator_root_hash
                        {
                            Self::build_result(
                                "Target node produced valid recent transaction".to_string(),
                                100,
                                format!(
                                    "We were able to pull the same transaction (version: {}) \
                                    from both your node and the baseline node. Great! This \
                                    implies that your node is returning valid transaction data.",
                                    middle_shared_version,
                                ),
                            )
```

**File:** types/src/transaction/mod.rs (L2023-2051)
```rust
#[derive(Clone, CryptoHasher, BCSCryptoHash, Debug, Eq, PartialEq, Serialize, Deserialize)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct TransactionInfoV0 {
    /// The amount of gas used.
    gas_used: u64,

    /// The vm status. If it is not `Executed`, this will provide the general error class. Execution
    /// failures and Move abort's receive more detailed information. But other errors are generally
    /// categorized with no status code or other information
    status: ExecutionStatus,

    /// The hash of this transaction.
    transaction_hash: HashValue,

    /// The root hash of Merkle Accumulator storing all events emitted during this transaction.
    event_root_hash: HashValue,

    /// The hash value summarizing all changes caused to the world state by this transaction.
    /// i.e. hash of the output write set.
    state_change_hash: HashValue,

    /// The root hash of the Sparse Merkle Tree describing the world state at the end of this
    /// transaction. Depending on the protocol configuration, this can be generated periodical
    /// only, like per block.
    state_checkpoint_hash: Option<HashValue>,

    /// The hash value summarizing PersistedAuxiliaryInfo.
    auxiliary_info_hash: Option<HashValue>,
}
```

**File:** execution/executor/src/workflow/do_state_checkpoint.rs (L75-86)
```rust
            if !execution_output.is_block {
                // We should enter this branch only in test.
                execution_output.to_commit.ensure_at_most_one_checkpoint()?;
            }

            let mut out = vec![None; num_txns];

            if let Some(index) = last_checkpoint_index {
                out[index] = Some(state_summary.last_checkpoint().root_hash());
            }

            Ok(out)
```

**File:** execution/executor-types/src/transactions_with_output.rs (L195-200)
```rust
        (
            transactions_with_output
                .iter()
                .positions(|(txn, output, _)| {
                    txn.is_non_reconfig_block_ending() || output.has_new_epoch_event()
                })
```

**File:** storage/scratchpad/src/sparse_merkle/updater.rs (L1-25)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::{
    sparse_merkle::{
        node::{InternalNode, Node, NodeHandle, NodeInner},
        utils::{partition, swap_if},
        HashValueRef, UpdateError,
    },
    ProofRead,
};
use aptos_crypto::{hash::SPARSE_MERKLE_PLACEHOLDER_HASH, HashValue};
use aptos_types::proof::{definition::NodeInProof, SparseMerkleLeafNode, SparseMerkleProofExt};
use aptos_vm::AptosVM;
use once_cell::sync::Lazy;
use std::cmp::Ordering;

static POOL: Lazy<rayon::ThreadPool> = Lazy::new(|| {
    rayon::ThreadPoolBuilder::new()
        .num_threads(AptosVM::get_num_proof_reading_threads())
        .thread_name(|index| format!("smt_update_{}", index))
        .build()
        .unwrap()
});

```

**File:** storage/storage-interface/src/state_store/state_summary.rs (L142-173)
```rust
    fn update_global_state_summary(
        &self,
        persisted: &ProvableStateSummary,
        updates: &BatchedStateUpdateRefs,
    ) -> Result<SparseMerkleTree> {
        let smt_updates = updates
            .shards
            .par_iter() // clone hashes and sort items in parallel
            // TODO(aldenhu): smt per shard?
            .flat_map(|shard| {
                shard
                    .iter()
                    .filter_map(|(k, u)| {
                        // Filter out `MakeHot` ops.
                        u.state_op
                            .as_state_value_opt()
                            .map(|value_opt| (k, value_opt))
                    })
                    .map(|(k, value_opt)| (*k, value_opt.map(|v| v.hash())))
                    // The keys in the shard are already unique, and shards are ordered by the
                    // first nibble of the key hash. `batch_update_sorted_uniq` can be
                    // called if within each shard items are sorted by key hash.
                    .sorted_by_key(|(k, _v)| k.crypto_hash_ref())
                    .collect_vec()
            })
            .collect::<Vec<_>>();

        Ok(self
            .global_state_summary
            .freeze(&persisted.global_state_summary)
            .batch_update_sorted_uniq(&smt_updates, &ColdProvableStateSummary::new(persisted))?
            .unfreeze())
```

**File:** storage/storage-interface/src/state_store/state_summary.rs (L284-323)
```rust
impl<'db> ProvableStateSummary<'db> {
    pub fn new_persisted(db: &'db (dyn DbReader + Sync)) -> Result<Self> {
        Ok(Self::new(db.get_persisted_state_summary()?, db))
    }

    pub fn new(state_summary: StateSummary, db: &'db (dyn DbReader + Sync)) -> Self {
        Self { state_summary, db }
    }

    fn get_proof(
        &self,
        key: &HashValue,
        version: Version,
        root_depth: usize,
        use_hot_state: bool,
    ) -> Result<SparseMerkleProofExt> {
        // TODO(HotState): we cannot verify proof yet. In order to verify the proof, we need to
        // fetch and construct the corresponding `HotStateValue` for `key` at `version`, including
        // `hot_since_version`. However, the current in-memory hot state does not support this
        // query, and we might need persist hot state KV to db first.
        if !use_hot_state && rand::random::<usize>() % 10000 == 0 {
            // 1 out of 10000 times, verify the proof.
            let (val_opt, proof) = self
                .db
                // check the full proof
                .get_state_value_with_proof_by_version_ext(
                    key, version, /* root_depth = */ 0, /* use_hot_state = */ false,
                )?;
            proof.verify(
                self.state_summary.global_state_summary.root_hash(),
                *key,
                val_opt.as_ref(),
            )?;
            Ok(proof)
        } else {
            Ok(self
                .db
                .get_state_proof_by_version_ext(key, version, root_depth, use_hot_state)?)
        }
    }
```
