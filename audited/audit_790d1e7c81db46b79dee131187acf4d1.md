# Audit Report

## Title
Backup Service Epoch Endpoint Missing Input Validation and Rate Limiting Enables Resource Exhaustion DoS

## Summary
The backup service's `epoch_ending_ledger_infos` endpoint lacks input validation and rate limiting, allowing an attacker to request an unbounded number of epochs (potentially millions) in a single request, causing significant disk I/O, CPU consumption, and network bandwidth exhaustion on validator/archive nodes.

## Finding Description

The backup service endpoint at [1](#0-0)  accepts arbitrary `start_epoch` and `end_epoch` parameters as u64 values without any validation.

This endpoint calls the backup handler's `get_epoch_ending_ledger_info_iter` function [2](#0-1) , which directly invokes the low-level metadata database iterator [3](#0-2)  without any bounds checking.

**Critical Missing Validations:**

1. **No upper bound check**: The code does not validate that `end_epoch` is within the blockchain's actual epoch range. In contrast, the AptosDbReader interface implements proper validation [4](#0-3)  that ensures `end_epoch <= latest_epoch`.

2. **No rate limiting**: The backup handler has no limit on the number of epochs that can be requested. The AptosDbReader limits requests to `MAX_NUM_EPOCH_ENDING_LEDGER_INFO = 100` epochs [5](#0-4)  and implements range limiting [6](#0-5) .

**Attack Vector:**

An attacker sends: `GET /epoch_ending_ledger_infos/0/18446744073709551615`

The iterator [7](#0-6)  will process all epochs from 0 to the blockchain's current epoch (potentially millions), reading each epoch ending ledger info from disk, serializing it, and streaming it over the network.

**Violated Invariant:**
- **Resource Limits**: "All operations must respect gas, storage, and computational limits" - This endpoint allows unbounded resource consumption without any limits.

## Impact Explanation

**Severity: High**

This vulnerability meets the **High Severity** criteria per the Aptos bug bounty program:
- **Validator node slowdowns**: The attack forces extensive disk I/O and CPU usage on nodes running the backup service (typically validator or archive nodes)
- **API crashes**: Sustained attacks could exhaust system resources leading to service degradation or crashes
- **Significant protocol violations**: Violates the resource limits invariant

An attacker can:
1. Consume significant disk I/O reading millions of epoch ending ledger infos
2. Exhaust CPU resources for serialization
3. Saturate network bandwidth streaming large amounts of data
4. Launch multiple concurrent requests to amplify the attack
5. Potentially cause service unavailability for legitimate backup operations

The backup service is exposed via HTTP [8](#0-7)  without apparent authentication, making it accessible to any network attacker.

## Likelihood Explanation

**Likelihood: High**

- **Attack complexity**: Very low - a single HTTP GET request
- **Attacker requirements**: Network access to the backup service endpoint (no authentication required)
- **Detection**: Difficult to distinguish from legitimate large backup requests
- **Reproducibility**: 100% - the vulnerability is deterministic

## Recommendation

Add input validation and rate limiting to match the protections in AptosDbReader:

```rust
pub fn get_epoch_ending_ledger_info_iter(
    &self,
    start_epoch: u64,
    end_epoch: u64,
) -> Result<impl Iterator<Item = Result<LedgerInfoWithSignatures>> + '_> {
    // Validate epoch range
    ensure!(
        start_epoch <= end_epoch,
        "Bad epoch range [{}, {})",
        start_epoch,
        end_epoch,
    );
    
    // Get the latest valid epoch
    let latest_epoch = self
        .ledger_db
        .metadata_db()
        .get_latest_ledger_info()?
        .ledger_info()
        .next_block_epoch();
    
    // Ensure end_epoch is within valid range
    ensure!(
        end_epoch <= latest_epoch,
        "Unable to provide epoch change ledger info for still open epoch. asked upper bound: {}, last sealed epoch: {}",
        end_epoch,
        latest_epoch - 1,
    );
    
    // Limit the range to prevent resource exhaustion
    const MAX_EPOCHS_PER_REQUEST: u64 = 100;
    let limited_end_epoch = std::cmp::min(
        end_epoch,
        start_epoch.saturating_add(MAX_EPOCHS_PER_REQUEST)
    );
    
    Ok(self
        .ledger_db
        .metadata_db()
        .get_epoch_ending_ledger_info_iter(start_epoch, limited_end_epoch)?
        .enumerate()
        .map(move |(idx, li)| {
            BACKUP_EPOCH_ENDING_EPOCH.set((start_epoch + idx as u64) as i64);
            li
        }))
}
```

Additionally, implement authentication/authorization for the backup service endpoint.

## Proof of Concept

```rust
#[test]
fn test_backup_service_dos_via_unbounded_epoch_request() {
    use std::net::{IpAddr, Ipv4Addr, SocketAddr};
    use aptos_temppath::TempPath;
    use aptos_config::utils::get_available_port;
    
    // Setup test database with some epochs
    let tmpdir = TempPath::new();
    let db = Arc::new(AptosDB::new_for_test(&tmpdir));
    
    // Start backup service
    let port = get_available_port();
    let addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), port);
    let _rt = start_backup_service(addr, db);
    
    // Attack: Request epochs from 0 to u64::MAX
    let start = std::time::Instant::now();
    let url = format!("http://127.0.0.1:{}/epoch_ending_ledger_infos/0/{}", 
                      port, u64::MAX);
    
    // This request will attempt to stream all epochs, causing resource exhaustion
    let response = reqwest::blocking::get(&url).unwrap();
    
    // Measure resource consumption
    let elapsed = start.elapsed();
    println!("DoS attack triggered - processing time: {:?}", elapsed);
    
    // In a real attack, this would consume significant disk I/O, 
    // CPU, and network bandwidth proportional to the number of epochs
    assert!(response.status().is_success() || response.status().is_server_error());
}
```

**Notes:**
- The question mentions "integer overflow" but the actual vulnerability is **missing input validation and rate limiting**, not integer overflow
- The iterator logic does not calculate `end_epoch - start_epoch` which could overflow
- The real attack vector is requesting all epochs from genesis to u64::MAX, forcing the node to process potentially millions of epochs
- This violates the Resource Limits invariant by allowing unbounded resource consumption

### Citations

**File:** storage/backup/backup-service/src/handlers/mod.rs (L90-99)
```rust
    // GET epoch_ending_ledger_infos/<start_epoch>/<end_epoch>/
    let bh = backup_handler.clone();
    let epoch_ending_ledger_infos = warp::path!(u64 / u64)
        .map(move |start_epoch, end_epoch| {
            reply_with_bytes_sender(&bh, EPOCH_ENDING_LEDGER_INFOS, move |bh, sender| {
                bh.get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L207-221)
```rust
    pub fn get_epoch_ending_ledger_info_iter(
        &self,
        start_epoch: u64,
        end_epoch: u64,
    ) -> Result<impl Iterator<Item = Result<LedgerInfoWithSignatures>> + '_> {
        Ok(self
            .ledger_db
            .metadata_db()
            .get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
            .enumerate()
            .map(move |(idx, li)| {
                BACKUP_EPOCH_ENDING_EPOCH.set((start_epoch + idx as u64) as i64);
                li
            }))
    }
```

**File:** storage/aptosdb/src/ledger_db/ledger_metadata_db.rs (L124-132)
```rust
    pub(crate) fn get_epoch_ending_ledger_info_iter(
        &self,
        start_epoch: u64,
        end_epoch: u64,
    ) -> Result<EpochEndingLedgerInfoIter<'_>> {
        let mut iter = self.db.iter::<LedgerInfoSchema>()?;
        iter.seek(&start_epoch)?;
        Ok(EpochEndingLedgerInfoIter::new(iter, start_epoch, end_epoch))
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L577-594)
```rust
        gauged_api("get_epoch_ending_ledger_info_iterator", || {
            self.check_epoch_ending_ledger_infos_request(start_epoch, end_epoch)?;
            let limit = std::cmp::min(
                end_epoch.saturating_sub(start_epoch),
                MAX_NUM_EPOCH_ENDING_LEDGER_INFO as u64,
            );
            let end_epoch = start_epoch.saturating_add(limit);

            let iter = self
                .ledger_db
                .metadata_db()
                .get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?;

            Ok(Box::new(iter)
                as Box<
                    dyn Iterator<Item = Result<LedgerInfoWithSignatures>> + '_,
                >)
        })
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L1007-1034)
```rust
    fn check_epoch_ending_ledger_infos_request(
        &self,
        start_epoch: u64,
        end_epoch: u64,
    ) -> Result<()> {
        ensure!(
            start_epoch <= end_epoch,
            "Bad epoch range [{}, {})",
            start_epoch,
            end_epoch,
        );
        // Note that the latest epoch can be the same with the current epoch (in most cases), or
        // current_epoch + 1 (when the latest ledger_info carries next validator set)

        let latest_epoch = self
            .ledger_db
            .metadata_db()
            .get_latest_ledger_info()?
            .ledger_info()
            .next_block_epoch();
        ensure!(
            end_epoch <= latest_epoch,
            "Unable to provide epoch change ledger info for still open epoch. asked upper bound: {}, last sealed epoch: {}",
            end_epoch,
            latest_epoch - 1,  // okay to -1 because genesis LedgerInfo has .next_block_epoch() == 1
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/common.rs (L9-9)
```rust
pub(crate) const MAX_NUM_EPOCH_ENDING_LEDGER_INFO: usize = 100;
```

**File:** storage/aptosdb/src/utils/iterators.rs (L190-242)
```rust
pub struct EpochEndingLedgerInfoIter<'a> {
    inner: SchemaIterator<'a, LedgerInfoSchema>,
    next_epoch: u64,
    end_epoch: u64,
}

impl<'a> EpochEndingLedgerInfoIter<'a> {
    pub(crate) fn new(
        inner: SchemaIterator<'a, LedgerInfoSchema>,
        next_epoch: u64,
        end_epoch: u64,
    ) -> Self {
        Self {
            inner,
            next_epoch,
            end_epoch,
        }
    }

    fn next_impl(&mut self) -> Result<Option<LedgerInfoWithSignatures>> {
        if self.next_epoch >= self.end_epoch {
            return Ok(None);
        }

        let ret = match self.inner.next().transpose()? {
            Some((epoch, li)) => {
                if !li.ledger_info().ends_epoch() {
                    None
                } else {
                    ensure!(
                        epoch == self.next_epoch,
                        "Epochs are not consecutive. expecting: {}, got: {}",
                        self.next_epoch,
                        epoch,
                    );
                    self.next_epoch += 1;
                    Some(li)
                }
            },
            _ => None,
        };

        Ok(ret)
    }
}

impl Iterator for EpochEndingLedgerInfoIter<'_> {
    type Item = Result<LedgerInfoWithSignatures>;

    fn next(&mut self) -> Option<Self::Item> {
        self.next_impl().transpose()
    }
}
```

**File:** storage/backup/backup-service/src/lib.rs (L12-30)
```rust
pub fn start_backup_service(address: SocketAddr, db: Arc<AptosDB>) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler);

    let runtime = aptos_runtimes::spawn_named_runtime("backup".into(), None);

    // Ensure that we actually bind to the socket first before spawning the
    // server tasks. This helps in tests to prevent races where a client attempts
    // to make a request before the server task is actually listening on the
    // socket.
    //
    // Note: we need to enter the runtime context first to actually bind, since
    //       tokio TcpListener can only be bound inside a tokio context.
    let _guard = runtime.enter();
    let server = warp::serve(routes).bind(address);
    runtime.handle().spawn(server);
    info!("Backup service spawned.");
    runtime
}
```
