# Audit Report

## Title
Module Read Validation Bypass Due to Partial Module Publishing Failure in BlockSTM Parallel Executor

## Summary
A critical vulnerability exists in the BlockSTM parallel execution engine where a transaction publishing multiple modules can fail partway through the publishing process, leaving some modules visible in the per-block cache while never triggering the module read validation requirements. This allows subsequent transactions to execute and validate without checking their module reads, potentially leading to consensus divergence across validators.

## Finding Description

The vulnerability lies in the interaction between module publishing and validation requirement tracking in `publish_module_write_set`. [1](#0-0) 

The critical flaw is in the loop structure: when a transaction publishes multiple modules, the code sets `published = true` on the first iteration, then calls `add_module_write_to_module_cache` for each module. If any module fails after the first one succeeds, the function returns early via the `?` operator without ever calling `record_validation_requirements`.

**For BlockSTMv1**, the validation flag is initialized as: [2](#0-1) 

This flag defaults to `true`, meaning module validation is **skipped by default**. The flag is only set to `false` when `record_validation_requirements` is called: [3](#0-2) 

During validation, module reads are only checked if the flag is `false`: [4](#0-3) 

**Attack Scenario**:
1. Transaction T1 publishes modules M1 and M2
2. During commit, M1 is successfully added to the per-block cache via `add_module_write_to_module_cache`
3. M2 fails deserialization or insertion, returning a `PanicError`
4. The function returns early; `record_validation_requirements` is never called
5. M1 is now visible in the module cache, but `skip_module_reads_validation` remains `true`
6. Transaction T2 executes speculatively and reads M1
7. T2's validation runs with `skip_module_reads_validation = true`, skipping module read checks
8. T2 commits with potentially stale module reads

This breaks the **Deterministic Execution** invariant: different validators may see different module versions depending on execution timing, leading to state root divergence.

**For BlockSTMv2**, the same issue exists but manifests differently: [5](#0-4) 

Without `record_validation_requirements` being called, no cold validation requirements are recorded, and subsequent transactions bypass module validation checks entirely.

## Impact Explanation

**Critical Severity** - This vulnerability qualifies for the highest severity tier because:

1. **Consensus Safety Violation**: Different validators executing the same block may produce different state roots if they process module publishing transactions at different speeds or encounter different error conditions. This violates Byzantine Fault Tolerance guarantees.

2. **Deterministic Execution Broken**: The core invariant that "all validators must produce identical state roots for identical blocks" is violated when module read validation is skipped.

3. **Chain Split Risk**: If some validators skip module validation while others don't (due to timing or error conditions), they will commit different states for the same block, potentially requiring a hard fork to resolve.

The vulnerability affects the core consensus mechanism and could lead to network-wide state inconsistencies that are non-recoverable without manual intervention.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability can be triggered whenever:
1. A transaction publishes multiple modules in the same transaction (common for protocol upgrades)
2. One module succeeds in deserialization/insertion while a subsequent one fails
3. This could occur due to:
   - Corrupted module bytecode in the transaction
   - Module cache insertion failures
   - Runtime environment inconsistencies
   - Resource exhaustion during module loading

While `add_module_write_to_module_cache` returns `PanicError::CodeInvariantError` for failures (suggesting they "shouldn't happen"), the defensive programming principle suggests these errors CAN occur in production under adversarial conditions or unexpected state.

The attack doesn't require validator collusion - any transaction sender can craft a payload that triggers this condition.

## Recommendation

**Immediate Fix**: Ensure atomic module publishing by either:

**Option 1 - Atomic Publishing** (Preferred):
```rust
pub(crate) fn publish_module_write_set(
    &self,
    txn_idx: TxnIndex,
    // ... parameters
) -> Result<bool, PanicError> {
    let output_wrapper = self.output_wrappers[txn_idx as usize].lock();
    let output_before_guard = output_wrapper
        .check_success_or_skip_status()?
        .before_materialization()?;

    let module_writes: Vec<_> = output_before_guard.module_write_set().values().collect();
    
    if module_writes.is_empty() {
        return Ok(false);
    }

    // First validate ALL modules can be added (no partial state)
    let mut module_ids_for_v2 = BTreeSet::new();
    for write in &module_writes {
        if scheduler.is_v2() {
            module_ids_for_v2.insert(write.module_id().clone());
        }
        // Pre-validate deserialization without inserting
        let state_value = write.write_op().as_state_value()
            .ok_or_else(|| PanicError::CodeInvariantError("Modules cannot be deleted".to_string()))?;
        runtime_environment.deserialize_into_compiled_module(state_value.bytes())
            .map_err(|err| {
                let msg = format!("Failed to construct module: {:?}", err);
                PanicError::CodeInvariantError(msg)
            })?;
    }
    
    // Only after ALL validations pass, publish modules
    for write in &module_writes {
        add_module_write_to_module_cache::<T>(
            write,
            txn_idx,
            runtime_environment,
            global_module_cache,
            versioned_cache.module_cache(),
        )?;
    }
    
    // ALWAYS called after successful publishing
    global_module_cache.flush_layout_cache();
    scheduler.record_validation_requirements(txn_idx, module_ids_for_v2)?;
    
    Ok(true)
}
```

**Option 2 - Rollback on Error**:
Add transaction-level rollback for module cache on any error, ensuring modules are never partially visible.

## Proof of Concept

This vulnerability can be demonstrated with a Rust unit test:

```rust
#[test]
fn test_partial_module_publish_skips_validation() {
    // Setup: Create a transaction that publishes 2 modules where the second fails
    let mut harness = create_test_harness();
    
    // Module 1: Valid Move bytecode
    let module1 = compile_move_module("module 0x1::ValidModule { }");
    
    // Module 2: Intentionally corrupted bytecode that fails deserialization
    let module2_corrupted = vec![0xFF; 100]; // Invalid bytecode
    
    let txn = create_module_publish_txn(vec![
        (module_id1, module1),
        (module_id2, module2_corrupted),
    ]);
    
    // Execute the transaction
    let result = harness.execute_txn(txn);
    
    // Expected: module1 is in cache, but skip_module_reads_validation is still true
    assert!(harness.module_cache.contains(&module_id1));
    assert_eq!(harness.skip_module_reads_validation.load(Ordering::Relaxed), true);
    
    // Create a second transaction that reads module1
    let txn2 = create_txn_reading_module(&module_id1);
    let validation_result = harness.validate_txn(txn2);
    
    // Vulnerability: Validation passes without checking module reads
    assert!(validation_result.is_valid);
    assert!(!validation_result.validated_module_reads); // This proves module reads were skipped
}
```

**Notes**

The vulnerability exists in production code paths but requires specific error conditions to trigger. The fix should be applied urgently as it affects core consensus safety guarantees. The atomic publishing approach (Option 1) is recommended as it prevents partial state visibility entirely, rather than attempting cleanup after errors occur.

### Citations

**File:** aptos-move/block-executor/src/txn_last_input_output.rs (L559-576)
```rust
        for write in output_before_guard.module_write_set().values() {
            published = true;
            if scheduler.is_v2() {
                module_ids_for_v2.insert(write.module_id().clone());
            }
            add_module_write_to_module_cache::<T>(
                write,
                txn_idx,
                runtime_environment,
                global_module_cache,
                versioned_cache.module_cache(),
            )?;
        }
        if published {
            // Record validation requirements after the modules are published.
            global_module_cache.flush_layout_cache();
            scheduler.record_validation_requirements(txn_idx, module_ids_for_v2)?;
        }
```

**File:** aptos-move/block-executor/src/executor.rs (L810-815)
```rust
            && (skip_module_reads_validation
                || read_set.validate_module_reads(
                    global_module_cache,
                    versioned_cache.module_cache(),
                    None,
                ))
```

**File:** aptos-move/block-executor/src/executor.rs (L1895-1895)
```rust
        let skip_module_reads_validation = AtomicBool::new(true);
```

**File:** aptos-move/block-executor/src/scheduler_wrapper.rs (L84-88)
```rust
            SchedulerWrapper::V1(_, skip_module_reads_validation) => {
                // Relaxed suffices as syncronization (reducing validation index) occurs after
                // setting the module read validation flag.
                skip_module_reads_validation.store(false, Ordering::Relaxed);
            },
```

**File:** aptos-move/block-executor/src/scheduler_wrapper.rs (L89-91)
```rust
            SchedulerWrapper::V2(scheduler, worker_id) => {
                scheduler.record_validation_requirements(*worker_id, txn_idx, module_ids)?;
            },
```
