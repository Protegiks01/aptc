# Audit Report

## Title
Pre-Verification Deserialization Memory Exhaustion in Batch Message Processing

## Summary
The consensus layer deserializes `BatchMsg` network messages containing batch vectors before validating their size, allowing an attacker to force allocation of hundreds of thousands of batch objects that exceed the configured `max_num_batches` limit. This creates a memory exhaustion window between deserialization and verification that can be exploited to crash or significantly slow down validator nodes.

## Finding Description

The vulnerability exists in the message processing flow where `ConsensusMsg::BatchMsg` deserialization occurs before the `max_num_batches` limit is enforced. The attack flow is:

1. **Network Layer Deserialization**: When a `BatchMsg` arrives from the network, the entire message is deserialized from bytes into a `BatchMsg<T>` structure containing a `Vec<Batch<T>>` [1](#0-0) 

2. **Delayed Verification**: The `process_message` method receives the already-deserialized `ConsensusMsg`, converts it to an `UnverifiedEvent`, and only then spawns a task to call `verify()` [2](#0-1) 

3. **Size Check After Allocation**: The `BatchMsg::verify()` method checks `batches.len() <= max_num_batches` (configured as 20), but this happens after all batch objects have already been allocated in memory during deserialization [3](#0-2) 

**Attack Vector**: An attacker can craft a `BatchMsg` with approximately 480,000 minimal batches (each with 0 transactions) that fits within the 64 MiB network message size limit [4](#0-3) . Each minimal batch requires roughly 140 bytes serialized (BatchInfoExt metadata + empty BatchPayload), allowing `64 * 1024 * 1024 / 140 ≈ 480,000` batches per message.

When deserialized, each `Batch<BatchInfoExt>` object occupies 1-2 KB in memory, resulting in 480-960 MB allocation per malicious message. Since multiple messages can be processed concurrently by different threads, an attacker sending 10 such messages simultaneously could force 4.8-9.6 GB of memory allocation before any are rejected by verification.

**Regarding `ensure_max_limits()`**: The question asks whether this function can handle large vectors. By the time `ensure_max_limits()` is called in `handle_batches_msg()`, the batches have already passed `BatchMsg::verify()`, ensuring `batches.len() <= 20` [5](#0-4) . Therefore, `ensure_max_limits()` itself is NOT vulnerable to large iteration DoS. However, the vulnerability exists in the pre-verification deserialization window.

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

- **Validator Node Slowdowns**: Repeated memory allocation and deserialization of hundreds of thousands of objects causes significant CPU and memory pressure
- **Potential Node Crashes**: If sufficient concurrent malicious messages are sent, validators could experience out-of-memory conditions leading to crashes
- **Network Availability Impact**: Degraded validator performance or crashes reduce consensus participation, potentially affecting network liveness

The impact aligns with "Validator node slowdowns" and potentially escalates to availability issues, qualifying for High severity ($50,000 tier).

## Likelihood Explanation

**High Likelihood**: 
- **No Authentication Required**: Any network peer can send `ConsensusMsg::BatchMsg` messages to validators
- **Trivial to Exploit**: Attacker only needs to craft a message with many minimal batches serialized within the 64 MiB limit
- **Low Resource Cost**: Attacker's bandwidth cost (~64 MiB per message) is much lower than the defender's memory/CPU cost (480-960 MB allocation + deserialization overhead)
- **Bypasses Intended Limits**: The `receiver_max_num_batches: 20` configuration [6](#0-5)  is intended to prevent this but only takes effect after deserialization completes

## Recommendation

Implement size validation before or during deserialization to prevent excessive memory allocation:

**Option 1: Custom Deserializer with Size Limits**
Implement a custom `Deserialize` for `BatchMsg` that checks vector length during deserialization:

```rust
impl<'de, T: TBatchInfo + Deserialize<'de>> Deserialize<'de> for BatchMsg<T> {
    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>
    where
        D: serde::Deserializer<'de>,
    {
        use serde::de::{SeqAccess, Visitor};
        
        struct BatchMsgVisitor<T>(std::marker::PhantomData<T>);
        
        impl<'de, T: TBatchInfo + Deserialize<'de>> Visitor<'de> for BatchMsgVisitor<T> {
            type Value = BatchMsg<T>;
            
            fn expecting(&self, formatter: &mut std::fmt::Formatter) -> std::fmt::Result {
                formatter.write_str("BatchMsg with limited batches")
            }
            
            fn visit_seq<A>(self, mut seq: A) -> Result<Self::Value, A::Error>
            where
                A: SeqAccess<'de>,
            {
                const MAX_BATCHES_DURING_DESER: usize = 100; // Safety margin above receiver_max_num_batches
                let mut batches = Vec::with_capacity(std::cmp::min(seq.size_hint().unwrap_or(0), MAX_BATCHES_DURING_DESER));
                
                while let Some(batch) = seq.next_element()? {
                    if batches.len() >= MAX_BATCHES_DURING_DESER {
                        return Err(serde::de::Error::custom(format!(
                            "BatchMsg exceeds maximum batch count during deserialization: {}", 
                            MAX_BATCHES_DURING_DESER
                        )));
                    }
                    batches.push(batch);
                }
                
                Ok(BatchMsg { batches })
            }
        }
        
        deserializer.deserialize_struct("BatchMsg", &["batches"], BatchMsgVisitor(std::marker::PhantomData))
    }
}
```

**Option 2: Network Layer Size Pre-Check**
Add a check in the network layer to inspect message structure before full deserialization, rejecting messages with excessive vector lengths.

## Proof of Concept

```rust
#[cfg(test)]
mod memory_exhaustion_test {
    use super::*;
    use aptos_consensus_types::proof_of_store::BatchInfo;
    use aptos_crypto::HashValue;
    use aptos_types::{transaction::SignedTransaction, PeerId};
    
    #[test]
    fn test_batch_msg_memory_exhaustion() {
        // Create a BatchMsg with excessive number of minimal batches
        let author = PeerId::random();
        let batch_id = HashValue::random();
        let epoch = 1;
        let expiration = 1000000;
        let gas_bucket_start = 0;
        
        // Create 100,000 batches with 0 transactions each (within memory limits for test)
        let mut batches = Vec::with_capacity(100_000);
        for i in 0..100_000 {
            let batch = Batch::new(
                batch_id,
                vec![], // Empty payload
                epoch,
                expiration,
                author,
                gas_bucket_start,
            );
            batches.push(batch);
        }
        
        let batch_msg = BatchMsg::new(batches);
        
        // Serialize to bytes
        let serialized = bcs::to_bytes(&batch_msg).unwrap();
        println!("Serialized size: {} bytes", serialized.len());
        
        // Deserialize - this allocates memory for all 100k batches
        let deserialized: BatchMsg<BatchInfo> = bcs::from_bytes(&serialized).unwrap();
        
        // Now verify - this should reject due to max_num_batches = 20
        let verifier = create_test_verifier();
        let result = deserialized.verify(author, 20, &verifier);
        
        // Verification should fail, but memory was already allocated
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("Too many batches"));
        
        println!("Memory allocated for {} batches before rejection", deserialized.batches.len());
    }
}
```

**Notes:**
- The vulnerability is in the ordering of operations: deserialize → allocate memory → verify → reject
- The `receiver_max_num_batches` limit should be enforced during deserialization, not after
- While `ensure_max_limits()` itself is safe (receives ≤20 batches), the pre-verification window is exploitable
- The 64 MiB network limit provides insufficient protection as it allows ~480k minimal batches to be serialized

### Citations

**File:** consensus/src/quorum_store/types.rs (L424-426)
```rust
pub struct BatchMsg<T: TBatchInfo> {
    batches: Vec<Batch<T>>,
}
```

**File:** consensus/src/quorum_store/types.rs (L440-445)
```rust
        ensure!(
            self.batches.len() <= max_num_batches,
            "Too many batches: {} > {}",
            self.batches.len(),
            max_num_batches
        );
```

**File:** consensus/src/epoch_manager.rs (L1587-1622)
```rust
            self.bounded_executor
                .spawn(async move {
                    match monitor!(
                        "verify_message",
                        unverified_event.clone().verify(
                            peer_id,
                            &epoch_state.verifier,
                            &proof_cache,
                            quorum_store_enabled,
                            peer_id == my_peer_id,
                            max_num_batches,
                            max_batch_expiry_gap_usecs,
                        )
                    ) {
                        Ok(verified_event) => {
                            Self::forward_event(
                                quorum_store_msg_tx,
                                round_manager_tx,
                                buffered_proposal_tx,
                                peer_id,
                                verified_event,
                                payload_manager,
                                pending_blocks,
                            );
                        },
                        Err(e) => {
                            error!(
                                SecurityEvent::ConsensusInvalidMessage,
                                remote_peer = peer_id,
                                error = ?e,
                                unverified_event = unverified_event
                            );
                        },
                    }
                })
                .await;
```

**File:** config/src/config/network_config.rs (L50-50)
```rust
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L137-171)
```rust
    fn ensure_max_limits(&self, batches: &[Batch<BatchInfoExt>]) -> anyhow::Result<()> {
        let mut total_txns = 0;
        let mut total_bytes = 0;
        for batch in batches.iter() {
            ensure!(
                batch.num_txns() <= self.max_batch_txns,
                "Exceeds batch txn limit {} > {}",
                batch.num_txns(),
                self.max_batch_txns,
            );
            ensure!(
                batch.num_bytes() <= self.max_batch_bytes,
                "Exceeds batch bytes limit {} > {}",
                batch.num_bytes(),
                self.max_batch_bytes,
            );

            total_txns += batch.num_txns();
            total_bytes += batch.num_bytes();
        }
        ensure!(
            total_txns <= self.max_total_txns,
            "Exceeds total txn limit {} > {}",
            total_txns,
            self.max_total_txns,
        );
        ensure!(
            total_bytes <= self.max_total_bytes,
            "Exceeds total bytes limit: {} > {}",
            total_bytes,
            self.max_total_bytes,
        );

        Ok(())
    }
```

**File:** config/src/config/quorum_store_config.rs (L122-122)
```rust
            receiver_max_num_batches: 20,
```
