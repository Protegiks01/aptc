# Audit Report

## Title
Transaction Censorship via Selective Mempool Broadcast Failure by Byzantine Validators

## Summary
Byzantine validators can selectively censor transactions by claiming `NetworkError` during mempool broadcast, preventing transaction propagation to other validators. The architecture's reliance on single-source broadcast without accountability mechanisms allows malicious validators to suppress specific transactions indefinitely.

## Finding Description

The Aptos mempool implements a shared broadcast system where each validator is solely responsible for broadcasting its own transactions to peers. [1](#0-0) 

When a client submits a transaction to a validator's API, the validator adds it to local mempool and broadcasts it to other validators via `execute_broadcast`. [2](#0-1) 

The broadcast mechanism uses `send_batch_to_peer`, which returns `BroadcastError::NetworkError` if the underlying network send fails. [3](#0-2) 

Critically, the network layer uses DirectSend, which is explicitly documented as "fire-and-forget" with no reliable delivery guarantees. [4](#0-3) [5](#0-4) 

When a broadcast fails with `NetworkError`, the error is merely logged with a warning, and metrics are incremented - there is no accountability mechanism. [6](#0-5) 

**Attack Execution:**
1. Client submits transaction to Byzantine validator V1
2. V1 inspects transaction content (sender, function, parameters)
3. For targeted transactions, V1 deliberately skips calling `send_to_peer` or returns a fabricated error
4. Transaction remains only in V1's mempool
5. Other validators (V2, V3, V4) never receive the transaction
6. When V2-V4 become consensus leaders, they cannot propose the transaction (not in their mempools)
7. Transaction only included if V1 becomes leader AND chooses to include it

The retry mechanism (for expired/ACK-failed broadcasts) doesn't help because it just asks the same Byzantine validator to resend, which they can refuse again. [7](#0-6) 

## Impact Explanation

**Severity: HIGH (per Aptos Bug Bounty)**

This constitutes a "Significant protocol violation" as it breaks the fundamental transaction propagation guarantee. While it doesn't violate consensus safety (blocks remain valid), it enables:

1. **Selective Censorship**: Byzantine validators can permanently suppress transactions from specific addresses, contract calls, or amounts
2. **Censorship Resistance Failure**: Users cannot reliably get transactions included unless they know to submit to multiple validators
3. **Unfair Transaction Ordering**: Byzantine validators gain unfair advantage by controlling which transactions propagate

The impact is limited by:
- Requires < 1/3 Byzantine validators for network to remain live
- Sophisticated clients can submit to multiple validators
- Eventually included if Byzantine validator becomes leader

However, this is still HIGH severity because:
- Single validator can censor without detection
- No built-in client redundancy in standard flow
- Violates transaction inclusion expectations
- No accountability or reputation system to penalize abuse

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

**Ease of Exploitation:**
- Byzantine validator can trivially implement by modifying `send_batch_to_peer` logic
- No cryptographic challenges or complex state manipulation required
- Simple conditional logic: `if should_censor(tx) { return Err(NetworkError) }`

**Detection Difficulty:**
- No cryptographic proof of broadcast required
- Metrics only show local errors, not visible to other validators
- Indistinguishable from legitimate network failures
- No reputation or accountability system

**Attacker Requirements:**
- Must control at least one validator node
- Clients must submit to the malicious validator

**Practical Constraints:**
- Most sophisticated clients likely submit to multiple validators
- Transaction will eventually be included when Byzantine validator is leader
- Limited to < 1/3 of validators in healthy network

## Recommendation

Implement a **broadcast accountability mechanism** with cryptographic proofs:

1. **Broadcast Certificates**: Validators should create signed commitments when receiving transactions, then gossip these commitments. If a validator commits to a transaction but doesn't broadcast it, this becomes provable misbehavior.

2. **Multi-path Broadcasting**: Implement redundant broadcast paths where receiving validators also forward to a subset of peers (with dampening to prevent broadcast storms).

3. **Reputation System**: Track broadcast reliability per validator using ACK success rates and expired message counts. Penalize validators with suspicious patterns.

4. **Client-Side Redundancy**: Document and encourage clients to submit to multiple validators (2-3) to ensure propagation even if one is Byzantine.

5. **Broadcast Proof Protocol**: Require validators to include proof-of-broadcast (signed ACKs from N peers) when proposing blocks containing their submitted transactions.

**Example Fix Pattern:**
```rust
// In execute_broadcast, require proof of delivery
async fn execute_broadcast_with_proof(...) -> Result<BroadcastProof, BroadcastError> {
    let mut acks = Vec::new();
    for peer in peers {
        if let Ok(ack) = self.send_and_wait_ack(peer, batch).await {
            acks.push(ack);
        }
    }
    if acks.len() < minimum_acks_required {
        return Err(BroadcastError::InsufficientAcks);
    }
    Ok(BroadcastProof { acks })
}
```

## Proof of Concept

**Conceptual PoC** (requires validator node access):

```rust
// In a modified validator's mempool/src/shared_mempool/network.rs

async fn send_batch_to_peer(
    &self,
    peer: PeerNetworkId,
    message_id: MempoolMessageId,
    transactions: Vec<(SignedTransaction, u64, BroadcastPeerPriority)>,
) -> Result<(), BroadcastError> {
    // Byzantine modification: selectively censor transactions
    for (txn, _, _) in &transactions {
        if should_censor_transaction(txn) {
            // Log as if network error occurred
            counters::network_send_fail_inc(counters::BROADCAST_TXNS);
            return Err(BroadcastError::NetworkError(
                peer, 
                anyhow::anyhow!("Fabricated network failure")
            ));
        }
    }
    
    // Normal path for non-censored transactions
    let request = MempoolSyncMsg::BroadcastTransactionsRequestWithReadyTime {
        message_id,
        transactions,
    };
    
    if let Err(e) = self.network_client.send_to_peer(request, peer) {
        counters::network_send_fail_inc(counters::BROADCAST_TXNS);
        return Err(BroadcastError::NetworkError(peer, e.into()));
    }
    Ok(())
}

fn should_censor_transaction(txn: &SignedTransaction) -> bool {
    // Censor based on sender address, function, or other criteria
    txn.sender() == TARGET_ADDRESS_TO_CENSOR
}
```

**Testing Steps:**
1. Submit transaction from `TARGET_ADDRESS_TO_CENSOR` to modified validator
2. Monitor other validators' mempools - transaction never appears
3. Monitor consensus proposals from other validators - transaction never proposed
4. Only when Byzantine validator becomes leader, transaction can be included

**Notes**
While this vulnerability is real and exploitable, it has several important constraints:

1. **Requires Validator Control**: This is explicitly a Byzantine validator attack, which assumes the attacker controls a validator node. The Aptos threat model already accounts for < 1/3 Byzantine validators in BFT consensus.

2. **Architectural Trade-off**: The single-source broadcast design (no rebroadcasting from peers) is an intentional performance optimization to reduce network overhead, documented in the mempool README.

3. **Existing Mitigations**: Sophisticated clients and SDKs typically submit transactions to multiple endpoints for redundancy, which naturally mitigates single-validator censorship.

4. **BFT Tolerance**: As long as < 1/3 of validators are Byzantine, the network maintains liveness - honest validators will eventually propose censored transactions when they become leaders.

This represents a **liveness vulnerability** rather than a **safety violation**. The recommended accountability mechanisms would significantly improve censorship resistance without requiring changes to the core BFT consensus protocol.

### Citations

**File:** mempool/README.md (L16-16)
```markdown
When a validator receives a transaction from another mempool, the transaction is ordered when itâ€™s added to the ordered queue of the recipient validator. To reduce network consumption in the shared mempool, each validator is responsible for the delivery of its own transactions. We don't rebroadcast transactions originating from a peer validator.
```

**File:** mempool/src/shared_mempool/network.rs (L424-450)
```rust
        // 1. Batch that did not receive ACK in configured window of time
        // 2. Batch that an earlier ACK marked as retriable
        let mut pending_broadcasts = 0;
        let mut expired_message_id = None;

        // Find earliest message in timeline index that expired.
        // Note that state.broadcast_info.sent_messages is ordered in decreasing order in the timeline index
        for (message, sent_time) in state.broadcast_info.sent_messages.iter() {
            let deadline = sent_time.add(Duration::from_millis(
                self.mempool_config.shared_mempool_ack_timeout_ms,
            ));
            if SystemTime::now().duration_since(deadline).is_ok() {
                expired_message_id = Some(message);
            } else {
                pending_broadcasts += 1;
            }

            // The maximum number of broadcasts sent to a single peer that are pending a response ACK at any point.
            // If the number of un-ACK'ed un-expired broadcasts reaches this threshold, we do not broadcast anymore
            // and wait until an ACK is received or a sent broadcast expires.
            // This helps rate-limit egress network bandwidth and not overload a remote peer or this
            // node's network sender.
            if pending_broadcasts >= self.mempool_config.max_broadcasts_per_peer {
                return Err(BroadcastError::TooManyPendingBroadcasts(peer));
            }
        }
        let retry_message_id = state.broadcast_info.retry_messages.iter().next_back();
```

**File:** mempool/src/shared_mempool/network.rs (L573-597)
```rust
    async fn send_batch_to_peer(
        &self,
        peer: PeerNetworkId,
        message_id: MempoolMessageId,
        // For each transaction, we include the ready time in millis since epoch
        transactions: Vec<(SignedTransaction, u64, BroadcastPeerPriority)>,
    ) -> Result<(), BroadcastError> {
        let request = if self.mempool_config.include_ready_time_in_broadcast {
            MempoolSyncMsg::BroadcastTransactionsRequestWithReadyTime {
                message_id,
                transactions,
            }
        } else {
            MempoolSyncMsg::BroadcastTransactionsRequest {
                message_id,
                transactions: transactions.into_iter().map(|(txn, _, _)| txn).collect(),
            }
        };

        if let Err(e) = self.network_client.send_to_peer(request, peer) {
            counters::network_send_fail_inc(counters::BROADCAST_TXNS);
            return Err(BroadcastError::NetworkError(peer, e.into()));
        }
        Ok(())
    }
```

**File:** mempool/src/shared_mempool/network.rs (L636-678)
```rust
    pub async fn execute_broadcast<TransactionValidator: TransactionValidation>(
        &self,
        peer: PeerNetworkId,
        scheduled_backoff: bool,
        smp: &mut SharedMempool<NetworkClient, TransactionValidator>,
    ) -> Result<(), BroadcastError> {
        // Start timer for tracking broadcast latency.
        let start_time = Instant::now();
        let (message_id, transactions, metric_label) =
            self.determine_broadcast_batch(peer, scheduled_backoff, smp)?;
        let num_txns = transactions.len();
        let send_time = SystemTime::now();
        self.send_batch_to_peer(peer, message_id.clone(), transactions)
            .await?;
        let num_pending_broadcasts =
            self.update_broadcast_state(peer, message_id.clone(), send_time)?;
        notify_subscribers(SharedMempoolNotification::Broadcast, &smp.subscribers);

        // Log all the metrics
        let latency = start_time.elapsed();
        trace!(
            LogSchema::event_log(LogEntry::BroadcastTransaction, LogEvent::Success)
                .peer(&peer)
                .message_id(&message_id)
                .backpressure(scheduled_backoff)
                .num_txns(num_txns)
        );
        let network_id = peer.network_id();
        counters::shared_mempool_broadcast_size(network_id, num_txns);
        // TODO: Rethink if this metric is useful
        counters::shared_mempool_pending_broadcasts(&peer).set(num_pending_broadcasts as i64);
        counters::shared_mempool_broadcast_latency(network_id, latency);
        if let Some(label) = metric_label {
            counters::shared_mempool_broadcast_type_inc(network_id, label);
        }
        if scheduled_backoff {
            counters::shared_mempool_broadcast_type_inc(
                network_id,
                counters::BACKPRESSURE_BROADCAST_LABEL,
            );
        }
        Ok(())
    }
```

**File:** network/framework/src/peer_manager/senders.rs (L39-55)
```rust
    /// Send a fire-and-forget direct-send message to remote peer.
    ///
    /// The function returns when the message has been enqueued on the network actor's event queue.
    /// It therefore makes no reliable delivery guarantees. An error is returned if the event queue
    /// is unexpectedly shutdown.
    pub fn send_to(
        &self,
        peer_id: PeerId,
        protocol_id: ProtocolId,
        mdata: Bytes,
    ) -> Result<(), PeerManagerError> {
        self.inner.push(
            (peer_id, protocol_id),
            PeerManagerRequest::SendDirectSend(peer_id, Message { protocol_id, mdata }),
        )?;
        Ok(())
    }
```

**File:** network/README.md (L19-19)
```markdown
* DirectSend: for fire-and-forget style message delivery.
```

**File:** mempool/src/shared_mempool/tasks.rs (L71-99)
```rust
        if let Err(err) = network_interface
            .execute_broadcast(peer, backoff, smp)
            .await
        {
            counters::shared_mempool_broadcast_event_inc(err.get_label(), peer.network_id());
            match err {
                BroadcastError::NoTransactions(_) => {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(BROADCAST_EVENT_LOG_SAMPLE_SECS)),
                        debug!("No transactions to broadcast: {:?}", err)
                    );
                },
                BroadcastError::PeerNotPrioritized(_, _) => {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(BROADCAST_EVENT_LOG_SAMPLE_SECS)),
                        debug!(
                            "Peer {} not prioritized. Skipping broadcast: {:?}",
                            peer, err
                        )
                    );
                },
                _ => {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(BROADCAST_ERROR_LOG_SAMPLE_SECS)),
                        warn!("Execute broadcast for peer {} failed: {:?}", peer, err)
                    );
                },
            }
        }
```
