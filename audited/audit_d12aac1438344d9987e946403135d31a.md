# Audit Report

## Title
Missing Timestamp Monotonicity Validation in Indexer GRPC Service Heartbeats Allows Time-Rollback Attacks

## Summary
The Aptos indexer GRPC manager fails to validate that timestamps in service heartbeat messages (LiveDataServiceInfo, HistoricalDataServiceInfo, FullnodeInfo, GrpcManagerInfo) are monotonically increasing. Data services create timestamps based on their own system clocks and the manager blindly accepts these timestamps without comparing them to previous values, allowing malicious or misconfigured services to manipulate staleness detection and health monitoring systems.

## Finding Description
The indexer GRPC system uses a heartbeat mechanism where data services periodically send service information updates containing timestamps to a central GRPC manager. These timestamps are used to determine if services are stale or unreachable.

The vulnerability exists because:

1. **Data services create timestamps from their own system clock**: When sending heartbeats, data services call `timestamp_now_proto()` which creates a timestamp from `SystemTime::now()` on their local machine. [1](#0-0) 

2. **Heartbeats include these untrusted timestamps**: The connection manager creates service info with timestamps from the data service's clock. [2](#0-1) 

3. **The GRPC manager blindly accepts these timestamps**: When the manager receives heartbeats, it stores the service info in a VecDeque without any validation that the new timestamp is greater than or equal to previous timestamps from that service. [3](#0-2) 

4. **Staleness checks only compare to current time**: The `is_stale_timestamp` function compares timestamps to the current system time on the manager, not to previous timestamps from the same service. [4](#0-3) 

**Attack Scenario:**
A malicious or compromised data service can:
- Set its system clock backward in time and send heartbeats with old timestamps
- Send heartbeats with timestamps far in the future to avoid being marked as stale for extended periods
- Continuously send the same old timestamp to appear "alive" even when actually unreachable
- Cause the manager to make incorrect routing decisions for transaction requests

The same vulnerability affects all service types: LiveDataServiceInfo, HistoricalDataServiceInfo, FullnodeInfo, and GrpcManagerInfo. [5](#0-4) 

## Impact Explanation
This is a **Medium severity** vulnerability according to Aptos bug bounty criteria:

1. **State inconsistencies requiring intervention**: The health monitoring system can be manipulated, causing the GRPC manager to incorrectly classify services as reachable or unreachable. This breaks the service discovery and load balancing invariants.

2. **Service availability impact**: While not causing total network failure, this can lead to:
   - Transaction routing to unreachable services
   - Healthy services being incorrectly marked as stale and removed
   - Degraded indexer performance affecting dependent applications
   - Incorrect service selection for data queries

3. **Limited scope**: The vulnerability is contained to the indexer GRPC subsystem and does not directly affect consensus, validator operations, or on-chain state. However, it compromises the integrity of the off-chain indexing infrastructure that many applications depend on.

## Likelihood Explanation
The likelihood of exploitation is **High**:

1. **Easy to exploit**: An attacker who controls or compromises a single data service can trivially manipulate their system clock. No cryptographic bypasses or complex attack chains are required.

2. **Accidental triggering possible**: Even without malicious intent, legitimate clock synchronization issues (NTP adjustments, system time corrections) could cause non-monotonic timestamps, leading to unpredictable behavior.

3. **Multiple attack surfaces**: The vulnerability affects four different service types, providing multiple exploitation paths.

4. **No authentication of timestamp source**: The manager trusts timestamps from any registered service without verification.

## Recommendation
Implement timestamp monotonicity validation in all service info handler functions. Before accepting a new service info update, verify that its timestamp is greater than or equal to the most recent timestamp stored for that service:

```rust
fn handle_live_data_service_info(
    &self,
    address: GrpcAddress,
    mut info: LiveDataServiceInfo,
) -> Result<()> {
    let mut entry = self
        .live_data_services
        .entry(address.clone())
        .or_insert(LiveDataService::new(address));
    
    // Validate timestamp monotonicity
    if let Some(new_timestamp) = &info.timestamp {
        if let Some(last_info) = entry.value().recent_states.back() {
            if let Some(last_timestamp) = &last_info.timestamp {
                // Reject if timestamp goes backward
                if new_timestamp.seconds < last_timestamp.seconds ||
                   (new_timestamp.seconds == last_timestamp.seconds && 
                    new_timestamp.nanos < last_timestamp.nanos) {
                    bail!(
                        "Timestamp rollback detected: new={}.{}, previous={}.{}",
                        new_timestamp.seconds, new_timestamp.nanos,
                        last_timestamp.seconds, last_timestamp.nanos
                    );
                }
            }
        }
    }
    
    if info.stream_info.is_none() {
        info.stream_info = Some(StreamInfo {
            active_streams: vec![],
        });
    }
    entry.value_mut().recent_states.push_back(info);
    if entry.value().recent_states.len() > MAX_NUM_OF_STATES_TO_KEEP {
        entry.value_mut().recent_states.pop_front();
    }

    Ok(())
}
```

Apply the same validation to:
- `handle_historical_data_service_info`
- `handle_fullnode_info` 
- `handle_grpc_manager_info`

Additionally, consider:
- Allowing small backward adjustments (e.g., within 1 second) to tolerate minor NTP corrections
- Logging timestamp violations for monitoring
- Incrementing error metrics when rejections occur
- Setting upper bounds on future timestamps (e.g., reject timestamps > 5 minutes in the future)

## Proof of Concept

```rust
// This test demonstrates the vulnerability
#[tokio::test]
async fn test_timestamp_rollback_accepted() {
    use std::time::{SystemTime, UNIX_EPOCH, Duration};
    use aptos_protos::indexer::v1::{LiveDataServiceInfo, ServiceInfo, service_info::Info};
    use aptos_protos::util::timestamp::Timestamp;
    
    let metadata_manager = MetadataManager::new(
        1, // chain_id
        "test_address".to_string(),
        vec![],
        vec![],
        None,
    );
    
    let address = "data_service_1".to_string();
    
    // Send first heartbeat with timestamp T
    let timestamp_t = Timestamp {
        seconds: 1000000,
        nanos: 0,
    };
    let info1 = LiveDataServiceInfo {
        chain_id: 1,
        timestamp: Some(timestamp_t),
        known_latest_version: Some(100),
        stream_info: None,
        min_servable_version: None,
    };
    
    // This succeeds
    metadata_manager.handle_heartbeat(
        address.clone(),
        Info::LiveDataServiceInfo(info1)
    ).unwrap();
    
    // Send second heartbeat with timestamp T-500 (rolled back!)
    let timestamp_rollback = Timestamp {
        seconds: 999500, // 500 seconds in the past
        nanos: 0,
    };
    let info2 = LiveDataServiceInfo {
        chain_id: 1,
        timestamp: Some(timestamp_rollback),
        known_latest_version: Some(101),
        stream_info: None,
        min_servable_version: None,
    };
    
    // VULNERABILITY: This also succeeds even though timestamp went backward!
    // The manager blindly accepts the rolled-back timestamp
    let result = metadata_manager.handle_heartbeat(
        address.clone(),
        Info::LiveDataServiceInfo(info2)
    );
    
    assert!(result.is_ok(), "Timestamp rollback was accepted without validation!");
    
    // The service now appears to have a stale timestamp even though
    // it just sent a heartbeat, breaking staleness detection
}
```

**Notes:**
- This vulnerability is in the indexer infrastructure, not in consensus-critical code, limiting its severity to Medium
- The fix is straightforward and has minimal performance impact
- The issue affects operational security and service availability rather than blockchain safety
- Consider implementing server-side timestamp generation as an additional mitigation, where the manager assigns timestamps based on receipt time rather than trusting client-provided values

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/lib.rs (L103-105)
```rust
pub fn timestamp_now_proto() -> Timestamp {
    system_time_to_proto(SystemTime::now())
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L249-276)
```rust
    async fn heartbeat(&self, address: &str) -> Result<(), tonic::Status> {
        info!("Sending heartbeat to GrpcManager {address}.");
        let timestamp = Some(timestamp_now_proto());
        let known_latest_version = Some(self.known_latest_version());
        let stream_info = Some(StreamInfo {
            active_streams: self.get_active_streams(),
        });

        let info = if self.is_live_data_service {
            let min_servable_version = match LIVE_DATA_SERVICE.get() {
                Some(svc) => Some(svc.get_min_servable_version().await),
                None => None,
            };
            Some(Info::LiveDataServiceInfo(LiveDataServiceInfo {
                chain_id: self.chain_id,
                timestamp,
                known_latest_version,
                stream_info,
                min_servable_version,
            }))
        } else {
            Some(Info::HistoricalDataServiceInfo(HistoricalDataServiceInfo {
                chain_id: self.chain_id,
                timestamp,
                known_latest_version,
                stream_info,
            }))
        };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L167-173)
```rust
    fn is_stale_timestamp(timestamp: Timestamp, threshold: Duration) -> bool {
        let timestamp_since_epoch = Duration::new(timestamp.seconds as u64, timestamp.nanos as u32);
        let now_since_epoch = SystemTime::now().duration_since(UNIX_EPOCH).unwrap();
        let staleness = now_since_epoch.saturating_sub(timestamp_since_epoch);

        staleness >= threshold
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L330-339)
```rust
    pub(crate) fn handle_heartbeat(&self, address: GrpcAddress, info: Info) -> Result<()> {
        match info {
            Info::LiveDataServiceInfo(info) => self.handle_live_data_service_info(address, info),
            Info::HistoricalDataServiceInfo(info) => {
                self.handle_historical_data_service_info(address, info)
            },
            Info::FullnodeInfo(info) => self.handle_fullnode_info(address, info),
            Info::GrpcManagerInfo(info) => self.handle_grpc_manager_info(address, info),
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L489-509)
```rust
    fn handle_live_data_service_info(
        &self,
        address: GrpcAddress,
        mut info: LiveDataServiceInfo,
    ) -> Result<()> {
        let mut entry = self
            .live_data_services
            .entry(address.clone())
            .or_insert(LiveDataService::new(address));
        if info.stream_info.is_none() {
            info.stream_info = Some(StreamInfo {
                active_streams: vec![],
            });
        }
        entry.value_mut().recent_states.push_back(info);
        if entry.value().recent_states.len() > MAX_NUM_OF_STATES_TO_KEEP {
            entry.value_mut().recent_states.pop_front();
        }

        Ok(())
    }
```
