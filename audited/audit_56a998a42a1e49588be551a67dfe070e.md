# Audit Report

## Title
Unauthenticated Remote Command Injection in Sharded Executor Service Allows Consensus Bypass and State Corruption

## Summary
The `ProcessExecutorService` in the executor-service module exposes a gRPC endpoint that accepts and executes arbitrary block execution commands from any network peer without authentication. An attacker who can reach the executor shard's network address can inject malicious execution commands, bypassing the consensus layer entirely and causing state divergence across shards.

## Finding Description

The vulnerability exists in the remote executor service architecture used for sharded block execution. The attack surface spans multiple components:

**Entry Point - ProcessExecutorService:**
The `ProcessExecutorService::new()` function creates an `ExecutorService` that binds to a network address and starts listening for execution commands. [1](#0-0) 

**No Authentication Layer:**
The underlying `NetworkController` uses a gRPC server (`GRPCNetworkMessageServiceServerWrapper`) that accepts messages from any sender without authentication. The `simple_msg_exchange` method processes incoming messages purely based on message type, with no verification of the sender's identity. [2](#0-1) 

**Command Deserialization and Execution:**
The `RemoteCoordinatorClient` receives these unauthenticated messages and deserializes them into `ExecutorShardCommand::ExecuteSubBlocks` commands containing transactions to execute, concurrency configuration, and onchain configuration. [3](#0-2) 

**Direct Transaction Execution:**
The `ShardedExecutorService` directly executes these commands by running the transactions through AptosVM, producing state changes that will be committed to the blockchain. [4](#0-3) 

**Attack Scenario:**

1. Attacker discovers the network address of an executor shard (e.g., through configuration leaks or network scanning)
2. Attacker crafts a malicious `RemoteExecutionRequest::ExecuteBlock` command containing:
   - Arbitrary transactions (potentially including malicious Move bytecode)
   - Modified concurrency levels to cause timing issues
   - Altered onchain configuration
3. Attacker sends this command via gRPC to the executor shard's endpoint
4. The executor shard processes the command without verifying it came from the legitimate coordinator
5. The shard executes the malicious transactions, producing incorrect state outputs
6. Different shards may receive different commands, causing state divergence
7. The consensus layer is completely bypassed as shards execute unauthorized commands

**Broken Invariants:**

This vulnerability violates multiple critical invariants:

1. **Deterministic Execution**: Different shards may execute different transactions if an attacker sends conflicting commands, breaking the guarantee that all validators produce identical state roots for identical blocks.

2. **Consensus Safety**: The execution layer should only process blocks that passed through consensus. This vulnerability allows execution without consensus approval, enabling potential double-spending and chain splits.

3. **Access Control**: Only the authorized coordinator should be able to command execution shards, but any network peer can do so.

## Impact Explanation

This vulnerability qualifies as **CRITICAL SEVERITY** per the Aptos Bug Bounty program criteria:

**Consensus/Safety Violations**: An attacker can cause different executor shards to execute different sets of transactions, leading to state divergence. When these inconsistent results are aggregated, it can cause:
- Consensus failures as validators cannot agree on state roots
- Potential chain splits requiring hard fork intervention
- Non-deterministic execution breaking the core consensus invariant

**Loss of Funds**: By injecting malicious transactions, an attacker could:
- Execute unauthorized token transfers
- Manipulate account balances
- Bypass transaction validation checks (signature verification, gas payment)
- Double-spend by executing conflicting transactions on different shards

**Network Partition**: If multiple attackers target different shards simultaneously with conflicting commands, the network could enter an unrecoverable state requiring a hard fork.

The impact is amplified because:
- The vulnerability is in the execution layer, which directly affects state transitions
- It bypasses ALL consensus-level protections
- Multiple shards can be attacked independently, causing inconsistent global state
- The attack leaves no forensic trail indicating the coordinator was not the sender

## Likelihood Explanation

**High Likelihood** for the following reasons:

**Low Attack Complexity**: 
- Attacker only needs network access to the executor shard's gRPC endpoint
- No special privileges, validator keys, or insider access required
- Attack can be automated with simple gRPC client code

**Realistic Deployment Scenarios**:
The executor service is designed to run as a separate process, potentially on different machines from the coordinator. [5](#0-4) 

If deployed in environments where:
- Executor shards are on separate machines with network-accessible endpoints
- Firewall rules are misconfigured
- Network segmentation is insufficient
- Service discovery exposes shard addresses

Then any peer who can reach these endpoints can exploit this vulnerability.

**No Compensating Controls**:
The codebase analysis reveals no authentication mechanisms at any layer:
- No TLS/mTLS with certificate validation
- No authentication tokens or API keys
- No IP allowlisting or MAC address filtering in the code
- No request signing or HMAC verification
- The NetworkController is confirmed to implement NO authentication [6](#0-5) 

**Production Usage**:
The remote sharded executor is actively used when remote addresses are configured, as shown in the execution workflow: [7](#0-6) 

## Recommendation

Implement mutual authentication between the coordinator and executor shards using one of these approaches:

**Option 1: Shared Secret Authentication (Quick Fix)**
```rust
// In NetworkController
pub struct NetworkController {
    // ... existing fields ...
    shared_secret: Option<Arc<[u8; 32]>>,
}

// In GRPCNetworkMessageServiceServerWrapper
impl NetworkMessageService for GRPCNetworkMessageServiceServerWrapper {
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        // Extract and verify HMAC from metadata
        let metadata = request.metadata();
        let auth_token = metadata
            .get("authorization")
            .ok_or_else(|| Status::unauthenticated("Missing authentication"))?;
        
        let network_message = request.into_inner();
        
        // Verify HMAC of message content
        if !self.verify_hmac(&network_message.message, auth_token.as_bytes()) {
            return Err(Status::unauthenticated("Invalid authentication"));
        }
        
        // ... rest of existing code ...
    }
}
```

**Option 2: Mutual TLS (Recommended)**
Configure the gRPC server and client to use mTLS with certificate-based authentication:

```rust
// In GRPCNetworkMessageServiceServerWrapper
use tonic::transport::{Certificate, Identity, ServerTlsConfig};

async fn start_async(
    self,
    server_addr: SocketAddr,
    rpc_timeout_ms: u64,
    server_shutdown_rx: oneshot::Receiver<()>,
    tls_config: Option<ServerTlsConfig>, // Add TLS config
) {
    let mut builder = Server::builder()
        .timeout(std::time::Duration::from_millis(rpc_timeout_ms));
    
    // Configure mTLS if provided
    if let Some(tls) = tls_config {
        builder = builder.tls_config(tls).unwrap();
    }
    
    builder
        .add_service(NetworkMessageServiceServer::new(self))
        // ... rest of existing code ...
}
```

**Option 3: Coordinator Public Key Authentication**
- Store coordinator's public key in each executor shard's configuration
- Require coordinator to sign all execution commands
- Verify signature before processing commands

**Additional Hardening**:
1. Implement network-level access control to restrict which IPs can connect
2. Add rate limiting to prevent command flooding
3. Log all incoming commands with sender information for audit trails
4. Implement command replay protection using nonces or timestamps
5. Add executor shard registration with the coordinator to establish trust

## Proof of Concept

```rust
// File: execution/executor-service/src/exploit_poc.rs
//
// This PoC demonstrates how an attacker can send unauthorized execution 
// commands to an executor shard.

use aptos_executor_service::{ExecuteBlockCommand, RemoteExecutionRequest};
use aptos_protos::remote_executor::v1::{
    network_message_service_client::NetworkMessageServiceClient,
    NetworkMessage,
};
use aptos_types::block_executor::partitioner::SubBlocksForShard;
use std::net::SocketAddr;

#[tokio::test]
async fn test_unauthenticated_command_injection() {
    // Step 1: Set up a legitimate executor shard (simulates target)
    let shard_port = aptos_config::utils::get_available_port();
    let shard_addr = SocketAddr::new(
        std::net::IpAddr::V4(std::net::Ipv4Addr::LOCALHOST), 
        shard_port
    );
    
    let coordinator_port = aptos_config::utils::get_available_port();
    let coordinator_addr = SocketAddr::new(
        std::net::IpAddr::V4(std::net::Ipv4Addr::LOCALHOST), 
        coordinator_port
    );

    // Start executor shard
    let _executor = aptos_executor_service::process_executor_service::ProcessExecutorService::new(
        0, // shard_id
        1, // num_shards
        4, // num_threads
        coordinator_addr,
        vec![shard_addr],
    );

    // Step 2: Attacker creates malicious execution command
    // (In real attack, this would contain malicious transactions)
    let malicious_command = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
        sub_blocks: SubBlocksForShard::empty(), // Simplified for PoC
        concurrency_level: 1,
        onchain_config: Default::default(),
    });

    // Step 3: Attacker connects to executor shard WITHOUT authentication
    let attacker_client = NetworkMessageServiceClient::connect(
        format!("http://{}", shard_addr)
    )
    .await
    .expect("Attacker can connect without authentication");

    // Step 4: Attacker sends malicious command
    let message = NetworkMessage {
        message: bcs::to_bytes(&malicious_command).unwrap(),
        message_type: "execute_command_0".to_string(), // Known message type
    };

    let response = attacker_client
        .simple_msg_exchange(tonic::Request::new(message))
        .await;

    // Step 5: Verify command was accepted and executed
    assert!(
        response.is_ok(),
        "VULNERABILITY CONFIRMED: Executor shard accepted unauthenticated command!"
    );
    
    // The executor shard will now execute this command without verifying
    // it came from the legitimate coordinator, demonstrating the vulnerability.
}
```

## Notes

**Deployment Context**: This vulnerability is particularly severe in distributed deployment scenarios where executor shards run as separate processes or on different machines from the coordinator. The codebase provides a `main.rs` entry point specifically for running executor shards as standalone processes, indicating this is an intended deployment pattern.

**Root Cause**: The `NetworkController` abstraction was designed as a simple message transport layer without security considerations. It uses plain gRPC without any authentication layer, assuming network-level protections would be sufficient.

**Comparison to Main Network**: The validator peer-to-peer network uses the Noise protocol with mutual authentication for consensus messages. However, the executor-service uses a separate networking stack (`NetworkController` + gRPC) that lacks these protections.

**Attack Variations**: An attacker could:
- Send stop commands to halt shards
- Send conflicting transactions to different shards
- Replay old execution commands
- Flood shards with invalid commands for DoS
- Extract state information by sending state view requests

This represents a fundamental architectural flaw in the sharded execution system's security model.

### Citations

**File:** execution/executor-service/src/process_executor_service.rs (L17-45)
```rust
    pub fn new(
        shard_id: ShardId,
        num_shards: usize,
        num_threads: usize,
        coordinator_address: SocketAddr,
        remote_shard_addresses: Vec<SocketAddr>,
    ) -> Self {
        let self_address = remote_shard_addresses[shard_id];
        info!(
            "Starting process remote executor service on {}; coordinator address: {}, other shard addresses: {:?}; num threads: {}",
            self_address, coordinator_address, remote_shard_addresses, num_threads
        );
        aptos_node_resource_metrics::register_node_metrics_collector(None);
        let _mp = MetricsPusher::start_for_local_run(
            &("remote-executor-service-".to_owned() + &shard_id.to_string()),
        );

        AptosVM::set_concurrency_level_once(num_threads);
        let mut executor_service = ExecutorService::new(
            shard_id,
            num_shards,
            num_threads,
            self_address,
            coordinator_address,
            remote_shard_addresses,
        );
        executor_service.start();
        Self { executor_service }
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L92-115)
```rust
impl NetworkMessageService for GRPCNetworkMessageServiceServerWrapper {
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L80-113)
```rust
    fn receive_execute_command(&self) -> ExecutorShardCommand<RemoteStateViewClient> {
        match self.command_rx.recv() {
            Ok(message) => {
                let _rx_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx"])
                    .start_timer();
                let bcs_deser_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx_bcs_deser"])
                    .start_timer();
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
                drop(bcs_deser_timer);

                match request {
                    RemoteExecutionRequest::ExecuteBlock(command) => {
                        let init_prefetch_timer = REMOTE_EXECUTOR_TIMER
                            .with_label_values(&[&self.shard_id.to_string(), "init_prefetch"])
                            .start_timer();
                        let state_keys = Self::extract_state_keys(&command);
                        self.state_view_client.init_for_block(state_keys);
                        drop(init_prefetch_timer);

                        let (sub_blocks, concurrency, onchain_config) = command.into();
                        ExecutorShardCommand::ExecuteSubBlocks(
                            self.state_view_client.clone(),
                            sub_blocks,
                            concurrency,
                            onchain_config,
                        )
                    },
                }
            },
            Err(_) => ExecutorShardCommand::Stop,
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-260)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
```

**File:** execution/executor-service/src/main.rs (L27-48)
```rust
fn main() {
    let args = Args::parse();
    aptos_logger::Logger::new().init();

    let (tx, rx) = crossbeam_channel::unbounded();
    ctrlc::set_handler(move || {
        tx.send(()).unwrap();
    })
    .expect("Error setting Ctrl-C handler");

    let _exe_service = ProcessExecutorService::new(
        args.shard_id,
        args.num_shards,
        args.num_executor_threads,
        args.coordinator_address,
        args.remote_executor_addresses,
    );

    rx.recv()
        .expect("Could not receive Ctrl-C msg from channel.");
    info!("Process executor service shutdown successfully.");
}
```

**File:** secure/net/src/network_controller/mod.rs (L72-93)
```rust
/// NetworkController is the main entry point for sending and receiving messages over the network.
/// 1. If a node acts as both client and server, albeit in different contexts, GRPC needs separate
///    runtimes for client context and server context. Otherwise we a hang in GRPC. This seems to be
///    an internal bug in GRPC.
/// 2. We want to use tokio runtimes because it is best for async IO and tonic GRPC
///    implementation is async. However, we want the rest of the system (remote executor service)
///    to use rayon thread pools because it is best for CPU bound tasks.
/// 3. NetworkController, InboundHandler and OutboundHandler work as a bridge between the sync and
///    async worlds.
/// 4. We need to shutdown all the async tasks spawned by the NetworkController runtimes, otherwise
///    the program will hang, or have resource leaks.
#[allow(dead_code)]
pub struct NetworkController {
    inbound_handler: Arc<Mutex<InboundHandler>>,
    outbound_handler: OutboundHandler,
    inbound_rpc_runtime: Runtime,
    outbound_rpc_runtime: Runtime,
    inbound_server_shutdown_tx: Option<oneshot::Sender<()>>,
    outbound_task_shutdown_tx: Option<Sender<Message>>,
    listen_addr: SocketAddr,
}

```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-270)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
```
