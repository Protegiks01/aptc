# Audit Report

## Title
Race Condition Between Block Commit and State Sync Causes Validator Node Crash

## Summary
Production code in the block executor contains `.expect()` calls that panic when the executor is not initialized. During recovery, state synchronization calls `finish()` to clear the executor state without first aborting pending commit operations. This creates a race condition where in-flight commit tasks panic when they attempt to access the cleared executor state, crashing the consensus thread and taking the validator node offline.

## Finding Description

The `BlockExecutor` in production code uses `.expect()` calls that assume the executor is always initialized: [1](#0-0) [2](#0-1) 

During recovery, when a validator node falls behind, the `RecoveryManager` triggers fast-forward sync: [3](#0-2) 

Note that `maybe_block_store` is passed as `None` (line 113), which means the pipeline abort step is skipped: [4](#0-3) 

The `execution_client.sync_to_target()` call proceeds to clear the executor without waiting for pending commits: [5](#0-4) 

Meanwhile, commit operations spawned in blocking tasks attempt to access the executor: [6](#0-5) 

When the spawn_blocking task executes and finds `inner == None`, the `.expect()` panics. The panic causes the JoinHandle to return an error, which triggers another panic at line 1104, crashing the consensus pipeline.

**Attack Path:**
1. Validator is normally processing blocks with pending commit operations
2. Node falls behind, triggering `RecoveryManager::sync()`
3. `fast_forward_sync()` is called with `maybe_block_store = None`
4. Pipeline abort is skipped, `sync_to_target()` calls `executor.finish()`
5. Pending commit task executes, accesses `inner.read().as_ref()`, gets `None`
6. `.expect("BlockExecutor is not reset")` panics in spawn_blocking thread
7. Pipeline builder's `.expect("spawn blocking failed")` panics
8. Consensus thread crashes, validator node stops participating

This breaks the **State Consistency** invariant (atomic state transitions) and **Consensus Safety** invariant (node must remain operational).

## Impact Explanation

**High Severity** - This vulnerability causes validator node crashes, qualifying as "Validator node slowdowns" and "Significant protocol violations" per the Aptos bug bounty program.

**Impact on Network:**
- Single validator: Loss of consensus participation until manual restart
- Multiple validators affected simultaneously: Network liveness degradation
- Repeated crashes: Potential extended downtime

The vulnerability affects production consensus code and can cause denial of service without requiring any privileged access or malicious behavior.

## Likelihood Explanation

**Likelihood: Medium-High**

The race condition occurs during recovery scenarios, which happen when:
- Validator nodes restart and need to catch up
- Network partitions cause nodes to fall behind
- Nodes experience temporary slowdowns

The timing window is small but realistic:
- Commit operations take milliseconds to execute
- State sync can be triggered during normal operation
- The race occurs if `finish()` executes between commit scheduling and execution

Given the frequency of recovery operations in production blockchain networks, this vulnerability is likely to trigger periodically, especially during network instability or validator restarts.

## Recommendation

**Fix 1: Ensure pipeline abort before finish() in all code paths**

In `RecoveryManager::sync()`, instead of passing `None` for `maybe_block_store`, track the block store reference and ensure pipeline abort:

```rust
// In recovery_manager.rs, before calling fast_forward_sync
if let Some(block_store) = self.block_store.as_ref() {
    block_store.abort_pipeline_for_state_sync().await;
}
```

**Fix 2: Replace .expect() with proper error handling**

In `execution/executor/src/block_executor/mod.rs`, replace the `.expect()` calls with proper error returns:

```rust
fn commit_ledger(&self, ledger_info_with_sigs: LedgerInfoWithSignatures) -> ExecutorResult<()> {
    let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "commit_ledger"]);
    
    self.inner
        .read()
        .as_ref()
        .ok_or_else(|| ExecutorError::InternalError {
            error: "BlockExecutor is not reset".into(),
        })?
        .commit_ledger(ledger_info_with_sigs)
}
```

Apply the same pattern to `pre_commit_block()` and other methods.

**Fix 3: Add synchronization in ExecutionProxy**

Ensure `write_mutex` is held during the entire state sync operation and checked before executor operations:

```rust
// In pipeline_builder.rs, before calling executor.commit_ledger
// Acquire a read lock on the sync mutex to prevent concurrent finish()
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_race_condition_commit_and_state_sync() {
    // Setup: Create executor and start a block commit operation
    let db_path = aptos_temppath::TempPath::new();
    db_path.create_as_dir().unwrap();
    let (db, dbrw) = DbReaderWriter::wrap(AptosDB::new_for_test(&db_path));
    let executor = Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(dbrw));
    
    // Initialize executor
    executor.reset().unwrap();
    
    // Start an async commit operation (simulating pipeline_builder behavior)
    let executor_clone = executor.clone();
    let commit_handle = tokio::spawn(async move {
        tokio::time::sleep(Duration::from_millis(10)).await;
        let ledger_info = create_test_ledger_info(); // helper function
        
        // This will panic if finish() was called
        tokio::task::spawn_blocking(move || {
            executor_clone.commit_ledger(ledger_info)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed") // This expect will panic on race condition
    });
    
    // Trigger finish() while commit is pending (simulating state sync)
    tokio::time::sleep(Duration::from_millis(5)).await;
    executor.finish(); // Clears inner to None
    
    // The commit operation should panic
    let result = commit_handle.await;
    assert!(result.is_err(), "Expected panic due to race condition");
}
```

**Expected Result:** The test panics with "BlockExecutor is not reset" when the commit operation executes after `finish()` is called, demonstrating the vulnerability.

## Notes

This vulnerability demonstrates a critical gap in error handling between test code and production code. While the test helpers use `.unwrap()` calls (which is acceptable for tests), the production executor code uses `.expect()` calls with the same failure mode. The proper fix requires both improved synchronization and defensive error handling to prevent validator crashes during normal recovery operations.

### Citations

**File:** execution/executor/src/block_executor/mod.rs (L131-139)
```rust
    fn pre_commit_block(&self, block_id: HashValue) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "pre_commit_block"]);

        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .pre_commit_block(block_id)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L141-149)
```rust
    fn commit_ledger(&self, ledger_info_with_sigs: LedgerInfoWithSignatures) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "commit_ledger"]);

        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .commit_ledger(ledger_info_with_sigs)
    }
```

**File:** consensus/src/recovery_manager.rs (L104-114)
```rust
        let recovery_data = BlockStore::fast_forward_sync(
            sync_info.highest_quorum_cert(),
            sync_info.highest_commit_cert(),
            &mut retriever,
            self.storage.clone(),
            self.execution_client.clone(),
            self.payload_manager.clone(),
            self.order_vote_enabled,
            self.window_size,
            None,
        )
```

**File:** consensus/src/block_storage/sync_manager.rs (L504-514)
```rust
        // abort any pending executor tasks before entering state sync
        // with zaptos, things can run before hitting buffer manager
        if let Some(block_store) = maybe_block_store {
            monitor!(
                "abort_pipeline_for_state_sync",
                block_store.abort_pipeline_for_state_sync().await
            );
        }
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```

**File:** consensus/src/state_computer.rs (L177-186)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();

```

**File:** consensus/src/pipeline/pipeline_builder.rs (L1098-1105)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .commit_ledger(ledger_info_with_sigs_clone)
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(Some(ledger_info_with_sigs))
```
