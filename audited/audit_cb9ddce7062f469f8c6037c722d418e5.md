# Audit Report

## Title
Epoch Confusion in BatchStore Cache Population Allows Stale Epoch Batches to be Retrieved Without Validation

## Summary
The `BatchStore` initialization logic fails to validate epochs when loading batches from persistent storage into the cache during non-epoch-boundary restarts. When `is_new_epoch = false`, the `populate_cache_and_gc_expired_batches_v2` method only checks time-based expiration, allowing batches from previous epochs to be cached and subsequently retrieved without epoch validation.

## Finding Description

The vulnerability exists in the batch cache population logic that determines which batches are loaded from persistent storage: [1](#0-0) 

The `is_new_epoch` flag is set based on whether the latest ledger info marks an epoch boundary. When `false`, the system calls `populate_cache_and_gc_expired_batches_v2` instead of the epoch-aware GC method: [2](#0-1) 

The populate method only validates time expiration, not epoch: [3](#0-2) 

Critically, at lines 319-325, batches are inserted into cache based solely on time expiration (`if expiration < gc_timestamp`), with no epoch validation. This contrasts with the GC method which explicitly removes batches from previous epochs: [4](#0-3) 

Once cached, batches are retrieved by digest without any epoch validation: [5](#0-4) [6](#0-5) 

The database schema stores batches keyed by digest (hash of payload transactions), not by epoch: [7](#0-6) [8](#0-7) 

## Impact Explanation

This issue represents a **Medium Severity** violation of the epoch isolation invariant. While block-level ProofOfStore verification ensures that blocks cannot include batches with incorrect epochs in their payload metadata, the cache pollution creates operational risks:

1. **State Inconsistency Risk**: Batches with stale epoch metadata remain accessible, potentially causing confusion in epoch-aware subsystems
2. **Resource Waste**: Storage and memory resources are consumed by batches that should have been purged
3. **Metadata Corruption**: Cached batches carry incorrect epoch metadata that could be used for logging, metrics, or diagnostics

The impact does NOT reach Critical/High severity because:
- Transaction execution uses only the transaction payload, not the BatchInfo epoch field
- Block verification still enforces epoch consistency at the ProofOfStore level
- No direct path exists for this to corrupt validator sets or governance state

## Likelihood Explanation

This vulnerability triggers under specific but realistic conditions:

**Triggering Conditions**:
1. Node crashes or restarts during an epoch (not at an epoch boundary)
2. The latest committed ledger info does not mark an epoch end
3. The database contains batches from previous epochs that have not yet time-expired
4. The node's current epoch is greater than the epochs of some cached batches

**Likelihood**: **Medium** - Node restarts are common operational events, and the 60-second expiration buffer means batches can persist across restarts within the same epoch.

## Recommendation

Add epoch validation to the populate method:

```rust
fn populate_cache_and_gc_expired_batches_v2(
    db: Arc<dyn QuorumStoreStorage>,
    current_epoch: u64,
    last_certified_time: u64,
    expiration_buffer_usecs: u64,
    batch_store: &BatchStore,
) {
    let db_content = db
        .get_all_batches_v2()
        .expect("failed to read v2 data from db");
    
    let mut expired_keys = Vec::new();
    let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
    
    for (digest, value) in db_content {
        let epoch = value.epoch();
        let expiration = value.expiration();
        
        // Add epoch validation here
        if epoch < current_epoch || expiration < gc_timestamp {
            expired_keys.push(digest);
        } else {
            batch_store
                .insert_to_cache(&value)
                .expect("Storage limit exceeded upon BatchReader construction");
        }
    }
    
    tokio::task::spawn_blocking(move || {
        db.delete_batches_v2(expired_keys)
            .expect("Deletion of expired keys should not fail");
    });
}
```

Apply the same fix to `populate_cache_and_gc_expired_batches_v1`.

Alternatively, add epoch validation to the retrieval path:

```rust
pub(crate) fn get_batch_from_local(
    &self,
    digest: &HashValue,
) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
    if let Some(value) = self.db_cache.get(digest) {
        // Add epoch validation
        if value.epoch() < self.epoch() {
            warn!("Batch {} has stale epoch {}, current epoch {}", 
                  digest, value.epoch(), self.epoch());
            return Err(ExecutorError::CouldNotGetData);
        }
        // ... rest of method
    }
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test_epoch_confusion {
    use super::*;
    use aptos_types::PeerId;
    
    #[test]
    fn test_stale_epoch_batch_cached() {
        // Setup: Create DB with batch from epoch 10
        let db = create_test_db();
        let batch_epoch_10 = create_test_batch(10, not_yet_expired_time);
        db.save_batch_v2(batch_epoch_10.clone()).unwrap();
        
        // Simulate: Node starts in epoch 11 with is_new_epoch=false
        let batch_store = BatchStore::new(
            11, // current_epoch
            false, // is_new_epoch - triggers populate instead of GC
            current_time(),
            db.clone(),
            1000, // memory_quota
            10000, // db_quota  
            100, // batch_quota
            test_signer(),
            Duration::from_secs(60).as_micros() as u64,
        );
        
        // Verify: Stale epoch 10 batch is cached and retrievable in epoch 11
        let retrieved = batch_store.get_batch_from_local(&batch_epoch_10.digest());
        assert!(retrieved.is_ok());
        assert_eq!(retrieved.unwrap().epoch(), 10); // Wrong epoch!
        assert_eq!(batch_store.epoch(), 11); // Current epoch is 11
    }
}
```

## Notes

While this vulnerability allows cross-epoch batch retrieval at the storage layer, the security impact is limited by higher-level validation in the consensus protocol. The ProofOfStore verification ensures that blocks cannot include batches with mismatched epochs. However, the lack of epoch validation at the storage layer represents a defense-in-depth failure that should be corrected to prevent potential future issues if assumptions about higher-level validation change.

### Citations

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L244-244)
```rust
        let is_new_epoch = latest_ledger_info_with_sigs.ledger_info().ends_epoch();
```

**File:** consensus/src/quorum_store/batch_store.rs (L156-176)
```rust
        if is_new_epoch {
            tokio::task::spawn_blocking(move || {
                Self::gc_previous_epoch_batches_from_db_v1(db_clone.clone(), epoch);
                Self::gc_previous_epoch_batches_from_db_v2(db_clone, epoch);
            });
        } else {
            Self::populate_cache_and_gc_expired_batches_v1(
                db_clone.clone(),
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
            Self::populate_cache_and_gc_expired_batches_v2(
                db_clone,
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L212-243)
```rust
    fn gc_previous_epoch_batches_from_db_v2(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L292-336)
```rust
    fn populate_cache_and_gc_expired_batches_v2(
        db: Arc<dyn QuorumStoreStorage>,
        current_epoch: u64,
        last_certified_time: u64,
        expiration_buffer_usecs: u64,
        batch_store: &BatchStore,
    ) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read v1 data from db");
        info!(
            epoch = current_epoch,
            "QS: Read v1 batches from storage. Len: {}, Last Cerified Time: {}",
            db_content.len(),
            last_certified_time
        );

        let mut expired_keys = Vec::new();
        let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
        for (digest, value) in db_content {
            let expiration = value.expiration();
            trace!(
                "QS: Batchreader recovery content exp {:?}, digest {}",
                expiration,
                digest
            );

            if expiration < gc_timestamp {
                expired_keys.push(digest);
            } else {
                batch_store
                    .insert_to_cache(&value)
                    .expect("Storage limit exceeded upon BatchReader construction");
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        tokio::task::spawn_blocking(move || {
            db.delete_batches_v2(expired_keys)
                .expect("Deletion of expired keys should not fail");
        });
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L545-569)
```rust
    fn get_batch_from_db(
        &self,
        digest: &HashValue,
        is_v2: bool,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        counters::GET_BATCH_FROM_DB_COUNT.inc();

        if is_v2 {
            match self.db.get_batch_v2(digest) {
                Ok(Some(value)) => Ok(value),
                Ok(None) | Err(_) => {
                    warn!("Could not get batch from db");
                    Err(ExecutorError::CouldNotGetData)
                },
            }
        } else {
            match self.db.get_batch(digest) {
                Ok(Some(value)) => Ok(value.into()),
                Ok(None) | Err(_) => {
                    warn!("Could not get batch from db");
                    Err(ExecutorError::CouldNotGetData)
                },
            }
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L571-585)
```rust
    pub(crate) fn get_batch_from_local(
        &self,
        digest: &HashValue,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        if let Some(value) = self.db_cache.get(digest) {
            if value.payload_storage_mode() == StorageMode::PersistedOnly {
                self.get_batch_from_db(digest, value.batch_info().is_v2())
            } else {
                // Available in memory.
                Ok(value.clone())
            }
        } else {
            Err(ExecutorError::CouldNotGetData)
        }
    }
```

**File:** consensus/src/quorum_store/schema.rs (L18-26)
```rust
#[derive(Debug)]
pub(crate) struct BatchSchema;

impl Schema for BatchSchema {
    type Key = HashValue;
    type Value = PersistedValue<BatchInfo>;

    const COLUMN_FAMILY_NAME: aptos_schemadb::ColumnFamilyName = BATCH_CF_NAME;
}
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L119-121)
```rust
    fn get_batch(&self, digest: &HashValue) -> Result<Option<PersistedValue<BatchInfo>>, DbError> {
        Ok(self.db.get::<BatchSchema>(digest)?)
    }
```
