# Audit Report

## Title
Critical Race Condition in BlockStore Rebuild Causes Validator Panic via Stale Block References

## Summary
The `rebuild()` function in `BlockStore` replaces the entire `BlockTree` structure without preventing concurrent threads from holding and using `Arc<PipelinedBlock>` references from the old tree. When pipeline callbacks or block window operations execute on these stale references after a rebuild, the validator node panics and crashes due to missing block lookups, causing a total loss of liveness.

## Finding Description

The vulnerability exists in the block tree rebuild mechanism during state synchronization. The core issue is a Time-of-Check-Time-of-Use (TOCTOU) race condition: [1](#0-0) 

The `rebuild()` function replaces the entire `BlockTree` by acquiring a write lock and swapping the tree contents: [2](#0-1) 

However, threads can obtain `Arc<PipelinedBlock>` references from the old tree before the rebuild occurs. These references remain valid after the rebuild because they are independent Arc-wrapped objects. When these old blocks are used, two critical panic scenarios occur:

**Scenario 1: Pipeline Callback Panic**

When blocks are inserted, pipeline callbacks are registered with a weak pointer to the tree: [3](#0-2) 

If an old block's pipeline callback fires after rebuild, it calls `commit_callback` on the NEW tree with the OLD block's ID. The `commit_callback` function then attempts to find this block: [4](#0-3) 

The `find_window_root` function expects the block to exist and panics if it doesn't: [5](#0-4) 

**Scenario 2: OrderedBlockWindow Panic**

Blocks contain an `OrderedBlockWindow` with weak pointers to parent blocks: [6](#0-5) 

When a thread holding an old block reference tries to access the block window, it attempts to upgrade these weak pointers: [7](#0-6) 

If the parent blocks were only referenced by the old tree and have been deallocated, the upgrade fails and triggers a panic.

**Attack Vector:**

A realistic exploitation occurs during normal consensus operations with concurrent state sync: [8](#0-7) 

1. Thread A calls `get_block()` at line 1658, obtaining an `Arc<PipelinedBlock>` from the current tree
2. Thread A enters the async `create_order_vote()` operation (await point at line 1661)
3. Thread B receives sync info triggering `add_certs()` → `sync_to_highest_quorum_cert()` → `rebuild()`
4. The rebuild replaces the tree with blocks from a different fork
5. Thread A resumes and accesses `proposed_block.pipeline_tx()` at line 1682
6. If the pipeline callback later fires or the block window is accessed, the validator panics

The rebuild is triggered via: [9](#0-8) 

## Impact Explanation

This vulnerability qualifies as **Critical Severity** under the Aptos Bug Bounty program because it causes **Total loss of liveness/network availability**. 

When a validator node panics due to this race condition, it crashes completely and becomes unavailable. If multiple validators experience this simultaneously (which is likely during network-wide state sync events), the network loses consensus capability. This breaks the fundamental liveness guarantee of the blockchain.

The impact is amplified because:
- State sync operations are normal and frequent during network catch-up
- No attacker privileges are required - any network peer can send sync messages
- The race window exists in multiple code paths (order votes, pipeline callbacks, block window access)
- Repeated crashes can prevent validator recovery

## Likelihood Explanation

**High Likelihood** - This vulnerability can occur during normal network operations:

1. **Trigger Frequency**: State synchronization occurs regularly when validators fall behind or during network partitions
2. **Race Window**: Multiple code paths create await points where the race can occur (async operations in vote processing, block insertion, execution pipeline)
3. **Timing**: The race window is wide enough to be naturally hit during concurrent consensus operations
4. **No Special Conditions**: Does not require attacker access, malicious validators, or specific network conditions
5. **Deterministic Outcome**: Once the race occurs, the panic is guaranteed

The `need_sync_for_ledger_info` check shows rebuilds occur when validators are behind: [10](#0-9) 

## Recommendation

Implement proper synchronization to prevent access to old tree references during and after rebuild:

1. **Add epoch/version tracking** to block references to detect staleness
2. **Acquire exclusive access** during rebuild to prevent concurrent block retrievals
3. **Gracefully handle missing blocks** instead of panicking - use `Result` return types
4. **Add reference invalidation** mechanism to mark old blocks as stale

**Specific Fix for Panic Points:**

Replace `expect()` with proper error handling:

```rust
// In block_tree.rs find_window_root()
let block = self.get_block(&block_to_commit_id)
    .ok_or_else(|| anyhow!("Block {} not found in tree after rebuild", block_to_commit_id))?;

// In pipelined_block.rs pipelined_blocks()
if let Some(block) = block.upgrade() {
    blocks.push(block);
} else {
    warn!("Block with id: {} not found during upgrade - tree was likely rebuilt", block_id);
    return Err(anyhow!("Block tree was rebuilt, stale reference"));
}
```

**Prevent Race at Source:**

```rust
// In block_store.rs rebuild()
pub async fn rebuild(...) {
    // Acquire exclusive access
    let _exclusive_guard = self.rebuild_lock.lock();
    
    // Abort ALL pending operations first
    self.abort_pipeline_for_state_sync().await;
    
    // Wait for any in-flight block retrievals to complete
    // before replacing the tree
    let mut tree_guard = self.inner.write();
    *tree_guard = new_tree;
}
```

## Proof of Concept

```rust
// Reproduction test demonstrating the race condition
#[tokio::test]
async fn test_rebuild_race_condition_panic() {
    // Setup: Create block store with initial tree
    let (mut block_store, mut runtime) = create_test_block_store();
    
    // Insert blocks B1 -> B2 -> B3
    let block1 = create_test_block(1);
    let block2 = create_test_block_with_parent(2, block1.id());
    let block3 = create_test_block_with_parent(3, block2.id());
    
    block_store.insert_block(block1).await.unwrap();
    block_store.insert_block(block2).await.unwrap();
    block_store.insert_block(block3).await.unwrap();
    
    // Thread A: Start processing order vote
    let block_store_clone = block_store.clone();
    let thread_a = tokio::spawn(async move {
        // Get block from current tree
        let old_block = block_store_clone.get_block(block3.id()).unwrap();
        
        // Simulate async operation (await point)
        tokio::time::sleep(Duration::from_millis(50)).await;
        
        // Try to access pipeline state after rebuild
        let _ = old_block.pipeline_tx().lock();
        
        // Try to access block window (will panic if parents deallocated)
        let window = old_block.block_window();
        window.pipelined_blocks() // PANIC HERE
    });
    
    // Thread B: Trigger rebuild during Thread A's await
    tokio::time::sleep(Duration::from_millis(10)).await;
    
    // Rebuild with different blocks (simulating state sync)
    let new_block1 = create_test_block(1);
    let new_block4 = create_test_block_with_parent(4, new_block1.id());
    let recovery_data = create_recovery_data(vec![new_block1, new_block4]);
    
    block_store.rebuild(
        recovery_data.root,
        recovery_data.root_metadata,
        recovery_data.blocks,
        recovery_data.quorum_certs,
    ).await;
    
    // Thread A will panic when trying to upgrade weak pointers
    let result = thread_a.await;
    assert!(result.is_err()); // Panicked due to missing blocks
}
```

**Notes**

This vulnerability represents a fundamental race condition in the block tree lifecycle management. The use of `Arc<RwLock<BlockTree>>` provides thread-safe access to the tree container, but does not prevent threads from holding stale `Arc<PipelinedBlock>` references after the tree contents are replaced. The panic-on-missing-block approach (via `expect()`) was likely chosen for simplicity but creates a critical DoS vector. Production systems should use `Result` types and graceful degradation instead of panics for validator availability.

### Citations

**File:** consensus/src/block_storage/block_store.rs (L259-264)
```rust
        let inner = if let Some(tree_to_replace) = tree_to_replace {
            *tree_to_replace.write() = tree;
            tree_to_replace
        } else {
            Arc::new(RwLock::new(tree))
        };
```

**File:** consensus/src/block_storage/block_store.rs (L352-395)
```rust
    pub async fn rebuild(
        &self,
        root: RootInfo,
        root_metadata: RootMetadata,
        blocks: Vec<Block>,
        quorum_certs: Vec<QuorumCert>,
    ) {
        info!(
            "Rebuilding block tree. root {:?}, blocks {:?}, qcs {:?}",
            root,
            blocks.iter().map(|b| b.id()).collect::<Vec<_>>(),
            quorum_certs
                .iter()
                .map(|qc| qc.certified_block().id())
                .collect::<Vec<_>>()
        );
        let max_pruned_blocks_in_mem = self.inner.read().max_pruned_blocks_in_mem();

        // Rollover the previous highest TC from the old tree to the new one.
        let prev_2chain_htc = self
            .highest_2chain_timeout_cert()
            .map(|tc| tc.as_ref().clone());
        let _ = Self::build(
            root,
            root_metadata,
            blocks,
            quorum_certs,
            prev_2chain_htc,
            self.execution_client.clone(),
            Arc::clone(&self.storage),
            max_pruned_blocks_in_mem,
            Arc::clone(&self.time_service),
            self.vote_back_pressure_limit,
            self.payload_manager.clone(),
            self.order_vote_enabled,
            self.window_size,
            self.pending_blocks.clone(),
            self.pipeline_builder.clone(),
            Some(self.inner.clone()),
        )
        .await;

        self.try_send_for_execution().await;
    }
```

**File:** consensus/src/block_storage/block_store.rs (L469-488)
```rust
            // need weak pointer to break the cycle between block tree -> pipeline block -> callback
            let block_tree = Arc::downgrade(&self.inner);
            let storage = self.storage.clone();
            let id = pipelined_block.id();
            let round = pipelined_block.round();
            let window_size = self.window_size;
            let callback = Box::new(
                move |finality_proof: WrappedLedgerInfo,
                      commit_decision: LedgerInfoWithSignatures| {
                    if let Some(tree) = block_tree.upgrade() {
                        tree.write().commit_callback(
                            storage,
                            id,
                            round,
                            finality_proof,
                            commit_decision,
                            window_size,
                        );
                    }
                },
```

**File:** consensus/src/block_storage/block_tree.rs (L477-480)
```rust
        // Try to get the block, then the ordered window, then the first block's parent ID
        let block = self
            .get_block(&block_to_commit_id)
            .expect("Block not found");
```

**File:** consensus/src/block_storage/block_tree.rs (L567-589)
```rust
    pub fn commit_callback(
        &mut self,
        storage: Arc<dyn PersistentLivenessStorage>,
        block_id: HashValue,
        block_round: Round,
        finality_proof: WrappedLedgerInfo,
        commit_decision: LedgerInfoWithSignatures,
        window_size: Option<u64>,
    ) {
        let current_round = self.commit_root().round();
        let committed_round = block_round;
        let commit_proof = finality_proof
            .create_merged_with_executed_state(commit_decision)
            .expect("Inconsistent commit proof and evaluation decision, cannot commit block");

        debug!(
            LogSchema::new(LogEvent::CommitViaBlock).round(current_round),
            committed_round = committed_round,
            block_id = block_id,
        );

        let window_root_id = self.find_window_root(block_id, window_size);
        let ids_to_remove = self.find_blocks_to_prune(window_root_id);
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L135-150)
```rust
#[derive(Clone)]
pub struct OrderedBlockWindow {
    /// `block_id` (HashValue) helps with logging in the unlikely case there are issues upgrading
    /// the `Weak` pointer (we can use `block_id`)
    blocks: Vec<(HashValue, Weak<PipelinedBlock>)>,
}

impl OrderedBlockWindow {
    pub fn new(blocks: Vec<Arc<PipelinedBlock>>) -> Self {
        Self {
            blocks: blocks
                .iter()
                .map(|x| (x.id(), Arc::downgrade(x)))
                .collect::<Vec<(HashValue, Weak<PipelinedBlock>)>>(),
        }
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L177-190)
```rust
    pub fn pipelined_blocks(&self) -> Vec<Arc<PipelinedBlock>> {
        let mut blocks: Vec<Arc<PipelinedBlock>> = Vec::new();
        for (block_id, block) in self.blocks.iter() {
            if let Some(block) = block.upgrade() {
                blocks.push(block);
            } else {
                panic!(
                    "Block with id: {} not found during upgrade in OrderedBlockWindow::pipelined_blocks()",
                    block_id
                )
            }
        }
        blocks
    }
```

**File:** consensus/src/round_manager.rs (L1653-1688)
```rust
    async fn broadcast_order_vote(
        &mut self,
        vote: &Vote,
        qc: Arc<QuorumCert>,
    ) -> anyhow::Result<()> {
        if let Some(proposed_block) = self.block_store.get_block(vote.vote_data().proposed().id()) {
            // Generate an order vote with ledger_info = proposed_block
            let order_vote = self
                .create_order_vote(proposed_block.clone(), qc.clone())
                .await?;
            if !proposed_block.block().is_nil_block() {
                observe_block(
                    proposed_block.block().timestamp_usecs(),
                    BlockStage::ORDER_VOTED,
                );
            }
            if proposed_block.block().is_opt_block() {
                observe_block(
                    proposed_block.block().timestamp_usecs(),
                    BlockStage::ORDER_VOTED_OPT_BLOCK,
                );
            }
            let order_vote_msg = OrderVoteMsg::new(order_vote, qc.as_ref().clone());
            info!(
                self.new_log(LogEvent::BroadcastOrderVote),
                "{}", order_vote_msg
            );
            self.network.broadcast_order_vote(order_vote_msg).await;
            if proposed_block.pipeline_futs().is_some() {
                if let Some(tx) = proposed_block.pipeline_tx().lock().as_mut() {
                    let _ = tx.order_vote_tx.take().map(|tx| tx.send(()));
                }
            }
            ORDER_VOTE_BROADCASTED.inc();
        }
        Ok(())
```

**File:** consensus/src/block_storage/sync_manager.rs (L65-80)
```rust
    pub fn need_sync_for_ledger_info(&self, li: &LedgerInfoWithSignatures) -> bool {
        const MAX_PRECOMMIT_GAP: u64 = 200;
        let block_not_exist = self.ordered_root().round() < li.commit_info().round()
            && !self.block_exists(li.commit_info().id());
        // TODO move min gap to fallback (30) to config, and if configurable make sure the value is
        // larger than buffer manager MAX_BACKLOG (20)
        let max_commit_gap = 30.max(2 * self.vote_back_pressure_limit);
        let min_commit_round = li.commit_info().round().saturating_sub(max_commit_gap);
        let current_commit_round = self.commit_root().round();

        if let Some(pre_commit_status) = self.pre_commit_status() {
            let mut status_guard = pre_commit_status.lock();
            if block_not_exist || status_guard.round() < min_commit_round {
                // pause the pre_commit so that pre_commit task doesn't over-commit
                // it can still commit if it receives the LI previously forwarded,
                // but it won't exceed the LI here
```

**File:** consensus/src/block_storage/sync_manager.rs (L279-314)
```rust
    async fn sync_to_highest_quorum_cert(
        &self,
        highest_quorum_cert: QuorumCert,
        highest_commit_cert: WrappedLedgerInfo,
        retriever: &mut BlockRetriever,
    ) -> anyhow::Result<()> {
        if !self.need_sync_for_ledger_info(highest_commit_cert.ledger_info()) {
            return Ok(());
        }

        if let Some(pre_commit_status) = self.pre_commit_status() {
            defer! {
                pre_commit_status.lock().resume();
            }
        }

        let (root, root_metadata, blocks, quorum_certs) = Self::fast_forward_sync(
            &highest_quorum_cert,
            &highest_commit_cert,
            retriever,
            self.storage.clone(),
            self.execution_client.clone(),
            self.payload_manager.clone(),
            self.order_vote_enabled,
            self.window_size,
            Some(self),
        )
        .await?
        .take();
        info!(
            LogSchema::new(LogEvent::CommitViaSync).round(self.ordered_root().round()),
            committed_round = root.commit_root_block.round(),
            block_id = root.commit_root_block.id(),
        );
        self.rebuild(root, root_metadata, blocks, quorum_certs)
            .await;
```
