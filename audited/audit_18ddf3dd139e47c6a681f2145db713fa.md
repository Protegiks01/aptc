# Audit Report

## Title
BlockExecutor Race Condition: Concurrent ledger_update Calls Cause Validator Node Panic

## Summary
The `BlockExecutor<V>` uses insufficient synchronization for Arc-shared concurrent access. While `execute_and_update_state` is protected by an execution lock, other mutating methods (`ledger_update`, `pre_commit_block`, `commit_ledger`) use only read locks, allowing concurrent execution that can cause validator node panics when multiple threads process the same block.

## Finding Description

The `BlockExecutor<V>` struct is designed to be wrapped in `Arc` and shared across consensus pipeline threads. However, its synchronization model is inconsistent and unsafe for concurrent access. [1](#0-0) 

The struct contains an `execution_lock: Mutex<()>` that is correctly used in `execute_and_update_state`: [2](#0-1) 

However, `ledger_update`, `pre_commit_block`, and `commit_ledger` methods only acquire read locks on `self.inner`, allowing concurrent execution: [3](#0-2) 

Within `BlockExecutorInner::ledger_update`, the code mutates shared state through `OnceCell::set()` operations: [4](#0-3) 

The `set_state_checkpoint_output` and `set_ledger_update_output` methods use `OnceCell::set()` which **panics if called twice**: [5](#0-4) 

**Race Condition Attack Path:**

1. Thread A calls `executor.ledger_update(block_id, parent_id)` through pipeline
2. Thread B calls `executor.ledger_update(block_id, parent_id)` through state sync or retry logic
3. Both acquire read locks successfully (line 122-128 in block_executor/mod.rs)
4. Both pass the `get_complete_result()` check seeing `None` (line 291 in block_executor/mod.rs)
5. Thread A calls `output.set_state_checkpoint_output()` - succeeds
6. Thread B calls `output.set_state_checkpoint_output()` - **PANICS** with "StateCheckpointOutput already set"
7. Validator node crashes

The TODO comment acknowledges this issue: [6](#0-5) 

The defensive check-then-act pattern is a classic Time-of-Check-Time-of-Use (TOCTOU) vulnerability when combined with insufficient locking.

In production, the `BlockExecutor` is Arc-wrapped and cloned across pipeline stages: [7](#0-6) [8](#0-7) 

Each pipeline stage spawns blocking tasks that could execute concurrently: [9](#0-8) 

## Impact Explanation

**Severity: HIGH ($50,000 category)**

When triggered, this vulnerability causes an unrecoverable panic that crashes the validator node, resulting in:

1. **Validator Node Crash** - The panic in `OnceCell::set()` terminates the validator process
2. **Consensus Liveness Impact** - Crashed validators cannot participate in consensus, reducing available voting power
3. **Service Unavailability** - The node must be manually restarted, causing downtime

This directly maps to the HIGH severity category: "Validator node slowdowns/API crashes/Significant protocol violations" per Aptos bug bounty guidelines.

While this doesn't cause fund loss or consensus safety violations, it breaks the **availability invariant** that validators must remain operational. If multiple validators are affected simultaneously (e.g., during synchronized state sync operations), it could significantly degrade network liveness.

## Likelihood Explanation

**Likelihood: MEDIUM**

While the normal consensus pipeline uses shared futures that should prevent duplicate execution, several scenarios can trigger this race:

1. **State Sync + Consensus Race**: During catch-up, state sync and consensus might process the same block concurrently
2. **Epoch Transition Edge Cases**: During validator set changes, multiple components might attempt ledger updates
3. **Future Handling Bugs**: Errors in async orchestration could cause duplicate task spawning
4. **Retry Logic**: The TODO comment "assuming no retries" indicates retries were considered/implemented at some point

The vulnerability is **latent** - it exists in the code and violates Arc safety guarantees, even if the specific conditions to trigger it are uncommon in normal operation. The inconsistent use of `execution_lock` (present but not uniformly applied) suggests incomplete synchronization design.

## Recommendation

**Fix: Extend execution_lock to all mutating methods**

The `execution_lock` should protect ALL methods that mutate state, not just `execute_and_update_state`. Modify the methods to acquire this lock:

```rust
fn ledger_update(
    &self,
    block_id: HashValue,
    parent_block_id: HashValue,
) -> ExecutorResult<StateComputeResult> {
    let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "ledger_update"]);
    
    // Add execution lock here
    let _lock = self.execution_lock.lock();
    
    self.inner
        .read()
        .as_ref()
        .ok_or_else(|| ExecutorError::InternalError {
            error: "BlockExecutor is not reset".into(),
        })?
        .ledger_update(block_id, parent_block_id)
}

fn pre_commit_block(&self, block_id: HashValue) -> ExecutorResult<()> {
    let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "pre_commit_block"]);
    
    // Add execution lock here
    let _lock = self.execution_lock.lock();
    
    self.inner
        .read()
        .as_ref()
        .expect("BlockExecutor is not reset")
        .pre_commit_block(block_id)
}

fn commit_ledger(&self, ledger_info_with_sigs: LedgerInfoWithSignatures) -> ExecutorResult<()> {
    let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "commit_ledger"]);
    
    // Add execution lock here
    let _lock = self.execution_lock.lock();
    
    self.inner
        .read()
        .as_ref()
        .expect("BlockExecutor is not reset")
        .commit_ledger(ledger_info_with_sigs)
}
```

**Alternative**: Use write locks instead of read locks for these methods, though the mutex approach is cleaner since these operations must be serialized anyway.

## Proof of Concept

```rust
use std::sync::Arc;
use std::thread;
use aptos_executor::block_executor::BlockExecutor;
use aptos_executor_types::BlockExecutorTrait;
use aptos_crypto::HashValue;

#[test]
fn test_concurrent_ledger_update_race() {
    // Setup: Create executor with test database
    let db_path = aptos_temppath::TempPath::new();
    db_path.create_as_dir().unwrap();
    
    let (genesis, _validators) = aptos_vm_genesis::test_genesis_change_set_and_validators(Some(1));
    let genesis_txn = Transaction::GenesisTransaction(WriteSetPayload::Direct(genesis));
    
    let (_, db, executor, _waypoint) = 
        create_db_and_executor(db_path.path(), &genesis_txn, false);
    
    // Execute a block first to create the target block
    let block_id = gen_block_id(1);
    let parent_id = executor.committed_block_id();
    
    // Execute block (this is properly synchronized)
    executor.execute_and_update_state(
        create_test_block(block_id).into(),
        parent_id,
        TEST_BLOCK_EXECUTOR_ONCHAIN_CONFIG,
    ).unwrap();
    
    // Wrap executor in Arc to enable sharing
    let executor_arc = Arc::new(executor);
    
    // Spawn two threads that concurrently call ledger_update on the same block
    let executor_clone1 = Arc::clone(&executor_arc);
    let executor_clone2 = Arc::clone(&executor_arc);
    
    let handle1 = thread::spawn(move || {
        // Thread 1: Call ledger_update
        executor_clone1.ledger_update(block_id, parent_id)
    });
    
    let handle2 = thread::spawn(move || {
        // Thread 2: Call ledger_update on same block_id concurrently
        executor_clone2.ledger_update(block_id, parent_id)
    });
    
    // Expected: One thread succeeds, the other panics with "StateCheckpointOutput already set"
    let result1 = handle1.join();
    let result2 = handle2.join();
    
    // At least one thread should panic or error due to race condition
    assert!(result1.is_err() || result2.is_err(), 
            "Expected race condition to cause panic in at least one thread");
}
```

**Notes:**
- This demonstrates the fundamental race condition in the Arc-shared BlockExecutor
- In production, the race would be triggered by concurrent pipeline stages or state sync operations
- The panic occurs in `OnceCell::set()` when both threads attempt to set the same cell
- Proper fix requires adding `execution_lock` to serialize these operations

### Citations

**File:** execution/executor/src/block_executor/mod.rs (L49-53)
```rust
pub struct BlockExecutor<V> {
    pub db: DbReaderWriter,
    inner: RwLock<Option<BlockExecutorInner<V>>>,
    execution_lock: Mutex<()>,
}
```

**File:** execution/executor/src/block_executor/mod.rs (L97-113)
```rust
    fn execute_and_update_state(
        &self,
        block: ExecutableBlock,
        parent_block_id: HashValue,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "execute_and_state_checkpoint"]);

        self.maybe_initialize()?;
        // guarantee only one block being executed at a time
        let _guard = self.execution_lock.lock();
        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .execute_and_update_state(block, parent_block_id, onchain_config)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L115-129)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "ledger_update"]);

        self.inner
            .read()
            .as_ref()
            .ok_or_else(|| ExecutorError::InternalError {
                error: "BlockExecutor is not reset".into(),
            })?
            .ledger_update(block_id, parent_block_id)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L260-334)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _timer = UPDATE_LEDGER.start_timer();
        info!(
            LogSchema::new(LogEntry::BlockExecutor).block_id(block_id),
            "ledger_update"
        );
        let committed_block_id = self.committed_block_id();
        let mut block_vec = self
            .block_tree
            .get_blocks_opt(&[block_id, parent_block_id])?;
        let parent_block = block_vec
            .pop()
            .expect("Must exist.")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
        // At this point of time two things must happen
        // 1. The block tree must also have the current block id with or without the ledger update output.
        // 2. We must have the ledger update output of the parent block.
        // Above is not ture if the block is on a forked branch.
        let block = block_vec
            .pop()
            .expect("Must exist")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
        parent_block.ensure_has_child(block_id)?;
        let output = &block.output;
        let parent_out = &parent_block.output;

        // TODO(aldenhu): remove, assuming no retries.
        if let Some(complete_result) = block.output.get_complete_result() {
            info!(block_id = block_id, "ledger_update already done.");
            return Ok(complete_result);
        }

        if parent_block_id != committed_block_id && parent_out.has_reconfiguration() {
            info!(block_id = block_id, "ledger_update for reconfig suffix.");

            // Parent must have done all state checkpoint and ledger update since this method
            // is being called.
            output.set_state_checkpoint_output(
                parent_out
                    .ensure_state_checkpoint_output()?
                    .reconfig_suffix(),
            );
            output.set_ledger_update_output(
                parent_out.ensure_ledger_update_output()?.reconfig_suffix(),
            );
        } else {
            THREAD_MANAGER.get_non_exe_cpu_pool().install(|| {
                // TODO(aldenhu): remove? no known strategy to recover from this failure
                fail_point!("executor::block_state_checkpoint", |_| {
                    Err(anyhow::anyhow!("Injected error in block state checkpoint."))
                });
                output.set_state_checkpoint_output(DoStateCheckpoint::run(
                    &output.execution_output,
                    parent_block.output.ensure_result_state_summary()?,
                    &ProvableStateSummary::new_persisted(self.db.reader.as_ref())?,
                    None,
                )?);
                output.set_ledger_update_output(DoLedgerUpdate::run(
                    &output.execution_output,
                    output.ensure_state_checkpoint_output()?,
                    parent_out
                        .ensure_ledger_update_output()?
                        .transaction_accumulator
                        .clone(),
                )?);
                Result::<_>::Ok(())
            })?;
        }

        Ok(block.output.expect_complete_result())
    }
```

**File:** execution/executor/src/types/partial_state_compute_result.rs (L76-92)
```rust
    pub fn set_state_checkpoint_output(&self, state_checkpoint_output: StateCheckpointOutput) {
        self.state_checkpoint_output
            .set(state_checkpoint_output)
            .expect("StateCheckpointOutput already set");
    }

    pub fn ensure_ledger_update_output(&self) -> Result<&LedgerUpdateOutput> {
        self.ledger_update_output
            .get()
            .context("LedgerUpdateOutput not set.")
    }

    pub fn set_ledger_update_output(&self, ledger_update_output: LedgerUpdateOutput) {
        self.ledger_update_output
            .set(ledger_update_output)
            .expect("LedgerUpdateOutput already set");
    }
```

**File:** consensus/src/state_computer.rs (L54-63)
```rust
pub struct ExecutionProxy {
    executor: Arc<dyn BlockExecutorTrait>,
    txn_notifier: Arc<dyn TxnNotifier>,
    state_sync_notifier: Arc<dyn ConsensusNotificationSender>,
    write_mutex: AsyncMutex<LogicalTime>,
    txn_filter_config: Arc<BlockTransactionFilterConfig>,
    state: RwLock<Option<MutableState>>,
    enable_pre_commit: bool,
    secret_share_config: Option<SecretShareConfig>,
}
```

**File:** consensus/src/state_computer.rs (L86-126)
```rust
    pub fn pipeline_builder(&self, commit_signer: Arc<ValidatorSigner>) -> PipelineBuilder {
        let MutableState {
            validators,
            payload_manager,
            transaction_shuffler,
            block_executor_onchain_config,
            transaction_deduper,
            is_randomness_enabled,
            consensus_onchain_config,
            persisted_auxiliary_info_version,
            network_sender,
        } = self
            .state
            .read()
            .as_ref()
            .cloned()
            .expect("must be set within an epoch");

        let block_preparer = Arc::new(BlockPreparer::new(
            payload_manager.clone(),
            self.txn_filter_config.clone(),
            transaction_deduper.clone(),
            transaction_shuffler.clone(),
        ));
        PipelineBuilder::new(
            block_preparer,
            self.executor.clone(),
            validators,
            block_executor_onchain_config,
            is_randomness_enabled,
            commit_signer,
            self.state_sync_notifier.clone(),
            payload_manager,
            self.txn_notifier.clone(),
            self.enable_pre_commit,
            &consensus_onchain_config,
            persisted_auxiliary_info_version,
            network_sender,
            self.secret_share_config.clone(),
        )
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L887-893)
```rust
        let result = tokio::task::spawn_blocking(move || {
            executor
                .ledger_update(block_clone.id(), block_clone.parent_id())
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```
