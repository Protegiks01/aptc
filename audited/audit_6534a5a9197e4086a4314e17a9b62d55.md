# Audit Report

## Title
Consensus Event Loop DoS via BoundedExecutor Permit Exhaustion

## Summary
The consensus message processing pipeline can be completely blocked by a malicious validator flooding the system with crafted messages that exhaust the BoundedExecutor's limited permit pool (16 concurrent tasks), preventing legitimate consensus messages from being processed and causing consensus delays or complete liveness failures. [1](#0-0) 

## Finding Description
The Aptos consensus layer implements a token bucket rate limiter that only restricts **byte throughput** (default 100 KiB/s per IP), but provides no protection against **message count flooding**. This creates a critical vulnerability in the message verification pipeline where consensus message processing can be completely stalled.

**The Attack Chain:**

1. **Insufficient Rate Limiting**: The rate limiter operates only at the network transport layer, limiting bytes but not message count. [2](#0-1) 

2. **Limited Verification Capacity**: The consensus layer uses a BoundedExecutor with only 16 concurrent verification tasks by default. [3](#0-2) [4](#0-3) 

3. **Blocking Permit Acquisition**: When spawning verification tasks, the executor's `spawn()` method blocks until a permit is available. [5](#0-4) 

4. **Event Loop Starvation**: The main consensus event loop calls `process_message().await`, which awaits the blocking `spawn()` call, halting all message processing. [6](#0-5) [7](#0-6) 

**Attack Execution:**

A Byzantine validator crafts minimal but structurally valid consensus messages (VoteMsg, SyncInfo, etc., ~200-300 bytes each) and sends them at maximum rate within the byte limit (100 KiB/s = ~400 messages/second). Since signature verification is CPU-intensive and takes 10-100ms per message:

- 16 verification threads can process ~160-1600 messages/second
- But the event loop blocks waiting for permits when all 16 are taken
- New messages from honest validators cannot be processed
- Consensus rounds timeout, causing liveness failures

The per-peer channel queues (size 10) provide minimal protection but don't prevent the core issue: once messages are dequeued for processing, they block waiting for permits regardless of which peer sent them. [8](#0-7) [9](#0-8) 

## Impact Explanation
**Severity: High to Critical**

This vulnerability enables a single Byzantine validator to cause:

1. **Validator Node Slowdowns** (High Severity - $50,000): Legitimate consensus messages experience significant processing delays, degrading network performance.

2. **Total Loss of Liveness** (Critical Severity - $1,000,000): If the attack is sustained, honest validators cannot process proposals, votes, or sync messages in time, causing consensus to stall completely.

3. **Invariant Violations**:
   - Breaks **Resource Limits** (Invariant #9): Verification tasks lack proper timeouts or bounded resource consumption
   - Breaks **Consensus Safety** (Invariant #2): Prevents the network from maintaining liveness under Byzantine conditions

The impact is amplified because AptosBFT is designed to tolerate up to 1/3 Byzantine validators, but this vulnerability allows a **single** malicious validator to disrupt the entire network.

## Likelihood Explanation
**Likelihood: High**

The attack is highly feasible because:

1. **Low Barrier to Entry**: Any validator can execute this attack with minimal resources
2. **Simple Exploitation**: Just send small, valid-looking messages at high rate
3. **Weak Defenses**: The byte-rate limiter (100 KiB/s) doesn't prevent message flooding
4. **No Message-Level Rate Limiting**: No per-peer message count restrictions exist
5. **No Verification Timeouts**: Signature verification tasks run until completion without deadlines

Example calculation:
- Minimal VoteMsg size: ~250 bytes
- Rate limit: 102,400 bytes/second
- Max messages: 409 messages/second
- Verification capacity: ~160-1600 messages/second (16 threads Ã— 10-100ms per message)
- Attack window: Messages 17+ block the event loop waiting for permits

## Recommendation

**Immediate Mitigations:**

1. **Implement Per-Peer Message Rate Limiting**: Add message-count based rate limiting at the consensus layer, not just byte-based at the network layer.

2. **Use Non-Blocking Verification Spawning**: Replace `bounded_executor.spawn().await` with `bounded_executor.try_spawn()` and handle capacity exhaustion gracefully by dropping messages or delaying processing without blocking the event loop.

```rust
// In epoch_manager.rs process_message():
match self.bounded_executor.try_spawn(async move {
    // verification logic
}) {
    Ok(_) => {
        // Successfully spawned
    },
    Err(_) => {
        // Capacity exhausted - drop message or queue for later
        warn!("Verification queue full, dropping message from peer {}", peer_id);
        counters::VERIFICATION_QUEUE_FULL.inc();
        return Ok(());
    }
}
```

3. **Add Verification Timeouts**: Implement per-message verification deadlines to prevent long-running verification tasks from monopolizing permits.

4. **Increase BoundedExecutor Capacity**: Dynamically scale verification threads based on CPU cores and add backpressure metrics.

5. **Implement Reputation Scoring**: Track per-peer invalid message rates and deprioritize or reject messages from suspicious peers.

## Proof of Concept

```rust
// Rust test demonstrating the vulnerability
#[tokio::test]
async fn test_consensus_event_loop_blocking() {
    use tokio::time::{timeout, Duration};
    use consensus::epoch_manager::EpochManager;
    
    // Setup: Create EpochManager with default BoundedExecutor (16 tasks)
    let (mut epoch_mgr, network_receivers) = setup_test_epoch_manager();
    
    // Attack: Send 20 consensus messages that trigger verification
    let attacker_peer = create_malicious_peer();
    for i in 0..20 {
        let vote_msg = create_minimal_vote_msg(i);
        network_receivers.consensus_messages.send((attacker_peer, vote_msg)).await.unwrap();
    }
    
    // Expected: Event loop should process all messages without blocking
    // Actual: Event loop blocks after 16 messages, timeout occurs
    let result = timeout(
        Duration::from_secs(5),
        process_all_messages(&mut epoch_mgr, &network_receivers)
    ).await;
    
    // Verification: Event loop is blocked
    assert!(result.is_err(), "Event loop blocked waiting for BoundedExecutor permits");
    
    // Impact: Honest validator messages cannot be processed
    let honest_peer = create_honest_validator();
    let critical_proposal = create_proposal_msg();
    let send_result = timeout(
        Duration::from_millis(100),
        send_and_process_message(&mut epoch_mgr, honest_peer, critical_proposal)
    ).await;
    
    assert!(send_result.is_err(), "Critical proposal from honest validator blocked");
}
```

**Notes:**

This vulnerability exists despite the token bucket rate limiter because the rate limiter operates at the wrong layer (bytes at transport) rather than where needed (messages at consensus). The combination of blocking permit acquisition and event loop design creates a critical single point of failure exploitable by any Byzantine validator.

### Citations

**File:** crates/aptos-rate-limiter/src/rate_limit.rs (L1-380)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use aptos_infallible::{Mutex, RwLock};
use aptos_logger::debug;
use aptos_metrics_core::HistogramVec;
use std::{cmp::min, collections::HashMap, fmt::Debug, hash::Hash, sync::Arc, time::Instant};
use tokio::time::Duration;

pub type SharedBucket = Arc<Mutex<Bucket>>;

const ONE_SEC: Duration = Duration::from_secs(1);

/// A generic token bucket filter
///
/// # Terms
/// ## Key
/// A `key` is an identifier of the item being rate limited
///
/// ## Token
/// A `token` is the smallest discrete value that we want to rate limit by.  In a situation involving
/// network requests, this may represent a request or a byte.  `Tokens` are the counters for the
/// rate limiting, and when there are no `tokens` left in a `bucket`, the `key` is throttled.
///
/// ## Bucket
/// A `bucket` is the tracker of the number of `tokens`.  It has a `bucket size`, and any additional
/// tokens added to it will "spill" out of the `bucket`.  The `buckets` are filled at an `interval`
/// with a given `fill rate`.
///
/// ## Interval
/// The `interval` at which we refill *all* of the `buckets` in the token bucket filter. Configured
/// across the whole token bucket filter.
///
/// ## Fill Rate
/// The rate at which we fill a `bucket` with tokens. Configured per bucket.
///
/// ## Bucket Size
/// Maximum size of a bucket.  A bucket saturates at this size.  Configured per bucket.
///
/// # Features
/// ## Keys
/// The token bucket takes any key as long as it's hashable.  This should allow it to apply to
/// many applications that need rate limiters.
///
/// ## Bucket sizes and Rates
/// ### Defaults
/// There are defaults for bucket size and fill rate, which will apply to unknown keys.
///
/// ### Refill Interval
/// Buckets are refilled automatically at an interval.  To do this synchronously, it calculates the
/// number of intervals that have passed.  This is done synchronously and in the future may be done
/// asynchronously.
///
pub struct TokenBucketRateLimiter<Key: Eq + Hash + Clone + Debug> {
    label: &'static str,
    log_info: String,
    buckets: RwLock<HashMap<Key, SharedBucket>>,
    new_bucket_start_percentage: u8,
    default_bucket_size: usize,
    default_fill_rate: usize,
    enabled: bool,
    metrics: Option<HistogramVec>,
}

impl<Key: Eq + Hash + Clone + Debug> TokenBucketRateLimiter<Key> {
    pub fn new(
        label: &'static str,
        log_info: String,
        new_bucket_start_percentage: u8,
        default_bucket_size: usize,
        default_fill_rate: usize,
        metrics: Option<HistogramVec>,
    ) -> Self {
        // Ensure that we can actually use the rate limiter
        assert!(new_bucket_start_percentage <= 100);
        assert!(default_bucket_size > 0);
        assert!(default_fill_rate > 0);

        Self {
            label,
            log_info,
            buckets: RwLock::new(HashMap::new()),
            new_bucket_start_percentage,
            default_bucket_size,
            default_fill_rate,
            enabled: true,
            metrics,
        }
    }

    pub fn test(default_bucket_size: usize, default_fill_rate: usize) -> Self {
        Self::new(
            "test",
            "test".to_string(),
            100,
            default_bucket_size,
            default_fill_rate,
            None,
        )
    }

    /// Used for testing and to not have a rate limiter
    pub fn open(label: &'static str) -> Self {
        Self {
            label,
            log_info: String::new(),
            buckets: RwLock::new(HashMap::new()),
            new_bucket_start_percentage: 100,
            default_bucket_size: usize::MAX,
            default_fill_rate: usize::MAX,
            enabled: false,
            metrics: None,
        }
    }

    /// Retrieve bucket, or create a new one
    pub fn bucket(&self, key: Key) -> SharedBucket {
        self.bucket_inner(key, |label, log_info, key, initial, size, rate, metrics| {
            Arc::new(Mutex::new(
                if self.enabled {
                    Bucket::new(label, log_info, key, initial, size, rate, metrics)
                } else {
                    Bucket::open(label)
                },
            ))
        })
    }

    fn bucket_inner<
        F: FnOnce(String, String, String, usize, usize, usize, Option<HistogramVec>) -> SharedBucket,
    >(
        &self,
        key: Key,
        bucket_create: F,
    ) -> SharedBucket {
        // Attempt to do a weaker read lock first, followed by a write lock if it's missing
        // For the common (read) case, there should be higher throughput
        // Note: This read must happen in a separate block, to ensure the read unlock for the write
        let maybe_bucket = { self.buckets.read().get(&key).cloned() };
        if let Some(bucket) = maybe_bucket {
            bucket
        } else {
            let size = self.default_bucket_size;
            let rate = self.default_fill_rate;

            // Write in a bucket, but make sure again that it isn't there first
            self.buckets
                .write()
                .entry(key.clone())
                .or_insert_with(|| {
                    bucket_create(
                        self.label.to_string(),
                        self.log_info.clone(),
                        format!("{:?}", key),
                        size.saturating_mul(self.new_bucket_start_percentage as usize) / 100,
                        size,
                        rate,
                        self.metrics.clone(),
                    )
                })
                .clone()
        }
    }

    /// Garbage collects a single key, if we know what it is
    pub fn try_garbage_collect_key(&self, key: &Key) -> bool {
        let mut buckets = self.buckets.write();
        let remove = buckets
            .get(key)
            .is_some_and(|bucket| Arc::strong_count(bucket) <= 1);
        if remove {
            buckets.remove(key);
        }
        remove
    }
}

/// A token bucket object that keeps track of everything related to a key
/// This can be used as a standalone rate limiter; however, to make it more useful
/// it should be wrapped in an `Arc` and a `Mutex` to be shared across threads.
#[derive(Debug)]
pub struct Bucket {
    /// Label for what rate limiter it's attached to for logging & metrics purposes
    label: String,
    /// Information to be logged, but can't be put in the metrics for performance reasons
    log_info: String,
    /// The key for metrics purposes
    key: String,
    /// The current number of available tokens to be used
    tokens: usize,
    /// Maximum number of `tokens` in the bucket
    size: usize,
    /// The fill rate of the bucket (`tokens/s`).  Amount added to `tokens` on a `refill`
    rate: usize,
    /// The last time buckets were refilled, to keep track of for amount to refill
    last_refresh_time: Instant,
    /// Determines whether the rate limiting should be ignored, useful for testing
    enabled: bool,
    /// Number of requests allowed through prior to next fill
    allowed_in_period: usize,
    /// Number of requests throttled prior to next fill
    throttled_in_period: usize,
    metrics: Option<HistogramVec>,
}

impl Bucket {
    pub fn new(
        label: String,
        log_info: String,
        key: String,
        initial: usize,
        size: usize,
        rate: usize,
        metrics: Option<HistogramVec>,
    ) -> Self {
        assert!(
            size >= rate,
            "Bucket size must be greater than or equal to fill rate"
        );
        // Store the stringified version of the key for logging
        Self {
            label,
            log_info,
            key,
            tokens: initial,
            size,
            rate,
            last_refresh_time: Instant::now(),
            enabled: true,
            allowed_in_period: 0,
            throttled_in_period: 0,
            metrics,
        }
    }

    /// A fully open rate limiter, to allow for ignoring rate limiting for tests
    pub fn open(label: String) -> Self {
        Self {
            label,
            log_info: String::new(),
            key: String::new(),
            tokens: usize::MAX,
            size: usize::MAX,
            rate: usize::MAX,
            last_refresh_time: Instant::now(),
            enabled: false,
            allowed_in_period: 0,
            throttled_in_period: 0,
            metrics: None,
        }
    }

    /// Refill tokens based on how many seconds have passed since last refresh
    pub(crate) fn refill(&mut self) {
        let num_intervals = self.last_refresh_time.elapsed().as_secs();
        if num_intervals > 0 {
            // Log how many were throttled in the period before refill
            if self.allowed_in_period > 0 || self.throttled_in_period > 0 {
                debug!(
                    throttle_label = self.label,
                    throttle_log_info = self.log_info,
                    throttle_key = self.key,
                    num_allowed_in_period = self.allowed_in_period,
                    num_throttled_in_period = self.throttled_in_period,
                    "{}-{}-{}={}/{}",
                    self.label,
                    self.log_info,
                    self.key,
                    self.allowed_in_period,
                    self.allowed_in_period
                        .saturating_add(self.throttled_in_period),
                );
            }

            // Optional metrics
            if let Some(metrics) = self.metrics.as_ref() {
                metrics
                    .with_label_values(&[self.label.as_str(), "allowed"])
                    .observe(self.allowed_in_period as f64);
                metrics
                    .with_label_values(&[self.label.as_str(), "throttled"])
                    .observe(self.throttled_in_period as f64);
            }
            self.allowed_in_period = 0;
            self.throttled_in_period = 0;
            self.add_tokens((num_intervals as usize).saturating_mul(self.rate));

            // We have to base everything off the original time, or we'll have drift where we slowly slow the bucket refill rate
            self.last_refresh_time += Duration::from_secs(num_intervals);
        }
    }

    /// Determine if an entire batch can be passed through
    /// This is important for message based rate limiting, where the whole message has
    /// to make it through, or else it must be rejected.  A result of `None` means it cannot
    /// ever be allowed through, as it's bigger than the size of the bucket.
    pub fn acquire_all_tokens(&mut self, requested: usize) -> Result<(), Option<Instant>> {
        // Skip over if we purposely have an open throttle
        if !self.enabled || requested == 0 {
            return Ok(());
        }

        // Refill if needed
        self.refill();

        if self.tokens >= requested {
            self.deduct_tokens(requested);
            self.allowed_in_period = self.allowed_in_period.saturating_add(requested);
            Ok(())
        } else {
            // Keep track of the requests we've throttled
            self.throttled_in_period = self.throttled_in_period.saturating_add(requested);
            Err(self.time_of_tokens_needed(requested))
        }
    }

    /// Returns `usize` of tokens allowed.  May be less than requested.
    /// For best effort, caller should return unused tokens with `add_tokens`
    pub fn acquire_tokens(&mut self, requested: usize) -> Result<usize, Instant> {
        // Skip over if we purposely have an open throttle
        if !self.enabled || requested == 0 {
            return Ok(requested);
        }

        // Refill if needed
        self.refill();

        let allowed = self.deduct_tokens(requested);
        if allowed > 0 {
            self.allowed_in_period = self.allowed_in_period.saturating_add(allowed);
            Ok(allowed)
        } else {
            // Keep track of the requests we've throttled
            self.throttled_in_period = self.throttled_in_period.saturating_add(requested);
            Err(self.time_of_next_refill())
        }
    }

    /// Retrieve the maximum amount of tokens up to `count`
    /// Tells us how much of the requested size we can send
    fn deduct_tokens(&mut self, requested: usize) -> usize {
        let tokens_allowed = min(self.tokens, requested);
        self.tokens = self.tokens.saturating_sub(requested);

        tokens_allowed
    }

    /// Tells us when the next refill is
    pub fn time_of_next_refill(&self) -> Instant {
        self.last_refresh_time + ONE_SEC
    }

    /// Tells us when an entire batch will make it through.  Useful for Async work to wait until
    /// all tokens are ready.  Returns `None` if it is never possible.
    pub fn time_of_tokens_needed(&self, requested: usize) -> Option<Instant> {
        if !self.enabled {
            Some(Instant::now())
        } else if self.size < requested {
            // This means the batch can never succeed
            None
        } else {
            let tokens_needed = requested.saturating_sub(self.tokens);

            let intervals = (tokens_needed as f64 / self.rate as f64).ceil() as u32;
            Some(self.last_refresh_time + (ONE_SEC * intervals))
        }
    }

    /// Add new tokens
    /// Ensures bucket doesn't overfill
    fn add_tokens(&mut self, new_tokens: usize) {
        self.tokens = min(self.size, self.tokens.saturating_add(new_tokens));
    }

    /// Returns tokens that were unused
    pub fn return_tokens(&mut self, new_tokens: usize) {
        self.allowed_in_period = self.allowed_in_period.saturating_sub(new_tokens);
        self.add_tokens(new_tokens);
    }
}
```

**File:** config/src/config/network_config.rs (L366-388)
```rust
#[derive(Clone, Copy, Debug, Deserialize, Eq, PartialEq, Serialize)]
#[serde(deny_unknown_fields)]
pub struct RateLimitConfig {
    /// Maximum number of bytes/s for an IP
    pub ip_byte_bucket_rate: usize,
    /// Maximum burst of bytes for an IP
    pub ip_byte_bucket_size: usize,
    /// Initial amount of tokens initially in the bucket
    pub initial_bucket_fill_percentage: u8,
    /// Allow for disabling the throttles
    pub enabled: bool,
}

impl Default for RateLimitConfig {
    fn default() -> Self {
        Self {
            ip_byte_bucket_rate: IP_BYTE_BUCKET_RATE,
            ip_byte_bucket_size: IP_BYTE_BUCKET_SIZE,
            initial_bucket_fill_percentage: 25,
            enabled: true,
        }
    }
}
```

**File:** config/src/config/consensus_config.rs (L97-97)
```rust
    pub num_bounded_executor_tasks: u64,
```

**File:** config/src/config/consensus_config.rs (L379-379)
```rust
            num_bounded_executor_tasks: 16,
```

**File:** crates/bounded-executor/src/executor.rs (L45-52)
```rust
    pub async fn spawn<F>(&self, future: F) -> JoinHandle<F::Output>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        let permit = self.acquire_permit().await;
        self.executor.spawn(future_with_permit(future, permit))
    }
```

**File:** consensus/src/epoch_manager.rs (L1587-1622)
```rust
            self.bounded_executor
                .spawn(async move {
                    match monitor!(
                        "verify_message",
                        unverified_event.clone().verify(
                            peer_id,
                            &epoch_state.verifier,
                            &proof_cache,
                            quorum_store_enabled,
                            peer_id == my_peer_id,
                            max_num_batches,
                            max_batch_expiry_gap_usecs,
                        )
                    ) {
                        Ok(verified_event) => {
                            Self::forward_event(
                                quorum_store_msg_tx,
                                round_manager_tx,
                                buffered_proposal_tx,
                                peer_id,
                                verified_event,
                                payload_manager,
                                pending_blocks,
                            );
                        },
                        Err(e) => {
                            error!(
                                SecurityEvent::ConsensusInvalidMessage,
                                remote_peer = peer_id,
                                error = ?e,
                                unverified_event = unverified_event
                            );
                        },
                    }
                })
                .await;
```

**File:** consensus/src/epoch_manager.rs (L1931-1936)
```rust
                (peer, msg) = network_receivers.consensus_messages.select_next_some() => {
                    monitor!("epoch_manager_process_consensus_messages",
                    if let Err(e) = self.process_message(peer, msg).await {
                        error!(epoch = self.epoch(), error = ?e, kind = error_kind(&e));
                    });
                },
```

**File:** consensus/src/network.rs (L757-769)
```rust
        let (consensus_messages_tx, consensus_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            10,
            Some(&counters::CONSENSUS_CHANNEL_MSGS),
        );
        let (quorum_store_messages_tx, quorum_store_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            // TODO: tune this value based on quorum store messages with backpressure
            50,
            Some(&counters::QUORUM_STORE_CHANNEL_MSGS),
        );
        let (rpc_tx, rpc_rx) =
            aptos_channel::new(QueueStyle::FIFO, 10, Some(&counters::RPC_CHANNEL_MSGS));
```

**File:** crates/channel/src/message_queues.rs (L134-147)
```rust
        if key_message_queue.len() >= self.max_queue_size.get() {
            if let Some(c) = self.counters.as_ref() {
                c.with_label_values(&["dropped"]).inc();
            }
            match self.queue_style {
                // Drop the newest message for FIFO
                QueueStyle::FIFO => Some(message),
                // Drop the oldest message for LIFO
                QueueStyle::LIFO | QueueStyle::KLAST => {
                    let oldest = key_message_queue.pop_front();
                    key_message_queue.push_back(message);
                    oldest
                },
            }
```
