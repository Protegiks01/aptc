# Audit Report

## Title
Epoch Snapshot Pruner Starvation Due to I/O Resource Exhaustion by Regular State Merkle Pruner

## Summary
The regular state merkle pruner and epoch snapshot pruner compete for the same RocksDB I/O resources without any coordination or fairness mechanism. Under high transaction throughput, the regular pruner processes significantly more stale nodes and can monopolize database write resources, preventing the epoch snapshot pruner from making progress. This leads to unbounded growth of epoch snapshot data in the `StaleNodeIndexCrossEpochSchema`.

## Finding Description

Both pruners share the same `state_merkle_db` instance and write to the same RocksDB database shards: [1](#0-0) 

Each pruner runs in an independent worker thread with identical behavior: [2](#0-1) 

The critical issue is that when `is_pruning_pending()` returns true (line 65), the worker immediately calls `prune()` again with **no sleep or delay**. This creates a tight loop that continuously submits write requests to RocksDB.

Database writes are synchronous and blocking: [3](#0-2) 

The `write_opt()` call on line 297 blocks when RocksDB applies backpressure due to write buffer pressure or L0 file accumulation.

Stale nodes are distributed asymmetrically between the two schemas based on epoch boundaries: [4](#0-3) 

In high-throughput networks, most stale nodes are recent (after the last epoch boundary) and assigned to `StaleNodeIndexSchema`, while only nodes at or before epoch boundaries go to `StaleNodeIndexCrossEpochSchema`. This creates a workload imbalance where the regular pruner has orders of magnitude more work than the epoch snapshot pruner.

Both pruners use identical default configurations: [5](#0-4) 

**The Starvation Mechanism:**

1. Under high transaction throughput, the regular pruner has millions of stale nodes to process
2. It continuously writes large batches (1,000 nodes) to RocksDB with no delays between batches
3. This keeps RocksDB under constant write pressure, potentially triggering write stalls
4. Both pruners' writes are blocked during RocksDB write stalls
5. When the stall clears, the regular pruner (having more pending work) immediately submits more writes
6. The epoch snapshot pruner's writes are queued behind the regular pruner's continuous stream
7. The epoch snapshot pruner cannot make progress and old epoch snapshot indices accumulate
8. Database grows unbounded with undeleted `StaleNodeIndexCrossEpochSchema` entries

## Impact Explanation

This qualifies as **Medium severity** under the Aptos bug bounty program: "State inconsistencies requiring intervention."

The unbounded growth of epoch snapshot data causes:
- Continuous disk space consumption requiring manual intervention
- Potential disk exhaustion if left unaddressed
- Degraded query performance as the index grows
- Operational overhead to monitor and manually prune old data

While this does not immediately affect consensus safety or cause fund loss, it creates a state inconsistency that requires administrator intervention to resolve, matching the Medium severity criteria.

## Likelihood Explanation

This vulnerability is **highly likely** to occur in production:

1. **No special conditions required**: Happens naturally under normal high-throughput operation
2. **No malicious actor needed**: Results from design flaw, not attack
3. **Realistic workload**: Modern blockchains regularly experience sustained high transaction volumes
4. **No configuration protects against it**: Default configurations are identical for both pruners
5. **Observable in metrics**: Operators would see `epoch_snapshot_pruner` progress stagnating while `state_merkle_pruner` progresses normally

The vulnerability manifests whenever transaction throughput generates stale nodes faster than the epoch snapshot pruner can process them due to I/O starvation.

## Recommendation

Implement fair resource allocation between the two pruners:

**Option 1: Add Priority-Based Scheduling**
```rust
// In PrunerWorkerInner::work()
fn work(&self) {
    while !self.quit_worker.load(Ordering::SeqCst) {
        let pruner_result = self.pruner.prune(self.batch_size);
        if pruner_result.is_err() {
            sample!(
                SampleRate::Duration(Duration::from_secs(1)),
                error!(error = ?pruner_result.err().unwrap(), "Pruner has error.")
            );
            sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            continue;
        }
        
        // Always sleep between batches to allow other pruners to make progress
        // Even when pruning is still pending
        sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
    }
}
```

**Option 2: Implement Adaptive Rate Limiting**
Monitor RocksDB write stall metrics and dynamically adjust batch sizes and sleep intervals based on database health. Give the epoch snapshot pruner higher priority when it falls behind.

**Option 3: Separate Write Queues**
Use separate RocksDB write queues or instance priorities to ensure the epoch snapshot pruner gets guaranteed I/O bandwidth.

The simplest fix is Option 1: ensure workers always sleep between batches regardless of pending work. This provides fairness and prevents any single pruner from monopolizing I/O resources.

## Proof of Concept

```rust
// Test to demonstrate starvation under high load
#[test]
fn test_epoch_pruner_starvation() {
    use std::time::Duration;
    use std::thread;
    
    // Setup: Create AptosDB with both pruners enabled
    let db = create_test_db_with_pruners();
    
    // Generate high-volume state updates over multiple epochs
    // Most updates create stale nodes for regular pruner
    for epoch in 0..3 {
        // Simulate epoch boundary
        db.save_epoch_ending_version(epoch * 10000);
        
        // Generate 100k transactions with state updates
        for version in (epoch * 10000)..((epoch + 1) * 10000) {
            let updates = generate_state_updates(100); // 100 state values per txn
            db.save_transactions(version, updates);
        }
    }
    
    // Monitor pruner progress for 60 seconds
    let mut regular_progress = Vec::new();
    let mut epoch_progress = Vec::new();
    
    for _ in 0..60 {
        regular_progress.push(
            db.state_store.state_merkle_pruner.progress()
        );
        epoch_progress.push(
            db.state_store.epoch_snapshot_pruner.progress()
        );
        thread::sleep(Duration::from_secs(1));
    }
    
    // Assert: Regular pruner makes consistent progress
    assert!(regular_progress.last() > regular_progress.first());
    
    // Assert: Epoch pruner progress is stagnant or minimal
    let epoch_delta = epoch_progress.last() - epoch_progress.first();
    let regular_delta = regular_progress.last() - regular_progress.first();
    
    // Epoch pruner should be falling far behind regular pruner
    assert!(epoch_delta < regular_delta / 100, 
        "Epoch pruner starvation detected: only progressed {} versions while regular pruner progressed {}",
        epoch_delta, regular_delta
    );
    
    // Verify unbounded growth: StaleNodeIndexCrossEpochSchema continues accumulating
    let epoch_index_count = db.state_merkle_db
        .iter::<StaleNodeIndexCrossEpochSchema>()
        .unwrap()
        .count();
    
    assert!(epoch_index_count > expected_pruned_count * 2,
        "Unbounded growth detected: {} unpruned epoch indices",
        epoch_index_count
    );
}
```

## Notes

This vulnerability affects any Aptos node running with default pruner configurations under sustained high transaction throughput. The lack of fairness in I/O resource allocation between the two pruners is a fundamental design issue that allows one pruner to starve the other. While metrics would show the issue (`epoch_snapshot_pruner` falling behind), operators have no configuration options to mitigate it without code changes.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L60-66)
```rust
        let state_merkle_pruner = StateMerklePrunerManager::new(
            Arc::clone(&state_merkle_db),
            pruner_config.state_merkle_pruner_config,
        );
        let epoch_snapshot_pruner = StateMerklePrunerManager::new(
            Arc::clone(&state_merkle_db),
            pruner_config.epoch_snapshot_pruner_config.into(),
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-68)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
```

**File:** storage/schemadb/src/lib.rs (L289-309)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
    }

    /// Writes a group of records wrapped in a [`SchemaBatch`].
    pub fn write_schemas(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &sync_write_option())
    }
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L376-386)
```rust
        stale_node_index_batch.iter().try_for_each(|row| {
            ensure!(row.node_key.get_shard_id() == shard_id, "shard_id mismatch");
            if previous_epoch_ending_version.is_some()
                && row.node_key.version() <= previous_epoch_ending_version.unwrap()
            {
                batch.put::<StaleNodeIndexCrossEpochSchema>(row, &())
            } else {
                // These are processed by the state merkle pruner.
                batch.put::<StaleNodeIndexSchema>(row, &())
            }
        })?;
```

**File:** config/src/config/storage_config.rs (L398-431)
```rust
impl Default for StateMerklePrunerConfig {
    fn default() -> Self {
        StateMerklePrunerConfig {
            enable: true,
            // This allows a block / chunk being executed to have access to a non-latest state tree.
            // It needs to be greater than the number of versions the state committing thread is
            // able to commit during the execution of the block / chunk. If the bad case indeed
            // happens due to this being too small, a node restart should recover it.
            // Still, defaulting to 1M to be super safe.
            prune_window: 1_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
}

impl Default for EpochSnapshotPrunerConfig {
    fn default() -> Self {
        Self {
            enable: true,
            // This is based on ~5K TPS * 2h/epoch * 2 epochs. -- epoch ending snapshots are used
            // by state sync in fast sync mode.
            // The setting is in versions, not epochs, because this makes it behave more like other
            // pruners: a slower network will have longer history in db with the same pruner
            // settings, but the disk space take will be similar.
            // settings.
            prune_window: 80_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
}
```
