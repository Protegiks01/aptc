# Audit Report

## Title
Gas Metering Bypass via Saturating Arithmetic in AbstractMemorySize/AbstractValueSize Calculations

## Summary
The Move VM's abstract memory/value size calculations use saturating arithmetic instead of checked arithmetic, allowing size computations to silently cap at `u64::MAX` when they should overflow. This enables attackers to craft extremely large Move values that consume far more computational resources than the gas they are charged for, violating the fundamental economic security model of the blockchain.

## Finding Description

The vulnerability exists in the core gas algebra implementation used throughout the Aptos Move VM. Both legacy (`AbstractMemorySize`) and modern (`AbstractValueSize`) size calculations rely on `GasQuantity<U>`, which implements addition using saturating arithmetic: [1](#0-0) 

When calculating the abstract size of Move values, the system accumulates sizes by repeatedly adding component sizes. In the legacy implementation: [2](#0-1) 

And in the modern production gas meter: [3](#0-2) 

**Attack Path:**

1. Attacker crafts a deeply nested Move value (e.g., vectors of vectors of structs)
2. Each structural component adds to the abstract size (40 per struct/vector base, plus per-element costs)
3. When accumulated size exceeds `u64::MAX` (18,446,744,073,709,551,615), arithmetic saturates at max value
4. Gas charging uses saturated value: `cost = unit_cost * saturated_size`
5. Actual computational cost vastly exceeds charged gas

**Production Code Affected:**

The native function `write_to_event_store` directly uses this vulnerable calculation: [4](#0-3) 

The modern production gas meter also uses the same vulnerable pattern: [5](#0-4) [6](#0-5) 

**Invariants Broken:**
- **Invariant #9**: "Resource Limits: All operations must respect gas, storage, and computational limits"
- **Invariant #3**: "Move VM Safety: Bytecode execution must respect gas limits and memory constraints"

## Impact Explanation

**Severity: HIGH to CRITICAL**

This vulnerability enables **gas metering bypass**, a critical economic security failure:

1. **Free/Underpriced Computation**: Attackers can perform expensive operations while paying minimal gas
2. **Validator Resource Exhaustion**: Can overload validators with underpriced workload
3. **Economic Model Violation**: Breaks the fundamental cost-for-computation guarantee
4. **Deterministic (No Consensus Split)**: All validators compute the same saturated value, so no fork occurs, but economic security is compromised

While this does not directly cause loss of funds or consensus splits (satisfying deterministic execution), it violates the economic security model that prevents resource exhaustion attacks. Per Aptos bug bounty criteria:
- Meets **High Severity**: "Validator node slowdowns" and "Significant protocol violations"
- Potentially **Critical**: If exploited at scale, could cause "Total loss of liveness/network availability"

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

- **Feasibility**: Moderate - requires crafting deeply nested structures to exceed u64::MAX
- **Requirements**: Any transaction sender with ability to create Move values
- **Detection**: Low - saturated calculations appear valid, no error raised
- **Complexity**: Moderate - attacker must calculate precise nesting to trigger overflow

With abstract sizes of 40+ units per structure level and vector elements, reaching u64::MAX requires approximately:
- `18,446,744,073,709,551,615 / 40 â‰ˆ 460 billion nested levels/elements`

However, with packed vectors (e.g., `vec<u256>` at 32 bytes per element), the threshold is much lower, making exploitation realistic for motivated attackers.

## Recommendation

Replace saturating arithmetic with **checked arithmetic** that returns an error on overflow:

```rust
// In gas_algebra.rs, replace saturating_add with checked_add
impl<U> Add<GasQuantity<U>> for GasQuantity<U> {
    type Output = Self;

    fn add(self, rhs: Self) -> Self::Output {
        // Option 1: Return max on overflow (current behavior, but explicit)
        Self::new(self.val.saturating_add(rhs.val))
        
        // Option 2: Panic on overflow (breaks backward compatibility)
        Self::new(self.val.checked_add(rhs.val).expect("Gas quantity overflow"))
        
        // Option 3 (RECOMMENDED): Introduce checked version
        Self::new(self.val.checked_add(rhs.val).unwrap_or(u64::MAX))
    }
}

// Better: Add a checked_add method that returns Result
impl<U> GasQuantity<U> {
    pub fn checked_add(self, other: Self) -> Option<Self> {
        self.val.checked_add(other.val).map(Self::new)
    }
}
```

Then update all size calculation visitors to use checked arithmetic and return errors:

```rust
// In views.rs and misc.rs visitors
fn visit_u8(&mut self, depth: u64, _val: u8) -> PartialVMResult<()> {
    self.check_depth(depth)?;
    self.size = self.size.checked_add(self.params.u8)
        .ok_or_else(|| PartialVMError::new(StatusCode::ARITHMETIC_OVERFLOW))?;
    Ok(())
}
```

## Proof of Concept

```rust
// Rust unit test demonstrating saturation
#[test]
fn test_abstract_memory_size_saturation() {
    use move_core_types::gas_algebra::AbstractMemorySize;
    
    // Create a size near u64::MAX
    let large_size = AbstractMemorySize::new(u64::MAX - 100);
    let addition = AbstractMemorySize::new(200);
    
    // This should overflow but instead saturates
    let result = large_size + addition;
    
    // Result is u64::MAX, not an error!
    assert_eq!(u64::from(result), u64::MAX);
    println!("Size saturated at: {}", result);
    
    // In production, this means gas charged = saturated_value
    // But actual resource consumption = would-be-overflowed value
    // Leading to massive undercharging
}
```

```move
// Move module demonstrating attack vector
module attacker::gas_bypass {
    use std::vector;
    
    // Create deeply nested vectors to exceed abstract size limits
    public fun create_massive_value(): vector<vector<vector<u128>>> {
        let outer = vector::empty();
        let i = 0;
        
        // Each nesting level adds to abstract size
        // With enough nesting/elements, size calculation saturates
        while (i < 1000000) {
            let middle = vector::empty();
            let j = 0;
            while (j < 1000) {
                let inner = vector::empty();
                let k = 0;
                while (k < 1000) {
                    vector::push_back(&mut inner, 12345678901234567890u128);
                    k = k + 1;
                };
                vector::push_back(&mut middle, inner);
                j = j + 1;
            };
            vector::push_back(&mut outer, middle);
            i = i + 1;
        };
        
        // When copied/moved, abstract size saturates
        // Gas charged: saturated_value * unit_cost << actual cost
        outer
    }
    
    public entry fun exploit_gas_undercharging() {
        let massive = create_massive_value();
        // Operations on 'massive' are severely undercharged
        let _copy = massive; // CopyLoc with saturated size
    }
}
```

**Notes:**

This vulnerability affects the fundamental gas metering infrastructure of the Aptos Move VM. While saturation prevents integer wrapping (avoiding consensus splits), it creates a severe economic security issue where computational costs can vastly exceed charged gas. The deterministic nature means all validators are affected uniformly, but the economic model is violated. This requires immediate patching with checked arithmetic and proper overflow handling throughout the gas calculation system.

### Citations

**File:** third_party/move/move-core/types/src/gas_algebra.rs (L205-217)
```rust
impl<U> Add<GasQuantity<U>> for GasQuantity<U> {
    type Output = Self;

    fn add(self, rhs: Self) -> Self::Output {
        Self::new(self.val.saturating_add(rhs.val))
    }
}

impl<U> AddAssign<GasQuantity<U>> for GasQuantity<U> {
    fn add_assign(&mut self, rhs: GasQuantity<U>) {
        *self = *self + rhs
    }
}
```

**File:** third_party/move/move-vm/types/src/views.rs (L47-209)
```rust
        struct Acc(AbstractMemorySize);

        impl ValueVisitor for Acc {
            fn visit_delayed(&mut self, _depth: u64, _id: DelayedFieldID) -> PartialVMResult<()> {
                // TODO[agg_v2](cleanup): `legacy_abstract_memory_size` is not used
                //   anyway, so this function will be removed soon (hopefully).
                //   Contributions are appreciated!
                Ok(())
            }

            fn visit_u8(&mut self, _depth: u64, _val: u8) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_u16(&mut self, _depth: u64, _val: u16) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_u32(&mut self, _depth: u64, _val: u32) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_u64(&mut self, _depth: u64, _val: u64) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_u128(&mut self, _depth: u64, _val: u128) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_u256(
                &mut self,
                _depth: u64,
                _val: &move_core_types::int256::U256,
            ) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_i8(&mut self, _depth: u64, _val: i8) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_i16(&mut self, _depth: u64, _val: i16) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_i32(&mut self, _depth: u64, _val: i32) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_i64(&mut self, _depth: u64, _val: i64) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_i128(&mut self, _depth: u64, _val: i128) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_i256(
                &mut self,
                _depth: u64,
                _val: &move_core_types::int256::I256,
            ) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_bool(&mut self, _depth: u64, _val: bool) -> PartialVMResult<()> {
                self.0 += LEGACY_CONST_SIZE;
                Ok(())
            }

            fn visit_address(&mut self, _depth: u64, _val: &AccountAddress) -> PartialVMResult<()> {
                self.0 += AbstractMemorySize::new(AccountAddress::LENGTH as u64);
                Ok(())
            }

            fn visit_struct(&mut self, _depth: u64, _len: usize) -> PartialVMResult<bool> {
                self.0 += LEGACY_STRUCT_SIZE;
                Ok(true)
            }

            fn visit_closure(&mut self, _depth: u64, _len: usize) -> PartialVMResult<bool> {
                self.0 += LEGACY_CLOSURE_SIZE;
                Ok(true)
            }

            fn visit_vec(&mut self, _depth: u64, _len: usize) -> PartialVMResult<bool> {
                self.0 += LEGACY_STRUCT_SIZE;
                Ok(true)
            }

            fn visit_vec_u8(&mut self, _depth: u64, vals: &[u8]) -> PartialVMResult<()> {
                self.0 += (size_of_val(vals) as u64).into();
                Ok(())
            }

            fn visit_vec_u16(&mut self, _depth: u64, vals: &[u16]) -> PartialVMResult<()> {
                self.0 += (size_of_val(vals) as u64).into();
                Ok(())
            }

            fn visit_vec_u32(&mut self, _depth: u64, vals: &[u32]) -> PartialVMResult<()> {
                self.0 += (size_of_val(vals) as u64).into();
                Ok(())
            }

            fn visit_vec_u64(&mut self, _depth: u64, vals: &[u64]) -> PartialVMResult<()> {
                self.0 += (size_of_val(vals) as u64).into();
                Ok(())
            }

            fn visit_vec_u128(&mut self, _depth: u64, vals: &[u128]) -> PartialVMResult<()> {
                self.0 += (size_of_val(vals) as u64).into();
                Ok(())
            }

            fn visit_vec_u256(
                &mut self,
                _depth: u64,
                vals: &[move_core_types::int256::U256],
            ) -> PartialVMResult<()> {
                self.0 += (size_of_val(vals) as u64).into();
                Ok(())
            }

            fn visit_vec_bool(&mut self, _depth: u64, vals: &[bool]) -> PartialVMResult<()> {
                self.0 += (size_of_val(vals) as u64).into();
                Ok(())
            }

            fn visit_vec_address(
                &mut self,
                _depth: u64,
                vals: &[AccountAddress],
            ) -> PartialVMResult<()> {
                self.0 += (size_of_val(vals) as u64).into();
                Ok(())
            }

            fn visit_ref(&mut self, _depth: u64, _is_global: bool) -> PartialVMResult<bool> {
                self.0 += LEGACY_REFERENCE_SIZE;
                Ok(false)
            }
        }

        let mut acc = Acc(0.into());
        self.visit(&mut acc)
            .expect("Legacy function: should not fail");

        acc.0
    }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/misc.rs (L183-467)
```rust
struct AbstractValueSizeVisitor<'a> {
    feature_version: u64,
    params: &'a AbstractValueSizeGasParameters,
    size: AbstractValueSize,
    max_value_nest_depth: Option<u64>,
}

impl<'a> AbstractValueSizeVisitor<'a> {
    check_depth_impl!();

    fn new(params: &'a AbstractValueSizeGasParameters, feature_version: u64) -> Self {
        Self {
            feature_version,
            params,
            size: 0.into(),
            max_value_nest_depth: Some(DEFAULT_MAX_VM_VALUE_NESTED_DEPTH),
        }
    }

    fn finish(self) -> AbstractValueSize {
        self.size
    }
}

impl ValueVisitor for AbstractValueSizeVisitor<'_> {
    #[inline]
    fn visit_delayed(&mut self, depth: u64, _id: DelayedFieldID) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.u64;
        Ok(())
    }

    #[inline]
    fn visit_u8(&mut self, depth: u64, _val: u8) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.u8;
        Ok(())
    }

    #[inline]
    fn visit_u16(&mut self, depth: u64, _val: u16) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.u16;
        Ok(())
    }

    #[inline]
    fn visit_u32(&mut self, depth: u64, _val: u32) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.u32;
        Ok(())
    }

    #[inline]
    fn visit_u64(&mut self, depth: u64, _val: u64) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.u64;
        Ok(())
    }

    #[inline]
    fn visit_u128(&mut self, depth: u64, _val: u128) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.u128;
        Ok(())
    }

    #[inline]
    fn visit_u256(&mut self, depth: u64, _val: &U256) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.u256;
        Ok(())
    }

    #[inline]
    fn visit_i8(&mut self, depth: u64, _val: i8) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.i8;
        Ok(())
    }

    #[inline]
    fn visit_i16(&mut self, depth: u64, _val: i16) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.i16;
        Ok(())
    }

    #[inline]
    fn visit_i32(&mut self, depth: u64, _val: i32) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.i32;
        Ok(())
    }

    #[inline]
    fn visit_i64(&mut self, depth: u64, _val: i64) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.i64;
        Ok(())
    }

    #[inline]
    fn visit_i128(&mut self, depth: u64, _val: i128) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.i128;
        Ok(())
    }

    #[inline]
    fn visit_i256(&mut self, depth: u64, _val: &I256) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.i256;
        Ok(())
    }

    #[inline]
    fn visit_bool(&mut self, depth: u64, _val: bool) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.bool;
        Ok(())
    }

    #[inline]
    fn visit_address(&mut self, depth: u64, _val: &AccountAddress) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size += self.params.address;
        Ok(())
    }

    #[inline]
    fn visit_struct(&mut self, depth: u64, _len: usize) -> PartialVMResult<bool> {
        self.check_depth(depth)?;
        self.size += self.params.struct_;
        Ok(true)
    }

    #[inline]
    fn visit_closure(&mut self, depth: u64, _len: usize) -> PartialVMResult<bool> {
        self.check_depth(depth)?;
        self.size += self.params.closure;
        Ok(true)
    }

    #[inline]
    fn visit_vec(&mut self, depth: u64, _len: usize) -> PartialVMResult<bool> {
        self.check_depth(depth)?;
        self.size += self.params.vector;
        Ok(true)
    }

    #[inline]
    fn visit_vec_u8(&mut self, depth: u64, vals: &[u8]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        let mut size = self.params.per_u8_packed * NumArgs::new(vals.len() as u64);
        if self.feature_version >= 3 {
            size += self.params.vector;
        }
        self.size += size;
        Ok(())
    }

    #[inline]
    fn visit_vec_u16(&mut self, depth: u64, vals: &[u16]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size +=
            self.params.vector + self.params.per_u16_packed * NumArgs::new(vals.len() as u64);
        Ok(())
    }

    #[inline]
    fn visit_vec_u32(&mut self, depth: u64, vals: &[u32]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size +=
            self.params.vector + self.params.per_u32_packed * NumArgs::new(vals.len() as u64);
        Ok(())
    }

    #[inline]
    fn visit_vec_u64(&mut self, depth: u64, vals: &[u64]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        let mut size = self.params.per_u64_packed * NumArgs::new(vals.len() as u64);
        if self.feature_version >= 3 {
            size += self.params.vector;
        }
        self.size += size;
        Ok(())
    }

    #[inline]
    fn visit_vec_u128(&mut self, depth: u64, vals: &[u128]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        let mut size = self.params.per_u128_packed * NumArgs::new(vals.len() as u64);
        if self.feature_version >= 3 {
            size += self.params.vector;
        }
        self.size += size;
        Ok(())
    }

    #[inline]
    fn visit_vec_u256(&mut self, depth: u64, vals: &[U256]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size +=
            self.params.vector + self.params.per_u256_packed * NumArgs::new(vals.len() as u64);
        Ok(())
    }

    #[inline]
    fn visit_vec_i8(&mut self, depth: u64, vals: &[i8]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size +=
            self.params.vector + self.params.per_i8_packed * NumArgs::new(vals.len() as u64);
        Ok(())
    }

    #[inline]
    fn visit_vec_i16(&mut self, depth: u64, vals: &[i16]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size +=
            self.params.vector + self.params.per_i16_packed * NumArgs::new(vals.len() as u64);
        Ok(())
    }

    #[inline]
    fn visit_vec_i32(&mut self, depth: u64, vals: &[i32]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size +=
            self.params.vector + self.params.per_i32_packed * NumArgs::new(vals.len() as u64);
        Ok(())
    }

    #[inline]
    fn visit_vec_i64(&mut self, depth: u64, vals: &[i64]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size +=
            self.params.vector + self.params.per_i64_packed * NumArgs::new(vals.len() as u64);
        Ok(())
    }

    #[inline]
    fn visit_vec_i128(&mut self, depth: u64, vals: &[i128]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size +=
            self.params.vector + self.params.per_i128_packed * NumArgs::new(vals.len() as u64);
        Ok(())
    }

    #[inline]
    fn visit_vec_i256(&mut self, depth: u64, vals: &[I256]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        self.size +=
            self.params.vector + self.params.per_i256_packed * NumArgs::new(vals.len() as u64);
        Ok(())
    }

    #[inline]
    fn visit_vec_bool(&mut self, depth: u64, vals: &[bool]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        let mut size = self.params.per_bool_packed * NumArgs::new(vals.len() as u64);
        if self.feature_version >= 3 {
            size += self.params.vector;
        }
        self.size += size;
        Ok(())
    }

    #[inline]
    fn visit_vec_address(&mut self, depth: u64, vals: &[AccountAddress]) -> PartialVMResult<()> {
        self.check_depth(depth)?;
        let mut size = self.params.per_address_packed * NumArgs::new(vals.len() as u64);
        if self.feature_version >= 3 {
            size += self.params.vector;
        }
        self.size += size;
        Ok(())
    }

    #[inline]
    fn visit_ref(&mut self, depth: u64, _is_global: bool) -> PartialVMResult<bool> {
        self.check_depth(depth)?;
        self.size += self.params.reference;
        Ok(false)
    }
}
```

**File:** third_party/move/move-stdlib/src/natives/event.rs (L28-41)
```rust
fn native_write_to_event_store(
    gas_params: &WriteToEventStoreGasParameters,
    _context: &mut NativeContext,
    ty_args: &[Type],
    mut arguments: VecDeque<Value>,
) -> PartialVMResult<NativeResult> {
    debug_assert!(ty_args.len() == 1);
    debug_assert!(arguments.len() == 3);

    let msg = arguments.pop_back().unwrap();
    let cost = gas_params.unit_cost * std::cmp::max(msg.legacy_abstract_memory_size(), 1.into());

    Ok(NativeResult::ok(cost, smallvec![]))
}
```

**File:** aptos-move/aptos-gas-meter/src/meter.rs (L303-311)
```rust
    fn charge_copy_loc(&mut self, val: impl ValueView) -> PartialVMResult<()> {
        let (stack_size, heap_size) = self
            .vm_gas_params()
            .misc
            .abs_val
            .abstract_value_size_stack_and_heap(val, self.feature_version())?;

        self.charge_copy_loc_cached(stack_size, heap_size)
    }
```

**File:** aptos-move/aptos-gas-meter/src/meter.rs (L398-408)
```rust
    fn charge_eq(&mut self, lhs: impl ValueView, rhs: impl ValueView) -> PartialVMResult<()> {
        let abs_val_params = &self.vm_gas_params().misc.abs_val;

        let cost = EQ_BASE
            + EQ_PER_ABS_VAL_UNIT
                * (abs_val_params.abstract_value_size_dereferenced(lhs, self.feature_version())?
                    + abs_val_params
                        .abstract_value_size_dereferenced(rhs, self.feature_version())?);

        self.algebra.charge_execution(cost)
    }
```
