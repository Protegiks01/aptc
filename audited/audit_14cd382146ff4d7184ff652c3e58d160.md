# Audit Report

## Title
Missing fsync() in OnDiskStorage Violates Durability Contract for Consensus Safety Data

## Summary
The `OnDiskStorage` implementation fails to call `fsync()` or `sync_all()` before returning from write operations, violating the explicit durability contract that "any set function is expected to sync to the remote system before returning." [1](#0-0)  This allows consensus-critical SafetyData (including `last_voted_round`) to remain in OS page cache, creating a window where system crashes can cause validators to lose safety state and potentially double-vote.

## Finding Description

The `KVStorage` trait provides no enforcement mechanism to ensure implementations sync data before returning from `set()` operations. [2](#0-1) 

The `OnDiskStorage::write()` implementation writes data to a temporary file and renames it, but never calls `fsync()`, `sync_all()`, or `sync_data()` to ensure durability: [3](#0-2) 

Despite documentation warnings, `OnDiskStorage` is configured as the backend for SafetyRules in production validator configurations: [4](#0-3) 

The SafetyRules `PersistentSafetyStorage` persists critical consensus state including `last_voted_round`, which is the primary mechanism preventing double-voting: [5](#0-4) 

When a validator votes, the first voting rule enforces that new votes must be for rounds strictly greater than `last_voted_round`: [6](#0-5) 

**Attack Scenario:**
1. Validator votes on round N
2. `set_safety_data()` is called, updating `last_voted_round = N` [7](#0-6) 
3. `OnDiskStorage::write()` returns successfully, but data remains in OS page cache
4. System crashes (power loss, kernel panic, hardware failure) before OS flushes to disk
5. Validator restarts and reads stale `last_voted_round < N` from disk
6. Validator can now vote again on round N or any round â‰¤ N, violating consensus safety

## Impact Explanation

**Critical Severity** - This breaks the fundamental consensus safety invariant: "AptosBFT must prevent double-spending and chain splits under < 1/3 Byzantine validators."

If multiple validators experience crashes during this vulnerability window simultaneously (e.g., datacenter power failure, widespread infrastructure issue), they could all revert to earlier safety states and double-vote, potentially causing:
- Chain splits requiring manual intervention
- Consensus stalls requiring epoch transitions
- Violation of BFT safety guarantees

The `OnDiskStorage` documentation explicitly warns "This should not be used in production" [8](#0-7)  yet it appears in production validator configurations.

## Likelihood Explanation

**Moderate-to-Low Likelihood:**

While system crashes are relatively rare in production environments with:
- Redundant power supplies
- ECC memory
- Enterprise-grade SSDs
- Kernel stability

The vulnerability window exists on EVERY write operation, and natural causes that trigger it include:
- Power outages affecting multiple datacenters
- Kernel panics from driver bugs
- Hardware failures (disk controller, memory corruption)
- Forced reboots during security updates

The impact severity is amplified when multiple validators crash simultaneously (correlated failures), which is exactly when consensus safety is most critical.

## Recommendation

**Fix for `OnDiskStorage::write()`:**

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    
    // CRITICAL: Ensure data is durably written before proceeding
    file.sync_all()?;
    
    fs::rename(&self.temp_path, &self.file_path)?;
    
    // Sync the directory to ensure rename is durable
    let dir = File::open(self.file_path.parent().unwrap())?;
    dir.sync_all()?;
    
    Ok(())
}
```

**Alternative:** Enforce the contract at the trait level by documenting sync requirements and adding validation. Better yet, strongly encourage production validators to use `VaultStorage` instead, which provides proper durability guarantees through external service persistence.

**Configuration Sanitization:** Add validation in `SafetyRulesConfig::sanitize()` to prevent mainnet validators from using `OnDiskStorage`: [9](#0-8) 

## Proof of Concept

```rust
// Test demonstrating the durability gap in OnDiskStorage
// This would need to be run with fault injection to simulate crashes

use aptos_secure_storage::{Storage, OnDiskStorage, KVStorage};
use aptos_consensus_types::safety_data::SafetyData;
use tempfile::TempDir;

#[test]
fn test_ondisk_storage_durability_gap() {
    let temp_dir = TempDir::new().unwrap();
    let storage_path = temp_dir.path().join("safety_data.json");
    
    let mut storage = Storage::from(OnDiskStorage::new(storage_path.clone()));
    
    // Write critical safety data
    let safety_data = SafetyData::new(1, 100, 90, 80, None, 0);
    storage.set("SAFETY_DATA", safety_data.clone()).unwrap();
    
    // At this point, data may still be in OS page cache
    // If process is killed here (SIGKILL, power loss), data is lost
    
    // Simulate reading after crash-and-restart
    let storage2 = Storage::from(OnDiskStorage::new(storage_path));
    match storage2.get::<SafetyData>("SAFETY_DATA") {
        Ok(retrieved) => {
            // With proper fsync, this should always succeed
            assert_eq!(retrieved.value.last_voted_round, 100);
        },
        Err(_) => {
            // Without fsync, this can fail after simulated crash
            // Validator would restart with stale or no safety data
            panic!("Data lost due to missing fsync!");
        }
    }
}
```

## Notes

This vulnerability highlights a critical gap between documented contracts and implementation. The `PersistentSafetyStorage` explicitly states the durability requirement, but the trait provides no mechanism to enforce it, and the implementation violates it. Production validators using `OnDiskStorage` are at risk of consensus safety violations during correlated failure scenarios, though the recommended `VaultStorage` backend likely provides proper durability guarantees through its remote persistence model.

### Citations

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L18-18)
```rust
/// Any set function is expected to sync to the remote system before returning.
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L150-169)
```rust
    pub fn set_safety_data(&mut self, data: SafetyData) -> Result<(), Error> {
        let _timer = counters::start_timer("set", SAFETY_DATA);
        counters::set_state(counters::EPOCH, data.epoch as i64);
        counters::set_state(counters::LAST_VOTED_ROUND, data.last_voted_round as i64);
        counters::set_state(
            counters::HIGHEST_TIMEOUT_ROUND,
            data.highest_timeout_round as i64,
        );
        counters::set_state(counters::PREFERRED_ROUND, data.preferred_round as i64);

        match self.internal_store.set(SAFETY_DATA, data.clone()) {
            Ok(_) => {
                self.cached_safety_data = Some(data);
                Ok(())
            },
            Err(error) => {
                self.cached_safety_data = None;
                Err(Error::SecureStorageUnexpectedError(error.to_string()))
            },
        }
```

**File:** secure/storage/src/kv_storage.rs (L21-23)
```rust
    /// Sets a value in storage and fails if the backend is unavailable or the process has
    /// invalid permissions.
    fn set<T: Serialize>(&mut self, key: &str, value: T) -> Result<(), Error>;
```

**File:** secure/storage/src/on_disk.rs (L16-22)
```rust
/// OnDiskStorage represents a key value store that is persisted to the local filesystem and is
/// intended for single threads (or must be wrapped by a Arc<RwLock<>>). This provides no permission
/// checks and simply offers a proof of concept to unblock building of applications without more
/// complex data stores. Internally, it reads and writes all data to a file, which means that it
/// must make copies of all key material which violates the code base. It violates it because
/// the anticipation is that data stores would securely handle key material. This should not be used
/// in production.
```

**File:** secure/storage/src/on_disk.rs (L64-69)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
```

**File:** docker/compose/aptos-node/validator.yaml (L11-13)
```yaml
    backend:
      type: "on_disk_storage"
      path: secure-data.json
```

**File:** consensus/consensus-types/src/safety_data.rs (L11-12)
```rust
    pub epoch: u64,
    pub last_voted_round: u64,
```

**File:** consensus/safety-rules/src/safety_rules.rs (L213-225)
```rust
    pub(crate) fn verify_and_update_last_vote_round(
        &self,
        round: Round,
        safety_data: &mut SafetyData,
    ) -> Result<(), Error> {
        if round <= safety_data.last_voted_round {
            return Err(Error::IncorrectLastVotedRound(
                round,
                safety_data.last_voted_round,
            ));
        }

        safety_data.last_voted_round = round;
```

**File:** config/src/config/safety_rules_config.rs (L86-96)
```rust
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```
