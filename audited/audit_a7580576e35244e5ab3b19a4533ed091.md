# Audit Report

## Title
Redis Cache Poisoning Enables Propagation of Fraudulent Transaction Data Across Indexer Infrastructure

## Summary
The indexer-grpc cache worker stores transaction data in Redis without cryptographic integrity verification. If an attacker compromises the `redis_main_instance_address`, they can inject or modify cached transaction data, which propagates to all dependent services (data service, file store, downstream processors) without validation, causing widespread state inconsistencies across the indexer infrastructure.

## Finding Description

The indexer-grpc architecture relies on Redis as a central caching layer between the fullnode and downstream consumers. The cache worker receives transactions from the fullnode gRPC stream and stores them in Redis: [1](#0-0) 

The critical vulnerability is that transactions are stored in Redis with **zero cryptographic integrity verification**. The cache worker:
1. Receives transactions from fullnode
2. Encodes them to protobuf (with optional compression)
3. Stores in Redis using simple SET commands with TTL [2](#0-1) 

When data service retrieves cached data, it performs **no validation**: [3](#0-2) 

The deserialization process simply decodes protobuf without verifying:
- Transaction hashes
- Cryptographic signatures
- Merkle proofs
- Cross-checks with authoritative sources [4](#0-3) 

**Attack Scenario:**

1. Attacker compromises Redis instance (via credential theft, network access, or infrastructure vulnerability)
2. Attacker injects malicious transaction data:
   - Modifies existing cached transactions (e.g., change transfer amounts, recipients)
   - Inserts fake transactions with fabricated events
   - Spoofs chain_id and version metadata
3. Data service retrieves poisoned data from Redis
4. Poisoned transactions propagate to:
   - In-memory cache layer (no additional validation)
   - File store (reads from Redis and uploads poisoned data)
   - Downstream indexer processors (index fake data into PostgreSQL)
5. Applications consuming indexed data (wallets, explorers, analytics) display fraudulent information [5](#0-4) 

The only "validation" is chain_id checking, but this can be trivially spoofed in a compromised Redis: [6](#0-5) 

## Impact Explanation

**Severity: Medium** - State inconsistencies requiring intervention

This vulnerability breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." The indexed state becomes inconsistent with the actual blockchain state, and there's no cryptographic verification mechanism.

**Scope of Impact:**
- All services consuming from indexer-grpc data service receive poisoned data
- Indexer processors write fraudulent data to their databases
- User-facing applications (wallets, explorers) display incorrect information:
  - False account balances
  - Non-existent transactions in history
  - Incorrect NFT/token ownership
  - Fabricated smart contract events
- File store becomes permanently poisoned (no recovery mechanism)

**Limitations:**
- Does NOT affect on-chain state or consensus (indexer is off-chain)
- Does NOT compromise validator nodes
- Does NOT enable theft of actual funds from blockchain
- Requires infrastructure-level compromise (Redis access)

This qualifies as Medium severity under the bug bounty criteria: "State inconsistencies requiring intervention" - the indexer infrastructure would need manual remediation and re-indexing from scratch.

## Likelihood Explanation

**Likelihood: Medium-to-High**

**Attack Requirements:**
- Compromise of Redis instance (credential theft, network intrusion, or misconfiguration)
- Knowledge of protobuf transaction format
- Understanding of indexer architecture

**Factors Increasing Likelihood:**
- Redis is a common attack target in infrastructure
- No defense-in-depth: single point of failure
- No anomaly detection or integrity monitoring
- Poisoned data propagates silently without alerts

**Factors Decreasing Likelihood:**
- Requires infrastructure-level access (not pure protocol exploit)
- Organizations typically secure Redis with authentication, network isolation
- Attack is detectable through comparison with fullnode data

## Recommendation

Implement **cryptographic integrity verification** at multiple layers:

### 1. Add Transaction Hash Verification

When retrieving from cache, verify transaction hashes match TransactionInfo:

```rust
// In cache_operator.rs - get_transactions method
pub async fn get_transactions_verified(
    &mut self,
    start_version: u64,
    transaction_count: u64,
) -> anyhow::Result<Vec<Transaction>> {
    let transactions = self.get_transactions(start_version, transaction_count).await?;
    
    // Verify each transaction's hash matches its version
    for txn in &transactions {
        let computed_hash = CryptoHash::hash(&txn);
        let expected_hash = /* fetch from authoritative source */;
        ensure!(computed_hash == expected_hash, 
                "Transaction hash mismatch at version {}", txn.version);
    }
    
    Ok(transactions)
}
```

### 2. Add Accumulator Proof Verification

Integrate Merkle accumulator proofs to verify transactions exist in the ledger: [7](#0-6) 

### 3. Implement Periodic Cross-Validation

Periodically sample cached data and verify against fullnode:

```rust
async fn verify_cache_integrity(
    cache_operator: &mut CacheOperator,
    fullnode_client: &mut RpcClient,
    sample_rate: f64,
) -> Result<()> {
    let cached_versions = sample_versions(sample_rate);
    for version in cached_versions {
        let cached_txn = cache_operator.get_transaction(version).await?;
        let authoritative_txn = fullnode_client.get_transaction(version).await?;
        ensure!(cached_txn == authoritative_txn, 
                "Cache poisoning detected at version {}", version);
    }
    Ok(())
}
```

### 4. Add Redis Authentication and Encryption

- Enable Redis AUTH with strong passwords
- Use TLS for Redis connections
- Implement Redis ACLs to restrict write access
- Enable Redis persistence and audit logging

### 5. Implement Signed Cache Entries

Have cache worker cryptographically sign each cache entry, verify signature on retrieval.

## Proof of Concept

```rust
// Proof of Concept: Redis Cache Poisoning Attack Simulation
// This demonstrates how compromised Redis enables data poisoning

use redis::{Commands, AsyncCommands};
use aptos_protos::transaction::v1::Transaction;
use prost::Message;

#[tokio::test]
async fn test_cache_poisoning_attack() -> anyhow::Result<()> {
    // Setup: Connect to Redis (simulating attacker with Redis access)
    let redis_client = redis::Client::open("redis://127.0.0.1:6379")?;
    let mut conn = redis_client.get_connection()?;
    
    // Step 1: Attacker creates fake transaction
    let poisoned_transaction = Transaction {
        version: 12345,
        // Malicious payload: fake transfer event
        info: Some(/* fabricated transaction info */),
        ..Default::default()
    };
    
    // Step 2: Encode fake transaction (same format as cache worker)
    let mut bytes = Vec::new();
    poisoned_transaction.encode(&mut bytes)?;
    let cache_key = format!("l4:{}", 12345); // Lz4 compressed format key
    
    // Step 3: Inject poisoned data into Redis (attacker action)
    conn.set::<_, _, ()>(&cache_key, bytes)?;
    
    // Step 4: Data service retrieves poisoned data (victim)
    let cached_bytes: Vec<u8> = conn.get(&cache_key)?;
    let retrieved_txn = Transaction::decode(&cached_bytes[..])?;
    
    // Verify poisoning succeeded
    assert_eq!(retrieved_txn.version, 12345);
    
    // Step 5: No validation prevents this - poisoned data propagates
    // to processors, file store, and all downstream consumers
    
    println!("Cache poisoning successful - fake transaction propagated");
    Ok(())
}

// Demonstration of missing validation in data service
#[tokio::test]  
async fn test_no_integrity_verification() -> anyhow::Result<()> {
    let mut cache_operator = /* setup cache operator */;
    
    // Retrieve transaction from cache
    let transactions = cache_operator.get_transactions(12345, 1).await?;
    
    // CRITICAL: No verification happens here
    // - No hash verification
    // - No signature verification  
    // - No Merkle proof verification
    // - No cross-check with fullnode
    
    // Poisoned data is treated as legitimate
    assert_eq!(transactions.len(), 1);
    Ok(())
}
```

**Running the PoC:**
1. Setup indexer-grpc infrastructure with Redis
2. Run cache worker to populate initial data
3. Execute attack script to inject poisoned transaction
4. Observe poisoned data served by data service
5. Verify no integrity check catches the poisoning

## Notes

**Scope Clarification:** While this is a legitimate infrastructure security concern, it's important to note that the indexer-grpc system is an **off-chain indexing service**. Compromising it does not affect:
- On-chain consensus or validator operations
- Actual blockchain state or funds
- Ability to query fullnodes directly

However, given that many production applications rely on indexed data, and the bug bounty program includes "State inconsistencies requiring intervention" as Medium severity, this vulnerability qualifies as it enables attackers to cause widespread data integrity issues requiring manual remediation across the indexer infrastructure.

The fix requires implementing defense-in-depth: cryptographic verification at cache retrieval, periodic validation against authoritative sources, and securing the Redis infrastructure itself.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L243-274)
```rust
                async move {
                    // Push to cache.
                    match cache_operator_clone
                        .update_cache_transactions(data.transactions)
                        .await
                    {
                        Ok(_) => {
                            log_grpc_step(
                                SERVICE_TYPE,
                                IndexerGrpcStep::CacheWorkerTxnsProcessed,
                                Some(first_transaction_version as i64),
                                Some(last_transaction_version as i64),
                                first_transaction_pb_timestamp.as_ref(),
                                last_transaction_pb_timestamp.as_ref(),
                                Some(cache_update_start_time.elapsed().as_secs_f64()),
                                Some(size_in_bytes),
                                Some(
                                    (last_transaction_version + 1 - first_transaction_version)
                                        as i64,
                                ),
                                None,
                            );
                            Ok(())
                        },
                        Err(e) => {
                            ERROR_COUNT
                                .with_label_values(&["failed_to_update_cache_version"])
                                .inc();
                            bail!("Update cache with version failed: {}", e);
                        },
                    }
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L286-325)
```rust
async fn verify_fullnode_init_signal(
    cache_operator: &mut CacheOperator<redis::aio::ConnectionManager>,
    init_signal: TransactionsFromNodeResponse,
    file_store_metadata: FileStoreMetadata,
) -> Result<(ChainID, StartingVersion)> {
    let (fullnode_chain_id, starting_version) = match init_signal
        .response
        .expect("[Indexer Cache] Response type does not exist.")
    {
        Response::Status(status_frame) => {
            match StatusType::try_from(status_frame.r#type)
                .expect("[Indexer Cache] Invalid status type.")
            {
                StatusType::Init => (init_signal.chain_id, status_frame.start_version),
                _ => {
                    bail!("[Indexer Cache] Streaming error: first frame is not INIT signal.");
                },
            }
        },
        _ => {
            bail!("[Indexer Cache] Streaming error: first frame is not siganl frame.");
        },
    };

    // Guaranteed that chain id is here at this point because we already ensure that fileworker did the set up
    let chain_id = cache_operator.get_chain_id().await?.unwrap();
    if chain_id != fullnode_chain_id as u64 {
        bail!("[Indexer Cache] Chain ID mismatch between fullnode init signal and cache.");
    }

    // It's required to start the worker with the same version as file store.
    if file_store_metadata.version != starting_version {
        bail!("[Indexer Cache] Starting version mismatch between filestore metadata and fullnode init signal.");
    }
    if file_store_metadata.chain_id != fullnode_chain_id as u64 {
        bail!("[Indexer Cache] Chain id mismatch between filestore metadata and fullnode.");
    }

    Ok((fullnode_chain_id, starting_version))
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L252-313)
```rust
    pub async fn update_cache_transactions(
        &mut self,
        transactions: Vec<Transaction>,
    ) -> anyhow::Result<()> {
        let start_version = transactions.first().unwrap().version;
        let end_version = transactions.last().unwrap().version;
        let num_transactions = transactions.len();
        let start_txn_timestamp = transactions.first().unwrap().timestamp;
        let end_txn_timestamp = transactions.last().unwrap().timestamp;
        let mut size_in_bytes = 0;
        let mut redis_pipeline = redis::pipe();
        let start_time = std::time::Instant::now();
        for transaction in transactions {
            let version = transaction.version;
            let cache_key = CacheEntry::build_key(version, self.storage_format).to_string();
            let timestamp_in_seconds = transaction.timestamp.map_or(0, |t| t.seconds as u64);
            let cache_entry: CacheEntry =
                CacheEntry::from_transaction(transaction, self.storage_format);
            let bytes = cache_entry.into_inner();
            size_in_bytes += bytes.len();
            redis_pipeline
                .cmd("SET")
                .arg(cache_key)
                .arg(bytes)
                .arg("EX")
                .arg(get_ttl_in_seconds(timestamp_in_seconds))
                .ignore();
            // Actively evict the expired cache. This is to avoid using Redis
            // eviction policy, which is probabilistic-based and may evict the
            // cache that is still needed.
            if version >= CACHE_SIZE_EVICTION_LOWER_BOUND {
                let key = CacheEntry::build_key(
                    version - CACHE_SIZE_EVICTION_LOWER_BOUND,
                    self.storage_format,
                )
                .to_string();
                redis_pipeline.cmd("DEL").arg(key).ignore();
            }
        }
        // Note: this method is and should be only used by `cache_worker`.
        let service_type = "cache_worker";
        log_grpc_step(
            service_type,
            IndexerGrpcStep::CacheWorkerTxnEncoded,
            Some(start_version as i64),
            Some(end_version as i64),
            start_txn_timestamp.as_ref(),
            end_txn_timestamp.as_ref(),
            Some(start_time.elapsed().as_secs_f64()),
            Some(size_in_bytes),
            Some(num_transactions as i64),
            None,
        );

        let redis_result: RedisResult<()> =
            redis_pipeline.query_async::<_, _>(&mut self.conn).await;

        match redis_result {
            Ok(_) => Ok(()),
            Err(err) => Err(err.into()),
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L367-407)
```rust
    pub async fn get_transactions_with_durations(
        &mut self,
        start_version: u64,
        transaction_count: u64,
    ) -> anyhow::Result<(Vec<Transaction>, f64, f64)> {
        let start_time = std::time::Instant::now();
        let versions = (start_version..start_version + transaction_count)
            .map(|e| CacheEntry::build_key(e, self.storage_format))
            .collect::<Vec<String>>();
        let encoded_transactions: Vec<Vec<u8>> = self
            .conn
            .mget(versions)
            .await
            .context("Failed to mget from Redis")?;
        let io_duration = start_time.elapsed().as_secs_f64();
        let start_time = std::time::Instant::now();
        let mut transactions = vec![];
        for encoded_transaction in encoded_transactions {
            let cache_entry: CacheEntry = CacheEntry::new(encoded_transaction, self.storage_format);
            let transaction = cache_entry.into_transaction();
            transactions.push(transaction);
        }
        ensure!(
            transactions.len() == transaction_count as usize,
            "Failed to get all transactions from cache."
        );
        let decoding_duration = start_time.elapsed().as_secs_f64();
        Ok((transactions, io_duration, decoding_duration))
    }

    /// Fail if not all transactions requested are returned
    pub async fn get_transactions(
        &mut self,
        start_version: u64,
        transaction_count: u64,
    ) -> anyhow::Result<Vec<Transaction>> {
        let (transactions, _, _) = self
            .get_transactions_with_durations(start_version, transaction_count)
            .await?;
        Ok(transactions)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L142-157)
```rust
    pub fn into_transaction(self) -> Transaction {
        match self {
            CacheEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                Transaction::decode(decompressed.as_slice()).expect("proto deserialization failed.")
            },
            CacheEntry::Base64UncompressedProto(bytes) => {
                let bytes: Vec<u8> = base64::decode(bytes).expect("base64 decoding failed.");
                Transaction::decode(bytes.as_slice()).expect("proto deserialization failed.")
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L713-777)
```rust
async fn data_fetch(
    starting_version: u64,
    cache_operator: &mut CacheOperator<redis::aio::ConnectionManager>,
    file_store_operator: Arc<Box<dyn FileStoreOperator>>,
    request_metadata: Arc<IndexerGrpcRequestMetadata>,
    storage_format: StorageFormat,
) -> anyhow::Result<TransactionsDataStatus> {
    let current_batch_start_time = std::time::Instant::now();
    let batch_get_result = cache_operator
        .batch_get_encoded_proto_data(starting_version)
        .await;

    match batch_get_result {
        // Data is not ready yet in the cache.
        Ok(CacheBatchGetStatus::NotReady) => Ok(TransactionsDataStatus::AheadOfCache),
        Ok(CacheBatchGetStatus::Ok(transactions)) => {
            let decoding_start_time = std::time::Instant::now();
            let size_in_bytes = transactions
                .iter()
                .map(|transaction| transaction.len())
                .sum::<usize>();
            let num_of_transactions = transactions.len();
            let duration_in_secs = current_batch_start_time.elapsed().as_secs_f64();

            let transactions =
                deserialize_cached_transactions(transactions, storage_format).await?;
            let start_version_timestamp = transactions.first().unwrap().timestamp.as_ref();
            let end_version_timestamp = transactions.last().unwrap().timestamp.as_ref();

            log_grpc_step(
                SERVICE_TYPE,
                IndexerGrpcStep::DataServiceDataFetchedCache,
                Some(starting_version as i64),
                Some(starting_version as i64 + num_of_transactions as i64 - 1),
                start_version_timestamp,
                end_version_timestamp,
                Some(duration_in_secs),
                Some(size_in_bytes),
                Some(num_of_transactions as i64),
                Some(&request_metadata),
            );
            log_grpc_step(
                SERVICE_TYPE,
                IndexerGrpcStep::DataServiceTxnsDecoded,
                Some(starting_version as i64),
                Some(starting_version as i64 + num_of_transactions as i64 - 1),
                start_version_timestamp,
                end_version_timestamp,
                Some(decoding_start_time.elapsed().as_secs_f64()),
                Some(size_in_bytes),
                Some(num_of_transactions as i64),
                Some(&request_metadata),
            );

            Ok(TransactionsDataStatus::Success(transactions))
        },
        Ok(CacheBatchGetStatus::EvictedFromCache) => {
            let transactions =
                data_fetch_from_filestore(starting_version, file_store_operator, request_metadata)
                    .await?;
            Ok(TransactionsDataStatus::Success(transactions))
        },
        Err(e) => Err(e),
    }
}
```

**File:** types/src/proof/mod.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

pub mod accumulator;
pub mod definition;
pub mod position;
#[cfg(any(test, feature = "fuzzing"))]
pub mod proptest_proof;

#[cfg(test)]
mod unit_tests;

pub use self::definition::{
    AccumulatorConsistencyProof, AccumulatorExtensionProof, AccumulatorProof,
    AccumulatorRangeProof, SparseMerkleProof, SparseMerkleProofExt, SparseMerkleRangeProof,
    TransactionAccumulatorProof, TransactionAccumulatorRangeProof, TransactionAccumulatorSummary,
    TransactionInfoListWithProof, TransactionInfoWithProof,
};
#[cfg(any(test, feature = "fuzzing"))]
pub use self::definition::{TestAccumulatorProof, TestAccumulatorRangeProof};
use crate::{
    ledger_info::LedgerInfo,
    transaction::{TransactionInfo, Version},
};
use anyhow::{ensure, Result};
use aptos_crypto::{
    hash::{
        CryptoHash, CryptoHasher, EventAccumulatorHasher, SparseMerkleInternalHasher,
        TestOnlyHasher, TransactionAccumulatorHasher,
    },
    HashValue,
};
use aptos_crypto_derive::CryptoHasher;
#[cfg(any(test, feature = "fuzzing"))]
use proptest_derive::Arbitrary;
use serde::{Deserialize, Serialize};
use std::marker::PhantomData;

/// Verifies that a given `transaction_info` exists in the ledger using provided proof.
fn verify_transaction_info(
    ledger_info: &LedgerInfo,
    transaction_version: Version,
    transaction_info: &TransactionInfo,
    ledger_info_to_transaction_info_proof: &TransactionAccumulatorProof,
) -> Result<()> {
    ensure!(
        transaction_version <= ledger_info.version(),
        "Transaction version {} is newer than LedgerInfo version {}.",
        transaction_version,
        ledger_info.version(),
```
