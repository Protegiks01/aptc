Audit Report

## Title
Block Structure Violation via Partial Transaction Backups Leads to Inconsistent Block Metadata and Validator Statistics

## Summary
The Aptos backup and restore mechanism allows backup of arbitrary ranges of transactions via the `get_transactions()` interface, with no requirement for alignment to block boundaries. This can lead to partial block backups that are missing required block structure transactions (such as `BlockMetadata`). On restore, the database state becomes inconsistent with respect to block metadata and validator reward calculations, violating critical Aptos state management invariants.

## Finding Description
The `BackupServiceClient::get_transactions()` API in `aptos-core/storage/backup/backup-cli/src/utils/backup_service_client.rs` allows clients to fetch a sequence of transactions by arbitrary start version and count, with no validation that the chunk aligns with block boundaries such as `BlockMetadata` system transactions at block start. This means a backup/restore administrator can take a backup beginning or ending in the middle of a block.

On restore, the default path (using `VerifyExecutionMode::NoVerify`) in the backup-restore control flow saves and restores transaction outputs and metadata as-is. There are no structural checks that ensure all block structure transactions are present. If the backup is missing a `BlockMetadata` transaction at a block boundary, the restore code writes the transactions and their effects directly into storage. The block info table, validator statistics, and global block resource (such as block height and timestamp) may become inconsistent or incomplete. Block reward and validator performance statistics are managed via logic in Move modules that is only triggered by block prologue transactions, and these are skipped if the associated transaction is not present in the backup chunk.

Thus, anyone running the backup/restore pipeline (including a validator operator, disaster recovery personnel, or any actor with backup access) can inadvertently or deliberately break block structure invariants, causing consensus state that does not correspond to actual chain history, and affecting subsequent consensus, state synchronization, and validator economics.

## Impact Explanation
This issue is categorized as **High Severity** per the Aptos Bug Bounty classification:

- **Significant protocol violation**: Restored node will be out of sync with block metadata (block heights, block timestamps) and validator performance records.
- **Validator reward/penalty miscalculation**: Missing prologue transactions omit performance update logic, affecting validator economics.
- **Potentially breaks state-dependent applications and system modules**: Anything reliant on accurate block structure will see inconsistencies.
- **Dangerous in disaster recovery**: If a validator or fullnode restores and begins to serve/produce blocks from such a state, its block structure and historic proofs can diverge from the canonical chain, potentially causing state propagation errors and liveness failures.

## Likelihood Explanation
- **Attack Complexity**: Moderate. Requires access to the backup/restore interface and choice of partial boundaries.
- **Operator Error**: Highly likelyâ€”an untrained operator or script specifying transaction ranges may inadvertently trigger this.
- **Malicious Usage**: Anyone with restore access could intentionally exploit the flaw to break chain invariants "quietly".

## Recommendation
- Backup and restore flows must be hardened to reject or warn when transaction ranges do not align with block boundaries (i.e., a range should always start with a `BlockMetadata` transaction). 
- During backup: automatically adjust requested ranges so they only ever include whole blocks.
- During restore: detect missing block structure transactions on chunk boundaries and error out or require explicit override with full warning and consensus operator opt-in.
  
Sample remediation pseudocode:
```rust
// Before accepting a backup range, check:
assert!(transactions.first().is_block_start());
assert!(transactions.last().is_block_end());
```

## Proof of Concept

**Step-by-step:**
1. Use `BackupServiceClient::get_transactions(start_version, num_transactions)` with a `start_version` in the middle of a block (not a `BlockMetadata` txn).
2. Complete a backup of these transactions.
3. Restore this backup using the standard control flow (no verify mode).
4. Observe that restored block info is missing for this block, block height is wrong, and validator stats/events are missing or miscounted.

**Key locations in codebase:**
- No alignment check: [1](#0-0) 
- Direct save with no structure validation: [2](#0-1) 
- Block structure logic in transaction enum: [3](#0-2) 
- Block prologue and validator update invoked only via BlockMetadata tx: [4](#0-3) 
- Block info only updated if event is present: [5](#0-4) 

---

Notes:
- Cannot be triggered by regular network participants, but is highly likely in any backup/restore scenario and can lead to subtle and deep integrity failures.
- Fixes require enforcing stronger invariants in both backup and restore logic.
- This flaw does NOT enable direct theft of funds or double-spends, but threatens critical blockchain state invariants, making it High Severity.

### Citations

**File:** storage/backup/backup-cli/src/utils/backup_service_client.rs (L147-157)
```rust
    pub async fn get_transactions(
        &self,
        start_version: Version,
        num_transactions: usize,
    ) -> Result<impl AsyncRead + use<>> {
        self.get(
            "transactions",
            &format!("{}/{}", start_version, num_transactions),
        )
        .await
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L424-551)
```rust
    async fn save_before_replay_version(
        &self,
        global_first_version: Version,
        loaded_chunk_stream: impl Stream<Item = Result<LoadedChunk>> + Unpin,
        restore_handler: &RestoreHandler,
    ) -> Result<
        Option<
            impl Stream<
                Item = Result<(
                    Transaction,
                    PersistedAuxiliaryInfo,
                    TransactionInfo,
                    WriteSet,
                    Vec<ContractEvent>,
                )>,
            >,
        >,
    > {
        // get the next expected transaction version of the current aptos db from txn_info CF
        let next_expected_version = self
            .global_opt
            .run_mode
            .get_next_expected_transaction_version()?;
        let start = Instant::now();

        let restore_handler_clone = restore_handler.clone();
        // DB doesn't allow replaying anything before what's in DB already.
        // self.replay_from_version is from cli argument. However, in fact, we either not replay or replay
        // after current DB's version.
        let first_to_replay = max(
            self.replay_from_version
                .map_or(Version::MAX, |(version, _)| version),
            next_expected_version,
        );
        let target_version = self.global_opt.target_version;

        let mut txns_to_execute_stream = loaded_chunk_stream
            .and_then(move |chunk| {
                let restore_handler = restore_handler_clone.clone();
                future::ok(async move {
                    let mut first_version = chunk.manifest.first_version;
                    let mut last_version = chunk.manifest.last_version;
                    let (
                        mut txns,
                        mut persisted_aux_info,
                        mut txn_infos,
                        mut event_vecs,
                        mut write_sets,
                    ) = chunk.unpack();

                    // remove the txns that exceeds the target_version to be restored
                    if target_version < last_version {
                        let num_to_keep = (target_version - first_version + 1) as usize;
                        txns.drain(num_to_keep..);
                        persisted_aux_info.drain(num_to_keep..);
                        txn_infos.drain(num_to_keep..);
                        event_vecs.drain(num_to_keep..);
                        write_sets.drain(num_to_keep..);
                        last_version = target_version;
                    }

                    // remove the txns that are before the global_first_version
                    if global_first_version > first_version {
                        let num_to_remove = (global_first_version - first_version) as usize;

                        txns.drain(..num_to_remove);
                        persisted_aux_info.drain(..num_to_remove);
                        txn_infos.drain(..num_to_remove);
                        event_vecs.drain(..num_to_remove);
                        write_sets.drain(..num_to_remove);
                        first_version = global_first_version;
                    }

                    // identify txns to be saved before the first_to_replay version
                    if first_version < first_to_replay {
                        let num_to_save =
                            (min(first_to_replay, last_version + 1) - first_version) as usize;
                        let txns_to_save: Vec<_> = txns.drain(..num_to_save).collect();
                        let persisted_aux_info_to_save: Vec<_> =
                            persisted_aux_info.drain(..num_to_save).collect();
                        let txn_infos_to_save: Vec<_> = txn_infos.drain(..num_to_save).collect();
                        let event_vecs_to_save: Vec<_> = event_vecs.drain(..num_to_save).collect();
                        let write_sets_to_save = write_sets.drain(..num_to_save).collect();
                        tokio::task::spawn_blocking(move || {
                            restore_handler.save_transactions(
                                first_version,
                                &txns_to_save,
                                &persisted_aux_info_to_save,
                                &txn_infos_to_save,
                                &event_vecs_to_save,
                                write_sets_to_save,
                            )
                        })
                        .await??;
                        let last_saved = first_version + num_to_save as u64 - 1;
                        TRANSACTION_SAVE_VERSION.set(last_saved as i64);
                        info!(
                            version = last_saved,
                            accumulative_tps = ((last_saved - global_first_version + 1) as f64
                                / start.elapsed().as_secs_f64())
                                as u64,
                            "Transactions saved."
                        );
                    }

                    // create iterator of txn and its outputs to be replayed after the snapshot.
                    Ok(stream::iter(
                        izip!(txns, persisted_aux_info, txn_infos, write_sets, event_vecs)
                            .map(Result::<_>::Ok),
                    ))
                })
            })
            .try_buffered_x(self.global_opt.concurrent_downloads, 1)
            .try_flatten()
            .peekable();

        // Finish saving transactions that are not to be replayed.
        let first_txn_to_replay = {
            Pin::new(&mut txns_to_execute_stream)
                .peek()
                .await
                .map(|res| res.as_ref().map_err(|e| anyhow!("Error: {}", e)))
                .transpose()?
                .map(|_| ())
        };

        Ok(first_txn_to_replay.map(|_| txns_to_execute_stream))
    }
```

**File:** types/src/transaction/mod.rs (L3064-3074)
```rust
    pub fn is_block_start(&self) -> bool {
        match self {
            Transaction::BlockMetadata(_) | Transaction::BlockMetadataExt(_) => true,
            Transaction::StateCheckpoint(_)
            | Transaction::BlockEpilogue(_)
            | Transaction::UserTransaction(_)
            | Transaction::GenesisTransaction(_)
            | Transaction::ValidatorTransaction(_) => false,
        }
    }
}
```

**File:** aptos-move/framework/aptos-framework/sources/block.move (L154-218)
```text
    fun block_prologue_common(
        vm: &signer,
        hash: address,
        epoch: u64,
        round: u64,
        proposer: address,
        failed_proposer_indices: vector<u64>,
        previous_block_votes_bitvec: vector<u8>,
        timestamp: u64
    ): u64 acquires BlockResource, CommitHistory {
        // Operational constraint: can only be invoked by the VM.
        system_addresses::assert_vm(vm);

        // Blocks can only be produced by a valid proposer or by the VM itself for Nil blocks (no user txs).
        assert!(
            proposer == @vm_reserved || stake::is_current_epoch_validator(proposer),
            error::permission_denied(EINVALID_PROPOSER),
        );

        let proposer_index = option::none();
        if (proposer != @vm_reserved) {
            proposer_index = option::some(stake::get_validator_index(proposer));
        };

        let block_metadata_ref = borrow_global_mut<BlockResource>(@aptos_framework);
        block_metadata_ref.height = event::counter(&block_metadata_ref.new_block_events);

        let new_block_event = NewBlockEvent {
            hash,
            epoch,
            round,
            height: block_metadata_ref.height,
            previous_block_votes_bitvec,
            proposer,
            failed_proposer_indices,
            time_microseconds: timestamp,
        };
        emit_new_block_event(vm, &mut block_metadata_ref.new_block_events, new_block_event);

        // Performance scores have to be updated before the epoch transition as the transaction that triggers the
        // transition is the last block in the previous epoch.
        stake::update_performance_statistics(proposer_index, failed_proposer_indices);
        state_storage::on_new_block(reconfiguration::current_epoch());

        block_metadata_ref.epoch_interval
    }

    /// Set the metadata for the current block.
    /// The runtime always runs this before executing the transactions in a block.
    fun block_prologue(
        vm: signer,
        hash: address,
        epoch: u64,
        round: u64,
        proposer: address,
        failed_proposer_indices: vector<u64>,
        previous_block_votes_bitvec: vector<u8>,
        timestamp: u64
    ) acquires BlockResource, CommitHistory {
        let epoch_interval = block_prologue_common(&vm, hash, epoch, round, proposer, failed_proposer_indices, previous_block_votes_bitvec, timestamp);
        randomness::on_new_block(&vm, epoch, round, option::none());
        if (timestamp - reconfiguration::last_reconfiguration_time() >= epoch_interval) {
            reconfiguration::reconfigure();
        };
    }
```

**File:** storage/aptosdb/src/backup/restore_utils.rs (L245-259)
```rust
    if ledger_db.enable_storage_sharding() {
        for (idx, txn_events) in events.iter().enumerate() {
            for event in txn_events {
                if let Some(event_key) = event.event_key() {
                    if *event_key == new_block_event_key() {
                        LedgerMetadataDb::put_block_info(
                            first_version + idx as Version,
                            event,
                            &mut ledger_db_batch.ledger_metadata_db_batches,
                        )?;
                    }
                }
            }
        }
    }
```
