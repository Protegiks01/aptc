# Audit Report

## Title
Subscription Stream Lag Detection Bypass via Storage Summary Manipulation

## Summary
A malicious peer can bypass the subscription stream lag detection mechanism by manipulating advertised ledger info versions in storage summaries, causing the lag timer to repeatedly reset and preventing the system from detecting permanently lagging streams.

## Finding Description

The subscription stream lag detection mechanism is designed to identify when a subscription stream falls too far behind and cannot recover. This is implemented through the `SubscriptionStreamLag::is_beyond_recovery()` function which tracks both lag duration and whether the lag is increasing. [1](#0-0) 

The vulnerability exists in how the lag detection interacts with the global data summary's advertised versions. When `check_subscription_stream_lag()` determines the stream has caught up, it completely resets the lag tracking including the start time: [2](#0-1) 

The `highest_advertised_version` is derived from peer storage summaries without signature verification on the contained `LedgerInfoWithSignatures`: [3](#0-2) 

When peers send storage summaries via the poller, these summaries are accepted without cryptographic validation: [4](#0-3) 

The global data summary calculation takes the maximum version from all non-ignored peers: [5](#0-4) [6](#0-5) 

**Attack Scenario:**

1. Malicious peer connects to victim node with a starting score of 50.0
2. Attacker monitors or estimates the victim's current subscription stream response version (e.g., version 1000)
3. Attacker sends `StorageServerSummary` with `synced_ledger_info` version set to 5000 (artificially high)
4. Victim's `highest_advertised_version` becomes 5000 (assuming attacker has highest among peers)
5. Subscription stream at version 1000 is detected as lagging, `SubscriptionStreamLag` created with `start_time = now()`
6. Before `max_subscription_stream_lag_secs` (default 10 seconds) elapses, attacker sends new summary with version 1000 or lower
7. Victim calculates `highest_response_version >= highest_advertised_version`, calls `reset_subscription_stream_lag()`
8. Lag tracking is completely cleared including `start_time`
9. Attacker repeats cycle, preventing lag duration from ever accumulating to threshold

The attack exploits that `reset_subscription_stream_lag()` completely clears the lag state: [7](#0-6) 

Each cycle creates a fresh `SubscriptionStreamLag` with a new `start_time`: [8](#0-7) [9](#0-8) 

## Impact Explanation

This vulnerability allows an attacker to prevent subscription streams from being detected as "beyond recovery" even when they are permanently failing. This has **Medium severity** impact:

1. **State Sync Integrity Violation**: The system cannot properly identify and remediate failing subscription streams, potentially causing nodes to remain in a degraded synchronization state indefinitely

2. **Resource Exhaustion**: Failed streams continue consuming resources (network bandwidth, memory, CPU) instead of being terminated and restarted with alternative strategies

3. **Operational Impact**: Nodes may fall progressively behind the chain without proper alerting or corrective action, requiring manual intervention

The impact aligns with **Medium Severity** criteria: "State inconsistencies requiring intervention" - the bypass prevents automatic detection and remediation of subscription stream failures, requiring manual operator intervention. [10](#0-9) 

## Likelihood Explanation

**Likelihood: Medium to High**

**Requirements for exploitation:**
- Attacker needs to connect as a peer to the target node (no special privileges required)
- Attacker must time storage summary submissions to alternate every ~8-9 seconds (before the 10-second threshold)
- No cryptographic material or validator credentials needed

**Mitigating factors:**
- If multiple honest peers advertise higher versions, the attacker's low-version summary may not reduce the global maximum sufficiently
- The `ignore_low_score_peers` configuration can filter out peers with low scores (below 25.0), but sending storage summaries doesn't directly affect peer scoring
- The attack requires some coordination with the actual stream response timing

**Amplifying factors:**
- Storage summaries from the poller are accepted without any validation
- A single peer's advertised version can influence the global maximum
- The attack doesn't trigger obvious error conditions that would reduce peer score
- The peer scoring system doesn't penalize invalid storage summary content

The attack is realistic and feasible for any motivated adversary with network access to Aptos nodes.

## Recommendation

**Primary Fix: Validate LedgerInfoWithSignatures in Storage Summaries**

Verify the cryptographic signatures on `synced_ledger_info` when receiving storage summaries:

```rust
// In state-sync/aptos-data-client/src/poller.rs, after line 422
let storage_summary = match result {
    Ok(storage_summary) => {
        // Validate the synced_ledger_info if present
        if let Some(synced_ledger_info) = &storage_summary.data_summary.synced_ledger_info {
            // Obtain validator verifier for the appropriate epoch
            if let Ok(epoch_state) = data_summary_poller.storage.get_epoch_ending_ledger_info(
                synced_ledger_info.ledger_info().epoch().saturating_sub(1)
            ) {
                if let Some(verifier) = epoch_state.verifier() {
                    if let Err(e) = synced_ledger_info.verify_signatures(verifier) {
                        warn!("Invalid signatures in synced_ledger_info from peer {:?}: {:?}", peer, e);
                        // Penalize peer for sending invalid data
                        data_summary_poller.data_client.update_peer_error(
                            peer, 
                            ErrorType::Malicious
                        );
                        return;
                    }
                }
            }
        }
        storage_summary
    },
    Err(error) => { /* existing error handling */ }
};
```

**Secondary Fix: Make Lag Accumulation More Resilient**

Modify `is_beyond_recovery()` to track cumulative lag time even across resets:

```rust
// In SubscriptionStreamLag struct
pub struct SubscriptionStreamLag {
    pub start_time: Instant,
    pub cumulative_lag_duration: Duration,  // NEW: Track total lag time
    pub time_service: TimeService,
    pub version_lag: u64,
}

// In is_beyond_recovery()
fn is_beyond_recovery(&mut self, ...) -> bool {
    let current_time = self.time_service.now();
    let current_lag_duration = current_time.duration_since(self.start_time);
    
    // Add to cumulative if lagging
    if current_stream_lag > 0 {
        self.cumulative_lag_duration += current_lag_duration;
    }
    
    // Check against cumulative duration
    let lag_duration_exceeded = self.cumulative_lag_duration >= max_stream_lag_duration;
    // ... rest of logic
}
```

**Tertiary Fix: Implement Consensus on Advertised Data**

Require a quorum of peers to agree on advertised versions rather than accepting the maximum from any single peer.

## Proof of Concept

```rust
// Test demonstrating the vulnerability
// Add to state-sync/data-streaming-service/src/tests/data_stream.rs

#[tokio::test]
async fn test_lag_detection_bypass_via_storage_summary_manipulation() {
    // Setup mock environment
    let data_client_config = AptosDataClientConfig::default();
    let streaming_service_config = DataStreamingServiceConfig::default();
    let time_service = TimeService::mock();
    
    // Create a subscription stream
    let mut data_stream = create_subscription_data_stream(/* ... */);
    
    // Simulate initial state: stream at version 100
    let response_version = 100;
    
    // Phase 1: Attacker advertises high version
    let mut malicious_summary = GlobalDataSummary::empty();
    malicious_summary.advertised_data.synced_ledger_infos.push(
        create_ledger_info_with_sigs(5000, /* malicious high version */)
    );
    
    // Check lag - should detect lagging and create SubscriptionStreamLag
    let response_payload = create_transaction_response(response_version);
    data_stream.check_subscription_stream_lag(&malicious_summary, &response_payload).unwrap();
    assert!(data_stream.subscription_stream_lag.is_some());
    
    let initial_start_time = data_stream.subscription_stream_lag.as_ref().unwrap().start_time;
    
    // Advance time by 8 seconds (before 10 second threshold)
    time_service.advance_secs(8);
    
    // Phase 2: Attacker advertises low version to trigger reset
    malicious_summary.advertised_data.synced_ledger_infos.clear();
    malicious_summary.advertised_data.synced_ledger_infos.push(
        create_ledger_info_with_sigs(100, /* matches response version */)
    );
    
    // Check lag - should reset because response_version >= advertised_version
    data_stream.check_subscription_stream_lag(&malicious_summary, &response_payload).unwrap();
    assert!(data_stream.subscription_stream_lag.is_none()); // Reset!
    
    // Phase 3: Repeat - advertise high version again
    malicious_summary.advertised_data.synced_ledger_infos.clear();
    malicious_summary.advertised_data.synced_ledger_infos.push(
        create_ledger_info_with_sigs(5000, /* high version again */)
    );
    
    data_stream.check_subscription_stream_lag(&malicious_summary, &response_payload).unwrap();
    assert!(data_stream.subscription_stream_lag.is_some());
    
    let new_start_time = data_stream.subscription_stream_lag.as_ref().unwrap().start_time;
    
    // VULNERABILITY: start_time was reset, cumulative lag duration lost
    assert_ne!(initial_start_time, new_start_time);
    assert!(new_start_time > initial_start_time);
    
    // The stream will never be detected as beyond recovery if this pattern continues
    // even though it has been lagging for more than 10 seconds cumulatively
}
```

## Notes

The vulnerability fundamentally stems from the lack of cryptographic validation on peer-provided storage summaries. While the Aptos protocol relies heavily on signed `LedgerInfoWithSignatures` for consensus and state verification, the data client's storage summary polling mechanism accepts these structures without verifying their signatures. This creates an asymmetry where untrusted peer data directly influences critical lag detection logic.

The default `max_subscription_stream_lag_secs` of 10 seconds makes the attack window relatively tight but still feasible. The attacker must coordinate storage summary submissions to alternate roughly every 8-9 seconds to maintain the bypass while avoiding detection.

This vulnerability is specific to subscription streams and does not directly affect consensus safety or validator operations, but it does compromise the reliability of state synchronization for fullnodes and other non-validator participants relying on subscription mechanisms.

### Citations

**File:** state-sync/data-streaming-service/src/data_stream.rs (L192-198)
```rust
    fn reset_subscription_stream_lag(&mut self) {
        // Reset the subscription stream lag metrics
        metrics::set_subscription_stream_lag(0);

        // Reset the stream lag
        self.subscription_stream_lag = None;
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L586-596)
```rust
        // Get the highest advertised version
        let highest_advertised_version = global_data_summary
            .advertised_data
            .highest_synced_ledger_info()
            .map(|ledger_info| ledger_info.ledger_info().version())
            .ok_or_else(|| {
                aptos_data_client::error::Error::UnexpectedErrorEncountered(
                    "The highest synced ledger info is missing from the global data summary!"
                        .into(),
                )
            })?;
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L598-601)
```rust
        // If the stream is not lagging behind, reset the lag and return
        if highest_response_version >= highest_advertised_version {
            self.reset_subscription_stream_lag();
            return Ok(());
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L623-628)
```rust
        } else {
            // The stream was not previously lagging, but it is now!
            let subscription_stream_lag =
                SubscriptionStreamLag::new(current_stream_lag, self.time_service.clone());
            self.set_subscription_stream_lag(subscription_stream_lag);
        }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L956-962)
```rust
    fn new(version_lag: u64, time_service: TimeService) -> Self {
        Self {
            start_time: time_service.now(),
            time_service,
            version_lag,
        }
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L967-992)
```rust
    fn is_beyond_recovery(
        &mut self,
        streaming_service_config: DataStreamingServiceConfig,
        current_stream_lag: u64,
    ) -> bool {
        // Calculate the total duration the stream has been lagging
        let current_time = self.time_service.now();
        let stream_lag_duration = current_time.duration_since(self.start_time);
        let max_stream_lag_duration =
            Duration::from_secs(streaming_service_config.max_subscription_stream_lag_secs);

        // If the lag is further behind and enough time has passed, the stream has failed
        let lag_has_increased = current_stream_lag > self.version_lag;
        let lag_duration_exceeded = stream_lag_duration >= max_stream_lag_duration;
        if lag_has_increased && lag_duration_exceeded {
            return true; // The stream is beyond recovery
        }

        // Otherwise, update the stream lag if we've caught up.
        // This will ensure the lag can only improve.
        if current_stream_lag < self.version_lag {
            self.version_lag = current_stream_lag;
        }

        false // The stream is not yet beyond recovery
    }
```

**File:** state-sync/aptos-data-client/src/poller.rs (L422-439)
```rust
        let storage_summary = match result {
            Ok(storage_summary) => storage_summary,
            Err(error) => {
                warn!(
                    (LogSchema::new(LogEntry::StorageSummaryResponse)
                        .event(LogEvent::PeerPollingError)
                        .message("Error encountered when polling peer!")
                        .error(&error)
                        .peer(&peer))
                );
                return;
            },
        };

        // Update the summary for the peer
        data_summary_poller
            .data_client
            .update_peer_storage_summary(peer, storage_summary);
```

**File:** state-sync/aptos-data-client/src/global_summary.rs (L184-198)
```rust
    pub fn highest_synced_ledger_info(&self) -> Option<LedgerInfoWithSignatures> {
        let highest_synced_position = self
            .synced_ledger_infos
            .iter()
            .map(|ledger_info_with_sigs| ledger_info_with_sigs.ledger_info().version())
            .position_max();

        if let Some(highest_synced_position) = highest_synced_position {
            self.synced_ledger_infos
                .get(highest_synced_position)
                .cloned()
        } else {
            None
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L339-408)
```rust
    pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
        // Gather all storage summaries, but exclude peers that are ignored
        let storage_summaries: Vec<StorageServerSummary> = self
            .peer_to_state
            .iter()
            .filter_map(|peer_state| {
                peer_state
                    .value()
                    .get_storage_summary_if_not_ignored()
                    .cloned()
            })
            .collect();

        // If we have no peers, return an empty global summary
        if storage_summaries.is_empty() {
            return GlobalDataSummary::empty();
        }

        // Calculate the global data summary using the advertised peer data
        let mut advertised_data = AdvertisedData::empty();
        let mut max_epoch_chunk_sizes = vec![];
        let mut max_state_chunk_sizes = vec![];
        let mut max_transaction_chunk_sizes = vec![];
        let mut max_transaction_output_chunk_sizes = vec![];
        for summary in storage_summaries {
            // Collect aggregate data advertisements
            if let Some(epoch_ending_ledger_infos) = summary.data_summary.epoch_ending_ledger_infos
            {
                advertised_data
                    .epoch_ending_ledger_infos
                    .push(epoch_ending_ledger_infos);
            }
            if let Some(states) = summary.data_summary.states {
                advertised_data.states.push(states);
            }
            if let Some(synced_ledger_info) = summary.data_summary.synced_ledger_info.as_ref() {
                advertised_data
                    .synced_ledger_infos
                    .push(synced_ledger_info.clone());
            }
            if let Some(transactions) = summary.data_summary.transactions {
                advertised_data.transactions.push(transactions);
            }
            if let Some(transaction_outputs) = summary.data_summary.transaction_outputs {
                advertised_data
                    .transaction_outputs
                    .push(transaction_outputs);
            }

            // Collect preferred max chunk sizes
            max_epoch_chunk_sizes.push(summary.protocol_metadata.max_epoch_chunk_size);
            max_state_chunk_sizes.push(summary.protocol_metadata.max_state_chunk_size);
            max_transaction_chunk_sizes.push(summary.protocol_metadata.max_transaction_chunk_size);
            max_transaction_output_chunk_sizes
                .push(summary.protocol_metadata.max_transaction_output_chunk_size);
        }

        // Calculate optimal chunk sizes based on the advertised data
        let optimal_chunk_sizes = calculate_optimal_chunk_sizes(
            &self.data_client_config,
            max_epoch_chunk_sizes,
            max_state_chunk_sizes,
            max_transaction_chunk_sizes,
            max_transaction_output_chunk_sizes,
        );
        GlobalDataSummary {
            advertised_data,
            optimal_chunk_sizes,
        }
    }
```

**File:** config/src/config/state_sync_config.rs (L278-278)
```rust
            max_subscription_stream_lag_secs: 10, // 10 seconds
```
