# Audit Report

## Title
Consensus Observer Payload Store DoS via Future Epoch Injection

## Summary
The consensus observer's `BlockPayloadStore::insert_block_payload()` function lacks epoch validation, allowing malicious peers to inject payloads with arbitrarily high epoch numbers (e.g., epoch + 100). These future epoch payloads fill the store to its capacity limit, causing legitimate current-epoch payloads to be dropped, which prevents the observer node from functioning correctly.

## Finding Description

The vulnerability exists in the consensus observer's block payload storage mechanism. When a consensus observer receives block payload messages from subscribed peers, it stores them in a `BlockPayloadStore` with a fixed capacity limit defined by `max_num_pending_blocks` (default 150 blocks). [1](#0-0) 

The critical flaw is at line 98 where the epoch is extracted directly from the attacker-controlled `block_payload` without any validation that the epoch is reasonable relative to the current epoch: [2](#0-1) 

The only check performed is a size limit check (lines 86-94), which treats all payloads equally regardless of their epoch. When processing incoming payloads, the system performs minimal validation: [3](#0-2) 

The `payload_out_of_date` check only verifies that payloads are not OLDER than the last ordered block, but does not check if they are too far in the FUTURE. Payloads for future epochs bypass signature verification (line 401-418) but are still inserted into the store as unverified.

**Attack Flow:**

1. A malicious subscribed peer sends 150 `BlockPayload` messages with future epochs (e.g., epochs 100-249, round 0)
2. These payloads pass the `payload_out_of_date` check since `(100, 0) > (0, last_round)` 
3. They pass basic digest verification
4. They bypass signature verification because `block_epoch != epoch_state.epoch`
5. They are inserted as unverified payloads, filling the store to capacity
6. When legitimate current-epoch payloads arrive, they are dropped with a warning

**Why Cleanup Fails:**

The cleanup mechanism uses `BTreeMap::split_off`: [4](#0-3) 

When committed blocks at epoch 0 trigger `remove_blocks_for_epoch_round(0, X)`, the split occurs at `(0, X+1)`. Since BTreeMap compares tuples lexicographically, keys like `(100, 0)` are greater than `(0, X+1)`, so future epoch payloads are KEPT rather than removed. They persist indefinitely.

## Impact Explanation

This is a **High severity** vulnerability per the Aptos bug bounty criteria, specifically:
- **Validator node slowdowns**: The observer node cannot process legitimate payloads and falls behind
- **Significant protocol violations**: The consensus observer mechanism is designed to allow nodes to observe consensus without participating, but this attack breaks that functionality

The impact includes:
- Observer nodes become unable to receive or store legitimate block payloads from the current epoch
- Observers fall behind and cannot fulfill their monitoring/observation role
- The attack is persistent (payloads are never cleaned up) and requires manual intervention
- All consensus observer nodes subscribed to the malicious peer are affected

This does not reach Critical severity because it does not directly cause consensus safety violations, loss of funds, or complete network partition, but it significantly degrades the functionality of affected nodes.

## Likelihood Explanation

**Likelihood: HIGH**

The attack has low barriers to execution:

1. **Attacker requirements**: The attacker needs to operate a peer node that gets selected for subscription by target observer nodes. Subscription selection is based on `distance_from_validators` and latency metrics. [5](#0-4) 

2. **Attack complexity**: Once subscribed, the attack is trivial - simply send 150 block payload messages with incremented future epoch numbers. No cryptographic attacks or complex exploitation required.

3. **Detection difficulty**: The attack may initially appear as network issues or high latency, making diagnosis difficult. The warning logs only indicate "Exceeded the maximum number of payloads" without identifying the root cause.

4. **Subscription filtering**: While messages from non-subscribed peers are rejected, there are no additional validation layers: [6](#0-5) 

An initially honest peer could get subscribed and then turn malicious, or a malicious operator could run a peer that appears optimal (low latency, claims low distance from validators) to increase subscription likelihood.

## Recommendation

**Immediate Fix**: Add epoch range validation in `insert_block_payload()`:

```rust
pub fn insert_block_payload(
    &mut self,
    block_payload: BlockPayload,
    verified_payload_signatures: bool,
    current_epoch: u64,  // Add current epoch parameter
) {
    // Reject payloads that are too far in the future
    let block_epoch = block_payload.epoch();
    let max_future_epoch_tolerance = 1; // Allow at most 1 epoch ahead
    if block_epoch > current_epoch + max_future_epoch_tolerance {
        warn!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Rejecting payload with future epoch: {:?} (current: {:?}). Block: {:?}",
                block_epoch,
                current_epoch,
                block_payload.block(),
            ))
        );
        return; // Drop payloads that are too far ahead
    }

    // Existing size check
    let max_num_pending_blocks = self.consensus_observer_config.max_num_pending_blocks as usize;
    if self.block_payloads.lock().len() >= max_num_pending_blocks {
        warn!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Exceeded the maximum number of payloads: {:?}. Dropping block: {:?}!",
                max_num_pending_blocks,
                block_payload.block(),
            ))
        );
        return;
    }

    // ... rest of the function remains the same
}
```

Update the caller to pass the current epoch: [7](#0-6) 

**Additional Hardening**:
1. Add per-peer rate limiting to prevent a single malicious peer from flooding the store
2. Implement periodic cleanup of unverified payloads older than a threshold
3. Add metrics to track the epoch distribution of stored payloads for better observability
4. Consider separate storage quotas for verified vs. unverified payloads

## Proof of Concept

```rust
#[test]
fn test_future_epoch_injection_dos() {
    use crate::consensus_observer::network::observer_message::BlockTransactionPayload;
    use aptos_types::block_info::BlockInfo;

    // Create a consensus observer config with small capacity
    let max_num_pending_blocks = 10;
    let consensus_observer_config = ConsensusObserverConfig {
        max_num_pending_blocks,
        ..ConsensusObserverConfig::default()
    };

    // Create a new block payload store
    let mut block_payload_store = BlockPayloadStore::new(consensus_observer_config);

    // Attacker injects future epoch payloads (epoch 100-109)
    for i in 0..max_num_pending_blocks {
        let future_epoch = 100 + i;
        let block_info = BlockInfo::random_with_epoch(future_epoch, 0);
        let block_payload = BlockPayload::new(block_info, BlockTransactionPayload::empty());
        
        // Insert as unverified (simulating bypass of signature verification)
        block_payload_store.insert_block_payload(block_payload, false);
    }

    // Verify store is full with future epoch payloads
    assert_eq!(
        block_payload_store.block_payloads.lock().len(),
        max_num_pending_blocks as usize
    );

    // Now try to insert a legitimate current epoch payload (epoch 0)
    let current_epoch = 0;
    let block_info = BlockInfo::random_with_epoch(current_epoch, 0);
    let legitimate_payload = BlockPayload::new(block_info, BlockTransactionPayload::empty());
    
    // This should be dropped due to store being full
    block_payload_store.insert_block_payload(legitimate_payload.clone(), true);

    // Verify the legitimate payload was NOT inserted
    let contains_legitimate = block_payload_store
        .block_payloads
        .lock()
        .contains_key(&(current_epoch, 0));
    assert!(!contains_legitimate, "Legitimate payload should have been dropped");

    // Verify future epoch payloads persist after cleanup
    block_payload_store.remove_blocks_for_epoch_round(current_epoch, 1000);
    
    // Future epoch payloads should still be present
    assert_eq!(
        block_payload_store.block_payloads.lock().len(),
        max_num_pending_blocks as usize,
        "Future epoch payloads should not be removed by cleanup"
    );
}
```

## Notes

This vulnerability demonstrates a classic resource exhaustion attack where insufficient input validation allows attackers to fill a bounded resource with junk data, preventing legitimate operations. The issue is exacerbated by the cleanup mechanism's inability to remove future epoch entries due to BTreeMap key ordering semantics. The fix requires both input validation (reject unreasonable future epochs) and potentially improved cleanup logic to handle edge cases like epoch transitions or malicious data injection.

### Citations

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L79-109)
```rust
    pub fn insert_block_payload(
        &mut self,
        block_payload: BlockPayload,
        verified_payload_signatures: bool,
    ) {
        // Verify that the number of payloads doesn't exceed the maximum
        let max_num_pending_blocks = self.consensus_observer_config.max_num_pending_blocks as usize;
        if self.block_payloads.lock().len() >= max_num_pending_blocks {
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Exceeded the maximum number of payloads: {:?}. Dropping block: {:?}!",
                    max_num_pending_blocks,
                    block_payload.block(),
                ))
            );
            return; // Drop the block if we've exceeded the maximum
        }

        // Create the new payload status
        let epoch_and_round = (block_payload.epoch(), block_payload.round());
        let payload_status = if verified_payload_signatures {
            BlockPayloadStatus::AvailableAndVerified(block_payload)
        } else {
            BlockPayloadStatus::AvailableAndUnverified(block_payload)
        };

        // Insert the new payload status
        self.block_payloads
            .lock()
            .insert(epoch_and_round, payload_status);
    }
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L112-119)
```rust
    pub fn remove_blocks_for_epoch_round(&self, epoch: u64, round: Round) {
        // Determine the round to split off
        let split_off_round = round.saturating_add(1);

        // Remove the blocks from the payload store
        let mut block_payloads = self.block_payloads.lock();
        *block_payloads = block_payloads.split_off(&(epoch, split_off_round));
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L366-418)
```rust
        // Determine if the payload is behind the last ordered block, or if it already exists
        let last_ordered_block = self.observer_block_data.lock().get_last_ordered_block();
        let payload_out_of_date =
            (block_epoch, block_round) <= (last_ordered_block.epoch(), last_ordered_block.round());
        let payload_exists = self
            .observer_block_data
            .lock()
            .existing_payload_entry(&block_payload);

        // If the payload is out of date or already exists, ignore it
        if payload_out_of_date || payload_exists {
            // Update the metrics for the dropped block payload
            update_metrics_for_dropped_block_payload_message(peer_network_id, &block_payload);
            return;
        }

        // Update the metrics for the received block payload
        update_metrics_for_block_payload_message(peer_network_id, &block_payload);

        // Verify the block payload digests
        if let Err(error) = block_payload.verify_payload_digests() {
            // Log the error and update the invalid message counter
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to verify block payload digests! Ignoring block: {:?}, from peer: {:?}. Error: {:?}",
                    block_payload.block(), peer_network_id,
                    error
                ))
            );
            increment_invalid_message_counter(&peer_network_id, metrics::BLOCK_PAYLOAD_LABEL);
            return;
        }

        // If the payload is for the current epoch, verify the proof signatures
        let epoch_state = self.get_epoch_state();
        let verified_payload = if block_epoch == epoch_state.epoch {
            // Verify the block proof signatures
            if let Err(error) = block_payload.verify_payload_signatures(&epoch_state) {
                // Log the error and update the invalid message counter
                error!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Failed to verify block payload signatures! Ignoring block: {:?}, from peer: {:?}. Error: {:?}",
                        block_payload.block(), peer_network_id, error
                    ))
                );
                increment_invalid_message_counter(&peer_network_id, metrics::BLOCK_PAYLOAD_LABEL);
                return;
            }

            true // We have successfully verified the signatures
        } else {
            false // We can't verify the signatures yet
        };
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L427-430)
```rust
        // Update the payload store with the payload
        self.observer_block_data
            .lock()
            .insert_block_payload(block_payload, verified_payload);
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L578-594)
```rust
        // Verify the message is from the peers we've subscribed to
        if let Err(error) = self
            .subscription_manager
            .verify_message_for_subscription(peer_network_id)
        {
            // Update the rejected message counter
            increment_rejected_message_counter(&peer_network_id, &message);

            // Log the error and return
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Received message that was not from an active subscription! Error: {:?}",
                    error,
                ))
            );
            return;
        }
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L283-312)
```rust
pub fn sort_peers_by_subscription_optimality(
    peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) -> Vec<PeerNetworkId> {
    // Group peers and latencies by validator distance, i.e., distance -> [(peer, latency)]
    let mut unsupported_peers = Vec::new();
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for (peer_network_id, peer_metadata) in peers_and_metadata {
        // Verify that the peer supports consensus observer
        if !supports_consensus_observer(peer_metadata) {
            unsupported_peers.push(*peer_network_id);
            continue; // Skip the peer
        }

        // Get the distance and latency for the peer
        let distance = get_distance_for_peer(peer_network_id, peer_metadata);
        let latency = get_latency_for_peer(peer_network_id, peer_metadata);

        // If the distance is not found, use the maximum distance
        let distance =
            distance.unwrap_or(aptos_peer_monitoring_service_types::MAX_DISTANCE_FROM_VALIDATORS);

        // If the latency is not found, use a large latency
        let latency = latency.unwrap_or(MAX_PING_LATENCY_SECS);

        // Add the peer and latency to the distance group
        peers_and_latencies_by_distance
            .entry(distance)
            .or_insert_with(Vec::new)
            .push((*peer_network_id, OrderedFloat(latency)));
    }
```
