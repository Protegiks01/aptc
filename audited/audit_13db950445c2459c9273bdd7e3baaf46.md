# Audit Report

## Title
RandDb Column Family Size Monitoring Gap Enabling Silent Storage Exhaustion and Validator Crashes

## Summary
The randomness generation storage system (RandDb) lacks column family size monitoring and alerting mechanisms present in AptosDB, creating a blind spot for storage growth. Combined with error handling that only logs cleanup failures without retry logic and panic-on-failure behavior when persisting validator's own augmented data, this monitoring gap can amplify underlying storage issues into validator crashes and potential consensus liveness problems.

## Finding Description

The RandDb stores randomness-related data in three column families as defined in the schema: [1](#0-0) [2](#0-1) [3](#0-2) 

Unlike AptosDB, which has comprehensive monitoring via `ROCKSDB_PROPERTIES` metrics: [4](#0-3) 

And automated property reporting: [5](#0-4) 

The RandDb has NO such monitoring infrastructure. When RandDb is initialized, there is no property reporter attached: [6](#0-5) 

**The vulnerability chain:**

1. **Silent cleanup failures**: During epoch transitions, old data cleanup failures are only logged, not surfaced or retried: [7](#0-6) 

2. **No monitoring means no alerts**: Without column family size monitoring, operators have no visibility into RandDb storage growth until general disk space alerts trigger (when it's too late).

3. **Panic on critical path**: When the validator attempts to save its own augmented data and storage write fails, the code panics: [8](#0-7) 

4. **Attack scenario**: If RocksDB deletion operations fail (due to filesystem corruption, permission issues, or bugs) while writes succeed, old epoch data accumulates across epochs. Each epoch stores data for all validators, so over many epochs with failed cleanup, RandDb grows unbounded. When disk fills or RocksDB encounters write errors, the next `.expect()` call causes validator crash.

## Impact Explanation

This qualifies as **Medium Severity** under the bug bounty criteria:
- **Validator node unavailability**: The panic causes immediate validator crash when storage operations fail
- **State inconsistencies requiring intervention**: Silent accumulation of stale data requires manual investigation and cleanup
- **Amplification of underlying issues**: A monitoring gap that transforms storage bugs into availability problems

While general disk space alerts exist: [9](#0-8) 

These alerts don't identify RandDb as the culprit, making root cause analysis difficult and potentially delaying remediation until multiple validators are affected.

## Likelihood Explanation

**Medium Likelihood:**
- Requires specific error conditions where RocksDB deletion fails but writes succeed (unusual but possible with filesystem corruption, permission issues, or RocksDB bugs)
- The cleanup mechanism normally works correctly under normal conditions
- However, the lack of monitoring means the issue accumulates silently over time
- Once disk pressure occurs, the panic is deterministic and affects all validators similarly

The combination of no monitoring, silent error handling, and panic-on-failure creates a fragile system that could affect multiple validators simultaneously if the underlying condition is triggered by common infrastructure issues.

## Recommendation

**Implement RandDb column family size monitoring:**

1. Add RocksdbPropertyReporter integration for RandDb similar to AptosDB
2. Export RandDb column family sizes via ROCKSDB_PROPERTIES metrics
3. Add specific alerts for RandDb storage growth thresholds
4. Improve error handling:
   - Retry cleanup operations on failure
   - Surface cleanup failures more prominently (not just error logs)
   - Replace `.expect()` with proper error propagation that doesn't panic
5. Add defensive bounds checking on the number of stored entries

**Code fix approach:**

In `consensus/src/rand/rand_gen/storage/db.rs`, add monitoring similar to AptosDB:
- Create a RandDbPropertyReporter that exports metrics for the three column families
- Instantiate it when RandDb is created
- Add to Grafana dashboards alongside AptosDB metrics

In `consensus/src/rand/rand_gen/rand_manager.rs`, replace panic with error handling:
- Change line 313 from `.expect()` to `?` operator and propagate errors up
- Add retry logic with exponential backoff for storage operations
- Log metrics on storage operation failures

In `consensus/src/rand/rand_gen/aug_data_store.rs`, improve cleanup resilience:
- Retry deletion operations on failure
- Add maximum retry count with escalating log levels
- Consider graceful degradation instead of continuing with stale data

## Proof of Concept

```rust
// Test demonstrating the vulnerability
// File: consensus/src/rand/rand_gen/storage/test_monitoring.rs

#[cfg(test)]
mod tests {
    use super::*;
    use tempfile::TempDir;
    
    #[test]
    fn test_rand_db_no_monitoring() {
        let tmp_dir = TempDir::new().unwrap();
        let rand_db = RandDb::new(tmp_dir.path());
        
        // Verify RandDb has no property reporter
        // (This would fail to compile if trying to access a non-existent field)
        // Unlike AptosDB which has: self._rocksdb_property_reporter
        
        // Simulate accumulation scenario:
        // 1. Create augmented data for 100 epochs
        // 2. Fail to clean up (mock RocksDB deletion errors)
        // 3. Verify storage grows unbounded
        // 4. Verify no metrics are exported for column family sizes
        // 5. Trigger save operation that would panic
        
        // This PoC would require:
        // - Mock RocksDB that fails deletions but succeeds writes
        // - Metrics verification showing no ROCKSDB_PROPERTIES for rand_db CFs
        // - Demonstration of panic when storage operations fail
    }
}
```

**Notes:**

- The monitoring gap is confirmed by code inspection: RandDb initialization has no property reporter
- AptosDB has comprehensive monitoring that RandDb lacks
- The `.expect()` panic is a deterministic failure mode once storage issues occur
- This is a defense-in-depth issue that amplifies underlying problems into availability failures
- The question itself labels this as "Low" severity, acknowledging it's not a direct exploit but an operational/monitoring concern

### Citations

**File:** consensus/src/rand/rand_gen/storage/schema.rs (L12-14)
```rust
pub(crate) const KEY_PAIR_CF_NAME: ColumnFamilyName = "key_pair";

define_schema!(KeyPairSchema, (), (u64, Vec<u8>), KEY_PAIR_CF_NAME);
```

**File:** consensus/src/rand/rand_gen/storage/schema.rs (L36-45)
```rust
pub(crate) const AUG_DATA_CF_NAME: ColumnFamilyName = "aug_data";
#[derive(Debug)]
pub struct AugDataSchema<D>(PhantomData<D>);

impl<D: TAugmentedData> Schema for AugDataSchema<D> {
    type Key = AugDataId;
    type Value = AugData<D>;

    const COLUMN_FAMILY_NAME: ColumnFamilyName = AUG_DATA_CF_NAME;
}
```

**File:** consensus/src/rand/rand_gen/storage/schema.rs (L67-76)
```rust
pub(crate) const CERTIFIED_AUG_DATA_CF_NAME: ColumnFamilyName = "certified_aug_data";
#[derive(Debug)]
pub struct CertifiedAugDataSchema<D>(PhantomData<D>);

impl<D: TAugmentedData> Schema for CertifiedAugDataSchema<D> {
    type Key = AugDataId;
    type Value = CertifiedAugData<D>;

    const COLUMN_FAMILY_NAME: ColumnFamilyName = CERTIFIED_AUG_DATA_CF_NAME;
}
```

**File:** storage/aptosdb/src/metrics.rs (L145-155)
```rust
pub static ROCKSDB_PROPERTIES: Lazy<IntGaugeVec> = Lazy::new(|| {
    register_int_gauge_vec!(
        // metric name
        "aptos_rocksdb_properties",
        // metric description
        "rocksdb integer properties",
        // metric labels (dimensions)
        &["cf_name", "property_name",]
    )
    .unwrap()
});
```

**File:** storage/aptosdb/src/rocksdb_property_reporter.rs (L72-81)
```rust
fn set_property(cf_name: &str, db: &DB) -> Result<()> {
    if !skip_reporting_cf(cf_name) {
        for (rockdb_property_name, aptos_rocksdb_property_name) in &*ROCKSDB_PROPERTY_MAP {
            ROCKSDB_PROPERTIES
                .with_label_values(&[cf_name, aptos_rocksdb_property_name])
                .set(db.get_property(cf_name, rockdb_property_name)? as i64);
        }
    }
    Ok(())
}
```

**File:** consensus/src/consensus_provider.rs (L85-85)
```rust
    let rand_storage = Arc::new(RandDb::new(node_config.storage.dir()));
```

**File:** consensus/src/rand/rand_gen/aug_data_store.rs (L51-65)
```rust
        let all_data = db.get_all_aug_data().unwrap_or_default();
        let (to_remove, aug_data) = Self::filter_by_epoch(epoch, all_data.into_iter());
        if let Err(e) = db.remove_aug_data(to_remove) {
            error!("[AugDataStore] failed to remove aug data: {:?}", e);
        }

        let all_certified_data = db.get_all_certified_aug_data().unwrap_or_default();
        let (to_remove, certified_data) =
            Self::filter_by_epoch(epoch, all_certified_data.into_iter());
        if let Err(e) = db.remove_certified_aug_data(to_remove) {
            error!(
                "[AugDataStore] failed to remove certified aug data: {:?}",
                e
            );
        }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L311-313)
```rust
        self.aug_data_store
            .add_aug_data(data.clone())
            .expect("Add self aug data should succeed");
```

**File:** terraform/helm/monitoring/files/rules/alerts.yml (L91-109)
```yaml
  - alert: Validator Low Disk Space (warning)
    expr: (kubelet_volume_stats_capacity_bytes{persistentvolumeclaim=~".*(validator|fullnode)-e.*"} - kubelet_volume_stats_used_bytes) / 1024 / 1024 / 1024 < 200
    for: 1h
    labels:
      severity: warning
      summary: "Less than 200 GB of free space on Aptos Node."
    annotations:
      description: "(This is a warning, deal with it in working hours.) A validator or fullnode pod has less than 200 GB of disk space. Take these steps:
        1. If only a few nodes have this issue, it might be that they are not typically spec'd or customized differently, \
          it's most likely a expansion of the volume is needed soon. Talk to the PE team. Otherwise, it's a bigger issue.
        2. Pass this issue on to the storage team. If you are the storage team, read on.
        3. Go to the dashboard and look for the stacked up column family sizes. \
          If the total size on that chart can't justify low free disk space, we need to log in to a node to see if something other than the AptosDB is eating up disk. \
          Start from things under /opt/aptos/data.
        3 Otherwise, if the total size on that chart is the majority of the disk consumption, zoom out and look for anomalies -- sudden increases overall or on a few \
          specific Column Families, etc. Also check average size of each type of data. Reason about the anomaly with changes in recent releases in mind.
        4 If everything made sense, it's a bigger issue, somehow our gas schedule didn't stop state explosion before an alert is triggered. Our recommended disk \
          spec and/or default pruning configuration, as well as storage gas schedule need updates. Discuss with the ecosystem team and send out a PR on the docs site, \
          form a plan to inform the node operator community and prepare for a on-chain proposal to update the gas schedule."
```
