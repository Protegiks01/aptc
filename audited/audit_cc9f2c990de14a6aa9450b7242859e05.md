# Audit Report

## Title
Non-Atomic State Updates in Block Storage Operations Create Race Condition Window

## Summary
The `insert_block_with_qc()` function and similar patterns in production code perform separate, non-atomic operations for QC insertion, ordered root updates, and block insertion, creating windows where the BlockStore state is observable in an inconsistent intermediate state.

## Finding Description

The vulnerability exists in the non-atomic sequence of operations in block insertion flows. While `insert_block_with_qc()` itself is test-only code [1](#0-0) , the same problematic pattern appears in production code within `insert_quorum_cert()` [2](#0-1) .

The core issue manifests in `send_for_execution()` where critical state updates occur in separate lock acquisitions:

**Step 1:** Update `ordered_root` [3](#0-2) 

**Step 2:** Update `ordered_cert` in a separate lock acquisition [4](#0-3) 

Between these operations, the write lock on `self.inner` (RwLock\<BlockTree\>) is released and reacquired, allowing concurrent readers to observe:
- `ordered_root` pointing to round R
- `highest_ordered_cert` still at round R-1

This violates the atomicity invariant for state consistency [5](#0-4) .

Additionally, in `insert_single_quorum_cert()`, the QC is persisted to storage and inserted into BlockTree as separate operations [6](#0-5) , followed later by block insertion [7](#0-6) .

## Impact Explanation

**Severity Assessment: Medium**

This qualifies as "State inconsistencies requiring intervention" per the bug bounty criteria. The impact includes:

1. **Transient State Inconsistencies**: Concurrent operations observing `ordered_root` and `ordered_cert` in mismatched states could make incorrect validation decisions
2. **Potential Validation Failures**: Proposals or votes processed during the inconsistent window might be incorrectly rejected or accepted
3. **No Direct Consensus Safety Violation**: The core BFT safety properties remain intact due to vote aggregation and other consensus checks, preventing this from being Critical severity

The vulnerability does not result in funds loss or permanent network partition, placing it at Medium rather than High or Critical severity.

## Likelihood Explanation

**Likelihood: Medium-High in high-throughput scenarios**

The race condition window is small but occurs during every block insertion and QC processing operation. In high-throughput networks with concurrent proposal processing, multiple validators could observe the inconsistent state simultaneously. The likelihood increases with:

- Network latency variations causing proposal processing overlap
- Multiple concurrent sync operations
- High block production rates

The race does not require attacker-controlled timing, making it a naturally occurring condition rather than a targeted exploit.

## Recommendation

Implement atomic state updates by performing all related modifications within a single write lock acquisition:

```rust
// In send_for_execution(), replace lines 338-341 with:
let mut tree = self.inner.write();
tree.update_ordered_root(block_to_commit.id());
tree.insert_ordered_cert(finality_proof_clone.clone());
drop(tree); // Explicit lock release
```

For `insert_block_with_qc()` and `insert_quorum_cert()`, consider using a transaction-like pattern or mutex to ensure atomicity across storage persistence and in-memory updates:

```rust
// Wrap the entire sequence in a higher-level lock
let _guard = self.insertion_lock.lock();
self.insert_single_quorum_cert(block.quorum_cert().clone())?;
if self.ordered_root().round() < block.quorum_cert().commit_info().round() {
    self.send_for_execution(block.quorum_cert().into_wrapped_ledger_info()).await?;
}
self.insert_block(block).await
```

## Proof of Concept

```rust
// Concurrent test demonstrating the race condition
#[tokio::test]
async fn test_race_condition_in_state_updates() {
    use std::sync::Arc;
    use tokio::task;
    
    let block_store = Arc::new(build_test_block_store());
    let block_store_clone = block_store.clone();
    
    // Thread 1: Insert block with QC (triggers send_for_execution)
    let handle1 = task::spawn(async move {
        let block = create_test_block_with_commit_info();
        block_store.insert_block_with_qc(block).await
    });
    
    // Thread 2: Concurrently read ordered_root and ordered_cert
    let handle2 = task::spawn(async move {
        tokio::time::sleep(Duration::from_micros(10)).await;
        let root_round = block_store_clone.ordered_root().round();
        let cert_round = block_store_clone.highest_ordered_cert().commit_info().round();
        // Race condition: root_round may be > cert_round due to non-atomic updates
        assert_eq!(root_round, cert_round, "Inconsistent state observed");
    });
    
    let _ = tokio::join!(handle1, handle2);
}
```

**Notes:**

While the primary `insert_block_with_qc()` function is test-only code, this analysis focuses on the production code pattern in `sync_manager.rs` and the non-atomic updates in `send_for_execution()` which affect all consensus nodes. The vulnerability represents a state consistency violation rather than a consensus safety break, warranting Medium severity classification.

### Citations

**File:** consensus/src/block_storage/block_store.rs (L338-338)
```rust
        self.inner.write().update_ordered_root(block_to_commit.id());
```

**File:** consensus/src/block_storage/block_store.rs (L339-341)
```rust
        self.inner
            .write()
            .insert_ordered_cert(finality_proof_clone.clone());
```

**File:** consensus/src/block_storage/block_store.rs (L512-515)
```rust
        self.storage
            .save_tree(vec![pipelined_block.block().clone()], vec![])
            .context("Insert block failed when saving block")?;
        self.inner.write().insert_block(pipelined_block)
```

**File:** consensus/src/block_storage/block_store.rs (L552-555)
```rust
        self.storage
            .save_tree(vec![], vec![qc.clone()])
            .context("Insert block failed when saving quorum")?;
        self.inner.write().insert_quorum_cert(qc)
```

**File:** consensus/src/block_storage/block_store.rs (L822-830)
```rust
    /// Helper function to insert the block with the qc together
    pub async fn insert_block_with_qc(&self, block: Block) -> anyhow::Result<Arc<PipelinedBlock>> {
        self.insert_single_quorum_cert(block.quorum_cert().clone())?;
        if self.ordered_root().round() < block.quorum_cert().commit_info().round() {
            self.send_for_execution(block.quorum_cert().into_wrapped_ledger_info())
                .await?;
        }
        self.insert_block(block).await
    }
```

**File:** consensus/src/block_storage/sync_manager.rs (L175-201)
```rust
    pub async fn insert_quorum_cert(
        &self,
        qc: &QuorumCert,
        retriever: &mut BlockRetriever,
    ) -> anyhow::Result<()> {
        match self.need_fetch_for_quorum_cert(qc) {
            NeedFetchResult::NeedFetch => self.fetch_quorum_cert(qc.clone(), retriever).await?,
            NeedFetchResult::QCBlockExist => self.insert_single_quorum_cert(qc.clone())?,
            NeedFetchResult::QCAlreadyExist => return Ok(()),
            _ => (),
        }
        if self.ordered_root().round() < qc.commit_info().round() {
            SUCCESSFUL_EXECUTED_WITH_REGULAR_QC.inc();
            self.send_for_execution(qc.into_wrapped_ledger_info())
                .await?;
            if qc.ends_epoch() {
                retriever
                    .network
                    .broadcast_epoch_change(EpochChangeProof::new(
                        vec![qc.ledger_info().clone()],
                        /* more = */ false,
                    ))
                    .await;
            }
        }
        Ok(())
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L73-101)
```rust
pub struct BlockTree {
    /// All the blocks known to this replica (with parent links)
    id_to_block: HashMap<HashValue, LinkableBlock>,
    /// Root of the tree. This is the root of ordering phase
    ordered_root_id: HashValue,
    /// Commit Root id: this is the root of commit phase
    commit_root_id: HashValue,
    /// Window Root id: this is the first item in the [`OrderedBlockWindow`](OrderedBlockWindow)
    window_root_id: HashValue,
    /// A certified block id with highest round
    highest_certified_block_id: HashValue,

    /// The quorum certificate of highest_certified_block
    highest_quorum_cert: Arc<QuorumCert>,
    /// The highest 2-chain timeout certificate (if any).
    highest_2chain_timeout_cert: Option<Arc<TwoChainTimeoutCertificate>>,
    /// The quorum certificate that has highest commit info.
    highest_ordered_cert: Arc<WrappedLedgerInfo>,
    /// The quorum certificate that has highest commit decision info.
    highest_commit_cert: Arc<WrappedLedgerInfo>,
    /// Map of block id to its completed quorum certificate (2f + 1 votes)
    id_to_quorum_cert: HashMap<HashValue, Arc<QuorumCert>>,
    /// To keep the IDs of the elements that have been pruned from the tree but not cleaned up yet.
    pruned_block_ids: VecDeque<HashValue>,
    /// Num pruned blocks to keep in memory.
    max_pruned_blocks_in_mem: usize,
    /// Round to Block index. We expect only one block per round.
    round_to_ids: BTreeMap<Round, HashValue>,
}
```
