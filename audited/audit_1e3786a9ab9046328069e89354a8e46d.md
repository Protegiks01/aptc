# Audit Report

## Title
Permanent Consensus Liveness Failure Due to Missing Retry Logic for ExecutorError::CouldNotGetData

## Summary
When block execution fails with `ExecutorError::CouldNotGetData` (a timeout error), the block is permanently stuck in "Ordered" state with no retry mechanism, causing total consensus liveness failure. All subsequent blocks are blocked from execution, halting the entire blockchain until manual intervention or epoch change.

## Finding Description

The consensus pipeline processes blocks sequentially through the BufferManager. When a block requires batch data from the quorum store and that data times out, execution fails with `CouldNotGetData`. [1](#0-0) 

This error is returned when batch requests timeout [2](#0-1)  or when batches expire [3](#0-2) .

When the ExecutionWaitPhase returns this error, the BufferManager's `process_execution_response()` method logs it and returns early without advancing the block's state: [4](#0-3) 

The block remains in "Ordered" state. Subsequently, `advance_execution_root()` is called, which finds the same block still unexecuted and returns `Some(block_id)` with a comment "Schedule retry": [5](#0-4) 

**However, this return value is completely ignored** in the main event loop: [6](#0-5) 

Unlike the signing phase which properly implements retry logic using `spawn_retry_request()` [7](#0-6) , the execution phase has no such mechanism.

The Buffer processes blocks sequentially [8](#0-7) , meaning if one block is stuck, all subsequent blocks cannot proceed. The periodic interval tick only updates metrics and rebroadcasts commit votes, it does not retry execution: [9](#0-8) 

## Impact Explanation

This is a **Critical Severity** vulnerability per Aptos bug bounty criteria as it causes:
- **Total loss of liveness/network availability**: All validators with the stuck block cannot make progress
- **Non-recoverable without manual intervention**: Only a reset signal or epoch change can clear the stuck block

The entire blockchain halts because:
1. The failed block blocks all execution advancement
2. No automatic retry mechanism exists
3. New rounds can propose new blocks, but they are queued behind the stuck block and cannot execute
4. All nodes experiencing the timeout are permanently stuck until manual reset

## Likelihood Explanation

This is **highly likely** to occur in production because:
- Network partitions are common and can prevent batch data retrieval
- Batch expiration is a normal occurrence when chain progresses faster than data propagates
- Peer nodes may legitimately not have requested batch data
- No special attacker capabilities required - occurs naturally from network conditions

An attacker could also intentionally trigger this by:
- Being selected as leader and proposing blocks with batches they never share
- Withholding batch data from specific validators to selectively halt them

## Recommendation

Implement retry logic for execution errors similar to the signing phase. Modify `advance_execution_root()` to call `spawn_retry_request()` when the execution root hasn't moved:

**File: `consensus/src/pipeline/buffer_manager.rs`**

Replace `advance_execution_root()` implementation at lines 429-452 with:

```rust
async fn advance_execution_root(&mut self) {
    let cursor = self.execution_root;
    self.execution_root = self
        .buffer
        .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
            item.is_ordered()
        });
    
    if self.execution_root.is_some() {
        let item = self.buffer.get(&self.execution_root);
        let ordered_item = item.unwrap_ordered_ref();
        let request = self.create_new_request(ExecutionRequest {
            ordered_blocks: ordered_item.ordered_blocks.clone(),
        });
        
        if cursor == self.execution_root {
            // Retry with exponential backoff
            let sender = self.execution_schedule_phase_tx.clone();
            Self::spawn_retry_request(sender, request, Duration::from_millis(100));
        } else {
            self.execution_schedule_phase_tx
                .send(request)
                .await
                .expect("Failed to send execution request");
        }
    }
}
```

Update the main event loop to make `advance_execution_root()` async and call it properly.

## Proof of Concept

```rust
// Rust integration test demonstrating the vulnerability
// File: consensus/src/pipeline/tests/execution_timeout_test.rs

use crate::pipeline::{
    buffer_manager::BufferManager,
    tests::test_utils::*,
};
use aptos_executor_types::ExecutorError;
use std::sync::Arc;

#[tokio::test]
async fn test_execution_timeout_causes_liveness_failure() {
    let (mut buffer_manager, block_tx, ..) = prepare_buffer_manager(..);
    
    // Start buffer manager in background
    let handle = tokio::spawn(async move {
        buffer_manager.start().await;
    });
    
    // Create block that will fail with CouldNotGetData
    let block_with_missing_batch = create_block_with_unavailable_batch(..);
    let ordered_blocks = OrderedBlocks {
        ordered_blocks: vec![block_with_missing_batch],
        ordered_proof: create_proof(..),
    };
    
    // Send first block - will fail with CouldNotGetData
    block_tx.send(ordered_blocks).await.unwrap();
    tokio::time::sleep(Duration::from_secs(5)).await;
    
    // Send second block - should execute but won't because first block is stuck
    let second_block = create_normal_block(..);
    let ordered_blocks2 = OrderedBlocks {
        ordered_blocks: vec![second_block],
        ordered_proof: create_proof(..),
    };
    block_tx.send(ordered_blocks2).await.unwrap();
    
    // Wait and verify that second block never executes
    tokio::time::sleep(Duration::from_secs(10)).await;
    
    // Check metrics - should show blocks stuck in "ordered" state
    // and no progress on execution_root
    assert!(check_liveness_failure(), "Second block should be stuck");
}
```

The PoC demonstrates that once a block fails with `CouldNotGetData`, all subsequent blocks are permanently stuck, causing complete consensus halt.

### Citations

**File:** execution/executor-types/src/error.rs (L41-42)
```rust
    #[error("request timeout")]
    CouldNotGetData,
```

**File:** consensus/src/quorum_store/batch_requester.rs (L148-150)
```rust
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
```

**File:** consensus/src/quorum_store/batch_requester.rs (L176-178)
```rust
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
```

**File:** consensus/src/pipeline/buffer_manager.rs (L436-438)
```rust
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
```

**File:** consensus/src/pipeline/buffer_manager.rs (L478-480)
```rust
            if cursor == self.signing_root {
                let sender = self.signing_phase_tx.clone();
                Self::spawn_retry_request(sender, request, Duration::from_millis(100));
```

**File:** consensus/src/pipeline/buffer_manager.rs (L617-626)
```rust
        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
```

**File:** consensus/src/pipeline/buffer_manager.rs (L954-960)
```rust
                Some(response) = self.execution_wait_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_wait_response", {
                    self.process_execution_response(response).await;
                    self.advance_execution_root();
                    if self.signing_root.is_none() {
                        self.advance_signing_root().await;
                    }});
```

**File:** consensus/src/pipeline/buffer_manager.rs (L986-990)
```rust
                _ = interval.tick().fuse() => {
                    monitor!("buffer_manager_process_interval_tick", {
                    self.update_buffer_manager_metrics();
                    self.rebroadcast_commit_votes_if_needed().await
                    });
```

**File:** consensus/src/pipeline/buffer.rs (L121-133)
```rust
    pub fn find_elem_from<F: Fn(&T) -> bool>(&self, cursor: Cursor, compare: F) -> Cursor {
        let mut current = cursor;
        if !self.exist(&cursor) {
            return None;
        }
        while current.is_some() {
            if compare(self.get(&current)) {
                return current;
            }
            current = self.get_next(&current);
        }
        None
    }
```
