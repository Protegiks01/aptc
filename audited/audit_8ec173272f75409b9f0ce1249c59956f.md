# Audit Report

## Title
State Corruption and Validator Crash in SecretShareStore Due to Unsafe Error Handling in add_share_with_metadata()

## Summary
The `SecretShareStore::add_self_share()` function contains a critical error handling bug that corrupts the secret share aggregator state and causes validator crashes during block reprocessing scenarios. The unsafe use of `std::mem::replace()` in `add_share_with_metadata()` leaves the store in an inconsistent state when errors occur, violating state consistency invariants and triggering node panics.

## Finding Description

The vulnerability exists in the `add_share_with_metadata()` method's error handling pattern. [1](#0-0) 

The function uses `std::mem::replace(self, Self::new(Author::ONE))` to temporarily move out the current state before checking validity. If the item is in `PendingDecision` state, the function bails with an error without restoring the original state. This leaves the `SecretShareItem` corrupted with a fresh `PendingMetadata` aggregator containing `Author::ONE` instead of the actual validator author, and all previously collected shares are permanently lost.

The bug is triggered when `add_self_share()` is called on a round that already has metadata (i.e., is in `PendingDecision` state). This occurs during the following realistic scenario:

1. **Normal block processing**: Validator processes block at round R, calling `add_self_share()` which transitions the entry from `PendingMetadata` to `PendingDecision` state [2](#0-1) 

2. **State sync/reset occurs**: The validator receives a `ResetRequest` (e.g., during state synchronization). The reset clears the `block_queue` but critically does NOT clear the `secret_share_map` [3](#0-2) 

3. **Block reprocessing**: After reset, consensus re-sends blocks including round R. The `process_incoming_block()` method calls `add_self_share()` again for the same round [4](#0-3) 

4. **State corruption**: The existing entry is in `PendingDecision` state, causing `add_share_with_metadata()` to bail at line 176, but the state has already been replaced at line 161 and is never restored

5. **Validator crash**: The error propagates to the `.expect()` at line 147 in the manager, causing the validator node to panic with "Add self dec share should succeed"

This breaks the **State Consistency** invariant (state transitions must be atomic) and can break **Consensus Safety** if different validators experience this at different times, leading to divergent randomness states.

## Impact Explanation

**Severity: HIGH** ($50,000 tier under Aptos bug bounty)

This vulnerability causes:

1. **Validator Node Crashes**: The `.expect()` in the caller immediately panics the validator when the error occurs, causing availability loss
2. **State Corruption**: Even if error handling were improved to prevent the crash, the corrupted state persists with wrong author (`Author::ONE`) and lost shares
3. **Consensus Liveness Failure**: Randomness generation fails for affected rounds, blocking blocks that depend on that randomness
4. **Non-Deterministic Failures**: Different validators may experience this at different times during state sync, causing consensus divergence

The impact qualifies as **High Severity** under "Validator node slowdowns / API crashes / Significant protocol violations" and potentially **Medium Severity** under "State inconsistencies requiring intervention."

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

This bug triggers during common operational scenarios:

- **State synchronization**: Validators regularly perform state sync when catching up
- **Network disruptions**: Temporary network issues trigger resets and block reprocessing  
- **Epoch transitions**: Boundary conditions during epoch changes may cause block reprocessing
- **No attacker required**: This is a pure implementation bug requiring no malicious actors

The window of vulnerability occurs when:
- A block has been processed (state is `PendingDecision`)
- A reset occurs before the share reaches `Decided` state
- The same block is reprocessed

Given that `FUTURE_ROUNDS_TO_ACCEPT` is 200 rounds [5](#0-4) , there's a significant window where rounds remain in the map during normal operations, increasing the likelihood of hitting this bug during resets.

## Recommendation

Fix the unsafe state mutation pattern in `add_share_with_metadata()`:

**Option 1: Check state before replacement**
```rust
fn add_share_with_metadata(
    &mut self,
    share: SecretShare,
    share_weights: &HashMap<Author, u64>,
) -> anyhow::Result<()> {
    // Check state BEFORE modifying
    match self {
        SecretShareItem::PendingDecision { .. } => {
            bail!("Cannot add self share in PendingDecision state");
        },
        SecretShareItem::Decided { .. } => return Ok(()),
        SecretShareItem::PendingMetadata(_) => {
            // Safe to proceed
        }
    }
    
    // Now safe to use mem::replace
    let item = std::mem::replace(self, Self::new(Author::ONE));
    // ... rest of logic
}
```

**Option 2: Clear the map during reset**
```rust
fn process_reset(&mut self, request: ResetRequest) {
    let ResetRequest { tx, signal } = request;
    let target_round = match signal {
        ResetSignal::Stop => 0,
        ResetSignal::TargetRound(round) => round,
    };
    self.block_queue = BlockQueue::new();
    let mut store = self.secret_share_store.lock();
    store.secret_share_map.clear(); // Clear all pending shares
    store.update_highest_known_round(target_round);
    drop(store);
    self.stop = matches!(signal, ResetSignal::Stop);
    let _ = tx.send(ResetAck::default());
}
```

**Option 3: Handle reprocessing gracefully** (Recommended)
```rust
pub fn add_self_share(&mut self, share: SecretShare) -> anyhow::Result<()> {
    // ... validation code ...
    
    let item = self
        .secret_share_map
        .entry(metadata.round)
        .or_insert_with(|| SecretShareItem::new(self.self_author));
    
    // Check if already decided - this is OK
    if item.has_decision() {
        return Ok(());
    }
    
    // Try to add the share, handle PendingDecision gracefully
    match item.add_share_with_metadata(share, peer_weights) {
        Ok(()) => {
            item.try_aggregate(&self.secret_share_config, self.decision_tx.clone());
            Ok(())
        },
        Err(e) if e.to_string().contains("PendingDecision") => {
            // Already have metadata for this round, this is a reprocessing scenario
            // Just try to aggregate with existing state
            item.try_aggregate(&self.secret_share_config, self.decision_tx.clone());
            Ok(())
        },
        Err(e) => Err(e),
    }
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_types::secret_sharing::{SecretShare, SecretShareMetadata};
    
    #[test]
    #[should_panic(expected = "Add self dec share should succeed")]
    fn test_add_self_share_corruption_on_reprocessing() {
        // Setup
        let epoch = 1;
        let round = 100;
        let author = Author::random();
        let config = create_test_config(author);
        let (tx, _rx) = unbounded();
        
        let mut store = SecretShareStore::new(epoch, author, config, tx);
        store.update_highest_known_round(round);
        
        // Create a self share for round 100
        let metadata = SecretShareMetadata::new(epoch, round, /* ... */);
        let share1 = create_test_share(author, metadata.clone());
        
        // First call - should succeed and transition to PendingDecision
        assert!(store.add_self_share(share1.clone()).is_ok());
        
        // Verify state is PendingDecision
        let item = store.secret_share_map.get(&round).unwrap();
        assert!(matches!(item, SecretShareItem::PendingDecision { .. }));
        
        // Simulate reset (doesn't clear the map!)
        // ... reset occurs here in real scenario ...
        
        // Second call with same share (simulating block reprocessing)
        // This triggers the bug:
        // 1. add_share_with_metadata() replaces state with Author::ONE
        // 2. Bails because state is PendingDecision
        // 3. State is left corrupted
        // 4. Panics at .expect()
        store.add_self_share(share1).expect("Add self dec share should succeed");
        
        // If we reach here without panic, check corruption
        let item = store.secret_share_map.get(&round).unwrap();
        match item {
            SecretShareItem::PendingMetadata(aggr) => {
                // State is corrupted! Author is wrong and shares are lost
                assert_eq!(aggr.self_author, Author::ONE); // Should be `author`, not ONE!
                assert_eq!(aggr.shares.len(), 0); // All shares lost!
            },
            _ => panic!("State should be corrupted to PendingMetadata"),
        }
    }
}
```

## Notes

This vulnerability demonstrates a classic Rust error handling anti-pattern: using `std::mem::replace()` to temporarily move a value without ensuring all error paths restore the original state. The bug is particularly dangerous because:

1. It corrupts validator state in a way that's difficult to detect and diagnose
2. The corruption persists across multiple operations on the same round
3. The `.expect()` in the caller masks the underlying state corruption with a crash
4. No external attacker is needed - normal network conditions trigger it

The fix should ensure either atomic state transitions (check-before-replace) or proper cleanup of stale state during resets to prevent reprocessing scenarios from occurring in the first place.

### Citations

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L156-182)
```rust
    fn add_share_with_metadata(
        &mut self,
        share: SecretShare,
        share_weights: &HashMap<Author, u64>,
    ) -> anyhow::Result<()> {
        let item = std::mem::replace(self, Self::new(Author::ONE));
        let share_weight = *share_weights
            .get(share.author())
            .expect("Author must exist in weights");
        let new_item = match item {
            SecretShareItem::PendingMetadata(mut share_aggregator) => {
                let metadata = share.metadata.clone();
                share_aggregator.retain(share.metadata(), share_weights);
                share_aggregator.add_share(share, share_weight);
                SecretShareItem::PendingDecision {
                    metadata,
                    share_aggregator,
                }
            },
            SecretShareItem::PendingDecision { .. } => {
                bail!("Cannot add self share in PendingDecision state");
            },
            SecretShareItem::Decided { .. } => return Ok(()),
        };
        let _ = std::mem::replace(self, new_item);
        Ok(())
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L237-257)
```rust
    pub fn add_self_share(&mut self, share: SecretShare) -> anyhow::Result<()> {
        assert!(
            self.self_author == share.author,
            "Only self shares can be added with metadata"
        );
        let peer_weights = self.secret_share_config.get_peer_weights();
        let metadata = share.metadata();
        ensure!(metadata.epoch == self.epoch, "Share from different epoch");
        ensure!(
            metadata.round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT,
            "Share from future round"
        );

        let item = self
            .secret_share_map
            .entry(metadata.round)
            .or_insert_with(|| SecretShareItem::new(self.self_author));
        item.add_share_with_metadata(share, peer_weights)?;
        item.try_aggregate(&self.secret_share_config, self.decision_tx.clone());
        Ok(())
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L132-158)
```rust
    async fn process_incoming_block(&self, block: &PipelinedBlock) -> DropGuard {
        let futures = block.pipeline_futs().expect("pipeline must exist");
        let self_secret_share = futures
            .secret_sharing_derive_self_fut
            .await
            .expect("Decryption share computation is expected to succeed")
            .expect("Must not be None");
        let metadata = self_secret_share.metadata().clone();

        // Now acquire lock and update store
        {
            let mut secret_share_store = self.secret_share_store.lock();
            secret_share_store.update_highest_known_round(block.round());
            secret_share_store
                .add_self_share(self_secret_share.clone())
                .expect("Add self dec share should succeed");
        }

        info!(LogSchema::new(LogEvent::BroadcastSecretShare)
            .epoch(self.epoch_state.epoch)
            .author(self.author)
            .round(block.round()));
        self.network_sender.broadcast_without_self(
            SecretShareMessage::Share(self_secret_share).into_network_message(),
        );
        self.spawn_share_requester_task(metadata)
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/types.rs (L16-16)
```rust
pub const FUTURE_ROUNDS_TO_ACCEPT: u64 = 200;
```
