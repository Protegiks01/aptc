# Audit Report

## Title
Missing Batch Existence Validation in ProofOfStore Acceptance Enables Proposal of Locally Unavailable Batches

## Summary
The `insert_proof()` method in `BatchProofQueue` accepts ProofOfStore messages without verifying that the validator has the corresponding batch data locally. This allows honest validators to include proofs in their proposals for batches they have never received, creating unnecessary dependencies on remote batch fetching during block execution and enabling potential liveness degradation attacks.

## Finding Description

The Quorum Store system handles batch proofs through two different pathways with inconsistent validation:

**Pathway 1 - SignedBatchInfo (Signature Collection):**
When validators receive `SignedBatchInfo` messages, the `ProofCoordinator::init_proof()` method explicitly verifies batch existence before processing signatures. [1](#0-0) 

This check ensures validators only sign batches they have actually received and stored.

**Pathway 2 - ProofOfStoreMsg (Proof Insertion):**
When validators receive `ProofOfStoreMsg` containing completed proofs, the `BatchProofQueue::insert_proof()` method performs only limited validation. [2](#0-1) 

**Critical Gap:** There is NO call to `batch_reader.exists()` to verify the batch is available locally before inserting the proof into the queue.

**Attack Scenario:**

1. Byzantine validator V_byz creates batch B with transaction data
2. V_byz sends batch B to exactly 2f+1 validators (minimum quorum) via selective network distribution
3. Those 2f+1 validators receive, store, and sign batch B following normal protocol
4. V_byz collects 2f+1 signatures and creates a cryptographically valid `ProofOfStore` P_B
5. V_byz broadcasts P_B to ALL validators including the f honest validators who never received batch B
6. Validators who never received batch B:
   - Receive ProofOfStoreMsg containing P_B
   - Verify P_B cryptographically (passes - signatures are valid)
   - Call `insert_proof()` which inserts P_B into their `batch_proof_queue`
   - **No check is performed to verify they have batch B locally**
7. When one of these f validators becomes proposer:
   - They call `pull_proofs()` which retrieves P_B from their queue
   - They create a proposal including proof P_B
   - **The proposal now references batch B that they don't have**
8. During block execution:
   - `get_transactions()` is called to extract transaction data
   - For proof P_B, `get_batch()` attempts local lookup which fails
   - System falls back to `request_batch()` for remote fetching [3](#0-2) 

9. If remote fetching fails due to:
   - Network delays or partitions
   - Byzantine signers refusing to respond
   - Batch expiration and deletion between proof broadcast and execution
   
   Then `request_batch()` times out and returns `ExecutorError::CouldNotGetData` [4](#0-3) 

**Breaking System Assumptions:**

The `check_payload_availability()` method contains a comment stating that proofs "guarantee network availability": [5](#0-4) 

This assumption is violated when validators accept proofs for batches they don't have, as network availability is only guaranteed if validators can fetch from the signers, which may fail.

## Impact Explanation

**Severity: High** - Significant protocol violations per Aptos bug bounty criteria.

**Protocol Invariant Violation:**
- Honest validators should only propose blocks containing data they can verify and execute locally
- This vulnerability allows validators to propose blocks referencing data they've never received
- Violates the principle of local data availability before proposal

**Concrete Impact:**

1. **Execution Dependency Risk:** Validators become dependent on remote fetching for block execution, introducing unnecessary failure points in the critical execution path

2. **Liveness Degradation:** Byzantine validators can orchestrate scenarios where:
   - Multiple batches are distributed to minimum quorums only
   - Proofs are broadcast network-wide
   - Validators without batches experience execution delays or failures when becoming proposers
   - Cumulative delays can slow consensus progression

3. **Resource Exhaustion:** Repeated remote batch fetches consume network bandwidth and increase latency, particularly problematic under heavy load

4. **Attack Amplification:** A single Byzantine validator can affect multiple honest validators by creating many batches with selective distribution patterns

This does not constitute a **Critical** severity issue because:
- It doesn't directly break consensus safety (no forks or double-spends)
- Consensus can still progress if 2f+1 validators can execute blocks
- No direct fund loss occurs

However, it represents a **significant protocol violation** enabling practical liveness degradation attacks.

## Likelihood Explanation

**Likelihood: Medium to High**

**Ease of Exploitation:**
- Requires Byzantine validator access (threshold of < 1/3 validator stake)
- Attack is straightforward to execute with standard network capabilities
- No cryptographic breaks required

**Detection Difficulty:**
- Validators experiencing remote fetch delays may attribute to normal network conditions
- No obvious consensus failure symptoms
- Metrics might show increased `MISSED_BATCHES_COUNT` but root cause unclear

**Real-World Constraints:**
- Batch expiration limits attack duration (proofs expire with batches)
- Effective quorum size means attack impacts only f validators at most
- System designed with redundancy makes complete liveness failure difficult

**Probability in Practice:**
Given that Byzantine validators are an expected threat model in BFT systems and this attack requires no sophisticated techniques, the likelihood is **medium to high** if an adversarial validator participates in the network.

## Recommendation

Add batch existence validation in `insert_proof()` to match the validation in `init_proof()`:

```rust
pub(crate) fn insert_proof(&mut self, proof: ProofOfStore<BatchInfoExt>) {
    if proof.expiration() <= self.latest_block_timestamp {
        counters::inc_rejected_pos_count(counters::POS_EXPIRED_LABEL);
        return;
    }
    
    let batch_key = BatchKey::from_info(proof.info());
    
    // ADDED: Verify batch exists locally before accepting proof
    if self.batch_store.exists(proof.digest()).is_none() {
        counters::inc_rejected_pos_count(counters::POS_BATCH_NOT_FOUND_LABEL);
        warn!(
            "Rejecting ProofOfStore for batch {} - batch not found locally",
            proof.digest()
        );
        return;
    }
    
    if self
        .items
        .get(&batch_key)
        .is_some_and(|item| item.proof.is_some() || item.is_committed())
    {
        counters::inc_rejected_pos_count(counters::POS_DUPLICATE_LABEL);
        return;
    }
    
    // ... rest of insertion logic
}
```

**Additional Recommendations:**

1. Add batch_store reference to BatchProofQueue constructor
2. Create new counter metric `POS_BATCH_NOT_FOUND_LABEL` for monitoring
3. Consider logging validators that send proofs for batches not locally available
4. Document the invariant that proofs should only be accepted if the batch is available

## Proof of Concept

```rust
// Rust integration test demonstrating the vulnerability

#[tokio::test]
async fn test_proof_insertion_without_batch() {
    // Setup: Create 4 validators (f=1, need 3 for quorum)
    let (mut env, validators) = create_test_environment(4).await;
    let byzantine_validator = validators[0];
    let honest_validators = &validators[1..];
    
    // Step 1: Byzantine validator creates batch
    let batch_info = BatchInfoExt::new_v2(
        byzantine_validator.peer_id(),
        BatchId::new(1),
        1, // epoch
        current_time() + 5_000_000, // expiration
        HashValue::random(), // digest (fake, batch doesn't exist)
        100, // num_txns
        1000, // num_bytes
        0, // gas_bucket
        BatchKind::Normal,
    );
    
    // Step 2: Send batch to only 3 validators (minimum quorum)
    // Deliberately exclude honest_validators[2]
    byzantine_validator.send_batch_to(&validators[0..3], batch_info.clone()).await;
    
    // Step 3: Collect signatures from those 3 validators
    let mut signatures = vec![];
    for v in &validators[0..3] {
        let signed = v.sign_batch_info(&batch_info);
        signatures.push(signed);
    }
    
    // Step 4: Create valid ProofOfStore with 3 signatures
    let aggregator = SignatureAggregator::new(batch_info.clone());
    for sig in &signatures {
        aggregator.add_signature(sig.signer(), sig.signature());
    }
    let (batch_info_final, agg_sig) = aggregator.aggregate_and_verify(&validator_verifier).unwrap();
    let proof = ProofOfStore::new(batch_info_final, agg_sig);
    
    // Step 5: Broadcast ProofOfStoreMsg to ALL validators
    let proof_msg = ProofOfStoreMsg::new(vec![proof.clone()]);
    byzantine_validator.broadcast_proof(proof_msg).await;
    
    // Step 6: Verify that honest_validators[2] accepts the proof WITHOUT having the batch
    let validator_3 = &honest_validators[2];
    
    // Verify batch doesn't exist locally for validator_3
    assert!(validator_3.batch_store.exists(batch_info.digest()).is_none());
    
    // Verify proof was inserted into queue (THIS IS THE VULNERABILITY)
    let proof_queue = validator_3.get_batch_proof_queue();
    assert!(proof_queue.has_proof_for_batch(batch_info.digest()));
    
    // Step 7: Make validator_3 the proposer and create proposal
    validator_3.set_as_proposer().await;
    let proposal = validator_3.create_proposal().await;
    
    // Verify proposal includes the proof for batch validator_3 doesn't have
    assert!(proposal.contains_proof_for_batch(batch_info.digest()));
    
    // Step 8: Try to execute - will require remote fetch
    let execution_result = validator_3.execute_block(&proposal.block).await;
    
    // Execution will either:
    // - Succeed after slow remote fetch (if signers respond)
    // - Fail with CouldNotGetData (if fetch times out)
    // Both outcomes demonstrate the vulnerability: validator proposed data it didn't have
}
```

**Notes:**
- The cryptographic verification of ProofOfStore is sound - I found NO way to create "fake" or "invalid" quorum certificates that pass validation
- The vulnerability is specifically in accepting valid proofs without checking local batch availability
- This represents a deviation from the principle that validators should only propose data they can immediately verify

### Citations

**File:** consensus/src/quorum_store/proof_coordinator.rs (L269-283)
```rust
    fn init_proof(
        &mut self,
        signed_batch_info: &SignedBatchInfo<BatchInfoExt>,
    ) -> Result<(), SignedBatchInfoError> {
        // Check if the signed digest corresponding to our batch
        if signed_batch_info.author() != self.peer_id {
            return Err(SignedBatchInfoError::WrongAuthor);
        }
        let batch_author = self
            .batch_reader
            .exists(signed_batch_info.digest())
            .ok_or(SignedBatchInfoError::NotFound)?;
        if batch_author != signed_batch_info.author() {
            return Err(SignedBatchInfoError::WrongAuthor);
        }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L175-210)
```rust
    pub(crate) fn insert_proof(&mut self, proof: ProofOfStore<BatchInfoExt>) {
        if proof.expiration() <= self.latest_block_timestamp {
            counters::inc_rejected_pos_count(counters::POS_EXPIRED_LABEL);
            return;
        }
        let batch_key = BatchKey::from_info(proof.info());
        if self
            .items
            .get(&batch_key)
            .is_some_and(|item| item.proof.is_some() || item.is_committed())
        {
            counters::inc_rejected_pos_count(counters::POS_DUPLICATE_LABEL);
            return;
        }

        let author = proof.author();
        let bucket = proof.gas_bucket_start();
        let num_txns = proof.num_txns();
        let expiration = proof.expiration();

        let batch_sort_key = BatchSortKey::from_info(proof.info());
        let batches_for_author = self.author_to_batches.entry(author).or_default();
        batches_for_author.insert(batch_sort_key.clone(), proof.info().clone());

        // Check if a batch with a higher batch Id (reverse sorted) exists
        if let Some((prev_batch_key, _)) = batches_for_author
            .range((Bound::Unbounded, Bound::Excluded(batch_sort_key.clone())))
            .next_back()
        {
            if prev_batch_key.gas_bucket_start() == batch_sort_key.gas_bucket_start() {
                counters::PROOF_MANAGER_OUT_OF_ORDER_PROOF_INSERTION
                    .with_label_values(&[author.short_str().as_str()])
                    .inc();
            }
        }

```

**File:** consensus/src/quorum_store/batch_store.rs (L663-723)
```rust
    fn get_or_fetch_batch(
        &self,
        batch_info: BatchInfo,
        responders: Vec<PeerId>,
    ) -> Shared<Pin<Box<dyn Future<Output = ExecutorResult<Vec<SignedTransaction>>> + Send>>> {
        let mut responders = responders.into_iter().collect();

        self.inflight_fetch_requests
            .lock()
            .entry(*batch_info.digest())
            .and_modify(|fetch_unit| {
                fetch_unit.responders.lock().append(&mut responders);
            })
            .or_insert_with(|| {
                let responders = Arc::new(Mutex::new(responders));
                let responders_clone = responders.clone();

                let inflight_requests_clone = self.inflight_fetch_requests.clone();
                let batch_store = self.batch_store.clone();
                let requester = self.batch_requester.clone();

                let fut = async move {
                    let batch_digest = *batch_info.digest();
                    defer!({
                        inflight_requests_clone.lock().remove(&batch_digest);
                    });
                    // TODO(ibalajiarun): Support V2 batch
                    if let Ok(mut value) = batch_store.get_batch_from_local(&batch_digest) {
                        Ok(value.take_payload().expect("Must have payload"))
                    } else {
                        // Quorum store metrics
                        counters::MISSED_BATCHES_COUNT.inc();
                        let subscriber_rx = batch_store.subscribe(*batch_info.digest());
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
                            .await?;
                        batch_store.persist(vec![PersistedValue::new(
                            batch_info.into(),
                            Some(payload.clone()),
                        )]);
                        Ok(payload)
                    }
                }
                .boxed()
                .shared();

                tokio::spawn(fut.clone());

                BatchFetchUnit {
                    responders: responders_clone,
                    fut,
                }
            })
            .fut
            .clone()
    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L176-178)
```rust
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L405-407)
```rust
                // The payload is considered available because it contains only proofs that guarantee network availabiliy
                // or inlined transactions.
                Ok(())
```
