# Audit Report

## Title
Transaction Commit Hooks Execute Out of Order During Parallel Post-Commit Processing in BlockSTM

## Summary
In Aptos BlockSTM parallel execution engine (both V1 and V2), transaction commit hooks can be invoked out of sequence during the post-commit materialization phase, violating the ordering guarantee that sequential execution provides. While transactions are committed sequentially, the subsequent materialization and hook invocation happens in parallel across multiple workers, allowing faster-processing transactions to invoke their hooks before slower ones with lower indices.

## Finding Description

The vulnerability exists in the parallel execution path of the block executor. The execution follows two distinct phases:

**Phase 1: Sequential Commit (Ordered)** [1](#0-0) 

Workers acquire the `queueing_commits_lock` and sequentially call `prepare_and_queue_commit_ready_txn`, which validates delayed fields, publishes modules, and adds transactions to the post-commit processing queue in strict order. [2](#0-1) 

The `start_commit` method enforces sequential ordering via `commit_marker_invariant_check`, ensuring transaction i cannot start committing until transaction i-1 is fully committed. [3](#0-2) 

**Phase 2: Parallel Post-Commit Processing (UNORDERED)** [4](#0-3) 

After sequential commit completes, multiple workers concurrently process `PostCommitProcessing` tasks. Each worker:
1. Pops a transaction index from the post-commit queue
2. Materializes the transaction output (varying complexity)
3. Calls `record_finalized_output` which invokes the commit hook [5](#0-4) 

The `record_finalized_output` method has **NO synchronization** to enforce ordering of hook invocations. The hook is called at line 1278 without any lock or ordering check. [6](#0-5) 

Workers simply pop from the concurrent queue - whichever worker finishes materialization first will invoke its hook first, regardless of transaction index.

**Exploitation Scenario:**
- Transaction 100 requires complex group materialization (slow)
- Transaction 101 requires simple materialization (fast)  
- Both committed sequentially and added to post_commit_processing_queue in order
- Worker A picks txn 100, starts slow materialization
- Worker B picks txn 101, starts fast materialization
- Worker B finishes first: `on_transaction_committed(101, ...)` executes
- Worker A finishes later: `on_transaction_committed(100, ...)` executes
- **Result: Hook for txn 101 executed BEFORE hook for txn 100**

For the `CrossShardCommitSender` implementation: [7](#0-6) 

This sends cross-shard state updates that could arrive out of order at dependent shards, violating causal ordering guarantees.

**Contrast with Sequential Execution:** [8](#0-7) 

Sequential execution calls the hook inline, synchronously, with guaranteed ordering - no parallelism or async operations.

## Impact Explanation

**Severity: Medium** ($10,000 range per Aptos bug bounty)

This qualifies as "State inconsistencies requiring intervention" because:

1. **Does NOT affect single-shard consensus safety**: Transaction outputs are correctly ordered in `final_results`, state roots remain deterministic
2. **Breaks sharded execution guarantees**: Cross-shard messages sent out of order can cause dependent shards to receive updates in wrong sequence, requiring manual intervention to resolve state divergence
3. **Violates documented ordering assumptions**: Hook implementations may reasonably expect sequential ordering, especially given that sequential execution provides it

This is NOT Critical/High because:
- No direct loss of funds in single-shard operation
- No consensus safety violation in primary execution path
- Core blockchain state remains consistent
- Only affects cross-shard coordination and external hook consumers

## Likelihood Explanation

**Likelihood: High**

This race condition will occur frequently in production:
- Triggers naturally during normal parallel execution (no malicious input required)
- Transaction materialization complexity varies significantly (resource groups vs simple writes)
- Multiple workers processing post-commit tasks concurrently
- More likely with higher worker counts and transaction variety

The issue is deterministically present in the code architecture - it's not a timing-dependent race but a fundamental lack of ordering enforcement.

## Recommendation

**Solution: Serialize hook invocations in post-commit processing**

Add sequential hook invocation before parallel materialization:

```rust
// In worker_loop_v2, after sequential commit hooks:
while scheduler.commit_hooks_try_lock() {
    while let Some((txn_idx, incarnation)) = scheduler.start_commit()? {
        self.prepare_and_queue_commit_ready_txn(...)?;
        
        // NEW: Invoke hook sequentially before parallel materialization
        if let Some(txn_commit_listener) = &self.transaction_commit_hook {
            last_input_output.notify_listener(txn_idx, txn_commit_listener)?;
        }
        
        scheduler.end_commit(txn_idx)?;
    }
    scheduler.commit_hooks_unlock();
}

// Then remove hook invocation from record_finalized_output
fn record_finalized_output(...) {
    // Remove lines 1277-1279 - hook already called during sequential phase
    let mut final_results = shared_sync_params.final_results.acquire();
    final_results[output_idx as usize] = last_input_output.take_output(txn_idx)?;
    Ok(())
}
```

This ensures hooks execute sequentially while still allowing parallel materialization for performance.

## Proof of Concept

```rust
// Reproduction test demonstrating out-of-order hook invocation
use aptos_block_executor::BlockExecutor;
use std::sync::{Arc, Mutex};
use std::collections::VecDeque;

struct OrderRecordingHook {
    invocations: Arc<Mutex<VecDeque<TxnIndex>>>,
}

impl TransactionCommitHook for OrderRecordingHook {
    fn on_transaction_committed(&self, txn_idx: TxnIndex, _output: &OnceCell<TransactionOutput>) {
        self.invocations.lock().unwrap().push_back(txn_idx);
    }
    fn on_execution_aborted(&self, _txn_idx: TxnIndex) {}
}

#[test]
fn test_parallel_hook_ordering_violation() {
    let invocations = Arc::new(Mutex::new(VecDeque::new()));
    let hook = OrderRecordingHook { invocations: invocations.clone() };
    
    // Execute block with transactions of varying materialization complexity
    // Transaction at even indices: complex group operations (slow)
    // Transaction at odd indices: simple writes (fast)
    let block = create_test_block_with_varying_complexity(100);
    
    let executor = BlockExecutor::new(
        BlockExecutorConfig { concurrency_level: 8, blockstm_v2: true, ... },
        executor_thread_pool,
        Some(hook),
    );
    
    executor.execute_block(&block, ...);
    
    // Verify hook invocation order
    let recorded = invocations.lock().unwrap();
    for i in 1..recorded.len() {
        // FAILS: Some hooks invoked out of order
        assert!(recorded[i-1] < recorded[i], 
            "Hook for txn {} invoked before txn {}", recorded[i], recorded[i-1]);
    }
}
```

**Expected Result**: Test fails, demonstrating that `on_transaction_committed` for transaction N+1 can be called before transaction N during parallel post-commit processing.

## Notes

The vulnerability is specific to parallel execution paths (BlockSTM/BlockSTMv2). Sequential execution maintains proper ordering as hooks are invoked inline. The issue affects both V1 and V2 schedulers identically since both use concurrent queues for post-commit processing without ordering enforcement during hook invocation.

### Citations

**File:** aptos-move/block-executor/src/executor.rs (L1263-1285)
```rust
    fn record_finalized_output(
        &self,
        txn_idx: TxnIndex,
        output_idx: TxnIndex,
        shared_sync_params: &SharedSyncParams<T, E, S>,
    ) -> Result<(), PanicError> {
        if output_idx < txn_idx {
            return Err(code_invariant_error(format!(
                "Index to record finalized output {} is less than txn index {}",
                output_idx, txn_idx
            )));
        }

        let last_input_output = shared_sync_params.last_input_output;
        if let Some(txn_commit_listener) = &self.transaction_commit_hook {
            last_input_output.notify_listener(txn_idx, txn_commit_listener)?;
        }

        let mut final_results = shared_sync_params.final_results.acquire();

        final_results[output_idx as usize] = last_input_output.take_output(txn_idx)?;
        Ok(())
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L1455-1472)
```rust
            while scheduler.commit_hooks_try_lock() {
                // Perform sequential commit hooks.
                while let Some((txn_idx, incarnation)) = scheduler.start_commit()? {
                    self.prepare_and_queue_commit_ready_txn(
                        txn_idx,
                        incarnation,
                        num_txns,
                        executor,
                        block,
                        num_workers as usize,
                        runtime_environment,
                        scheduler_wrapper,
                        shared_sync_params,
                    )?;
                }

                scheduler.commit_hooks_unlock();
            }
```

**File:** aptos-move/block-executor/src/executor.rs (L1507-1515)
```rust
                TaskKind::PostCommitProcessing(txn_idx) => {
                    self.materialize_txn_commit(
                        txn_idx,
                        scheduler_wrapper,
                        environment,
                        shared_sync_params,
                    )?;
                    self.record_finalized_output(txn_idx, txn_idx, shared_sync_params)?;
                },
```

**File:** aptos-move/block-executor/src/executor.rs (L2493-2497)
```rust
                    if let Some(commit_hook) = &self.transaction_commit_hook {
                        commit_hook
                            .on_transaction_committed(idx as TxnIndex, output.committed_output());
                    }
                    ret.push(output);
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L606-680)
```rust
    pub(crate) fn start_commit(&self) -> Result<Option<(TxnIndex, Incarnation)>, PanicError> {
        // Relaxed ordering due to armed lock acq-rel.
        let next_to_commit_idx = self.next_to_commit_idx.load(Ordering::Relaxed);
        assert!(next_to_commit_idx <= self.num_txns);

        if self.is_halted() || next_to_commit_idx == self.num_txns {
            // All sequential commit hooks are already dispatched.
            return Ok(None);
        }

        let incarnation = self.txn_statuses.incarnation(next_to_commit_idx);
        if self.txn_statuses.is_executed(next_to_commit_idx) {
            self.commit_marker_invariant_check(next_to_commit_idx)?;

            // All prior transactions are committed and the latest incarnation of the transaction
            // at next_to_commit_idx has finished but has not been aborted. If any of its reads was
            // incorrect, it would have been invalidated by the respective transaction's last
            // (committed) (re-)execution, and led to an abort in the corresponding finish execution
            // (which, inductively, must occur before the transaction is committed). Hence, it
            // must also be safe to commit the current transaction.
            //
            // The only exception is if there are unsatisfied cold validation requirements,
            // blocking the commit. These may not yet be scheduled for validation, or deferred
            // until after the txn finished execution, whereby deferral happens before txn status
            // becomes Executed, while validation and unblocking happens after.
            if self
                .cold_validation_requirements
                .is_commit_blocked(next_to_commit_idx, incarnation)
            {
                // May not commit a txn with an unsatisfied validation requirement. This will be
                // more rare than !is_executed in the common case, hence the order of checks.
                return Ok(None);
            }
            // The check might have passed after the validation requirement has been fulfilled.
            // Yet, if validation failed, the status would be aborted before removing the block,
            // which would increase the incarnation number. It is also important to note that
            // blocking happens during sequential commit hook, while holding the lock (which is
            // also held here), hence before the call of this method.
            if incarnation != self.txn_statuses.incarnation(next_to_commit_idx) {
                return Ok(None);
            }

            if self
                .committed_marker
                .get(next_to_commit_idx as usize)
                .is_some_and(|marker| {
                    marker.swap(CommitMarkerFlag::CommitStarted as u8, Ordering::Relaxed)
                        != CommitMarkerFlag::NotCommitted as u8
                })
            {
                return Err(code_invariant_error(format!(
                    "Marking {} as PENDING_COMMIT_HOOK, but previous marker != NOT_COMMITTED",
                    next_to_commit_idx
                )));
            }

            // TODO(BlockSTMv2): fetch_add as a RMW instruction causes a barrier even with
            // Relaxed ordering. The read is only used to check an invariant, so we can
            // eventually change to just a relaxed write.
            let prev_idx = self.next_to_commit_idx.fetch_add(1, Ordering::Relaxed);
            if prev_idx != next_to_commit_idx {
                return Err(code_invariant_error(format!(
                    "Scheduler committing {}, stored next to commit idx = {}",
                    next_to_commit_idx, prev_idx
                )));
            }

            return Ok(Some((
                next_to_commit_idx,
                self.txn_statuses.incarnation(next_to_commit_idx),
            )));
        }

        Ok(None)
    }
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L807-810)
```rust
        match self.pop_post_commit_task()? {
            Some(txn_idx) => {
                return Ok(TaskKind::PostCommitProcessing(txn_idx));
            },
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L1160-1175)
```rust
    fn commit_marker_invariant_check(
        &self,
        next_to_commit_idx: TxnIndex,
    ) -> Result<(), PanicError> {
        if next_to_commit_idx > 0 {
            let prev_committed_marker =
                self.committed_marker[next_to_commit_idx as usize - 1].load(Ordering::Relaxed);
            if prev_committed_marker != CommitMarkerFlag::Committed as u8 {
                return Err(code_invariant_error(format!(
                    "Trying to get commit hook for {}, but previous index marker {} != {} (COMMITTED)",
                    next_to_commit_idx, prev_committed_marker, CommitMarkerFlag::Committed as u8,
                )));
            };
        }
        Ok(())
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L137-147)
```rust
impl TransactionCommitHook for CrossShardCommitSender {
    fn on_transaction_committed(
        &self,
        txn_idx: TxnIndex,
        txn_output: &OnceCell<TransactionOutput>,
    ) {
        let global_txn_idx = txn_idx + self.index_offset;
        if self.dependent_edges.contains_key(&global_txn_idx) {
            self.send_remote_update_for_success(global_txn_idx, txn_output);
        }
    }
```
