# Audit Report

## Title
Byzantine Validators Can Cause Permanent Liveness Failure Through Strategic Peer Score Manipulation

## Summary
A colluding minority of Byzantine validators (<1/3) can cause honest validators to permanently halt synchronization by strategically advertising but failing to serve critical epoch-ending ledger infos. This causes Byzantine validators' peer scores to drop below the ignore threshold, excluding them from the global data summary. Once ignored, if they were the only peers with the required data, honest validators cannot create streams and remain stuck indefinitely.

## Finding Description

The vulnerability exploits the interaction between the peer scoring system and global data summary aggregation in Aptos state sync.

**Core Issue**: The `calculate_global_data_summary` function excludes peers whose scores have fallen below the `IGNORE_PEER_THRESHOLD` (25.0) from the aggregated global data summary. [1](#0-0) 

When a peer is ignored, `get_storage_summary_if_not_ignored` returns `None`, preventing their advertised data from being included in network-wide data availability calculations. [2](#0-1) 

The ignore mechanism is triggered when a peer's score drops to or below the threshold, with `ignore_low_score_peers` enabled by default. [3](#0-2) [4](#0-3) 

**Attack Execution**:

1. **Setup**: Byzantine validators (<1/3) are the only peers currently holding a specific epoch-ending ledger info needed by an honest validator to make progress. This can occur during network partitions or when honest validators are syncing from an earlier state.

2. **Advertisement**: Byzantine validators include the epoch-ending ledger info in their storage summaries, making it appear in the global data summary aggregation. [5](#0-4) 

3. **Stream Creation**: The victim validator's `fetch_epoch_ending_ledger_infos` function successfully creates an `EpochEndingStreamEngine` because the advertised data contains the required epoch range. [6](#0-5) 

4. **Strategic Failure**: When the victim sends requests for the data, Byzantine validators return errors (timeouts, invalid responses, etc.), causing each peer's score to be multiplied by `NOT_USEFUL_MULTIPLIER` (0.95). [7](#0-6) [8](#0-7) 

5. **Score Degradation**: After approximately 66 consecutive failures, Byzantine validators' scores drop below the `IGNORE_PEER_THRESHOLD` of 25.0 (starting from 50.0: 50.0 × 0.95^66 ≈ 24.8).

6. **Exclusion from Summary**: Once ignored, Byzantine validators are excluded from the global data summary calculation, removing all advertised epoch-ending ledger infos from the network-wide view. [9](#0-8) 

7. **Stream Creation Failure**: On the next `drive_progress` iteration, `EpochEndingStreamEngine::new` returns `Error::DataIsUnavailable` because the advertised data no longer contains any epoch-ending ledger infos. [10](#0-9) 

8. **Permanent Halt**: The bootstrapper repeatedly fails to create streams, and the driver enters an infinite retry loop without resolution. [11](#0-10) 

The retry limit (`max_request_retry` = 5) only applies to in-flight stream requests, not to stream creation attempts. [12](#0-11) [13](#0-12) 

**No Recovery Mechanism**: There is no automatic score recovery for ignored peers. Scores only increase through successful responses, but ignored peers are excluded from request routing. [14](#0-13) 

## Impact Explanation

**Critical Severity** - This vulnerability causes **Total loss of liveness/network availability** as defined in the Aptos Bug Bounty program.

**Affected Systems**:
- Validators attempting to bootstrap or sync across epoch boundaries
- Nodes recovering from extended downtime
- Validators joining the network for the first time

**Concrete Impact**:
1. **Permanent Synchronization Halt**: Affected validators cannot progress past the epoch boundary and remain stuck indefinitely
2. **Network Availability Degradation**: If multiple validators are affected simultaneously, network availability and consensus participation decrease
3. **Validator Set Disruption**: New validators cannot join the network, and recovering validators cannot resume participation
4. **No Automatic Recovery**: Manual intervention (node restart or peer reconnection) is required to restore functionality

The attack violates the **Liveness** invariant: the system cannot guarantee that honest nodes eventually make progress when Byzantine validators control < 1/3 of the network.

## Likelihood Explanation

**Likelihood: Medium-High**

**Required Conditions**:
1. Byzantine validators (<1/3) collude
2. Critical epoch-ending ledger info is temporarily available only on Byzantine validators
3. Honest validators need this specific data to make progress

**Attack Feasibility**:
- **Collusion**: Requires coordinated behavior among Byzantine validators, but only a minority (<1/3) is needed
- **Data Exclusivity**: Can occur naturally during network partitions, epoch transitions, or when honest validators are syncing from earlier states
- **Detection**: The attack is difficult to detect as it appears as normal network failures initially
- **Cost**: Low cost for attackers - only requires failing requests, not computational resources

**Real-World Scenarios**:
1. **New Validator Onboarding**: New validators joining the network must sync epoch-ending ledger infos from existing peers
2. **Post-Downtime Recovery**: Validators recovering from extended downtime need to sync missed epochs
3. **Network Partitions**: During network splits, some validators may only have access to Byzantine peers for specific data ranges
4. **Epoch Boundaries**: Critical epoch-ending ledger infos are required at epoch transitions

## Recommendation

**Immediate Mitigations**:

1. **Implement Data Availability Fallback**: When all peers advertising required data are ignored, temporarily bypass the ignore mechanism for critical data types (epoch-ending ledger infos).

2. **Add Score Recovery Mechanism**: Implement time-based score recovery that gradually increases ignored peers' scores, allowing them to be reconsidered after a timeout period.

3. **Diversify Data Sources**: Ensure honest validators maintain and advertise epoch-ending ledger infos persistently, reducing dependency on any specific peer subset.

**Proposed Code Fix** (state-sync/aptos-data-client/src/peer_states.rs):

```rust
pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
    // First attempt: gather summaries excluding ignored peers
    let storage_summaries: Vec<StorageServerSummary> = self
        .peer_to_state
        .iter()
        .filter_map(|peer_state| {
            peer_state
                .value()
                .get_storage_summary_if_not_ignored()
                .cloned()
        })
        .collect();

    // If we have summaries, proceed normally
    if !storage_summaries.is_empty() {
        return self.calculate_summary_from_peers(storage_summaries);
    }

    // Fallback: include ALL peers (even ignored) for critical data availability
    let all_storage_summaries: Vec<StorageServerSummary> = self
        .peer_to_state
        .iter()
        .filter_map(|peer_state| peer_state.value().get_storage_summary().cloned())
        .collect();

    if all_storage_summaries.is_empty() {
        return GlobalDataSummary::empty();
    }

    warn!("Using ignored peers for data summary due to no non-ignored peers available");
    self.calculate_summary_from_peers(all_storage_summaries)
}
```

**Long-Term Solutions**:
1. Implement periodic score resets or gradual score increases for all peers
2. Add separate tracking for critical data types (epoch-ending ledger infos) with different ignore thresholds
3. Implement peer rotation strategies to prevent prolonged dependency on specific peer subsets
4. Add monitoring and alerting for scenarios where all peers with required data are ignored

## Proof of Concept

The vulnerability can be demonstrated through the following test scenario:

**Setup**:
1. Configure a test network with 4 validators (1 Byzantine, 3 honest)
2. Byzantine validator holds epoch-ending ledger info for epoch N
3. Honest validators are at epoch N-1 and need the epoch-ending ledger info to progress

**Execution**:
1. Byzantine validator advertises epoch-ending ledger info in storage summary
2. Honest validator V1 attempts to sync:
   - Calls `fetch_epoch_ending_ledger_infos` with start_epoch = N
   - Stream created successfully (data advertised)
   - Sends requests to Byzantine validator
3. Byzantine validator returns `RpcError::TimedOut` for all requests
4. After 66 failed requests, Byzantine validator's score drops below 25.0
5. Byzantine validator is marked as ignored
6. V1's next `drive_progress` call fails:
   - `calculate_global_data_summary` excludes Byzantine validator
   - Global summary has no epoch-ending ledger infos for epoch N
   - `EpochEndingStreamEngine::new` returns `Error::DataIsUnavailable`
7. V1 enters infinite retry loop, unable to make progress

**Validation**:
- Monitor peer scores: Verify Byzantine validator score drops below 25.0
- Check global data summary: Confirm epoch-ending ledger infos disappear
- Observe stream creation failures: Verify `Error::DataIsUnavailable` is returned
- Confirm no recovery: Validate that V1 remains stuck without manual intervention

**Notes**

This vulnerability represents a critical gap in the state sync liveness guarantees. While the peer scoring system correctly identifies and isolates misbehaving peers, it creates a situation where critical data can become completely unavailable if all peers holding that data are ignored. The lack of a recovery mechanism or fallback strategy means affected nodes require manual intervention to resume operation.

The attack is particularly concerning because:
1. It requires only a minority (<1/3) of Byzantine validators
2. It can occur during normal network operations (epoch transitions, new validator onboarding)
3. There is no automatic detection or recovery mechanism
4. The impact is permanent until manual intervention

The recommended fixes balance security (maintaining peer scoring for misbehavior detection) with availability (ensuring critical data remains accessible even from low-scored peers when necessary).

### Citations

**File:** state-sync/aptos-data-client/src/peer_states.rs (L143-149)
```rust
    pub fn get_storage_summary_if_not_ignored(&self) -> Option<&StorageServerSummary> {
        if self.is_ignored() {
            None
        } else {
            self.storage_summary.as_ref()
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L152-160)
```rust
    fn is_ignored(&self) -> bool {
        // Only ignore peers if the config allows it
        if !self.data_client_config.ignore_low_score_peers {
            return false;
        }

        // Otherwise, ignore peers with a low score
        self.score <= IGNORE_PEER_THRESHOLD
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L163-164)
```rust
    fn update_score_success(&mut self) {
        self.score = f64::min(self.score + SUCCESSFUL_RESPONSE_DELTA, MAX_SCORE);
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L168-174)
```rust
    fn update_score_error(&mut self, error: ErrorType) {
        let multiplier = match error {
            ErrorType::NotUseful => NOT_USEFUL_MULTIPLIER,
            ErrorType::Malicious => MALICIOUS_MULTIPLIER,
        };
        self.score = f64::max(self.score * multiplier, MIN_SCORE);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L339-354)
```rust
    pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
        // Gather all storage summaries, but exclude peers that are ignored
        let storage_summaries: Vec<StorageServerSummary> = self
            .peer_to_state
            .iter()
            .filter_map(|peer_state| {
                peer_state
                    .value()
                    .get_storage_summary_if_not_ignored()
                    .cloned()
            })
            .collect();

        // If we have no peers, return an empty global summary
        if storage_summaries.is_empty() {
            return GlobalDataSummary::empty();
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L365-369)
```rust
            if let Some(epoch_ending_ledger_infos) = summary.data_summary.epoch_ending_ledger_infos
            {
                advertised_data
                    .epoch_ending_ledger_infos
                    .push(epoch_ending_ledger_infos);
```

**File:** config/src/config/state_sync_config.rs (L256-256)
```rust
    pub max_request_retry: u64,
```

**File:** config/src/config/state_sync_config.rs (L466-466)
```rust
            ignore_low_score_peers: true,
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L856-860)
```rust
            let epoch_ending_stream = self
                .streaming_client
                .get_all_epoch_ending_ledger_infos(next_epoch_end)
                .await?;
            self.active_data_stream = Some(epoch_ending_stream);
```

**File:** state-sync/aptos-data-client/src/client.rs (L865-865)
```rust
                self.notify_bad_response(id, peer, &request, ErrorType::NotUseful);
```

**File:** state-sync/data-streaming-service/src/stream_engine.rs (L1487-1494)
```rust
        let end_epoch = advertised_data
            .highest_epoch_ending_ledger_info()
            .ok_or_else(|| {
                Error::DataIsUnavailable(format!(
                    "Unable to find any epoch ending ledger info in the network: {:?}",
                    advertised_data
                ))
            })?;
```

**File:** state-sync/state-sync-driver/src/driver.rs (L711-719)
```rust
        } else if let Err(error) = self.bootstrapper.drive_progress(&global_data_summary).await {
            sample!(
                    SampleRate::Duration(Duration::from_secs(DRIVER_ERROR_LOG_FREQ_SECS)),
                    warn!(LogSchema::new(LogEntry::Driver)
                        .error(&error)
                        .message("Error found when checking the bootstrapper progress!"));
            );
            metrics::increment_counter(&metrics::BOOTSTRAPPER_ERRORS, error.get_label());
        };
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L446-448)
```rust
        if self.stream_engine.is_stream_complete()
            || self.request_failure_count >= self.streaming_service_config.max_request_retry
            || self.send_failure
```
