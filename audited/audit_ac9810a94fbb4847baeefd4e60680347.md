# Audit Report

## Title
Non-Atomic Parallel Database Commits Enable Cross-Database Version Skew Leading to Consensus Divergence

## Summary
The parallel commit mechanism in AptosDB commits to 7 sub-databases independently without atomic guarantees across all databases. When a node crashes during commit, databases can end up at different versions. The recovery mechanism (truncation) processes each database independently based on its own max version rather than validating consistency across all databases, potentially leaving validators in divergent states.

## Finding Description

The vulnerability exists in two critical code paths:

**1. Parallel Commit Without Atomicity:** [1](#0-0) 

The `calculate_and_commit_ledger_and_state_kv` function spawns 7 parallel threads to commit different databases. Each thread uses `.unwrap()` on failure, causing panics. The explicit TODO acknowledges the lack of per-database progress tracking.

**2. Non-Atomic Sequential Writes in Recovery:** [2](#0-1) 

The `write_schemas` method commits to each sub-database sequentially, not atomically. If a crash occurs during truncation (recovery), only some databases will be cleaned up.

**Attack Scenario:**

1. Validator commits transactions 1000-1010 via parallel commit
2. Thread committing `ledger_metadata_db` writes `LedgerCommitProgress = 1010`
3. Some database commits succeed (event_db, transaction_info_db, write_set_db reach v1010), others fail (transaction_db crashes at v1005 due to disk error)
4. Node crashes from the panic

**At Restart:**
5. `sync_commit_progress` reads `overall_commit_progress = 999`, `ledger_commit_progress = 1010` [3](#0-2) 

6. Truncation is triggered to remove versions 1000-1010 [4](#0-3) 

7. **Critical Issue:** Truncation reads each database's max version independently: [5](#0-4) 

Each database truncates from `start_version` to its own `latest_version`:
- transaction_db: deletes 1000-1005
- event_db: deletes 1000-1010  
- transaction_info_db: deletes 1000-1010

8. The truncation itself commits non-atomically across databases. If another crash occurs during truncation, databases remain in inconsistent states.

**Consensus Divergence:**

Different validators experiencing crashes at different points will have subtly different database states:
- Validator A: transaction_db failed at v1005, event_db succeeded to v1010
- Validator B: event_db failed at v1007, transaction_db succeeded to v1010

After truncation, queries may return different results:
- `get_transaction(1008)` → Success on Validator B, Failure on Validator A
- `get_events(1008)` → Failure on Validator A, Failure on Validator B (if truncation completed)

This breaks the **Deterministic Execution** invariant where all validators must produce identical results for identical queries.

## Impact Explanation

**Critical Severity** - This qualifies as a Consensus/Safety violation:

1. **Consensus Divergence Risk:** Validators with different database states may query different transaction histories, potentially leading to different execution outcomes when validating blocks that depend on historical state lookups.

2. **State Inconsistency:** The storage layer violates the invariant that "State transitions must be atomic and verifiable via Merkle proofs" because databases can be at different versions simultaneously.

3. **Non-Deterministic Recovery:** The same crash scenario on different nodes produces different post-recovery states depending on which specific database commits succeeded, making the system non-deterministic.

## Likelihood Explanation

**High Likelihood:**

1. **Natural Occurrence:** Node crashes during commit are common (OOM kills, disk failures, power outages, kernel panics)

2. **Parallel Commits:** Every transaction commit uses parallel writes across 7 databases, maximizing exposure window

3. **No Validation:** The TODO explicitly states this is unhandled - there's no cross-database consistency validation at startup: [6](#0-5) 

4. **Cascading Failures:** If truncation itself crashes (secondary failure during recovery), the inconsistency becomes permanent

## Recommendation

Implement per-database progress markers and cross-database consistency validation:

```rust
// In aptosdb_writer.rs, calculate_and_commit_ledger_and_state_kv:
// Before parallel commits, write a "commit intent" marker with target version
// After parallel commits succeed, write "commit complete" marker
// Each database should track its own max committed version

// In ledger_db/mod.rs, after line 281:
pub fn validate_database_consistency(&self) -> Result<()> {
    if !self.enable_storage_sharding {
        return Ok(());
    }
    
    // Get max version from each database
    let event_max = get_latest_version_for_schema::<EventSchema>(self.event_db.db())?;
    let txn_max = get_latest_version_for_schema::<TransactionSchema>(self.transaction_db.db())?;
    let txn_info_max = get_latest_version_for_schema::<TransactionInfoSchema>(self.transaction_info_db.db())?;
    let write_set_max = get_latest_version_for_schema::<WriteSetSchema>(self.write_set_db.db())?;
    
    // All databases must be within MAX_COMMIT_PROGRESS_DIFFERENCE of each other
    let versions = [event_max, txn_max, txn_info_max, write_set_max]
        .into_iter()
        .flatten()
        .collect::<Vec<_>>();
    
    if let (Some(&min_v), Some(&max_v)) = (versions.iter().min(), versions.iter().max()) {
        ensure!(
            max_v - min_v <= MAX_COMMIT_PROGRESS_DIFFERENCE,
            "Database version skew detected: min={} max={} diff={}",
            min_v, max_v, max_v - min_v
        );
    }
    
    Ok(())
}
```

Call this validation immediately after parallel database initialization in `LedgerDb::new()`.

Additionally, make `write_schemas` atomic across all databases by collecting all write batches and committing them in a two-phase commit protocol, or at minimum, validate that all databases are at the expected version before beginning writes.

## Proof of Concept

```rust
// Test to reproduce version skew
#[test]
fn test_partial_commit_creates_version_skew() {
    use std::sync::atomic::{AtomicBool, Ordering};
    use std::sync::Arc;
    
    let tmp_dir = TempPath::new();
    let db = AptosDB::new_for_test_with_sharding(&tmp_dir, 1000);
    
    // Commit some initial transactions
    let (txns, li) = create_test_transactions(100);
    db.save_transactions_for_test(&txns, 0, Some(&li), true).unwrap();
    
    // Simulate crash during next commit by injecting failure in one database
    let crash_flag = Arc::new(AtomicBool::new(false));
    
    // This would require modifying the database to inject failures
    // In practice, simulate by:
    // 1. Starting a commit
    // 2. Manually killing the process mid-commit (kill -9)
    // 3. Restarting and checking database versions
    
    // Verification after crash:
    let (ledger_db, _, _, _) = AptosDB::open_dbs(...);
    
    let event_max = get_latest_version_for_schema::<EventSchema>(
        ledger_db.event_db().db()
    ).unwrap();
    let txn_max = get_latest_version_for_schema::<TransactionSchema>(
        ledger_db.transaction_db().db()
    ).unwrap();
    
    // Assert that databases have different max versions
    assert_ne!(event_max, txn_max, "Version skew not detected!");
}
```

**Notes:**

1. The issue is explicitly acknowledged by the TODO but never addressed
2. The db_debugger tool can detect this (print_db_versions.rs) but it's not run automatically
3. Production validators are at risk whenever crashes occur during high transaction throughput periods
4. The non-atomic truncation during recovery can make the problem worse, not better

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L263-322)
```rust
    fn calculate_and_commit_ledger_and_state_kv(
        &self,
        chunk: &ChunkToCommit,
        skip_index_and_usage: bool,
    ) -> Result<HashValue> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions__work"]);

        let mut new_root_hash = HashValue::zero();
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });

        Ok(new_root_hash)
    }
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L281-281)
```rust
        // TODO(grao): Handle data inconsistency.
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L531-548)
```rust
    pub fn write_schemas(&self, schemas: LedgerDbSchemaBatches) -> Result<()> {
        self.write_set_db
            .write_schemas(schemas.write_set_db_batches)?;
        self.transaction_info_db
            .write_schemas(schemas.transaction_info_db_batches)?;
        self.transaction_db
            .write_schemas(schemas.transaction_db_batches)?;
        self.persisted_auxiliary_info_db
            .write_schemas(schemas.persisted_auxiliary_info_db_batches)?;
        self.event_db.write_schemas(schemas.event_db_batches)?;
        self.transaction_accumulator_db
            .write_schemas(schemas.transaction_accumulator_db_batches)?;
        self.transaction_auxiliary_data_db
            .write_schemas(schemas.transaction_auxiliary_data_db_batches)?;
        // TODO: remove this after sharding migration
        self.ledger_metadata_db
            .write_schemas(schemas.ledger_metadata_db_batches)
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L325-361)
```rust
fn truncate_ledger_db_single_batch(
    ledger_db: &LedgerDb,
    transaction_store: &TransactionStore,
    start_version: Version,
) -> Result<()> {
    let mut batch = LedgerDbSchemaBatches::new();

    delete_transaction_index_data(
        ledger_db,
        transaction_store,
        start_version,
        &mut batch.transaction_db_batches,
    )?;
    delete_per_epoch_data(
        &ledger_db.metadata_db_arc(),
        start_version,
        &mut batch.ledger_metadata_db_batches,
    )?;
    delete_per_version_data(ledger_db, start_version, &mut batch)?;

    delete_event_data(ledger_db, start_version, &mut batch.event_db_batches)?;

    truncate_transaction_accumulator(
        ledger_db.transaction_accumulator_db_raw(),
        start_version,
        &mut batch.transaction_accumulator_db_batches,
    )?;

    let mut progress_batch = SchemaBatch::new();
    progress_batch.put::<DbMetadataSchema>(
        &DbMetadataKey::LedgerCommitProgress,
        &DbMetadataValue::Version(start_version - 1),
    )?;
    ledger_db.metadata_db().write_schemas(progress_batch)?;

    ledger_db.write_schemas(batch)
}
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L494-510)
```rust
fn delete_per_version_data_impl<S>(
    ledger_db: &DB,
    start_version: Version,
    batch: &mut SchemaBatch,
) -> Result<()>
where
    S: Schema<Key = Version>,
{
    let mut iter = ledger_db.iter::<S>()?;
    iter.seek_to_last();
    if let Some((latest_version, _)) = iter.next().transpose()? {
        if latest_version >= start_version {
            info!(
                start_version = start_version,
                latest_version = latest_version,
                cf_name = S::COLUMN_FAMILY_NAME,
                "Truncate per version data."
```
