# Audit Report

## Title
Incomplete Replay Verification for Validator Set Updates in Large Epochs

## Summary
The replay verification job generation in `gen_replay_verify_jobs.rs` creates partial ranges for large epochs that explicitly omit transactions, potentially including critical reconfiguration transactions. This causes validator set update logic (`stake::on_new_epoch()`) to never be verified for these epochs, allowing bugs in voting power calculations, pending validator processing, or reward distributions to go undetected.

## Finding Description

The `run()` function in `gen_replay_verify_jobs.rs` generates verification job ranges based on state snapshots. When an epoch spans more transactions than `max_versions_per_range`, the code creates a "partial" job that covers only the first portion of the epoch and explicitly omits the remaining transactions: [1](#0-0) 

The message clearly states "another X versions omitted, until end.version", meaning these transactions are never replayed or verified by any job.

**The Critical Issue:**

Reconfiguration transactions occur at epoch boundaries. When `reconfiguration::reconfigure()` is called, it invokes `stake::on_new_epoch()` which performs critical validator set updates: [2](#0-1) 

The `on_new_epoch()` function updates the ValidatorSet resource by:
1. Processing pending_active validators to add them to the active set
2. Removing pending_inactive validators  
3. Recalculating voting power for all validators
4. Distributing staking rewards and transaction fees
5. Updating validator indices and performance counters [3](#0-2) 

**How Verification is Bypassed:**

During replay verification, the `ChunkExecutor` detects epoch boundaries by scanning for `new_epoch_event` in the transaction event vectors: [4](#0-3) 

However, if a job's partial range ends BEFORE the epoch boundary (reconfiguration transaction), the reconfiguration logic is in the omitted portion and never gets replayed. The verification compares replayed transaction infos against expected values: [5](#0-4) 

Since the reconfiguration transaction isn't replayed, bugs in `on_new_epoch()` that produce incorrect validator sets, voting power, or reward distributions would not be detected.

**Example Scenario:**
- Large epoch spans versions 1,000,000 to 2,000,000 (e.g., during load testing)
- Reconfiguration transaction at version 2,000,000
- With `max_versions_per_range = 500,000`:
  - Job 1: versions 1,000,000 - 1,499,999 (partial)
  - **Omitted: versions 1,500,000 - 2,000,000** (includes reconfiguration)
  - Job 2: starts at version 2,000,001 (next epoch)

The validator set update at version 2,000,000 is never verified.

## Impact Explanation

This is a **High Severity** issue based on Aptos bug bounty criteria because:

1. **Significant Protocol Violations**: Undetected bugs in validator set updates could lead to:
   - Incorrect voting power calculations affecting consensus
   - Wrong validators being added/removed from the active set
   - Incorrect staking reward distributions
   - Corrupted validator performance tracking

2. **Verification Integrity**: The replay verification system is a critical safeguard for detecting execution bugs. This gap undermines its effectiveness for one of the most critical state transitions (epoch boundaries).

3. **Impact Scope**: All large epochs (common during load testing or high transaction volumes) are affected. The code comment "this hopefully automatically skips load tests" confirms this is expected to occur.

While this is not a direct exploit vulnerability, it represents a significant gap in the verification system that could allow critical consensus and staking bugs to reach production.

## Likelihood Explanation

**Likelihood: High**

The vulnerability manifests whenever:
1. An epoch contains more transactions than `max_versions_per_range` (configurable parameter)
2. During network load tests or periods of high transaction volume
3. The code explicitly creates partial ranges with the message "versions omitted"

The code comment explicitly acknowledges this behavior ("this hopefully automatically skips load tests"), indicating it's an expected and recurring scenario. This is not an edge case but a deliberate design choice that creates a verification gap.

## Recommendation

**Short-term Fix:**

Ensure partial ranges always include epoch boundaries by detecting reconfiguration events when splitting ranges:

```rust
if end.version - begin.version >= self.max_versions_per_range {
    // Find the last reconfiguration event before max_versions_per_range
    // and split there instead of at an arbitrary point
    let split_point = find_last_epoch_boundary_before(
        begin.version,
        begin.version + self.max_versions_per_range,
        metadata_view
    ).unwrap_or(begin.version + self.max_versions_per_range - 1);
    
    Some((
        split_point < begin.version + self.max_versions_per_range - 1, // partial flag
        begin.version,
        split_point,
        format!("Replay epoch {} - {}, {} txns...", ...)
    ))
}
```

**Long-term Fix:**

1. Modify job generation to always include complete epochs in verification jobs
2. Split very large epochs into multiple jobs that each span full sub-epochs (if possible) or ensure critical boundary transactions are always verified
3. Add explicit tracking of which reconfiguration transactions have been verified
4. Consider a separate "epoch boundary verification" job type that specifically verifies all reconfiguration transactions

## Proof of Concept

**Test Setup:**

1. Create a test scenario with a large epoch (>1M transactions)
2. Set `max_versions_per_range = 500000`
3. Run job generation:

```rust
use aptos_db_tool::gen_replay_verify_jobs::Opt;

#[test]
fn test_partial_range_omits_reconfiguration() {
    // Setup: Create metadata with large epoch
    // Epoch 100: versions 1,000,000 to 2,000,000
    // Reconfiguration at version 2,000,000
    
    let opt = Opt {
        max_versions_per_range: 500000,
        // ... other params
    };
    
    let jobs = opt.run().await.unwrap();
    
    // Verify: Check if reconfiguration at version 2,000,000 is in any job
    let mut found_reconfig = false;
    for job in &jobs {
        for range in job {
            let (_, first, last, desc) = parse_job_range(range);
            if first <= 2_000_000 && last >= 2_000_000 {
                found_reconfig = true;
                break;
            }
        }
    }
    
    // Expected: reconfiguration transaction is omitted
    assert!(!found_reconfig, 
        "Reconfiguration transaction should be omitted in partial ranges");
}
```

**Demonstration:**

Run the existing job generation on mainnet/testnet data with large epochs and observe:
1. Partial range jobs created with "versions omitted" messages
2. Count of omitted versions includes epoch boundary transactions
3. Verification logs show no replay for omitted reconfiguration transactions

The code itself proves the vulnerability through the explicit "omitted" message at: [6](#0-5) 

## Notes

This vulnerability is particularly concerning because:
1. The verification gap is silent - no warning that critical transactions are unverified
2. Load testing scenarios (where large epochs occur) are precisely when edge cases in validator set logic might manifest
3. The `stake::on_new_epoch()` function contains complex logic for validator set updates, reward calculations, and voting power adjustments that should be thoroughly verified
4. State snapshots are taken AFTER reconfigurations, so starting from a snapshot doesn't verify the reconfiguration logic itself

### Citations

**File:** storage/db-tool/src/gen_replay_verify_jobs.rs (L96-117)
```rust
                        if end.version - begin.version >= self.max_versions_per_range {
                            // cut big range short, this hopefully automatically skips load tests
                            let msg = if end.epoch - begin.epoch > 15 {
                                "!!! Need more snapshots !!!"
                            } else {
                                ""
                            };
                            Some((
                                true,
                                begin.version,
                                begin.version + self.max_versions_per_range - 1,
                                format!(
                                    "Partial replay epoch {} - {}, {} txns starting from version {}, another {} versions omitted, until {}. {}",
                                    begin.epoch,
                                    end.epoch - 1,
                                    self.max_versions_per_range,
                                    begin.version,
                                    end.version - begin.version - self.max_versions_per_range,
                                    end.version,
                                    msg
                                )
                            ))
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration.move (L106-159)
```text
    public(friend) fun reconfigure() acquires Configuration {
        // Do not do anything if genesis has not finished.
        if (chain_status::is_genesis() || timestamp::now_microseconds() == 0 || !reconfiguration_enabled()) {
            return
        };

        let config_ref = borrow_global_mut<Configuration>(@aptos_framework);
        let current_time = timestamp::now_microseconds();

        // Do not do anything if a reconfiguration event is already emitted within this transaction.
        //
        // This is OK because:
        // - The time changes in every non-empty block
        // - A block automatically ends after a transaction that emits a reconfiguration event, which is guaranteed by
        //   VM spec that all transactions comming after a reconfiguration transaction will be returned as Retry
        //   status.
        // - Each transaction must emit at most one reconfiguration event
        //
        // Thus, this check ensures that a transaction that does multiple "reconfiguration required" actions emits only
        // one reconfiguration event.
        //
        if (current_time == config_ref.last_reconfiguration_time) {
            return
        };

        reconfiguration_state::on_reconfig_start();

        // Call stake to compute the new validator set and distribute rewards and transaction fees.
        stake::on_new_epoch();
        storage_gas::on_reconfig();

        assert!(current_time > config_ref.last_reconfiguration_time, error::invalid_state(EINVALID_BLOCK_TIME));
        config_ref.last_reconfiguration_time = current_time;
        spec {
            assume config_ref.epoch + 1 <= MAX_U64;
        };
        config_ref.epoch = config_ref.epoch + 1;

        if (std::features::module_event_migration_enabled()) {
            event::emit(
                NewEpoch {
                    epoch: config_ref.epoch,
                },
            );
        };
        event::emit_event<NewEpochEvent>(
            &mut config_ref.events,
            NewEpochEvent {
                epoch: config_ref.epoch,
            },
        );

        reconfiguration_state::on_reconfig_finish();
    }
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L1344-1464)
```text
    public(friend) fun on_new_epoch(
    ) acquires AptosCoinCapabilities, PendingTransactionFee, StakePool, TransactionFeeConfig, ValidatorConfig, ValidatorPerformance, ValidatorSet {
        let validator_set = borrow_global_mut<ValidatorSet>(@aptos_framework);
        let config = staking_config::get();
        let validator_perf = borrow_global_mut<ValidatorPerformance>(@aptos_framework);

        // Process pending stake and distribute transaction fees and rewards for each currently active validator.
        vector::for_each_ref(&validator_set.active_validators, |validator| {
            let validator: &ValidatorInfo = validator;
            update_stake_pool(validator_perf, validator.addr, &config);
        });

        // Process pending stake and distribute transaction fees and rewards for each currently pending_inactive validator
        // (requested to leave but not removed yet).
        vector::for_each_ref(&validator_set.pending_inactive, |validator| {
            let validator: &ValidatorInfo = validator;
            update_stake_pool(validator_perf, validator.addr, &config);
        });

        // Activate currently pending_active validators.
        append(&mut validator_set.active_validators, &mut validator_set.pending_active);

        // Officially deactivate all pending_inactive validators. They will now no longer receive rewards.
        validator_set.pending_inactive = vector::empty();

        // Update active validator set so that network address/public key change takes effect.
        // Moreover, recalculate the total voting power, and deactivate the validator whose
        // voting power is less than the minimum required stake.
        let next_epoch_validators = vector::empty();
        let (minimum_stake, _) = staking_config::get_required_stake(&config);
        let vlen = vector::length(&validator_set.active_validators);
        let total_voting_power = 0;
        let i = 0;
        while ({
            spec {
                invariant spec_validators_are_initialized(next_epoch_validators);
                invariant i <= vlen;
            };
            i < vlen
        }) {
            let old_validator_info = vector::borrow_mut(&mut validator_set.active_validators, i);
            let pool_address = old_validator_info.addr;
            let validator_config = borrow_global<ValidatorConfig>(pool_address);
            let stake_pool = borrow_global<StakePool>(pool_address);
            let new_validator_info = generate_validator_info(pool_address, stake_pool, *validator_config);

            // A validator needs at least the min stake required to join the validator set.
            if (new_validator_info.voting_power >= minimum_stake) {
                spec {
                    assume total_voting_power + new_validator_info.voting_power <= MAX_U128;
                };
                total_voting_power = total_voting_power + (new_validator_info.voting_power as u128);
                vector::push_back(&mut next_epoch_validators, new_validator_info);
            };
            i = i + 1;
        };

        validator_set.active_validators = next_epoch_validators;
        validator_set.total_voting_power = total_voting_power;
        validator_set.total_joining_power = 0;

        // Update validator indices, reset performance scores, and renew lockups.
        validator_perf.validators = vector::empty();
        let recurring_lockup_duration_secs = staking_config::get_recurring_lockup_duration(&config);
        let vlen = vector::length(&validator_set.active_validators);
        let validator_index = 0;
        while ({
            spec {
                invariant spec_validators_are_initialized(validator_set.active_validators);
                invariant len(validator_set.pending_active) == 0;
                invariant len(validator_set.pending_inactive) == 0;
                invariant 0 <= validator_index && validator_index <= vlen;
                invariant vlen == len(validator_set.active_validators);
                invariant forall i in 0..validator_index:
                    global<ValidatorConfig>(validator_set.active_validators[i].addr).validator_index < validator_index;
                invariant forall i in 0..validator_index:
                    validator_set.active_validators[i].config.validator_index < validator_index;
                invariant len(validator_perf.validators) == validator_index;
            };
            validator_index < vlen
        }) {
            let validator_info = vector::borrow_mut(&mut validator_set.active_validators, validator_index);
            validator_info.config.validator_index = validator_index;
            let validator_config = borrow_global_mut<ValidatorConfig>(validator_info.addr);
            validator_config.validator_index = validator_index;

            vector::push_back(&mut validator_perf.validators, IndividualValidatorPerformance {
                successful_proposals: 0,
                failed_proposals: 0,
            });

            // Automatically renew a validator's lockup for validators that will still be in the validator set in the
            // next epoch.
            let stake_pool = borrow_global_mut<StakePool>(validator_info.addr);
            let now_secs = timestamp::now_seconds();
            let reconfig_start_secs = if (chain_status::is_operating()) {
                get_reconfig_start_time_secs()
            } else {
                now_secs
            };
            if (stake_pool.locked_until_secs <= reconfig_start_secs) {
                spec {
                    assume now_secs + recurring_lockup_duration_secs <= MAX_U64;
                };
                stake_pool.locked_until_secs = now_secs + recurring_lockup_duration_secs;
            };

            validator_index = validator_index + 1;
        };

        if (exists<PendingTransactionFee>(@aptos_framework)) {
            let pending_fee_by_validator = &mut borrow_global_mut<PendingTransactionFee>(@aptos_framework).pending_fee_by_validator;
            assert!(pending_fee_by_validator.is_empty(), error::internal(ETRANSACTION_FEE_NOT_FULLY_DISTRIBUTED));
            validator_set.active_validators.for_each_ref(|v| pending_fee_by_validator.add(v.config.validator_index, aggregator_v2::create_unbounded_aggregator<u64>()));
        };

        if (features::periodical_reward_rate_decrease_enabled()) {
            // Update rewards rate after reward distribution.
            staking_config::calculate_and_save_latest_epoch_rewards_rate();
        };
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L461-497)
```rust
        // Find epoch boundaries.
        let mut epochs = Vec::new();
        let mut epoch_begin = chunk_begin; // epoch begin version
        for (version, events) in multizip((chunk_begin..chunk_end, event_vecs.iter())) {
            let is_epoch_ending = events.iter().any(ContractEvent::is_new_epoch_event);
            if is_epoch_ending {
                epochs.push((epoch_begin, version + 1));
                epoch_begin = version + 1;
            }
        }
        if epoch_begin < chunk_end {
            epochs.push((epoch_begin, chunk_end));
        }

        let mut chunks_enqueued = 0;
        // Replay epoch by epoch.
        for (begin, end) in epochs {
            chunks_enqueued += self.remove_and_replay_epoch(
                &mut transactions,
                &mut persisted_aux_info,
                &mut transaction_infos,
                &mut write_sets,
                &mut event_vecs,
                begin,
                end,
                verify_execution_mode,
            )?;
        }

        info!(
            num_txns = num_txns,
            tps = (num_txns as f64 / started.elapsed().as_secs_f64()),
            "TransactionReplayer::replay() OK"
        );

        Ok(chunks_enqueued)
    }
```

**File:** execution/executor-types/src/ledger_update_output.rs (L90-112)
```rust
    pub fn ensure_transaction_infos_match(
        &self,
        transaction_infos: &[TransactionInfo],
    ) -> Result<()> {
        ensure!(
            self.transaction_infos.len() == transaction_infos.len(),
            "Lengths don't match. {} vs {}",
            self.transaction_infos.len(),
            transaction_infos.len(),
        );

        let mut version = self.first_version();
        for (txn_info, expected_txn_info) in
            zip_eq(self.transaction_infos.iter(), transaction_infos.iter())
        {
            ensure!(
                txn_info == expected_txn_info,
                "Transaction infos don't match. version:{version}, txn_info:{txn_info}, expected_txn_info:{expected_txn_info}",
            );
            version += 1;
        }
        Ok(())
    }
```
