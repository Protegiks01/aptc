# Audit Report

## Title
Unbounded Memory Accumulation in StateKvShardPruner Leading to OOM Crashes and Validator Liveness Failures

## Summary
The `StateKvShardPruner::prune()` method accumulates all deletion operations in a single `SchemaBatch` without any size limit or chunking mechanism. When pruning large version ranges or versions with many state updates, this causes unbounded memory growth that can trigger OOM crashes or severe memory pressure, affecting validator node liveness.

## Finding Description

The vulnerability exists in the state KV shard pruning logic where the `prune()` method creates a single `SchemaBatch` and iterates through ALL stale state entries between `current_progress` and `target_version`, adding deletions for each without any chunking mechanism. [1](#0-0) 

Each stale entry requires TWO deletion operations to be added to the batch - one for the stale index and one for the state value. The underlying `SchemaBatch` implementation stores all operations in memory as a `HashMap<ColumnFamilyName, Vec<WriteOp>>`: [2](#0-1) 

Each deletion operation stores the full key as `Vec<u8>` in the `WriteOp::Deletion` enum: [3](#0-2) 

The key sizes are:
- `StaleStateValueIndexByKeyHashSchema`: 48 bytes (8 + 8 + 32) [4](#0-3) 
- `StateValueByKeyHashSchema`: 40 bytes (32 + 8) [5](#0-4) 

**Critical Issue**: The number of stale entries is NOT bounded by the version difference `(target_version - current_progress)`. A single version can contain millions of state updates, all of which become stale entries when subsequently updated or pruned.

**Most Critical Scenario - Initialization Catch-up**: When `StateKvShardPruner::new()` is called during node startup, it immediately calls `prune(progress, metadata_progress)` to catch up the shard: [6](#0-5) 

If the shard has fallen behind by many versions (e.g., after node restart, crash recovery, or processing delays), ALL stale entries in that range are loaded into memory at once.

**Regular Pruning Vulnerability**: While the parent pruner uses `batch_size` to limit version ranges, this only constrains the version difference, not the number of entries: [7](#0-6) 

The default batch_size is 5,000 versions: [8](#0-7) 

With potentially millions of state updates per version (e.g., large airdrop, protocol migration), all deletions still accumulate in memory.

**Contrast with Correct Implementation**: The `StateMerkleShardPruner` implements proper chunking with a loop structure and `max_nodes_to_prune` limit: [9](#0-8) 

The `get_stale_node_indices` method respects the limit parameter for chunking: [10](#0-9) 

This demonstrates that chunking is a known necessary pattern, making its absence in `StateKvShardPruner` a critical oversight.

## Impact Explanation

**Severity: HIGH** (per Aptos Bug Bounty Category #8: "Validator node slowdowns")

**Impact on Validator Liveness**:

1. **OOM Crashes**: When memory accumulation exceeds available RAM, the validator process crashes, taking the node offline immediately. This directly impacts network decentralization and consensus reliability.

2. **Memory Pressure**: Even without full OOM, severe memory pressure causes:
   - Garbage collection pauses affecting block processing timing
   - Swap thrashing degrading overall system performance
   - Slower transaction execution and delayed consensus participation
   - Potential missed block proposals or voting deadlines

3. **Network-Wide Impact**: Multiple validators can be affected simultaneously during:
   - Network upgrades with large state migrations
   - Coordinated maintenance windows when validators restart
   - Recovery periods after temporary outages

**Memory Consumption Calculation**:
- Per stale entry: ~200 bytes (2 deletions × ~100 bytes each including Vec overhead, HashMap entries, enum discriminants)
- 1 million stale entries: ~200 MB
- 10 million stale entries: ~2 GB
- 50 million stale entries: ~10 GB

**Realistic Trigger Conditions**:
- Node restart after being offline (guaranteed trigger during initialization)
- Large on-chain transactions (airdrops to millions of accounts, protocol migrations)
- Protocol upgrades touching many state keys
- Normal blockchain activity accumulating over time

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability triggers automatically during normal validator operations:

1. **Guaranteed Trigger on Node Restart**: Every validator node restart calls `StateKvShardPruner::new()`, which immediately invokes catch-up pruning. If any shard has fallen behind, it will attempt to prune all stale entries at once without chunking.

2. **Common Operational Scenarios**:
   - Validator maintenance and routine restarts (weekly/monthly occurrence)
   - Node crashes followed by recovery
   - Database migration or upgrade procedures
   - Temporary network partitions or processing delays
   - Hardware maintenance requiring node shutdown

3. **No Attacker Required**: This is a design flaw in the pruning implementation that triggers during legitimate operations. No malicious input, crafted transactions, or privileged access is needed.

4. **Real-World Evidence**: The proper chunking implementation in `StateMerkleShardPruner` demonstrates that this pattern is understood to be necessary for handling large data volumes, making the absence in `StateKvShardPruner` a clear oversight.

5. **Growing Risk Over Time**: As the blockchain grows and state accumulates, the likelihood and severity of this issue increases, making it a ticking time bomb for long-running validators.

## Recommendation

Implement chunking in `StateKvShardPruner::prune()` following the pattern used in `StateMerkleShardPruner`:

1. Add a `max_entries_to_prune` parameter to the `prune()` method
2. Implement a loop that processes deletions in chunks
3. Commit each chunk separately with incremental progress updates
4. Only update the final progress metadata when completely done

The implementation should follow this pattern:
- Accept `max_entries_to_prune` parameter (e.g., default 100,000 entries)
- Use a loop to fetch limited batches of stale indices
- Create a new `SchemaBatch` per iteration
- Write each batch and check for completion
- Only mark full progress when the target version is reached

This ensures memory consumption is bounded regardless of the number of stale entries in the version range.

## Proof of Concept

A realistic scenario demonstrating the vulnerability:

1. **Setup**: A validator node with sharding enabled runs for several months
2. **State Accumulation**: Over time, millions of state updates occur (normal blockchain activity)
3. **Node Restart**: Operator performs routine maintenance requiring node restart
4. **Trigger**: During initialization, `StateKvShardPruner::new()` is called for each shard
5. **Catch-up**: If a shard has fallen behind by even a few thousand versions containing millions of state updates, the `prune()` method attempts to load ALL deletions into a single `SchemaBatch`
6. **Memory Exhaustion**: With 10 million stale entries × ~200 bytes = ~2GB of memory consumed just for the deletion batch
7. **OOM Crash**: Process crashes with out-of-memory error, validator goes offline
8. **Recovery Failure**: Upon restart, the same issue occurs, creating a crash loop

This demonstrates a HIGH severity vulnerability that affects validator liveness through automatic triggering during normal operations, matching Aptos Bug Bounty Category #8 criteria.

## Notes

The vulnerability is particularly concerning because:

1. **Silent Accumulation**: The issue only manifests during pruning operations, which occur periodically or during initialization, making it difficult to detect until it causes a crash

2. **No Circuit Breaker**: There's no memory limit check or circuit breaker to prevent unbounded accumulation

3. **Sharding Amplification**: With multiple shards, each shard's pruner operates independently, potentially multiplying memory consumption across all shards simultaneously

4. **Initialization Critical Path**: The vulnerability occurs in the initialization path (`StateKvShardPruner::new()`), making recovery from crashes difficult as each restart attempt triggers the same issue

5. **Contrast with Correct Pattern**: The existence of proper chunking in `StateMerkleShardPruner` clearly demonstrates that the development team understands this pattern is necessary, making this a high-priority oversight to address

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L25-45)
```rust
    pub(in crate::pruner) fn new(
        shard_id: usize,
        db_shard: Arc<DB>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/schemadb/src/batch.rs (L122-133)
```rust
pub enum WriteOp {
    Value { key: Vec<u8>, value: Vec<u8> },
    Deletion { key: Vec<u8> },
}

/// `SchemaBatch` holds a collection of updates that can be applied to a DB atomically. The updates
/// will be applied in the order in which they are added to the `SchemaBatch`.
#[derive(Debug, Default)]
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}
```

**File:** storage/schemadb/src/batch.rs (L165-172)
```rust
    fn raw_delete(&mut self, cf_name: ColumnFamilyName, key: Vec<u8>) -> DbResult<()> {
        self.rows
            .entry(cf_name)
            .or_default()
            .push(WriteOp::Deletion { key });

        Ok(())
    }
```

**File:** storage/aptosdb/src/schema/stale_state_value_index_by_key_hash/mod.rs (L40-47)
```rust
    fn encode_key(&self) -> Result<Vec<u8>> {
        let mut encoded = vec![];
        encoded.write_u64::<BigEndian>(self.stale_since_version)?;
        encoded.write_u64::<BigEndian>(self.version)?;
        encoded.write_all(self.state_key_hash.as_ref())?;

        Ok(encoded)
    }
```

**File:** storage/aptosdb/src/schema/state_value_by_key_hash/mod.rs (L38-43)
```rust
    fn encode_key(&self) -> Result<Vec<u8>> {
        let mut encoded = vec![];
        encoded.write_all(self.0.as_ref())?;
        encoded.write_u64::<BigEndian>(!self.1)?;
        Ok(encoded)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L49-78)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_pruner__prune"]);

        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning state kv data."
            );
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;
```

**File:** config/src/config/storage_config.rs (L387-395)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L58-100)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
        max_nodes_to_prune: usize,
    ) -> Result<()> {
        loop {
            let mut batch = SchemaBatch::new();
            let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
                &self.db_shard,
                current_progress,
                target_version,
                max_nodes_to_prune,
            )?;

            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;

            let mut done = true;
            if let Some(next_version) = next_version {
                if next_version <= target_version {
                    done = false;
                }
            }

            if done {
                batch.put::<DbMetadataSchema>(
                    &S::progress_metadata_key(Some(self.shard_id)),
                    &DbMetadataValue::Version(target_version),
                )?;
            }

            self.db_shard.write_schemas(batch)?;

            if done {
                break;
            }
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L191-217)
```rust
    pub(in crate::pruner::state_merkle_pruner) fn get_stale_node_indices(
        state_merkle_db_shard: &DB,
        start_version: Version,
        target_version: Version,
        limit: usize,
    ) -> Result<(Vec<StaleNodeIndex>, Option<Version>)> {
        let mut indices = Vec::new();
        let mut iter = state_merkle_db_shard.iter::<S>()?;
        iter.seek(&StaleNodeIndex {
            stale_since_version: start_version,
            node_key: NodeKey::new_empty_path(0),
        })?;

        let mut next_version = None;
        while indices.len() < limit {
            if let Some((index, _)) = iter.next().transpose()? {
                next_version = Some(index.stale_since_version);
                if index.stale_since_version <= target_version {
                    indices.push(index);
                    continue;
                }
            }
            break;
        }

        Ok((indices, next_version))
    }
```
