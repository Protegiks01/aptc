# Audit Report

## Title
Unbounded Memory Allocation in Move Type Parser Enables API Resource Exhaustion

## Summary
The `tokenize()` function in the Move type parser allocates an unbounded vector of tokens proportional to input size without any length limits. An attacker can exploit this by submitting transaction payloads with extremely long type tag strings containing millions of simple tokens, causing each API request to allocate 100-200 MB of memory. With concurrent malicious requests, this can exhaust available memory and cause API server slowdowns or unavailability.

## Finding Description
The Move type parser's `tokenize()` function creates a token vector without any bounds checking on the number of tokens generated: [1](#0-0) 

This function processes input strings character-by-character and pushes tokens into a vector until the entire input is consumed. While the HTTP API enforces an 8 MB request body limit: [2](#0-1) 

This is still sufficient to generate millions of tokens. The parser is invoked when deserializing `MoveType` from JSON strings: [3](#0-2) 

When processing transaction submissions, type arguments are parsed without token count validation: [4](#0-3) 

**Attack Path:**

1. Attacker crafts malicious JSON payload with a type tag string like: `"0x1::module::Struct<u64,u64,u64,...>"` where the type argument list contains millions of `u64,` sequences filling ~4-6 MB
2. Attacker submits transaction via `/transactions` POST endpoint
3. API deserializes JSON, calling `MoveType::from_str()` which invokes `parse_type_tag()`
4. `tokenize()` allocates a vector and generates ~2-4 million tokens (each `u64,` pair becomes 2 tokens)
5. Token vector consumes ~64-128 MB (at ~32 bytes per Token enum)
6. Additionally, `parse_comma_list()` creates another vector with millions of `TypeTag::U64` entries consuming ~16-32 MB
7. **Total memory per request: 80-160 MB**
8. Attacker sends 50-100 concurrent requests
9. **Total memory consumption: 4-16 GB**, causing memory pressure and API slowdowns

The recursion depth limit only prevents deep nesting, not wide type argument lists: [5](#0-4) 

## Impact Explanation
This vulnerability enables **High severity** impact per Aptos bug bounty criteria: "Validator node slowdowns" and "API crashes". 

If the REST API is enabled on validator nodes (which is common in production deployments as shown in configuration files): [6](#0-5) 

An attacker can cause memory exhaustion leading to:
- API service unavailability preventing transaction submissions
- Node performance degradation affecting consensus participation
- Potential out-of-memory crashes requiring node restart

Even on fullnode-only deployments, this impacts network accessibility and user experience.

## Likelihood Explanation
**Likelihood: HIGH**

The attack is easily exploitable:
- No authentication required for transaction submission API
- Attacker only needs standard HTTP client capabilities
- No rate limiting middleware is applied in the API server code: [7](#0-6) 

- Each malicious request requires only ~8 MB bandwidth
- Modern botnets or cloud resources can easily generate 50-100 concurrent connections
- Attack is difficult to distinguish from legitimate traffic initially

## Recommendation
Implement multiple layers of defense:

**1. Add token count limit in `tokenize()`:**
```rust
const MAX_TOKENS: usize = 100_000; // Reasonable limit for valid type tags

fn tokenize(mut s: &str) -> Result<Vec<Token>> {
    let mut v = vec![];
    while let Some((tok, n)) = next_token(s)? {
        if v.len() >= MAX_TOKENS {
            bail!("Token limit exceeded: maximum {} tokens allowed", MAX_TOKENS);
        }
        v.push(tok);
        s = &s[n..];
    }
    Ok(v)
}
```

**2. Add type argument count limit in `parse_comma_list()`:**
```rust
const MAX_TYPE_ARGS: usize = 256; // Generous limit for practical use

fn parse_comma_list<F, R>(&mut self, parse_list_item: F, end_token: Token, allow_trailing_comma: bool) -> Result<Vec<R>>
where F: Fn(&mut Self) -> Result<R>, R: std::fmt::Debug,
{
    let mut v = vec![];
    if !(self.peek() == Some(&end_token)) {
        loop {
            if v.len() >= MAX_TYPE_ARGS {
                bail!("Type argument limit exceeded: maximum {} allowed", MAX_TYPE_ARGS);
            }
            v.push(parse_list_item(self)?);
            if self.peek() == Some(&end_token) { break; }
            self.consume(Token::Comma)?;
            if self.peek() == Some(&end_token) && allow_trailing_comma { break; }
        }
    }
    Ok(v)
}
```

**3. Add rate limiting middleware to API server** in `runtime.rs`

## Proof of Concept

```rust
#[test]
fn test_tokenize_memory_exhaustion() {
    use move_core_types::parser::parse_type_tag;
    
    // Generate a malicious type tag with many type arguments
    // Format: "0x1::m::S<u64,u64,u64,...>"
    let mut malicious_input = String::from("0x1::m::S<");
    
    // Add 1 million type arguments (each "u64," is 4 bytes)
    // Total: ~4 MB input string
    for i in 0..1_000_000 {
        if i > 0 {
            malicious_input.push_str(",");
        }
        malicious_input.push_str("u64");
    }
    malicious_input.push_str(">");
    
    println!("Input size: {} MB", malicious_input.len() / 1024 / 1024);
    
    // This will allocate millions of tokens and consume ~100+ MB memory
    let result = parse_type_tag(&malicious_input);
    
    // Currently succeeds (or fails due to other limits) but consumes excessive memory
    // After fix, should fail with token/type argument limit exceeded error
    match result {
        Ok(_) => println!("VULNERABLE: Successfully parsed {} type arguments", 1_000_000),
        Err(e) => println!("Result: {}", e),
    }
}
```

Run with: `cargo test test_tokenize_memory_exhaustion --release -- --nocapture` and monitor memory usage with `/usr/bin/time -v` or similar tools.

## Notes
This vulnerability breaks the "Resource Limits" invariant (#9): "All operations must respect gas, storage, and computational limits." The parser performs unbounded memory allocation during transaction validation, before any gas metering occurs. While the recursion depth limit (`MAX_TYPE_TAG_NESTING = 8`) prevents stack overflow, it does not prevent wide horizontal expansion through many sibling type arguments.

### Citations

**File:** third_party/move/move-core/types/src/parser.rs (L198-205)
```rust
fn tokenize(mut s: &str) -> Result<Vec<Token>> {
    let mut v = vec![];
    while let Some((tok, n)) = next_token(s)? {
        v.push(tok);
        s = &s[n..];
    }
    Ok(v)
}
```

**File:** third_party/move/move-core/types/src/parser.rs (L286-289)
```rust
    fn parse_type_tag(&mut self, depth: u8) -> Result<TypeTag> {
        if depth > crate::safe_serialize::MAX_TYPE_TAG_NESTING {
            bail!("Exceeded TypeTag nesting limit during parsing: {}", depth);
        }
```

**File:** config/src/config/api_config.rs (L97-97)
```rust
const DEFAULT_REQUEST_CONTENT_LENGTH_LIMIT: u64 = 8 * 1024 * 1024; // 8 MB
```

**File:** api/types/src/move_types.rs (L813-842)
```rust
impl FromStr for MoveType {
    type Err = anyhow::Error;

    fn from_str(mut s: &str) -> Result<Self, Self::Err> {
        let mut is_ref = false;
        let mut is_mut = false;
        if s.starts_with('&') {
            s = &s[1..];
            is_ref = true;
        }
        if is_ref && s.starts_with("mut ") {
            s = &s[4..];
            is_mut = true;
        }
        // Previously this would just crap out, but this meant the API could
        // return a serialized version of an object and not be able to
        // deserialize it using that same object.
        let inner = match parse_type_tag(s) {
            Ok(inner) => (&inner).into(),
            Err(_e) => MoveType::Unparsable(s.to_string()),
        };
        if is_ref {
            Ok(MoveType::Reference {
                mutable: is_mut,
                to: Box::new(inner),
            })
        } else {
            Ok(inner)
        }
    }
```

**File:** api/types/src/transaction.rs (L983-991)
```rust
impl VerifyInput for EntryFunctionPayload {
    fn verify(&self) -> anyhow::Result<()> {
        self.function.verify()?;
        for type_arg in self.type_arguments.iter() {
            type_arg.verify(0)?;
        }
        Ok(())
    }
}
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L1-10)
```yaml
###
### This is the base validator NodeConfig to work with this helm chart
### Additional overrides to the NodeConfig can be specified via .Values.validator.config or .Values.overrideNodeConfig
###
base:
  role: validator
  waypoint:
    from_file: /opt/aptos/genesis/waypoint.txt

consensus:
```

**File:** api/src/runtime.rs (L253-259)
```rust
            .with(cors)
            .with_if(config.api.compression_enabled, Compression::new())
            .with(PostSizeLimit::new(size_limit))
            .with(CatchPanic::new().with_handler(panic_handler))
            // NOTE: Make sure to keep this after all the `with` middleware.
            .catch_all_error(convert_error)
            .around(middleware_log);
```
