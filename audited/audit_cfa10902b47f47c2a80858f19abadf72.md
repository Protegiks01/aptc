# Audit Report

## Title
State Sync Peer Monopolization Through Distance Claim Manipulation and Early Return Exploitation

## Summary
An attacker can monopolize state synchronization peer selection by claiming false low validator distances (distance ≥ 2) and exploiting the early return logic in `choose_random_peers_by_distance_and_latency()`. This allows the attacker to eclipse victim nodes, causing denial of service, state sync stalling, and preventing victims from syncing with honest peers.

## Finding Description

The vulnerability exists in the peer selection algorithm for state synchronization, specifically in how peers are chosen based on their self-reported distance from validators. [1](#0-0) 

The function groups peers by distance using a BTreeMap (which sorts in ascending order) and processes them sequentially from lowest to highest distance. When enough peers are selected from a low-distance group, the function returns early without considering higher-distance peers.

The critical vulnerability chain is:

1. **Self-Reported Distance Without Verification**: Peers report their `distance_from_validators` in the `NetworkInformationResponse`: [2](#0-1) 

The distance calculation relies on peer-reported distances from connected peers. An attacker can falsely report distance values.

2. **Weak Validation for distance ≥ 2**: The client-side validation only enforces basic sanity checks: [3](#0-2) 

For distance values ≥ 2, the validation only checks if `distance_from_validators <= MAX_DISTANCE_FROM_VALIDATORS` (100). An attacker with `PeerRole::Unknown` can claim any distance from 2-100 without cryptographic proof.

3. **Critical Usage in Subscription Selection**: This flawed peer selection is used for subscription requests, which are critical for state synchronization: [4](#0-3) 

When selecting a peer for state sync subscription, the system calls `choose_random_peers_by_distance_and_latency()` with `num_peers_to_choose = 1`.

4. **Timeout Recovery Re-selects Attackers**: Even if the victim detects timeout issues and terminates the stream, the peer selection algorithm runs again: [5](#0-4) 

After consecutive timeouts, the stream is terminated, but when a new subscription is created, the same biased selection algorithm runs, choosing another attacker peer.

**Attack Execution Path:**

1. Attacker deploys 10+ malicious full nodes on the public network
2. Each malicious node claims `distance_from_validators = 2` (the minimum claimable value for non-VFN peers)
3. Honest peers have actual distances of 3, 4, 5, etc.
4. Victim node calls `choose_peers_for_request()` for a subscription
5. The BTreeMap groups peers: `{2: [attacker1, attacker2, ..., attacker10], 3: [honest1], 4: [honest2], ...}`
6. The algorithm processes distance=2 group first
7. Selects one peer randomly from the 10 attacker peers (100% chance of selecting an attacker)
8. Returns early at line 58, never considering honest peers at distance 3+
9. Victim subscribes to attacker's peer for all state sync operations
10. Attacker can stall, eclipse, or selectively censor data
11. If victim detects timeout and terminates, steps 4-9 repeat, selecting another attacker

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria:

- **Validator node slowdowns**: Victim nodes cannot sync state efficiently, falling behind the network
- **Significant protocol violations**: State sync mechanism is compromised, breaking the availability guarantee
- **Eclipse attacks**: Victims are isolated from the honest network, seeing only attacker-controlled views
- **Denial of Service**: Attackers can prevent victims from syncing indefinitely

The attack does not directly cause fund loss or consensus safety violations (since state data is cryptographically verified against trusted ledger info), but it severely impacts network availability and node operation. Affected nodes cannot:
- Serve API requests with current state
- Participate effectively in the network
- Provide reliable data to applications
- Bootstrap new nodes

## Likelihood Explanation

**Likelihood: High**

The attack is:
- **Trivial to execute**: Attacker only needs to run multiple nodes and set a configuration value
- **Low cost**: Running public full nodes requires minimal resources
- **No special privileges required**: Any entity can connect to the public network
- **Difficult to detect**: The attacker's distance claims pass validation
- **Scalable**: Works against any number of victim nodes
- **Persistent**: Survives timeout-based peer rotation

The only barrier is that the attacker needs multiple nodes to increase the probability of selection, but even 5-10 nodes (which cost ~$100-200/month) provide high success rates.

## Recommendation

Implement multiple defense layers:

**1. Add randomization to break distance bias:**
```rust
pub fn choose_random_peers_by_distance_and_latency(
    peers: HashSet<PeerNetworkId>,
    peers_and_metadata: Arc<PeersAndMetadata>,
    num_peers_to_choose: usize,
) -> HashSet<PeerNetworkId> {
    // Group peers by distance
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for peer in peers {
        if let Some((distance, latency)) = get_distance_and_latency_for_peer(&peers_and_metadata, peer) {
            let latency_weight = convert_latency_to_weight(latency);
            peers_and_latencies_by_distance
                .entry(distance)
                .or_insert_with(Vec::new)
                .push((peer, latency_weight));
        }
    }

    // NEW: Randomly shuffle distance groups to prevent bias
    let mut distance_groups: Vec<_> = peers_and_latencies_by_distance.into_iter().collect();
    
    // NEW: Add probabilistic selection - don't always start with lowest distance
    // Use weighted random selection where lower distances have higher probability but not 100%
    distance_groups.sort_by_key(|(distance, _)| *distance);
    
    let mut selected_peers = HashSet::new();
    let mut rng = rand::thread_rng();
    
    // Process groups with distance-based weights, not strict ordering
    while !distance_groups.is_empty() && selected_peers.len() < num_peers_to_choose {
        // Calculate weights for remaining groups (exponential decay by distance)
        let weights: Vec<f64> = distance_groups
            .iter()
            .map(|(dist, _)| 1.0 / (1.0 + *dist as f64))
            .collect();
        
        // Select a group probabilistically
        let group_idx = select_weighted_index(&weights, &mut rng);
        let (_, peers_and_latencies) = distance_groups.remove(group_idx);
        
        let num_peers_remaining = num_peers_to_choose.saturating_sub(selected_peers.len()) as u64;
        let peers = choose_random_peers_by_weight(num_peers_remaining, peers_and_latencies);
        selected_peers.extend(peers);
    }

    selected_peers
}
```

**2. Add distance verification through consensus:**
- Cross-validate peer distances by checking if their reported distances are consistent with their peers' distances
- Track distance claims over time and penalize inconsistent peers

**3. Implement peer diversity requirements:**
- Require selecting peers from multiple distance buckets
- Prevent all selections from a single distance value

**4. Add reputation-based filtering:**
- Track peer performance (timeout rates, invalid data)
- Demote peers with poor track records regardless of distance

## Proof of Concept

**Setup:**
1. Deploy 10 malicious Aptos full nodes
2. Configure each node to report `distance_from_validators = 2` in their peer monitoring responses
3. Connect all malicious nodes to the public network

**Exploitation Steps:**

```rust
// On attacker's nodes: Modify peer-monitoring-service/server/src/lib.rs
fn get_distance_from_validators(
    base_config: &BaseConfig,
    peers_and_metadata: Arc<PeersAndMetadata>,
) -> u64 {
    // MALICIOUS: Always return 2 (lowest claimable for non-VFN)
    2
}

// Verification on victim node:
// 1. Monitor peer selection in state sync logs
// 2. Observe that chosen peers are always from the attacker's set
// 3. Measure state sync stall: synced_version remains constant
// 4. Check subscription peer via metrics: always distance=2 peers

// Expected result:
// - Victim selects attacker peer with ~100% probability (10/10 at distance 2)
// - State sync fails or stalls
// - After timeout and re-subscription, another attacker peer is selected
// - Honest peers at distance 3+ never selected
```

**Demonstration Code (Test):**
```rust
#[test]
fn test_distance_monopolization_attack() {
    let mut peers_and_metadata = create_test_peers_and_metadata();
    
    // Add 10 attacker peers at distance 2
    for i in 0..10 {
        let attacker_peer = create_peer_with_distance(i, 2);
        peers_and_metadata.add_peer(attacker_peer);
    }
    
    // Add 20 honest peers at distances 3-7
    for i in 0..20 {
        let honest_peer = create_peer_with_distance(100 + i, 3 + (i % 5));
        peers_and_metadata.add_peer(honest_peer);
    }
    
    // Run peer selection 100 times
    let mut attacker_selections = 0;
    for _ in 0..100 {
        let all_peers = peers_and_metadata.get_all_peers();
        let selected = choose_random_peers_by_distance_and_latency(
            all_peers,
            Arc::new(peers_and_metadata.clone()),
            1
        );
        
        if is_attacker_peer(selected.iter().next().unwrap()) {
            attacker_selections += 1;
        }
    }
    
    // VULNERABLE: Attacker peers selected ~100% of the time
    assert!(attacker_selections > 95); // Should be ~100/100
    
    // EXPECTED AFTER FIX: Should be closer to 33% (10/(10+20))
}
```

## Notes

This vulnerability represents a fundamental flaw in trust assumptions: the system trusts self-reported network topology metrics (distance from validators) without verification. While cryptographic verification protects against incorrect state data, it does not protect against peer selection manipulation that can lead to availability attacks. The early return optimization, while improving performance in honest scenarios, creates a critical attack surface when combined with unverified distance claims.

The attack is particularly severe for nodes joining the network (bootstrapping) or nodes that have fallen behind, as they rely entirely on state sync to catch up. Enterprise applications and critical infrastructure using Aptos nodes would be vulnerable to targeted eclipse attacks.

### Citations

**File:** state-sync/aptos-data-client/src/utils.rs (L26-64)
```rust
pub fn choose_random_peers_by_distance_and_latency(
    peers: HashSet<PeerNetworkId>,
    peers_and_metadata: Arc<PeersAndMetadata>,
    num_peers_to_choose: usize,
) -> HashSet<PeerNetworkId> {
    // Group peers and latency weights by validator distance, i.e., distance -> [(peer, latency weight)]
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for peer in peers {
        if let Some((distance, latency)) =
            get_distance_and_latency_for_peer(&peers_and_metadata, peer)
        {
            let latency_weight = convert_latency_to_weight(latency);
            peers_and_latencies_by_distance
                .entry(distance)
                .or_insert_with(Vec::new)
                .push((peer, latency_weight));
        }
    }

    // Select the peers by distance and latency weights. Note: BTreeMaps are
    // sorted by key, so the entries will be sorted by distance in ascending order.
    let mut selected_peers = HashSet::new();
    for (_, peers_and_latencies) in peers_and_latencies_by_distance {
        // Select the peers by latency weights
        let num_peers_remaining = num_peers_to_choose.saturating_sub(selected_peers.len()) as u64;
        let peers = choose_random_peers_by_weight(num_peers_remaining, peers_and_latencies);

        // Add the peers to the entire set
        selected_peers.extend(peers);

        // If we have selected enough peers, return early
        if selected_peers.len() >= num_peers_to_choose {
            return selected_peers;
        }
    }

    // Return the selected peers
    selected_peers
}
```

**File:** peer-monitoring-service/server/src/lib.rs (L296-340)
```rust
/// Returns the distance from the validators using the given base config
/// and the peers and metadata information.
fn get_distance_from_validators(
    base_config: &BaseConfig,
    peers_and_metadata: Arc<PeersAndMetadata>,
) -> u64 {
    // Get the connected peers and metadata
    let connected_peers_and_metadata = match peers_and_metadata.get_connected_peers_and_metadata() {
        Ok(connected_peers_and_metadata) => connected_peers_and_metadata,
        Err(error) => {
            warn!(LogSchema::new(LogEntry::PeerMonitoringServiceError).error(&error.into()));
            return MAX_DISTANCE_FROM_VALIDATORS;
        },
    };

    // If we're a validator and we have active validator peers, we're in the validator set.
    // TODO: figure out if we need to deal with validator set forks here.
    if base_config.role.is_validator() {
        for peer_metadata in connected_peers_and_metadata.values() {
            if peer_metadata.get_connection_metadata().role.is_validator() {
                return 0;
            }
        }
    }

    // Otherwise, go through our peers, find the min, and return a distance relative to the min
    let mut min_peer_distance_from_validators = MAX_DISTANCE_FROM_VALIDATORS;
    for peer_metadata in connected_peers_and_metadata.values() {
        if let Some(ref latest_network_info_response) = peer_metadata
            .get_peer_monitoring_metadata()
            .latest_network_info_response
        {
            min_peer_distance_from_validators = min(
                min_peer_distance_from_validators,
                latest_network_info_response.distance_from_validators,
            );
        }
    }

    // We're one hop away from the peer
    min(
        MAX_DISTANCE_FROM_VALIDATORS,
        min_peer_distance_from_validators + 1,
    )
}
```

**File:** peer-monitoring-service/client/src/peer_states/network_info.rs (L116-154)
```rust
        // Sanity check the response depth from the peer metadata
        let network_id = peer_network_id.network_id();
        let is_valid_depth = match network_info_response.distance_from_validators {
            0 => {
                // Verify the peer is a validator and has the correct network id
                let peer_is_validator = peer_metadata.get_connection_metadata().role.is_validator();
                let peer_has_correct_network = match self.base_config.role {
                    RoleType::Validator => network_id.is_validator_network(), // We're a validator
                    RoleType::FullNode => network_id.is_vfn_network(),        // We're a VFN
                };
                peer_is_validator && peer_has_correct_network
            },
            1 => {
                // Verify the peer is a VFN and has the correct network id
                let peer_is_vfn = peer_metadata.get_connection_metadata().role.is_vfn();
                let peer_has_correct_network = match self.base_config.role {
                    RoleType::Validator => network_id.is_vfn_network(), // We're a validator
                    RoleType::FullNode => network_id.is_public_network(), // We're a VFN or PFN
                };
                peer_is_vfn && peer_has_correct_network
            },
            distance_from_validators => {
                // The distance must be less than or equal to the max
                distance_from_validators <= MAX_DISTANCE_FROM_VALIDATORS
            },
        };

        // If the depth did not pass our sanity checks, handle a failure
        if !is_valid_depth {
            warn!(LogSchema::new(LogEntry::NetworkInfoRequest)
                .event(LogEvent::InvalidResponse)
                .peer(peer_network_id)
                .message(&format!(
                    "Peer returned invalid depth from validators: {}",
                    network_info_response.distance_from_validators
                )));
            self.handle_request_failure();
            return;
        }
```

**File:** state-sync/aptos-data-client/src/client.rs (L505-518)
```rust
        // Otherwise, choose a new peer to handle the subscription request
        let selected_peer = self
            .choose_random_peers_by_distance_and_latency(serviceable_peers, 1)
            .into_iter()
            .next();

        // If a peer was selected, update the active subscription state
        if let Some(selected_peer) = selected_peer {
            let subscription_state = SubscriptionState::new(selected_peer, request_stream_id);
            *active_subscription_state = Some(subscription_state);
        }

        Ok(selected_peer)
    }
```

**File:** state-sync/state-sync-driver/src/utils.rs (L200-237)
```rust
pub async fn get_data_notification(
    max_stream_wait_time_ms: u64,
    max_num_stream_timeouts: u64,
    active_data_stream: Option<&mut DataStreamListener>,
) -> Result<DataNotification, Error> {
    let active_data_stream = active_data_stream
        .ok_or_else(|| Error::UnexpectedError("The active data stream does not exist!".into()))?;

    let timeout_ms = Duration::from_millis(max_stream_wait_time_ms);
    if let Ok(data_notification) = timeout(timeout_ms, active_data_stream.select_next_some()).await
    {
        // Update the metrics for the data notification receive latency
        metrics::observe_duration(
            &metrics::DATA_NOTIFICATION_LATENCIES,
            metrics::NOTIFICATION_CREATE_TO_RECEIVE,
            data_notification.creation_time,
        );

        // Reset the number of consecutive timeouts for the data stream
        active_data_stream.num_consecutive_timeouts = 0;
        Ok(data_notification)
    } else {
        // Increase the number of consecutive timeouts for the data stream
        active_data_stream.num_consecutive_timeouts += 1;

        // Check if we've timed out too many times
        if active_data_stream.num_consecutive_timeouts >= max_num_stream_timeouts {
            Err(Error::CriticalDataStreamTimeout(format!(
                "{:?}",
                max_num_stream_timeouts
            )))
        } else {
            Err(Error::DataStreamNotificationTimeout(format!(
                "{:?}",
                timeout_ms
            )))
        }
    }
```
