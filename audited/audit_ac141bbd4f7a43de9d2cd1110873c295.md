# Audit Report

## Title
Indexer Crash Due to Missing Table Metadata in Token Claims Processing Across Batch Boundaries

## Summary
The token claims indexer has an inconsistent error handling pattern where write operations silently fail with a warning when table metadata is missing, but delete operations panic and crash the indexer. This occurs when a `PendingClaims` resource is created in one transaction batch but table items are added/removed in subsequent batches, causing the `table_handle_to_owner` mapping to be incomplete. [1](#0-0) [2](#0-1) 

## Finding Description

The vulnerability exists in the token claims indexing logic due to a fundamental architectural limitation: the `table_handle_to_owner` mapping is rebuilt from scratch for each transaction batch and is not persisted across batches. [3](#0-2) 

When processing transactions, the indexer:
1. Builds `table_handle_to_owner` by scanning WriteResource changes in the current batch only
2. Uses this mapping to resolve owner addresses when processing WriteTableItem and DeleteTableItem changes [4](#0-3) 

The problem occurs across batch boundaries (default batch size: 500 transactions):

**Attack Scenario:**
1. **Transaction 1000 (Batch 1)**: User calls `token_transfers::offer()` for the first time
   - Creates `PendingClaims` resource with new table handle (WriteResource)
   - Adds first token offer to table (WriteTableItem)
   - Both operations indexed successfully [5](#0-4) 

2. **Transaction 2000 (Batch 2)**: User calls `offer()` again
   - Only WriteTableItem emitted (resource already exists, no WriteResource)
   - Batch 2's `table_handle_to_owner` doesn't contain the PendingClaims table handle from Batch 1
   - `from_write_table_item()` finds no metadata, logs warning, returns `Ok(None)`
   - Token claim is NOT indexed (permanent data gap)

3. **Transaction 3000 (Batch 3)**: User calls `claim()` or `cancel_offer()`
   - DeleteTableItem is emitted
   - Batch 3's `table_handle_to_owner` still doesn't contain the metadata
   - `from_delete_table_item()` calls `unwrap_or_else(|| panic!())`
   - **INDEXER CRASHES** [6](#0-5) 

The inconsistency is critical: writes fail silently but deletes crash the entire indexer process. Unlike collections which have a database fallback mechanism, token claims have no such protection: [7](#0-6) 

## Impact Explanation

**High Severity** - This vulnerability meets the "API crashes" category from the Aptos bug bounty program.

Impact:
- **Indexer service unavailability**: The panic crashes the indexer process, requiring manual restart
- **Cascading failures**: Multiple claims/cancellations across batch boundaries trigger repeated crashes
- **Operational disruption**: Affects REST API and GraphQL API serving blockchain data to dApps, wallets, and explorers
- **Data integrity issues**: Even before the crash, permanent indexing gaps occur for token claims that weren't captured

While this doesn't affect consensus or on-chain state, it severely impacts the ecosystem's ability to query blockchain data, which is critical for user-facing applications.

## Likelihood Explanation

**High Likelihood** - This vulnerability is easily triggered through normal user behavior:

- Default batch size is 500 transactions
- Any user making multiple token offers with >500 transactions between operations will trigger this
- High-volume users or popular NFT collections will frequently hit this condition
- No special privileges or malicious intent required
- The warning messages at line 97 suggest this is already occurring in production

The vulnerability is deterministic and reproducible with predictable transaction patterns.

## Recommendation

Implement a database fallback mechanism for token claims, similar to what exists for collections. The `current_token_pending_claims` table already stores both `table_handle` and `from_address` (owner address), making a database lookup possible: [8](#0-7) 

**Recommended Fix:**

1. Add a database query method in `token_claims.rs`:
```rust
pub fn get_claim_owner_by_table_handle(
    conn: &mut PgPoolConnection,
    table_handle: &str,
) -> anyhow::Result<String> {
    let mut retried = 0;
    while retried < QUERY_RETRIES {
        retried += 1;
        match current_token_pending_claims::table
            .filter(current_token_pending_claims::table_handle.eq(table_handle))
            .select(current_token_pending_claims::from_address)
            .first::<String>(conn) {
            Ok(owner_address) => return Ok(owner_address),
            Err(_) => {
                std::thread::sleep(std::time::Duration::from_millis(QUERY_RETRY_DELAY_MS));
            },
        }
    }
    Err(anyhow::anyhow!("Failed to get claim owner"))
}
```

2. Modify `from_write_table_item()` to use database fallback instead of warning
3. Modify `from_delete_table_item()` to use database fallback instead of panicking
4. Both methods should consistently handle missing metadata with graceful degradation

## Proof of Concept

The vulnerability can be reproduced with the following sequence:

```rust
// Pseudo-code for reproduction steps

// Batch 1 (transactions 1000-1499)
// User A creates first offer
transaction_1000 {
    sender: user_a,
    function: "token_transfers::offer_script",
    // Creates PendingClaims resource + adds offer
    // WriteResource: PendingClaims with table handle 0xabc...
    // WriteTableItem: Add offer to table 0xabc...
}

// Batch 2 (transactions 1500-1999)  
// User A creates second offer (500+ transactions later)
transaction_1800 {
    sender: user_a,
    function: "token_transfers::offer_script",
    // Only WriteTableItem emitted (resource exists)
    // WriteTableItem: Add offer to table 0xabc...
    // RESULT: Warning logged, claim NOT indexed
}

// Batch 3 (transactions 2000-2499)
// User B claims the token
transaction_2100 {
    sender: user_b,
    function: "token_transfers::claim_script",
    // Only DeleteTableItem emitted
    // DeleteTableItem: Remove offer from table 0xabc...
    // RESULT: PANIC - indexer crashes!
}
```

To verify in production:
1. Monitor indexer logs for warnings: "Missing table handle metadata for TokenClaim"
2. Check if subsequent delete operations on those table handles cause panics
3. Verify indexer crashes requiring manual restart

The fix should eliminate both the warnings and the panics by implementing consistent database fallback logic.

## Notes

This vulnerability highlights a broader architectural issue: the indexer's batch-scoped metadata caching creates timing dependencies that don't exist in the blockchain itself. The blockchain state is always consistent, but the indexer's view becomes inconsistent due to batch boundary effects. A comprehensive solution should consider:

1. Persisting table metadata across batches
2. Implementing consistent error handling patterns across all token types
3. Adding database fallbacks for all table-based lookups
4. Monitoring and alerting on metadata lookup failures

### Citations

**File:** crates/indexer/src/models/token_models/token_claims.rs (L97-102)
```rust
                    aptos_logger::warn!(
                        transaction_version = txn_version,
                        table_handle = table_handle,
                        "Missing table handle metadata for TokenClaim. {:?}",
                        table_handle_to_owner
                    );
```

**File:** crates/indexer/src/models/token_models/token_claims.rs (L135-141)
```rust
            let table_metadata = table_handle_to_owner.get(&table_handle).unwrap_or_else(|| {
                panic!(
                    "Missing table handle metadata for claim. \
                    Version: {}, table handle for PendingClaims: {}, all metadata: {:?}",
                    txn_version, table_handle, table_handle_to_owner
                )
            });
```

**File:** crates/indexer/src/processors/token_processor.rs (L860-863)
```rust
        // First get all token related table metadata from the batch of transactions. This is in case
        // an earlier transaction has metadata (in resources) that's missing from a later transaction.
        let table_handle_to_owner =
            TableMetadataForToken::get_table_handle_to_owner_from_transactions(&transactions);
```

**File:** crates/indexer/src/models/token_models/tokens.rs (L156-164)
```rust
                    APIWriteSetChange::DeleteTableItem(delete_table_item) => {
                        CurrentTokenPendingClaim::from_delete_table_item(
                            delete_table_item,
                            txn_version,
                            txn_timestamp,
                            table_handle_to_owner,
                        )
                        .unwrap()
                    },
```

**File:** crates/indexer/src/models/token_models/tokens.rs (L350-373)
```rust
    pub fn get_table_handle_to_owner_from_transactions(
        transactions: &[APITransaction],
    ) -> TableHandleToOwner {
        let mut table_handle_to_owner: TableHandleToOwner = HashMap::new();
        // Do a first pass to get all the table metadata in the batch.
        for transaction in transactions {
            if let APITransaction::UserTransaction(user_txn) = transaction {
                let txn_version = user_txn.info.version.0 as i64;
                for wsc in &user_txn.info.changes {
                    if let APIWriteSetChange::WriteResource(write_resource) = wsc {
                        let maybe_map = TableMetadataForToken::get_table_handle_to_owner(
                            write_resource,
                            txn_version,
                        )
                        .unwrap();
                        if let Some(map) = maybe_map {
                            table_handle_to_owner.extend(map);
                        }
                    }
                }
            }
        }
        table_handle_to_owner
    }
```

**File:** aptos-move/framework/aptos-token/sources/token_transfers.move (L115-128)
```text
        if (!exists<PendingClaims>(sender_addr)) {
            initialize_token_transfers(sender)
        };

        let pending_claims =
            &mut PendingClaims[sender_addr].pending_claims;
        let token_offer_id = create_token_offer_id(receiver, token_id);
        let token = token::withdraw_token(sender, token_id, amount);
        if (!pending_claims.contains(token_offer_id)) {
            pending_claims.add(token_offer_id, token);
        } else {
            let dst_token = pending_claims.borrow_mut(token_offer_id);
            token::merge(dst_token, token);
        };
```

**File:** crates/indexer/src/models/token_models/collection_datas.rs (L107-119)
```rust
            let mut creator_address = match maybe_creator_address {
                Some(ca) => ca,
                None => match Self::get_collection_creator(conn, &table_handle) {
                    Ok(creator) => creator,
                    Err(_) => {
                        aptos_logger::error!(
                            transaction_version = txn_version,
                            lookup_key = &table_handle,
                            "Failed to get collection creator for table handle. You probably should backfill db."
                        );
                        return Ok(None);
                    },
                },
```

**File:** crates/indexer/migrations/2022-09-22-185845_token_offers/up.sql (L54-77)
```sql
CREATE TABLE current_token_pending_claims (
  token_data_id_hash VARCHAR(64) NOT NULL,
  property_version NUMERIC NOT NULL,
  from_address VARCHAR(66) NOT NULL,
  to_address VARCHAR(66) NOT NULL,
  collection_data_id_hash VARCHAR(64) NOT NULL,
  creator_address VARCHAR(66) NOT NULL,
  collection_name VARCHAR(128) NOT NULL,
  name VARCHAR(128) NOT NULL,
  -- 0 means either claimed or canceled
  amount NUMERIC NOT NULL,
  table_handle VARCHAR(66) NOT NULL,
  last_transaction_version BIGINT NOT NULL,
  inserted_at TIMESTAMP NOT NULL DEFAULT NOW(),
  -- Constraints
  PRIMARY KEY (
    -- This is basically the token offer id
    token_data_id_hash,
    property_version,
    from_address,
    to_address
  )
);
CREATE INDEX ctpc_th_index ON current_token_pending_claims (table_handle);
```
