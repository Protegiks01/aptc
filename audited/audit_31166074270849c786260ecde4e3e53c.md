# Audit Report

## Title
Resource Leak in Secret Share Manager: DropGuards and Tasks Accumulate Indefinitely When Blocks Never Become Ready

## Summary
The `SecretShareManager` stores `DropGuard` objects in `QueueItem` structures that manage spawned share requester tasks. When secret share aggregation fails to reach threshold or encounters errors, blocks never become "ready" and remain queued indefinitely. This causes `DropGuard` objects and their associated tasks to leak, leading to unbounded memory consumption, task accumulation, and network resource exhaustion on validator nodes.

## Finding Description

The vulnerability exists in the secret sharing consensus component's queue management logic. When blocks arrive for processing, the system spawns background tasks to request secret shares from validators and stores abort handles (`DropGuard`) for these tasks. [1](#0-0) 

Each block gets a spawned task that continuously requests shares via `ReliableBroadcast.multicast()`, which retries indefinitely with exponential backoff until aggregation completes: [2](#0-1) 

The tasks are spawned with `tokio::spawn` (unbounded), and wrapped with `Abortable` controlled by the `DropGuard`. The `ReliableBroadcast.multicast()` implementation retries forever on failures: [3](#0-2) 

Queue items are only removed when all their blocks become "fully secret shared": [4](#0-3) 

A block is only considered ready when its secret shared key is received: [5](#0-4) 

**Failure scenarios where blocks never become ready:**

1. **Insufficient shares**: When < threshold of validators respond (e.g., due to Byzantine behavior, network partitions, or offline validators), aggregation never completes
2. **Aggregation errors**: The cryptographic aggregation can fail even with enough shares: [6](#0-5) 

3. **Byzantine validators**: Malicious validators can deliberately withhold shares to prevent aggregation

**Resource accumulation:**
- Each stuck batch adds a `QueueItem` to the unbounded queue
- Each `QueueItem` holds `Vec<DropGuard>` preventing task cleanup
- Tasks continue retrying RPCs indefinitely with exponential backoff
- Memory grows with queue size and task count
- Network bandwidth consumed by continuous retry attempts
- CPU cycles wasted on failed retry attempts

The only cleanup mechanism is `process_reset()` which replaces the entire queue, but this only occurs during epoch transitions or explicit resets, not during normal operation: [7](#0-6) 

## Impact Explanation

This is a **High Severity** vulnerability per Aptos bug bounty criteria:

1. **Validator node slowdowns**: As resources accumulate, nodes experience degraded performance due to:
   - Increased memory usage from growing queue
   - CPU contention from accumulating background tasks
   - Network saturation from continuous RPC retries

2. **Consensus degradation**: Over time, affected validators become less responsive, potentially impacting:
   - Block proposal latency
   - Vote responsiveness
   - Overall network liveness

3. **Resource exhaustion attack surface**: Byzantine validators can intentionally trigger this by withholding shares, causing honest validators to accumulate leaked resources until performance severely degrades or nodes crash.

4. **Cascading failures**: As nodes slow down, they may fail to respond to share requests from other nodes, creating a positive feedback loop of resource accumulation across the network.

The vulnerability breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits" - the unbounded queue growth and task accumulation violate computational resource limits.

## Likelihood Explanation

**Likelihood: Medium to High**

This issue is likely to occur in production environments because:

1. **Natural network conditions**: Temporary network partitions, validator downtime, or high latency can prevent reaching aggregation threshold
2. **Byzantine tolerance design**: The system is designed to tolerate up to 1/3 Byzantine validators, meaning share aggregation failures are expected operational scenarios
3. **Cryptographic failures**: Legitimate aggregation errors can occur due to implementation bugs or edge cases
4. **No timeout mechanism**: Tasks retry indefinitely without any cleanup timeout, so even brief failures cause permanent resource leaks
5. **Accumulation over time**: Each failed batch adds to the leak, compounding the problem during extended periods of network stress

The vulnerability requires no attacker sophistication - natural consensus failures trigger it, and Byzantine validators can deliberately exploit it.

## Recommendation

Implement bounded queue management with timeout-based cleanup:

1. **Add TTL to QueueItems**: Track insertion timestamp and remove items that exceed a configurable timeout (e.g., 60 seconds)
2. **Implement periodic cleanup**: In the main loop, periodically scan the queue and remove stale items:
   - Drop their `DropGuard` objects to abort tasks
   - Log appropriate warnings for monitoring
3. **Add queue size limits**: Enforce maximum queue depth and reject/drop oldest items when exceeded
4. **Add task timeout**: Modify `spawn_share_requester_task` to include a timeout on the reliable broadcast operation
5. **Monitor queue metrics**: The existing `DEC_QUEUE_SIZE` metric should trigger alerts when queue grows beyond expected bounds

Example fix outline:

```rust
// Add to QueueItem
struct QueueItem {
    ordered_blocks: OrderedBlocks,
    offsets_by_round: HashMap<Round, usize>,
    pending_secret_key_rounds: HashSet<Round>,
    share_requester_handles: Option<Vec<DropGuard>>,
    created_at: Instant,  // NEW
}

// In SecretShareManager::start() loop
if let Some(expired_items) = self.block_queue.remove_expired_items(Duration::from_secs(60)) {
    warn!("Cleaned up {} expired secret sharing queue items", expired_items.len());
    // DropGuards automatically abort tasks when dropped
}
```

## Proof of Concept

```rust
// Rust reproduction test case
#[tokio::test]
async fn test_queue_item_leak_on_failed_aggregation() {
    // Setup: Create SecretShareManager with mock components
    let (mut manager, mut incoming_blocks_rx, _rpc_rx, mut reset_rx) = 
        setup_test_secret_share_manager();
    
    // Simulate blocks arriving that will never aggregate
    let blocks = create_test_ordered_blocks(vec![1, 2, 3]);
    
    // Send blocks to manager
    incoming_blocks_rx.send(blocks).await.unwrap();
    
    // Allow processing
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    // Observe: Queue contains items
    assert!(manager.block_queue.queue().len() > 0);
    
    // Simulate Byzantine validators not sending shares
    // (don't send any share responses)
    
    // Wait extended period
    tokio::time::sleep(Duration::from_secs(30)).await;
    
    // BUG: Queue items still present, tasks still running
    assert!(manager.block_queue.queue().len() > 0);
    
    // Monitor system resources
    // - Memory usage: Growing with queue size
    // - Task count: Accumulating in tokio runtime
    // - Network activity: Continuous RPC retries
    
    // Expected: Items should be cleaned up after timeout
    // Actual: Items remain indefinitely, tasks keep retrying
}
```

To demonstrate in a running network:
1. Deploy validator with instrumented code to track queue size and task count
2. Introduce network partition or Byzantine behavior (withhold shares)
3. Monitor metrics over time showing unbounded growth
4. Observe performance degradation as resources accumulate

## Notes

The vulnerability is particularly concerning because:
- It can be triggered passively by network conditions without active attacks
- Byzantine validators can deliberately exploit it to DoS honest nodes
- The exponential backoff provides some mitigation but tasks never truly stop
- Production networks under stress are most vulnerable when they need reliability most
- The issue compounds over time rather than self-correcting [8](#0-7) [9](#0-8)

### Citations

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L112-130)
```rust
    async fn process_incoming_blocks(&mut self, blocks: OrderedBlocks) {
        let rounds: Vec<u64> = blocks.ordered_blocks.iter().map(|b| b.round()).collect();
        info!(rounds = rounds, "Processing incoming blocks.");

        let mut share_requester_handles = Vec::new();
        let mut pending_secret_key_rounds = HashSet::new();
        for block in blocks.ordered_blocks.iter() {
            let handle = self.process_incoming_block(block).await;
            share_requester_handles.push(handle);
            pending_secret_key_rounds.insert(block.round());
        }

        let queue_item = QueueItem::new(
            blocks,
            Some(share_requester_handles),
            pending_secret_key_rounds,
        );
        self.block_queue.push_back(queue_item);
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L237-277)
```rust
    fn spawn_share_requester_task(&self, metadata: SecretShareMetadata) -> DropGuard {
        let rb = self.reliable_broadcast.clone();
        let aggregate_state = Arc::new(SecretShareAggregateState::new(
            self.secret_share_store.clone(),
            metadata.clone(),
            self.config.clone(),
        ));
        let epoch_state = self.epoch_state.clone();
        let secret_share_store = self.secret_share_store.clone();
        let task = async move {
            // TODO(ibalajiarun): Make this configurable
            tokio::time::sleep(Duration::from_millis(300)).await;
            let maybe_existing_shares = secret_share_store.lock().get_all_shares_authors(&metadata);
            if let Some(existing_shares) = maybe_existing_shares {
                let epoch = epoch_state.epoch;
                let request = RequestSecretShare::new(metadata.clone());
                let targets = epoch_state
                    .verifier
                    .get_ordered_account_addresses_iter()
                    .filter(|author| !existing_shares.contains(author))
                    .collect::<Vec<_>>();
                info!(
                    epoch = epoch,
                    round = metadata.round,
                    "[SecretShareManager] Start broadcasting share request for {}",
                    targets.len(),
                );
                rb.multicast(request, aggregate_state, targets)
                    .await
                    .expect("Broadcast cannot fail");
                info!(
                    epoch = epoch,
                    round = metadata.round,
                    "[SecretShareManager] Finish broadcasting share request",
                );
            }
        };
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(task, abort_registration));
        DropGuard::new(abort_handle)
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L167-205)
```rust
            loop {
                tokio::select! {
                    Some((receiver, result)) = rpc_futures.next() => {
                        let aggregating = aggregating.clone();
                        let future = executor.spawn(async move {
                            (
                                    receiver,
                                    result
                                        .and_then(|msg| {
                                            msg.try_into().map_err(|e| anyhow::anyhow!("{:?}", e))
                                        })
                                        .and_then(|ack| aggregating.add(receiver, ack)),
                            )
                        }).await;
                        aggregate_futures.push(future);
                    },
                    Some(result) = aggregate_futures.next() => {
                        let (receiver, result) = result.expect("spawned task must succeed");
                        match result {
                            Ok(may_be_aggragated) => {
                                if let Some(aggregated) = may_be_aggragated {
                                    return Ok(aggregated);
                                }
                            },
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
                        }
                    },
                    else => unreachable!("Should aggregate with all responses")
                }
            }
```

**File:** crates/reliable-broadcast/src/lib.rs (L222-236)
```rust
pub struct DropGuard {
    abort_handle: AbortHandle,
}

impl DropGuard {
    pub fn new(abort_handle: AbortHandle) -> Self {
        Self { abort_handle }
    }
}

impl Drop for DropGuard {
    fn drop(&mut self) {
        self.abort_handle.abort();
    }
}
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L17-22)
```rust
pub struct QueueItem {
    ordered_blocks: OrderedBlocks,
    offsets_by_round: HashMap<Round, usize>,
    pending_secret_key_rounds: HashSet<Round>,
    share_requester_handles: Option<Vec<DropGuard>>,
}
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L60-77)
```rust
    pub fn is_fully_secret_shared(&self) -> bool {
        self.pending_secret_key_rounds.is_empty()
    }

    pub fn set_secret_shared_key(&mut self, round: Round, key: SecretSharedKey) {
        let offset = self.offset(round);
        if self.pending_secret_key_rounds.contains(&round) {
            observe_block(
                self.blocks()[offset].timestamp_usecs(),
                BlockStage::SECRET_SHARING_ADD_DECISION,
            );
            let block = &self.blocks_mut()[offset];
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
                tx.secret_shared_key_tx.take().map(|tx| tx.send(Some(key)));
            }
            self.pending_secret_key_rounds.remove(&round);
        }
    }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L112-127)
```rust
    pub fn dequeue_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.is_fully_secret_shared() {
                let (_, item) = self.queue.pop_first().expect("First key must exist");
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::SECRET_SHARING_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        ready_prefix
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L55-70)
```rust
        tokio::task::spawn_blocking(move || {
            let maybe_key = SecretShare::aggregate(self.shares.values(), &dec_config);
            match maybe_key {
                Ok(key) => {
                    let dec_key = SecretSharedKey::new(metadata, key);
                    let _ = decision_tx.unbounded_send(dec_key);
                },
                Err(e) => {
                    warn!(
                        epoch = metadata.epoch,
                        round = metadata.round,
                        "Aggregation error: {e}"
                    );
                },
            }
        });
```
