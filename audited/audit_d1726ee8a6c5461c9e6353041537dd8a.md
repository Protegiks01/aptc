# Audit Report

## Title
Database Iterator Length Mismatch Causes Indexer Service Crash and Infinite Loop

## Summary
The `DBIndexer::get_main_db_iter()` function uses Rust's `zip()` operation to combine three database iterators (transactions, events, writesets). When database corruption causes these iterators to have different lengths, the zipped iterator terminates early, leading to an assertion panic that crashes the indexer service. If corruption persists, this creates an infinite crash-restart loop, causing a denial of service for the indexer component.

## Finding Description

The vulnerability exists in the iterator pattern used to process transactions for indexing. The function creates three separate iterators that are expected to have identical lengths: [1](#0-0) 

These iterators have different behaviors when data is missing:

1. **Transaction iterator**: Uses `ContinuousVersionIter` which returns `Ok(None)` if the underlying database iterator exhausts before reaching `end_version` [2](#0-1) 

2. **Events iterator**: Uses `EventsByVersionIter` which always produces exactly `num_versions` items (possibly empty vectors) [3](#0-2) 

3. **WriteSet iterator**: Uses `ContinuousVersionIter` (same behavior as transaction iterator) [4](#0-3) 

**Critical flaw**: When `ContinuousVersionIter`'s underlying database iterator returns `None` (due to missing data), it returns `Ok(None)` without verifying it has produced the expected number of items. This causes Rust's `zip()` to terminate early when the shortest iterator exhausts.

The processing code then hits an assertion that expects all items to have been processed: [5](#0-4) 

**Attack scenario**: 
1. Database corruption occurs (hardware failure, incomplete write, crash during commit, pruning bug)
2. One database store (e.g., WriteSetDB) has fewer entries than others for a version range
3. Indexer attempts to process the corrupted range
4. The shortest iterator exhausts first (e.g., after 3 items instead of 5)
5. `zip()` terminates with only 3 items processed
6. `try_for_each` completes successfully since no error occurred
7. Assertion `assert_eq!(num_transactions, version - start_version)` fails: `5 != 3`
8. **Panic crashes the indexer service**
9. Service restarts and attempts same batch â†’ infinite crash loop

**Contrast with backup handler**: The backup code explicitly avoids this pattern and uses manual iterator coordination with proper error checking: [6](#0-5) 

This pattern explicitly returns `NotFound` errors when iterators have mismatched lengths, rather than silently terminating.

**Important clarifications**:
- This does NOT "fail silently" - it fails with a loud panic
- This does NOT "create indexing gaps" - the partial batch is never committed to the indexer DB
- The actual impact is **service availability**, not data corruption

## Impact Explanation

**Severity: HIGH** (per Aptos Bug Bounty criteria: "API crashes")

This vulnerability causes:

1. **Indexer Service Crashes**: The assertion panic terminates the indexer process [7](#0-6) 

2. **Infinite Crash Loop**: If database corruption persists (e.g., permanent data loss), the indexer enters an infinite crash-restart cycle

3. **Denial of Service**: The indexer component becomes unavailable, breaking:
   - Event indexing and queries
   - Transaction-by-account lookups  
   - State key indexing
   - Downstream services depending on the indexer API

4. **Requires Manual Intervention**: Operators must manually identify and repair database corruption to restore service

**Why not Critical**: 
- Does not affect consensus or validator operation
- Does not cause fund loss or state corruption
- Does not affect the main ledger DB (only indexer DB)
- No incorrect data is persisted (partial batches are discarded)

**Why High**:
- Matches "API crashes" category explicitly
- Can cause extended service unavailability
- Affects critical indexing infrastructure

## Likelihood Explanation

**Likelihood: MEDIUM**

**When this can occur**:
- Hardware failures causing incomplete writes to RocksDB
- Process crashes during database commits
- Bugs in pruning logic causing inconsistent deletion across stores
- Incomplete backup restoration creating mismatched data
- Database corruption from power failures or disk errors
- Manual administrative errors (selective deletion from one store)

**Why not High likelihood**:
- Requires database-level corruption (not normal operation)
- AptosDB includes consistency checks and atomic commit mechanisms
- Multiple safeguards exist in the write path

**Why not Low likelihood**:
- Database corruption scenarios do occur in production systems
- The indexer continuously processes large volumes of data, increasing exposure
- No defensive checks exist to detect and handle this condition gracefully

## Recommendation

**Primary Fix**: Adopt the explicit iterator coordination pattern used in the backup handler. Replace `zip()` with manual iteration and explicit length checking:

```rust
fn get_main_db_iter(
    &self,
    start_version: Version,
    num_transactions: u64,
) -> Result<impl Iterator<Item = Result<(Transaction, Vec<ContractEvent>, WriteSet)>> + '_> {
    let txn_iter = self
        .main_db_reader
        .get_transaction_iterator(start_version, num_transactions)?;
    let mut event_vec_iter = self
        .main_db_reader
        .get_events_iterator(start_version, num_transactions)?;
    let mut writeset_iter = self
        .main_db_reader
        .get_write_set_iterator(start_version, num_transactions)?;

    Ok(txn_iter.enumerate().map(move |(idx, txn_res)| {
        let version = start_version + idx;
        
        let txn = txn_res?;
        let events = event_vec_iter.next().ok_or_else(|| {
            AptosDbError::NotFound(format!(
                "Events not found when Transaction exists, version {}",
                version
            ))
        })??;
        let writeset = writeset_iter.next().ok_or_else(|| {
            AptosDbError::NotFound(format!(
                "WriteSet not found when Transaction exists, version {}",
                version
            ))
        })??;
        
        Ok((txn, events, writeset))
    }))
}
```

**Secondary Fix**: Improve `ContinuousVersionIter` to validate it has produced the expected count:

```rust
fn next_impl(&mut self) -> Result<Option<T>> {
    if self.expected_next_version >= self.end_version {
        return Ok(None);
    }

    let ret = match self.inner.next().transpose()? {
        Some((version, transaction)) => {
            ensure!(
                version == self.expected_next_version,
                "{} iterator: first version {}, expecting version {}, got {} from underlying iterator.",
                std::any::type_name::<T>(),
                self.first_version,
                self.expected_next_version,
                version,
            );
            self.expected_next_version += 1;
            Some(transaction)
        },
        None => {
            // NEW: Validate we produced the expected number of items
            ensure!(
                self.expected_next_version == self.end_version,
                "{} iterator: expected {} items (versions {}-{}), but only produced {} items",
                std::any::type_name::<T>(),
                self.end_version - self.first_version,
                self.first_version,
                self.end_version - 1,
                self.expected_next_version - self.first_version,
            );
            None
        }
    };

    Ok(ret)
}
```

**Additional Recommendation**: Add database consistency validation:
- Periodic checks comparing record counts across stores
- Corruption detection during indexer startup
- Graceful error handling with operator alerts

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_types::transaction::Transaction;
    use std::sync::Arc;

    // Mock DbReader that simulates database inconsistency
    struct InconsistentMockDbReader {
        // Transaction DB has versions 0-9
        // Events DB has versions 0-9  
        // WriteSet DB has versions 0-6 (missing 7-9)
    }

    impl DbReader for InconsistentMockDbReader {
        fn get_transaction_iterator(
            &self,
            start_version: Version,
            limit: u64,
        ) -> Result<Box<dyn Iterator<Item = Result<Transaction>> + '_>> {
            // Returns 10 transactions (versions 0-9)
            Ok(Box::new((start_version..start_version + limit).map(|v| {
                Ok(Transaction::StateCheckpoint(HashValue::zero()))
            })))
        }

        fn get_events_iterator(
            &self,
            start_version: Version,
            limit: u64,
        ) -> Result<Box<dyn Iterator<Item = Result<Vec<ContractEvent>>> + '_>> {
            // Returns 10 event vectors (versions 0-9)
            Ok(Box::new((start_version..start_version + limit).map(|_| {
                Ok(vec![])
            })))
        }

        fn get_write_set_iterator(
            &self,
            start_version: Version,
            limit: u64,
        ) -> Result<Box<dyn Iterator<Item = Result<WriteSet>> + '_>> {
            // Simulates corruption: only returns 7 writesets (versions 0-6)
            // Missing versions 7-9
            let corrupted_limit = std::cmp::min(limit, 7);
            Ok(Box::new((start_version..start_version + corrupted_limit).map(|_| {
                Ok(WriteSet::default())
            })))
        }
        
        // ... other required trait methods with default implementations
    }

    #[test]
    #[should_panic(expected = "assertion failed")]
    fn test_iterator_length_mismatch_causes_panic() {
        let config = InternalIndexerDBConfig::default();
        let db = Arc::new(/* create test DB */);
        let indexer_db = InternalIndexerDB::new(db, config);
        
        let mock_reader = Arc::new(InconsistentMockDbReader {});
        let indexer = DBIndexer::new(indexer_db, mock_reader);
        
        // This will panic when assertion fails:
        // assert_eq!(10, 7) at line 503
        let result = indexer.process_a_batch(0, 10);
        
        // We never reach here - the panic occurs first
        assert!(result.is_ok());
    }
}
```

**To reproduce in a real environment**:
1. Set up an AptosDB instance with indexer enabled
2. Manually delete entries from WriteSetDB for a version range (e.g., versions 1003-1005)
3. Start the indexer to process that range
4. Observe panic with message: `assertion failed: num_transactions == version - start_version`
5. Observe indexer crash-restart loop as it repeatedly attempts the corrupted batch

## Notes

**Key Distinction**: The original security question states this can "fail silently and create indexing gaps" - both characterizations are incorrect:
- **Not silent**: Fails with a visible assertion panic that crashes the service
- **No indexing gaps**: The partial batch is never committed, so no incorrect state is persisted

However, the **core vulnerability is valid**: database inconsistencies cause service crashes and potential infinite crash loops, which constitutes a High severity availability issue per Aptos Bug Bounty criteria.

### Citations

**File:** storage/indexer/src/db_indexer.rs (L356-380)
```rust
    fn get_main_db_iter(
        &self,
        start_version: Version,
        num_transactions: u64,
    ) -> Result<impl Iterator<Item = Result<(Transaction, Vec<ContractEvent>, WriteSet)>> + '_>
    {
        let txn_iter = self
            .main_db_reader
            .get_transaction_iterator(start_version, num_transactions)?;
        let event_vec_iter = self
            .main_db_reader
            .get_events_iterator(start_version, num_transactions)?;
        let writeset_iter = self
            .main_db_reader
            .get_write_set_iterator(start_version, num_transactions)?;
        let zipped = txn_iter.zip(event_vec_iter).zip(writeset_iter).map(
            |((txn_res, event_vec_res), writeset_res)| {
                let txn = txn_res?;
                let event_vec = event_vec_res?;
                let writeset = writeset_res?;
                Ok((txn, event_vec, writeset))
            },
        );
        Ok(zipped)
    }
```

**File:** storage/indexer/src/db_indexer.rs (L418-503)
```rust
        db_iter.try_for_each(|res| {
            let (txn, events, writeset) = res?;
            if let Some(signed_txn) = txn.try_as_signed_user_txn() {
                if self.indexer_db.transaction_enabled() {
                    if let ReplayProtector::SequenceNumber(seq_num) = signed_txn.replay_protector()
                    {
                        batch.put::<OrderedTransactionByAccountSchema>(
                            &(signed_txn.sender(), seq_num),
                            &version,
                        )?;
                    }
                }
            }

            if self.indexer_db.event_enabled() {
                events.iter().enumerate().try_for_each(|(idx, event)| {
                    if let ContractEvent::V1(v1) = event {
                        batch
                            .put::<EventByKeySchema>(
                                &(*v1.key(), v1.sequence_number()),
                                &(version, idx as u64),
                            )
                            .expect("Failed to put events by key to a batch");
                        batch
                            .put::<EventByVersionSchema>(
                                &(*v1.key(), version, v1.sequence_number()),
                                &(idx as u64),
                            )
                            .expect("Failed to put events by version to a batch");
                    }
                    if self.indexer_db.event_v2_translation_enabled() {
                        if let ContractEvent::V2(v2) = event {
                            if let Some(translated_v1_event) =
                                self.translate_event_v2_to_v1(v2).map_err(|e| {
                                    anyhow::anyhow!(
                                        "Failed to translate event: {:?}. Error: {}",
                                        v2,
                                        e
                                    )
                                })?
                            {
                                let key = *translated_v1_event.key();
                                let sequence_number = translated_v1_event.sequence_number();
                                self.event_v2_translation_engine
                                    .cache_sequence_number(&key, sequence_number);
                                event_keys.insert(key);
                                batch
                                    .put::<EventByKeySchema>(
                                        &(key, sequence_number),
                                        &(version, idx as u64),
                                    )
                                    .expect("Failed to put events by key to a batch");
                                batch
                                    .put::<EventByVersionSchema>(
                                        &(key, version, sequence_number),
                                        &(idx as u64),
                                    )
                                    .expect("Failed to put events by version to a batch");
                                batch
                                    .put::<TranslatedV1EventSchema>(
                                        &(version, idx as u64),
                                        &translated_v1_event,
                                    )
                                    .expect("Failed to put translated v1 events to a batch");
                            }
                        }
                    }
                    Ok::<(), AptosDbError>(())
                })?;
            }

            if self.indexer_db.statekeys_enabled() {
                writeset.write_op_iter().for_each(|(state_key, write_op)| {
                    if write_op.is_creation() || write_op.is_modification() {
                        batch
                            .put::<StateKeysSchema>(state_key, &())
                            .expect("Failed to put state keys to a batch");
                    }
                });
            }
            version += 1;
            Ok::<(), AptosDbError>(())
        })?;
        assert!(version > 0, "batch number should be greater than 0");

        assert_eq!(num_transactions, version - start_version);
```

**File:** storage/aptosdb/src/utils/iterators.rs (L40-62)
```rust
    fn next_impl(&mut self) -> Result<Option<T>> {
        if self.expected_next_version >= self.end_version {
            return Ok(None);
        }

        let ret = match self.inner.next().transpose()? {
            Some((version, transaction)) => {
                ensure!(
                    version == self.expected_next_version,
                    "{} iterator: first version {}, expecting version {}, got {} from underlying iterator.",
                    std::any::type_name::<T>(),
                    self.first_version,
                    self.expected_next_version,
                    version,
                );
                self.expected_next_version += 1;
                Some(transaction)
            },
            None => None,
        };

        Ok(ret)
    }
```

**File:** storage/aptosdb/src/utils/iterators.rs (L263-285)
```rust
    fn next_impl(&mut self) -> Result<Option<Vec<ContractEvent>>> {
        if self.expected_next_version >= self.end_version {
            return Ok(None);
        }

        let mut ret = Vec::new();
        while let Some(res) = self.inner.peek() {
            let ((version, _index), _event) = res
                .as_ref()
                .map_err(|e| AptosDbError::Other(format!("Hit error iterating events: {}", e)))?;
            if *version != self.expected_next_version {
                break;
            }
            let ((_version, _index), event) =
                self.inner.next().transpose()?.expect("Known to exist.");
            ret.push(event);
        }
        self.expected_next_version = self
            .expected_next_version
            .checked_add(1)
            .ok_or_else(|| AptosDbError::Other("expected version overflowed.".to_string()))?;
        Ok(Some(ret))
    }
```

**File:** storage/aptosdb/src/ledger_db/write_set_db.rs (L63-72)
```rust
    /// Returns an iterator that yields `num_transactions` write sets starting from `start_version`.
    pub(crate) fn get_write_set_iter(
        &self,
        start_version: Version,
        num_transactions: usize,
    ) -> Result<impl Iterator<Item = Result<WriteSet>> + '_> {
        let mut iter = self.db.iter::<WriteSetSchema>()?;
        iter.seek(&start_version)?;
        iter.expect_continuous_versions(start_version, num_transactions)
    }
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L77-99)
```rust
        let zipped = txn_iter.enumerate().map(move |(idx, txn_res)| {
            let version = start_version + idx as u64; // overflow is impossible since it's check upon txn_iter construction.

            let txn = txn_res?;
            let txn_info = txn_info_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "TransactionInfo not found when Transaction exists, version {}",
                    version
                ))
            })??;
            let event_vec = event_vec_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "Events not found when Transaction exists., version {}",
                    version
                ))
            })??;
            let write_set = write_set_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "WriteSet not found when Transaction exists, version {}",
                    version
                ))
            })??;
            let persisted_aux_info = persisted_aux_info_iter.next().ok_or_else(|| {
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/internal_indexer_db_service.rs (L183-183)
```rust
            let next_version = self.db_indexer.process(start_version, target_version)?;
```
