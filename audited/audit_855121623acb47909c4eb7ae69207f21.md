# Audit Report

## Title
Unhandled Panic in Pruner Worker Causes Validator Denial of Service via Database Inconsistency and Crash Loop

## Summary
The pruner worker thread in `pruner_worker.rs` does not catch panics from the underlying `pruner.prune()` call. When a panic occurs during parallel sub-pruner execution, it leaves the database in an inconsistent state with mismatched progress markers between the main pruner and sub-pruners. If the panic is caused by reproducible conditions (corrupted data, edge case bug), the validator enters a permanent crash loop during recovery, causing a denial of service.

## Finding Description

The `work()` function calls `pruner.prune()` without panic handling: [1](#0-0) 

When `LedgerPruner::prune()` executes, it follows this sequence: [2](#0-1) 

The critical issue occurs at the parallel sub-pruner execution step. The ledger metadata pruner atomically updates the `LedgerPrunerProgress` BEFORE the sub-pruners run: [3](#0-2) 

If any sub-pruner panics during the parallel execution (line 78-84 of ledger_pruner/mod.rs), rayon propagates the panic to the calling thread, terminating the worker. This leaves:

1. **LedgerPrunerProgress** = updated to target version
2. **Some sub-pruners** = completed with updated progress  
3. **Some sub-pruners** = incomplete with old progress
4. **In-memory progress** (line 87) = never updated

On restart, the recovery mechanism attempts to reconcile this: [4](#0-3) 

Each sub-pruner catches up during initialization: [5](#0-4) 

**The vulnerability**: If the panic was caused by reproducible conditions (e.g., corrupted database entries, integer overflow, unexpected data format in specific version range), the catch-up will hit the same panic condition, creating an infinite crash loop.

Additionally, the Drop implementation panics when detecting worker thread panic: [6](#0-5) 

This creates a double-panic scenario (panic during unwinding) which causes immediate process abortion in Rust, preventing any further cleanup.

## Impact Explanation

**Severity: High** - Validator node unavailability

This vulnerability causes **total loss of validator liveness** when triggered:

1. **Validator becomes permanently unavailable**: The crash loop prevents the validator from participating in consensus, processing transactions, or serving queries.

2. **Network degradation**: Loss of validators reduces network capacity and moves closer to the 1/3 Byzantine threshold for consensus failure.

3. **No automatic recovery**: Manual intervention is required to either fix the corrupted data or patch the pruning code.

4. **State inconsistency persists**: The database remains in a partially-pruned state with mismatched progress markers between different pruners, potentially causing issues even after recovery.

This meets the **High Severity** criteria from the Aptos bug bounty program: "Validator node slowdowns" and "Significant protocol violations". While not a complete network partition, permanent loss of validator availability significantly impacts network security and performance.

## Likelihood Explanation

**Likelihood: Medium**

This vulnerability requires specific triggering conditions:

1. **A panic-causing condition must exist** in the pruning code or data. Potential sources:
   - Corrupted database entries in specific version ranges
   - Edge cases in pruning logic (integer overflow, unexpected data format)
   - Resource exhaustion (OOM) during large pruning batches
   - Bugs in RocksDB write operations

2. **The condition must be reproducible** across restarts to create the crash loop. One-time transient panics would only cause temporary unavailability.

3. **The condition must be reachable** through normal network operation or attacker-controlled inputs.

While specific exploitation paths are implementation-dependent, the lack of panic handling is a significant robustness gap. Historical examples of database corruption or pruning bugs in blockchain systems suggest this is not merely theoretical.

## Recommendation

Implement panic catching at the thread boundary using `std::panic::catch_unwind`:

```rust
fn work(&self) {
    while !self.quit_worker.load(Ordering::SeqCst) {
        // Catch any panics from the pruner
        let pruner_result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
            self.pruner.prune(self.batch_size)
        }));
        
        match pruner_result {
            Ok(Ok(_)) => {
                // Successful pruning
                if !self.pruner.is_pruning_pending() {
                    sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                }
            }
            Ok(Err(e)) => {
                // Regular error from prune()
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?e, "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
            Err(panic_err) => {
                // Panic was caught
                error!(
                    panic = ?panic_err,
                    "Pruner panicked! This indicates a serious bug. Pausing pruning."
                );
                // Sleep longer after panic to avoid tight crash loop
                sleep(Duration::from_secs(10));
            }
        }
    }
}
```

Additionally, modify the Drop implementation to not panic on worker thread panic:

```rust
impl Drop for PrunerWorker {
    fn drop(&mut self) {
        self.inner.stop_pruning();
        if let Some(worker_thread) = self.worker_thread.take() {
            if let Err(e) = worker_thread.join() {
                error!(
                    worker_name = self.worker_name,
                    error = ?e,
                    "Pruner worker thread panicked during execution"
                );
            }
        }
    }
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod panic_safety_test {
    use super::*;
    use std::sync::atomic::{AtomicBool, Ordering};
    use std::sync::Arc;

    // Mock pruner that panics on second call
    struct PanickyPruner {
        call_count: Arc<AtomicBool>,
    }

    impl DBPruner for PanickyPruner {
        fn name(&self) -> &'static str { "panicky" }
        
        fn prune(&self, _batch_size: usize) -> Result<Version> {
            if self.call_count.swap(true, Ordering::SeqCst) {
                panic!("Simulated panic in pruning");
            }
            Ok(100)
        }
        
        fn progress(&self) -> Version { 100 }
        fn set_target_version(&self, _: Version) {}
        fn target_version(&self) -> Version { 200 }
        fn record_progress(&self, _: Version) {}
    }

    #[test]
    #[should_panic(expected = "thread panicked")]
    fn test_panic_in_pruner_crashes_worker() {
        let pruner = Arc::new(PanickyPruner {
            call_count: Arc::new(AtomicBool::new(false)),
        });
        
        let worker = PrunerWorker::new(pruner, 100, "test");
        
        // Wait for worker to panic
        std::thread::sleep(std::time::Duration::from_secs(1));
        
        // Drop will panic when joining panicked thread
        drop(worker);
    }
}
```

## Notes

The security question's claim that "Drop is never called" is incorrect - the Drop implementation IS called when PrunerWorker goes out of scope. However, the Drop implementation itself panics when detecting the worker thread panic, causing a double-panic that aborts the process.

The database inconsistency is recoverable through the catch-up mechanism, but only if the panic condition is transient. Reproducible panics cause permanent validator unavailability requiring manual intervention.

### Citations

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L105-119)
```rust
impl Drop for PrunerWorker {
    fn drop(&mut self) {
        self.inner.stop_pruning();
        self.worker_thread
            .take()
            .unwrap_or_else(|| panic!("Pruner worker ({}) thread must exist.", self.worker_name))
            .join()
            .unwrap_or_else(|e| {
                panic!(
                    "Pruner worker ({}) thread should join peacefully: {e:?}",
                    self.worker_name
                )
            });
    }
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L118-134)
```rust
    pub fn new(
        ledger_db: Arc<LedgerDb>,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        info!(name = LEDGER_PRUNER_NAME, "Initializing...");

        let ledger_metadata_pruner = Box::new(
            LedgerMetadataPruner::new(ledger_db.metadata_db_arc())
                .expect("Failed to initialize ledger_metadata_pruner."),
        );

        let metadata_progress = ledger_metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created ledger metadata pruner, start catching up all sub pruners."
        );
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_metadata_pruner.rs (L42-56)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();
        for version in current_progress..target_version {
            batch.delete::<VersionDataSchema>(&version)?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::LedgerPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_metadata_db.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L77-104)
```rust
impl TransactionPruner {
    pub(in crate::pruner) fn new(
        transaction_store: Arc<TransactionStore>,
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_db_raw(),
            &DbMetadataKey::TransactionPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionPruner {
            transaction_store,
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```
