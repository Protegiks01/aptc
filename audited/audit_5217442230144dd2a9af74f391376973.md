# Audit Report

## Title
Unbounded Memory Allocation in Backup Restore Causes Validator Node Memory Exhaustion

## Summary
The backup restore process lacks size validation on transaction chunks, allowing a maliciously crafted backup file to trigger excessive memory allocation in the Merkle accumulator's `to_freeze` vector during the `append()` operation. This can consume gigabytes of validator memory, causing node slowdowns or crashes.

## Finding Description

The vulnerability exists in the backup restore code path where transaction chunks are processed without size limits. The attack chain is:

1. **Chunk Loading Without Bounds**: The `LoadedChunk::load()` function reads all transactions from a backup chunk file into memory without enforcing size limits, only validating that the count matches the manifest. [1](#0-0) 

2. **Unbounded Batch Processing**: In `save_before_replay_version()`, when transactions are before `first_to_replay`, they are ALL passed to `save_transactions()` in a single call without chunking: [2](#0-1) 

The variable `num_to_save` at line 500 can equal the entire chunk size with no upper bound.

3. **Memory Allocation in Accumulator**: This triggers `MerkleAccumulator::append()` with potentially millions of leaves, which allocates: [3](#0-2) 

The capacity calculation `num_new_leaves * 2 + root_level` means:
- 1 million txns → 2 million * 40 bytes = 80 MB
- 10 million txns → 20 million * 40 bytes = 800 MB  
- 100 million txns → 200 million * 40 bytes = 8 GB

Each Node consists of `(Position, HashValue)` which is 8 bytes + 32 bytes = 40 bytes. [4](#0-3) 

4. **Bypass of Normal Limits**: This bypasses the `BATCH_SIZE = 10000` limit used during replay operations: [5](#0-4) 

The replay path correctly uses `.try_chunks(BATCH_SIZE)` but the save-before-replay path does not. [6](#0-5) 

**Attack Scenario:**
1. Attacker creates a backup file with a single `TransactionChunk` spanning millions of real transactions
2. The chunk has valid `TransactionAccumulatorRangeProof` and `LedgerInfoWithSignatures` (obtained from historical blockchain data)
3. Victim node restores from this backup during disaster recovery
4. All transactions in the chunk are loaded and processed in one batch
5. Validator allocates gigabytes of memory for the `to_freeze` vector
6. Node experiences memory exhaustion, slowdown, or OOM crash

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program criteria: "Validator node slowdowns" and "API crashes."

The vulnerability can cause:
- **Memory Exhaustion**: Allocation of 1-10+ GB for a single operation
- **Node Slowdown**: Massive allocation impacts overall node performance  
- **Potential Crashes**: OOM killer may terminate the validator process
- **Degraded Availability**: Restore operations fail or take excessive time

While it requires the victim to restore from a malicious backup, disaster recovery is a critical operation where nodes may accept backups from various sources. The impact on validator availability during restore is significant.

## Likelihood Explanation

**Likelihood: Medium**

**Attacker Requirements:**
- Access to historical blockchain transaction data (publicly available)
- Ability to construct valid backup files with cryptographic proofs
- Distribution channel to provide backup to victims (e.g., compromised backup storage)

**Feasibility:**
- Aptos mainnet has billions of transactions, making large chunks technically valid
- Backup files with valid signatures from historical ledger states can be constructed
- During network disruptions, operators may restore from any available source
- No code-level validation prevents oversized chunks during restore

**Mitigating Factors:**
- Operators typically use trusted backup sources
- Requires specific timing (during restore operations)
- Doesn't affect normal consensus operations

## Recommendation

Add size validation in the restore path to enforce the same `BATCH_SIZE` limit used during replay:

```rust
// In save_before_replay_version() around line 498-517
const MAX_SAVE_BATCH_SIZE: usize = 10000;

if first_version < first_to_replay {
    let num_to_save = (min(first_to_replay, last_version + 1) - first_version) as usize;
    
    // Process in bounded batches to prevent memory exhaustion
    for chunk_start in (0..num_to_save).step_by(MAX_SAVE_BATCH_SIZE) {
        let chunk_end = min(chunk_start + MAX_SAVE_BATCH_SIZE, num_to_save);
        let chunk_size = chunk_end - chunk_start;
        
        let txns_to_save: Vec<_> = txns.drain(..chunk_size).collect();
        let persisted_aux_info_to_save: Vec<_> = persisted_aux_info.drain(..chunk_size).collect();
        let txn_infos_to_save: Vec<_> = txn_infos.drain(..chunk_size).collect();
        let event_vecs_to_save: Vec<_> = event_vecs.drain(..chunk_size).collect();
        let write_sets_to_save = write_sets.drain(..chunk_size).collect();
        
        tokio::task::spawn_blocking(move || {
            restore_handler.save_transactions(
                first_version + chunk_start as u64,
                &txns_to_save,
                &persisted_aux_info_to_save,
                &txn_infos_to_save,
                &event_vecs_to_save,
                write_sets_to_save,
            )
        }).await??;
    }
    // Update metrics...
}
```

Additionally, add validation when loading chunks: [7](#0-6) 

Add after line 145:
```rust
const MAX_CHUNK_TRANSACTIONS: usize = 1_000_000; // 1 million transaction limit
ensure!(
    txns.len() <= MAX_CHUNK_TRANSACTIONS,
    "Transaction chunk too large: {} transactions exceeds limit of {}",
    txns.len(),
    MAX_CHUNK_TRANSACTIONS
);
```

## Proof of Concept

```rust
#[cfg(test)]
mod excessive_memory_test {
    use super::*;
    
    #[test]
    fn test_large_chunk_memory_allocation() {
        // Simulate loading a backup chunk with 10 million transactions
        const LARGE_CHUNK_SIZE: usize = 10_000_000;
        
        // Create mock transaction data
        let mut txn_hashes = Vec::with_capacity(LARGE_CHUNK_SIZE);
        for i in 0..LARGE_CHUNK_SIZE {
            txn_hashes.push(HashValue::sha3_256_of(&i.to_le_bytes()));
        }
        
        // This will attempt to allocate:
        // capacity = 10_000_000 * 2 + 63 = 20_000_063 nodes
        // memory = 20_000_063 * 40 bytes = 800 MB
        
        let reader = MockHashReader::new();
        let result = MerkleAccumulator::<_, TransactionAccumulatorHasher>::append(
            &reader,
            0, // num_existing_leaves
            &txn_hashes,
        );
        
        // This should trigger excessive memory allocation
        // In production, this could be 100M+ transactions = 8GB+ memory
        assert!(result.is_ok() || result.is_err()); // Either succeeds with huge alloc or OOMs
    }
}
```

To demonstrate the vulnerability practically, create a backup file with the structure shown in the manifest types and observe memory consumption during restore with monitoring tools.

## Notes

This vulnerability specifically affects the backup restore code path and does not impact normal consensus operations, which are protected by block size limits. The issue arises from the inconsistency between the batching strategy used for replay operations (which correctly limits batches to 10,000) and the save-before-replay path (which processes entire chunks).

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L62-62)
```rust
const BATCH_SIZE: usize = if cfg!(test) { 2 } else { 10000 };
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L100-145)
```rust
    async fn load(
        manifest: TransactionChunk,
        storage: &Arc<dyn BackupStorage>,
        epoch_history: Option<&Arc<EpochHistory>>,
    ) -> Result<Self> {
        let mut file = BufReader::new(storage.open_for_read(&manifest.transactions).await?);
        let mut txns = Vec::new();
        let mut persisted_aux_info = Vec::new();
        let mut txn_infos = Vec::new();
        let mut event_vecs = Vec::new();
        let mut write_sets = Vec::new();

        while let Some(record_bytes) = file.read_record_bytes().await? {
            let (txn, aux_info, txn_info, events, write_set): (
                _,
                PersistedAuxiliaryInfo,
                _,
                _,
                WriteSet,
            ) = match manifest.format {
                TransactionChunkFormat::V0 => {
                    let (txn, txn_info, events, write_set) = bcs::from_bytes(&record_bytes)?;
                    (
                        txn,
                        PersistedAuxiliaryInfo::None,
                        txn_info,
                        events,
                        write_set,
                    )
                },
                TransactionChunkFormat::V1 => bcs::from_bytes(&record_bytes)?,
            };
            txns.push(txn);
            persisted_aux_info.push(aux_info);
            txn_infos.push(txn_info);
            event_vecs.push(events);
            write_sets.push(write_set);
        }

        ensure!(
            manifest.first_version + (txns.len() as Version) == manifest.last_version + 1,
            "Number of items in chunks doesn't match that in manifest. first_version: {}, last_version: {}, items in chunk: {}",
            manifest.first_version,
            manifest.last_version,
            txns.len(),
        );
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L498-517)
```rust
                    if first_version < first_to_replay {
                        let num_to_save =
                            (min(first_to_replay, last_version + 1) - first_version) as usize;
                        let txns_to_save: Vec<_> = txns.drain(..num_to_save).collect();
                        let persisted_aux_info_to_save: Vec<_> =
                            persisted_aux_info.drain(..num_to_save).collect();
                        let txn_infos_to_save: Vec<_> = txn_infos.drain(..num_to_save).collect();
                        let event_vecs_to_save: Vec<_> = event_vecs.drain(..num_to_save).collect();
                        let write_sets_to_save = write_sets.drain(..num_to_save).collect();
                        tokio::task::spawn_blocking(move || {
                            restore_handler.save_transactions(
                                first_version,
                                &txns_to_save,
                                &persisted_aux_info_to_save,
                                &txn_infos_to_save,
                                &event_vecs_to_save,
                                write_sets_to_save,
                            )
                        })
                        .await??;
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L575-576)
```rust
        let db_commit_stream = txns_to_execute_stream
            .try_chunks(BATCH_SIZE)
```

**File:** storage/accumulator/src/lib.rs (L125-125)
```rust
type Node = (Position, HashValue);
```

**File:** storage/accumulator/src/lib.rs (L258-258)
```rust
        let mut to_freeze = Vec::with_capacity(Self::max_to_freeze(num_new_leaves, root_level));
```
