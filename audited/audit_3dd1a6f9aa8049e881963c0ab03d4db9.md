# Audit Report

## Title
Resource Leak in ExecutorService: Incomplete Shutdown Causes Thread and Network Resource Accumulation on Repeated Start/Shutdown Cycles

## Summary
The `ExecutorService::shutdown()` function fails to properly clean up spawned threads, network connections, and thread pool resources. The spawned executor service thread's `JoinHandle` is never stored, making it impossible to join the thread during shutdown. Repeated start/shutdown cycles leak threads and network resources, eventually leading to system failure through thread exhaustion or file descriptor limits. [1](#0-0) [2](#0-1) 

## Finding Description

The vulnerability exists in multiple layers of improper resource cleanup:

**1. Lost Thread Handle:** In the `start()` method, a thread is spawned to run `executor_service.start()`, but the `JoinHandle` returned by `spawn()` is immediately dropped. This makes it impossible to wait for the thread to complete during shutdown. [1](#0-0) 

**2. Incomplete Shutdown:** The `shutdown()` method only calls `self.controller.shutdown()` but never:
- Joins the spawned executor service thread
- Sends a `Stop` command to the `ShardedExecutorService`
- Waits for the rayon thread pool to terminate [2](#0-1) 

**3. Infinite Loop in ShardedExecutorService:** The spawned thread runs `ShardedExecutorService::start()`, which loops indefinitely waiting for `ExecutorShardCommand::Stop` via `coordinator_client.receive_execute_command()`. [3](#0-2) 

**4. Stop Command Never Sent:** The `RemoteCoordinatorClient::receive_execute_command()` only returns `ExecutorShardCommand::Stop` when the command channel is closed, but `shutdown()` doesn't ensure this happens. [4](#0-3) 

**5. NetworkController Incomplete Cleanup:** The `NetworkController::shutdown()` admits it's "not a very clean shutdown" - it sends shutdown signals but doesn't wait for full cleanup completion. [5](#0-4) 

**6. Rayon Thread Pool Leak:** Each `ShardedExecutorService` creates a rayon `ThreadPool` with `num_threads + 2` threads that is never explicitly shut down. [6](#0-5) 

**Attack Scenario:**
If an operator or automated system attempts to restart the executor service without killing the process (error recovery, maintenance, configuration reload), each cycle leaks:
- 1 main executor service thread (blocked on channel receive)
- `num_threads + 2` rayon thread pool threads
- gRPC server threads and outbound handler tokio tasks
- Network channel resources

After sufficient cycles, the system exhausts thread limits or file descriptors, causing the validator node to fail.

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria:

**Medium Severity: State inconsistencies requiring intervention** - Resource exhaustion leads to validator node unavailability, requiring operator intervention to restart the process. While not permanent, this breaks the availability guarantees critical for blockchain consensus participation.

The impact affects:
- **Validator Availability**: Exhausted resources prevent the validator from processing new blocks
- **Consensus Participation**: A failed validator reduces network decentralization and resilience
- **Operational Reliability**: Operators cannot safely restart services without process termination

This violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits" - extended to system resources like threads and file descriptors.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability requires:
- Operator action to call start/shutdown multiple times without process restart
- OR automated restart logic triggered by errors/maintenance
- No direct attacker control over the lifecycle

While not easily triggered by external attackers, this is realistic in:
- **Error Recovery Systems**: Automated retry logic attempting to restart failed services
- **Configuration Reloads**: Hot-reload mechanisms that restart services
- **Maintenance Operations**: Operators attempting graceful service restarts
- **Testing/Development**: Repeated initialization cycles during testing

The `ProcessExecutorService` and `ThreadExecutorService` wrappers call `start()` in constructors and `shutdown()` in destructors, but repeated instantiation without process exit would trigger this leak. [7](#0-6) 

## Recommendation

**Fix 1: Store and Join Thread Handle**
```rust
pub struct ExecutorService {
    shard_id: ShardId,
    controller: NetworkController,
    executor_service: Arc<ShardedExecutorService<RemoteStateViewClient>>,
    executor_thread: Option<thread::JoinHandle<()>>,
}

pub fn start(&mut self) {
    self.controller.start();
    let thread_name = format!("ExecutorService-{}", self.shard_id);
    let builder = thread::Builder::new().name(thread_name);
    let executor_service_clone = self.executor_service.clone();
    let handle = builder
        .spawn(move || {
            executor_service_clone.start();
        })
        .expect("Failed to spawn thread");
    self.executor_thread = Some(handle);
}

pub fn shutdown(&mut self) {
    self.controller.shutdown();
    
    // Send stop signal by closing command channel
    // (requires refactoring to store shutdown sender)
    
    // Join the executor service thread
    if let Some(handle) = self.executor_thread.take() {
        handle.join().expect("Failed to join executor thread");
    }
}
```

**Fix 2: Implement Proper Stop Mechanism**
Add a shutdown channel to `RemoteCoordinatorClient` that gets closed during shutdown, causing `receive_execute_command()` to return `Stop`.

**Fix 3: NetworkController Wait for Shutdown**
Implement proper async shutdown waiting in `NetworkController` instead of just sending signals.

## Proof of Concept

```rust
#[test]
fn test_executor_service_resource_leak() {
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    
    // Setup addresses for coordinator and shards
    let coordinator_addr = "127.0.0.1:8000".parse().unwrap();
    let shard_addr = "127.0.0.1:8001".parse().unwrap();
    let remote_shard_addrs = vec![shard_addr];
    
    // Get initial thread count
    let initial_threads = count_process_threads();
    
    // Perform multiple start/shutdown cycles
    for i in 0..10 {
        let mut service = ExecutorService::new(
            0, // shard_id
            1, // num_shards
            4, // num_threads
            shard_addr,
            coordinator_addr,
            remote_shard_addrs.clone(),
        );
        
        service.start();
        thread::sleep(Duration::from_millis(100));
        service.shutdown();
        thread::sleep(Duration::from_millis(100));
        
        println!("Cycle {}: Threads = {}", i, count_process_threads());
    }
    
    // Final thread count should show significant leak
    let final_threads = count_process_threads();
    let leaked_threads = final_threads - initial_threads;
    
    // Each cycle leaks: 1 main thread + 6 rayon threads (4 + 2) + network threads
    // After 10 cycles, we expect ~70+ leaked threads
    assert!(leaked_threads > 50, 
        "Expected significant thread leak, but only {} threads leaked", 
        leaked_threads);
}

fn count_process_threads() -> usize {
    // Platform-specific thread counting
    #[cfg(target_os = "linux")]
    {
        std::fs::read_dir("/proc/self/task").unwrap().count()
    }
    #[cfg(not(target_os = "linux"))]
    {
        // Fallback: not testable on other platforms
        0
    }
}
```

## Notes

The vulnerability is compounded by the TODO comment in `NetworkController` acknowledging the incomplete shutdown implementation. The developers assumed shutdown occurs only before process exit, but this assumption breaks when services need to be restarted in-process for error recovery or maintenance scenarios. This is a reliability-critical issue for long-running validator nodes that must maintain high availability.

### Citations

**File:** execution/executor-service/src/remote_executor_service.rs (L57-67)
```rust
    pub fn start(&mut self) {
        self.controller.start();
        let thread_name = format!("ExecutorService-{}", self.shard_id);
        let builder = thread::Builder::new().name(thread_name);
        let executor_service_clone = self.executor_service.clone();
        builder
            .spawn(move || {
                executor_service_clone.start();
            })
            .expect("Failed to spawn thread");
    }
```

**File:** execution/executor-service/src/remote_executor_service.rs (L69-71)
```rust
    pub fn shutdown(&mut self) {
        self.controller.shutdown();
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L58-66)
```rust
        let executor_thread_pool = Arc::new(
            rayon::ThreadPoolBuilder::new()
                // We need two extra threads for the cross-shard commit receiver and the thread
                // that is blocked on waiting for execute block to finish.
                .thread_name(move |i| format!("sharded-executor-shard-{}-{}", shard_id, i))
                .num_threads(num_threads + 2)
                .build()
                .unwrap(),
        );
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-260)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L80-113)
```rust
    fn receive_execute_command(&self) -> ExecutorShardCommand<RemoteStateViewClient> {
        match self.command_rx.recv() {
            Ok(message) => {
                let _rx_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx"])
                    .start_timer();
                let bcs_deser_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx_bcs_deser"])
                    .start_timer();
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
                drop(bcs_deser_timer);

                match request {
                    RemoteExecutionRequest::ExecuteBlock(command) => {
                        let init_prefetch_timer = REMOTE_EXECUTOR_TIMER
                            .with_label_values(&[&self.shard_id.to_string(), "init_prefetch"])
                            .start_timer();
                        let state_keys = Self::extract_state_keys(&command);
                        self.state_view_client.init_for_block(state_keys);
                        drop(init_prefetch_timer);

                        let (sub_blocks, concurrency, onchain_config) = command.into();
                        ExecutorShardCommand::ExecuteSubBlocks(
                            self.state_view_client.clone(),
                            sub_blocks,
                            concurrency,
                            onchain_config,
                        )
                    },
                }
            },
            Err(_) => ExecutorShardCommand::Stop,
        }
    }
```

**File:** secure/net/src/network_controller/mod.rs (L152-166)
```rust
    // TODO: This is still not a very clean shutdown. We don't wait for the full shutdown after
    //       sending the signal. May not matter much for now because we shutdown before exiting the
    //       process. Ideally, we want to fix this.
    pub fn shutdown(&mut self) {
        info!("Shutting down network controller at {}", self.listen_addr);
        if let Some(shutdown_signal) = self.inbound_server_shutdown_tx.take() {
            shutdown_signal.send(()).unwrap();
        }

        if let Some(shutdown_signal) = self.outbound_task_shutdown_tx.take() {
            shutdown_signal.send(Message::new(vec![])).unwrap_or_else(|_| {
                warn!("Failed to send shutdown signal to outbound task; probably already shutdown");
            })
        }
    }
```

**File:** execution/executor-service/src/process_executor_service.rs (L52-56)
```rust
impl Drop for ProcessExecutorService {
    fn drop(&mut self) {
        self.shutdown();
    }
}
```
