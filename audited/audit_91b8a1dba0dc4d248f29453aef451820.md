# Audit Report

## Title
State Sync Storage Service Single-Item Size Check Bypass Enables Resource Exhaustion on Syncing Nodes

## Summary
The `get_transaction_outputs_with_proof_by_size_legacy()` function in the state sync storage service contains a critical logic flaw at line 759 that bypasses size validation when exactly one transaction output is requested. This allows responses containing single massive transaction outputs (up to ~20-25 MB) to be sent even when they exceed the configured `max_response_size` limit (default 10 MB), enabling resource exhaustion attacks against nodes during state synchronization. [1](#0-0) 

## Finding Description

The storage service implements size-aware chunking to prevent overwhelming peers with excessively large responses during state synchronization. The legacy implementation uses a binary search approach: when a response exceeds `max_response_size`, it halves the chunk size and retries until the data fits within limits. [2](#0-1) 

However, a bypass exists when `num_outputs_to_fetch == 1`. The code returns the response immediately without performing the overflow check, based on the assumption that "we cannot return less than a single item."

This assumption is violated because individual transaction outputs can legitimately be much larger than the configured response size limit:

**Maximum Transaction Output Size:**
- Write operations: 10 MB (max_bytes_all_write_ops_per_transaction)
- Events: 10 MB (max_bytes_all_events_per_transaction)  
- Transaction data + proofs + auxiliary: ~2-5 MB
- **Total: ~20-25 MB per transaction output** [3](#0-2) 

**Default Storage Service Limit:**
- `SERVER_MAX_MESSAGE_SIZE` = 10 MB [4](#0-3) 

**Attack Path:**

1. Attacker submits transactions maximizing write set and events (10 MB each)
2. These transactions are validated and committed (they comply with gas parameter limits)
3. During state sync, when a peer requests transaction outputs and the request happens to contain exactly one massive output, the size check is bypassed
4. The storage service sends a 20+ MB response despite the 10 MB limit
5. Multiple concurrent such requests exhaust memory and bandwidth on syncing nodes [5](#0-4) 

**Systemic Issue:**

This same pattern exists in multiple storage service functions, creating a systemic vulnerability:

- `get_transactions_with_proof_by_size_legacy()` at line 537
- `get_transaction_outputs_with_proof_by_size_legacy()` at line 759  
- `get_transactions_or_outputs_with_proof_by_size_legacy()` at line 874
- `get_epoch_ending_ledger_infos_by_size_legacy()` at line 319
- `get_state_value_chunk_with_proof_by_size_legacy()` at line 1006 [6](#0-5) 

## Impact Explanation

This vulnerability meets **High Severity** criteria per the Aptos bug bounty program, which explicitly includes "Validator node slowdowns" as a High Severity impact (up to $50,000).

**Specific Impacts:**

1. **Resource Exhaustion**: An attacker can force storage service nodes to send responses 2-2.5x larger than intended limits, causing memory pressure and bandwidth saturation

2. **State Sync Degradation**: Nodes attempting to sync from the network will experience significant slowdowns when requesting blocks containing large transaction outputs, as they receive unexpectedly massive responses

3. **Bypassed Rate Limiting**: The storage service's size-based chunking and rate limiting mechanisms are defeated, allowing attackers to extract more data per request than intended

4. **Network Congestion**: Multiple concurrent requests for single massive outputs can congest the network layer's streaming buffers, affecting other state sync operations

While the network layer can technically handle messages up to 64 MB via streaming, the storage service's limits exist specifically to prevent resource exhaustion. This bypass defeats that protection mechanism. [7](#0-6) 

## Likelihood Explanation

**Likelihood: High**

1. **No Privileged Access Required**: Any user can submit transactions with large outputs by paying gas costs

2. **Happens During Normal Operation**: This occurs naturally during state sync when requesting transaction outputs, not requiring any special protocol manipulation

3. **Economically Feasible**: While creating 20 MB transactions is expensive, attackers only need to create them sporadically to impact syncing nodes. Once on-chain, they affect every node that syncs through that portion of history

4. **Attack Persistence**: These large transactions remain in the blockchain permanently, creating a persistent resource exhaustion vector

5. **No Special Timing Required**: The vulnerability is triggered whenever a storage service request happens to ask for exactly one of these large outputs

## Recommendation

**Immediate Fix**: Implement proper size validation for single-item responses. When a single item exceeds the limit, the function should return an error rather than bypassing the check.

**Recommended Code Change (storage.rs, line 758-760):**

```rust
if num_outputs_to_fetch == 1 {
    // Check if even a single item overflows
    let (overflow_frame, num_bytes) =
        check_overflow_network_frame(&response, max_response_size)?;
    
    if overflow_frame {
        // Cannot serve request - single item too large
        return Err(Error::UnexpectedErrorEncountered(format!(
            "Single transaction output at version {:?} exceeds max response size! \
            Output size: {:?} bytes, limit: {:?} bytes. Consider requesting as a transaction \
            (without output data) instead.",
            start_version, num_bytes, max_response_size
        )));
    }
    return Ok(response);
}
```

**Long-term Fix**: 

1. Implement a separate, higher limit specifically for single-item responses (e.g., 40 MB for v2, which already exists as SERVER_MAX_MESSAGE_SIZE_V2)
2. Add metrics tracking when single items exceed standard limits
3. Consider protocol changes to allow requesting transaction data without output data when outputs are excessively large
4. Apply the same fix to all affected functions (lines 319, 537, 759, 874, 1006) [8](#0-7) 

## Proof of Concept

**Step 1: Create Maximum-Sized Transaction Output**

```rust
// In a Move module deployed to the chain
module attacker::large_output {
    use std::vector;
    use aptos_framework::account;
    
    // Create transaction with maximum write set
    public entry fun create_large_output(account: &signer) {
        // Write ~10 MB of data across multiple resources
        let i = 0;
        while (i < 1000) {
            // Each write ~10 KB, totaling ~10 MB
            let large_data = vector::empty<u8>();
            let j = 0;
            while (j < 10000) {
                vector::push_back(&mut large_data, (j as u8));
                j = j + 1;
            };
            
            // Store in unique resource
            move_to(account, LargeData { 
                id: i,
                data: large_data 
            });
            i = i + 1;
        };
    }
    
    struct LargeData has key {
        id: u64,
        data: vector<u8>,
    }
}
```

**Step 2: Trigger the Vulnerability via State Sync Request**

```rust
// In storage service client test
#[tokio::test]
async fn test_single_large_output_bypass() {
    // Setup: Deploy and execute the large output transaction
    // This transaction creates ~20 MB output
    let large_txn_version = execute_large_output_transaction();
    
    // Make storage service request for exactly 1 output
    let request = StorageServiceRequest::GetTransactionDataWithProof(
        GetTransactionDataWithProofRequest {
            start_version: large_txn_version,
            end_version: large_txn_version,  // Exactly 1 output
            proof_version: large_txn_version,
            request_type: TransactionDataRequestType::TransactionOutputData,
            max_num_output_reductions: 0,
        }
    );
    
    // Send request with 10 MB limit
    let max_response_size = 10 * 1024 * 1024; // 10 MB
    let response = storage_service.handle_request(request, max_response_size).await;
    
    // Verify: Response succeeds despite exceeding limit
    assert!(response.is_ok());
    let response_size = bcs::to_bytes(&response.unwrap()).unwrap().len();
    
    // BUG: Response is ~20 MB but limit was 10 MB
    assert!(response_size > max_response_size);
    println!("Bypassed limit! Response size: {} MB, Limit: {} MB", 
             response_size / (1024*1024), 
             max_response_size / (1024*1024));
}
```

**Expected Result**: The request succeeds and returns a ~20 MB response despite the 10 MB limit, demonstrating the bypass.

**Reproduction Steps:**
1. Deploy the `large_output` module to a test network
2. Execute `create_large_output()` to generate a transaction with maximum-sized output
3. Configure a storage service client with `max_response_size = 10 MB`
4. Request exactly 1 transaction output at the version created in step 2
5. Observe that the response succeeds and contains ~20 MB of data, exceeding the configured limit

## Notes

This vulnerability represents a fundamental assumption violation in the storage service's resource management strategy. The code assumes that "we cannot return less than a single item" and therefore must return it regardless of size. However, this defeats the entire purpose of size-based chunking and rate limiting.

The issue is particularly concerning because:

1. **It's a systemic pattern** appearing in 5 different functions
2. **It affects both legacy and current implementations** (though the newer size-aware chunking may have mitigations)
3. **It creates a permanent attack vector** since large transactions remain in blockchain history
4. **It impacts critical infrastructure** (validator nodes during sync)

The fix is straightforward but requires careful consideration of the protocol semantics when a single item genuinely cannot fit within limits.

### Citations

**File:** state-sync/storage-service/server/src/storage.rs (L536-538)
```rust
            if num_transactions_to_fetch == 1 {
                return Ok(response); // We cannot return less than a single item
            }
```

**File:** state-sync/storage-service/server/src/storage.rs (L758-760)
```rust
            if num_outputs_to_fetch == 1 {
                return Ok(response); // We cannot return less than a single item
            }
```

**File:** state-sync/storage-service/server/src/storage.rs (L762-776)
```rust
            // Attempt to divide up the request if it overflows the message size
            let (overflow_frame, num_bytes) =
                check_overflow_network_frame(&response, max_response_size)?;
            if !overflow_frame {
                return Ok(response);
            } else {
                metrics::increment_chunk_truncation_counter(
                    metrics::TRUNCATION_FOR_SIZE,
                    DataResponse::TransactionDataWithProof(response).get_label(),
                );
                let new_num_outputs_to_fetch = num_outputs_to_fetch / 2;
                debug!("The request for {:?} outputs was too large (num bytes: {:?}, limit: {:?}). Retrying with {:?}.",
                    num_outputs_to_fetch, num_bytes, max_response_size, new_num_outputs_to_fetch);
                num_outputs_to_fetch = new_num_outputs_to_fetch; // Try again with half the amount of data
            }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L154-162)
```rust
            max_bytes_per_write_op: NumBytes,
            { 5.. => "max_bytes_per_write_op" },
            1 << 20, // a single state item is 1MB max
        ],
        [
            max_bytes_all_write_ops_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_write_ops_per_transaction" },
            10 << 20, // all write ops from a single transaction are 10MB max
        ],
```

**File:** config/src/config/state_sync_config.rs (L17-17)
```rust
const SERVER_MAX_MESSAGE_SIZE: usize = 10 * 1024 * 1024; // 10 MiB
```

**File:** config/src/config/state_sync_config.rs (L19-21)
```rust
// The maximum message size per state sync message (for v2 data requests)
const CLIENT_MAX_MESSAGE_SIZE_V2: usize = 20 * 1024 * 1024; // 20 MiB (used for v2 data requests)
const SERVER_MAX_MESSAGE_SIZE_V2: usize = 40 * 1024 * 1024; // 40 MiB (used for v2 data requests)
```

**File:** aptos-move/aptos-vm-types/src/storage/change_set_configs.rs (L86-128)
```rust
    pub fn check_change_set(&self, change_set: &impl ChangeSetInterface) -> Result<(), VMStatus> {
        let storage_write_limit_reached = |maybe_message: Option<&str>| {
            let mut err = PartialVMError::new(StatusCode::STORAGE_WRITE_LIMIT_REACHED);
            if let Some(message) = maybe_message {
                err = err.with_message(message.to_string())
            }
            Err(err.finish(Location::Undefined).into_vm_status())
        };

        if self.max_write_ops_per_transaction != 0
            && change_set.num_write_ops() as u64 > self.max_write_ops_per_transaction
        {
            return storage_write_limit_reached(Some("Too many write ops."));
        }

        let mut write_set_size = 0;
        for (key, op_size) in change_set.write_set_size_iter() {
            if let Some(len) = op_size.write_len() {
                let write_op_size = len + (key.size() as u64);
                if write_op_size > self.max_bytes_per_write_op {
                    return storage_write_limit_reached(None);
                }
                write_set_size += write_op_size;
            }
            if write_set_size > self.max_bytes_all_write_ops_per_transaction {
                return storage_write_limit_reached(None);
            }
        }

        let mut total_event_size = 0;
        for event in change_set.events_iter() {
            let size = event.event_data().len() as u64;
            if size > self.max_bytes_per_event {
                return storage_write_limit_reached(None);
            }
            total_event_size += size;
            if total_event_size > self.max_bytes_all_events_per_transaction {
                return storage_write_limit_reached(None);
            }
        }

        Ok(())
    }
```

**File:** config/src/config/network_config.rs (L49-50)
```rust
pub const MAX_FRAME_SIZE: usize = 4 * 1024 * 1024; /* 4 MiB large messages will be chunked into multiple frames and streamed */
pub const MAX_MESSAGE_SIZE: usize = 64 * 1024 * 1024; /* 64 MiB */
```
