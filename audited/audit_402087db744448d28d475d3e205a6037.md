# Audit Report

## Title
State Inconsistency in sync_to_target: Logical Time Updated Despite State Sync Failure

## Summary
The `ExecutionProxy::sync_to_target()` function unconditionally updates the internal logical time tracker even when the underlying state synchronization fails, creating a critical state inconsistency where the consensus pipeline believes it has synchronized to a target round while the actual execution and storage layers remain at stale state.

## Finding Description

The vulnerability exists in the state synchronization flow across two critical files: [1](#0-0) [2](#0-1) 

**Attack Flow:**

1. **Reset Phase Succeeds**: When `sync_to_target()` is called, it first invokes `reset(&target)` which successfully resets both the rand manager and buffer manager to the target round: [3](#0-2) 

2. **State Sync Fails**: The underlying `execution_proxy.sync_to_target(target)` is then called, which invokes state sync. If state sync fails (network issues, invalid data, timeout), the result is stored but not checked before updating state.

3. **Critical Flaw - Unconditional Logical Time Update**: Despite the state sync failure, the code unconditionally updates the logical time: [4](#0-3) 

4. **Buffer Manager State**: The buffer manager's reset already updated its internal round tracking: [5](#0-4) 

**Result**: The node's consensus components (buffer manager, rand manager, logical time) all believe they are synchronized to the target round, but the actual storage and execution state remains at the pre-sync state.

**There is even a TODO acknowledging this issue:** [6](#0-5) 

## Impact Explanation

**Critical Severity** - This breaks the fundamental **State Consistency** invariant (Invariant #4).

When a validator operates with this inconsistency:

1. **Consensus Participation with Stale State**: The validator's consensus pipeline accepts and processes blocks for rounds beyond its actual execution state, potentially voting on blocks it cannot properly validate.

2. **Execution Divergence**: When the validator attempts to execute new blocks, it will be executing them against a stale state root, producing different results than validators with correct state, breaking the **Deterministic Execution** invariant (Invariant #1).

3. **Potential Chain Split**: If multiple validators enter this inconsistent state, they may commit different blocks at the same round, violating **Consensus Safety** (Invariant #2) and potentially causing a chain fork.

4. **Non-Recoverable Desynchronization**: The validator continues to believe it is synchronized, so it won't trigger recovery mechanisms, requiring manual intervention or node restart.

While epoch transitions have crash guards (`expect()` calls), other sync paths like the consensus observer simply log errors and continue: [7](#0-6) 

## Likelihood Explanation

**High Likelihood** in production environments:

1. **State sync failures are common**: Network partitions, timeouts, high load, or corrupted data can cause state sync to fail.

2. **Reset succeeds more reliably**: The reset operation is a simple in-memory channel communication, while state sync requires network communication and storage operations, making the "reset succeeds, sync fails" scenario realistic.

3. **No explicit validation**: The code does not verify that logical time matches actual committed state before continuing consensus operations.

4. **Affects all validators**: Any validator can encounter this condition during normal operation, not requiring attacker manipulation.

## Recommendation

The logical time should only be updated if state synchronization succeeds. The code should be modified as follows:

```rust
// In consensus/src/state_computer.rs, sync_to_target():

let result = monitor!(
    "sync_to_target",
    self.state_sync_notifier.sync_to_target(target).await
);

// Only update logical time if sync succeeded
if result.is_ok() {
    *latest_logical_time = target_logical_time;
}

self.executor.reset()?;

result.map_err(|error| {
    let anyhow_error: anyhow::Error = error.into();
    anyhow_error.into()
})
```

Additionally, in `execution_client.rs`, if reset succeeds but sync fails, the buffer manager and rand manager should be re-reset to their previous state, or the error handling should trigger a node crash to prevent continuing with inconsistent state.

## Proof of Concept

```rust
// Reproduction steps:
// 1. Set up a validator node with state sync mocked to fail
// 2. Trigger sync_to_target with a target ledger info at round N+100
// 3. Observe that:
//    - reset() completes successfully
//    - buffer_manager.highest_committed_round = N+100
//    - ExecutionProxy.latest_logical_time = N+100
//    - But actual storage is still at round N
// 4. Send consensus messages for round N+101
// 5. Node processes them based on round N+100 state but executes against round N storage
// 6. State root mismatch causes consensus divergence

// Test case structure:
#[tokio::test]
async fn test_sync_to_target_state_inconsistency() {
    // Mock execution_proxy with state_sync that returns error
    let mock_executor = MockExecutor::new_with_failing_sync();
    let execution_proxy = ExecutionProxy::new(mock_executor, ...);
    
    let target_li = create_ledger_info_at_round(100);
    let result = execution_proxy.sync_to_target(target_li.clone()).await;
    
    // Sync should fail
    assert!(result.is_err());
    
    // But logical time should NOT be updated (current bug: it IS updated)
    // This assertion would FAIL with current code:
    let logical_time = execution_proxy.get_logical_time();
    assert_ne!(logical_time.round, 100); // Should still be at old round
}
```

**Notes**

The vulnerability is confirmed by examining the execution flow where the logical time update at line 222 of `state_computer.rs` occurs unconditionally after calling state sync but before checking the result. Combined with the buffer manager reset that has already updated internal round tracking, this creates a state where consensus components believe synchronization succeeded while storage remains stale. The TODO comment at execution_client.rs:669-670 confirms developers are aware of inadequate error handling in this path.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L661-672)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/state_computer.rs (L176-233)
```rust
    /// Synchronize to a commit that is not present locally.
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        // Grab the logical time lock and calculate the target logical time
        let mut latest_logical_time = self.write_mutex.lock().await;
        let target_logical_time =
            LogicalTime::new(target.ledger_info().epoch(), target.ledger_info().round());

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // The pipeline phase already committed beyond the target block timestamp, just return.
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }

        // This is to update QuorumStore with the latest known commit in the system,
        // so it can set batches expiration accordingly.
        // Might be none if called in the recovery path, or between epoch stop and start.
        if let Some(inner) = self.state.read().as_ref() {
            let block_timestamp = target.commit_info().timestamp_usecs();
            inner
                .payload_manager
                .notify_commit(block_timestamp, Vec::new());
        }

        // Inject an error for fail point testing
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Invoke state sync to synchronize to the specified target. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );

        // Update the latest logical time
        *latest_logical_time = target_logical_time;

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L579-596)
```rust
    async fn process_reset_request(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        info!("Receive reset");

        match signal {
            ResetSignal::Stop => self.stop = true,
            ResetSignal::TargetRound(round) => {
                self.highest_committed_round = round;
                self.latest_round = round;

                let _ = self.drain_pending_commit_proof_till(round);
            },
        }

        self.reset().await;
        let _ = tx.send(ResetAck::default());
        info!("Reset finishes");
    }
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L219-231)
```rust
                if let Err(error) = execution_client
                    .clone()
                    .sync_to_target(commit_decision.commit_proof().clone())
                    .await
                {
                    error!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Failed to sync to commit decision: {:?}! Error: {:?}",
                            commit_decision, error
                        ))
                    );
                    return;
                }
```
