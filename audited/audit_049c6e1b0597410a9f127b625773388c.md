# Audit Report

## Title
State Inconsistency Due to Failed Batch Commit Notifications in Quorum Store

## Summary
The `notify_commit()` function in QuorumStorePayloadManager calls `update_certified_timestamp()` before notifying coordinator components about committed batches. If the notification fails via `try_send()`, BatchStore's timestamp is updated and expired batches are deleted, but ProofManager, BatchGenerator, and ProofCoordinator never receive the update, creating persistent state inconsistency across quorum store components.

## Finding Description

In `notify_commit()`, the execution order creates a vulnerability: [1](#0-0) 

The function first calls `update_certified_timestamp()` which updates BatchStore's internal state: [2](#0-1) 

This atomically updates the certified timestamp and deletes all batches with expiration <= block_timestamp from both memory cache and persistent storage: [3](#0-2) 

Then, the notification is sent using `try_send()`: [4](#0-3) 

**The vulnerability**: `try_send()` can fail if the channel is full or closed, but only logs a warning without retrying or propagating the error. This causes three coordinator components to never receive critical updates:

1. **ProofManager** never marks committed batches as committed and never updates its timestamp: [5](#0-4) 

2. **BatchGenerator** never updates its timestamp and never cleans up expired batches: [6](#0-5) 

3. **ProofCoordinator** never records batch completion status.

**State Inconsistency Result**:
- BatchStore: `last_certified_time = T`, all batches with expiration â‰¤ T deleted
- ProofManager: `latest_block_timestamp = T_old`, expired batches remain in queue, committed batches not marked as committed
- BatchGenerator: `latest_block_timestamp = T_old`, expired batches remain in tracking

**Exploitation Scenario**:

The race condition creates several issues:

1. **Memory Leak**: Expired batches and proofs remain indefinitely in ProofManager and BatchGenerator until the next successful notification, accumulating over multiple failed notifications. [7](#0-6) 

2. **Potential Batch Re-inclusion**: If committed batches B1, B2, B3 are not marked as committed in ProofManager due to notification failure, and the committed block is pruned from BlockStore before the next proposal, the `exclude_payload` mechanism won't filter them: [8](#0-7) 

ProofManager's `pull_proofs()` checks if batches are committed: [9](#0-8) 

Without the committed flag set, these batches could be pulled again for a new proposal.

3. **Incorrect Quota Tracking**: BatchStore frees quota for deleted batches, but BatchGenerator still tracks them as in-progress, causing quota accounting drift.

## Impact Explanation

This qualifies as **Medium Severity** under Aptos bug bounty criteria as "State inconsistencies requiring intervention":

- **State Consistency Violation**: Different components have divergent views of certified time and batch lifecycle states, violating the "State Consistency" invariant that state transitions should be atomic.

- **Potential Consensus Impact**: While `exclude_payload` provides protection, a window exists where committed batches could theoretically be re-included if block pruning happens before the next successful notification, potentially violating deterministic execution across validators if they have different notification failure patterns.

- **Operational Impact**: Memory accumulation from unpruned expired data degrades node performance over time, potentially causing validator slowdowns (High severity category).

- **Limited Direct Harm**: The vulnerability doesn't directly lead to fund loss or immediate consensus failure due to multiple protective layers (expiration checks, exclude_payload filtering), but creates operational risks and state inconsistency.

## Likelihood Explanation

**Likelihood: Medium to High**

The `try_send()` failure occurs when:
1. **Channel Full**: Coordinator processing falls behind notification rate under high load
2. **Channel Closed**: During epoch shutdown sequences [10](#0-9) 

**Triggering Conditions**:
- High transaction volume causing backpressure in coordinator message processing
- Slow proof/batch processing creating queue buildup
- Natural occurrence during epoch transitions when channels close
- Sustained load from network activity

**Realistic Scenarios**:
- During mainnet stress periods with high TPS
- Epoch boundary transitions (happens regularly)
- Node recovery/sync operations creating temporary slowdowns

The vulnerability is **not** easily exploitable by external attackers (no direct trigger mechanism), but occurs naturally under operational stress, making it a **robustness issue** that manifests as a security concern through state inconsistency.

## Recommendation

**Fix Approach**: Ensure notification delivery or fail safely

**Option 1: Use Blocking Send (Recommended)**
Replace `try_send()` with blocking `send()` to guarantee delivery:

```rust
fn notify(&self, block_timestamp: u64, batches: Vec<BatchInfoExt>) {
    let mut tx = self.coordinator_tx.clone();
    
    // Block until message is delivered
    if let Err(e) = tx.send(CoordinatorCommand::CommitNotification(
        block_timestamp,
        batches,
    )).await {
        error!(
            "CRITICAL: CommitNotification failed - coordinator channel closed: {}",
            e
        );
        // Trigger epoch shutdown or panic to prevent inconsistent state
        panic!("Cannot proceed with inconsistent quorum store state");
    }
}
```

**Option 2: Implement Retry Logic**
Add exponential backoff retry mechanism before giving up.

**Option 3: Reorder Operations**
Move `update_certified_timestamp()` AFTER successful notification, though this delays garbage collection.

**Option 4: Add Reconciliation**
Implement periodic state reconciliation to detect and repair inconsistencies.

## Proof of Concept

```rust
// Rust unit test demonstrating the race condition
#[tokio::test]
async fn test_notify_commit_race_condition() {
    use futures::channel::mpsc;
    use consensus::quorum_store::quorum_store_coordinator::CoordinatorCommand;
    
    // Create a channel with capacity 1
    let (tx, mut rx) = mpsc::channel::<CoordinatorCommand>(1);
    
    // Fill the channel to capacity
    tx.try_send(CoordinatorCommand::Shutdown(oneshot::channel().0)).unwrap();
    
    // Create notifier with full channel
    let notifier = QuorumStoreCommitNotifier::new(tx);
    
    // Create test batches
    let batches = vec![create_test_batch_info(100, 1000000)];
    
    // Call notify - this will fail with try_send because channel is full
    notifier.notify(1000000, batches.clone());
    
    // Verify: Message was NOT delivered (channel still has only shutdown message)
    if let Some(msg) = rx.try_next().unwrap() {
        match msg {
            CoordinatorCommand::Shutdown(_) => {
                // First message is still the shutdown
                assert!(true, "Channel was full, CommitNotification not delivered");
            }
            CoordinatorCommand::CommitNotification(_, _) => {
                panic!("Should not have received CommitNotification");
            }
        }
    }
    
    // At this point:
    // - BatchStore has updated timestamp (in real code)
    // - ProofManager never received notification
    // - State is inconsistent
}
```

**Note**: This PoC demonstrates the notification failure mechanism. A full reproduction would require mocking BatchStore, ProofManager, and measuring the resulting state inconsistency across components.

---

## Notes

While this vulnerability demonstrates a clear state inconsistency issue, its security impact is somewhat mitigated by:
1. The `exclude_payload` mechanism in proposal generation
2. Expiration checks during batch retrieval  
3. Limited external attacker control over trigger conditions

However, it remains a **valid Medium severity finding** because it violates state consistency invariants and can lead to operational degradation, with a theoretical path to more serious issues under specific timing conditions (aggressive pruning + notification failure + new proposal generation).

### Citations

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L44-58)
```rust
impl TQuorumStoreCommitNotifier for QuorumStoreCommitNotifier {
    fn notify(&self, block_timestamp: u64, batches: Vec<BatchInfoExt>) {
        let mut tx = self.coordinator_tx.clone();

        if let Err(e) = tx.try_send(CoordinatorCommand::CommitNotification(
            block_timestamp,
            batches,
        )) {
            warn!(
                "CommitNotification failed. Is the epoch shutting down? error: {}",
                e
            );
        }
    }
}
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L168-208)
```rust
    fn notify_commit(&self, block_timestamp: u64, payloads: Vec<Payload>) {
        self.batch_reader
            .update_certified_timestamp(block_timestamp);

        let batches: Vec<_> = payloads
            .into_iter()
            .flat_map(|payload| match payload {
                Payload::DirectMempool(_) => {
                    unreachable!("InQuorumStore should be used");
                },
                Payload::InQuorumStore(proof_with_status) => proof_with_status
                    .proofs
                    .iter()
                    .map(|proof| proof.info().clone().into())
                    .collect::<Vec<_>>(),
                Payload::InQuorumStoreWithLimit(proof_with_status) => proof_with_status
                    .proof_with_data
                    .proofs
                    .iter()
                    .map(|proof| proof.info().clone().into())
                    .collect::<Vec<_>>(),
                Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _)
                | Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _) => {
                    inline_batches
                        .iter()
                        .map(|(batch_info, _)| batch_info.clone().into())
                        .chain(
                            proof_with_data
                                .proofs
                                .iter()
                                .map(|proof| proof.info().clone().into()),
                        )
                        .collect::<Vec<_>>()
                },
                Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => p.get_all_batch_infos(),
                Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => p.get_all_batch_infos(),
            })
            .collect();

        self.commit_notifier.notify(block_timestamp, batches);
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L443-472)
```rust
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L530-539)
```rust
    pub fn update_certified_timestamp(&self, certified_time: u64) {
        trace!("QS: batch reader updating time {:?}", certified_time);
        self.last_certified_time
            .fetch_max(certified_time, Ordering::SeqCst);

        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
            debug!("Error deleting batches: {:?}", e)
        }
    }
```

**File:** consensus/src/quorum_store/proof_manager.rs (L88-101)
```rust
    pub(crate) fn handle_commit_notification(
        &mut self,
        block_timestamp: u64,
        batches: Vec<BatchInfoExt>,
    ) {
        trace!(
            "QS: got clean request from execution at block timestamp {}",
            block_timestamp
        );
        self.batch_proof_queue.mark_committed(batches);
        self.batch_proof_queue
            .handle_updated_block_timestamp(block_timestamp);
        self.update_remaining_txns_and_proofs();
    }
```

**File:** consensus/src/quorum_store/batch_generator.rs (L517-532)
```rust
                        BatchGeneratorCommand::CommitNotification(block_timestamp, batches) => {
                            trace!(
                                "QS: got clean request from execution, block timestamp {}",
                                block_timestamp
                            );
                            // Block timestamp is updated asynchronously, so it may race when it enters state sync.
                            if self.latest_block_timestamp > block_timestamp {
                                continue;
                            }
                            self.latest_block_timestamp = block_timestamp;

                            for (author, batch_id) in batches.iter().map(|b| (b.author(), b.batch_id())) {
                                if self.remove_batch_in_progress(author, batch_id) {
                                    counters::BATCH_IN_PROGRESS_COMMITTED.inc();
                                }
                            }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L614-616)
```rust
                    if item.is_committed() {
                        return None;
                    }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L716-769)
```rust
    pub(crate) fn handle_updated_block_timestamp(&mut self, block_timestamp: u64) {
        // tolerate asynchronous notification
        if self.latest_block_timestamp > block_timestamp {
            return;
        }
        let start = Instant::now();
        self.latest_block_timestamp = block_timestamp;
        if let Some(time_lag) = aptos_infallible::duration_since_epoch()
            .checked_sub(Duration::from_micros(block_timestamp))
        {
            counters::TIME_LAG_IN_BATCH_PROOF_QUEUE.observe_duration(time_lag);
        }

        let expired = self.expirations.expire(block_timestamp);
        let mut num_expired_but_not_committed = 0;
        for key in &expired {
            if let Some(mut queue) = self.author_to_batches.remove(&key.author()) {
                if let Some(batch) = queue.remove(key) {
                    let item = self
                        .items
                        .get(&key.batch_key)
                        .expect("Entry for unexpired batch must exist");
                    if item.proof.is_some() {
                        // not committed proof that is expired
                        num_expired_but_not_committed += 1;
                        counters::GAP_BETWEEN_BATCH_EXPIRATION_AND_CURRENT_TIME_WHEN_COMMIT
                            .observe((block_timestamp - batch.expiration()) as f64);
                        if let Some(ref txn_summaries) = item.txn_summaries {
                            for txn_summary in txn_summaries {
                                if let Some(count) =
                                    self.txn_summary_num_occurrences.get_mut(txn_summary)
                                {
                                    *count -= 1;
                                    if *count == 0 {
                                        self.txn_summary_num_occurrences.remove(txn_summary);
                                    }
                                };
                            }
                        }
                        self.dec_remaining_proofs(&batch.author(), batch.num_txns());
                        counters::GARBAGE_COLLECTED_IN_PROOF_QUEUE_COUNTER
                            .with_label_values(&["expired_proof"])
                            .inc();
                    }
                    claims::assert_some!(self.items.remove(&key.batch_key));
                }
                if !queue.is_empty() {
                    self.author_to_batches.insert(key.author(), queue);
                }
            }
        }
        counters::PROOF_QUEUE_UPDATE_TIMESTAMP_DURATION.observe_duration(start.elapsed());
        counters::NUM_PROOFS_EXPIRED_WHEN_COMMIT.inc_by(num_expired_but_not_committed);
    }
```

**File:** consensus/src/liveness/proposal_generator.rs (L585-589)
```rust
        let exclude_payload: Vec<_> = pending_blocks
            .iter()
            .flat_map(|block| block.payload())
            .collect();
        let payload_filter = PayloadFilter::from(&exclude_payload);
```

**File:** consensus/src/quorum_store/quorum_store_coordinator.rs (L52-81)
```rust
    pub async fn start(self, mut rx: futures_channel::mpsc::Receiver<CoordinatorCommand>) {
        while let Some(cmd) = rx.next().await {
            monitor!("quorum_store_coordinator_loop", {
                match cmd {
                    CoordinatorCommand::CommitNotification(block_timestamp, batches) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["QSCoordinator::commit_notification"])
                            .inc();
                        // TODO: need a callback or not?
                        self.proof_coordinator_cmd_tx
                            .send(ProofCoordinatorCommand::CommitNotification(batches.clone()))
                            .await
                            .expect("Failed to send to ProofCoordinator");

                        self.proof_manager_cmd_tx
                            .send(ProofManagerCommand::CommitNotification(
                                block_timestamp,
                                batches.clone(),
                            ))
                            .await
                            .expect("Failed to send to ProofManager");

                        self.batch_generator_cmd_tx
                            .send(BatchGeneratorCommand::CommitNotification(
                                block_timestamp,
                                batches,
                            ))
                            .await
                            .expect("Failed to send to BatchGenerator");
                    },
```
