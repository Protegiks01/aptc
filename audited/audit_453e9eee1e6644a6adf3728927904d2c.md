# Audit Report

## Title
Unbounded Epoch Retrieval Request Storm: Missing Deduplication Enables Validator Node Resource Exhaustion

## Summary
The `check_epoch()` function in `consensus/src/epoch_manager.rs` lacks deduplication logic for epoch retrieval requests, allowing a malicious validator to trigger an unlimited stream of outbound `EpochRetrievalRequest` messages by sending consensus messages with monotonically increasing fake epoch numbers. This causes network bandwidth exhaustion, message queue congestion, and validator node slowdowns.

## Finding Description

The vulnerability exists in the epoch synchronization mechanism of the Aptos consensus protocol. When a validator node receives a consensus message from a higher epoch than its current local epoch, it automatically sends an `EpochRetrievalRequest` to the peer without tracking whether a request for that epoch is already pending. [1](#0-0) 

The `check_epoch()` function processes multiple message types (ProposalMsg, VoteMsg, SyncInfo, etc.) and extracts the epoch **before** signature verification. When the message epoch differs from the local epoch, it calls `process_different_epoch()`: [2](#0-1) 

The critical flaw is in lines 519-536: when receiving a message from a higher epoch (`Ordering::Greater`), the code immediately constructs and sends an `EpochRetrievalRequest` with no checks for:
- Whether a request for this epoch range is already pending
- Whether multiple requests have been sent recently to the same peer
- Any application-layer rate limiting

**Attack Flow:**
1. Attacker (malicious validator with network authentication) sends ProposalMsg with epoch N+1
2. Victim node (at epoch N) calls `check_epoch()` â†’ `process_different_epoch(N+1, attacker)`
3. Victim immediately sends `EpochRetrievalRequest(start: N, end: N+1)` back to attacker
4. Attacker sends VoteMsg with epoch N+2
5. Victim sends `EpochRetrievalRequest(start: N, end: N+2)`
6. Attacker continues with epochs N+3, N+4, ... N+10000

Each incoming message triggers a new outbound request without any deduplication. The attack bypasses the bounded executor protection because epoch checking occurs before message verification: [3](#0-2) 

The bounded executor only rate-limits signature verification (line 1587), not epoch checking or request sending.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria for "Validator node slowdowns" because:

1. **Network Bandwidth Exhaustion**: Each epoch retrieval request consumes outbound bandwidth. An attacker sending 1000 messages/second with different epochs triggers 1000 outbound requests/second, potentially saturating network capacity.

2. **Message Queue Congestion**: The network sender queues become congested with epoch retrieval requests, delaying legitimate consensus messages and degrading consensus performance.

3. **CPU Overhead**: Serialization and network stack processing for thousands of unnecessary messages consumes CPU resources that should be dedicated to consensus operations.

4. **Cascading Effects**: If the attacker also responds with crafted `EpochChangeProof` messages, it could trigger expensive epoch transition processing via `initiate_new_epoch()`: [4](#0-3) 

While network-layer rate limiting provides some mitigation, it is not sufficient because:
- Rate limits apply equally to all message types, so epoch retrieval requests compete with critical consensus messages
- Application-layer deduplication is still necessary to prevent protocol abuse
- The lack of tracking violates the invariant that "all operations must respect resource limits"

## Likelihood Explanation

**Likelihood: Medium-to-High**

The attack is highly likely to occur because:

1. **Low Barrier to Entry**: Any malicious or compromised validator can execute this attack. No collusion required.

2. **Simple Exploitation**: The attacker only needs to send consensus messages with incrementing epoch numbers. No complex cryptographic operations or timing requirements.

3. **No Signature Required**: Epoch extraction happens before signature verification, so the attacker can send invalid messages that still trigger the vulnerability: [5](#0-4) 

4. **Multiple Message Types**: 12 different consensus message types trigger this code path, giving the attacker flexibility.

5. **Economic Incentive**: A malicious validator could use this to slow down competitors or destabilize the network during critical periods.

The only requirement is network-layer authentication (Noise handshake), which all validators possess. The AptosBFT protocol is designed to tolerate < 1/3 Byzantine validators, so having one malicious validator is within the threat model.

## Recommendation

Implement request deduplication and rate limiting for epoch retrieval requests. Add tracking state to the `EpochManager` struct:

```rust
// Add to EpochManager struct
pending_epoch_requests: HashMap<AccountAddress, (u64, Instant)>,  // peer -> (target_epoch, timestamp)
```

Modify `process_different_epoch()` to check for pending requests:

```rust
fn process_different_epoch(
    &mut self,
    different_epoch: u64,
    peer_id: AccountAddress,
) -> anyhow::Result<()> {
    // ... existing code ...
    
    Ordering::Greater => {
        // Check if we already have a pending request to this peer
        if let Some((pending_epoch, timestamp)) = self.pending_epoch_requests.get(&peer_id) {
            // If request is recent (< 5 seconds) and for similar epoch, skip
            if timestamp.elapsed() < Duration::from_secs(5) && *pending_epoch >= different_epoch {
                debug!(
                    "[EpochManager] Skipping duplicate epoch retrieval request to {} for epoch {}",
                    peer_id, different_epoch
                );
                return Ok(());
            }
        }
        
        let request = EpochRetrievalRequest {
            start_epoch: self.epoch(),
            end_epoch: different_epoch,
        };
        let msg = ConsensusMsg::EpochRetrievalRequest(Box::new(request));
        
        if let Err(err) = self.network_sender.send_to(peer_id, msg) {
            // ... existing error handling ...
        } else {
            // Track the request
            self.pending_epoch_requests.insert(peer_id, (different_epoch, Instant::now()));
        }
        
        Ok(())
    },
    // ... rest of code ...
}
```

Clear pending requests when epoch proof is received or timeout:

```rust
// In check_epoch() when processing EpochChangeProof
ConsensusMsg::EpochChangeProof(proof) => {
    let msg_epoch = proof.epoch()?;
    if msg_epoch == self.epoch() {
        monitor!("process_epoch_proof", self.initiate_new_epoch(*proof).await)?;
        // Clear pending request for this peer
        self.pending_epoch_requests.remove(&peer_id);
    }
    // ... existing code ...
}
```

Additionally, implement periodic cleanup to remove stale entries (> 30 seconds old).

## Proof of Concept

```rust
// Proof of Concept - Rust test demonstrating the vulnerability
// This can be added to consensus/src/epoch_manager.rs test module

#[tokio::test]
async fn test_epoch_retrieval_request_storm() {
    use aptos_consensus_types::proposal_msg::ProposalMsg;
    use aptos_types::account_address::AccountAddress;
    
    // Setup: Create a mock EpochManager at epoch 10
    let mut epoch_manager = setup_epoch_manager_for_test(10).await;
    let attacker_id = AccountAddress::random();
    
    // Attack: Send 1000 messages with increasing epochs
    let mut request_count = 0;
    let mock_network_sender = epoch_manager.network_sender.clone();
    
    // Override network sender with counting mock
    let (request_tx, mut request_rx) = mpsc::channel(1000);
    let counting_sender = CountingNetworkSender::new(mock_network_sender, request_tx);
    epoch_manager.network_sender = counting_sender;
    
    // Send 1000 ProposalMsg with epochs 11-1010
    for fake_epoch in 11..=1010 {
        let fake_proposal = create_fake_proposal_with_epoch(fake_epoch);
        let msg = ConsensusMsg::ProposalMsg(Box::new(fake_proposal));
        
        // Process the message
        let _ = epoch_manager.process_message(attacker_id, msg).await;
    }
    
    // Verify: Count how many EpochRetrievalRequest messages were sent
    while let Ok(sent_msg) = request_rx.try_recv() {
        if matches!(sent_msg, ConsensusMsg::EpochRetrievalRequest(_)) {
            request_count += 1;
        }
    }
    
    // Expected result: 1000 requests sent (one per fake message)
    // This demonstrates the lack of deduplication
    assert_eq!(request_count, 1000, 
        "Should have sent 1000 epoch retrieval requests without deduplication");
    
    println!("VULNERABILITY CONFIRMED: {} epoch retrieval requests sent", request_count);
    println!("Expected: 1 request (with proper deduplication)");
    println!("Actual: {} requests (no deduplication!)", request_count);
}

fn create_fake_proposal_with_epoch(epoch: u64) -> ProposalMsg {
    // Create a ProposalMsg with the specified epoch
    // The signature can be invalid since epoch checking happens before verification
    // Implementation details omitted for brevity
}

struct CountingNetworkSender {
    inner: ConsensusNetworkClient<NetworkClient<ConsensusMsg>>,
    counter: mpsc::Sender<ConsensusMsg>,
}

impl CountingNetworkSender {
    fn new(inner: ConsensusNetworkClient<NetworkClient<ConsensusMsg>>, 
           counter: mpsc::Sender<ConsensusMsg>) -> Self {
        Self { inner, counter }
    }
    
    fn send_to(&self, peer: AccountAddress, msg: ConsensusMsg) -> Result<(), Error> {
        // Record the message
        let _ = self.counter.try_send(msg.clone());
        // Forward to actual sender (or mock in tests)
        self.inner.send_to(peer, msg)
    }
}
```

**Notes**

This vulnerability represents a **significant protocol violation** in the Aptos consensus layer. While not directly compromising consensus safety or causing fund loss, it enables a single malicious validator to degrade network performance and slow down all validators in the network. The lack of deduplication violates the fundamental principle that "all operations must respect resource limits" and represents incomplete implementation of Byzantine fault tolerance measures.

The fix is straightforward and should be prioritized to prevent potential exploitation by malicious actors or compromised validators. The vulnerability is particularly concerning because it bypasses the bounded executor protection by occurring in the pre-verification phase of message processing.

### Citations

**File:** consensus/src/epoch_manager.rs (L478-542)
```rust
    fn process_different_epoch(
        &mut self,
        different_epoch: u64,
        peer_id: AccountAddress,
    ) -> anyhow::Result<()> {
        debug!(
            LogSchema::new(LogEvent::ReceiveMessageFromDifferentEpoch)
                .remote_peer(peer_id)
                .epoch(self.epoch()),
            remote_epoch = different_epoch,
        );
        match different_epoch.cmp(&self.epoch()) {
            Ordering::Less => {
                if self
                    .epoch_state()
                    .verifier
                    .get_voting_power(&self.author)
                    .is_some()
                {
                    // Ignore message from lower epoch if we're part of the validator set, the node would eventually see messages from
                    // higher epoch and request a proof
                    sample!(
                        SampleRate::Duration(Duration::from_secs(1)),
                        debug!("Discard message from lower epoch {} from {}", different_epoch, peer_id);
                    );
                    Ok(())
                } else {
                    // reply back the epoch change proof if we're not part of the validator set since we won't broadcast
                    // timeout in this epoch
                    monitor!(
                        "process_epoch_retrieval",
                        self.process_epoch_retrieval(
                            EpochRetrievalRequest {
                                start_epoch: different_epoch,
                                end_epoch: self.epoch(),
                            },
                            peer_id
                        )
                    )
                }
            },
            // We request proof to join higher epoch
            Ordering::Greater => {
                let request = EpochRetrievalRequest {
                    start_epoch: self.epoch(),
                    end_epoch: different_epoch,
                };
                let msg = ConsensusMsg::EpochRetrievalRequest(Box::new(request));
                if let Err(err) = self.network_sender.send_to(peer_id, msg) {
                    warn!(
                        "[EpochManager] Failed to send epoch retrieval to {}, {:?}",
                        peer_id, err
                    );
                    counters::EPOCH_MANAGER_ISSUES_DETAILS
                        .with_label_values(&["failed_to_send_epoch_retrieval"])
                        .inc();
                }

                Ok(())
            },
            Ordering::Equal => {
                bail!("[EpochManager] Same epoch should not come to process_different_epoch");
            },
        }
    }
```

**File:** consensus/src/epoch_manager.rs (L1561-1562)
```rust
        // we can't verify signatures from a different epoch
        let maybe_unverified_event = self.check_epoch(peer_id, consensus_msg).await?;
```

**File:** consensus/src/epoch_manager.rs (L1627-1653)
```rust
    async fn check_epoch(
        &mut self,
        peer_id: AccountAddress,
        msg: ConsensusMsg,
    ) -> anyhow::Result<Option<UnverifiedEvent>> {
        match msg {
            ConsensusMsg::ProposalMsg(_)
            | ConsensusMsg::OptProposalMsg(_)
            | ConsensusMsg::SyncInfo(_)
            | ConsensusMsg::VoteMsg(_)
            | ConsensusMsg::RoundTimeoutMsg(_)
            | ConsensusMsg::OrderVoteMsg(_)
            | ConsensusMsg::CommitVoteMsg(_)
            | ConsensusMsg::CommitDecisionMsg(_)
            | ConsensusMsg::BatchMsg(_)
            | ConsensusMsg::BatchRequestMsg(_)
            | ConsensusMsg::SignedBatchInfo(_)
            | ConsensusMsg::ProofOfStoreMsg(_) => {
                let event: UnverifiedEvent = msg.into();
                if event.epoch()? == self.epoch() {
                    return Ok(Some(event));
                } else {
                    monitor!(
                        "process_different_epoch_consensus_msg",
                        self.process_different_epoch(event.epoch()?, peer_id)
                    )?;
                }
```

**File:** consensus/src/epoch_manager.rs (L1663-1665)
```rust
                if msg_epoch == self.epoch() {
                    monitor!("process_epoch_proof", self.initiate_new_epoch(*proof).await)?;
                } else {
```
