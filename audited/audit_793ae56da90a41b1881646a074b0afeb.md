# Audit Report

## Title
Message Loss During Consensus Publisher Shutdown Causes Observers to Miss Final Consensus Decisions

## Summary
The consensus publisher's `start()` function exits without waiting for the spawned message serializer task to complete, causing pending consensus messages (including critical commit decisions) to be lost during shutdown. This results in consensus observers missing final block commitments and serving stale blockchain state.

## Finding Description

The vulnerability exists in the consensus observer publisher shutdown logic. When the `start()` function's event loop exits via the `else` branch, it immediately returns without ensuring that all pending outbound messages have been flushed to observers. [1](#0-0) 

The `start()` function spawns an independent message serializer task that processes messages from the `outbound_message_receiver` channel: [2](#0-1) 

This spawned task runs asynchronously and processes messages through a serialization pipeline: [3](#0-2) 

**The critical flaw**: When the `tokio::select!` loop's `else` branch is triggered (line 265-266), the function breaks and returns immediately (line 272-273), but the spawned serializer task continues running independently. If the Tokio runtime is subsequently dropped during node shutdown, the spawned task is cancelled mid-flight, causing message loss.

**Attack Vector**: During normal consensus operation, validators publish critical messages via `publish_message()`: [4](#0-3) 

These commit decision messages contain `LedgerInfoWithSignatures` proving that blocks have been committed by a quorum. When observers receive these messages, they update their view of the committed blockchain state: [5](#0-4) 

**Exploitation Scenario**:
1. Validator commits blocks and calls `publish_message()` with commit decisions
2. Messages are queued in the `outbound_message_sender` channel (mpsc bounded channel)
3. The spawned serializer task begins processing messages asynchronously
4. Node shutdown occurs (SIGTERM, component restart, network handler shutdown)
5. The `publisher_message_receiver` channel closes, triggering the `else` branch
6. `start()` returns immediately without awaiting the spawned task
7. The publisher runtime is dropped when `AptosHandle` is dropped: [6](#0-5) 

8. The spawned task is cancelled, losing all messages in the pipeline (queued, being serialized, or being sent)
9. Observers never receive these critical commit decisions

**Invariant Violation**: This breaks the **State Consistency** invariant - consensus observers will have an inconsistent view of the blockchain state, as they miss final commit decisions proving which blocks have been finalized by the network.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under Aptos bug bounty criteria for "Significant protocol violations" because:

1. **Loss of Critical Consensus Messages**: Commit decision messages containing `LedgerInfoWithSignatures` are lost, preventing observers from learning about committed blocks

2. **State Inconsistency**: Observers serve stale blockchain state to clients during the window before state sync triggers, potentially causing applications to make incorrect decisions based on outdated data

3. **Protocol Violation**: The consensus observer protocol assumes reliable message delivery for critical consensus updates, but this assumption is violated during shutdown

4. **Cascading Impact**: If multiple commit decisions are lost during shutdown (e.g., rapid-fire commits followed by shutdown), observers fall significantly behind and require full state synchronization

5. **No Graceful Degradation**: There is no mechanism to retry or recover lost messages - they are permanently lost, requiring observers to fall back to state sync

The impact does not reach CRITICAL severity because:
- No funds are directly at risk
- No consensus safety violations occur (validators still commit correctly)
- Observers eventually recover via state sync mechanisms
- The issue is self-correcting on observer restart

## Likelihood Explanation

**Likelihood: HIGH**

This issue will occur during every normal node shutdown scenario:

1. **Validator Restarts**: Routine validator maintenance or upgrades trigger graceful shutdown
2. **Component Reconfiguration**: Consensus observer network handler shutdown during reconfiguration
3. **Process Termination**: Any SIGTERM or controlled shutdown signal
4. **Runtime Cleanup**: When the `AptosHandle` is dropped during node teardown

The vulnerability is guaranteed to manifest when:
- Messages are in-flight during shutdown (common during active consensus)
- The time window between loop exit and runtime drop is shorter than message processing time
- Multiple subscribers are active (increases message backlog)

The issue is exacerbated by:
- High consensus throughput (more messages queued)
- Network latency (messages stuck in serialization)
- Multiple active observers (larger fanout increases backlog)

## Recommendation

The fix requires ensuring the spawned message serializer task completes before `start()` returns. This can be achieved by:

1. **Return a JoinHandle**: Modify `spawn_message_serializer_and_sender()` to return a `JoinHandle`
2. **Await Task Completion**: Before `start()` returns, await the join handle with a timeout
3. **Graceful Shutdown**: Add explicit shutdown signaling to allow controlled message flushing

**Recommended Fix** (conceptual code structure):

```rust
// In spawn_message_serializer_and_sender(), return the JoinHandle
fn spawn_message_serializer_and_sender(...) -> JoinHandle<()> {
    tokio::spawn(async move {
        // ... existing serialization logic ...
    })
}

// In start(), await the spawned task before exiting
pub async fn start(self, ...) {
    let serializer_task = spawn_message_serializer_and_sender(...);
    
    // ... existing event loop ...
    
    // Before returning, ensure all messages are flushed
    drop(self.outbound_message_sender); // Close the channel
    
    // Wait for serializer task to finish with timeout
    let timeout_duration = Duration::from_secs(5);
    match tokio::time::timeout(timeout_duration, serializer_task).await {
        Ok(_) => info!("Message serializer completed gracefully"),
        Err(_) => warn!("Message serializer did not complete within timeout"),
    }
}
```

**Alternative Approach**: Implement explicit shutdown signaling using a dedicated shutdown channel that the serializer task monitors, allowing it to flush remaining messages before terminating.

## Proof of Concept

```rust
#[tokio::test]
async fn test_message_loss_on_publisher_shutdown() {
    use consensus_observer::publisher::consensus_publisher::ConsensusPublisher;
    use consensus_observer::network::observer_client::ConsensusObserverClient;
    use aptos_config::config::ConsensusObserverConfig;
    use aptos_network::application::interface::NetworkClient;
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::sync::Arc;
    
    // Create test network client and consensus observer client
    let network_client = NetworkClient::new(vec![], vec![], hashmap![], PeersAndMetadata::new(&[NetworkId::Validator]));
    let consensus_observer_client = Arc::new(ConsensusObserverClient::new(network_client));
    
    // Create consensus publisher
    let config = ConsensusObserverConfig::default();
    let (consensus_publisher, outbound_message_receiver) = 
        ConsensusPublisher::new(config, consensus_observer_client.clone());
    
    // Create test subscriber
    let peer_network_id = PeerNetworkId::new(NetworkId::Validator, PeerId::random());
    consensus_publisher.add_active_subscriber(peer_network_id);
    
    // Counter for messages actually sent
    let messages_sent = Arc::new(AtomicUsize::new(0));
    let messages_sent_clone = messages_sent.clone();
    
    // Spawn task to count messages that actually get sent
    let _counter_task = tokio::spawn(async move {
        let mut receiver = outbound_message_receiver;
        while let Some(_) = receiver.next().await {
            messages_sent_clone.fetch_add(1, Ordering::SeqCst);
        }
    });
    
    // Publish many messages rapidly
    let num_messages = 1000;
    for i in 0..num_messages {
        let message = ConsensusObserverMessage::new_commit_decision_message(
            LedgerInfoWithSignatures::new(
                LedgerInfo::new(BlockInfo::empty(), HashValue::random()),
                AggregateSignature::empty(),
            )
        );
        consensus_publisher.publish_message(message);
    }
    
    // Simulate immediate shutdown by dropping the publisher
    // (mimics what happens when runtime is dropped)
    drop(consensus_publisher);
    
    // Small delay to see if any messages were processed
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    let actual_sent = messages_sent.load(Ordering::SeqCst);
    println!("Published {} messages, but only {} were sent before shutdown", 
             num_messages, actual_sent);
    
    // Assert that messages were lost
    assert!(actual_sent < num_messages, 
            "Expected message loss during shutdown, but all messages were sent");
}
```

**Expected Result**: The test demonstrates that when the publisher is dropped immediately after publishing messages, many messages are lost because the serializer task is cancelled before completing its work.

## Notes

This vulnerability is particularly concerning because:

1. **Silent Failures**: Messages are lost without any indication to the sender or receiver
2. **No Retry Mechanism**: The publish_message() method is non-blocking (line 212) and uses `try_send()`, so there's no built-in retry for lost messages
3. **Critical Path**: Commit decisions are on the critical consensus path - observers rely on these to understand blockchain finality
4. **Widespread Impact**: All consensus observers subscribed to the publisher are affected simultaneously

The issue highlights a broader pattern in the codebase where spawned tasks lack proper lifecycle management during shutdown. A comprehensive audit of all `tokio::spawn()` calls without corresponding join handle tracking would be beneficial.

### Citations

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L234-274)
```rust
    /// Starts the consensus publisher
    pub async fn start(
        self,
        outbound_message_receiver: mpsc::Receiver<(PeerNetworkId, ConsensusObserverDirectSend)>,
        mut publisher_message_receiver: Receiver<(), ConsensusPublisherNetworkMessage>,
    ) {
        // Spawn the message serializer and sender
        spawn_message_serializer_and_sender(
            self.consensus_observer_client.clone(),
            self.consensus_observer_config,
            outbound_message_receiver,
        );

        // Create a garbage collection ticker
        let mut garbage_collection_interval = IntervalStream::new(interval(Duration::from_millis(
            self.consensus_observer_config
                .garbage_collection_interval_ms,
        )))
        .fuse();

        // Start the publisher garbage collection loop
        info!(LogSchema::new(LogEntry::ConsensusPublisher)
            .message("Starting the consensus publisher garbage collection loop!"));
        loop {
            tokio::select! {
                Some(network_message) = publisher_message_receiver.next() => {
                    self.process_network_message(network_message);
                },
                _ = garbage_collection_interval.select_next_some() => {
                    self.garbage_collect_subscriptions();
                },
                else => {
                    break; // Exit the consensus publisher loop
                }
            }
        }

        // Log the exit of the consensus publisher loop
        error!(LogSchema::new(LogEntry::ConsensusPublisher)
            .message("The consensus publisher loop exited unexpectedly!"));
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L286-349)
```rust
    tokio::spawn(async move {
        // Create the message serialization task
        let consensus_observer_client_clone = consensus_observer_client.clone();
        let serialization_task =
            outbound_message_receiver.map(move |(peer_network_id, message)| {
                // Spawn a new blocking task to serialize the message
                let consensus_observer_client_clone = consensus_observer_client_clone.clone();
                tokio::task::spawn_blocking(move || {
                    let message_label = message.get_label();
                    let serialized_message = consensus_observer_client_clone
                        .serialize_message_for_peer(&peer_network_id, message);
                    (peer_network_id, serialized_message, message_label)
                })
            });

        // Execute the serialization task with in-order buffering
        let consensus_observer_client_clone = consensus_observer_client.clone();
        serialization_task
            .buffered(consensus_observer_config.max_parallel_serialization_tasks)
            .map(|serialization_result| {
                // Attempt to send the serialized message to the peer
                match serialization_result {
                    Ok((peer_network_id, serialized_message, message_label)) => {
                        match serialized_message {
                            Ok(serialized_message) => {
                                // Send the serialized message to the peer
                                if let Err(error) = consensus_observer_client_clone
                                    .send_serialized_message_to_peer(
                                        &peer_network_id,
                                        serialized_message,
                                        message_label,
                                    )
                                {
                                    // We failed to send the message
                                    warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                                        .event(LogEvent::SendDirectSendMessage)
                                        .message(&format!(
                                            "Failed to send message to peer: {:?}. Error: {:?}",
                                            peer_network_id, error
                                        )));
                                }
                            },
                            Err(error) => {
                                // We failed to serialize the message
                                warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                                    .event(LogEvent::SendDirectSendMessage)
                                    .message(&format!(
                                        "Failed to serialize message for peer: {:?}. Error: {:?}",
                                        peer_network_id, error
                                    )));
                            },
                        }
                    },
                    Err(error) => {
                        // We failed to spawn the serialization task
                        warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                            .event(LogEvent::SendDirectSendMessage)
                            .message(&format!("Failed to spawn the serializer task: {:?}", error)));
                    },
                }
            })
            .collect::<()>()
            .await;
    });
```

**File:** consensus/src/pipeline/buffer_manager.rs (L514-517)
```rust
                if let Some(consensus_publisher) = &self.consensus_publisher {
                    let message =
                        ConsensusObserverMessage::new_commit_decision_message(commit_proof.clone());
                    consensus_publisher.publish_message(message);
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L442-528)
```rust
    fn process_commit_decision_message(
        &mut self,
        peer_network_id: PeerNetworkId,
        message_received_time: Instant,
        commit_decision: CommitDecision,
    ) {
        // Get the commit decision epoch and round
        let commit_epoch = commit_decision.epoch();
        let commit_round = commit_decision.round();

        // If the commit message is behind our highest committed block, ignore it
        let get_highest_committed_epoch_round = self
            .observer_block_data
            .lock()
            .get_highest_committed_epoch_round();
        if (commit_epoch, commit_round) <= get_highest_committed_epoch_round {
            // Update the metrics for the dropped commit decision
            update_metrics_for_dropped_commit_decision_message(peer_network_id, &commit_decision);
            return;
        }

        // Update the metrics for the received commit decision
        update_metrics_for_commit_decision_message(peer_network_id, &commit_decision);

        // If the commit decision is for the current epoch, verify and process it
        let epoch_state = self.get_epoch_state();
        if commit_epoch == epoch_state.epoch {
            // Verify the commit decision
            if let Err(error) = commit_decision.verify_commit_proof(&epoch_state) {
                // Log the error and update the invalid message counter
                error!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Failed to verify commit decision! Ignoring: {:?}, from peer: {:?}. Error: {:?}",
                        commit_decision.proof_block_info(),
                        peer_network_id,
                        error
                    ))
                );
                increment_invalid_message_counter(&peer_network_id, metrics::COMMIT_DECISION_LABEL);
                return;
            }

            // Update the latency metrics for commit processing
            update_message_processing_latency_metrics(
                message_received_time,
                &peer_network_id,
                metrics::COMMIT_DECISION_LABEL,
            );

            // Update the pending blocks with the commit decision
            if self.process_commit_decision_for_pending_block(&commit_decision) {
                return; // The commit decision was successfully processed
            }
        }

        // TODO: identify the best way to handle an invalid commit decision
        // for a future epoch. In such cases, we currently rely on state sync.

        // Otherwise, we failed to process the commit decision. If the commit
        // is for a future epoch or round, we need to state sync.
        let last_block = self.observer_block_data.lock().get_last_ordered_block();
        let epoch_changed = commit_epoch > last_block.epoch();
        if epoch_changed || commit_round > last_block.round() {
            // If we're waiting for state sync to transition into a new epoch,
            // we should just wait and not issue a new state sync request.
            if self.state_sync_manager.is_syncing_through_epoch() {
                info!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Already waiting for state sync to reach new epoch: {:?}. Dropping commit decision: {:?}!",
                        self.observer_block_data.lock().root().commit_info(),
                        commit_decision.proof_block_info()
                    ))
                );
                return;
            }

            // Otherwise, we should start the state sync process for the commit.
            // Update the block data (to the commit decision).
            self.observer_block_data
                .lock()
                .update_blocks_for_state_sync_commit(&commit_decision);

            // Start state syncing to the commit decision
            self.state_sync_manager
                .sync_to_commit(commit_decision, epoch_changed);
        }
    }
```

**File:** aptos-node/src/lib.rs (L196-215)
```rust
/// Runtime handle to ensure that all inner runtimes stay in scope
pub struct AptosHandle {
    _admin_service: AdminService,
    _api_runtime: Option<Runtime>,
    _backup_runtime: Option<Runtime>,
    _consensus_observer_runtime: Option<Runtime>,
    _consensus_publisher_runtime: Option<Runtime>,
    _consensus_runtime: Option<Runtime>,
    _dkg_runtime: Option<Runtime>,
    _indexer_grpc_runtime: Option<Runtime>,
    _indexer_runtime: Option<Runtime>,
    _indexer_table_info_runtime: Option<Runtime>,
    _jwk_consensus_runtime: Option<Runtime>,
    _mempool_runtime: Runtime,
    _network_runtimes: Vec<Runtime>,
    _peer_monitoring_service_runtime: Runtime,
    _state_sync_runtimes: StateSyncRuntimes,
    _telemetry_runtime: Option<Runtime>,
    _indexer_db_runtime: Option<Runtime>,
}
```
