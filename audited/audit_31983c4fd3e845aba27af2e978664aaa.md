# Audit Report

## Title
Health Checker Connection Event Channel Overflow Causes Stale Peer State and Network Monitoring Degradation

## Summary
The health checker's connection event subscription mechanism uses a bounded channel that silently drops events when full, leading to stale peer state tracking. This causes the health checker to monitor disconnected peers while ignoring newly connected ones, degrading network health monitoring and wasting resources.

## Finding Description

The health checker subscribes to connection events (NewPeer/LostPeer) to track which peers to monitor. [1](#0-0) 

This subscription returns a bounded `tokio::sync::mpsc::Receiver` with capacity `NOTIFICATION_BACKLOG` (1000 messages). [2](#0-1) 

When new peers connect or disconnect, the `PeersAndMetadata::broadcast()` method attempts to send events to all subscribers. However, it uses `try_send()` which **silently drops events** when the channel is full: [3](#0-2) 

The critical flaw is that when `TrySendError::Full(_)` occurs, the event is dropped with only a sampled warning log, and no error is propagated to the health checker.

The health checker maintains its own internal state `health_check_data` mapping peers to their health status: [4](#0-3) 

When connection events are processed, the health checker updates this map: [5](#0-4) 

The health checker only pings peers in `health_check_data`, retrieved via `connected_peers()`: [6](#0-5) 

**Attack Scenario:**

1. **Network Startup/Churn**: During validator node startup or network partition recovery, many peers connect simultaneously (e.g., 100+ validators reconnecting)
2. **Channel Overflow**: The rapid burst of NewPeer events exceeds the 1000-message buffer while the health checker is processing pings
3. **Missed NewPeer Events**: New connection events are silently dropped, so `create_peer_and_health_data()` is never called for those peers
4. **Missed LostPeer Events**: When peers disconnect, if those events are dropped, `remove_peer_and_health_data()` isn't called, leaving stale entries
5. **Cascade Effect**: The health checker continues pinging dead peers (with 50ms timeout per disconnect attempt), further blocking event processing and increasing overflow likelihood [7](#0-6) 

**Invariant Violation:**
- Network health monitoring integrity is compromised - the health checker's view of connected peers diverges from actual network state
- Resource exhaustion - CPU and network bandwidth wasted on dead peer monitoring

## Impact Explanation

This qualifies as **High Severity** under Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: The health checker continuously attempts to ping disconnected peers, each requiring a 20-second timeout before failure. With multiple stale peers, this significantly degrades performance.

2. **Significant Protocol Violations**: The health checker is a critical network component that ensures peer liveness. When it has stale state:
   - New validator peers aren't health-checked, potentially allowing degraded nodes to participate in consensus
   - Resources are wasted pinging ghosts instead of monitoring active peers
   - Network health metrics become unreliable, affecting operational decisions

3. **Resource Exhaustion**: Each missed LostPeer event means the health checker will eternally ping that dead peer every 10 seconds (PING_INTERVAL_MS), wait 20 seconds for timeout (PING_TIMEOUT_MS), then eventually call disconnect with a 50ms timeout - all pure waste.

While this doesn't directly compromise consensus safety, it degrades the network's ability to detect and respond to unhealthy validators, which indirectly impacts network reliability and availability.

## Likelihood Explanation

**Likelihood: High** during specific operational scenarios:

1. **Validator Node Startup**: When a node starts, it may connect to 100+ peers simultaneously, generating a burst of events that can exceed the 1000-message buffer if the health checker's event loop is processing other operations.

2. **Network Partition Recovery**: After network splits resolve, mass reconnections generate event bursts.

3. **Epoch Transitions**: Validator set changes during epoch boundaries can cause rapid connection churn.

4. **Normal Operation**: Even under normal conditions, the health checker's event loop processes multiple event sources concurrently (pings, responses, connection events, ticks). If ping response processing or disconnect operations delay the loop, connection events accumulate.

The issue is exacerbated by the fact that failed ping attempts trigger slow disconnect operations (50ms timeout), which block the event loop and make overflow more likely - creating a positive feedback loop.

## Recommendation

**Fix 1: Use Unbounded Channel or Larger Buffer**
Replace the bounded channel with an unbounded one for critical connection events, or significantly increase `NOTIFICATION_BACKLOG`:

```rust
// In storage.rs, subscribe() method
pub fn subscribe(&self) -> tokio::sync::mpsc::Receiver<ConnectionNotification> {
    // Use unbounded channel for critical connection events
    let (sender, receiver) = tokio::sync::mpsc::unbounded_channel();
    // ... rest of implementation
}
```

**Fix 2: Fail Fast on Overflow**
Instead of silently dropping events, close the receiver to force subscribers to detect the issue:

```rust
// In broadcast() method
TrySendError::Full(_) => {
    // Don't silently drop - this is a critical failure
    warn!("Connection event channel full for subscriber - closing channel");
    to_del.push(i);  // Remove subscriber to force resubscription
}
```

**Fix 3: Periodic State Reconciliation**
Add periodic reconciliation where the health checker queries actual connected peers and syncs its internal state:

```rust
// In health_checker mod.rs, add to event loop
const RECONCILIATION_INTERVAL: Duration = Duration::from_secs(60);
let reconciliation_ticker = self.time_service.interval(RECONCILIATION_INTERVAL);

// In select! loop:
_ = reconciliation_ticker.select_next_some() => {
    let actual_connected = self.network_interface.get_peers_and_metadata()
        .get_connected_peers_and_metadata()
        .unwrap_or_default();
    
    // Remove stale entries
    let tracked_peers: HashSet<_> = self.network_interface.connected_peers()
        .into_iter().collect();
    let actual_peers: HashSet<_> = actual_connected.keys()
        .map(|pni| pni.peer_id()).collect();
    
    for stale_peer in tracked_peers.difference(&actual_peers) {
        self.network_interface.remove_peer_and_health_data(stale_peer);
    }
    
    // Add missing entries
    for new_peer in actual_peers.difference(&tracked_peers) {
        self.network_interface.create_peer_and_health_data(*new_peer, self.round);
    }
}
```

**Recommended Solution**: Implement Fix 3 (periodic reconciliation) as defense-in-depth, plus increase buffer size to 10,000 to handle typical network conditions.

## Proof of Concept

```rust
// Add to network/framework/src/protocols/health_checker/test.rs

#[tokio::test]
async fn test_connection_event_overflow_causes_stale_state() {
    use crate::peer_manager::ConnectionNotification;
    use aptos_types::PeerId;
    
    let (mut harness, mut health_checker) = TestHarness::new_strict();
    let health_checker_handle = tokio::spawn(async move {
        health_checker.start().await;
    });
    
    // Simulate overflow by sending more than NOTIFICATION_BACKLOG events
    // The channel created in test harness has capacity 10, so we overflow easily
    let peer_ids: Vec<PeerId> = (0..20)
        .map(|_| PeerId::random())
        .collect();
    
    // Send NewPeer events
    for peer_id in &peer_ids {
        let conn_metadata = ConnectionMetadata::mock(*peer_id);
        let notif = ConnectionNotification::NewPeer(
            conn_metadata,
            NetworkId::Validator
        );
        
        // Some sends will succeed, some will overflow and be dropped
        let _ = harness.connection_notifs_tx.try_send(notif);
    }
    
    // Give health checker time to process events
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Add peer to actual network state
    for peer_id in &peer_ids[0..10] {
        harness.peers_and_metadata.insert_connection_metadata(
            PeerNetworkId::new(NetworkId::Validator, *peer_id),
            ConnectionMetadata::mock(*peer_id),
        ).unwrap();
    }
    
    // Verify: Health checker should track all peers, but due to overflow,
    // it will have missed some NewPeer events
    // This demonstrates stale state where health_check_data diverges from reality
    
    // The vulnerability is proven when health_checker.connected_peers() 
    // returns fewer peers than actually connected due to missed events
}
```

## Notes

This vulnerability affects all network instances (validator, VFN, PFN) that use the health checker. The impact is most severe on validator nodes where accurate peer health monitoring is critical for consensus participation and network stability.

The root cause is the architectural decision to use bounded channels with silent drop semantics for critical state synchronization events. While bounded channels prevent unbounded memory growth, connection events are inherently bounded by the maximum number of peers, making an unbounded or much larger channel acceptable for this use case.

### Citations

**File:** network/framework/src/protocols/health_checker/mod.rs (L160-165)
```rust
        let connection_events = self
            .connection_events_injection
            .take()
            .unwrap_or_else(|| self.network_interface.get_peers_and_metadata().subscribe());
        let mut connection_events =
            tokio_stream::wrappers::ReceiverStream::new(connection_events).fuse();
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L209-228)
```rust
                conn_event = connection_events.select_next_some() => {
                    match conn_event {
                        ConnectionNotification::NewPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.create_peer_and_health_data(
                                    metadata.remote_peer_id, self.round
                                );
                            }
                        }
                        ConnectionNotification::LostPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.remove_peer_and_health_data(
                                    &metadata.remote_peer_id
                                );
                            }
                        }
                    }
                }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L373-391)
```rust
                    if let Err(err) = timeout(
                        Duration::from_millis(50),
                        self.network_interface.disconnect_peer(
                            peer_network_id,
                            DisconnectReason::NetworkHealthCheckFailure,
                        ),
                    )
                    .await
                    {
                        warn!(
                            NetworkSchema::new(&self.network_context)
                                .remote_peer(&peer_id),
                            error = ?err,
                            "{} Failed to disconnect from peer: {} with error: {:?}",
                            self.network_context,
                            peer_id.short_str(),
                            err
                        );
                    }
```

**File:** network/framework/src/application/storage.rs (L31-35)
```rust
// notification_backlog is how many ConnectionNotification items can be queued waiting for an app to receive them.
// Beyond this, new messages will be dropped if the app is not handling them fast enough.
// We make this big enough to fit an initial burst of _all_ the connected peers getting notified.
// Having 100 connected peers is common, 500 not unexpected
const NOTIFICATION_BACKLOG: usize = 1000;
```

**File:** network/framework/src/application/storage.rs (L371-395)
```rust
    fn broadcast(&self, event: ConnectionNotification) {
        let mut listeners = self.subscribers.lock();
        let mut to_del = vec![];
        for i in 0..listeners.len() {
            let dest = listeners.get_mut(i).unwrap();
            if let Err(err) = dest.try_send(event.clone()) {
                match err {
                    TrySendError::Full(_) => {
                        // Tried to send to an app, but the app isn't handling its messages fast enough.
                        // Drop message. Maybe increment a metrics counter?
                        sample!(
                            SampleRate::Duration(Duration::from_secs(1)),
                            warn!("PeersAndMetadata.broadcast() failed, some app is slow"),
                        );
                    },
                    TrySendError::Closed(_) => {
                        to_del.push(i);
                    },
                }
            }
        }
        for evict in to_del.into_iter() {
            listeners.swap_remove(evict);
        }
    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L38-43)
```rust
/// HealthChecker's view into networking
pub struct HealthCheckNetworkInterface<NetworkClient> {
    health_check_data: RwLock<HashMap<PeerId, HealthCheckData>>,
    network_client: NetworkClient,
    receiver: HealthCheckerNetworkEvents,
}
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L58-61)
```rust
    /// Returns all connected peers
    pub fn connected_peers(&self) -> Vec<PeerId> {
        self.health_check_data.read().keys().cloned().collect()
    }
```
