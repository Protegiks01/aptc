# Audit Report

## Title
Concurrent Migration Race Condition Leading to Database Schema Corruption in Indexer

## Summary
The `run_migrations()` function in the Aptos indexer lacks concurrency control when multiple indexer instances start simultaneously, leading to race conditions that can corrupt the PostgreSQL database schema and cause service unavailability.

## Finding Description

The indexer's `run_migrations()` function executes database schema migrations without any distributed locking or coordination mechanism between multiple indexer instances. [1](#0-0) 

When multiple indexer instances start concurrently (common in Kubernetes deployments, rolling updates, or high-availability setups), they all attempt to apply pending migrations simultaneously. The diesel_migrations library (version 2.1.0) [2](#0-1)  tracks applied migrations in a `__diesel_schema_migrations` table, but without proper distributed locking, the following race condition occurs:

1. **Instance A** reads `__diesel_schema_migrations`, sees migration X is pending
2. **Instance B** simultaneously reads `__diesel_schema_migrations`, sees migration X is pending
3. Both instances attempt to execute migration X concurrently
4. Both instances try to create the same tables, indexes, and constraints

This results in:
- "Relation already exists" errors when both try to CREATE TABLE
- Unique constraint violations when both try to INSERT into `__diesel_schema_migrations`
- Partially applied migrations if one instance fails midway through a multi-statement migration
- Inconsistent schema state requiring manual intervention

The Aptos codebase explicitly acknowledges this issue in the local testnet processor code, where a `OnceCell` workaround is implemented to prevent concurrent migrations within a single process: [3](#0-2) 

However, this workaround only applies to the local testnet scenario where multiple processors run in the same process. The main indexer code running in production deployments has no such protection and uses separate processes/containers, making the `OnceCell` approach ineffective.

The configuration system defaults to running migrations on every startup: [4](#0-3) 

This means unless operators explicitly set `skip_migrations: true`, all indexer instances will race to apply migrations.

## Impact Explanation

This vulnerability qualifies as **High to Critical Severity** based on the following impacts:

**Service Unavailability (High Severity - $50,000):**
- Failed migrations cause indexer instances to panic and fail startup [5](#0-4) 
- The indexer is critical infrastructure for blockchain data availability and API queries
- Operators must manually intervene to fix corrupted schema state

**State Inconsistencies Requiring Intervention (Medium Severity - $10,000):**
- Partially applied migrations leave the database in an inconsistent state
- Recovery requires manual database inspection and potential rollback
- May require dropping and recreating affected tables
- Risk of data loss if migrations are reapplied incorrectly

**Potential for Critical Impact:**
If schema corruption occurs during a critical migration that affects transaction processing or state synchronization, this could escalate to **Critical Severity** by causing:
- Loss of blockchain data indexing capability
- Inability to serve blockchain state to applications
- Potential for incorrect data being served if schema inconsistencies affect query results

## Likelihood Explanation

This vulnerability has **HIGH** likelihood of occurring because:

1. **Common Deployment Pattern**: Multiple indexer instances are standard practice for:
   - High availability deployments
   - Rolling updates in Kubernetes
   - Horizontal scaling for performance
   - Blue-green deployments

2. **Default Configuration**: The `skip_migrations` flag defaults to `false`, meaning migrations run automatically unless explicitly disabled [4](#0-3) 

3. **Natural Trigger**: This occurs during normal operations, not requiring attacker action:
   - Restarting all indexer instances simultaneously
   - Deploying new versions with pending migrations
   - Scaling up indexer replicas

4. **Known Issue**: The codebase itself acknowledges this problem exists, referencing Stack Overflow discussions about diesel migration conflicts [6](#0-5) 

## Recommendation

Implement distributed locking for database migrations using one of the following approaches:

**Option 1: PostgreSQL Advisory Locks**
```rust
pub fn run_migrations(&self) {
    let mut conn = self
        .connection_pool
        .get()
        .expect("Could not get connection for migrations");
    
    // Acquire PostgreSQL advisory lock with a unique key
    // This blocks until the lock is available
    diesel::sql_query("SELECT pg_advisory_lock(123456789)")
        .execute(&mut conn)
        .expect("Failed to acquire migration lock");
    
    match conn.run_pending_migrations(MIGRATIONS) {
        Ok(_) => info!("Migrations completed successfully"),
        Err(e) => {
            // Release lock before panicking
            diesel::sql_query("SELECT pg_advisory_unlock(123456789)")
                .execute(&mut conn)
                .ok();
            panic!("Migrations failed: {:?}", e);
        }
    }
    
    // Release the advisory lock
    diesel::sql_query("SELECT pg_advisory_unlock(123456789)")
        .execute(&mut conn)
        .expect("Failed to release migration lock");
}
```

**Option 2: Make Migrations Idempotent**
Modify all migration files to use `CREATE TABLE IF NOT EXISTS`, `CREATE INDEX IF NOT EXISTS`, etc., and handle the `__diesel_schema_migrations` table insertions gracefully.

**Option 3: Operational Fix**
Update deployment documentation to require operators to:
- Set `skip_migrations: true` in configuration for all indexer instances
- Run migrations manually from a single dedicated job/pod before deploying indexer instances
- Use Kubernetes init containers or Helm hooks to run migrations once

**Recommended Immediate Action:**
Implement Option 1 (advisory locks) as it provides the strongest guarantee without requiring changes to all migration files and works transparently with diesel's existing migration system.

## Proof of Concept

```rust
// Reproduction steps:
// 1. Set up a PostgreSQL database
// 2. Configure two indexer instances pointing to the same database
// 3. Ensure both have skip_migrations = false (default)
// 4. Start both instances simultaneously
// 5. Observe migration conflicts and panics

use diesel::Connection;
use diesel_migrations::MigrationHarness;
use std::thread;

#[test]
fn test_concurrent_migration_race_condition() {
    let database_url = "postgresql://user:pass@localhost/test_db";
    
    // Simulate two indexer instances starting concurrently
    let handle1 = thread::spawn(move || {
        let mut conn = diesel::pg::PgConnection::establish(database_url)
            .expect("Failed to connect");
        conn.run_pending_migrations(MIGRATIONS)
            .expect("Instance 1 migrations failed");
    });
    
    let handle2 = thread::spawn(move || {
        let mut conn = diesel::pg::PgConnection::establish(database_url)
            .expect("Failed to connect");
        conn.run_pending_migrations(MIGRATIONS)
            .expect("Instance 2 migrations failed");
    });
    
    // At least one thread will likely fail with:
    // - "relation already exists" error
    // - unique constraint violation on __diesel_schema_migrations
    let result1 = handle1.join();
    let result2 = handle2.join();
    
    // Verify at least one failed
    assert!(result1.is_err() || result2.is_err());
}
```

**Notes:**
- This vulnerability is already partially acknowledged in the codebase's local testnet implementation, confirming its validity
- The issue affects production indexer deployments where multiple instances run as separate processes
- The lack of distributed locking makes this a systemic issue requiring immediate remediation

### Citations

**File:** crates/indexer/src/indexer/tailer.rs (L56-63)
```rust
    pub fn run_migrations(&self) {
        let _ = &self
            .connection_pool
            .get()
            .expect("Could not get connection for migrations")
            .run_pending_migrations(MIGRATIONS)
            .expect("migrations failed!");
    }
```

**File:** Cargo.toml (L598-598)
```text
diesel_migrations = { version = "2.1.0", features = ["postgres"] }
```

**File:** crates/aptos/src/node/local_testnet/processors.rs (L173-193)
```rust
        // By default, when a processor starts up (specifically in Worker.run) it runs
        // any pending migrations. Unfortunately, if you start multiple processors at
        // the same time, they can sometimes clash with errors like this:
        //
        // https://stackoverflow.com/q/54351783/3846032
        //
        // To fix this, we run the migrations ourselves here in the CLI first. We use
        // OnceCell to make sure we only run the migration once. When all the processor
        // ServiceManagers reach this point, one of them will run the code and the rest
        // will wait. Doing it at this point in the code is safer than relying on
        // coordiation outside of this manager.
        RUN_MIGRATIONS_ONCE
            .get_or_init(|| async {
                info!("Running DB migrations for the indexer processors");
                self.run_migrations()
                    .await
                    .expect("Failed to run DB migrations");
                info!("Ran DB migrations for the indexer processors");
                true
            })
            .await;
```

**File:** config/src/config/indexer_config.rs (L174-174)
```rust
        indexer_config.skip_migrations = indexer_config.skip_migrations.or(Some(false));
```
