# Audit Report

## Title
Orphaned Transaction Entries Due to Ignored Batch Persistence Failures in Quorum Store

## Summary
The `batch_generator` creates batches and marks transactions as "in-progress" before persisting them to storage. When batch persistence fails due to quota exhaustion or other errors, the failure is silently ignored, but transactions remain marked as in-progress. These orphaned transactions are excluded from future mempool pulls until expiration timeout (default 60 seconds), degrading transaction throughput and availability.

## Finding Description

The vulnerability exists in the batch creation and persistence flow in the Quorum Store system. The issue occurs across two key functions:

**Step 1: Transaction Marking (create_new_batch â†’ insert_batch)** [1](#0-0) 

In `create_new_batch()`, the function calls `insert_batch()` which adds transaction summaries to the `txns_in_progress_sorted` data structure. This marks transactions as "in-progress" and excludes them from future mempool pulls. [2](#0-1) 

These transactions are now tracked with incremented reference counts in the in-progress map.

**Step 2: Ignored Persistence Failure** [3](#0-2) 

The `persist()` method is called but its return value (which indicates success/failure) is completely ignored. The batches are then broadcasted to the network regardless of persistence outcome.

**Step 3: Persistence Can Fail** [4](#0-3) 

The `persist()` implementation returns a vector of successfully persisted batch signatures. If `persist_inner()` returns `None` for any batch, it's simply not added to the returned vector - the failure is silent. [5](#0-4) 

Persistence fails when `save()` returns an error, which happens when quota is exceeded: [6](#0-5) 

**Step 4: Exclusion from Mempool** [7](#0-6) 

The `txns_in_progress_sorted` map is used to exclude transactions from mempool pulls, preventing these orphaned transactions from being re-batched.

**Attack Scenario:**
1. Attacker or high load causes quota exhaustion (batch quota or storage quota)
2. Node creates batch, marks transactions as in-progress via `insert_batch()`
3. Batch persistence fails due to quota exceeded error
4. Error is silently ignored, batch broadcasts anyway
5. Transactions remain in `txns_in_progress_sorted` 
6. Transactions cannot be re-pulled from mempool
7. Batch doesn't exist in local storage/cache
8. Other validators may request this batch but node cannot serve it
9. Transactions stuck until expiration timeout (60 seconds default) or proof timeout

## Impact Explanation

This is a **Medium Severity** vulnerability per Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: Transactions are marked as in-progress but their containing batch doesn't exist in persistent storage. This creates inconsistent state between the batch tracking structures and actual stored batches.

- **Reduced transaction throughput**: Orphaned transactions cannot be processed until timeout expires. During high load when quotas are exceeded, this could affect many transactions simultaneously.

- **Temporary availability degradation**: Transactions become temporarily unavailable for consensus, reducing system throughput by up to the number of transactions that fit in batches before quota exhaustion.

- **Not consensus-breaking**: Does not violate consensus safety or cause chain splits. Eventually cleaned up via expiration mechanisms.

## Likelihood Explanation

**High Likelihood** during:

1. **High network load**: When many batches are created rapidly, quotas can be legitimately exceeded before cleanup occurs
2. **Slow block certification**: If block timestamps lag behind, batch expirations don't clear old batches quickly enough
3. **Attack scenarios**: Malicious actors could flood the network with batches to intentionally trigger quota exhaustion
4. **Per-peer quota**: Each peer has independent quotas, so malicious peers could exhaust their quota deliberately

The quota limits are enforced per-peer and include:
- Batch count quota (max number of batches)
- Storage quota (total bytes)
- Memory quota (in-memory bytes)

Default expiration is 60 seconds, meaning affected transactions are orphaned for up to a minute per occurrence.

## Recommendation

**Fix 1: Check persist() return value and handle failures**

Modify the batch generation flow to check which batches were successfully persisted:

```rust
// In batch_generator.rs, around line 486-501
let persist_start = Instant::now();
let mut persist_requests = vec![];
for batch in batches.clone().into_iter() {
    persist_requests.push(batch.into());
}
let signed_batch_infos = self.batch_writer.persist(persist_requests);
counters::BATCH_CREATION_PERSIST_LATENCY.observe_duration(persist_start.elapsed());

// Only broadcast batches that were successfully persisted
let successfully_persisted_batch_ids: HashSet<_> = signed_batch_infos
    .iter()
    .map(|info| info.batch_id())
    .collect();

let batches_to_broadcast: Vec<_> = batches
    .into_iter()
    .filter(|batch| successfully_persisted_batch_ids.contains(&batch.batch_id()))
    .collect();

// Clean up failed batches
for batch in &batches {
    if !successfully_persisted_batch_ids.contains(&batch.batch_id()) {
        self.remove_batch_in_progress(self.my_peer_id, batch.batch_id());
        counters::BATCH_PERSIST_FAILED.inc();
    }
}

if !batches_to_broadcast.is_empty() {
    if self.config.enable_batch_v2 {
        network_sender.broadcast_batch_msg_v2(batches_to_broadcast).await;
    } else {
        let batches = batches_to_broadcast.into_iter().map(|batch| {
            batch.try_into().expect("Cannot send V2 batch with flag disabled")
        }).collect();
        network_sender.broadcast_batch_msg(batches).await;
    }
}
```

**Fix 2: Reorder operations - persist before insert_batch**

Alternatively, move the `insert_batch()` call to after successful persistence verification, but this would require significant refactoring of the `create_new_batch()` function.

## Proof of Concept

```rust
// Rust test to demonstrate the vulnerability
#[tokio::test]
async fn test_orphaned_transactions_on_persist_failure() {
    // Setup batch generator with very low quotas
    let config = QuorumStoreConfig {
        sender_max_batch_txns: 10,
        batch_quota: 2, // Very low quota
        memory_quota: 1024,
        db_quota: 1024,
        ..Default::default()
    };
    
    let mut batch_gen = BatchGenerator::new(
        epoch,
        peer_id,
        config,
        db,
        batch_writer, // Mock that tracks persist calls
        mempool_tx,
        100,
    );
    
    // Pull transactions from mempool
    let txns = create_test_transactions(20);
    
    // Create multiple batches to exceed quota
    let batches1 = batch_gen.handle_scheduled_pull(10).await;
    let batches2 = batch_gen.handle_scheduled_pull(10).await;
    
    // Verify transactions are in txns_in_progress_sorted
    assert_eq!(batch_gen.txns_in_progress_sorted_len(), 20);
    
    // Simulate persist failure by setting quota to 0
    // (In real code this happens when quota is naturally exhausted)
    
    // Try to create another batch - persist will fail
    let batches3 = batch_gen.handle_scheduled_pull(10).await;
    
    // Mock batch_writer.persist() returns empty vector (all failed)
    // But transactions remain in txns_in_progress_sorted
    
    // Verify orphaned state:
    // 1. Transactions still marked as in-progress
    assert_eq!(batch_gen.txns_in_progress_sorted_len(), 30); // All transactions
    
    // 2. But batches3 was not actually persisted
    assert!(batch_store.get_batch_from_local(&batches3[0].digest()).is_err());
    
    // 3. These transactions won't be pulled again from mempool
    let new_pull = batch_gen.handle_scheduled_pull(30).await;
    assert!(new_pull.is_empty()); // Can't pull same transactions
    
    // 4. Only cleanup is via timeout (60 seconds default)
    // Transactions are stuck until expiration
}
```

## Notes

This vulnerability demonstrates a critical gap in error handling where system invariants are not properly maintained across failure boundaries. The `insert_batch()` operation creates a state change that assumes the subsequent `persist()` will succeed, but there's no rollback mechanism when it doesn't. This violates the atomicity principle for state transitions and creates an orphaned state that degrades system availability until timeout-based cleanup occurs.

### Citations

**File:** consensus/src/quorum_store/batch_generator.rs (L149-158)
```rust
        let mut txns = vec![];
        for (summary, info) in txns_in_progress {
            let txn_info = self
                .txns_in_progress_sorted
                .entry(summary)
                .or_insert_with(|| TransactionInProgress::new(info.gas_unit_price));
            txn_info.increment();
            txn_info.gas_unit_price = info.gas_unit_price.max(txn_info.gas_unit_price);
            txns.push(summary);
        }
```

**File:** consensus/src/quorum_store/batch_generator.rs (L173-185)
```rust
    fn create_new_batch(
        &mut self,
        txns: Vec<SignedTransaction>,
        expiry_time: u64,
        bucket_start: u64,
    ) -> Batch<BatchInfoExt> {
        let batch_id = self.batch_id;
        self.batch_id.increment();
        self.db
            .save_batch_id(self.epoch, self.batch_id)
            .expect("Could not save to db");

        self.insert_batch(self.my_peer_id, batch_id, txns.clone(), expiry_time);
```

**File:** consensus/src/quorum_store/batch_generator.rs (L352-360)
```rust
        let mut pulled_txns = self
            .mempool_proxy
            .pull_internal(
                max_count,
                self.config.sender_max_total_bytes as u64,
                self.txns_in_progress_sorted.clone(),
            )
            .await
            .unwrap_or_default();
```

**File:** consensus/src/quorum_store/batch_generator.rs (L486-501)
```rust
                            let persist_start = Instant::now();
                            let mut persist_requests = vec![];
                            for batch in batches.clone().into_iter() {
                                persist_requests.push(batch.into());
                            }
                            self.batch_writer.persist(persist_requests);
                            counters::BATCH_CREATION_PERSIST_LATENCY.observe_duration(persist_start.elapsed());

                            if self.config.enable_batch_v2 {
                                network_sender.broadcast_batch_msg_v2(batches).await;
                            } else {
                                let batches = batches.into_iter().map(|batch| {
                                    batch.try_into().expect("Cannot send V2 batch with flag disabled")
                                }).collect();
                                network_sender.broadcast_batch_msg(batches).await;
                            }
```

**File:** consensus/src/quorum_store/batch_store.rs (L64-84)
```rust
    pub(crate) fn update_quota(&mut self, num_bytes: usize) -> anyhow::Result<StorageMode> {
        if self.batch_balance == 0 {
            counters::EXCEEDED_BATCH_QUOTA_COUNT.inc();
            bail!("Batch quota exceeded ");
        }

        if self.db_balance >= num_bytes {
            self.batch_balance -= 1;
            self.db_balance -= num_bytes;

            if self.memory_balance >= num_bytes {
                self.memory_balance -= num_bytes;
                Ok(StorageMode::MemoryAndPersisted)
            } else {
                Ok(StorageMode::PersistedOnly)
            }
        } else {
            counters::EXCEEDED_STORAGE_QUOTA_COUNT.inc();
            bail!("Storage quota exceeded ");
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L497-527)
```rust
        match self.save(&persist_request) {
            Ok(needs_db) => {
                trace!("QS: sign digest {}", persist_request.digest());
                if needs_db {
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
                }
                if !batch_info.is_v2() {
                    self.generate_signed_batch_info(batch_info.info().clone())
                        .ok()
                        .map(|inner| inner.into())
                } else {
                    self.generate_signed_batch_info(batch_info).ok()
                }
            },
            Err(e) => {
                debug!("QS: failed to store to cache {:?}", e);
                None
            },
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L613-627)
```rust
impl BatchWriter for BatchStore {
    fn persist(
        &self,
        persist_requests: Vec<PersistedValue<BatchInfoExt>>,
    ) -> Vec<SignedBatchInfo<BatchInfoExt>> {
        let mut signed_infos = vec![];
        for persist_request in persist_requests.into_iter() {
            let batch_info = persist_request.batch_info().clone();
            if let Some(signed_info) = self.persist_inner(batch_info, persist_request.clone()) {
                self.notify_subscribers(persist_request);
                signed_infos.push(signed_info);
            }
        }
        signed_infos
    }
```
