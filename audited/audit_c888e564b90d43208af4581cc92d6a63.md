# Audit Report

## Title
State Snapshot Backup Completes Successfully with Incomplete Data Due to Missing Completeness Validation

## Summary
The state snapshot backup process can complete successfully even when the record stream terminates early, resulting in incomplete backups that appear valid. The backup controller does not validate that the number of records processed matches the expected count from `get_state_item_count()`, allowing partial backups to be written with no error indication.

## Finding Description

The vulnerability exists in the backup flow where early termination of the record stream causes incomplete state snapshots to be created without any error or detection mechanism. [1](#0-0) 

The backup process:
1. Calls `get_state_item_count()` to determine the total number of state items
2. Creates a record stream that spawns tasks to fetch records
3. Processes records through a chunker until the stream returns None
4. Collects chunks and writes the manifest [2](#0-1) 

The record stream spawns tasks with discarded join handles. If a spawned task is aborted due to resource exhaustion, system interruption, or other failure before completing, the sender is dropped and the receiver returns None early. [3](#0-2) 

While `send_records_inner()` validates that each chunk receives the expected count, this check only executes if the task completes normally. Task abortion or cancellation before reaching line 349 bypasses this validation.

The critical missing validation is in the manifest writing phase - there is no check that the last chunk's `last_idx` equals the expected total count minus one. [4](#0-3) 

Unlike `EpochEndingBackup`, `StateSnapshotBackup` has no `verify()` method to validate chunk completeness: [5](#0-4) 

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos Bug Bounty criteria: "State inconsistencies requiring intervention."

**Impact:**
1. **Operational Risk**: Incomplete backups appear successful, providing false confidence in disaster recovery capabilities
2. **Resource Waste**: Storage and compute resources consumed by invalid backups  
3. **Recovery Failure**: During disaster recovery, incomplete backups will fail to restore correctly, potentially delaying recovery by hours/days
4. **Silent Failure**: No indication during backup that data is incomplete; detection only occurs during restore when the restored state's root hash doesn't match expected
5. **State Inconsistency**: If nodes restore from incomplete backups without proper validation, they could have divergent state

While this doesn't directly cause consensus violations or fund loss, in a disaster recovery scenario where backups are relied upon, this could lead to extended downtime and potential secondary impacts.

## Likelihood Explanation

**Likelihood: Medium to High**

Scenarios that could trigger early termination:
1. **Resource Exhaustion**: OOM conditions, file descriptor limits, disk space exhaustion during high load
2. **Network Interruptions**: Connection failures to backup service causing unrecoverable errors
3. **System Maintenance**: Process restarts or updates during backup operations
4. **Bugs in Backup Service**: Race conditions or errors in `get_state_snapshot_chunk()` implementation

The vulnerability doesn't require attacker exploitation - it can occur naturally during operational stress. Given that backups typically run during high-load periods and may compete for resources, the likelihood of encountering this condition is not negligible.

## Recommendation

Add validation to ensure backup completeness before writing the manifest:

```rust
async fn run_impl(mut self) -> Result<FileHandle> {
    self.version = Some(self.get_version_for_epoch_ending(self.epoch).await?);
    
    // Get expected count BEFORE creating backup handle
    let expected_count = self.client.get_state_item_count(self.version()).await?;
    
    let backup_handle = self
        .storage
        .create_backup_with_random_suffix(&self.backup_name())
        .await?;

    let record_stream = Box::pin(self.record_stream(self.concurrent_data_requests).await?);
    let chunker = Chunker::new(record_stream, self.max_chunk_size).await?;

    let start = Instant::now();
    let chunk_stream = futures::stream::try_unfold(chunker, |mut chunker| async {
        Ok(chunker.next_chunk().await?.map(|chunk| (chunk, chunker)))
    });

    let chunk_manifest_fut_stream =
        chunk_stream.map_ok(|chunk| self.write_chunk(&backup_handle, chunk));

    let chunks: Vec<_> = chunk_manifest_fut_stream
        .try_buffered_x(8, 4)
        .map_ok(|chunk_manifest| {
            let last_idx = chunk_manifest.last_idx;
            info!(
                last_idx = last_idx,
                values_per_second =
                    ((last_idx + 1) as f64 / start.elapsed().as_secs_f64()) as u64,
                "Chunk written."
            );
            chunk_manifest
        })
        .try_collect()
        .await?;

    // VALIDATION: Ensure we backed up all expected records
    if let Some(last_chunk) = chunks.last() {
        let actual_count = last_chunk.last_idx + 1;
        ensure!(
            actual_count == expected_count,
            "Incomplete backup: expected {} records, but only backed up {} records",
            expected_count,
            actual_count
        );
    } else {
        ensure!(
            expected_count == 0,
            "Incomplete backup: expected {} records, but no chunks were created",
            expected_count
        );
    }

    self.write_manifest(&backup_handle, chunks).await
}
```

Additionally, add a `verify()` method to `StateSnapshotBackup` similar to `EpochEndingBackup`:

```rust
impl StateSnapshotBackup {
    pub fn verify(&self, expected_count: Option<usize>) -> Result<()> {
        // Check chunks are not empty
        ensure!(!self.chunks.is_empty(), "No chunks in backup");
        
        // Check chunk indices are continuous
        let mut next_idx = 0;
        for chunk in &self.chunks {
            ensure!(
                chunk.first_idx == next_idx,
                "Chunk indices not continuous. Expected first_idx: {}, actual: {}",
                next_idx,
                chunk.first_idx
            );
            ensure!(
                chunk.last_idx >= chunk.first_idx,
                "Invalid chunk range [{}, {}]",
                chunk.first_idx,
                chunk.last_idx
            );
            next_idx = chunk.last_idx + 1;
        }
        
        // If expected count provided, validate total
        if let Some(expected) = expected_count {
            let actual = next_idx;
            ensure!(
                actual == expected,
                "Incomplete backup: expected {} records, got {}",
                expected,
                actual
            );
        }
        
        Ok(())
    }
}
```

## Proof of Concept

The following test demonstrates the vulnerability:

```rust
#[tokio::test]
async fn test_incomplete_backup_appears_complete() {
    // Setup: Create a mock backup service that returns fewer records than count
    let expected_count = 1000;
    let actual_records_returned = 500; // Simulating early termination
    
    // Mock client that returns inflated count but delivers fewer records
    struct MockClient {
        actual_records: usize,
    }
    
    impl MockClient {
        async fn get_state_item_count(&self, _version: Version) -> Result<usize> {
            Ok(1000) // Claims 1000 items
        }
        
        async fn get_state_snapshot_chunk(&self, _version: Version, _start: usize, _limit: usize) 
            -> Result<impl Stream<Item = Result<Bytes>>> {
            // Only returns 500 records then ends stream
            let records: Vec<Result<Bytes>> = (0..500)
                .map(|i| Ok(bcs::to_bytes(&(StateKey::mock(i), StateValue::mock(i))).unwrap().into()))
                .collect();
            Ok(stream::iter(records))
        }
    }
    
    // Run backup
    let controller = StateSnapshotBackupController::new(
        StateSnapshotBackupOpt { epoch: 1 },
        GlobalBackupOpt::default(),
        Arc::new(MockClient { actual_records: 500 }),
        Arc::new(MockStorage::new()),
    );
    
    // Backup completes successfully despite being incomplete
    let manifest_handle = controller.run().await.expect("Backup should succeed");
    
    // Load manifest
    let manifest: StateSnapshotBackup = load_manifest(&manifest_handle).await.unwrap();
    
    // Manifest shows incomplete data but no error was raised
    assert_eq!(manifest.chunks.last().unwrap().last_idx, 499); // Only 500 records
    // Expected: Should have failed with error indicating incomplete backup
}
```

## Notes

This vulnerability represents a gap in backup integrity guarantees. While the Merkle proof validation during restore provides eventual detection, the lack of upfront validation in the backup process means:

1. Operators cannot trust backup success messages
2. Incomplete backups consume storage resources
3. Disaster recovery testing may not catch this if validation is not explicit  
4. The expected state count is computed but never validated against actual backed-up data

The fix should be implemented alongside improvements to the restore process to explicitly validate the final root hash after `finish()` completes, as the test suite does but production code does not. [6](#0-5)

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L235-269)
```rust
    async fn run_impl(mut self) -> Result<FileHandle> {
        self.version = Some(self.get_version_for_epoch_ending(self.epoch).await?);
        let backup_handle = self
            .storage
            .create_backup_with_random_suffix(&self.backup_name())
            .await?;

        let record_stream = Box::pin(self.record_stream(self.concurrent_data_requests).await?);
        let chunker = Chunker::new(record_stream, self.max_chunk_size).await?;

        let start = Instant::now();
        let chunk_stream = futures::stream::try_unfold(chunker, |mut chunker| async {
            Ok(chunker.next_chunk().await?.map(|chunk| (chunk, chunker)))
        });

        let chunk_manifest_fut_stream =
            chunk_stream.map_ok(|chunk| self.write_chunk(&backup_handle, chunk));

        let chunks: Vec<_> = chunk_manifest_fut_stream
            .try_buffered_x(8, 4) // 4 concurrently, at most 8 results in buffer.
            .map_ok(|chunk_manifest| {
                let last_idx = chunk_manifest.last_idx;
                info!(
                    last_idx = last_idx,
                    values_per_second =
                        ((last_idx + 1) as f64 / start.elapsed().as_secs_f64()) as u64,
                    "Chunk written."
                );
                chunk_manifest
            })
            .try_collect()
            .await?;

        self.write_manifest(&backup_handle, chunks).await
    }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L271-315)
```rust
    async fn record_stream(
        &self,
        concurrency: usize,
    ) -> Result<impl TryStream<Ok = Bytes, Error = anyhow::Error, Item = Result<Bytes>> + use<>>
    {
        const CHUNK_SIZE: usize = if cfg!(test) { 2 } else { 100_000 };

        let count = self.client.get_state_item_count(self.version()).await?;
        let version = self.version();
        let client = self.client.clone();

        let chunks_stream = futures::stream::unfold(0, move |start_idx| async move {
            if start_idx >= count {
                return None;
            }

            let next_start_idx = start_idx + CHUNK_SIZE;
            let chunk_size = CHUNK_SIZE.min(count - start_idx);

            Some(((start_idx, chunk_size), next_start_idx))
        })
        .map(Result::<_>::Ok);

        let record_stream_stream = chunks_stream.map_ok(move |(start_idx, chunk_size)| {
            let client = client.clone();
            async move {
                let (tx, rx) = tokio::sync::mpsc::channel(chunk_size);
                // spawn and forget, propagate error through channel
                let _join_handle = tokio::spawn(send_records(
                    client.clone(),
                    version,
                    start_idx,
                    chunk_size,
                    tx,
                ));

                Ok(ReceiverStream::new(rx))
            }
        });

        Ok(record_stream_stream
            .try_buffered_x(concurrency * 2, concurrency)
            .try_flatten())
    }
}
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L329-356)
```rust
async fn send_records_inner(
    client: Arc<BackupServiceClient>,
    version: Version,
    start_idx: usize,
    chunk_size: usize,
    sender: &Sender<Result<Bytes>>,
) -> Result<()> {
    let _timer = BACKUP_TIMER.timer_with(&["state_snapshot_record_stream_all"]);
    let mut input = client
        .get_state_snapshot_chunk(version, start_idx, chunk_size)
        .await?;
    let mut count = 0;
    while let Some(record_bytes) = {
        let _timer = BACKUP_TIMER.timer_with(&["state_snapshot_read_record_bytes"]);
        input.read_record_bytes().await?
    } {
        let _timer = BACKUP_TIMER.timer_with(&["state_snapshot_record_stream_send_bytes"]);
        count += 1;
        sender.send(Ok(record_bytes)).await?;
    }
    ensure!(
        count == chunk_size,
        "expecting {} records, got {}",
        chunk_size,
        count
    );
    Ok(())
}
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L449-492)
```rust
    async fn write_manifest(
        &self,
        backup_handle: &BackupHandleRef,
        chunks: Vec<StateSnapshotChunk>,
    ) -> Result<FileHandle> {
        let proof_bytes = self.client.get_state_root_proof(self.version()).await?;
        let (txn_info, _): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            bcs::from_bytes(&proof_bytes)?;

        let (proof_handle, mut proof_file) = self
            .storage
            .create_for_write(backup_handle, Self::proof_name())
            .await?;
        proof_file.write_all(&proof_bytes).await?;
        proof_file.shutdown().await?;

        let manifest = StateSnapshotBackup {
            epoch: self.epoch,
            version: self.version(),
            root_hash: txn_info.transaction_info().ensure_state_checkpoint_hash()?,
            chunks,
            proof: proof_handle,
        };

        let (manifest_handle, mut manifest_file) = self
            .storage
            .create_for_write(backup_handle, Self::manifest_name())
            .await?;
        manifest_file
            .write_all(&serde_json::to_vec(&manifest)?)
            .await?;
        manifest_file.shutdown().await?;

        let metadata = Metadata::new_state_snapshot_backup(
            self.epoch,
            self.version(),
            manifest_handle.clone(),
        );
        self.storage
            .save_metadata_line(&metadata.name(), &metadata.to_text_line()?)
            .await?;

        Ok(manifest_handle)
    }
```

**File:** storage/backup/backup-cli/src/backup_types/epoch_ending/manifest.rs (L28-69)
```rust
impl EpochEndingBackup {
    pub fn verify(&self) -> Result<()> {
        // check number of waypoints
        ensure!(
            self.first_epoch <= self.last_epoch
                && self.last_epoch - self.first_epoch + 1 == self.waypoints.len() as u64,
            "Malformed manifest. first epoch: {}, last epoch {}, num waypoints {}",
            self.first_epoch,
            self.last_epoch,
            self.waypoints.len(),
        );

        // check chunk ranges
        ensure!(!self.chunks.is_empty(), "No chunks.");
        let mut next_epoch = self.first_epoch;
        for chunk in &self.chunks {
            ensure!(
                chunk.first_epoch == next_epoch,
                "Chunk ranges not continuous. Expected first epoch: {}, actual: {}.",
                next_epoch,
                chunk.first_epoch,
            );
            ensure!(
                chunk.last_epoch >= chunk.first_epoch,
                "Chunk range invalid. [{}, {}]",
                chunk.first_epoch,
                chunk.last_epoch,
            );
            next_epoch = chunk.last_epoch + 1;
        }

        // check last epoch in chunk matches manifest
        ensure!(
            next_epoch - 1 == self.last_epoch, // okay to -1 because chunks is not empty.
            "Last epoch in chunks: {}, in manifest: {}",
            next_epoch - 1,
            self.last_epoch,
        );

        Ok(())
    }
}
```

**File:** storage/aptosdb/src/state_restore/restore_test.rs (L231-257)
```rust
fn assert_success<V>(
    db: &MockSnapshotStore<V, V>,
    expected_root_hash: HashValue,
    btree: &BTreeMap<HashValue, (V, V)>,
    version: Version,
) where
    V: TestKey + TestValue,
{
    let tree = JellyfishMerkleTree::new(db);
    for (key, value) in btree.values() {
        let (value_hash, value_index) = tree
            .get_with_proof(CryptoHash::hash(key), version)
            .unwrap()
            .0
            .unwrap();
        let value_in_db = db.get_value_at_version(&value_index).unwrap();
        assert_eq!(CryptoHash::hash(value), value_hash);
        assert_eq!(&value_in_db, value);
    }

    let actual_root_hash = tree.get_root_hash(version).unwrap();
    assert_eq!(actual_root_hash, expected_root_hash);
    let usage_calculated = db.calculate_usage(version);
    let usage_stored = db.get_stored_usage(version);
    assert_eq!(usage_calculated, usage_stored);
    assert_eq!(usage_stored.items(), tree.get_leaf_count(version).unwrap());
}
```
