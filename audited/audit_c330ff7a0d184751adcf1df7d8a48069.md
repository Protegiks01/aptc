# Audit Report

## Title
Permanent State Sync Failure Due to Missing Pruning Check in Persisted Auxiliary Info Iterator

## Summary
The `get_persisted_auxiliary_info_iterator` method lacks a critical pruning check that exists in other similar iterator methods, causing it to return an empty iterator when requesting pruned data. This results in a multizip iterator mismatch during state sync, causing syncing nodes to receive zero transactions and enter an infinite retry loop, permanently preventing synchronization.

## Finding Description

The vulnerability exists in the inconsistent handling of pruned data between different ledger database iterators used during state sync.

**The Core Bug:**

In `get_persisted_auxiliary_info_iter`, when data at the requested `start_version` has been pruned, the method returns an empty iterator instead of failing with a proper error: [1](#0-0) 

This behavior differs critically from other iterator methods. For example, `get_transaction_iterator` properly checks if data has been pruned: [2](#0-1) 

However, `get_persisted_auxiliary_info_iterator` omits this check entirely: [3](#0-2) 

**How State Sync Fails:**

During state sync, the storage service uses `multizip` to iterate over four data sources simultaneously: [4](#0-3) 

When the persisted auxiliary info iterator returns empty (due to pruned data) while other iterators contain data, the multizip immediately returns `None`, causing the fetch loop to terminate with zero transactions: [5](#0-4) 

**The Infinite Retry Loop:**

When state sync receives zero transactions, it creates a retry request for the same version range: [6](#0-5) 

Since `num_received_transactions = 0`, the new `start_version` remains unchanged (line 1172: `start_version + 0`), causing the exact same request to be retried. After exhausting `max_request_retry` attempts (default 5), the stream terminates and a new stream is created starting from the same synced version: [7](#0-6) 

This creates a permanent failure loop where the node repeatedly attempts to sync from the same pruned version.

## Impact Explanation

**Severity: HIGH**

This vulnerability meets the **High Severity** criteria under the Aptos Bug Bounty program for "Significant protocol violations" and potentially "Validator node slowdowns."

**Impact on Network:**
- **Node Availability**: Syncing nodes (including new validators joining the network) cannot complete synchronization if they encounter pruned auxiliary data
- **Network Health**: Reduces the number of viable archival nodes, as nodes with pruned data cannot serve sync requests to other nodes
- **Validator Onboarding**: New validators cannot join if existing nodes have pruned auxiliary data from early chain history
- **Recovery Scenarios**: Nodes recovering from failures or data loss cannot resync if the network has pruned the required data

**Scope:**
- Any full node or validator attempting to sync from genesis or from a version range that has been pruned
- Any node requesting data from a peer that has enabled ledger pruning with the `PersistedAuxiliaryInfoPruner` active

The issue does NOT cause consensus violations or fund loss, but significantly impacts network liveness and node availability.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability occurs in real-world scenarios:

1. **Pruning is a Standard Feature**: The ledger pruner, including the `PersistedAuxiliaryInfoPruner`, is part of the standard AptosDB implementation and is enabled on many nodes to manage storage: [8](#0-7) 

2. **Common Trigger**: Any new node joining the network or any node performing a full resync will request historical data. If archival nodes have pruned this data, the bug triggers immediately.

3. **No Workaround**: Once a node enters the retry loop, there is no automatic recovery mechanism. Manual intervention would require:
   - Changing the node's configuration to skip the pruned range (if possible)
   - Finding a different peer with unpruned data
   - Accepting a state snapshot from a trusted source

4. **Increases Over Time**: As the network ages and more nodes enable pruning to manage storage costs, the likelihood of encountering this bug increases.

## Recommendation

**Immediate Fix: Add Pruning Check**

Add the missing `error_if_ledger_pruned` check to `get_persisted_auxiliary_info_iterator`, consistent with other iterator methods:

```rust
fn get_persisted_auxiliary_info_iterator(
    &self,
    start_version: Version,
    num_persisted_auxiliary_info: usize,
) -> Result<Box<dyn Iterator<Item = Result<PersistedAuxiliaryInfo>> + '_>> {
    gauged_api("get_persisted_auxiliary_info_iterator", || {
        // Add this check to match other iterators
        self.error_if_ledger_pruned("PersistedAuxiliaryInfo", start_version)?;
        
        let iter = self
            .ledger_db
            .persisted_auxiliary_info_db()
            .get_persisted_auxiliary_info_iter(start_version, num_persisted_auxiliary_info)?;
        Ok(Box::new(iter)
            as Box<
                dyn Iterator<Item = Result<PersistedAuxiliaryInfo>> + '_,
            >)
    })
}
```

**Secondary Fix: Improve Iterator Logic**

Fix the logic in `get_persisted_auxiliary_info_iter` to return an error instead of an empty iterator when data is pruned but the database is not empty:

```rust
} else {
    let mut iter = self.db.iter::<PersistedAuxiliaryInfoSchema>()?;
    iter.seek_to_last();
    if iter.next().transpose()?.is_some() {
        // Data exists in DB but not at start_version - this indicates pruning
        return Err(AptosDbError::NotFound(format!(
            "PersistedAuxiliaryInfo at version {} is not available (likely pruned)",
            start_version
        )).into());
    }
    // DB is completely empty - return all Nones for backward compatibility
    start_version + num_persisted_auxiliary_info as u64
};
```

## Proof of Concept

**Reproduction Steps:**

1. **Setup Phase:**
   - Start Node A (archival node) and sync to version 10000
   - Enable ledger pruning on Node A with a small retention window
   - Allow Node A to prune persisted auxiliary info for versions 0-5000

2. **Trigger Phase:**
   - Start Node B (new syncing node) attempting to sync from genesis
   - Node B requests transactions starting from version 100 from Node A

3. **Observe Failure:**
   - Node A's storage service calls:
     - `get_transaction_iterator(100, ...)` → succeeds (has transaction data)
     - `get_persisted_auxiliary_info_iterator(100, ...)` → returns empty iterator
   - Multizip returns `None` immediately
   - Node B receives 0 transactions
   - After 5 retries (all requesting version 100), stream terminates
   - Node B creates new stream starting from version 100
   - Infinite loop begins

**Test Case (Rust):**

```rust
#[test]
fn test_persisted_auxiliary_info_pruning_sync_failure() {
    // Create a database with transactions and auxiliary info
    let db = setup_test_db_with_data(0, 1000);
    
    // Prune auxiliary info for versions 0-500
    prune_persisted_auxiliary_info(&db, 0, 501);
    
    // Attempt to get iterator for pruned version
    let result = db.get_persisted_auxiliary_info_iterator(400, 100);
    assert!(result.is_ok()); // Should fail but currently succeeds
    
    let mut iter = result.unwrap();
    let count = iter.count();
    assert_eq!(count, 0); // Returns empty iterator - BUG!
    
    // Compare with transaction iterator behavior
    let txn_result = db.get_transaction_iterator(400, 100);
    assert!(txn_result.is_err()); // Correctly fails with pruning error
}
```

**Notes:**
- The vulnerability is deterministic and reproducible
- No special privileges or timing windows are required
- The bug manifests whenever pruned auxiliary data is requested during state sync

### Citations

**File:** storage/aptosdb/src/ledger_db/persisted_auxiliary_info_db.rs (L70-73)
```rust
            let mut iter = self.db.iter::<PersistedAuxiliaryInfoSchema>()?;
            iter.seek_to_last();
            if iter.next().transpose()?.is_some() {
                return Ok(Box::new(std::iter::empty()));
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L446-461)
```rust
    fn get_persisted_auxiliary_info_iterator(
        &self,
        start_version: Version,
        num_persisted_auxiliary_info: usize,
    ) -> Result<Box<dyn Iterator<Item = Result<PersistedAuxiliaryInfo>> + '_>> {
        gauged_api("get_persisted_auxiliary_info_iterator", || {
            let iter = self
                .ledger_db
                .persisted_auxiliary_info_db()
                .get_persisted_auxiliary_info_iter(start_version, num_persisted_auxiliary_info)?;
            Ok(Box::new(iter)
                as Box<
                    dyn Iterator<Item = Result<PersistedAuxiliaryInfo>> + '_,
                >)
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L484-484)
```rust
            self.error_if_ledger_pruned("Transaction", start_version)?;
```

**File:** state-sync/storage-service/server/src/storage.rs (L396-401)
```rust
        let mut multizip_iterator = itertools::multizip((
            transaction_iterator,
            transaction_info_iterator,
            transaction_events_iterator,
            persisted_auxiliary_info_iterator,
        ));
```

**File:** state-sync/storage-service/server/src/storage.rs (L457-469)
```rust
                None => {
                    // Log a warning that the iterators did not contain all the expected data
                    warn!(
                        "The iterators for transactions, transaction infos, events and \
                        persisted auxiliary infos are missing data! Start version: {:?}, \
                        end version: {:?}, num transactions to fetch: {:?}, num fetched: {:?}.",
                        start_version,
                        end_version,
                        num_transactions_to_fetch,
                        transactions.len()
                    );
                    break;
                },
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L1168-1181)
```rust
            let num_received_transactions = transactions_with_proof.get_num_transactions() as u64;
            if num_received_transactions < num_requested_transactions {
                let start_version = request
                    .start_version
                    .checked_add(num_received_transactions)
                    .ok_or_else(|| Error::IntegerOverflow("Start version has overflown!".into()))?;
                Ok(Some(DataClientRequest::TransactionsWithProof(
                    TransactionsWithProofRequest {
                        start_version,
                        end_version: request.end_version,
                        proof_version: request.proof_version,
                        include_events: request.include_events,
                    },
                )))
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L108-109)
```rust
        let (highest_synced_version, highest_synced_epoch) =
            self.get_highest_synced_version_and_epoch()?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L143-146)
```rust
        let persisted_auxiliary_info_pruner = Box::new(PersistedAuxiliaryInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
```
