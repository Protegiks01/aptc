# Audit Report

## Title
Partial Commit Vulnerability During IoError in Parallel Database Writes Breaks Atomicity Guarantees

## Summary
During transaction commit finalization, multiple databases are written in parallel without proper cross-database transaction coordination. If an IoError occurs in one database write after others have succeeded, the system panics leaving a partially committed state that recovery mechanisms fail to detect and repair, violating the critical atomicity invariant.

## Finding Description
The vulnerability exists in the transaction commit finalization path where `calculate_and_commit_ledger_and_state_kv` spawns 7 parallel threads to write to different databases concurrently. [1](#0-0) 

Each parallel thread calls `.unwrap()` on database write operations, causing a panic on any IoError. However, these writes use RocksDB's atomic write with `sync=true`, meaning successful writes are immediately durable to disk. [2](#0-1) 

The critical flaw is that `LedgerCommitProgress` is updated as part of one of these parallel threads: [3](#0-2) 

**Attack Scenario:**
1. Transaction at version 100 enters commit phase (current version is 99)
2. Parallel thread 1: Writes events to `event_db` - **succeeds, data persisted**
3. Parallel thread 2: Writes write_sets to `write_set_db` - **succeeds, data persisted**
4. Parallel thread 3: Writes transactions to `transaction_db` - **succeeds, data persisted**
5. Parallel thread 5: Attempts to write `LedgerCommitProgress=100` but encounters **IoError** (disk full, I/O error)
6. Thread 5 panics before `LedgerCommitProgress` is updated
7. Process crashes, `OverallCommitProgress` never written (still at 99)

On restart, the recovery mechanism reads both progress markers: [4](#0-3) 

Since both `LedgerCommitProgress=99` and `OverallCommitProgress=99`, recovery assumes consistency and performs no truncation. However, `event_db`, `write_set_db`, and `transaction_db` contain version 100 data while other databases don't.

The developers acknowledge this issue: [5](#0-4) 

The recovery code attempts to handle this by truncating all ledger databases, but it relies on `LedgerCommitProgress` being updated only after all databases successfully write. This assumption is violated by the parallel write architecture. [6](#0-5) 

## Impact Explanation
**Critical Severity** - This vulnerability breaks the fundamental "State Consistency" invariant (#4 in the specification): "State transitions must be atomic and verifiable via Merkle proofs."

**Impact:**
1. **Consensus Safety Violation**: Different validators experiencing IoErrors at different points will have different database states, leading to divergent state roots and consensus failure
2. **State Corruption**: Partial commits create inconsistent relationships between transactions, events, write_sets, and state updates
3. **Non-Deterministic Execution**: Validators replaying the same blocks will compute different state roots depending on their partial commit state
4. **Merkle Proof Invalidation**: State Merkle tree becomes inconsistent with transaction data, breaking verifiability

This meets the Critical severity criteria: "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)."

## Likelihood Explanation
**High Likelihood:**
- IoErrors can occur naturally (disk full, hardware failure, filesystem issues)
- Can be triggered by resource exhaustion attacks (filling disk with spam transactions)
- Affects all validator nodes running standard configuration
- No special permissions required - any transaction sender can trigger high write volume
- Parallel writes increase the probability window for partial commits
- Production systems under high load are particularly vulnerable

## Recommendation
Implement proper two-phase commit or write-ahead logging to ensure atomicity across all databases:

**Option 1: Sequential Writes with Single Progress Marker**
- Write all databases sequentially within a single scope
- Update progress markers only after all writes succeed
- Use `?` operator instead of `.unwrap()` to propagate errors properly

**Option 2: Per-Database Progress Markers (as suggested in TODO)**
- Implement individual progress markers for each database
- Recovery truncates each database to its own progress marker
- Ensure all progress markers are written atomically

**Option 3: Single Atomic Batch**
- Combine all database writes into a single RocksDB atomic batch
- Use RocksDB's transaction API for cross-database atomicity
- Update single progress marker after entire batch commits

**Immediate Fix:**
Replace parallel writes with error propagation:
```rust
// Replace .unwrap() with ? operator and handle errors properly
// Ensure LedgerCommitProgress is written ONLY after all databases succeed
// Add per-database progress markers as suggested in TODO
```

## Proof of Concept
```rust
// Test demonstrating partial commit vulnerability
#[test]
fn test_partial_commit_on_ioerror() {
    // 1. Setup AptosDB with version 99 committed
    let db = setup_test_db();
    
    // 2. Prepare chunk with version 100 transactions
    let chunk = create_test_chunk(100, 1);
    
    // 3. Inject IoError into one of the parallel writes
    // (This would require dependency injection or fault injection framework)
    // Simulate: event_db succeeds, write_set_db succeeds, transaction_db fails
    
    // 4. Call pre_commit_ledger - should panic due to IoError
    assert_panics!(db.pre_commit_ledger(chunk, false));
    
    // 5. Verify partial state:
    // - event_db has version 100 (succeeds before panic)
    // - write_set_db has version 100 (succeeds before panic)  
    // - transaction_db does NOT have version 100 (failed with IoError)
    // - LedgerCommitProgress = 99 (never updated)
    // - OverallCommitProgress = 99 (never reached)
    
    // 6. Restart database (simulating crash recovery)
    let recovered_db = AptosDB::open(db_path, ...);
    
    // 7. Verify inconsistent state persists after recovery:
    assert!(recovered_db.event_store.has_version(100)); // TRUE - partial data!
    assert!(recovered_db.write_set_db.has_version(100)); // TRUE - partial data!
    assert!(!recovered_db.transaction_db.has_version(100)); // FALSE - missing!
    
    // This proves atomicity violation - databases are inconsistent
}
```

## Notes
This vulnerability is explicitly acknowledged in the codebase with a TODO comment but remains unimplemented. The parallel write architecture prioritizes performance over atomicity, creating a critical safety violation. Any production deployment should immediately implement per-database progress tracking or switch to sequential writes until proper cross-database transaction support is added.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L271-319)
```rust
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L360-381)
```rust
        ledger_metadata_batch
            .put::<DbMetadataSchema>(
                &DbMetadataKey::LedgerCommitProgress,
                &DbMetadataValue::Version(chunk.expect_last_version()),
            )
            .unwrap();

        let _timer =
            OTHER_TIMERS_SECONDS.timer_with(&["commit_state_kv_and_ledger_metadata___commit"]);
        rayon::scope(|s| {
            s.spawn(|_| {
                self.ledger_db
                    .metadata_db()
                    .write_schemas(ledger_metadata_batch)
                    .unwrap();
            });
            s.spawn(|_| {
                self.state_kv_db
                    .commit(chunk.expect_last_version(), None, sharded_state_kv_batches)
                    .unwrap();
            });
        });
```

**File:** storage/schemadb/src/lib.rs (L289-308)
```rust
    fn write_schemas_inner(&self, batch: impl IntoRawBatch, option: &WriteOptions) -> DbResult<()> {
        let labels = [self.name.as_str()];
        let _timer = APTOS_SCHEMADB_BATCH_COMMIT_LATENCY_SECONDS.timer_with(&labels);

        let raw_batch = batch.into_raw_batch(self)?;

        let serialized_size = raw_batch.inner.size_in_bytes();
        self.inner
            .write_opt(raw_batch.inner, option)
            .into_db_res()?;

        raw_batch.stats.commit();
        APTOS_SCHEMADB_BATCH_COMMIT_BYTES.observe_with(&[&self.name], serialized_size as f64);

        Ok(())
    }

    /// Writes a group of records wrapped in a [`SchemaBatch`].
    pub fn write_schemas(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &sync_write_option())
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-449)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L325-361)
```rust
fn truncate_ledger_db_single_batch(
    ledger_db: &LedgerDb,
    transaction_store: &TransactionStore,
    start_version: Version,
) -> Result<()> {
    let mut batch = LedgerDbSchemaBatches::new();

    delete_transaction_index_data(
        ledger_db,
        transaction_store,
        start_version,
        &mut batch.transaction_db_batches,
    )?;
    delete_per_epoch_data(
        &ledger_db.metadata_db_arc(),
        start_version,
        &mut batch.ledger_metadata_db_batches,
    )?;
    delete_per_version_data(ledger_db, start_version, &mut batch)?;

    delete_event_data(ledger_db, start_version, &mut batch.event_db_batches)?;

    truncate_transaction_accumulator(
        ledger_db.transaction_accumulator_db_raw(),
        start_version,
        &mut batch.transaction_accumulator_db_batches,
    )?;

    let mut progress_batch = SchemaBatch::new();
    progress_batch.put::<DbMetadataSchema>(
        &DbMetadataKey::LedgerCommitProgress,
        &DbMetadataValue::Version(start_version - 1),
    )?;
    ledger_db.metadata_db().write_schemas(progress_batch)?;

    ledger_db.write_schemas(batch)
}
```
