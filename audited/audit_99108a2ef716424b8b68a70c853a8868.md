# Audit Report

## Title
Synchronous Blocking Metrics Collection Can Starve Inspection Service Worker Threads and Prevent Critical Health Check Responses

## Summary
The `handle_forge_metrics()` function performs expensive synchronous metrics gathering operations directly on tokio worker threads without using `spawn_blocking`. This allows attackers to exhaust all worker threads through concurrent requests, preventing the critical `/consensus_health_check` endpoint from responding and potentially causing validator penalties.

## Finding Description

The inspection service runs on a tokio multi-threaded runtime with a default number of worker threads equal to CPU cores. [1](#0-0)  The service handles multiple endpoints including `/forge_metrics` and `/consensus_health_check`.

The `handle_forge_metrics()` function is declared as a synchronous (non-async) function [2](#0-1)  that directly calls `utils::get_all_metrics()` [3](#0-2) . This function performs CPU-intensive synchronous operations:

1. Calls `aptos_metrics_core::gather()` which locks the global metrics registry [4](#0-3) 
2. Iterates through all metric families, with warnings for families containing over 2000 dimensions [5](#0-4) 
3. Performs extensive string formatting and HashMap operations for each metric [6](#0-5) 

The codebase acknowledges this overhead with a comment stating "Take metrics of metric gathering so we know possible overhead of this process" [7](#0-6) .

When called from the async `serve_requests()` handler [8](#0-7) , this synchronous blocking operation occupies a tokio worker thread for its entire duration. An attacker can send concurrent requests equal to the number of CPU cores (e.g., 8 requests on an 8-core machine), blocking all worker threads and preventing other critical endpoints like `/consensus_health_check` [9](#0-8)  from being serviced.

The codebase demonstrates the correct pattern in the admin service, where database operations use `spawn_blocking` to avoid blocking the async runtime [10](#0-9)  via a utility wrapper [11](#0-10) .

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Availability Impact**: The `/consensus_health_check` endpoint is critical for external monitoring systems to verify validator participation in consensus. If this endpoint fails to respond due to thread starvation, monitoring systems may incorrectly mark the validator as unhealthy, potentially leading to validator removal from active sets or reputation penalties.

2. **Service Degradation**: All inspection service endpoints become unresponsive during the attack, including `/metrics`, `/configuration`, and `/peer_information`, which are used for operational monitoring and debugging.

3. **Limited Scope**: While serious, the impact is limited to the inspection service and does not directly compromise consensus safety, state integrity, or funds. The attack does not affect the core blockchain functionality, only the monitoring/debugging layer.

This aligns with Medium severity criteria: "State inconsistencies requiring intervention" (operational state monitoring failures requiring manual intervention).

## Likelihood Explanation

**Likelihood: High**

The attack is trivial to execute:
- No authentication or special privileges required
- Inspection service endpoints are typically exposed for monitoring purposes
- Attack requires only N concurrent HTTP GET requests (where N = number of CPU cores)
- Each blocked thread remains occupied for the duration of metrics gathering (potentially several seconds with thousands of metrics)
- Attack can be repeated continuously to maintain denial of service

The only mitigation factor is that production deployments may implement network-level rate limiting or firewall rules, but the vulnerability exists at the code level regardless of deployment configuration.

## Recommendation

Wrap the synchronous metrics gathering operation in `tokio::task::spawn_blocking` to prevent blocking the async runtime worker threads:

```rust
pub async fn handle_forge_metrics() -> (StatusCode, Body, String) {
    // Move metrics gathering to a blocking task
    match tokio::task::spawn_blocking(|| {
        utils::get_all_metrics()
    }).await {
        Ok(metrics) => {
            let encoded_metrics = match serde_json::to_string(&metrics) {
                Ok(encoded) => encoded,
                Err(error) => format!("Failed to get forge metrics! Error: {}", error),
            };
            (StatusCode::OK, Body::from(encoded_metrics), CONTENT_TYPE_JSON.into())
        },
        Err(error) => {
            (
                StatusCode::INTERNAL_SERVER_ERROR,
                Body::from(format!("Failed to gather metrics: {}", error)),
                CONTENT_TYPE_TEXT.into(),
            )
        }
    }
}
```

Note: The function signature must change from `fn` to `async fn` for this fix.

Additionally, apply the same fix to other metric handler functions: `handle_json_metrics_request()` and `handle_metrics_request()`, as they suffer from the same issue. Consider also fixing `handle_consensus_health_check()` which is already async but still calls the blocking operation directly.

## Proof of Concept

```rust
// File: test_inspection_service_dos.rs
// This test demonstrates how concurrent metric requests can block the service

use hyper::{Client, Uri};
use tokio::time::{timeout, Duration};

#[tokio::test]
async fn test_metrics_blocking_dos() {
    // Assuming inspection service is running on localhost:9101
    let base_url = "http://localhost:9101";
    let client = Client::new();
    
    // Get number of CPU cores (worker threads)
    let num_cores = num_cpus::get();
    println!("System has {} CPU cores", num_cores);
    
    // Spawn concurrent requests to /forge_metrics equal to worker threads
    let mut handles = vec![];
    for i in 0..num_cores {
        let client = client.clone();
        let url: Uri = format!("{}/forge_metrics", base_url).parse().unwrap();
        
        let handle = tokio::spawn(async move {
            println!("Request {} starting", i);
            let start = std::time::Instant::now();
            let result = client.get(url).await;
            println!("Request {} completed in {:?}", i, start.elapsed());
            result
        });
        handles.push(handle);
    }
    
    // While those requests are running, try to access health check
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    let health_url: Uri = format!("{}/consensus_health_check", base_url).parse().unwrap();
    let health_check = timeout(
        Duration::from_secs(2),
        client.get(health_url)
    ).await;
    
    // If worker threads are blocked, health check will timeout
    match health_check {
        Ok(Ok(_)) => println!("Health check succeeded (vulnerability NOT triggered)"),
        Ok(Err(e)) => println!("Health check failed: {}", e),
        Err(_) => println!("Health check TIMED OUT - vulnerability confirmed!"),
    }
    
    // Wait for all requests to complete
    for handle in handles {
        handle.await.unwrap().unwrap();
    }
}
```

To demonstrate:
1. Start an Aptos validator node with inspection service enabled
2. Run the PoC test with `num_cores` concurrent requests to `/forge_metrics`
3. Observe that the `/consensus_health_check` request times out or experiences significant delay
4. This demonstrates worker thread starvation preventing critical endpoint responses

## Notes

- The comparison in the security question between `handle_forge_metrics()` (synchronous) and `handle_consensus_health_check()` (async) is somewhat misleading, as both functions actually call the blocking `get_all_metrics()` operation. The real issue is that neither uses `spawn_blocking`.
- The tokio runtime configuration shows `MAX_BLOCKING_THREADS = 64` [12](#0-11) , but this limit only applies to tasks explicitly spawned with `spawn_blocking`, not to regular async tasks performing blocking operations.
- This vulnerability pattern should be audited across other async services in the codebase that may perform synchronous blocking operations without proper isolation.

### Citations

**File:** crates/aptos-inspection-service/src/server/mod.rs (L72-72)
```rust
    let runtime = aptos_runtimes::spawn_named_runtime("inspection".into(), None);
```

**File:** crates/aptos-inspection-service/src/server/mod.rs (L120-120)
```rust
            metrics::handle_consensus_health_check(&node_config).await
```

**File:** crates/aptos-inspection-service/src/server/mod.rs (L125-125)
```rust
            metrics::handle_forge_metrics()
```

**File:** crates/aptos-inspection-service/src/server/metrics.rs (L51-51)
```rust
pub fn handle_forge_metrics() -> (StatusCode, Body, String) {
```

**File:** crates/aptos-inspection-service/src/server/metrics.rs (L53-53)
```rust
    let metrics = utils::get_all_metrics();
```

**File:** crates/aptos-inspection-service/src/server/utils.rs (L51-51)
```rust
    let metric_families = aptos_metrics_core::gather();
```

**File:** crates/aptos-inspection-service/src/server/utils.rs (L55-55)
```rust
    // Take metrics of metric gathering so we know possible overhead of this process
```

**File:** crates/aptos-inspection-service/src/server/utils.rs (L58-67)
```rust
        if family_count > 2000 {
            families_over_2000 = families_over_2000.saturating_add(1);
            let name = metric_family.get_name();
            warn!(
                count = family_count,
                metric_family = name,
                "Metric Family '{}' over 2000 dimensions '{}'",
                name,
                family_count
            );
```

**File:** crates/aptos-inspection-service/src/server/utils.rs (L114-122)
```rust
        let metric_names = metric_family.get_metric().iter().map(|m| {
            let label_strings: Vec<String> = m
                .get_label()
                .iter()
                .map(|l| format!("{}={}", l.get_name(), l.get_value()))
                .collect();
            let labels_string = format!("{{{}}}", label_strings.join(","));
            format!("{}{}", metric_family.get_name(), labels_string)
        });
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L23-23)
```rust
    match spawn_blocking(move || dump_consensus_db(consensus_db.as_ref())).await {
```

**File:** crates/aptos-system-utils/src/utils.rs (L14-22)
```rust
pub async fn spawn_blocking<F, T>(func: F) -> Result<T>
where
    F: FnOnce() -> Result<T> + Send + 'static,
    T: Send + 'static,
{
    tokio::task::spawn_blocking(func)
        .await
        .map_err(Error::msg)?
}
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```
