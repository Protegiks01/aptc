# Audit Report

## Title
Missing Validation of Shard Result Consistency Enables Incomplete Block Execution and Coordinator Crashes

## Summary
The remote executor coordinator fails to validate that all shards return execution results with the same number of rounds. This allows a malicious or buggy shard to cause either (1) incomplete block execution by silently dropping transactions, or (2) coordinator crashes via index-out-of-bounds panics. The vulnerability breaks the deterministic execution invariant critical to consensus safety.

## Finding Description

The sharded block execution system partitions transactions across multiple executor shards and aggregates their results. The coordinator assumes all shards return results with identical round structure but performs **no validation** of this invariant.

**Vulnerable Code Locations:**

1. **Result Collection**: The `RemoteExecutorClient::get_output_from_shards()` method collects results from all shards without validating structural consistency. [1](#0-0) 

2. **Result Aggregation**: The `ShardedBlockExecutor::execute_block()` determines the number of rounds using only the first shard's result length, then blindly indexes into results from all shards. [2](#0-1) 

3. **Total Supply Aggregation**: The `aggregate_and_update_total_supply()` function similarly assumes all shards have the same round count and directly indexes without validation. [3](#0-2) 

**Attack Scenarios:**

**Scenario A - Incomplete Execution (Shard Returns Fewer Rounds):**
- Coordinator determines `num_rounds = 3` from shard 0
- Malicious shard 1 returns only 2 rounds
- Aggregation loop creates `ordered_results[6]` (assuming 2 shards)
- Shard 1's missing round 3 leaves `ordered_results[5]` as empty `vec![]`
- Empty vector is extended into final results, silently dropping transactions
- Different validators may produce different state roots depending on shard behavior

**Scenario B - Coordinator Crash (Shard Returns More Rounds):**
- Coordinator determines `num_rounds = 2` from shard 0  
- Malicious shard 1 returns 3 rounds
- Index calculation `round * num_executor_shards + shard_id` with round=2 exceeds bounds
- Direct indexing `ordered_results[4]` panics on vector of size 4
- Validator node crashes, causing liveness failure

**Contrast with Test Validation**: The test utilities explicitly validate this invariant, demonstrating awareness of the requirement: [4](#0-3) 

However, this validation is **absent** from production code paths.

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria:

1. **Validator Node Crashes**: Index-out-of-bounds panics crash the coordinator process, causing temporary validator unavailability. Multiple affected validators could impact network liveness.

2. **Incomplete Execution**: Silent transaction dropping violates deterministic execution. If different validators experience different shard failures, they will compute different state roots, potentially causing consensus violations.

3. **State Inconsistencies**: Missing transactions lead to incomplete state transitions that may require manual intervention to diagnose and resolve.

The vulnerability maps to "Validator node slowdowns" and "Significant protocol violations" (High Severity) and potentially "State inconsistencies requiring intervention" (Medium Severity).

## Likelihood Explanation

**Moderate to High Likelihood:**

1. **Triggering Conditions:**
   - Remote sharded execution must be enabled
   - A shard process must malfunction, crash, or be compromised
   - Network/serialization errors during result transmission

2. **Realistic Scenarios:**
   - **Software Bugs**: Race conditions or edge cases in shard execution logic could cause inconsistent round counts
   - **Resource Exhaustion**: Out-of-memory conditions might cause partial execution
   - **Network Issues**: Corrupted serialization/deserialization could produce malformed results
   - **Deliberate Attack**: Compromised shard process intentionally returns inconsistent results

3. **Detection Difficulty**: Silent transaction dropping may go unnoticed until state divergence is detected, making post-mortem analysis difficult.

The vulnerability is exploitable without requiring validator insider access - only the ability to cause a shard to malfunction (through bugs, resource exhaustion, or compromise).

## Recommendation

**Add explicit validation** that all shards return the same number of rounds before aggregation:

```rust
// In ShardedBlockExecutor::execute_block(), after line 94:
let (sharded_output, global_output) = self
    .executor_client
    .execute_block(...)?.into_inner();

// ADD VALIDATION HERE:
if !sharded_output.is_empty() {
    let expected_rounds = sharded_output[0].len();
    for (shard_id, results) in sharded_output.iter().enumerate().skip(1) {
        if results.len() != expected_rounds {
            return Err(VMStatus::Error(
                StatusCode::UNEXPECTED_ERROR_FROM_KNOWN_MOVE_FUNCTION,
                Some(format!(
                    "Shard {} returned {} rounds, expected {}",
                    shard_id, results.len(), expected_rounds
                ))
            ));
        }
    }
}
```

Apply similar validation in `aggregate_and_update_total_supply()`:

```rust
// In aggregate_and_update_total_supply(), after line 175:
let num_rounds = sharded_output[0].len();

// ADD VALIDATION:
for (shard_id, shard_output) in sharded_output.iter().enumerate().skip(1) {
    assert_eq!(
        shard_output.len(), num_rounds,
        "Shard {} has {} rounds, expected {}",
        shard_id, shard_output.len(), num_rounds
    );
}
```

## Proof of Concept

```rust
#[test]
fn test_inconsistent_shard_rounds_causes_incomplete_execution() {
    use aptos_types::transaction::TransactionOutput;
    
    // Simulate coordinator receiving inconsistent results from shards
    let num_executor_shards = 2;
    
    // Shard 0 returns 3 rounds of results
    let shard_0_results = vec![
        vec![create_dummy_txn_output()], // Round 0
        vec![create_dummy_txn_output()], // Round 1  
        vec![create_dummy_txn_output()], // Round 2
    ];
    
    // Shard 1 returns only 2 rounds (MALICIOUS/BUGGY)
    let shard_1_results = vec![
        vec![create_dummy_txn_output()], // Round 0
        vec![create_dummy_txn_output()], // Round 1
        // Round 2 is MISSING
    ];
    
    let sharded_output = vec![shard_0_results, shard_1_results];
    
    // Replicate the aggregation logic from ShardedBlockExecutor::execute_block
    let num_rounds = sharded_output[0].len(); // = 3
    let mut ordered_results = vec![vec![]; num_executor_shards * num_rounds]; // size 6
    
    for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
        for (round, result) in results_from_shard.into_iter().enumerate() {
            ordered_results[round * num_executor_shards + shard_id] = result;
        }
    }
    
    // Check the vulnerability: ordered_results[5] is empty (transaction dropped)
    assert!(ordered_results[5].is_empty(), 
        "BUG: Shard 1's round 2 transaction was silently dropped!");
    
    // In production, this empty vec gets extended into final results,
    // causing incomplete execution
    let mut aggregated_results = vec![];
    for result in ordered_results.into_iter() {
        aggregated_results.extend(result);
    }
    
    // Expected 6 transactions (3 rounds Ã— 2 shards), but only got 5
    assert_eq!(aggregated_results.len(), 5, 
        "Incomplete execution: expected 6 transactions, got {}", 
        aggregated_results.len());
}

#[test]  
#[should_panic(expected = "index out of bounds")]
fn test_excess_shard_rounds_causes_panic() {
    // Shard 0 returns 2 rounds
    let shard_0_results = vec![
        vec![create_dummy_txn_output()],
        vec![create_dummy_txn_output()],
    ];
    
    // Shard 1 returns 3 rounds (MALICIOUS/BUGGY)  
    let shard_1_results = vec![
        vec![create_dummy_txn_output()],
        vec![create_dummy_txn_output()],
        vec![create_dummy_txn_output()], // Extra round
    ];
    
    let sharded_output = vec![shard_0_results, shard_1_results];
    let num_executor_shards = 2;
    let num_rounds = sharded_output[0].len(); // = 2
    let mut ordered_results = vec![vec![]; num_executor_shards * num_rounds]; // size 4
    
    for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
        for (round, result) in results_from_shard.into_iter().enumerate() {
            // When shard_id=1, round=2: index = 2*2+1 = 5, but vector size is 4
            // PANIC: index out of bounds
            ordered_results[round * num_executor_shards + shard_id] = result;
        }
    }
}

fn create_dummy_txn_output() -> TransactionOutput {
    // Create minimal valid TransactionOutput for testing
    TransactionOutput::new(
        WriteSet::default(),
        vec![],
        0,
        TransactionStatus::Keep(ExecutionStatus::Success),
    )
}
```

## Notes

The vulnerability exists in both `RemoteExecutorClient` (network-based sharding) and `LocalExecutorClient` (thread-based sharding), as both follow the same pattern. The partitioner (`PartitionerV2`) correctly ensures all shards receive the same number of rounds during partitioning [5](#0-4) , but the executor coordinator fails to validate this invariant holds after execution.

This represents a **defense-in-depth failure**: while the partitioner provides correct inputs, the coordinator must not blindly trust that shards return structurally consistent results, especially in distributed/remote execution scenarios where network errors, serialization bugs, or compromised processes could produce malformed outputs.

### Citations

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L98-106)
```rust
        let num_rounds = sharded_output[0].len();
        let mut aggregated_results = vec![];
        let mut ordered_results = vec![vec![]; num_executor_shards * num_rounds];
        // Append the output from individual shards in the round order
        for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
            for (round, result) in results_from_shard.into_iter().enumerate() {
                ordered_results[round * num_executor_shards + shard_id] = result;
            }
        }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_aggregator_service.rs (L174-191)
```rust
    let num_shards = sharded_output.len();
    let num_rounds = sharded_output[0].len();

    // The first element is 0, which is the delta for shard 0 in round 0. +1 element will contain
    // the delta for the global shard
    let mut aggr_total_supply_delta = vec![DeltaU128::default(); num_shards * num_rounds + 1];

    // No need to parallelize this as the runtime is O(num_shards * num_rounds)
    // TODO: Get this from the individual shards while getting 'sharded_output'
    let mut aggr_ts_idx = 1;
    for round in 0..num_rounds {
        sharded_output.iter().for_each(|shard_output| {
            let mut curr_delta = DeltaU128::default();
            // Though we expect all the txn_outputs to have total_supply, there can be
            // exceptions like 'block meta' (first txn in the block) and 'chkpt info' (last txn
            // in the block) which may not have total supply. Hence we iterate till we find the
            // last txn with total supply.
            for txn in shard_output[round].iter().rev() {
```

**File:** execution/block-partitioner/src/test_utils.rs (L165-172)
```rust
    let num_rounds = output
        .sharded_txns()
        .first()
        .map(|sbs| sbs.sub_blocks.len())
        .unwrap_or(0);
    for sub_block_list in output.sharded_txns().iter().take(num_shards).skip(1) {
        assert_eq!(num_rounds, sub_block_list.sub_blocks.len());
    }
```

**File:** execution/block-partitioner/src/v2/build_edge.rs (L72-86)
```rust
        let final_num_rounds = state.sub_block_matrix.len();
        let sharded_txns = (0..state.num_executor_shards)
            .map(|shard_id| {
                let sub_blocks: Vec<SubBlock<AnalyzedTransaction>> = (0..final_num_rounds)
                    .map(|round_id| {
                        state.sub_block_matrix[round_id][shard_id]
                            .lock()
                            .unwrap()
                            .take()
                            .unwrap()
                    })
                    .collect();
                SubBlocksForShard::new(shard_id, sub_blocks)
            })
            .collect();
```
