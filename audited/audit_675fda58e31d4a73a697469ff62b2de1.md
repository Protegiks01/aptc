# Audit Report

## Title
Network-Wide Denial of Service via Zero Window Size Consensus Configuration

## Summary
A critical configuration validation vulnerability exists where setting `window_size` to `Some(0)` through on-chain governance causes all validator nodes to panic during consensus operations, resulting in total network liveness failure. The root cause is missing value validation in the Move framework's consensus configuration module.

## Finding Description

The vulnerability stems from a validation gap between the on-chain governance layer and consensus implementation:

**Root Cause Chain:**

1. The Move framework's `consensus_config.move` module only validates that configuration bytes are non-empty, not the actual `window_size` field value. [1](#0-0) 

2. The `OnChainConsensusConfig` enum in V4 and V5 variants accepts `window_size: Option<u64>` without range validation during deserialization. [2](#0-1) 

3. The `calculate_window_start_round` utility function contains an assertion that panics if `window_size` is 0. [3](#0-2) 

**Critical Failure Points:**

This function is invoked in multiple consensus-critical code paths:

- **Node Recovery Path**: During startup when recovering from persistent storage [4](#0-3) 

- **Ordered Block Window**: During normal consensus operation [5](#0-4) 

- **Window Root Finding**: With explicit validation that panics on zero [6](#0-5) 

- **Fast Forward Sync**: During block synchronization [7](#0-6) 

A test explicitly documents this panic behavior. [8](#0-7) 

**Attack Vector:**

A governance proposal updates consensus configuration with `window_size: Some(0)`. The Move module accepts this as valid since it only checks bytes are non-empty. Once applied via reconfiguration, all validators panic when executing any consensus operation involving the execution pool window.

## Impact Explanation

**Severity: Critical** - This meets the "Total loss of liveness/network availability" criteria.

**Impact Quantification:**
- **Scope:** All validator nodes network-wide
- **Effect:** Immediate panic on consensus operations after configuration application
- **Recovery:** Requires emergency hard fork or out-of-band coordination
- **Persistence:** Configuration persists on-chain across restarts

This causes a non-recoverable network halt because:
1. Configuration is stored on-chain and survives restarts
2. All validators panic simultaneously
3. No validator can participate in consensus to revert the change
4. Requires external intervention to bypass the configuration

## Likelihood Explanation

**Likelihood: Medium**

**Factors Increasing Likelihood:**
- No bounds checking in Move validation layer
- Can be triggered accidentally by governance participants attempting to "disable" execution pool (correct method is `None`, not `Some(0)`)
- Configuration appears benign at proposal level
- No runtime warnings before application

**Factors Decreasing Likelihood:**
- Requires governance proposal approval
- Default values are safe (`None` or `Some(1)`)
- Technical proposals may receive scrutiny

**Realistic Scenario:**
A governance participant proposes setting `window_size: Some(0)` believing it disables the feature. The proposal passes review without recognizing the technical implication. Upon reconfiguration, all validators enter panic state and halt.

## Recommendation

Add value validation in the Move module before accepting configuration:

```move
public fun set_for_next_epoch(account: &signer, config: vector<u8>) {
    system_addresses::assert_aptos_framework(account);
    assert!(vector::length(&config) > 0, error::invalid_argument(EINVALID_CONFIG));
    
    // Deserialize and validate window_size field
    let consensus_config: OnChainConsensusConfig = bcs::from_bytes(&config);
    if (let Some(window_size) = consensus_config.window_size()) {
        assert!(window_size > 0, error::invalid_argument(EINVALID_WINDOW_SIZE));
    }
    
    std::config_buffer::upsert<ConsensusConfig>(ConsensusConfig {config});
}
```

Alternatively, add validation in the Rust deserialization layer to reject `window_size == 0` during configuration load.

## Proof of Concept

```move
#[test]
fun test_zero_window_size_causes_network_halt() {
    use aptos_framework::consensus_config;
    use aptos_framework::aptos_governance;
    
    let framework = create_signer(@aptos_framework);
    
    // Create config with window_size = Some(0)
    let malicious_config = create_consensus_config_v5(
        /* ... other fields ... */
        window_size: Some(0u64),  // Invalid value
        /* ... other fields ... */
    );
    let config_bytes = bcs::to_bytes(&malicious_config);
    
    // This will be accepted (only checks length > 0)
    consensus_config::set_for_next_epoch(&framework, config_bytes);
    
    // Apply via reconfiguration
    aptos_governance::reconfigure(&framework);
    
    // All validators will panic on next consensus operation
    // No recovery possible without hard fork
}
```

## Notes

This vulnerability exploits a **configuration validation gap** rather than a governance mechanism exploit. The framework's statement that "governance attacks are valid if they enable consensus breaks" applies here. The issue can manifest accidentally through well-intentioned but technically incorrect proposals, representing a defense-in-depth failure where invalid configuration values should be rejected at the Move layer before reaching consensus code.

### Citations

**File:** aptos-move/framework/aptos-framework/sources/configs/consensus_config.move (L52-56)
```text
    public fun set_for_next_epoch(account: &signer, config: vector<u8>) {
        system_addresses::assert_aptos_framework(account);
        assert!(vector::length(&config) > 0, error::invalid_argument(EINVALID_CONFIG));
        std::config_buffer::upsert<ConsensusConfig>(ConsensusConfig {config});
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L199-213)
```rust
    V4 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
        // Execution pool block window
        window_size: Option<u64>,
    },
    V5 {
        alg: ConsensusAlgorithmConfig,
        vtxn: ValidatorTxnConfig,
        // Execution pool block window
        window_size: Option<u64>,
        // Whether to check if we can skip generating randomness for blocks
        rand_check_enabled: bool,
    },
}
```

**File:** consensus/src/util/mod.rs (L26-29)
```rust
pub fn calculate_window_start_round(current_round: Round, window_size: u64) -> Round {
    assert!(window_size > 0);
    (current_round + 1).saturating_sub(window_size)
}
```

**File:** consensus/src/persistent_liveness_storage.rs (L165-165)
```rust
        let window_start_round = calculate_window_start_round(commit_block.round(), window_size);
```

**File:** consensus/src/block_storage/block_tree.rs (L282-282)
```rust
        let window_start_round = calculate_window_start_round(round, window_size);
```

**File:** consensus/src/block_storage/block_tree.rs (L474-474)
```rust
            assert_ne!(window_size, 0, "Window size must be greater than 0");
```

**File:** consensus/src/block_storage/sync_manager.rs (L350-353)
```rust
                let target_round = calculate_window_start_round(
                    highest_commit_cert.ledger_info().ledger_info().round(),
                    window_size,
                )
```

**File:** consensus/src/block_storage/execution_pool/block_window_test.rs (L128-142)
```rust
#[should_panic]
#[tokio::test]
async fn test_window_root_window_size_0_failure() {
    const NUM_BLOCKS: usize = 5;
    let window_size = Some(1u64);
    let (_, block_store, pipelined_blocks) =
        create_block_tree_no_forks::<{ NUM_BLOCKS }>(NUM_BLOCKS as u64, window_size).await;

    // Genesis ──> A1 ──> ... ──> A4
    let [genesis_block, _, _, _, _] = pipelined_blocks;

    // Window size must be greater than 0, should panic
    let window_size = Some(0u64);
    block_store.find_window_root(genesis_block.id(), window_size);
}
```
