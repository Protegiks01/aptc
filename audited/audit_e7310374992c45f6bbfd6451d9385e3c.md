# Audit Report

## Title
Peer Monitoring Client Lacks Retry Backoff and Peer Disconnection on Network Errors, Enabling DoS via Transient Error Injection

## Summary
The peer-monitoring-service client does not implement exponential backoff for network error retries and fails to disconnect peers after repeated failures, allowing attackers to inject transient network errors to cause resource exhaustion on validator nodes.

## Finding Description

When the peer-monitoring-service client encounters network errors (timeouts, connection failures, etc.), it handles them incorrectly in multiple ways that enable denial-of-service attacks:

**1. Network errors are converted to a generic error type:** [1](#0-0) 

**2. Errors trigger failure counting but no peer disconnection:** [2](#0-1) 

**3. The LatencyInfoState acknowledges the need to disconnect peers after failures but doesn't implement it:** [3](#0-2) 

**4. The RequestTracker retries at a fixed interval with no exponential backoff:** [4](#0-3) [5](#0-4) 

**5. Default configuration allows 3 failures before warning, but peer is never disconnected:** [6](#0-5) 

**Attack Path:**
1. Attacker identifies target validator nodes
2. Attacker injects transient network errors (packet drops, connection resets, timeouts) at the network layer
3. Monitoring client's RPC calls fail with `RpcError::TimedOut`, `RpcError::NotConnected`, or `RpcError::IoError`
4. These are converted to `Error::NetworkError` and handled by incrementing failure counter
5. After 3 consecutive failures, only a warning is logged - the peer is NEVER disconnected
6. The monitoring client retries at the same fixed interval (30 seconds for latency pings) indefinitely
7. Target node wastes CPU, memory, and network bandwidth on unreliable peers that should have been disconnected
8. With enough unreliable peers, the node's monitoring capacity is exhausted

This breaks the **Resource Limits** invariant: all operations must respect computational limits. The monitoring client continues allocating resources to peers that consistently fail.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per the Aptos bug bounty program because it enables:

- **Validator node slowdowns**: Nodes waste resources on peers that should be disconnected, degrading monitoring performance
- **Resource exhaustion**: Without peer disconnection or backoff, monitoring slots fill with unreliable peers
- **Network health degradation**: Honest nodes cannot efficiently identify and maintain connections with reliable peers

The attack does not directly cause consensus violations or fund loss, but it degrades validator node performance by exhausting peer monitoring resources, which is classified as Medium severity.

## Likelihood Explanation

This vulnerability is **highly likely** to be exploited because:

1. **Low attacker requirements**: Only requires network-level manipulation (packet drops, connection resets), no validator access needed
2. **Easy to trigger**: Transient network errors are common in distributed systems and can be artificially induced
3. **Persistent effect**: The lack of backoff means failed requests retry indefinitely at the same interval
4. **No peer disconnection**: The TODO comment confirms the feature is unimplemented, making exploitation straightforward
5. **Default configuration is vulnerable**: `max_latency_ping_failures: 3` triggers only a warning, not disconnection

## Recommendation

Implement the following fixes:

**1. Implement exponential backoff in RequestTracker:**
```rust
pub struct RequestTracker {
    // existing fields...
    backoff_multiplier: f64,
    current_backoff_ms: u64,
    max_backoff_ms: u64,
}

impl RequestTracker {
    pub fn record_response_failure(&mut self) {
        self.num_consecutive_request_failures += 1;
        // Increase backoff exponentially
        self.current_backoff_ms = min(
            (self.current_backoff_ms as f64 * self.backoff_multiplier) as u64,
            self.max_backoff_ms
        );
    }

    pub fn record_response_success(&mut self) {
        self.last_response_time = Some(self.time_service.now());
        self.num_consecutive_request_failures = 0;
        // Reset backoff on success
        self.current_backoff_ms = self.request_interval_usec / 1000;
    }

    pub fn new_request_required(&self) -> bool {
        if self.in_flight_request() {
            return false;
        }
        match self.last_request_time {
            Some(last_request_time) => {
                let backoff_duration = if self.num_consecutive_request_failures > 0 {
                    Duration::from_millis(self.current_backoff_ms)
                } else {
                    Duration::from_micros(self.request_interval_usec)
                };
                self.time_service.now() > last_request_time.add(backoff_duration)
            },
            None => true,
        }
    }
}
```

**2. Implement peer disconnection in LatencyInfoState (complete the TODO):**
```rust
fn handle_request_failure(&mut self, peer_network_id: &PeerNetworkId, network_interface: &mut HealthCheckNetworkInterface) {
    self.request_tracker.write().record_response_failure();
    
    let num_consecutive_failures = self.request_tracker.read().get_num_consecutive_failures();
    if num_consecutive_failures >= self.latency_monitoring_config.max_latency_ping_failures {
        warn!(LogSchema::new(LogEntry::LatencyPing)
            .event(LogEvent::TooManyPingFailures)
            .peer(peer_network_id)
            .message("Too many ping failures occurred for the peer! Disconnecting."));
        
        // Actually disconnect the peer
        let _ = network_interface.disconnect_peer(
            *peer_network_id,
            DisconnectReason::TooManyFailures
        ).await;
    }
}
```

**3. Distinguish between transient and persistent errors:**
Handle `RpcError::TimedOut` with more lenient backoff than `RpcError::NotConnected` or `RpcError::IoError`.

## Proof of Concept

```rust
// Test demonstrating the vulnerability
#[tokio::test]
async fn test_no_backoff_on_repeated_failures() {
    let config = LatencyMonitoringConfig::default();
    let time_service = TimeService::mock();
    let mut tracker = RequestTracker::new(config.latency_ping_interval_ms, time_service.clone());
    
    let mock_time = time_service.into_mock();
    
    // Simulate repeated failures
    for i in 0..10 {
        // Request should be required at fixed interval
        assert!(tracker.new_request_required());
        
        tracker.request_started();
        tracker.request_completed();
        tracker.record_response_failure(); // Network error
        
        // Advance by fixed interval - NO BACKOFF
        mock_time.advance(Duration::from_millis(config.latency_ping_interval_ms));
        
        // Verify failures accumulate
        assert_eq!(tracker.get_num_consecutive_failures(), i + 1);
    }
    
    // After 10 failures, still retrying at same interval with no backoff
    // Peer was never disconnected despite max_latency_ping_failures = 3
    assert_eq!(tracker.get_num_consecutive_failures(), 10);
}
```

## Notes

The TODO comment at line 64 in `latency_info.rs` explicitly acknowledges this issue: "TODO: If the number of ping failures is too high, disconnect from the node". This confirms the vulnerability is a known but unimplemented security feature. Similar issues exist in `NetworkInfoState` and `NodeInfoState` which also lack peer disconnection logic. [7](#0-6) [8](#0-7)

### Citations

**File:** peer-monitoring-service/client/src/error.rs (L37-40)
```rust
impl From<aptos_network::application::error::Error> for Error {
    fn from(error: aptos_network::application::error::Error) -> Self {
        Error::NetworkError(error.to_string())
    }
```

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L124-131)
```rust
            let monitoring_service_response = match monitoring_service_response {
                Ok(monitoring_service_response) => monitoring_service_response,
                Err(error) => {
                    peer_state_value
                        .write()
                        .handle_monitoring_service_response_error(&peer_network_id, error);
                    return;
                },
```

**File:** peer-monitoring-service/client/src/peer_states/latency_info.rs (L59-72)
```rust
    /// Handles a ping failure for the specified peer
    fn handle_request_failure(&self, peer_network_id: &PeerNetworkId) {
        // Update the number of ping failures for the request tracker
        self.request_tracker.write().record_response_failure();

        // TODO: If the number of ping failures is too high, disconnect from the node
        let num_consecutive_failures = self.request_tracker.read().get_num_consecutive_failures();
        if num_consecutive_failures >= self.latency_monitoring_config.max_latency_ping_failures {
            warn!(LogSchema::new(LogEntry::LatencyPing)
                .event(LogEvent::TooManyPingFailures)
                .peer(peer_network_id)
                .message("Too many ping failures occurred for the peer!"));
        }
    }
```

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L74-90)
```rust
    /// Returns true iff a new request should be sent (based
    /// on the latest response time).
    pub fn new_request_required(&self) -> bool {
        // There's already an in-flight request. A new one should not be sent.
        if self.in_flight_request() {
            return false;
        }

        // Otherwise, check the last request time for freshness
        match self.last_request_time {
            Some(last_request_time) => {
                self.time_service.now()
                    > last_request_time.add(Duration::from_micros(self.request_interval_usec))
            },
            None => true, // A request should be sent immediately
        }
    }
```

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L101-104)
```rust
    /// Records a failure for the request
    pub fn record_response_failure(&mut self) {
        self.num_consecutive_request_failures += 1;
    }
```

**File:** config/src/config/peer_monitoring_config.rs (L47-55)
```rust
impl Default for LatencyMonitoringConfig {
    fn default() -> Self {
        Self {
            latency_ping_interval_ms: 30_000, // 30 seconds
            latency_ping_timeout_ms: 20_000,  // 20 seconds
            max_latency_ping_failures: 3,
            max_num_latency_pings_to_retain: 10,
        }
    }
```

**File:** peer-monitoring-service/client/src/peer_states/network_info.rs (L160-174)
```rust
    fn handle_monitoring_service_response_error(
        &mut self,
        peer_network_id: &PeerNetworkId,
        error: Error,
    ) {
        // Handle the failure
        self.handle_request_failure();

        // Log the error
        warn!(LogSchema::new(LogEntry::NetworkInfoRequest)
            .event(LogEvent::ResponseError)
            .message("Error encountered when requesting network information from the peer!")
            .peer(peer_network_id)
            .error(&error));
    }
```

**File:** peer-monitoring-service/client/src/peer_states/node_info.rs (L108-122)
```rust
    fn handle_monitoring_service_response_error(
        &mut self,
        peer_network_id: &PeerNetworkId,
        error: Error,
    ) {
        // Handle the failure
        self.handle_request_failure();

        // Log the error
        warn!(LogSchema::new(LogEntry::NodeInfoRequest)
            .event(LogEvent::ResponseError)
            .message("Error encountered when requesting node information from the peer!")
            .peer(peer_network_id)
            .error(&error));
    }
```
