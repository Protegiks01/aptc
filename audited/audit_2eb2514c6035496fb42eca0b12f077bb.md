# Audit Report

## Title
Race Condition in Concurrent Pruner Progress Metadata Updates

## Summary
Multiple threads can concurrently update the same pruner progress metadata keys (including `EventPrunerProgress`) without synchronization, causing lost updates and inconsistent pruner state. This occurs when `finalize_state_snapshot` executes while the `PrunerWorker` thread is actively pruning via parallel sub-pruner execution.

## Finding Description

The vulnerability exists in the concurrent execution of two code paths that both write to pruner progress metadata:

**Path 1: Sub-pruner execution from PrunerWorker thread**
The `EventStorePruner` updates its progress during pruning operations. [1](#0-0) 

This pruner is invoked as part of parallel sub-pruner execution via rayon's `par_iter()`: [2](#0-1) 

The pruner runs in a dedicated worker thread: [3](#0-2) 

**Path 2: Finalize state snapshot execution**
During fast sync completion, `finalize_state_snapshot` calls `save_min_readable_version` on all pruner managers: [4](#0-3) 

This triggers writes to ALL sub-pruner progress keys, including `EventPrunerProgress`: [5](#0-4) 

Which calls: [6](#0-5) 

Specifically writing to `EventPrunerProgress`: [7](#0-6) 

**The Race Condition:**
Both paths write to `DbMetadataKey::EventPrunerProgress` (and other sub-pruner progress keys) without any synchronization mechanism. The writes use direct RocksDB `put()` operations that provide no compare-and-swap semantics.

In non-sharding mode, all sub-databases share the same underlying RocksDB instance: [8](#0-7) 

**Scenario 1 - Lost Update (Progress Regresses):**
1. PrunerWorker thread: EventStorePruner completes pruning to version 200, writes `EventPrunerProgress = 200`
2. State sync thread: `finalize_state_snapshot` at version 150 writes `EventPrunerProgress = 150`
3. Result: Progress incorrectly regresses from 200 to 150

**Scenario 2 - Lost Update (Progress Skips):**
1. State sync thread: `finalize_state_snapshot` at version 200 writes `EventPrunerProgress = 200`
2. PrunerWorker thread: EventStorePruner completes pruning to version 150, writes `EventPrunerProgress = 150`
3. Result: Progress incorrectly regresses, or on next read, pruner believes it's at 150 when snapshot assumes 200

This violates the **State Consistency** invariant that "state transitions must be atomic and verifiable."

## Impact Explanation

**Severity: Medium (potentially High)**

This issue causes **state inconsistencies requiring intervention**, fitting the Medium severity category. Consequences include:

1. **Data Corruption Risk**: If pruner progress regresses, the pruner may attempt to re-prune already-pruned data, causing errors or database corruption
2. **Storage Bloat**: If progress advances incorrectly, the pruner may skip versions that should be pruned, violating pruning invariants
3. **Inconsistent State**: Different sub-pruners may have inconsistent progress states if their updates interleave with `write_pruner_progress` calls
4. **Node Operational Issues**: Validator or fullnode reliability could be compromised due to pruning errors

While not directly causing consensus violations or fund loss, this creates storage system instability that could cascade into more severe issues.

## Likelihood Explanation

**Likelihood: Medium-High**

This race condition can occur during normal node operation:
- **Trigger**: Any time `finalize_state_snapshot` is called while `PrunerWorker` is actively pruning
- **Frequency**: Happens during fast sync completion on bootstrapping nodes or nodes catching up after downtime
- **Concurrency Window**: The race window spans the entire duration of parallel sub-pruner execution (potentially seconds) overlapping with `write_pruner_progress` sequential writes
- **No Special Conditions**: Requires no attacker interaction, happens naturally in production

The race is more likely in:
- Non-sharding mode where all pruners share one RocksDB instance
- Nodes frequently performing fast sync
- Systems with high pruning activity

## Recommendation

**Solution: Add synchronization between pruner operations and snapshot finalization**

**Option 1: Use a read-write lock to coordinate pruner and snapshot operations**

Add a `RwLock` in `AptosDB` that:
- Pruner operations acquire read lock (allows concurrent sub-pruners)
- `finalize_state_snapshot` and `write_pruner_progress` acquire write lock (exclusive access)

```rust
// In AptosDB struct
pruner_coordination_lock: Arc<RwLock<()>>,

// In LedgerPruner::prune()
let _guard = self.coordination_lock.read();
self.sub_pruners.par_iter().try_for_each(...)

// In LedgerDb::write_pruner_progress()
let _guard = self.coordination_lock.write();
self.event_db.write_pruner_progress(version)?;
// ... other writes
```

**Option 2: Use atomic compare-and-swap for progress updates**

Modify progress tracking to only accept monotonically increasing versions:

```rust
// In event_db.rs write_pruner_progress()
pub(super) fn write_pruner_progress(&self, version: Version) -> Result<()> {
    let current = self.db.get::<DbMetadataSchema>(&DbMetadataKey::EventPrunerProgress)?
        .map(|v| v.expect_version())
        .unwrap_or(0);
    
    if version > current {
        self.db.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(version),
        )
    } else {
        Ok(()) // Don't regress progress
    }
}
```

**Option 3: Sequence progress updates through a single coordinator thread**

This ensures total ordering of all pruner progress updates, eliminating races.

## Proof of Concept

```rust
// Concurrent test demonstrating the race condition
#[tokio::test]
async fn test_concurrent_pruner_progress_race() {
    let db = setup_test_db();
    let ledger_pruner = LedgerPruner::new(db.ledger_db.clone(), None).unwrap();
    
    // Simulate pruner worker thread
    let pruner_handle = tokio::spawn({
        let ledger_pruner = ledger_pruner.clone();
        async move {
            // Prune to version 200
            ledger_pruner.prune(1000).unwrap();
        }
    });
    
    // Simulate state sync thread calling finalize_state_snapshot
    let snapshot_handle = tokio::spawn({
        let db = db.clone();
        async move {
            tokio::time::sleep(Duration::from_millis(50)).await;
            // Write pruner progress to version 150 (simulating snapshot finalize)
            db.ledger_db.write_pruner_progress(150).unwrap();
        }
    });
    
    pruner_handle.await.unwrap();
    snapshot_handle.await.unwrap();
    
    // Check final progress - may be 150 or 200 depending on race outcome
    let progress = db.ledger_db.event_db()
        .db()
        .get::<DbMetadataSchema>(&DbMetadataKey::EventPrunerProgress)
        .unwrap()
        .unwrap()
        .expect_version();
    
    // Race condition: progress could be either value, not deterministic
    assert!(progress == 150 || progress == 200);
    // Expected: progress should be max(150, 200) = 200 with proper synchronization
}
```

## Notes

This vulnerability affects all sub-pruner progress metadata keys, not just `EventPrunerProgress`. The race exists for:
- `EventPrunerProgress`
- `TransactionPrunerProgress`
- `TransactionInfoPrunerProgress`
- `TransactionAccumulatorPrunerProgress`
- `WriteSetPrunerProgress`
- `PersistedAuxiliaryInfoPrunerProgress`
- `TransactionAuxiliaryDataPrunerProgress`

The issue is more pronounced in non-sharding mode where all databases share a single RocksDB instance, but can occur in sharding mode as well since the metadata schema is still shared.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L66-69)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-68)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L225-234)
```rust
            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L80-89)
```rust
    fn save_min_readable_version(&self, min_readable_version: Version) -> Result<()> {
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.ledger_db.write_pruner_progress(min_readable_version)
    }
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L150-171)
```rust
        if !sharding {
            info!("Individual ledger dbs are not enabled!");
            return Ok(Self {
                ledger_metadata_db: LedgerMetadataDb::new(Arc::clone(&ledger_metadata_db)),
                event_db: EventDb::new(
                    Arc::clone(&ledger_metadata_db),
                    EventStore::new(Arc::clone(&ledger_metadata_db)),
                ),
                persisted_auxiliary_info_db: PersistedAuxiliaryInfoDb::new(Arc::clone(
                    &ledger_metadata_db,
                )),
                transaction_accumulator_db: TransactionAccumulatorDb::new(Arc::clone(
                    &ledger_metadata_db,
                )),
                transaction_auxiliary_data_db: TransactionAuxiliaryDataDb::new(Arc::clone(
                    &ledger_metadata_db,
                )),
                transaction_db: TransactionDb::new(Arc::clone(&ledger_metadata_db)),
                transaction_info_db: TransactionInfoDb::new(Arc::clone(&ledger_metadata_db)),
                write_set_db: WriteSetDb::new(Arc::clone(&ledger_metadata_db)),
                enable_storage_sharding: false,
            });
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L373-388)
```rust
    pub(crate) fn write_pruner_progress(&self, version: Version) -> Result<()> {
        info!("Fast sync is done, writing pruner progress {version} for all ledger sub pruners.");
        self.event_db.write_pruner_progress(version)?;
        self.persisted_auxiliary_info_db
            .write_pruner_progress(version)?;
        self.transaction_accumulator_db
            .write_pruner_progress(version)?;
        self.transaction_auxiliary_data_db
            .write_pruner_progress(version)?;
        self.transaction_db.write_pruner_progress(version)?;
        self.transaction_info_db.write_pruner_progress(version)?;
        self.write_set_db.write_pruner_progress(version)?;
        self.ledger_metadata_db.write_pruner_progress(version)?;

        Ok(())
    }
```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L47-52)
```rust
    pub(super) fn write_pruner_progress(&self, version: Version) -> Result<()> {
        self.db.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(version),
        )
    }
```
