# Audit Report

## Title
Mempool Peer Health Check Bypass via Self-Reported Ledger Timestamp Manipulation

## Summary
The mempool's peer health check mechanism relies on self-reported `ledger_timestamp_usecs` values from peers without any validation. Malicious or lagging peers can report fake timestamps to appear synchronized, bypassing health checks and gaining priority in transaction broadcast routing, leading to network-wide transaction propagation delays.

## Finding Description

The vulnerability exists in the peer monitoring service's health check mechanism used by mempool for peer prioritization. The attack flow is:

**Step 1: Self-Reported Timestamp Generation** [1](#0-0) 

The server reads `ledger_timestamp_usecs` directly from its own storage and includes it in the `NodeInformationResponse` without any proof or verification mechanism. This value is entirely self-reported.

**Step 2: Unvalidated Storage** [2](#0-1) 

The client accepts and stores the peer's self-reported timestamp without any validation, sanity checks, or cross-verification against other data sources.

**Step 3: Health Check Based on Self-Reported Data** [3](#0-2) 

The `check_peer_metadata_health()` function uses the self-reported `ledger_timestamp_usecs` to determine peer health by checking if it's within 30 seconds (by default) of the current time. A peer reporting `ledger_timestamp_usecs = current_time` will always pass this check, regardless of its actual sync state.

**Step 4: Peer Prioritization Favors "Healthy" Peers** [4](#0-3) 

The `compare_peer_health()` function is used during intelligent peer prioritization, where healthy peers are given higher priority than unhealthy ones.

**Step 5: Primary vs Failover Broadcast Priority Assignment** [5](#0-4) 

Top-priority peers (including those passing health checks) are assigned sender buckets with Primary broadcast priority, while lower-priority peers get Failover priority.

**Step 6: Differential Transaction Delivery** [6](#0-5) 

Primary peers receive transactions immediately (`before = None`), while Failover peers only receive transactions older than 500ms. This means a malicious peer with Primary priority becomes the main transaction propagation path.

**Attack Scenario:**

1. Attacker runs a modified Aptos node that always reports `ledger_timestamp_usecs = current_time_in_usecs()` regardless of actual sync state
2. The attacker's node could be hours or days behind the chain, or completely stalled
3. Honest nodes query the attacker via peer monitoring service
4. Attacker responds with fake timestamp, passing health checks
5. During peer prioritization (every 10 minutes by default per configuration), attacker's node is marked as "healthy"
6. Attacker receives Primary priority for sender bucket assignments
7. Attacker receives all transactions immediately for assigned buckets
8. If attacker drops/delays forwarding transactions, this causes network degradation
9. Multiple honest nodes may prioritize the same attacker, amplifying the impact

**Configuration Context:** [7](#0-6) 

The default configuration enables intelligent peer prioritization with a 30-second sync lag tolerance and 10-minute peer priority update intervals, making this vulnerability exploitable for extended periods.

## Impact Explanation

**Severity: High** (up to $50,000 per Aptos Bug Bounty)

This vulnerability meets the **High Severity** criteria:

1. **Validator node slowdowns**: If validators prioritize malicious peers that don't forward transactions properly, transaction propagation to validators is delayed, slowing block production and consensus.

2. **Significant protocol violations**: The peer prioritization mechanism is designed to ensure optimal transaction propagation by routing transactions through healthy, well-connected peers. This vulnerability allows malicious actors to subvert this mechanism entirely.

**Specific Impacts:**

- **Transaction Propagation Delays**: Transactions routed through malicious Primary peers experience delays proportional to the attacker's delay/drop behavior
- **Network-Wide Degradation**: If multiple nodes independently prioritize the same malicious peer(s), the effect compounds across the network
- **Consensus Performance Impact**: Reduced transaction availability to validators can slow block production and increase confirmation times
- **Mempool Inefficiency**: Failover mechanisms (500ms delay) activate only after Primary peers fail, adding latency
- **Persistent Effect**: Once prioritized, a malicious peer maintains Primary status for up to 10 minutes (default `shared_mempool_priority_update_interval_secs`)

The vulnerability does **not** directly cause:
- Fund theft or loss (not Critical severity)
- Consensus safety violations (not Critical severity)
- Complete network partition (not Critical severity)

However, it enables targeted **denial of service** against transaction propagation, which qualifies as validator node slowdowns and significant protocol violations.

## Likelihood Explanation

**Likelihood: High**

The attack is:

1. **Trivial to Execute**: Requires only modifying the peer monitoring service response to report `ledger_timestamp_usecs = current_time`. No complex exploit chain or timing requirements.

2. **No Special Privileges Required**: Any network peer can participate in peer monitoring exchanges. No validator status or staking required.

3. **Low Detection Risk**: The fake timestamp appears legitimate since there's no cross-validation. Honest nodes have no way to verify the reported timestamp against actual chain state without additional mechanisms.

4. **High Impact Probability**: The default configuration (`enable_intelligent_peer_prioritization: true`) means most nodes are vulnerable.

5. **Sustained Effect**: Once achieving Primary priority, the attacker maintains it for the full peer priority update interval (10 minutes by default).

**Attacker Requirements:**
- Ability to run a modified Aptos node
- Network connectivity to target nodes
- No stake, validator status, or other credentials required

## Recommendation

Implement **cryptographic verification** of peer sync state instead of trusting self-reported timestamps. Several approaches:

### Option 1: Verify Ledger State with Block Headers (Recommended)

Modify the peer monitoring service to include recent block headers or state proof with the node information response, and verify these against the local chain state:

```rust
// In NodeInformationResponse, add:
pub struct NodeInformationResponse {
    // ... existing fields ...
    pub ledger_timestamp_usecs: u64,
    pub recent_block_info: Option<BlockInfo>, // NEW: includes height, timestamp, state root
    pub block_proof: Option<LedgerInfoWithSignatures>, // NEW: QC proving the block
}

// In check_peer_metadata_health, verify the proof:
fn check_peer_metadata_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
    local_ledger_info: &LedgerInfo, // NEW: local chain state
) -> bool {
    monitoring_metadata
        .and_then(|metadata| {
            metadata.latest_node_info_response.as_ref().and_then(|response| {
                // NEW: Verify the block proof before trusting the timestamp
                if let (Some(block_info), Some(proof)) = (&response.recent_block_info, &response.block_proof) {
                    // Verify the proof is valid and matches the reported block
                    if !verify_block_proof(block_info, proof, local_ledger_info) {
                        return None; // Reject if proof is invalid
                    }
                    
                    // Now use the VERIFIED timestamp from the proven block
                    let peer_ledger_timestamp_usecs = block_info.timestamp_usecs;
                    let current_timestamp_usecs = get_timestamp_now_usecs(time_service);
                    let max_sync_lag_usecs = mempool_config.max_sync_lag_before_unhealthy_secs as u64 * MICROS_PER_SECOND;
                    
                    Some(current_timestamp_usecs.saturating_sub(peer_ledger_timestamp_usecs) < max_sync_lag_usecs)
                } else {
                    None // No proof provided, treat as unhealthy
                }
            })
        })
        .unwrap_or(false)
}
```

### Option 2: Cross-Validate with Consensus Data

For validators, use consensus messages (votes, block proposals) as implicit proof of sync state, since these are already cryptographically signed and verified through the consensus protocol.

### Option 3: State Sync Service Integration

Use the existing state sync service's optimistic fetch mechanism to verify peer sync state by requesting specific version data and checking consistency.

**Additional Hardening:**
1. Add rate limiting on peer monitoring responses to prevent rapid switching of reported timestamps
2. Track peer timestamp consistency over time and penalize peers with erratic reporting
3. Implement a reputation system that gradually increases trust in consistently accurate peers
4. Add configuration option to disable intelligent peer prioritization if verification overhead is prohibitive

## Proof of Concept

This PoC demonstrates the vulnerability by simulating a malicious peer that reports false sync state:

```rust
// File: mempool/src/shared_mempool/priority_exploit_test.rs
#[cfg(test)]
mod exploit_test {
    use super::*;
    use aptos_config::config::MempoolConfig;
    use aptos_peer_monitoring_service_types::{
        response::NodeInformationResponse,
        PeerMonitoringMetadata,
    };
    use aptos_time_service::TimeService;
    use std::time::Duration;

    /// Demonstrates that a peer can bypass health checks by reporting fake timestamps
    #[test]
    fn test_health_check_bypass_exploit() {
        let mempool_config = MempoolConfig {
            max_sync_lag_before_unhealthy_secs: 30,
            ..MempoolConfig::default()
        };
        let time_service = TimeService::mock();
        
        // Simulate that current time is T = 1000 seconds
        time_service.clone().into_mock().advance_secs(1000);
        let current_time_usecs = time_service.now_unix_time().as_micros() as u64;
        
        // SCENARIO 1: Honest peer that is actually lagging by 100 seconds (should fail health check)
        let lagging_peer_actual_timestamp = current_time_usecs - (100 * 1_000_000); // 100 seconds behind
        let honest_lagging_node_info = NodeInformationResponse {
            ledger_timestamp_usecs: lagging_peer_actual_timestamp,
            ..Default::default()
        };
        let honest_lagging_metadata = PeerMonitoringMetadata {
            latest_node_info_response: Some(honest_lagging_node_info),
            ..Default::default()
        };
        
        // Verify honest lagging peer fails health check (as expected)
        let is_healthy = check_peer_metadata_health(
            &mempool_config,
            &time_service,
            &Some(&honest_lagging_metadata),
        );
        assert!(!is_healthy, "Honest lagging peer should fail health check");
        
        // SCENARIO 2: Malicious peer that is ALSO lagging by 100 seconds but LIES about it
        // The malicious peer reports current_time instead of its actual ledger timestamp
        let malicious_node_info = NodeInformationResponse {
            ledger_timestamp_usecs: current_time_usecs, // LYING: reports current time despite being 100s behind
            ..Default::default()
        };
        let malicious_metadata = PeerMonitoringMetadata {
            latest_node_info_response: Some(malicious_node_info),
            ..Default::default()
        };
        
        // EXPLOIT: Malicious peer PASSES health check despite being equally lagged
        let is_healthy = check_peer_metadata_health(
            &mempool_config,
            &time_service,
            &Some(&malicious_metadata),
        );
        assert!(is_healthy, "VULNERABILITY: Malicious peer bypasses health check by lying about timestamp");
        
        // SCENARIO 3: Demonstrate prioritization impact
        // When comparing peers, the malicious peer is prioritized over the honest one
        let ordering = compare_peer_health(
            &mempool_config,
            &time_service,
            &Some(&malicious_metadata),
            &Some(&honest_lagging_metadata),
        );
        assert_eq!(
            ordering,
            std::cmp::Ordering::Greater,
            "VULNERABILITY: Malicious peer is prioritized higher than honest peer despite both being equally lagged"
        );
    }
    
    /// Demonstrates the sustained impact: malicious peer maintains priority for extended period
    #[test]
    fn test_sustained_priority_exploit() {
        let mempool_config = MempoolConfig {
            max_sync_lag_before_unhealthy_secs: 30,
            shared_mempool_priority_update_interval_secs: 600, // 10 minutes
            enable_intelligent_peer_prioritization: true,
            ..MempoolConfig::default()
        };
        let time_service = TimeService::mock();
        let mut prioritized_peers_state = PrioritizedPeersState::new(
            mempool_config.clone(),
            NodeType::PublicFullnode,
            time_service.clone(),
        );
        
        time_service.clone().into_mock().advance_secs(1000);
        let current_time_usecs = time_service.now_unix_time().as_micros() as u64;
        
        // Create malicious peer with fake timestamp
        let malicious_peer = PeerNetworkId::random();
        let malicious_node_info = NodeInformationResponse {
            ledger_timestamp_usecs: current_time_usecs, // Fake: claims to be synced
            ..Default::default()
        };
        let malicious_metadata = PeerMonitoringMetadata {
            latest_node_info_response: Some(malicious_node_info),
            average_ping_latency_secs: Some(0.01), // Low latency to get high priority
            ..Default::default()
        };
        
        // Update peer priorities
        let peers = vec![(malicious_peer, Some(&malicious_metadata))];
        prioritized_peers_state.update_prioritized_peers(peers.clone(), 1000, 1000);
        
        // Verify malicious peer gets top priority (index 0)
        let priority = prioritized_peers_state.get_peer_priority(&malicious_peer);
        assert_eq!(priority, 0, "Malicious peer should get top priority");
        
        // Verify malicious peer is assigned Primary broadcast priority
        let sender_bucket = 0;
        let broadcast_priority = prioritized_peers_state
            .get_sender_bucket_priority_for_peer(&malicious_peer, sender_bucket);
        assert_eq!(
            broadcast_priority,
            Some(BroadcastPeerPriority::Primary),
            "VULNERABILITY: Malicious peer gets Primary broadcast priority, receiving all transactions immediately"
        );
        
        // Even after 5 minutes pass, malicious peer maintains priority
        // (because peer priority update interval is 10 minutes)
        time_service.clone().into_mock().advance_secs(300); // +5 minutes
        assert!(!prioritized_peers_state.ready_for_update(false),
            "Peer priorities not updated yet, malicious peer maintains Primary status for extended period");
    }
}
```

**To reproduce:**
1. Add the test file to `mempool/src/shared_mempool/`
2. Import the test module in `mempool/src/shared_mempool/priority.rs`
3. Run: `cargo test test_health_check_bypass_exploit -p aptos-mempool`
4. Observe that malicious peer bypasses health check and gains Primary priority

The PoC confirms that self-reported timestamps are trusted without validation, allowing trivial bypasses of the health check mechanism.

---

**Notes:**

This vulnerability violates the implicit trust assumption that peer-reported metadata accurately reflects their actual state. The mempool's intelligent peer prioritization feature, while designed to optimize transaction propagation, becomes an attack vector when peers can falsify health indicators. The fix requires shifting from a trust-based to a verify-based model using cryptographic proofs of sync state rather than self-reported timestamps.

### Citations

**File:** peer-monitoring-service/server/src/lib.rs (L259-280)
```rust
    fn get_node_information(&self) -> Result<PeerMonitoringServiceResponse, Error> {
        // Get the node information
        let build_information = aptos_build_info::get_build_information();
        let current_time: Instant = self.time_service.now();
        let uptime = current_time.duration_since(self.start_time);
        let (highest_synced_epoch, highest_synced_version) =
            self.storage.get_highest_synced_epoch_and_version()?;
        let ledger_timestamp_usecs = self.storage.get_ledger_timestamp_usecs()?;
        let lowest_available_version = self.storage.get_lowest_available_version()?;

        // Create and return the response
        let node_information_response = NodeInformationResponse {
            build_information,
            highest_synced_epoch,
            highest_synced_version,
            ledger_timestamp_usecs,
            lowest_available_version,
            uptime,
        };
        Ok(PeerMonitoringServiceResponse::NodeInformation(
            node_information_response,
        ))
```

**File:** peer-monitoring-service/client/src/peer_states/node_info.rs (L46-53)
```rust
    /// Records the new node info response for the peer
    pub fn record_node_info_response(&mut self, node_info_response: NodeInformationResponse) {
        // Update the request tracker with a successful response
        self.request_tracker.write().record_response_success();

        // Save the node info
        self.recorded_node_info_response = Some(node_info_response);
    }
```

**File:** mempool/src/shared_mempool/priority.rs (L272-432)
```rust
    fn update_sender_bucket_for_peers(
        &mut self,
        peer_monitoring_data: &HashMap<PeerNetworkId, Option<&PeerMonitoringMetadata>>,
        num_mempool_txns_received_since_peers_updated: u64,
        num_committed_txns_received_since_peers_updated: u64,
    ) {
        // TODO: If the top peer set didn't change, then don't change the Primary sender bucket assignment.
        // TODO: (Minor) If the load is low, don't do load balancing for Failover buckets.
        assert!(self.prioritized_peers.read().len() == peer_monitoring_data.len());

        // Obtain the top peers to assign the sender buckets with Primary priority
        let mut top_peers = vec![];
        let secs_elapsed_since_last_update =
            self.last_peer_priority_update.map_or(0, |last_update| {
                self.time_service
                    .now()
                    .duration_since(last_update)
                    .as_secs()
            });

        // When the node is in state sync mode, it will receive more mempool commit notifications than the actual
        // commits that happens on the blockchain during the same time period. As secs_elapsed_since_last_update is
        // local time and not the on chain time, the average_committed_traffic_observed is only a local estimate of
        // the traffic and could differ from the actual traffic observed by the blockchain. If the estimate differs
        // from the actual traffic observed on the blockchain, we could end up load balancing more or less than required.
        let average_mempool_traffic_observed = num_mempool_txns_received_since_peers_updated as f64
            / max(1, secs_elapsed_since_last_update) as f64;
        let average_committed_traffic_observed = num_committed_txns_received_since_peers_updated
            as f64
            / max(1, secs_elapsed_since_last_update) as f64;

        // Obtain the highest threshold from mempool_config.load_balancing_thresholds for which avg_mempool_traffic_threshold_in_tps exceeds average_mempool_traffic_observed
        let threshold_config = self
            .mempool_config
            .load_balancing_thresholds
            .clone()
            .into_iter()
            .rev()
            .find(|threshold_config| {
                threshold_config.avg_mempool_traffic_threshold_in_tps
                    <= max(
                        average_mempool_traffic_observed as u64,
                        average_committed_traffic_observed as u64,
                    )
            })
            .unwrap_or_default();

        let num_top_peers = max(
            1,
            min(
                self.mempool_config.num_sender_buckets,
                if self.mempool_config.enable_max_load_balancing_at_any_load {
                    u8::MAX
                } else {
                    threshold_config.max_number_of_upstream_peers
                },
            ),
        );
        info!(
            "Time elapsed since last peer update: {:?}\n
            Number of mempool transactions received since last peer update: {:?},\n
            Average mempool traffic observed: {:?},\n
            Number of committed transactions received since last peer update: {:?},\n
            Average committed traffic observed: {:?},\n
            Load balancing threshold config: {:?},\n
            Number of top peers picked: {:?}",
            secs_elapsed_since_last_update,
            num_mempool_txns_received_since_peers_updated,
            average_mempool_traffic_observed,
            num_committed_txns_received_since_peers_updated,
            average_committed_traffic_observed,
            threshold_config,
            num_top_peers
        );

        if self.node_type.is_validator_fullnode() {
            // Use the peer on the VFN network with lowest ping latency as the primary peer
            let peers_in_vfn_network = self
                .prioritized_peers
                .read()
                .iter()
                .cloned()
                .filter(|peer| peer.network_id() == NetworkId::Vfn)
                .collect::<Vec<_>>();

            if !peers_in_vfn_network.is_empty() {
                top_peers = vec![peers_in_vfn_network[0]];
            }
        }

        if top_peers.is_empty() {
            let base_ping_latency = self.prioritized_peers.read().first().and_then(|peer| {
                peer_monitoring_data
                    .get(peer)
                    .and_then(|metadata| get_peer_ping_latency(metadata))
            });

            // Extract top peers with ping latency less than base_ping_latency + 50 ms
            for peer in self.prioritized_peers.read().iter() {
                if top_peers.len() >= num_top_peers as usize {
                    break;
                }

                let ping_latency = peer_monitoring_data
                    .get(peer)
                    .and_then(|metadata| get_peer_ping_latency(metadata));

                if base_ping_latency.is_none()
                    || ping_latency.is_none()
                    || ping_latency.unwrap()
                        < base_ping_latency.unwrap()
                            + (threshold_config.latency_slack_between_top_upstream_peers as f64)
                                / 1000.0
                {
                    top_peers.push(*peer);
                }
            }
        }
        info!(
            "Identified top peers: {:?}, node_type: {:?}",
            top_peers, self.node_type
        );

        assert!(top_peers.len() <= num_top_peers as usize);
        // Top peers shouldn't be empty if prioritized_peers is not zero
        assert!(self.prioritized_peers.read().is_empty() || !top_peers.is_empty());

        self.peer_to_sender_buckets = HashMap::new();
        if !self.prioritized_peers.read().is_empty() {
            // Assign sender buckets with Primary priority
            let mut peer_index = 0;
            for bucket_index in 0..self.mempool_config.num_sender_buckets {
                self.peer_to_sender_buckets
                    .entry(*top_peers.get(peer_index).unwrap())
                    .or_default()
                    .insert(bucket_index, BroadcastPeerPriority::Primary);
                peer_index = (peer_index + 1) % top_peers.len();
            }

            // Assign sender buckets with Failover priority. Use Round Robin.
            peer_index = 0;
            let num_prioritized_peers = self.prioritized_peers.read().len();
            for _ in 0..self.mempool_config.default_failovers {
                for bucket_index in 0..self.mempool_config.num_sender_buckets {
                    // Find the first peer that already doesn't have the sender bucket, and add the bucket
                    for _ in 0..num_prioritized_peers {
                        let peer = self.prioritized_peers.read()[peer_index];
                        let sender_bucket_list =
                            self.peer_to_sender_buckets.entry(peer).or_default();
                        if let std::collections::hash_map::Entry::Vacant(e) =
                            sender_bucket_list.entry(bucket_index)
                        {
                            e.insert(BroadcastPeerPriority::Failover);
                            break;
                        }
                        peer_index = (peer_index + 1) % num_prioritized_peers;
                    }
                }
            }
        }
    }
```

**File:** mempool/src/shared_mempool/priority.rs (L562-589)
```rust
fn check_peer_metadata_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
) -> bool {
    monitoring_metadata
        .and_then(|metadata| {
            metadata
                .latest_node_info_response
                .as_ref()
                .map(|node_information_response| {
                    // Get the peer's ledger timestamp and the current timestamp
                    let peer_ledger_timestamp_usecs =
                        node_information_response.ledger_timestamp_usecs;
                    let current_timestamp_usecs = get_timestamp_now_usecs(time_service);

                    // Calculate the max sync lag before the peer is considered unhealthy (in microseconds)
                    let max_sync_lag_secs =
                        mempool_config.max_sync_lag_before_unhealthy_secs as u64;
                    let max_sync_lag_usecs = max_sync_lag_secs * MICROS_PER_SECOND;

                    // Determine if the peer is healthy
                    current_timestamp_usecs.saturating_sub(peer_ledger_timestamp_usecs)
                        < max_sync_lag_usecs
                })
        })
        .unwrap_or(false) // If metadata is missing, consider the peer unhealthy
}
```

**File:** mempool/src/shared_mempool/priority.rs (L593-611)
```rust
fn compare_peer_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata_a: &Option<&PeerMonitoringMetadata>,
    monitoring_metadata_b: &Option<&PeerMonitoringMetadata>,
) -> Ordering {
    // Check the health of the peer monitoring metadata
    let is_healthy_a =
        check_peer_metadata_health(mempool_config, time_service, monitoring_metadata_a);
    let is_healthy_b =
        check_peer_metadata_health(mempool_config, time_service, monitoring_metadata_b);

    // Compare the health statuses
    match (is_healthy_a, is_healthy_b) {
        (true, false) => Ordering::Greater, // A is healthy, B is unhealthy
        (false, true) => Ordering::Less,    // A is unhealthy, B is healthy
        _ => Ordering::Equal,               // Both are healthy or unhealthy
    }
}
```

**File:** mempool/src/shared_mempool/network.rs (L527-545)
```rust
                    for (sender_bucket, peer_priority) in sender_buckets {
                        let before = match peer_priority {
                            BroadcastPeerPriority::Primary => None,
                            BroadcastPeerPriority::Failover => Some(
                                Instant::now()
                                    - Duration::from_millis(
                                        self.mempool_config.shared_mempool_failover_delay_ms,
                                    ),
                            ),
                        };
                        if max_txns > 0 {
                            let old_timeline_id = state.timelines.get(&sender_bucket).unwrap();
                            let (txns, new_timeline_id) = mempool.read_timeline(
                                sender_bucket,
                                old_timeline_id,
                                max_txns,
                                before,
                                peer_priority.clone(),
                            );
```

**File:** config/src/config/mempool_config.rs (L108-128)
```rust
impl Default for MempoolConfig {
    fn default() -> MempoolConfig {
        MempoolConfig {
            shared_mempool_tick_interval_ms: 10,
            shared_mempool_backoff_interval_ms: 30_000,
            shared_mempool_batch_size: 300,
            shared_mempool_max_batch_bytes: MAX_APPLICATION_MESSAGE_SIZE as u64,
            shared_mempool_ack_timeout_ms: 2_000,
            shared_mempool_max_concurrent_inbound_syncs: 4,
            max_broadcasts_per_peer: 20,
            max_sync_lag_before_unhealthy_secs: 30, // 30 seconds
            max_network_channel_size: 1024,
            mempool_snapshot_interval_secs: 180,
            capacity: 2_000_000,
            capacity_bytes: 2 * 1024 * 1024 * 1024,
            capacity_per_user: 100,
            default_failovers: 1,
            enable_intelligent_peer_prioritization: true,
            shared_mempool_peer_update_interval_ms: 1_000,
            shared_mempool_priority_update_interval_secs: 600, // 10 minutes (frequent reprioritization is expensive)
            shared_mempool_failover_delay_ms: 500,
```
