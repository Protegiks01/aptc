# Audit Report

## Title
Critical Consensus Messages Silently Dropped Due to Fire-and-Forget Network Layer Without Retry Mechanism

## Summary
The Aptos consensus layer sends critical messages (votes, timeout votes, proposals) using a fire-and-forget `send_to()` mechanism with no acknowledgment or retry logic. Combined with a small consensus message queue (10 messages), this can cause validators to silently drop incoming votes, leading to missed quorum formation and unnecessary round timeouts that degrade network liveness.

## Finding Description

The vulnerability exists in the message sending path from consensus to the network layer: [1](#0-0) 

The `send_to()` function explicitly documents "makes no reliable delivery guarantees" and only confirms the message was enqueued, not delivered.

When consensus sends votes, the critical flow is: [2](#0-1) 

The validator records the vote as sent BEFORE network transmission. If the network layer subsequently drops the message, the sender has no knowledge of the failure and will not retry.

The network layer uses bounded queues that drop messages when full: [3](#0-2) 

When the queue reaches capacity, messages are silently dropped (lines 101-107). The consensus message receiving queue is particularly small: [4](#0-3) 

With only **10 message capacity**, the consensus channel can easily fill under modest load, causing critical votes to be dropped.

When `send()` fails in the consensus network layer, only a warning is logged: [5](#0-4) 

There is no retry mechanism for regular votes, unlike commit votes which have dedicated retry logic in the buffer manager.

## Impact Explanation

**High Severity** - This qualifies as "Validator node slowdowns" and "Significant protocol violations" per the Aptos bug bounty program.

**Impact on Consensus Liveness:**
- When validators miss votes due to dropped messages, they cannot form quorum certificates (QCs)
- This forces unnecessary round timeouts, increasing block production latency
- Under sustained load or adversarial conditions, this can significantly degrade network throughput
- Multiple validators experiencing drops simultaneously can cascade into prolonged consensus delays

**Attack Amplification:**
- A validator processing slow due to disk I/O, CPU load, or network congestion will drop incoming messages
- Other validators don't know their votes were dropped, so they don't retry
- The slow validator triggers timeouts affecting the entire network
- This creates a feedback loop where timeouts generate more messages (timeout votes), further congesting the system

The queue size of 10 means only 10 concurrent consensus messages can be buffered. During active consensus with 100+ validators, this is easily exceeded.

## Likelihood Explanation

**High Likelihood** - This will occur regularly under normal network conditions:

1. **Natural High Load**: During periods of high transaction volume, validators process blocks, execute transactions, and handle state sync simultaneously, causing temporary CPU/disk bottlenecks that slow message processing

2. **Network Variability**: Brief network delays or packet reordering can cause message arrival bursts that exceed the queue capacity

3. **No Attacker Required**: The vulnerability manifests without malicious activity - it's a race condition between message arrival rate and processing rate

4. **Small Queue Size**: With only 10 message capacity versus potentially 100+ validators sending votes, the queue fills easily during normal consensus rounds

5. **No Backpressure Propagation**: The sender doesn't receive backpressure signals, so it continues sending while the receiver's queue is full

Evidence this occurs in practice: The codebase includes metrics specifically for tracking dropped messages: [6](#0-5) 

The existence of "dropped" metrics suggests this is a known operational issue.

## Recommendation

Implement a retry mechanism for critical consensus messages similar to the existing commit vote rebroadcast logic:

1. **Add Vote Tracking**: Maintain a cache of recently sent votes with timestamps
2. **Implement Periodic Rebroadcast**: Similar to `rebroadcast_commit_votes_if_needed()`, create a periodic task that rebroadcasts votes that haven't resulted in QC formation
3. **Add Delivery Confirmation**: Use the `push_with_feedback()` method to get notifications when messages are dropped
4. **Increase Queue Size**: Consider increasing consensus message queue from 10 to at least 100 to provide more buffering
5. **Implement Backpressure**: Add flow control so senders slow down when receivers are overwhelmed

Alternatively, use RPC-style messaging for critical votes to ensure delivery confirmation, or implement an application-level acknowledgment scheme.

## Proof of Concept

```rust
// Scenario demonstrating the vulnerability
// 
// Setup:
// - 100 validator network
// - Validator A processes blocks slowly due to disk I/O
// - Other validators broadcast votes for round R
//
// Attack Flow:
// 1. Round R begins, 100 validators vote for proposal P
// 2. Validator A's consensus queue (size 10) receives votes
// 3. Validator A is slow processing, votes 11-100 are DROPPED
// 4. Validator A only receives 10 votes, cannot form QC (needs 67)
// 5. Validator A times out after round_initial_timeout_ms (1000ms)
// 6. Other validators don't know A dropped their votes, don't retry
// 7. Round R fails unnecessarily despite 100/100 validators being honest
// 8. Network proceeds to round R+1 with timeout certificate
// 9. Consensus latency increases by 1000ms+ due to preventable timeout
//
// The dropped votes can be observed in metrics:
// aptos_consensus_channel_msgs_count{state="dropped"} increases
// 
// But there is NO mechanism to:
// - Notify senders their votes were dropped
// - Trigger automatic retry of dropped votes
// - Alert operators that votes are being lost
//
// This violates the liveness expectation that if >2/3 validators
// are online and honest, consensus should proceed without timeouts.
```

---

**Notes**

The vulnerability is subtle because it doesn't violate consensus **safety** (no forks or double-spends), but significantly impacts consensus **liveness** (throughput and latency). The fire-and-forget design is intentional for performance, but lacks the retry mechanisms necessary to maintain liveness guarantees when combined with small queue sizes.

The contrast with commit votes is telling - commit votes have explicit retry logic (`rebroadcast_commit_votes_if_needed`), suggesting the developers recognized the need for reliable delivery for critical messages. Regular votes deserve the same treatment.

### Citations

**File:** network/framework/src/peer_manager/senders.rs (L39-55)
```rust
    /// Send a fire-and-forget direct-send message to remote peer.
    ///
    /// The function returns when the message has been enqueued on the network actor's event queue.
    /// It therefore makes no reliable delivery guarantees. An error is returned if the event queue
    /// is unexpectedly shutdown.
    pub fn send_to(
        &self,
        peer_id: PeerId,
        protocol_id: ProtocolId,
        mdata: Bytes,
    ) -> Result<(), PeerManagerError> {
        self.inner.push(
            (peer_id, protocol_id),
            PeerManagerRequest::SendDirectSend(peer_id, Message { protocol_id, mdata }),
        )?;
        Ok(())
    }
```

**File:** consensus/src/round_manager.rs (L1400-1424)
```rust
        self.round_state.record_vote(vote.clone());
        let vote_msg = VoteMsg::new(vote.clone(), self.block_store.sync_info());

        self.broadcast_fast_shares(vote.ledger_info().commit_info())
            .await;

        if self.local_config.broadcast_vote {
            info!(self.new_log(LogEvent::Vote), "{}", vote);
            PROPOSAL_VOTE_BROADCASTED.inc();
            self.network.broadcast_vote(vote_msg).await;
        } else {
            let recipient = self
                .proposer_election
                .get_valid_proposer(proposal_round + 1);
            info!(
                self.new_log(LogEvent::Vote).remote_peer(recipient),
                "{}", vote
            );
            self.network.send_vote(vote_msg, vec![recipient]).await;
        }

        if let Err(e) = self.start_next_opt_round(vote, parent_qc) {
            debug!("Cannot start next opt round: {}", e);
        };
        Ok(())
```

**File:** crates/channel/src/aptos_channel.rs (L85-112)
```rust
    pub fn push(&self, key: K, message: M) -> Result<()> {
        self.push_with_feedback(key, message, None)
    }

    /// Same as `push`, but this function also accepts a oneshot::Sender over which the sender can
    /// be notified when the message eventually gets delivered or dropped.
    pub fn push_with_feedback(
        &self,
        key: K,
        message: M,
        status_ch: Option<oneshot::Sender<ElementStatus<M>>>,
    ) -> Result<()> {
        let mut shared_state = self.shared_state.lock();
        ensure!(!shared_state.receiver_dropped, "Channel is closed");
        debug_assert!(shared_state.num_senders > 0);

        let dropped = shared_state.internal_queue.push(key, (message, status_ch));
        // If this or an existing message had to be dropped because of the queue being full, we
        // notify the corresponding status channel if it was registered.
        if let Some((dropped_val, Some(dropped_status_ch))) = dropped {
            // Ignore errors.
            let _err = dropped_status_ch.send(ElementStatus::Dropped(dropped_val));
        }
        if let Some(w) = shared_state.waker.take() {
            w.wake();
        }
        Ok(())
    }
```

**File:** consensus/src/network.rs (L411-433)
```rust
    async fn send(&self, msg: ConsensusMsg, recipients: Vec<Author>) {
        fail_point!("consensus::send::any", |_| ());
        let network_sender = self.consensus_network_client.clone();
        let mut self_sender = self.self_sender.clone();
        for peer in recipients {
            if self.author == peer {
                let self_msg = Event::Message(self.author, msg.clone());
                if let Err(err) = self_sender.send(self_msg).await {
                    warn!(error = ?err, "Error delivering a self msg");
                }
                continue;
            }
            counters::CONSENSUS_SENT_MSGS
                .with_label_values(&[msg.name()])
                .inc();
            if let Err(e) = network_sender.send_to(peer, msg.clone()) {
                warn!(
                    remote_peer = peer,
                    error = ?e, "Failed to send a msg {:?} to peer", msg
                );
            }
        }
    }
```

**File:** consensus/src/network.rs (L757-761)
```rust
        let (consensus_messages_tx, consensus_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            10,
            Some(&counters::CONSENSUS_CHANNEL_MSGS),
        );
```

**File:** consensus/src/counters.rs (L1067-1075)
```rust
/// Counters(queued,dequeued,dropped) related to consensus channel
pub static CONSENSUS_CHANNEL_MSGS: Lazy<IntCounterVec> = Lazy::new(|| {
    register_int_counter_vec!(
        "aptos_consensus_channel_msgs_count",
        "Counters(queued,dequeued,dropped) related to consensus channel",
        &["state"]
    )
    .unwrap()
});
```
