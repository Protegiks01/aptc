# Audit Report

## Title
Lock Contention DoS in Peer Monitoring Service Metadata Updater

## Summary
The peer monitoring service client's metadata updater holds read locks on the `peer_states` RwLock while performing expensive JSON serialization operations, creating a lock contention vector that malicious peers can exploit to delay peer management operations.

## Finding Description

The vulnerability exists in the metadata updater loop that processes peer monitoring data. When the updater extracts metadata from peer states, it holds a read lock during expensive JSON serialization operations. [1](#0-0) 

For each connected peer (up to 100 by default), the metadata updater:

1. Acquires a read lock on `peer_states`
2. Calls `extract_peer_monitoring_metadata()` on the peer state
3. This function internally calls `get_internal_client_state()` [2](#0-1) 

The `get_internal_client_state()` function performs JSON serialization with pretty printing while the outer read lock is still held: [3](#0-2) 

Malicious peers can send maximum-sized monitoring responses (100 KB limit) containing large `connected_peers` maps or `build_information` data structures: [4](#0-3) [5](#0-4) 

With the 100 KB size limit enforced: [6](#0-5) 

**Attack Scenario:**
1. Attacker connects maximum allowed peers (100 inbound connections): [7](#0-6) 

2. Each malicious peer responds to monitoring requests with 100 KB of complex nested data
3. The metadata updater (running every 5 seconds) processes all peers sequentially
4. For each peer, it holds a read lock while performing JSON serialization of up to 100 KB
5. Write operations (peer connect/disconnect) that require the write lock are delayed by the repeated read lock acquisitions
6. The `std::sync::RwLock` used provides no fairness guarantees: [8](#0-7) 

Write operations that get blocked include peer state creation and garbage collection: [9](#0-8) [10](#0-9) 

## Impact Explanation

This qualifies as **High Severity** per the Aptos bug bounty criteria for "Validator node slowdowns." 

The attack degrades validator performance by:
- Delaying peer connection handling by hundreds of milliseconds
- Blocking peer state garbage collection during the metadata update window
- Creating artificial latency spikes in peer management operations every 5 seconds
- Potentially preventing rapid peer reconnection during network instability

While not causing total liveness failure, this creates measurable performance degradation in validator nodes' ability to manage peer connections efficiently.

## Likelihood Explanation

**Likelihood: High**

The attack is straightforward to execute:
- No special privileges required - any network peer can connect
- Connection limits are generous (100 inbound peers)
- The response size limit (100 KB) is large enough to trigger significant serialization delays
- The metadata updater runs automatically every 5 seconds
- Standard library RwLock doesn't provide writer priority, making writer starvation realistic

## Recommendation

**Solution 1: Clone peer state before lock release**
```rust
// In lib.rs, line 234-249
for peer_network_id in all_peers {
    // Clone the peer state while holding the lock briefly
    let peer_state = peer_monitor_state.peer_states.read()
        .get(&peer_network_id)
        .cloned();
    
    // Extract metadata without holding the lock
    let peer_monitoring_metadata = match peer_state {
        Some(peer_state) => {
            peer_state
                .extract_peer_monitoring_metadata()
                .unwrap_or_else(|error| { ... })
        },
        None => PeerMonitoringMetadata::default(),
    };
    // ... rest of code
}
```

**Solution 2: Cache serialized metadata**
Compute the internal client state asynchronously and cache it, avoiding expensive serialization on the critical path.

**Solution 3: Use a fair RwLock implementation**
Replace `std::sync::RwLock` with a fair lock implementation like `parking_lot::RwLock` that provides writer priority.

## Proof of Concept

```rust
// Reproduction steps:
// 1. Deploy a test validator node
// 2. Connect 100 malicious peer clients
// 3. Each peer responds to GetNetworkInformation with maximum data:
//    - connected_peers: BTreeMap with 1000+ entries
//    - Each entry with large NetworkAddress strings
// 4. Each peer responds to GetNodeInformation with:
//    - build_information: BTreeMap with 100+ large string key-value pairs
// 5. Monitor the timing of peer connection attempts from legitimate peers
// 6. Observe delays of 500ms-1000ms every 5 seconds when metadata updater runs
// 7. The delays correlate with write lock acquisition in create_states_for_new_peers

// Test harness:
use aptos_peer_monitoring_service_types::response::{
    NetworkInformationResponse, NodeInformationResponse, ConnectionMetadata
};
use std::collections::BTreeMap;

fn create_maximum_sized_response() -> NetworkInformationResponse {
    let mut connected_peers = BTreeMap::new();
    // Add maximum number of peers to approach 100 KB limit
    for i in 0..1000 {
        let peer_id = PeerId::random();
        let network_address = NetworkAddress::from_str(
            &format!("/ip4/192.168.{}.{}/tcp/6180", i / 256, i % 256)
        ).unwrap();
        connected_peers.insert(
            PeerNetworkId::new(NetworkId::Public, peer_id),
            ConnectionMetadata::new(network_address, peer_id, PeerRole::Unknown)
        );
    }
    NetworkInformationResponse {
        connected_peers,
        distance_from_validators: 2,
    }
}

// Attack: Each of 100 malicious peers returns this response
// Result: Metadata updater spends 1-10ms per peer serializing to JSON
// Total: 100-1000ms of cumulative read lock hold time
// Impact: Write operations delayed, peer management degraded
```

## Notes

The vulnerability is exacerbated by the use of `serde_json::to_string_pretty()` which is more expensive than standard serialization. The Aptos networking layer elsewhere uses caching to mitigate similar lock contention issues, suggesting awareness of this pattern's risks. [11](#0-10)

### Citations

**File:** peer-monitoring-service/client/src/lib.rs (L167-176)
```rust
        let state_exists = peer_monitor_state
            .peer_states
            .read()
            .contains_key(peer_network_id);
        if !state_exists {
            peer_monitor_state.peer_states.write().insert(
                *peer_network_id,
                PeerState::new(node_config.clone(), time_service.clone()),
            );
        }
```

**File:** peer-monitoring-service/client/src/lib.rs (L194-200)
```rust
    for peer_network_id in peers_with_existing_states {
        if !connected_peers_and_metadata.contains_key(&peer_network_id) {
            peer_monitor_state
                .peer_states
                .write()
                .remove(&peer_network_id);
        }
```

**File:** peer-monitoring-service/client/src/lib.rs (L234-249)
```rust
                let peer_monitoring_metadata =
                    match peer_monitor_state.peer_states.read().get(&peer_network_id) {
                        Some(peer_state) => {
                            peer_state
                                .extract_peer_monitoring_metadata()
                                .unwrap_or_else(|error| {
                                    // Log the error and return the default
                                    warn!(LogSchema::new(LogEntry::MetadataUpdateLoop)
                                        .event(LogEvent::UnexpectedErrorEncountered)
                                        .peer(&peer_network_id)
                                        .error(&error));
                                    PeerMonitoringMetadata::default()
                                })
                        },
                        None => PeerMonitoringMetadata::default(), // Use the default
                    };
```

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L186-214)
```rust
    /// Extracts peer monitoring metadata from the overall peer state
    pub fn extract_peer_monitoring_metadata(&self) -> Result<PeerMonitoringMetadata, Error> {
        // Create an empty metadata entry for the peer
        let mut peer_monitoring_metadata = PeerMonitoringMetadata::default();

        // Get and store the average latency ping
        let latency_info_state = self.get_latency_info_state()?;
        let average_latency_ping_secs = latency_info_state.get_average_latency_ping_secs();
        peer_monitoring_metadata.average_ping_latency_secs = average_latency_ping_secs;

        let latest_ping_latency_secs = latency_info_state.get_latest_latency_ping_secs();
        peer_monitoring_metadata.latest_ping_latency_secs = latest_ping_latency_secs;

        // Get and store the detailed monitoring metadata
        let internal_client_state = self.get_internal_client_state()?;
        peer_monitoring_metadata.internal_client_state = internal_client_state;

        // Get and store the latest network info response
        let network_info_state = self.get_network_info_state()?;
        let network_info_response = network_info_state.get_latest_network_info_response();
        peer_monitoring_metadata.latest_network_info_response = network_info_response;

        // Get and store the latest node info response
        let node_info_state = self.get_node_info_state()?;
        let node_info_response = node_info_state.get_latest_node_info_response();
        peer_monitoring_metadata.latest_node_info_response = node_info_response;

        Ok(peer_monitoring_metadata)
    }
```

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L276-294)
```rust
    fn get_internal_client_state(&self) -> Result<Option<String>, Error> {
        // Construct a string map for each of the state entries
        let mut client_state_strings = HashMap::new();
        for (state_key, state_value) in self.state_entries.read().iter() {
            let peer_state_label = state_key.get_label().to_string();
            let peer_state_value = format!("{}", state_value.read().deref());
            client_state_strings.insert(peer_state_label, peer_state_value);
        }

        // Pretty print and return the client state string
        let client_state_string =
            serde_json::to_string_pretty(&client_state_strings).map_err(|error| {
                Error::UnexpectedError(format!(
                    "Failed to serialize the client state string: {:?}",
                    error
                ))
            })?;
        Ok(Some(client_state_string))
    }
```

**File:** peer-monitoring-service/types/src/response.rs (L51-55)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub struct NetworkInformationResponse {
    pub connected_peers: BTreeMap<PeerNetworkId, ConnectionMetadata>, // Connected peers
    pub distance_from_validators: u64, // The distance of the peer from the validator set
}
```

**File:** peer-monitoring-service/types/src/response.rs (L94-102)
```rust
#[derive(Clone, Debug, Default, Deserialize, Eq, PartialEq, Serialize)]
pub struct NodeInformationResponse {
    pub build_information: BTreeMap<String, String>, // The build information of the node
    pub highest_synced_epoch: u64,                   // The highest synced epoch of the node
    pub highest_synced_version: u64,                 // The highest synced version of the node
    pub ledger_timestamp_usecs: u64, // The latest timestamp of the blockchain (in microseconds)
    pub lowest_available_version: u64, // The lowest stored version of the node (in storage)
    pub uptime: Duration,            // The amount of time the peer has been running
}
```

**File:** config/src/config/peer_monitoring_config.rs (L28-28)
```rust
            max_num_response_bytes: 100 * 1024, // 100 KB
```

**File:** config/src/config/network_config.rs (L44-44)
```rust
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
```

**File:** crates/aptos-infallible/src/rwlock.rs (L1-30)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use std::sync::RwLock as StdRwLock;
pub use std::sync::{RwLockReadGuard, RwLockWriteGuard};

/// A simple wrapper around the lock() function of a std::sync::RwLock
/// The only difference is that you don't need to call unwrap() on it.
#[derive(Debug, Default)]
pub struct RwLock<T>(StdRwLock<T>);

impl<T> RwLock<T> {
    /// creates a read-write lock
    pub fn new(t: T) -> Self {
        Self(StdRwLock::new(t))
    }

    /// lock the rwlock in read mode
    pub fn read(&self) -> RwLockReadGuard<'_, T> {
        self.0
            .read()
            .expect("Cannot currently handle a poisoned lock")
    }

    /// lock the rwlock in write mode
    pub fn write(&self) -> RwLockWriteGuard<'_, T> {
        self.0
            .write()
            .expect("Cannot currently handle a poisoned lock")
    }
```

**File:** network/framework/src/application/storage.rs (L46-51)
```rust
    // We maintain a cached copy of the peers and metadata. This is useful to
    // reduce lock contention, as we expect very heavy and frequent reads,
    // but infrequent writes. The cache is updated on all underlying updates.
    //
    // TODO: should we remove this when generational versioning is supported?
    cached_peers_and_metadata: Arc<ArcSwap<HashMap<NetworkId, HashMap<PeerId, PeerMetadata>>>>,
```
