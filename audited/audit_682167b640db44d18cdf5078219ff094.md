# Audit Report

## Title
Indexer Denial of Service via Panic on Malformed Token Pending Claim Data

## Summary
The token indexer contains multiple unhandled `.unwrap()` calls when deserializing token pending claim data from blockchain state. If malformed JSON data is encountered during deserialization, the indexer will panic and crash, causing a Denial of Service condition for indexing infrastructure. [1](#0-0) 

## Finding Description
The `Token::from_transaction()` function processes write set changes to extract token data, including pending claims. At lines 148-154 and 156-163, the code calls `.unwrap()` on the `Result` returned by `CurrentTokenPendingClaim::from_write_table_item()` and `CurrentTokenPendingClaim::from_delete_table_item()`. [2](#0-1) 

These functions perform JSON deserialization of `TokenOfferId` and `Token` structures using `serde_json::from_value()`, which can fail if:

1. **BigDecimal parsing fails** - Token structures contain `amount` and `property_version` fields deserialized as `BigDecimal` from strings
2. **BCS hex decoding fails** - Property maps contain BCS-encoded hex strings that must be decoded
3. **Missing required fields** - JSON structure doesn't match expected schema
4. **Type mismatches** - Field types don't match expected types [3](#0-2) 

When deserialization fails, `TokenWriteSet::from_table_item_type()` returns an `Err`, which propagates up. The `.unwrap()` at line 154 causes a panic that crashes the entire indexer process. [4](#0-3) 

The runtime explicitly panics when transaction processing errors occur, bringing down the entire indexer service.

## Impact Explanation
This qualifies as **High Severity** per the Aptos bug bounty criteria under "API crashes". While the indexer is not part of core blockchain consensus, it represents critical infrastructure for:

- Wallet applications querying token balances
- NFT marketplaces displaying collections
- Analytics platforms tracking token transfers
- Block explorers showing transaction history

An indexer crash causes:
- **Service unavailability** for all downstream applications
- **Data staleness** as new transactions are not indexed
- **Manual intervention required** to restart and potentially skip problematic transactions
- **User experience degradation** across the Aptos ecosystem

However, this does **NOT** affect:
- Blockchain consensus or validator operations
- On-chain state or transaction execution
- User funds or asset security
- Network availability

## Likelihood Explanation
The likelihood is **Medium** because:

**Triggering conditions:**
1. Malformed JSON must appear in the API response (requires API bug or data corruption)
2. Move VM type safety prevents direct creation of malformed on-chain data
3. Property maps with invalid BCS encoding could theoretically cause issues
4. Protocol upgrades introducing schema changes without indexer updates

**Realistic scenarios:**
- API node bug in BCS-to-JSON conversion
- Database corruption in API node state storage
- Indexer running outdated code against upgraded token contracts
- Edge cases in custom deserializers for property maps

**Not directly exploitable:** An unprivileged attacker cannot craft a transaction that directly causes this crash, as Move VM validation prevents storing malformed data on-chain.

## Recommendation
Replace all `.unwrap()` calls with proper error handling using the `?` operator or `.unwrap_or_else()` with logging:

```rust
let maybe_current_token_claim = match wsc {
    APIWriteSetChange::WriteTableItem(write_table_item) => {
        CurrentTokenPendingClaim::from_write_table_item(
            write_table_item,
            txn_version,
            txn_timestamp,
            table_handle_to_owner,
        )
        .unwrap_or_else(|e| {
            aptos_logger::warn!(
                transaction_version = txn_version,
                error = ?e,
                "Failed to parse TokenPendingClaim from write table item"
            );
            None
        })
    },
    APIWriteSetChange::DeleteTableItem(delete_table_item) => {
        CurrentTokenPendingClaim::from_delete_table_item(
            delete_table_item,
            txn_version,
            txn_timestamp,
            table_handle_to_owner,
        )
        .unwrap_or_else(|e| {
            aptos_logger::warn!(
                transaction_version = txn_version,
                error = ?e,
                "Failed to parse TokenPendingClaim from delete table item"
            );
            None
        })
    },
    _ => None,
};
```

Similar fixes should be applied to all `.unwrap()` calls in the token processing pipeline. [5](#0-4) 

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use serde_json::json;

    #[test]
    #[should_panic(expected = "failed to parse type")]
    fn test_malformed_bigdecimal_causes_panic() {
        // Simulate malformed JSON with invalid BigDecimal value
        let malformed_json = json!({
            "to_addr": "0x1",
            "token_id": {
                "token_data_id": {
                    "creator": "0x1",
                    "collection": "test",
                    "name": "test"
                },
                "property_version": "not_a_number" // Invalid BigDecimal
            }
        });
        
        // This will panic when trying to deserialize
        let result = TokenWriteSet::from_table_item_type(
            "0x3::token_transfers::TokenOfferId",
            &malformed_json,
            12345,
        );
        
        // Force unwrap to trigger panic (simulating production code)
        result.unwrap();
    }

    #[test]
    #[should_panic]
    fn test_missing_required_field_causes_panic() {
        // Simulate JSON missing required field
        let malformed_json = json!({
            "token_id": {
                "token_data_id": {
                    "creator": "0x1",
                    "collection": "test",
                    "name": "test"
                },
                "property_version": "0"
            }
            // Missing "to_addr" field
        });
        
        let result = TokenWriteSet::from_table_item_type(
            "0x3::token_transfers::TokenOfferId",
            &malformed_json,
            12345,
        );
        
        result.unwrap();
    }
}
```

## Notes

**Important Limitations:**
- This vulnerability affects **indexer infrastructure only**, not core blockchain consensus or validator nodes
- It is **not directly exploitable** by attackers crafting malicious transactions, as Move VM type safety prevents storing malformed data on-chain
- Triggering requires either API bugs, data corruption, or protocol upgrade mismatches
- The indexer is an **off-chain component** separate from blockchain consensus

While this is a legitimate availability issue warranting a fix, it does not compromise blockchain security guarantees, consensus safety, or on-chain asset security. The impact is limited to the availability of indexing services for external applications.

### Citations

**File:** crates/indexer/src/models/token_models/tokens.rs (L108-143)
```rust
                let (maybe_token_w_ownership, maybe_token_data, maybe_collection_data) = match wsc {
                    APIWriteSetChange::WriteTableItem(write_table_item) => (
                        Self::from_write_table_item(
                            write_table_item,
                            txn_version,
                            txn_timestamp,
                            table_handle_to_owner,
                        )
                        .unwrap(),
                        TokenData::from_write_table_item(
                            write_table_item,
                            txn_version,
                            txn_timestamp,
                        )
                        .unwrap(),
                        CollectionData::from_write_table_item(
                            write_table_item,
                            txn_version,
                            txn_timestamp,
                            table_handle_to_owner,
                            conn,
                        )
                        .unwrap(),
                    ),
                    APIWriteSetChange::DeleteTableItem(delete_table_item) => (
                        Self::from_delete_table_item(
                            delete_table_item,
                            txn_version,
                            txn_timestamp,
                            table_handle_to_owner,
                        )
                        .unwrap(),
                        None,
                        None,
                    ),
                    _ => (None, None, None),
```

**File:** crates/indexer/src/models/token_models/tokens.rs (L146-166)
```rust
                let maybe_current_token_claim = match wsc {
                    APIWriteSetChange::WriteTableItem(write_table_item) => {
                        CurrentTokenPendingClaim::from_write_table_item(
                            write_table_item,
                            txn_version,
                            txn_timestamp,
                            table_handle_to_owner,
                        )
                        .unwrap()
                    },
                    APIWriteSetChange::DeleteTableItem(delete_table_item) => {
                        CurrentTokenPendingClaim::from_delete_table_item(
                            delete_table_item,
                            txn_version,
                            txn_timestamp,
                            table_handle_to_owner,
                        )
                        .unwrap()
                    },
                    _ => None,
                };
```

**File:** crates/indexer/src/models/token_models/token_claims.rs (L38-114)
```rust
    pub fn from_write_table_item(
        table_item: &APIWriteTableItem,
        txn_version: i64,
        txn_timestamp: chrono::NaiveDateTime,
        table_handle_to_owner: &TableHandleToOwner,
    ) -> anyhow::Result<Option<Self>> {
        let table_item_data = table_item.data.as_ref().unwrap();

        let maybe_offer = match TokenWriteSet::from_table_item_type(
            table_item_data.key_type.as_str(),
            &table_item_data.key,
            txn_version,
        )? {
            Some(TokenWriteSet::TokenOfferId(inner)) => Some(inner),
            _ => None,
        };
        if let Some(offer) = maybe_offer {
            let maybe_token = match TokenWriteSet::from_table_item_type(
                table_item_data.value_type.as_str(),
                &table_item_data.value,
                txn_version,
            )? {
                Some(TokenWriteSet::Token(inner)) => Some(inner),
                _ => None,
            };
            if let Some(token) = maybe_token {
                let table_handle = standardize_address(&table_item.handle.to_string());

                let maybe_table_metadata = table_handle_to_owner.get(&table_handle);

                if let Some(table_metadata) = maybe_table_metadata {
                    let token_id = offer.token_id;
                    let token_data_id_struct = token_id.token_data_id;
                    let collection_data_id_hash =
                        token_data_id_struct.get_collection_data_id_hash();
                    let token_data_id_hash = token_data_id_struct.to_hash();
                    // Basically adding 0x prefix to the previous 2 lines. This is to be consistent with Token V2
                    let collection_id = token_data_id_struct.get_collection_id();
                    let token_data_id = token_data_id_struct.to_id();
                    let collection_name = token_data_id_struct.get_collection_trunc();
                    let name = token_data_id_struct.get_name_trunc();

                    return Ok(Some(Self {
                        token_data_id_hash,
                        property_version: token_id.property_version,
                        from_address: standardize_address(&table_metadata.owner_address),
                        to_address: standardize_address(&offer.to_addr),
                        collection_data_id_hash,
                        creator_address: standardize_address(&token_data_id_struct.creator),
                        collection_name,
                        name,
                        amount: token.amount,
                        table_handle,
                        last_transaction_version: txn_version,
                        last_transaction_timestamp: txn_timestamp,
                        token_data_id,
                        collection_id,
                    }));
                } else {
                    aptos_logger::warn!(
                        transaction_version = txn_version,
                        table_handle = table_handle,
                        "Missing table handle metadata for TokenClaim. {:?}",
                        table_handle_to_owner
                    );
                }
            } else {
                aptos_logger::warn!(
                    transaction_version = txn_version,
                    value_type = table_item_data.value_type,
                    value = table_item_data.value,
                    "Expecting token as value for key = token_offer_id",
                );
            }
        }
        Ok(None)
    }
```

**File:** crates/indexer/src/models/token_models/token_utils.rs (L322-347)
```rust
    pub fn from_table_item_type(
        data_type: &str,
        data: &serde_json::Value,
        txn_version: i64,
    ) -> Result<Option<TokenWriteSet>> {
        match data_type {
            "0x3::token::TokenDataId" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::TokenDataId(inner))),
            "0x3::token::TokenId" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::TokenId(inner))),
            "0x3::token::TokenData" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::TokenData(inner))),
            "0x3::token::Token" => {
                serde_json::from_value(data.clone()).map(|inner| Some(TokenWriteSet::Token(inner)))
            },
            "0x3::token::CollectionData" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::CollectionData(inner))),
            "0x3::token_transfers::TokenOfferId" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::TokenOfferId(inner))),
            _ => Ok(None),
        }
        .context(format!(
            "version {} failed! failed to parse type {}, data {:?}",
            txn_version, data_type, data
        ))
    }
```

**File:** crates/indexer/src/runtime.rs (L230-243)
```rust
                Some(Err(tpe)) => {
                    let (err, start_version, end_version, _) = tpe.inner();
                    error!(
                        processor_name = processor_name,
                        start_version = start_version,
                        end_version = end_version,
                        error =? err,
                        "Error processing batch!"
                    );
                    panic!(
                        "Error in '{}' while processing batch: {:?}",
                        processor_name, err
                    );
                },
```
