# Audit Report

## Title
Missing Round Count Validation in Sharded Block Execution Causes Consensus Divergence

## Summary
The `ShardedExecutionOutput` aggregation logic assumes all executor shards return the same number of execution rounds without validation. In distributed execution mode with remote executors, a malicious or buggy remote executor can return a different number of rounds, causing validators to produce different transaction orderings and divergent state roots, breaking consensus safety.

## Finding Description

The sharded block executor aggregates execution results from multiple shards by reconstructing transaction ordering based on a round-major, shard-minor pattern. The aggregation code assumes all shards executed the same number of rounds by reading only the first shard's round count. [1](#0-0) 

The critical flaw is that `num_rounds` is determined solely from `sharded_output[0].len()`, with no validation that other shards match this count. The same vulnerability exists in two other locations: [2](#0-1) [3](#0-2) 

In local execution mode, all shards are guaranteed to have identical round counts by construction: [4](#0-3) 

However, in **remote execution mode**, results come from distributed executor nodes over the network. The remote executor client deserializes results without validating round count consistency: [5](#0-4) 

The remote execution path is active in production when remote addresses are configured: [6](#0-5) 

**Attack Scenario:**

1. Validator A and Validator B both execute the same block with 4 shards and expect 3 rounds per shard
2. A malicious/buggy remote executor for Validator B's shard 2 returns only 2 rounds instead of 3
3. Validator A computes: `num_rounds = 3` from all shards, creates `ordered_results` of size 12, correctly orders all transactions
4. Validator B computes: `num_rounds = 2` from shard 0, creates `ordered_results` of size 8, **silently omits round 2 transactions from all shards**
5. Validators produce different transaction orderings → different state roots → consensus divergence

## Impact Explanation

**Severity: CRITICAL** (Consensus/Safety Violation)

This vulnerability breaks two fundamental invariants:
- **Invariant #1 (Deterministic Execution)**: Validators no longer produce identical state roots for identical blocks
- **Invariant #2 (Consensus Safety)**: Can cause chain splits without requiring 1/3+ Byzantine validators

The impact qualifies as Critical per the bug bounty program because it enables:
- **Consensus/Safety violations**: Different validators computing different state roots for the same committed block
- **Non-recoverable network partition**: Chain split requiring hard fork to reconcile divergent states
- Exploitable with a single compromised remote executor (no quorum collusion needed)

## Likelihood Explanation

**Likelihood: HIGH** in deployments using remote execution

The vulnerability is highly likely to manifest because:
1. **Remote execution is production code**: The remote sharded executor path is implemented and used when `get_remote_addresses()` returns non-empty
2. **No input validation**: Zero defensive checks exist to detect mismatched round counts
3. **Multiple attack vectors**:
   - Malicious remote executor deliberately returning wrong round count
   - Software bugs in remote executor causing incorrect result structure
   - Network corruption during BCS serialization/deserialization
4. **Silent failure mode**: Mismatched rounds cause either panic (DoS) or silent transaction omission (consensus divergence), both unacceptable
5. **Test suite validates this invariant but production code doesn't**: The test utility enforces round count equality, but production doesn't [7](#0-6) 

## Recommendation

Add explicit validation that all shards return identical round counts before aggregation. Insert this check immediately after receiving shard outputs:

```rust
// In ShardedBlockExecutor::execute_block (mod.rs)
let (sharded_output, global_output) = self
    .executor_client
    .execute_block(/* ... */)?.into_inner();

// VALIDATION: Ensure all shards have the same number of rounds
let num_shards = sharded_output.len();
if num_shards > 0 {
    let expected_rounds = sharded_output[0].len();
    for (shard_id, shard_results) in sharded_output.iter().enumerate().skip(1) {
        if shard_results.len() != expected_rounds {
            return Err(VMStatus::Error(
                StatusCode::UNEXPECTED_ERROR_FROM_KNOWN_MOVE_FUNCTION,
                Some(format!(
                    "Shard {} returned {} rounds but expected {} rounds",
                    shard_id, shard_results.len(), expected_rounds
                ))
            ));
        }
    }
}

// Continue with existing aggregation logic...
let num_rounds = sharded_output[0].len();
```

Apply identical validation in:
- `aggregate_and_update_total_supply` (sharded_aggregator_service.rs)
- `SubBlocksForShard::flatten` (partitioner.rs)

## Proof of Concept

```rust
// Test demonstrating consensus divergence from mismatched round counts
#[test]
fn test_consensus_divergence_from_mismatched_rounds() {
    use aptos_vm::sharded_block_executor::executor_client::ShardedExecutionOutput;
    use aptos_types::transaction::TransactionOutput;
    
    // Simulate Validator A: receives 3 rounds from all shards
    let validator_a_output = ShardedExecutionOutput::new(
        vec![
            vec![vec![TransactionOutput::default()], vec![TransactionOutput::default()], vec![TransactionOutput::default()]], // Shard 0: 3 rounds
            vec![vec![TransactionOutput::default()], vec![TransactionOutput::default()], vec![TransactionOutput::default()]], // Shard 1: 3 rounds
        ],
        vec![]
    );
    
    // Simulate Validator B: shard 1 maliciously returns only 2 rounds
    let validator_b_output = ShardedExecutionOutput::new(
        vec![
            vec![vec![TransactionOutput::default()], vec![TransactionOutput::default()], vec![TransactionOutput::default()]], // Shard 0: 3 rounds
            vec![vec![TransactionOutput::default()], vec![TransactionOutput::default()]], // Shard 1: ONLY 2 rounds (malicious)
        ],
        vec![]
    );
    
    // Current code will compute different num_rounds:
    // Validator A: num_rounds = 3 (from shard 0)
    // Validator B: num_rounds = 3 (from shard 0)
    // But during iteration over shard 1's rounds:
    // Validator A processes all 3 rounds from shard 1
    // Validator B only processes 2 rounds from shard 1
    
    // Result: Different transaction orderings → different state roots → consensus divergence
    
    // This test would require implementing mock ShardedBlockExecutor to demonstrate
    // the full aggregation divergence, showing that the final aggregated_results
    // vectors have different lengths and orderings between the two validators.
}
```

**Notes**

This vulnerability exists because defensive validation was not ported from test utilities to production code. The assumption that all shards return matching round counts holds for local execution by construction, but breaks for remote execution where deserialized data structures from untrusted network sources are not validated. The fix is straightforward: add explicit validation before aggregation, ensuring all shards have identical round counts before reconstructing transaction ordering.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L98-110)
```rust
        let num_rounds = sharded_output[0].len();
        let mut aggregated_results = vec![];
        let mut ordered_results = vec![vec![]; num_executor_shards * num_rounds];
        // Append the output from individual shards in the round order
        for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
            for (round, result) in results_from_shard.into_iter().enumerate() {
                ordered_results[round * num_executor_shards + shard_id] = result;
            }
        }

        for result in ordered_results.into_iter() {
            aggregated_results.extend(result);
        }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_aggregator_service.rs (L175-175)
```rust
    let num_rounds = sharded_output[0].len();
```

**File:** types/src/block_executor/partitioner.rs (L381-385)
```rust
        let num_rounds = block[0].num_sub_blocks();
        let mut ordered_blocks = vec![SubBlock::empty(); num_shards * num_rounds];
        for (shard_id, sub_blocks) in block.into_iter().enumerate() {
            for (round, sub_block) in sub_blocks.into_sub_blocks().into_iter().enumerate() {
                ordered_blocks[round * num_shards + shard_id] = sub_block;
```

**File:** execution/block-partitioner/src/v2/build_edge.rs (L72-86)
```rust
        let final_num_rounds = state.sub_block_matrix.len();
        let sharded_txns = (0..state.num_executor_shards)
            .map(|shard_id| {
                let sub_blocks: Vec<SubBlock<AnalyzedTransaction>> = (0..final_num_rounds)
                    .map(|round_id| {
                        state.sub_block_matrix[round_id][shard_id]
                            .lock()
                            .unwrap()
                            .take()
                            .unwrap()
                    })
                    .collect();
                SubBlocksForShard::new(shard_id, sub_blocks)
            })
            .collect();
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L261-267)
```rust
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
```

**File:** execution/block-partitioner/src/test_utils.rs (L170-172)
```rust
    for sub_block_list in output.sharded_txns().iter().take(num_shards).skip(1) {
        assert_eq!(num_rounds, sub_block_list.sub_blocks.len());
    }
```
