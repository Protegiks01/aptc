# Audit Report

## Title
Resource Group Metadata-Content Divergence via Non-Atomic Multi-Step Write Operations

## Summary
The BlockSTMv2 executor performs resource group writes through two separate, non-atomic operations: writing metadata to `versioned_cache.data()` and writing group content to `versioned_cache.group_data()`. If the second operation fails after the first succeeds, there is no rollback mechanism, creating inconsistent state where metadata exists without corresponding content.

## Finding Description

The vulnerability exists in the `process_resource_group_output_v2` function where resource group writes are performed as two independent operations without transactional guarantees: [1](#0-0) 

The code writes metadata first (line 259), then writes group data (line 268). Both operations use the `?` operator for error propagation, but there is **no rollback mechanism** if the second write fails after the first succeeds.

Within the `write_v2` method itself, there's another partial write scenario: [2](#0-1) 

The `data_write_impl` call at line 271 writes all inner operations to `self.values`, modifying the versioned cache. If the subsequent `group_sizes.get_mut()` call at line 275 fails (returning a code invariant error when the group is uninitialized), the function returns an error **without rolling back** the inner operations already written by `data_write_impl`.

The TODO comment explicitly acknowledges this architectural weakness: [3](#0-2) 

**Breaking the State Consistency Invariant:**

This violates Aptos's critical invariant #4: "State transitions must be atomic and verifiable via Merkle proofs." The multi-step write creates intermediate states where:
- Metadata indicates a group write occurred at transaction index `txn_idx`
- But the group size entry doesn't exist in `group_sizes`, or
- Inner operations are partially written but size validation failed

The code relies on a "read-before-write" invariant to prevent writes to uninitialized groups: [4](#0-3) 

However, this invariant is **assumed but not enforced** at the write boundary. If through any means (initialization bug, race condition in parallel execution, speculative execution error, or re-execution after abort) a write occurs to an uninitialized group, the partial write manifests.

## Impact Explanation

**Severity: High** (State inconsistencies requiring intervention)

The vulnerability creates inconsistent state in the multi-version data structure used for parallel execution:

1. **Transaction Validation Failures**: Subsequent transactions reading the group may encounter `MVGroupError::Uninitialized` or inconsistent size information, causing unnecessary validation failures and re-executions.

2. **State Merkle Tree Inconsistency**: When the block is finalized, the inconsistent versioned cache state must be resolved into final storage writes. The metadata may indicate a group modification while the group content is incomplete, potentially creating divergent state roots.

3. **Consensus Divergence Risk**: If different validators encounter this condition at different times (due to non-deterministic timing in parallel execution), they could produce different state roots for the same block, breaking consensus safety.

4. **Speculative Execution Pollution**: The partial writes remain in the versioned cache during speculative execution, potentially causing cascading validation failures in dependent transactions.

While this requires specific conditions to trigger (uninitialized group write), the impact on state consistency and potential for consensus issues justifies High severity.

## Likelihood Explanation

**Likelihood: Low to Medium**

The vulnerability can manifest under several scenarios:

1. **Bug in Initialization Logic**: If the group initialization code has any bug where `set_raw_base_values` fails to properly create the `group_sizes` entry, subsequent writes will encounter the uninitialized group error after partial state modification.

2. **Race Condition in Parallel Execution**: During BlockSTM parallel execution, if transaction A initializes a group but then aborts, and transaction B speculatively writes to that group before observing the abort, the timing could create the uninitialized condition.

3. **Re-execution After Speculation Error**: When a transaction is re-executed due to speculation errors, the cleanup of previous execution state may be incomplete, leaving inner operations written but size entries missing.

4. **Edge Cases in BlockSTMv2**: The transition from V1 to V2 execution models introduces complexity in dependency tracking and validation that may expose previously untested code paths.

The likelihood is constrained by the read-before-write pattern that normally prevents this, but not eliminated because the pattern is a convention rather than an enforced guarantee at the write operation level.

## Recommendation

**Solution: Implement atomic resource group writes with rollback capability**

1. **Unified Atomic Write API**: Implement the TODO suggestion to create a single atomic API that handles metadata, size, and inner operations together:

```rust
pub fn write_resource_group_atomic(
    &self,
    group_key: K,
    txn_idx: TxnIndex,
    incarnation: Incarnation,
    metadata_op: WriteOp,
    group_ops: impl IntoIterator<Item = (T, (V, Option<Arc<MoveTypeLayout>>))>,
    group_size: ResourceGroupSize,
    prev_tags: HashSet<&T>,
) -> Result<BTreeMap<TxnIndex, Incarnation>, PanicError> {
    // First validate all preconditions without modifying state
    let group_sizes = self.group_sizes.get_mut(&group_key).ok_or_else(|| {
        code_invariant_error("Group must be initialized before write")
    })?;
    
    // Perform all writes only after validation succeeds
    // Combine data write and metadata write atomically
    // ...
}
```

2. **Pre-validation Check**: Add explicit validation that the group is initialized before attempting any writes:

```rust
// In process_resource_group_output_v2, before writing:
if !versioned_cache.group_data().is_initialized(&group_key) {
    return Err(code_invariant_error("Attempting to write to uninitialized group"));
}
```

3. **Rollback Mechanism**: If unified atomicity cannot be achieved, implement explicit rollback:

```rust
// Attempt metadata write
let metadata_result = versioned_cache.data().write_v2(...);
if metadata_result.is_err() {
    return metadata_result;
}

// Attempt group data write
let group_data_result = versioned_cache.group_data().write_v2(...);
if group_data_result.is_err() {
    // Rollback metadata write
    versioned_cache.data().remove_v2(&group_key, txn_idx)?;
    return group_data_result;
}
```

## Proof of Concept

The following test would demonstrate the vulnerability if an uninitialized group write could be triggered:

```rust
#[test]
fn test_metadata_content_divergence() {
    // Setup: Create versioned cache without initializing a specific group
    let cache = MVHashMap::new();
    let group_key = StateKey::raw(&[1, 2, 3]);
    
    // Transaction 0: Initialize group (but simulate initialization failure)
    // ... setup code that leaves group_sizes empty for group_key ...
    
    // Transaction 1: Attempt to write to uninitialized group
    let metadata_op = WriteOp::modification(vec![].into(), StateValueMetadata::default());
    let inner_ops = vec![(tag1, (value1, None))];
    
    // Step 1: Write metadata (succeeds)
    let metadata_result = cache.data().write_v2(
        group_key.clone(), 1, 0,
        Arc::new(metadata_op), None
    );
    assert!(metadata_result.is_ok());
    
    // Step 2: Write group data (fails due to uninitialized group)
    let group_result = cache.group_data().write_v2(
        group_key.clone(), 1, 0,
        inner_ops.into_iter(),
        ResourceGroupSize::new(100),
        HashSet::new(),
    );
    assert!(group_result.is_err()); // Fails with code_invariant_error
    
    // Verify inconsistent state: metadata exists but group not properly initialized
    let metadata_read = cache.data().fetch_data(&group_key, 2);
    assert!(metadata_read.is_ok()); // Metadata is readable
    
    let size_read = cache.group_data().get_group_size(&group_key, 2);
    assert!(size_read.is_err()); // Size is not available - inconsistency!
}
```

**Note**: This PoC cannot be executed without the ability to create an uninitialized group condition in the test environment, which would require reproducing the exact race condition or bug that bypasses normal initialization.

## Notes

This architectural weakness is acknowledged by the development team via the TODO comment requesting API unification. While the read-before-write convention provides protection under normal operation, the lack of enforcement at the write boundary combined with the non-atomic multi-step write creates a vulnerability surface. The issue is particularly relevant in BlockSTMv2's complex parallel execution environment where speculation, validation failures, and re-execution create numerous edge cases.

### Citations

**File:** aptos-move/block-executor/src/executor.rs (L255-256)
```rust
                        // TODO(BlockSTMv2): After MVHashMap refactoring, expose a single API
                        // for groups handling everything (inner resources, metadata & size).
```

**File:** aptos-move/block-executor/src/executor.rs (L259-276)
```rust
                            versioned_cache.data().write_v2::<true>(
                                group_key.clone(),
                                idx_to_execute,
                                incarnation,
                                TriompheArc::new(group_metadata_op),
                                None,
                            )?,
                        )?;
                        abort_manager.invalidate_dependencies(
                            versioned_cache.group_data().write_v2(
                                group_key,
                                idx_to_execute,
                                incarnation,
                                group_ops.into_iter(),
                                group_size,
                                prev_tags,
                            )?,
                        )?;
```

**File:** aptos-move/mvhashmap/src/versioned_group_data.rs (L261-318)
```rust
    pub fn write_v2(
        &self,
        group_key: K,
        txn_idx: TxnIndex,
        incarnation: Incarnation,
        values: impl IntoIterator<Item = (T, (V, Option<Arc<MoveTypeLayout>>))>,
        size: ResourceGroupSize,
        prev_tags: HashSet<&T>,
    ) -> Result<BTreeMap<TxnIndex, Incarnation>, PanicError> {
        let (_, mut invalidated_dependencies) =
            self.data_write_impl::<true>(&group_key, txn_idx, incarnation, values, prev_tags)?;

        // We write data first, without holding the sizes lock, then write size.
        // Hence when size is observed, values should already be written.
        let mut group_sizes = self.group_sizes.get_mut(&group_key).ok_or_else(|| {
            // Currently, we rely on read-before-write to make sure the group would have
            // been initialized, which would have created an entry in group_sizes. Group
            // being initialized sets up data-structures, such as superset_tags, which
            // is used in write_v2, hence the code invariant error. Note that in read API
            // (fetch_tagged_data) we return Uninitialized / TagNotFound errors, because
            // currently that is a part of expected initialization flow.
            // TODO(BlockSTMv2): when we refactor MVHashMap and group initialization logic,
            // also revisit and address the read-before-write assumption.
            code_invariant_error("Group (sizes) must be initialized to write to")
        })?;

        // In store deps, we compute any read dependencies of txns that, based on the
        // index, would now read the same size but from the new entry created at txn_idx.
        // In other words, reads that can be kept valid, even though they were previously
        // reading an entry by a lower txn index. However, if the size has changed, then
        // those read dependencies will be added to invalidated_dependencies, and the
        // store_deps variable will be empty.
        let store_deps: BTreeMap<TxnIndex, Incarnation> = Self::get_latest_entry(
            &group_sizes.size_entries,
            txn_idx,
            ReadPosition::AfterCurrentTxn,
        )
        .map_or_else(BTreeMap::new, |(_, size_entry)| {
            let new_deps = size_entry.value.dependencies.lock().split_off(txn_idx + 1);

            if size_entry.value.size == size {
                // Validation passed.
                new_deps
            } else {
                invalidated_dependencies.extend(new_deps);
                BTreeMap::new()
            }
        });

        group_sizes.size_entries.insert(
            ShiftedTxnIndex::new(txn_idx),
            SizeEntry::new(SizeAndDependencies::from_size_and_dependencies(
                size, store_deps,
            )),
        );

        Ok(invalidated_dependencies.take())
    }
```
