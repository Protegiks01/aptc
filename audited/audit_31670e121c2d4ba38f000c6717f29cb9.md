# Audit Report

## Title
State KV Shard Pruner Progress Validation Missing - Stale Data Accumulation via Progress Inversion

## Summary
The `StateKvShardPruner::prune()` function lacks validation that `current_progress <= target_version`, allowing backward progress updates when shard progress exceeds metadata progress. This causes permanent skipping of stale entries in the affected version range, leading to unbounded storage accumulation.

## Finding Description

In `StateKvShardPruner::new()`, the shard pruner initializes by reading its stored progress and calling `prune(progress, metadata_progress)` to catch up. However, there is no validation that `progress <= metadata_progress`. [1](#0-0) 

The `prune()` function uses `current_progress` as a seek key to position an iterator at entries with `stale_since_version >= current_progress`, then deletes entries up to `target_version`: [2](#0-1) 

**Vulnerability Trigger**: When `current_progress > target_version` (e.g., shard progress corrupted to `u64::MAX` while `metadata_progress = 1000`):

1. The seek positions at `stale_since_version >= u64::MAX` (iterator at end/invalid)
2. Loop condition `index.stale_since_version > target_version` is immediately true
3. No entries are deleted
4. Progress updates to `target_version` (moving **backwards** from `u64::MAX` to `1000`)
5. Entries with `stale_since_version âˆˆ (1000, u64::MAX)` are permanently skipped

The schema uses `stale_since_version` as the first 8 bytes of the key in big-endian format: [3](#0-2) 

The `SeekKeyCodec` for `Version` only encodes 8 bytes, creating a prefix seek: [4](#0-3) 

This allows seeking to positions beyond all valid entries when using boundary values.

**Preconditions for Exploitation**:
- Shard progress must be set to a value > metadata progress
- This can occur through: database corruption (hardware failure), inconsistent backup restoration, or bugs in progress tracking elsewhere in the codebase

**For `current_progress = 0`**: Works correctly - seeks to beginning, no vulnerability.

**For `current_progress = u64::MAX`**: When `target_version < u64::MAX`, causes the described issue.

## Impact Explanation

This qualifies as **Medium Severity** under "State inconsistencies requiring intervention":

1. **Storage Bloat**: Stale entries accumulate indefinitely, never cleaned up
2. **Performance Degradation**: Database scans must traverse unprune entries
3. **Resource Exhaustion**: Disk usage grows without bound over time
4. **Silent Failure**: Pruner believes work is complete (progress updated), masking the problem
5. **Persistent State**: Skipped range is never revisited in subsequent prune operations

The issue does NOT:
- Affect consensus (state roots remain correct)
- Cause funds loss or theft
- Break liveness (nodes continue operating)
- Enable direct attacker exploitation (requires pre-existing corruption/bug)

However, it breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable" by leaving the database in an inconsistent state where pruning invariants are violated.

## Likelihood Explanation

**Likelihood: Low-Medium**

This vulnerability requires an abnormal precondition (`progress > metadata_progress`) which can occur through:

1. **Hardware/Disk Corruption**: Bit flips, partial writes during crashes
2. **Backup/Restore Operations**: Restoring shards from newer backups than metadata
3. **Bugs in Progress Tracking**: Other code paths that incorrectly update shard progress
4. **Manual Database Manipulation**: Operator errors during maintenance

While not directly triggerable by an external attacker, these scenarios are realistic in production blockchain deployments running continuously with multiple database shards.

## Recommendation

Add validation before calling `prune()` in the initialization path to ensure progress never exceeds the target:

```rust
pub(in crate::pruner) fn new(
    shard_id: usize,
    db_shard: Arc<DB>,
    metadata_progress: Version,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        &db_shard,
        &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
        metadata_progress,
    )?;
    let myself = Self { shard_id, db_shard };

    // Add validation and correction
    let actual_progress = if progress > metadata_progress {
        info!(
            shard_id = shard_id,
            stored_progress = progress,
            metadata_progress = metadata_progress,
            "Detected shard progress ahead of metadata progress. Resetting to metadata progress."
        );
        // Reset to metadata progress and re-initialize
        db_shard.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            &DbMetadataValue::Version(metadata_progress),
        )?;
        metadata_progress
    } else {
        progress
    };

    info!(
        progress = actual_progress,
        metadata_progress = metadata_progress,
        "Catching up state kv shard {shard_id}."
    );
    myself.prune(actual_progress, metadata_progress)?;

    Ok(myself)
}
```

Additionally, add defensive assertion in `prune()`:

```rust
pub(in crate::pruner) fn prune(
    &self,
    current_progress: Version,
    target_version: Version,
) -> Result<()> {
    // Defensive check
    if current_progress > target_version {
        return Err(anyhow::anyhow!(
            "Invalid prune parameters: current_progress ({}) > target_version ({})",
            current_progress,
            target_version
        ));
    }
    // ... rest of function
}
```

## Proof of Concept

```rust
#[test]
fn test_prune_with_progress_ahead_of_target() {
    // Setup: Create test database with entries
    let tmp_dir = TempPath::new();
    let db = DB::open(
        tmp_dir.path(),
        "test",
        vec![STALE_STATE_VALUE_INDEX_BY_KEY_HASH_CF_NAME],
        &default_db_opts(),
    ).unwrap();
    
    // Insert test entries with various stale_since_version values
    let mut batch = SchemaBatch::new();
    for version in 1..=1000 {
        let index = StaleStateValueByKeyHashIndex {
            stale_since_version: version,
            version: version - 1,
            state_key_hash: HashValue::random(),
        };
        batch.put::<StaleStateValueIndexByKeyHashSchema>(&index, &()).unwrap();
    }
    db.write_schemas(batch).unwrap();
    
    // Verify entries exist
    let count_before = db.iter::<StaleStateValueIndexByKeyHashSchema>()
        .unwrap()
        .count();
    assert_eq!(count_before, 1000);
    
    // Simulate corrupted progress: shard progress = u64::MAX
    db.put::<DbMetadataSchema>(
        &DbMetadataKey::StateKvShardPrunerProgress(0),
        &DbMetadataValue::Version(u64::MAX),
    ).unwrap();
    
    // Create pruner with metadata_progress = 500
    // This should prune entries 1..=500, but due to bug, it won't
    let pruner = StateKvShardPruner::new(0, Arc::new(db), 500).unwrap();
    
    // Verify bug: entries were NOT pruned despite progress being updated to 500
    let count_after = pruner.db_shard
        .iter::<StaleStateValueIndexByKeyHashSchema>()
        .unwrap()
        .count();
    
    // BUG: All 1000 entries still exist (should be 500)
    assert_eq!(count_after, 1000, "Entries were not pruned due to progress inversion");
    
    // Verify progress was incorrectly updated to 500
    let progress = pruner.db_shard
        .get::<DbMetadataSchema>(&DbMetadataKey::StateKvShardPrunerProgress(0))
        .unwrap()
        .unwrap()
        .expect_version();
    assert_eq!(progress, 500);
    
    // These entries will NEVER be pruned in subsequent operations
    // because progress is now 500 and future calls will be prune(500, 600), prune(600, 700), etc.
}
```

## Notes

- The vulnerability is specific to the **initialization path** in `StateKvShardPruner::new()` at line 42, not the regular pruning path which has proper invariants maintained by the parent `StateKvPruner`
- The regular pruning loop in the parent ensures `progress < target_version` via the while loop condition [5](#0-4) 
- Similar validation should be added to `StateMerkleShardPruner` which has the same initialization pattern
- The issue is not a "silent failure" of the seek operation itself (RocksDB behaves correctly), but rather a semantic logic error in how boundary conditions are handled

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L30-42)
```rust
        let progress = get_or_initialize_subpruner_progress(
            &db_shard,
            &DbMetadataKey::StateKvShardPrunerProgress(shard_id),
            metadata_progress,
        )?;
        let myself = Self { shard_id, db_shard };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up state kv shard {shard_id}."
        );
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L54-69)
```rust
        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;
```

**File:** storage/aptosdb/src/schema/stale_state_value_index_by_key_hash/mod.rs (L39-47)
```rust
impl KeyCodec<StaleStateValueIndexByKeyHashSchema> for StaleStateValueByKeyHashIndex {
    fn encode_key(&self) -> Result<Vec<u8>> {
        let mut encoded = vec![];
        encoded.write_u64::<BigEndian>(self.stale_since_version)?;
        encoded.write_u64::<BigEndian>(self.version)?;
        encoded.write_all(self.state_key_hash.as_ref())?;

        Ok(encoded)
    }
```

**File:** storage/aptosdb/src/schema/stale_state_value_index_by_key_hash/mod.rs (L76-80)
```rust
impl SeekKeyCodec<StaleStateValueIndexByKeyHashSchema> for Version {
    fn encode_seek_key(&self) -> Result<Vec<u8>> {
        Ok(self.to_be_bytes().to_vec())
    }
}
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L55-57)
```rust
        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);
```
