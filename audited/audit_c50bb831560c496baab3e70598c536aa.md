# Audit Report

## Title
Sequential Timeout Multiplication in Indexer gRPC Data Service Enables Resource Exhaustion

## Summary
The `channel_send_multiple_with_timeout()` function in the indexer-grpc-data-service applies a 120-second timeout sequentially to each chunk when sending transaction data to clients. A slow or malicious client can force the server to wait up to 120 seconds per chunk, and with multiple chunks per batch, this creates a multiplicative resource exhaustion attack where a single request can hold server resources for extended periods (potentially 10+ minutes).

## Finding Description

The vulnerability exists in the data streaming mechanism of the indexer-grpc-data-service. When serving transaction data to clients, the service:

1. Fetches transaction data in batches (up to 5,000 transactions per fetch cycle in live mode) [1](#0-0) 

2. Chunks the transactions into multiple response messages limited by MESSAGE_SIZE_LIMIT (15MB) [2](#0-1) 

3. Sends each chunk sequentially with a 120-second timeout [3](#0-2) 

The critical flaw is in `channel_send_multiple_with_timeout()` which loops through all chunks and applies the timeout to each one sequentially: [4](#0-3) 

The response channel has a default buffer size of only 3 messages [5](#0-4) , so when a client is slow to consume:

- The first 3 chunks fill the buffer immediately
- Starting from the 4th chunk, each `send_timeout()` call can block for up to 120 seconds
- With N chunks, total blocking time = (N - 3) Ã— 120 seconds in worst case

**Attack Path:**
1. Attacker connects to indexer-grpc-data-service
2. Requests transactions without specifying count (live mode triggers MAX_FETCH_TASKS_PER_REQUEST) [6](#0-5) 
3. Server fetches ~5,000 transactions which may chunk into 10+ response messages
4. Attacker intentionally consumes data slowly (but responds to HTTP2 keepalives to avoid disconnection)
5. Server blocks for up to 1,200+ seconds (20 minutes) waiting to send all chunks
6. Multiple concurrent slow clients exhaust server resources (spawned tasks, Redis connections, memory)

Each client connection spawns a dedicated tokio task that holds resources throughout the blocking period [7](#0-6) 

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program's "API crashes" category. The resource exhaustion can:

- **Crash or freeze the indexer-grpc-data-service API**: Multiple slow clients can exhaust the tokio task pool, causing the service to become unresponsive
- **Denial of service for legitimate indexers**: Block explorers, analytics platforms, and other services depending on this API are impacted
- **Resource starvation**: Each blocked connection holds Redis connections, file store references, and memory for transaction data

While this does not directly affect blockchain consensus or validator operations, the indexer-grpc-data-service is critical infrastructure for the Aptos ecosystem, providing transaction data to numerous downstream services.

## Likelihood Explanation

**Likelihood: High**

The attack is trivial to execute:
- No authentication bypass required (legitimate clients can exploit this)
- No special privileges needed
- Attack can be automated with a simple gRPC client
- Multiple attackers can amplify the effect
- HTTP2 keepalive settings (60s ping interval, 10s timeout) don't prevent the attack since clients can respond to pings while being slow on the data stream [8](#0-7) 

## Recommendation

**Fix 1: Implement per-connection total timeout instead of per-chunk timeout**

Replace the sequential per-chunk timeout with a single overall timeout for the entire multi-chunk send operation. Track elapsed time across all chunks and fail fast if exceeded.

**Fix 2: Add dynamic backpressure mechanism**

Instead of blindly waiting 120 seconds per chunk, implement exponential backoff or a shorter initial timeout that increases only if the client demonstrates good consumption rates.

**Fix 3: Implement connection-level rate limiting**

Add per-client connection limits and track consumption rates, disconnecting clients that fall below minimum thresholds.

**Fix 4: Reduce RESPONSE_CHANNEL_SEND_TIMEOUT**

120 seconds is excessive. Consider reducing to 10-30 seconds with a maximum total wait time across all chunks.

## Proof of Concept

```rust
// Malicious Client PoC (pseudocode)
use aptos_protos::indexer::v1::raw_data_client::RawDataClient;
use aptos_protos::indexer::v1::GetTransactionsRequest;

#[tokio::main]
async fn main() {
    // Connect to indexer-grpc-data-service
    let mut client = RawDataClient::connect("http://indexer-grpc.example.com:50051")
        .await
        .unwrap();
    
    // Request transactions in live mode (no count specified)
    let request = GetTransactionsRequest {
        starting_version: None, // Live mode
        transactions_count: None, // No limit - triggers MAX_FETCH_TASKS_PER_REQUEST
        batch_size: None,
    };
    
    let mut stream = client.get_transactions(request).await.unwrap().into_inner();
    
    // Consume data extremely slowly
    while let Some(response) = stream.message().await.unwrap() {
        println!("Received {} transactions", response.transactions.len());
        
        // Sleep for long periods between consuming chunks
        // This forces server to wait at send_timeout for each chunk
        tokio::time::sleep(tokio::time::Duration::from_secs(110)).await;
        
        // Still respond to HTTP2 keepalive pings to avoid disconnection
    }
}

// Amplification: Run 10+ concurrent instances to exhaust server resources
```

**Test scenario:**
1. Deploy indexer-grpc-data-service with default configuration
2. Run 10 concurrent instances of the malicious client
3. Observe server resource exhaustion (CPU, memory, connection pool)
4. Legitimate clients experience service degradation or timeouts
5. Service becomes unresponsive after several minutes

**Notes**

This vulnerability is specific to the indexer-grpc-data-service infrastructure component, not the core blockchain consensus layer. However, it represents a significant availability risk for the Aptos ecosystem's data infrastructure. The sequential timeout multiplication is the root cause - what should be a bounded operation (sending data to a client) becomes unbounded when multiple chunks are involved.

The issue is exacerbated by the small channel buffer size (3 messages) combined with the large timeout value (120 seconds) and potential for many chunks per batch when fetching thousands of transactions in live mode.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L60-60)
```rust
const RESPONSE_CHANNEL_SEND_TIMEOUT: Duration = Duration::from_secs(120);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L192-208)
```rust
        tokio::spawn({
            let request_metadata = request_metadata.clone();
            async move {
                data_fetcher_task(
                    redis_client,
                    file_store_operator,
                    cache_storage_format,
                    request_metadata,
                    transactions_count,
                    tx,
                    txns_to_strip_filter,
                    current_version,
                    in_memory_cache,
                )
                .await;
            }
        });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L265-270)
```rust
        Ok(CacheCoverageStatus::CacheEvicted) => match transactions_count {
            None => MAX_FETCH_TASKS_PER_REQUEST,
            Some(transactions_count) => {
                let num_tasks = transactions_count / TRANSACTIONS_PER_STORAGE_BLOCK;
                num_tasks.clamp(1, MAX_FETCH_TASKS_PER_REQUEST)
            },
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L862-887)
```rust
    for resp_item in resp_items {
        let send_start_time = Instant::now();
        let response_size = resp_item.encoded_len();
        let num_of_transactions = resp_item.transactions.len();
        let start_version = resp_item.transactions.first().unwrap().version;
        let end_version = resp_item.transactions.last().unwrap().version;
        let start_version_txn_timestamp = resp_item
            .transactions
            .first()
            .unwrap()
            .timestamp
            .as_ref()
            .unwrap();
        let end_version_txn_timestamp = resp_item
            .transactions
            .last()
            .unwrap()
            .timestamp
            .as_ref()
            .unwrap();

        tx.send_timeout(
            Result::<TransactionsResponse, Status>::Ok(resp_item.clone()),
            RESPONSE_CHANNEL_SEND_TIMEOUT,
        )
        .await?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/constants.rs (L19-19)
```rust
pub const MESSAGE_SIZE_LIMIT: usize = 1024 * 1024 * 15;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/config.rs (L24-24)
```rust
const DEFAULT_MAX_RESPONSE_CHANNEL_SIZE: usize = 3;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/config.rs (L29-30)
```rust
const HTTP2_PING_INTERVAL_DURATION: std::time::Duration = std::time::Duration::from_secs(60);
const HTTP2_PING_TIMEOUT_DURATION: std::time::Duration = std::time::Duration::from_secs(10);
```
