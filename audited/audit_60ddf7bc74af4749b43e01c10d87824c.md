# Audit Report

## Title
Consensus Safety Violation via Non-Deterministic Block Execution Failures with `discard_failed_blocks=true`

## Summary
The `discard_failed_blocks` configuration field introduces a critical consensus safety vulnerability when enabled. If BlockSTM parallel execution failures are non-deterministic (due to race conditions or hardware differences), validators with `discard_failed_blocks=true` will compute different state roots for the same block, violating the Deterministic Execution invariant and enabling consensus splits.

## Finding Description

The vulnerability exists in the interaction between the `discard_failed_blocks` configuration and the BlockSTM parallel execution engine's error handling.

**Configuration Location:** [1](#0-0) 

**Critical Error Handling Code:** [2](#0-1) 

When `discard_failed_blocks=true`, if block execution fails after both parallel and sequential fallback attempts, the executor does not return an error. Instead, it creates "discarded" outputs for all transactions and returns success.

**The Attack Path:**

1. A transaction is submitted that triggers non-deterministic behavior in BlockSTM's parallel execution (e.g., race conditions in multi-version data structures, incarnation limit timing issues)

2. All validators receive and order the same block containing this transaction

3. During execution:
   - **Validator Group A**: The race condition does NOT trigger, block executes successfully, all transactions committed
   - **Validator Group B**: The race condition DOES trigger, execution fails, all transactions discarded

4. With `discard_failed_blocks=true`:
   - Group A: Computes `executed_state_id_A` reflecting all committed transactions
   - Group B: Computes `executed_state_id_B` with no state changes (discarded transactions don't modify state)

**State Root Divergence:** [3](#0-2) 

Only committed transactions (`to_commit`) affect the state root. Discarded transactions (`to_discard`) are filtered out before state computation.

5. **Consensus Protocol Violation**:
Both groups sign commit votes with different `executed_state_id` values: [4](#0-3) 

The `CommitVote` contains a `LedgerInfo` which includes the `executed_state_id` (transaction accumulator root hash) that validators sign.

6. **Safety Rule Check Bypassed**: [5](#0-4) 

The `match_ordered_only` check only validates epoch, round, id, and timestampâ€”NOT the `executed_state_id`. This allows conflicting signatures to pass safety validation.

**Evidence of Non-Determinism:**

The codebase acknowledges parallel execution issues: [6](#0-5) 

The `IncarnationTooHigh` error is explicitly timing-dependent and can cause fallback to sequential execution.

**Current Default Protection:** [7](#0-6) 

With `discard_failed_blocks=false` (default), execution errors cause the validator to log the error and **not advance the block**, preventing the validator from signing commit votes. This creates liveness issues but preserves safety.

## Impact Explanation

**Severity: Critical** (Consensus/Safety Violation - up to $1,000,000)

This vulnerability directly breaks **Critical Invariant #1**: "Deterministic Execution: All validators must produce identical state roots for identical blocks."

**Concrete Impact:**
- Validators compute different state roots for the same ordered block
- Conflicting quorum certificates can be formed
- Blockchain state divergence across validator sets
- Potential chain split requiring hard fork to resolve
- Loss of finality guarantees
- Double-spend opportunities if different validator sets commit conflicting transactions

This meets the "Critical Severity" criteria per the Aptos bug bounty program: **Consensus/Safety violations** and **Non-recoverable network partition (requires hardfork)**.

## Likelihood Explanation

**Current Likelihood: Low** (default is `discard_failed_blocks=false`)

However, the likelihood increases significantly if:
1. Network operators enable `discard_failed_blocks=true` to handle known BlockSTM bugs
2. Testnet or devnet configurations use this setting for resilience
3. Per-network configuration is added (as the security question suggests)

**Triggering Conditions:**
- Requires BlockSTM to have non-deterministic failure modes (race conditions)
- Requires a transaction that triggers such failures
- Could be triggered accidentally by legitimate transactions under specific timing/hardware conditions
- Could be weaponized by an attacker who discovers reproducible trigger conditions

The code comment at line 2650 suggests developers are aware of issues: [8](#0-7) 

## Recommendation

**Primary Fix: Remove the `discard_failed_blocks` feature entirely**

Block execution failures indicate serious bugs that should halt the validator for investigation, not be masked by discarding transactions.

**Alternative Fix: Make discarding deterministic and verifiable**

If the feature must be retained:

1. **Ensure deterministic failure detection**: Add a deterministic pre-execution check that ALL validators perform identically before block execution
2. **Cryptographic proof of failure**: Require validators to produce a verifiable proof of WHY execution failed
3. **Consensus on failure**: Before discarding, validators should reach consensus on the failure itself
4. **State root verification**: Add explicit checks that all validators computed the same state root, even for discarded blocks

**Code Fix Example:**

```rust
// In executor.rs, line 2648-2665
if self.config.local.discard_failed_blocks {
    // SECURITY: We should NOT silently discard blocks.
    // Execution failures indicate bugs that require investigation.
    // Return error to halt the validator instead of risking consensus divergence.
    warn!("discard_failed_blocks is enabled but execution failed - this risks consensus safety");
    return Err(sequential_error);
}
```

**Configuration Recommendation:**

Never enable `discard_failed_blocks=true` in production. The current default of `false` is correct from a safety perspective. For testnets, consider using separate failure handling with explicit validator coordination rather than silent discarding.

## Proof of Concept

Due to the complexity of reproducing race conditions in BlockSTM, a full working PoC requires significant setup. However, the vulnerability can be demonstrated conceptually:

```rust
// Theoretical PoC demonstrating the divergence

// Step 1: Submit transaction that triggers race condition
// (exact trigger depends on specific BlockSTM race condition)

// Step 2: Two validators execute same block
let validator_a_result = execute_block_with_config(
    block.clone(),
    config_with_discard_enabled,
);
let validator_b_result = execute_block_with_config(
    block.clone(), 
    config_with_discard_enabled,
);

// Step 3: Non-deterministic failure occurs
// Validator A: race condition doesn't trigger
assert!(validator_a_result.is_ok());
let state_root_a = validator_a_result.unwrap().executed_state_id;

// Validator B: race condition triggers, block discarded  
assert!(validator_b_result.is_ok()); // Returns Ok with discarded transactions
let state_root_b = validator_b_result.unwrap().executed_state_id;

// Step 4: Consensus safety violated
assert_ne!(state_root_a, state_root_b); // DIFFERENT STATE ROOTS!

// Both validators sign commit votes with different state roots
// Leading to consensus divergence
```

**Reproduction Steps:**
1. Enable `discard_failed_blocks=true` in validator config
2. Deploy transaction with high parallelism that stresses BlockSTM
3. Run multiple validators with different hardware/timing characteristics  
4. Observe non-deterministic execution failures on some validators
5. Verify different state roots are computed and signed
6. Observe consensus split

## Notes

The security question asks whether `discard_failed_blocks` should be "configurable per-network to handle different threat models." The answer is: **No configuration is safe that enables this feature**, because it fundamentally breaks the deterministic execution guarantee that consensus depends on.

The current default (`false`) is the correct safety-first approach, even though it causes liveness issues when execution bugs occur. The proper solution is to fix execution bugs, not to mask them by discarding transactions non-deterministically.

### Citations

**File:** config/src/config/execution_config.rs (L88-88)
```rust
            discard_failed_blocks: false,
```

**File:** aptos-move/block-executor/src/executor.rs (L2648-2665)
```rust
        if self.config.local.discard_failed_blocks {
            // We cannot execute block, discard everything (including block metadata and validator transactions)
            // (TODO: maybe we should add fallback here to first try BlockMetadataTransaction alone)
            let error_code = match sequential_error {
                BlockExecutionError::FatalBlockExecutorError(_) => {
                    StatusCode::DELAYED_FIELD_OR_BLOCKSTM_CODE_INVARIANT_ERROR
                },
                BlockExecutionError::FatalVMError(_) => {
                    StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR
                },
            };
            let ret = (0..signature_verified_block.num_txns())
                .map(|_| E::Output::discard_output(error_code))
                .collect();
            return Ok(BlockOutput::new(ret, None));
        }

        Err(sequential_error)
```

**File:** execution/executor-types/src/execution_output.rs (L46-46)
```rust
        let next_version = first_version + to_commit.len() as Version;
```

**File:** consensus/consensus-types/src/pipeline/commit_vote.rs (L1-10)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::common::{Author, Round};
use anyhow::{ensure, Context};
use aptos_crypto::{bls12381, CryptoMaterialError};
use aptos_short_hex_str::AsShortHexStr;
use aptos_types::{
    block_info::BlockInfo,
    ledger_info::{LedgerInfo, SignatureWithStatus},
```

**File:** consensus/safety-rules/src/safety_rules.rs (L395-403)
```rust
        if !old_ledger_info
            .commit_info()
            .match_ordered_only(new_ledger_info.commit_info())
        {
            return Err(Error::InconsistentExecutionResult(
                old_ledger_info.commit_info().to_string(),
                new_ledger_info.commit_info().to_string(),
            ));
        }
```

**File:** aptos-move/block-executor/src/errors.rs (L10-13)
```rust
    // Incarnation number that is higher than a threshold is observed during parallel execution.
    // This might be indicative of some sort of livelock, or at least some sort of inefficiency
    // that would warrants investigating the root cause. Execution can fallback to sequential.
    IncarnationTooHigh,
```

**File:** consensus/src/pipeline/buffer_manager.rs (L617-626)
```rust
        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
```
