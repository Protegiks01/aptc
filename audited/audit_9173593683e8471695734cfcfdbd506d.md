# Audit Report

## Title
Non-Atomic State Modification with Verification Bypass Leading to Irrecoverable Node Liveness Failure

## Summary
The `update_ledger()` function in the chunk executor modifies critical in-memory state **before** performing verification. When verification fails, there is no rollback mechanism, leaving the commit queue in a corrupted state that prevents all future chunk processing, causing permanent node liveness failure until manual restart.

## Finding Description

The vulnerability exists in the chunk processing pipeline where state modifications occur non-atomically across the `enqueue_chunk()` and `update_ledger()` execution phases.

**Critical Code Path:**

1. **State Modification Before Verification** - In `enqueue_chunk()`, after chunk execution/application, the commit queue's `latest_state` is immediately updated: [1](#0-0) 

This happens **before any verification** of the chunk results.

2. **Verification Point** - Later, when `update_ledger()` is called, verification occurs at line 365: [2](#0-1) 

However, before this verification, the chunk is already taken from the queue (replaced with `None`): [3](#0-2) 

3. **No Rollback on Failure** - If verification fails at line 365, the function returns early via the `?` operator, and `save_ledger_update_output()` is never called: [4](#0-3) 

The `save_ledger_update_output()` function is responsible for:
- Updating `latest_state_summary` and `latest_txn_accumulator` to match `latest_state`
- Removing the `None` entry from the `to_update_ledger` queue
- Moving the chunk to the `to_commit` queue [5](#0-4) 

**Resulting State Corruption:**

After verification failure:
- `latest_state` = includes failed chunk's state changes (at version N+k)
- `latest_state_summary` = does NOT include failed chunk (at version N)
- `latest_txn_accumulator` = does NOT include failed chunk (at version N)
- `to_update_ledger` queue = has `None` entry blocking all future operations

**Exploitation Path:**

A malicious state sync peer can:
1. Send a chunk with valid transactions but crafted invalid `TransactionInfoListWithProof` (e.g., wrong transaction info hashes, invalid accumulator proof)
2. The victim node executes the chunk via `enqueue_chunk()`, updating `latest_state`
3. When `update_ledger()` is called, verification fails at line 365 because the proof doesn't match
4. The node's commit queue is now permanently corrupted
5. Any subsequent call to `update_ledger()` fails with "Next chunk to update ledger has already been processed"
6. The node cannot process any more chunks and must restart

**Invariant Violations:**

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs". The state transition is non-atomic - `latest_state` is modified but the corresponding summary and accumulator updates are conditional on verification success, without rollback on failure.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Node Complete Failure** - The affected node cannot process any chunks after the attack, which is more severe than "validator node slowdowns" but is a complete operational failure of the chunk executor component.

2. **Remote Exploitability** - Any malicious state sync peer can trigger this without authentication or special privileges, simply by sending chunks with invalid proofs.

3. **Requires Manual Intervention** - Recovery requires a full node restart, as there is no automatic cleanup mechanism. The in-memory corruption persists until the `ChunkExecutorInner` is reinitialized via `reset()`.

4. **Potential Consensus Impact** - If multiple validators are simultaneously state syncing from malicious peers during network partitions or after downtime, this could disrupt consensus by taking validators offline.

5. **State Inconsistency** - The three-way mismatch between `latest_state`, `latest_state_summary`, and `latest_txn_accumulator` violates critical system invariants and could lead to incorrect execution of subsequent operations before the node crashes or is restarted.

While this doesn't directly cause "Total loss of liveness/network availability" (Critical severity) network-wide, it represents a "Significant protocol violation" and causes individual validator failures that could cascade to consensus disruption.

## Likelihood Explanation

**High Likelihood** for the following reasons:

1. **Common Attack Vector** - State sync is a standard operation where nodes request chunks from peers. Malicious peers are a realistic threat model in any peer-to-peer blockchain network.

2. **Easy to Trigger** - The attacker simply needs to:
   - Run a state sync server
   - Send chunks with valid transaction data but invalid proofs (wrong hashes)
   - The verification will deterministically fail

3. **No Authentication Required** - State sync peers don't need special authentication to serve chunks. Any node can act as a state sync source.

4. **Affects State Sync Operation** - This is triggered during state sync, which occurs when:
   - New nodes join the network
   - Validators recover from downtime
   - Nodes fall behind and need to catch up
   These are regular operational scenarios.

## Recommendation

**Implement atomic state updates with rollback capability:**

The fix requires ensuring that `latest_state` is only updated **after** verification succeeds, or implementing proper rollback when verification fails.

**Option 1: Defer State Update (Recommended)**

Modify the commit queue to not update `latest_state` in `enqueue_for_ledger_update()`. Instead, update all three state components (`latest_state`, `latest_state_summary`, `latest_txn_accumulator`) atomically in `save_ledger_update_output()` after successful verification.

```rust
// In chunk_commit_queue.rs
pub(crate) fn enqueue_for_ledger_update(
    &mut self,
    chunk_to_update_ledger: ChunkToUpdateLedger,
) -> Result<()> {
    // REMOVE this line - don't update state before verification:
    // self.latest_state = chunk_to_update_ledger.output.result_state().clone();
    
    self.to_update_ledger.push_back(Some(chunk_to_update_ledger));
    Ok(())
}

pub(crate) fn save_ledger_update_output(&mut self, chunk: ExecutedChunk) -> Result<()> {
    ensure!(!self.to_update_ledger.is_empty(), "to_update_ledger is empty.");
    ensure!(
        self.to_update_ledger.front().unwrap().is_none(),
        "Head of to_update_ledger has not been processed."
    );
    
    // Update ALL state components atomically after verification:
    self.latest_state = chunk.output.execution_output.result_state.clone();
    self.latest_state_summary = chunk.output.ensure_state_checkpoint_output()?.state_summary.clone();
    self.latest_txn_accumulator = chunk.output.ensure_ledger_update_output()?.transaction_accumulator.clone();
    
    self.to_update_ledger.pop_front();
    self.to_commit.push_back(Some(chunk));
    Ok(())
}
```

**Option 2: Implement Rollback on Verification Failure**

Add error handling in `update_ledger()` to restore the previous `latest_state` value if verification fails:

```rust
pub fn update_ledger(&self) -> Result<()> {
    let mut queue = self.commit_queue.lock();
    let previous_state = queue.latest_state().clone(); // Backup state
    let (parent_state_summary, parent_accumulator, chunk) =
        queue.next_chunk_to_update_ledger()?;
    drop(queue); // Release lock
    
    // ... DoStateCheckpoint, DoLedgerUpdate ...
    
    // Verify - if this fails, rollback
    if let Err(e) = chunk_verifier.verify_chunk_result(&parent_accumulator, &ledger_update_output) {
        // Rollback: restore previous state and cleanup queue
        let mut queue = self.commit_queue.lock();
        queue.rollback_failed_verification(previous_state)?;
        return Err(e);
    }
    
    // ... rest of function ...
}
```

This would require adding a `rollback_failed_verification()` method to `ChunkCommitQueue` that restores state and cleans up the queue.

## Proof of Concept

```rust
// This test demonstrates the vulnerability
// File: execution/executor/src/chunk_executor/tests/verification_failure_test.rs

#[cfg(test)]
mod verification_failure_tests {
    use super::*;
    use aptos_crypto::{hash::CryptoHash, HashValue};
    use aptos_storage_interface::DbReaderWriter;
    use aptos_temppath::TempPath;
    use aptos_types::{
        ledger_info::LedgerInfo,
        proof::TransactionInfoListWithProof,
        transaction::{Transaction, TransactionInfo},
    };

    #[test]
    fn test_state_corruption_on_verification_failure() {
        // Setup: Initialize chunk executor with empty database
        let tmp_dir = TempPath::new();
        let db = DbReaderWriter::new(AptosDB::new_for_test(&tmp_dir));
        let executor = ChunkExecutor::<AptosVM>::new(db.clone());
        executor.reset().unwrap();

        // Step 1: Create a valid chunk with transactions
        let transactions = vec![Transaction::dummy()]; // Use appropriate test transaction
        let first_version = 0;
        
        // Step 2: Create INVALID proof (wrong transaction info hash)
        let correct_txn_info = TransactionInfo::new(
            transactions[0].hash(),
            HashValue::zero(),
            HashValue::zero(),
            None,
            0,
            ExecutionStatus::Success,
            None,
        );
        
        // Create WRONG transaction info for verification failure
        let wrong_txn_info = TransactionInfo::new(
            HashValue::random(), // WRONG HASH!
            HashValue::zero(),
            HashValue::zero(),
            None,
            0,
            ExecutionStatus::Success,
            None,
        );
        
        let txn_infos_with_proof = TransactionInfoListWithProof::new(
            vec![wrong_txn_info],
            None,
        );
        
        let target_li = create_test_ledger_info(first_version);
        let txn_list_with_proof = create_test_transaction_list_with_proof(
            transactions,
            first_version,
            txn_infos_with_proof,
        );
        
        // Step 3: Enqueue chunk - this will execute and update latest_state
        let result = executor.enqueue_chunk_by_execution(
            txn_list_with_proof,
            &target_li,
            None,
        );
        assert!(result.is_ok(), "Enqueue should succeed");
        
        // Step 4: Call update_ledger - this will FAIL verification
        let update_result = executor.update_ledger();
        assert!(update_result.is_err(), "Update ledger should fail due to bad proof");
        
        // Step 5: Verify state corruption
        // The commit queue now has:
        // - latest_state at version 1 (chunk was applied)
        // - latest_state_summary at version 0 (not updated due to failed verification)
        // - to_update_ledger with None entry blocking further processing
        
        // Try to call update_ledger again - should fail with "already processed" error
        let retry_result = executor.update_ledger();
        assert!(retry_result.is_err());
        assert!(retry_result.unwrap_err().to_string().contains("already been processed"));
        
        // Node is now stuck and cannot process any more chunks
        // The only recovery is to call reset() which reinitializes from database
    }
}
```

**Demonstration Steps:**

1. Deploy a malicious state sync peer
2. Have the peer serve chunks with correct transactions but incorrect `TransactionInfoListWithProof` (wrong hashes)
3. Target nodes will execute chunks via `enqueue_chunk_by_execution()`
4. When `update_ledger()` is called, verification at line 365 fails
5. Nodes' commit queues become corrupted with inconsistent state
6. Nodes cannot process further chunks - **complete liveness failure**
7. Nodes require restart to recover, during which they're offline

**Notes**

The vulnerability stems from a design flaw where the **commit queue state machine allows state updates before verification**. Specifically:

1. The `latest_state` field in `ChunkCommitQueue` is updated eagerly in `enqueue_for_ledger_update()` [6](#0-5)  which happens during `enqueue_chunk()`, before any verification.

2. The corresponding `latest_state_summary` and `latest_txn_accumulator` are only updated in `save_ledger_update_output()` [7](#0-6)  which happens **after** successful verification in `update_ledger()`.

3. When verification fails, `next_chunk_to_update_ledger()` has already mutated the queue by replacing the chunk with `None` [8](#0-7) , but the cleanup that removes this `None` entry only happens in `save_ledger_update_output()` [9](#0-8) .

This creates a permanent state inconsistency where:
- The queue has a `None` entry blocking all future operations
- `latest_state` diverges from `latest_state_summary` by the failed chunk's version range
- No automatic recovery mechanism exists

The error propagation in `with_inner()` only panics if there's a pending database pre-commit, but doesn't handle in-memory queue corruption: [10](#0-9) 

This violates the **State Consistency** invariant requiring atomic, verifiable state transitions, and causes **total node liveness failure** for affected validators.

### Citations

**File:** execution/executor/src/chunk_executor/chunk_commit_queue.rs (L73-83)
```rust
    pub(crate) fn enqueue_for_ledger_update(
        &mut self,
        chunk_to_update_ledger: ChunkToUpdateLedger,
    ) -> Result<()> {
        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["enqueue_for_ledger_update"]);

        self.latest_state = chunk_to_update_ledger.output.result_state().clone();
        self.to_update_ledger
            .push_back(Some(chunk_to_update_ledger));
        Ok(())
    }
```

**File:** execution/executor/src/chunk_executor/chunk_commit_queue.rs (L85-104)
```rust
    pub(crate) fn next_chunk_to_update_ledger(
        &mut self,
    ) -> Result<(
        LedgerStateSummary,
        Arc<InMemoryTransactionAccumulator>,
        ChunkToUpdateLedger,
    )> {
        let chunk_opt = self
            .to_update_ledger
            .front_mut()
            .ok_or_else(|| anyhow!("No chunk to update ledger."))?;
        let chunk = chunk_opt
            .take()
            .ok_or_else(|| anyhow!("Next chunk to update ledger has already been processed."))?;
        Ok((
            self.latest_state_summary.clone(),
            self.latest_txn_accumulator.clone(),
            chunk,
        ))
    }
```

**File:** execution/executor/src/chunk_executor/chunk_commit_queue.rs (L106-131)
```rust
    pub(crate) fn save_ledger_update_output(&mut self, chunk: ExecutedChunk) -> Result<()> {
        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["save_ledger_update_output"]);

        ensure!(
            !self.to_update_ledger.is_empty(),
            "to_update_ledger is empty."
        );
        ensure!(
            self.to_update_ledger.front().unwrap().is_none(),
            "Head of to_update_ledger has not been processed."
        );
        self.latest_state_summary = chunk
            .output
            .ensure_state_checkpoint_output()?
            .state_summary
            .clone();
        self.latest_txn_accumulator = chunk
            .output
            .ensure_ledger_update_output()?
            .transaction_accumulator
            .clone();
        self.to_update_ledger.pop_front();
        self.to_commit.push_back(Some(chunk));

        Ok(())
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L96-105)
```rust
        let has_pending_pre_commit = inner.has_pending_pre_commit.load(Ordering::Acquire);
        f(inner).map_err(|error| {
            if has_pending_pre_commit {
                panic!(
                    "Hit error with pending pre-committed ledger, panicking. {:?}",
                    error,
                );
            }
            error
        })
```

**File:** execution/executor/src/chunk_executor/mod.rs (L365-365)
```rust
        chunk_verifier.verify_chunk_result(&parent_accumulator, &ledger_update_output)?;
```

**File:** execution/executor/src/chunk_executor/mod.rs (L381-383)
```rust
        self.commit_queue
            .lock()
            .save_ledger_update_output(executed_chunk)?;
```
