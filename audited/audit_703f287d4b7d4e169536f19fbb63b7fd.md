# Audit Report

## Title
Critical Consensus Halt via Uninitialized Delayed Field Access in Sequential Execution Fallback

## Summary
A critical vulnerability exists in the sequential execution fallback path where the code panics when processing delayed field snapshots that reference base aggregators not loaded into the UnsyncMap. This causes all validators to crash when executing specific transactions involving delayed fields from previous blocks, resulting in complete network liveness failure.

## Finding Description
The vulnerability occurs in the sequential execution fallback mechanism within the block executor. When parallel execution fails and the system falls back to sequential execution, the `apply_output_sequential` function processes delayed field changes. [1](#0-0) 

The critical flaw is at this location where delayed field base values are retrieved: [2](#0-1) 

The code calls `.unwrap()` on `unsync_map.fetch_delayed_field(&base_id)`, which returns `Option<DelayedFieldValue>`. [3](#0-2) 

**Attack Mechanism:**

1. When the native `snapshot` function is called on an aggregator, it extracts the DelayedFieldID without loading the aggregator's actual value into the UnsyncMap. [4](#0-3) 

2. The ID extraction uses `get_aggregator_value_as_id` which only retrieves the identifier, not the value. [5](#0-4) 

3. A SnapshotDelta is created with this base_aggregator ID. [6](#0-5) 

4. In sequential execution, when `apply_to_base` is called for the SnapshotDelta, it expects to retrieve the base aggregator value. [7](#0-6) 

5. Since the value was never loaded into the UnsyncMap, `fetch_delayed_field` returns `None`, causing the `.unwrap()` to panic and halt the validator.

**Exploitation Path:**
- Block N: Attacker creates and commits an aggregator to storage
- Block N+1: Attacker submits transaction that takes snapshot of the aggregator from Block N
- Attacker forces sequential execution fallback (by triggering parallel execution complexity/failures)
- Sequential execution attempts to process the snapshot
- `.unwrap()` on None value causes validator panic
- All validators executing this block crash simultaneously

The sequential execution is invoked from the main block execution entry point used by consensus. [8](#0-7) 

## Impact Explanation
**Critical Severity - Total Loss of Network Availability**

This vulnerability meets the Critical severity criteria for "Total loss of liveness/network availability" from the Aptos bug bounty program. When exploited:

1. **Complete Consensus Halt**: All validators executing the malicious block will panic and crash
2. **Network-Wide Impact**: Since all validators must execute the same blocks to reach consensus, the entire network halts
3. **Requires Hard Fork**: Recovery requires manual intervention and potentially a hard fork to skip the malicious transactions
4. **Deterministic Failure**: The panic is deterministic - all validators will crash on the same block, preventing any node from making progress

This breaks the fundamental "Deterministic Execution" and "Consensus Safety" invariants, as validators cannot continue processing blocks.

## Likelihood Explanation
**High Likelihood - Easily Exploitable**

The attack has high likelihood because:

1. **No Special Privileges Required**: Any transaction sender can create aggregators and snapshots using public Move functions
2. **Predictable Trigger**: Sequential fallback occurs naturally under high transaction load or can be induced by submitting complex transaction patterns
3. **Simple Attack Vector**: Only requires calling standard aggregator_v2 native functions available in the Aptos Framework
4. **Low Cost**: Attack costs only standard transaction fees
5. **Reliable Trigger**: The panic condition is deterministic and reproducible

The attacker only needs to:
- Submit a transaction in block N creating an aggregator
- Submit a transaction in block N+1 taking a snapshot of that aggregator
- Wait for or induce sequential execution fallback

## Recommendation
The fix requires checking if the base delayed field exists in UnsyncMap and loading it from storage if not present. Replace the unsafe `.unwrap()` calls with proper error handling:

**Option 1 - Load from storage when missing:**
```rust
ApplyBase::Previous(base_id) => {
    let base_value = unsync_map.fetch_delayed_field(&base_id)
        .ok_or_else(|| {
            // Fetch from base storage view if not in UnsyncMap
            // This handles delayed fields from previous blocks
            base_view.get_delayed_field_value(&base_id)
        })??;
    
    updates.insert(
        id,
        expect_ok(apply.apply_to_base(base_value))?
    );
}
```

**Option 2 - Pre-load all referenced delayed fields:**
Before processing delayed field changes, scan all ApplyBase references and pre-load their values into UnsyncMap from storage.

**Option 3 - Fail gracefully:**
Return an error instead of panicking, allowing the transaction to abort rather than crashing the validator.

The proper solution is Option 1, as it maintains the semantic correctness of referencing delayed fields from previous blocks while preventing the panic.

## Proof of Concept

```move
// File: test_consensus_halt.move
module attacker::consensus_halt {
    use aptos_framework::aggregator_v2;
    use std::signer;

    /// Step 1: Create and store an aggregator (execute in block N)
    public entry fun create_aggregator(account: &signer) {
        let aggregator = aggregator_v2::create_aggregator<u64>(1000000);
        // Store in account resource (aggregator gets committed to storage)
        move_to(account, AggregatorHolder {
            agg: aggregator
        });
    }

    /// Step 2: Take snapshot of stored aggregator (execute in block N+1)
    /// This will create a SnapshotDelta referencing the aggregator from block N
    /// If sequential fallback occurs, validator will panic
    public entry fun trigger_panic(account: &signer) acquires AggregatorHolder {
        let holder = borrow_global<AggregatorHolder>(signer::address_of(account));
        let _snapshot = aggregator_v2::snapshot(&holder.agg);
        // When sequential execution processes this snapshot,
        // it will try to fetch the base aggregator value
        // which is not in UnsyncMap -> panic on unwrap()
    }

    struct AggregatorHolder has key {
        agg: aggregator_v2::Aggregator<u64>
    }
}
```

**Rust Test Reproduction:**
The vulnerability can be triggered by:
1. Creating a block with a transaction that creates an aggregator
2. Committing that block to storage
3. Creating a second block with a transaction that snapshots the aggregator
4. Forcing sequential execution fallback on the second block
5. Observing the panic when `apply_output_sequential` is called

The exact line where the panic occurs is when processing SnapshotDelta changes in the sequential execution path, attempting to unwrap a None value from `fetch_delayed_field`.

## Notes
This vulnerability specifically affects the sequential execution fallback path, which is a critical safety mechanism used when parallel execution encounters issues. The bug demonstrates a discrepancy between how delayed fields are handled in parallel vs sequential execution:

- **Parallel execution** properly handles missing delayed fields by fetching from versioned storage
- **Sequential execution** assumes all delayed fields are pre-loaded in UnsyncMap, causing panics when this assumption is violated

The root cause is that when snapshot operations reference delayed fields from previous blocks, those base values are never materialized into the current execution's UnsyncMap, creating a gap that triggers the panic condition.

### Citations

**File:** aptos-move/block-executor/src/executor.rs (L2086-2181)
```rust
    fn apply_output_sequential(
        txn_idx: TxnIndex,
        runtime_environment: &RuntimeEnvironment,
        global_module_cache: &GlobalModuleCache<
            ModuleId,
            CompiledModule,
            Module,
            AptosModuleExtension,
        >,
        unsync_map: &UnsyncMap<T::Key, T::Tag, T::Value, DelayedFieldID>,
        output_before_guard: &<E::Output as TransactionOutput>::BeforeMaterializationGuard<'_>,
        resource_write_set: HashMap<
            T::Key,
            (TriompheArc<T::Value>, Option<TriompheArc<MoveTypeLayout>>),
        >,
    ) -> Result<(), SequentialBlockExecutionError<E::Error>> {
        for (key, (write_op, layout)) in resource_write_set.into_iter() {
            unsync_map.write(key, write_op, layout);
        }

        for (group_key, (metadata_op, group_size, group_ops)) in
            output_before_guard.resource_group_write_set().into_iter()
        {
            unsync_map.insert_group_ops(&group_key, group_ops, group_size)?;
            unsync_map.write(group_key, TriompheArc::new(metadata_op), None);
        }

        for (key, write_op) in output_before_guard.aggregator_v1_write_set().into_iter() {
            unsync_map.write(key, TriompheArc::new(write_op), None);
        }

        let mut modules_published = false;
        for write in output_before_guard.module_write_set().values() {
            add_module_write_to_module_cache::<T>(
                write,
                txn_idx,
                runtime_environment,
                global_module_cache,
                unsync_map.module_cache(),
            )?;
            modules_published = true;
        }
        // For simplicity, flush layout cache on module publish.
        if modules_published {
            global_module_cache.flush_layout_cache();
        }

        let mut second_phase = Vec::new();
        let mut updates = HashMap::new();
        for (id, change) in output_before_guard.delayed_field_change_set().into_iter() {
            match change {
                DelayedChange::Create(value) => {
                    assert_none!(
                        unsync_map.fetch_delayed_field(&id),
                        "Sequential execution must not create duplicate aggregators"
                    );
                    updates.insert(id, value);
                },
                DelayedChange::Apply(apply) => {
                    match apply.get_apply_base_id(&id) {
                        ApplyBase::Previous(base_id) => {
                            updates.insert(
                                id,
                                expect_ok(apply.apply_to_base(
                                    unsync_map.fetch_delayed_field(&base_id).unwrap(),
                                ))
                                .unwrap(),
                            );
                        },
                        ApplyBase::Current(base_id) => {
                            second_phase.push((id, base_id, apply));
                        },
                    };
                },
            }
        }
        for (id, base_id, apply) in second_phase.into_iter() {
            updates.insert(
                id,
                expect_ok(
                    apply.apply_to_base(
                        updates
                            .get(&base_id)
                            .cloned()
                            .unwrap_or_else(|| unsync_map.fetch_delayed_field(&base_id).unwrap()),
                    ),
                )
                .unwrap(),
            );
        }
        for (id, value) in updates.into_iter() {
            unsync_map.write_delayed_field(id, value);
        }

        Ok(())
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L2548-2640)
```rust
    pub fn execute_block(
        &self,
        signature_verified_block: &TP,
        base_view: &S,
        transaction_slice_metadata: &TransactionSliceMetadata,
        module_cache_manager_guard: &mut AptosModuleCacheManagerGuard,
    ) -> BlockExecutionResult<BlockOutput<T, E::Output>, E::Error> {
        let _timer = BLOCK_EXECUTOR_INNER_EXECUTE_BLOCK.start_timer();

        if self.config.local.concurrency_level > 1 {
            let parallel_result = if self.config.local.blockstm_v2 {
                BLOCKSTM_VERSION_NUMBER.set(2);
                self.execute_transactions_parallel_v2(
                    signature_verified_block,
                    base_view,
                    transaction_slice_metadata,
                    module_cache_manager_guard,
                )
            } else {
                BLOCKSTM_VERSION_NUMBER.set(1);
                self.execute_transactions_parallel(
                    signature_verified_block,
                    base_view,
                    transaction_slice_metadata,
                    module_cache_manager_guard,
                )
            };

            // If parallel gave us result, return it
            if let Ok(output) = parallel_result {
                return Ok(output);
            }

            if !self.config.local.allow_fallback {
                panic!("Parallel execution failed and fallback is not allowed");
            }

            // All logs from the parallel execution should be cleared and not reported.
            // Clear by re-initializing the speculative logs.
            init_speculative_logs(signature_verified_block.num_txns() + 1);

            // Flush all caches to re-run from the "clean" state.
            module_cache_manager_guard
                .environment()
                .runtime_environment()
                .flush_all_caches();
            module_cache_manager_guard.module_cache_mut().flush();

            info!("parallel execution requiring fallback");
        }

        // If we didn't run parallel, or it didn't finish successfully - run sequential
        let sequential_result = self.execute_transactions_sequential(
            signature_verified_block,
            base_view,
            transaction_slice_metadata,
            module_cache_manager_guard,
            false,
        );

        // If sequential gave us result, return it
        let sequential_error = match sequential_result {
            Ok(output) => {
                return Ok(output);
            },
            Err(SequentialBlockExecutionError::ResourceGroupSerializationError) => {
                if !self.config.local.allow_fallback {
                    panic!("Parallel execution failed and fallback is not allowed");
                }

                // TODO[agg_v2](cleanup): check if sequential execution logs anything in the speculative logs,
                // and whether clearing them below is needed at all.
                // All logs from the first pass of sequential execution should be cleared and not reported.
                // Clear by re-initializing the speculative logs.
                init_speculative_logs(signature_verified_block.num_txns());

                let sequential_result = self.execute_transactions_sequential(
                    signature_verified_block,
                    base_view,
                    transaction_slice_metadata,
                    module_cache_manager_guard,
                    true,
                );

                // If sequential gave us result, return it
                match sequential_result {
                    Ok(output) => {
                        return Ok(output);
                    },
                    Err(SequentialBlockExecutionError::ResourceGroupSerializationError) => {
                        BlockExecutionError::FatalBlockExecutorError(code_invariant_error(
                            "resource group serialization during bcs fallback should not happen",
                        ))
```

**File:** aptos-move/mvhashmap/src/unsync_map.rs (L311-313)
```rust
    pub fn fetch_delayed_field(&self, id: &I) -> Option<DelayedFieldValue> {
        self.delayed_field_map.borrow().get(id).cloned()
    }
```

**File:** aptos-move/framework/src/natives/aggregator_natives/aggregator_v2.rs (L334-363)
```rust
fn native_snapshot(
    context: &mut SafeNativeContext,
    ty_args: &[Type],
    mut args: VecDeque<Value>,
) -> SafeNativeResult<SmallVec<[Value; 1]>> {
    debug_assert_eq!(args.len(), 1);
    debug_assert_eq!(ty_args.len(), 1);
    context.charge(AGGREGATOR_V2_SNAPSHOT_BASE)?;

    let aggregator_value_ty = &ty_args[0];
    let aggregator = safely_pop_arg!(args, StructRef);

    let result_value = if let Some((resolver, mut delayed_field_data)) = get_context_data(context) {
        let width = get_width_by_type(aggregator_value_ty, EUNSUPPORTED_AGGREGATOR_TYPE)?;
        let id = get_aggregator_value_as_id(&aggregator, aggregator_value_ty, resolver)?;
        let max_value = get_aggregator_max_value(&aggregator, aggregator_value_ty)?;

        let snapshot_id = delayed_field_data.snapshot(id, max_value, width, resolver)?;
        Value::delayed_value(snapshot_id)
    } else {
        let value = get_aggregator_value(&aggregator, aggregator_value_ty)?;
        create_value_by_type(
            aggregator_value_ty,
            value,
            EUNSUPPORTED_AGGREGATOR_SNAPSHOT_TYPE,
        )?
    };

    Ok(smallvec![Value::struct_(Struct::pack(vec![result_value]))])
}
```

**File:** aptos-move/framework/src/natives/aggregator_natives/helpers_v2.rs (L81-85)
```rust
get_value_as_id_impl!(
    get_aggregator_value_as_id,
    AGGREGATOR_VALUE_FIELD_INDEX,
    EUNSUPPORTED_AGGREGATOR_TYPE
);
```

**File:** aptos-move/aptos-aggregator/src/delayed_field_extension.rs (L197-201)
```rust
                DelayedChange::Apply(DelayedApplyChange::SnapshotDelta {
                    base_aggregator: aggregator_id,
                    delta: *delta,
                })
            },
```

**File:** aptos-move/aptos-aggregator/src/delayed_change.rs (L79-81)
```rust
            SnapshotDelta { delta, .. } => {
                DelayedFieldValue::Snapshot(delta.apply_to(base_value.into_aggregator_value()?)?)
            },
```
