# Audit Report

## Title
Byzantine Validator Can Cause Denial-of-Service via SignedBatchInfo Flooding in ProofCoordinator

## Summary
A Byzantine validator can flood honest validators with excessive `SignedBatchInfo` messages, causing the bounded `proof_coordinator_tx` channel to fill up and block the `NetworkListener` task. This prevents processing of all quorum store messages (batches, proofs, and signatures) from all validators, effectively creating a denial-of-service condition that degrades consensus liveness.

## Finding Description

The Aptos consensus quorum store architecture has a critical bottleneck in how `SignedBatchInfo` messages are processed. The vulnerability exists in the message flow from network receipt to proof coordination:

**Message Flow:**
1. `SignedBatchInfo` messages are received from the network and verified
2. Verified messages are forwarded to `quorum_store_msg_tx` (aptos_channel, per-key queue)
3. `NetworkListener` receives messages and forwards them to various coordinators
4. For `SignedBatchInfo`, the `NetworkListener` forwards to `proof_coordinator_tx` using a **blocking** `.send().await` call
5. `ProofCoordinator` processes signatures and aggregates them [1](#0-0) 

**The Critical Vulnerability:**

The `proof_coordinator_tx` channel is a bounded tokio mpsc channel with a default size of 1000: [2](#0-1) [3](#0-2) 

When the `NetworkListener` sends to this channel using `.send().await`, it **blocks** if the channel is full. The `NetworkListener` is a single task processing ALL quorum store messages in a sequential loop: [4](#0-3) 

**Attack Scenario:**

A Byzantine validator can exploit this by:

1. **Receiving batches from honest validators**: When honest validators create and broadcast batches, all validators (including Byzantine ones) receive them
2. **Creating valid signatures**: The Byzantine validator creates `SignedBatchInfo` messages for these batches with valid signatures
3. **Flooding honest validators**: The Byzantine validator rapidly broadcasts many `SignedBatchInfo` messages to honest validators

Each `SignedBatchInfoMsg` can contain up to `receiver_max_num_batches` (default 20) signatures: [5](#0-4) 

The verification only checks that messages don't exceed this per-message limit: [6](#0-5) 

**There is no per-validator rate limiting on the number of messages sent per second.**

When honest validator V receives these flood messages:
- The signatures pass verification (valid BLS signatures from a validator in the validator set)
- Messages are forwarded to `proof_coordinator_tx`
- The channel fills up (1000 messages)
- `NetworkListener` blocks on `.send().await`
- **ALL** quorum store message processing stops, including:
  - `BatchMsg` from other validators
  - `ProofOfStoreMsg` from other validators  
  - Other `SignedBatchInfo` messages

The `ProofCoordinator` accepts these signatures because: [7](#0-6) 

The batch exists locally (honest validators receive and store batches from peers) and the batch author matches the local validator's peer_id (since the honest validator created the batch).

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Node Slowdowns**: The attack causes honest validator nodes to stop processing quorum store messages, severely degrading their ability to participate in consensus. This directly matches the "Validator node slowdowns" category (High Severity, up to $50,000).

2. **Consensus Liveness Impact**: While not a complete network halt, this significantly impacts consensus efficiency:
   - Honest validators cannot aggregate signatures to form proofs-of-store
   - Batch propagation is blocked
   - Proof broadcasting is blocked
   - This degrades transaction throughput and consensus performance

3. **Resource Exhaustion Attack**: Violates the "Resource Limits" invariant - the system fails to properly rate-limit or throttle excessive valid messages from Byzantine validators.

4. **Significant Protocol Violation**: The attack breaks the expected behavior of the quorum store protocol by preventing message processing through channel exhaustion.

The impact is limited to liveness degradation rather than safety violations, but it constitutes a significant denial-of-service vector against consensus participants.

## Likelihood Explanation

**Likelihood: HIGH**

The attack is highly likely to occur because:

1. **Low Barrier to Entry**: Any Byzantine validator (up to 1/3 of stake) can execute this attack without requiring additional privileges or collusion.

2. **Simple Execution**: The attack requires only:
   - Receiving batches from honest validators (automatic)
   - Creating valid signatures (standard cryptographic operations)
   - Broadcasting messages rapidly (no special network capabilities needed)

3. **No Detection or Rate Limiting**: The current implementation has:
   - No per-validator rate limits on `SignedBatchInfo` messages
   - No detection mechanisms for abnormal signature volumes
   - No circuit breakers or backpressure mechanisms

4. **Deterministic Success**: Once the `proof_coordinator_tx` channel fills (1000 messages), the `NetworkListener` will definitively block, guaranteeing the denial-of-service effect.

5. **Cost-Benefit for Attackers**: Byzantine validators could use this to:
   - Degrade competitor validators' performance
   - Reduce overall network throughput
   - Create instability during critical periods

## Recommendation

Implement multiple layers of defense:

### 1. Per-Validator Rate Limiting

Add rate limiting for `SignedBatchInfo` messages before they're forwarded to the `ProofCoordinator`:

```rust
// In NetworkListener or during verification
struct SignatureRateLimiter {
    limiters: HashMap<PeerId, TokenBucketRateLimiter>,
    max_signatures_per_second: u64,
}

impl SignatureRateLimiter {
    fn check_rate_limit(&mut self, peer: PeerId) -> bool {
        self.limiters
            .entry(peer)
            .or_insert_with(|| TokenBucketRateLimiter::new(
                self.max_signatures_per_second,
                self.max_signatures_per_second,
            ))
            .acquire_tokens(1)
    }
}
```

### 2. Use Non-Blocking Channel Send

Replace the blocking `.send().await` with `.try_send()` and handle full channel gracefully:

```rust
match self.proof_coordinator_tx.try_send(cmd) {
    Ok(_) => {},
    Err(tokio::sync::mpsc::error::TrySendError::Full(_)) => {
        warn!("ProofCoordinator channel full, dropping message from {}", sender);
        counters::PROOF_COORDINATOR_CHANNEL_FULL.inc();
        // Drop the message to avoid blocking NetworkListener
    },
    Err(tokio::sync::mpsc::error::TrySendError::Closed(_)) => {
        error!("ProofCoordinator channel closed");
        break;
    },
}
```

### 3. Increase Channel Capacity with Monitoring

Increase the channel size and add monitoring:

```rust
// In QuorumStoreConfig
pub struct QuorumStoreConfig {
    pub channel_size: usize, // Increase from 1000 to 10000
    pub max_signatures_per_validator_per_second: u64, // Add rate limit config
}
```

### 4. Add Per-Validator Signature Tracking

Track signature volume per validator and reject excessive signers:

```rust
// In ProofCoordinator
struct SignatureTracker {
    signatures_per_validator: HashMap<PeerId, usize>,
    max_signatures_per_validator_per_batch: usize,
}
```

### Recommended Fix Priority
1. **Immediate**: Change to non-blocking send (prevents complete DoS)
2. **Short-term**: Add per-validator rate limiting (prevents abuse)
3. **Long-term**: Implement comprehensive monitoring and adaptive rate limiting

## Proof of Concept

```rust
// PoC demonstrating the channel blocking behavior
// This would be added as a test in consensus/src/quorum_store/tests/

use tokio::sync::mpsc;
use tokio::time::{timeout, Duration};
use futures::stream::StreamExt;

#[tokio::test]
async fn test_proof_coordinator_channel_blocking() {
    // Simulate the bounded channel setup
    let channel_size = 1000;
    let (proof_coordinator_tx, mut proof_coordinator_rx) = 
        mpsc::channel(channel_size);
    
    // Spawn a task that simulates NetworkListener sending messages
    let sender_task = tokio::spawn(async move {
        for i in 0..2000 {
            // This will block after 1000 messages
            let result = timeout(
                Duration::from_millis(100),
                proof_coordinator_tx.send(i)
            ).await;
            
            if result.is_err() {
                println!("Blocked at message {}", i);
                return i; // Return the count where we blocked
            }
        }
        2000
    });
    
    // Simulate slow ProofCoordinator processing
    tokio::spawn(async move {
        tokio::time::sleep(Duration::from_secs(5)).await;
        while let Some(_msg) = proof_coordinator_rx.recv().await {
            // Slowly process messages
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
    });
    
    // Wait for sender to block
    let blocked_at = sender_task.await.unwrap();
    
    // Verify that NetworkListener blocked after channel filled
    assert!(blocked_at <= channel_size + 1, 
        "Expected blocking around {} messages, but blocked at {}", 
        channel_size, blocked_at);
    
    println!("Successfully demonstrated: NetworkListener blocks after {} messages", 
        blocked_at);
}

// To demonstrate the full attack in a live network, a Byzantine validator would:
// 1. Monitor incoming batches from honest validators
// 2. For each batch B with author A:
//    - Create SignedBatchInfo(B) with Byzantine validator's signature
//    - Send to validator A
// 3. Repeat rapidly with many batches to fill the proof_coordinator_tx channel
// 4. Observe that validator A stops processing quorum store messages
```

## Notes

This vulnerability demonstrates a common pattern in distributed systems: **bounded channels with blocking sends can create cascading failures**. The issue is particularly severe in consensus systems where message processing delays can impact network-wide liveness.

The attack does not require the Byzantine validator to control batch creation - they only need to sign batches created by others, which is a legitimate operation in the quorum store protocol. The vulnerability lies in the lack of rate limiting on these legitimate operations when performed excessively by malicious actors.

### Citations

**File:** consensus/src/quorum_store/network_listener.rs (L40-111)
```rust
    pub async fn start(mut self) {
        info!("QS: starting networking");
        let mut next_batch_coordinator_idx = 0;
        while let Some((sender, msg)) = self.network_msg_rx.next().await {
            monitor!("qs_network_listener_main_loop", {
                match msg {
                    // TODO: does the assumption have to be that network listener is shutdown first?
                    VerifiedEvent::Shutdown(ack_tx) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["NetworkListener::shutdown"])
                            .inc();
                        info!("QS: shutdown network listener received");
                        ack_tx
                            .send(())
                            .expect("Failed to send shutdown ack to QuorumStore");
                        break;
                    },
                    VerifiedEvent::SignedBatchInfo(signed_batch_infos) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["NetworkListener::signedbatchinfo"])
                            .inc();
                        let cmd =
                            ProofCoordinatorCommand::AppendSignature(sender, *signed_batch_infos);
                        self.proof_coordinator_tx
                            .send(cmd)
                            .await
                            .expect("Could not send signed_batch_info to proof_coordinator");
                    },
                    VerifiedEvent::BatchMsg(batch_msg) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["NetworkListener::batchmsg"])
                            .inc();
                        // Batch msg verify function alreay ensures that the batch_msg is not empty.
                        let author = batch_msg.author().expect("Empty batch message");
                        let batches = batch_msg.take();
                        counters::RECEIVED_BATCH_MSG_COUNT.inc();

                        // Round-robin assignment to batch coordinator.
                        let idx = next_batch_coordinator_idx;
                        next_batch_coordinator_idx = (next_batch_coordinator_idx + 1)
                            % self.remote_batch_coordinator_tx.len();
                        trace!(
                            "QS: peer_id {:?},  # network_worker {}, hashed to idx {}",
                            author,
                            self.remote_batch_coordinator_tx.len(),
                            idx
                        );
                        counters::BATCH_COORDINATOR_NUM_BATCH_REQS
                            .with_label_values(&[&idx.to_string()])
                            .inc();
                        self.remote_batch_coordinator_tx[idx]
                            .send(BatchCoordinatorCommand::NewBatches(author, batches))
                            .await
                            .expect("Could not send remote batch");
                    },
                    VerifiedEvent::ProofOfStoreMsg(proofs) => {
                        counters::QUORUM_STORE_MSG_COUNT
                            .with_label_values(&["NetworkListener::proofofstore"])
                            .inc();
                        let cmd = ProofManagerCommand::ReceiveProofs(*proofs);
                        self.proof_manager_tx
                            .send(cmd)
                            .await
                            .expect("could not push Proof proof_of_store");
                    },
                    _ => {
                        unreachable!()
                    },
                };
            });
        }
    }
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L181-182)
```rust
        let (proof_coordinator_cmd_tx, proof_coordinator_cmd_rx) =
            tokio::sync::mpsc::channel(config.channel_size);
```

**File:** config/src/config/quorum_store_config.rs (L108-108)
```rust
            channel_size: 1000,
```

**File:** config/src/config/quorum_store_config.rs (L122-122)
```rust
            receiver_max_num_batches: 20,
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L369-381)
```rust
    ) -> anyhow::Result<()> {
        ensure!(!self.signed_infos.is_empty(), "Empty message");
        ensure!(
            self.signed_infos.len() <= max_num_batches,
            "Too many batches: {} > {}",
            self.signed_infos.len(),
            max_num_batches
        );
        for signed_info in &self.signed_infos {
            signed_info.verify(sender, max_batch_expiry_gap_usecs, validator)?
        }
        Ok(())
    }
```

**File:** consensus/src/quorum_store/proof_coordinator.rs (L269-311)
```rust
    fn init_proof(
        &mut self,
        signed_batch_info: &SignedBatchInfo<BatchInfoExt>,
    ) -> Result<(), SignedBatchInfoError> {
        // Check if the signed digest corresponding to our batch
        if signed_batch_info.author() != self.peer_id {
            return Err(SignedBatchInfoError::WrongAuthor);
        }
        let batch_author = self
            .batch_reader
            .exists(signed_batch_info.digest())
            .ok_or(SignedBatchInfoError::NotFound)?;
        if batch_author != signed_batch_info.author() {
            return Err(SignedBatchInfoError::WrongAuthor);
        }

        self.timeouts.add(
            signed_batch_info.batch_info().clone(),
            self.proof_timeout_ms,
        );
        if signed_batch_info.batch_info().is_v2() {
            self.batch_info_to_proof.insert(
                signed_batch_info.batch_info().clone(),
                IncrementalProofState::new_batch_info_ext(signed_batch_info.batch_info().clone()),
            );
        } else {
            self.batch_info_to_proof.insert(
                signed_batch_info.batch_info().clone(),
                IncrementalProofState::new_batch_info(
                    signed_batch_info.batch_info().info().clone(),
                ),
            );
        }
        self.batch_info_to_time
            .entry(signed_batch_info.batch_info().clone())
            .or_insert(Instant::now());
        debug!(
            LogSchema::new(LogEvent::ProofOfStoreInit),
            digest = signed_batch_info.digest(),
            batch_id = signed_batch_info.batch_id().id,
        );
        Ok(())
    }
```
