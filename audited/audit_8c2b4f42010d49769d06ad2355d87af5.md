# Audit Report

## Title
Memory Exhaustion in Transaction Accumulator Pruner Catch-Up During Node Initialization

## Summary
The `TransactionAccumulatorPruner::new()` function performs unbounded catch-up pruning during initialization. When the gap between saved progress and current metadata is large (e.g., millions of versions), it accumulates all delete operations in a single in-memory batch without chunking, causing memory exhaustion and preventing validator nodes from starting.

## Finding Description

During the initialization of the ledger pruner subsystem, each sub-pruner must "catch up" to the current metadata progress. The `TransactionAccumulatorPruner::new()` function performs this catch-up synchronously by calling `prune(progress, metadata_progress)` without any chunking mechanism. [1](#0-0) 

The `prune()` implementation creates a single `SchemaBatch` and passes it to `TransactionAccumulatorDb::prune()`: [2](#0-1) 

The `TransactionAccumulatorDb::prune()` function iterates through the entire version range without any memory limits: [3](#0-2) 

For each version in the range, multiple delete operations are added to the batch. The complexity is O(N log N) where N is the version gap. The `SchemaBatch` structure stores all operations in an unbounded HashMap: [4](#0-3) 

**Attack Scenario:**
1. Validator node operates with pruning disabled for an extended period, accumulating 100M+ versions
2. Default prune window is 90 million versions: [5](#0-4) 
3. Operator enables pruning or restarts the node
4. During initialization, `LedgerPruner::new()` creates all sub-pruners with current `metadata_progress`: [6](#0-5) 
5. Each sub-pruner attempts catch-up in a single batch:
   - For 100M versions: ~50M odd versions
   - Each requires ~27 delete operations (log₂(100M) ≈ 26.6)
   - Total: ~1.35 billion delete operations
   - At ~100 bytes per operation: **~135 GB of memory**
6. Node crashes with OOM error, fails to initialize
7. Node cannot start, causing validator unavailability

**Affected Sub-Pruners:**
The same unbounded catch-up pattern exists in all ledger sub-pruners, including `TransactionInfoPruner`: [7](#0-6) 

## Impact Explanation

**Severity: HIGH**

This vulnerability causes validator node unavailability, meeting the HIGH severity criteria: "Validator node slowdowns" and preventing node startup entirely.

**Impact Quantification:**
- **Availability**: Validator nodes cannot start after restart if large pruning gap exists
- **Network Liveness**: If multiple validators restart simultaneously (e.g., during coordinated upgrades), network consensus could be impacted
- **Operational Risk**: Forces operators to either:
  - Disable pruning permanently (disk space exhaustion risk)
  - Manually delete and re-sync the database (hours to days of downtime)
  - Wait for potential timeout (unpredictable duration)

**Invariant Violations:**
- **Resource Limits**: Violates "All operations must respect gas, storage, and computational limits" - no memory bounds enforced
- **Liveness Guarantee**: Prevents validator nodes from participating in consensus

**Why Not Critical:**
- Does not cause loss of funds
- Does not violate consensus safety
- Does not cause permanent network partition (nodes can recover by disabling pruning or re-syncing)
- Requires specific operational conditions (large gap + restart)

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

**Triggering Conditions:**
1. Node has accumulated significant history without pruning (90M+ versions)
2. Operator enables pruning or restarts node
3. Gap between saved pruner progress and current ledger version is large

**Common Scenarios:**
- **First-time pruning enablement**: Nodes bootstrapped without pruning that later enable it
- **Long-running nodes**: Validators that disable pruning during critical periods then re-enable
- **Network upgrades**: Coordinated validator restarts could trigger this across multiple nodes simultaneously
- **Fast-sync recovery**: Nodes that fast-sync and then enable pruning without intermediate gradual catch-up

**Mitigating Factors:**
- Default configuration enables pruning, reducing long-term gap accumulation
- Operators may notice warnings during long initialization
- Production nodes typically maintain continuous pruning

**Realistic Probability:**
- Individual node: MEDIUM (happens during specific operational events)
- Network-wide impact: LOW (requires coordinated restart of multiple affected validators)

## Recommendation

Implement chunked catch-up pruning with configurable batch sizes during initialization. The normal `LedgerPruner::prune()` already implements proper chunking - this pattern should be applied to sub-pruner initialization.

**Fix Implementation:**

1. **Add chunked catch-up to `TransactionAccumulatorPruner::new()`:**

Modify the initialization to prune in batches:

```rust
pub(in crate::pruner) fn new(
    ledger_db: Arc<LedgerDb>,
    metadata_progress: Version,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        ledger_db.transaction_accumulator_db_raw(),
        &DbMetadataKey::TransactionAccumulatorPrunerProgress,
        metadata_progress,
    )?;

    let myself = TransactionAccumulatorPruner { ledger_db };

    // Chunk size for catch-up pruning (e.g., 10k versions at a time)
    const CATCHUP_BATCH_SIZE: Version = 10_000;
    
    info!(
        progress = progress,
        metadata_progress = metadata_progress,
        "Catching up TransactionAccumulatorPruner in batches."
    );
    
    let mut current_progress = progress;
    while current_progress < metadata_progress {
        let batch_target = std::cmp::min(
            current_progress + CATCHUP_BATCH_SIZE,
            metadata_progress
        );
        
        myself.prune(current_progress, batch_target)?;
        current_progress = batch_target;
        
        info!(
            progress = current_progress,
            target = metadata_progress,
            "TransactionAccumulatorPruner catch-up progress."
        );
    }

    Ok(myself)
}
```

2. **Apply same fix to all affected sub-pruners:**
   - `TransactionInfoPruner`
   - `EventStorePruner`
   - `PersistedAuxiliaryInfoPruner`
   - `TransactionAuxiliaryDataPruner`
   - `TransactionPruner`
   - `WriteSetPruner`

3. **Make batch size configurable:**
   Add to `LedgerPrunerConfig`:
   ```rust
   pub catchup_batch_size: usize,  // Default: 10_000
   ```

4. **Add timeout protection:**
   Implement maximum initialization time with graceful degradation (e.g., skip catch-up and start fresh from current progress).

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use crate::AptosDB;
    use aptos_temppath::TempPath;
    use aptos_types::transaction::{Transaction, TransactionInfo};
    use std::sync::Arc;

    #[test]
    #[should_panic(expected = "out of memory")]
    fn test_catchup_memory_exhaustion() {
        // Create a test database
        let tmpdir = TempPath::new();
        let db = AptosDB::new_for_test(&tmpdir);
        let ledger_db = db.ledger_db.clone();

        // Simulate a large gap by setting metadata progress far ahead
        // In production, this could be 90M+ versions
        const LARGE_GAP: Version = 1_000_000;
        
        // Initialize metadata progress to simulate existing ledger
        ledger_db.metadata_db().put::<DbMetadataSchema>(
            &DbMetadataKey::LedgerCommitProgress,
            &DbMetadataValue::Version(LARGE_GAP),
        ).unwrap();

        // Set pruner progress to 0 (never pruned)
        // This creates the gap condition
        
        // Attempt to create TransactionAccumulatorPruner
        // This should exhaust memory trying to catch up in single batch
        let result = TransactionAccumulatorPruner::new(
            ledger_db,
            LARGE_GAP,
        );
        
        // With 1M versions gap:
        // - ~500k odd versions
        // - ~27 operations each = ~13.5M operations
        // - At ~100 bytes each = ~1.35 GB memory
        // 
        // With 100M versions (realistic production):
        // - ~50M odd versions  
        // - ~27 operations each = ~1.35B operations
        // - At ~100 bytes each = ~135 GB memory -> OOM crash
        
        assert!(result.is_err(), "Should fail due to memory exhaustion");
    }
}
```

**Notes**

The vulnerability affects all ledger sub-pruners that perform catch-up during initialization. The default configuration with 90M version prune window makes this particularly severe, as nodes can accumulate massive gaps before hitting the issue. While normal pruning operations are properly chunked via `LedgerPruner::prune()`, the initialization path lacks this protection, creating an asymmetric resource usage pattern that can crash validator nodes during restarts.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_accumulator_pruner.rs (L25-35)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        TransactionAccumulatorDb::prune(current_progress, target_version, &mut batch)?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionAccumulatorPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_db
            .transaction_accumulator_db()
            .write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_accumulator_pruner.rs (L39-59)
```rust
    pub(in crate::pruner) fn new(
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_accumulator_db_raw(),
            &DbMetadataKey::TransactionAccumulatorPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionAccumulatorPruner { ledger_db };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionAccumulatorPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L149-172)
```rust
    pub(crate) fn prune(begin: Version, end: Version, db_batch: &mut SchemaBatch) -> Result<()> {
        for version_to_delete in begin..end {
            db_batch.delete::<TransactionAccumulatorRootHashSchema>(&version_to_delete)?;
            // The even version will be pruned in the iteration of version + 1.
            if version_to_delete % 2 == 0 {
                continue;
            }

            let first_ancestor_that_is_a_left_child =
                Self::find_first_ancestor_that_is_a_left_child(version_to_delete);

            // This assertion is true because we skip the leaf nodes with address which is a
            // a multiple of 2.
            assert!(!first_ancestor_that_is_a_left_child.is_leaf());

            let mut current = first_ancestor_that_is_a_left_child;
            while !current.is_leaf() {
                db_batch.delete::<TransactionAccumulatorSchema>(&current.left_child())?;
                db_batch.delete::<TransactionAccumulatorSchema>(&current.right_child())?;
                current = current.right_child();
            }
        }
        Ok(())
    }
```

**File:** storage/schemadb/src/batch.rs (L127-173)
```rust
/// `SchemaBatch` holds a collection of updates that can be applied to a DB atomically. The updates
/// will be applied in the order in which they are added to the `SchemaBatch`.
#[derive(Debug, Default)]
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}

impl SchemaBatch {
    /// Creates an empty batch.
    pub fn new() -> Self {
        Self::default()
    }

    /// keep these on the struct itself so that we don't need to update each call site.
    pub fn put<S: Schema>(&mut self, key: &S::Key, value: &S::Value) -> DbResult<()> {
        <Self as WriteBatch>::put::<S>(self, key, value)
    }

    pub fn delete<S: Schema>(&mut self, key: &S::Key) -> DbResult<()> {
        <Self as WriteBatch>::delete::<S>(self, key)
    }
}

impl WriteBatch for SchemaBatch {
    fn stats(&mut self) -> &mut SampledBatchStats {
        &mut self.stats
    }

    fn raw_put(&mut self, cf_name: ColumnFamilyName, key: Vec<u8>, value: Vec<u8>) -> DbResult<()> {
        self.rows
            .entry(cf_name)
            .or_default()
            .push(WriteOp::Value { key, value });

        Ok(())
    }

    fn raw_delete(&mut self, cf_name: ColumnFamilyName, key: Vec<u8>) -> DbResult<()> {
        self.rows
            .entry(cf_name)
            .or_default()
            .push(WriteOp::Deletion { key });

        Ok(())
    }
}
```

**File:** config/src/config/storage_config.rs (L387-396)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L147-150)
```rust
        let transaction_accumulator_pruner = Box::new(TransactionAccumulatorPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_info_pruner.rs (L37-57)
```rust
    pub(in crate::pruner) fn new(
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_info_db_raw(),
            &DbMetadataKey::TransactionInfoPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionInfoPruner { ledger_db };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionInfoPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```
