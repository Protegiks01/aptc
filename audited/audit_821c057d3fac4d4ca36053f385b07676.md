# Audit Report

## Title
TOCTOU Race Condition in Event Query Operations During Concurrent Pruning

## Summary
A Time-of-Check to Time-of-Use (TOCTOU) race condition exists in event query operations where the lookup of event indices and the subsequent fetch of actual events are performed as separate, non-atomic database operations. When the background pruner deletes events between these two operations, queries fail mid-execution with "NotFound" errors, causing API requests to return InternalError responses to clients.

## Finding Description

The vulnerability exists in the multi-step event retrieval process that lacks snapshot isolation between operations:

**Primary Vulnerable Path (Indexer):**

The `get_events_by_event_key()` function performs two separate database operations without shared snapshot protection: [1](#0-0) 

First, it queries the index to obtain event locations (version, index pairs). Then, in a separate operation, it fetches each event: [2](#0-1) 

**Secondary Vulnerable Path (EventStore):**

A similar pattern exists in the direct EventStore access path: [3](#0-2) 

**Concurrent Pruner Execution:**

The EventStorePruner runs continuously in a background thread, executing atomic deletion batches: [4](#0-3) 

The pruner commits deletions atomically via `write_schemas()`: [5](#0-4) 

**Root Cause:**

RocksDB iterators create implicit snapshots valid only for their lifetime. When the iterator is dropped after step 1, step 2 uses a fresh database read without snapshot protection: [6](#0-5) 

**Evidence of Developer Awareness:**

The code contains acknowledgment of this issue but lacks complete mitigation: [7](#0-6) 

This breaks the **State Consistency** invariant: query operations should provide consistent views of data, not fail mid-execution due to concurrent background operations.

## Impact Explanation

**Severity: High** per Aptos Bug Bounty criteria - "API crashes" and "Significant protocol violations"

**User-Facing Impact:**
- API requests to `/accounts/:address/events/:creation_number` fail with InternalError
- Event query endpoints become unreliable during active pruning periods
- Monitoring systems and indexers experience intermittent failures
- User applications cannot reliably query historical events

**System Impact:**
- Degrades API service availability
- Breaks deterministic query behavior expectations
- No data corruption, but operational reliability compromised [8](#0-7) 

The failure propagates as InternalError to API clients, providing poor user experience and potentially causing cascading failures in dependent systems.

## Likelihood Explanation

**Likelihood: Medium-High** in production environments with active pruning.

**Factors Increasing Likelihood:**
- Background pruner runs continuously when enabled (production default)
- Race window exists between index lookup (~microseconds) and event fetch (~microseconds)
- Higher probability when:
  - Pruning is actively removing recent events
  - Query load is high
  - Events near pruning boundary are frequently queried

**Exploitation Requirements:**
- No special privileges required
- Pruning must be enabled (standard production configuration)
- Timing-dependent but probabilistically guaranteed over time
- More frequent with aggressive pruning configurations

**Attack Scenario:**
An attacker can amplify this issue by:
1. Monitoring blockchain state to identify pruning boundaries
2. Repeatedly querying events at the edge of pruned ranges
3. Increasing query frequency to maximize race condition probability
4. Causing targeted service degradation on event query endpoints

## Recommendation

**Solution: Implement Snapshot Isolation for Multi-Step Reads**

Use explicit RocksDB snapshots to ensure both the index lookup and event fetch operations see the same consistent database state:

```rust
pub fn get_event_by_key(
    &self,
    event_key: &EventKey,
    seq_num: u64,
    ledger_version: Version,
) -> Result<(Version, ContractEvent)> {
    // Create explicit snapshot for both operations
    let snapshot = self.event_db.create_snapshot();
    let read_opts = {
        let mut opts = ReadOptions::default();
        opts.set_snapshot(&snapshot);
        opts
    };
    
    // Perform both lookups using the same snapshot
    let (version, index) = self.lookup_event_by_key_with_opts(
        event_key, 
        seq_num, 
        ledger_version,
        &read_opts
    )?;
    
    let event = self.event_db
        .get_with_opts::<EventSchema>(&(version, index), &read_opts)?
        .ok_or_else(|| AptosDbError::NotFound(
            format!("Event {} of Txn {}", index, version)
        ))?;
    
    Ok((version, event))
}
```

**Alternative: Retry Logic with Exponential Backoff**

For scenarios where snapshot overhead is unacceptable, implement graceful retry:

```rust
const MAX_RETRIES: usize = 3;
for attempt in 0..MAX_RETRIES {
    match self.get_event_by_key_internal(event_key, seq_num, ledger_version) {
        Ok(result) => return Ok(result),
        Err(AptosDbError::NotFound(_)) if attempt < MAX_RETRIES - 1 => {
            // Event may have been pruned between index lookup and fetch
            continue;
        }
        Err(e) => return Err(e),
    }
}
```

**Apply fix to both vulnerable paths:**
1. `storage/indexer/src/db_indexer.rs::get_events_by_event_key()`
2. `storage/aptosdb/src/event_store/mod.rs::get_event_by_key()`

## Proof of Concept

```rust
// Rust test demonstrating the race condition
#[tokio::test]
async fn test_event_query_pruning_race_condition() {
    use std::sync::Arc;
    use std::thread;
    
    // Setup: Initialize AptosDB with events and pruning enabled
    let tmpdir = TempDir::new().unwrap();
    let db = Arc::new(AptosDB::open(
        &tmpdir.path(),
        false, // readonly
        PrunerConfig::default(), // Enable pruning
        RocksdbConfigs::default(),
        false, // enable_indexer
        1000, // buffered_state_target_items
        1000, // max_num_nodes_per_lru_cache_shard
        None, // internal_indexer_db
        HotStateConfig::default(),
    ).unwrap());
    
    // Populate database with events at versions 0-1000
    let event_key = EventKey::random();
    for version in 0..1000 {
        let events = vec![create_test_event(&event_key, version)];
        db.save_transactions(version, &events, /* ... */).unwrap();
    }
    
    // Start aggressive pruning in background thread
    let db_clone = Arc::clone(&db);
    let pruner_handle = thread::spawn(move || {
        for target in 100..900 {
            db_clone.ledger_pruner.prune(target).unwrap();
            thread::sleep(Duration::from_millis(1));
        }
    });
    
    // Concurrent query thread attempting to read events
    let db_clone = Arc::clone(&db);
    let query_handle = thread::spawn(move || {
        let mut failures = 0;
        for _ in 0..1000 {
            // Query events in the actively-pruned range
            let result = db_clone.get_events(
                &event_key,
                0, // start
                Order::Ascending,
                100, // limit
                999, // ledger_version
            );
            
            if let Err(AptosDbError::NotFound(_)) = result {
                failures += 1;
            }
            thread::sleep(Duration::from_micros(100));
        }
        failures
    });
    
    pruner_handle.join().unwrap();
    let failure_count = query_handle.join().unwrap();
    
    // Assert: Race condition causes query failures
    assert!(failure_count > 0, 
        "Expected race condition to cause query failures, but got 0 failures");
    println!("Race condition triggered {} query failures", failure_count);
}
```

**Expected Behavior:** The test should demonstrate query failures occurring when pruning runs concurrently with event queries, confirming the TOCTOU vulnerability.

## Notes

This vulnerability represents a violation of the **State Consistency** invariant where query operations should provide atomic, consistent views of the database. While RocksDB provides MVCC guarantees within individual operations, the multi-step query pattern breaks these guarantees by not maintaining snapshot isolation across operations. The issue is exacerbated in production environments with active pruning, making it a significant reliability concern for API consumers and dependent services.

### Citations

**File:** storage/indexer/src/db_indexer.rs (L671-676)
```rust
        let mut event_indices = self.indexer_db.lookup_events_by_key(
            event_key,
            first_seq,
            real_limit,
            ledger_version,
        )?;
```

**File:** storage/indexer/src/db_indexer.rs (L695-698)
```rust
                let event = match self
                    .main_db_reader
                    .get_event_by_version_and_index(ver, idx)?
                {
```

**File:** storage/aptosdb/src/event_store/mod.rs (L47-49)
```rust
        self.event_db
            .get::<EventSchema>(&(version, index))?
            .ok_or_else(|| AptosDbError::NotFound(format!("Event {} of Txn {}", index, version)))
```

**File:** storage/aptosdb/src/event_store/mod.rs (L68-71)
```rust
        let (version, index) = self.lookup_event_by_key(event_key, seq_num, ledger_version)?;
        Ok((
            version,
            self.get_event_by_version_and_index(version, index)?,
```

**File:** storage/aptosdb/src/event_store/mod.rs (L130-136)
```rust
            if seq != cur_seq {
                let msg = if cur_seq == start_seq_num {
                    "First requested event is probably pruned."
                } else {
                    "DB corruption: Sequence number not continuous."
                };
                db_other_bail!("{} expected: {}, actual: {}", msg, cur_seq, seq);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L55-65)
```rust
        let num_events_per_version = self.ledger_db.event_db().prune_event_indices(
            current_progress,
            target_version,
            indices_batch,
        )?;
        self.ledger_db.event_db().prune_events(
            num_events_per_version,
            current_progress,
            target_version,
            &mut batch,
        )?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L80-80)
```rust
        self.ledger_db.event_db().write_schemas(batch)
```

**File:** api/src/events.rs (L163-178)
```rust
        let events = self
            .context
            .get_events(
                &event_key,
                page.start_option(),
                page.limit(&latest_ledger_info)?,
                ledger_version,
            )
            .context(format!("Failed to find events by key {}", event_key))
            .map_err(|err| {
                BasicErrorWith404::internal_with_code(
                    err,
                    AptosErrorCode::InternalError,
                    &latest_ledger_info,
                )
            })?;
```
