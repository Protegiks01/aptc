# Audit Report

## Title
Resource Leak and Liveness Failure Due to Improper Stream Cleanup on CriticalDataStreamTimeout

## Summary
When a data stream experiences critical timeout and the subsequent stream termination fails, the `active_data_stream` is never cleaned up, causing the node to repeatedly attempt using a dead stream in an infinite loop, preventing state synchronization progress.

## Finding Description

The vulnerability exists in the stream cleanup logic when handling `CriticalDataStreamTimeout` errors. When `get_data_notification()` detects that a stream has timed out too many consecutive times, it returns a `CriticalDataStreamTimeout` error. [1](#0-0) 

The callers (bootstrapper and continuous syncer) detect this error and attempt to reset the stream: [2](#0-1) 

However, the `reset_active_stream()` function has a critical flaw. It attempts to terminate the stream with the streaming service before clearing the local stream reference: [3](#0-2) 

If `terminate_stream_with_feedback()` fails (line 1549's `?` operator) due to network issues, streaming service unavailability, or channel errors, the function returns early. This means lines 1553-1554 (`self.active_data_stream = None` and `self.speculative_stream_state = None`) are **never executed**.

The termination can fail when `send_stream_request()` encounters channel send errors: [4](#0-3) 

When termination fails, the error propagates through `fetch_next_data_notification()`, which returns the termination error (not the original timeout error) to `process_active_stream_notifications()`, and then to the driver's main loop. [5](#0-4) 

The driver logs the error but **continues execution**. On the next progress check interval, `drive_progress()` is called again: [6](#0-5) 

Since `self.active_data_stream` was never set to `None`, the condition on line 424 evaluates to `true`, and the node attempts to process notifications from the same dead stream. This stream will timeout again, repeating the cycle indefinitely.

## Impact Explanation

This vulnerability causes **node liveness failure**, meeting **Medium Severity** criteria per the Aptos bug bounty program:

1. **State Synchronization Stuck**: The affected node cannot sync new state, falling progressively behind the network
2. **Resource Leak**: The `DataStreamListener` object (containing an `mpsc::Receiver<DataNotification>`) persists indefinitely, leaking memory resources
3. **Requires Manual Intervention**: The only recovery is to restart the node
4. **Affects Node Availability**: The node becomes unable to participate in consensus or serve state sync queries [7](#0-6) 

While this doesn't directly cause consensus violations or fund loss, it creates a denial-of-service condition for individual nodes, qualifying as "state inconsistencies requiring intervention" under Medium severity.

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of occurring in production:

**Natural Triggers**:
- Network connectivity issues between state sync driver and streaming service
- Streaming service crashes or restarts during cleanup
- Channel buffer exhaustion or receiver drops
- Timeout during streaming service shutdown

**Attack Vectors**:
- An adversary could disrupt network connectivity at precise moments
- An adversary operating malicious peers could trigger streaming service failures
- Resource exhaustion attacks on the streaming service could cause termination failures

The vulnerability requires no special privileges and can manifest during normal operation under network stress.

## Recommendation

The fix is to **unconditionally clear the stream reference** before attempting termination, or use error handling that ensures cleanup occurs regardless of termination success:

```rust
pub async fn reset_active_stream(
    &mut self,
    notification_and_feedback: Option<NotificationAndFeedback>,
) -> Result<(), Error> {
    // Extract stream ID before clearing the reference
    let stream_id_to_terminate = self.active_data_stream
        .as_ref()
        .map(|stream| stream.data_stream_id);
    
    // Clear the stream references FIRST, before termination
    self.active_data_stream = None;
    self.speculative_stream_state = None;
    
    // Then attempt termination (failure is non-fatal now)
    if let Some(data_stream_id) = stream_id_to_terminate {
        if let Err(error) = utils::terminate_stream_with_feedback(
            &mut self.streaming_client,
            data_stream_id,
            notification_and_feedback,
        )
        .await
        {
            warn!(
                "Failed to terminate stream {}, but local cleanup succeeded: {:?}",
                data_stream_id, error
            );
        }
    }
    
    Ok(())
}
```

This ensures the local state is always cleaned up, allowing the driver to create a new stream on the next iteration, even if remote termination fails.

## Proof of Concept

**Reproduction Steps**:

1. Set up an Aptos node with state sync enabled
2. Configure the streaming service to have an artificially low channel buffer size
3. Inject network latency to cause stream timeouts
4. When `CriticalDataStreamTimeout` occurs, inject a failure in the streaming service's request handler (e.g., drop the termination request or close the channel)
5. Observe that the node's error metrics show repeated `critical_data_stream_timeout` errors
6. Verify via logs that `active_data_stream.is_some()` remains true across multiple progress check intervals
7. Confirm that no new stream is created and state sync progress halts

**Expected Behavior**: After timeout, the node should clean up the old stream and create a new one on the next iteration.

**Actual Behavior**: The node repeatedly attempts to use the dead stream, incrementing timeout counters indefinitely without making progress.

**Rust Unit Test Outline**:
```rust
#[tokio::test]
async fn test_stream_cleanup_on_termination_failure() {
    // Create a bootstrapper with a mock streaming client
    // that fails termination requests
    let mut bootstrapper = create_test_bootstrapper_with_failing_termination();
    
    // Trigger CriticalDataStreamTimeout
    let result = bootstrapper.fetch_next_data_notification().await;
    assert!(matches!(result, Err(Error::CriticalDataStreamTimeout(_))));
    
    // Verify active_data_stream was NOT cleared (bug)
    assert!(bootstrapper.active_data_stream.is_some());
    
    // Next progress attempt will fail again with the same stream
    let result = bootstrapper.drive_progress(&global_summary).await;
    assert!(result.is_err());
}
```

## Notes

The same vulnerability exists in the continuous syncer implementation at identical code locations, indicating this is a systematic issue in the stream cleanup pattern: [8](#0-7) 

Both implementations require the same fix to ensure reliable stream cleanup under all error conditions.

### Citations

**File:** state-sync/state-sync-driver/src/utils.rs (L226-230)
```rust
        if active_data_stream.num_consecutive_timeouts >= max_num_stream_timeouts {
            Err(Error::CriticalDataStreamTimeout(format!(
                "{:?}",
                max_num_stream_timeouts
            )))
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L424-436)
```rust
        if self.active_data_stream.is_some() {
            // We have an active data stream. Process any notifications!
            self.process_active_stream_notifications().await?;
        } else if self.storage_synchronizer.pending_storage_data() {
            // Wait for any pending data to be processed
            sample!(
                SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                info!("Waiting for the storage synchronizer to handle pending data!")
            );
        } else {
            // Fetch a new data stream to start streaming data
            self.initialize_active_data_stream(global_data_summary)
                .await?;
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L592-596)
```rust
        if matches!(result, Err(Error::CriticalDataStreamTimeout(_))) {
            // If the stream has timed out too many times, we need to reset it
            warn!("Resetting the currently active data stream due to too many timeouts!");
            self.reset_active_stream(None).await?;
        }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1543-1555)
```rust
        if let Some(active_data_stream) = &self.active_data_stream {
            let data_stream_id = active_data_stream.data_stream_id;
            utils::terminate_stream_with_feedback(
                &mut self.streaming_client,
                data_stream_id,
                notification_and_feedback,
            )
            .await?;
        }

        self.active_data_stream = None;
        self.speculative_stream_state = None;
        Ok(())
```

**File:** state-sync/data-streaming-service/src/streaming_client.rs (L311-323)
```rust
    async fn send_stream_request(
        &self,
        client_request: StreamRequest,
    ) -> Result<oneshot::Receiver<Result<DataStreamListener, Error>>, Error> {
        let mut request_sender = self.request_sender.clone();
        let (response_sender, response_receiver) = oneshot::channel();
        let request_message = StreamRequestMessage {
            stream_request: client_request,
            response_sender,
        };
        request_sender.send(request_message).await?;

        Ok(response_receiver)
```

**File:** state-sync/state-sync-driver/src/driver.rs (L711-719)
```rust
        } else if let Err(error) = self.bootstrapper.drive_progress(&global_data_summary).await {
            sample!(
                    SampleRate::Duration(Duration::from_secs(DRIVER_ERROR_LOG_FREQ_SECS)),
                    warn!(LogSchema::new(LogEntry::Driver)
                        .error(&error)
                        .message("Error found when checking the bootstrapper progress!"));
            );
            metrics::increment_counter(&metrics::BOOTSTRAPPER_ERRORS, error.get_label());
        };
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L997-1003)
```rust
pub struct DataStreamListener {
    pub data_stream_id: DataStreamId,
    notification_receiver: mpsc::Receiver<DataNotification>,

    /// Stores the number of consecutive timeouts encountered when listening to this stream
    pub num_consecutive_timeouts: u64,
}
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L529-541)
```rust
        if let Some(active_data_stream) = &self.active_data_stream {
            let data_stream_id = active_data_stream.data_stream_id;
            utils::terminate_stream_with_feedback(
                &mut self.streaming_client,
                data_stream_id,
                notification_and_feedback,
            )
            .await?;
        }

        self.active_data_stream = None;
        self.speculative_stream_state = None;
        Ok(())
```
