# Audit Report

## Title
Race Condition in ConsensusPublisher Causes Partial Message Streams to Observers

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) race condition exists between `publish_message()` and `remove_active_subscriber()` in the ConsensusPublisher, allowing observers to receive incomplete consensus message sequences. This leads to temporary state desynchronization requiring fallback to state sync for recovery.

## Finding Description

The vulnerability exists in the interaction between two concurrent operations in the ConsensusPublisher: [1](#0-0) [2](#0-1) 

The race condition occurs as follows:

**Thread Timeline:**

1. **T0**: `publish_message(OrderedBlock)` begins execution [3](#0-2) 
   - Takes a snapshot of `active_subscribers` containing {Peer1, Peer2, Peer3}

2. **T1**: Peer2 sends unsubscribe request, processed concurrently [4](#0-3) 
   - `remove_active_subscriber()` removes Peer2 from the set

3. **T2**: Thread from T0 continues sending OrderedBlock [5](#0-4) 
   - Uses the stale snapshot still containing Peer2
   - Successfully queues OrderedBlock for Peer2

4. **T3**: `publish_message(BlockPayload)` begins with fresh snapshot
   - New snapshot excludes Peer2 (already removed at T1)
   - BlockPayload NOT sent to Peer2

5. **T4**: `publish_message(CommitDecision)` begins
   - Snapshot excludes Peer2
   - CommitDecision NOT sent to Peer2

**Observer Side Impact:**

Peer2 receives OrderedBlock but not subsequent BlockPayload or CommitDecision messages. When the OrderedBlock arrives: [6](#0-5) 

If payloads don't exist, the block is stored as pending: [7](#0-6) 

The observer waits indefinitely for BlockPayload that will never arrive because the publisher excluded it from subsequent message batches.

**Root Cause Analysis:**

The `get_active_subscribers()` method returns a cloned snapshot: [8](#0-7) 

While the RwLock prevents data corruption, it does NOT prevent TOCTOU races because:
- The read lock is released immediately after cloning
- `remove_active_subscriber()` can modify the set between snapshot creation and message sending
- Each `publish_message()` call gets an independent snapshot

## Impact Explanation

**Severity Assessment: Medium**

This vulnerability fits the **"State inconsistencies requiring intervention"** category from the Aptos bug bounty Medium severity criteria:

1. **State Inconsistency**: Observers maintain incomplete consensus state with pending blocks waiting for payloads that will never arrive

2. **Requires Intervention**: While the system has automatic fallback mechanisms, state sync intervention is still required: [9](#0-8) 

3. **Impact Scope**:
   - Affects observer nodes (non-consensus participants)
   - Does NOT affect validator consensus safety
   - Causes temporary resource waste (pending blocks stored)
   - Delays block processing until timeout triggers fallback
   - Multiple observers could be affected simultaneously during subscription churn

4. **Recovery Path**: The system eventually recovers via `check_syncing_progress()` timeout and fallback to state sync, but this represents degraded operation requiring state sync intervention.

**Why Not Higher Severity:**
- No validator consensus impact
- No fund loss or manipulation
- Temporary condition with built-in recovery
- No critical invariant violations

**Why Not Lower Severity:**
- Causes actual state inconsistency (not just minor bugs)
- Requires system intervention (state sync fallback)
- Can affect multiple nodes simultaneously
- Wastes resources during degraded operation

## Likelihood Explanation

**Likelihood: Medium to High**

This race condition is likely to occur in production because:

1. **Natural Occurrence**: The race window exists during normal operations when:
   - Subscription health checks trigger unsubscribes
   - Network conditions cause peer disconnections
   - Consensus is actively producing blocks with multiple message types

2. **Timing Factors**: 
   - Consensus regularly publishes message sequences (OrderedBlock → BlockPayload → CommitDecision)
   - These publishes happen in rapid succession
   - Subscription changes occur asynchronously

3. **No Synchronization**: The current implementation has no mechanism to prevent this race - each `publish_message()` call independently snapshots active_subscribers

4. **Observable in Testing**: A stress test with concurrent subscription changes and message publishing would reliably reproduce this issue

## Recommendation

**Fix: Ensure Atomic Snapshot Consistency Across Related Message Sequences**

Modify the publisher to maintain snapshot consistency when publishing related messages. Two approaches:

**Option 1: Batch Publishing with Snapshot Lock**
```rust
// Add method to publish multiple related messages atomically
pub fn publish_message_batch(&self, messages: Vec<ConsensusObserverDirectSend>) {
    // Get snapshot once for entire batch
    let active_subscribers = self.get_active_subscribers();
    
    // Send all messages to same subscriber set
    for message in messages {
        for peer_network_id in &active_subscribers {
            let mut outbound_message_sender = self.outbound_message_sender.clone();
            if let Err(error) = outbound_message_sender.try_send((*peer_network_id, message.clone())) {
                // error handling
            }
        }
    }
}
```

**Option 2: Sequence-Based Delivery with Generation Counters**
```rust
// Add generation counter to track subscription changes
struct ConsensusPublisher {
    active_subscribers: Arc<RwLock<HashSet<PeerNetworkId>>>,
    subscriber_generation: Arc<AtomicU64>,  // Increment on any change
    // ...
}

// Track generation when getting snapshot
pub fn get_active_subscribers_with_generation(&self) -> (HashSet<PeerNetworkId>, u64) {
    let subscribers = self.active_subscribers.read().clone();
    let generation = self.subscriber_generation.load(Ordering::SeqCst);
    (subscribers, generation)
}

// Reject stale snapshots during send
fn remove_active_subscriber(&self, peer_network_id: &PeerNetworkId) {
    self.active_subscribers.write().remove(peer_network_id);
    self.subscriber_generation.fetch_add(1, Ordering::SeqCst);
}
```

**Preferred Solution**: Implement Option 1 for related consensus messages that must be delivered together, ensuring atomic snapshot consistency.

## Proof of Concept

```rust
#[tokio::test]
async fn test_race_condition_partial_message_stream() {
    use std::sync::Arc;
    use tokio::time::{sleep, Duration};
    
    // Setup: Create consensus publisher and subscribe observer
    let (consensus_publisher, mut outbound_receiver) = 
        ConsensusPublisher::new(
            ConsensusObserverConfig::default(),
            consensus_observer_client,
        );
    
    let peer_network_id = PeerNetworkId::new(NetworkId::Public, PeerId::random());
    
    // Observer subscribes
    consensus_publisher.add_active_subscriber(peer_network_id);
    assert!(consensus_publisher.get_active_subscribers().contains(&peer_network_id));
    
    // Spawn concurrent tasks to reproduce race
    let publisher_clone = consensus_publisher.clone();
    let publish_task = tokio::spawn(async move {
        // Publish sequence of related messages
        for i in 0..3 {
            let message = create_test_message(i);
            publisher_clone.publish_message(message);
            sleep(Duration::from_micros(10)).await; // Small delay between publishes
        }
    });
    
    let publisher_clone2 = consensus_publisher.clone();
    let unsubscribe_task = tokio::spawn(async move {
        sleep(Duration::from_micros(15)).await; // Unsubscribe during message sequence
        publisher_clone2.remove_active_subscriber(&peer_network_id);
    });
    
    publish_task.await.unwrap();
    unsubscribe_task.await.unwrap();
    
    // Collect messages received by observer
    let mut received_messages = vec![];
    while let Ok(Some((peer, msg))) = timeout(Duration::from_millis(100), outbound_receiver.next()).await {
        if peer == peer_network_id {
            received_messages.push(msg);
        }
    }
    
    // VULNERABILITY: Observer may receive partial message stream (e.g., message 0 and 1, but not 2)
    // Expected: Either all 3 messages or none (atomic subscription state)
    // Actual: Can receive subset due to race condition
    assert!(received_messages.len() < 3 && received_messages.len() > 0, 
        "Race condition: received {} messages instead of 0 or 3", received_messages.len());
}
```

## Notes

This vulnerability represents a classic TOCTOU (Time-of-Check-Time-of-Use) race condition where the snapshot of active subscribers becomes stale between acquisition and use. While observer nodes have built-in recovery mechanisms via fallback to state sync, the race condition causes temporary state inconsistencies and resource waste that meet the Medium severity threshold of "state inconsistencies requiring intervention."

The fix requires ensuring atomic consistency of the subscriber snapshot across related message sequences, either through batch publishing or generation-based validation.

### Citations

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L158-160)
```rust
    pub fn get_active_subscribers(&self) -> HashSet<PeerNetworkId> {
        self.active_subscribers.read().clone()
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L163-165)
```rust
    fn remove_active_subscriber(&self, peer_network_id: &PeerNetworkId) {
        self.active_subscribers.write().remove(peer_network_id);
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L194-205)
```rust
            ConsensusObserverRequest::Unsubscribe => {
                // Remove the peer from the set of active subscribers
                self.remove_active_subscriber(&peer_network_id);
                info!(LogSchema::new(LogEntry::ConsensusPublisher)
                    .event(LogEvent::Subscription)
                    .message(&format!(
                        "Peer unsubscribed from consensus updates! Peer: {:?}",
                        peer_network_id
                    )));

                // Send a simple unsubscription ACK
                response_sender.send(ConsensusObserverResponse::UnsubscribeAck);
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L212-232)
```rust
    pub fn publish_message(&self, message: ConsensusObserverDirectSend) {
        // Get the active subscribers
        let active_subscribers = self.get_active_subscribers();

        // Send the message to all active subscribers
        for peer_network_id in &active_subscribers {
            // Send the message to the outbound receiver for publishing
            let mut outbound_message_sender = self.outbound_message_sender.clone();
            if let Err(error) =
                outbound_message_sender.try_send((*peer_network_id, message.clone()))
            {
                // The message send failed
                warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::SendDirectSendMessage)
                        .message(&format!(
                            "Failed to send outbound message to the receiver for peer {:?}! Error: {:?}",
                            peer_network_id, error
                    )));
            }
        }
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L156-165)
```rust
    /// Returns true iff all payloads exist for the given blocks
    fn all_payloads_exist(&self, blocks: &[Arc<PipelinedBlock>]) -> bool {
        // If quorum store is disabled, all payloads exist (they're already in the blocks)
        if !self.observer_epoch_state.is_quorum_store_enabled() {
            return true;
        }

        // Otherwise, check if all the payloads exist in the payload store
        self.observer_block_data.lock().all_payloads_exist(blocks)
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L191-200)
```rust
        if let Err(error) = self.observer_fallback_manager.check_syncing_progress() {
            // Log the error and enter fallback mode
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Failed to make syncing progress! Entering fallback mode! Error: {:?}",
                    error
                ))
            );
            self.enter_fallback_mode().await;
            return;
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L704-713)
```rust
        // If all payloads exist, process the block. Otherwise, store it
        // in the pending block store and wait for the payloads to arrive.
        if self.all_payloads_exist(pending_block_with_metadata.ordered_block().blocks()) {
            self.process_ordered_block(pending_block_with_metadata)
                .await;
        } else {
            self.observer_block_data
                .lock()
                .insert_pending_block(pending_block_with_metadata);
        }
```
