# Audit Report

## Title
State View Corruption via Cross-Block KV Response Race Condition in Remote Sharded Execution

## Summary
A critical race condition exists in the remote sharded execution system where KV (key-value) responses from a previous block can be written into the state view of a subsequent block, causing state corruption and consensus divergence between validators.

## Finding Description

The vulnerability exists in the interaction between `RemoteStateViewClient::init_for_block()` and `RemoteStateValueReceiver::handle_message()`. The system lacks block identifiers in KV responses and proper synchronization when transitioning between block executions.

**Architecture Overview:**
The remote sharded execution uses a coordinator-shard model where shards request state values from the coordinator. Remote sharded execution is a production feature with a standalone executable. [1](#0-0) 

The coordinator decides whether to use remote sharded execution based on configured addresses. [2](#0-1) 

**The Critical Flaw:**

KV responses contain no block identifier - only state key-value pairs: [3](#0-2) 

When a shard receives an execute command, it calls `init_for_block()` which acquires a WRITE lock and replaces the entire state view with a new empty one, without draining pending messages: [4](#0-3) 

The execute command is received and processed here: [5](#0-4) 

Meanwhile, KV responses arrive asynchronously via a persistent crossbeam channel and are processed by spawning handlers to a thread pool: [6](#0-5) 

Each response handler acquires a READ lock and writes values without validating which block's state view it's writing to: [7](#0-6) 

**Attack Scenario:**

1. Shard executes block N, sends KV requests to coordinator
2. Multiple KV responses arrive and are queued in the `kv_rx` crossbeam channel
3. `RemoteStateValueReceiver::start()` receives them and spawns each to thread pool for concurrent processing
4. Some handlers are processing, holding READ locks
5. Block N execution completes, results are sent back
6. Coordinator immediately sends execute command for block N+1
7. `init_for_block()` is called for block N+1, acquires WRITE lock (blocks until READ locks released)
8. Once acquired, `init_for_block()` replaces the entire state view with a new empty one
9. **Critical Race**: KV responses for block N that were still queued in the channel are now processed
10. These handlers acquire READ locks on the **new** state view (for block N+1)
11. They write block N's state values into block N+1's state view
12. Block N+1 execution reads these stale values instead of querying for fresh ones
13. Different validators process messages at different speeds, causing **non-deterministic state views**
14. Validators produce different state roots for block N+1
15. **Consensus divergence**

The RwLock provides memory safety but doesn't prevent logical errors. Handlers spawned before `init_for_block()` will write to whichever state view exists when they acquire their READ lock, with no validation that it's the correct block's state view.

## Impact Explanation

**Critical Severity - Consensus Safety Violation**

This vulnerability breaks the fundamental **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks."

**Concrete Impact:**
- **Consensus Divergence**: Different validators produce different state roots for the same block, causing permanent chain splits
- **State Corruption**: Blocks execute with incorrect state values (pre-block-N state instead of post-block-N state), leading to wrong transaction outputs
- **Non-recoverable**: Once validators diverge, they cannot reconcile without manual intervention or hard fork
- **Network Partition**: The blockchain fragments into incompatible forks

This meets **Critical Severity** criteria per Aptos Bug Bounty:
- Consensus/Safety violations: Different validators commit different state roots
- Non-recoverable network partition (requires hardfork to resolve)

The vulnerability is particularly severe because it's **timing-dependent**: validators with different network latencies or processing speeds will experience different message interleavings, making the divergence appear random and difficult to debug.

## Likelihood Explanation

**High Likelihood** - The vulnerability triggers under normal operation conditions:

**Triggering Conditions:**
1. Remote sharded execution is enabled (production feature with standalone executable)
2. Multiple blocks are processed in succession (always true in blockchain operation)
3. Network latency causes KV responses to arrive after execution completes (common in distributed systems)
4. Coordinator sends next execute command immediately after receiving results (normal flow)

**No Special Attacker Capabilities Required:**
- No malicious validator required
- No Byzantine behavior needed
- Occurs naturally due to asynchronous message processing
- Network timing variations are sufficient

**Frequency:**
- Likely to occur multiple times per minute in a busy sharded execution environment
- Higher probability during high transaction throughput
- Exacerbated by network latency between coordinator and shards

The race window is non-trivial: from when the last READ lock is released until all queued KV responses are drained. With batch sizes up to 200 keys and concurrent processing, this window can be milliseconds to seconds. [8](#0-7) 

## Recommendation

Add block identifiers to all KV messages and validate them before processing:

1. **Add block identifier to RemoteKVRequest and RemoteKVResponse**:
```rust
pub struct RemoteKVRequest {
    pub shard_id: ShardId,
    pub block_id: u64,  // Add block counter or hash
    pub state_keys: Vec<StateKey>,
}

pub struct RemoteKVResponse {
    pub block_id: u64,  // Add matching block identifier
    pub(crate) inner: Vec<(StateKey, Option<StateValue>)>,
}
```

2. **Track current block ID in RemoteStateView**:
```rust
pub struct RemoteStateView {
    block_id: u64,
    state_values: DashMap<StateKey, RemoteStateValue>,
}
```

3. **Validate block ID in handle_message**:
```rust
fn handle_message(...) {
    let response: RemoteKVResponse = bcs::from_bytes(&message.data).unwrap();
    let state_view_lock = state_view.read().unwrap();
    
    // Validate block ID matches
    if response.block_id != state_view_lock.block_id {
        // Drop stale responses
        return;
    }
    
    // Process responses...
}
```

4. **Increment block ID in init_for_block**:
```rust
pub fn init_for_block(&self, block_id: u64, state_keys: Vec<StateKey>) {
    let mut state_view = self.state_view.write().unwrap();
    *state_view = RemoteStateView::new(block_id);
    // Continue with prefetch...
}
```

Alternatively, drain the channel before transitioning blocks, but this is more complex and error-prone.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Setting up a remote sharded execution environment with coordinator and shards
2. Introducing artificial network delays to KV responses
3. Processing two blocks in quick succession
4. Observing that block N+1's state view contains values from block N's responses
5. Comparing state roots across validators with different network conditions

A full PoC would require integration testing infrastructure, but the code analysis clearly shows the race condition exists due to:
- Lack of block identifiers in messages
- Persistent channel across block transitions
- No validation of message relevance to current block
- Concurrent processing without coordination

## Notes

This vulnerability affects the remote sharded execution feature, which is a production component with its own standalone executable. The issue is a classic example of a logical race condition that passes memory safety checks (RwLock) but violates application-level invariants (deterministic execution). The fix requires adding block identifiers to all KV messages and validating them before processing to ensure handlers only write to the correct block's state view.

### Citations

**File:** execution/executor-service/src/main.rs (L1-48)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use aptos_executor_service::process_executor_service::ProcessExecutorService;
use aptos_logger::info;
use clap::Parser;
use std::net::SocketAddr;

#[derive(Debug, Parser)]
struct Args {
    #[clap(long, default_value_t = 8)]
    pub num_executor_threads: usize,

    #[clap(long)]
    pub shard_id: usize,

    #[clap(long)]
    pub num_shards: usize,

    #[clap(long, num_args = 1..)]
    pub remote_executor_addresses: Vec<SocketAddr>,

    #[clap(long)]
    pub coordinator_address: SocketAddr,
}

fn main() {
    let args = Args::parse();
    aptos_logger::Logger::new().init();

    let (tx, rx) = crossbeam_channel::unbounded();
    ctrlc::set_handler(move || {
        tx.send(()).unwrap();
    })
    .expect("Error setting Ctrl-C handler");

    let _exe_service = ProcessExecutorService::new(
        args.shard_id,
        args.num_shards,
        args.num_executor_threads,
        args.coordinator_address,
        args.remote_executor_addresses,
    );

    rx.recv()
        .expect("Could not receive Ctrl-C msg from channel.");
    info!("Process executor service shutdown successfully.");
}
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```

**File:** execution/executor-service/src/lib.rs (L83-91)
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct RemoteKVResponse {
    pub(crate) inner: Vec<(StateKey, Option<StateValue>)>,
}

impl RemoteKVResponse {
    pub fn new(inner: Vec<(StateKey, Option<StateValue>)>) -> Self {
        Self { inner }
    }
```

**File:** execution/executor-service/src/remote_state_view.rs (L27-27)
```rust
pub static REMOTE_STATE_KEY_BATCH_SIZE: usize = 200;
```

**File:** execution/executor-service/src/remote_state_view.rs (L118-124)
```rust
    pub fn init_for_block(&self, state_keys: Vec<StateKey>) {
        *self.state_view.write().unwrap() = RemoteStateView::new();
        REMOTE_EXECUTOR_REMOTE_KV_COUNT
            .with_label_values(&[&self.shard_id.to_string(), "prefetch_kv"])
            .inc_by(state_keys.len() as u64);
        self.pre_fetch_state_values(state_keys, false);
    }
```

**File:** execution/executor-service/src/remote_state_view.rs (L233-241)
```rust
    fn start(&self) {
        while let Ok(message) = self.kv_rx.recv() {
            let state_view = self.state_view.clone();
            let shard_id = self.shard_id;
            self.thread_pool.spawn(move || {
                Self::handle_message(shard_id, message, state_view);
            });
        }
    }
```

**File:** execution/executor-service/src/remote_state_view.rs (L243-272)
```rust
    fn handle_message(
        shard_id: ShardId,
        message: Message,
        state_view: Arc<RwLock<RemoteStateView>>,
    ) {
        let _timer = REMOTE_EXECUTOR_TIMER
            .with_label_values(&[&shard_id.to_string(), "kv_responses"])
            .start_timer();
        let bcs_deser_timer = REMOTE_EXECUTOR_TIMER
            .with_label_values(&[&shard_id.to_string(), "kv_resp_deser"])
            .start_timer();
        let response: RemoteKVResponse = bcs::from_bytes(&message.data).unwrap();
        drop(bcs_deser_timer);

        REMOTE_EXECUTOR_REMOTE_KV_COUNT
            .with_label_values(&[&shard_id.to_string(), "kv_responses"])
            .inc();
        let state_view_lock = state_view.read().unwrap();
        trace!(
            "Received state values for shard {} with size {}",
            shard_id,
            response.inner.len()
        );
        response
            .inner
            .into_iter()
            .for_each(|(state_key, state_value)| {
                state_view_lock.set_state_value(&state_key, state_value);
            });
    }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L80-113)
```rust
    fn receive_execute_command(&self) -> ExecutorShardCommand<RemoteStateViewClient> {
        match self.command_rx.recv() {
            Ok(message) => {
                let _rx_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx"])
                    .start_timer();
                let bcs_deser_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx_bcs_deser"])
                    .start_timer();
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
                drop(bcs_deser_timer);

                match request {
                    RemoteExecutionRequest::ExecuteBlock(command) => {
                        let init_prefetch_timer = REMOTE_EXECUTOR_TIMER
                            .with_label_values(&[&self.shard_id.to_string(), "init_prefetch"])
                            .start_timer();
                        let state_keys = Self::extract_state_keys(&command);
                        self.state_view_client.init_for_block(state_keys);
                        drop(init_prefetch_timer);

                        let (sub_blocks, concurrency, onchain_config) = command.into();
                        ExecutorShardCommand::ExecuteSubBlocks(
                            self.state_view_client.clone(),
                            sub_blocks,
                            concurrency,
                            onchain_config,
                        )
                    },
                }
            },
            Err(_) => ExecutorShardCommand::Stop,
        }
    }
```
