# Audit Report

## Title
Validator Performance Statistics Escape via Limited Failed Authors Tracking

## Summary
The default value of `max_failed_authors_to_store = 10` in `ConsensusConfigV1` creates a vulnerability where validators who fail to propose blocks during extended network disruptions (>10 consecutive failed rounds) can escape performance penalties and continue receiving full staking rewards despite their failures.

## Finding Description

The vulnerability stems from the interaction between consensus block metadata tracking and staking reward distribution: [1](#0-0) 

The `max_failed_authors_to_store` parameter limits how many consecutive failed proposers are recorded in block metadata. When generating proposals, the system computes failed authors but caps the list at this maximum: [2](#0-1) 

The critical issue is in the `compute_failed_authors` function where the start round is calculated as `max(previous_round + 1, end_round - max_failed_authors_to_store)`. This means if there are more than 10 consecutive failed rounds, only the 10 most recent failed proposers are included. **Failed proposers from earlier rounds are silently omitted.**

These failed authors are then converted to indices and passed to the Move framework during block execution: [3](#0-2) 

The `update_performance_statistics` function only processes validators in the `failed_proposer_indices` list: [4](#0-3) 

**Attack Scenario:**

1. Network experiences >10 consecutive failed rounds (e.g., during DDoS attack, network partition, or coordinated failure)
2. Validator A fails to propose at round N
3. Validators B-K fail at rounds N+1 through N+10
4. At round N+11, a successful proposal includes only failed_authors for rounds N+1 through N+10
5. Validator A's failure at round N is **not recorded** in the block metadata
6. Validator A's `failed_proposals` counter is **not incremented** in `ValidatorPerformance`
7. Validator A receives full staking rewards despite the failure

**Evidence from Test Suite:**

The test suite explicitly demonstrates this behavior: [5](#0-4) 

The test shows that when there are 12 consecutive failed rounds (3-14), only the last 10 are included (rounds 5-14). Rounds 3 and 4 are omitted entirely.

## Impact Explanation

This vulnerability breaks the **Staking Security** invariant: "Validator rewards and penalties must be calculated correctly."

**Severity: LOW** (per Aptos Bug Bounty criteria - up to $1,000)
- Category: "Non-critical implementation bugs"
- Does not break consensus safety or cause direct fund theft
- Creates economic unfairness in validator reward distribution
- Validators escape performance penalties under specific network conditions

**Economic Impact:**
- During periods with >10 consecutive failed rounds, early-failing validators avoid penalties
- With 100 validators and 1% epoch rewards (~$10,000 for large validators), escaped penalties could be $100-200 per occurrence
- Over multiple epochs or with coordinated behavior, this compounds

**Realistic Conditions:**
- Network partitions causing >10 failed rounds
- DDoS attacks on validator nodes
- Natural network degradation periods
- Potentially exploitable by validators timing failures strategically

## Likelihood Explanation

**Likelihood: MEDIUM**

The vulnerability requires >10 consecutive failed rounds to manifest. While not common under normal operations, this threshold is reachable:

1. **Network Partitions:** Aptos's BFT consensus requires >2/3 validators to be reachable. Partial partitions affecting leader selection can cause extended failed rounds.

2. **DDoS Attacks:** Targeted attacks on sequential proposers can create consecutive failures.

3. **Natural Network Issues:** Internet routing problems, data center outages, or cloud provider issues can cause extended disruptions.

4. **Strategic Exploitation:** Malicious validators could intentionally fail early in a sequence they anticipate will exceed 10 rounds, knowing they'll escape penalties.

The codebase demonstrates awareness of this scenario through dedicated test cases, indicating it's considered a realistic edge case rather than theoretical.

## Recommendation

**Solution 1: Increase Default Value**
Change `max_failed_authors_to_store` to a higher value (e.g., 50 or 100) to better handle extended network disruptions:

```rust
impl Default for ConsensusConfigV1 {
    fn default() -> Self {
        Self {
            decoupled_execution: true,
            back_pressure_limit: 10,
            exclude_round: 40,
            max_failed_authors_to_store: 50,  // Increased from 10
            proposer_election_type: ProposerElectionType::LeaderReputation(
                // ... rest unchanged
            ),
        }
    }
}
```

**Solution 2: Implement Overflow Tracking**
When failed authors exceed the limit, record the overflow count in block metadata and distribute penalties proportionally to all validators who were eligible proposers during the overflow period.

**Solution 3: Emit Warning Events**
When failed authors are truncated, emit blockchain events to alert the network that performance tracking has incomplete data, enabling off-chain monitoring and governance response.

**Trade-offs:**
- Higher limits increase block metadata size slightly
- Must balance between accurate tracking and practical constraints
- Consider network bandwidth and storage implications

**Governance Path:**
The parameters can be updated via on-chain governance: [6](#0-5) 

## Proof of Concept

The existing test suite already demonstrates this vulnerability:

```rust
// From consensus/src/liveness/proposal_generator_test.rs lines 145-158

// Create scenario with 13 failed rounds (b1 at round 2, proposal at round 15)
inserter.insert_qc_for_block(b1.as_ref(), None);
let b1_child_res = proposal_generator
    .generate_proposal(15, proposer_election.clone())
    .await
    .unwrap();

// Verify only last 10 failed authors are recorded
assert_eq!(b1_child_res.failed_authors().unwrap().len(), 10);
assert_eq!(b1_child_res.failed_authors().unwrap().first().unwrap().0, 5);
assert_eq!(b1_child_res.failed_authors().unwrap().last().unwrap().0, 14);

// Rounds 3 and 4 are MISSING from failed_authors
// Validators who failed at rounds 3-4 escape performance penalties
```

To reproduce the full attack:

1. Set up a test network with multiple validators
2. Simulate >10 consecutive failed rounds (via network partition or controlled failure)
3. Observe that early-failing validators' `failed_proposals` counters are not incremented
4. Verify these validators still receive full staking rewards at epoch end
5. Confirm this violates the staking invariant requiring correct penalty calculation

## Notes

The `exclude_round = 40` parameter appears appropriately sized for its purpose of excluding recent unstable data from leader reputation calculations. The primary vulnerability lies in the `max_failed_authors_to_store` limitation creating an economic fairness issue rather than a consensus safety problem.

This is a **design limitation with security implications** rather than a critical bug. The 10-round limit makes sense for normal operations but creates an exploitable edge case during extended network issues. Governance should consider increasing this value or implementing overflow handling mechanisms.

### Citations

**File:** types/src/on_chain_config/consensus_config.rs (L471-506)
```rust
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
pub struct ConsensusConfigV1 {
    pub decoupled_execution: bool,
    // Deprecated and unused, cannot be renamed easily, due to yaml on framework_upgrade test
    pub back_pressure_limit: u64,
    pub exclude_round: u64,
    pub proposer_election_type: ProposerElectionType,
    pub max_failed_authors_to_store: usize,
}

impl Default for ConsensusConfigV1 {
    fn default() -> Self {
        Self {
            decoupled_execution: true,
            back_pressure_limit: 10,
            exclude_round: 40,
            max_failed_authors_to_store: 10,
            proposer_election_type: ProposerElectionType::LeaderReputation(
                LeaderReputationType::ProposerAndVoterV2(ProposerAndVoterConfig {
                    active_weight: 1000,
                    inactive_weight: 10,
                    failed_weight: 1,
                    failure_threshold_percent: 10, // = 10%
                    // In each round we get stastics for the single proposer
                    // and large number of validators. So the window for
                    // the proposers needs to be significantly larger
                    // to have enough useful statistics.
                    proposer_window_num_validators_multiplier: 10,
                    voter_window_num_validators_multiplier: 1,
                    weight_by_voting_power: true,
                    use_history_from_previous_epoch_max_count: 5,
                }),
            ),
        }
    }
}
```

**File:** consensus/src/liveness/proposal_generator.rs (L882-902)
```rust
    /// Compute the list of consecutive proposers from the
    /// immediately preceeding rounds that didn't produce a successful block
    pub fn compute_failed_authors(
        &self,
        round: Round,
        previous_round: Round,
        include_cur_round: bool,
        proposer_election: Arc<dyn ProposerElection>,
    ) -> Vec<(Round, Author)> {
        let end_round = round + u64::from(include_cur_round);
        let mut failed_authors = Vec::new();
        let start = std::cmp::max(
            previous_round + 1,
            end_round.saturating_sub(self.max_failed_authors_to_store as u64),
        );
        for i in start..end_round {
            failed_authors.push((i, proposer_election.get_valid_proposer(i)));
        }

        failed_authors
    }
```

**File:** aptos-move/framework/aptos-framework/sources/block.move (L154-199)
```text
    fun block_prologue_common(
        vm: &signer,
        hash: address,
        epoch: u64,
        round: u64,
        proposer: address,
        failed_proposer_indices: vector<u64>,
        previous_block_votes_bitvec: vector<u8>,
        timestamp: u64
    ): u64 acquires BlockResource, CommitHistory {
        // Operational constraint: can only be invoked by the VM.
        system_addresses::assert_vm(vm);

        // Blocks can only be produced by a valid proposer or by the VM itself for Nil blocks (no user txs).
        assert!(
            proposer == @vm_reserved || stake::is_current_epoch_validator(proposer),
            error::permission_denied(EINVALID_PROPOSER),
        );

        let proposer_index = option::none();
        if (proposer != @vm_reserved) {
            proposer_index = option::some(stake::get_validator_index(proposer));
        };

        let block_metadata_ref = borrow_global_mut<BlockResource>(@aptos_framework);
        block_metadata_ref.height = event::counter(&block_metadata_ref.new_block_events);

        let new_block_event = NewBlockEvent {
            hash,
            epoch,
            round,
            height: block_metadata_ref.height,
            previous_block_votes_bitvec,
            proposer,
            failed_proposer_indices,
            time_microseconds: timestamp,
        };
        emit_new_block_event(vm, &mut block_metadata_ref.new_block_events, new_block_event);

        // Performance scores have to be updated before the epoch transition as the transaction that triggers the
        // transition is the last block in the previous epoch.
        stake::update_performance_statistics(proposer_index, failed_proposer_indices);
        state_storage::on_new_block(reconfiguration::current_epoch());

        block_metadata_ref.epoch_interval
    }
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L1282-1330)
```text
    public(friend) fun update_performance_statistics(
        proposer_index: Option<u64>,
        failed_proposer_indices: vector<u64>
    ) acquires ValidatorPerformance {
        // Validator set cannot change until the end of the epoch, so the validator index in arguments should
        // match with those of the validators in ValidatorPerformance resource.
        let validator_perf = borrow_global_mut<ValidatorPerformance>(@aptos_framework);
        let validator_len = vector::length(&validator_perf.validators);

        spec {
            update ghost_valid_perf = validator_perf;
            update ghost_proposer_idx = proposer_index;
        };
        // proposer_index is an option because it can be missing (for NilBlocks)
        if (option::is_some(&proposer_index)) {
            let cur_proposer_index = option::extract(&mut proposer_index);
            // Here, and in all other vector::borrow, skip any validator indices that are out of bounds,
            // this ensures that this function doesn't abort if there are out of bounds errors.
            if (cur_proposer_index < validator_len) {
                let validator = vector::borrow_mut(&mut validator_perf.validators, cur_proposer_index);
                spec {
                    assume validator.successful_proposals + 1 <= MAX_U64;
                };
                validator.successful_proposals = validator.successful_proposals + 1;
            };
        };

        let f = 0;
        let f_len = vector::length(&failed_proposer_indices);
        while ({
            spec {
                invariant len(validator_perf.validators) == validator_len;
                invariant (option::is_some(ghost_proposer_idx) && option::borrow(
                    ghost_proposer_idx
                ) < validator_len) ==>
                    (validator_perf.validators[option::borrow(ghost_proposer_idx)].successful_proposals ==
                        ghost_valid_perf.validators[option::borrow(ghost_proposer_idx)].successful_proposals + 1);
            };
            f < f_len
        }) {
            let validator_index = *vector::borrow(&failed_proposer_indices, f);
            if (validator_index < validator_len) {
                let validator = vector::borrow_mut(&mut validator_perf.validators, validator_index);
                spec {
                    assume validator.failed_proposals + 1 <= MAX_U64;
                };
                validator.failed_proposals = validator.failed_proposals + 1;
            };
            f = f + 1;
```

**File:** consensus/src/liveness/proposal_generator_test.rs (L154-158)
```rust
    // test that we have authors for the skipped rounds (5,  .. 14), as the limit of 10 has been reached
    assert_eq!(b1_child_res.failed_authors().unwrap().len(), 10);
    assert_eq!(b1_child_res.failed_authors().unwrap().first().unwrap().0, 5);
    assert_eq!(b1_child_res.failed_authors().unwrap().last().unwrap().0, 14);
}
```

**File:** consensus/src/epoch_manager.rs (L338-340)
```rust
                let seek_len = onchain_config.leader_reputation_exclude_round() as usize
                    + onchain_config.max_failed_authors_to_store()
                    + PROPOSER_ROUND_BEHIND_STORAGE_BUFFER;
```
