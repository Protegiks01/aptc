# Audit Report

## Title
Unbounded Request Queue Allows DoS via Expensive State Synchronization Requests

## Summary
The storage service server processes requests without adequate per-peer rate limiting on valid requests, allowing attackers to fill the request queue with expensive `GetStateValuesWithProof` operations. This blocks legitimate peers from synchronizing and can severely degrade node performance through database resource exhaustion.

## Finding Description
The storage service implements a request processing architecture where incoming requests are queued in a network channel (configurable size, default 4000 messages) before being processed. [1](#0-0) 

Each request pulled from the queue immediately spawns a blocking task without any global concurrency limit: [2](#0-1) 

The system has per-peer RPC concurrency limits (100 concurrent inbound RPCs per peer) [3](#0-2)  and a total inbound connection limit (100 connections) [4](#0-3) , but these are insufficient protection.

The `RequestModerator` only validates whether requests are serviceable (i.e., the requested data exists), but does not validate the cost or size of requests: [5](#0-4) 

For `GetStateValuesWithProof` requests, the moderator only checks if the version exists and a proof can be created, but not the range size (end_index - start_index): [6](#0-5) 

An attacker can send requests with extremely large ranges (e.g., start_index=0, end_index=u64::MAX-1), which pass validation. Although responses are truncated to `max_state_chunk_size`, each blocking task still initiates expensive database operations: [7](#0-6) 

**Attack Vector:**
1. Attacker establishes multiple peer connections (up to 100)
2. Each connection sends 100 concurrent expensive `GetStateValuesWithProof` requests with large ranges
3. Total: Up to 10,000 requests can be initiated, filling the 4000-message network queue
4. Each dequeued request spawns a blocking task that creates database iterators and begins fetching state values
5. Hundreds to thousands of concurrent database operations saturate I/O and lock resources
6. Legitimate peers experience queue full errors, long delays, or slow responses

The moderator tracks invalid requests per peer but has no rate limiting on valid requests: [8](#0-7) 

## Impact Explanation
This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria due to:

1. **Validator node slowdowns**: Saturating the storage service with expensive requests degrades node performance, affecting state synchronization critical for validator operations
2. **Significant protocol violations**: Blocking state synchronization prevents nodes from catching up to the latest blockchain state, violating the system's availability guarantees
3. **Affects network-wide synchronization**: All peers attempting to sync through the affected node are impacted

The attack does not directly cause loss of funds or consensus violations, but significantly degrades the network's ability to maintain availability and synchronization, which is a critical protocol requirement.

## Likelihood Explanation
The attack is **highly likely** to occur because:

1. **Low barrier to entry**: Any actor can establish peer connections without requiring validator privileges or stake
2. **Easy to execute**: Standard RPC calls with valid but expensive parameters
3. **No authentication required**: Public fullnodes accept connections from any peer
4. **Economically rational for competitors**: Degrading sync performance gives advantage to attacking validators
5. **Difficult to detect**: Valid requests that pass all existing validation checks

The only limiting factors are:
- Network bandwidth to send requests
- Ability to maintain multiple peer connections (100 max)
- Per-peer RPC limits (100 concurrent per connection)

However, these limits are insufficient as 40+ connections Ã— 100 requests = 4000+ concurrent expensive operations.

## Recommendation

Implement multi-layered request cost management:

1. **Add Request Cost Validation**: In the `RequestModerator`, reject requests with excessively large ranges before processing:

```rust
// In moderator.rs validate_request()
match &request.data_request {
    DataRequest::GetStateValuesWithProof(req) => {
        let range_size = req.end_index.saturating_sub(req.start_index);
        if range_size > config.max_state_chunk_size * 2 {
            return Err(Error::InvalidRequest(format!(
                "Requested range too large: {} exceeds limit",
                range_size
            )));
        }
    },
    // Similar checks for other request types
    _ => {}
}
```

2. **Implement Per-Peer Rate Limiting on Valid Requests**: Track and limit the number of concurrent valid requests per peer:

```rust
struct RequestModerator {
    // ... existing fields ...
    active_requests_per_peer: Arc<DashMap<PeerNetworkId, u32>>,
    max_concurrent_requests_per_peer: u32,
}

// Before processing, check and increment counter
// After response sent, decrement counter
```

3. **Add Global Concurrency Limit**: Use a semaphore to limit total concurrent blocking tasks:

```rust
struct StorageServiceServer<T> {
    // ... existing fields ...
    request_semaphore: Arc<Semaphore>,
}

// Acquire semaphore before spawn_blocking
let permit = self.request_semaphore.acquire().await;
self.runtime.spawn_blocking(move || {
    let _permit = permit; // Hold permit until task completes
    // ... process request ...
});
```

4. **Implement Request Prioritization**: Priority queue based on request type, with legitimate sync requests prioritized over potentially expensive queries.

## Proof of Concept

```rust
// Integration test demonstrating the attack
#[tokio::test]
async fn test_storage_service_queue_dos() {
    // Setup: Create storage service with small queue for testing
    let config = StorageServiceConfig {
        max_network_channel_size: 100, // Smaller for test
        max_state_chunk_size: 1000,
        ..Default::default()
    };
    
    // Establish multiple peer connections
    let num_attackers = 10;
    let requests_per_attacker = 10;
    
    let mut attack_tasks = vec![];
    for attacker_id in 0..num_attackers {
        let peer_id = PeerId::random();
        let network_id = NetworkId::Public;
        let peer_network_id = PeerNetworkId::new(network_id, peer_id);
        
        // Each attacker sends expensive requests
        for _ in 0..requests_per_attacker {
            let request = StorageServiceRequest::new(
                DataRequest::GetStateValuesWithProof(
                    StateValuesWithProofRequest {
                        version: 1000,
                        start_index: 0,
                        end_index: u64::MAX - 1, // Extremely large range
                    }
                ),
                false,
            );
            
            attack_tasks.push(send_request(peer_network_id, request));
        }
    }
    
    // Launch attack
    let start = Instant::now();
    let _results = futures::future::join_all(attack_tasks).await;
    
    // Now try to send legitimate sync request
    let legitimate_peer = PeerNetworkId::new(NetworkId::Validator, PeerId::random());
    let legitimate_request = StorageServiceRequest::new(
        DataRequest::GetTransactionsWithProof(
            TransactionsWithProofRequest {
                proof_version: 1000,
                start_version: 900,
                end_version: 1000,
                include_events: false,
            }
        ),
        false,
    );
    
    // Measure response time
    let response_time = measure_request_latency(legitimate_peer, legitimate_request).await;
    
    // Assert: Legitimate request is significantly delayed
    // Under normal conditions: < 100ms
    // During attack: > 5000ms or timeout
    assert!(
        response_time > Duration::from_secs(5) || response_time.is_none(),
        "Attack did not significantly delay legitimate requests"
    );
}
```

**Notes**

The vulnerability stems from a fundamental architectural issue: the storage service processes all valid requests equally without considering their computational cost or the aggregate resource consumption. While individual protections exist (per-peer RPC limits, queue size limits), there is no holistic resource management that accounts for the cumulative impact of many expensive but valid requests from multiple peers.

The attack is particularly concerning for public fullnodes that provide state synchronization services to the network, as they must accept connections from untrusted peers but lack adequate protection against resource exhaustion attacks through valid but expensive request patterns.

### Citations

**File:** config/src/config/state_sync_config.rs (L168-168)
```rust
    pub max_network_channel_size: u64,
```

**File:** state-sync/storage-service/server/src/lib.rs (L389-419)
```rust
        while let Some(network_request) = self.network_requests.next().await {
            // All handler methods are currently CPU-bound and synchronous
            // I/O-bound, so we want to spawn on the blocking thread pool to
            // avoid starving other async tasks on the same runtime.
            let storage = self.storage.clone();
            let config = self.storage_service_config;
            let cached_storage_server_summary = self.cached_storage_server_summary.clone();
            let optimistic_fetches = self.optimistic_fetches.clone();
            let subscriptions = self.subscriptions.clone();
            let lru_response_cache = self.lru_response_cache.clone();
            let request_moderator = self.request_moderator.clone();
            let time_service = self.time_service.clone();
            self.runtime.spawn_blocking(move || {
                Handler::new(
                    cached_storage_server_summary,
                    optimistic_fetches,
                    lru_response_cache,
                    request_moderator,
                    storage,
                    subscriptions,
                    time_service,
                )
                .process_request_and_respond(
                    config,
                    network_request.peer_network_id,
                    network_request.protocol_id,
                    network_request.storage_service_request,
                    network_request.response_sender,
                );
            });
        }
```

**File:** network/framework/src/protocols/rpc/mod.rs (L183-183)
```rust
    max_concurrent_inbound_rpcs: u32,
```

**File:** config/src/config/network_config.rs (L44-44)
```rust
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
```

**File:** state-sync/storage-service/server/src/moderator.rs (L134-196)
```rust
    pub fn validate_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> Result<(), Error> {
        // Validate the request and time the operation
        let validate_request = || {
            // If the peer is being ignored, return an error
            if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
                if peer_state.is_ignored() {
                    return Err(Error::TooManyInvalidRequests(format!(
                        "Peer is temporarily ignored. Unable to handle request: {:?}",
                        request
                    )));
                }
            }

            // Get the latest storage server summary
            let storage_server_summary = self.cached_storage_server_summary.load();

            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }

            Ok(()) // The request is valid
        };
        utils::execute_and_time_duration(
            &metrics::STORAGE_REQUEST_VALIDATION_LATENCY,
            Some((peer_network_id, request)),
            None,
            validate_request,
            None,
        )
    }
```

**File:** state-sync/storage-service/types/src/responses.rs (L727-742)
```rust
            GetStateValuesWithProof(request) => {
                let proof_version = request.version;

                let can_serve_states = self
                    .states
                    .map(|range| range.contains(request.version))
                    .unwrap_or(false);

                let can_create_proof = self
                    .synced_ledger_info
                    .as_ref()
                    .map(|li| li.ledger_info().version() >= proof_version)
                    .unwrap_or(false);

                can_serve_states && can_create_proof
            },
```

**File:** state-sync/storage-service/server/src/storage.rs (L908-929)
```rust
        // Calculate the number of state values to fetch
        let expected_num_state_values = inclusive_range_len(start_index, end_index)?;
        let max_num_state_values = self.config.max_state_chunk_size;
        let num_state_values_to_fetch = min(expected_num_state_values, max_num_state_values);

        // If size and time-aware chunking are disabled, use the legacy implementation
        if !use_size_and_time_aware_chunking {
            return self.get_state_value_chunk_with_proof_by_size_legacy(
                version,
                start_index,
                end_index,
                num_state_values_to_fetch,
                max_response_size,
            );
        }

        // Get the state value chunk iterator
        let mut state_value_iterator = self.storage.get_state_value_chunk_iter(
            version,
            start_index as usize,
            num_state_values_to_fetch as usize,
        )?;
```
