# Audit Report

## Title
Channel Backpressure Bypass Through Multi-Channel Message Type Exploitation in Consensus Network Layer

## Summary
A Byzantine validator can bypass per-channel backpressure limits by strategically sending invalid consensus messages of different types to fill multiple independent channels (total capacity: 70 messages) before signature verification occurs. This allows amplified DoS attacks on consensus message processing compared to attacking a single channel.

## Finding Description

The consensus network layer routes incoming messages to three separate channels based on message type discriminants **before** performing any signature verification or validation: [1](#0-0) 

Messages are routed based on type alone without authentication: [2](#0-1) [3](#0-2) 

Signature verification only occurs **after** messages have already consumed channel slots: [4](#0-3) 

A Byzantine validator can exploit this by:

1. **Phase 1**: Send 10 invalid `ProposalMsg`/`VoteMsg`/`SyncInfo` messages → fills `consensus_messages_tx` (capacity 10)
2. **Phase 2**: Send 50 invalid `SignedBatchInfo`/`BatchMsg`/`ProofOfStoreMsg` messages → fills `quorum_store_messages_tx` (capacity 50, note TODO comment about tuning)
3. **Phase 3**: Send 10 invalid RPC requests → fills `rpc_tx` (capacity 10)

This achieves **70 unverified messages queued** across all channels, compared to only 10-50 if attacking a single channel. When channels are full, the `aptos_channel` silently drops messages: [5](#0-4) 

Legitimate consensus messages from honest validators will be dropped, causing processing delays and potential liveness degradation.

## Impact Explanation

This vulnerability falls under **High Severity** per Aptos bug bounty criteria:
- **Validator node slowdowns**: Legitimate proposals, votes, and quorum store messages are dropped when channels fill with invalid messages, delaying consensus rounds
- **Significant protocol violations**: Violates the "Resource Limits: All operations must respect gas, storage, and computational limits" invariant by allowing unvalidated messages to exhaust channel capacity

The attack can cause measurable impact on validator performance by forcing message retransmissions and delaying consensus progress. While not a safety violation, it degrades liveness under Byzantine conditions.

## Likelihood Explanation

**High likelihood** of exploitation:
- Any Byzantine validator in the active set can execute this attack
- Requires no special privileges beyond being an authenticated validator peer
- Simple to execute: just send invalid messages of different types
- No cryptographic expertise needed
- AptosBFT assumes up to f Byzantine validators, making this a realistic threat scenario

The attack is particularly effective because the quorum store channel has 5x the capacity of the consensus channel, providing significant amplification potential.

## Recommendation

**Option 1: Validate before routing (Recommended)**
Perform lightweight authentication/validation before pushing messages to channels:
- Verify sender is in current validator set
- Check epoch matches current epoch
- Validate message format (basic structure checks)
- Add per-peer rate limiting counters

**Option 2: Unified channel**
Consolidate all consensus messages into a single channel with unified backpressure, eliminating the bypass opportunity.

**Option 3: Equal capacities**
Set all channels to the same capacity (e.g., 10) and remove the capacity imbalance that amplifies the attack.

**Option 4: Per-peer message counting**
Track messages per peer across all channels and enforce global per-peer limits before channel routing.

## Proof of Concept

```rust
// Conceptual PoC (not fully compilable without test harness)
// Shows how a Byzantine validator would exploit this

use aptos_consensus_types::*;
use consensus::network::*;

async fn byzantine_channel_flood_attack(
    network_sender: &NetworkSender,
    target_validators: Vec<Author>
) {
    // Phase 1: Fill consensus channel (capacity 10)
    for _ in 0..10 {
        let invalid_proposal = create_invalid_proposal(); // Wrong signature
        network_sender.broadcast_proposal(invalid_proposal).await;
    }
    
    // Phase 2: Fill quorum store channel (capacity 50) 
    for _ in 0..50 {
        let invalid_batch_info = create_invalid_signed_batch_info(); // Wrong signature
        network_sender.send_signed_batch_info_msg(
            vec![invalid_batch_info], 
            target_validators.clone()
        ).await;
    }
    
    // Phase 3: Fill RPC channel (capacity 10)
    for _ in 0..10 {
        let invalid_commit_vote = create_invalid_commit_vote(); // Wrong signature
        let _ = network_sender.send_commit_vote(
            invalid_commit_vote,
            target_validators[0]
        ).await;
    }
    
    // Now all 70 channel slots are filled with unverified invalid messages
    // Legitimate messages from honest validators will be dropped
}
```

**Expected behavior**: Target validator's consensus message processing experiences significant delays as legitimate messages are dropped and must be retransmitted, measurable through increased round latency and dropped message counters.

### Citations

**File:** consensus/src/network.rs (L757-769)
```rust
        let (consensus_messages_tx, consensus_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            10,
            Some(&counters::CONSENSUS_CHANNEL_MSGS),
        );
        let (quorum_store_messages_tx, quorum_store_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            // TODO: tune this value based on quorum store messages with backpressure
            50,
            Some(&counters::QUORUM_STORE_CHANNEL_MSGS),
        );
        let (rpc_tx, rpc_rx) =
            aptos_channel::new(QueueStyle::FIFO, 10, Some(&counters::RPC_CHANNEL_MSGS));
```

**File:** consensus/src/network.rs (L822-831)
```rust
                    match msg {
                        quorum_store_msg @ (ConsensusMsg::SignedBatchInfo(_)
                        | ConsensusMsg::BatchMsg(_)
                        | ConsensusMsg::ProofOfStoreMsg(_)) => {
                            Self::push_msg(
                                peer_id,
                                quorum_store_msg,
                                &self.quorum_store_messages_tx,
                            );
                        },
```

**File:** consensus/src/network.rs (L863-901)
```rust
                        consensus_msg @ (ConsensusMsg::ProposalMsg(_)
                        | ConsensusMsg::OptProposalMsg(_)
                        | ConsensusMsg::VoteMsg(_)
                        | ConsensusMsg::RoundTimeoutMsg(_)
                        | ConsensusMsg::OrderVoteMsg(_)
                        | ConsensusMsg::SyncInfo(_)
                        | ConsensusMsg::EpochRetrievalRequest(_)
                        | ConsensusMsg::EpochChangeProof(_)) => {
                            if let ConsensusMsg::ProposalMsg(proposal) = &consensus_msg {
                                observe_block(
                                    proposal.proposal().timestamp_usecs(),
                                    BlockStage::NETWORK_RECEIVED,
                                );
                                info!(
                                    LogSchema::new(LogEvent::NetworkReceiveProposal)
                                        .remote_peer(peer_id),
                                    block_round = proposal.proposal().round(),
                                    block_hash = proposal.proposal().id(),
                                );
                            }
                            if let ConsensusMsg::OptProposalMsg(proposal) = &consensus_msg {
                                observe_block(
                                    proposal.timestamp_usecs(),
                                    BlockStage::NETWORK_RECEIVED,
                                );
                                observe_block(
                                    proposal.timestamp_usecs(),
                                    BlockStage::NETWORK_RECEIVED_OPT_PROPOSAL,
                                );
                                info!(
                                    LogSchema::new(LogEvent::NetworkReceiveOptProposal)
                                        .remote_peer(peer_id),
                                    block_author = proposal.proposer(),
                                    block_epoch = proposal.epoch(),
                                    block_round = proposal.round(),
                                );
                            }
                            Self::push_msg(peer_id, consensus_msg, &self.consensus_messages_tx);
                        },
```

**File:** consensus/src/epoch_manager.rs (L1587-1622)
```rust
            self.bounded_executor
                .spawn(async move {
                    match monitor!(
                        "verify_message",
                        unverified_event.clone().verify(
                            peer_id,
                            &epoch_state.verifier,
                            &proof_cache,
                            quorum_store_enabled,
                            peer_id == my_peer_id,
                            max_num_batches,
                            max_batch_expiry_gap_usecs,
                        )
                    ) {
                        Ok(verified_event) => {
                            Self::forward_event(
                                quorum_store_msg_tx,
                                round_manager_tx,
                                buffered_proposal_tx,
                                peer_id,
                                verified_event,
                                payload_manager,
                                pending_blocks,
                            );
                        },
                        Err(e) => {
                            error!(
                                SecurityEvent::ConsensusInvalidMessage,
                                remote_peer = peer_id,
                                error = ?e,
                                unverified_event = unverified_event
                            );
                        },
                    }
                })
                .await;
```

**File:** crates/channel/src/aptos_channel.rs (L101-107)
```rust
        let dropped = shared_state.internal_queue.push(key, (message, status_ch));
        // If this or an existing message had to be dropped because of the queue being full, we
        // notify the corresponding status channel if it was registered.
        if let Some((dropped_val, Some(dropped_status_ch))) = dropped {
            // Ignore errors.
            let _err = dropped_status_ch.send(ElementStatus::Dropped(dropped_val));
        }
```
