# Audit Report

## Title
Unbounded Memory Consumption in Admin Service Quorum Store Database Dump Endpoint

## Summary
The `/debug/consensus/quorumstoredb` endpoint in the admin service can cause unbounded memory consumption when called without a digest parameter. The `get_all_batches()` function loads the entire quorum store database into memory (up to 300MB and 300,000 batches) without pagination or streaming, causing validator node resource exhaustion on testnet/devnet deployments where the admin service is enabled by default without authentication.

## Finding Description
The vulnerability exists in the interaction between three components:

1. **Admin Service Endpoint Handler**: The endpoint at `/debug/consensus/quorumstoredb` accepts an optional `digest` query parameter. When no digest is provided, it calls `dump_quorum_store_db()` with `digest = None`. [1](#0-0) 

2. **Dump Function**: When `digest` is `None`, the function calls `quorum_store_db.get_all_batches()` which loads ALL batches from the database into a HashMap. [2](#0-1) 

3. **Database Query**: The `get_all_batches()` implementation creates an iterator over the entire database and collects ALL entries into a HashMap without any bounds checking. [3](#0-2) 

The database stores `PersistedValue<BatchInfo>` structures which are BCS-serialized including the full transaction payloads when available. [4](#0-3) 

**Attack Path:**
1. Attacker sends HTTP GET request to `http://validator:9102/debug/consensus/quorumstoredb` (without digest parameter)
2. On testnet/devnet, authentication is bypassed (default configuration has empty `authentication_configs`)
3. The function loads up to 300,000 batches totaling up to 300MB from database into memory
4. This causes significant memory and CPU consumption during deserialization
5. Repeated requests can exhaust node resources

**Configuration Context:**
- Default `db_quota`: 300MB
- Default `batch_quota`: 300,000 batches  
- Admin service enabled by default on testnet/devnet without authentication [5](#0-4) [6](#0-5) [7](#0-6) 

This breaks **Invariant #9: Resource Limits** - all operations must respect gas, storage, and computational limits.

## Impact Explanation
This is a **High Severity** vulnerability per the Aptos bug bounty program criteria for the following reasons:

1. **Validator Node Slowdowns**: The unbounded memory allocation (up to 300MB) and CPU-intensive BCS deserialization of hundreds of thousands of database entries directly causes validator node performance degradation.

2. **Denial of Service Vector**: An attacker can repeatedly call this endpoint to continuously exhaust node resources, potentially affecting consensus participation and block production.

3. **Testnet/Devnet Impact**: While mainnet deployments disable the admin service by default, testnet and devnet nodes (which are critical for ecosystem development and testing) are vulnerable by default configuration.

The attack does NOT cause:
- Permanent network damage (nodes recover after request completes)
- Consensus safety violations
- Fund theft or manipulation

Therefore, this aligns with **High Severity: Validator node slowdowns** ($50,000 bounty tier).

## Likelihood Explanation
**Likelihood: Medium to High**

**Factors increasing likelihood:**
- Simple HTTP GET request (no special permissions or complex setup required)
- Admin service enabled by default on testnet/devnet
- No authentication required on non-mainnet deployments
- Endpoint is discoverable and documented
- Attack can be automated and repeated

**Factors decreasing likelihood:**
- Mainnet deployments are protected (service disabled by default)
- Even if enabled on mainnet, authentication is enforced
- Temporary impact (node recovers after request completes)

An attacker with knowledge of validator node endpoints on testnet/devnet can trivially exploit this vulnerability.

## Recommendation

Implement pagination and streaming for the `get_all_batches()` function to prevent unbounded memory consumption:

**Option 1: Add Pagination to Admin Endpoint**
Modify `dump_quorum_store_db()` to accept `limit` and `offset` parameters, and implement a paginated database query method that doesn't load all entries into memory at once.

**Option 2: Add Iterator-Based Streaming**
Replace `get_all_batches()` with a streaming iterator that yields batches incrementally without collecting them all into a HashMap first.

**Option 3: Add Configurable Limits**
Add a configuration parameter `max_batches_per_dump` with a reasonable default (e.g., 1000) and enforce this limit in the dump function.

**Recommended Fix for `dump_quorum_store_db()`:**

```rust
fn dump_quorum_store_db(
    quorum_store_db: &dyn QuorumStoreStorage,
    digest: Option<HashValue>,
    limit: Option<usize>,
) -> anyhow::Result<String> {
    let mut body = String::new();
    const DEFAULT_LIMIT: usize = 1000;
    
    if let Some(digest) = digest {
        body.push_str(&format!("{digest:?}:\n"));
        body.push_str(&format!(
            "{:?}",
            quorum_store_db.get_batch(&digest).map_err(Error::msg)?
        ));
    } else {
        let max_items = limit.unwrap_or(DEFAULT_LIMIT);
        let mut count = 0;
        
        // Use iterator instead of collecting all into HashMap
        let mut iter = quorum_store_db.get_batch_iterator()?;
        iter.seek_to_first();
        
        for result in iter {
            if count >= max_items {
                body.push_str(&format!("\n... (limited to {} items)\n", max_items));
                break;
            }
            let (digest, _batch) = result?;
            body.push_str(&format!("{digest:?}:\n"));
            count += 1;
        }
    }
    
    Ok(body)
}
```

Additionally, enforce rate limiting on admin service endpoints, even on testnet/devnet deployments.

## Proof of Concept

**Attack Script:**
```bash
#!/bin/bash
# Target a testnet validator node with admin service enabled
VALIDATOR_IP="testnet-validator.example.com"
ADMIN_PORT="9102"

echo "Triggering unbounded memory consumption..."

# Single request loads all batches into memory
curl "http://${VALIDATOR_IP}:${ADMIN_PORT}/debug/consensus/quorumstoredb" \
  -o /dev/null -s -w "Response size: %{size_download} bytes, Time: %{time_total}s\n"

# Repeated requests for DoS
for i in {1..10}; do
  echo "Request $i..."
  curl "http://${VALIDATOR_IP}:${ADMIN_PORT}/debug/consensus/quorumstoredb" \
    -o /dev/null -s &
done

wait
echo "Attack complete. Monitor validator memory usage and block production."
```

**Reproduction Steps:**
1. Deploy an Aptos validator node on testnet with default configuration
2. Ensure admin service is enabled (default on testnet)
3. Allow quorum store database to accumulate batches (natural operation)
4. Execute the attack script targeting the node's admin service endpoint
5. Monitor node memory consumption (should spike to 300+ MB per request)
6. Observe potential degradation in consensus participation

**Expected Behavior (Vulnerable):**
- Memory consumption increases by up to 300MB per request
- CPU spike during BCS deserialization
- Multiple concurrent requests cause significant node slowdown

**Expected Behavior (After Fix):**
- Memory consumption bounded to configured limit
- Response includes pagination information
- Node performance remains stable under repeated requests

## Notes

This vulnerability demonstrates a common pattern where debugging/administrative endpoints lack proper resource controls. While the endpoint serves a legitimate debugging purpose, the lack of bounds checking creates a DoS vector. The issue is particularly concerning because:

1. The same pattern exists in other functions that call `get_all_batches()` during initialization, though those are less directly exploitable. [8](#0-7) 

2. There's no rate limiting on admin service endpoints, allowing rapid repeated exploitation.

3. The vulnerability affects the critical consensus infrastructure, making validator availability a primary concern.

The fix should be applied consistently across all code paths that call `get_all_batches()` to prevent similar issues.

### Citations

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L40-72)
```rust
pub async fn handle_dump_quorum_store_db_request(
    req: Request<Body>,
    quorum_store_db: Arc<dyn QuorumStoreStorage>,
) -> hyper::Result<Response<Body>> {
    let query = req.uri().query().unwrap_or("");
    let query_pairs: HashMap<_, _> = url::form_urlencoded::parse(query.as_bytes()).collect();

    let digest: Option<HashValue> = match query_pairs.get("digest") {
        Some(val) => match val.parse() {
            Ok(val) => Some(val),
            Err(err) => return Ok(reply_with_status(StatusCode::BAD_REQUEST, err.to_string())),
        },
        None => None,
    };

    info!("Dumping quorum store db.");

    match spawn_blocking(move || dump_quorum_store_db(quorum_store_db.as_ref(), digest)).await {
        Ok(result) => {
            info!("Finished dumping quorum store db.");
            let headers: Vec<(_, HeaderValue)> =
                vec![(CONTENT_LENGTH, HeaderValue::from(result.len()))];
            Ok(reply_with(headers, result))
        },
        Err(e) => {
            info!("Failed to dump quorum store db: {e:?}");
            Ok(reply_with_status(
                StatusCode::INTERNAL_SERVER_ERROR,
                e.to_string(),
            ))
        },
    }
}
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L158-177)
```rust
fn dump_quorum_store_db(
    quorum_store_db: &dyn QuorumStoreStorage,
    digest: Option<HashValue>,
) -> anyhow::Result<String> {
    let mut body = String::new();

    if let Some(digest) = digest {
        body.push_str(&format!("{digest:?}:\n"));
        body.push_str(&format!(
            "{:?}",
            quorum_store_db.get_batch(&digest).map_err(Error::msg)?
        ));
    } else {
        for (digest, _batch) in quorum_store_db.get_all_batches()? {
            body.push_str(&format!("{digest:?}:\n"));
        }
    }

    Ok(body)
}
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L103-108)
```rust
    fn get_all_batches(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfo>>> {
        let mut iter = self.db.iter::<BatchSchema>()?;
        iter.seek_to_first();
        iter.map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<HashValue, PersistedValue<BatchInfo>>>>()
    }
```

**File:** consensus/src/quorum_store/schema.rs (L38-46)
```rust
impl ValueCodec<BatchSchema> for PersistedValue<BatchInfo> {
    fn encode_value(&self) -> Result<Vec<u8>> {
        Ok(bcs::to_bytes(&self)?)
    }

    fn decode_value(data: &[u8]) -> Result<Self> {
        Ok(bcs::from_bytes(data)?)
    }
}
```

**File:** config/src/config/quorum_store_config.rs (L133-135)
```rust
            memory_quota: 120_000_000,
            db_quota: 300_000_000,
            batch_quota: 300_000,
```

**File:** config/src/config/admin_service_config.rs (L21-22)
```rust
    // If empty, will allow all requests without authentication. (Not allowed on mainnet.)
    pub authentication_configs: Vec<AuthenticationConfig>,
```

**File:** config/src/config/admin_service_config.rs (L41-50)
```rust
impl Default for AdminServiceConfig {
    fn default() -> Self {
        Self {
            enabled: None,
            address: "0.0.0.0".to_string(),
            port: 9102,
            authentication_configs: vec![],
            malloc_stats_max_len: 2 * 1024 * 1024,
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L181-210)
```rust
    fn gc_previous_epoch_batches_from_db_v1(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db.get_all_batches().expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```
