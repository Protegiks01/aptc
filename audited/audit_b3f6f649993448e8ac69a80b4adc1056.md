# Audit Report

## Title
Byzantine Validators Can Cause CPU Exhaustion Through Invalid Signature Flooding in JWK Consensus

## Summary
Byzantine validators can repeatedly submit observations with invalid BLS signatures to the JWK consensus mechanism, forcing victim nodes to perform expensive signature verification operations without proper deduplication. This causes CPU exhaustion and delays quorum formation, degrading validator node performance.

## Finding Description

The vulnerability exists in the `ObservationAggregationState::add()` method where the duplicate voter check occurs before signature verification, but failed verifications never record the sender. [1](#0-0) 

The attack flow:

1. A validator initiates JWK observation broadcast via `start_produce()`: [2](#0-1) 

2. Byzantine validators receive the request and respond with observations containing:
   - Valid epoch (current epoch)
   - Correct author field (their own address)
   - Matching `observed` view (matching the broadcast request)
   - **Invalid signature** (random bytes or signature of different data)

3. The response processing checks pass in order:
   - Epoch validation (passes - correct epoch)
   - Author == sender check (passes - Byzantine validator sets both correctly)
   - Voting power check (passes - Byzantine validators are in the validator set)
   - **Duplicate check (passes - sender not yet in `partial_sigs` because no successful verification)**
   - View match check (passes - Byzantine validator copies the view)
   - **Signature verification (FAILS - expensive BLS operation performed)**

4. Since verification fails, the function returns an error and **never reaches line 92** where the sender would be added to `partial_sigs`.

5. The ReliableBroadcast retry mechanism catches the error and retries: [3](#0-2) 

6. The retry sends another request to the same Byzantine validator, which responds with another invalid signature, repeating the cycle.

**Amplification factors:**
- Up to 1/3 of validators can be Byzantine (e.g., 33 out of 100)
- Each Byzantine validator triggers an independent retry stream with exponential backoff [4](#0-3) 

- BoundedExecutor capacity limits parallelism to 8 concurrent tasks: [5](#0-4) 

- Each signature verification performs expensive BLS operations: [6](#0-5) 

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty criteria:

**"Validator node slowdowns"** - Byzantine validators can cause significant CPU exhaustion on victim nodes through:
1. **Repeated signature verifications**: Each Byzantine validator can trigger multiple BLS signature verifications (expensive cryptographic operations)
2. **Executor queue saturation**: With only 8 parallel task slots in the BoundedExecutor, Byzantine responses can fill the queue, blocking honest validator responses from being processed
3. **Delayed quorum formation**: If Byzantine validators respond faster than honest validators, they can delay the aggregation of legitimate signatures, preventing timely quorum certification
4. **Multiplied impact**: With 33 Byzantine validators (1/3 of 100), this creates 33 parallel attack streams, each performing repeated verifications

The attack does not cause permanent liveness failure (quorum can eventually be reached), but significantly degrades node performance and delays JWK consensus operations.

## Likelihood Explanation

**Likelihood: High**

Required attacker capabilities:
- Must be a Byzantine validator in the active validator set (up to 1/3 allowed by BFT assumptions)
- No special network position or timing requirements needed
- Simple attack: just respond with invalid signatures to JWK observation requests

The attack is:
- **Easy to execute**: Byzantine validators only need to send malformed responses
- **Hard to detect**: Appears as legitimate protocol traffic with signature verification failures
- **Sustainable**: Exponential backoff delays eventually slow the attack, but initial bursts cause significant damage
- **Repeatable**: Can be triggered on every JWK consensus round

## Recommendation

**Fix: Record sender before signature verification to enable proper deduplication**

Modify `ObservationAggregationState::add()` to track attempted submissions before signature verification:

```rust
// Add tracking of attempted signers (not shown in citation, but recommendation)
pub struct ObservationAggregationState<ConsensusMode> {
    epoch_state: Arc<EpochState>,
    local_view: ProviderJWKs,
    inner_state: Mutex<PartialSignatures>,
    attempted_signers: Mutex<BTreeSet<AccountAddress>>, // NEW
    _phantom: PhantomData<ConsensusMode>,
}

// In add() method, check attempted_signers before verification:
fn add(&self, sender: Author, response: Self::Response) -> anyhow::Result<Option<Self::Aggregated>> {
    // ... existing epoch, author, voting power checks ...
    
    let mut partial_sigs = self.inner_state.lock();
    if partial_sigs.contains_voter(&sender) {
        return Ok(None);
    }
    
    // NEW: Check if this sender already attempted (valid or invalid)
    let mut attempted = self.attempted_signers.lock();
    if attempted.contains(&sender) {
        return Ok(None); // Reject duplicate attempts
    }
    attempted.insert(sender); // Record attempt BEFORE verification
    drop(attempted); // Release lock
    
    ensure!(self.local_view == peer_view, "mismatched view");
    
    // Verify peer signature (now protected by attempted_signers dedup)
    self.epoch_state.verifier.verify(sender, &peer_view, &signature)?;
    
    // ... rest of aggregation logic ...
}
```

This ensures each validator can only trigger signature verification **once** per broadcast round, regardless of whether verification succeeds or fails.

## Proof of Concept

```rust
#[cfg(test)]
mod byzantine_flood_test {
    use super::*;
    use aptos_crypto::{bls12381, SigningKey};
    use std::time::Instant;

    #[test]
    fn test_invalid_signature_flooding_causes_cpu_exhaustion() {
        // Setup: 4 validators, Byzantine validator at index 3
        let num_validators = 4;
        let addrs: Vec<AccountAddress> = (0..num_validators)
            .map(|_| AccountAddress::random())
            .collect();
        let private_keys: Vec<bls12381::PrivateKey> = (0..num_validators)
            .map(|_| bls12381::PrivateKey::generate_for_testing())
            .collect();
        let public_keys: Vec<bls12381::PublicKey> = (0..num_validators)
            .map(|i| bls12381::PublicKey::from(&private_keys[i]))
            .collect();
        let voting_powers = [1, 1, 1, 1];
        let validator_infos: Vec<ValidatorConsensusInfo> = (0..num_validators)
            .map(|i| ValidatorConsensusInfo::new(addrs[i], public_keys[i].clone(), voting_powers[i]))
            .collect();
        let verifier = ValidatorVerifier::new(validator_infos);
        let epoch_state = Arc::new(EpochState::new(999, verifier));
        
        let view = ProviderJWKs {
            issuer: b"https://example.com".to_vec(),
            version: 1,
            jwks: vec![],
        };
        
        let agg_state = Arc::new(ObservationAggregationState::<PerIssuerMode>::new(
            epoch_state.clone(),
            view.clone(),
        ));
        
        // Byzantine validator (index 3) sends multiple invalid signatures
        let byzantine_addr = addrs[3];
        let wrong_view = ProviderJWKs {
            issuer: b"https://different.com".to_vec(),
            version: 1,
            jwks: vec![],
        };
        
        // Create invalid signature (signed wrong data)
        let invalid_sig = private_keys[3].sign(&wrong_view).unwrap();
        
        let start = Instant::now();
        let mut verification_count = 0;
        
        // Simulate 10 repeated invalid submissions
        for _ in 0..10 {
            let result = agg_state.add(
                byzantine_addr,
                ObservedUpdateResponse {
                    epoch: 999,
                    update: ObservedUpdate {
                        author: byzantine_addr,
                        observed: view.clone(), // Correct view
                        signature: invalid_sig.clone(), // But wrong signature
                    },
                },
            );
            
            // Each attempt fails verification
            assert!(result.is_err());
            verification_count += 1;
        }
        
        let duration = start.elapsed();
        
        // VULNERABILITY: All 10 attempts triggered signature verification
        // Without the fix, verification_count == 10
        // With the fix, only the first attempt would verify (count == 1)
        println!("Performed {} signature verifications in {:?}", verification_count, duration);
        println!("VULNERABILITY: Byzantine validator triggered {} redundant verifications", verification_count - 1);
        
        // Demonstrate that honest validator still works
        let honest_result = agg_state.add(
            addrs[0],
            ObservedUpdateResponse {
                epoch: 999,
                update: ObservedUpdate {
                    author: addrs[0],
                    observed: view.clone(),
                    signature: private_keys[0].sign(&view).unwrap(),
                },
            },
        );
        assert!(honest_result.is_ok());
    }
}
```

This PoC demonstrates that a Byzantine validator can repeatedly trigger signature verification by submitting observations with invalid signatures. Each submission forces an expensive BLS verification operation, causing CPU waste. With the recommended fix, only the first attempt would trigger verification, and subsequent attempts would be rejected by the `attempted_signers` check.

## Notes

This vulnerability specifically affects the JWK consensus subsystem but the pattern may exist in other reliable broadcast implementations in the codebase. The root cause is the ordering of deduplication (which depends on successful processing) versus expensive validation operations. The fix must ensure deduplication occurs before any expensive operations, regardless of whether those operations succeed or fail.

### Citations

**File:** crates/aptos-jwk-consensus/src/observation_aggregation/mod.rs (L76-92)
```rust
        let mut partial_sigs = self.inner_state.lock();
        if partial_sigs.contains_voter(&sender) {
            return Ok(None);
        }

        ensure!(
            self.local_view == peer_view,
            "adding peer observation failed with mismatched view"
        );

        // Verify peer signature.
        self.epoch_state
            .verifier
            .verify(sender, &peer_view, &signature)?;

        // All checks passed. Aggregating.
        partial_sigs.add_signature(sender, signature);
```

**File:** crates/aptos-jwk-consensus/src/update_certifier.rs (L49-83)
```rust
    fn start_produce(
        &self,
        epoch_state: Arc<EpochState>,
        payload: ProviderJWKs,
        qc_update_tx: aptos_channel::Sender<
            ConsensusMode::ConsensusSessionKey,
            QuorumCertifiedUpdate,
        >,
    ) -> anyhow::Result<AbortHandle> {
        ConsensusMode::log_certify_start(epoch_state.epoch, &payload);
        let rb = self.reliable_broadcast.clone();
        let epoch = epoch_state.epoch;
        let req = ConsensusMode::new_rb_request(epoch, &payload)
            .context("UpdateCertifier::start_produce failed at rb request construction")?;
        let agg_state = Arc::new(ObservationAggregationState::<ConsensusMode>::new(
            epoch_state,
            payload,
        ));
        let task = async move {
            let qc_update = rb.broadcast(req, agg_state).await.expect("cannot fail");
            ConsensusMode::log_certify_done(epoch, &qc_update);
            let session_key = ConsensusMode::session_key_from_qc(&qc_update);
            match session_key {
                Ok(key) => {
                    let _ = qc_update_tx.push(key, qc_update);
                },
                Err(e) => {
                    error!("JWK update QCed but could not identify the session key: {e}");
                },
            }
        };
        let (abort_handle, abort_registration) = AbortHandle::new_pair();
        tokio::spawn(Abortable::new(task, abort_registration));
        Ok(abort_handle)
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L191-200)
```rust
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L208-208)
```rust
                ExponentialBackoff::from_millis(5),
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L211-211)
```rust
                BoundedExecutor::new(8, tokio::runtime::Handle::current()),
```

**File:** types/src/validator_verifier.rs (L255-267)
```rust
    pub fn verify<T: Serialize + CryptoHash>(
        &self,
        author: AccountAddress,
        message: &T,
        signature: &bls12381::Signature,
    ) -> std::result::Result<(), VerifyError> {
        match self.get_public_key(&author) {
            Some(public_key) => public_key
                .verify_struct_signature(message, signature)
                .map_err(|_| VerifyError::InvalidMultiSignature),
            None => Err(VerifyError::UnknownAuthor),
        }
    }
```
