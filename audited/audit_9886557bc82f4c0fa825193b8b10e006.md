# Audit Report

## Title
Drop Panic in JellyfishMerkleRestore Causes Validator Node Crash and State Corruption

## Summary
The `Drop` implementation for `JellyfishMerkleRestore` contains double `unwrap()` calls that panic when async commit operations fail or when the receiver is dropped prematurely. This can cause validator node crashes and leave the Jellyfish Merkle tree in a corrupted state, violating the State Consistency invariant. [1](#0-0) 

## Finding Description

The vulnerability lies in the unsafe drop implementation that assumes async commit operations always succeed. The drop handler attempts to wait for pending async commits with double unwrap calls: [1](#0-0) 

This breaks the **State Consistency** invariant (atomic state transitions) in several scenarios:

**Scenario 1: Early Return During Restoration**

When `StateSnapshotRestore` is used in Default mode, if `kv_restore.finish()` fails, the method returns early without calling `tree_restore.finish_impl()`: [2](#0-1) 

The `JellyfishMerkleRestore` remains in the `Arc<Mutex<Option<>>>` with a pending async commit: [3](#0-2) 

When `StateSnapshotRestore` is dropped, the `JellyfishMerkleRestore::drop()` is triggered. If the async commit failed (e.g., database write error), the drop unwraps the error and panics.

**Scenario 2: Async Write Failure**

During `add_chunk_impl`, async commits are spawned to the IO thread pool: [4](#0-3) 

If `store.write_node_batch(&frozen_nodes)` fails due to:
- Disk full conditions
- I/O errors  
- Database corruption
- Permission issues during `commit_no_progress`: [5](#0-4) 

The error is sent through the channel, and when drop unwraps it, the node panics.

**Scenario 3: Double Panic = Process Abort**

If the original panic occurs during unwinding (from another panic elsewhere), and the drop itself panics, Rust's panic handler **aborts the entire process** per standard library behavior. This crashes the validator node completely.

## Impact Explanation

**High Severity** per Aptos bug bounty categories:

1. **Validator Node Crashes**: The panic in drop causes validator nodes to crash during state restoration, particularly during state sync operations. This affects network availability and liveness.

2. **State Corruption**: When drop panics, the Jellyfish Merkle tree is left in a partially restored state:
   - Some frozen nodes written to storage, others not
   - Partial batches committed
   - No root node written
   - Future restoration attempts may fail or produce incorrect state roots

3. **Consensus Impact**: Multiple validators experiencing this during state sync could reduce active validator count, potentially affecting network liveness if enough validators are offline simultaneously.

4. **Non-Deterministic Failures**: The timing-dependent nature means validators may fail at different points, creating debugging challenges and operational complexity.

This qualifies as "API crashes" and "Significant protocol violations" under High Severity criteria.

## Likelihood Explanation

**Likelihood: Medium-High** 

The vulnerability occurs naturally under realistic operational conditions:

1. **Disk Space Exhaustion**: During rapid state growth or snapshot restoration on nodes with limited disk space, write operations can fail naturally.

2. **I/O Errors**: Hardware failures, network storage issues, or filesystem corruption trigger database write failures during `commit_no_progress`.

3. **Database Constraint Violations**: Schema batch writes can fail due to shard count mismatches or internal database constraints.

4. **Concurrent Operations**: If `kv_restore.finish()` fails (e.g., metadata write failure), the early return leaves async commits pending. [6](#0-5) 

The likelihood increases during:
- High network activity requiring frequent state sync
- Validators joining the network
- Snapshot restoration after downtime
- Resource-constrained environments

## Recommendation

Replace the panicking drop implementation with graceful error handling:

```rust
impl<K> Drop for JellyfishMerkleRestore<K> {
    fn drop(&mut self) {
        if let Some(rx) = self.async_commit_result.take() {
            match rx.recv() {
                Ok(Ok(())) => {
                    // Async commit succeeded
                }
                Ok(Err(e)) => {
                    // Log the error but don't panic in drop
                    aptos_logger::error!(
                        "Async commit failed during JellyfishMerkleRestore drop: {:?}",
                        e
                    );
                }
                Err(e) => {
                    // Sender was dropped (likely due to thread pool panic)
                    aptos_logger::error!(
                        "Failed to receive async commit result during drop: {:?}",
                        e
                    );
                }
            }
        }
    }
}
```

Additionally, ensure `finish_impl()` is always called before drop by:

1. Making `finish_impl()` mandatory through type state pattern
2. Adding explicit `wait_for_async_commit()` calls before any early returns
3. Using RAII guards to ensure cleanup even on error paths

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use std::sync::Arc;
    
    // Mock writer that fails on batch commit
    struct FailingWriter {
        fail_after: AtomicUsize,
    }
    
    impl<K: crate::Key + CryptoHash> TreeWriter<K> for FailingWriter {
        fn write_node_batch(&self, _batch: &NodeBatch) -> Result<()> {
            let count = self.fail_after.fetch_sub(1, Ordering::SeqCst);
            if count == 0 {
                Err(AptosDbError::Other("Simulated disk full".to_string()))
            } else {
                Ok(())
            }
        }
    }
    
    #[test]
    #[should_panic(expected = "Simulated disk full")]
    fn test_drop_panic_on_async_commit_failure() {
        let writer = Arc::new(FailingWriter {
            fail_after: AtomicUsize::new(1), // Fail on first write
        });
        
        let mut restore = JellyfishMerkleRestore::new(
            writer,
            0,
            HashValue::zero(),
            true, // async_commit = true
        ).unwrap();
        
        // Add a chunk which triggers async commit
        let chunk = vec![
            (&StateKey::raw(b"key1"), HashValue::random()),
        ];
        let proof = SparseMerkleRangeProof::new(vec![]);
        
        // This will spawn async commit that will fail
        restore.add_chunk_impl(chunk, proof).unwrap();
        
        // Don't call finish_impl(), just drop
        // This simulates early return due to kv_restore.finish() failure
        drop(restore);
        
        // Drop will panic when unwrapping the failed async commit result
    }
}
```

This demonstrates that dropping `JellyfishMerkleRestore` with a pending failed async commit causes a panic, which would crash the validator node and corrupt state during restoration operations.

### Citations

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L394-410)
```rust
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L792-798)
```rust
impl<K> Drop for JellyfishMerkleRestore<K> {
    fn drop(&mut self) {
        if let Some(rx) = self.async_commit_result.take() {
            rx.recv().unwrap().unwrap();
        }
    }
}
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L145-149)
```rust
pub struct StateSnapshotRestore<K, V> {
    tree_restore: Arc<Mutex<Option<JellyfishMerkleRestore<K>>>>,
    kv_restore: Arc<Mutex<Option<StateValueRestore<K, V>>>>,
    restore_mode: StateSnapshotRestoreMode,
}
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L266-270)
```rust
            StateSnapshotRestoreMode::Default => {
                // for tree only mode, we also need to write the usage to DB
                self.kv_restore.lock().take().unwrap().finish()?;
                self.tree_restore.lock().take().unwrap().finish_impl()?
            },
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L174-190)
```rust
    pub(crate) fn commit_no_progress(
        &self,
        top_level_batch: SchemaBatch,
        batches_for_shards: Vec<SchemaBatch>,
    ) -> Result<()> {
        ensure!(
            batches_for_shards.len() == NUM_STATE_SHARDS,
            "Shard count mismatch."
        );
        let mut batches = batches_for_shards.into_iter();
        for shard_id in 0..NUM_STATE_SHARDS {
            let state_merkle_batch = batches.next().unwrap();
            self.state_merkle_db_shards[shard_id].write_schemas(state_merkle_batch)?;
        }

        self.state_merkle_metadata_db.write_schemas(top_level_batch)
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1281-1310)
```rust
    fn kv_finish(&self, version: Version, usage: StateStorageUsage) -> Result<()> {
        self.ledger_db.metadata_db().put_usage(version, usage)?;
        if let Some(internal_indexer_db) = self.internal_indexer_db.as_ref() {
            if version > 0 {
                let mut batch = SchemaBatch::new();
                batch.put::<InternalIndexerMetadataSchema>(
                    &MetadataKey::LatestVersion,
                    &MetadataValue::Version(version - 1),
                )?;
                if internal_indexer_db.statekeys_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::StateVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                if internal_indexer_db.transaction_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::TransactionVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                if internal_indexer_db.event_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::EventVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                internal_indexer_db
                    .get_inner_db_ref()
                    .write_schemas(batch)?;
```
