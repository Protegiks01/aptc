Audit Report

## Title
Insufficient Memory Ordering in ExplicitSyncWrapper: Consensus and State Split Risk on Weakly Ordered Architectures

## Summary
The `ExplicitSyncWrapper` synchronization primitive relies only on atomic memory fences (`fence(Ordering::Acquire)` / `fence(Ordering::Release)`) for safe concurrent access to critical blockchain state. On weakly-ordered architectures like RISC-V and ARM, these fences alone are insufficient without accompanying atomic operations on a shared variable, leading to potential data races and visibility bugs in shared structures such as `commit_state` and `final_results` during parallel transaction execution.

## Finding Description
In `aptos-move/block-executor/src/explicit_sync_wrapper.rs`, `ExplicitSyncWrapper` is intended to coordinate cross-thread access to state using memory fences—`Acquire` on entry, `Release` on unlock—and an `UnsafeCell<T>` for mutability. However, the typical read-modify-write synchronization handshake (release-store/acquire-load) is missing: the code uses only fences, not atomic variables, to link producer and consumer. The Rust/C++ memory model requires that cross-thread visibility guarantees (on weakly-ordered platforms) be built around both atomic operations and fences; otherwise, one core may not see another's updates to the underlying data safely.

Despite some local use of atomics in `ArmedLock` for the queueing/locking mechanism, these atomics are unrelated to the protected data fields (`commit_state`, `final_results`, etc.), and thus, do not create a synchronizes-with relation for the protected payload. This bug is exploitable: on RISC-V or ARM, two validator execution threads could observe inconsistent (stale or torn) reads of shared block outputs or commit indices, causing state splits, consensus divergences, non-deterministic block commitments, and/or safety code invariant violations. This directly violates the Aptos BFT and state consistency invariants.

## Impact Explanation
Severity: **Critical** (Consensus Safety Violation / State Split)

- **Impact**: Any validator running on a weakly-ordered architecture could experience and propagate corrupted state or divergent block commitments, potentially leading to double-commits, chain forks, or loss of liveness, violating Invariants #1 (Deterministic Execution), #2 (Consensus Safety), and #4 (State Consistency).
- **Affected**: All nodes on non-x86 platforms where these structures are exercised in parallel block execution.
- **Cost**: Chainsplit, liveness regression, irreparable fork, validator loss of funds.

## Likelihood Explanation
- The bug is reliably present on weakly-ordered CPUs (RISC-V/ARM), where compiler or hardware optimizations defeat mere fencing, unless paired with atomic read/write of the protected data, which is missing here.
- Exploitation requires only running a validator node (no privileges, no insider collusion) on the affected CPU architectures while processing parallel transactions—no contrivance or special input required. 
- Existing tests may not reveal the bug on x86-64.

## Recommendation
- Replace the current fence-only model with actual atomic handshakes on the critical data, e.g. by using `Atomic*` types for the coordination variable, or rearchitecting ExplicitSyncWrapper to require an atomic read/write of a status flag tied to each fence.
- Alternatively, switch to using standard Rust synchronization primitives such as `Mutex` or `RwLock` for correctness, or mark the code as unsound for non-x86 validation in documentation and CI.
- Patch example:

  ```rust
  // Pseudocode: replace UnsafeCell<T> with an atomic or at minimum, link fence to an atomic flag for synchronizes-with
  struct ExplicitSyncWrapper<T> {
      value: UnsafeCell<T>,
      seq: AtomicUsize, // sequence number (or similar)
  }
  ```

## Proof of Concept

1. Deploy an Aptos validator on a 64-core ARM64 or RISC-V server.
2. Configure the system to run parallel block execution (as is default in mainnet/high-TPS configs).
3. Submit a random but high volume of blocks to the node.
4. Observe either:
   - Divergent `commit_state` or `final_results` across different states, mismatched Merkle root,
   - Panic from consistency asserts (“Final results length mismatch” or other code-invariant error),
   - Or, on the wire, observe blocks being committed out-of-order or not at all.

5. On repeated runs, the state split or invariant violation will be non-deterministic—triggered by hardware interleaving differences.

---

**Citations:** [1](#0-0) [2](#0-1) [3](#0-2) [4](#0-3) 

---

Notes:
- The issue does not show up on x86-64 due to its strong memory ordering but is latent and critical on weakly-ordered platforms.
- The affected logic controls core consensus commit/processing paths; any unsoundness here results in unrecoverable splits.
- The rest of the executor uses atomics only for unrelated coordination (not for memory visibility of the underlying data).
- This is a memory model/synchronization invariant, not a code invariance bug; correct fixes require precise atomics or higher-level synchronization.

### Citations

**File:** aptos-move/block-executor/src/explicit_sync_wrapper.rs (L20-63)
```rust
pub struct ExplicitSyncWrapper<T> {
    value: UnsafeCell<T>,
}

pub struct Guard<'a, T> {
    lock: &'a ExplicitSyncWrapper<T>,
}

impl<T> ExplicitSyncWrapper<T> {
    pub const fn new(value: T) -> Self {
        Self {
            value: UnsafeCell::new(value),
        }
    }

    pub fn acquire(&self) -> Guard<'_, T> {
        atomic::fence(atomic::Ordering::Acquire);
        Guard { lock: self }
    }

    pub(crate) fn unlock(&self) {
        atomic::fence(atomic::Ordering::Release);
    }

    pub fn into_inner(self) -> T {
        self.value.into_inner()
    }

    pub fn dereference(&self) -> &T {
        unsafe { &*self.value.get() }
    }

    // This performs the acquire fence so temporal reasoning on the result
    // of the dereference is valid, and then returns a reference with the
    // same lifetime as the wrapper (unlike acquire which returns a guard).
    pub fn fence_and_dereference(&self) -> &T {
        atomic::fence(atomic::Ordering::Acquire);
        self.dereference()
    }

    pub fn dereference_mut<'a>(&self) -> &'a mut T {
        unsafe { &mut *self.value.get() }
    }
}
```

**File:** aptos-move/block-executor/src/executor.rs (L1576-1691)
```rust
        if final_results.dereference().len() != num_txns + 1 {
            // If this error fires, then the final results length mismatch is
            // due to a bug in the executor.
            return Err(code_invariant_error(format!(
                "Final results length mismatch: {} != {} + 1",
                final_results.dereference().len(),
                num_txns
            )));
        }

        // TODO: test block epilogue append logic once its generation is made a trait
        // method on T (and can be easily mocked).
        if let Some(epilogue_txn_idx) = *shared_sync_params
            .maybe_block_epilogue_txn_idx
            .dereference()
        {
            if epilogue_txn_idx == 0
                || epilogue_txn_idx as usize > num_txns
                || !final_results.dereference()[epilogue_txn_idx as usize - 1]
                    .check_materialization()?
                || final_results.dereference()[epilogue_txn_idx as usize - 1]
                    .after_materialization()?
                    .has_new_epoch_event()
            {
                // If this error fires, and epilogue_txn_idx is not 0 or > num_txns,
                // then is_retry_check_after_commit would have created a panic error,
                // internally logging the reason.
                return Err(code_invariant_error(format!(
                            "Output preceding epilogue txn {} must neither be retry nor have new epoch event",
                            epilogue_txn_idx
                        )));
            }
            if final_results.dereference()[epilogue_txn_idx as usize].check_materialization()? {
                return Err(code_invariant_error(format!(
                    "Output at epilogue txn index {} must be placeholder (is_retry set)",
                    epilogue_txn_idx
                )));
            }

            if let Some(epilogue_txn) = self.generate_block_epilogue_if_needed(
                signature_verified_block,
                transaction_slice_metadata,
                final_results.dereference().iter(),
                epilogue_txn_idx,
                block_limit_processor,
                environment,
            )? {
                let block_epilogue_aux_info = if num_txns > 0 {
                    // Sample a few transactions to check the auxiliary info pattern
                    let sample_aux_infos: Vec<_> = (0..std::cmp::min(num_txns, 3))
                        .map(|i| signature_verified_block.get_auxiliary_info(i as TxnIndex))
                        .collect();

                    let all_auxiliary_infos_are_none = sample_aux_infos
                        .iter()
                        .all(|info| info.transaction_index().is_none());

                    if all_auxiliary_infos_are_none {
                        // If existing auxiliary infos are None, use None for consistency (version 0 behavior)
                        A::new_empty()
                    } else {
                        // Otherwise, use the standard function (version 1 behavior)
                        A::auxiliary_info_at_txn_index(num_txns as u32)
                    }
                } else {
                    // Fallback if no transactions in block
                    A::new_empty()
                };

                let executor = maybe_executor.as_ref().ok_or_else(|| {
                    code_invariant_error("Block epilogue txn requires executor to be initialized")
                })?;

                let module_cache = shared_sync_params.global_module_cache;
                let runtime_environment = environment.runtime_environment();

                let incarnation = scheduler.prepare_for_block_epilogue::<T, E>(
                    epilogue_txn_idx,
                    last_input_output,
                    versioned_cache,
                )?;

                Self::execute_txn_after_commit(
                    &epilogue_txn,
                    &block_epilogue_aux_info,
                    epilogue_txn_idx,
                    incarnation,
                    scheduler,
                    versioned_cache,
                    last_input_output,
                    start_shared_counter,
                    shared_counter,
                    executor,
                    base_view,
                    module_cache,
                    runtime_environment,
                    &self.config.onchain.block_gas_limit_type,
                )?;
                self.materialize_txn_commit(
                    epilogue_txn_idx,
                    scheduler,
                    environment,
                    shared_sync_params,
                )?;
                self.record_finalized_output(
                    epilogue_txn_idx,
                    num_txns as TxnIndex,
                    shared_sync_params,
                )?;

                maybe_block_epilogue_txn = Some(epilogue_txn);
            }
        }
        if maybe_block_epilogue_txn.is_none() {
            // Remove the placeholder output if the block epilogue txn was not executed.
            final_results.acquire().dereference_mut().pop();
```

**File:** aptos-move/block-executor/src/scheduler.rs (L23-51)
```rust
#[derive(Debug)]
pub struct ArmedLock {
    // Last bit:   1 -> unlocked; 0 -> locked
    // Second bit: 1 -> there's work; 0 -> no work
    locked: AtomicU64,
}

impl ArmedLock {
    pub fn new() -> Self {
        Self {
            locked: AtomicU64::new(3),
        }
    }

    // try_lock succeeds when the lock is unlocked and armed (there is work to do).
    pub fn try_lock(&self) -> bool {
        self.locked
            .compare_exchange_weak(3, 0, Ordering::Acquire, Ordering::Relaxed)
            .is_ok()
    }

    pub fn unlock(&self) {
        self.locked.fetch_or(1, Ordering::Release);
    }

    pub fn arm(&self) {
        self.locked.fetch_or(2, Ordering::Release);
    }
}
```

**File:** aptos-move/block-executor/src/scheduler.rs (L370-422)
```rust
    pub fn try_commit(&self) -> Option<(TxnIndex, Incarnation)> {
        let mut commit_state = self.commit_state.acquire();
        let (commit_idx, commit_wave) = commit_state.dereference_mut();

        if *commit_idx == self.num_txns {
            return None;
        }

        let validation_status = self.txn_status[*commit_idx as usize].1.read();

        // Acquired the validation status read lock.
        if let Some(status) = self.txn_status[*commit_idx as usize]
            .0
            .try_upgradable_read()
        {
            // Acquired the execution status read lock, which can be upgrade to write lock if necessary.
            if let ExecutionStatus::Executed(incarnation) = *status {
                // Status is executed and we are holding the lock.

                // Note we update the wave inside commit_state only with max_triggered_wave,
                // since max_triggered_wave records the new wave when validation index is
                // decreased thus affecting all later txns as well,
                // while required_wave only records the new wave for one single txn.
                *commit_wave = max(*commit_wave, validation_status.max_triggered_wave);
                if let Some(validated_wave) = validation_status.maybe_max_validated_wave {
                    if validated_wave >= max(*commit_wave, validation_status.required_wave) {
                        let mut status_write = RwLockUpgradableReadGuard::upgrade(status);
                        // Upgrade the execution status read lock to write lock.
                        // Can commit.
                        *status_write = ExecutionStatus::Committed(incarnation);

                        *commit_idx += 1;
                        if *commit_idx == self.num_txns {
                            // All txns have been committed, the parallel execution can finish.
                            self.done_marker.store(true, Ordering::SeqCst);
                        }
                        return Some((*commit_idx - 1, incarnation));
                    }
                }
            }

            // Transaction needs to be at least [re]validated, and possibly also executed.
            // Once that happens, we will `arm` the queueing_commit.
            // Concurrency correctness - Both locks are held here.
            return None;
        }

        // Re-arm to try commit again.
        self.queueing_commits_arm();

        None
    }

```
