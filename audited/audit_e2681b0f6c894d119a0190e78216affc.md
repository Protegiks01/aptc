# Audit Report

## Title
Insufficient Input Validation in BatchRequester Allows Single Malicious Validator to Deny Batch Availability

## Summary
The `BatchRequester::new()` function in `consensus/src/quorum_store/batch_requester.rs` accepts `retry_limit` and `batch_request_num_peers` parameters without validation, allowing validators to configure dangerously low values (e.g., `retry_limit=1`, `batch_request_num_peers=1`). This creates a single point of failure where a single malicious validator can deny batch availability by refusing to respond, causing the affected validator to enter an infinite retry loop and fail to participate in consensus. [1](#0-0) 

## Finding Description

The vulnerability exists in the configuration validation layer and batch request retry mechanism:

1. **No Configuration Validation**: The `QuorumStoreConfig` sanitizer validates batch size limits but does NOT validate `batch_request_retry_limit` or `batch_request_num_peers`: [2](#0-1) 

2. **Misleading Retry Logic**: Despite the parameter name `retry_limit`, the actual behavior in `next_request_peers()` allows only `retry_limit` total attempts (not retries). With `retry_limit=1`, only ONE request attempt is made: [3](#0-2) 

3. **Infinite Retry on Failure**: When batch fetching fails, the pipeline enters an infinite retry loop: [4](#0-3) 

**Attack Scenario:**

1. Validator V misconfigures their node with `batch_request_retry_limit: 1` and `batch_request_num_peers: 1`
2. A batch B is created and signed by a quorum of validators (including malicious validator M)
3. V needs to fetch batch B to execute a block containing it
4. V's `BatchRequesterState::next_request_peers()` randomly selects 1 peer from the signers
5. If M is selected (probability 1/|signers|), M refuses to respond to the batch request
6. After RPC timeout (5000ms), V's `request_batch()` returns `ExecutorError::CouldNotGetData`
7. V's `materialize_block()` fails and enters infinite retry loop with 100ms delays
8. V cannot execute blocks, cannot participate in consensus, effectively offline

This breaks the **Byzantine Fault Tolerance** invariant: the protocol should tolerate f malicious validators, but with these misconfigurations, a single malicious validator can selectively DoS any misconfigured validator. [5](#0-4) 

## Impact Explanation

**High Severity** per Aptos bug bounty criteria: "Validator node slowdowns"

- Affected validator enters infinite retry loop, consuming CPU cycles repeatedly
- Validator cannot execute blocks containing batches from the malicious validator
- Validator falls behind and cannot participate in consensus voting
- If multiple validators are misconfigured, network liveness degrades
- Recovery requires manual intervention (node restart with corrected configuration)

The impact is not Critical because:
- Does not affect consensus safety (no double-spending or chain splits)
- Does not cause permanent fund loss
- Requires victim misconfiguration

But qualifies as High because:
- Causes validator unavailability and consensus participation failure
- Creates asymmetric attack advantage (single malicious validator affects multiple misconfigured honest validators)
- No automatic recovery mechanism

## Likelihood Explanation

**Medium Likelihood:**

**Factors increasing likelihood:**
- No validation warnings prevent dangerous configurations
- Default values (retry_limit=10, num_peers=5) are safe, but operators may "optimize" by reducing values
- New validator operators may not understand security implications
- Testnet environments often use minimal configurations for resource conservation
- Documentation does not warn about minimum safe values [6](#0-5) 

**Factors decreasing likelihood:**
- Production validators likely use default or conservative values
- Requires coordination with a malicious validator in the active set
- Attack only affects misconfigured validators, not the entire network

**Probability calculation:** In a 4-validator network with 1 malicious validator, if a batch requires 3 signatures, probability of DoS = 1/3 per batch request. Over time, this accumulates to near-certain DoS.

## Recommendation

Implement mandatory validation in the `QuorumStoreConfig` sanitizer to enforce minimum safe values:

```rust
// Add to QuorumStoreConfig::sanitize() method
fn sanitize_batch_request_parameters(
    sanitizer_name: &str,
    config: &QuorumStoreConfig,
) -> Result<(), Error> {
    const MIN_RETRY_LIMIT: usize = 3;
    const MIN_NUM_PEERS: usize = 2;
    
    if config.batch_request_retry_limit < MIN_RETRY_LIMIT {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name.to_owned(),
            format!(
                "batch_request_retry_limit ({}) must be at least {}",
                config.batch_request_retry_limit, MIN_RETRY_LIMIT
            ),
        ));
    }
    
    if config.batch_request_num_peers < MIN_NUM_PEERS {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name.to_owned(),
            format!(
                "batch_request_num_peers ({}) must be at least {}",
                config.batch_request_num_peers, MIN_NUM_PEERS
            ),
        ));
    }
    
    Ok(())
}
```

Call this function from the existing sanitize method.

**Better approach:** Enforce Byzantine fault tolerance by requiring `retry_limit * batch_request_num_peers >= f + 1` where f is the Byzantine threshold, ensuring sufficient redundancy to tolerate f malicious validators.

## Proof of Concept

```rust
#[tokio::test]
async fn test_batch_request_single_retry_dos() {
    use std::sync::Arc;
    use aptos_infallible::Mutex;
    use maplit::btreeset;
    use move_core_types::account_address::AccountAddress;
    use tokio::sync::oneshot;
    
    // MockBatchRequester that never responds (simulates malicious validator)
    #[derive(Clone)]
    struct NonResponsiveMockBatchRequester;
    
    #[async_trait::async_trait]
    impl QuorumStoreSender for NonResponsiveMockBatchRequester {
        async fn request_batch(
            &self,
            _request: BatchRequest,
            _recipient: Author,
            _timeout: Duration,
        ) -> anyhow::Result<BatchResponse> {
            // Simulate timeout by sleeping longer than RPC timeout
            tokio::time::sleep(Duration::from_secs(10)).await;
            Err(anyhow::anyhow!("Timeout"))
        }
        // ... other trait methods unimplemented ...
    }
    
    let validator_signer = ValidatorSigner::random(None);
    
    // Configure with dangerous values: retry_limit=1, num_peers=1
    let batch_requester = BatchRequester::new(
        1,
        AccountAddress::random(),
        1,  // batch_request_num_peers = 1
        1,  // batch_request_retry_limit = 1
        500,
        5_000,
        NonResponsiveMockBatchRequester,
        ValidatorVerifier::new_single(validator_signer.author(), validator_signer.public_key()).into(),
    );
    
    let digest = HashValue::random();
    let expiration = u64::MAX;
    let responders = Arc::new(Mutex::new(btreeset![AccountAddress::random()]));
    let (_, subscriber_rx) = oneshot::channel();
    
    let start = std::time::Instant::now();
    let result = batch_requester
        .request_batch(digest, expiration, responders, subscriber_rx)
        .await;
    let elapsed = start.elapsed();
    
    // Verify attack succeeds: batch fetch fails after ~5 seconds (single RPC timeout)
    assert!(result.is_err());
    assert!(elapsed.as_secs() >= 5 && elapsed.as_secs() < 7);
    assert_eq!(result.unwrap_err(), ExecutorError::CouldNotGetData);
}
```

This PoC demonstrates that with `retry_limit=1` and `batch_request_num_peers=1`, a single non-responsive validator causes batch fetch failure, leading to the infinite retry loop in the pipeline.

## Notes

The vulnerability stems from the lack of defense-in-depth in configuration validation. While the default values are safe, the protocol should actively prevent dangerous misconfigurations that break Byzantine fault tolerance assumptions. The naming of `retry_limit` is also misleadingâ€”it controls total attempts, not retries after the first attempt.

### Citations

**File:** consensus/src/quorum_store/batch_requester.rs (L40-64)
```rust
    fn next_request_peers(&mut self, num_peers: usize) -> Option<Vec<PeerId>> {
        let signers = self.signers.lock();
        if self.num_retries == 0 {
            let mut rng = rand::thread_rng();
            // make sure nodes request from the different set of nodes
            self.next_index = rng.r#gen::<usize>() % signers.len();
            counters::SENT_BATCH_REQUEST_COUNT.inc_by(num_peers as u64);
        } else {
            counters::SENT_BATCH_REQUEST_RETRY_COUNT.inc_by(num_peers as u64);
        }
        if self.num_retries < self.retry_limit {
            self.num_retries += 1;
            let ret = signers
                .iter()
                .cycle()
                .skip(self.next_index)
                .take(num_peers)
                .cloned()
                .collect();
            self.next_index = (self.next_index + num_peers) % signers.len();
            Some(ret)
        } else {
            None
        }
    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L79-99)
```rust
    pub(crate) fn new(
        epoch: u64,
        my_peer_id: PeerId,
        request_num_peers: usize,
        retry_limit: usize,
        retry_interval_ms: usize,
        rpc_timeout_ms: usize,
        network_sender: T,
        validator_verifier: Arc<ValidatorVerifier>,
    ) -> Self {
        Self {
            epoch,
            my_peer_id,
            request_num_peers,
            retry_limit,
            retry_interval_ms,
            rpc_timeout_ms,
            network_sender,
            validator_verifier,
        }
    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L101-180)
```rust
    pub(crate) async fn request_batch(
        &self,
        digest: HashValue,
        expiration: u64,
        responders: Arc<Mutex<BTreeSet<PeerId>>>,
        mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
    ) -> ExecutorResult<Vec<SignedTransaction>> {
        let validator_verifier = self.validator_verifier.clone();
        let mut request_state = BatchRequesterState::new(responders, self.retry_limit);
        let network_sender = self.network_sender.clone();
        let request_num_peers = self.request_num_peers;
        let my_peer_id = self.my_peer_id;
        let epoch = self.epoch;
        let retry_interval = Duration::from_millis(self.retry_interval_ms as u64);
        let rpc_timeout = Duration::from_millis(self.rpc_timeout_ms as u64);

        monitor!("batch_request", {
            let mut interval = time::interval(retry_interval);
            let mut futures = FuturesUnordered::new();
            let request = BatchRequest::new(my_peer_id, epoch, digest);
            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        // send batch request to a set of peers of size request_num_peers
                        if let Some(request_peers) = request_state.next_request_peers(request_num_peers) {
                            for peer in request_peers {
                                futures.push(network_sender.request_batch(request.clone(), peer, rpc_timeout));
                            }
                        } else if futures.is_empty() {
                            // end the loop when the futures are drained
                            break;
                        }
                    },
                    Some(response) = futures.next() => {
                        match response {
                            Ok(BatchResponse::Batch(batch)) => {
                                counters::RECEIVED_BATCH_RESPONSE_COUNT.inc();
                                let payload = batch.into_transactions();
                                return Ok(payload);
                            }
                            // Short-circuit if the chain has moved beyond expiration
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
                            }
                            Ok(BatchResponse::BatchV2(_)) => {
                                error!("Batch V2 response is not supported");
                            }
                            Err(e) => {
                                counters::RECEIVED_BATCH_RESPONSE_ERROR_COUNT.inc();
                                debug!("QS: batch request error, digest:{}, error:{:?}", digest, e);
                            }
                        }
                    },
                    result = &mut subscriber_rx => {
                        match result {
                            Ok(persisted_value) => {
                                counters::RECEIVED_BATCH_FROM_SUBSCRIPTION_COUNT.inc();
                                let (_, maybe_payload) = persisted_value.unpack();
                                return Ok(maybe_payload.expect("persisted value must exist"));
                            }
                            Err(err) => {
                                debug!("channel closed: {}", err);
                            }
                        };
                    },
                }
            }
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
        })
    }
```

**File:** config/src/config/quorum_store_config.rs (L127-128)
```rust
            batch_request_num_peers: 5,
            batch_request_retry_limit: 10,
```

**File:** config/src/config/quorum_store_config.rs (L253-271)
```rust
impl ConfigSanitizer for QuorumStoreConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        // Sanitize the send/recv batch limits
        Self::sanitize_send_recv_batch_limits(
            &sanitizer_name,
            &node_config.consensus.quorum_store,
        )?;

        // Sanitize the batch total limits
        Self::sanitize_batch_total_limits(&sanitizer_name, &node_config.consensus.quorum_store)?;

        Ok(())
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L634-646)
```rust
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
```
