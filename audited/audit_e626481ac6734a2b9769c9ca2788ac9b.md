# Audit Report

## Title
Fullnode Ping RPC Lacks Timeout Causing Metadata Manager Health Check Loop to Hang

## Summary
The `ping()` RPC call in the indexer-grpc-manager's metadata manager has no timeout configuration. When a fullnode becomes unresponsive, the ping call hangs indefinitely, blocking the entire health check loop within a `tokio_scoped::scope`. This prevents detection of unhealthy nodes and failover to healthy nodes, degrading indexer service availability.

## Finding Description

The vulnerability exists in the metadata manager's health check mechanism. The `ping_fullnode()` function calls the gRPC client's ping method without any timeout wrapper. [1](#0-0) 

The client implementation itself has no inherent timeout on unary RPC calls: [2](#0-1) 

In the main health check loop, these ping operations are spawned inside a `tokio_scoped::scope`, which waits for ALL spawned tasks to complete before continuing: [3](#0-2) 

The scope blocks until all tasks finish: [4](#0-3) 

**Attack Path:**
1. A fullnode becomes unresponsive (network partition, hung process, or malicious behavior)
2. The metadata manager attempts to ping this fullnode
3. The `client.ping(request).await?` hangs indefinitely with no timeout
4. The spawned task never completes
5. The `tokio_scoped::scope` blocks waiting for all tasks
6. The entire metadata manager loop freezes
7. No further health checks execute
8. Unhealthy nodes are not detected or removed
9. The `get_fullnode_for_request()` function continues selecting from stale node information
10. Clients experience degraded service as requests route to unhealthy nodes

The same vulnerability affects `ping_live_data_service` and `ping_historical_data_service` functions.

## Impact Explanation

This qualifies as **Medium Severity** per the Aptos bug bounty criteria for "State inconsistencies requiring intervention."

The impact includes:
- **Service Availability Degradation**: The indexer-grpc-manager becomes unresponsive when its health check loop hangs
- **Failed Failover**: Unhealthy fullnodes are not detected, preventing automatic failover to healthy nodes
- **Stale Metadata**: Node health information becomes stale, affecting routing decisions in `get_fullnode_for_request()`
- **Manual Intervention Required**: The metadata manager process must be restarted to recover
- **Cascading Effects**: Downstream services relying on the metadata manager for node selection experience degraded performance

The issue does NOT affect:
- Core blockchain consensus or validator operations  
- Transaction processing or state integrity
- Fund security or cryptographic guarantees

The vulnerability is limited to the indexer infrastructure layer, which provides data access APIs for applications but is not part of the core blockchain protocol.

## Likelihood Explanation

**High Likelihood** of occurrence:
- Fullnode unavailability is a common operational scenario (network issues, maintenance, crashes)
- No attacker sophistication required - any unresponsive fullnode triggers the bug
- A single unresponsive fullnode is sufficient to hang the entire loop
- The bug manifests automatically without requiring specific timing or race conditions

The proper timeout pattern is already used elsewhere in the codebase: [5](#0-4) 

## Recommendation

Wrap all gRPC ping calls with `tokio::time::timeout()` to prevent indefinite hangs. Suggested implementation:

```rust
async fn ping_fullnode(
    &self,
    address: GrpcAddress,
    mut client: FullnodeDataClient<Channel>,
) -> Result<()> {
    trace!("Pinging fullnode {address}.");
    let request = PingFullnodeRequest {};
    
    // Add timeout wrapper
    let timeout = Duration::from_secs(10); // Reasonable timeout for ping
    let response = tokio::time::timeout(timeout, client.ping(request))
        .await
        .map_err(|_| anyhow::anyhow!("Ping timeout after {:?}", timeout))??;
    
    if let Some(info) = response.into_inner().info {
        self.handle_fullnode_info(address, info)
    } else {
        bail!("Bad response.")
    }
}
```

Apply the same fix to `ping_live_data_service` and `ping_historical_data_service`.

Additionally, consider configuring timeout at the channel level: [6](#0-5) 

Add `.timeout(Duration::from_secs(10))` when building the channel.

## Proof of Concept

**Reproduction Steps:**

1. Set up an indexer-grpc-manager with at least one fullnode configured
2. Simulate unresponsive fullnode using network tools:
   ```bash
   # Block outgoing responses from fullnode to metadata manager
   iptables -A OUTPUT -p tcp --sport 50051 -j DROP
   ```
3. Observe metadata manager logs - ping will be attempted
4. Monitor the main loop timer metric - it will stop incrementing
5. Check that subsequent health checks don't execute
6. Verify the process requires restart to recover

**Test Case (Rust):**

```rust
#[tokio::test]
async fn test_ping_timeout_vulnerability() {
    // Create a mock fullnode that never responds
    let (tx, _rx) = tokio::sync::oneshot::channel::<()>();
    let mock_server = tokio::spawn(async move {
        // Accept connection but never respond
        let listener = tokio::net::TcpListener::bind("127.0.0.1:0").await.unwrap();
        let (_socket, _) = listener.accept().await.unwrap();
        // Hang forever
        tx.await.ok();
    });
    
    // Create metadata manager with the unresponsive fullnode
    let manager = MetadataManager::new(
        1, // chain_id
        "http://localhost:8080".to_string(),
        vec![],
        vec!["http://127.0.0.1:50051".to_string()],
        None,
    );
    
    // The start() call should timeout, but it won't due to the bug
    let result = tokio::time::timeout(
        Duration::from_secs(5),
        manager.start()
    ).await;
    
    // This assertion fails - the loop hangs indefinitely
    assert!(result.is_err(), "Metadata manager should timeout but hangs instead");
}
```

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L67-80)
```rust
    fn new(address: GrpcAddress) -> Self {
        let channel = Channel::from_shared(address)
            .expect("Bad address.")
            .connect_lazy();
        let client = FullnodeDataClient::new(channel)
            .send_compressed(CompressionEncoding::Zstd)
            .accept_compressed(CompressionEncoding::Zstd)
            .max_encoding_message_size(MAX_MESSAGE_SIZE)
            .max_decoding_message_size(MAX_MESSAGE_SIZE);
        Self {
            client,
            recent_states: VecDeque::new(),
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L182-215)
```rust
            tokio_scoped::scope(|s| {
                for kv in &self.grpc_managers {
                    let address = kv.key().clone();
                    let grpc_manager = kv.value();
                    let client = grpc_manager.client.clone();
                    s.spawn(async move {
                        if let Err(e) = self.heartbeat(client).await {
                            warn!("Failed to send heartbeat to other grpc manager ({address}): {e:?}.");
                        } else {
                            trace!("Successfully sent heartbeat to other grpc manager ({address}).");
                        }
                    });
                }

                for kv in &self.fullnodes {
                    let (address, fullnode) = kv.pair();
                    let need_ping = fullnode.recent_states.back().is_none_or(|s| {
                        Self::is_stale_timestamp(
                            s.timestamp.unwrap_or_default(),
                            Duration::from_secs(1),
                        )
                    });
                    if need_ping {
                        let address = address.clone();
                        let client = fullnode.client.clone();
                        s.spawn(async move {
                            if let Err(e) = self.ping_fullnode(address.clone(), client).await {
                                warn!("Failed to ping FN ({address}): {e:?}.");
                            } else {
                                trace!("Successfully pinged FN ({address}).");
                            }
                        });
                    }
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L290-328)
```rust
            });

            for address in unreachable_live_data_services {
                COUNTER
                    .with_label_values(&["unreachable_live_data_service"])
                    .inc();
                self.live_data_services.remove(&address);
            }

            for address in unreachable_historical_data_services {
                COUNTER
                    .with_label_values(&["unreachable_historical_data_service"])
                    .inc();
                self.historical_data_services.remove(&address);
            }

            // NOTE: We don't remove FNs and GrpcManagers here intentionally.

            CONNECTED_INSTANCES
                .with_label_values(&["fullnode"])
                .set(self.fullnodes.len() as i64);

            CONNECTED_INSTANCES
                .with_label_values(&["live_data_service"])
                .set(self.live_data_services.len() as i64);

            CONNECTED_INSTANCES
                .with_label_values(&["historical_data_service"])
                .set(self.historical_data_services.len() as i64);

            CONNECTED_INSTANCES
                .with_label_values(&["grpc_manager"])
                .set(self.grpc_managers.len() as i64);

            // TODO(grao): Double check if we should change this value, and/or we should separate
            // ping for different services to different loops.
            tokio::time::sleep(Duration::from_secs(1)).await;
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/metadata_manager.rs (L430-443)
```rust
    async fn ping_fullnode(
        &self,
        address: GrpcAddress,
        mut client: FullnodeDataClient<Channel>,
    ) -> Result<()> {
        trace!("Pinging fullnode {address}.");
        let request = PingFullnodeRequest {};
        let response = client.ping(request).await?;
        if let Some(info) = response.into_inner().info {
            self.handle_fullnode_info(address, info)
        } else {
            bail!("Bad response.")
        }
    }
```

**File:** protos/rust/src/pb/aptos.internal.fullnode.v1.tonic.rs (L92-118)
```rust
        pub async fn ping(
            &mut self,
            request: impl tonic::IntoRequest<super::PingFullnodeRequest>,
        ) -> std::result::Result<
            tonic::Response<super::PingFullnodeResponse>,
            tonic::Status,
        > {
            self.inner
                .ready()
                .await
                .map_err(|e| {
                    tonic::Status::new(
                        tonic::Code::Unknown,
                        format!("Service was not ready: {}", e.into()),
                    )
                })?;
            let codec = tonic::codec::ProstCodec::default();
            let path = http::uri::PathAndQuery::from_static(
                "/aptos.internal.fullnode.v1.FullnodeData/Ping",
            );
            let mut req = request.into_request();
            req.extensions_mut()
                .insert(
                    GrpcMethod::new("aptos.internal.fullnode.v1.FullnodeData", "Ping"),
                );
            self.inner.unary(req, path, codec).await
        }
```

**File:** storage/backup/backup-cli/src/utils/backup_service_client.rs (L63-65)
```rust
        let timeout = Duration::from_secs(Self::TIMEOUT_SECS);
        let reader = tokio::time::timeout(timeout, self.client.get(&url).send())
            .await?
```
