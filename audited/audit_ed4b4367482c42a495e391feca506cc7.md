# Audit Report

## Title
Missing Configuration Validation Enables Validator Liveness Denial-of-Service Through Invalid DAG Consensus Parameters

## Summary

The DAG consensus configuration structures (`DagFetcherConfig`, `ReliableBroadcastConfig`, `DagRoundStateConfig`, and `DagHealthConfig`) lack validation and test coverage, allowing invalid configuration values that cause complete validator liveness failure. A validator misconfigured with `min_concurrent_responders = 0` or `voter_pipeline_latency_limit_ms = 0` will be unable to synchronize state or participate in consensus, effectively removing itself from the network. [1](#0-0) 

## Finding Description

The `DagConsensusConfig` sanitizer only validates `DagPayloadConfig`, leaving four other critical configuration structures completely unvalidated: [2](#0-1) [3](#0-2) [4](#0-3) [5](#0-4) 

**Critical Issue #1: Zero `min_concurrent_responders` Causes Fetch Failure**

When `DagFetcherConfig.min_concurrent_responders` is set to 0, the `ExponentialNumberGenerator` in `RpcWithFallback` always returns 0, causing no RPC requests to be sent: [6](#0-5) 

The generator's `next()` method returns `current` (which is 0), then attempts `0 * 2 = 0`, maintaining the zero value forever. This causes `next_to_request()` to return empty vectors: [7](#0-6) 

When used in DAG fetching, this results in immediate failure: [8](#0-7) 

**Critical Issue #2: Zero `voter_pipeline_latency_limit_ms` Stops Voting**

When `DagHealthConfig.voter_pipeline_latency_limit_ms` is 0, the validator immediately stops voting on any proposals with non-zero pipeline latency: [9](#0-8) 

This causes vote refusal: [10](#0-9) 

## Impact Explanation

This vulnerability enables **validator liveness denial-of-service** and qualifies as **High Severity** per Aptos bug bounty criteria ("Validator node slowdowns" / "Significant protocol violations").

**Attack Scenario:**
1. A validator operator deploys with misconfigured parameters (e.g., `min_concurrent_responders: 0`)
2. The validator fails to fetch missing DAG nodes from peers
3. The validator cannot synchronize and participate in consensus
4. The validator effectively removes itself from the active validator set
5. Network voting power is reduced by the stake weight of the affected validator

**Impact Quantification:**
- **Affected validators:** Any validator with invalid configuration
- **Network impact:** Reduced voting power (but consensus continues if >2/3 honest stake remains)
- **Recovery:** Requires manual configuration fix and validator restart
- **Scope:** Individual validator liveness failure, not network-wide

While this doesn't cause network-wide liveness failure or consensus safety violations (thus not Critical severity), it represents a significant operational risk where configuration errors can silently disable validators.

## Likelihood Explanation

**Likelihood: Medium to High**

Configuration errors are common in production deployments:
- Automated configuration generation scripts may produce invalid values
- Copy-paste errors in YAML/TOML configuration files
- Misunderstanding of parameter semantics (e.g., believing 0 means "unlimited")
- Default value overrides without validation
- Configuration template errors affecting multiple validators

The absence of sanitization and tests increases the probability that invalid configurations reach production undetected. Unlike code bugs that trigger during testing, configuration issues only manifest in specific deployment scenarios.

## Recommendation

Implement comprehensive validation for all DAG consensus configuration structures:

```rust
impl ConfigSanitizer for DagFetcherConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.dag_consensus.fetcher_config;
        
        // Validate concurrent responders
        if config.min_concurrent_responders == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name.to_owned(),
                "min_concurrent_responders must be > 0".to_string(),
            ));
        }
        if config.min_concurrent_responders > config.max_concurrent_responders {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name.to_owned(),
                format!(
                    "min_concurrent_responders ({}) must be <= max_concurrent_responders ({})",
                    config.min_concurrent_responders, config.max_concurrent_responders
                ),
            ));
        }
        if config.max_concurrent_fetches == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name.to_owned(),
                "max_concurrent_fetches must be > 0".to_string(),
            ));
        }
        if config.retry_interval_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name.to_owned(),
                "retry_interval_ms must be > 0".to_string(),
            ));
        }
        if config.rpc_timeout_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name.to_owned(),
                "rpc_timeout_ms must be > 0".to_string(),
            ));
        }
        Ok(())
    }
}

impl ConfigSanitizer for DagHealthConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.dag_consensus.health_config;
        
        if config.voter_pipeline_latency_limit_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name.to_owned(),
                "voter_pipeline_latency_limit_ms must be > 0".to_string(),
            ));
        }
        Ok(())
    }
}

// Similar sanitizers for ReliableBroadcastConfig and DagRoundStateConfig

impl ConfigSanitizer for DagConsensusConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        DagPayloadConfig::sanitize(node_config, node_type, chain_id)?;
        DagFetcherConfig::sanitize(node_config, node_type, chain_id)?;
        DagHealthConfig::sanitize(node_config, node_type, chain_id)?;
        ReliableBroadcastConfig::sanitize(node_config, node_type, chain_id)?;
        DagRoundStateConfig::sanitize(node_config, node_type, chain_id)?;
        Ok(())
    }
}
```

Add comprehensive test coverage for all configuration structures to verify invalid values are rejected.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_zero_min_concurrent_responders_rejected() {
        let node_config = NodeConfig {
            dag_consensus: DagConsensusConfig {
                fetcher_config: DagFetcherConfig {
                    min_concurrent_responders: 0,
                    ..Default::default()
                },
                ..Default::default()
            },
            ..Default::default()
        };

        let error = DagFetcherConfig::sanitize(&node_config, NodeType::Validator, None)
            .unwrap_err();
        assert!(matches!(error, Error::ConfigSanitizerFailed(_, _)));
        assert!(error.to_string().contains("min_concurrent_responders must be > 0"));
    }

    #[test]
    fn test_min_greater_than_max_responders_rejected() {
        let node_config = NodeConfig {
            dag_consensus: DagConsensusConfig {
                fetcher_config: DagFetcherConfig {
                    min_concurrent_responders: 10,
                    max_concurrent_responders: 5,
                    ..Default::default()
                },
                ..Default::default()
            },
            ..Default::default()
        };

        let error = DagFetcherConfig::sanitize(&node_config, NodeType::Validator, None)
            .unwrap_err();
        assert!(matches!(error, Error::ConfigSanitizerFailed(_, _)));
    }

    #[test]
    fn test_zero_voter_pipeline_latency_rejected() {
        let node_config = NodeConfig {
            dag_consensus: DagConsensusConfig {
                health_config: DagHealthConfig {
                    voter_pipeline_latency_limit_ms: 0,
                    ..Default::default()
                },
                ..Default::default()
            },
            ..Default::default()
        };

        let error = DagHealthConfig::sanitize(&node_config, NodeType::Validator, None)
            .unwrap_err();
        assert!(matches!(error, Error::ConfigSanitizerFailed(_, _)));
        assert!(error.to_string().contains("voter_pipeline_latency_limit_ms must be > 0"));
    }

    #[test]
    fn test_valid_fetcher_config_accepted() {
        let node_config = NodeConfig {
            dag_consensus: DagConsensusConfig {
                fetcher_config: DagFetcherConfig {
                    min_concurrent_responders: 1,
                    max_concurrent_responders: 4,
                    max_concurrent_fetches: 4,
                    retry_interval_ms: 500,
                    rpc_timeout_ms: 1000,
                },
                ..Default::default()
            },
            ..Default::default()
        };

        assert!(DagFetcherConfig::sanitize(&node_config, NodeType::Validator, None).is_ok());
    }
}
```

## Notes

This vulnerability demonstrates a **defense-in-depth failure** where critical configuration parameters lack validation at the application layer, relying solely on operator vigilance. While the default values are sensible, the absence of bounds checking allows catastrophic misconfigurations to pass silently into production.

The issue affects validator **availability** rather than consensus **safety** - the network continues operating with reduced capacity, but affected validators become non-functional. This represents a significant operational risk, particularly in scenarios where multiple validators deploy from a shared configuration template containing invalid values.

The fix requires minimal code changes but provides substantial robustness improvements, preventing an entire class of operational failures that currently require manual intervention and validator restarts to resolve.

### Citations

**File:** config/src/config/dag_consensus_config.rs (L82-100)
```rust
pub struct DagFetcherConfig {
    pub retry_interval_ms: u64,
    pub rpc_timeout_ms: u64,
    pub min_concurrent_responders: u32,
    pub max_concurrent_responders: u32,
    pub max_concurrent_fetches: usize,
}

impl Default for DagFetcherConfig {
    fn default() -> Self {
        Self {
            retry_interval_ms: 500,
            rpc_timeout_ms: 1000,
            min_concurrent_responders: 1,
            max_concurrent_responders: 4,
            max_concurrent_fetches: 4,
        }
    }
}
```

**File:** config/src/config/dag_consensus_config.rs (L104-123)
```rust
pub struct ReliableBroadcastConfig {
    pub backoff_policy_base_ms: u64,
    pub backoff_policy_factor: u64,
    pub backoff_policy_max_delay_ms: u64,

    pub rpc_timeout_ms: u64,
}

impl Default for ReliableBroadcastConfig {
    fn default() -> Self {
        Self {
            // A backoff policy that starts at 100ms and doubles each iteration up to 3secs.
            backoff_policy_base_ms: 2,
            backoff_policy_factor: 50,
            backoff_policy_max_delay_ms: 3000,

            rpc_timeout_ms: 1000,
        }
    }
}
```

**File:** config/src/config/dag_consensus_config.rs (L127-137)
```rust
pub struct DagRoundStateConfig {
    pub adaptive_responsive_minimum_wait_time_ms: u64,
}

impl Default for DagRoundStateConfig {
    fn default() -> Self {
        Self {
            adaptive_responsive_minimum_wait_time_ms: 500,
        }
    }
}
```

**File:** config/src/config/dag_consensus_config.rs (L141-155)
```rust
pub struct DagHealthConfig {
    pub chain_backoff_config: Vec<ChainHealthBackoffValues>,
    pub voter_pipeline_latency_limit_ms: u64,
    pub pipeline_backpressure_config: Vec<PipelineBackpressureValues>,
}

impl Default for DagHealthConfig {
    fn default() -> Self {
        Self {
            chain_backoff_config: Vec::new(),
            voter_pipeline_latency_limit_ms: 30_000,
            pipeline_backpressure_config: Vec::new(),
        }
    }
}
```

**File:** config/src/config/dag_consensus_config.rs (L169-179)
```rust
impl ConfigSanitizer for DagConsensusConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        DagPayloadConfig::sanitize(node_config, node_type, chain_id)?;

        Ok(())
    }
}
```

**File:** consensus/src/dag/dag_network.rs (L56-79)
```rust
impl Responders {
    fn new(mut peers: Vec<Author>, initial_request_count: u32, max_request_count: u32) -> Self {
        peers.shuffle(&mut rand::thread_rng());
        Self {
            peers,
            generator: ExponentialNumberGenerator::new(initial_request_count, 2, max_request_count),
        }
    }

    fn next_to_request(&mut self) -> Option<Vec<Author>> {
        // We want to immediately stop if the number generator is not returning any value.
        // expect will panic if the generator is not returning any value.
        #[allow(clippy::unwrap_in_result)]
        let count = self.generator.next().expect("should return a number");

        if self.peers.is_empty() {
            return None;
        }
        Some(
            self.peers
                .split_off(self.peers.len().saturating_sub(count as usize)),
        )
    }
}
```

**File:** consensus/src/dag/dag_network.rs (L180-211)
```rust
struct ExponentialNumberGenerator {
    current: u32,
    factor: u32,
    max_limit: u32,
}

impl ExponentialNumberGenerator {
    fn new(starting_value: u32, factor: u32, max_limit: u32) -> Self {
        Self {
            current: starting_value,
            factor,
            max_limit,
        }
    }
}

impl Iterator for ExponentialNumberGenerator {
    type Item = u32;

    fn next(&mut self) -> Option<Self::Item> {
        let result = self.current;
        if self.current < self.max_limit {
            self.current = self
                .current
                .checked_mul(self.factor)
                .unwrap_or(self.max_limit)
                .min(self.max_limit)
        }

        Some(result)
    }
}
```

**File:** consensus/src/dag/dag_fetcher.rs (L316-363)
```rust
        let mut rpc = RpcWithFallback::new(
            responders,
            remote_request.clone().into(),
            Duration::from_millis(self.config.retry_interval_ms),
            Duration::from_millis(self.config.rpc_timeout_ms),
            self.network.clone(),
            self.time_service.clone(),
            self.config.min_concurrent_responders,
            self.config.max_concurrent_responders,
        );

        while let Some(RpcResultWithResponder { responder, result }) = rpc.next().await {
            match result {
                Ok(DAGRpcResult(Ok(response))) => {
                    match FetchResponse::try_from(response).and_then(|response| {
                        response.verify(&remote_request, &self.epoch_state.verifier)
                    }) {
                        Ok(fetch_response) => {
                            let certified_nodes = fetch_response.certified_nodes();
                            // TODO: support chunk response or fallback to state sync
                            {
                                for node in certified_nodes.into_iter().rev() {
                                    if let Err(e) = dag.add_node(node) {
                                        error!(error = ?e, "failed to add node");
                                    }
                                }
                            }

                            if dag.read().all_exists(remote_request.targets()) {
                                return Ok(());
                            }
                        },
                        Err(err) => {
                            info!(error = ?err, "failure parsing/verifying fetch response from {}", responder);
                        },
                    };
                },
                Ok(DAGRpcResult(Err(dag_rpc_error))) => {
                    info!(error = ?dag_rpc_error, responder = responder, "fetch failure: target {} returned error", responder);
                },
                Err(err) => {
                    info!(error = ?err, responder = responder, "rpc failed to {}", responder);
                },
            }
        }
        Err(DagFetchError::Failed)
    }
}
```

**File:** consensus/src/dag/health/pipeline_health.rs (L77-80)
```rust
    fn stop_voting(&self) -> bool {
        let latency = self.adapter.pipeline_pending_latency();
        latency > self.voter_pipeline_latency_limit
    }
```

**File:** consensus/src/dag/rb_handler.rs (L218-222)
```rust
    async fn process(&self, node: Self::Request) -> anyhow::Result<Self::Response> {
        ensure!(
            !self.health_backoff.stop_voting(),
            NodeBroadcastHandleError::VoteRefused
        );
```
