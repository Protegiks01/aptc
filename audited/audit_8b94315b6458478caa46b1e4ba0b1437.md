# Audit Report

## Title
Redis Rate Limit Corruption via Multiple RedisRatelimit Checkers with Same Database Configuration

## Summary
The Aptos Faucet allows multiple `RedisRatelimit` checkers to be configured with identical Redis database settings but different rate limit thresholds. When multiple such checkers execute on the same request, they increment the same Redis counter multiple times, causing rate limit corruption where users are restricted faster than intended and the counter no longer accurately reflects the number of actual requests.

## Finding Description

The vulnerability exists in how the faucet processes checker configurations and executes rate limiting logic. [1](#0-0) 

Each `RedisRatelimitChecker` is instantiated independently from the configuration, with no validation to detect when multiple checkers share the same Redis database coordinates. [2](#0-1) 

The Redis key generation is deterministic based on the database connection, ratelimit key provider type, and time period: [3](#0-2) 

When a request arrives, all checkers run sequentially: [4](#0-3) 

Each `RedisRatelimitChecker.check()` performs these operations on the SAME Redis key: [5](#0-4) 

**Attack Scenario:**

Configuration file with two RedisRatelimit checkers:
```yaml
checker_configs:
  - type: "RedisRatelimit"
    database_address: "127.0.0.1"
    database_port: 6379
    database_number: 0
    max_requests_per_day: 10
    ratelimit_key_provider_config:
      type: "Ip"
  - type: "RedisRatelimit"
    database_address: "127.0.0.1"
    database_port: 6379
    database_number: 0
    max_requests_per_day: 100
    ratelimit_key_provider_config:
      type: "Ip"
```

**Execution Flow for IP 1.2.3.4:**
1. Request arrives, counter in Redis is at 5
2. **Checker A** (limit=10) executes:
   - Reads key `ip:1.2.3.4:100` → value is 5
   - Checks: 5 ≤ 10, so continues
   - Increments counter → value becomes 6
3. **Checker B** (limit=100) executes:
   - Reads key `ip:1.2.3.4:100` → value is now 6 (already incremented!)
   - Checks: 6 ≤ 100, so continues  
   - Increments counter → value becomes 7
4. Single request caused double increment: 5 → 7

Additionally, on error responses (500 status), both checkers decrement the counter: [6](#0-5) [7](#0-6) 

This causes double decrements on error conditions, further corrupting the counter.

## Impact Explanation

This vulnerability breaks the rate limiting invariant that users should be restricted to exactly `max_requests_per_day` requests. With N duplicate checkers, users effectively have a limit of `max_requests_per_day / N`, causing premature rate limiting.

Per the Aptos bug bounty criteria, this qualifies as **Medium Severity**: "State inconsistencies requiring intervention" - the Redis state becomes inconsistent with intended rate limits, requiring manual database cleanup or service restart to resolve. The faucet no longer enforces its configured rate limits correctly.

**Note:** While the security question classifies this as "High", the actual impact is limited to the faucet service (a testnet utility) and doesn't affect consensus, validator operations, or mainnet funds. The severity is more accurately Medium-to-Low rather than High.

## Likelihood Explanation

**Likelihood: Medium**

This misconfiguration can easily occur when:
1. Operators copy-paste checker configurations without modifying database settings
2. Different rate limits are desired for different user types (IP vs JWT) but database_number isn't changed
3. Configuration files are merged from multiple sources

The current validation only checks if configurations parse correctly: [8](#0-7) 

There is no validation to detect duplicate Redis database references, making this error silent until runtime behavior reveals the issue.

## Recommendation

Implement configuration validation to detect and reject multiple `RedisRatelimit` checkers sharing the same Redis database coordinates. Add this validation during configuration parsing:

```rust
// In crates/aptos-faucet/core/src/server/run.rs or validate_config.rs
fn validate_no_duplicate_redis_databases(checker_configs: &[CheckerConfig]) -> Result<()> {
    use std::collections::HashSet;
    
    let mut redis_databases = HashSet::new();
    
    for config in checker_configs {
        if let CheckerConfig::RedisRatelimit(redis_config) = config {
            let db_key = (
                redis_config.database_address.clone(),
                redis_config.database_port,
                redis_config.database_number,
                format!("{:?}", redis_config.ratelimit_key_provider_config),
            );
            
            if !redis_databases.insert(db_key.clone()) {
                anyhow::bail!(
                    "Duplicate Redis database configuration detected: {}:{}:{} with same ratelimit_key_provider. \
                    Multiple RedisRatelimit checkers cannot share the same database coordinates as this causes \
                    rate limit corruption. Use different database_number values for each checker.",
                    db_key.0, db_key.1, db_key.2
                );
            }
        }
    }
    
    Ok(())
}
```

Call this validation in `RunConfig::run_impl()` before building checkers, and in `ValidateConfig::validate_config()`.

**Alternative Solution:** Modify `RedisRatelimitChecker` to use namespaced keys that include a unique checker identifier, but this would break backward compatibility with existing Redis data.

## Proof of Concept

Create a configuration file `duplicate_redis_poc.yaml`:

```yaml
server_config:
  listen_address: "0.0.0.0"
  listen_port: 8081
  api_path_base: ""
metrics_server_config:
  listen_port: 9105
  disable: false
bypasser_configs: []
checker_configs:
  - type: "RedisRatelimit"
    database_address: "127.0.0.1"
    database_port: 6379
    database_number: 0
    max_requests_per_day: 5
    ratelimit_key_provider_config:
      type: "Ip"
  - type: "RedisRatelimit"
    database_address: "127.0.0.1"
    database_port: 6379
    database_number: 0  # SAME database as above
    max_requests_per_day: 10
    ratelimit_key_provider_config:
      type: "Ip"  # SAME provider as above
funder_config:
  type: "FakeFunder"
handler_config:
  use_helpful_errors: true
  return_rejections_early: false
```

**Steps to reproduce:**
1. Start Redis: `redis-server`
2. Run faucet: `cargo run -p aptos-faucet-service -- run -c duplicate_redis_poc.yaml`
3. Monitor Redis counter: `redis-cli MONITOR` in another terminal
4. Send 3 requests from same IP:
   ```bash
   for i in {1..3}; do
     curl -X POST http://localhost:8081/fund \
       -H "Content-Type: application/json" \
       -d '{"address": "0x1"}' 
   done
   ```
5. Observe Redis counter increments by 2 per request (instead of 1)
6. Fourth request is rate-limited even though limit is 5 (counter reached 6 after only 3 requests)

**Expected:** Counter should be 3 after 3 requests, rate limited after 5 requests  
**Actual:** Counter is 6 after 3 requests, rate limited after only 3 requests

## Notes

This vulnerability is specific to the **Aptos Faucet** service configuration and does not affect consensus, validator operations, or mainnet security. The faucet is a testnet utility for distributing test tokens. The impact is limited to incorrect rate limit enforcement on the faucet service itself.

### Citations

**File:** crates/aptos-faucet/core/src/server/run.rs (L128-139)
```rust
        let mut checkers: Vec<Checker> = Vec::new();
        for checker_config in &self.checker_configs {
            let checker = checker_config
                .clone()
                .build(captcha_manager.clone())
                .await
                .with_context(|| {
                    format!("Failed to build Checker with args: {:?}", checker_config)
                })?;
            checker.spawn_periodic_tasks(&mut join_set);
            checkers.push(checker);
        }
```

**File:** crates/aptos-faucet/core/src/checkers/redis_ratelimit.rs (L54-81)
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct RedisRatelimitCheckerConfig {
    /// The database address to connect to, not including port,
    /// e.g. db.example.com or 234.121.222.42.
    pub database_address: String,

    /// The port to connect to.
    #[serde(default = "RedisRatelimitCheckerConfig::default_database_port")]
    pub database_port: u16,

    /// The number of the database to use. If it doesn't exist, it will be created (todo verify this)
    #[serde(default = "RedisRatelimitCheckerConfig::default_database_number")]
    pub database_number: i64,

    /// The name of the user to use, if necessary.
    pub database_user: Option<String>,

    /// The password of the given user, if necessary.
    pub database_password: Option<String>,

    /// Max number of requests per key per day. 500s are not counted, because they are
    /// not the user's fault, but everything else is.
    pub max_requests_per_day: u32,

    /// This defines how we ratelimit, e.g. either by IP or by JWT (Firebase UID).
    #[serde(default)]
    pub ratelimit_key_provider_config: RatelimitKeyProviderConfig,
}
```

**File:** crates/aptos-faucet/core/src/checkers/redis_ratelimit.rs (L186-200)
```rust
    fn get_key_and_secs_until_next_day(
        &self,
        ratelimit_key_prefix: &str,
        ratelimit_key_value: &str,
    ) -> (String, u64) {
        let now_secs = get_current_time_secs();
        let seconds_until_next_day = seconds_until_next_day(now_secs);
        let key = format!(
            "{}:{}:{}",
            ratelimit_key_prefix,
            ratelimit_key_value,
            days_since_tap_epoch(now_secs)
        );
        (key, seconds_until_next_day)
    }
```

**File:** crates/aptos-faucet/core/src/checkers/redis_ratelimit.rs (L226-301)
```rust
    async fn check(
        &self,
        data: CheckerData,
        dry_run: bool,
    ) -> Result<Vec<RejectionReason>, AptosTapError> {
        let mut conn = self
            .get_redis_connection()
            .await
            .map_err(|e| AptosTapError::new_with_error_code(e, AptosTapErrorCode::StorageError))?;

        // Generate a key corresponding to this identifier and the current day.
        let key_prefix = self.ratelimit_key_provider.ratelimit_key_prefix();
        let key_value = self
            .ratelimit_key_provider
            .ratelimit_key_value(&data)
            .await?;
        let (key, seconds_until_next_day) =
            self.get_key_and_secs_until_next_day(key_prefix, &key_value);

        // Get the value for the key, indicating how many non-500 requests we have
        // serviced for it today.
        let limit_value: Option<i64> = conn.get(&key).await.map_err(|e| {
            AptosTapError::new_with_error_code(
                format!("Failed to get value for redis key {}: {}", key, e),
                AptosTapErrorCode::StorageError,
            )
        })?;

        // If the limit value is greater than what we allow per day, signal that we
        // should reject this request.
        if let Some(rejection_reason) = self.check_limit_value(limit_value, seconds_until_next_day)
        {
            return Ok(vec![rejection_reason]);
        }

        // Atomically increment the counter for the given key, creating it and setting
        // the expiration time if it doesn't already exist.
        if !dry_run {
            let incremented_limit_value = match limit_value {
                Some(_) => conn.incr(&key, 1).await.map_err(|e| {
                    AptosTapError::new_with_error_code(
                        format!("Failed to increment redis key {}: {}", key, e),
                        AptosTapErrorCode::StorageError,
                    )
                })?,
                // If the limit value doesn't exist, create it and set the
                // expiration time.
                None => {
                    let (incremented_limit_value,): (i64,) = redis::pipe()
                        .atomic()
                        .incr(&key, 1)
                        // Expire at the end of the day roughly.
                        .expire(&key, seconds_until_next_day as usize)
                        // Only set the expiration if one isn't already set.
                        // Only works with Redis 7 sadly.
                        // .arg("NX")
                        .ignore()
                        .query_async(&mut *conn)
                        .await
                        .map_err(|e| {
                            AptosTapError::new_with_error_code(
                                format!("Failed to increment value for redis key {}: {}", key, e),
                                AptosTapErrorCode::StorageError,
                            )
                        })?;
                    incremented_limit_value
                },
            };

            // Check limit again, to ensure there wasn't a get / set race.
            if let Some(rejection_reason) =
                self.check_limit_value(Some(incremented_limit_value), seconds_until_next_day)
            {
                return Ok(vec![rejection_reason]);
            }
        }
```

**File:** crates/aptos-faucet/core/src/checkers/redis_ratelimit.rs (L308-335)
```rust
    async fn complete(&self, data: CompleteData) -> Result<(), AptosTapError> {
        if !data.response_is_500 {
            return Ok(());
        }

        let mut conn = self
            .get_redis_connection()
            .await
            .map_err(|e| AptosTapError::new_with_error_code(e, AptosTapErrorCode::StorageError))?;

        // Generate a key corresponding to this identifier and the current day. In the
        // JWT case we re-verify the JWT. This is inefficient, but these failures are
        // extremely rare so I don't refactor for now.
        let key_prefix = self.ratelimit_key_provider.ratelimit_key_prefix();
        let key_value = self
            .ratelimit_key_provider
            .ratelimit_key_value(&data.checker_data)
            .await?;
        let (key, _) = self.get_key_and_secs_until_next_day(key_prefix, &key_value);

        let _: () = conn.decr(&key, 1).await.map_err(|e| {
            AptosTapError::new_with_error_code(
                format!("Failed to decrement value for redis key {}: {}", key, e),
                AptosTapErrorCode::StorageError,
            )
        })?;
        Ok(())
    }
```

**File:** crates/aptos-faucet/core/src/endpoints/fund.rs (L261-270)
```rust
        // Ensure request passes checkers.
        let mut rejection_reasons = Vec::new();
        for checker in &self.checkers {
            rejection_reasons.extend(checker.check(checker_data.clone(), dry_run).await.map_err(
                |e| AptosTapError::new_with_error_code(e, AptosTapErrorCode::CheckerError),
            )?);
            if !rejection_reasons.is_empty() && self.return_rejections_early {
                break;
            }
        }
```

**File:** crates/aptos-faucet/core/src/endpoints/fund.rs (L332-347)
```rust
        if !bypass {
            let response_is_500 = match &fund_result {
                Ok(_) => false,
                Err(e) => e.error_code.status().is_server_error(),
            };
            let complete_data = CompleteData {
                checker_data,
                txn_hashes: txn_hashes.clone(),
                response_is_500,
            };
            for checker in &self.checkers {
                checker.complete(complete_data.clone()).await.map_err(|e| {
                    AptosTapError::new_with_error_code(e, AptosTapErrorCode::CheckerError)
                })?;
            }
        }
```

**File:** crates/aptos-faucet/core/src/server/validate_config.rs (L17-35)
```rust
    pub async fn validate_config(&self) -> Result<()> {
        let file = File::open(&self.config_path).with_context(|| {
            format!(
                "Failed to load config at {}",
                self.config_path.to_string_lossy()
            )
        })?;
        let reader = BufReader::new(file);
        let run_config: RunConfig = serde_yaml::from_reader(reader).with_context(|| {
            format!(
                "Failed to parse config at {}",
                self.config_path.to_string_lossy()
            )
        })?;

        info!("Config is valid: {:#?}", run_config);

        Ok(())
    }
```
