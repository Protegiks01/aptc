# Audit Report

## Title
Exponential Backoff Bypass via Peer Reconnection in Storage Service Request Moderator

## Summary
The storage service request moderator's exponential backoff mechanism can be completely bypassed by malicious peers who disconnect and reconnect. When a peer disconnects, their accumulated backoff state is garbage collected, allowing them to start fresh with the initial penalty duration upon reconnection, effectively circumventing the intended escalating penalties for repeated misbehavior.

## Finding Description

The storage service implements a request moderation system to protect against peers sending excessive invalid requests. This system uses exponential backoff: after a peer sends too many invalid requests, they are ignored for a duration that doubles with each offense cycle. [1](#0-0) 

However, the `refresh_unhealthy_peer_states()` function garbage collects disconnected peers from the tracking map: [2](#0-1) 

When a peer reconnects and sends another invalid request, the `validate_request()` function creates a completely new `UnhealthyPeerState` with fresh initial values from the configuration: [3](#0-2) 

The default configuration values are: [4](#0-3) [5](#0-4) 

**Attack Flow:**
1. Malicious public fullnode (PFN) sends 500 invalid storage requests
2. Request moderator ignores the peer for 300 seconds (5 minutes)
3. Peer disconnects before the ignore period expires
4. `refresh_unhealthy_peer_states()` runs every 1 second and removes the peer's entry
5. Peer reconnects with the same `PeerNetworkId`
6. Peer sends another 500 invalid requests
7. Instead of being ignored for 600 seconds (doubled), peer is ignored for only 300 seconds again
8. Process repeats indefinitely, preventing escalation of penalties

The `PeerId` is derived from the peer's x25519 public key and remains constant across connections, meaning the same peer identity can exploit this indefinitely.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program criteria:

- **Resource Exhaustion**: The storage service must validate invalid requests that should have been blocked by accumulated backoff penalties
- **State Inconsistencies**: The backoff state intended to protect node resources is not preserved correctly across peer reconnections
- **Amplification**: Multiple malicious peers can exploit this simultaneously to magnify the resource consumption impact

The issue does not reach High or Critical severity because:
- It doesn't cause consensus violations or safety breaks
- It doesn't result in fund theft or permanent state corruption
- It doesn't cause complete node unavailability, but rather increased resource usage

However, it does enable persistent low-level denial of service attacks that bypass the intended protection mechanisms.

## Likelihood Explanation

This vulnerability is **highly likely** to be exploited:

- **Low Complexity**: The attack only requires normal peer connection/disconnection capabilities
- **No Special Access**: Any public fullnode can execute this attack
- **Easily Automated**: The disconnect/reconnect cycle can be scripted
- **No Detection**: The system has no mechanism to detect or prevent this pattern
- **Repeatable**: Can be executed continuously without limitation

The refresh interval of 1 second means the peer state is garbage collected almost immediately after disconnection, making the timing trivial to exploit.

## Recommendation

Implement persistent backoff state that survives peer disconnections. The backoff state should be keyed by `PeerNetworkId` and include a timestamp-based expiry rather than relying on connection state.

**Recommended Fix:**

Modify `UnhealthyPeerState` to track when the backoff state should expire rather than when to remove disconnected peers immediately:

```rust
pub struct UnhealthyPeerState {
    ignore_start_time: Option<Instant>,
    invalid_request_count: u64,
    max_invalid_requests: u64,
    min_time_to_ignore_secs: u64,
    backoff_expiry_time: Option<Instant>, // NEW: When to reset backoff state
    time_service: TimeService,
}
```

In `refresh_unhealthy_peer_states()`, modify the retention logic:

```rust
self.unhealthy_peer_states.retain(|peer_network_id, unhealthy_peer_state| {
    // Check if backoff expiry time has passed
    if let Some(expiry_time) = unhealthy_peer_state.backoff_expiry_time {
        if time_service.now() > expiry_time {
            return false; // Remove expired entries
        }
    }
    
    // Keep the entry even if peer is disconnected
    // Only remove if backoff state has expired (e.g., after 24 hours of no activity)
    true
});
```

Set `backoff_expiry_time` to a long duration (e.g., 24 hours) after the last backoff period ends, ensuring that frequent reconnections don't reset the exponential backoff accumulation.

## Proof of Concept

```rust
#[tokio::test]
async fn test_backoff_bypass_via_reconnection() {
    use crate::tests::{mock::MockClient, utils};
    use aptos_config::config::{PeerRole, StorageServiceConfig};
    use aptos_config::network_id::{NetworkId, PeerNetworkId};
    use aptos_network::application::metadata::ConnectionState;
    use aptos_storage_service_types::{
        requests::{DataRequest, StorageServiceRequest, TransactionsWithProofRequest},
        StorageServiceError,
    };
    use aptos_types::PeerId;
    use claims::assert_matches;
    
    // Create config with low thresholds for testing
    let max_invalid_requests = 3;
    let min_time_to_ignore_secs = 5;
    let storage_service_config = StorageServiceConfig {
        max_invalid_requests_per_peer: max_invalid_requests,
        min_time_to_ignore_peers_secs: min_time_to_ignore_secs,
        ..Default::default()
    };
    
    // Create the storage client and server
    let (mut mock_client, mut service, _, time_service, peers_and_metadata) =
        MockClient::new(None, Some(storage_service_config));
    utils::update_storage_server_summary(&mut service, 100, 10);
    
    let request_moderator = service.get_request_moderator();
    let unhealthy_peer_states = request_moderator.get_unhealthy_peer_states();
    
    // Create a malicious peer
    let peer_network_id = PeerNetworkId::new(NetworkId::Public, PeerId::random());
    peers_and_metadata
        .insert_connection_metadata(
            peer_network_id,
            create_test_connection_metadata(peer_network_id.peer_id(), 0),
        )
        .unwrap();
    
    tokio::spawn(service.start());
    
    // First cycle: Send invalid requests until ignored
    for _ in 0..max_invalid_requests {
        send_invalid_request(100, &mut mock_client, peer_network_id).await.unwrap_err();
    }
    
    // Verify peer is ignored
    let response = send_invalid_request(100, &mut mock_client, peer_network_id).await;
    assert_matches!(response.unwrap_err(), StorageServiceError::TooManyInvalidRequests(_));
    
    // Record the initial backoff duration
    let initial_backoff = unhealthy_peer_states
        .get(&peer_network_id)
        .unwrap()
        .min_time_to_ignore_secs;
    assert_eq!(initial_backoff, min_time_to_ignore_secs);
    
    // EXPLOIT: Disconnect the peer before backoff expires
    peers_and_metadata
        .update_connection_state(peer_network_id, ConnectionState::Disconnecting)
        .unwrap();
    
    // Wait for garbage collection
    time_service.into_mock().advance_ms_async(1500).await; // > refresh interval
    
    // Verify peer state was garbage collected
    assert!(!unhealthy_peer_states.contains_key(&peer_network_id));
    
    // Reconnect the same peer
    peers_and_metadata
        .update_connection_state(peer_network_id, ConnectionState::Connected)
        .unwrap();
    
    // Second cycle: Send invalid requests again
    for _ in 0..max_invalid_requests {
        send_invalid_request(100, &mut mock_client, peer_network_id).await.unwrap_err();
    }
    
    // VULNERABILITY: Peer should have doubled backoff (10 seconds), but has initial backoff (5 seconds)
    let peer_state = unhealthy_peer_states.get(&peer_network_id).unwrap();
    let actual_backoff = peer_state.min_time_to_ignore_secs;
    
    // This assertion FAILS, demonstrating the vulnerability
    // Expected: 10 seconds (doubled), Actual: 5 seconds (reset to initial)
    assert_eq!(actual_backoff, initial_backoff); // Should be initial_backoff * 2
    
    println!("VULNERABILITY CONFIRMED: Backoff reset to {} instead of doubling to {}", 
             actual_backoff, initial_backoff * 2);
}
```

**Notes:**

The vulnerability exists because the system prioritizes removing disconnected peers for memory management without considering that the same peer identity (`PeerNetworkId`) will likely reconnect. The exponential backoff is designed to provide increasing deterrence against repeated misbehavior, but this protection is completely nullified when peers can reset their penalty state through simple reconnections. This is particularly concerning because `PeerId` is cryptographically derived from the peer's public key and remains constant, meaning the system should be capable of tracking persistent peer behavior across connections.

### Citations

**File:** state-sync/storage-service/server/src/moderator.rs (L79-90)
```rust
    pub fn refresh_peer_state(&mut self, peer_network_id: &PeerNetworkId) {
        if let Some(ignore_start_time) = self.ignore_start_time {
            let ignored_duration = self.time_service.now().duration_since(ignore_start_time);
            if ignored_duration >= Duration::from_secs(self.min_time_to_ignore_secs) {
                // Reset the invalid request count
                self.invalid_request_count = 0;

                // Reset the ignore start time
                self.ignore_start_time = None;

                // Double the min time to ignore the peer
                self.min_time_to_ignore_secs *= 2;
```

**File:** state-sync/storage-service/server/src/moderator.rs (L161-177)
```rust
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
```

**File:** state-sync/storage-service/server/src/moderator.rs (L213-228)
```rust
        self.unhealthy_peer_states
            .retain(|peer_network_id, unhealthy_peer_state| {
                if connected_peers_and_metadata.contains_key(peer_network_id) {
                    // Refresh the ignored peer state
                    unhealthy_peer_state.refresh_peer_state(peer_network_id);

                    // If the peer is ignored, increment the ignored peer count
                    if unhealthy_peer_state.is_ignored() {
                        num_ignored_peers += 1;
                    }

                    true // The peer is still connected, so we should keep it
                } else {
                    false // The peer is no longer connected, so we should remove it
                }
            });
```

**File:** config/src/config/state_sync_config.rs (L201-201)
```rust
            max_invalid_requests_per_peer: 500,
```

**File:** config/src/config/state_sync_config.rs (L213-213)
```rust
            min_time_to_ignore_peers_secs: 300, // 5 minutes
```
