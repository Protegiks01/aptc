# Audit Report

## Title
Resource Leak and Node Stall Due to Failed Stream Cleanup in CriticalDataStreamTimeout Handler

## Summary
When a data stream exceeds the maximum number of consecutive timeouts and triggers a `CriticalDataStreamTimeout` error, the cleanup mechanism in `reset_active_stream()` can fail if the underlying MPSC channel to the streaming service is disconnected. This failure prevents the stream from being removed from memory, causing the node to become stuck in a loop attempting to use the dead stream, resulting in permanent sync stall and resource leaks.

## Finding Description

The vulnerability exists in the error handling path when `get_data_notification()` returns a `CriticalDataStreamTimeout` error. [1](#0-0) 

When this error occurs, both the bootstrapper and continuous syncer attempt cleanup by calling `reset_active_stream(None)`: [2](#0-1) [3](#0-2) 

The `reset_active_stream()` function attempts to terminate the stream via the streaming client: [4](#0-3) 

The critical flaw is that if `terminate_stream_with_feedback()` fails (line 1545-1550), the function returns early via the `?` operator, and the essential cleanup code at lines 1553-1554 (`self.active_data_stream = None` and `self.speculative_stream_state = None`) never executes.

The failure occurs when the underlying MPSC channel is disconnected, as the termination request is sent via an unbounded channel: [5](#0-4) 

When the streaming service crashes or the channel receiver is dropped, the `send()` operation fails with a `SendError`, which is converted to an `UnexpectedErrorEncountered` error: [6](#0-5) 

**Attack Path:**
1. Node's data stream times out consecutively, exceeding `max_num_stream_timeouts`
2. `CriticalDataStreamTimeout` error is triggered
3. Driver calls `reset_active_stream(None)` to clean up
4. Streaming service channel is disconnected (due to service crash, restart, or channel closure)
5. `terminate_stream_with_feedback()` fails when attempting to send termination request
6. `reset_active_stream()` returns early, leaving `active_data_stream` as `Some(...)`
7. On next `drive_progress()` call, the condition `if self.active_data_stream.is_some()` remains true
8. Driver attempts to process notifications from the same dead stream
9. Stream times out again immediately, repeating the cycle
10. Node is permanently stuck, cannot establish new streams, stops syncing [7](#0-6) 

## Impact Explanation

This vulnerability has **Medium to High Severity** impact:

**Medium Severity Impacts:**
- **State Inconsistency**: The node maintains a dead stream reference that cannot be cleared, leading to inconsistent internal state requiring manual intervention (node restart)
- **Resource Leak**: The `DataStreamListener` object and associated resources remain allocated in memory indefinitely

**High Severity Impacts:**
- **Node Slowdown/Stall**: The affected node stops making sync progress and falls behind the network, matching the "Validator node slowdowns" category in High Severity
- **Availability Impact**: If multiple nodes are affected simultaneously, network availability is reduced

The vulnerability does not cause consensus violations or fund loss, but significantly impacts individual node availability and reliability, particularly for validator nodes.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability requires specific conditions:
- A data stream must timeout consecutively beyond the threshold (controlled by `max_num_stream_timeouts` configuration)
- The streaming service MPSC channel must be disconnected at the exact moment cleanup is attempted

While not directly exploitable by external attackers, this can occur naturally due to:
- Software bugs causing streaming service crashes
- Resource exhaustion triggering service restarts
- Channel buffer saturation or memory pressure
- Unhandled panics in the streaming service task
- Race conditions during node shutdown/restart sequences

Once triggered, the node remains stuck until manual restart, making this a realistic operational issue affecting node reliability.

## Recommendation

Implement defensive cleanup that ensures the stream is always removed from memory, even if termination fails. Separate the "best-effort termination notification" from the "critical cleanup of local state."

**Recommended Fix:**

```rust
pub async fn reset_active_stream(
    &mut self,
    notification_and_feedback: Option<NotificationAndFeedback>,
) -> Result<(), Error> {
    // Always clear the stream references first (critical operation)
    let stream_to_terminate = self.active_data_stream.take();
    self.speculative_stream_state = None;
    
    // Then attempt to notify the streaming service (best-effort)
    if let Some(active_data_stream) = stream_to_terminate {
        let data_stream_id = active_data_stream.data_stream_id;
        if let Err(e) = utils::terminate_stream_with_feedback(
            &mut self.streaming_client,
            data_stream_id,
            notification_and_feedback,
        )
        .await
        {
            // Log the failure but don't propagate - cleanup already happened
            warn!(
                "Failed to notify streaming service of termination for stream {:?}: {:?}. \
                Stream resources cleaned up locally.",
                data_stream_id, e
            );
        }
    }
    
    Ok(())
}
```

This approach ensures:
1. Local cleanup always succeeds via `Option::take()` (infallible)
2. Stream termination notification is best-effort
3. Node can always recover by establishing a new stream
4. Streaming service is notified when possible, but failures don't prevent recovery

## Proof of Concept

```rust
#[tokio::test]
async fn test_stream_cleanup_failure_recovery() {
    use tokio::sync::mpsc;
    use futures::channel::oneshot;
    
    // Create a driver with a dead streaming service channel
    let (tx, _rx) = mpsc::unbounded_channel();
    drop(_rx); // Drop receiver to simulate channel disconnect
    
    let mut mock_client = MockStreamingClient::new();
    mock_client.request_sender = tx;
    
    let mut driver = create_test_driver(mock_client);
    
    // Create a stream listener and set it as active
    let (_, listener) = create_data_stream_listener();
    driver.active_data_stream = Some(listener);
    
    // Simulate critical timeout scenario
    // This should trigger reset_active_stream()
    let result = driver.fetch_next_data_notification().await;
    
    // Verify the error is returned
    assert!(matches!(result, Err(Error::CriticalDataStreamTimeout(_))));
    
    // BUG: active_data_stream should be None, but remains Some
    // because reset_active_stream failed to clean up
    assert!(driver.active_data_stream.is_some(), 
            "Stream should be None after cleanup, but cleanup failed!");
    
    // Next progress attempt will use the same dead stream
    let result2 = driver.drive_progress(&global_summary).await;
    
    // Node is now stuck in a loop with the dead stream
    assert!(result2.is_err(), "Node stuck with dead stream");
    
    // Stream was never cleaned up - resource leak
    assert!(driver.active_data_stream.is_some(), "Resource leak: stream still present");
}
```

## Notes

The vulnerability is confirmed by examining the test suite behavior. The existing timeout tests assume successful cleanup: [8](#0-7) 

These tests verify normal timeout handling but do not test the scenario where `terminate_stream_with_feedback()` fails. The mock client in tests always succeeds, masking this edge case in production environments where channel disconnection can occur.

### Citations

**File:** state-sync/state-sync-driver/src/utils.rs (L200-238)
```rust
pub async fn get_data_notification(
    max_stream_wait_time_ms: u64,
    max_num_stream_timeouts: u64,
    active_data_stream: Option<&mut DataStreamListener>,
) -> Result<DataNotification, Error> {
    let active_data_stream = active_data_stream
        .ok_or_else(|| Error::UnexpectedError("The active data stream does not exist!".into()))?;

    let timeout_ms = Duration::from_millis(max_stream_wait_time_ms);
    if let Ok(data_notification) = timeout(timeout_ms, active_data_stream.select_next_some()).await
    {
        // Update the metrics for the data notification receive latency
        metrics::observe_duration(
            &metrics::DATA_NOTIFICATION_LATENCIES,
            metrics::NOTIFICATION_CREATE_TO_RECEIVE,
            data_notification.creation_time,
        );

        // Reset the number of consecutive timeouts for the data stream
        active_data_stream.num_consecutive_timeouts = 0;
        Ok(data_notification)
    } else {
        // Increase the number of consecutive timeouts for the data stream
        active_data_stream.num_consecutive_timeouts += 1;

        // Check if we've timed out too many times
        if active_data_stream.num_consecutive_timeouts >= max_num_stream_timeouts {
            Err(Error::CriticalDataStreamTimeout(format!(
                "{:?}",
                max_num_stream_timeouts
            )))
        } else {
            Err(Error::DataStreamNotificationTimeout(format!(
                "{:?}",
                timeout_ms
            )))
        }
    }
}
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L424-437)
```rust
        if self.active_data_stream.is_some() {
            // We have an active data stream. Process any notifications!
            self.process_active_stream_notifications().await?;
        } else if self.storage_synchronizer.pending_storage_data() {
            // Wait for any pending data to be processed
            sample!(
                SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                info!("Waiting for the storage synchronizer to handle pending data!")
            );
        } else {
            // Fetch a new data stream to start streaming data
            self.initialize_active_data_stream(global_data_summary)
                .await?;
        }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L592-596)
```rust
        if matches!(result, Err(Error::CriticalDataStreamTimeout(_))) {
            // If the stream has timed out too many times, we need to reset it
            warn!("Resetting the currently active data stream due to too many timeouts!");
            self.reset_active_stream(None).await?;
        }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1539-1556)
```rust
    pub async fn reset_active_stream(
        &mut self,
        notification_and_feedback: Option<NotificationAndFeedback>,
    ) -> Result<(), Error> {
        if let Some(active_data_stream) = &self.active_data_stream {
            let data_stream_id = active_data_stream.data_stream_id;
            utils::terminate_stream_with_feedback(
                &mut self.streaming_client,
                data_stream_id,
                notification_and_feedback,
            )
            .await?;
        }

        self.active_data_stream = None;
        self.speculative_stream_state = None;
        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L192-196)
```rust
        if matches!(result, Err(Error::CriticalDataStreamTimeout(_))) {
            // If the stream has timed out too many times, we need to reset it
            warn!("Resetting the currently active data stream due to too many timeouts!");
            self.reset_active_stream(None).await?;
        }
```

**File:** state-sync/data-streaming-service/src/streaming_client.rs (L311-324)
```rust
    async fn send_stream_request(
        &self,
        client_request: StreamRequest,
    ) -> Result<oneshot::Receiver<Result<DataStreamListener, Error>>, Error> {
        let mut request_sender = self.request_sender.clone();
        let (response_sender, response_receiver) = oneshot::channel();
        let request_message = StreamRequestMessage {
            stream_request: client_request,
            response_sender,
        };
        request_sender.send(request_message).await?;

        Ok(response_receiver)
    }
```

**File:** state-sync/data-streaming-service/src/error.rs (L47-50)
```rust
impl From<SendError> for Error {
    fn from(error: SendError) -> Self {
        Error::UnexpectedErrorEncountered(error.to_string())
    }
```

**File:** state-sync/state-sync-driver/src/tests/bootstrapper.rs (L282-343)
```rust
async fn test_critical_timeout() {
    // Create a driver configuration with a genesis waypoint and a stream timeout of 1 second
    let mut driver_configuration = create_full_node_driver_configuration();
    driver_configuration.config.max_stream_wait_time_ms = 1000;
    driver_configuration.config.max_num_stream_timeouts = 6;

    // Create the mock streaming client
    let mut mock_streaming_client = create_mock_streaming_client();
    let mut expectation_sequence = Sequence::new();
    let (_notification_sender_1, data_stream_listener_1) = create_data_stream_listener();
    let (_notification_sender_2, data_stream_listener_2) = create_data_stream_listener();
    let data_stream_id_1 = data_stream_listener_1.data_stream_id;
    for data_stream_listener in [data_stream_listener_1, data_stream_listener_2] {
        mock_streaming_client
            .expect_get_all_epoch_ending_ledger_infos()
            .times(1)
            .with(eq(1))
            .return_once(move |_| Ok(data_stream_listener))
            .in_sequence(&mut expectation_sequence);
    }
    mock_streaming_client
        .expect_terminate_stream_with_feedback()
        .with(eq(data_stream_id_1), eq(None))
        .return_const(Ok(()));

    // Create the bootstrapper
    let (mut bootstrapper, _) =
        create_bootstrapper(driver_configuration, mock_streaming_client, None, true);

    // Create a global data summary where epoch 0 and 1 have ended
    let global_data_summary = create_global_summary(1);

    // Drive progress to initialize the epoch ending data stream
    drive_progress(&mut bootstrapper, &global_data_summary, false)
        .await
        .unwrap();

    // Drive progress and verify we get non-critical timeouts
    for _ in 0..5 {
        let error = drive_progress(&mut bootstrapper, &global_data_summary, false)
            .await
            .unwrap_err();
        assert_matches!(error, Error::DataStreamNotificationTimeout(_));
    }

    // Drive progress again and verify we get a critical timeout
    let error = drive_progress(&mut bootstrapper, &global_data_summary, false)
        .await
        .unwrap_err();
    assert_matches!(error, Error::CriticalDataStreamTimeout(_));

    // Drive progress to initialize the epoch ending data stream again
    drive_progress(&mut bootstrapper, &global_data_summary, false)
        .await
        .unwrap();

    // Drive progress again and verify we get a non-critical timeout
    let error = drive_progress(&mut bootstrapper, &global_data_summary, false)
        .await
        .unwrap_err();
    assert_matches!(error, Error::DataStreamNotificationTimeout(_));
}
```
