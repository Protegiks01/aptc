# Audit Report

## Title
TOCTOU Race in Sync Request Validation Allows Spurious SyncedBeyondTarget Errors

## Summary
A time-of-check-time-of-use (TOCTOU) race condition exists in `check_sync_request_progress()` where the ledger state can change between the initial satisfaction check and the final request handling, causing consensus to receive spurious `SyncedBeyondTarget` errors when storage advances during the race window.

## Finding Description

The vulnerability occurs in the `check_sync_request_progress()` function where there is a temporal gap between checking if a sync request is satisfied and actually handling the satisfied request. [1](#0-0) 

The function first fetches ledger info and checks if the sync request is satisfied. For `SyncTarget` requests, the check uses `>=` comparison: [2](#0-1) 

However, after this check passes, the function enters a drain loop with `yield_now().await`: [3](#0-2) 

During this yield, the async runtime can execute other branches of the driver's `select!` loop, including consensus commit notifications: [4](#0-3) 

When consensus commits advance the storage state beyond the original sync target, the subsequent handling step fetches fresh ledger info and performs a stricter validation: [5](#0-4) [6](#0-5) 

This validation uses `>` comparison and returns a `SyncedBeyondTarget` error if the storage has advanced past the target during the race window.

**Attack Scenario:**
1. Consensus requests sync to version 1000
2. State sync checks satisfaction: storage at version 1000 ≥ target 1000 ✓
3. During `yield_now()`, consensus commits blocks advancing storage to version 1050
4. State sync resumes, fetches fresh ledger info at version 1050
5. Validation check: 1050 > 1000 → returns `SyncedBeyondTarget` error to consensus
6. Consensus receives error for a successfully completed sync operation

This breaks the invariant that once a sync request is satisfied, it should be acknowledged successfully. The race creates inconsistent behavior where valid sync operations are rejected based on timing.

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:
- **Significant protocol violations**: The TOCTOU race violates the atomicity guarantee of sync request validation, causing state sync to report errors for successfully completed operations
- **Validator node slowdowns**: Spurious errors force unnecessary sync retries, degrading validator performance during high-throughput periods
- **Liveness impact**: Repeated failures in consensus-state sync coordination can delay block processing and reduce network throughput

The vulnerability affects the critical consensus-state sync handover mechanism. When consensus requests state sync to reach a specific target and state sync successfully reaches that target, consensus should receive acknowledgment, not an error. The race condition undermines this coordination protocol.

## Likelihood Explanation

**High likelihood** of natural occurrence:
- The race window is guaranteed by `yield_now().await` in the drain loop
- On high-throughput networks, consensus commits happen continuously
- Multiple validators experiencing this simultaneously can amplify network-wide impact
- No attacker action required - manifests naturally under load

**Moderate likelihood** of deliberate exploitation:
- Byzantine validators could time commits to maximize race window hits
- Coordinated validators could amplify the effect across the network
- Requires validator-level access for optimal exploitation

The vulnerability is particularly concerning because it manifests naturally during normal operation and doesn't require malicious actors. Any production deployment under moderate-to-high load will encounter this race condition.

## Recommendation

Implement atomic snapshot semantics for sync request validation by capturing ledger state once and using it consistently throughout the function:

```rust
async fn check_sync_request_progress(&mut self) -> Result<(), Error> {
    // Capture sync request and ledger state atomically at the start
    let consensus_sync_request = self.consensus_notification_handler.get_sync_request();
    let sync_request_snapshot = match consensus_sync_request.lock().as_ref() {
        Some(consensus_sync_request) => {
            let latest_synced_ledger_info =
                utils::fetch_latest_synced_ledger_info(self.storage.clone())?;
            if !consensus_sync_request
                .sync_request_satisfied(&latest_synced_ledger_info, self.time_service.clone())
            {
                return Ok(()); 
            }
            // Store the validated state for later use
            Some((consensus_sync_request.clone(), latest_synced_ledger_info))
        },
        None => return Ok(()),
    };
    
    // Drain storage synchronizer (race window exists here, but we've captured state)
    while self.storage_synchronizer.pending_storage_data() {
        yield_now().await;
    }
    
    // Use the captured state for validation
    if let Some((sync_request, captured_ledger_info)) = sync_request_snapshot {
        // Duration request check using captured state...
        
        // Handle the satisfied sync request with captured ledger info
        // This prevents using fresh ledger info that may have changed
        self.consensus_notification_handler
            .handle_satisfied_sync_request(captured_ledger_info)
            .await?;
    }
    
    // ... rest of function
}
```

Additionally, modify `handle_satisfied_sync_request` to accept the timing semantics of the check by using `>=` instead of `>` for consistency, or document that overshooting the target is acceptable behavior.

## Proof of Concept

```rust
#[tokio::test]
async fn test_toctou_race_in_sync_request() {
    // Setup: Create driver with sync request for version 1000
    let storage = Arc::new(MockStorage::new_with_version(1000));
    let mut driver = create_test_driver(storage.clone());
    
    // Consensus requests sync to version 1000
    let sync_request = ConsensusSyncTargetNotification::new(
        create_ledger_info_at_version(1000)
    );
    driver.consensus_notification_handler
        .initialize_sync_target_request(sync_request, 999, 
            create_ledger_info_at_version(999))
        .await
        .unwrap();
    
    // Start check_sync_request_progress in background task
    let driver_clone = driver.clone();
    let check_task = tokio::spawn(async move {
        // This will reach the yield_now() in the drain loop
        driver_clone.check_sync_request_progress().await
    });
    
    // Simulate consensus commit during the race window
    tokio::time::sleep(Duration::from_millis(10)).await;
    storage.advance_to_version(1050); // Advance beyond target
    
    // Commit notification triggers during yield
    let commit_notification = create_commit_notification(1050);
    driver.handle_consensus_commit_notification(commit_notification).await;
    
    // The check task should receive SyncedBeyondTarget error
    let result = check_task.await.unwrap();
    assert!(matches!(result, Err(Error::SyncedBeyondTarget(1050, 1000))));
    
    // Consensus receives error for successfully completed sync
    // Expected: Ok(()), Actual: Err(SyncedBeyondTarget)
}
```

## Notes

The vulnerability is exacerbated by the semantic mismatch between `sync_request_satisfied()` (uses `>=`) and the validation in `handle_satisfied_sync_request()` (uses `>`). This inconsistency means that even though the initial check considers reaching or exceeding the target as success, the final handling treats exceeding as failure. The TOCTOU race exploits this gap by allowing storage to advance from "exactly at target" to "beyond target" between the two checks.

### Citations

**File:** state-sync/state-sync-driver/src/driver.rs (L316-350)
```rust
    async fn handle_consensus_commit_notification(
        &mut self,
        commit_notification: ConsensusCommitNotification,
    ) -> Result<(), Error> {
        info!(
            LogSchema::new(LogEntry::ConsensusNotification).message(&format!(
                "Received a consensus commit notification! Total transactions: {:?}, events: {:?}",
                commit_notification.get_transactions().len(),
                commit_notification.get_subscribable_events().len()
            ))
        );
        self.update_consensus_commit_metrics(&commit_notification);

        // Handle the commit notification
        let committed_transactions = CommittedTransactions {
            events: commit_notification.get_subscribable_events().clone(),
            transactions: commit_notification.get_transactions().clone(),
        };
        utils::handle_committed_transactions(
            committed_transactions,
            self.storage.clone(),
            self.mempool_notification_handler.clone(),
            self.event_subscription_service.clone(),
            self.storage_service_notification_handler.clone(),
        )
        .await;

        // Respond successfully
        self.consensus_notification_handler
            .respond_to_commit_notification(commit_notification, Ok(()))?;

        // Check the progress of any sync requests. We need this here because
        // consensus might issue a sync request and then commit (asynchronously).
        self.check_sync_request_progress().await
    }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L541-547)
```rust
                let latest_synced_ledger_info =
                    utils::fetch_latest_synced_ledger_info(self.storage.clone())?;
                if !consensus_sync_request
                    .sync_request_satisfied(&latest_synced_ledger_info, self.time_service.clone())
                {
                    return Ok(()); // The sync request hasn't been satisfied yet
                }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L556-564)
```rust
        while self.storage_synchronizer.pending_storage_data() {
            sample!(
                SampleRate::Duration(Duration::from_secs(PENDING_DATA_LOG_FREQ_SECS)),
                info!("Waiting for the storage synchronizer to handle pending data!")
            );

            // Yield to avoid starving the storage synchronizer threads.
            yield_now().await;
        }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L595-599)
```rust
        let latest_synced_ledger_info =
            utils::fetch_latest_synced_ledger_info(self.storage.clone())?;
        self.consensus_notification_handler
            .handle_satisfied_sync_request(latest_synced_ledger_info)
            .await?;
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L198-207)
```rust
            ConsensusSyncRequest::SyncTarget(sync_target_notification) => {
                // Get the sync target version and latest synced version
                let sync_target = sync_target_notification.get_target();
                let sync_target_version = sync_target.ledger_info().version();
                let latest_synced_version = latest_synced_ledger_info.ledger_info().version();

                // Check if we've satisfied the target
                latest_synced_version >= sync_target_version
            },
        }
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L345-356)
```rust
                // Check if we've synced beyond the target. If so, notify consensus with an error.
                if latest_synced_version > sync_target_version {
                    let error = Err(Error::SyncedBeyondTarget(
                        latest_synced_version,
                        sync_target_version,
                    ));
                    self.respond_to_sync_target_notification(
                        sync_target_notification,
                        error.clone(),
                    )?;
                    return error;
                }
```
