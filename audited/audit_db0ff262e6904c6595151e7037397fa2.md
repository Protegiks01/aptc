# Audit Report

## Title
Double Voting Vulnerability Due to Non-Durable Writes in ConsensusDB and SafetyRules Storage

## Summary
Validators can double-vote on the same consensus round after machine crashes due to non-durable write operations in ConsensusDB and SafetyRules storage. Both systems lack fsync operations, allowing OS buffer cache data to be lost during power failures or kernel panics, bypassing consensus safety mechanisms designed to prevent equivocation.

## Finding Description

The vulnerability exists in two critical storage systems that fail to provide durability guarantees for consensus safety-critical data:

**1. ConsensusDB Relaxed Writes**

ConsensusDB persists vote data using `write_schemas_relaxed()` which explicitly does not sync writes to disk: [1](#0-0) 

The implementation documentation explicitly warns that machine crashes cause data loss: [2](#0-1) 

**2. SafetyRules OnDiskStorage Lacks Fsync**

OnDiskStorage performs write-then-rename without fsync, making writes non-durable: [3](#0-2) 

The implementation explicitly states it should not be used in production: [4](#0-3) 

However, production-oriented configuration files configure validators with OnDiskStorage: [5](#0-4) [6](#0-5) 

The configuration sanitizer permits OnDiskStorage on mainnet (only blocking InMemoryStorage): [7](#0-6) 

**Vote Persistence Flow**

When a validator votes on a block, SafetyRules first persists the vote in SafetyData: [8](#0-7) 

Then RoundManager persists the vote to ConsensusDB: [9](#0-8) 

**SafetyRules Enforcement**

SafetyRules enforces the "first voting rule" to prevent double voting: [10](#0-9) 

The SafetyData structure contains the critical `last_voted_round` field: [11](#0-10) 

**Recovery Flow**

On restart, the recovery process loads vote data from ConsensusDB: [12](#0-11) 

RoundManager initialization restores the vote_sent state: [13](#0-12) 

However, RoundState.vote_sent is reset to None when a new round starts: [14](#0-13) 

**Exploitation Scenario**

1. Validator votes on block B₁ in round R
2. SafetyRules persists `SafetyData{last_voted_round: R}` via OnDiskStorage.write() - no fsync, data remains in OS buffer
3. RoundManager persists vote to ConsensusDB via write_schemas_relaxed() - no fsync, data remains in OS buffer
4. Machine crashes (power failure, kernel panic) before OS flushes buffers to disk
5. On recovery:
   - ConsensusDB.get_data() returns old data without the vote for round R
   - SafetyRules loads old SafetyData with `last_voted_round < R`
   - RoundState initializes with `vote_sent = None`
6. Different block B₂ ≠ B₁ is proposed for round R
7. RoundManager check passes: `vote_sent().is_none()` ✓
8. SafetyRules check passes: `R > last_voted_round` ✓
9. Validator creates and signs vote for B₂ in round R
10. **Result: Double voting on round R** (voted for both B₁ and B₂)

While equivocation detection exists in PendingVotes, it only detects double-votes after they're received from the network - it cannot prevent the validator from creating the second vote: [15](#0-14) 

## Impact Explanation

**Severity: Critical**

This vulnerability directly violates AptosBFT consensus safety guarantees, qualifying for Critical severity under the Aptos Bug Bounty program's "Consensus/Safety Violations" category (up to $1,000,000).

**Specific Impacts:**

1. **Consensus Safety Violation**: Validators can equivocate (double-vote), violating the BFT assumption that ≥2/3 of validators behave honestly
2. **Potential Chain Splits**: If multiple validators experience crashes during overlapping timing windows, sufficient equivocating votes could enable conflicting blocks to achieve quorum certificates, leading to chain divergence
3. **Loss of Funds**: Chain splits enable double-spending attacks where transactions are committed on one fork but not the other
4. **Network Partition**: Resolving consensus safety violations may require emergency hard forks with manual intervention

The vulnerability affects ALL validators because ConsensusDB always uses write_schemas_relaxed() regardless of the SafetyRules backend configuration.

## Likelihood Explanation

**Likelihood: Medium**

While the vulnerability requires a machine crash during a specific timing window, this is realistic in production deployments:

1. **Crash Frequency**: Validators experience crashes from power outages, hardware failures, OOM kills, and kernel panics
2. **Timing Window**: OS buffer cache flush delays typically range from 5-30 seconds (configurable via vm.dirty_writeback_centisecs), providing a substantial vulnerability window
3. **Universal Vulnerability**: ALL validators are affected due to ConsensusDB's use of write_schemas_relaxed()
4. **Multiple Validators**: With dozens of active validators, the probability that some experience crash-recovery during overlapping periods increases significantly
5. **Standard Fault Model**: Crash-recovery failures are expected in distributed systems and explicitly part of the BFT threat model

The vulnerability doesn't require active exploitation - it's triggered by normal operational failures that occur in large-scale deployments.

## Recommendation

**For ConsensusDB:**

Replace `write_schemas_relaxed()` with `write_schemas()` for vote persistence to ensure durability:

```rust
fn commit(&self, batch: SchemaBatch) -> Result<(), DbError> {
    self.db.write_schemas(batch)?;  // Use sync writes instead of relaxed
    Ok(())
}
```

**For SafetyRules OnDiskStorage:**

Add fsync calls after write operations:

```rust
fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
    let contents = serde_json::to_vec(data)?;
    let mut file = File::create(self.temp_path.path())?;
    file.write_all(&contents)?;
    file.sync_all()?;  // Add fsync before rename
    
    // Also sync parent directory after rename
    fs::rename(&self.temp_path, &self.file_path)?;
    let parent = self.file_path.parent().unwrap();
    File::open(parent)?.sync_all()?;
    
    Ok(())
}
```

**Configuration:**

Update default validator configurations to use Vault storage for SafetyRules in production, as documented in the README.

## Proof of Concept

A proof of concept would require:

1. Setting up a local validator with OnDiskStorage configuration
2. Triggering a vote on a test block
3. Forcefully killing the validator process with SIGKILL before OS buffer flush
4. Restarting the validator
5. Proposing a different block for the same round
6. Observing that the validator creates a second vote for the same round

This can be tested using Aptos local testnet with modified configurations to use shorter buffer flush times and instrumentation to verify double-vote creation.

## Notes

The core issue affects all validators through ConsensusDB's use of write_schemas_relaxed(), regardless of whether they use OnDiskStorage or Vault for SafetyRules storage. While the SafetyRules README states OnDiskStorage "should not be used in production," the default configuration templates (including Helm charts for Kubernetes deployments) configure validators with OnDiskStorage, and the configuration sanitizer permits this on mainnet. This creates a double failure mode where both SafetyRules and ConsensusDB lack durability guarantees.

### Citations

**File:** consensus/src/consensusdb/mod.rs (L156-159)
```rust
    fn commit(&self, batch: SchemaBatch) -> Result<(), DbError> {
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** storage/schemadb/src/lib.rs (L311-318)
```rust
    /// Writes without sync flag in write option.
    /// If this flag is false, and the machine crashes, some recent
    /// writes may be lost.  Note that if it is just the process that
    /// crashes (i.e., the machine does not reboot), no writes will be
    /// lost even if sync==false.
    pub fn write_schemas_relaxed(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &WriteOptions::default())
    }
```

**File:** secure/storage/src/on_disk.rs (L16-22)
```rust
/// OnDiskStorage represents a key value store that is persisted to the local filesystem and is
/// intended for single threads (or must be wrapped by a Arc<RwLock<>>). This provides no permission
/// checks and simply offers a proof of concept to unblock building of applications without more
/// complex data stores. Internally, it reads and writes all data to a file, which means that it
/// must make copies of all key material which violates the code base. It violates it because
/// the anticipation is that data stores would securely handle key material. This should not be used
/// in production.
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```

**File:** docker/compose/aptos-node/validator.yaml (L8-19)
```yaml
  safety_rules:
    service:
      type: "local"
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
    initial_safety_rules_config:
      from_file:
        waypoint:
          from_file: /opt/aptos/genesis/waypoint.txt
        identity_blob_path: /opt/aptos/genesis/validator-identity.yaml
```

**File:** terraform/helm/aptos-node/files/configs/validator-base.yaml (L10-22)
```yaml
consensus:
  safety_rules:
    service:
      type: "local"
    backend:
      type: "on_disk_storage"
      path: secure-data.json
      namespace: ~
    initial_safety_rules_config:
      from_file:
        waypoint:
          from_file: /opt/aptos/genesis/waypoint.txt
        identity_blob_path: /opt/aptos/genesis/validator-identity.yaml
```

**File:** config/src/config/safety_rules_config.rs (L86-96)
```rust
            // Verify that the secure backend is appropriate for mainnet validators
            if chain_id.is_mainnet()
                && node_type.is_validator()
                && safety_rules_config.backend.is_in_memory()
            {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "The secure backend should not be set to in memory storage in mainnet!"
                        .to_string(),
                ));
            }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L91-92)
```rust
        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;
```

**File:** consensus/src/round_manager.rs (L1539-1541)
```rust
        self.storage
            .save_vote(&vote)
            .context("[RoundManager] Fail to persist last vote")?;
```

**File:** consensus/src/round_manager.rs (L2018-2026)
```rust
    pub async fn init(&mut self, last_vote_sent: Option<Vote>) {
        let epoch_state = self.epoch_state.clone();
        let new_round_event = self
            .round_state
            .process_certificates(self.block_store.sync_info(), &epoch_state.verifier)
            .expect("Can not jump start a round_state from existing certificates.");
        if let Some(vote) = last_vote_sent {
            self.round_state.record_vote(vote);
        }
```

**File:** consensus/safety-rules/src/safety_rules.rs (L213-225)
```rust
    pub(crate) fn verify_and_update_last_vote_round(
        &self,
        round: Round,
        safety_data: &mut SafetyData,
    ) -> Result<(), Error> {
        if round <= safety_data.last_voted_round {
            return Err(Error::IncorrectLastVotedRound(
                round,
                safety_data.last_voted_round,
            ));
        }

        safety_data.last_voted_round = round;
```

**File:** consensus/consensus-types/src/safety_data.rs (L8-21)
```rust
/// Data structure for safety rules to ensure consensus safety.
#[derive(Debug, Deserialize, Eq, PartialEq, Serialize, Clone, Default)]
pub struct SafetyData {
    pub epoch: u64,
    pub last_voted_round: u64,
    // highest 2-chain round, used for 3-chain
    pub preferred_round: u64,
    // highest 1-chain round, used for 2-chain
    #[serde(default)]
    pub one_chain_round: u64,
    pub last_vote: Option<Vote>,
    #[serde(default)]
    pub highest_timeout_round: u64,
}
```

**File:** consensus/src/persistent_liveness_storage.rs (L404-418)
```rust
        Ok(RecoveryData {
            last_vote: match last_vote {
                Some(v) if v.epoch() == epoch => Some(v),
                _ => None,
            },
            root,
            root_metadata,
            blocks,
            quorum_certs,
            blocks_to_prune,
            highest_2chain_timeout_certificate: match highest_2chain_timeout_cert {
                Some(tc) if tc.epoch() == epoch => Some(tc),
                _ => None,
            },
        })
```

**File:** consensus/src/liveness/round_state.rs (L254-261)
```rust
        if new_round > self.current_round {
            let (prev_round_votes, prev_round_timeout_votes) = self.pending_votes.drain_votes();

            // Start a new round.
            self.current_round = new_round;
            self.pending_votes = PendingVotes::new();
            self.vote_sent = None;
            self.timeout_sent = None;
```

**File:** consensus/src/pending_votes.rs (L287-309)
```rust
        if let Some((previously_seen_vote, previous_li_digest)) =
            self.author_to_vote.get(&vote.author())
        {
            // is it the same vote?
            if &li_digest == previous_li_digest {
                // we've already seen an equivalent vote before
                let new_timeout_vote = vote.is_timeout() && !previously_seen_vote.is_timeout();
                if !new_timeout_vote {
                    // it's not a new timeout vote
                    return VoteReceptionResult::DuplicateVote;
                }
            } else {
                // we have seen a different vote for the same round
                error!(
                    SecurityEvent::ConsensusEquivocatingVote,
                    remote_peer = vote.author(),
                    vote = vote,
                    previous_vote = previously_seen_vote
                );

                return VoteReceptionResult::EquivocateVote;
            }
        }
```
