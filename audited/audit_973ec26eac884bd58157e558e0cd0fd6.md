# Audit Report

## Title
Digest Non-Determinism Due to Unvalidated Duplicate IDs in Batch Encryption Causing Consensus Divergence

## Summary
The `digest()` function in the FPTX batch encryption scheme does not validate for duplicate ciphertext IDs when creating the `IdSet`. This allows the same ciphertext to be included multiple times, producing different digests for logically identical batches. Since decryption keys are derived from digests, validators processing the same block with different ID multiplicities will compute incompatible decryption keys, breaking consensus determinism.

## Finding Description

The vulnerability exists in the `digest()` function's IdSet creation logic: [1](#0-0) 

The `IdSet::from_slice` method accepts any slice of IDs without duplicate validation: [2](#0-1) 

The `add` method blindly appends IDs to the polynomial roots vector: [3](#0-2) 

When duplicate IDs exist, the polynomial construction creates repeated factors. For example, IDs `[a, a, c]` produce polynomial `P(x) = (x-a)²(x-c)` instead of `P(x) = (x-a)(x-c)`: [4](#0-3) 

The digest is computed as a KZG commitment to this polynomial: [5](#0-4) 

Critically, the decryption key is directly derived from the digest: [6](#0-5) 

In the consensus decryption pipeline, ciphertexts are extracted from transactions without deduplication: [7](#0-6) 

**Attack Scenario:**
1. A block contains encrypted transactions where the same transaction appears multiple times (e.g., due to block construction bug or malicious proposer)
2. Validator A processes the block and extracts ciphertexts: `[C1, C1, C2]` (with duplicate)
3. Validator B applies deduplication: `[C1, C2]`
4. Validator A computes `digest_A = Commit((x-id₁)²(x-id₂))`
5. Validator B computes `digest_B = Commit((x-id₁)(x-id₂))`
6. `digest_A ≠ digest_B`
7. Validators derive different decryption key shares
8. Threshold reconstruction fails or produces different keys
9. Validators decrypt transactions differently
10. Different state roots → **consensus failure**

## Impact Explanation

**Critical Severity** - This vulnerability breaks the fundamental consensus invariant of deterministic execution. When validators compute different digests for the same logical block:

1. **Consensus Safety Violation**: Validators cannot agree on the canonical state, potentially causing chain splits
2. **Liveness Failure**: Threshold decryption requires agreement on the digest; disagreement prevents transaction decryption
3. **Non-Recoverable State**: Divergent execution produces incompatible state roots requiring hard fork to resolve

The impact aligns with Critical severity criteria: "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)."

## Likelihood Explanation

**Medium-High Likelihood**: While normal block construction should prevent duplicate transactions, the vulnerability is exploitable through:

1. **Implementation inconsistencies**: Different validator implementations may handle edge cases differently
2. **Block validation gaps**: If block validation doesn't explicitly reject duplicate transactions
3. **Malicious block proposer**: Byzantine proposer could craft blocks with duplicates
4. **Race conditions**: Network delays causing transaction replication in mempool

The lack of defensive validation in cryptographic code is a critical oversight—cryptographic operations must be robust against malformed inputs, not rely on upstream validation.

## Recommendation

Add duplicate detection to `IdSet::from_slice`:

```rust
pub fn from_slice(ids: &[Id]) -> Option<Self> {
    let mut result = Self::with_capacity(ids.len())?;
    let mut seen = std::collections::HashSet::new();
    
    for id in ids {
        if !seen.insert(id.x()) {
            return None; // Reject duplicates
        }
        result.add(id);
    }
    Some(result)
}
```

In `digest()`, handle the None case:

```rust
let mut ids: IdSet<UncomputedCoeffs> =
    IdSet::from_slice(&cts.iter().map(|ct| ct.id()).collect::<Vec<Id>>())
        .ok_or(anyhow!("Duplicate ciphertext IDs detected in batch"))?;
```

Additionally, add explicit deduplication in the consensus pipeline before digest computation:

```rust
let txn_ciphertexts: Vec<Ciphertext> = {
    let mut seen = std::collections::HashSet::new();
    encrypted_txns
        .iter()
        .filter_map(|txn| {
            let ct = txn.payload()
                .as_encrypted_payload()?
                .ciphertext()
                .clone();
            seen.insert(ct.id()).then_some(ct)
        })
        .collect()
};
```

## Proof of Concept

```rust
#[test]
fn test_duplicate_ids_cause_different_digests() {
    use crate::{
        shared::ids::{Id, IdSet},
        shared::digest::DigestKey,
    };
    use ark_std::rand::thread_rng;
    use ark_ff::One;
    
    let mut rng = thread_rng();
    let dk = DigestKey::new(&mut rng, 8, 1).unwrap();
    
    // Create two IdSets with same unique IDs but different multiplicities
    let id_a = Id::new(Fr::one());
    let id_b = Id::new(Fr::one() + Fr::one());
    
    // Set 1: no duplicates
    let mut ids1 = IdSet::with_capacity(4).unwrap();
    ids1.add(&id_a);
    ids1.add(&id_b);
    let (digest1, _) = dk.digest(&mut ids1, 0).unwrap();
    
    // Set 2: duplicate id_a
    let mut ids2 = IdSet::with_capacity(4).unwrap();
    ids2.add(&id_a);
    ids2.add(&id_a); // DUPLICATE!
    ids2.add(&id_b);
    let (digest2, _) = dk.digest(&mut ids2, 0).unwrap();
    
    // Digests are different despite same logical set
    assert_ne!(digest1, digest2, "Duplicate IDs produce different digests!");
    
    // This means different decryption keys would be derived
    // → Consensus divergence
}
```

**Notes**

This vulnerability demonstrates a critical failure in input validation for consensus-critical cryptographic operations. The batch encryption scheme's digest function must operate deterministically on the **logical set** of ciphertexts, not their multiset representation. The current implementation violates defense-in-depth principles by assuming upstream components will prevent duplicates, making the system fragile to implementation changes or Byzantine behavior. The fix requires both local validation in `IdSet::from_slice` and defensive deduplication in the consensus pipeline to ensure all validators compute identical digests for identical blocks.

### Citations

**File:** crates/aptos-batch-encryption/src/schemes/fptx.rs (L105-107)
```rust
        let mut ids: IdSet<UncomputedCoeffs> =
            IdSet::from_slice(&cts.iter().map(|ct| ct.id()).collect::<Vec<Id>>())
                .ok_or(anyhow!(""))?;
```

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L63-69)
```rust
    pub fn from_slice(ids: &[Id]) -> Option<Self> {
        let mut result = Self::with_capacity(ids.len())?;
        for id in ids {
            result.add(id);
        }
        Some(result)
    }
```

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L84-89)
```rust
    pub fn add(&mut self, id: &Id) {
        if self.poly_roots.len() >= self.capacity {
            panic!("Number of ids must be less than capacity");
        }
        self.poly_roots.push(id.root_x);
    }
```

**File:** crates/aptos-batch-encryption/src/shared/algebra/mult_tree.rs (L7-12)
```rust
pub fn compute_mult_tree<F: FftField>(roots: &[F]) -> Vec<Vec<DensePolynomial<F>>> {
    let mut bases: Vec<DensePolynomial<F>> = roots
        .iter()
        .cloned()
        .map(|u| DenseUVPolynomial::from_coefficients_vec(vec![-u, F::one()]))
        .collect();
```

**File:** crates/aptos-batch-encryption/src/shared/digest.rs (L123-132)
```rust
            let ids = ids.compute_poly_coeffs();
            let mut coeffs = ids.poly_coeffs();
            coeffs.resize(self.tau_powers_g1[round].len(), Fr::zero());

            let digest = Digest {
                digest_g1: G1Projective::msm(&self.tau_powers_g1[round], &coeffs)
                    .unwrap()
                    .into(),
                round,
            };
```

**File:** crates/aptos-batch-encryption/src/shared/key_derivation.rs (L107-115)
```rust
    pub fn derive_decryption_key_share(&self, digest: &Digest) -> Result<BIBEDecryptionKeyShare> {
        let hashed_encryption_key: G1Affine = symmetric::hash_g2_element(self.mpk_g2)?;

        Ok((self.player, BIBEDecryptionKeyShareValue {
            signature_share_eval: G1Affine::from(
                (digest.as_g1() + hashed_encryption_key) * self.shamir_share_eval,
            ),
        }))
    }
```

**File:** consensus/src/pipeline/decryption_pipeline_builder.rs (L78-93)
```rust
        let txn_ciphertexts: Vec<Ciphertext> = encrypted_txns
            .iter()
            .map(|txn| {
                // TODO(ibalajiarun): Avoid clone and use reference instead
                txn.payload()
                    .as_encrypted_payload()
                    .expect("must be a encrypted txn")
                    .ciphertext()
                    .clone()
            })
            .collect();

        // TODO(ibalajiarun): Consider using commit block height to reduce trusted setup size
        let encryption_round = block.round();
        let (digest, proofs_promise) =
            FPTXWeighted::digest(&digest_key, &txn_ciphertexts, encryption_round)?;
```
