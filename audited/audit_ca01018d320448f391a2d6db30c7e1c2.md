# Audit Report

## Title
Poisoned Mutex in Metrics Collectors Creates Monitoring Blind Spot for Validator Consensus Participation

## Summary
The `aptos_infallible::Mutex` wrapper panics when encountering poisoned locks, and metrics collectors use this mutex to protect system information state. If OS-level operations panic during metrics collection, the mutex becomes poisoned and all subsequent health check attempts panic, preventing orchestration systems from detecting when validators stop participating in consensus.

## Finding Description

The vulnerability arises from the interaction between three system components:

**1. Infallible Mutex Design** [1](#0-0) 

The `lock()` method explicitly panics on poisoned locks rather than returning a `Result`. This design choice means any panic while holding a lock permanently breaks all future lock acquisition attempts.

**2. Metrics Collectors with Panic-Prone Operations** [2](#0-1) [3](#0-2) 

Both collectors acquire the mutex and then call OS-level system information refresh functions (`system.refresh_memory()`, `system.refresh_process()`). These operations can panic due to:
- Insufficient permissions to read `/proc` filesystem
- Corrupted system state
- Resource exhaustion
- OS-level errors

**3. Health Check Dependency on Metrics Collection** [4](#0-3) 

The consensus health check endpoint calls `get_all_metrics()` which triggers Prometheus metric collection across all registered collectors. [5](#0-4) 

There is no panic recovery mechanism - if any collector panics, the entire health check fails.

**4. Kubernetes Configuration Without Proper Monitoring** [6](#0-5) 

Validators use only a `startupProbe` with `failureThreshold: 2147483647` (effectively infinite) and no `livenessProbe` or `readinessProbe`. The comment explicitly states "we don't want to restart the pod automatically even if it can't participate in consensus."

**Attack Scenario:**

1. Validator starts successfully and participates in consensus
2. During routine metrics collection, `system.refresh_memory()` or similar panics (OS error, permission issue, resource exhaustion)
3. Panic occurs while holding `Arc<Mutex<System>>` → mutex becomes poisoned
4. Next health check attempt calls `self.system.lock()` → panics with "Cannot currently handle a poisoned lock"
5. Health check endpoint returns HTTP error/timeout
6. Kubernetes sees failed probe but doesn't restart pod (infinite failureThreshold)
7. Pod status remains "Running/Ready" to orchestration systems
8. Validator later stops participating in consensus (network partition, key rotation failure, etc.)
9. **Monitoring blind spot**: Health check cannot detect consensus participation loss because it panics on every request
10. Operators see "healthy" pod in Kubernetes but validator is non-functional for consensus

## Impact Explanation

This qualifies as **Medium Severity** under "State inconsistencies requiring intervention":

- **Monitoring Blind Spot**: Once the mutex is poisoned, the health check endpoint permanently fails, preventing detection of actual consensus participation status
- **Reduced Network Capacity**: Failed validators remain deployed and consuming resources while appearing operational
- **Manual Intervention Required**: Operators must manually identify and restart affected pods since automated health checks are dysfunctional
- **Not Directly Exploitable**: While the condition can occur naturally, it's not directly exploitable by external attackers for fund theft or consensus break
- **Operational Impact**: Affects validator observability and incident response, but doesn't directly violate consensus safety or cause fund loss

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability requires specific conditions but is realistically achievable:

- **Trigger Condition**: System info refresh operations can panic due to various OS-level issues (documented in sysinfo crate issues, permission errors, corrupted `/proc` state, resource exhaustion)
- **No Recovery**: Once triggered, the condition is permanent until pod restart
- **No Attacker Required**: Can occur naturally in production environments under stress or misconfigurations
- **Wide Impact**: Affects all validators using the standard Helm deployment configuration
- **Detection Difficulty**: The monitoring system itself becomes dysfunctional, making the issue hard to diagnose

## Recommendation

**Immediate Fix**: Implement panic recovery in the health check endpoint:

```rust
// In crates/aptos-inspection-service/src/server/metrics.rs
use std::panic::{catch_unwind, AssertUnwindSafe};

pub async fn handle_consensus_health_check(node_config: &NodeConfig) -> (StatusCode, Body, String) {
    // Verify the node is a validator
    if !node_config.base.role.is_validator() {
        return (
            StatusCode::BAD_REQUEST,
            Body::from("This node is not a validator!"),
            CONTENT_TYPE_TEXT.into(),
        );
    }

    // Wrap metrics collection in panic recovery
    let metrics_result = catch_unwind(AssertUnwindSafe(|| {
        utils::get_all_metrics()
    }));

    let metrics = match metrics_result {
        Ok(m) => m,
        Err(_) => {
            error!("Metrics collection panicked - possible poisoned mutex");
            return (
                StatusCode::INTERNAL_SERVER_ERROR,
                Body::from("Health check failed: metrics collection error"),
                CONTENT_TYPE_TEXT.into(),
            );
        }
    };

    // Check consensus execution gauge
    if let Some(gauge_value) = metrics.get(CONSENSUS_EXECUTION_GAUGE) {
        if gauge_value == "1" {
            return (
                StatusCode::OK,
                Body::from("Consensus health check passed!"),
                CONTENT_TYPE_TEXT.into(),
            );
        }
    }

    (
        StatusCode::INTERNAL_SERVER_ERROR,
        Body::from("Consensus health check failed! Consensus is not executing!"),
        CONTENT_TYPE_TEXT.into(),
    )
}
```

**Long-term Fixes**:

1. **Replace aptos_infallible::Mutex** in metrics collectors with standard `Mutex` and handle poisoning gracefully
2. **Add livenessProbe and readinessProbe** to validator Kubernetes configuration to detect unhealthy pods
3. **Implement fallback metrics collection** that skips problematic collectors when they fail
4. **Add monitoring alerts** for repeated health check failures

## Proof of Concept

```rust
#[cfg(test)]
mod mutex_poisoning_test {
    use aptos_infallible::Mutex;
    use std::sync::Arc;
    use std::thread;

    #[test]
    #[should_panic(expected = "Cannot currently handle a poisoned lock")]
    fn test_poisoned_mutex_causes_panic() {
        // Create a mutex-protected value
        let data = Arc::new(Mutex::new(0));
        let data_clone = data.clone();

        // Thread 1: Panic while holding the lock, poisoning the mutex
        let handle = thread::spawn(move || {
            let mut guard = data_clone.lock();
            *guard = 1;
            panic!("Simulating panic during system info refresh");
        });

        // Wait for thread to panic
        let _ = handle.join();

        // Thread 2: Try to acquire the poisoned lock
        // This will panic with "Cannot currently handle a poisoned lock"
        let _guard = data.lock();
    }
}
```

To test the full vulnerability in an integration environment:

1. Deploy a validator with the standard Helm configuration
2. Inject a panic into `MemoryMetricsCollector::collect()` by simulating an OS error
3. Verify the health check endpoint returns errors on subsequent requests
4. Observe that the Kubernetes pod remains in Running/Ready state
5. Simulate validator consensus participation failure (network partition)
6. Confirm that operators cannot detect the consensus failure via health checks

**Notes**

This vulnerability represents a **monitoring blind spot** rather than a direct consensus or fund security issue. The core problem is the interaction between:

1. Design choice in `aptos_infallible::Mutex` to panic on poison rather than handle it
2. Use of this mutex in OS-level monitoring code that can legitimately panic
3. Lack of panic recovery in the health check path
4. Kubernetes configuration that doesn't restart pods on health check failures

While not directly exploitable for fund theft or consensus violations, it creates an operational security gap where validator failures become undetectable through standard monitoring, requiring manual intervention and potentially reducing effective network capacity.

### Citations

**File:** crates/aptos-infallible/src/mutex.rs (L19-23)
```rust
    pub fn lock(&self) -> MutexGuard<'_, T> {
        self.0
            .lock()
            .expect("Cannot currently handle a poisoned lock")
    }
```

**File:** crates/node-resource-metrics/src/collectors/memory_metrics_collector.rs (L101-106)
```rust
    fn collect(&self) -> Vec<MetricFamily> {
        let _measure = MeasureLatency::new("memory".into());

        let mut system = self.system.lock();
        system.refresh_memory();

```

**File:** crates/node-resource-metrics/src/collectors/process_metrics_collector.rs (L114-124)
```rust
    fn collect(&self) -> Vec<MetricFamily> {
        let _measure = MeasureLatency::new("process".into());

        let mut system = self.system.lock();

        let pid = if let Ok(pid) = sysinfo::get_current_pid() {
            system.refresh_process(pid);
            pid
        } else {
            return Vec::new();
        };
```

**File:** crates/aptos-inspection-service/src/server/metrics.rs (L20-40)
```rust
pub async fn handle_consensus_health_check(node_config: &NodeConfig) -> (StatusCode, Body, String) {
    // Verify the node is a validator. If not, return an error.
    if !node_config.base.role.is_validator() {
        return (
            StatusCode::BAD_REQUEST,
            Body::from("This node is not a validator!"),
            CONTENT_TYPE_TEXT.into(),
        );
    }

    // Check the value of the consensus execution gauge
    let metrics = utils::get_all_metrics();
    if let Some(gauge_value) = metrics.get(CONSENSUS_EXECUTION_GAUGE) {
        if gauge_value == "1" {
            return (
                StatusCode::OK,
                Body::from("Consensus health check passed!"),
                CONTENT_TYPE_TEXT.into(),
            );
        }
    }
```

**File:** crates/aptos-inspection-service/src/server/utils.rs (L49-51)
```rust
/// A simple utility function that returns all metric families
fn get_metric_families() -> Vec<MetricFamily> {
    let metric_families = aptos_metrics_core::gather();
```

**File:** terraform/helm/aptos-node/templates/validator.yaml (L137-147)
```yaml
        {{- if $.Values.validator.useConsensusHealthCheckAsStartupProbe }}
        startupProbe:
          httpGet:
            path: /consensus_health_check
            port: 9101
            scheme: HTTP
          failureThreshold: 2147483647 # set it to the max value since we don't want to restart the pod automatically even if it can't participate in consensus
          periodSeconds: 1
          successThreshold: 1
          timeoutSeconds: 3
        {{- end }}
```
