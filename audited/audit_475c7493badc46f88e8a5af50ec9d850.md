# Audit Report

## Title
Race Condition in Indexer Data Service Initialization: Health Check Port Exposed Before Secure Connections Established

## Summary
The indexer-grpc-data-service exposes its health check port with readiness endpoints to the network concurrently with (and potentially before) establishing secure Redis connections and initializing the main gRPC service. This creates a window during initialization where attackers can exploit monitoring endpoints for DoS attacks and premature traffic routing causes service failures.

## Finding Description

The initialization sequence in the indexer-grpc-data-service creates a race condition between health check port binding and main service initialization: [1](#0-0) 

The `run_server_with_config` function spawns two concurrent tasks:
1. **Health check server task** (line 53-56): Immediately binds to network and exposes endpoints
2. **Main service task** (line 57-58): Establishes Redis connections, then binds gRPC service [2](#0-1) 

The health check server exposes several endpoints immediately upon binding:
- `/readiness` (line 200-201): Returns "ready" unconditionally
- `/metrics` (line 203-217): Exposes Prometheus metrics  
- `/profilez` (line 226-249): Triggers CPU profiling (Linux only)
- `/` (line 219-222): Status page endpoint

Meanwhile, the main service initialization proceeds sequentially: [3](#0-2) 

The Redis connection is established and InMemoryCache initialized only after the health check task has already started. [4](#0-3) 

The main gRPC service binds only after Redis initialization completes, but the health check port is already serving requests.

**Attack Scenarios:**

1. **DoS via CPU Profiling**: An attacker monitoring for new service instances can repeatedly call `/profilez` during the initialization window, consuming CPU resources and delaying or preventing proper service initialization.

2. **Premature Traffic Routing**: Kubernetes/load balancers checking `/readiness` receive "ready" status before the gRPC service has bound to its ports, causing connection failures when traffic is routed.

3. **Information Disclosure**: The `/metrics` endpoint exposes Prometheus metrics about the partially initialized service, potentially leaking configuration details or internal state before security measures are fully in place.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program criteria:

- **Service Availability Impact**: The premature `/readiness` signal causes orchestration systems to route traffic to not-yet-ready instances, leading to connection failures and service disruption. This affects API availability which falls under "state inconsistencies requiring intervention" (Medium severity).

- **Resource Exhaustion**: The `/profilez` endpoint can be abused during initialization to exhaust CPU resources, preventing the service from becoming operational. While not a validator node, this affects critical indexing infrastructure.

- **Information Disclosure**: Early exposure of `/metrics` could leak sensitive configuration or operational details during the vulnerable initialization window (Low to Medium severity).

While this is an indexer service rather than core consensus infrastructure, it provides critical data availability for the Aptos ecosystem. Service disruption impacts applications relying on blockchain data access.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability manifests on every service startup:
- The race condition is guaranteed due to concurrent task spawning
- No special timing or conditions are required
- Attackers can easily monitor for new service instances (e.g., via Kubernetes events or DNS changes)
- The health check port is typically accessible from networks where load balancers and monitoring systems operate
- The `/profilez` endpoint requires no authentication and is exposed by design

The initialization window may be brief (seconds), but automated attacks can reliably exploit this window, especially during deployment scaling events or rolling updates where multiple instances start simultaneously.

## Recommendation

Implement proper initialization ordering to ensure the main service is ready before the health check server signals readiness:

**Solution 1: Add State-Aware Readiness Check**

Modify the `/readiness` endpoint to check actual service state rather than returning "ready" unconditionally. Use a shared atomic flag or channel that the main service sets only after completing initialization:

```rust
// In lib.rs, modify GenericConfig to track initialization state
pub struct GenericConfig<T> {
    pub health_check_port: u16,
    pub server_config: T,
    pub ready_signal: Arc<AtomicBool>, // Add initialization flag
}

// In register_probes_and_metrics_handler
let ready_signal = config.ready_signal.clone();
let readiness = warp::path("readiness").map(move || {
    if ready_signal.load(Ordering::Acquire) {
        warp::reply::with_status("ready", warp::http::StatusCode::OK)
    } else {
        warp::reply::with_status("not ready", warp::http::StatusCode::SERVICE_UNAVAILABLE)
    }
});

// In config.run(), set the flag after initialization completes
// After line 237 (after gRPC servers are spawned)
ready_signal.store(true, Ordering::Release);
```

**Solution 2: Sequential Initialization**

Delay health check server binding until after main service initialization:

```rust
pub async fn run_server_with_config<C>(config: GenericConfig<C>) -> Result<()>
where
    C: RunnableConfig,
{
    let health_port = config.health_check_port;
    
    // Initialize main service FIRST
    let main_service_ready = Arc::new(Notify::new());
    let ready_clone = main_service_ready.clone();
    
    let main_task_handler = tokio::spawn(async move {
        config.run().await.expect("task should exit with Ok.");
        ready_clone.notify_one(); // Signal that service is ready
    });
    
    // Wait for main service to be ready before starting health checks
    main_service_ready.notified().await;
    
    let config_clone = config.clone();
    let task_handler = tokio::spawn(async move {
        register_probes_and_metrics_handler(config_clone, health_port).await;
        anyhow::Ok(())
    });
    
    // Continue with tokio::select! as before
}
```

**Solution 3: Rate Limit Resource-Intensive Endpoints**

Add rate limiting to `/profilez` to prevent abuse:

```rust
// Add rate limiting middleware for profilez endpoint
let rate_limiter = Arc::new(RateLimiter::new(1, Duration::from_secs(60)));
let profilez = warp::path("profilez")
    .and(warp::any().map(move || rate_limiter.clone()))
    .and_then(|limiter: Arc<RateLimiter>| async move {
        if !limiter.check() {
            return Ok::<_, Rejection>(
                warp::reply::with_status(
                    "Rate limit exceeded",
                    warp::http::StatusCode::TOO_MANY_REQUESTS
                )
            );
        }
        // Proceed with profiling...
    });
```

## Proof of Concept

```rust
// Rust test demonstrating the race condition
#[cfg(test)]
mod race_condition_test {
    use super::*;
    use std::sync::atomic::{AtomicBool, Ordering};
    use std::sync::Arc;
    use std::time::Duration;
    use tokio::time::sleep;

    #[tokio::test]
    async fn test_health_check_ready_before_service_init() {
        // Simulate the current initialization behavior
        let health_check_ready = Arc::new(AtomicBool::new(false));
        let service_ready = Arc::new(AtomicBool::new(false));
        
        let health_check_ready_clone = health_check_ready.clone();
        let service_ready_clone = service_ready.clone();
        
        // Spawn health check task (immediate)
        let health_task = tokio::spawn(async move {
            // Simulate health check binding
            sleep(Duration::from_millis(10)).await;
            health_check_ready_clone.store(true, Ordering::Release);
            println!("Health check port bound and serving /readiness");
        });
        
        // Spawn main service task (delayed by initialization)
        let service_task = tokio::spawn(async move {
            // Simulate Redis connection establishment
            println!("Connecting to Redis...");
            sleep(Duration::from_millis(100)).await;
            
            // Simulate cache initialization
            println!("Initializing cache...");
            sleep(Duration::from_millis(100)).await;
            
            // Simulate gRPC server binding
            println!("Binding gRPC service...");
            sleep(Duration::from_millis(50)).await;
            
            service_ready_clone.store(true, Ordering::Release);
            println!("Main service ready");
        });
        
        // Check state during initialization
        sleep(Duration::from_millis(50)).await;
        
        let health_ready = health_check_ready.load(Ordering::Acquire);
        let svc_ready = service_ready.load(Ordering::Acquire);
        
        // VULNERABILITY: Health check is ready but service is not
        assert!(health_ready, "Health check should be ready");
        assert!(!svc_ready, "Service should NOT be ready yet");
        
        println!("RACE CONDITION DETECTED:");
        println!("  Health check ready: {}", health_ready);
        println!("  Main service ready: {}", svc_ready);
        println!("  Traffic could be routed to not-yet-ready service!");
        
        // Wait for tasks to complete
        health_task.await.unwrap();
        service_task.await.unwrap();
    }
    
    #[tokio::test]
    async fn test_profilez_dos_during_init() {
        use std::sync::atomic::AtomicUsize;
        
        let profiling_calls = Arc::new(AtomicUsize::new(0));
        let profiling_clone = profiling_calls.clone();
        
        // Simulate attacker repeatedly calling /profilez
        let attacker_task = tokio::spawn(async move {
            for i in 0..10 {
                println!("Attacker calling /profilez (attempt {})", i + 1);
                profiling_clone.fetch_add(1, Ordering::Relaxed);
                sleep(Duration::from_millis(20)).await;
            }
        });
        
        // Simulate service trying to initialize
        let service_task = tokio::spawn(async move {
            println!("Service initializing...");
            sleep(Duration::from_millis(250)).await;
            println!("Service initialization complete");
        });
        
        attacker_task.await.unwrap();
        service_task.await.unwrap();
        
        let total_calls = profiling_calls.load(Ordering::Relaxed);
        println!("DoS VULNERABILITY: /profilez called {} times during initialization", total_calls);
        assert!(total_calls > 5, "Multiple profiling calls succeeded during init window");
    }
}
```

**Notes**

This vulnerability is specific to the initialization ordering in the indexer-grpc-data-service framework. While the main gRPC service ports do establish Redis connections before binding (as intended), the health check monitoring port creates a separate race condition that exposes the service to attacks and operational failures during startup. The issue is exacerbated in containerized deployments where orchestration systems rely on readiness probes to make traffic routing decisions.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L46-77)
```rust
pub async fn run_server_with_config<C>(config: GenericConfig<C>) -> Result<()>
where
    C: RunnableConfig,
{
    let health_port = config.health_check_port;
    // Start liveness and readiness probes.
    let config_clone = config.clone();
    let task_handler = tokio::spawn(async move {
        register_probes_and_metrics_handler(config_clone, health_port).await;
        anyhow::Ok(())
    });
    let main_task_handler =
        tokio::spawn(async move { config.run().await.expect("task should exit with Ok.") });
    tokio::select! {
        res = task_handler => {
            if let Err(e) = res {
                error!("Probes and metrics handler panicked or was shutdown: {:?}", e);
                process::exit(1);
            } else {
                panic!("Probes and metrics handler exited unexpectedly");
            }
        },
        res = main_task_handler => {
            if let Err(e) = res {
                error!("Main task panicked or was shutdown: {:?}", e);
                process::exit(1);
            } else {
                panic!("Main task exited unexpectedly");
            }
        },
    }
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L196-264)
```rust
async fn register_probes_and_metrics_handler<C>(config: GenericConfig<C>, port: u16)
where
    C: RunnableConfig,
{
    let readiness = warp::path("readiness")
        .map(move || warp::reply::with_status("ready", warp::http::StatusCode::OK));

    let metrics_endpoint = warp::path("metrics").map(|| {
        // Metrics encoding.
        let metrics = aptos_metrics_core::gather();
        let mut encode_buffer = vec![];
        let encoder = TextEncoder::new();
        // If metrics encoding fails, we want to panic and crash the process.
        encoder
            .encode(&metrics, &mut encode_buffer)
            .context("Failed to encode metrics")
            .unwrap();

        Response::builder()
            .header("Content-Type", "text/plain")
            .body(encode_buffer)
    });

    let status_endpoint = warp::path::end().and_then(move || {
        let config = config.clone();
        async move { config.status_page().await }
    });

    if cfg!(target_os = "linux") {
        #[cfg(target_os = "linux")]
        let profilez = warp::path("profilez").and_then(|| async move {
            // TODO(grao): Consider make the parameters configurable.
            Ok::<_, Infallible>(match start_cpu_profiling(10, 99, false).await {
                Ok(body) => {
                    let response = Response::builder()
                        .header("Content-Length", body.len())
                        .header("Content-Disposition", "inline")
                        .header("Content-Type", "image/svg+xml")
                        .body(body);

                    match response {
                        Ok(res) => warp::reply::with_status(res, warp::http::StatusCode::OK),
                        Err(e) => warp::reply::with_status(
                            Response::new(format!("Profiling failed: {e:?}.").as_bytes().to_vec()),
                            warp::http::StatusCode::INTERNAL_SERVER_ERROR,
                        ),
                    }
                },
                Err(e) => warp::reply::with_status(
                    Response::new(format!("Profiling failed: {e:?}.").as_bytes().to_vec()),
                    warp::http::StatusCode::INTERNAL_SERVER_ERROR,
                ),
            })
        });
        #[cfg(target_os = "linux")]
        warp::serve(
            readiness
                .or(metrics_endpoint)
                .or(status_endpoint)
                .or(profilez),
        )
        .run(([0, 0, 0, 0], port))
        .await;
    } else {
        warp::serve(readiness.or(metrics_endpoint).or(status_endpoint))
            .run(([0, 0, 0, 0], port))
            .await;
    }
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/config.rs (L162-178)
```rust
        println!(
            ">>>> Starting Redis connection: {:?}",
            &self.redis_read_replica_address.0
        );
        let redis_conn = redis::Client::open(self.redis_read_replica_address.0.clone())?
            .get_tokio_connection_manager()
            .await?;
        println!(">>>> Redis connection established");
        // InMemoryCache.
        let in_memory_cache =
            aptos_indexer_grpc_utils::in_memory_cache::InMemoryCache::new_with_redis_connection(
                self.in_memory_cache_config.clone(),
                redis_conn,
                cache_storage_format,
            )
            .await?;
        println!(">>>> InMemoryCache established");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/config.rs (L204-214)
```rust
            tasks.push(tokio::spawn(async move {
                Server::builder()
                    .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
                    .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
                    .add_service(svc_clone)
                    .add_service(reflection_service_clone)
                    .serve(listen_address)
                    .await
                    .map_err(|e| anyhow::anyhow!(e))
            }));
        }
```
