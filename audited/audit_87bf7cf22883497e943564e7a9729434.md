# Audit Report

## Title
Thread Pool Exhaustion via Infinite Retry Loop in Table Info Retrieval Leading to API Denial of Service

## Summary
The `get_table_info_with_retry()` function contains an infinite retry loop with no timeout or maximum retry count, which can exhaust the blocking thread pool when called from API request handlers. While this does not create a classical lock-based deadlock, it creates a resource exhaustion vulnerability that prevents forward progress of the API layer, effectively causing a Denial of Service.

## Finding Description

The vulnerability exists in the interaction between the indexer's table info retrieval mechanism and the API layer's thread pool architecture.

**Core Issue:** [1](#0-0) 

The `get_table_info_with_retry()` function implements an **infinite retry loop** with no timeout, maximum retry count, or escape condition other than successful retrieval.

**Critical Call Path:**

1. API requests trigger transaction conversion in spawn_blocking contexts: [2](#0-1) 

2. Transaction conversion requires table info lookup via MoveConverter: [3](#0-2) 

3. MoveConverter delegates to IndexerReader: [4](#0-3) 

4. IndexerReader calls the problematic infinite retry: [5](#0-4) 

**No Lock-Based Deadlock:**

Analysis of the RocksDB operations confirms that `get_table_info()` performs simple database reads that do NOT acquire locks conflicting with writes: [6](#0-5) 

The underlying RocksDB read operation uses MVCC: [7](#0-6) 

**The Actual Vulnerability - Thread Pool Exhaustion:**

The tokio runtime is configured with a limited blocking thread pool (64 threads): [8](#0-7) 

**Attack Scenario:**

1. Indexer is catching up or lagging behind ledger state [9](#0-8) 

2. Attacker sends multiple API requests for transactions containing table items whose metadata hasn't been indexed yet

3. Each request spawns a blocking task that enters the infinite retry loop in `get_table_info_with_retry()`

4. All 64 blocking threads become occupied with sleeping threads waiting for table info

5. No new blocking operations can execute, including legitimate API requests

6. API layer becomes completely unresponsive - Denial of Service

This is not a classical deadlock (no circular lock dependency), but achieves the same result: **complete prevention of forward progress** through resource exhaustion.

## Impact Explanation

**Severity: High** per Aptos Bug Bounty criteria

This vulnerability causes **"API crashes"** and **"Significant protocol violations"** as defined in the High Severity category. Specifically:

- **API Unavailability**: The API layer becomes completely unresponsive when all blocking threads are exhausted
- **Validator Node Impact**: Validator nodes running public APIs would be unable to serve requests
- **Cascading Failures**: Other components dependent on blocking thread availability may fail
- **User Experience**: All API users experience complete service denial

While this does not directly affect consensus safety or cause loss of funds, it violates the critical **liveness invariant** that the system must remain available to process requests.

## Likelihood Explanation

**Likelihood: Medium to High**

The attack is likely to occur because:

1. **Natural Trigger Conditions**: The indexer legitimately lags during:
   - Node startup/catchup
   - High transaction throughput periods  
   - After network interruptions

2. **Low Attacker Requirements**: Any API user can trigger this without special privileges by:
   - Querying transactions with table operations
   - Sending multiple concurrent requests
   - No special authentication or resources needed

3. **No Built-in Protections**: The code has:
   - No timeout mechanism
   - No maximum retry count
   - No circuit breaker pattern
   - Only a 10ms sleep between retries (insufficient protection)

4. **API is Public**: Most Aptos nodes expose public APIs, making this widely exploitable

## Recommendation

Implement multiple layers of protection:

**1. Add Timeout and Retry Limits:**

```rust
pub fn get_table_info_with_retry(&self, handle: TableHandle) -> Result<Option<TableInfo>> {
    const MAX_RETRIES: u64 = 100; // ~1 second total with 10ms sleep
    const RETRY_TIMEOUT_SECS: u64 = 5;
    
    let start_time = std::time::Instant::now();
    let mut retried = 0;
    
    loop {
        if let Ok(Some(table_info)) = self.get_table_info(handle) {
            return Ok(Some(table_info));
        }

        // Check timeout
        if start_time.elapsed().as_secs() >= RETRY_TIMEOUT_SECS {
            return Err(AptosDbError::Other(format!(
                "Timeout waiting for table info for handle {}",
                handle.0.to_canonical_string()
            )));
        }

        // Check max retries
        if retried >= MAX_RETRIES {
            return Err(AptosDbError::Other(format!(
                "Max retries exceeded waiting for table info for handle {}",
                handle.0.to_canonical_string()
            )));
        }

        if retried == 0 {
            log_table_info_failure(handle, retried);
        } else {
            sample!(
                SampleRate::Duration(Duration::from_secs(1)),
                log_table_info_failure(handle, retried)
            );
        }

        retried += 1;
        std::thread::sleep(Duration::from_millis(TABLE_INFO_RETRY_TIME_MILLIS));
    }
}
```

**2. Handle Missing Table Info Gracefully in API Layer:**

The MoveConverter already has fallback logic but it's bypassed by the retry: [10](#0-9) 

Ensure errors from `get_table_info_with_retry()` are caught and handled gracefully rather than failing requests.

**3. Implement Rate Limiting:**

Add rate limiting on API endpoints that trigger table info lookups to prevent thread pool exhaustion attacks.

## Proof of Concept

The following Rust test demonstrates the thread exhaustion scenario:

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;

    #[test]
    #[ignore] // Run manually to observe thread exhaustion
    fn test_thread_pool_exhaustion_via_retry_loop() {
        // Setup: Create indexer with empty database (no table info)
        let tmpdir = tempfile::tempdir().unwrap();
        let db = DB::open(
            tmpdir.path(),
            "test_indexer_db",
            vec![TableInfoSchema::COLUMN_FAMILY_NAME],
            &rocksdb::Options::default(),
        ).unwrap();
        
        let indexer = Arc::new(IndexerAsyncV2::new(db).unwrap());
        
        // Create a tokio runtime with limited blocking threads
        let runtime = tokio::runtime::Builder::new_multi_thread()
            .max_blocking_threads(4) // Small pool to demonstrate exhaustion
            .build()
            .unwrap();
        
        // Spawn multiple tasks that will get stuck in infinite retry
        let mut handles = vec![];
        for i in 0..10 {
            let indexer_clone = indexer.clone();
            let handle = runtime.spawn_blocking(move || {
                println!("Thread {} starting table info lookup", i);
                let table_handle = TableHandle(AccountAddress::random());
                // This will loop forever since the table info doesn't exist
                let result = indexer_clone.get_table_info_with_retry(table_handle);
                println!("Thread {} completed: {:?}", i, result);
            });
            handles.push(handle);
            thread::sleep(Duration::from_millis(10));
        }
        
        // Attempt to spawn another blocking task
        // This should be queued/blocked because all threads are stuck
        let timeout_handle = runtime.spawn_blocking(|| {
            println!("Timeout task executed!");
        });
        
        // Wait briefly and verify the timeout task never executes
        thread::sleep(Duration::from_secs(2));
        
        // In a real scenario, the timeout task would never execute
        // because all blocking threads are stuck in retry loops
        println!("Thread pool exhausted - system unable to make forward progress");
        
        // Cleanup: This test intentionally doesn't await handles as they loop forever
    }
}
```

**To observe the vulnerability in production:**

1. Start an Aptos node with API enabled
2. Let the indexer lag (simulate by stopping/starting the node)
3. Send concurrent API requests for transactions with table operations
4. Monitor blocking thread pool utilization
5. Observe API becoming unresponsive as threads are exhausted

## Notes

**Clarification on "Deadlock" Terminology:**

This vulnerability does NOT create a classical lock-based deadlock (circular wait on mutexes). RocksDB operations use MVCC and reads do not conflict with writes. However, it creates a **resource deadlock** through thread pool exhaustion, which achieves the same practical outcome: complete prevention of forward progress and system liveness failure.

The infinite retry loop design is fundamentally flawed for use in resource-constrained contexts like thread pools. Any blocking operation must have bounded execution time to prevent resource exhaustion attacks.

### Citations

**File:** storage/indexer/src/db_v2.rs (L149-151)
```rust
    pub fn get_table_info(&self, handle: TableHandle) -> Result<Option<TableInfo>> {
        self.db.get::<TableInfoSchema>(&handle)
    }
```

**File:** storage/indexer/src/db_v2.rs (L153-173)
```rust
    pub fn get_table_info_with_retry(&self, handle: TableHandle) -> Result<Option<TableInfo>> {
        let mut retried = 0;
        loop {
            if let Ok(Some(table_info)) = self.get_table_info(handle) {
                return Ok(Some(table_info));
            }

            // Log the first failure, and then sample subsequent failures to avoid log spam
            if retried == 0 {
                log_table_info_failure(handle, retried);
            } else {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    log_table_info_failure(handle, retried)
                );
            }

            retried += 1;
            std::thread::sleep(Duration::from_millis(TABLE_INFO_RETRY_TIME_MILLIS));
        }
    }
```

**File:** api/src/transactions.rs (L974-977)
```rust
        api_spawn_blocking(move || {
            api.get_transaction_inner(&accept_type, txn_data, &latest_ledger_info)
        })
        .await
```

**File:** api/src/transactions.rs (L1023-1027)
```rust
                            .as_converter(
                                self.context.db.clone(),
                                self.context.indexer_reader.clone(),
                            )
                            .try_into_onchain_transaction(timestamp, txn)
```

**File:** api/types/src/convert.rs (L561-567)
```rust
        let table_info = match self.get_table_info(handle)? {
            Some(ti) => ti,
            None => {
                log_missing_table_info(handle);
                return Ok(None); // if table item not found return None anyway to avoid crash
            },
        };
```

**File:** api/types/src/convert.rs (L1060-1064)
```rust
    fn get_table_info(&self, handle: TableHandle) -> Result<Option<TableInfo>> {
        if let Some(indexer_reader) = self.indexer_reader.as_ref() {
            return Ok(indexer_reader.get_table_info(handle).unwrap_or(None));
        }
        Ok(None)
```

**File:** storage/indexer/src/indexer_reader.rs (L47-52)
```rust
    fn get_table_info(&self, handle: TableHandle) -> anyhow::Result<Option<TableInfo>> {
        if let Some(table_info_reader) = &self.table_info_reader {
            return Ok(table_info_reader.get_table_info_with_retry(handle)?);
        }
        anyhow::bail!("Table info reader is not available")
    }
```

**File:** storage/schemadb/src/lib.rs (L216-232)
```rust
    pub fn get<S: Schema>(&self, schema_key: &S::Key) -> DbResult<Option<S::Value>> {
        let _timer = APTOS_SCHEMADB_GET_LATENCY_SECONDS.timer_with(&[S::COLUMN_FAMILY_NAME]);

        let k = <S::Key as KeyCodec<S>>::encode_key(schema_key)?;
        let cf_handle = self.get_cf_handle(S::COLUMN_FAMILY_NAME)?;

        let result = self.inner.get_cf(cf_handle, k).into_db_res()?;
        APTOS_SCHEMADB_GET_BYTES.observe_with(
            &[S::COLUMN_FAMILY_NAME],
            result.as_ref().map_or(0.0, |v| v.len() as f64),
        );

        result
            .map(|raw_value| <S::Value as ValueCodec<S>>::decode_value(&raw_value))
            .transpose()
            .map_err(Into::into)
    }
```

**File:** crates/aptos-runtimes/src/lib.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use rayon::{ThreadPool, ThreadPoolBuilder};
use std::sync::atomic::{AtomicUsize, Ordering};
use tokio::runtime::{Builder, Runtime};

/// The max thread name length before the name will be truncated
/// when it's displayed. Note: the max display length is 15, but
/// we need to leave space for the thread IDs.
const MAX_THREAD_NAME_LENGTH: usize = 12;

/// Returns a tokio runtime with named threads.
/// This is useful for tracking threads when debugging.
pub fn spawn_named_runtime(thread_name: String, num_worker_threads: Option<usize>) -> Runtime {
    spawn_named_runtime_with_start_hook(thread_name, num_worker_threads, || {})
}

pub fn spawn_named_runtime_with_start_hook<F>(
    thread_name: String,
    num_worker_threads: Option<usize>,
    on_thread_start: F,
) -> Runtime
where
    F: Fn() + Send + Sync + 'static,
{
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L207-228)
```rust
        if indexer.next_version() < ledger_next_version {
            use aptos_storage_interface::state_store::state_view::db_state_view::DbStateViewAtVersion;
            let db: Arc<dyn DbReader> = self.state_store.clone();

            let state_view = db.state_view_at_version(Some(ledger_next_version - 1))?;
            let annotator = AptosValueAnnotator::new(&state_view);

            const BATCH_SIZE: Version = 10000;
            let mut next_version = indexer.next_version();
            while next_version < ledger_next_version {
                info!(next_version = next_version, "AptosDB Indexer catching up. ",);
                let end_version = std::cmp::min(ledger_next_version, next_version + BATCH_SIZE);
                let write_sets = self
                    .ledger_db
                    .write_set_db()
                    .get_write_sets(next_version, end_version)?;
                let write_sets_ref: Vec<_> = write_sets.iter().collect();
                indexer.index_with_annotator(&annotator, next_version, &write_sets_ref)?;

                next_version = end_version;
            }
        }
```
