# Audit Report

## Title
Silent Pruning Failures Leading to Unbounded State Accumulation and Node Availability Loss

## Summary
The `PrunerWorker` silently swallows pruning errors by only logging them (with sampling) and continuing the retry loop indefinitely. When persistent errors occur (disk full, corruption, permissions), pruning progress stops advancing while state continues accumulating, eventually causing disk exhaustion and node failure without explicit alerting or recovery mechanisms.

## Finding Description

The vulnerability exists in the error handling path of the state pruning system. The `StateKvShardPruner::prune()` function returns `Result<()>` and can fail for various reasons (I/O errors, disk full, corruption, etc.). [1](#0-0) 

When errors occur, they propagate through `StateKvPruner::prune()` correctly: [2](#0-1) 

However, the critical flaw is in `PrunerWorker::work()`, which runs in a background thread. When `pruner.prune()` returns an error, the code only logs it with sampling (once per second) and continues the loop: [3](#0-2) 

This creates a silent failure scenario where:
1. Pruning fails (e.g., disk near capacity causes I/O errors during batch write)
2. Error is logged with sampling rate of 1 second, meaning 999/1000 errors may not be logged in production (1ms sleep interval)
3. Progress is NOT updated because `prune()` returned early before reaching the progress update code
4. Worker continues and immediately retries the same batch
5. If the error persists, the infinite retry loop continues silently
6. Meanwhile, new blocks continue adding state to the database
7. Gap between `target_version` and `progress` grows, but no explicit alert is triggered
8. Eventually disk fills completely, causing node crash or unresponsiveness

The RocksDB errors that cause this include `IOError` conditions mapped to `AptosDbError`: [4](#0-3) 

## Impact Explanation

This is a **Medium Severity** vulnerability per the Aptos bug bounty criteria: "State inconsistencies requiring intervention."

**Impact:**
- **Unbounded state accumulation**: Pruning silently fails while state grows unchecked
- **Disk exhaustion**: Eventually leads to complete disk space exhaustion
- **Node unavailability**: When disk fills, the node crashes or becomes unresponsive
- **No automatic recovery**: Requires manual operator intervention to diagnose and fix
- **Degraded observability**: Error sampling means failures may not be noticed in logs
- **Affects all node types**: Validators, full nodes, and archive nodes all run pruning

While disk space alerts exist, they are generic and don't indicate pruner-specific failures. Operators may not correlate disk growth with pruning failure until it's too late.

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability can occur in several realistic scenarios:

1. **Disk approaching capacity**: As disk usage approaches limits, write operations may fail intermittently or permanently
2. **File system corruption**: Corrupted database files cause persistent read/write errors
3. **Permission issues**: If database files lose write permissions during operation
4. **Resource exhaustion**: File descriptor limits or other OS-level constraints
5. **Hardware failures**: Disk I/O errors from failing hardware

The vulnerability requires a persistent error condition rather than transient failures. However, in production systems with high write loads and limited disk space, this is a realistic scenario. The lack of circuit breakers, error counters, or recovery mechanisms makes the issue more likely to cause prolonged outages.

## Recommendation

Implement comprehensive error handling with alerting and recovery mechanisms:

**1. Add error counters and metrics:**
```rust
// In PrunerWorkerInner
consecutive_failures: AtomicUsize,
```

**2. Implement circuit breaker pattern:**
```rust
const MAX_CONSECUTIVE_FAILURES: usize = 10;

fn work(&self) {
    while !self.quit_worker.load(Ordering::SeqCst) {
        let pruner_result = self.pruner.prune(self.batch_size);
        if pruner_result.is_err() {
            let failures = self.consecutive_failures.fetch_add(1, Ordering::SeqCst) + 1;
            
            // Always log errors, not just sampled
            error!(
                error = ?pruner_result.err().unwrap(),
                consecutive_failures = failures,
                "Pruner has error."
            );
            
            // Update error metric
            PRUNER_ERROR_COUNT
                .with_label_values(&[self.pruner.name()])
                .inc();
            
            // Circuit breaker: if too many consecutive failures, alert and backoff
            if failures >= MAX_CONSECUTIVE_FAILURES {
                error!(
                    "Pruner has failed {} consecutive times. Entering extended backoff.",
                    failures
                );
                // Longer backoff for persistent errors
                sleep(Duration::from_secs(60));
            } else {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
            continue;
        }
        
        // Reset counter on success
        self.consecutive_failures.store(0, Ordering::SeqCst);
        
        if !self.pruner.is_pruning_pending() {
            sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
        }
    }
}
```

**3. Add Prometheus metrics:**
```rust
pub static PRUNER_ERROR_COUNT: Lazy<IntCounterVec> = Lazy::new(|| {
    register_int_counter_vec!(
        "aptos_pruner_error_count",
        "Total number of pruner errors",
        &["pruner_name"]
    ).unwrap()
});

pub static PRUNER_CONSECUTIVE_FAILURES: Lazy<IntGaugeVec> = Lazy::new(|| {
    register_int_gauge_vec!(
        "aptos_pruner_consecutive_failures",
        "Number of consecutive pruner failures",
        &["pruner_name"]
    ).unwrap()
});
```

**4. Add Prometheus alert rules:**
```yaml
- alert: PrunerConsecutiveFailures
  expr: aptos_pruner_consecutive_failures > 5
  for: 5m
  labels:
    severity: warning
  annotations:
    summary: "Pruner experiencing consecutive failures"
```

## Proof of Concept

Create a test that simulates persistent pruning failures:

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic::{AtomicBool, Ordering};
    use std::sync::Arc;
    
    struct FailingPruner {
        should_fail: Arc<AtomicBool>,
        call_count: Arc<AtomicUsize>,
    }
    
    impl DBPruner for FailingPruner {
        fn name(&self) -> &'static str {
            "test_pruner"
        }
        
        fn prune(&self, _max_versions: usize) -> Result<Version> {
            self.call_count.fetch_add(1, Ordering::SeqCst);
            if self.should_fail.load(Ordering::SeqCst) {
                Err(AptosDbError::IoError("Disk full".to_string()).into())
            } else {
                Ok(0)
            }
        }
        
        fn progress(&self) -> Version { 0 }
        fn set_target_version(&self, _: Version) {}
        fn target_version(&self) -> Version { 100 }
        fn record_progress(&self, _: Version) {}
    }
    
    #[test]
    fn test_pruner_silent_failure() {
        // Simulate persistent pruning failure
        let should_fail = Arc::new(AtomicBool::new(true));
        let call_count = Arc::new(AtomicUsize::new(0));
        
        let pruner = Arc::new(FailingPruner {
            should_fail: should_fail.clone(),
            call_count: call_count.clone(),
        });
        
        let worker = PrunerWorker::new(pruner, 1000, "test");
        
        // Wait for multiple pruning attempts
        std::thread::sleep(std::time::Duration::from_millis(500));
        
        // Verify that pruner was called multiple times despite failures
        let calls = call_count.load(Ordering::SeqCst);
        assert!(calls > 100, "Expected many retries, got {}", calls);
        
        // This demonstrates that errors are silently swallowed and retried indefinitely
        // In production, this would continue until disk fills and node crashes
    }
}
```

This test demonstrates that pruning errors are retried indefinitely without any backoff, circuit breaking, or explicit alerting beyond sampled logs.

## Notes

The vulnerability affects all three pruner types in the system:
- `StateKvPruner` (state key-value data)
- `StateMerklePruner` (Merkle tree nodes) 
- `LedgerPruner` (transaction data)

All use the same `PrunerWorker` pattern and are equally vulnerable. The fix should be applied consistently across all pruner implementations.

### Citations

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L67-78)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/schemadb/src/lib.rs (L389-408)
```rust
fn to_db_err(rocksdb_err: rocksdb::Error) -> AptosDbError {
    match rocksdb_err.kind() {
        ErrorKind::Incomplete => AptosDbError::RocksDbIncompleteResult(rocksdb_err.to_string()),
        ErrorKind::NotFound
        | ErrorKind::Corruption
        | ErrorKind::NotSupported
        | ErrorKind::InvalidArgument
        | ErrorKind::IOError
        | ErrorKind::MergeInProgress
        | ErrorKind::ShutdownInProgress
        | ErrorKind::TimedOut
        | ErrorKind::Aborted
        | ErrorKind::Busy
        | ErrorKind::Expired
        | ErrorKind::TryAgain
        | ErrorKind::CompactionTooLarge
        | ErrorKind::ColumnFamilyDropped
        | ErrorKind::Unknown => AptosDbError::OtherRocksDbError(rocksdb_err.to_string()),
    }
}
```
