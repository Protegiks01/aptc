# Audit Report

## Title
Missing Equivocation Detection in Certified Augmented Data Storage Allows Consensus Safety Violation

## Summary
The `add_certified_aug_data()` function in `AugDataStore` lacks an equivocation check that verifies whether newly received certified augmented data matches existing data from the same author. This allows a malicious validator to broadcast different certified augmented data containing different cryptographic deltas to different network partitions, causing validators to compute different randomness values and breaking consensus safety.

## Finding Description

The randomness generation protocol in Aptos consensus requires all validators to agree on augmented public keys (APKs) for each validator, which are derived from cryptographic deltas. These deltas are distributed via certified augmented data that must be consistent network-wide.

The vulnerability exists in how certified augmented data is validated and stored: [1](#0-0) 

The function only checks if certified data exists (`contains_key`) but does NOT verify that the content matches. Compare this with the uncertified augmented data handler: [2](#0-1) 

The uncertified handler explicitly checks for equivocation (lines 103-108), ensuring that if data from the same author already exists, the new data must equal the existing data. The certified handler lacks this critical validation.

**Attack Flow:**

1. A Byzantine validator creates two different `AugData` objects (AugData_A with delta_A, AugData_B with delta_B) for the same epoch
2. Through network partitioning or timing manipulation, they send AugData_A to network partition P1 and AugData_B to partition P2
3. Validators in P1 sign AugData_A; validators in P2 sign AugData_B (each partition has validators with >2/3 voting power through overlap)
4. The attacker collects sufficient signatures and creates CertifiedAugData_A and CertifiedAugData_B
5. They broadcast CertifiedAugData_A to P1 nodes and CertifiedAugData_B to P2 nodes
6. P1 nodes accept CertifiedAugData_A (first arrival), P2 nodes accept CertifiedAugData_B
7. When nodes from different partitions exchange certified data, the check at line 121 causes silent rejection (early return) without detecting the equivocation
8. The network now has permanently inconsistent delta values for the malicious validator

**Consensus Impact:**

The inconsistent deltas directly affect randomness generation: [3](#0-2) 

Lines 119-126 retrieve certified APKs (which are derived from the deltas), and lines 134-146 use these APKs to derive the final randomness. Different deltas lead to different APKs, causing `WVUF::derive_eval()` to produce different evaluation results, and ultimately different randomness values (line 146).

This violates the fundamental invariant that all honest validators must produce identical randomness for the same round, breaking consensus safety. [4](#0-3) 

The `augment()` call adds the delta to `RandConfig`, permanently affecting all subsequent randomness computations for that epoch.

## Impact Explanation

**Severity: Critical**

This vulnerability directly violates **Consensus Safety** - a Critical severity category in the Aptos bug bounty program. Specifically:

1. **Deterministic Execution Violation**: Different validators produce different randomness for identical inputs, breaking the core invariant that "all validators must produce identical state roots for identical blocks"

2. **Consensus Split**: When validators attempt to use the randomness for block ordering or other consensus decisions, they will disagree on outcomes, potentially causing network partition

3. **Non-Recoverable Without Intervention**: Once different deltas are embedded in validators' `RandConfig` state, they persist for the entire epoch. Recovery requires manual intervention or epoch transition

4. **Randomness Integrity Compromise**: The on-chain randomness beacon becomes unreliable as different validators see different randomness values, breaking any protocols dependent on consensus randomness

This meets the Critical severity criteria: "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)".

## Likelihood Explanation

**Likelihood: Medium**

The attack is feasible but requires specific conditions:

**Required Capabilities:**
- Control of at least one validator with valid signing keys
- Ability to manipulate network timing or create temporary partitions (achievable through network-level attacks or strategic peering)
- Precise timing to ensure different honest validators see different versions before cross-gossip occurs

**Mitigating Factors:**
- Requires collecting >2/3 voting power signatures for each version, though overlap in signing sets is possible if honest validators are partitioned
- Network gossip may cause honest validators to detect conflicts at the uncertified stage (where equivocation check exists)
- Attack window is limited to the certification period before network-wide synchronization

**Realistic Attack Scenario:**
A malicious validator with modest network control could exploit this by strategically timing their broadcasts during periods of high network latency or by targeting geographically distributed validators. The attack becomes more feasible in networks with natural partitioning (e.g., cross-region deployments).

## Recommendation

Add equivocation detection to `add_certified_aug_data()` matching the protection in `add_aug_data()`:

```rust
pub fn add_certified_aug_data(
    &mut self,
    certified_data: CertifiedAugData<D>,
) -> anyhow::Result<CertifiedAugDataAck> {
    // Check if certified data already exists for this author
    if let Some(existing_data) = self.certified_data.get(certified_data.author()) {
        // Verify the new data matches the existing data
        ensure!(
            existing_data == &certified_data,
            "[AugDataStore] equivocate certified data from {}. \
             Existing: {:?}, New: {:?}",
            certified_data.author(),
            existing_data,
            certified_data
        );
        return Ok(CertifiedAugDataAck::new(self.epoch));
    }
    self.db.save_certified_aug_data(&certified_data)?;
    certified_data
        .data()
        .augment(&self.config, &self.fast_config, certified_data.author());
    self.certified_data
        .insert(*certified_data.author(), certified_data);
    Ok(CertifiedAugDataAck::new(self.epoch))
}
```

**Additional Hardening:**
1. Log equivocation attempts for forensics and slashing
2. Consider broadcasting equivocation proofs to alert the network
3. Implement at-most-once semantic enforcement at the protocol level

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_types::{
        validator_signer::ValidatorSigner,
        aggregate_signature::AggregateSignature,
    };
    
    #[test]
    fn test_certified_aug_data_equivocation_not_detected() {
        // Setup two validators
        let signer = ValidatorSigner::random(None);
        let epoch = 1;
        
        // Create two different augmented data for the same (epoch, author)
        let aug_data_1 = AugData::new(
            epoch,
            signer.author(),
            AugmentedData { 
                delta: Delta::random(), // Different delta 1
                fast_delta: None 
            }
        );
        
        let aug_data_2 = AugData::new(
            epoch,
            signer.author(),
            AugmentedData { 
                delta: Delta::random(), // Different delta 2
                fast_delta: None 
            }
        );
        
        // Create valid signatures for both (simulating different validator sets)
        let sig_1 = AggregateSignature::new(/* validators P1 signatures */);
        let sig_2 = AggregateSignature::new(/* validators P2 signatures */);
        
        let certified_1 = CertifiedAugData::new(aug_data_1, sig_1);
        let certified_2 = CertifiedAugData::new(aug_data_2, sig_2);
        
        // Simulate Node A receiving certified_1 first
        let mut store_a = AugDataStore::new(/* ... */);
        store_a.add_certified_aug_data(certified_1.clone()).unwrap();
        
        // Simulate Node B receiving certified_2 first
        let mut store_b = AugDataStore::new(/* ... */);
        store_b.add_certified_aug_data(certified_2.clone()).unwrap();
        
        // Now when Node A receives certified_2 from Node B:
        let result_a = store_a.add_certified_aug_data(certified_2.clone());
        // BUG: Returns Ok without detecting equivocation!
        assert!(result_a.is_ok());
        
        // Similarly, when Node B receives certified_1:
        let result_b = store_b.add_certified_aug_data(certified_1.clone());
        // BUG: Returns Ok without detecting equivocation!
        assert!(result_b.is_ok());
        
        // Result: Nodes have different deltas for the same validator
        // This causes them to compute different randomness values
        assert_ne!(
            store_a.get_my_certified_aug_data().unwrap().data().delta,
            store_b.get_my_certified_aug_data().unwrap().data().delta
        );
        
        // When both generate randomness for the same round,
        // they will produce DIFFERENT outputs, breaking consensus
    }
}
```

## Notes

The vulnerability is particularly severe because:
1. It bypasses the intentional equivocation protection that exists for uncertified data
2. The silent failure mode (early return without error) makes detection and debugging extremely difficult
3. The inconsistency persists throughout the epoch, affecting all randomness-dependent consensus operations
4. There is no automated recovery mechanism - manual intervention would be required

The storage layer functions (`save_certified_aug_data()` in both in-memory and database implementations) correctly implement overwrite semantics as this is standard for key-value stores. The bug is in the caller (`add_certified_aug_data()`) which must enforce consistency but fails to do so.

### Citations

**File:** consensus/src/rand/rand_gen/aug_data_store.rs (L102-115)
```rust
    pub fn add_aug_data(&mut self, data: AugData<D>) -> anyhow::Result<AugDataSignature> {
        if let Some(existing_data) = self.data.get(data.author()) {
            ensure!(
                existing_data == &data,
                "[AugDataStore] equivocate data from {}",
                data.author()
            );
        } else {
            self.db.save_aug_data(&data)?;
        }
        let sig = AugDataSignature::new(self.epoch, self.signer.sign(&data)?);
        self.data.insert(*data.author(), data);
        Ok(sig)
    }
```

**File:** consensus/src/rand/rand_gen/aug_data_store.rs (L117-131)
```rust
    pub fn add_certified_aug_data(
        &mut self,
        certified_data: CertifiedAugData<D>,
    ) -> anyhow::Result<CertifiedAugDataAck> {
        if self.certified_data.contains_key(certified_data.author()) {
            return Ok(CertifiedAugDataAck::new(self.epoch));
        }
        self.db.save_certified_aug_data(&certified_data)?;
        certified_data
            .data()
            .augment(&self.config, &self.fast_config, certified_data.author());
        self.certified_data
            .insert(*certified_data.author(), certified_data);
        Ok(CertifiedAugDataAck::new(self.epoch))
    }
```

**File:** consensus/src/rand/rand_gen/types.rs (L97-148)
```rust
    fn aggregate<'a>(
        shares: impl Iterator<Item = &'a RandShare<Self>>,
        rand_config: &RandConfig,
        rand_metadata: RandMetadata,
    ) -> anyhow::Result<Randomness>
    where
        Self: Sized,
    {
        let timer = std::time::Instant::now();
        let mut apks_and_proofs = vec![];
        for share in shares {
            let id = rand_config
                .validator
                .address_to_validator_index()
                .get(share.author())
                .copied()
                .ok_or_else(|| {
                    anyhow!(
                        "Share::aggregate failed with invalid share author: {}",
                        share.author
                    )
                })?;
            let apk = rand_config
                .get_certified_apk(share.author())
                .ok_or_else(|| {
                    anyhow!(
                        "Share::aggregate failed with missing apk for share from {}",
                        share.author
                    )
                })?;
            apks_and_proofs.push((Player { id }, apk.clone(), share.share().share));
        }

        let proof = WVUF::aggregate_shares(&rand_config.wconfig, &apks_and_proofs);
        let metadata_serialized = bcs::to_bytes(&rand_metadata).map_err(|e| {
            anyhow!("Share::aggregate failed with metadata serialization error: {e}")
        })?;
        let eval = WVUF::derive_eval(
            &rand_config.wconfig,
            &rand_config.vuf_pp,
            metadata_serialized.as_slice(),
            &rand_config.get_all_certified_apk(),
            &proof,
            THREAD_MANAGER.get_exe_cpu_pool(),
        )
        .map_err(|e| anyhow!("Share::aggregate failed with WVUF derive_eval error: {e}"))?;
        debug!("WVUF derivation time: {} ms", timer.elapsed().as_millis());
        let eval_bytes = bcs::to_bytes(&eval)
            .map_err(|e| anyhow!("Share::aggregate failed with eval serialization error: {e}"))?;
        let rand_bytes = Sha3_256::digest(eval_bytes.as_slice()).to_vec();
        Ok(Randomness::new(rand_metadata, rand_bytes))
    }
```
