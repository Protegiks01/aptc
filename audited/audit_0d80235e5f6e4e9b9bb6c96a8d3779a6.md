# Audit Report

## Title
ConsensusDB Silent Data Loss on WAL Corruption Due to Unsynchronized Writes and Permissive Recovery Mode

## Summary
ConsensusDB uses unsynchronized writes (`write_schemas_relaxed`) combined with RocksDB's default `kPointInTimeRecovery` WAL recovery mode. When the Write-Ahead Log is corrupted (e.g., due to power failure or system crash), RocksDB silently truncates the WAL at the corruption point and opens successfully without raising an error. This results in silent loss of critical consensus state (last_vote, blocks, quorum certificates, timeout certificates) without triggering any recovery mechanism, causing validators to operate with stale consensus state and potentially leading to liveness failures.

## Finding Description

The vulnerability exists in the ConsensusDB write path and database initialization: [1](#0-0) 

ConsensusDB's `commit()` method uses `write_schemas_relaxed()` instead of `write_schemas()`, which means writes are not synchronized to disk: [2](#0-1) 

The comment explicitly warns: "If this flag is false, and the machine crashes, some recent writes may be lost." [3](#0-2) 

ConsensusDB stores critical consensus state including `last_vote`, `highest_2chain_timeout_certificate`, blocks, and quorum certificates: [4](#0-3) 

When ConsensusDB opens, it uses default RocksDB options which include the default WAL recovery mode (`kPointInTimeRecovery`): [5](#0-4) 

The critical issue is at line 68-69: if RocksDB fails to open, the node panics. However, with `kPointInTimeRecovery` mode, RocksDB will successfully open even with WAL corruption by truncating at the first corrupted record. This means the `.expect()` never triggers for WAL corruption.

**Attack/Failure Scenario:**

1. Validator votes on block B at round R, calls `save_vote()`
2. Data written to WAL buffer (in memory, not fsynced due to `write_schemas_relaxed`)
3. Machine crashes before OS flushes WAL to disk
4. WAL file has partial/corrupted write
5. On restart, RocksDB opens with `kPointInTimeRecovery` mode
6. RocksDB detects corrupted WAL entry, truncates at corruption point
7. RocksDB opens successfully (returns `Ok()`)
8. Recent consensus data after truncation point is lost
9. `ConsensusDB::new()` succeeds without panic
10. Recovery process reads stale data from ConsensusDB: [6](#0-5) 

11. `RecoveryData::new()` constructs recovery data with missing/stale information
12. Since DB opened successfully, `LivenessStorageData::FullRecoveryData` is returned (not `PartialRecoveryData`)
13. RecoveryManager is never started (only triggers on `PartialRecoveryData`)
14. Validator continues with stale consensus state

## Impact Explanation

This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria:

1. **"Validator node slowdowns"**: Lost blocks and quorum certificates cause validators to repeatedly request missing data from peers, degrading performance
2. **"Significant protocol violations"**: Silent data loss violates the consensus state consistency invariant - validators should either fully recover or enter explicit recovery mode
3. **Consensus liveness impact**: Missing timeout certificates can prevent validators from advancing rounds, blocking consensus progress

The impact is amplified when multiple validators experience simultaneous crashes (e.g., datacenter power outage), as each independently loses recent consensus state without coordination, potentially causing network-wide liveness degradation.

**Broken Invariants:**
- **Consensus Safety**: While not a direct safety violation, the lack of proper recovery creates conditions for inconsistent validator behavior
- **State Consistency**: Atomic state transitions are violated when WAL writes are lost without detection

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability does not require malicious intent - it occurs naturally through common failure scenarios:

1. **Power failures**: Datacenter or local power loss before OS buffer flush
2. **System crashes**: Kernel panics, hardware failures, OOM kills
3. **Disk corruption**: Bad sectors, storage controller failures
4. **Forced shutdowns**: Emergency system maintenance

The vulnerability triggers automatically whenever:
- ConsensusDB has pending writes in WAL buffer (common under load)
- System crashes before OS flush (typically ~30 seconds interval)
- RocksDB detects corrupted WAL on restart

With thousands of validator nodes operating 24/7, this scenario is statistically guaranteed to occur regularly across the network. The unsynchronized write pattern (`write_schemas_relaxed`) makes it far more likely than if proper fsync was used.

## Recommendation

**Fix 1: Use Synchronized Writes for Critical Consensus Data**

Change ConsensusDB to use synchronized writes for critical data:

```rust
// consensus/src/consensusdb/mod.rs
fn commit(&self, batch: SchemaBatch) -> Result<(), DbError> {
    self.db.write_schemas(batch)?;  // Use write_schemas instead of write_schemas_relaxed
    Ok(())
}
```

This ensures writes are fsynced before returning, preventing silent data loss on crash.

**Fix 2: Configure Explicit WAL Recovery Mode**

Add configuration to set RocksDB WAL recovery mode to `kAbsoluteConsistency`:

```rust
// consensus/src/consensusdb/mod.rs
pub fn new<P: AsRef<Path> + Clone>(db_root_path: P) -> Self {
    let column_families = vec![...];
    let path = db_root_path.as_ref().join(CONSENSUS_DB_NAME);
    let instant = Instant::now();
    let mut opts = Options::default();
    opts.create_if_missing(true);
    opts.create_missing_column_families(true);
    // Set WAL recovery mode to fail on corruption rather than silently truncate
    opts.set_wal_recovery_mode(rocksdb::DBRecoveryMode::AbsoluteConsistency);
    
    let db = DB::open(path.clone(), "consensus", column_families, &opts)
        .expect("ConsensusDB open failed; unable to continue");
    // ...
}
```

With `kAbsoluteConsistency`, RocksDB will fail to open if WAL is corrupted, triggering the panic and allowing proper recovery procedures (node restart, state sync from peers).

**Fix 3: Add WAL Corruption Detection**

Implement explicit error handling for database opening:

```rust
pub fn new<P: AsRef<Path> + Clone>(db_root_path: P) -> Self {
    // ... setup code ...
    
    let db = match DB::open(path.clone(), "consensus", column_families, &opts) {
        Ok(db) => db,
        Err(e) => {
            error!("ConsensusDB open failed: {:?}", e);
            // Delete corrupted DB and allow recovery from peers
            std::fs::remove_dir_all(&path).ok();
            panic!("ConsensusDB corrupted, removed for clean recovery");
        }
    };
    // ...
}
```

**Recommended Approach:** Implement both Fix 1 and Fix 2 to provide defense in depth. Synchronized writes prevent the issue, while `kAbsoluteConsistency` ensures any corruption is detected rather than silently ignored.

## Proof of Concept

```rust
#[test]
fn test_consensusdb_wal_corruption_silent_data_loss() {
    use tempfile::TempDir;
    use std::fs::OpenOptions;
    use std::io::Write;
    
    // Setup: Create ConsensusDB and write data
    let temp_dir = TempDir::new().unwrap();
    let db = ConsensusDB::new(temp_dir.path());
    
    // Save a vote (critical consensus state)
    let vote_data = vec![1, 2, 3, 4, 5];
    db.save_vote(vote_data.clone()).unwrap();
    
    // Save blocks and QCs
    let test_block = create_test_block(1, 100);
    let test_qc = create_test_qc(&test_block);
    db.save_blocks_and_quorum_certificates(vec![test_block.clone()], vec![test_qc.clone()]).unwrap();
    
    // Drop db to close cleanly
    drop(db);
    
    // Simulate WAL corruption by truncating the WAL file
    let wal_dir = temp_dir.path().join("consensus_db");
    let wal_files: Vec<_> = std::fs::read_dir(&wal_dir)
        .unwrap()
        .filter_map(|e| e.ok())
        .filter(|e| e.path().extension().and_then(|s| s.to_str()) == Some("log"))
        .collect();
    
    for wal_file in wal_files {
        let path = wal_file.path();
        let metadata = std::fs::metadata(&path).unwrap();
        if metadata.len() > 100 {
            // Truncate WAL to simulate corruption
            let mut file = OpenOptions::new().write(true).open(&path).unwrap();
            file.set_len(metadata.len() / 2).unwrap();
            // Write garbage to create corruption
            file.write_all(&[0xFF; 50]).unwrap();
        }
    }
    
    // Reopen database - should succeed with kPointInTimeRecovery (default)
    let db = ConsensusDB::new(temp_dir.path());
    
    // Verify data loss: vote and recent blocks are missing
    let recovered_data = db.get_data().unwrap();
    let (last_vote, _, blocks, qcs) = recovered_data;
    
    // This demonstrates the vulnerability:
    // - Database opened successfully despite WAL corruption
    // - Recent data is silently lost
    // - No error or recovery mechanism triggered
    assert!(last_vote.is_none() || blocks.is_empty() || qcs.is_empty(), 
        "Data loss occurred silently - validator will operate with stale state");
}
```

## Notes

This vulnerability is particularly insidious because:

1. **No error indication**: The system provides no warning that data was lost
2. **No recovery trigger**: RecoveryManager never starts because the DB opens "successfully"
3. **Distributed impact**: Multiple validators experiencing crashes compound the problem
4. **Production frequency**: Power outages and crashes are common in datacenter operations

The fix should prioritize data durability over performance for consensus-critical writes. While `write_schemas_relaxed` improves throughput by avoiding fsync, the cost of silent data loss far outweighs the performance benefit for validator consensus state.

### Citations

**File:** consensus/src/consensusdb/mod.rs (L51-78)
```rust
    pub fn new<P: AsRef<Path> + Clone>(db_root_path: P) -> Self {
        let column_families = vec![
            /* UNUSED CF = */ DEFAULT_COLUMN_FAMILY_NAME,
            BLOCK_CF_NAME,
            QC_CF_NAME,
            SINGLE_ENTRY_CF_NAME,
            NODE_CF_NAME,
            CERTIFIED_NODE_CF_NAME,
            DAG_VOTE_CF_NAME,
            "ordered_anchor_id", // deprecated CF
        ];

        let path = db_root_path.as_ref().join(CONSENSUS_DB_NAME);
        let instant = Instant::now();
        let mut opts = Options::default();
        opts.create_if_missing(true);
        opts.create_missing_column_families(true);
        let db = DB::open(path.clone(), "consensus", column_families, &opts)
            .expect("ConsensusDB open failed; unable to continue");

        info!(
            "Opened ConsensusDB at {:?} in {} ms",
            path,
            instant.elapsed().as_millis()
        );

        Self { db }
    }
```

**File:** consensus/src/consensusdb/mod.rs (L80-106)
```rust
    pub fn get_data(
        &self,
    ) -> Result<(
        Option<Vec<u8>>,
        Option<Vec<u8>>,
        Vec<Block>,
        Vec<QuorumCert>,
    )> {
        let last_vote = self.get_last_vote()?;
        let highest_2chain_timeout_certificate = self.get_highest_2chain_timeout_certificate()?;
        let consensus_blocks = self
            .get_all::<BlockSchema>()?
            .into_iter()
            .map(|(_, block)| block)
            .collect();
        let consensus_qcs = self
            .get_all::<QCSchema>()?
            .into_iter()
            .map(|(_, qc)| qc)
            .collect();
        Ok((
            last_vote,
            highest_2chain_timeout_certificate,
            consensus_blocks,
            consensus_qcs,
        ))
    }
```

**File:** consensus/src/consensusdb/mod.rs (L154-159)
```rust
    /// Write the whole schema batch including all data necessary to mutate the ledger
    /// state of some transaction by leveraging rocksdb atomicity support.
    fn commit(&self, batch: SchemaBatch) -> Result<(), DbError> {
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** storage/schemadb/src/lib.rs (L311-318)
```rust
    /// Writes without sync flag in write option.
    /// If this flag is false, and the machine crashes, some recent
    /// writes may be lost.  Note that if it is just the process that
    /// crashes (i.e., the machine does not reboot), no writes will be
    /// lost even if sync==false.
    pub fn write_schemas_relaxed(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &WriteOptions::default())
    }
```

**File:** storage/schemadb/src/lib.rs (L371-378)
```rust
/// For now we always use synchronous writes. This makes sure that once the operation returns
/// `Ok(())` the data is persisted even if the machine crashes. In the future we might consider
/// selectively turning this off for some non-critical writes to improve performance.
fn sync_write_option() -> rocksdb::WriteOptions {
    let mut opts = rocksdb::WriteOptions::default();
    opts.set_sync(true);
    opts
}
```

**File:** consensus/src/persistent_liveness_storage.rs (L519-596)
```rust
    fn start(&self, order_vote_enabled: bool, window_size: Option<u64>) -> LivenessStorageData {
        info!("Start consensus recovery.");
        let raw_data = self
            .db
            .get_data()
            .expect("unable to recover consensus data");

        let last_vote = raw_data
            .0
            .map(|bytes| bcs::from_bytes(&bytes[..]).expect("unable to deserialize last vote"));

        let highest_2chain_timeout_cert = raw_data.1.map(|b| {
            bcs::from_bytes(&b).expect("unable to deserialize highest 2-chain timeout cert")
        });
        let blocks = raw_data.2;
        let quorum_certs: Vec<_> = raw_data.3;
        let blocks_repr: Vec<String> = blocks.iter().map(|b| format!("\n\t{}", b)).collect();
        info!(
            "The following blocks were restored from ConsensusDB : {}",
            blocks_repr.concat()
        );
        let qc_repr: Vec<String> = quorum_certs
            .iter()
            .map(|qc| format!("\n\t{}", qc))
            .collect();
        info!(
            "The following quorum certs were restored from ConsensusDB: {}",
            qc_repr.concat()
        );
        // find the block corresponding to storage latest ledger info
        let latest_ledger_info = self
            .aptos_db
            .get_latest_ledger_info()
            .expect("Failed to get latest ledger info.");
        let accumulator_summary = self
            .aptos_db
            .get_accumulator_summary(latest_ledger_info.ledger_info().version())
            .expect("Failed to get accumulator summary.");
        let ledger_recovery_data = LedgerRecoveryData::new(latest_ledger_info);

        match RecoveryData::new(
            last_vote,
            ledger_recovery_data.clone(),
            blocks,
            accumulator_summary.into(),
            quorum_certs,
            highest_2chain_timeout_cert,
            order_vote_enabled,
            window_size,
        ) {
            Ok(mut initial_data) => {
                (self as &dyn PersistentLivenessStorage)
                    .prune_tree(initial_data.take_blocks_to_prune())
                    .expect("unable to prune dangling blocks during restart");
                if initial_data.last_vote.is_none() {
                    self.db
                        .delete_last_vote_msg()
                        .expect("unable to cleanup last vote");
                }
                if initial_data.highest_2chain_timeout_certificate.is_none() {
                    self.db
                        .delete_highest_2chain_timeout_certificate()
                        .expect("unable to cleanup highest 2-chain timeout cert");
                }
                info!(
                    "Starting up the consensus state machine with recovery data - [last_vote {}], [highest timeout certificate: {}]",
                    initial_data.last_vote.as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                    initial_data.highest_2chain_timeout_certificate().as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                );

                LivenessStorageData::FullRecoveryData(initial_data)
            },
            Err(e) => {
                error!(error = ?e, "Failed to construct recovery data");
                LivenessStorageData::PartialRecoveryData(ledger_recovery_data)
            },
        }
    }
```
