# Audit Report

## Title
Race Condition Between Transaction Validation and Reconfiguration Allows Stale Transactions in Mempool

## Summary
A critical race condition exists in the mempool coordinator where transactions can be validated against an old blockchain configuration (epoch N) but added to the mempool after a reconfiguration event has updated the validator to a new configuration (epoch N+1). This violates the invariant that all mempool transactions must be valid under the current configuration, leading to failed transactions, consensus inefficiency, and potential mempool pollution.

## Finding Description

The vulnerability occurs in the coordination between three components:

1. **Transaction validation path**: Network events trigger transaction processing that validates transactions in parallel using a thread pool
2. **Reconfiguration path**: Epoch changes trigger validator restarts with updated on-chain configurations
3. **Mempool insertion**: Validated transactions are added to mempool after validation completes

The race condition occurs because validation and reconfiguration are handled as independent asynchronous tasks without proper synchronization: [1](#0-0) 

Both `handle_network_event` (line 122) and `handle_mempool_reconfig_event` (line 116) spawn separate tasks via the bounded executor. These tasks can interleave in the following critical sequence:

**Step 1**: Transaction validation begins in `process_incoming_transactions`: [2](#0-1) 

Transactions are validated in parallel using the validator's current configuration, which reads features and gas parameters from the VM environment: [3](#0-2) 

**Step 2**: During or immediately after validation completes, a reconfiguration event arrives and `process_config_update` is called: [4](#0-3) 

This triggers a restart of all VM validators in the pool: [5](#0-4) 

The restart calls `reset_all` which creates a completely new environment with updated on-chain configurations: [6](#0-5) 

This new environment loads the latest features, gas parameters, and other configs from the updated blockchain state: [7](#0-6) 

**Step 3**: The validation task resumes and adds the already-validated transactions to mempool: [8](#0-7) 

**Result**: Transactions validated under epoch N configuration are now in the mempool under epoch N+1 configuration. These transactions may:
- Use features that are now disabled (e.g., `SINGLE_SENDER_AUTHENTICATOR`, `WEBAUTHN_SIGNATURE`)
- Have insufficient gas under the new gas schedule
- Violate new prologue checks or execution constraints
- Fail when proposed for execution in consensus

## Impact Explanation

This is a **High Severity** vulnerability according to Aptos bug bounty criteria:

1. **Consensus Inefficiency**: Invalid transactions are broadcast to other validators and may be proposed in blocks, wasting consensus resources when they fail execution.

2. **Failed Transactions**: Users' transactions that were valid when submitted will fail execution after the epoch change, causing poor user experience and potential loss of transaction fees.

3. **Mempool Pollution**: The mempool can accumulate many invalid transactions during each epoch transition, reducing effective capacity for valid transactions.

4. **Potential DoS Vector**: An attacker timing transaction submissions around known epoch changes could intentionally flood the mempool with transactions that will become invalid, causing sustained mempool pollution across multiple epochs.

5. **Deterministic Execution Violation**: Different validators may have different timing for when the race condition occurs, potentially leading to divergent mempool states and block proposal variations.

While this does not cause direct fund loss or consensus safety violations, it represents a significant protocol violation that affects network performance and reliability, especially during epoch transitions which occur regularly in Aptos.

## Likelihood Explanation

**Likelihood: High**

This vulnerability will trigger during every epoch change with high probability:
- Epoch changes (reconfigurations) are regular occurrences in Aptos (typically every 2 hours)
- Transaction validation and reconfiguration are processed concurrently via `futures::select!`
- The validation process involves parallel execution across multiple threads, increasing the window for the race
- No synchronization mechanism exists to prevent the race condition
- The window is particularly large because validation uses a thread pool (`VALIDATION_POOL`) and can take significant time for transaction batches

The race does not require attacker coordination - it occurs naturally during normal operation when:
1. A validator is processing incoming transactions from the network
2. An epoch change occurs (triggered by on-chain reconfiguration)
3. The timing causes validated transactions to be inserted after the validator restart

## Recommendation

Implement proper synchronization between reconfiguration and transaction validation:

**Option 1: Version-based validation (Recommended)**
- Add an epoch/version number to the validation result
- When inserting to mempool, verify the validation was performed under the current epoch
- Discard transactions validated under stale epochs

**Option 2: Lock-based synchronization**
- Use a single RwLock around both validation and reconfiguration
- Validation tasks hold a read lock during the entire process (validation + insertion)
- Reconfiguration tasks must acquire write lock, preventing concurrent validations

**Example fix for Option 1** (in `mempool/src/shared_mempool/tasks.rs`):

```rust
// After validation completes at line 503, before locking mempool:
let current_validator_epoch = smp.validator.read().get_current_epoch();
if validation_epoch != current_validator_epoch {
    // Validator was restarted, re-validate transactions
    continue; // or retry validation
}

// Then proceed with mempool insertion under the same validator lock
```

This ensures that transactions are only inserted if the validator configuration hasn't changed since validation.

## Proof of Concept

```rust
#[tokio::test]
async fn test_validation_reconfig_race() {
    use std::sync::{Arc, RwLock};
    use tokio::time::{sleep, Duration};
    
    // Simulate the mempool validator
    let validator = Arc::new(RwLock::new(MockValidator {
        epoch: 0,
        feature_enabled: true,
    }));
    
    let validator_clone = validator.clone();
    
    // Task 1: Validate transactions (simulates handle_network_event)
    let validation_task = tokio::spawn(async move {
        // Step 1: Start validation (acquires read lock)
        let feature_enabled = {
            let v = validator_clone.read().unwrap();
            v.feature_enabled
        }; // Read lock released
        
        // Step 2: Validation happens here (in parallel, takes time)
        sleep(Duration::from_millis(50)).await;
        
        // Step 3: Validation completes, check result
        assert!(feature_enabled, "Transaction validated with feature enabled");
        
        // Step 4: Small delay before mempool insertion
        sleep(Duration::from_millis(50)).await;
        
        // Step 5: Add to mempool (should check current config, but doesn't)
        let current_feature = {
            let v = validator_clone.read().unwrap();
            v.feature_enabled
        };
        
        (feature_enabled, current_feature)
    });
    
    // Task 2: Reconfiguration (simulates handle_mempool_reconfig_event)
    let reconfig_task = tokio::spawn(async move {
        // Wait for validation to start
        sleep(Duration::from_millis(25)).await;
        
        // Acquire write lock and update config (epoch change)
        let mut v = validator.write().unwrap();
        v.epoch = 1;
        v.feature_enabled = false; // Feature disabled in new epoch
    });
    
    // Wait for both tasks
    reconfig_task.await.unwrap();
    let (validated_with, inserted_under) = validation_task.await.unwrap();
    
    // VULNERABILITY: Transaction validated with feature=true
    // but inserted when feature=false
    assert!(validated_with == true);
    assert!(inserted_under == false);
    println!("RACE CONDITION DETECTED:");
    println!("  Validated with feature enabled: {}", validated_with);
    println!("  Mempool config has feature enabled: {}", inserted_under);
    println!("  Transaction will FAIL when executed!");
}

struct MockValidator {
    epoch: u64,
    feature_enabled: bool,
}
```

This test demonstrates that a transaction validated under one configuration can be added to mempool after the configuration changes, which is the core vulnerability.

## Notes

The race condition is exacerbated by the use of parallel validation pools and the asynchronous task spawning model. The `futures::select!` macro provides no ordering guarantees between branches, and the `BoundedExecutor` only limits concurrency without providing happens-before relationships between tasks.

The vulnerability is particularly severe because:
1. It affects every validator node during epoch transitions
2. The timing window is substantial (milliseconds to seconds depending on validation load)
3. It can cause cascading effects as invalid transactions are broadcast network-wide
4. The impact compounds when multiple validators experience the race simultaneously

### Citations

**File:** mempool/src/shared_mempool/coordinator.rs (L108-122)
```rust
        ::futures::select! {
            msg = client_events.select_next_some() => {
                handle_client_request(&mut smp, &bounded_executor, msg).await;
            },
            msg = quorum_store_requests.select_next_some() => {
                tasks::process_quorum_store_request(&smp, msg);
            },
            reconfig_notification = mempool_reconfig_events.select_next_some() => {
                handle_mempool_reconfig_event(&mut smp, &bounded_executor, reconfig_notification.on_chain_configs).await;
            },
            (peer, backoff) = scheduled_broadcasts.select_next_some() => {
                tasks::execute_broadcast(peer, backoff, &mut smp, &mut scheduled_broadcasts, executor.clone()).await;
            },
            (network_id, event) = events.select_next_some() => {
                handle_network_event(&bounded_executor, &mut smp, network_id, event).await;
```

**File:** mempool/src/shared_mempool/tasks.rs (L490-503)
```rust
    let validation_results = VALIDATION_POOL.install(|| {
        transactions
            .par_iter()
            .map(|t| {
                let result = smp.validator.read().validate_transaction(t.0.clone());
                // Pre-compute the hash and length if the transaction is valid, before locking mempool
                if result.is_ok() {
                    t.0.committed_hash();
                    t.0.txn_bytes_len();
                }
                result
            })
            .collect::<Vec<_>>()
    });
```

**File:** mempool/src/shared_mempool/tasks.rs (L506-523)
```rust
        let mut mempool = smp.mempool.lock();
        for (idx, (transaction, account_sequence_number, ready_time_at_sender, priority)) in
            transactions.into_iter().enumerate()
        {
            if let Ok(validation_result) = &validation_results[idx] {
                match validation_result.status() {
                    None => {
                        let ranking_score = validation_result.score();
                        let mempool_status = mempool.add_txn(
                            transaction.clone(),
                            ranking_score,
                            account_sequence_number,
                            timeline_state,
                            client_submitted,
                            ready_time_at_sender,
                            priority.clone(),
                        );
                        statuses.push((transaction, (mempool_status, None)));
```

**File:** mempool/src/shared_mempool/tasks.rs (L775-775)
```rust
    if let Err(e) = validator.write().restart() {
```

**File:** aptos-move/aptos-vm/src/aptos_vm.rs (L3172-3227)
```rust
        if !self
            .features()
            .is_enabled(FeatureFlag::SINGLE_SENDER_AUTHENTICATOR)
        {
            if let aptos_types::transaction::authenticator::TransactionAuthenticator::SingleSender{ .. } = transaction.authenticator_ref() {
                return VMValidatorResult::error(StatusCode::FEATURE_UNDER_GATING);
            }
        }

        if !self.features().is_enabled(FeatureFlag::WEBAUTHN_SIGNATURE) {
            if let Ok(sk_authenticators) = transaction
                .authenticator_ref()
                .to_single_key_authenticators()
            {
                for authenticator in sk_authenticators {
                    if let AnySignature::WebAuthn { .. } = authenticator.signature() {
                        return VMValidatorResult::error(StatusCode::FEATURE_UNDER_GATING);
                    }
                }
            } else {
                return VMValidatorResult::error(StatusCode::INVALID_SIGNATURE);
            }
        }

        if !self
            .features()
            .is_enabled(FeatureFlag::SLH_DSA_SHA2_128S_SIGNATURE)
        {
            if let Ok(sk_authenticators) = transaction
                .authenticator_ref()
                .to_single_key_authenticators()
            {
                for authenticator in sk_authenticators {
                    if let AnySignature::SlhDsa_Sha2_128s { .. } = authenticator.signature() {
                        return VMValidatorResult::error(StatusCode::FEATURE_UNDER_GATING);
                    }
                }
            } else {
                return VMValidatorResult::error(StatusCode::INVALID_SIGNATURE);
            }
        }

        if !self
            .features()
            .is_enabled(FeatureFlag::ALLOW_SERIALIZED_SCRIPT_ARGS)
        {
            if let Ok(TransactionExecutableRef::Script(script)) =
                transaction.payload().executable_ref()
            {
                for arg in script.args() {
                    if let TransactionArgument::Serialized(_) = arg {
                        return VMValidatorResult::error(StatusCode::FEATURE_UNDER_GATING);
                    }
                }
            }
        }
```

**File:** vm-validator/src/vm_validator.rs (L172-177)
```rust
    fn restart(&mut self) -> Result<()> {
        for vm_validator in &self.vm_validators {
            vm_validator.lock().unwrap().restart()?;
        }
        Ok(())
    }
```

**File:** aptos-move/aptos-resource-viewer/src/module_view.rs (L134-138)
```rust
    pub fn reset_all(&mut self, state_view: S) {
        self.state_view = state_view;
        self.environment = AptosEnvironment::new(&self.state_view);
        self.module_cache = UnsyncModuleCache::empty();
    }
```

**File:** aptos-move/aptos-vm-environment/src/environment.rs (L219-247)
```rust
        let features =
            fetch_config_and_update_hash::<Features>(&mut sha3_256, state_view).unwrap_or_default();

        // If no chain ID is in storage, we assume we are in a testing environment.
        let chain_id = fetch_config_and_update_hash::<ChainId>(&mut sha3_256, state_view)
            .unwrap_or_else(ChainId::test);
        let timestamp_micros =
            fetch_config_and_update_hash::<ConfigurationResource>(&mut sha3_256, state_view)
                .map(|config| config.last_reconfiguration_time_micros())
                .unwrap_or(0);

        let mut timed_features_builder = TimedFeaturesBuilder::new(chain_id, timestamp_micros);
        if let Some(profile) = get_timed_feature_override() {
            // We need to ensure the override is taken into account for the hash.
            let profile_bytes = bcs::to_bytes(&profile)
                .expect("Timed features override should always be serializable");
            sha3_256.update(&profile_bytes);

            timed_features_builder = timed_features_builder.with_override_profile(profile)
        }
        let timed_features = timed_features_builder.build();

        // TODO(Gas):
        //   Right now, we have to use some dummy values for gas parameters if they are not found
        //   on-chain. This only happens in a edge case that is probably related to write set
        //   transactions or genesis, which logically speaking, shouldn't be handled by the VM at
        //   all. We should clean up the logic here once we get that refactored.
        let (gas_params, storage_gas_params, gas_feature_version) =
            get_gas_parameters(&mut sha3_256, &features, state_view);
```
