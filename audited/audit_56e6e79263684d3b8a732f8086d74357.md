# Audit Report

## Title
TransactionPruner Catch-Up Logic Can Block Node Startup Causing Validator Downtime

## Summary
The `TransactionPruner::new()` initialization performs unbounded catch-up by loading all pending transactions into memory synchronously, potentially causing extended node startup delays or out-of-memory crashes when the pruner has fallen behind by millions of versions. This violates the availability invariant and can lead to validator downtime.

## Finding Description

During `TransactionPruner` initialization, the pruner performs a "catch-up" operation to synchronize its progress with the metadata progress. The catch-up logic loads **all transactions in the gap range into memory at once**, bypassing the batch_size configuration that limits memory usage during normal operation. [1](#0-0) 

The critical issue occurs in the `get_pruning_candidate_transactions()` method, which allocates a `Vec` with capacity equal to the full range and loads all transactions: [2](#0-1) 

The code comment claims "The capacity is capped by the max number of txns we prune in a single batch" but this is **false during initialization**. The catch-up call bypasses the `DBPruner::prune()` interface which respects batch_size limits: [3](#0-2) 

During normal operation, the `PrunerWorker` calls `DBPruner::prune(batch_size)` which processes data in batches. However, during initialization, `TransactionPruner::new()` directly calls the `DBSubPruner::prune()` method with the full range, which has no batch_size parameter: [4](#0-3) 

**Scenarios where large gaps occur:**

1. **Pruner disabled then re-enabled**: Operators may disable pruning for maintenance, debugging, or to save disk space temporarily. When re-enabled, the gap could be days or weeks worth of transactions.

2. **Database restoration from old backup**: Restoring from a backup taken days/weeks ago while metadata has progressed.

3. **First-time pruner enablement**: Enabling pruning on an existing node that never had it enabled.

4. **Pruner failure to persist progress**: If the pruner crashes or fails to write its progress correctly.

The default batch size is 5,000 versions: [5](#0-4) 

However, on Aptos mainnet running at ~5,000 TPS:
- 1 day without pruning = ~432 million versions
- 1 week = ~3 billion versions
- Even 1 hour = ~18 million versions

Loading millions of `Transaction` objects (which include signed transactions with payloads, signatures, etc.) into memory will:
1. Cause excessive memory allocation (potentially GBs)
2. Take extended time for disk I/O from RocksDB
3. Block node startup synchronously
4. Prevent validator from participating in consensus

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria:

**"Validator node slowdowns"** - A validator unable to start for extended periods (potentially hours if the gap is large enough) will:
- Miss consensus rounds and fail to propose/vote
- Lose staking rewards
- Potentially face penalties for downtime
- Reduce network decentralization if multiple validators are affected

The issue also approaches **Critical Severity** territory as:
- With extreme gaps (weeks of disabled pruning), OOM could cause complete node failure requiring manual intervention
- If widespread (e.g., common configuration issue), could affect network liveness
- Recovery requires either manual database intervention or waiting for the catch-up to complete

The impact is **deterministic** - it will happen every time a node restarts with a large pruning gap, not just occasionally.

## Likelihood Explanation

**High Likelihood** - This can occur through legitimate operational scenarios:

1. **Common operational practice**: Operators frequently disable pruning temporarily for various reasons (debugging, disk space management, testing). This is a documented configuration option.

2. **Natural occurrence**: If a pruner worker thread crashes or encounters errors, it may fall behind naturally.

3. **Configuration changes**: Changing pruning windows or enabling pruning for the first time.

4. **No warnings or safeguards**: The code provides no warnings about large gaps or time estimates for catch-up.

5. **Documented feature**: The pruner enable/disable toggle is an official configuration option with no documentation warning about catch-up delays.

The attack complexity is **zero** - it happens naturally through configuration changes or operational practices. No malicious intent is required, though a malicious validator operator could intentionally exploit this to sabotage their own node.

## Recommendation

**Implement batched catch-up during initialization** to respect the configured batch_size limit:

```rust
pub(in crate::pruner) fn new(
    transaction_store: Arc<TransactionStore>,
    ledger_db: Arc<LedgerDb>,
    metadata_progress: Version,
    internal_indexer_db: Option<InternalIndexerDB>,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        ledger_db.transaction_db_raw(),
        &DbMetadataKey::TransactionPrunerProgress,
        metadata_progress,
    )?;

    let myself = TransactionPruner {
        transaction_store,
        ledger_db,
        internal_indexer_db,
    };

    // Use a reasonable batch size for catch-up (e.g., 5000 from config)
    const CATCH_UP_BATCH_SIZE: u64 = 5_000;
    
    let mut current_progress = progress;
    info!(
        progress = progress,
        metadata_progress = metadata_progress,
        gap = metadata_progress - progress,
        "Catching up TransactionPruner in batches."
    );
    
    while current_progress < metadata_progress {
        let batch_end = std::cmp::min(
            current_progress + CATCH_UP_BATCH_SIZE,
            metadata_progress
        );
        
        info!(
            current = current_progress,
            batch_end = batch_end,
            "Pruning batch during catch-up."
        );
        
        myself.prune(current_progress, batch_end)?;
        current_progress = batch_end;
    }

    Ok(myself)
}
```

**Additional safeguards:**

1. Add a warning log if the gap exceeds a threshold (e.g., 1 million versions)
2. Add a configuration option to limit maximum catch-up range at startup
3. Update the misleading comment in `get_pruning_candidate_transactions()`
4. Consider making catch-up asynchronous to allow the node to start serving requests

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_temppath::TempPath;
    use aptos_types::transaction::Transaction;
    
    #[test]
    fn test_large_catch_up_gap_causes_memory_spike() {
        // Setup: Create a test database with 100K transactions
        let tmp_dir = TempPath::new();
        let db = setup_test_db(&tmp_dir); // Helper to create DB
        
        // Simulate scenario: Pruner was disabled, now catching up
        let large_gap = 100_000u64;
        
        // Write transactions to DB
        for version in 0..large_gap {
            let txn = Transaction::StateCheckpoint(HashValue::random());
            db.save_transaction(version, &txn, /* ... */).unwrap();
        }
        
        // Set pruner progress to 0 but metadata progress to large_gap
        db.write_pruner_progress(0).unwrap();
        let metadata_progress = large_gap;
        
        // Measure memory before initialization
        let mem_before = get_current_memory_usage();
        
        // This will attempt to load all 100K transactions into memory at once
        let start = std::time::Instant::now();
        let pruner = TransactionPruner::new(
            transaction_store,
            ledger_db,
            metadata_progress,
            None,
        );
        let duration = start.elapsed();
        
        let mem_after = get_current_memory_usage();
        let mem_increase = mem_after - mem_before;
        
        // Assert: Memory spike is proportional to gap size
        // With 100K transactions, expect significant memory increase
        assert!(mem_increase > 100_000_000); // > 100MB
        
        // Assert: Startup delay is significant
        assert!(duration.as_secs() > 5); // Takes > 5 seconds
        
        println!("Catch-up gap: {}", large_gap);
        println!("Memory increase: {} bytes", mem_increase);
        println!("Time taken: {:?}", duration);
    }
}
```

**To reproduce manually:**
1. Start an Aptos validator node with pruning enabled
2. Let it sync to current height
3. Stop the node and set `ledger_pruner_config.enable = false` in config
4. Restart and let it run for several days/weeks
5. Stop the node and set `ledger_pruner_config.enable = true`
6. Restart and observe extended startup time proportional to the gap

## Notes

This vulnerability affects **all ledger sub-pruners** that follow the same initialization pattern (EventStorePruner, WriteSetPruner, etc.). However, TransactionPruner is the most severe case because:

1. It explicitly loads all transactions into a Vec in memory
2. Transaction objects are among the largest storage objects
3. Other pruners use iterators or DB-level operations that don't require full materialization

The issue is particularly concerning for production validators where:
- Pruning might be disabled during incidents or maintenance
- Database restorations from backups are common disaster recovery procedures  
- Startup time directly impacts validator availability and rewards

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L78-104)
```rust
    pub(in crate::pruner) fn new(
        transaction_store: Arc<TransactionStore>,
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_db_raw(),
            &DbMetadataKey::TransactionPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionPruner {
            transaction_store,
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L106-131)
```rust
    fn get_pruning_candidate_transactions(
        &self,
        start: Version,
        end: Version,
    ) -> Result<Vec<(Version, Transaction)>> {
        ensure!(end >= start, "{} must be >= {}", end, start);

        let mut iter = self
            .ledger_db
            .transaction_db_raw()
            .iter::<TransactionSchema>()?;
        iter.seek(&start)?;

        // The capacity is capped by the max number of txns we prune in a single batch. It's a
        // relatively small number set in the config, so it won't cause high memory usage here.
        let mut txns = Vec::with_capacity((end - start) as usize);
        for item in iter {
            let (version, txn) = item?;
            if version >= end {
                break;
            }
            txns.push((version, txn));
        }

        Ok(txns)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/db_sub_pruner.rs (L6-14)
```rust
/// Defines the trait for sub-pruner of a parent DB pruner
pub trait DBSubPruner {
    /// Returns the name of the sub pruner.
    fn name(&self) -> &str;

    /// Performs the actual pruning, a target version is passed, which is the target the pruner
    /// tries to prune.
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()>;
}
```

**File:** config/src/config/storage_config.rs (L387-395)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
```
