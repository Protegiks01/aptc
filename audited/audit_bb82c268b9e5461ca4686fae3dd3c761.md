# Audit Report

## Title
Shutdown Panic Vulnerability on Failed gRPC Server Initialization in NetworkController

## Summary
The `shutdown()` method in `NetworkController` can panic when called after a failed `start()` attempt, causing validator crashes during recovery procedures. While double-shutdown and never-started scenarios are safe, a failed gRPC server binding (e.g., port already in use during recovery) causes the shutdown receiver to be dropped, leading to a panic when `shutdown()` is subsequently called.

## Finding Description

The vulnerability exists in the shutdown flow of `NetworkController` used by `ThreadExecutorService`: [1](#0-0) 

This delegates to `ExecutorService::shutdown()`: [2](#0-1) 

Which calls `NetworkController::shutdown()`: [3](#0-2) 

The critical issue is at line 158: `shutdown_signal.send(()).unwrap()` will panic if the oneshot receiver has been dropped.

The receiver can be dropped when the gRPC server fails to start: [4](#0-3) 

At line 86, if `serve_with_shutdown()` returns an error (e.g., address already in use), `.unwrap()` panics. This panic aborts the async task, dropping the `server_shutdown_rx` receiver passed at line 82.

**Attack Scenario During Recovery:**
1. Validator crashes and begins recovery procedure
2. Recovery code attempts to restart `ThreadExecutorService` by calling `new()` which calls `start()`
3. The gRPC server fails to bind (port still held by crashed process or another instance)
4. Line 86 in `grpc_network_service/mod.rs` panics, dropping the shutdown receiver
5. Recovery code attempts cleanup by calling `shutdown()`
6. Line 158 in `network_controller/mod.rs` panics because sending to a dropped receiver fails
7. Validator process crashes again, preventing recovery

**Note:** Direct double-shutdown and never-started scenarios are **safe** due to `Option::take()` handling, but the failed-start scenario is not protected.

## Impact Explanation

This is a **Medium Severity** issue under Aptos bug bounty criteria:
- **State inconsistencies requiring intervention**: Validators cannot recover automatically from restart failures
- **Validator availability impact**: Failed recovery attempts cascade into additional panics, requiring manual intervention
- Does not directly affect consensus safety or cause fund loss, but impacts validator liveness during operational failures

The vulnerability breaks the **availability invariant** - validators must be able to recover from transient failures. This creates a denial-of-service condition during recovery where the validator cannot restart without manual port cleanup.

## Likelihood Explanation

**High likelihood** during recovery scenarios:
- Port conflicts are common during rapid restart attempts after crashes
- Kubernetes/orchestration systems often restart crashed validators immediately
- TCP ports can remain in TIME_WAIT state after abnormal termination
- The affected code path executes on every executor service initialization

**Triggering conditions:**
- No attacker action required - normal operational failures trigger this
- Common during validator upgrades, restarts, or crash recovery
- More likely in environments with aggressive restart policies

## Recommendation

**Fix 1: Handle gRPC server binding errors gracefully**

In `secure/net/src/grpc_network_service/mod.rs`, replace the panic with error propagation:

```rust
// Change line 86 from:
.await.unwrap();

// To:
.await.unwrap_or_else(|e| {
    error!("Failed to start gRPC server at {:?}: {}", server_addr, e);
    // Server task exits cleanly, receiver drops but sender will handle it
});
```

**Fix 2: Make shutdown panic-free**

In `secure/net/src/network_controller/mod.rs`, use the same error handling pattern for both handlers:

```rust
// Change lines 157-159 from:
if let Some(shutdown_signal) = self.inbound_server_shutdown_tx.take() {
    shutdown_signal.send(()).unwrap();
}

// To:
if let Some(shutdown_signal) = self.inbound_server_shutdown_tx.take() {
    shutdown_signal.send(()).unwrap_or_else(|_| {
        warn!("Failed to send shutdown signal to inbound server; probably already shutdown");
    });
}
```

This mirrors the existing safe pattern used for outbound handler at lines 162-164.

## Proof of Concept

```rust
#[test]
fn test_shutdown_panic_on_failed_start() {
    use aptos_config::utils;
    use std::net::{IpAddr, Ipv4Addr, SocketAddr};
    use std::thread;
    
    // Bind a port to simulate it being in use
    let port = utils::get_available_port();
    let addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), port);
    let _listener = std::net::TcpListener::bind(addr).unwrap();
    
    // Try to create NetworkController on same port
    let mut controller = NetworkController::new(
        "test".to_string(),
        addr,
        5000,
    );
    
    // Create a dummy inbound channel to trigger server start
    let _rx = controller.create_inbound_channel("test".to_string());
    
    // Start will fail due to port conflict, panicking on line 86
    // This drops the shutdown receiver
    controller.start();
    
    // Give time for the async task to panic
    thread::sleep(std::time::Duration::from_millis(100));
    
    // This will panic on line 158 because receiver was dropped
    // EXPECTED: Should not panic, but currently does
    controller.shutdown();
}
```

**Expected behavior:** Graceful error handling without panics.  
**Actual behavior:** Double panic - first on failed bind, second on shutdown attempt.

## Notes

The specific scenarios mentioned in the security question (double-shutdown and never-started) are **already safe** due to proper `Option` handling with `.take()`. However, the related scenario of failed initialization followed by shutdown is vulnerable and highly relevant to validator recovery procedures as described in the question.

The inconsistency between the outbound handler (safe with `unwrap_or_else`) and inbound handler (unsafe with `unwrap`) suggests this may be an oversight rather than intentional design.

### Citations

**File:** execution/executor-service/src/thread_executor_service.rs (L38-40)
```rust
    pub fn shutdown(&mut self) {
        self.executor_service.shutdown()
    }
```

**File:** execution/executor-service/src/remote_executor_service.rs (L69-71)
```rust
    pub fn shutdown(&mut self) {
        self.controller.shutdown();
    }
```

**File:** secure/net/src/network_controller/mod.rs (L155-166)
```rust
    pub fn shutdown(&mut self) {
        info!("Shutting down network controller at {}", self.listen_addr);
        if let Some(shutdown_signal) = self.inbound_server_shutdown_tx.take() {
            shutdown_signal.send(()).unwrap();
        }

        if let Some(shutdown_signal) = self.outbound_task_shutdown_tx.take() {
            shutdown_signal.send(Message::new(vec![])).unwrap_or_else(|_| {
                warn!("Failed to send shutdown signal to outbound task; probably already shutdown");
            })
        }
    }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L75-88)
```rust
        Server::builder()
            .timeout(std::time::Duration::from_millis(rpc_timeout_ms))
            .add_service(
                NetworkMessageServiceServer::new(self).max_decoding_message_size(MAX_MESSAGE_SIZE),
            )
            .add_service(reflection_service)
            .serve_with_shutdown(server_addr, async {
                server_shutdown_rx.await.ok();
                info!("Received signal to shutdown server at {:?}", server_addr);
            })
            .await
            .unwrap();
        info!("Server shutdown at {:?}", server_addr);
    }
```
