# Audit Report

## Title
Pruning-Induced Loss of Historical Epoch Data Causes Unfair Leader Election in Consensus

## Summary
The `extract_epoch_proposers()` function in the consensus epoch manager silently falls back to using only the current epoch's validator set when historical epoch data retrieval fails due to database pruning. This configuration mismatch between the default pruning window (90M versions, ~2.5 epochs) and the requested historical epochs (5 epochs) causes the leader reputation system to lose all historical performance data, resulting in unfair leader election that defeats the purpose of reputation-based proposer selection. [1](#0-0) 

## Finding Description

The vulnerability exists in the interaction between database pruning configuration and consensus leader reputation history requirements:

**The Fallback Behavior:**

When `get_epoch_ending_ledger_infos()` fails, the error is logged but the system continues with a degraded fallback: [2](#0-1) 

This returns a HashMap containing only the current epoch's proposers, discarding all historical epoch data.

**Why the DB Query Fails:**

The LedgerPruner has a default pruning window of 90,000,000 versions: [3](#0-2) 

At approximately 5000 TPS with 2-hour epochs, this translates to ~36M versions per epoch, meaning only about 2.5 epochs of data are retained.

However, the leader reputation system requests 5 epochs of historical data by default: [4](#0-3) 

When the DB query attempts to fetch epoch ending ledger infos beyond the pruned range, it fails with a missing ledger info error: [5](#0-4) 

**Impact on Leader Reputation:**

The limited epoch map directly affects leader reputation calculation. The history filtering logic only considers epochs present in the map: [6](#0-5) 

When only the current epoch is present, and that epoch has just started with few committed blocks, the reputation heuristic assigns all validators the `inactive_weight` since they have no recent voting or proposal activity: [7](#0-6) 

This means:
1. Validators who performed poorly in previous epochs (failed proposals, no votes) are not penalized with `failed_weight` (1)
2. Validators who performed well are not rewarded with `active_weight` (100)
3. All validators receive equal `inactive_weight` (10), making leader election essentially random (weighted only by stake)

## Impact Explanation

**Medium Severity** - This meets the "State inconsistencies requiring intervention" category because:

1. **Fairness Violation**: The leader reputation system is specifically designed to improve network performance by selecting better-performing validators. This bug completely defeats that mechanism.

2. **Performance Degradation**: Poorly-performing validators get equal opportunities to become leaders, potentially causing more failed rounds and reduced consensus throughput.

3. **Reputation System Bypass**: Malicious or underperforming validators that should be penalized based on historical behavior are given a "clean slate" each time historical data is lost.

4. **Requires Intervention**: Once operators discover the issue, they must either:
   - Increase pruning window significantly (disk space impact)
   - Reduce `use_history_from_previous_epoch_max_count` (reduces effectiveness of reputation)
   - Implement better error handling (code change)

This does not reach High/Critical severity because it does not cause consensus safety violations, fund loss, or network partition. However, it materially undermines the fairness and efficiency guarantees of the consensus protocol.

## Likelihood Explanation

**HIGH Likelihood** - This occurs automatically in production under default configuration:

1. **Default Configuration Mismatch**: The default `prune_window` (90M versions â‰ˆ 2.5 epochs) is insufficient for the default `use_history_from_previous_epoch_max_count` (5 epochs).

2. **Time-Based Trigger**: Any node running with defaults for more than 2-3 epochs will experience this issue.

3. **No Special Conditions**: No attacker action or special network conditions required - this is a deterministic configuration issue.

4. **Silent Degradation**: The error is logged but operation continues with degraded behavior, so operators may not immediately notice the fairness issue.

## Recommendation

**Immediate Fix**: Ensure configuration consistency between pruning and reputation history requirements:

```rust
// In consensus/src/epoch_manager.rs, extract_epoch_proposers()
fn extract_epoch_proposers(
    &self,
    epoch_state: &EpochState,
    use_history_from_previous_epoch_max_count: u32,
    proposers: Vec<AccountAddress>,
    needed_rounds: u64,
) -> HashMap<u64, Vec<AccountAddress>> {
    let first_epoch_to_consider = std::cmp::max(
        if epoch_state.epoch == 1 { 1 } else { 2 },
        epoch_state
            .epoch
            .saturating_sub(use_history_from_previous_epoch_max_count as u64),
    );
    
    if epoch_state.epoch > first_epoch_to_consider {
        self.storage
            .aptos_db()
            .get_epoch_ending_ledger_infos(first_epoch_to_consider - 1, epoch_state.epoch)
            .map_err(Into::into)
            .and_then(|proof| {
                ensure!(
                    proof.ledger_info_with_sigs.len() as u64
                        == (epoch_state.epoch - (first_epoch_to_consider - 1))
                );
                extract_epoch_to_proposers(proof, epoch_state.epoch, &proposers, needed_rounds)
            })
            .unwrap_or_else(|err| {
                // CRITICAL: Historical data unavailable, reputation system degraded!
                // This indicates pruning configuration is insufficient.
                error!(
                    "CRITICAL: Cannot load {} epochs of history for leader reputation. \
                     Falling back to current epoch only. This defeats the reputation system! \
                     Error: {:?}. Consider increasing ledger_pruner.prune_window or reducing \
                     use_history_from_previous_epoch_max_count.",
                    use_history_from_previous_epoch_max_count,
                    err
                );
                
                // Emit metric for monitoring
                counters::LEADER_REPUTATION_HISTORY_UNAVAILABLE.inc();
                
                HashMap::from([(epoch_state.epoch, proposers)])
            })
    } else {
        HashMap::from([(epoch_state.epoch, proposers)])
    }
}
```

**Long-term Fixes**:

1. **Configuration Validation**: Add startup validation that `prune_window` (in versions) is sufficient for `use_history_from_previous_epoch_max_count` (in epochs), estimating epochs based on target TPS.

2. **Increase Default Pruning Window**: Update the default from 90M to at least 200M versions to support 5 epochs of history at typical throughput rates.

3. **Graceful Degradation**: Instead of all-or-nothing, attempt to fetch whatever historical epochs are available and use partial history rather than none.

## Proof of Concept

**Configuration Analysis PoC:**

```rust
// Demonstrate the mismatch in default configuration
fn test_pruning_vs_reputation_history_mismatch() {
    // Default pruning configuration
    let prune_window_versions: u64 = 90_000_000; // from LedgerPrunerConfig::default()
    
    // Typical Aptos throughput
    let tps: u64 = 5_000;
    let epoch_duration_hours: u64 = 2;
    let versions_per_epoch = tps * 60 * 60 * epoch_duration_hours;
    
    // Calculate retained epochs
    let epochs_retained = prune_window_versions / versions_per_epoch;
    println!("Epochs retained by pruner: {}", epochs_retained); // ~2.5
    
    // Leader reputation requirement
    let use_history_from_previous_epoch_max_count: u32 = 5; // from ConsensusConfigV1::default()
    
    println!("Epochs required for reputation: {}", use_history_from_previous_epoch_max_count); // 5
    
    // Verify mismatch
    assert!(
        (epochs_retained as u32) < use_history_from_previous_epoch_max_count,
        "Configuration mismatch: pruner retains {} epochs but reputation needs {} epochs",
        epochs_retained,
        use_history_from_previous_epoch_max_count
    );
}
```

**Runtime Observation:**

1. Deploy a validator node with default configuration
2. Wait for ~3 epochs to pass (allowing pruning to remove epoch 0)
3. Observe logs for: "Couldn't create leader reputation with history across epochs"
4. Query consensus metrics to verify all validators have equal reputation weights
5. Compare with expected behavior where historical performance should differentiate validator weights

## Notes

This vulnerability highlights a critical disconnect between storage layer configuration (pruning) and consensus layer requirements (reputation history). The issue is exacerbated by:

- Silent degradation that makes the problem difficult to detect in production
- The default configuration being insufficient for the intended feature behavior
- No validation at startup or runtime to detect this misconfiguration

The fix requires both immediate configuration adjustments and architectural improvements to prevent similar issues in the future.

### Citations

**File:** consensus/src/epoch_manager.rs (L409-449)
```rust
    fn extract_epoch_proposers(
        &self,
        epoch_state: &EpochState,
        use_history_from_previous_epoch_max_count: u32,
        proposers: Vec<AccountAddress>,
        needed_rounds: u64,
    ) -> HashMap<u64, Vec<AccountAddress>> {
        // Genesis is epoch=0
        // First block (after genesis) is epoch=1, and is the only block in that epoch.
        // It has no votes, so we skip it unless we are in epoch 1, as otherwise it will
        // skew leader elections for exclude_round number of rounds.
        let first_epoch_to_consider = std::cmp::max(
            if epoch_state.epoch == 1 { 1 } else { 2 },
            epoch_state
                .epoch
                .saturating_sub(use_history_from_previous_epoch_max_count as u64),
        );
        // If we are considering beyond the current epoch, we need to fetch validators for those epochs
        if epoch_state.epoch > first_epoch_to_consider {
            self.storage
                .aptos_db()
                .get_epoch_ending_ledger_infos(first_epoch_to_consider - 1, epoch_state.epoch)
                .map_err(Into::into)
                .and_then(|proof| {
                    ensure!(
                        proof.ledger_info_with_sigs.len() as u64
                            == (epoch_state.epoch - (first_epoch_to_consider - 1))
                    );
                    extract_epoch_to_proposers(proof, epoch_state.epoch, &proposers, needed_rounds)
                })
                .unwrap_or_else(|err| {
                    error!(
                        "Couldn't create leader reputation with history across epochs, {:?}",
                        err
                    );
                    HashMap::from([(epoch_state.epoch, proposers)])
                })
        } else {
            HashMap::from([(epoch_state.epoch, proposers)])
        }
    }
```

**File:** config/src/config/storage_config.rs (L387-395)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L501-501)
```rust
                    use_history_from_previous_epoch_max_count: 5,
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L1056-1062)
```rust
        ensure!(
            lis.len() == (paging_epoch - start_epoch) as usize,
            "DB corruption: missing epoch ending ledger info for epoch {}",
            lis.last()
                .map(|li| li.ledger_info().next_block_epoch() - 1)
                .unwrap_or(start_epoch),
        );
```

**File:** consensus/src/liveness/leader_reputation.rs (L323-326)
```rust
        sub_history
            .iter()
            .filter(move |&meta| epoch_to_candidates.contains_key(&meta.epoch()))
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L541-549)
```rust
                if cur_failed_proposals * 100
                    > (cur_proposals + cur_failed_proposals) * self.failure_threshold_percent
                {
                    self.failed_weight
                } else if cur_proposals > 0 || cur_votes > 0 {
                    self.active_weight
                } else {
                    self.inactive_weight
                }
```
