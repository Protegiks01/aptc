# Audit Report

## Title
Blocking Thread Pool Exhaustion in API Event Queries Causes Service Degradation

## Summary
The `get_events_by_event_handle()` API endpoint uses a shared blocking thread pool limited to 64 threads. Multiple concurrent requests can exhaust this pool, causing subsequent requests to queue indefinitely and resulting in API unavailability or severe performance degradation.

## Finding Description

The Aptos REST API uses `api_spawn_blocking()` to offload synchronous database operations to a dedicated blocking thread pool. This pool is configured with a hard limit of 64 threads. [1](#0-0) [2](#0-1) 

The `get_events_by_event_handle()` endpoint spawns a blocking task that performs multiple synchronous database operations: [3](#0-2) 

Inside this blocking task, the code:
1. Creates an `Account` object (queries database for ledger info and account state)
2. Calls `find_event_key()` (queries database for resource data and deserializes)
3. Calls `list()` which invokes `get_events()` (queries database for up to 100 events) [4](#0-3) 

The `api_spawn_blocking()` function is a simple wrapper with no timeout mechanism: [5](#0-4) 

**Attack Scenario:**
1. Attacker sends 100+ concurrent requests to `/accounts/:address/events/:event_handle/:field_name`
2. The first 64 requests acquire blocking threads and begin database queries
3. Remaining requests wait in Tokio's internal queue for available threads
4. If database queries are slow (large result sets, disk I/O latency, etc.), threads remain occupied for extended periods
5. Legitimate user requests also queue, causing API slowdown or timeouts
6. API becomes effectively unavailable under sustained concurrent load

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The API lacks proper concurrency control and can be exhausted through resource starvation.

## Impact Explanation

This vulnerability enables application-level resource exhaustion leading to API unavailability. According to Aptos bug bounty criteria:

- **High Severity**: "API crashes" - While not a complete crash, sustained thread pool exhaustion causes the API to become unresponsive, effectively equivalent to a crash from a user perspective
- **Medium Severity**: "State inconsistencies requiring intervention" - API unavailability may require manual intervention to restore service

This issue is classified as **Medium** severity because:
- It affects service availability but not consensus or fund security
- It requires sustained concurrent requests (64+) to trigger
- Impact is limited to the REST API layer, not core blockchain functionality
- Recovery is automatic once attack traffic ceases

The vulnerability differs from network-level DoS attacks (which are out of scope) because it exploits application-level resource management rather than network bandwidth or connection limits.

## Likelihood Explanation

**Likelihood: High**

This vulnerability is highly likely to be exploited because:

1. **No authentication required**: Public API endpoints are accessible to anyone
2. **Simple attack vector**: Basic HTTP client can send concurrent requests
3. **Low resource requirements**: Attacker needs ~100 concurrent connections (easily achieved with basic scripting)
4. **No rate limiting at application layer**: While HAProxy may limit to 500 connections, the 64-thread bottleneck occurs well before that limit
5. **Deterministic behavior**: Thread pool exhaustion is guaranteed once 64+ concurrent blocking operations are active

The attack is easier if database queries are naturally slow (large event logs, disk contention, etc.), but even moderately fast queries (100-500ms) can cause issues under concurrent load.

## Recommendation

**Short-term mitigations:**

1. **Implement per-endpoint concurrency limits** using semaphores:
```rust
// In Context struct
pub struct Context {
    // ... existing fields ...
    events_query_semaphore: Arc<Semaphore>,
}

// Initialize with reasonable limit (e.g., 32 concurrent event queries)
events_query_semaphore: Arc::new(Semaphore::new(32))

// In get_events_by_event_handle()
async fn get_events_by_event_handle(...) -> BasicResultWith404<Vec<VersionedEvent>> {
    let _permit = self.context.events_query_semaphore
        .acquire()
        .await
        .map_err(|_| BasicErrorWith404::internal_with_code(
            "Service temporarily unavailable",
            AptosErrorCode::InternalError,
            &ledger_info,
        ))?;
    
    let api = self.clone();
    api_spawn_blocking(move || {
        // ... existing logic ...
    }).await
}
```

2. **Add timeout to blocking operations**: [6](#0-5) 

Wrap `api_spawn_blocking` with timeout:
```rust
pub async fn api_spawn_blocking_with_timeout<F, T, E>(
    func: F,
    timeout: Duration,
) -> Result<T, E>
where
    F: FnOnce() -> Result<T, E> + Send + 'static,
    T: Send + 'static,
    E: InternalError + Send + 'static,
{
    tokio::time::timeout(timeout, tokio::task::spawn_blocking(func))
        .await
        .map_err(|_| E::internal_with_code_no_info(
            "Request timeout",
            AptosErrorCode::InternalError
        ))?
        .map_err(|err| E::internal_with_code_no_info(err, AptosErrorCode::InternalError))?
}
```

**Long-term solution:**

Convert blocking database operations to async operations using an async database driver, eliminating the need for the blocking thread pool entirely.

## Proof of Concept

```rust
// Add to api/src/tests/ or run as standalone Rust test
#[tokio::test]
async fn test_thread_pool_exhaustion() {
    use std::sync::Arc;
    use std::time::Duration;
    use tokio::time::Instant;
    
    // Simulate the API's blocking thread pool limit
    const MAX_BLOCKING_THREADS: usize = 64;
    const CONCURRENT_REQUESTS: usize = 100;
    
    // Create a test that spawns blocking tasks
    let start = Instant::now();
    let mut handles = vec![];
    
    for i in 0..CONCURRENT_REQUESTS {
        let handle = tokio::spawn(async move {
            let request_start = Instant::now();
            
            // Simulate api_spawn_blocking with a slow database query
            tokio::task::spawn_blocking(move || {
                // Simulate slow database operation (e.g., reading 100 events)
                std::thread::sleep(Duration::from_millis(500));
                i
            })
            .await
            .unwrap();
            
            request_start.elapsed()
        });
        handles.push(handle);
    }
    
    // Wait for all requests to complete
    let mut durations = vec![];
    for handle in handles {
        durations.push(handle.await.unwrap());
    }
    
    let total_duration = start.elapsed();
    
    // Analysis:
    // - First 64 requests should complete in ~500ms
    // - Remaining 36 requests must wait for threads, taking 1000ms+
    durations.sort();
    
    println!("Total duration: {:?}", total_duration);
    println!("Median request duration: {:?}", durations[CONCURRENT_REQUESTS / 2]);
    println!("P95 request duration: {:?}", durations[(CONCURRENT_REQUESTS * 95) / 100]);
    println!("Max request duration: {:?}", durations[CONCURRENT_REQUESTS - 1]);
    
    // Verify thread pool exhaustion occurred
    // Requests beyond the 64th should take significantly longer
    assert!(durations[80].as_millis() > 800, 
        "Request #80 should be delayed by thread pool exhaustion");
    assert!(durations[95].as_millis() > 900,
        "Request #95 should be significantly delayed");
}
```

**To demonstrate on live API:**
```bash
# Send 100 concurrent requests to events endpoint
seq 1 100 | xargs -P 100 -I {} curl -s -w "%{time_total}\n" \
  "http://localhost:8080/v1/accounts/0x1/events/0x1::account::Account/coin_register_events?limit=100" \
  -o /dev/null
```

Expected result: Later requests take significantly longer than early requests, demonstrating thread pool contention.

## Notes

This vulnerability is distinct from network-level DoS (which is out of scope) because it exploits application-layer resource management in the Tokio blocking thread pool. The issue exists in production code and affects API availability under concurrent load. While HAProxy connection limits (500 max) provide some protection, the 64-thread bottleneck is reached well before connection limits, making this a practical attack vector for API degradation.

### Citations

**File:** crates/aptos-runtimes/src/lib.rs (L27-27)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;
```

**File:** crates/aptos-runtimes/src/lib.rs (L48-50)
```rust
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** api/src/events.rs (L143-149)
```rust
        let api = self.clone();
        api_spawn_blocking(move || {
            let account = Account::new(api.context.clone(), address.0, None, None, None)?;
            let key = account.find_event_key(event_handle.0, field_name.0.into())?;
            api.list(account.latest_ledger_info, accept_type, page, key)
        })
        .await
```

**File:** api/src/events.rs (L163-170)
```rust
        let events = self
            .context
            .get_events(
                &event_key,
                page.start_option(),
                page.limit(&latest_ledger_info)?,
                ledger_version,
            )
```

**File:** api/src/context.rs (L1643-1654)
```rust
/// This function just calls tokio::task::spawn_blocking with the given closure and in
/// the case of an error when joining the task converts it into a 500.
pub async fn api_spawn_blocking<F, T, E>(func: F) -> Result<T, E>
where
    F: FnOnce() -> Result<T, E> + Send + 'static,
    T: Send + 'static,
    E: InternalError + Send + 'static,
{
    tokio::task::spawn_blocking(func)
        .await
        .map_err(|err| E::internal_with_code_no_info(err, AptosErrorCode::InternalError))?
}
```
