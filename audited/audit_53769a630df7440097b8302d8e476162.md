# Audit Report

## Title
Unbounded Disk Space Consumption via Malicious Backup Service Proof Responses

## Summary
The backup client's `write_chunk()` function uses `tokio::io::copy()` to write transaction range proofs and state range proofs directly to disk without any size validation. A malicious or compromised backup service can send arbitrarily large responses that will be written to disk before any validation occurs, potentially exhausting disk space and causing backup failures or node operational issues.

## Finding Description

The vulnerability exists in two backup operations that retrieve cryptographic proofs from backup services:

**1. Transaction Backup Proofs:**

The `write_chunk()` function in transaction backup unconditionally copies all data from the backup service's transaction range proof endpoint to disk: [1](#0-0) 

**2. State Snapshot Backup Proofs:**

Similarly, the state snapshot backup copies account range proofs without size validation: [2](#0-1) 

The backup service client implements no response size limits, only timeout constraints: [3](#0-2) 

**Legitimate Proof Sizes:**

Transaction accumulator range proofs have bounded size due to maximum tree depth constraints: [4](#0-3) 

With `MAX_ACCUMULATOR_PROOF_DEPTH` set to 63: [5](#0-4) 

A legitimate proof should contain at most 126 hash values (63 left siblings + 63 right siblings = ~4KB) plus a `LedgerInfoWithSignatures` (~few KB), totaling approximately 10KB maximum.

**Validation Occurs After Disk Write:**

Critically, proof validation only happens during the restore phase, after files have already been written to disk: [6](#0-5) 

**Attack Sequence:**

1. Attacker controls or compromises a backup service endpoint
2. Backup client initiates backup operation, requesting transaction/state range proofs
3. Malicious service responds with gigabytes of arbitrary data claiming to be BCS-encoded proofs
4. Client's `tokio::io::copy()` writes all received data directly to disk without size checks
5. For a typical backup with hundreds of chunks, this process repeats, rapidly consuming disk space
6. Even if later BCS deserialization or proof verification fails, disk space has already been consumed
7. Disk exhaustion causes backup failures, potential node crashes, or operational disruptions

**Invariant Violation:**

This breaks the documented invariant: "Resource Limits: All operations must respect gas, storage, and computational limits." The backup operation fails to enforce storage limits on external input.

## Impact Explanation

**Severity: Medium** per Aptos Bug Bounty criteria:

This vulnerability enables resource exhaustion attacks against nodes performing backup operations:

- **Disk Space Exhaustion**: Attackers can cause rapid disk space consumption (limited by network bandwidth and 60-second read timeouts, potentially hundreds of MB to several GB per backup operation)
- **Backup Failures**: Exhausted disk space prevents successful backup completion, compromising disaster recovery capabilities
- **Node Operational Issues**: If the backup target is the node's main disk partition, filling disk space can impact node operations, log writing, and database operations
- **Amplification Effect**: A single backup operation typically involves many chunks (hundreds to thousands), amplifying the attack impact

While this doesn't directly cause consensus violations or fund loss, it constitutes a **state inconsistency requiring intervention** (Medium severity category) and could contribute to **validator node slowdowns** if disk I/O is saturated (High severity characteristics).

The attack has limited blast radius (only affects nodes performing backups from malicious services) but can significantly impact node availability and recovery capabilities.

## Likelihood Explanation

**Likelihood: Medium**

**Required Attacker Capabilities:**
- Control over a backup service endpoint (either by running a malicious service or compromising an existing one)
- Network access to target nodes' backup clients
- No validator privileges or consensus participation required

**Attack Feasibility:**
- Backup services are often external infrastructure (cloud storage, dedicated backup servers)
- Operators may configure backup clients to use third-party or self-hosted services
- Compromise of backup infrastructure is a realistic threat vector
- Attack execution is straightforward (respond with oversized data to backup requests)

**Limitations:**
- Only affects nodes actively performing backups
- 60-second read timeout limits single-request attack size
- Requires victim to configure backup service pointing to attacker-controlled endpoint
- Detection possible through monitoring disk usage and backup metrics

The attack is realistic for scenarios where operators use external or compromised backup services, making this a medium-likelihood threat.

## Recommendation

Implement strict size validation before writing proof data to disk:

```rust
// Define maximum expected proof sizes
const MAX_TRANSACTION_RANGE_PROOF_SIZE: usize = 50_000; // ~50KB safety margin
const MAX_ACCOUNT_RANGE_PROOF_SIZE: usize = 100_000; // ~100KB safety margin

async fn write_chunk(
    &self,
    backup_handle: &BackupHandleRef,
    chunk_bytes: &[u8],
    first_version: u64,
    last_version: u64,
) -> Result<TransactionChunk> {
    let (proof_handle, mut proof_file) = self
        .storage
        .create_for_write(
            backup_handle,
            &Self::chunk_proof_name(first_version, last_version),
        )
        .await?;
    
    // Wrap reader with size limit
    let mut proof_reader = self
        .client
        .get_transaction_range_proof(first_version, last_version)
        .await?;
    
    let mut limited_reader = proof_reader.take(MAX_TRANSACTION_RANGE_PROOF_SIZE as u64);
    let bytes_copied = tokio::io::copy(&mut limited_reader, &mut proof_file).await?;
    
    ensure!(
        bytes_copied < MAX_TRANSACTION_RANGE_PROOF_SIZE as u64,
        "Proof data exceeds maximum expected size ({} bytes). Possible malicious backup service.",
        bytes_copied
    );
    
    proof_file.shutdown().await?;
    // ... rest of function
}
```

Apply similar fixes to state snapshot backup: [2](#0-1) 

**Additional Hardening:**
1. Add configuration option for maximum proof size with sensible defaults
2. Implement monitoring/alerting for unusually large backup responses
3. Log warnings when proof sizes exceed expected bounds
4. Consider implementing progressive rate limiting for repeated oversized responses

## Proof of Concept

```rust
// PoC: Malicious Backup Service Mock
// File: storage/backup/backup-cli/src/backup_types/transaction/tests_poc.rs

#[cfg(test)]
mod resource_exhaustion_poc {
    use super::*;
    use std::io::Cursor;
    use tokio::io::AsyncRead;
    
    /// Mock malicious backup service that returns gigabytes of fake proof data
    struct MaliciousBackupService;
    
    impl MaliciousBackupService {
        async fn get_transaction_range_proof_malicious(
            &self,
            _first: u64,
            _last: u64,
        ) -> impl AsyncRead {
            // Return a reader that yields 1GB of fake data
            // In real attack, this would be sent over HTTP
            let fake_proof_data = vec![0u8; 1_000_000_000]; // 1GB
            Cursor::new(fake_proof_data)
        }
    }
    
    #[tokio::test]
    async fn test_disk_exhaustion_via_oversized_proof() {
        // Setup: Create backup controller with malicious service
        let malicious_service = MaliciousBackupService;
        
        // Simulate receiving oversized proof
        let mut fake_reader = malicious_service
            .get_transaction_range_proof_malicious(0, 1000)
            .await;
        
        let temp_dir = tempfile::tempdir().unwrap();
        let proof_file_path = temp_dir.path().join("malicious_proof.bin");
        let mut proof_file = tokio::fs::File::create(&proof_file_path).await.unwrap();
        
        // This will write 1GB to disk without validation
        let bytes_written = tokio::io::copy(&mut fake_reader, &mut proof_file).await.unwrap();
        
        // Verify attack success
        assert_eq!(bytes_written, 1_000_000_000);
        
        let file_size = tokio::fs::metadata(&proof_file_path).await.unwrap().len();
        assert_eq!(file_size, 1_000_000_000);
        
        println!("PoC: Successfully wrote {} bytes to disk without validation", bytes_written);
        println!("Expected legitimate size: ~10KB");
        println!("Attack amplification: {}x", bytes_written / 10_000);
    }
    
    #[tokio::test] 
    async fn test_multiple_chunk_amplification() {
        // Demonstrate amplification across multiple chunks
        let chunks_in_typical_backup = 100; // Conservative estimate
        let malicious_size_per_chunk = 10_000_000; // 10MB per chunk (achievable within timeout)
        
        let total_disk_consumed = chunks_in_typical_backup * malicious_size_per_chunk;
        
        println!("Attack amplification scenario:");
        println!("  Chunks in backup: {}", chunks_in_typical_backup);
        println!("  Malicious size per chunk: {} bytes", malicious_size_per_chunk);
        println!("  Total disk consumed: {} GB", total_disk_consumed / 1_000_000_000);
        println!("  Legitimate total size: ~1 MB");
        
        assert!(total_disk_consumed > 1_000_000_000); // Over 1GB total
    }
}
```

**Running the PoC:**
```bash
cd storage/backup/backup-cli
cargo test resource_exhaustion_poc -- --nocapture
```

The PoC demonstrates that without size validation, a malicious backup service can cause arbitrary disk space consumption, with the attack amplified across multiple chunks in a typical backup operation.

## Notes

This vulnerability is specific to backup operations and does not directly affect consensus or transaction processing. However, it poses a significant operational risk for nodes that rely on backups for disaster recovery. The issue affects both transaction backups and state snapshot backups, with similar vulnerable code patterns in both implementations.

The 60-second read timeout provides some natural rate limiting but does not prevent the attack, as modern networks can transfer hundreds of megabytes within this window, and the attack amplifies across multiple chunks.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/backup.rs (L163-171)
```rust
        tokio::io::copy(
            &mut self
                .client
                .get_transaction_range_proof(first_version, last_version)
                .await?,
            &mut proof_file,
        )
        .await?;
        proof_file.shutdown().await?;
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L429-435)
```rust
        tokio::io::copy(
            &mut self
                .client
                .get_account_range_proof(last_key, self.version())
                .await?,
            &mut proof_file,
        )
```

**File:** storage/backup/backup-cli/src/utils/backup_service_client.rs (L55-84)
```rust
    async fn get(&self, endpoint: &'static str, params: &str) -> Result<impl AsyncRead + use<>> {
        let _timer = BACKUP_TIMER.timer_with(&[&format!("backup_service_client_get_{endpoint}")]);

        let url = if params.is_empty() {
            format!("{}/{}", self.address, endpoint)
        } else {
            format!("{}/{}/{}", self.address, endpoint, params)
        };
        let timeout = Duration::from_secs(Self::TIMEOUT_SECS);
        let reader = tokio::time::timeout(timeout, self.client.get(&url).send())
            .await?
            .err_notes(&url)?
            .error_for_status()
            .err_notes(&url)?
            .bytes_stream()
            .map_ok(|bytes| {
                THROUGHPUT_COUNTER.inc_with_by(&[endpoint], bytes.len() as u64);
                bytes
            })
            .map_err(futures::io::Error::other)
            .into_async_read()
            .compat();

        // Adding the timeout here instead of on the response because we do use long living
        // connections. For example, we stream the entire state snapshot in one request.
        let mut reader_with_read_timeout = TimeoutReader::new(reader);
        reader_with_read_timeout.set_timeout(Some(timeout));

        Ok(Box::pin(reader_with_read_timeout))
    }
```

**File:** types/src/proof/definition.rs (L46-47)
```rust
pub const MAX_ACCUMULATOR_PROOF_DEPTH: usize = 63;
pub const MAX_ACCUMULATOR_LEAVES: LeafCount = 1 << MAX_ACCUMULATOR_PROOF_DEPTH;
```

**File:** types/src/proof/definition.rs (L637-647)
```rust
            self.left_siblings.len() <= MAX_ACCUMULATOR_PROOF_DEPTH,
            "Proof has more than {} ({}) left siblings.",
            MAX_ACCUMULATOR_PROOF_DEPTH,
            self.left_siblings.len(),
        );
        ensure!(
            self.right_siblings.len() <= MAX_ACCUMULATOR_PROOF_DEPTH,
            "Proof has more than {} ({}) right siblings.",
            MAX_ACCUMULATOR_PROOF_DEPTH,
            self.right_siblings.len(),
        );
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L147-167)
```rust
        let (range_proof, ledger_info) = storage
            .load_bcs_file::<(TransactionAccumulatorRangeProof, LedgerInfoWithSignatures)>(
                &manifest.proof,
            )
            .await?;
        if let Some(epoch_history) = epoch_history {
            epoch_history.verify_ledger_info(&ledger_info)?;
        }

        // make a `TransactionListWithProof` to reuse its verification code.
        let txn_list_with_proof =
            TransactionListWithProofV2::new(TransactionListWithAuxiliaryInfos::new(
                TransactionListWithProof::new(
                    txns,
                    Some(event_vecs),
                    Some(manifest.first_version),
                    TransactionInfoListWithProof::new(range_proof, txn_infos),
                ),
                persisted_aux_info,
            ));
        txn_list_with_proof.verify(ledger_info.ledger_info(), Some(manifest.first_version))?;
```
