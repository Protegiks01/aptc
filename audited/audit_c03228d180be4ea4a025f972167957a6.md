Based on my deep technical validation of the codebase, this is a **valid vulnerability**. I have traced the complete execution path and verified all claims with code evidence.

# Audit Report

## Title
Validator Transactions Become Invalid During Same-Block Epoch Transitions

## Summary
During epoch transitions, validator transactions (specifically DKGResult) pulled from the pool can become invalid by the time they execute because BlockMetadata processes first and may increment the epoch, causing deterministic transaction discard failures.

## Finding Description

The vulnerability exists in the interaction between three components verified in the codebase:

**1. Transaction Pulling Without Epoch Transition Check**

The proposal generator pulls validator transactions without checking if the block's timestamp will trigger an epoch transition: [1](#0-0) [2](#0-1) 

**2. Execution Order Places BlockMetadata First**

The execution order is strictly defined with BlockMetadata first, then validator transactions: [3](#0-2) 

**3. BlockMetadata Triggers Epoch Transition**

When the block timestamp satisfies the epoch interval condition, `block_prologue` calls `reconfiguration::reconfigure()`: [4](#0-3) 

This increments the epoch before validator transactions execute: [5](#0-4) 

**4. DKGResult Epoch Validation Fails**

DKGResult transactions validate that their embedded epoch matches the current on-chain epoch: [6](#0-5) 

When validation fails, the transaction is discarded: [7](#0-6) 

**5. Reconfiguration Suffix Rule Doesn't Prevent This**

The reconfiguration suffix rule only prevents blocks AFTER reconfiguration from carrying transactions, not the block that triggers it: [8](#0-7) 

This checks if the PARENT has reconfiguration, not if the CURRENT block will trigger one.

**Attack Path:**
1. DKG manager creates ValidatorTransaction::DKGResult with epoch N and places it in the pool
2. Proposal generator pulls this transaction when creating a block with timestamp T satisfying: `T - last_reconfiguration_time >= epoch_interval`
3. During block execution, BlockMetadata processes first and triggers reconfiguration, incrementing epoch to N+1
4. DKGResult with epoch N attempts to execute but fails epoch validation
5. Transaction is deterministically discarded with `TransactionStatus::Discard(StatusCode::ABORTED)`

## Impact Explanation

**Severity: Medium** (Limited Protocol Violation)

While the report claims "High," this more accurately fits the **Medium** severity category per Aptos bug bounty criteria: "Limited Protocol Violations - State inconsistencies requiring manual intervention, Limited funds loss or manipulation, Temporary liveness issues."

Impact:
1. **DKG Process Disruption**: DKGResult transactions are discarded, potentially delaying distributed key generation and affecting randomness availability
2. **Epoch Transition Reliability**: Critical validator transactions fail during epoch transitions, creating operational issues
3. **Resource Waste**: Consensus bandwidth and execution resources wasted on deterministically discarded transactions
4. **Timing-Dependent Behavior**: Bug manifests only when block timestamps cross epoch boundaries

**Why not High/Critical:** 
- No consensus split occurs (all validators deterministically agree on discard)
- No funds loss or theft
- No permanent network partition or liveness failure
- This is a protocol correctness issue affecting reliability, not a safety violation

## Likelihood Explanation

**Likelihood: Medium-High**

This issue occurs when:
- A validator transaction exists in the pool for epoch N
- A block is proposed with timestamp triggering epoch transition (`timestamp - last_reconfiguration_time >= epoch_interval`)
- The system uses immediate reconfiguration path (non-DKG `block_prologue`)

Likelihood increases when:
- Epoch intervals are shorter
- Multiple validator transactions exist in the pool near epoch boundaries
- The immediate `reconfigure()` path is used instead of DKG-based reconfiguration

The vulnerability can occur naturally during normal epoch transitions without any malicious activity.

## Recommendation

Add epoch transition detection in the proposal generator before pulling validator transactions:

1. Check if the proposed block's timestamp will trigger an epoch transition
2. If so, either:
   - Skip pulling validator transactions for that block, or
   - Pull only validator transactions with the next epoch number, or
   - Delay the epoch transition to the subsequent block

Example fix location: [9](#0-8) 

Before pulling validator transactions, add a check:
```rust
// Check if this block will trigger epoch transition
if will_trigger_epoch_transition(timestamp, epoch_interval, last_reconfiguration_time) {
    // Don't include validator transactions with current epoch
    validator_txns = vec![];
} else {
    // Pull validator transactions normally
}
```

## Proof of Concept

No executable PoC is provided, but the vulnerability can be triggered by:

1. Creating a DKGResult transaction with current epoch N
2. Proposing a block with timestamp that satisfies the epoch interval condition
3. Observing the transaction is discarded during execution with EpochNotCurrent error

The vulnerability is demonstrable through code analysis of the execution flow documented above.

## Notes

This is a valid protocol correctness issue that violates the expectation that validator transactions returned by `pull()` should remain valid through execution. While all validators deterministically agree on the outcome (making it not a consensus safety issue), it represents a reliability problem in epoch transition handling that can disrupt critical operations like DKG.

### Citations

**File:** consensus/src/liveness/proposal_generator.rs (L505-515)
```rust
        let (validator_txns, payload, timestamp) = if hqc.certified_block().has_reconfiguration() {
            // Reconfiguration rule - we propose empty blocks with parents' timestamp
            // after reconfiguration until it's committed
            (
                vec![],
                Payload::empty(
                    self.quorum_store_enabled,
                    self.allow_batches_without_pos_in_proposal,
                ),
                hqc.certified_block().timestamp_usecs(),
            )
```

**File:** consensus/src/liveness/proposal_generator.rs (L558-686)
```rust
    async fn generate_proposal_inner(
        &self,
        round: Round,
        parent_id: HashValue,
        proposer_election: Arc<dyn ProposerElection + Send + Sync>,
        maybe_optqs_payload_pull_params: Option<OptQSPayloadPullParams>,
    ) -> anyhow::Result<(Vec<ValidatorTransaction>, Payload, u64)> {
        {
            let mut last_round_generated = self.last_round_generated.lock();
            if *last_round_generated < round {
                *last_round_generated = round;
            } else {
                bail!("Already proposed in the round {}", round);
            }
        }
        // One needs to hold the blocks with the references to the payloads while get_block is
        // being executed: pending blocks vector keeps all the pending ancestors of the extended branch.
        let mut pending_blocks = self
            .block_store
            .path_from_commit_root(parent_id)
            .ok_or_else(|| format_err!("Parent block {} already pruned", parent_id))?;
        // Avoid txn manager long poll if the root block has txns, so that the leader can
        // deliver the commit proof to others without delay.
        pending_blocks.push(self.block_store.commit_root());

        // Exclude all the pending transactions: these are all the ancestors of
        // parent (including) up to the root (including).
        let exclude_payload: Vec<_> = pending_blocks
            .iter()
            .flat_map(|block| block.payload())
            .collect();
        let payload_filter = PayloadFilter::from(&exclude_payload);

        let pending_ordering = self
            .block_store
            .path_from_ordered_root(parent_id)
            .ok_or_else(|| format_err!("Parent block {} already pruned", parent_id))?
            .iter()
            .any(|block| !block.payload().is_none_or(|txns| txns.is_empty()));

        // All proposed blocks in a branch are guaranteed to have increasing timestamps
        // since their predecessor block will not be added to the BlockStore until
        // the local time exceeds it.
        let timestamp = self.time_service.get_current_timestamp();

        let voting_power_ratio = proposer_election.get_voting_power_participation_ratio(round);

        let (
            max_block_txns,
            max_block_txns_after_filtering,
            max_txns_from_block_to_execute,
            block_gas_limit_override,
            proposal_delay,
        ) = self
            .calculate_max_block_sizes(voting_power_ratio, timestamp, round)
            .await;

        PROPOSER_MAX_BLOCK_TXNS_AFTER_FILTERING.observe(max_block_txns_after_filtering as f64);
        if let Some(max_to_execute) = max_txns_from_block_to_execute {
            PROPOSER_MAX_BLOCK_TXNS_TO_EXECUTE.observe(max_to_execute as f64);
        }

        PROPOSER_DELAY_PROPOSAL.observe(proposal_delay.as_secs_f64());
        if !proposal_delay.is_zero() {
            tokio::time::sleep(proposal_delay).await;
        }

        let max_pending_block_size = pending_blocks
            .iter()
            .map(|block| {
                block.payload().map_or(PayloadTxnsSize::zero(), |p| {
                    PayloadTxnsSize::new(p.len() as u64, p.size() as u64)
                })
            })
            .reduce(PayloadTxnsSize::maximum)
            .unwrap_or_default();
        // Use non-backpressure reduced values for computing fill_fraction
        let max_fill_fraction =
            (max_pending_block_size.count() as f32 / self.max_block_txns.count() as f32).max(
                max_pending_block_size.size_in_bytes() as f32
                    / self.max_block_txns.size_in_bytes() as f32,
            );
        PROPOSER_PENDING_BLOCKS_COUNT.set(pending_blocks.len() as i64);
        PROPOSER_PENDING_BLOCKS_FILL_FRACTION.set(max_fill_fraction as f64);

        let pending_validator_txn_hashes: HashSet<HashValue> = pending_blocks
            .iter()
            .filter_map(|block| block.validator_txns())
            .flatten()
            .map(ValidatorTransaction::hash)
            .collect();
        let validator_txn_filter =
            vtxn_pool::TransactionFilter::PendingTxnHashSet(pending_validator_txn_hashes);

        let (validator_txns, mut payload) = self
            .payload_client
            .pull_payload(
                PayloadPullParameters {
                    max_poll_time: self.quorum_store_poll_time.saturating_sub(proposal_delay),
                    max_txns: max_block_txns,
                    max_txns_after_filtering: max_block_txns_after_filtering,
                    soft_max_txns_after_filtering: max_txns_from_block_to_execute
                        .unwrap_or(max_block_txns_after_filtering),
                    max_inline_txns: self.max_inline_txns,
                    maybe_optqs_payload_pull_params,
                    user_txn_filter: payload_filter,
                    pending_ordering,
                    pending_uncommitted_blocks: pending_blocks.len(),
                    recent_max_fill_fraction: max_fill_fraction,
                    block_timestamp: timestamp,
                },
                validator_txn_filter,
            )
            .await
            .context("Fail to retrieve payload")?;

        if !payload.is_direct()
            && max_txns_from_block_to_execute.is_some()
            && max_txns_from_block_to_execute.is_some_and(|v| payload.len() as u64 > v)
        {
            payload = payload.transform_to_quorum_store_v2(
                max_txns_from_block_to_execute,
                block_gas_limit_override,
            );
        } else if block_gas_limit_override.is_some() {
            payload = payload.transform_to_quorum_store_v2(None, block_gas_limit_override);
        }
        Ok((validator_txns, payload, timestamp.as_micros() as u64))
    }
```

**File:** consensus/consensus-types/src/block.rs (L553-566)
```rust
    pub fn combine_to_input_transactions(
        validator_txns: Vec<ValidatorTransaction>,
        txns: Vec<SignedTransaction>,
        metadata: BlockMetadataExt,
    ) -> Vec<Transaction> {
        once(Transaction::from(metadata))
            .chain(
                validator_txns
                    .into_iter()
                    .map(Transaction::ValidatorTransaction),
            )
            .chain(txns.into_iter().map(Transaction::UserTransaction))
            .collect()
    }
```

**File:** aptos-move/framework/aptos-framework/sources/block.move (L215-217)
```text
        if (timestamp - reconfiguration::last_reconfiguration_time() >= epoch_interval) {
            reconfiguration::reconfigure();
        };
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration.move (L142-142)
```text
        config_ref.epoch = config_ref.epoch + 1;
```

**File:** aptos-move/aptos-vm/src/validator_txns/dkg.rs (L68-77)
```rust
            Err(Expected(failure)) => {
                // Pretend we are inside Move, and expected failures are like Move aborts.
                Ok((
                    VMStatus::MoveAbort {
                        location: AbortLocation::Script,
                        code: failure as u64,
                        message: None,
                    },
                    VMOutput::empty_with_status(TransactionStatus::Discard(StatusCode::ABORTED)),
                ))
```

**File:** aptos-move/aptos-vm/src/validator_txns/dkg.rs (L100-102)
```rust
        if dkg_node.metadata.epoch != config_resource.epoch() {
            return Err(Expected(EpochNotCurrent));
        }
```
