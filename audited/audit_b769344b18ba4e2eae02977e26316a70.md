# Audit Report

## Title
Consensus Liveness Failure Due to Mismatched `broadcast_vote` Configuration Causing Vote Rejection

## Summary
Validators with `broadcast_vote = false` unconditionally reject incoming votes unless they are the proposer for the next round, regardless of how those votes were sent. When validators have mismatched `broadcast_vote` settings, votes broadcast by validators with `broadcast_vote = true` will be silently dropped by validators with `broadcast_vote = false`, preventing quorum certificate formation and causing consensus liveness failures.

## Finding Description

The `broadcast_vote` configuration in [1](#0-0)  controls whether validators broadcast their votes to all peers or send them only to the next round's proposer. However, the critical vulnerability lies in the vote reception logic.

When a validator with `broadcast_vote = false` receives a vote, it performs a check in the receiving validator's `process_vote` method: [2](#0-1) 

This code checks the **receiver's own** `local_config.broadcast_vote` setting and rejects the vote if the receiver is not the proposer for the next round. This creates an asymmetric behavior:

**Sending behavior** (lines 1406-1419): [3](#0-2) 

- If `broadcast_vote = true`: Validator broadcasts votes to ALL validators
- If `broadcast_vote = false`: Validator sends votes only to next round's proposer

**Receiving behavior** (lines 1746-1756):
- If receiver has `broadcast_vote = true`: Accepts all votes
- If receiver has `broadcast_vote = false`: **Rejects all votes unless receiver is next proposer**

**The Vulnerability:**
When validators have mismatched configurations:
1. Validator A (with `broadcast_vote = true`) creates a vote and broadcasts it to all validators including B and C
2. Validator B (with `broadcast_vote = false`) receives the vote but checks: "Am I the proposer for round+1?"
3. If B is NOT the next proposer, B rejects the vote with error message and returns early
4. Validator C (with `broadcast_vote = false`) does the same check and also rejects if not the proposer
5. Only the actual next proposer among validators with `broadcast_vote = false` will accept the vote

**Result:** Votes from validators with `broadcast_vote = true` are silently dropped by most validators with `broadcast_vote = false`, preventing the vote aggregation in [4](#0-3)  from reaching quorum voting power to form a QuorumCertificate.

**No Validation Exists:**
The configuration sanitizer [5](#0-4)  performs no validation to ensure `broadcast_vote` consistency across validators. Additionally, `broadcast_vote` is a local node configuration (not an on-chain configuration in [6](#0-5) ), meaning each validator independently configures it with no coordination mechanism.

## Impact Explanation

**HIGH SEVERITY** - This vulnerability can cause:

1. **Consensus Liveness Failure**: If a sufficient number of validators have `broadcast_vote = false` and are not the next proposer, the network cannot form QuorumCertificates, stalling consensus completely.

2. **Partial Network Partition**: Different validators may have different views of which votes have been received, leading to inconsistent state where some validators can form QCs while others cannot.

3. **Safety Violation Risk**: In edge cases where different subsets of validators form different QCs due to asymmetric vote reception, this could lead to consensus divergence.

This meets the **High Severity** criteria per the Aptos bug bounty program as it causes "significant protocol violations" and "validator node slowdowns" (or complete stalls). In worst cases where it prevents consensus entirely, it approaches **Critical Severity** as "total loss of liveness/network availability."

The vulnerability breaks the critical consensus invariant: "Consensus Safety: AptosBFT must prevent double-spending and chain splits under < 1/3 Byzantine" by introducing non-Byzantine liveness failures.

## Likelihood Explanation

**HIGHLY LIKELY** - This vulnerability has high probability of occurrence because:

1. **No Validation**: The codebase contains no validation in [5](#0-4)  to detect or prevent configuration mismatches.

2. **Local Configuration**: The `broadcast_vote` setting is a local node configuration, not coordinated through on-chain governance, making accidental mismatches highly probable during:
   - Network upgrades where some validators update configs while others don't
   - New validator onboarding with different configuration templates
   - Manual configuration changes by different operators

3. **Non-Obvious Behavior**: Operators would naturally assume `broadcast_vote` only controls sending behavior, not receiving behavior, making misconfigurations more likely.

4. **Default Value**: While the default is `true` [7](#0-6) , operators may change it for performance optimization without realizing the impact.

5. **Silent Failure**: The error message "I am not a valid proposer for round X, ignore" is misleading and doesn't indicate a configuration problem, making diagnosis difficult.

## Recommendation

**Immediate Fix**: Add validation in the configuration sanitizer to detect and prevent `broadcast_vote` inconsistencies:

```rust
// In config/src/config/consensus_config.rs, add to sanitize() method:

impl ConfigSanitizer for ConsensusConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        
        // ... existing checks ...
        
        // NEW: Warn if broadcast_vote is disabled (non-default)
        if !node_config.consensus.broadcast_vote {
            warn!(
                "broadcast_vote is disabled. This MUST be consistent across ALL validators \
                in the network, otherwise consensus liveness failures will occur. \
                Ensure all validators have the same broadcast_vote setting."
            );
        }
        
        Ok(())
    }
}
```

**Long-term Fix**: Either:

1. **Remove the receiving-side check entirely** - The check at lines 1746-1756 should be removed or modified to only apply when the sender's configuration is known to be `broadcast_vote = false` (which is impossible to determine from the receiving side).

2. **Make broadcast_vote an on-chain configuration** - Move this setting into `OnChainConsensusConfig` so it's coordinated across all validators through governance.

3. **Modify the receiving logic** to always accept votes regardless of local `broadcast_vote` setting:

```rust
// In consensus/src/round_manager.rs, modify process_vote():

async fn process_vote(&mut self, vote: &Vote) -> anyhow::Result<()> {
    let round = vote.vote_data().proposed().round();
    
    // ... existing logging ...
    
    // REMOVED: The problematic check that rejects votes based on receiver's config
    // Only validate that we can process the vote, not who sent it
    
    let block_id = vote.vote_data().proposed().id();
    if self.block_store.get_quorum_cert_for_block(block_id).is_some() {
        return Ok(());
    }
    let vote_reception_result = self
        .round_state
        .insert_vote(vote, &self.epoch_state.verifier);
    self.process_vote_reception_result(vote, vote_reception_result).await
}
```

## Proof of Concept

```rust
// Rust test in consensus/src/round_manager_tests/

#[tokio::test]
async fn test_broadcast_vote_config_mismatch_causes_vote_loss() {
    // Setup: 4 validators with mixed broadcast_vote configurations
    // Validator 0: broadcast_vote = true (broadcasts to all)
    // Validator 1: broadcast_vote = false (expects targeted, rejects broadcasts)
    // Validator 2: broadcast_vote = false (expects targeted, rejects broadcasts)
    // Validator 3: broadcast_vote = true (next proposer)
    
    let mut playground = NetworkPlayground::new(runtime.handle().clone());
    let mut nodes = vec![];
    
    for i in 0..4 {
        let mut config = ConsensusConfig::default();
        // Set broadcast_vote based on validator index
        config.broadcast_vote = (i == 0 || i == 3);
        
        let node = SMRNode::start(
            &mut playground,
            Arc::new(ValidatorSigner::random([i as u8; 32])),
            i,
            config,
            // ... other params
        );
        nodes.push(node);
    }
    
    // Validator 0 proposes block and votes (broadcasts to all)
    let proposal = nodes[0].generate_proposal().await;
    nodes[0].process_proposal(proposal.clone()).await;
    
    // Expected: All validators receive the vote and form QC
    // Actual: Validators 1 and 2 reject the vote because they have
    // broadcast_vote=false and are not the next proposer (validator 3 is)
    
    // Wait for potential QC formation
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // Check: Only validator 3 (next proposer) accepted the vote from validator 0
    // Validators 1 and 2 rejected it due to configuration mismatch
    assert!(nodes[1].get_votes_for_round(1).len() == 0); // Rejected
    assert!(nodes[2].get_votes_for_round(1).len() == 0); // Rejected  
    assert!(nodes[3].get_votes_for_round(1).len() == 1); // Accepted
    
    // Result: QC cannot be formed with only 2 votes (validators 0 and 3)
    // Need 3 out of 4 for quorum
    // Consensus stalls due to configuration mismatch
}
```

**Notes:**
- The vulnerability stems from conflating sending policy with receiving policy in the same configuration flag
- The receiving-side check at [2](#0-1)  was likely intended as an optimization to avoid processing votes not meant for this validator, but it incorrectly assumes all validators have the same `broadcast_vote` setting
- This is a **configuration consistency bug**, not a Byzantine fault - it can occur through accidental operator error
- The error message produced is misleading and doesn't indicate the true problem (configuration mismatch)

### Citations

**File:** config/src/config/consensus_config.rs (L94-94)
```rust
    pub broadcast_vote: bool,
```

**File:** config/src/config/consensus_config.rs (L371-371)
```rust
            broadcast_vote: true,
```

**File:** config/src/config/consensus_config.rs (L503-532)
```rust
impl ConfigSanitizer for ConsensusConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        // Verify that the safety rules and quorum store configs are valid
        SafetyRulesConfig::sanitize(node_config, node_type, chain_id)?;
        QuorumStoreConfig::sanitize(node_config, node_type, chain_id)?;

        // Verify that the consensus-only feature is not enabled in mainnet
        if let Some(chain_id) = chain_id {
            if chain_id.is_mainnet() && is_consensus_only_perf_test_enabled() {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "consensus-only-perf-test should not be enabled in mainnet!".to_string(),
                ));
            }
        }

        // Sender block limits must be <= receiver block limits
        Self::sanitize_send_recv_block_limits(&sanitizer_name, &node_config.consensus)?;

        // Quorum store batches must be <= consensus blocks
        Self::sanitize_batch_block_limits(&sanitizer_name, &node_config.consensus)?;

        Ok(())
    }
```

**File:** consensus/src/round_manager.rs (L1406-1419)
```rust
        if self.local_config.broadcast_vote {
            info!(self.new_log(LogEvent::Vote), "{}", vote);
            PROPOSAL_VOTE_BROADCASTED.inc();
            self.network.broadcast_vote(vote_msg).await;
        } else {
            let recipient = self
                .proposer_election
                .get_valid_proposer(proposal_round + 1);
            info!(
                self.new_log(LogEvent::Vote).remote_peer(recipient),
                "{}", vote
            );
            self.network.send_vote(vote_msg, vec![recipient]).await;
        }
```

**File:** consensus/src/round_manager.rs (L1746-1756)
```rust
        if !self.local_config.broadcast_vote && !vote.is_timeout() {
            // Unlike timeout votes regular votes are sent to the leaders of the next round only.
            let next_round = round + 1;
            ensure!(
                self.proposer_election
                    .is_valid_proposer(self.proposal_generator.author(), next_round),
                "[RoundManager] Received {}, but I am not a valid proposer for round {}, ignore.",
                vote,
                next_round
            );
        }
```

**File:** consensus/src/pending_votes.rs (L275-416)
```rust
    pub fn insert_vote(
        &mut self,
        vote: &Vote,
        validator_verifier: &ValidatorVerifier,
    ) -> VoteReceptionResult {
        // derive data from vote
        let li_digest = vote.ledger_info().hash();

        //
        // 1. Has the author already voted for this round?
        //

        if let Some((previously_seen_vote, previous_li_digest)) =
            self.author_to_vote.get(&vote.author())
        {
            // is it the same vote?
            if &li_digest == previous_li_digest {
                // we've already seen an equivalent vote before
                let new_timeout_vote = vote.is_timeout() && !previously_seen_vote.is_timeout();
                if !new_timeout_vote {
                    // it's not a new timeout vote
                    return VoteReceptionResult::DuplicateVote;
                }
            } else {
                // we have seen a different vote for the same round
                error!(
                    SecurityEvent::ConsensusEquivocatingVote,
                    remote_peer = vote.author(),
                    vote = vote,
                    previous_vote = previously_seen_vote
                );

                return VoteReceptionResult::EquivocateVote;
            }
        }

        //
        // 2. Store new vote (or update, in case it's a new timeout vote)
        //

        self.author_to_vote
            .insert(vote.author(), (vote.clone(), li_digest));

        //
        // 3. Let's check if we can create a QC
        //

        let len = self.li_digest_to_votes.len() + 1;
        // obtain the ledger info with signatures associated to the vote's ledger info
        let (hash_index, status) = self.li_digest_to_votes.entry(li_digest).or_insert_with(|| {
            (
                len,
                VoteStatus::NotEnoughVotes(SignatureAggregator::new(vote.ledger_info().clone())),
            )
        });

        let validator_voting_power = validator_verifier.get_voting_power(&vote.author());

        if validator_voting_power.is_none() {
            warn!("Received vote from an unknown author: {}", vote.author());
            return VoteReceptionResult::UnknownAuthor(vote.author());
        }
        let validator_voting_power =
            validator_voting_power.expect("Author must exist in the validator set.");
        if validator_voting_power == 0 {
            warn!("Received vote with no voting power, from {}", vote.author());
        }
        let cur_epoch = vote.vote_data().proposed().epoch() as i64;
        let cur_round = vote.vote_data().proposed().round() as i64;
        counters::CONSENSUS_CURRENT_ROUND_QUORUM_VOTING_POWER
            .set(validator_verifier.quorum_voting_power() as f64);

        if !vote.is_timeout() {
            counters::CONSENSUS_CURRENT_ROUND_VOTED_POWER
                .with_label_values(&[&vote.author().to_string(), &hash_index_to_str(*hash_index)])
                .set(validator_voting_power as f64);
            counters::CONSENSUS_LAST_VOTE_EPOCH
                .with_label_values(&[&vote.author().to_string()])
                .set(cur_epoch);
            counters::CONSENSUS_LAST_VOTE_ROUND
                .with_label_values(&[&vote.author().to_string()])
                .set(cur_round);
        }

        let voting_power = match status {
            VoteStatus::EnoughVotes(li_with_sig) => {
                return VoteReceptionResult::NewQuorumCertificate(Arc::new(QuorumCert::new(
                    vote.vote_data().clone(),
                    li_with_sig.clone(),
                )));
            },
            VoteStatus::NotEnoughVotes(sig_aggregator) => {
                // add this vote to the signature aggregator
                sig_aggregator.add_signature(vote.author(), vote.signature_with_status());

                // check if we have enough signatures to create a QC
                match sig_aggregator.check_voting_power(validator_verifier, true) {
                    // a quorum of signature was reached, a new QC is formed
                    Ok(aggregated_voting_power) => {
                        assert!(
                                aggregated_voting_power >= validator_verifier.quorum_voting_power(),
                                "QC aggregation should not be triggered if we don't have enough votes to form a QC"
                            );
                        let verification_result = {
                            let _timer = counters::VERIFY_MSG
                                .with_label_values(&["vote_aggregate_and_verify"])
                                .start_timer();

                            sig_aggregator.aggregate_and_verify(validator_verifier).map(
                                |(ledger_info, aggregated_sig)| {
                                    LedgerInfoWithSignatures::new(ledger_info, aggregated_sig)
                                },
                            )
                        };
                        match verification_result {
                            Ok(ledger_info_with_sig) => {
                                *status = VoteStatus::EnoughVotes(ledger_info_with_sig.clone());
                                return VoteReceptionResult::NewQuorumCertificate(Arc::new(
                                    QuorumCert::new(vote.vote_data().clone(), ledger_info_with_sig),
                                ));
                            },
                            Err(VerifyError::TooLittleVotingPower { voting_power, .. }) => {
                                voting_power
                            },
                            Err(e) => return VoteReceptionResult::ErrorAggregatingSignature(e),
                        }
                    },

                    // not enough votes
                    Err(VerifyError::TooLittleVotingPower { voting_power, .. }) => voting_power,

                    // error
                    Err(error) => {
                        error!(
                            "MUST_FIX: vote received could not be added: {}, vote: {}",
                            error, vote
                        );
                        return VoteReceptionResult::ErrorAddingVote(error);
                    },
                }
            },
        };
```

**File:** types/src/on_chain_config/consensus_config.rs (L15-123)
```rust
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
pub enum ConsensusAlgorithmConfig {
    Jolteon {
        main: ConsensusConfigV1,
        quorum_store_enabled: bool,
    },
    DAG(DagConsensusConfigV1),
    JolteonV2 {
        main: ConsensusConfigV1,
        quorum_store_enabled: bool,
        order_vote_enabled: bool,
    },
}

impl ConsensusAlgorithmConfig {
    pub fn default_for_genesis() -> Self {
        Self::JolteonV2 {
            main: ConsensusConfigV1::default(),
            quorum_store_enabled: true,
            order_vote_enabled: true,
        }
    }

    pub fn default_with_quorum_store_disabled() -> Self {
        Self::JolteonV2 {
            main: ConsensusConfigV1::default(),
            quorum_store_enabled: false,
            order_vote_enabled: true,
        }
    }

    pub fn default_if_missing() -> Self {
        Self::JolteonV2 {
            main: ConsensusConfigV1::default(),
            quorum_store_enabled: true,
            order_vote_enabled: false,
        }
    }

    pub fn quorum_store_enabled(&self) -> bool {
        match self {
            ConsensusAlgorithmConfig::Jolteon {
                quorum_store_enabled,
                ..
            }
            | ConsensusAlgorithmConfig::JolteonV2 {
                quorum_store_enabled,
                ..
            } => *quorum_store_enabled,
            ConsensusAlgorithmConfig::DAG(_) => true,
        }
    }

    pub fn order_vote_enabled(&self) -> bool {
        match self {
            ConsensusAlgorithmConfig::JolteonV2 {
                order_vote_enabled, ..
            } => *order_vote_enabled,
            _ => false,
        }
    }

    pub fn is_dag_enabled(&self) -> bool {
        match self {
            ConsensusAlgorithmConfig::Jolteon { .. }
            | ConsensusAlgorithmConfig::JolteonV2 { .. } => false,
            ConsensusAlgorithmConfig::DAG(_) => true,
        }
    }

    pub fn leader_reputation_exclude_round(&self) -> u64 {
        match self {
            ConsensusAlgorithmConfig::Jolteon { main, .. }
            | ConsensusAlgorithmConfig::JolteonV2 { main, .. } => main.exclude_round,
            _ => unimplemented!("method not supported"),
        }
    }

    pub fn max_failed_authors_to_store(&self) -> usize {
        match self {
            ConsensusAlgorithmConfig::Jolteon { main, .. }
            | ConsensusAlgorithmConfig::JolteonV2 { main, .. } => main.max_failed_authors_to_store,
            _ => unimplemented!("method not supported"),
        }
    }

    pub fn proposer_election_type(&self) -> &ProposerElectionType {
        match self {
            ConsensusAlgorithmConfig::Jolteon { main, .. } => &main.proposer_election_type,
            ConsensusAlgorithmConfig::JolteonV2 { main, .. } => &main.proposer_election_type,
            _ => unimplemented!("method not supported"),
        }
    }

    pub fn unwrap_dag_config_v1(&self) -> &DagConsensusConfigV1 {
        match self {
            ConsensusAlgorithmConfig::DAG(dag) => dag,
            _ => unreachable!("not a dag config"),
        }
    }

    pub fn unwrap_jolteon_config_v1(&self) -> &ConsensusConfigV1 {
        match self {
            ConsensusAlgorithmConfig::Jolteon { main, .. } => main,
            ConsensusAlgorithmConfig::JolteonV2 { main, .. } => main,
            _ => unreachable!("not a jolteon config"),
        }
    }
}
```
