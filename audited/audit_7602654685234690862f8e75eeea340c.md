# Audit Report

## Title
Token Ownership Timestamp Staleness in Indexer Database Causes Incorrect Time-Based Query Results

## Summary
The `insert_current_token_ownerships` function in the Aptos indexer fails to update the `last_transaction_timestamp` field during database upserts, causing timestamps to become stale and inconsistent with `last_transaction_version`. This breaks time-based queries and creates data integrity issues in the indexer API. [1](#0-0) 

## Finding Description

The `CurrentTokenOwnership` struct contains both `last_transaction_version` (line 56) and `last_transaction_timestamp` (line 59) fields that should remain synchronized - both pointing to the same transaction that last modified the ownership record. [2](#0-1) 

The critical bug is in the upsert operation at lines 395-405. When a conflict occurs on the primary key `(token_data_id_hash, property_version, owner_address)`, the UPDATE SET clause includes:
- `last_transaction_version` (line 401) ✓
- But **NOT** `last_transaction_timestamp` ✗

The WHERE clause at line 406 correctly prevents out-of-order updates by checking `last_transaction_version <= excluded.last_transaction_version`, but this only guards the version field.

**Exploitation Path:**

1. **Initial State**: Batch 1 processes transaction at version 100, timestamp T1 (2024-01-01 10:00:00)
   - Database record: `{version: 100, timestamp: T1}`

2. **Update State**: Batch 2 processes transaction at version 200, timestamp T2 (2024-01-01 11:00:00) updating the same token ownership
   - Upsert detects PK conflict
   - WHERE clause: `100 <= 200` evaluates TRUE ✓
   - UPDATE executes, setting `last_transaction_version = 200`
   - But `last_transaction_timestamp` is NOT in SET clause
   - Database record: `{version: 200, timestamp: T1}` ← **Inconsistent!**

3. **Query Breakage**: Application queries "all token ownerships updated after 2024-01-01 10:30:00"
   - Expected: Should return this record (updated at 11:00:00)
   - Actual: Misses this record (timestamp still shows 10:00:00)
   - **Result**: Incorrect, incomplete query results

This same bug affects three V1 tables:
- `insert_current_token_ownerships` (line 380)
- `insert_current_token_datas` (line 412) 
- `insert_current_collection_datas` (line 455) [3](#0-2) [4](#0-3) 

Notably, all V2 token tables correctly include `last_transaction_timestamp` in their UPDATE SET clauses, showing this is a fixable oversight in the V1 implementation: [5](#0-4) 

## Impact Explanation

This is a **Medium severity** data integrity issue per the Aptos bug bounty classification:

**"State inconsistencies requiring intervention"** - While this doesn't affect blockchain consensus or validator state, it causes persistent data inconsistencies in the indexer database that break API functionality. Applications relying on the indexer for:
- Time-based filtering of token ownership changes
- Analytics on ownership transfer timing
- Historical queries using timestamp ranges
- Display of "last updated" information

Will receive incorrect, incomplete, or misleading results. The inconsistency accumulates over time as more updates occur, requiring database migration or reindexing to fix.

This affects the **State Consistency** invariant for the indexer component: timestamp and version fields must remain synchronized to accurately represent when state changes occurred.

## Likelihood Explanation

**High likelihood** - This bug triggers automatically during normal indexer operation:
- No attacker action required
- Occurs whenever a token ownership is updated across different transaction batches
- Given active NFT marketplaces on Aptos, token transfers happen frequently
- Bug affects all V1 token ownership, token data, and collection data records
- Accumulates over time as the indexer processes more transactions

The bug has likely already manifested in production indexer databases processing mainnet data.

## Recommendation

Add `last_transaction_timestamp` to the UPDATE SET clause in all three affected functions:

**For `insert_current_token_ownerships` (line 395):**
```rust
.set((
    creator_address.eq(excluded(creator_address)),
    collection_name.eq(excluded(collection_name)),
    name.eq(excluded(name)),
    amount.eq(excluded(amount)),
    token_properties.eq(excluded(token_properties)),
    last_transaction_version.eq(excluded(last_transaction_version)),
    last_transaction_timestamp.eq(excluded(last_transaction_timestamp)), // ADD THIS LINE
    collection_data_id_hash.eq(excluded(collection_data_id_hash)),
    table_type.eq(excluded(table_type)),
    inserted_at.eq(excluded(inserted_at)),
))
```

Apply the same fix to:
- `insert_current_token_datas` (line 427)
- `insert_current_collection_datas` (line 470)

**Additionally**, existing production databases should be migrated to fix stale timestamps by joining with the historical transaction tables to backfill correct timestamps based on `last_transaction_version`.

## Proof of Concept

```rust
// Rust integration test demonstrating the bug
#[tokio::test]
async fn test_timestamp_staleness_bug() {
    use chrono::NaiveDateTime;
    use crate::models::token_models::token_ownerships::CurrentTokenOwnership;
    
    let mut conn = establish_test_db_connection();
    
    // Step 1: Insert initial ownership at version 100, timestamp T1
    let ownership_v100 = CurrentTokenOwnership {
        token_data_id_hash: "hash123".to_string(),
        property_version: BigDecimal::from(0),
        owner_address: "0xabc".to_string(),
        creator_address: "0x1".to_string(),
        collection_name: "Test".to_string(),
        name: "Token1".to_string(),
        amount: BigDecimal::from(1),
        token_properties: json!({}),
        last_transaction_version: 100,
        collection_data_id_hash: "coll_hash".to_string(),
        table_type: "0x3::token::TokenStore".to_string(),
        last_transaction_timestamp: NaiveDateTime::parse_from_str(
            "2024-01-01 10:00:00", "%Y-%m-%d %H:%M:%S"
        ).unwrap(),
    };
    
    insert_current_token_ownerships(&mut conn, &[ownership_v100]).unwrap();
    
    // Step 2: Update same ownership at version 200, timestamp T2
    let ownership_v200 = CurrentTokenOwnership {
        token_data_id_hash: "hash123".to_string(),
        property_version: BigDecimal::from(0),
        owner_address: "0xabc".to_string(),
        creator_address: "0x1".to_string(),
        collection_name: "Test".to_string(),
        name: "Token1".to_string(),
        amount: BigDecimal::from(2), // Changed amount
        token_properties: json!({}),
        last_transaction_version: 200,
        collection_data_id_hash: "coll_hash".to_string(),
        table_type: "0x3::token::TokenStore".to_string(),
        last_transaction_timestamp: NaiveDateTime::parse_from_str(
            "2024-01-01 11:00:00", "%Y-%m-%d %H:%M:%S"
        ).unwrap(),
    };
    
    insert_current_token_ownerships(&mut conn, &[ownership_v200]).unwrap();
    
    // Step 3: Query the database
    let result = query_current_token_ownership(&mut conn, "hash123", 0, "0xabc");
    
    // BUG: Version is 200 but timestamp is still 10:00:00 (T1), not 11:00:00 (T2)
    assert_eq!(result.last_transaction_version, 200);
    assert_eq!(
        result.last_transaction_timestamp.format("%Y-%m-%d %H:%M:%S").to_string(),
        "2024-01-01 10:00:00" // WRONG! Should be 11:00:00
    );
    
    // Step 4: Time-based query fails
    let after_10_30 = query_ownerships_after_timestamp(
        &mut conn, 
        "2024-01-01 10:30:00"
    );
    
    // BUG: Record is missing from results even though it was updated at 11:00:00
    assert_eq!(after_10_30.len(), 0); // Should be 1, but returns 0!
}
```

## Notes

This bug is specific to V1 token tables (`current_token_ownerships`, `current_token_datas`, `current_collection_datas`). The V2 implementations already include the timestamp in their UPDATE clauses, demonstrating that this pattern is correct and the V1 code needs updating to match.

The indexer processes transactions in batches, and while batches maintain version ordering via the WHERE clause guard, the missing timestamp update creates a permanent inconsistency between version and timestamp fields that persists in the database and affects all downstream queries.

### Citations

**File:** crates/indexer/src/models/token_models/token_ownerships.rs (L47-60)
```rust
pub struct CurrentTokenOwnership {
    pub token_data_id_hash: String,
    pub property_version: BigDecimal,
    pub owner_address: String,
    pub creator_address: String,
    pub collection_name: String,
    pub name: String,
    pub amount: BigDecimal,
    pub token_properties: serde_json::Value,
    pub last_transaction_version: i64,
    pub collection_data_id_hash: String,
    pub table_type: String,
    pub last_transaction_timestamp: chrono::NaiveDateTime,
}
```

**File:** crates/indexer/src/processors/token_processor.rs (L380-410)
```rust
fn insert_current_token_ownerships(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentTokenOwnership],
) -> Result<(), diesel::result::Error> {
    use schema::current_token_ownerships::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentTokenOwnership::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_token_ownerships::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((token_data_id_hash, property_version, owner_address))
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    name.eq(excluded(name)),
                    amount.eq(excluded(amount)),
                    token_properties.eq(excluded(token_properties)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    collection_data_id_hash.eq(excluded(collection_data_id_hash)),
                    table_type.eq(excluded(table_type)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_token_ownerships.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/token_processor.rs (L412-453)
```rust
fn insert_current_token_datas(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentTokenData],
) -> Result<(), diesel::result::Error> {
    use schema::current_token_datas::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentTokenData::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_token_datas::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict(token_data_id_hash)
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    name.eq(excluded(name)),
                    maximum.eq(excluded(maximum)),
                    supply.eq(excluded(supply)),
                    largest_property_version.eq(excluded(largest_property_version)),
                    metadata_uri.eq(excluded(metadata_uri)),
                    payee_address.eq(excluded(payee_address)),
                    royalty_points_numerator.eq(excluded(royalty_points_numerator)),
                    royalty_points_denominator.eq(excluded(royalty_points_denominator)),
                    maximum_mutable.eq(excluded(maximum_mutable)),
                    uri_mutable.eq(excluded(uri_mutable)),
                    description_mutable.eq(excluded(description_mutable)),
                    properties_mutable.eq(excluded(properties_mutable)),
                    royalty_mutable.eq(excluded(royalty_mutable)),
                    default_properties.eq(excluded(default_properties)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    collection_data_id_hash.eq(excluded(collection_data_id_hash)),
                    description.eq(excluded(description)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_token_datas.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/token_processor.rs (L455-488)
```rust
fn insert_current_collection_datas(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentCollectionData],
) -> Result<(), diesel::result::Error> {
    use schema::current_collection_datas::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentCollectionData::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_collection_datas::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict(collection_data_id_hash)
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    description.eq(excluded(description)),
                    metadata_uri.eq(excluded(metadata_uri)),
                    supply.eq(excluded(supply)),
                    maximum.eq(excluded(maximum)),
                    maximum_mutable.eq(excluded(maximum_mutable)),
                    uri_mutable.eq(excluded(uri_mutable)),
                    description_mutable.eq(excluded(description_mutable)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    table_handle.eq(excluded(table_handle)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_collection_datas.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/token_processor.rs (L762-796)
```rust
fn insert_current_token_ownerships_v2(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentTokenOwnershipV2],
) -> Result<(), diesel::result::Error> {
    use schema::current_token_ownerships_v2::dsl::*;

    let chunks = get_chunks(
        items_to_insert.len(),
        CurrentTokenOwnershipV2::field_count(),
    );

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_token_ownerships_v2::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((token_data_id, property_version_v1, owner_address, storage_id))
                .do_update()
                .set((
                    amount.eq(excluded(amount)),
                    table_type_v1.eq(excluded(table_type_v1)),
                    token_properties_mutated_v1.eq(excluded(token_properties_mutated_v1)),
                    is_soulbound_v2.eq(excluded(is_soulbound_v2)),
                    token_standard.eq(excluded(token_standard)),
                    is_fungible_v2.eq(excluded(is_fungible_v2)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    last_transaction_timestamp.eq(excluded(last_transaction_timestamp)),
                    inserted_at.eq(excluded(inserted_at)),
                    non_transferrable_by_owner.eq(excluded(non_transferrable_by_owner)),
                )),
            Some(" WHERE current_token_ownerships_v2.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```
