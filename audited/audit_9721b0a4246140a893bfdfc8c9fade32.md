# Audit Report

## Title
Race Condition in Transaction Replay Causing Restore Failure Due to Out-of-Order Commit Queue Population

## Summary
A race condition in `replay_transactions()` allows concurrent `update_ledger()` calls to complete out-of-order, causing chunks to be added to the commit queue in non-sequential order. This triggers database validation failures during commit, causing restore operations to fail and preventing node recovery from backups.

## Finding Description

The vulnerability exists in the transaction replay mechanism used during database restore operations. The root cause is a dangerous interaction between concurrent execution and stateful side effects.

The `replay_transactions()` function creates a shared `ChunkExecutor` instance [1](#0-0)  that is used by all concurrent tasks. The ledger update stream uses `try_buffered_x(3, 1)` [2](#0-1)  allowing up to 3 concurrent `update_ledger()` calls via `tokio::task::spawn_blocking` [3](#0-2) .

While `FuturesOrderedX` ensures that future results are yielded in order [4](#0-3) , it does NOT prevent side effects within each future from executing out of order. Each concurrent task polls futures independently, and completed futures are buffered until their turn comes to be yielded.

The `ChunkCommitQueue` implements a two-stage pipeline where chunks move from `to_update_ledger` to `to_commit` [5](#0-4) . The `commit_queue` is protected by a `Mutex` [6](#0-5) , but this doesn't prevent the race condition.

The vulnerability occurs because `update_ledger()` performs operations in two separate critical sections:
1. First lock: `next_chunk_to_update_ledger()` retrieves a chunk and marks it as `None` [7](#0-6)  and [8](#0-7) 
2. Computation happens without holding the lock [9](#0-8) 
3. Second lock: `save_ledger_update_output()` pops from the front of `to_update_ledger` and pushes to `to_commit` [10](#0-9)  and [11](#0-10) 

The critical flaw: `save_ledger_update_output()` always pops the front element [12](#0-11)  and pushes the provided chunk [13](#0-12) , assuming it corresponds to that front element. If tasks complete out of order, this assumption breaks, causing chunks to be pushed to `to_commit` in the wrong order.

When commits execute sequentially [14](#0-13) , the database validation checks that `chunk.first_version == next_version` [15](#0-14) . When chunks are out of order, this validation fails, aborting the entire restore operation.

## Impact Explanation

**Severity: High** - This causes critical restore operations to fail, preventing node recovery from backups.

**Impact:**
- **Restore Operations Fail**: Nodes cannot successfully restore from backups when chunks process out-of-order due to database validation failures
- **Disaster Recovery Risk**: During disaster recovery scenarios, affected nodes cannot rejoin the network through backup restoration
- **Non-Deterministic Failures**: The bug depends on thread scheduling and processing time variance, making it difficult to diagnose
- **Resource Waste**: Failed restores waste significant time and computational resources
- **Operational Risk**: Organizations relying on backup/restore for disaster recovery face operational risk

While the database validation prevents data corruption (maintaining safety), the failure of restore functionality represents an availability issue. In scenarios where multiple validators need to restore simultaneously (e.g., after a widespread database corruption issue), this could contribute to network recovery delays.

## Likelihood Explanation

**Likelihood: High**

The vulnerability triggers under realistic conditions:
- Default concurrent buffering of 3 chunks is enabled by the code
- Standard multi-core systems provide sufficient parallelism for the race condition
- Processing time variance between chunks (due to different sizes, I/O latency, computation complexity) increases the probability of out-of-order completion
- Reproduces non-deterministically but can manifest frequently under load
- No special attacker actions required - affects normal restore operations
- The issue is latent in production code and will manifest during routine backup restore operations, especially with larger datasets where processing time variance is higher

## Recommendation

The fix requires ensuring that chunks are saved in the same order they were retrieved. Several approaches are possible:

**Option 1: Sequential Processing**
Remove concurrent buffering for the update_ledger phase by changing `.try_buffered_x(3, 1)` to `.try_buffered_x(1, 1)` or using a sequential processing pattern. This sacrifices parallelism but eliminates the race condition.

**Option 2: Associate Chunks with Indices**
Modify `save_ledger_update_output()` to accept both the chunk and its index, then insert it at the correct position in `to_commit` rather than always pushing to the back. This requires tracking chunk indices through the pipeline.

**Option 3: Atomic Operation**
Combine `next_chunk_to_update_ledger()`, computation, and `save_ledger_update_output()` into a single critical section. This would require refactoring to hold the lock during computation, which may impact performance.

The recommended approach is **Option 1** (sequential processing) as it provides the simplest fix with guaranteed correctness, though at the cost of some performance. For restore operations, correctness is more critical than maximum throughput.

## Proof of Concept

No executable PoC is provided, but the vulnerability can be demonstrated through code analysis:

1. The concurrent execution pattern with `try_buffered_x(3, 1)` allows up to 3 tasks to run simultaneously
2. Each task acquires chunks from the queue in order (0, 1, 2), but releases the lock during computation
3. If task 1 completes before task 0, it calls `save_ledger_update_output()` which pops the front (chunk 0's placeholder) and pushes chunk 1's output
4. When task 0 completes, it pops the new front (chunk 1's placeholder) and pushes chunk 0's output
5. Result: `to_commit` = `[chunk 1, chunk 0, chunk 2]` (out of order)
6. Sequential commit attempts to commit chunk 1 first, but database expects chunk 0's first_version
7. Validation at `aptosdb_writer.rs:253-258` fails with version mismatch error

The race condition is non-deterministic and depends on relative completion times of concurrent tasks, but will occur whenever tasks complete out of their original order.

## Notes

This vulnerability represents a subtle concurrency bug where mutex protection is insufficient because the state mutation (`save_ledger_update_output`) occurs in a different critical section than the state retrieval (`next_chunk_to_update_ledger`). The intermediate computation phase allows tasks to complete in any order, violating the implicit assumption that chunks are processed sequentially.

The impact is classified as **High** rather than **Critical** because it affects backup/restore operations rather than the live network consensus or transaction processing. However, in disaster recovery scenarios requiring simultaneous restoration of multiple validators, this could contribute to network availability issues.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L657-657)
```rust
        let chunk_replayer = Arc::new(ChunkExecutor::<AptosVMBlockExecutor>::new(db));
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L701-703)
```rust
                    tokio::task::spawn_blocking(move || chunk_replayer.update_ledger())
                        .await
                        .expect("spawn_blocking failed")
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L706-706)
```rust
            .try_buffered_x(3, 1);
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L708-733)
```rust
        let total_replayed = db_commit_stream
            .and_then(|()| {
                let chunk_replayer = chunk_replayer.clone();
                async move {
                    let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit"]);

                    tokio::task::spawn_blocking(move || {
                        let v = chunk_replayer.commit()?;

                        let total_replayed = v - first_version + 1;
                        TRANSACTION_REPLAY_VERSION.set(v as i64);
                        info!(
                            version = v,
                            accumulative_tps = (total_replayed as f64
                                / replay_start.elapsed().as_secs_f64())
                                as u64,
                            "Transactions replayed."
                        );
                        Ok(total_replayed)
                    })
                    .await
                    .expect("spawn_blocking failed")
                }
            })
            .try_fold(0, |_prev_total, total| future::ok(total))
            .await?;
```

**File:** storage/backup/backup-cli/src/utils/stream/futures_ordered_x.rs (L124-148)
```rust
    fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        let this = &mut *self;

        // Check to see if we've already received the next value
        if let Some(next_output) = this.queued_outputs.peek_mut() {
            if next_output.index == this.next_outgoing_index {
                this.next_outgoing_index += 1;
                return Poll::Ready(Some(PeekMut::pop(next_output).data));
            }
        }

        loop {
            match ready!(this.in_progress_queue.poll_next_unpin(cx)) {
                Some(output) => {
                    if output.index == this.next_outgoing_index {
                        this.next_outgoing_index += 1;
                        return Poll::Ready(Some(output.data));
                    } else {
                        this.queued_outputs.push(output)
                    }
                },
                None => return Poll::Ready(None),
            }
        }
    }
```

**File:** execution/executor/src/chunk_executor/chunk_commit_queue.rs (L30-46)
```rust
/// It's a two stage pipeline:
///           (front)     (front)
///          /           /
///    ... | to_commit | to_update_ledger | ---> (txn version increases)
///                     \                \
///                      \                latest_state
///                       latest_state_summary
///                       latest_txn_accumulator
///
pub struct ChunkCommitQueue {
    /// Notice that latest_state and latest_txn_accumulator are at different versions.
    latest_state: LedgerState,
    latest_state_summary: LedgerStateSummary,
    latest_txn_accumulator: Arc<InMemoryTransactionAccumulator>,
    to_commit: VecDeque<Option<ExecutedChunk>>,
    to_update_ledger: VecDeque<Option<ChunkToUpdateLedger>>,
}
```

**File:** execution/executor/src/chunk_executor/chunk_commit_queue.rs (L85-103)
```rust
    pub(crate) fn next_chunk_to_update_ledger(
        &mut self,
    ) -> Result<(
        LedgerStateSummary,
        Arc<InMemoryTransactionAccumulator>,
        ChunkToUpdateLedger,
    )> {
        let chunk_opt = self
            .to_update_ledger
            .front_mut()
            .ok_or_else(|| anyhow!("No chunk to update ledger."))?;
        let chunk = chunk_opt
            .take()
            .ok_or_else(|| anyhow!("Next chunk to update ledger has already been processed."))?;
        Ok((
            self.latest_state_summary.clone(),
            self.latest_txn_accumulator.clone(),
            chunk,
        ))
```

**File:** execution/executor/src/chunk_executor/chunk_commit_queue.rs (L106-131)
```rust
    pub(crate) fn save_ledger_update_output(&mut self, chunk: ExecutedChunk) -> Result<()> {
        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["save_ledger_update_output"]);

        ensure!(
            !self.to_update_ledger.is_empty(),
            "to_update_ledger is empty."
        );
        ensure!(
            self.to_update_ledger.front().unwrap().is_none(),
            "Head of to_update_ledger has not been processed."
        );
        self.latest_state_summary = chunk
            .output
            .ensure_state_checkpoint_output()?
            .state_summary
            .clone();
        self.latest_txn_accumulator = chunk
            .output
            .ensure_ledger_update_output()?
            .transaction_accumulator
            .clone();
        self.to_update_ledger.pop_front();
        self.to_commit.push_back(Some(chunk));

        Ok(())
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L230-230)
```rust
    commit_queue: Mutex<ChunkCommitQueue>,
```

**File:** execution/executor/src/chunk_executor/mod.rs (L339-340)
```rust
        let (parent_state_summary, parent_accumulator, chunk) =
            self.commit_queue.lock().next_chunk_to_update_ledger()?;
```

**File:** execution/executor/src/chunk_executor/mod.rs (L346-373)
```rust
        let state_checkpoint_output = DoStateCheckpoint::run(
            &output.execution_output,
            &parent_state_summary,
            &ProvableStateSummary::new_persisted(self.db.reader.as_ref())?,
            Some(
                chunk_verifier
                    .transaction_infos()
                    .iter()
                    .map(|t| t.state_checkpoint_hash())
                    .collect_vec(),
            ),
        )?;

        let ledger_update_output = DoLedgerUpdate::run(
            &output.execution_output,
            &state_checkpoint_output,
            parent_accumulator.clone(),
        )?;

        chunk_verifier.verify_chunk_result(&parent_accumulator, &ledger_update_output)?;

        let ledger_info_opt = chunk_verifier.maybe_select_chunk_ending_ledger_info(
            &ledger_update_output,
            output.execution_output.next_epoch_state.as_ref(),
        )?;
        output.set_state_checkpoint_output(state_checkpoint_output);
        output.set_ledger_update_output(ledger_update_output);

```

**File:** execution/executor/src/chunk_executor/mod.rs (L381-383)
```rust
        self.commit_queue
            .lock()
            .save_ledger_update_output(executed_chunk)?;
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L253-258)
```rust
        ensure!(
            chunk.first_version == next_version,
            "The first version passed in ({}), and the next version expected by db ({}) are inconsistent.",
            chunk.first_version,
            next_version,
        );
```
