# Audit Report

## Title
Silent RPC Message Drops Due to Write Queue Capacity Leading to Validator Slowdowns

## Summary
The RPC layer's `handle_outbound_request()` and `send_outbound_response()` functions use `aptos_channel::Sender::push()` to enqueue messages to the write queue, which has a fixed capacity of 1024 messages. When this queue is full, `push()` silently drops messages while returning `Ok(())`, causing the RPC layer to believe messages were successfully enqueued. This results in RPC requests/responses being lost without notification, leading to timeouts and validator slowdowns affecting consensus and state synchronization.

## Finding Description

The vulnerability exists in the interaction between the RPC protocol layer and the aptos_channel implementation: [1](#0-0) 

At this location, outbound RPC requests are pushed onto the write queue. Similarly, inbound RPC responses are pushed at: [2](#0-1) 

The write queue is created with limited capacity: [3](#0-2) 

The critical flaw is in the `aptos_channel::Sender::push()` implementation: [4](#0-3) 

When the internal queue is full, the `PerKeyQueue::push()` method drops messages: [5](#0-4) 

**The core issue:** `push()` returns `Ok(())` even when messages are dropped due to capacity limits. The only way to detect drops is through an optional status channel, but the RPC layer calls `push()` without providing one.

**Attack Scenario:**
1. An attacker or network congestion causes the write queue to fill (1024 messages)
2. New RPC requests/responses call `push()` which silently drops messages (oldest with KLAST policy)
3. For outbound requests: The RPC layer creates a pending task waiting for a response that will never arrive
4. For inbound responses: The remote peer waits indefinitely for a response
5. Both cases eventually timeout, causing delays in consensus messages, state sync, or mempool operations
6. Repeated occurrences cause cumulative validator slowdowns

**Invariant Broken:** Network reliability - RPC operations must either succeed or fail explicitly with clear error indication, not silently fail.

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria:

**"Validator node slowdowns"** is explicitly listed as a High severity impact category. This vulnerability directly causes such slowdowns through:

1. **Consensus Impact**: Consensus RPC requests (proposals, votes, sync requests) may be dropped, causing validators to timeout and miss rounds
2. **State Sync Impact**: State sync RPC responses may be dropped, preventing nodes from catching up with the network
3. **Mempool Impact**: Transaction propagation RPCs may be dropped, affecting transaction processing
4. **Silent Failures**: No error is surfaced to the application layer, making the issue difficult to diagnose and mitigate
5. **Cascading Effects**: Timeouts lead to retries, which further fill the queue, creating a negative feedback loop

The vulnerability affects critical network operations without requiring any privileged access or insider capabilities.

## Likelihood Explanation

**High Likelihood** - This can occur in normal network conditions:

1. **High Traffic Scenarios**: During network congestion, epoch changes, or validator catchup, message bursts can easily exceed 1024 pending messages
2. **Slow Peers**: Any peer that reads data slowly (due to network issues, resource constraints, or malicious behavior) can cause the write queue to back up
3. **No Rate Limiting**: There's no backpressure mechanism to slow down message generation when the queue fills
4. **KLAST Policy**: With KLAST queue style, oldest messages are dropped, which may include critical consensus messages that were just about to be sent

The 1024 capacity seems reasonable for normal operation but can be exceeded in:
- Consensus leader sending proposals to 100+ validators simultaneously
- State sync burst during node startup
- Network partition recovery when accumulated messages are sent
- Multiple concurrent RPC-heavy operations (consensus + state sync + mempool)

## Recommendation

Implement proper error handling for queue capacity exhaustion:

**Option 1: Use push_with_feedback and handle drops**
```rust
// In handle_outbound_request()
let (status_tx, status_rx) = oneshot::channel();
write_reqs_tx.push_with_feedback((), message, Some(status_tx))?;

// Monitor status and notify application if dropped
let request_id_clone = request_id;
let app_tx_clone = application_response_tx.clone();
self.executor.spawn(async move {
    if let Ok(ElementStatus::Dropped(_)) = status_rx.await {
        let _ = app_tx_clone.send(Err(RpcError::WriteQueueFull));
    }
});
```

**Option 2: Check queue capacity before push**
Add a method to check available capacity and return an error if the queue is near full, allowing the application layer to apply backpressure.

**Option 3: Fail fast on capacity issues**
Modify `aptos_channel::Sender::push()` to return an error when messages are dropped:
```rust
pub fn push(&self, key: K, message: M) -> Result<()> {
    let (status_tx, status_rx) = oneshot::channel();
    self.push_with_feedback(key, message, Some(status_tx))?;
    
    // Check synchronously if message was dropped
    if let Ok(ElementStatus::Dropped(_)) = status_rx.try_recv() {
        return Err(anyhow!("Queue full, message dropped"));
    }
    Ok(())
}
```

**Option 4: Increase capacity dynamically**
Implement dynamic capacity expansion under load, though this has memory implications.

**Recommended: Option 1** - It provides immediate feedback while maintaining backward compatibility and allows application-level decisions on how to handle drops.

## Proof of Concept

```rust
// Test that demonstrates the vulnerability
#[tokio::test]
async fn test_rpc_silent_drop_on_full_queue() {
    use aptos_channels::aptos_channel;
    use aptos_channels::message_queues::QueueStyle;
    use network::protocols::rpc::{OutboundRpcs, OutboundRpcRequest};
    use network::protocols::wire::messaging::v1::NetworkMessage;
    use bytes::Bytes;
    use futures::channel::oneshot;
    use std::time::Duration;
    
    // Create write queue with capacity 2 for easy testing
    let (mut write_reqs_tx, mut write_reqs_rx) = aptos_channel::new(
        QueueStyle::KLAST,
        2,  // Small capacity to trigger drops easily
        None
    );
    
    // Create OutboundRpcs handler
    let network_context = NetworkContext::mock();
    let time_service = TimeService::mock();
    let remote_peer_id = PeerId::random();
    let mut outbound_rpcs = OutboundRpcs::new(
        network_context,
        time_service,
        remote_peer_id,
        10,
    );
    
    // Fill the write queue
    write_reqs_tx.push((), NetworkMessage::DirectSendMsg(DirectSendMsg::default())).unwrap();
    write_reqs_tx.push((), NetworkMessage::DirectSendMsg(DirectSendMsg::default())).unwrap();
    
    // Now queue is full (2 messages)
    // Try to send an RPC request - it will be pushed, dropping oldest message
    let (res_tx, res_rx) = oneshot::channel();
    let request = OutboundRpcRequest {
        protocol_id: ProtocolId::ConsensusRpcBcs,
        data: Bytes::from("test"),
        res_tx,
        timeout: Duration::from_secs(5),
    };
    
    // This should succeed but message may be dropped
    let result = outbound_rpcs.handle_outbound_request(request, &mut write_reqs_tx);
    assert!(result.is_ok()); // Returns Ok even though oldest message was dropped
    
    // The RPC task is now waiting for a response that will never arrive
    // because the request was dropped from the queue
    
    // Wait for timeout
    let response = tokio::time::timeout(Duration::from_secs(6), res_rx).await;
    
    // Verify the RPC times out due to dropped message
    match response {
        Ok(Ok(Err(RpcError::TimedOut))) => {
            println!("RPC timed out as expected - message was silently dropped");
        }
        _ => panic!("Expected RPC timeout due to dropped message"),
    }
}
```

**Notes:**
- The vulnerability is exploitable under realistic network conditions without requiring privileged access
- The silent failure nature makes it particularly dangerous as operators cannot easily diagnose the root cause
- The 1024 capacity can be exhausted through legitimate high-traffic scenarios or intentional slowdown attacks
- Critical consensus and state sync operations are affected, directly impacting validator performance and network liveness

### Citations

**File:** network/framework/src/protocols/rpc/mod.rs (L354-354)
```rust
        write_reqs_tx.push((), message)?;
```

**File:** network/framework/src/protocols/rpc/mod.rs (L499-499)
```rust
        write_reqs_tx.push((), message)?;
```

**File:** network/framework/src/peer/mod.rs (L340-345)
```rust
        let (write_reqs_tx, mut write_reqs_rx): (aptos_channel::Sender<(), NetworkMessage>, _) =
            aptos_channel::new(
                QueueStyle::KLAST,
                1024,
                Some(&counters::PENDING_WIRE_MESSAGES),
            );
```

**File:** crates/channel/src/aptos_channel.rs (L85-112)
```rust
    pub fn push(&self, key: K, message: M) -> Result<()> {
        self.push_with_feedback(key, message, None)
    }

    /// Same as `push`, but this function also accepts a oneshot::Sender over which the sender can
    /// be notified when the message eventually gets delivered or dropped.
    pub fn push_with_feedback(
        &self,
        key: K,
        message: M,
        status_ch: Option<oneshot::Sender<ElementStatus<M>>>,
    ) -> Result<()> {
        let mut shared_state = self.shared_state.lock();
        ensure!(!shared_state.receiver_dropped, "Channel is closed");
        debug_assert!(shared_state.num_senders > 0);

        let dropped = shared_state.internal_queue.push(key, (message, status_ch));
        // If this or an existing message had to be dropped because of the queue being full, we
        // notify the corresponding status channel if it was registered.
        if let Some((dropped_val, Some(dropped_status_ch))) = dropped {
            // Ignore errors.
            let _err = dropped_status_ch.send(ElementStatus::Dropped(dropped_val));
        }
        if let Some(w) = shared_state.waker.take() {
            w.wake();
        }
        Ok(())
    }
```

**File:** crates/channel/src/message_queues.rs (L134-147)
```rust
        if key_message_queue.len() >= self.max_queue_size.get() {
            if let Some(c) = self.counters.as_ref() {
                c.with_label_values(&["dropped"]).inc();
            }
            match self.queue_style {
                // Drop the newest message for FIFO
                QueueStyle::FIFO => Some(message),
                // Drop the oldest message for LIFO
                QueueStyle::LIFO | QueueStyle::KLAST => {
                    let oldest = key_message_queue.pop_front();
                    key_message_queue.push_back(message);
                    oldest
                },
            }
```
