# Audit Report

## Title
Missing Validation for HotStateConfig.max_items_per_shard Allows Validator Node Crashes and Memory Exhaustion

## Summary
The `HotStateConfig` struct lacks validation for the `max_items_per_shard` configuration parameter. Setting this value to 0 causes immediate validator node panic during state updates, while extremely large values (e.g., billions) can trigger memory exhaustion. No bounds checking exists in the config sanitizer, allowing misconfigured nodes to crash during normal operation.

## Finding Description

The `HotStateConfig` struct defines `max_items_per_shard` without enforcing any bounds validation. [1](#0-0) 

The configuration sanitizer performs extensive validation for other storage parameters but completely omits validation for `max_items_per_shard`. [2](#0-1) 

**Vulnerability 1: Zero Value Panic**

During state updates, the code creates an LRU cache using the unchecked configuration value. [3](#0-2) 

When `max_items_per_shard` is 0, `NonZeroUsize::new(0)` returns `None`, causing `.unwrap()` to panic and crash the validator node immediately during any state update operation.

**Vulnerability 2: Memory Exhaustion**

The hot state creates DashMap instances with capacity set to `max_items_per_shard`. [4](#0-3) 

With 16 shards, setting `max_items_per_shard` to extremely large values (e.g., 1 billion) attempts to pre-allocate or allow capacity for 16 billion entries total, consuming terabytes of memory and triggering OOM kills.

The state update mechanism is invoked during normal transaction processing. [5](#0-4) 

## Impact Explanation

**Severity: Medium**

This vulnerability can cause:
- **Validator node crashes** (zero value → immediate panic)
- **Memory exhaustion and OOM kills** (extreme values → terabytes allocation)
- **Network liveness degradation** if multiple validators are misconfigured

This meets **Medium Severity** criteria per the bug bounty program: "State inconsistencies requiring intervention" and validator availability issues. While not directly causing fund loss or consensus safety violations, it can degrade network liveness and requires manual intervention to recover.

## Likelihood Explanation

**Likelihood: Low to Medium**

This vulnerability requires:
1. Validator operator to create/modify node configuration
2. Set `max_items_per_shard` to invalid value (0 or extreme number)
3. Node restart to load new config

**Factors increasing likelihood:**
- No validation provides zero defense-in-depth against configuration errors
- Operators copying/modifying configs may introduce typos (e.g., "0" instead of "250000")
- No warnings or sanity checks during node startup
- If an attacker compromises a validator's system, this becomes an easy DoS vector

**Factors decreasing likelihood:**
- Requires operator access (not exploitable by external unprivileged attackers)
- Default value (250,000) is safe
- Most operators use default configurations

## Recommendation

Add validation in the `ConfigSanitizer` implementation for `StorageConfig`:

```rust
impl ConfigSanitizer for StorageConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.storage;
        
        // Add validation for hot_state_config
        let max_items = config.hot_state_config.max_items_per_shard;
        if max_items == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "hot_state_config.max_items_per_shard cannot be zero, it will cause node panic.".to_string(),
            ));
        }
        if max_items < 1_000 {
            warn!("hot_state_config.max_items_per_shard is very low ({}), may cause excessive evictions.", max_items);
        }
        if max_items > 10_000_000 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                format!("hot_state_config.max_items_per_shard is too large ({}), may cause memory exhaustion. Maximum recommended: 10,000,000.", max_items),
            ));
        }

        // ... existing validation code ...
        Ok(())
    }
}
```

Additionally, replace the `.unwrap()` with a more defensive check:

```rust
let capacity = NonZeroUsize::new(self.hot_state_config.max_items_per_shard)
    .expect("max_items_per_shard must be non-zero (config validation should prevent this)");
```

## Proof of Concept

**Setup:**
Create a custom node configuration file with `max_items_per_shard: 0`:

```yaml
storage:
  hot_state_config:
    max_items_per_shard: 0
    refresh_interval_versions: 100000
    delete_on_restart: true
    compute_root_hash: true
```

**Execution Steps:**
1. Start validator node with the modified config
2. Node initializes successfully (no validation)
3. Submit any transaction to trigger state updates
4. Node calls `State::update()` → `HotStateLRU::new()`
5. `NonZeroUsize::new(0).unwrap()` panics with:
   ```
   thread 'main' panicked at 'called `Option::unwrap()` on a `None` value'
   ```
6. Validator node crashes, requiring restart with corrected config

**For memory exhaustion variant:**
Set `max_items_per_shard: 10000000000` (10 billion) and observe memory allocation spike attempting to create HashMaps with 160 billion total capacity across 16 shards, leading to OOM termination.

## Notes

This is a defense-in-depth issue rather than a direct external attack vector. While it requires validator operator access to exploit, the lack of validation violates security best practices:

1. **No safety net for operator errors**: Typos or misconfigurations can crash production nodes
2. **Reduced attack surface if compromised**: If an attacker gains access to a validator's configuration system, this becomes a trivial DoS method
3. **Violates principle of least surprise**: Other storage config parameters have validation; this one should too

The finding is valid as the question specifically asks whether bounds are enforced (they are not) and whether extreme values cause the described issues (they do).

### Citations

**File:** config/src/config/storage_config.rs (L243-254)
```rust
pub struct HotStateConfig {
    /// Max number of items in each shard.
    pub max_items_per_shard: usize,
    /// Every now and then refresh `hot_since_version` for hot items to prevent them from being
    /// evicted.
    pub refresh_interval_versions: u64,
    /// Whether to delete persisted data on disk on restart. Used during development.
    pub delete_on_restart: bool,
    /// Whether we compute root hashes for hot state in executor and commit the resulting JMT to
    /// db.
    pub compute_root_hash: bool,
}
```

**File:** config/src/config/storage_config.rs (L682-799)
```rust
impl ConfigSanitizer for StorageConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.storage;

        let ledger_prune_window = config
            .storage_pruner_config
            .ledger_pruner_config
            .prune_window;
        let state_merkle_prune_window = config
            .storage_pruner_config
            .state_merkle_pruner_config
            .prune_window;
        let epoch_snapshot_prune_window = config
            .storage_pruner_config
            .epoch_snapshot_pruner_config
            .prune_window;
        let user_pruning_window_offset = config
            .storage_pruner_config
            .ledger_pruner_config
            .user_pruning_window_offset;

        if ledger_prune_window < 50_000_000 {
            warn!("Ledger prune_window is too small, harming network data availability.");
        }
        if state_merkle_prune_window < 100_000 {
            warn!("State Merkle prune_window is too small, node might stop functioning.");
        }
        if epoch_snapshot_prune_window < 50_000_000 {
            warn!("Epoch snapshot prune_window is too small, harming network data availability.");
        }
        if user_pruning_window_offset > 1_000_000 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset too large, so big a buffer is unlikely necessary. Set something < 1 million.".to_string(),
            ));
        }
        if user_pruning_window_offset > ledger_prune_window {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset is larger than the ledger prune window, the API will refuse to return any data.".to_string(),
            ));
        }

        if let Some(db_path_overrides) = config.db_path_overrides.as_ref() {
            if !config.rocksdb_configs.enable_storage_sharding {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "db_path_overrides is allowed only if sharding is enabled.".to_string(),
                ));
            }

            if let Some(ledger_db_path) = db_path_overrides.ledger_db_path.as_ref() {
                if !ledger_db_path.is_absolute() {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        format!(
                            "Path {ledger_db_path:?} in db_path_overrides is not an absolute path."
                        ),
                    ));
                }
            }

            if let Some(state_kv_db_path) = db_path_overrides.state_kv_db_path.as_ref() {
                if let Some(metadata_path) = state_kv_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = state_kv_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }

            if let Some(state_merkle_db_path) = db_path_overrides.state_merkle_db_path.as_ref() {
                if let Some(metadata_path) = state_merkle_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = state_merkle_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }

            if let Some(hot_state_merkle_db_path) =
                db_path_overrides.hot_state_merkle_db_path.as_ref()
            {
                if let Some(metadata_path) = hot_state_merkle_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = hot_state_merkle_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }
        }

        Ok(())
    }
}
```

**File:** storage/storage-interface/src/state_store/state.rs (L197-204)
```rust
                    let mut lru = HotStateLRU::new(
                        NonZeroUsize::new(self.hot_state_config.max_items_per_shard).unwrap(),
                        Arc::clone(&persisted_hot_state),
                        overlay,
                        hot_metadata.latest.clone(),
                        hot_metadata.oldest.clone(),
                        hot_metadata.num_items,
                    );
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L42-46)
```rust
    fn new(max_items: usize) -> Self {
        Self {
            inner: DashMap::with_capacity(max_items),
        }
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L420-425)
```rust
        let (result_state, hot_state_updates) = parent_state.update_with_memorized_reads(
            base_state_view.persisted_hot_state(),
            base_state_view.persisted_state(),
            to_commit.state_update_refs(),
            base_state_view.memorized_reads(),
        );
```
