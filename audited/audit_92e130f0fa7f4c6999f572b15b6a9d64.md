# Audit Report

## Title
Indexer Panic on TokenDataId Deserialization Failure Due to Struct Schema Mismatch

## Summary
The token indexer contains `.unwrap()` calls that will panic and halt the indexer if the Move `TokenDataId` struct schema diverges from the Rust `TokenDataIdType` struct, causing deserialization failures during transaction processing.

## Finding Description
The indexer's token processing code calls `.unwrap()` on Result values returned by `CurrentTokenPendingClaim::from_write_table_item()` and `from_delete_table_item()` without proper error handling. These functions perform serde JSON deserialization of `TokenDataId` structs from on-chain table items. [1](#0-0) [2](#0-1) 

The deserialization chain flows through `TokenWriteSet::from_table_item_type()`, which uses `serde_json::from_value()` to parse JSON data into the Rust `TokenDataIdType` struct: [3](#0-2) 

The Rust struct expects three specific fields: [4](#0-3) 

If a Move framework upgrade modifies the `TokenDataId` struct (e.g., adds a required field, removes a field, or renames a field) without corresponding updates to the indexer code, serde deserialization will fail with a "missing field" error. This error propagates up through the Result chain and hits the `.unwrap()` call, causing a panic that crashes the token processor. [5](#0-4) 

The processor does not use `catch_unwind`, so panics are not recovered and will halt the indexer thread.

## Impact Explanation
This qualifies as **High Severity** under the Aptos bug bounty program criteria for "API crashes" and "Validator node slowdowns." The indexer is critical API infrastructure that provides queryable blockchain state. When it halts:

1. API queries for token data fail or return stale data
2. Applications depending on real-time token information break
3. Users cannot query their token balances or pending claims
4. Manual intervention is required to identify the issue and update the indexer code

While not as severe as consensus failures or fund loss, this represents a significant availability failure for a core blockchain service.

## Likelihood Explanation
**Medium Likelihood**. This scenario requires:
- A Move framework upgrade that modifies the `TokenDataId` struct definition
- The indexer code not being updated in sync with the framework upgrade

Framework upgrades are governance actions that happen through the on-chain upgrade process. While not frequent, they are a normal part of blockchain evolution. The Aptos framework has already gone through multiple versions, and Token V2 introduced breaking changes to token structures.

However, this is not exploitable by an unprivileged attacker - it requires either:
1. Governance approval for a framework upgrade, OR
2. An oversight where framework and indexer code get out of sync during an upgrade

## Recommendation
Replace all `.unwrap()` calls in the token processing chain with proper error handling that logs the issue and allows the indexer to skip the problematic transaction rather than panicking:

```rust
// In tokens.rs, replace:
.unwrap()

// With:
.unwrap_or_else(|e| {
    aptos_logger::error!(
        transaction_version = txn_version,
        error = ?e,
        "Failed to parse TokenPendingClaim, skipping"
    );
    None
})
```

Alternatively, add schema versioning to handle multiple TokenDataId formats gracefully, or add `#[serde(default)]` attributes to make fields optional with sensible defaults.

Additionally, add monitoring and alerting for deserialization failures so operators are notified of schema mismatches before they cause widespread indexer failures.

## Proof of Concept

**Scenario**: Framework upgrade adds a new required field to TokenDataId

**Step 1**: Current Move struct (simplified):
```move
struct TokenDataId has copy, drop, store {
    creator: address,
    collection: String,
    name: String,
}
```

**Step 2**: Framework is upgraded to add a field:
```move
struct TokenDataId has copy, drop, store {
    creator: address,
    collection: String,
    name: String,
    uri: String,  // NEW FIELD
}
```

**Step 3**: A transaction creates a token with the new schema. The on-chain JSON representation now includes:
```json
{
  "creator": "0x123...",
  "collection": "MyCollection",
  "name": "Token1",
  "uri": "https://..."
}
```

**Step 4**: Indexer processes this transaction, attempts to deserialize into the old Rust struct (which doesn't have `uri`). If the new field is required for hash computation in Move, the Rust struct would compute a different hash, but more critically, if serde is in strict mode or if the field order matters, deserialization fails.

**Step 5**: The `.unwrap()` at line 154 of `tokens.rs` panics with "missing field" or similar error.

**Step 6**: Token processor crashes, indexer stops processing new transactions.

---

**Note**: This vulnerability cannot be triggered by an unprivileged attacker as it requires governance-approved framework upgrades. It represents a defensive programming and operational resilience issue rather than a direct security exploit. The severity rating reflects the impact on service availability rather than exploitability by malicious actors.

### Citations

**File:** crates/indexer/src/models/token_models/tokens.rs (L148-154)
```rust
                        CurrentTokenPendingClaim::from_write_table_item(
                            write_table_item,
                            txn_version,
                            txn_timestamp,
                            table_handle_to_owner,
                        )
                        .unwrap()
```

**File:** crates/indexer/src/models/token_models/tokens.rs (L157-163)
```rust
                        CurrentTokenPendingClaim::from_delete_table_item(
                            delete_table_item,
                            txn_version,
                            txn_timestamp,
                            table_handle_to_owner,
                        )
                        .unwrap()
```

**File:** crates/indexer/src/models/token_models/token_utils.rs (L34-39)
```rust
#[derive(Serialize, Deserialize, Debug, Clone)]
pub struct TokenDataIdType {
    pub creator: String,
    collection: String,
    name: String,
}
```

**File:** crates/indexer/src/models/token_models/token_utils.rs (L322-347)
```rust
    pub fn from_table_item_type(
        data_type: &str,
        data: &serde_json::Value,
        txn_version: i64,
    ) -> Result<Option<TokenWriteSet>> {
        match data_type {
            "0x3::token::TokenDataId" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::TokenDataId(inner))),
            "0x3::token::TokenId" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::TokenId(inner))),
            "0x3::token::TokenData" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::TokenData(inner))),
            "0x3::token::Token" => {
                serde_json::from_value(data.clone()).map(|inner| Some(TokenWriteSet::Token(inner)))
            },
            "0x3::token::CollectionData" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::CollectionData(inner))),
            "0x3::token_transfers::TokenOfferId" => serde_json::from_value(data.clone())
                .map(|inner| Some(TokenWriteSet::TokenOfferId(inner))),
            _ => Ok(None),
        }
        .context(format!(
            "version {} failed! failed to parse type {}, data {:?}",
            txn_version, data_type, data
        ))
    }
```

**File:** crates/indexer/src/indexer/transaction_processor.rs (L66-91)
```rust
    async fn process_transactions_with_status(
        &self,
        txns: Vec<Transaction>,
    ) -> Result<ProcessingResult, TransactionProcessingError> {
        assert!(
            !txns.is_empty(),
            "Must provide at least one transaction to this function"
        );
        PROCESSOR_INVOCATIONS
            .with_label_values(&[self.name()])
            .inc();

        let start_version = txns.first().unwrap().version().unwrap();
        let end_version = txns.last().unwrap().version().unwrap();

        self.mark_versions_started(start_version, end_version);
        let res = self
            .process_transactions(txns, start_version, end_version)
            .await;
        // Handle block success/failure
        match res.as_ref() {
            Ok(processing_result) => self.update_status_success(processing_result),
            Err(tpe) => self.update_status_err(tpe),
        };
        res
    }
```
