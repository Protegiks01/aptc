# Audit Report

## Title
Optimistic Fetch Storage Error Allows Bounded Peer Retry Without Penalty Leading to Resource Exhaustion

## Summary
When `get_epoch_ending_ledger_info` fails during optimistic fetch processing, the peer is not marked as invalid, allowing repeated retry attempts every 100ms for up to 5 seconds (~50 retries). This wastes server resources through repeated storage reads, blocking task spawns, and log spam without penalizing the offending peer.

## Finding Description

In the optimistic fetch mechanism, when a peer requests new data that spans epoch boundaries, the storage service must retrieve the epoch ending ledger info. The vulnerability exists in how storage errors are handled during this retrieval. [1](#0-0) 

When `get_epoch_ending_ledger_info` fails, the code logs an error and returns early without adding the peer to `peers_with_invalid_optimistic_fetches`. This means the peer's optimistic fetch request remains active in the `optimistic_fetches` DashMap and will be retried on the next handler iteration.

The optimistic fetch handler runs periodically: [2](#0-1) 

The retry frequency and timeout duration are configured as: [3](#0-2) 

This means:
- Handler runs every 100ms (`storage_summary_refresh_interval_ms`)
- Optimistic fetch expires after 5000ms (`max_optimistic_fetch_period_ms`)
- **Result: ~50 retry attempts before timeout**

Each retry spawns a blocking task that performs storage I/O: [4](#0-3) 

Critically, storage errors do NOT increment the peer's invalid request counter in the RequestModerator: [5](#0-4) 

The RequestModerator's invalid request tracking only applies to request validation failures, not internal storage errors that occur during request processing.

**Attack Scenario:**
A malicious peer can send optimistic fetch requests that intentionally trigger persistent storage errors by:
1. Requesting a future epoch that doesn't exist yet
2. Requesting a pruned epoch that's no longer in storage
3. Crafting epoch values that cause storage read failures
4. Multiple malicious peers amplifying the attack

Each malicious request will:
- Retry every 100ms for 5 seconds (~50 times)
- Spawn 50 blocking tasks consuming thread pool resources
- Perform 50 storage I/O operations (potentially expensive reads)
- Generate 50 error log entries (log spam)
- NOT be penalized or tracked by RequestModerator

After the 5-second timeout expires, the peer can immediately submit another malicious optimistic fetch request, repeating the cycle indefinitely.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per the Aptos bug bounty program, specifically under "Validator node slowdowns."

**Resource Exhaustion Impact:**
- **CPU**: Each retry spawns a blocking task in the runtime thread pool, consuming CPU cycles
- **I/O**: Each retry performs storage reads, potentially hitting disk for epoch ending ledger infos
- **Memory**: Active blocking tasks and storage read buffers consume memory
- **Logs**: Repeated error logging can fill disk space and impact log analysis

**Attack Amplification:**
Multiple malicious peers can simultaneously exploit this, with each peer maintaining one active malicious optimistic fetch. With N malicious peers, the server processes N Ã— 50 retries over 5 seconds (10N retries per second).

**Why Medium, not High/Critical:**
- Does not affect consensus safety or liveness
- Does not cause fund loss or state corruption
- Bounded by timeout (not truly infinite retries)
- Each peer limited to one active optimistic fetch
- Server continues functioning, just with degraded performance

This aligns with Medium severity: "Validator node slowdowns" and "State inconsistencies requiring intervention" (resource state, not blockchain state).

## Likelihood Explanation

**High Likelihood** - This vulnerability is easily exploitable:

1. **No Special Permissions**: Any peer on the network can send optimistic fetch requests
2. **Simple Exploitation**: Just send requests with epochs that trigger storage errors (future epochs, pruned epochs, invalid epochs)
3. **Trigger Conditions**: Storage errors are realistic and can occur naturally:
   - Requesting epochs beyond current blockchain state
   - Requesting recently pruned historical epochs
   - Storage corruption or temporary failures
4. **No Rate Limiting**: The peer is not penalized, allowing immediate re-exploitation after timeout
5. **Detection Difficulty**: Error logs appear legitimate (storage read failures), making malicious requests hard to distinguish from honest node issues

**Realistic Attack Path:**
```
1. Malicious peer establishes connection to storage service
2. Peer sends GetNewTransactionOutputsWithProof with:
   - known_version: 1000
   - known_epoch: 9999 (far future epoch)
3. Server processes optimistic fetch every 100ms
4. Each iteration calls get_epoch_ending_ledger_info(9999)
5. Storage returns error (epoch doesn't exist)
6. Function returns early without marking peer
7. Repeat 50 times over 5 seconds
8. After timeout, peer sends another malicious request
9. Cycle continues indefinitely
```

## Recommendation

**Solution 1: Mark peers on storage errors (Recommended)**

Add the peer to `peers_with_invalid_optimistic_fetches` when storage errors occur, treating persistent storage failures as invalid requests:

```rust
let epoch_ending_ledger_info = match utils::get_epoch_ending_ledger_info(
    // ... parameters ...
) {
    Ok(epoch_ending_ledger_info) => epoch_ending_ledger_info,
    Err(error) => {
        // Log the failure
        error!(LogSchema::new(LogEntry::OptimisticFetchRefresh)
            .error(&error)
            .message(&format!(
                "Failed to get the epoch ending ledger info for epoch: {:?} !",
                highest_known_epoch
            )));

        // Mark peer as invalid to prevent retry loop
        peers_with_invalid_optimistic_fetches
            .lock()
            .push(peer_network_id);
        
        return;
    },
};
```

**Solution 2: Exponential backoff**

Implement exponential backoff for retries per peer, tracking retry attempts and increasing delay between retries.

**Solution 3: Separate error categories**

Distinguish between transient storage errors (allow retries) and persistent errors (mark peer invalid). This requires enhanced error typing from the storage layer.

**Recommended Approach**: Implement Solution 1 (mark peers on storage errors) as it's the simplest and most effective. Storage errors during optimistic fetch processing indicate either:
- Malicious peer sending invalid requests
- Storage corruption requiring operator intervention
- Legitimate transient errors (will be resolved when peer resubmits after timeout)

In all cases, marking the peer as invalid and removing the optimistic fetch is the correct behavior.

## Proof of Concept

**Scenario**: Malicious peer sends optimistic fetch with future epoch

```rust
// Test demonstrating the vulnerability
#[tokio::test]
async fn test_storage_error_retry_without_penalty() {
    // Setup test environment
    let config = StorageServiceConfig {
        max_optimistic_fetch_period_ms: 5000,
        storage_summary_refresh_interval_ms: 100,
        ..Default::default()
    };
    let storage = MockStorageReader::new();
    let optimistic_fetches = Arc::new(DashMap::new());
    
    // Malicious peer creates optimistic fetch with future epoch
    let malicious_peer = PeerNetworkId::new(NetworkId::Public, PeerId::random());
    let request = StorageServiceRequest::new(
        DataRequest::GetNewTransactionOutputsWithProof(
            NewTransactionOutputsWithProofRequest {
                known_version: 1000,
                known_epoch: 9999, // Future epoch that doesn't exist
            }
        ),
        false,
    );
    
    // Add optimistic fetch
    let optimistic_fetch = OptimisticFetchRequest::new(
        request,
        response_sender,
        TimeService::mock(),
    );
    optimistic_fetches.insert(malicious_peer, optimistic_fetch);
    
    // Configure storage to fail epoch lookup
    storage.set_epoch_ending_ledger_info_error(9999, 
        Error::StorageErrorEncountered("Epoch not found".to_string()));
    
    // Track retry attempts
    let retry_count = Arc::new(AtomicU64::new(0));
    storage.on_epoch_ending_ledger_info_call(clone!(retry_count => move || {
        retry_count.fetch_add(1, Ordering::SeqCst);
    }));
    
    // Run handler for 5 seconds
    let start = Instant::now();
    while start.elapsed() < Duration::from_secs(5) {
        handle_active_optimistic_fetches(/* ... */).await;
        tokio::time::sleep(Duration::from_millis(100)).await;
    }
    
    // Verify multiple retries occurred without peer penalty
    let final_retry_count = retry_count.load(Ordering::SeqCst);
    assert!(final_retry_count >= 40); // Should retry ~50 times
    assert!(final_retry_count <= 60); // Allow some timing variance
    
    // Verify peer was NOT marked as invalid
    let moderator_state = request_moderator.get_unhealthy_peer_states();
    assert!(!moderator_state.contains_key(&malicious_peer));
}
```

**Expected Behavior**: Peer should be marked invalid after first storage error.

**Actual Behavior**: Peer retries ~50 times over 5 seconds without penalty, wasting server resources.

## Notes

This vulnerability is classified as Medium severity because while it doesn't compromise consensus or funds, it allows resource exhaustion attacks that can degrade validator node performance. The bounded nature of retries (5-second timeout) prevents it from being High severity, but the ease of exploitation and ability to amplify across multiple peers makes it a legitimate security concern requiring remediation.

### Citations

**File:** state-sync/storage-service/server/src/optimistic_fetch.rs (L500-528)
```rust
        let active_task = runtime.spawn_blocking(move || {
            // Check if we have synced beyond the highest known version
            if highest_known_version < highest_synced_version {
                if highest_known_epoch < highest_synced_epoch {
                    // Fetch the epoch ending ledger info from storage (the
                    // peer needs to sync to their epoch ending ledger info).
                    let epoch_ending_ledger_info = match utils::get_epoch_ending_ledger_info(
                        cached_storage_server_summary.clone(),
                        optimistic_fetches.clone(),
                        subscriptions.clone(),
                        highest_known_epoch,
                        lru_response_cache.clone(),
                        request_moderator.clone(),
                        &peer_network_id,
                        storage.clone(),
                        time_service.clone(),
                    ) {
                        Ok(epoch_ending_ledger_info) => epoch_ending_ledger_info,
                        Err(error) => {
                            // Log the failure to fetch the epoch ending ledger info
                            error!(LogSchema::new(LogEntry::OptimisticFetchRefresh)
                                .error(&error)
                                .message(&format!(
                                    "Failed to get the epoch ending ledger info for epoch: {:?} !",
                                    highest_known_epoch
                                )));

                            return;
                        },
```

**File:** state-sync/storage-service/server/src/lib.rs (L242-262)
```rust
                // Create a ticker for the refresh interval
                let duration = Duration::from_millis(config.storage_summary_refresh_interval_ms);
                let ticker = time_service.interval(duration);
                futures::pin_mut!(ticker);

                // Continuously handle the optimistic fetches
                loop {
                    futures::select! {
                        _ = ticker.select_next_some() => {
                            // Handle the optimistic fetches periodically
                            handle_active_optimistic_fetches(
                                runtime.clone(),
                                cached_storage_server_summary.clone(),
                                config,
                                optimistic_fetches.clone(),
                                lru_response_cache.clone(),
                                request_moderator.clone(),
                                storage.clone(),
                                subscriptions.clone(),
                                time_service.clone(),
                            ).await;
```

**File:** config/src/config/state_sync_config.rs (L207-215)
```rust
            max_optimistic_fetch_period_ms: 5000, // 5 seconds
            max_state_chunk_size: MAX_STATE_CHUNK_SIZE,
            max_storage_read_wait_time_ms: 10_000, // 10 seconds
            max_subscription_period_ms: 30_000,    // 30 seconds
            max_transaction_chunk_size: MAX_TRANSACTION_CHUNK_SIZE,
            max_transaction_output_chunk_size: MAX_TRANSACTION_OUTPUT_CHUNK_SIZE,
            min_time_to_ignore_peers_secs: 300, // 5 minutes
            request_moderator_refresh_interval_ms: 1000, // 1 second
            storage_summary_refresh_interval_ms: 100, // Optimal for <= 10 blocks per second
```

**File:** state-sync/storage-service/server/src/moderator.rs (L47-69)
```rust
    /// Increments the invalid request count for the peer and marks
    /// the peer to be ignored if it has sent too many invalid requests.
    /// Note: we only ignore peers on the public network.
    pub fn increment_invalid_request_count(&mut self, peer_network_id: &PeerNetworkId) {
        // Increment the invalid request count
        self.invalid_request_count += 1;

        // If the peer is a PFN and has sent too many invalid requests, start ignoring it
        if self.ignore_start_time.is_none()
            && peer_network_id.network_id().is_public_network()
            && self.invalid_request_count >= self.max_invalid_requests
        {
            // TODO: at some point we'll want to terminate the connection entirely

            // Start ignoring the peer
            self.ignore_start_time = Some(self.time_service.now());

            // Log the fact that we're now ignoring the peer
            warn!(LogSchema::new(LogEntry::RequestModeratorIgnoredPeer)
                .peer_network_id(peer_network_id)
                .message("Ignoring peer due to too many invalid requests!"));
        }
    }
```
