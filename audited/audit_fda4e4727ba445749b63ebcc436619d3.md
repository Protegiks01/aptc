# Audit Report

## Title
Unbounded Disk Space Growth in Executor-Benchmark When Storage Sharding Enabled Without Pruning

## Summary
The executor-benchmark tool allows `enable_storage_sharding=true` with all pruners disabled, causing each of the 16 database shards to accumulate historical state data indefinitely until disk space is exhausted. No validation prevents this misconfiguration.

## Finding Description

The `StorageOpt` struct in the executor-benchmark allows independent configuration of storage sharding and pruning mechanisms. [1](#0-0) 

The `PrunerOpt` struct defines three pruner enable flags that all default to `false`: [2](#0-1) 

When `enable_storage_sharding=true`, the system creates 16 separate database shards via `StateKvDb`: [3](#0-2) 

Each transaction that modifies blockchain state creates entries in `StateValueByKeyHashSchema` with the key format `(state_key_hash, version)`: [4](#0-3) 

When a state key is updated, the old value becomes "stale" and is tracked in `StaleStateValueIndexByKeyHashSchema` for pruning: [5](#0-4) 

The `StateKvPrunerManager` only creates a pruner worker when explicitly enabled in the configuration: [6](#0-5) 

Similarly, the `StateMerklePrunerManager` only creates pruner workers when enabled: [7](#0-6) 

The `StateKvShardPruner` is responsible for deleting old state values from individual shards, but is never instantiated when the pruner is disabled: [8](#0-7) 

The storage configuration sanitizer validates prune window sizes but does NOT check if pruning is enabled when sharding is active: [9](#0-8) 

**Exploitation Path:**
1. User runs executor-benchmark with `--enable-storage-sharding` flag
2. User does not specify any `--enable-*-pruner` flags (all default to false)
3. The benchmark creates 16 database shards
4. As blocks are processed, each state update creates new versioned entries in the appropriate shard
5. Without pruners, stale state values are never deleted
6. Each shard grows continuously as historical versions accumulate
7. Eventually, all available disk space is consumed
8. The benchmark/node crashes or stops functioning

## Impact Explanation

This qualifies as **Medium severity** under Aptos bug bounty criteria: "State inconsistencies requiring intervention."

The vulnerability causes:
- **Resource exhaustion**: Uncontrolled disk space growth across 16 shards
- **Operational failure**: Benchmark runs fail when disk space is exhausted
- **Manual intervention required**: Requires deleting the database and restarting
- **CI/CD impact**: The executor-benchmark is used in production CI/CD workflows, potentially causing pipeline failures

While this doesn't directly affect consensus or cause fund loss, it violates the critical invariant: "Resource Limits: All operations must respect gas, storage, and computational limits."

## Likelihood Explanation

**High likelihood** of occurrence:

1. **Default configuration vulnerability**: All pruner flags default to `false`, making the unsafe configuration the path of least resistance
2. **No validation**: The codebase provides no warnings or errors when this misconfiguration is detected
3. **Production usage**: The executor-benchmark is actively used in GitHub Actions CI/CD workflows [10](#0-9) 
4. **Easy to trigger**: Any user running `--enable-storage-sharding` without explicit pruner flags triggers the issue
5. **Silent failure**: The problem manifests gradually rather than immediately, making it harder to detect

## Recommendation

Add configuration validation to prevent enabling storage sharding without at least one pruner enabled:

```rust
impl StorageOpt {
    fn storage_test_config(&self) -> StorageTestConfig {
        // Validate that if sharding is enabled, at least one pruner must be enabled
        if self.enable_storage_sharding {
            let has_any_pruner = self.pruner_opt.enable_state_pruner
                || self.pruner_opt.enable_epoch_snapshot_pruner
                || self.pruner_opt.enable_ledger_pruner;
            
            if !has_any_pruner {
                panic!(
                    "Storage sharding requires at least one pruner to be enabled. \
                     Each shard will grow unbounded without pruning, exhausting disk space. \
                     Enable at least one of: --enable-state-pruner, --enable-ledger-pruner, \
                     or --enable-epoch-snapshot-pruner"
                );
            }
        }
        
        StorageTestConfig {
            pruner_config: self.pruner_opt.pruner_config(),
            enable_storage_sharding: self.enable_storage_sharding,
            enable_indexer_grpc: self.enable_indexer_grpc,
        }
    }
}
```

Additionally, update the `ConfigSanitizer` in `storage_config.rs` to add this validation for general storage configurations.

## Proof of Concept

```bash
# Build the executor-benchmark
cargo build --release --bin aptos-executor-benchmark

# Create a test database with sharding enabled but no pruners
./target/release/aptos-executor-benchmark \
  --enable-storage-sharding \
  create-db \
  --data-dir /tmp/test-sharding \
  --num-accounts 100000

# Run the benchmark - this will cause unbounded growth
./target/release/aptos-executor-benchmark \
  --enable-storage-sharding \
  run-executor \
  --blocks 10000 \
  --block-size 1000 \
  --data-dir /tmp/test-sharding \
  --checkpoint-dir /tmp/test-checkpoint

# Monitor disk usage growth across shards
watch -n 5 'du -h /tmp/test-sharding/state_kv_db/shard_*'

# Observe: Each shard grows continuously without any pruning
# After sufficient blocks, disk space will be exhausted
```

To verify the issue, compare with the same benchmark run with pruning enabled:

```bash
# Run with pruning enabled - disk usage will stabilize
./target/release/aptos-executor-benchmark \
  --enable-storage-sharding \
  --enable-ledger-pruner \
  run-executor \
  --blocks 10000 \
  --block-size 1000 \
  --data-dir /tmp/test-sharding-pruned \
  --checkpoint-dir /tmp/test-checkpoint-pruned
```

### Citations

**File:** execution/executor-benchmark/src/main.rs (L67-94)
```rust
struct PrunerOpt {
    #[clap(long)]
    enable_state_pruner: bool,

    #[clap(long)]
    enable_epoch_snapshot_pruner: bool,

    #[clap(long)]
    enable_ledger_pruner: bool,

    #[clap(long, default_value_t = 100000)]
    state_prune_window: u64,

    #[clap(long, default_value_t = 100000)]
    epoch_snapshot_prune_window: u64,

    #[clap(long, default_value_t = 100000)]
    ledger_prune_window: u64,

    #[clap(long, default_value_t = 500)]
    ledger_pruning_batch_size: usize,

    #[clap(long, default_value_t = 500)]
    state_pruning_batch_size: usize,

    #[clap(long, default_value_t = 500)]
    epoch_snapshot_pruning_batch_size: usize,
}
```

**File:** execution/executor-benchmark/src/main.rs (L119-139)
```rust
#[derive(Debug, Parser)]
struct StorageOpt {
    #[clap(flatten)]
    pruner_opt: PrunerOpt,

    #[clap(long)]
    enable_storage_sharding: bool,

    #[clap(long)]
    enable_indexer_grpc: bool,
}

impl StorageOpt {
    fn storage_test_config(&self) -> StorageTestConfig {
        StorageTestConfig {
            pruner_config: self.pruner_opt.pruner_config(),
            enable_storage_sharding: self.enable_storage_sharding,
            enable_indexer_grpc: self.enable_indexer_grpc,
        }
    }
}
```

**File:** storage/aptosdb/src/state_kv_db.rs (L107-125)
```rust
        let state_kv_db_shards = (0..NUM_STATE_SHARDS)
            .into_par_iter()
            .map(|shard_id| {
                let shard_root_path = db_paths.state_kv_db_shard_root_path(shard_id);
                let db = Self::open_shard(
                    shard_root_path,
                    shard_id,
                    &state_kv_db_config,
                    env,
                    block_cache,
                    readonly,
                    /* is_hot = */ false,
                )
                .unwrap_or_else(|e| panic!("Failed to open state kv db shard {shard_id}: {e:?}."));
                Arc::new(db)
            })
            .collect::<Vec<_>>()
            .try_into()
            .unwrap();
```

**File:** storage/aptosdb/src/schema/state_value_by_key_hash/mod.rs (L28-35)
```rust
type Key = (HashValue, Version);

define_schema!(
    StateValueByKeyHashSchema,
    Key,
    Option<StateValue>,
    STATE_VALUE_BY_KEY_HASH_CF_NAME
);
```

**File:** storage/aptosdb/src/schema/stale_state_value_index_by_key_hash/mod.rs (L7-16)
```rust
//! An index entry in this data set has 3 pieces of information:
//!     1. The version since which a state value (in another data set) becomes stale, meaning,
//! replaced by an updated value.
//!     2. The version this state value was updated identified by the state key.
//!     3. The state_key to identify the stale state value.
//!
//! ```text
//! |<-------------------key------------------------>|
//! | stale_since_version | version | state_key_hash |
//! ```
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_pruner_manager.rs (L84-92)
```rust
    pub fn new(state_kv_db: Arc<StateKvDb>, state_kv_pruner_config: LedgerPrunerConfig) -> Self {
        let pruner_worker = if state_kv_pruner_config.enable {
            Some(Self::init_pruner(
                Arc::clone(&state_kv_db),
                state_kv_pruner_config,
            ))
        } else {
            None
        };
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_pruner_manager.rs (L106-117)
```rust
    pub fn new(
        state_merkle_db: Arc<StateMerkleDb>,
        state_merkle_pruner_config: StateMerklePrunerConfig,
    ) -> Self {
        let pruner_worker = if state_merkle_pruner_config.enable {
            Some(Self::init_pruner(
                Arc::clone(&state_merkle_db),
                state_merkle_pruner_config,
            ))
        } else {
            None
        };
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/state_kv_shard_pruner.rs (L47-72)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
    ) -> Result<()> {
        let mut batch = SchemaBatch::new();

        let mut iter = self
            .db_shard
            .iter::<StaleStateValueIndexByKeyHashSchema>()?;
        iter.seek(&current_progress)?;
        for item in iter {
            let (index, _) = item?;
            if index.stale_since_version > target_version {
                break;
            }
            batch.delete::<StaleStateValueIndexByKeyHashSchema>(&index)?;
            batch.delete::<StateValueByKeyHashSchema>(&(index.state_key_hash, index.version))?;
        }
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateKvShardPrunerProgress(self.shard_id),
            &DbMetadataValue::Version(target_version),
        )?;

        self.db_shard.write_schemas(batch)
    }
```

**File:** config/src/config/storage_config.rs (L682-798)
```rust
impl ConfigSanitizer for StorageConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.storage;

        let ledger_prune_window = config
            .storage_pruner_config
            .ledger_pruner_config
            .prune_window;
        let state_merkle_prune_window = config
            .storage_pruner_config
            .state_merkle_pruner_config
            .prune_window;
        let epoch_snapshot_prune_window = config
            .storage_pruner_config
            .epoch_snapshot_pruner_config
            .prune_window;
        let user_pruning_window_offset = config
            .storage_pruner_config
            .ledger_pruner_config
            .user_pruning_window_offset;

        if ledger_prune_window < 50_000_000 {
            warn!("Ledger prune_window is too small, harming network data availability.");
        }
        if state_merkle_prune_window < 100_000 {
            warn!("State Merkle prune_window is too small, node might stop functioning.");
        }
        if epoch_snapshot_prune_window < 50_000_000 {
            warn!("Epoch snapshot prune_window is too small, harming network data availability.");
        }
        if user_pruning_window_offset > 1_000_000 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset too large, so big a buffer is unlikely necessary. Set something < 1 million.".to_string(),
            ));
        }
        if user_pruning_window_offset > ledger_prune_window {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset is larger than the ledger prune window, the API will refuse to return any data.".to_string(),
            ));
        }

        if let Some(db_path_overrides) = config.db_path_overrides.as_ref() {
            if !config.rocksdb_configs.enable_storage_sharding {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "db_path_overrides is allowed only if sharding is enabled.".to_string(),
                ));
            }

            if let Some(ledger_db_path) = db_path_overrides.ledger_db_path.as_ref() {
                if !ledger_db_path.is_absolute() {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        format!(
                            "Path {ledger_db_path:?} in db_path_overrides is not an absolute path."
                        ),
                    ));
                }
            }

            if let Some(state_kv_db_path) = db_path_overrides.state_kv_db_path.as_ref() {
                if let Some(metadata_path) = state_kv_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = state_kv_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }

            if let Some(state_merkle_db_path) = db_path_overrides.state_merkle_db_path.as_ref() {
                if let Some(metadata_path) = state_merkle_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = state_merkle_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }

            if let Some(hot_state_merkle_db_path) =
                db_path_overrides.hot_state_merkle_db_path.as_ref()
            {
                if let Some(metadata_path) = hot_state_merkle_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = hot_state_merkle_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }
        }

        Ok(())
    }
```

**File:** .github/workflows/workflow-run-execution-performance.yaml (L1-50)
```yaml
name: "*run execution-performance reusable workflow"

on:
  # This allows the workflow to be triggered from another workflow
  workflow_call:
    inputs:
      GIT_SHA:
        required: true
        type: string
        description: The git SHA1 to test.
      RUNNER_NAME:
        required: false
        default: executor-benchmark-runner 
        type: string
      FLOW:
        required: false
        default: CONTINUOUS
        type: string
        description: Which set of tests to run.
      IGNORE_TARGET_DETERMINATION:
        required: false
        default: false
        type: boolean
        description: Ignore target determination and run the tests
      SKIP_MOVE_E2E:
        required: false
        default: false
        type: boolean
        description: Whether to run or skip move-only e2e tests at the beginning.
      USE_COIN_APT:
        required: false
        default: false
        type: boolean
        description: By default, FA APT is exclusively used. If set Coin APT is used instead.
      SOURCE:
        required: false
        default: CI
        type: string
      NUMBER_OF_EXECUTION_THREADS:
        required: false
        default: "32"
        type: string
  # This allows the workflow to be triggered manually from the Github UI or CLI
  # NOTE: because the "number" type is not supported, we default to 720 minute timeout
  workflow_dispatch:
    inputs:
      GIT_SHA:
        required: true
        type: string
        description: The git SHA1 to test.
```
