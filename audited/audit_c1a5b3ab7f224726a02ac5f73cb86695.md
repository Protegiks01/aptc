# Audit Report

## Title
Hardcoded 0.0.0.0 Bind Address in Indexer-GRPC Health Endpoints Exposes Metrics and Profiling to External Networks

## Summary
The `register_probes_and_metrics_handler()` function in the indexer-grpc-server-framework hardcodes the bind address to `0.0.0.0` (all network interfaces) without providing any configuration option to restrict it to localhost or specific interfaces. This unconditionally exposes health, metrics, and CPU profiling endpoints to external networks, creating information disclosure and potential denial-of-service vectors. [1](#0-0) [2](#0-1) 

## Finding Description

The indexer-grpc-server-framework provides a common server bootstrap for multiple Aptos indexer services including the data service, cache worker, file store, and gateway. When these services start, the framework spawns a separate task that registers health and metrics endpoints via `register_probes_and_metrics_handler()`. [3](#0-2) 

This function exposes four endpoints on the configured `health_check_port`:
1. `/readiness` - Returns HTTP 200 with "ready" status
2. `/metrics` - Exposes Prometheus metrics via the TextEncoder
3. `/` - Status page (service-specific implementation)
4. `/profilez` - CPU profiling endpoint (Linux only) [4](#0-3) 

The critical security issue is that both the Linux and non-Linux code paths bind unconditionally to `[0, 0, 0, 0]` (equivalent to `0.0.0.0`), making these endpoints accessible from any network interface. Operators cannot configure this to bind to `127.0.0.1` or a specific private interface.

**Contrast with Other Aptos Services:**

The Aptos inspection service, which serves similar diagnostic endpoints for validator and fullnode operations, properly implements a configurable bind address: [5](#0-4) [6](#0-5) 

The inspection service reads the `address` field from configuration, allowing operators to restrict binding: [7](#0-6) 

**Attack Vectors:**

1. **Information Disclosure via /metrics**: The metrics endpoint exposes operational data that could reveal:
   - Request rates and patterns
   - Error rates and types
   - System resource utilization
   - Internal service state [8](#0-7) 

2. **CPU Profiling Abuse via /profilez**: The profiling endpoint allows remote triggering of CPU profiling for configurable duration: [9](#0-8) 

While mutex-protected to prevent concurrent profiling, an attacker can repeatedly invoke this endpoint as soon as one profiling operation completes, causing performance degradation: [10](#0-9) 

The profiling data exposes internal code execution patterns that could aid in discovering other vulnerabilities. [11](#0-10) 

**Affected Services:**

Multiple production indexer services use this framework and are affected:
- `indexer-grpc-data-service` [12](#0-11) 
- `indexer-grpc-cache-worker`
- `indexer-grpc-file-store`
- `indexer-grpc-gateway`
- `indexer-grpc-manager`

Each service can independently configure its main GRPC listener address, but the health endpoints remain exposed on all interfaces: [13](#0-12) 

## Impact Explanation

This issue constitutes a **security hardening deficiency** but does **NOT** meet the Aptos Bug Bounty Medium severity threshold. Based on the official severity criteria:

- **NOT Critical**: Does not affect consensus, funds, validator operations, or cause network partition
- **NOT High**: Indexers are not validator nodes; "validator node slowdowns" criteria does not apply
- **NOT Medium**: No "limited funds loss" or "state inconsistencies requiring intervention"
- **IS Low**: Qualifies as "minor information leaks" and "non-critical implementation bug"

The actual impact is limited to:
1. **Information Disclosure**: Operational metrics and profiling data exposure
2. **Performance Degradation**: Potential for profiling-based resource consumption (though network-level DoS is out of scope per bug bounty rules)
3. **Defense-in-Depth Violation**: Inconsistent with security best practices established elsewhere in Aptos codebase

**Important Context**: Indexer services are typically public-facing infrastructure designed to serve blockchain data queries. Unlike validator nodes, they do not participate in consensus or handle private keys. The exposed metrics do not directly compromise blockchain security invariants.

## Likelihood Explanation

**HIGH likelihood** of exposure in production deployments:

1. The default configuration binds to all interfaces by design
2. No configuration option exists to change this behavior
3. Kubernetes deployments commonly expose service ports via LoadBalancers or Ingress
4. Operators may not realize health endpoints are network-accessible when they configure the main GRPC service to bind privately
5. Port scanning would quickly discover the `health_check_port`

**LOW-to-MEDIUM likelihood** of exploitation impact:

1. Indexer compromise does not affect blockchain consensus or funds
2. Most production environments use network-level controls (firewalls, security groups)
3. The information disclosed is operational, not cryptographic or financial
4. Profiling DoS is rate-limited by the mutex lock

## Recommendation

Add a configurable bind address field to the `GenericConfig` struct, following the pattern established by the inspection service:

```rust
#[derive(Deserialize, Clone, Debug, Serialize)]
pub struct GenericConfig<T> {
    pub health_check_port: u16,
    #[serde(default = "GenericConfig::<T>::default_health_check_address")]
    pub health_check_address: String,
    pub server_config: T,
}

impl<T> GenericConfig<T> {
    fn default_health_check_address() -> String {
        "0.0.0.0".to_string()
    }
}
```

Update the bind calls to use the configured address:

```rust
async fn register_probes_and_metrics_handler<C>(config: GenericConfig<C>, port: u16)
where
    C: RunnableConfig,
{
    // ... endpoint setup ...
    
    let addr: SocketAddr = (config.health_check_address.as_str(), port)
        .to_socket_addrs()
        .expect("Failed to parse health check address")
        .next()
        .unwrap();
    
    warp::serve(routes).run(addr).await;
}
```

**Default to `0.0.0.0`** for backward compatibility with containerized deployments, but allow operators to set `health_check_address: "127.0.0.1"` in their YAML configuration when stronger isolation is required.

Additionally, consider:
1. Documenting the security implications of exposing these endpoints
2. Adding authentication/authorization for the `/profilez` endpoint
3. Implementing rate limiting for profiling requests beyond the mutex

## Proof of Concept

**Step 1**: Deploy an indexer service with the framework:
```yaml
# config.yaml
health_check_port: 8080
server_config:
  # ... service-specific config ...
```

**Step 2**: Verify external accessibility:
```bash
# From external host
curl http://<indexer-host>:8080/metrics
# Returns Prometheus metrics

curl http://<indexer-host>:8080/profilez?seconds=30&frequency=99
# Triggers 30-second CPU profiling, returns flamegraph
```

**Step 3**: Demonstrate information leakage:
```bash
# Scrape metrics continuously
while true; do
  curl -s http://<indexer-host>:8080/metrics | grep -E "(request_count|error_rate|memory_usage)"
  sleep 5
done
```

**Step 4**: Demonstrate profiling abuse (Linux only):
```bash
# Repeatedly trigger profiling
for i in {1..10}; do
  curl -o profile_${i}.svg http://<indexer-host>:8080/profilez?seconds=10
  # Wait for mutex release
  sleep 11
done
```

The profiling data reveals internal function call patterns and execution hotspots that could inform further attacks.

## Notes

**Severity Reassessment**: While the question categorizes this as "Medium" severity, according to the strict Aptos Bug Bounty criteria, this issue more accurately fits **Low severity** as it constitutes:
- A minor information leak (operational metrics)
- A non-critical implementation bug (missing configuration option)

It does NOT involve funds, consensus, state manipulation, or significant protocol violations as required for Medium+ severity.

**Mitigations**: Production deployments should implement network-level controls (security groups, firewall rules, Kubernetes NetworkPolicies) to restrict access to health check ports regardless of this code issue. The lack of code-level configurability represents a defense-in-depth gap rather than a critical vulnerability.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L50-56)
```rust
    let health_port = config.health_check_port;
    // Start liveness and readiness probes.
    let config_clone = config.clone();
    let task_handler = tokio::spawn(async move {
        register_probes_and_metrics_handler(config_clone, health_port).await;
        anyhow::Ok(())
    });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L200-223)
```rust
    let readiness = warp::path("readiness")
        .map(move || warp::reply::with_status("ready", warp::http::StatusCode::OK));

    let metrics_endpoint = warp::path("metrics").map(|| {
        // Metrics encoding.
        let metrics = aptos_metrics_core::gather();
        let mut encode_buffer = vec![];
        let encoder = TextEncoder::new();
        // If metrics encoding fails, we want to panic and crash the process.
        encoder
            .encode(&metrics, &mut encode_buffer)
            .context("Failed to encode metrics")
            .unwrap();

        Response::builder()
            .header("Content-Type", "text/plain")
            .body(encode_buffer)
    });

    let status_endpoint = warp::path::end().and_then(move || {
        let config = config.clone();
        async move { config.status_page().await }
    });

```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L226-249)
```rust
        let profilez = warp::path("profilez").and_then(|| async move {
            // TODO(grao): Consider make the parameters configurable.
            Ok::<_, Infallible>(match start_cpu_profiling(10, 99, false).await {
                Ok(body) => {
                    let response = Response::builder()
                        .header("Content-Length", body.len())
                        .header("Content-Disposition", "inline")
                        .header("Content-Type", "image/svg+xml")
                        .body(body);

                    match response {
                        Ok(res) => warp::reply::with_status(res, warp::http::StatusCode::OK),
                        Err(e) => warp::reply::with_status(
                            Response::new(format!("Profiling failed: {e:?}.").as_bytes().to_vec()),
                            warp::http::StatusCode::INTERNAL_SERVER_ERROR,
                        ),
                    }
                },
                Err(e) => warp::reply::with_status(
                    Response::new(format!("Profiling failed: {e:?}.").as_bytes().to_vec()),
                    warp::http::StatusCode::INTERNAL_SERVER_ERROR,
                ),
            })
        });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L257-257)
```rust
        .run(([0, 0, 0, 0], port))
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L261-261)
```rust
            .run(([0, 0, 0, 0], port))
```

**File:** config/src/config/inspection_service_config.rs (L17-24)
```rust
pub struct InspectionServiceConfig {
    pub address: String,
    pub port: u16,
    pub expose_configuration: bool,
    pub expose_identity_information: bool,
    pub expose_peer_information: bool,
    pub expose_system_information: bool,
}
```

**File:** config/src/config/inspection_service_config.rs (L26-37)
```rust
impl Default for InspectionServiceConfig {
    fn default() -> InspectionServiceConfig {
        InspectionServiceConfig {
            address: "0.0.0.0".to_string(),
            port: 9101,
            expose_configuration: false,
            expose_identity_information: true,
            expose_peer_information: true,
            expose_system_information: true,
        }
    }
}
```

**File:** crates/aptos-inspection-service/src/server/mod.rs (L55-69)
```rust
    // Fetch the service port and address
    let service_port = node_config.inspection_service.port;
    let service_address = node_config.inspection_service.address.clone();

    // Create the inspection service socket address
    let address: SocketAddr = (service_address.as_str(), service_port)
        .to_socket_addrs()
        .unwrap_or_else(|_| {
            panic!(
                "Failed to parse {}:{} as address",
                service_address, service_port
            )
        })
        .next()
        .unwrap();
```

**File:** crates/aptos-system-utils/src/profiling.rs (L80-98)
```rust
pub async fn start_cpu_profiling(
    seconds: u64,
    frequency: i32,
    use_proto: bool,
) -> anyhow::Result<Vec<u8>> {
    info!(
        seconds = seconds,
        frequency = frequency,
        use_proto = use_proto,
        "Starting cpu profiling."
    );
    let lock = CPU_PROFILE_MUTEX.try_lock();
    ensure!(lock.is_some(), "A profiling task is already running.");

    // TODO(grao): Consolidate the code with aptos-profiler crate.
    let guard = pprof::ProfilerGuard::new(frequency)
        .map_err(|e| anyhow!("Failed to start cpu profiling: {e:?}."))?;

    tokio::time::sleep(Duration::from_secs(seconds)).await;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/main.rs (L15-16)
```rust
    let args = ServerArgs::parse();
    args.run::<IndexerGrpcDataServiceConfig>().await
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/config.rs (L35-46)
```rust
    /// The address for the TLS GRPC server to listen on.
    pub data_service_grpc_listen_address: SocketAddr,
    pub cert_path: String,
    pub key_path: String,
}

#[derive(Clone, Debug, Deserialize, Serialize)]
#[serde(deny_unknown_fields)]
pub struct NonTlsConfig {
    /// The address for the TLS GRPC server to listen on.
    pub data_service_grpc_listen_address: SocketAddr,
}
```
