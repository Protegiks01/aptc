# Audit Report

## Title
State Inconsistency and Data Loss in ChunkExecutor Due to Missing has_pending_pre_commit Flag Update

## Summary
The `ChunkExecutor` fails to update the `has_pending_pre_commit` flag when `pre_commit_ledger()` succeeds but `commit_ledger()` fails. This causes chunk data loss, state inconsistency between in-memory and database state, and bypasses the intended panic-on-error safety mechanism designed to protect consensus integrity.

## Finding Description

The vulnerability exists in the interaction between `commit_chunk_impl()` and the two-phase commit mechanism in `save_transactions()`. [1](#0-0) 

The `commit_chunk_impl()` function extracts the chunk from the queue using `next_chunk_to_commit()`, which uses `.take()` to move the chunk out of the `Option`, leaving `None` in its place: [2](#0-1) 

Then it calls `save_transactions()`, which performs a two-phase commit: [3](#0-2) 

The design intent documented in the code states that if pre-commit succeeds but commit fails, the node should panic: [4](#0-3) 

This panic mechanism is implemented via the `has_pending_pre_commit` flag: [5](#0-4) 

However, **the flag is never set to true during runtime**. It is only:
- Initialized from database state at startup
- Set to false after successful commit [6](#0-5) [7](#0-6) 

**Attack Scenario:**
1. `commit_chunk_impl()` extracts chunk via `next_chunk_to_commit()` (chunk moved, `None` left in queue)
2. `save_transactions()` calls `pre_commit_ledger()` which **succeeds** (database now has pre-committed data)
3. `save_transactions()` calls `commit_ledger()` which **fails** (disk error, corruption, resource exhaustion)
4. Error propagates back, `dequeue_committed()` is never called
5. Chunk data is dropped (permanently lost)
6. `has_pending_pre_commit` remains `false` (should be `true`!)
7. No panic occurs (should panic per design!)
8. Subsequent `next_chunk_to_commit()` calls return "already been processed" error
9. System continues with inconsistent state: database has orphaned pre-committed data, in-memory flag says otherwise

This violates the **State Consistency** invariant: the database has pre-committed data that is not tracked by the executor, and the chunk cannot be re-committed because its data is lost.

## Impact Explanation

This is a **CRITICAL** severity vulnerability per the Aptos bug bounty criteria:

1. **Consensus/Safety Violation**: Different validator nodes may end up in different states if some succeed at `commit_ledger()` while others fail, violating deterministic execution and potentially causing chain splits.

2. **State Inconsistency**: The database contains pre-committed data that the executor doesn't track, breaking atomic state transitions and Merkle proof verifiability.

3. **Data Loss**: The chunk being committed is permanently lost and cannot be recovered or retried, potentially leading to missing transactions in the ledger.

4. **Safety Mechanism Bypass**: The design explicitly states that pre-commit without commit should cause a panic/restart, but the bug allows the system to continue in an inconsistent state instead.

This meets the Critical Severity criteria of "Consensus/Safety violations" and "State inconsistencies requiring intervention" (potentially requiring a hardfork to resolve if multiple nodes diverge).

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability can be triggered by:
1. **Disk failures** during the `commit_ledger()` operation (I/O errors, out of space)
2. **Database corruption** that prevents commit but allows pre-commit
3. **Resource exhaustion** (memory, file descriptors) at the critical moment
4. **Crash/kill signal** received between pre-commit and commit operations

These are realistic scenarios in production environments, especially under high load or during infrastructure issues. The vulnerability does not require any attacker-specific privileges or insider access - it can occur naturally or be induced through resource exhaustion attacks.

## Recommendation

The `has_pending_pre_commit` flag must be set to `true` immediately after `pre_commit_ledger()` succeeds but before `commit_ledger()` is attempted. Additionally, the chunk should not be extracted from the queue until the entire `save_transactions()` operation succeeds.

**Recommended Fix:**

```rust
// In commit_chunk_impl(), change the flow to:

fn commit_chunk_impl(&self) -> Result<ExecutedChunk> {
    let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__total"]);
    
    // 1. First peek at the chunk without taking it
    let chunk = {
        let queue = self.commit_queue.lock();
        queue.to_commit
            .front()
            .ok_or_else(|| anyhow!("No chunk to commit."))?
            .as_ref()
            .ok_or_else(|| anyhow!("Next chunk to commit has already been processed."))?
            .clone() // Clone instead of take
    };

    let output = chunk.output.expect_complete_result();
    let num_txns = output.num_transactions_to_commit();
    
    if chunk.ledger_info_opt.is_some() || num_txns != 0 {
        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__save_txns"]);
        
        // 2. Before save_transactions, prepare to track pre-commit
        self.db.writer.save_transactions(
            output.as_chunk_to_commit(),
            chunk.ledger_info_opt.as_ref(),
            false,
        )?;
    }

    // 3. Only after successful save, take the chunk and dequeue
    let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__dequeue_and_return"]);
    self.commit_queue.lock().next_chunk_to_commit()?; // Now take it
    self.commit_queue.lock().dequeue_committed()?;

    Ok(chunk)
}
```

Alternatively, modify `save_transactions()` to accept a callback that sets `has_pending_pre_commit = true` after `pre_commit_ledger()` succeeds.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    
    #[test]
    fn test_partial_commit_failure_causes_inconsistency() {
        // Setup: Initialize ChunkExecutor with mock database
        let (mock_db, db_reader_writer) = create_mock_db_with_fail_on_commit();
        let executor = ChunkExecutor::<MockVM>::new(db_reader_writer);
        executor.reset().unwrap();
        
        // Enqueue a chunk for commit
        let chunk = create_test_chunk(/* ... */);
        executor.enqueue_chunk_by_execution(chunk, &li, None).unwrap();
        executor.update_ledger().unwrap();
        
        // First commit attempt - pre_commit succeeds, commit fails
        let result = executor.commit_chunk();
        assert!(result.is_err()); // Should fail on commit_ledger
        
        // Verify inconsistent state:
        // 1. Database has pre-committed data
        assert!(mock_db.has_pre_committed_data());
        
        // 2. has_pending_pre_commit flag is FALSE (should be TRUE!)
        assert_eq!(executor.inner.read().as_ref().unwrap()
            .has_pending_pre_commit.load(Ordering::Acquire), false);
        
        // 3. Chunk is lost from queue
        let result2 = executor.commit_chunk();
        assert!(result2.is_err());
        assert!(result2.unwrap_err().to_string().contains("already been processed"));
        
        // 4. No panic occurred (should have panicked!)
        // System continues in inconsistent state
    }
}
```

The PoC demonstrates that after a partial commit failure, the system enters an inconsistent state where the database has pre-committed data but the executor doesn't track it, the chunk is lost, and no panic occurs to force recovery.

### Citations

**File:** execution/executor/src/chunk_executor/mod.rs (L96-106)
```rust
        let has_pending_pre_commit = inner.has_pending_pre_commit.load(Ordering::Acquire);
        f(inner).map_err(|error| {
            if has_pending_pre_commit {
                panic!(
                    "Hit error with pending pre-committed ledger, panicking. {:?}",
                    error,
                );
            }
            error
        })
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L236-250)
```rust
    pub fn new(db: DbReaderWriter) -> Result<Self> {
        let commit_queue = ChunkCommitQueue::new_from_db(&db.reader)?;

        let next_pre_committed_version = commit_queue.expecting_version();
        let next_synced_version = db.reader.get_synced_version()?.map_or(0, |v| v + 1);
        assert!(next_synced_version <= next_pre_committed_version);
        let has_pending_pre_commit = next_synced_version < next_pre_committed_version;

        Ok(Self {
            db,
            commit_queue: Mutex::new(commit_queue),
            has_pending_pre_commit: AtomicBool::new(has_pending_pre_commit),
            _phantom: PhantomData,
        })
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L261-288)
```rust
    fn commit_chunk_impl(&self) -> Result<ExecutedChunk> {
        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__total"]);
        let chunk = {
            let _timer =
                CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__next_chunk_to_commit"]);
            self.commit_queue.lock().next_chunk_to_commit()?
        };

        let output = chunk.output.expect_complete_result();
        let num_txns = output.num_transactions_to_commit();
        if chunk.ledger_info_opt.is_some() || num_txns != 0 {
            let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__save_txns"]);
            // TODO(aldenhu): remove since there's no practical strategy to recover from this error.
            fail_point!("executor::commit_chunk", |_| {
                Err(anyhow::anyhow!("Injected error in commit_chunk"))
            });
            self.db.writer.save_transactions(
                output.as_chunk_to_commit(),
                chunk.ledger_info_opt.as_ref(),
                false, // sync_commit
            )?;
        }

        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__dequeue_and_return"]);
        self.commit_queue.lock().dequeue_committed()?;

        Ok(chunk)
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L394-409)
```rust
    fn commit_chunk(&self) -> Result<ChunkCommitNotification> {
        let _timer = COMMIT_CHUNK.start_timer();
        let executed_chunk = self.commit_chunk_impl()?;
        self.has_pending_pre_commit.store(false, Ordering::Release);

        let commit_notification = {
            let _timer =
                CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk__into_chunk_commit_notification"]);
            executed_chunk
                .output
                .expect_complete_result()
                .make_chunk_commit_notification()
        };

        Ok(commit_notification)
    }
```

**File:** execution/executor/src/chunk_executor/chunk_commit_queue.rs (L133-142)
```rust
    pub(crate) fn next_chunk_to_commit(&mut self) -> Result<ExecutedChunk> {
        let chunk_opt = self
            .to_commit
            .front_mut()
            .ok_or_else(|| anyhow!("No chunk to commit."))?;
        let chunk = chunk_opt
            .take()
            .ok_or_else(|| anyhow!("Next chunk to commit has already been processed."))?;
        Ok(chunk)
    }
```

**File:** storage/storage-interface/src/lib.rs (L608-628)
```rust
    fn save_transactions(
        &self,
        chunk: ChunkToCommit,
        ledger_info_with_sigs: Option<&LedgerInfoWithSignatures>,
        sync_commit: bool,
    ) -> Result<()> {
        // For reconfig suffix.
        if ledger_info_with_sigs.is_none() && chunk.is_empty() {
            return Ok(());
        }

        if !chunk.is_empty() {
            self.pre_commit_ledger(chunk.clone(), sync_commit)?;
        }
        let version_to_commit = if let Some(ledger_info_with_sigs) = ledger_info_with_sigs {
            ledger_info_with_sigs.ledger_info().version()
        } else {
            chunk.expect_last_version()
        };
        self.commit_ledger(version_to_commit, ledger_info_with_sigs, Some(chunk))
    }
```

**File:** storage/storage-interface/src/lib.rs (L630-641)
```rust
    /// Optimistically persist transactions to the ledger.
    ///
    /// Called by consensus to pre-commit blocks before execution result is agreed on by the
    /// validators.
    ///
    ///   If these blocks are later confirmed to be included in the ledger, commit_ledger should be
    ///       called with a `LedgerInfoWithSignatures`.
    ///   If not, the consensus needs to panic, resulting in a reboot of the node where the DB will
    ///       truncate the unconfirmed data.
    fn pre_commit_ledger(&self, chunk: ChunkToCommit, sync_commit: bool) -> Result<()> {
        unimplemented!()
    }
```
