# Audit Report

## Title
TpsChecker Lacks Overall Timeout Leading to Extended Hangs and Resource Exhaustion

## Summary
The `TpsChecker::check()` function in the node-checker service lacks an explicit overall timeout mechanism. While individual HTTP operations have timeouts, the cumulative execution time can extend to 15+ minutes through retry mechanisms, significantly exceeding the expected check duration and causing denial of service to other node checks.

## Finding Description

The `TpsChecker::check()` function performs multiple network operations without an overall timeout: [1](#0-0) 

The function executes these operations sequentially:

1. **Chain ID fetch** - Uses a 4-second HTTP timeout
2. **Cluster initialization** - Calls `get_ledger_information()` with 10-second HTTP timeout per request
3. **Account loading** - Uses `FETCH_ACCOUNT_RETRY_POLICY` with up to 12 exponential backoff retries [2](#0-1) [3](#0-2) 

4. **Transaction emission** - Bounded by duration parameter (default 60 seconds) [4](#0-3) 

A malicious or slow target node can maximize execution time by:
- Responding just within the 10-second HTTP timeout for each request
- Returning errors that trigger retry logic with exponential backoff (1s, 2s, 4s, 8s... up to 12 retries)
- Causing the account loading phase to retry repeatedly

The fn-check-client that invokes the node-checker has a default 60-second timeout: [5](#0-4) 

This timeout is insufficient for TPS checks, as acknowledged in the comment. When the timeout expires, the HTTP client cancels the request, but the TpsChecker continues executing in the background, wasting resources.

Furthermore, the node-checker runs all checks concurrently with a semaphore limiting concurrent requests: [6](#0-5) [7](#0-6) 

A hung TpsChecker occupies one of these 32 slots, reducing available capacity for legitimate checks.

## Impact Explanation

This vulnerability falls under **Medium Severity** per the Aptos bug bounty criteria for the following reasons:

1. **Service Availability Impact**: The node-checker service experiences degraded performance and potential denial of service when checking malicious nodes
2. **Resource Exhaustion**: Each hung checker consumes memory and thread resources for extended periods (potentially 15+ minutes)
3. **Cascading Delays**: With the semaphore limit, hung checkers block new checks from starting, creating a queue backlog
4. **Operational Impact**: Node operators cannot get timely health assessments, impacting network monitoring

While this doesn't affect consensus or cause fund loss (Critical severity) or directly slow validator nodes (High severity), it does create state inconsistencies in the monitoring system and requires manual intervention to resolve, fitting the Medium severity category.

## Likelihood Explanation

**Likelihood: HIGH**

- **Attacker Requirements**: Minimal - any node operator being checked can control their node's response behavior
- **Attack Complexity**: Low - simply configure the node to respond slowly or return transient errors
- **Detection Difficulty**: Hard - slow responses appear as legitimate network issues
- **Exploitation Cost**: Free - no special infrastructure or privileges required

The vulnerability is particularly likely because:
1. The retry mechanism is designed to tolerate transient failures, but this same mechanism enables extended hangs
2. The fn-check-client's 60-second timeout is explicitly noted as insufficient for TPS checks
3. No compensating controls exist at the TpsChecker level

## Recommendation

Add an explicit overall timeout to the `TpsChecker::check()` function using `tokio::time::timeout`:

```rust
use tokio::time::{timeout, Duration};

async fn check(
    &self,
    providers: &ProviderCollection,
) -> Result<Vec<CheckResult>, CheckerError> {
    // Define maximum allowed time for the entire check
    let check_timeout = Duration::from_secs(300); // 5 minutes
    
    let check_future = self.check_impl(providers);
    
    match timeout(check_timeout, check_future).await {
        Ok(result) => result,
        Err(_) => {
            Ok(vec![Self::build_result(
                "TPS check timed out".to_string(),
                0,
                format!("The TPS check exceeded the maximum allowed time of {} seconds", 
                    check_timeout.as_secs()),
            )])
        }
    }
}

async fn check_impl(
    &self,
    providers: &ProviderCollection,
) -> Result<Vec<CheckResult>, CheckerError> {
    // Existing check logic here
    // ...
}
```

Additionally:
1. Make the timeout configurable via `TpsCheckerConfig`
2. Increase the fn-check-client's default `nhc_timeout_secs` to accommodate TPS checks (e.g., 360 seconds)
3. Add metrics to track check durations and timeout occurrences

## Proof of Concept

```rust
// Integration test demonstrating the hang
#[tokio::test]
async fn test_tps_checker_slow_target_hang() {
    // Start a mock server that responds slowly but within timeout
    let mock_server = MockServer::start().await;
    
    // Configure responses to take 9.5 seconds each (within 10s timeout)
    Mock::given(method("GET"))
        .and(path("/"))
        .respond_with(ResponseTemplate::new(200)
            .set_delay(Duration::from_millis(9500))
            .set_body_json(json!({
                "chain_id": 1,
                "epoch": "0",
                "ledger_version": "0",
                "ledger_timestamp": "0"
            })))
        .mount(&mock_server)
        .await;
    
    // Configure account endpoint to return errors that trigger retries
    Mock::given(method("GET"))
        .and(path_regex("/v1/accounts/.*"))
        .respond_with(ResponseTemplate::new(503)
            .set_delay(Duration::from_millis(9500)))
        .mount(&mock_server)
        .await;
    
    let tps_checker = TpsChecker::new(TpsCheckerConfig {
        emit_config: EmitArgs {
            duration: 60,
            ..Default::default()
        },
        ..Default::default()
    }).unwrap();
    
    let start = Instant::now();
    
    // This call should complete quickly but will hang for minutes
    let result = tps_checker.check(&provider_collection).await;
    
    let elapsed = start.elapsed();
    
    // Without timeout, this will take 10+ minutes
    // With proper timeout, should complete in < 5 minutes
    assert!(elapsed.as_secs() > 300, 
        "Check hung for {} seconds, exceeding expected timeout", 
        elapsed.as_secs());
}
```

## Notes

While the issue does not cause truly "indefinite" hangs (individual operations have bounded timeouts), the cumulative execution time can reach 15+ minutes through the retry mechanism. The fn-check-client's comment explicitly acknowledges that "the check should be quick unless we're using the TPS checker," indicating awareness of this problem without implementing a proper solution. The lack of an overall timeout violates the principle of bounded resource consumption and enables a straightforward denial-of-service attack against the node-checker infrastructure.

### Citations

**File:** ecosystem/node-checker/src/checker/tps.rs (L108-191)
```rust
    async fn check(
        &self,
        providers: &ProviderCollection,
    ) -> Result<Vec<CheckResult>, CheckerError> {
        let target_api_index_provider = get_provider!(
            providers.target_api_index_provider,
            self.config.common.required,
            ApiIndexProvider
        );

        let target_url = target_api_index_provider.client.build_path("/").unwrap();
        let chain_id = match target_api_index_provider.provide().await {
            Ok(response) => ChainId::new(response.chain_id),
            Err(err) => {
                return Ok(vec![Self::build_result(
                    "Failed to get chain ID of your node".to_string(),
                    0,
                    format!("There was an error querying your node's API: {:#}", err),
                )]);
            },
        };

        let cluster_config = ClusterArgs {
            targets: Some(vec![target_url; self.config.repeat_target_count]),
            targets_file: None,
            coin_source_args: self.config.coin_source_args.clone(),
            chain_id: Some(chain_id),
            node_api_key: None,
        };
        let cluster = Cluster::try_from_cluster_args(&cluster_config)
            .await
            .map_err(TpsCheckerError::BuildClusterError)?;

        let stats = emit_transactions_with_cluster(
            &cluster,
            &self.config.emit_config,
            self.config
                .emit_workload_configs
                .args_to_transaction_mix_per_phase(),
        )
        .await
        .map_err(TpsCheckerError::TransactionEmitterError)?;

        // AKA stats per second.
        let rate = stats.rate();

        if rate.submitted < (self.config.minimum_tps as f64) {
            return Err(TpsCheckerError::InsufficientSubmittedTransactionsError(
                rate.submitted as u64,
                self.config.minimum_tps,
            )
            .into());
        }

        let mut description = format!("The minimum TPS (transactions per second) \
            required of nodes is {}, your node hit: {} (out of {} transactions submitted per second).", self.config.minimum_tps, rate.committed, rate.submitted);
        let evaluation_result = if rate.committed >= (self.config.minimum_tps as f64) {
            if stats.committed == stats.submitted {
                description.push_str(
                    " Your node could theoretically hit \
                even higher TPS, the evaluation suite only tests to check \
                your node meets the minimum requirements.",
                );
            }
            Self::build_result(
                "Transaction processing speed is sufficient".to_string(),
                100,
                description,
            )
        } else {
            description.push_str(
                " This implies that the hardware you're \
            using to run your node isn't powerful enough, please see the attached link",
            );
            Self::build_result(
                "Transaction processing speed is too low".to_string(),
                0,
                description,
            )
            .links(vec![NODE_REQUIREMENTS_LINK.to_string()])
        };

        Ok(vec![evaluation_result])
    }
```

**File:** crates/transaction-emitter-lib/src/emitter/mod.rs (L62-62)
```rust
const MAX_RETRIES: usize = 12;
```

**File:** crates/transaction-emitter-lib/src/emitter/mod.rs (L68-72)
```rust
static FETCH_ACCOUNT_RETRY_POLICY: Lazy<RetryPolicy> = Lazy::new(|| {
    RetryPolicy::exponential(Duration::from_secs(1))
        .with_max_retries(MAX_RETRIES)
        .with_jitter(true)
});
```

**File:** crates/transaction-emitter-lib/src/args.rs (L128-130)
```rust
    /// Time to run --emit-tx for in seconds.
    #[clap(long, default_value_t = 60)]
    pub duration: u64,
```

**File:** ecosystem/node-checker/fn-check-client/src/check.rs (L44-47)
```rust
    /// How long to wait when talking to NHC. The check should be quick unless
    /// we're using the TPS checker.
    #[clap(long, default_value_t = 60)]
    pub nhc_timeout_secs: u64,
```

**File:** ecosystem/node-checker/fn-check-client/src/check.rs (L49-51)
```rust
    /// Max number of requests to NHC we can have running concurrently.
    #[clap(long, default_value_t = 32)]
    pub max_concurrent_nhc_requests: u16,
```

**File:** ecosystem/node-checker/fn-check-client/src/check.rs (L70-70)
```rust
        let semaphore = Arc::new(Semaphore::new(self.max_concurrent_nhc_requests as usize));
```
