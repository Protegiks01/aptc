# Audit Report

## Title
Race Condition in Backup Storage Allows Partial Metadata Reads Leading to Inconsistent State Restoration

## Summary
A race condition exists in the backup storage system where `open_for_read()` can read metadata files while `create_for_write()` is still writing them. This allows partial, incomplete backup metadata to be incorporated into node restoration, potentially causing state inconsistencies across nodes and consensus failures in disaster recovery scenarios.

## Finding Description

The `BackupStorage` trait implementations in both `LocalFs` and `CommandAdapter` lack synchronization mechanisms to prevent concurrent read/write access to the same files. This breaks the critical invariant that all validators must produce identical state roots.

**The Race Condition:**

In the LocalFs implementation, `save_metadata_lines()` creates and writes metadata files without any locking mechanism: [1](#0-0) 

The file becomes visible in the directory immediately after creation (via `create_new(true).open()`), but the actual write and flush operations happen afterward. Meanwhile, `list_metadata_files()` returns ALL files in the metadata directory: [2](#0-1) 

When verification or restoration operations call `sync_and_load()`, they download ALL listed metadata files concurrently: [3](#0-2) 

The metadata is then deserialized line-by-line as JSON: [4](#0-3) 

**Exploitation Path:**

1. Continuous backup runs as a StatefulSet, writing new metadata files via `save_metadata_lines()` [5](#0-4) 

2. Backup verification runs as a scheduled CronJob accessing the same storage: [6](#0-5) 

3. During concurrent execution:
   - Backup writes a multi-line metadata file (e.g., compacted transaction metadata with multiple transaction ranges)
   - After writing the first complete JSON line, but before writing subsequent lines
   - Verify lists metadata files and sees the new file
   - Verify reads the file via `open_for_read()`, getting only the first line
   - The partial metadata deserializes successfully (one valid JSON line)
   - Missing transaction ranges are silently omitted

4. If this occurs during actual node restoration (not just verification), the node incorporates incomplete state:
   - Missing transaction ranges
   - Incomplete epoch endings
   - Partial state snapshots

5. In a network-wide recovery scenario where multiple nodes restore from the same corrupted backup storage simultaneously:
   - Different nodes read metadata at different times
   - Each gets different partial transaction ranges depending on timing
   - Nodes restore to different state roots
   - Consensus cannot be achieved among the restored nodes
   - Network partition requires manual intervention or hard fork

**Unix/Linux File System Behavior:**

On Unix/Linux systems (where Aptos nodes run), there is no automatic file locking. When a file is opened for writing, other processes can simultaneously open it for reading and will see partial data. The `write_all()` operation may require multiple system calls for larger content, creating a race window where readers observe incomplete data.

## Impact Explanation

**Critical Severity** - This vulnerability can cause **"Non-recoverable network partition (requires hardfork)"** and **"Consensus/Safety violations"** per the Aptos bug bounty program.

In normal operations with single node restoration, the impact is limited to that node failing to sync (High severity). However, in disaster recovery scenarios where multiple validators must restore from backup simultaneously:

1. Each node may restore different transaction ranges based on read timing
2. State roots diverge across the validator set
3. Consensus cannot be achieved (violates safety invariant)
4. Requires manual intervention, re-restoration, or hard fork to recover

This directly violates the **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks" - if validators restore different transaction histories, they cannot maintain consensus.

## Likelihood Explanation

**Likelihood: Medium-High** in production deployments.

The vulnerability occurs naturally without requiring any attacker action:

1. **Continuous backup is standard deployment**: Production Aptos nodes run continuous backup as documented in the Kubernetes manifests [7](#0-6) 

2. **Automated verification runs regularly**: Backup verification is scheduled (typically daily) to ensure backup integrity

3. **Race window exists for multi-line metadata**: 
   - Single-line metadata files have small race windows (microseconds)
   - Compacted metadata files contain multiple entries, significantly larger windows [8](#0-7) 

4. **Disaster recovery triggers the critical path**: Network-wide outages requiring simultaneous restoration from backups, while rare, are precisely when this vulnerability causes maximum damage

## Recommendation

Implement file locking or atomic write operations to ensure readers cannot access files until they are completely written:

**Solution 1: Atomic Write with Rename**
```rust
async fn save_metadata_lines(
    &self,
    name: &ShellSafeName,
    lines: &[TextLine],
) -> Result<FileHandle> {
    let dir = self.metadata_dir();
    create_dir_all(&dir).await.err_notes(name)?;
    let content = lines.iter().map(|e| e.as_ref()).collect::<Vec<&str>>().join("");
    
    // Write to temporary file first
    let tmp_name = format!(".tmp_{}", name.as_ref());
    let tmp_path = dir.join(&tmp_name);
    let path = dir.join(name.as_ref());
    
    let mut file = OpenOptions::new()
        .write(true)
        .create_new(true)
        .open(&tmp_path)
        .await
        .err_notes(&tmp_path)?;
    
    file.write_all(content.as_bytes()).await.err_notes(&tmp_path)?;
    file.shutdown().await.err_notes(&tmp_path)?;
    
    // Atomically rename to final name
    tokio::fs::rename(&tmp_path, &path).await.err_notes(&path)?;
    
    let fh = PathBuf::from(Self::METADATA_DIR)
        .join(name.as_ref())
        .path_to_string()?;
    Ok(fh)
}
```

**Solution 2: File Locking (Alternative)**
Use `fs2` crate for advisory file locks on both read and write operations to prevent concurrent access.

**Solution 3: Write Completion Marker**
Store a completion marker file alongside metadata that is only created after the metadata file is fully written. Readers check for this marker before processing.

## Proof of Concept

```rust
#[tokio::test]
async fn test_concurrent_metadata_read_write_race() {
    use std::sync::Arc;
    use tokio::fs;
    use tempfile::TempDir;
    
    let temp_dir = TempDir::new().unwrap();
    let storage = Arc::new(LocalFs::new(temp_dir.path().to_path_buf()));
    
    // Prepare large metadata content (multiple transaction ranges)
    let mut metadata_lines = Vec::new();
    for i in 0..100 {
        let meta = Metadata::new_transaction_backup(
            i * 1000,
            (i + 1) * 1000 - 1,
            format!("manifest_{}.json", i),
        );
        metadata_lines.push(meta.to_text_line().unwrap());
    }
    
    let name: ShellSafeName = "test_metadata.meta".parse().unwrap();
    
    // Writer task - writes metadata file
    let storage_write = storage.clone();
    let lines_clone = metadata_lines.clone();
    let name_clone = name.clone();
    let write_handle = tokio::spawn(async move {
        storage_write.save_metadata_lines(&name_clone, &lines_clone).await.unwrap();
    });
    
    // Give writer a head start to create the file
    tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;
    
    // Reader task - lists and reads metadata files concurrently
    let storage_read = storage.clone();
    let read_handle = tokio::spawn(async move {
        let files = storage_read.list_metadata_files().await.unwrap();
        if !files.is_empty() {
            // File is visible but may not be fully written
            let content = fs::read_to_string(
                storage_read.dir.join(files[0].clone())
            ).await.unwrap();
            
            let lines: Vec<&str> = content.lines().collect();
            lines.len() // Return number of lines read
        } else {
            0
        }
    });
    
    write_handle.await.unwrap();
    let lines_read = read_handle.await.unwrap();
    
    // Race condition: reader may see fewer lines than written
    // Expected: 100 lines, but may see anywhere from 0 to 100 depending on timing
    println!("Lines read: {} (expected: 100)", lines_read);
    assert!(lines_read < 100, "Race condition occurred: partial read detected");
}
```

This test demonstrates the race by having concurrent writer and reader tasks. The reader can observe the file before writing completes, reading partial data. In production, this manifests as nodes restoring different transaction ranges from the same backup storage.

### Citations

**File:** storage/backup/backup-cli/src/storage/local_fs/mod.rs (L111-123)
```rust
    async fn list_metadata_files(&self) -> Result<Vec<FileHandle>> {
        let dir = self.metadata_dir();
        let rel_path = Path::new(Self::METADATA_DIR);

        let mut res = Vec::new();
        if path_exists(&dir).await {
            let mut entries = read_dir(&dir).await.err_notes(&dir)?;
            while let Some(entry) = entries.next_entry().await.err_notes(&dir)? {
                res.push(rel_path.join(entry.file_name()).path_to_string()?)
            }
        }
        Ok(res)
    }
```

**File:** storage/backup/backup-cli/src/storage/local_fs/mod.rs (L149-181)
```rust
    async fn save_metadata_lines(
        &self,
        name: &ShellSafeName,
        lines: &[TextLine],
    ) -> Result<FileHandle> {
        let dir = self.metadata_dir();
        create_dir_all(&dir).await.err_notes(name)?; // in case not yet created
        let content = lines
            .iter()
            .map(|e| e.as_ref())
            .collect::<Vec<&str>>()
            .join("");
        let path = dir.join(name.as_ref());
        let file = OpenOptions::new()
            .write(true)
            .create_new(true)
            .open(&path)
            .await;
        match file {
            Ok(mut f) => {
                f.write_all(content.as_bytes()).await.err_notes(&path)?;
                f.shutdown().await.err_notes(&path)?;
            },
            Err(e) if e.kind() == io::ErrorKind::AlreadyExists => {
                info!("File {} already exists, Skip", name.as_ref());
            },
            _ => bail!("Unexpected Error in saving metadata file {}", name.as_ref()),
        }
        let fh = PathBuf::from(Self::METADATA_DIR)
            .join(name.as_ref())
            .path_to_string()?;
        Ok(fh)
    }
```

**File:** storage/backup/backup-cli/src/metadata/cache.rs (L67-86)
```rust
async fn download_file(
    storage_ref: &dyn BackupStorage,
    file_handle: &FileHandle,
    local_tmp_file: &Path,
) -> Result<()> {
    tokio::io::copy(
        &mut storage_ref
            .open_for_read(file_handle)
            .await
            .err_notes(file_handle)?,
        &mut OpenOptions::new()
            .write(true)
            .create_new(true)
            .open(local_tmp_file)
            .await
            .err_notes(local_tmp_file)?,
    )
    .await
    .map_err(|e| anyhow!("Failed to download file: {}", e))?;
    Ok(())
```

**File:** storage/backup/backup-cli/src/metadata/cache.rs (L236-246)
```rust
impl<R: AsyncRead + Send + Unpin> LoadMetadataLines for R {
    async fn load_metadata_lines(&mut self) -> Result<Vec<Metadata>> {
        let mut buf = String::new();
        self.read_to_string(&mut buf)
            .await
            .err_notes((file!(), line!(), &buf))?;
        Ok(buf
            .lines()
            .map(serde_json::from_str::<Metadata>)
            .collect::<Result<_, serde_json::error::Error>>()?)
    }
```

**File:** terraform/helm/fullnode/templates/backup.yaml (L14-55)
```yaml
kind: StatefulSet
metadata:
  name: {{ include "backup.fullname" . }}-backup
  labels:
    {{- include "backup.labels" . | nindent 4 }}
    app.kubernetes.io/name: backup
spec:
  serviceName: {{ include "backup.fullname" . }}-backup
  replicas: 1
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      {{- include "backup.selectorLabels" . | nindent 6 }}
      app.kubernetes.io/name: backup
  template:
    metadata:
      labels:
        {{- include "backup.selectorLabels" . | nindent 8 }}
        app.kubernetes.io/name: backup
        {{- if or $.Values.chain.label $.Values.chain.name }}
        chain_name: {{ $.Values.chain.label | default $.Values.chain.name }}
        {{- end}}
      annotations:
        {{- if $.Values.metrics.destination }}
        aptos.dev/metrics-destination: {{ $.Values.metrics.destination }}
        {{- end}}
    spec:
      containers:
      - name: backup
        {{- if and $backup_statefulset (not $.Values.manageImages) }} # if the statefulset already exists and we do not want helm to simply overwrite the image, use the existing image
        image: {{ (first $backup_statefulset.spec.template.spec.containers).image }}
        {{- else }}
        image: {{ .Values.backup.image.repo }}:{{ .Values.backup.image.tag | default .Values.imageTag }}
        {{- end }}
        imagePullPolicy: {{ .Values.backup.image.pullPolicy }}
        resources:
          {{- toYaml .Values.backup.resources | nindent 10 }}
        command: ["/usr/local/bin/aptos-debugger"]
        args:
        - "aptos-db"
        - "backup"
        - "continuously"
```

**File:** terraform/helm/fullnode/templates/backup-verify.yaml (L1-54)
```yaml
{{- if $.Values.backup.enable }}
{{ $backup_verify_cronjob := lookup "batch/v1" "CronJob" $.Release.Namespace (print (include "backup.fullname" .) "-backup-verify")}}
apiVersion: batch/v1
kind: CronJob
metadata:
  name: {{ include "backup.fullname" . }}-backup-verify
  labels:
    {{- include "backup.labels" . | nindent 4 }}
    app.kubernetes.io/name: backup-verify
spec:
  concurrencyPolicy: Replace
  schedule: {{ .Values.backup_verify.schedule | quote }}
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            {{- include "backup.selectorLabels" . | nindent 12 }}
            app.kubernetes.io/name: backup-verify
            {{- if or $.Values.chain.label $.Values.chain.name }}
            chain_name: {{ $.Values.chain.label | default $.Values.chain.name }}
            {{- end}}
          annotations:
            {{- if $.Values.metrics.destination }}
            aptos.dev/metrics-destination: {{ $.Values.metrics.destination }}
            {{- end}}
        spec:
          restartPolicy: Never
          containers:
          - name: backup-verify
            {{- if and $backup_verify_cronjob (not $.Values.manageImages) }} # if the statefulset already exists and we do not want helm to simply overwrite the image, use the existing image
            image: {{ (first $backup_verify_cronjob.spec.jobTemplate.spec.template.spec.containers).image }}
            {{- else }}
            image: {{ .Values.backup.image.repo }}:{{ .Values.backup.image.tag | default .Values.imageTag }}
            {{- end }}
            imagePullPolicy: {{ .Values.backup.image.pullPolicy }}
            command:
            - /usr/local/bin/aptos-debugger
            - aptos-db
            - backup
            - verify
            {{- range $.Values.restore.config.trusted_waypoints }}
            - --trust-waypoint
            - {{ . }}
            {{- end }}
            - --metadata-cache-dir
            - /tmp/aptos-backup-verify-metadata
            {{- if .Values.backup_verify.config.concurrent_downloads }}
            - --concurrent-downloads
            - "{{ .Values.backup_verify.config.concurrent_downloads }}"
            {{- end }}
            - --command-adapter-config
            # use the same config with the backup sts
            - /opt/aptos/etc/{{ .Values.backup.config.location }}.yaml
```

**File:** storage/backup/backup-cli/src/coordinators/backup.rs (L114-135)
```rust
    pub async fn run(&self) -> Result<()> {
        // Connect to both the local node and the backup storage.
        let backup_state = metadata::cache::sync_and_load(
            &self.metadata_cache_opt,
            Arc::clone(&self.storage),
            self.concurrent_downloads,
        )
        .await?
        .get_storage_state()?;

        // On new DbState retrieved:
        // `watch_db_state` informs `backup_epoch_endings` via channel 1,
        // and the latter informs the other backup type workers via channel 2, after epoch
        // ending is properly backed up, if necessary. This way, the epoch ending LedgerInfo needed
        // for proof verification is always available in the same backup storage.
        let (tx1, rx1) = watch::channel::<Option<DbState>>(None);
        let (tx2, rx2) = watch::channel::<Option<DbState>>(None);

        // Schedule work streams.
        let watch_db_state = IntervalStream::new(interval(Duration::from_secs(1)))
            .then(|_| self.try_refresh_db_state(&tx1))
            .boxed_local();
```

**File:** storage/backup/backup-cli/src/coordinators/backup.rs (L409-449)
```rust
    pub async fn run(self) -> Result<()> {
        info!("Backup compaction started");
        // sync the metadata from backup storage
        let mut metaview = metadata::cache::sync_and_load(
            &self.metadata_cache_opt,
            Arc::clone(&self.storage),
            self.concurrent_downloads,
        )
        .await?;

        let files = metaview.get_file_handles();

        info!("Start compacting backup metadata files.");
        let mut new_files: HashSet<FileHandle> = HashSet::new(); // record overwrite file names
        for range in metaview.compact_epoch_ending_backups(self.epoch_ending_file_compact_factor)? {
            let (epoch_range, file_name) =
                Metadata::compact_epoch_ending_backup_range(range.to_vec())?;
            let file_handle = self
                .storage
                .save_metadata_lines(&file_name, epoch_range.as_slice())
                .await?;
            new_files.insert(file_handle);
        }
        for range in metaview.compact_transaction_backups(self.transaction_file_compact_factor)? {
            let (txn_range, file_name) =
                Metadata::compact_transaction_backup_range(range.to_vec())?;
            let file_handle = self
                .storage
                .save_metadata_lines(&file_name, txn_range.as_slice())
                .await?;
            new_files.insert(file_handle);
        }
        for range in metaview.compact_state_backups(self.state_snapshot_file_compact_factor)? {
            let (state_range, file_name) =
                Metadata::compact_statesnapshot_backup_range(range.to_vec())?;
            let file_handle = self
                .storage
                .save_metadata_lines(&file_name, state_range.as_slice())
                .await?;
            new_files.insert(file_handle);
        }
```
