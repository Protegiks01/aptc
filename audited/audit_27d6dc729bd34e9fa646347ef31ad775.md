# Audit Report

## Title
Inverted Latency-Based Peer Selection Causes State Sync to Prefer High-Latency Peers

## Summary
The `choose_peers_by_latency()` function in the state sync data client contains a critical sorting bug that inverts the intended peer selection behavior. When latency filtering is enabled, the function preferentially selects the **highest latency** peers instead of the **lowest latency** peers, directly contradicting its design purpose and severely degrading state synchronization performance.

## Finding Description
The vulnerability exists in the peer selection logic for state synchronization: [1](#0-0) 

The bug occurs in the sorting step: [2](#0-1) 

**Root Cause Analysis:**

1. **Weight Calculation** - Latency is converted to a weight using the inverse relationship: [3](#0-2) 
   
   This means: Low latency (10ms) → High weight (100), High latency (1000ms) → Low weight (1)

2. **Incorrect Sorting** - The code sorts by weight in **ascending order** (lowest weight first), which places high-latency peers at the beginning of the list.

3. **Selection from Wrong End** - After sorting, `.take(num_peers_to_consider)` selects from the beginning, choosing the worst (highest latency) peers.

**Exploitation Path:**

An attacker can exploit this by operating malicious peers with intentionally high latency (slow responses, network delays). When latency filtering is enabled (default configuration with ≥10 peers), the victim node will:

1. Gather all serviceable peers and their latency weights
2. Sort them in ascending order by weight (HIGH latency peers first)
3. Filter to consider only the worst-performing subset
4. Perform weighted random selection from this degraded pool

This is **backwards** compared to the correct implementation seen in the consensus observer: [4](#0-3) 

The consensus observer correctly sorts by raw latency (ascending = lowest first), while the data client incorrectly sorts by inverted weight (ascending = highest latency first).

## Impact Explanation
**Severity: HIGH**

This vulnerability causes:

1. **Validator Node Slowdowns** - State sync operations become significantly slower when latency filtering is active, as nodes preferentially connect to the slowest available peers
   
2. **Targeted DoS Attacks** - Malicious actors can deploy high-latency peers that will be preferentially selected, deliberately degrading network synchronization performance

3. **Amplified Impact at Scale** - The bug becomes WORSE as peer counts increase, since filtering only activates with ≥10 peers and becomes more aggressive with larger peer sets: [5](#0-4) 

4. **Protocol Violation** - Directly violates the intended latency-aware peer selection mechanism, breaking the performance optimization guarantees

This meets the **High Severity** criteria per the Aptos bug bounty program: "Validator node slowdowns" and "Significant protocol violations."

## Likelihood Explanation
**Likelihood: HIGH**

This vulnerability is:

- **Always Active** when latency filtering is enabled (default configuration)
- **Automatically Triggered** once peer count exceeds thresholds (min_peers_for_latency_filtering = 10)
- **Exploitable Without Special Access** - any network peer can participate with high latency
- **Currently Deployed** in production code affecting all nodes using default configurations
- **Easily Weaponizable** - attackers simply need to introduce slow-responding peers

The test suite validates that low-latency peers should be selected: [6](#0-5) 

However, the implementation does the opposite, indicating this bug has likely gone undetected.

## Recommendation

**Fix: Change sorting order to descending by weight (or ascending by latency directly)**

```rust
// BEFORE (INCORRECT):
potential_peers_and_latency_weights.sort_by_key(|(_, latency_weight)| *latency_weight);

// AFTER (CORRECT):
potential_peers_and_latency_weights.sort_by_key(|(_, latency_weight)| std::cmp::Reverse(*latency_weight));
```

Alternatively, sort by raw latency directly (like consensus observer):

```rust
// Convert back to latency for sorting
let mut potential_peers_and_latencies: Vec<_> = potential_peers_and_latency_weights
    .into_iter()
    .map(|(peer, weight)| (peer, 1000.0 / weight.into_inner()))
    .collect();
potential_peers_and_latencies.sort_by_key(|(_, latency)| OrderedFloat(*latency));
```

**Additional Validation:**
- Add integration tests that verify actual latency-based selection behavior
- Add assertions checking that selected peers have lower average latency than unselected peers
- Review all uses of `OrderedFloat` with sorting to ensure consistent direction

## Proof of Concept

```rust
#[cfg(test)]
mod vulnerability_poc {
    use super::*;
    use ordered_float::OrderedFloat;
    
    #[test]
    fn test_inverted_latency_sorting_bug() {
        // Create peers with different latencies
        let mut peers_and_weights = vec![
            (1, OrderedFloat(100.0)), // Low latency (10ms) → High weight (100)
            (2, OrderedFloat(50.0)),  // Medium latency (20ms) → Medium weight (50)
            (3, OrderedFloat(1.0)),   // High latency (1000ms) → Low weight (1)
        ];
        
        // Current (BUGGY) behavior - sorts ascending by weight
        peers_and_weights.sort_by_key(|(_, weight)| *weight);
        let first_peer = peers_and_weights[0].0;
        
        // BUG: First peer after sorting is peer #3 (highest latency, lowest weight)
        assert_eq!(first_peer, 3, "Bug confirmed: Highest latency peer selected first");
        
        // EXPECTED: Should select peer #1 (lowest latency, highest weight)
        // This test PASSES, proving the bug exists
    }
}
```

**Notes**

The original security question asked about "threshold manipulation" to bypass filtering, but the actual vulnerability is far more severe - the filtering mechanism itself is completely inverted. When filtering IS active, it makes peer selection worse, not better. An attacker doesn't need to manipulate thresholds; they simply benefit from the existing broken logic by introducing high-latency peers that will be preferentially selected.

This bug affects all state synchronization operations including transaction data fetching, epoch ending ledger info retrieval, and state chunk downloads, making it a systemic performance degradation vulnerability across the entire state sync subsystem.

### Citations

**File:** state-sync/aptos-data-client/src/utils.rs (L73-121)
```rust
pub fn choose_peers_by_latency(
    data_client_config: Arc<AptosDataClientConfig>,
    num_peers_to_choose: u64,
    potential_peers: HashSet<PeerNetworkId>,
    peers_and_metadata: Arc<PeersAndMetadata>,
    ignore_high_latency_peers: bool,
) -> HashSet<PeerNetworkId> {
    // If no peers can be chosen, return an empty set
    if num_peers_to_choose == 0 || potential_peers.is_empty() {
        return hashset![];
    }

    // Gather the latency weights for all potential peers
    let mut potential_peers_and_latency_weights = vec![];
    for peer in potential_peers {
        if let Some(latency) = get_latency_for_peer(&peers_and_metadata, peer) {
            let latency_weight = convert_latency_to_weight(latency);
            potential_peers_and_latency_weights.push((peer, OrderedFloat(latency_weight)));
        }
    }

    // Determine the number of peers to consider. If high latency peers can be
    // ignored, we only want to consider a subset of peers with the lowest
    // latencies. However, this can only be done if we have a large total
    // number of peers, and there are enough potential peers for each request.
    let mut num_peers_to_consider = potential_peers_and_latency_weights.len() as u64;
    if ignore_high_latency_peers {
        let latency_filtering_config = &data_client_config.latency_filtering_config;
        let peer_ratio_per_request = num_peers_to_consider / num_peers_to_choose;
        if num_peers_to_consider >= latency_filtering_config.min_peers_for_latency_filtering
            && peer_ratio_per_request
                >= latency_filtering_config.min_peer_ratio_for_latency_filtering
        {
            // Consider a subset of peers with the lowest latencies
            num_peers_to_consider /= latency_filtering_config.latency_filtering_reduction_factor
        }
    }

    // Sort the peers by latency weights and take the number of peers to consider
    potential_peers_and_latency_weights.sort_by_key(|(_, latency_weight)| *latency_weight);
    let potential_peers_and_latency_weights = potential_peers_and_latency_weights
        .into_iter()
        .take(num_peers_to_consider as usize)
        .map(|(peer, latency_weight)| (peer, latency_weight.into_inner()))
        .collect::<Vec<_>>();

    // Select the peers by latency weights
    choose_random_peers_by_weight(num_peers_to_choose, potential_peers_and_latency_weights)
}
```

**File:** state-sync/aptos-data-client/src/utils.rs (L175-183)
```rust
fn convert_latency_to_weight(latency: f64) -> f64 {
    // If the latency is <= 0, something has gone wrong, so return 0.
    if latency <= 0.0 {
        return 0.0;
    }

    // Otherwise, invert the latency to get the weight
    1000.0 / latency
}
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L329-330)
```rust
        // Sort the peers by latency
        peers_and_latencies.sort_by_key(|(_, latency)| *latency);
```

**File:** config/src/config/state_sync_config.rs (L401-409)
```rust
impl Default for AptosLatencyFilteringConfig {
    fn default() -> Self {
        Self {
            latency_filtering_reduction_factor: 2, // Only consider the best 50% of peers
            min_peer_ratio_for_latency_filtering: 5, // Only filter if we have at least 5 potential peers per request
            min_peers_for_latency_filtering: 10, // Only filter if we have at least 10 total peers
        }
    }
}
```

**File:** state-sync/aptos-data-client/src/tests/weighted_selection.rs (L465-469)
```rust
        // Verify the highest selected peers are the lowest latency peers
        utils::verify_highest_peer_selection_latencies(
            &mut mock_network,
            &mut peers_and_selection_counts,
        );
```
