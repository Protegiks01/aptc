# Audit Report

## Title
Consensus Pipeline Halts Without Recovery When Execution Fails - Missing Retry Mechanism for ExecutorError

## Summary
When an `ExecutorError` occurs during block execution at line 72 of `execution_schedule_phase.rs`, the consensus pipeline lacks an automatic retry mechanism. The error is logged but the affected block remains stuck in the "Ordered" state indefinitely, causing the entire consensus pipeline to halt and requiring manual intervention or state sync to recover.

## Finding Description

The vulnerability exists in the error handling path between the execution schedule phase and the buffer manager. When `wait_for_compute_result().await?` fails at line 72, the error propagates through the pipeline but is not properly recovered. [1](#0-0) 

This error is caught in the buffer manager's `process_execution_response()` method, where it is logged but no retry is initiated: [2](#0-1) 

The critical issue is that `advance_execution_root()` returns `Some(block_id)` to indicate retry is needed when the execution root hasn't advanced: [3](#0-2) 

However, this return value is **completely ignored** at all three call sites in the main event loop: [4](#0-3) 

This is in stark contrast to the signing phase, which **does** implement retry logic: [5](#0-4) 

**Attack Vector:**
When an `ExecutorError` occurs (which can be caused by transient failures like `CouldNotGetData` timeouts, `BlockNotFound` during state transitions, or `InternalError` from various subsystem failures), the block becomes permanently stuck. Since blocks must execute sequentially (block N+1 depends on state from block N), this blocks the entire consensus pipeline. [6](#0-5) 

## Impact Explanation

**Severity: HIGH** - This vulnerability causes validator node slowdowns and potential unavailability, matching the "Validator node slowdowns" category in the Aptos bug bounty program (up to $50,000).

**Specific impacts:**
1. **Liveness Violation**: The consensus pipeline halts for the affected validator, preventing it from processing new blocks
2. **Cascading Effect**: All subsequent blocks remain stuck in "Ordered" state and cannot progress to execution
3. **No Automatic Recovery**: The only recovery paths are:
   - State sync (which triggers a reset)
   - Manual intervention by node operators
   - Epoch change boundary
4. **Transient Errors Become Permanent**: Temporary failures (network timeouts, brief unavailability) that could succeed on retry instead cause prolonged outages

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This vulnerability is likely to occur because:

1. **Multiple Error Sources**: `ExecutorError` can be triggered by various conditions:
   - Network timeouts (`CouldNotGetData`)
   - State sync race conditions (`BlockNotFound`)
   - Internal subsystem failures (`InternalError`)
   - Database access issues

2. **No Protection Against Transient Failures**: Temporary issues that would resolve with a simple retry cause permanent pipeline halts

3. **Production Environment Conditions**: In distributed systems, transient failures are common (network partitions, temporary resource contention, etc.)

4. **Design Inconsistency**: The signing phase has retry logic, suggesting the developers recognized the need for retries but failed to implement it consistently across all pipeline stages

## Recommendation

Implement automatic retry logic for execution failures, consistent with the signing phase approach:

```rust
fn advance_execution_root(&mut self) -> Option<HashValue> {
    let cursor = self.execution_root;
    self.execution_root = self
        .buffer
        .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
            item.is_ordered()
        });
    if self.execution_root.is_some() && cursor == self.execution_root {
        // Schedule retry - return the block_id that needs retry
        self.execution_root
    } else {
        // ... existing code ...
        None
    }
}
```

Then in the main event loop, handle the retry signal:

```rust
Some(response) = self.execution_wait_phase_rx.next() => {
    monitor!("buffer_manager_process_execution_wait_response", {
        self.process_execution_response(response).await;
        if let Some(retry_block_id) = self.advance_execution_root() {
            // Retry execution for the failed block
            if let Some(cursor) = self.buffer.find_elem_by_key(self.execution_root, retry_block_id) {
                let item = self.buffer.get(&cursor);
                if let Some(ordered_item) = item.as_ordered() {
                    let sender = self.execution_schedule_phase_tx.clone();
                    let request = self.create_new_request(ExecutionRequest {
                        ordered_blocks: ordered_item.ordered_blocks.clone(),
                    });
                    Self::spawn_retry_request(sender, request, Duration::from_millis(100));
                }
            }
        }
        if self.signing_root.is_none() {
            self.advance_signing_root().await;
        }
    });
},
```

Additionally, consider implementing exponential backoff for repeated failures and maximum retry limits to prevent infinite retry loops.

## Proof of Concept

```rust
#[tokio::test]
async fn test_execution_error_causes_pipeline_halt() {
    // Setup: Create a buffer manager with mocked execution phases
    let (exec_schedule_tx, mut exec_schedule_rx) = create_channel();
    let (exec_wait_tx, exec_wait_rx) = create_channel();
    let (signing_tx, signing_rx) = create_channel();
    let (persist_tx, persist_rx) = create_channel();
    let (block_tx, block_rx) = unbounded();
    let (reset_tx, reset_rx) = unbounded();
    
    // ... (setup code for buffer manager) ...
    
    // Step 1: Send ordered blocks to the buffer manager
    let ordered_blocks = create_test_blocks(3); // blocks at rounds 1, 2, 3
    block_tx.send(OrderedBlocks {
        ordered_blocks: ordered_blocks.clone(),
        ordered_proof: create_test_proof(),
    }).await.unwrap();
    
    // Step 2: Receive execution schedule request
    let exec_request = exec_schedule_rx.next().await.unwrap();
    
    // Step 3: Simulate execution failure by sending ExecutorError
    exec_wait_tx.send(ExecutionResponse {
        block_id: ordered_blocks[0].id(),
        inner: Err(ExecutorError::CouldNotGetData), // Simulate timeout
    }).await.unwrap();
    
    // Step 4: Verify that no retry is attempted
    // Wait for a reasonable period (e.g., 500ms)
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    // Assert: No new execution requests were sent
    assert!(exec_schedule_rx.try_next().is_err(), 
        "Expected no retry request, but found one");
    
    // Step 5: Send more ordered blocks to verify pipeline is stuck
    let more_blocks = create_test_blocks(1); // block at round 4
    block_tx.send(OrderedBlocks {
        ordered_blocks: more_blocks.clone(),
        ordered_proof: create_test_proof(),
    }).await.unwrap();
    
    // Assert: Even with new blocks, the failed block is not retried
    // and subsequent blocks cannot progress
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    // The signing phase should receive nothing because execution is blocked
    assert!(signing_rx.try_next().is_err(),
        "Expected no signing request due to blocked execution");
    
    println!("✓ Confirmed: ExecutorError causes permanent pipeline halt");
    println!("✓ No automatic retry mechanism exists");
    println!("✓ Subsequent blocks cannot progress");
}
```

## Notes

This vulnerability represents a critical design flaw where the execution phase lacks the retry resilience that was correctly implemented in the signing phase. The comment "// Schedule retry" at line 437 of `buffer_manager.rs` indicates the developers intended to implement retry logic, but the implementation is incomplete—the return value signaling retry is never consumed.

The issue is particularly concerning because blockchain consensus requires high availability, and transient failures should not cause prolonged validator unavailability. The current implementation treats all `ExecutorError` instances as permanent failures requiring external intervention, when many could be resolved through simple retry logic.

### Citations

**File:** consensus/src/pipeline/execution_schedule_phase.rs (L70-76)
```rust
        let fut = async move {
            for b in ordered_blocks.iter_mut() {
                let (compute_result, execution_time) = b.wait_for_compute_result().await?;
                b.set_compute_result(compute_result, execution_time);
            }
            Ok(ordered_blocks)
        }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L429-452)
```rust
    fn advance_execution_root(&mut self) -> Option<HashValue> {
        let cursor = self.execution_root;
        self.execution_root = self
            .buffer
            .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
                item.is_ordered()
            });
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
        } else {
            sample!(
                SampleRate::Frequency(2),
                info!(
                    "Advance execution root from {:?} to {:?}",
                    cursor, self.execution_root
                )
            );
            // Otherwise do nothing, because the execution wait phase is driven by the response of
            // the execution schedule phase, which is in turn fed as soon as the ordered blocks
            // come in.
            None
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L478-486)
```rust
            if cursor == self.signing_root {
                let sender = self.signing_phase_tx.clone();
                Self::spawn_retry_request(sender, request, Duration::from_millis(100));
            } else {
                self.signing_phase_tx
                    .send(request)
                    .await
                    .expect("Failed to send signing request");
            }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L609-627)
```rust
    async fn process_execution_response(&mut self, response: ExecutionResponse) {
        let ExecutionResponse { block_id, inner } = response;
        // find the corresponding item, may not exist if a reset or aggregated happened
        let current_cursor = self.buffer.find_elem_by_key(self.execution_root, block_id);
        if current_cursor.is_none() {
            return;
        }

        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
        };
```

**File:** consensus/src/pipeline/buffer_manager.rs (L954-960)
```rust
                Some(response) = self.execution_wait_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_wait_response", {
                    self.process_execution_response(response).await;
                    self.advance_execution_root();
                    if self.signing_root.is_none() {
                        self.advance_signing_root().await;
                    }});
```

**File:** execution/executor-types/src/error.rs (L11-43)
```rust
#[derive(Debug, Deserialize, Error, PartialEq, Eq, Serialize, Clone)]
/// Different reasons for proposal rejection
pub enum ExecutorError {
    #[error("Cannot find speculation result for block id {0}")]
    BlockNotFound(HashValue),

    #[error("Cannot get data for batch id {0}")]
    DataNotFound(HashValue),

    #[error(
        "Bad num_txns_to_commit. first version {}, num to commit: {}, target version: {}",
        first_version,
        to_commit,
        target_version
    )]
    BadNumTxnsToCommit {
        first_version: Version,
        to_commit: usize,
        target_version: Version,
    },

    #[error("Internal error: {:?}", error)]
    InternalError { error: String },

    #[error("Serialization error: {0}")]
    SerializationError(String),

    #[error("Received Empty Blocks")]
    EmptyBlocks,

    #[error("request timeout")]
    CouldNotGetData,
}
```
