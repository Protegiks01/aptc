# Audit Report

## Title
Cache Flooding via TOCTOU Race Condition in Indexer-GRPC Data Manager

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) race condition in the cache garbage collection logic allows an attacker to flood the cache with large transactions, causing it to exceed the configured `max_cache_size` limit. This can result in memory exhaustion and denial-of-service of the indexer-grpc-manager service.

## Finding Description

The vulnerability exists in the interaction between `maybe_gc()` and `put_transactions()` methods in the `Cache` struct. The GC check validates that cache size is within limits, but immediately afterward, new transactions are added without re-checking, creating a TOCTOU window. [1](#0-0) 

The `maybe_gc()` function returns `true` if `cache_size <= max_cache_size` without performing any eviction. This creates a false assumption that the cache has room for new data. [2](#0-1) 

The `put_transactions()` method adds transactions without checking if this will cause the cache to exceed `max_cache_size`. It simply accumulates the encoded length of all transactions and extends the deque. [3](#0-2) 

In the main processing loop, for each `response_item` received from the fullnode stream, the code first ensures GC succeeds (line 235-256), then immediately adds the transactions (line 266). The problem is that between these operations, the cache can grow beyond `max_cache_size` by the size of the entire batch.

**Attack Path:**

1. Attacker creates transactions with large events and state changes. Each transaction can include up to 10 MB of events and 10 MB of write operations.

2. These transactions are processed by the blockchain and stored in fullnodes.

3. The indexer-grpc-manager requests transactions from fullnodes in batches. [4](#0-3) 

4. Fullnodes stream responses in chunks of `output_batch_size` (default 100) transactions per response item. [5](#0-4) 

5. When cache is at 4.99 GB (just under the 5 GB default `max_cache_size`), `maybe_gc()` returns `true` immediately without evicting anything. [6](#0-5) 

6. A batch of 100 transactions Ã— 10 MB each = 1 GB is added via `put_transactions()`, bringing cache to 5.99 GB.

7. The cache now exceeds `max_cache_size` by approximately 1 GB.

8. On the next iteration, GC attempts to evict but can only remove data up to `file_store_version`. If the FileStoreUploader is slow (due to network latency or slow cloud storage), `file_store_version` lags behind, preventing effective garbage collection. [7](#0-6) 

9. The system enters the waiting loop, but the cache has already exceeded the limit. If the attacker continues sending large transactions faster than the file store can process them, repeated cache overflows occur, leading to sustained memory pressure. [8](#0-7) 

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The cache is designed with explicit `max_cache_size` and `target_cache_size` limits, but these are not enforced atomically during transaction insertion.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty criteria:
- **"API crashes"**: The indexer-grpc-manager provides gRPC API services for transaction data. Memory exhaustion can crash this service.
- **Service disruption**: Indexing pipeline disruption affects downstream applications relying on transaction data.

While this doesn't directly impact consensus or validator operations, it causes availability issues for critical infrastructure components. Memory exhaustion can lead to:
- Out-of-memory (OOM) killer terminating the process
- System-wide memory pressure causing swap thrashing
- Degraded performance of co-located services
- Complete service outage requiring manual intervention

## Likelihood Explanation

**High likelihood** due to:

1. **Attacker control**: Any user can submit transactions with large payloads, events, and state changes (up to gas limits).

2. **Natural occurrence**: File store lag is common in production due to:
   - Network latency to cloud storage (S3, GCS, Azure Blob)
   - Storage service rate limiting or throttling
   - Temporary network partitions
   - High write volume during peak periods

3. **No special privileges required**: Attack works through normal transaction submission to any fullnode.

4. **Default configuration vulnerable**: Default `max_cache_size` of 5 GB with `output_batch_size` of 100 transactions allows ~1 GB overflows.

5. **Deterministic behavior**: The TOCTOU window is consistent and predictable, making exploitation straightforward.

## Recommendation

Implement atomic size checking in `put_transactions()` to enforce hard limits:

```rust
fn put_transactions(&mut self, transactions: Vec<Transaction>) -> Result<(), String> {
    let batch_size: usize = transactions
        .iter()
        .map(|transaction| transaction.encoded_len())
        .sum();
    
    // Hard limit check BEFORE adding
    if self.cache_size + batch_size > self.max_cache_size {
        return Err(format!(
            "Cannot add batch: would exceed max_cache_size ({} + {} > {})",
            self.cache_size, batch_size, self.max_cache_size
        ));
    }
    
    self.cache_size += batch_size;
    self.transactions.extend(transactions);
    CACHE_SIZE.set(self.cache_size as i64);
    CACHE_END_VERSION.set(self.start_version as i64 + self.transactions.len() as i64);
    Ok(())
}
```

In the main loop, handle the error by waiting for GC to free more space:

```rust
loop {
    if self.cache.write().await.maybe_gc() {
        match self.cache.write().await.put_transactions(data.transactions) {
            Ok(_) => break,
            Err(e) => {
                warn!("Cannot add transactions: {}, waiting for GC", e);
                tokio::time::sleep(Duration::from_millis(100)).await;
                continue;
            }
        }
    }
    // existing waiting logic...
}
```

Additionally, consider:
- Adding metrics to track cache overflow events
- Implementing backpressure to slow down transaction fetching when cache is near capacity
- Making `output_batch_size` configurable and reducing it for nodes with memory constraints

## Proof of Concept

```rust
#[cfg(test)]
mod cache_overflow_poc {
    use super::*;
    use aptos_protos::transaction::v1::Transaction;
    use prost::Message;

    #[test]
    fn test_cache_exceeds_max_size_via_toctou() {
        // Create cache with 100 MB max
        let cache_config = CacheConfig {
            max_cache_size: 100 * (1 << 20), // 100 MB
            target_cache_size: 80 * (1 << 20), // 80 MB
        };
        let mut cache = Cache::new(cache_config, 0);
        
        // Fill cache to just under max (99 MB)
        let mut filler_txns = Vec::new();
        let single_txn_size = 1 << 20; // 1 MB each
        for i in 0..99 {
            let mut txn = Transaction::default();
            txn.version = i;
            // Create transaction with ~1 MB protobuf encoding
            txn.info = Some(create_large_transaction_info(single_txn_size));
            filler_txns.push(txn);
        }
        cache.put_transactions(filler_txns);
        
        assert!(cache.cache_size <= cache.max_cache_size);
        println!("Cache size before attack: {} bytes", cache.cache_size);
        
        // GC check passes because cache_size <= max_cache_size
        assert!(cache.maybe_gc());
        
        // Now add a large batch (20 MB) simulating attacker's large transactions
        let mut attack_batch = Vec::new();
        for i in 100..120 {
            let mut txn = Transaction::default();
            txn.version = i;
            txn.info = Some(create_large_transaction_info(single_txn_size));
            attack_batch.push(txn);
        }
        
        cache.put_transactions(attack_batch);
        
        // VULNERABILITY: Cache now exceeds max_cache_size
        println!("Cache size after attack: {} bytes", cache.cache_size);
        assert!(cache.cache_size > cache.max_cache_size,
                "Cache exceeded max_cache_size: {} > {}",
                cache.cache_size, cache.max_cache_size);
        
        let overflow = cache.cache_size - cache.max_cache_size;
        println!("Cache overflow: {} bytes ({:.2} MB)", 
                 overflow, overflow as f64 / (1 << 20) as f64);
    }
    
    fn create_large_transaction_info(target_size: usize) -> TransactionInfo {
        // Create TransactionInfo with large events/changes to reach target size
        let mut info = TransactionInfo::default();
        // Add dummy changes until we reach target encoded size
        let dummy_change = create_dummy_write_set_change();
        let change_size = dummy_change.encoded_len();
        let num_changes = target_size / change_size;
        
        for _ in 0..num_changes {
            info.changes.push(dummy_change.clone());
        }
        info
    }
    
    fn create_dummy_write_set_change() -> WriteSetChange {
        // Create a moderately-sized write set change
        // Implementation details omitted for brevity
        WriteSetChange::default()
    }
}
```

## Notes

The vulnerability specifically affects the indexer-grpc-manager component, which is part of the indexing infrastructure rather than the core consensus layer. However, it represents a clear DoS vector that can disrupt transaction indexing services, affecting the broader ecosystem's ability to query and process blockchain data. The TOCTOU pattern is a classic synchronization bug that violates the intended resource limits and should be fixed regardless of the affected component's criticality.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L63-80)
```rust
    fn maybe_gc(&mut self) -> bool {
        if self.cache_size <= self.max_cache_size {
            return true;
        }

        while self.start_version < self.file_store_version.load(Ordering::SeqCst)
            && self.cache_size > self.target_cache_size
        {
            let transaction = self.transactions.pop_front().unwrap();
            self.cache_size -= transaction.encoded_len();
            self.start_version += 1;
        }

        CACHE_SIZE.set(self.cache_size as i64);
        CACHE_START_VERSION.set(self.start_version as i64);

        self.cache_size <= self.max_cache_size
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L82-90)
```rust
    fn put_transactions(&mut self, transactions: Vec<Transaction>) {
        self.cache_size += transactions
            .iter()
            .map(|transaction| transaction.encoded_len())
            .sum::<usize>();
        self.transactions.extend(transactions);
        CACHE_SIZE.set(self.cache_size as i64);
        CACHE_END_VERSION.set(self.start_version as i64 + self.transactions.len() as i64);
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L207-210)
```rust
            let request = GetTransactionsFromNodeRequest {
                starting_version: Some(cache.start_version + cache.transactions.len() as u64),
                transactions_count: Some(100000),
            };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L233-266)
```rust
            while let Some(response_item) = response.next().await {
                trace!("Processing 1 response item.");
                loop {
                    trace!("Maybe running GC.");
                    if self.cache.write().await.maybe_gc() {
                        IS_FILE_STORE_LAGGING.set(0);
                        trace!("GC is done, file store is not lagging.");
                        break;
                    }
                    IS_FILE_STORE_LAGGING.set(1);
                    // If file store is lagging, we are not inserting more data.
                    let cache = self.cache.read().await;
                    warn!("Filestore is lagging behind, cache is full [{}, {}), known_latest_version ({}).",
                          cache.start_version,
                          cache.start_version + cache.transactions.len() as u64,
                          self.metadata_manager.get_known_latest_version());
                    tokio::time::sleep(Duration::from_millis(100)).await;
                    if watch_file_store_version {
                        self.update_file_store_version_in_cache(
                            &cache, /*version_can_go_backward=*/ false,
                        )
                        .await;
                    }
                }
                match response_item {
                    Ok(r) => {
                        if let Some(response) = r.response {
                            match response {
                                Response::Data(data) => {
                                    trace!(
                                        "Putting data into cache, {} transaction(s).",
                                        data.transactions.len()
                                    );
                                    self.cache.write().await.put_transactions(data.transactions);
```

**File:** config/src/config/indexer_grpc_config.rs (L18-18)
```rust
const DEFAULT_OUTPUT_BATCH_SIZE: u16 = 100;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/config.rs (L44-49)
```rust
const fn default_cache_config() -> CacheConfig {
    CacheConfig {
        max_cache_size: 5 * (1 << 30),
        target_cache_size: 4 * (1 << 30),
    }
}
```
