# Audit Report

## Title
State Restore Progress Marker Accumulation Leading to Stale Version Selection

## Summary
The `get_in_progress_state_kv_snapshot_version()` function deterministically returns the first (smallest version) `StateSnapshotKvRestoreProgress` marker when multiple exist, but lacks cleanup logic to delete these markers after restore completion. This creates a scenario where stale progress markers from previous restore attempts can cause the restore coordinator to resume an outdated restore operation, leading to state inconsistency. [1](#0-0) 

## Finding Description

The vulnerability stems from two interconnected issues:

1. **Missing Cleanup Logic**: Progress markers are created during state KV restore operations but are never deleted upon completion. [2](#0-1) 

The `kv_finish` method completes the restore but does not delete the progress marker: [3](#0-2) 

2. **Non-Discriminating Version Selection**: When checking for in-progress restores, the function iterates through metadata and returns the first `StateSnapshotKvRestoreProgress` entry it encounters. Due to BCS encoding ordering by the Version field, this returns the **smallest version number**: [1](#0-0) 

**Attack Scenario:**
1. Node operator starts a state restore for version 100, which partially completes and leaves a progress marker
2. The restore is interrupted (crash, manual stop, etc.) without cleanup
3. Operator starts a new restore for version 200, which also partially completes
4. Database now contains two progress markers: `StateSnapshotKvRestoreProgress(100)` and `StateSnapshotKvRestoreProgress(200)`
5. When `get_in_progress_state_kv_snapshot_version()` is called, it returns version 100
6. The restore coordinator attempts to resume the stale version 100 restore: [4](#0-3) 
7. However, the actual database state contains partial data from the version 200 restore
8. This creates an inconsistent state where the KV data doesn't match the expected version

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs."

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria: "State inconsistencies requiring intervention."

**Specific Impacts:**
- **State Corruption**: Database contains mixed state data from different restore versions
- **Non-Deterministic Restore Behavior**: Which version gets resumed depends on which stale markers exist
- **Failed Recovery**: Attempting to resume a stale restore that doesn't match current DB state leads to restore failures
- **Manual Intervention Required**: Operators must manually identify and delete stale progress markers from the metadata database

While this doesn't directly cause consensus violations or fund loss, it creates operational state inconsistencies that require manual database intervention to resolve.

## Likelihood Explanation

**Likelihood: Medium**

This issue can occur in several realistic scenarios:
1. **Operator Error**: Node operator starts a new restore without cleaning up a previous incomplete restore
2. **System Crashes**: Node crashes during restore, leaving orphaned progress markers
3. **Database Corruption**: Underlying RocksDB corruption creates duplicate entries
4. **Restore Strategy Changes**: Operator switches between different restore versions without proper cleanup

The issue is **not exploitable by external attackers** as it requires either:
- Node operator access to initiate restore operations
- Physical database corruption

However, it can occur naturally through operational mistakes or system failures, making manual intervention necessary.

## Recommendation

**Fix 1: Add Cleanup Logic**

Modify the `kv_finish` method to delete the progress marker after successful completion:

```rust
fn kv_finish(&self, version: Version, usage: StateStorageUsage) -> Result<()> {
    self.ledger_db.metadata_db().put_usage(version, usage)?;
    
    // Delete the progress marker after successful completion
    let mut batch = SchemaBatch::new();
    batch.delete::<DbMetadataSchema>(
        &DbMetadataKey::StateSnapshotKvRestoreProgress(version)
    )?;
    self.state_kv_db.metadata_db().write_schemas(batch)?;
    
    // ... rest of existing code for internal_indexer_db ...
    
    Ok(())
}
```

**Fix 2: Validate Only One Progress Marker Exists**

Add validation in `get_in_progress_state_kv_snapshot_version()`:

```rust
pub fn get_in_progress_state_kv_snapshot_version(&self) -> Result<Option<Version>> {
    let db = self.aptosdb.state_kv_db.metadata_db_arc();
    let mut iter = db.iter::<DbMetadataSchema>()?;
    iter.seek_to_first();
    
    let mut found_version: Option<Version> = None;
    while let Some((k, _v)) = iter.next().transpose()? {
        if let DbMetadataKey::StateSnapshotKvRestoreProgress(version) = k {
            if found_version.is_some() {
                bail!(
                    "Multiple StateSnapshotKvRestoreProgress markers found: {:?} and {}. Database corruption detected.",
                    found_version,
                    version
                );
            }
            found_version = Some(version);
        }
    }
    Ok(found_version)
}
```

**Fix 3: Add Cleanup on Restore Start**

Before starting a new restore, explicitly clean up any existing progress markers for that version.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_db_indexer_schemas::metadata::StateSnapshotProgress;
    use aptos_types::state_store::state_storage_usage::StateStorageUsage;
    
    #[test]
    fn test_multiple_progress_markers() {
        // Setup: Create AptosDB instance
        let tmpdir = aptos_temppath::TempPath::new();
        let db = AptosDB::new_for_test(&tmpdir);
        let state_store = Arc::new(db.state_store);
        
        // Simulate two incomplete restores
        let version_100 = 100;
        let version_200 = 200;
        
        // Create progress marker for version 100
        let mut batch1 = SchemaBatch::new();
        batch1.put::<DbMetadataSchema>(
            &DbMetadataKey::StateSnapshotKvRestoreProgress(version_100),
            &DbMetadataValue::StateSnapshotProgress(StateSnapshotProgress::new(
                HashValue::zero(),
                StateStorageUsage::zero(),
            )),
        ).unwrap();
        state_store.state_kv_db.metadata_db().write_schemas(batch1).unwrap();
        
        // Create progress marker for version 200
        let mut batch2 = SchemaBatch::new();
        batch2.put::<DbMetadataSchema>(
            &DbMetadataKey::StateSnapshotKvRestoreProgress(version_200),
            &DbMetadataValue::StateSnapshotProgress(StateSnapshotProgress::new(
                HashValue::zero(),
                StateStorageUsage::zero(),
            )),
        ).unwrap();
        state_store.state_kv_db.metadata_db().write_schemas(batch2).unwrap();
        
        // Create restore handler
        let restore_handler = RestoreHandler::new(Arc::new(db), state_store.clone());
        
        // BUG: This returns version 100 (smallest), not version 200 (most recent)
        let in_progress = restore_handler.get_in_progress_state_kv_snapshot_version().unwrap();
        
        assert_eq!(in_progress, Some(version_100));
        // Expected: Should either return version 200 or error due to multiple markers
        // Actual: Returns version 100, which may be stale
    }
}
```

**Notes**

The function behavior is **deterministic, not undefined** - it consistently returns the smallest version due to BCS encoding order of the Version field in the enum variant. The security issue is that this deterministic behavior leads to selecting stale restore points when multiple progress markers exist from interrupted operations, creating state inconsistencies that require manual database intervention to resolve.

### Citations

**File:** storage/aptosdb/src/backup/restore_handler.rs (L139-149)
```rust
    pub fn get_in_progress_state_kv_snapshot_version(&self) -> Result<Option<Version>> {
        let db = self.aptosdb.state_kv_db.metadata_db_arc();
        let mut iter = db.iter::<DbMetadataSchema>()?;
        iter.seek_to_first();
        while let Some((k, _v)) = iter.next().transpose()? {
            if let DbMetadataKey::StateSnapshotKvRestoreProgress(version) = k {
                return Ok(Some(version));
            }
        }
        Ok(None)
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1254-1257)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateSnapshotKvRestoreProgress(version),
            &DbMetadataValue::StateSnapshotProgress(progress),
        )?;
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1281-1314)
```rust
    fn kv_finish(&self, version: Version, usage: StateStorageUsage) -> Result<()> {
        self.ledger_db.metadata_db().put_usage(version, usage)?;
        if let Some(internal_indexer_db) = self.internal_indexer_db.as_ref() {
            if version > 0 {
                let mut batch = SchemaBatch::new();
                batch.put::<InternalIndexerMetadataSchema>(
                    &MetadataKey::LatestVersion,
                    &MetadataValue::Version(version - 1),
                )?;
                if internal_indexer_db.statekeys_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::StateVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                if internal_indexer_db.transaction_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::TransactionVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                if internal_indexer_db.event_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::EventVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                internal_indexer_db
                    .get_inner_db_ref()
                    .write_schemas(batch)?;
            }
        }

        Ok(())
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L157-181)
```rust
        let kv_snapshot = match self.global_opt.run_mode.get_in_progress_state_kv_snapshot() {
            Ok(Some(ver)) => {
                if db_next_version >= ver {
                    // already restored the kv snapshot, no need to restore again
                    None
                } else {
                    let snapshot = metadata_view.select_state_snapshot(ver)?;
                    ensure!(
                        snapshot.is_some() && snapshot.as_ref().unwrap().version == ver,
                        "cannot find in-progress state snapshot {}",
                        ver
                    );
                    snapshot
                }
            },
            Ok(None) | Err(_) => {
                assert_eq!(
                    db_next_version, 0,
                    "DB should be empty if no in-progress state snapshot found"
                );
                metadata_view
                    .select_state_snapshot(std::cmp::min(lhs, max_txn_ver))
                    .expect("Cannot find any snapshot before ledger history start version")
            },
        };
```
