# Audit Report

## Title
Consensus Safety Violation Due to Inconsistent Leader Election When Database Epoch History Retrieval Fails

## Summary
When `get_epoch_ending_ledger_infos` fails with a `DbError` during epoch initialization in `extract_epoch_proposers`, the error is silently caught and falls back to using only current epoch validator data. If different validators experience different database states (some with complete historical data, others with corrupted or incomplete data), they will construct inconsistent `epoch_to_proposers` mappings, leading to different leader election results for the same round and breaking consensus safety.

## Finding Description

The vulnerability exists in the error handling of `extract_epoch_proposers` method in `consensus/src/epoch_manager.rs`. This method is called during epoch initialization to construct the `epoch_to_proposers` mapping used by `LeaderReputation` for reputation-based leader election. [1](#0-0) 

When `get_epoch_ending_ledger_infos` fails (line 430), the error is caught with `unwrap_or_else` (line 439) and the function falls back to returning only the current epoch's proposers (line 444). While this prevents a crash, it creates a critical inconsistency when validators have different database states.

**Attack Scenario:**

1. Network transitions from epoch N to epoch N+1
2. Most validators (V1, V2, V3) have complete databases with historical epoch ending ledger infos
3. One or more validators (V4) have database corruption, I/O errors, or incomplete sync causing `get_epoch_ending_ledger_infos` to fail
4. During `create_proposer_election` for LeaderReputation type:
   - V1, V2, V3: Successfully retrieve historical data, construct `epoch_to_proposers` with epochs (N-k) through N+1
   - V4: Database query fails, falls back to `epoch_to_proposers` containing only epoch N+1 [2](#0-1) 

5. `LeaderReputation::new` is called with different `epoch_to_proposers` maps: [3](#0-2) 

6. The reputation heuristic (`ProposerAndVoterHeuristic`) computes weights using `epoch_to_proposers`: [4](#0-3) 

7. For round R, validators compute different reputation weights and elect different leaders:
   - V1, V2, V3 elect Leader A (based on historical reputation)
   - V4 elects Leader B (based only on current epoch)

8. Consensus safety is violated as validators disagree on the expected leader for round R

The same issue exists in DAG consensus initialization: [5](#0-4) 

**Database Error Conditions:**

The error occurs when `get_epoch_ending_ledger_infos` fails, which can happen when: [6](#0-5) 

- `end_epoch > latest_epoch` in the database (database not synced)
- Missing epoch ending ledger info (database corruption, line 1056-1062)
- Database I/O errors during iteration

## Impact Explanation

This is a **Critical Severity** vulnerability (Consensus Safety violation) because:

1. **Breaks Consensus Safety Invariant**: Different validators elect different leaders for the same round, violating the fundamental requirement that all honest validators must agree on consensus-critical state

2. **Potential Chain Split**: If validators vote for blocks from different leaders, it can cause:
   - Equivocation detection triggering on honest validators
   - Validators building on different block chains
   - Network partition requiring manual intervention or hard fork

3. **Non-Deterministic Execution**: Validators no longer produce identical state transitions for the same inputs, breaking deterministic execution guarantees

While the attack requires database corruption or sync issues (not directly attacker-controlled), the impact is severe enough to warrant Critical severity. Similar database-related consensus bugs have been classified as Critical in other blockchain systems.

## Likelihood Explanation

**Likelihood: Medium**

While not trivial to trigger, this scenario can occur in production:

1. **State Sync Delays**: During epoch transitions, some validators may complete state sync faster than others. If a reconfig notification arrives before database sync completes, the validator will have incomplete historical data.

2. **Database Corruption**: Hardware failures, disk errors, or bugs in database write paths can cause missing or corrupted epoch ending ledger infos.

3. **I/O Errors**: Transient filesystem or storage errors during `get_epoch_ending_ledger_infos` can cause failures for some validators.

4. **Race Conditions**: The reconfig notification path and database commit path may have timing windows where `epoch_state` is updated before the database has all required data.

5. **Partial Restarts**: Validators restored from backups or joining after being offline may have incomplete epoch history.

The error is logged but not visible to operators until consensus failures manifest, making it harder to detect and debug.

## Recommendation

The error should be propagated instead of silently degrading. This forces explicit handling at a higher level and prevents silent consensus divergence:

```rust
fn extract_epoch_proposers(
    &self,
    epoch_state: &EpochState,
    use_history_from_previous_epoch_max_count: u32,
    proposers: Vec<AccountAddress>,
    needed_rounds: u64,
) -> anyhow::Result<HashMap<u64, Vec<AccountAddress>>> {
    let first_epoch_to_consider = std::cmp::max(
        if epoch_state.epoch == 1 { 1 } else { 2 },
        epoch_state
            .epoch
            .saturating_sub(use_history_from_previous_epoch_max_count as u64),
    );
    
    if epoch_state.epoch > first_epoch_to_consider {
        let proof = self.storage
            .aptos_db()
            .get_epoch_ending_ledger_infos(first_epoch_to_consider - 1, epoch_state.epoch)
            .context("Failed to get epoch ending ledger infos for leader election")?;
        
        ensure!(
            proof.ledger_info_with_sigs.len() as u64
                == (epoch_state.epoch - (first_epoch_to_consider - 1))
        );
        
        let epoch_to_proposers = extract_epoch_to_proposers(
            proof, 
            epoch_state.epoch, 
            &proposers, 
            needed_rounds
        )?;
        
        Ok(epoch_to_proposers)
    } else {
        Ok(HashMap::from([(epoch_state.epoch, proposers)]))
    }
}
```

Then handle the error in the caller (`start_new_epoch`) by:
1. Logging a critical alert
2. Attempting state sync recovery
3. Panicking if unable to proceed (preventing silent divergence)

Additionally, add validation during epoch initialization to ensure the database has all required epoch ending ledger infos before setting `epoch_state`.

## Proof of Concept

```rust
// Reproduction steps (requires test infrastructure):

#[tokio::test]
async fn test_inconsistent_leader_election_from_database_error() {
    // Setup: Create 4 validators in epoch 5
    let mut validator_1 = create_validator_with_complete_db(5);
    let mut validator_2 = create_validator_with_complete_db(5);
    let mut validator_3 = create_validator_with_complete_db(5);
    
    // Validator 4 has corrupted database missing epoch 4 ledger info
    let mut validator_4 = create_validator_with_incomplete_db(5, missing_epoch: 4);
    
    // Trigger epoch transition to epoch 6 with LeaderReputation enabled
    let reconfig_payload = create_reconfig_with_leader_reputation();
    
    validator_1.process_reconfig(reconfig_payload.clone()).await;
    validator_2.process_reconfig(reconfig_payload.clone()).await;
    validator_3.process_reconfig(reconfig_payload.clone()).await;
    validator_4.process_reconfig(reconfig_payload.clone()).await;
    
    // Check leader election for round 100
    let round = 100;
    let leader_1 = validator_1.get_proposer_election().get_valid_proposer(round);
    let leader_2 = validator_2.get_proposer_election().get_valid_proposer(round);
    let leader_3 = validator_3.get_proposer_election().get_valid_proposer(round);
    let leader_4 = validator_4.get_proposer_election().get_valid_proposer(round);
    
    // V1, V2, V3 should agree (they have complete history)
    assert_eq!(leader_1, leader_2);
    assert_eq!(leader_2, leader_3);
    
    // V4 will likely elect a different leader (only has current epoch)
    // This assertion should fail, demonstrating the vulnerability
    assert_eq!(leader_1, leader_4, "Consensus safety violated: validators elected different leaders!");
}
```

**Notes:**
- The vulnerability requires one validator to have database corruption/incompleteness while others have complete data
- The impact manifests as consensus safety violations during leader election
- The error handling in `process_epoch_retrieval` is correct (propagates errors), but the vulnerability exists in `extract_epoch_proposers` which is called during epoch initialization, not during epoch retrieval request handling
- The security question asked about error propagation in the context of `get_epoch_ending_ledger_infos` - while `process_epoch_retrieval` handles errors correctly, the same database function is called in `extract_epoch_proposers` where the error handling is unsafe

### Citations

**File:** consensus/src/epoch_manager.rs (L361-366)
```rust
                let epoch_to_proposers = self.extract_epoch_proposers(
                    epoch_state,
                    use_history_from_previous_epoch_max_count,
                    proposers,
                    (window_size + seek_len) as u64,
                );
```

**File:** consensus/src/epoch_manager.rs (L378-387)
```rust
                let proposer_election = Box::new(LeaderReputation::new(
                    epoch_state.epoch,
                    epoch_to_proposers,
                    voting_powers,
                    backend,
                    heuristic,
                    onchain_config.leader_reputation_exclude_round(),
                    leader_reputation_type.use_root_hash_for_seed(),
                    self.config.window_for_chain_health,
                ));
```

**File:** consensus/src/epoch_manager.rs (L409-449)
```rust
    fn extract_epoch_proposers(
        &self,
        epoch_state: &EpochState,
        use_history_from_previous_epoch_max_count: u32,
        proposers: Vec<AccountAddress>,
        needed_rounds: u64,
    ) -> HashMap<u64, Vec<AccountAddress>> {
        // Genesis is epoch=0
        // First block (after genesis) is epoch=1, and is the only block in that epoch.
        // It has no votes, so we skip it unless we are in epoch 1, as otherwise it will
        // skew leader elections for exclude_round number of rounds.
        let first_epoch_to_consider = std::cmp::max(
            if epoch_state.epoch == 1 { 1 } else { 2 },
            epoch_state
                .epoch
                .saturating_sub(use_history_from_previous_epoch_max_count as u64),
        );
        // If we are considering beyond the current epoch, we need to fetch validators for those epochs
        if epoch_state.epoch > first_epoch_to_consider {
            self.storage
                .aptos_db()
                .get_epoch_ending_ledger_infos(first_epoch_to_consider - 1, epoch_state.epoch)
                .map_err(Into::into)
                .and_then(|proof| {
                    ensure!(
                        proof.ledger_info_with_sigs.len() as u64
                            == (epoch_state.epoch - (first_epoch_to_consider - 1))
                    );
                    extract_epoch_to_proposers(proof, epoch_state.epoch, &proposers, needed_rounds)
                })
                .unwrap_or_else(|err| {
                    error!(
                        "Couldn't create leader reputation with history across epochs, {:?}",
                        err
                    );
                    HashMap::from([(epoch_state.epoch, proposers)])
                })
        } else {
            HashMap::from([(epoch_state.epoch, proposers)])
        }
    }
```

**File:** consensus/src/epoch_manager.rs (L1473-1484)
```rust
        let epoch_to_validators = self.extract_epoch_proposers(
            &epoch_state,
            onchain_dag_consensus_config.dag_ordering_causal_history_window as u32,
            epoch_state.verifier.get_ordered_account_addresses(),
            onchain_dag_consensus_config.dag_ordering_causal_history_window as u64,
        );
        let dag_storage = Arc::new(StorageAdapter::new(
            epoch,
            epoch_to_validators,
            self.storage.consensus_db(),
            self.storage.aptos_db(),
        ));
```

**File:** consensus/src/liveness/leader_reputation.rs (L704-708)
```rust
        let mut weights =
            self.heuristic
                .get_weights(self.epoch, &self.epoch_to_proposers, &sliding_window);
        let proposers = &self.epoch_to_proposers[&self.epoch];
        assert_eq!(weights.len(), proposers.len());
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L1027-1033)
```rust
        ensure!(
            end_epoch <= latest_epoch,
            "Unable to provide epoch change ledger info for still open epoch. asked upper bound: {}, last sealed epoch: {}",
            end_epoch,
            latest_epoch - 1,  // okay to -1 because genesis LedgerInfo has .next_block_epoch() == 1
        );
        Ok(())
```
