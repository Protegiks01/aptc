# Audit Report

## Title
Corrupted Epoch Ending Ledger Info Causes Permanent Validator Failure Due to Unrecoverable Bootstrapper Panic

## Summary
If the epoch ending ledger info for the immediately previous epoch becomes corrupted in a validator's local database (due to hardware failure, disk corruption, or software bugs), the validator node will fail to start indefinitely due to a panic in the state sync bootstrapper initialization. The node cannot recover automatically and requires manual intervention with offline database repair tools.

## Finding Description

The vulnerability exists in the state sync driver's bootstrapper initialization path. When a validator node starts, the `Bootstrapper::new` function attempts to fetch the latest epoch state from storage: [1](#0-0) 

This call uses `.expect()`, which panics if the underlying storage read fails. The `fetch_latest_epoch_state` function calls `storage.get_latest_epoch_state()`: [2](#0-1) 

When the latest ledger info does not contain a `next_epoch_state` (indicating it's mid-epoch rather than an epoch boundary), the code calls `get_epoch_state(epoch)` to retrieve the epoch state from the previous epoch's ending ledger info: [3](#0-2) 

This function attempts to read the epoch ending ledger info for epoch N-1 from the database. If this ledger info is corrupted (e.g., due to disk bit flips, filesystem corruption, or database file damage), the deserialization will fail: [4](#0-3) 

When BCS deserialization fails on corrupted bytes, the error propagates back through the call chain and triggers the panic in `Bootstrapper::new`.

**Additional Impact on Batch Requests:**

The `MAX_NUM_EPOCH_ENDING_LEDGER_INFO` constant limits epoch ending ledger info requests to 100 epochs: [5](#0-4) 

The batch retrieval function strictly validates that ALL requested epoch ending ledger infos are present: [6](#0-5) 

If corruption exists anywhere within a requested 100-epoch range, the entire batch request fails with a "DB corruption" error, preventing state synchronization even after manual database repair if the corruption persists.

**Exploitation Path:**

1. A validator is running normally at version X in epoch N
2. Storage corruption occurs (hardware failure, disk errors, software bug) affecting the epoch ending ledger info for epoch N-1
3. The validator process crashes or is restarted
4. During startup:
   - `start_state_sync_and_get_notification_handles` is called
   - `DriverFactory::create_and_spawn_driver` creates a `StateSyncDriver`
   - `StateSyncDriver::new` calls `Bootstrapper::new`
   - `Bootstrapper::new` calls `fetch_latest_epoch_state().expect(...)`
   - `get_latest_epoch_state` tries to read the corrupted ledger info
   - Deserialization fails → Error returned
   - `.expect()` panics → **Node crashes**
5. Every subsequent restart attempt fails identically
6. Validator is permanently offline until manual operator intervention

**Recovery Requirements:**

Manual intervention using offline tools:
- `aptos-db-tool truncate` to truncate database to a version before the corruption
- Database restore from backup
- Complete database deletion and re-sync from genesis

All recovery methods require node downtime and operator intervention.

## Impact Explanation

**High Severity** per Aptos Bug Bounty criteria:

- **Validator node slowdowns**: The node doesn't slow down—it completely stops and cannot restart
- **Significant protocol violations**: The validator cannot participate in consensus, violating availability guarantees
- **Loss of liveness**: Affected validator(s) lose all participation capability

The impact is particularly severe because:
1. **No automatic recovery**: The panic occurs before any recovery mechanisms activate
2. **Complete availability loss**: The validator is entirely offline, not degraded
3. **Requires manual intervention**: Operators must use offline repair tools
4. **Potential network impact**: If multiple validators are affected, network liveness could be compromised
5. **Economic harm**: Validator loses rewards and may face penalties during downtime

While this requires storage corruption (not a remote exploit), storage corruption is a realistic failure mode in production systems due to hardware failures, and resilient systems should handle it gracefully rather than with unrecoverable panics.

## Likelihood Explanation

**Likelihood: Medium to High** in production environments

Storage corruption can occur through:
- **Hardware failures**: Disk bit flips, bad sectors, controller failures
- **Filesystem corruption**: Crash during write, filesystem bugs
- **Software bugs**: Serialization errors, race conditions in write paths
- **Memory corruption**: ECC errors before data reaches disk

The RocksDB database uses atomic write batches for consistency: [7](#0-6) 

However, atomic writes protect against partial writes during normal operation—they do not prevent corruption from hardware failures, filesystem issues, or post-write bit flips.

The likelihood is elevated because:
1. Long-running validators accumulate storage over time, increasing corruption probability
2. Epoch ending ledger infos are frequently accessed data
3. No checksums or corruption detection at the application layer
4. The failure mode is deterministic—once corruption occurs, the node cannot recover

## Recommendation

**Short-term fix**: Replace the `.expect()` with proper error handling that attempts recovery:

```rust
// In state-sync/state-sync-driver/src/bootstrapper.rs
pub fn new(
    driver_configuration: DriverConfiguration,
    metadata_storage: MetadataStorage,
    output_fallback_handler: OutputFallbackHandler,
    streaming_client: StreamingClient,
    storage: Arc<dyn DbReader>,
    storage_synchronizer: StorageSyncer,
) -> Result<Self, Error> {
    // Load the latest epoch state from storage with error handling
    let latest_epoch_state = match utils::fetch_latest_epoch_state(storage.clone()) {
        Ok(epoch_state) => epoch_state,
        Err(e) => {
            error!("Failed to fetch latest epoch state: {:?}. Attempting recovery from network.", e);
            // Attempt to fetch from genesis or a trusted checkpoint
            // If that fails, return error instead of panicking
            return Err(Error::StorageError(format!(
                "Cannot initialize bootstrapper: failed to fetch epoch state: {:?}", e
            )));
        }
    };
    // ... rest of initialization
}
```

**Long-term fixes**:

1. **Add corruption detection**: Implement checksums for epoch ending ledger infos
2. **Graceful degradation**: Allow bootstrapper to initialize with a fallback epoch state and attempt to fetch missing data from the network
3. **Network recovery**: If local epoch state is corrupted, fetch from trusted peers before panicking
4. **Database integrity checks**: Run periodic integrity checks and alert operators before corruption causes crashes
5. **Incremental validation**: In `get_epoch_ending_ledger_infos_impl`, instead of failing the entire batch, skip corrupted entries and set a flag indicating partial data

Example of safer batch retrieval:

```rust
pub(super) fn get_epoch_ending_ledger_infos_impl(
    &self,
    start_epoch: u64,
    end_epoch: u64,
    limit: usize,
) -> Result<(Vec<LedgerInfoWithSignatures>, bool)> {
    self.check_epoch_ending_ledger_infos_request(start_epoch, end_epoch)?;

    let (paging_epoch, more) = if end_epoch - start_epoch > limit as u64 {
        (start_epoch + limit as u64, true)
    } else {
        (end_epoch, false)
    };

    let lis = self
        .ledger_db
        .metadata_db()
        .get_epoch_ending_ledger_info_iter(start_epoch, paging_epoch)?
        .filter_map(|result| match result {
            Ok(li) => Some(li),
            Err(e) => {
                warn!("Skipping corrupted epoch ending ledger info: {:?}", e);
                None
            }
        })
        .collect::<Vec<_>>();

    // Return what we have with a warning instead of failing
    if lis.len() < (paging_epoch - start_epoch) as usize {
        warn!(
            "Incomplete epoch ending ledger infos: expected {}, got {}",
            paging_epoch - start_epoch,
            lis.len()
        );
    }
    
    Ok((lis, more))
}
```

## Proof of Concept

Due to the nature of this vulnerability (requiring database corruption), a complete PoC requires manually corrupting the database. However, the vulnerability can be demonstrated through the following steps:

**Step 1: Set up a test validator node**
```bash
# Start a local testnet
cargo run --bin aptos-node -- --test
```

**Step 2: Let it run and commit several epochs**
```bash
# Wait for at least 2 epochs to pass
# Check current epoch with:
# curl http://localhost:8080/v1/ | jq .ledger_info.epoch
```

**Step 3: Stop the node and corrupt the database**
```bash
# Stop the node
# Locate the epoch ending ledger info in RocksDB
# Path: <data_dir>/db/ledger_db/

# Use a hex editor or custom tool to corrupt the BCS-encoded
# LedgerInfoWithSignatures for the previous epoch
# The corruption can be as simple as flipping random bits
```

**Step 4: Attempt to restart the node**
```bash
cargo run --bin aptos-node -- --test --test-dir <corrupted_data_dir>
```

**Expected Result:**
The node will panic during bootstrapper initialization with an error message similar to:
```
thread 'main' panicked at 'Unable to fetch latest epoch state!: StorageError("Failed to get the latest epoch state from storage: ...")'
```

The panic stack trace will show the failure originating from:
- `state-sync/state-sync-driver/src/bootstrapper.rs:343`
- `state-sync/state-sync-driver/src/utils.rs:259`
- `storage/aptosdb/src/db/aptosdb_reader.rs:698-704`
- `storage/aptosdb/src/ledger_db/ledger_metadata_db.rs:138-143`

## Notes

This vulnerability represents a **resilience failure** rather than a traditional remote exploit. However, it qualifies as a High severity security issue because:

1. Storage corruption is a realistic failure mode in production systems
2. The impact (permanent validator failure) is severe
3. Defense-in-depth principles require graceful handling of corruption
4. The Aptos bug bounty explicitly includes "Validator node slowdowns" as High severity

The issue is exacerbated by the lack of recovery mechanisms and the deterministic nature of the failure—once corruption occurs, the validator cannot self-heal and requires manual intervention.

### Citations

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L342-343)
```rust
        let latest_epoch_state = utils::fetch_latest_epoch_state(storage.clone())
            .expect("Unable to fetch latest epoch state!");
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L696-707)
```rust
    fn get_latest_epoch_state(&self) -> Result<EpochState> {
        gauged_api("get_latest_epoch_state", || {
            let latest_ledger_info = self.ledger_db.metadata_db().get_latest_ledger_info()?;
            match latest_ledger_info.ledger_info().next_epoch_state() {
                Some(epoch_state) => Ok(epoch_state.clone()),
                None => self
                    .ledger_db
                    .metadata_db()
                    .get_epoch_state(latest_ledger_info.ledger_info().epoch()),
            }
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L1056-1062)
```rust
        ensure!(
            lis.len() == (paging_epoch - start_epoch) as usize,
            "DB corruption: missing epoch ending ledger info for epoch {}",
            lis.last()
                .map(|li| li.ledger_info().next_block_epoch() - 1)
                .unwrap_or(start_epoch),
        );
```

**File:** storage/aptosdb/src/ledger_db/ledger_metadata_db.rs (L135-154)
```rust
    pub(crate) fn get_epoch_state(&self, epoch: u64) -> Result<EpochState> {
        ensure!(epoch > 0, "EpochState only queryable for epoch >= 1.",);

        let ledger_info_with_sigs =
            self.db
                .get::<LedgerInfoSchema>(&(epoch - 1))?
                .ok_or_else(|| {
                    AptosDbError::NotFound(format!("Last LedgerInfo of epoch {}", epoch - 1))
                })?;
        let latest_epoch_state = ledger_info_with_sigs
            .ledger_info()
            .next_epoch_state()
            .ok_or_else(|| {
                AptosDbError::Other(
                    "Last LedgerInfo in epoch must carry next_epoch_state.".to_string(),
                )
            })?;

        Ok(latest_epoch_state.clone())
    }
```

**File:** storage/aptosdb/src/schema/ledger_info/mod.rs (L49-51)
```rust
    fn decode_value(data: &[u8]) -> Result<Self> {
        bcs::from_bytes(data).map_err(Into::into)
    }
```

**File:** storage/aptosdb/src/common.rs (L9-9)
```rust
pub(crate) const MAX_NUM_EPOCH_ENDING_LEDGER_INFO: usize = 100;
```

**File:** storage/schemadb/src/batch.rs (L1-100)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::{
    metrics::{APTOS_SCHEMADB_DELETES_SAMPLED, APTOS_SCHEMADB_PUT_BYTES_SAMPLED, TIMER},
    schema::{KeyCodec, Schema, ValueCodec},
    ColumnFamilyName, DB,
};
use aptos_drop_helper::DropHelper;
use aptos_metrics_core::{IntCounterVecHelper, TimerHelper};
use aptos_storage_interface::Result as DbResult;
use std::{
    collections::HashMap,
    fmt::{Debug, Formatter},
};

#[derive(Debug, Default)]
pub struct BatchStats {
    put_sizes: HashMap<ColumnFamilyName, Vec<usize>>,
    num_deletes: HashMap<ColumnFamilyName, usize>,
}

impl BatchStats {
    fn put(&mut self, cf_name: ColumnFamilyName, size: usize) {
        self.put_sizes.entry(cf_name).or_default().push(size);
    }

    fn delete(&mut self, cf_name: ColumnFamilyName) {
        *self.num_deletes.entry(cf_name).or_default() += 1
    }

    fn commit(&self) {
        for (cf_name, put_sizes) in &self.put_sizes {
            for put_size in put_sizes {
                APTOS_SCHEMADB_PUT_BYTES_SAMPLED.observe_with(&[cf_name], *put_size as f64);
            }
        }
        for (cf_name, num_deletes) in &self.num_deletes {
            APTOS_SCHEMADB_DELETES_SAMPLED.inc_with_by(&[cf_name], *num_deletes as u64);
        }
    }
}

#[derive(Debug)]
pub struct SampledBatchStats {
    inner: Option<BatchStats>,
}

impl SampledBatchStats {
    pub fn put(&mut self, cf_name: ColumnFamilyName, size: usize) {
        if let Some(inner) = self.inner.as_mut() {
            inner.put(cf_name, size)
        }
    }

    pub fn delete(&mut self, cf_name: ColumnFamilyName) {
        if let Some(inner) = self.inner.as_mut() {
            inner.delete(cf_name)
        }
    }

    pub fn commit(&self) {
        if let Some(inner) = self.inner.as_ref() {
            inner.commit()
        }
    }
}

impl Default for SampledBatchStats {
    fn default() -> Self {
        const SAMPLING_PCT: usize = 1;

        Self {
            inner: (rand::random::<usize>() % 100 < SAMPLING_PCT).then_some(Default::default()),
        }
    }
}

#[derive(Default)]
pub struct RawBatch {
    pub inner: rocksdb::WriteBatch,
    pub stats: SampledBatchStats,
}

pub trait IntoRawBatch {
    fn into_raw_batch(self, db: &DB) -> DbResult<RawBatch>;
}

impl IntoRawBatch for RawBatch {
    fn into_raw_batch(self, _db: &DB) -> DbResult<RawBatch> {
        Ok(self)
    }
}

pub trait WriteBatch: IntoRawBatch {
    fn stats(&mut self) -> &mut SampledBatchStats;

    /// Adds an insert/update operation to the batch.
    fn put<S: Schema>(&mut self, key: &S::Key, value: &S::Value) -> DbResult<()> {
        let key = <S::Key as KeyCodec<S>>::encode_key(key)?;
```
