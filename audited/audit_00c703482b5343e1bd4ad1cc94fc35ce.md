# Audit Report

## Title
Unrecoverable Validator Crash Due to Panic in Discovery Stream and Critical Subsystems

## Summary
The `ValidatorSetStream` and multiple critical validator subsystems (consensus, DKG, JWK consensus) use `.expect()` when extracting `ValidatorSet` from reconfiguration payloads. When this panic occurs, the global panic handler terminates the entire validator process via `process::exit(12)` with no recovery mechanism, causing complete validator unavailability requiring manual restart. [1](#0-0) 

## Finding Description
When a reconfiguration event is received, the discovery stream attempts to extract the `ValidatorSet` configuration using `.expect()`. If this operation fails (due to missing config, deserialization error, or database issues), it triggers a panic. [2](#0-1) 

The discovery listener is spawned as an async task via `spawn_named!` macro, which uses `tokio::spawn()` without saving the `JoinHandle`. This means there is no monitoring of task health. [3](#0-2) 

When the panic occurs, Aptos Core's global panic handler intercepts it: [4](#0-3) 

The panic handler logs the crash and calls `process::exit(12)`, terminating the **entire validator process**, not just the discovery task. The only exception is for Move verifier/deserializer panics. [5](#0-4) 

This panic handler is installed at validator startup, confirming this behavior affects production validators.

**Systemic Issue**: This same vulnerable pattern exists across multiple critical subsystems:
- **Consensus epoch manager**: [6](#0-5) 
- **DKG epoch manager**: [7](#0-6) 
- **JWK consensus epoch manager**: [8](#0-7) 

**Design Contract Violation**: The event notification system explicitly documents that subscribers must handle missing configs: [9](#0-8) 

However, all critical subsystems violate this contract by using `.expect()` instead of proper error handling.

## Impact Explanation
This is a **High Severity** issue per Aptos bug bounty criteria ("Validator node slowdowns" / "API crashes"):

1. **Complete Validator Unavailability**: Any panic in discovery, consensus, DKG, or JWK consensus crashes the entire validator process, not just the affected component
2. **Manual Restart Required**: The validator requires external intervention (manual restart or process supervisor like systemd/Kubernetes) to recover
3. **No Graceful Degradation**: Unlike typical error handling that might disable a feature or retry, this causes catastrophic failure
4. **Multiple Attack Surfaces**: Four critical subsystems share this vulnerability, multiplying risk
5. **Network Liveness Impact**: If multiple validators crash simultaneously due to a common trigger (e.g., malformed reconfiguration event), network liveness could be affected

While `ValidatorSet` should always exist after genesis, potential failure scenarios include:
- Database corruption or read failures
- BCS deserialization bugs or version mismatches  
- State sync race conditions during reconfiguration
- Disk I/O errors when reading on-chain configs [10](#0-9) 

## Likelihood Explanation
**Likelihood: Low to Medium** under normal operation, but with catastrophic consequences:

- **Low under normal conditions**: `ValidatorSet` is initialized at genesis and maintained through the staking module with strict validation
- **Medium during exceptional conditions**: Database corruption, hardware failures, or bugs in state sync/deserialization could trigger this
- **High systemic risk**: The same vulnerability pattern exists in 4 critical subsystems, creating multiple failure points
- **No mitigation layers**: Unlike other parts of the codebase that use `catch_unwind`, there's no safety net here

The severity is elevated because:
1. The design explicitly requires error handling but the implementation ignores it
2. The blast radius is entire validator process, not isolated component
3. Recovery requires external intervention (defeats Byzantine fault tolerance goals)

## Recommendation

Replace `.expect()` with proper error handling that logs the error and continues operation or gracefully degrades:

**For Discovery Stream (validator_set.rs)**:
```rust
fn extract_updates(&mut self, payload: OnChainConfigPayload<P>) -> Result<PeerSet, DiscoveryError> {
    let _process_timer = EVENT_PROCESSING_LOOP_BUSY_DURATION_S.start_timer();
    
    let node_set: ValidatorSet = payload
        .get()
        .map_err(|e| {
            error!(
                NetworkSchema::new(&self.network_context),
                "Failed to get ValidatorSet from payload: {:?}", e
            );
            DiscoveryError::Parsing(format!("ValidatorSet missing or malformed: {}", e))
        })?;
    
    // ... rest of function
}
```

Update `poll_next` to handle the error:
```rust
fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
    Pin::new(&mut self.reconfig_events)
        .poll_next(cx)
        .map(|maybe_notification| {
            maybe_notification
                .map(|notification| self.extract_updates(notification.on_chain_configs))
        })
}
```

**For Critical Subsystems (consensus, DKG, JWK)**:
These should handle the error at the `start_new_epoch` level, potentially:
- Logging the error and refusing to start the new epoch
- Falling back to the previous epoch configuration
- Initiating a controlled shutdown rather than panic

The return type should be changed to propagate errors rather than panicking.

## Proof of Concept

```rust
#[cfg(test)]
mod panic_recovery_test {
    use super::*;
    use aptos_channels::{aptos_channel, message_queues::QueueStyle};
    use aptos_event_notifications::ReconfigNotification;
    use aptos_types::on_chain_config::InMemoryOnChainConfig;
    use std::collections::HashMap;
    
    #[test]
    #[should_panic(expected = "failed to get ValidatorSet from payload")]
    fn test_missing_validator_set_causes_panic() {
        let (conn_mgr_reqs_tx, _rx) = aptos_channels::new_test(1);
        let (mut reconfig_sender, reconfig_events) = aptos_channel::new(QueueStyle::LIFO, 1, None);
        let reconfig_listener = ReconfigNotificationListener {
            notification_receiver: reconfig_events,
        };
        
        let network_context = NetworkContext::mock();
        let pubkey = x25519::PrivateKey::generate_for_testing().public_key();
        
        let mut stream = ValidatorSetStream::new(
            network_context,
            pubkey,
            reconfig_listener,
        );
        
        // Send a reconfiguration WITHOUT ValidatorSet in payload
        let empty_configs = HashMap::new();
        let payload = OnChainConfigPayload::new(1, InMemoryOnChainConfig::new(empty_configs));
        
        // This will panic when extract_updates is called
        let _ = stream.extract_updates(payload);
    }
}
```

**Demonstrating Process Exit**: In a real validator environment, when the above panic occurs, the global panic handler installed at startup will catch it and call `process::exit(12)`, terminating the entire validator process with exit code 12.

**Notes**

This vulnerability represents a critical fault tolerance failure where components designed to handle errors instead cause catastrophic process termination. The explicit documentation stating that subscribers "must be able to handle on-chain configs not existing" combined with the widespread use of `.expect()` indicates a systemic design-implementation mismatch that undermines validator availability and network resilience.

### Citations

**File:** network/discovery/src/validator_set.rs (L71-73)
```rust
        let node_set: ValidatorSet = payload
            .get()
            .expect("failed to get ValidatorSet from payload");
```

**File:** network/discovery/src/lib.rs (L127-129)
```rust
    pub fn start(self, executor: &Handle) {
        spawn_named!("DiscoveryChangeListener", executor, Box::pin(self).run());
    }
```

**File:** crates/aptos-logger/src/macros.rs (L7-9)
```rust
macro_rules! spawn_named {
      ($name:expr, $func:expr) => { tokio::spawn($func); };
      ($name:expr, $handler:expr, $func:expr) => { $handler.spawn($func); };
```

**File:** crates/crash-handler/src/lib.rs (L26-57)
```rust
pub fn setup_panic_handler() {
    panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}

// Formats and logs panic information
fn handle_panic(panic_info: &PanicHookInfo<'_>) {
    // The Display formatter for a PanicHookInfo contains the message, payload and location.
    let details = format!("{}", panic_info);
    let backtrace = format!("{:#?}", Backtrace::new());

    let info = CrashInfo { details, backtrace };
    let crash_info = toml::to_string_pretty(&info).unwrap();
    error!("{}", crash_info);
    // TODO / HACK ALARM: Write crash info synchronously via eprintln! to ensure it is written before the process exits which error! doesn't guarantee.
    // This is a workaround until https://github.com/aptos-labs/aptos-core/issues/2038 is resolved.
    eprintln!("{}", crash_info);

    // Wait till the logs have been flushed
    aptos_logger::flush();

    // Do not kill the process if the panics happened at move-bytecode-verifier.
    // This is safe because the `state::get_state()` uses a thread_local for storing states. Thus the state can only be mutated to VERIFIER by the thread that's running the bytecode verifier.
    //
    // TODO: once `can_unwind` is stable, we should assert it. See https://github.com/rust-lang/rust/issues/92988.
    if state::get_state() == VMState::VERIFIER || state::get_state() == VMState::DESERIALIZER {
        return;
    }

    // Kill the process
    process::exit(12);
```

**File:** aptos-node/src/lib.rs (L233-234)
```rust
    // Setup panic handler
    aptos_crash_handler::setup_panic_handler();
```

**File:** consensus/src/epoch_manager.rs (L1165-1167)
```rust
        let validator_set: ValidatorSet = payload
            .get()
            .expect("failed to get ValidatorSet from payload");
```

**File:** dkg/src/epoch_manager.rs (L158-160)
```rust
        let validator_set: ValidatorSet = payload
            .get()
            .expect("failed to get ValidatorSet from payload");
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L155-157)
```rust
        let validator_set: ValidatorSet = payload
            .get()
            .expect("failed to get ValidatorSet from payload");
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L277-280)
```rust
    /// Fetches the configs on-chain at the specified version.
    /// Note: We cannot assume that all configs will exist on-chain. As such, we
    /// must fetch each resource one at a time. Reconfig subscribers must be able
    /// to handle on-chain configs not existing in a reconfiguration notification.
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L400-412)
```rust
            .reader
            .get_state_value_by_version(&StateKey::on_chain_config::<T>()?, self.version)?
            .ok_or_else(|| {
                anyhow!(
                    "no config {} found in aptos root account state",
                    T::CONFIG_ID
                )
            })?
            .bytes()
            .clone();

        T::deserialize_into_config(&bytes)
    }
```
