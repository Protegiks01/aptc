# Audit Report

## Title
DKG Epoch Manager Panic on Failed Shutdown Leading to Validator DKG Participation Failure

## Summary
The DKG `EpochManager::shutdown_current_processor()` method uses `.unwrap()` when sending a shutdown signal through a oneshot channel. If the `DKGManager` task has already crashed or exited, the receiver end is dropped, causing the send to fail and the epoch manager to panic. This escalates a localized DKG manager failure into a complete DKG runtime crash, preventing the validator from participating in future DKG sessions. [1](#0-0) 

## Finding Description
The vulnerability exists in the shutdown mechanism between the DKG `EpochManager` and its spawned `DKGManager` task. When a new epoch begins, the epoch manager calls `shutdown_current_processor()` to gracefully stop the previous epoch's DKG manager. [2](#0-1) 

The shutdown process uses a `futures_channel::oneshot` channel for bidirectional acknowledgment: [3](#0-2) 

The `DKGManager::run()` method receives the close channel and processes shutdown requests: [4](#0-3) [5](#0-4) 

The shutdown is handled by `process_close_cmd()`: [6](#0-5) 

**The vulnerability:** If the `DKGManager` task panics or exits unexpectedly (due to a bug, resource exhaustion, or any runtime error), the receiver end of the close channel is dropped. When `shutdown_current_processor()` attempts to send the shutdown signal, `tx.send(ack_tx)` returns `Err` because the receiver is gone, and the `.unwrap()` causes a panic.

This panic crashes the DKG `EpochManager`, which runs in its own dedicated tokio runtime: [7](#0-6) 

The codebase shows the standard pattern for handling oneshot send failures is to ignore them with `let _ = tx.send(...)` or convert to a domain error, not to panic. This defensive handling is missing in the DKG epoch manager.

## Impact Explanation
**Severity: Medium to High**

This vulnerability causes cascading failures that affect validator participation in the DKG protocol:

1. **Validator DKG Participation Loss**: Once the DKG epoch manager crashes, the validator can no longer participate in distributed key generation for subsequent epochs, preventing the chain from generating on-chain randomness if enough validators are affected.

2. **Protocol Violation**: The DKG protocol requires honest validators to reliably participate. This bug can cause validators to unexpectedly drop out, violating liveness guarantees.

3. **Escalation of Localized Failures**: A transient error or bug in the DKG manager (which should be recoverable) gets escalated to a permanent epoch manager crash that requires validator restart.

According to Aptos bug bounty categories:
- **High Severity**: "Validator node slowdowns" and "Significant protocol violations" - the inability to participate in DKG is a significant protocol violation
- **Medium Severity**: "State inconsistencies requiring intervention" - requires validator operator intervention to restart

## Likelihood Explanation
**Likelihood: Medium**

This bug will trigger whenever:
1. The `DKGManager` task panics or exits unexpectedly (due to any bug in DKG implementation, resource exhaustion, or runtime error)
2. Followed by an epoch transition that calls `shutdown_current_processor()`

While the DKG implementation should be robust, real-world production systems can encounter:
- Unexpected panics from bugs in dependencies
- Resource exhaustion (memory, file descriptors)
- Stack overflows on deeply nested operations
- Assertion failures in development code paths

The fail point injected in the code demonstrates that panic scenarios are explicitly considered: [8](#0-7) 

## Recommendation
Replace the `.unwrap()` calls with proper error handling that logs the failure but does not panic:

```rust
async fn shutdown_current_processor(&mut self) {
    if let Some(tx) = self.dkg_manager_close_tx.take() {
        let (ack_tx, ack_rx) = oneshot::channel();
        if let Err(_) = tx.send(ack_tx) {
            warn!(
                epoch = self.epoch_state.as_ref().map(|s| s.epoch),
                "[DKG] Failed to send close signal - DKGManager already terminated"
            );
            return;
        }
        if let Err(_) = ack_rx.await {
            warn!(
                epoch = self.epoch_state.as_ref().map(|s| s.epoch),
                "[DKG] Failed to receive close acknowledgment - DKGManager already terminated"
            );
        }
    }
}
```

This allows the epoch manager to gracefully handle the case where the DKG manager has already exited, preventing panic escalation.

## Proof of Concept

```rust
#[tokio::test]
async fn test_dkg_manager_panic_causes_epoch_manager_panic() {
    use futures_channel::oneshot;
    
    // Simulate the DKG manager setup
    let (close_tx, close_rx) = oneshot::channel::<oneshot::Sender<()>>();
    
    // Simulate DKG manager task that immediately drops the receiver (simulating a panic)
    drop(close_rx);
    
    // Simulate epoch manager shutdown attempt
    let (ack_tx, _ack_rx) = oneshot::channel();
    
    // This will panic because receiver was dropped
    let result = std::panic::catch_unwind(|| {
        close_tx.send(ack_tx).unwrap(); // This unwrap panics!
    });
    
    assert!(result.is_err(), "Expected panic when receiver is already dropped");
}
```

To reproduce in the actual codebase:
1. Inject a panic in `DKGManager::process_dkg_start_event()` using the existing fail point
2. Trigger a DKG start event to cause the manager to panic
3. Trigger an epoch transition via reconfig notification
4. Observe the epoch manager panic at line 273 with "called `Result::unwrap()` on an `Err` value"

## Notes

This is a **defensive programming vulnerability** where improper error handling escalates failures. The root cause is the use of `.unwrap()` on a fallible operation that can legitimately fail if the DKG manager has crashed. The fix is straightforward: handle the error case gracefully with logging instead of panicking.

### Citations

**File:** dkg/src/epoch_manager.rs (L232-233)
```rust
            let (dkg_manager_close_tx, dkg_manager_close_rx) = oneshot::channel();
            self.dkg_manager_close_tx = Some(dkg_manager_close_tx);
```

**File:** dkg/src/epoch_manager.rs (L263-268)
```rust
    async fn on_new_epoch(&mut self, reconfig_notification: ReconfigNotification<P>) -> Result<()> {
        self.shutdown_current_processor().await;
        self.start_new_epoch(reconfig_notification.on_chain_configs)
            .await?;
        Ok(())
    }
```

**File:** dkg/src/epoch_manager.rs (L270-276)
```rust
    async fn shutdown_current_processor(&mut self) {
        if let Some(tx) = self.dkg_manager_close_tx.take() {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ack_tx).unwrap();
            ack_rx.await.unwrap();
        }
    }
```

**File:** dkg/src/dkg_manager/mod.rs (L120-129)
```rust
    pub async fn run(
        mut self,
        in_progress_session: Option<DKGSessionState>,
        mut dkg_start_event_rx: aptos_channel::Receiver<(), DKGStartEvent>,
        mut rpc_msg_rx: aptos_channel::Receiver<
            AccountAddress,
            (AccountAddress, IncomingRpcRequest),
        >,
        close_rx: oneshot::Receiver<oneshot::Sender<()>>,
    ) {
```

**File:** dkg/src/dkg_manager/mod.rs (L164-194)
```rust
        let mut close_rx = close_rx.into_stream();
        while !self.stopped {
            let handling_result = tokio::select! {
                dkg_start_event = dkg_start_event_rx.select_next_some() => {
                    self.process_dkg_start_event(dkg_start_event)
                        .await
                        .map_err(|e|anyhow!("[DKG] process_dkg_start_event failed: {e}"))
                },
                (_sender, msg) = rpc_msg_rx.select_next_some() => {
                    self.process_peer_rpc_msg(msg)
                        .await
                        .map_err(|e|anyhow!("[DKG] process_peer_rpc_msg failed: {e}"))
                },
                agg_transcript = agg_trx_rx.select_next_some() => {
                    self.process_aggregated_transcript(agg_transcript)
                        .await
                        .map_err(|e|anyhow!("[DKG] process_aggregated_transcript failed: {e}"))

                },
                dkg_txn = self.pull_notification_rx.select_next_some() => {
                    self.process_dkg_txn_pulled_notification(dkg_txn)
                        .await
                        .map_err(|e|anyhow!("[DKG] process_dkg_txn_pulled_notification failed: {e}"))
                },
                close_req = close_rx.select_next_some() => {
                    self.process_close_cmd(close_req.ok())
                },
                _ = interval.tick().fuse() => {
                    self.observe()
                },
            };
```

**File:** dkg/src/dkg_manager/mod.rs (L217-252)
```rust
    fn process_close_cmd(&mut self, ack_tx: Option<oneshot::Sender<()>>) -> Result<()> {
        self.stopped = true;

        match std::mem::take(&mut self.state) {
            InnerState::NotStarted => {},
            InnerState::InProgress { abort_handle, .. } => {
                abort_handle.abort();
            },
            InnerState::Finished {
                vtxn_guard,
                start_time,
                ..
            } => {
                let epoch_change_time = duration_since_epoch();
                let secs_since_dkg_start =
                    epoch_change_time.as_secs_f64() - start_time.as_secs_f64();
                DKG_STAGE_SECONDS
                    .with_label_values(&[self.my_addr.to_hex().as_str(), "epoch_change"])
                    .observe(secs_since_dkg_start);
                info!(
                    epoch = self.epoch_state.epoch,
                    my_addr = self.my_addr,
                    secs_since_dkg_start = secs_since_dkg_start,
                    "[DKG] txn executed and entering new epoch.",
                );

                drop(vtxn_guard);
            },
        }

        if let Some(tx) = ack_tx {
            let _ = tx.send(());
        }

        Ok(())
    }
```

**File:** dkg/src/dkg_manager/mod.rs (L433-433)
```rust
        fail_point!("dkg::process_dkg_start_event");
```

**File:** dkg/src/lib.rs (L26-56)
```rust
pub fn start_dkg_runtime(
    my_addr: AccountAddress,
    safety_rules_config: &SafetyRulesConfig,
    network_client: NetworkClient<DKGMessage>,
    network_service_events: NetworkServiceEvents<DKGMessage>,
    reconfig_events: ReconfigNotificationListener<DbBackedOnChainConfig>,
    dkg_start_events: EventNotificationListener,
    vtxn_pool: VTxnPoolState,
    rb_config: ReliableBroadcastConfig,
    randomness_override_seq_num: u64,
) -> Runtime {
    let runtime = aptos_runtimes::spawn_named_runtime("dkg".into(), Some(4));
    let (self_sender, self_receiver) = aptos_channels::new(1_024, &counters::PENDING_SELF_MESSAGES);
    let dkg_network_client = DKGNetworkClient::new(network_client);

    let dkg_epoch_manager = EpochManager::new(
        safety_rules_config,
        my_addr,
        reconfig_events,
        dkg_start_events,
        self_sender,
        dkg_network_client,
        vtxn_pool,
        rb_config,
        randomness_override_seq_num,
    );
    let (network_task, network_receiver) = NetworkTask::new(network_service_events, self_receiver);
    runtime.spawn(network_task.start());
    runtime.spawn(dkg_epoch_manager.start(network_receiver));
    runtime
}
```
