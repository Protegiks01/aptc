# Audit Report

## Title
Network-Wide Liveness Failure via Unconstrained HotStateConfig in Future OnChain Configuration

## Summary
If `HotStateConfig` parameters (`max_items_per_shard` and `refresh_interval_versions`) are converted to on-chain configuration as indicated by the TODO comment, a lack of validation would allow governance proposals to set catastrophic values causing immediate validator panics (value=0) or gradual memory exhaustion (value=usize::MAX), resulting in total network liveness failure. [1](#0-0) 

## Finding Description

The codebase contains a TODO comment indicating that `HotStateConfig` parameters should be fetched from on-chain storage, converting them from local configuration to governance-controlled on-chain configuration.

Currently, `HotStateConfig` is defined with hardcoded defaults in the local configuration: [2](#0-1) 

The `max_items_per_shard` parameter directly controls the LRU cache capacity for hot state management. During state updates, this value is used to create the LRU with: [3](#0-2) 

The critical vulnerability lies in line 198: `NonZeroUsize::new(self.hot_state_config.max_items_per_shard).unwrap()`. If `max_items_per_shard` is 0, `NonZeroUsize::new()` returns `None`, causing an immediate panic.

Following the existing OnChainConfig implementation patterns (ExecutionConfig, ConsensusConfig), there is **no validation** of the actual configuration values beyond checking for non-empty bytes: [4](#0-3) [5](#0-4) 

The Rust-side deserialization also performs no value validation: [6](#0-5) 

**Attack Scenarios:**

1. **Panic Attack (max_items_per_shard = 0)**:
   - Attacker submits governance proposal setting `max_items_per_shard: 0`
   - Proposal passes through voting (requires sufficient stake but no technical validation)
   - At next epoch/reconfiguration, all validators load the new config
   - All validators attempt to process the next block
   - All validators panic at state update when creating `HotStateLRU`
   - **Total network halt** - requires emergency hard fork

2. **Memory Exhaustion Attack (max_items_per_shard = usize::MAX)**:
   - Attacker sets `max_items_per_shard` to `usize::MAX` (18,446,744,073,709,551,615 on 64-bit)
   - LRU eviction never triggers: `while self.num_items > self.capacity.get()` never true
   - Hot state grows unbounded as transactions touch more state keys
   - Validators gradually exhaust memory at different rates (hardware-dependent)
   - **Non-deterministic consensus failure** as nodes crash at different times
   - Network degradation until quorum loss [7](#0-6) 

3. **Cache Starvation (max_items_per_shard = 1)**:
   - Setting extremely low values causes constant eviction
   - Severe performance degradation but not immediate failure
   - Could make network unusable in practice

## Impact Explanation

This vulnerability qualifies as **Critical Severity** under Aptos Bug Bounty criteria:

- **Total loss of liveness/network availability**: Setting `max_items_per_shard = 0` causes all validators to panic simultaneously, halting the entire network with no recovery mechanism except hard fork.

- **Non-recoverable network partition (requires hardfork)**: The panic occurs in the critical state update path during block execution. Once triggered, the network cannot process any blocks until the configuration is manually reverted through a hard fork.

- **Consensus Safety violations**: Setting `max_items_per_shard` to extremely large values causes non-deterministic memory exhaustion across validators, potentially leading to different nodes committing different state roots as some crash mid-execution.

This breaks multiple critical invariants:
- **Invariant 1 (Deterministic Execution)**: Memory exhaustion timing varies by hardware
- **Invariant 9 (Resource Limits)**: Unbounded memory growth violates resource constraints
- **Consensus Safety**: Network partition due to validator crashes

The StorageConfig sanitizer validates some parameters but has no checks for `hot_state_config`: [8](#0-7) 

## Likelihood Explanation

**Current State**: Not exploitable (feature doesn't exist)

**Future State (if implemented without validation)**: HIGH likelihood

The TODO comment indicates this is a planned feature. If implemented following the existing OnChainConfig pattern without adding validation:

1. **Low barrier to entry**: Any actor with sufficient stake to create/pass a governance proposal can trigger this
2. **No technical complexity**: Simply requires submitting BCS-serialized config with malicious values
3. **Guaranteed impact**: The vulnerable code path is executed on every state update
4. **Precedent exists**: Other OnChainConfigs have no value validation, suggesting this pattern would be followed

Governance proposals require stake and voting, but provide no protection against technically valid but dangerous configurations.

## Recommendation

When implementing HotStateConfig as an OnChainConfig, add strict validation:

**Move-side validation** (in the hypothetical `hot_state_config.move` module):

```move
public fun set_for_next_epoch(account: &signer, config: HotStateConfig) {
    system_addresses::assert_aptos_framework(account);
    
    // Validate max_items_per_shard bounds
    assert!(
        config.max_items_per_shard >= MIN_ITEMS_PER_SHARD && 
        config.max_items_per_shard <= MAX_ITEMS_PER_SHARD,
        error::invalid_argument(EINVALID_CACHE_SIZE)
    );
    
    // Validate refresh_interval_versions
    assert!(
        config.refresh_interval_versions >= MIN_REFRESH_INTERVAL &&
        config.refresh_interval_versions <= MAX_REFRESH_INTERVAL,
        error::invalid_argument(EINVALID_REFRESH_INTERVAL)
    );
    
    config_buffer::upsert(config);
}
```

**Rust-side validation** (defensive programming):

```rust
impl HotStateConfig {
    const MIN_ITEMS_PER_SHARD: usize = 1000;
    const MAX_ITEMS_PER_SHARD: usize = 10_000_000;
    
    pub fn validate(&self) -> Result<()> {
        ensure!(
            self.max_items_per_shard >= Self::MIN_ITEMS_PER_SHARD,
            "max_items_per_shard too low: {}", self.max_items_per_shard
        );
        ensure!(
            self.max_items_per_shard <= Self::MAX_ITEMS_PER_SHARD,
            "max_items_per_shard too high: {}", self.max_items_per_shard
        );
        Ok(())
    }
}
```

**Safe LRU initialization**:

```rust
let capacity = NonZeroUsize::new(self.hot_state_config.max_items_per_shard)
    .ok_or_else(|| anyhow!("Invalid hot state config: max_items_per_shard cannot be zero"))?;
```

Replace `.unwrap()` with proper error handling to fail gracefully rather than panic.

## Proof of Concept

**Note**: This PoC demonstrates the vulnerability that WOULD exist if HotStateConfig becomes an OnChainConfig without validation. It is not currently exploitable.

```rust
#[test]
#[should_panic(expected = "called `Option::unwrap()` on a `None` value")]
fn test_zero_max_items_causes_panic() {
    use aptos_config::config::HotStateConfig;
    use std::num::NonZeroUsize;
    
    // Simulate malicious governance proposal setting max_items_per_shard to 0
    let malicious_config = HotStateConfig {
        max_items_per_shard: 0,
        refresh_interval_versions: 100_000,
        delete_on_restart: true,
        compute_root_hash: true,
    };
    
    // This is what happens in state.rs line 198
    let _capacity = NonZeroUsize::new(malicious_config.max_items_per_shard).unwrap();
    // PANIC: thread panicked at 'called `Option::unwrap()` on a `None` value'
}

#[test]
fn test_unbounded_memory_growth_with_max_capacity() {
    use aptos_config::config::HotStateConfig;
    use std::num::NonZeroUsize;
    
    // Simulate governance proposal setting max_items_per_shard to usize::MAX
    let malicious_config = HotStateConfig {
        max_items_per_shard: usize::MAX,
        refresh_interval_versions: 100_000,
        delete_on_restart: true,
        compute_root_hash: true,
    };
    
    let capacity = NonZeroUsize::new(malicious_config.max_items_per_shard).unwrap();
    
    // With this capacity, eviction would never trigger:
    // while self.num_items > self.capacity.get() { ... }
    // 
    // Even with billions of items, num_items < usize::MAX,
    // so eviction never happens and memory grows unbounded
    
    assert_eq!(capacity.get(), usize::MAX);
    println!("Capacity set to {} - eviction will never trigger", capacity);
}
```

**Exploitability**: This vulnerability would become exploitable only when/if the TODO comment at line 371 is implemented to make HotStateConfig an OnChainConfig. The current codebase loads HotStateConfig from local files which are not governance-controlled.

### Citations

**File:** storage/aptosdb/src/state_store/mod.rs (L371-371)
```rust
        // TODO(HotState): probably fetch onchain config from storage.
```

**File:** config/src/config/storage_config.rs (L243-265)
```rust
pub struct HotStateConfig {
    /// Max number of items in each shard.
    pub max_items_per_shard: usize,
    /// Every now and then refresh `hot_since_version` for hot items to prevent them from being
    /// evicted.
    pub refresh_interval_versions: u64,
    /// Whether to delete persisted data on disk on restart. Used during development.
    pub delete_on_restart: bool,
    /// Whether we compute root hashes for hot state in executor and commit the resulting JMT to
    /// db.
    pub compute_root_hash: bool,
}

impl Default for HotStateConfig {
    fn default() -> Self {
        Self {
            max_items_per_shard: 250_000,
            refresh_interval_versions: 100_000,
            delete_on_restart: true,
            compute_root_hash: true,
        }
    }
}
```

**File:** config/src/config/storage_config.rs (L682-799)
```rust
impl ConfigSanitizer for StorageConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let config = &node_config.storage;

        let ledger_prune_window = config
            .storage_pruner_config
            .ledger_pruner_config
            .prune_window;
        let state_merkle_prune_window = config
            .storage_pruner_config
            .state_merkle_pruner_config
            .prune_window;
        let epoch_snapshot_prune_window = config
            .storage_pruner_config
            .epoch_snapshot_pruner_config
            .prune_window;
        let user_pruning_window_offset = config
            .storage_pruner_config
            .ledger_pruner_config
            .user_pruning_window_offset;

        if ledger_prune_window < 50_000_000 {
            warn!("Ledger prune_window is too small, harming network data availability.");
        }
        if state_merkle_prune_window < 100_000 {
            warn!("State Merkle prune_window is too small, node might stop functioning.");
        }
        if epoch_snapshot_prune_window < 50_000_000 {
            warn!("Epoch snapshot prune_window is too small, harming network data availability.");
        }
        if user_pruning_window_offset > 1_000_000 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset too large, so big a buffer is unlikely necessary. Set something < 1 million.".to_string(),
            ));
        }
        if user_pruning_window_offset > ledger_prune_window {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "user_pruning_window_offset is larger than the ledger prune window, the API will refuse to return any data.".to_string(),
            ));
        }

        if let Some(db_path_overrides) = config.db_path_overrides.as_ref() {
            if !config.rocksdb_configs.enable_storage_sharding {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "db_path_overrides is allowed only if sharding is enabled.".to_string(),
                ));
            }

            if let Some(ledger_db_path) = db_path_overrides.ledger_db_path.as_ref() {
                if !ledger_db_path.is_absolute() {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        format!(
                            "Path {ledger_db_path:?} in db_path_overrides is not an absolute path."
                        ),
                    ));
                }
            }

            if let Some(state_kv_db_path) = db_path_overrides.state_kv_db_path.as_ref() {
                if let Some(metadata_path) = state_kv_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = state_kv_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }

            if let Some(state_merkle_db_path) = db_path_overrides.state_merkle_db_path.as_ref() {
                if let Some(metadata_path) = state_merkle_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = state_merkle_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }

            if let Some(hot_state_merkle_db_path) =
                db_path_overrides.hot_state_merkle_db_path.as_ref()
            {
                if let Some(metadata_path) = hot_state_merkle_db_path.metadata_path.as_ref() {
                    if !metadata_path.is_absolute() {
                        return Err(Error::ConfigSanitizerFailed(
                            sanitizer_name,
                            format!("Path {metadata_path:?} in db_path_overrides is not an absolute path."),
                        ));
                    }
                }

                if let Err(e) = hot_state_merkle_db_path.get_shard_paths() {
                    return Err(Error::ConfigSanitizerFailed(sanitizer_name, e.to_string()));
                }
            }
        }

        Ok(())
    }
}
```

**File:** storage/storage-interface/src/state_store/state.rs (L197-204)
```rust
                    let mut lru = HotStateLRU::new(
                        NonZeroUsize::new(self.hot_state_config.max_items_per_shard).unwrap(),
                        Arc::clone(&persisted_hot_state),
                        overlay,
                        hot_metadata.latest.clone(),
                        hot_metadata.oldest.clone(),
                        hot_metadata.num_items,
                    );
```

**File:** aptos-move/framework/aptos-framework/sources/configs/execution_config.move (L48-52)
```text
    public fun set_for_next_epoch(account: &signer, config: vector<u8>) {
        system_addresses::assert_aptos_framework(account);
        assert!(vector::length(&config) > 0, error::invalid_argument(EINVALID_CONFIG));
        config_buffer::upsert(ExecutionConfig { config });
    }
```

**File:** aptos-move/framework/aptos-framework/sources/configs/consensus_config.move (L52-56)
```text
    public fun set_for_next_epoch(account: &signer, config: vector<u8>) {
        system_addresses::assert_aptos_framework(account);
        assert!(vector::length(&config) > 0, error::invalid_argument(EINVALID_CONFIG));
        std::config_buffer::upsert<ConsensusConfig>(ConsensusConfig {config});
    }
```

**File:** types/src/on_chain_config/mod.rs (L162-173)
```rust
    fn deserialize_default_impl(bytes: &[u8]) -> Result<Self> {
        bcs::from_bytes::<Self>(bytes)
            .map_err(|e| format_err!("[on-chain config] Failed to deserialize into config: {}", e))
    }

    // Function for deserializing bytes to `Self`
    // It will by default try one round of BCS deserialization directly to `Self`
    // The implementation for the concrete type should override this function if this
    // logic needs to be customized
    fn deserialize_into_config(bytes: &[u8]) -> Result<Self> {
        Self::deserialize_default_impl(bytes)
    }
```

**File:** storage/storage-interface/src/state_store/hot_state.rs (L82-106)
```rust
    pub fn maybe_evict(&mut self) -> Vec<(StateKey, StateSlot)> {
        let mut current = match &self.tail {
            Some(tail) => tail.clone(),
            None => {
                assert_eq!(self.num_items, 0);
                return Vec::new();
            },
        };

        let mut evicted = Vec::new();
        while self.num_items > self.capacity.get() {
            let slot = self
                .delete(&current)
                .expect("There must be entries to evict when current size is above capacity.");
            let prev_key = slot
                .prev()
                .cloned()
                .expect("There must be at least one newer entry (num_items > capacity >= 1).");
            evicted.push((current.clone(), slot.clone()));
            self.pending.insert(current, slot.to_cold());
            current = prev_key;
            self.num_items -= 1;
        }
        evicted
    }
```
