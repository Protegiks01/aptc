# Audit Report

## Title
Consensus Publisher Non-Blocking Guarantee Violated by Blocking Drop Handlers in PipelinedBlock

## Summary
The `publish_message()` function in the consensus publisher is documented as non-blocking to avoid blocking consensus operations. However, when the underlying `try_send()` fails due to channel backpressure, the dropped error triggers a chain of drop handlers that acquire blocking `std::sync::Mutex` locks, violating the non-blocking guarantee and potentially causing consensus liveness degradation.

## Finding Description

The consensus publisher's `publish_message()` function is explicitly documented as non-blocking [1](#0-0) , which is critical because it's called directly from the consensus critical path during block ordering and commit decision broadcasting [2](#0-1) .

The function uses `mpsc::Sender::try_send()` which is designed to be non-blocking [3](#0-2) . However, when `try_send()` fails (e.g., when the channel is full), it returns a `TrySendError` containing the message that failed to send. When this error is dropped after logging the warning [4](#0-3) , it triggers a complex drop chain:

1. **Error Drop**: `TrySendError<(PeerNetworkId, ConsensusObserverDirectSend)>` is dropped
2. **Message Drop**: The contained `ConsensusObserverDirectSend` is dropped
3. **Block Drop**: For `OrderedBlock` variants, this drops `Vec<Arc<PipelinedBlock>>` [5](#0-4) 
4. **Arc Drop**: When the last `Arc` reference is dropped, `PipelinedBlock::drop()` is called
5. **Blocking Operation**: The Drop implementation calls `abort_pipeline()` [6](#0-5) 

The `abort_pipeline()` function acquires **blocking mutex locks** via `aptos_infallible::Mutex`, which is a wrapper around `std::sync::Mutex` [7](#0-6) . Specifically, it locks:
- `self.pipeline_abort_handle.lock()` [8](#0-7) 
- `self.pipeline_futs.lock()` [9](#0-8) 

Additionally, the infallible mutex wrapper will **panic** if the mutex is poisoned [7](#0-6) , which could crash the entire consensus node if another thread panicked while holding the lock.

The channel can become full under normal operation as it has a default buffer size of 1000 messages [10](#0-9) . When multiple consensus observer subscribers are slow or disconnected, the channel fills up, causing `try_send()` failures.

## Impact Explanation

This vulnerability falls under **Medium Severity** (up to $10,000) based on the Aptos bug bounty criteria as it can cause:

1. **Validator Node Slowdowns**: The blocking mutex acquisition in the consensus critical path directly violates the liveness guarantee, causing unpredictable delays in block ordering and commit decision broadcasting.

2. **State Inconsistencies Requiring Intervention**: If the mutex panic occurs (due to poisoning), the validator node crashes during consensus operations, potentially requiring manual intervention to restore the node.

3. **Consensus Liveness Degradation**: Multiple validators experiencing this issue could slow down the entire network's consensus, though it doesn't cause complete safety violations or permanent liveness loss.

The impact is limited from Critical/High because:
- It doesn't directly cause consensus safety violations or fund loss
- It requires specific conditions (channel backpressure + Arc refcount reaching zero)
- It causes temporary slowdowns rather than permanent failures
- Recovery is possible without a hardfork

## Likelihood Explanation

The likelihood is **Medium** for the following reasons:

**Factors Increasing Likelihood:**
1. **Realistic Trigger Condition**: Channel backpressure is common when consensus observers are slow, disconnected, or under heavy load
2. **Multiple Call Sites**: The function is called from critical consensus paths for both ordered blocks [2](#0-1)  and commit decisions [11](#0-10) 
3. **Arc Refcount Scenarios**: Old blocks that have been processed may only be referenced in outgoing messages, making Arc refcount drops to zero plausible
4. **Multiple Subscribers**: With N subscribers, if channels fill up, multiple error drops occur in sequence, increasing the chance one hits the last Arc reference

**Factors Reducing Likelihood:**
1. **Requires Arc Refcount Zero**: The block must not be referenced elsewhere in the consensus system at the time of the error drop
2. **Channel Size Buffer**: The 1000-message buffer provides some protection against transient slowdowns
3. **Active Monitoring**: Garbage collection removes disconnected subscribers [12](#0-11) 

## Recommendation

Implement one or more of the following mitigations:

**Option 1: Use Non-Blocking Drop (Recommended)**
Replace the blocking drop handler with an async-safe implementation. Move the abort handles to a separate background task:

```rust
// In publish_message(), after try_send failure:
if let Err(error) = outbound_message_sender.try_send((*peer_network_id, message.clone())) {
    // Extract any blocks from the error and schedule cleanup asynchronously
    // instead of relying on synchronous Drop
    warn!(...);
    // Spawn cleanup task if needed
}
```

**Option 2: Pre-Abort Pipelines**
Before passing blocks to the publisher, ensure pipeline futures are already aborted:

```rust
// In buffer_manager before calling publish_message:
for block in &ordered_blocks {
    block.abort_pipeline(); // Abort proactively while not in critical path
}
consensus_publisher.publish_message(message);
```

**Option 3: Arc-Free Messages**
Change the message structure to avoid holding Arc references, using block IDs instead:

```rust
// Store blocks in a separate cache, send only references
pub struct OrderedBlockRef {
    block_ids: Vec<HashValue>,
    ordered_proof: LedgerInfoWithSignatures,
}
```

**Option 4: Fallback to Async Send**
When try_send fails, schedule the send asynchronously instead of dropping:

```rust
if let Err(error) = outbound_message_sender.try_send(msg) {
    // Instead of dropping, schedule async send
    let sender = self.outbound_message_sender.clone();
    tokio::spawn(async move {
        let _ = sender.send(error.into_inner()).await;
    });
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod blocking_drop_poc {
    use super::*;
    use futures_channel::mpsc;
    use std::sync::{Arc, Mutex as StdMutex};
    use std::time::Duration;
    use tokio::time::timeout;
    
    #[tokio::test]
    async fn test_publish_message_blocks_on_drop() {
        // Create a consensus publisher with small channel buffer
        let mut config = ConsensusObserverConfig::default();
        config.max_network_channel_size = 2; // Small buffer to trigger easily
        
        let network_id = NetworkId::Public;
        let peers_and_metadata = PeersAndMetadata::new(&[network_id]);
        let network_client = NetworkClient::new(vec![], vec![], hashmap![], peers_and_metadata);
        let consensus_observer_client = Arc::new(ConsensusObserverClient::new(network_client));
        
        let (consensus_publisher, mut receiver) = ConsensusPublisher::new(
            config,
            consensus_observer_client,
        );
        
        // Add a subscriber
        let peer = PeerNetworkId::new(network_id, PeerId::random());
        consensus_publisher.add_active_subscriber(peer);
        
        // Create a block with mutex that we can monitor
        let block = create_pipelined_block_with_mutex();
        let blocks = vec![Arc::new(block)];
        
        // Fill the channel
        for _ in 0..2 {
            let msg = ConsensusObserverMessage::new_ordered_block_message(
                blocks.clone(),
                create_dummy_ledger_info(),
            );
            consensus_publisher.publish_message(msg);
        }
        
        // This call should be non-blocking, but will block if drop handler runs
        let start = std::time::Instant::now();
        let msg = ConsensusObserverMessage::new_ordered_block_message(
            blocks.clone(),
            create_dummy_ledger_info(),
        );
        
        // Measure time - if Drop handler runs with blocking mutex, this takes longer
        let result = timeout(Duration::from_millis(100), async {
            consensus_publisher.publish_message(msg);
        }).await;
        
        let elapsed = start.elapsed();
        
        // If the function blocked acquiring mutexes in Drop, this assertion fails
        assert!(result.is_ok(), "publish_message blocked for {:?}", elapsed);
    }
}
```

## Notes

The vulnerability is subtle because:
1. The blocking behavior only manifests under specific conditions (channel full + Arc refcount zero)
2. The Drop implementation is in a different module (consensus-types) from the call site (consensus-observer)
3. The aptos_infallible::Mutex wrapper hides the fact that it's using blocking std::sync::Mutex

This represents a violation of Rust's async safety principles where operations advertised as non-blocking should never acquire blocking locks or perform blocking I/O.

### Citations

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L98-155)
```rust
    /// Garbage collect inactive subscriptions by removing peers that are no longer connected
    fn garbage_collect_subscriptions(&self) {
        // Get the set of active subscribers
        let active_subscribers = self.get_active_subscribers();

        // Get the connected peers and metadata
        let peers_and_metadata = self.consensus_observer_client.get_peers_and_metadata();
        let connected_peers_and_metadata =
            match peers_and_metadata.get_connected_peers_and_metadata() {
                Ok(connected_peers_and_metadata) => connected_peers_and_metadata,
                Err(error) => {
                    // We failed to get the connected peers and metadata
                    warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::UnexpectedError)
                        .message(&format!(
                            "Failed to get connected peers and metadata! Error: {:?}",
                            error
                        )));
                    return;
                },
            };

        // Identify the active subscribers that are no longer connected
        let connected_peers: HashSet<PeerNetworkId> =
            connected_peers_and_metadata.keys().cloned().collect();
        let disconnected_subscribers: HashSet<PeerNetworkId> = active_subscribers
            .difference(&connected_peers)
            .cloned()
            .collect();

        // Remove any subscriptions from peers that are no longer connected
        for peer_network_id in &disconnected_subscribers {
            self.remove_active_subscriber(peer_network_id);
            info!(LogSchema::new(LogEntry::ConsensusPublisher)
                .event(LogEvent::Subscription)
                .message(&format!(
                    "Removed peer subscription due to disconnection! Peer: {:?}",
                    peer_network_id
                )));
        }

        // Update the number of active subscribers for each network
        let active_subscribers = self.get_active_subscribers();
        for network_id in peers_and_metadata.get_registered_networks() {
            // Calculate the number of active subscribers for the network
            let num_active_subscribers = active_subscribers
                .iter()
                .filter(|peer_network_id| peer_network_id.network_id() == network_id)
                .count() as i64;

            // Update the active subscriber metric
            metrics::set_gauge(
                &metrics::PUBLISHER_NUM_ACTIVE_SUBSCRIBERS,
                &network_id,
                num_active_subscribers,
            );
        }
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L210-211)
```rust
    /// Publishes a direct send message to all active subscribers. Note: this method
    /// is non-blocking (to avoid blocking callers during publishing, e.g., consensus).
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L220-221)
```rust
            if let Err(error) =
                outbound_message_sender.try_send((*peer_network_id, message.clone()))
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L223-230)
```rust
                // The message send failed
                warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::SendDirectSendMessage)
                        .message(&format!(
                            "Failed to send outbound message to the receiver for peer {:?}! Error: {:?}",
                            peer_network_id, error
                    )));
            }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L400-406)
```rust
        if let Some(consensus_publisher) = &self.consensus_publisher {
            let message = ConsensusObserverMessage::new_ordered_block_message(
                ordered_blocks.clone(),
                ordered_proof.clone(),
            );
            consensus_publisher.publish_message(message);
        }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L514-518)
```rust
                if let Some(consensus_publisher) = &self.consensus_publisher {
                    let message =
                        ConsensusObserverMessage::new_commit_decision_message(commit_proof.clone());
                    consensus_publisher.publish_message(message);
                }
```

**File:** consensus/src/consensus_observer/network/observer_message.rs (L179-184)
```rust
/// OrderedBlock message contains the ordered blocks and the proof of the ordering
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub struct OrderedBlock {
    blocks: Vec<Arc<PipelinedBlock>>,
    ordered_proof: LedgerInfoWithSignatures,
}
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L361-364)
```rust
impl Drop for PipelinedBlock {
    fn drop(&mut self) {
        let _ = self.abort_pipeline();
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L529-529)
```rust
        if let Some(abort_handles) = self.pipeline_abort_handle.lock().take() {
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L546-546)
```rust
        self.pipeline_futs.lock().take()
```

**File:** crates/aptos-infallible/src/mutex.rs (L19-23)
```rust
    pub fn lock(&self) -> MutexGuard<'_, T> {
        self.0
            .lock()
            .expect("Cannot currently handle a poisoned lock")
    }
```

**File:** config/src/config/consensus_observer_config.rs (L28-28)
```rust
    pub max_network_channel_size: u64,
```
