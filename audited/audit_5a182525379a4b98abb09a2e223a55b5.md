# Audit Report

## Title
Chunked Module Publishing Bypasses Module Size Validation Allowing Arbitrary Large Module Deployment

## Summary
The chunked publishing mechanism allows attackers to deploy Move modules of arbitrary size by bypassing the intended MAX_MODULE_SIZE limit (65,355 bytes). Individual modules are chunked without size validation, and the production bytecode verifier does not enforce serialized module size limits, enabling storage bloat and potential consensus issues.

## Finding Description

The vulnerability exists across multiple components in the module publishing pipeline:

**1. Missing Size Check Before Chunking:**
In the transaction generator, when `publish_transaction_payload()` determines whether to use chunked publishing, it only checks the total package size, not individual module sizes. [1](#0-0) 

The check at line 208 compares the total serialized size against `CHUNK_SIZE_IN_BYTES` but does not validate individual module sizes against any maximum limit.

**2. Unrestricted Module Chunking:**
Individual modules are split into chunks without any size validation: [2](#0-1) 

At line 61, `create_chunks(module_code, chunk_size)` splits a module of any size into manageable chunks. There is no check to ensure the total `module_code` size is within acceptable limits before chunking.

**3. On-Chain Assembly Without Validation:**
On-chain, chunks are accumulated and reassembled without size validation: [3](#0-2) [4](#0-3) 

The `stage_code_chunk_internal` function accumulates chunks in a SmartTable, and `assemble_module_code` reconstructs the full modules without checking their final sizes.

**4. Missing Production Verifier Size Check:**
The bytecode verifier has a `MAX_MODULE_SIZE` constant (65,355 bytes) but only enforces it in test code: [5](#0-4) 

The test function `verify_module_with_config_for_test` checks module size at lines 106-130, but the production function `verify_module_with_config` does not: [6](#0-5) 

**5. LimitsVerifier Only Checks Structural Limits:**
The `LimitsVerifier` validates structural properties (max functions, max structs, etc.) but not serialized module size: [7](#0-6) 

**Attack Scenario:**
1. Attacker creates a Move module with serialized size of 10 MB (far exceeding the 65,355 byte test limit)
2. Uses the Aptos CLI with `--chunked-publish` flag or SDK's chunked publishing API
3. The module is split into ~182 chunks (at 55,000 bytes per chunk)
4. Each chunk transaction is under the transaction size limit (~1 MB) and accepted by validators
5. Chunks are staged on-chain via `large_packages::stage_code_chunk`
6. Final transaction calls `stage_code_chunk_and_publish_to_account`, assembling the 10 MB module
7. Module verification passes because production verifier doesn't check serialized size
8. Attacker can repeat this for multiple modules, causing significant storage bloat

**Broken Invariants:**
- **Resource Limits** (Invariant #9): The system fails to enforce storage and computational limits on individual module sizes
- **Deterministic Execution** (Invariant #1): If different validators have different implicit infrastructure limits for module sizes, this could cause non-deterministic behavior

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: Extremely large modules (10+ MB) can cause:
   - Slow module loading and verification times
   - Increased memory consumption during verification
   - Degraded performance when the module is accessed during transaction execution

2. **Significant Protocol Violations**: The bypass of intended size limits violates the resource constraint invariant that underpins the blockchain's security model.

3. **Storage Bloat**: Attackers can fill on-chain storage with arbitrarily large modules, forcing validators to maintain excessive state. This increases operational costs for all validators.

4. **State Sync Degradation**: New validators syncing state must download and process these oversized modules, significantly slowing synchronization.

5. **Potential Consensus Risk**: If different validator implementations have different implicit limits on module handling (e.g., memory allocations, deserialization buffers), extremely large modules could cause divergent behavior across the validator set.

## Likelihood Explanation

**Likelihood: High**

The attack is highly likely because:

1. **Ease of Exploitation**: The chunked publishing mechanism is a documented feature accessible via CLI (`--chunked-publish`) and SDK APIs. No special permissions or validator access required.

2. **No Economic Barrier**: While publishing large modules requires gas fees, the cost is proportional to transaction count, not module size. An attacker can publish a 10 MB module for the cost of ~200 transactions.

3. **Immediate Impact**: Each published oversized module immediately affects all validators who must store and potentially process it.

4. **No Detection**: There are no obvious warnings or checks that would alert validators to abnormally large modules being published.

5. **Amplification**: An attacker with modest resources can publish multiple large modules, multiplying the impact.

## Recommendation

Implement module size validation at multiple layers:

**1. Pre-Chunking Validation:**
Add size check in `publish_transaction_payload()`:

```rust
pub fn publish_transaction_payload(&self, chain_id: &ChainId) -> Vec<TransactionPayload> {
    let (metadata_serialized, code) = self.get_publish_args();
    
    // Validate individual module sizes before chunking
    const MAX_MODULE_SIZE: usize = 65_355; // Match test limit
    for (idx, module_code) in code.iter().enumerate() {
        if module_code.len() > MAX_MODULE_SIZE {
            panic!("Module {} exceeds maximum size: {} > {}", 
                   idx, module_code.len(), MAX_MODULE_SIZE);
        }
    }
    
    if metadata_serialized.len() + code.iter().map(|v| v.len()).sum::<usize>()
        > CHUNK_SIZE_IN_BYTES
    {
        // ... existing chunking logic
    }
}
```

**2. Production Verifier Validation:**
Enforce MAX_MODULE_SIZE in `verify_module_with_config()`:

```rust
pub fn verify_module_with_config(config: &VerifierConfig, module: &CompiledModule) -> VMResult<()> {
    if config.verify_nothing() {
        return Ok(());
    }
    
    // Add module size check in production
    const MAX_MODULE_SIZE: usize = 65_355;
    let mut bytes = vec![];
    module.serialize(&mut bytes)
        .map_err(|e| PartialVMError::new(StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR)
            .with_message(format!("Module serialization failed: {}", e))
            .finish(Location::Undefined))?;
    
    if bytes.len() > MAX_MODULE_SIZE {
        return Err(PartialVMError::new(StatusCode::MODULE_TOO_LARGE)
            .with_message(format!("Module size {} exceeds maximum {}", bytes.len(), MAX_MODULE_SIZE))
            .finish(Location::Undefined));
    }
    
    // ... existing verification logic
}
```

**3. On-Chain Validation:**
Add size check in `large_packages.move` during assembly:

```move
inline fun assemble_module_code(staging_area: &mut StagingArea): vector<vector<u8>> {
    const MAX_MODULE_SIZE: u64 = 65355;
    let last_module_idx = staging_area.last_module_idx;
    let code = vector[];
    let i = 0;
    while (i <= last_module_idx) {
        let module_bytes = *smart_table::borrow(&staging_area.code, i);
        assert!(
            vector::length(&module_bytes) <= MAX_MODULE_SIZE,
            error::invalid_argument(EMODULE_TOO_LARGE)
        );
        vector::push_back(&mut code, module_bytes);
        i = i + 1;
    };
    code
}
```

**4. Add VerifierConfig Field:**
Make module size limit configurable via VerifierConfig:

```rust
pub struct VerifierConfig {
    // ... existing fields
    pub max_module_size: Option<usize>,
}
```

## Proof of Concept

**Rust Test Demonstrating Bypass:**

```rust
#[test]
fn test_oversized_module_via_chunked_publish() {
    use move_binary_format::file_format::{
        CompiledModule, empty_module, Visibility, FunctionDefinition, 
        CodeUnit, Bytecode, Signature, SignatureToken
    };
    use aptos_framework::chunked_publish::{chunk_package_and_create_payloads, PublishType, CHUNK_SIZE_IN_BYTES};
    
    // Create a module with many functions to exceed 65KB
    let mut module = empty_module();
    
    // Add 1000 large functions to create a module > 65KB
    for i in 0..1000 {
        let mut code = CodeUnit::default();
        // Add many NOP instructions to bloat the code
        for _ in 0..100 {
            code.code.push(Bytecode::Nop);
        }
        code.code.push(Bytecode::Ret);
        
        let func_def = FunctionDefinition {
            function: module.function_handles.len() as u16,
            visibility: Visibility::Public,
            is_entry: false,
            acquires_global_resources: vec![],
            code: Some(code),
        };
        module.function_defs.push(func_def);
    }
    
    // Serialize the oversized module
    let mut module_bytes = vec![];
    module.serialize(&mut module_bytes).unwrap();
    
    println!("Module size: {} bytes (exceeds MAX_MODULE_SIZE: 65355)", module_bytes.len());
    assert!(module_bytes.len() > 65_355, "Module should exceed test size limit");
    
    // Demonstrate chunked publishing bypasses size check
    let metadata = bcs::to_bytes(&PackageMetadata::default()).unwrap();
    let package_code = vec![module_bytes];
    
    // This succeeds even though module exceeds MAX_MODULE_SIZE
    let payloads = chunk_package_and_create_payloads(
        metadata,
        package_code,
        PublishType::AccountDeploy,
        None,
        AccountAddress::ONE,
        CHUNK_SIZE_IN_BYTES,
    );
    
    println!("Successfully created {} chunk payloads for oversized module", payloads.len());
    assert!(payloads.len() > 1, "Should require multiple chunks");
    
    // In production, these chunks would be submitted as separate transactions
    // and assembled on-chain without size validation
}
```

**Move Integration Test:**

```move
#[test(publisher = @0x123)]
fun test_publish_oversized_module_via_chunking(publisher: &signer) {
    // Create an oversized module (simulated with large vector constant)
    // In practice, this would be a legitimate compiled module > 65KB
    let oversized_module_bytes = vector::empty<u8>();
    let i = 0;
    // Create 100KB of module data
    while (i < 100000) {
        vector::push_back(&mut oversized_module_bytes, (i % 256 as u8));
        i = i + 1;
    };
    
    let metadata = x""; // Empty metadata for simplicity
    let chunk_size = 55000; // CHUNK_SIZE_IN_BYTES
    
    // Chunk the oversized module
    let chunks = chunk_data(oversized_module_bytes, chunk_size);
    
    // Stage chunks via large_packages (would be multiple transactions)
    let chunk_idx = 0;
    while (chunk_idx < vector::length(&chunks) - 1) {
        large_packages::stage_code_chunk(
            publisher,
            vector::empty(),
            vector[0], // module index 0
            vector[*vector::borrow(&chunks, chunk_idx)]
        );
        chunk_idx = chunk_idx + 1;
    };
    
    // Final chunk triggers assembly and publish
    large_packages::stage_code_chunk_and_publish_to_account(
        publisher,
        metadata,
        vector[0],
        vector[*vector::borrow(&chunks, chunk_idx)]
    );
    
    // Module is now published despite exceeding intended size limit
    // No error was raised during chunking or assembly
}
```

**Notes:**
The vulnerability exists because the intended module size limit (65,355 bytes as defined in test code) is not enforced in production. The chunked publishing mechanism allows this limit to be bypassed by splitting large modules into smaller chunks that individually pass transaction size validation. This breaks the Resource Limits invariant and could lead to validator node slowdowns, storage bloat, and potential consensus issues if different validator implementations handle extremely large modules differently.

### Citations

**File:** crates/transaction-generator-lib/src/publishing/publish_util.rs (L205-226)
```rust
    pub fn publish_transaction_payload(&self, chain_id: &ChainId) -> Vec<TransactionPayload> {
        let (metadata_serialized, code) = self.get_publish_args();

        if metadata_serialized.len() + code.iter().map(|v| v.len()).sum::<usize>()
            > CHUNK_SIZE_IN_BYTES
        {
            chunk_package_and_create_payloads(
                metadata_serialized,
                code,
                PublishType::AccountDeploy,
                None,
                AccountAddress::from_str_strict(default_large_packages_module_address(chain_id))
                    .unwrap(),
                CHUNK_SIZE_IN_BYTES,
            )
        } else {
            vec![aptos_stdlib::code_publish_package_txn(
                metadata_serialized,
                code,
            )]
        }
    }
```

**File:** aptos-move/framework/src/chunked_publish.rs (L60-82)
```rust
    for (idx, module_code) in package_code.into_iter().enumerate() {
        let chunked_module = create_chunks(module_code, chunk_size);
        for chunk in chunked_module {
            if taken_size + chunk.len() > chunk_size {
                // Create a payload and reset accumulators
                let payload = large_packages_stage_code_chunk(
                    metadata_chunk,
                    code_indices.clone(),
                    code_chunks.clone(),
                    large_packages_module_address,
                );
                payloads.push(payload);

                metadata_chunk = vec![];
                code_indices.clear();
                code_chunks.clear();
                taken_size = 0;
            }

            code_indices.push(idx as u16);
            taken_size += chunk.len();
            code_chunks.push(chunk);
        }
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L162-178)
```text
        let i = 0;
        while (i < vector::length(&code_chunks)) {
            let inner_code = *vector::borrow(&code_chunks, i);
            let idx = (*vector::borrow(&code_indices, i) as u64);

            if (smart_table::contains(&staging_area.code, idx)) {
                vector::append(
                    smart_table::borrow_mut(&mut staging_area.code, idx), inner_code
                );
            } else {
                smart_table::add(&mut staging_area.code, idx, inner_code);
                if (idx > staging_area.last_module_idx) {
                    staging_area.last_module_idx = idx;
                }
            };
            i = i + 1;
        };
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L213-225)
```text
    inline fun assemble_module_code(staging_area: &mut StagingArea): vector<vector<u8>> {
        let last_module_idx = staging_area.last_module_idx;
        let code = vector[];
        let i = 0;
        while (i <= last_module_idx) {
            vector::push_back(
                &mut code,
                *smart_table::borrow(&staging_area.code, i)
            );
            i = i + 1;
        };
        code
    }
```

**File:** third_party/move/move-bytecode-verifier/src/verifier.rs (L92-132)
```rust
pub fn verify_module_with_config_for_test(
    name: &str,
    config: &VerifierConfig,
    module: &CompiledModule,
) -> VMResult<()> {
    verify_module_with_config_for_test_with_version(name, config, module, None)
}

pub fn verify_module_with_config_for_test_with_version(
    name: &str,
    config: &VerifierConfig,
    module: &CompiledModule,
    bytecode_version: Option<u32>,
) -> VMResult<()> {
    const MAX_MODULE_SIZE: usize = 65355;
    let mut bytes = vec![];
    module
        .serialize_for_version(bytecode_version, &mut bytes)
        .unwrap();
    let now = Instant::now();
    let result = verify_module_with_config(config, module);
    eprintln!(
        "--> {}: verification time: {:.3}ms, result: {}, size: {}kb",
        name,
        (now.elapsed().as_micros() as f64) / 1000.0,
        if let Err(e) = &result {
            format!("{:?}", e.major_status())
        } else {
            "Ok".to_string()
        },
        bytes.len() / 1000
    );
    // Also check whether the module actually fits into our payload size
    assert!(
        bytes.len() <= MAX_MODULE_SIZE,
        "test module exceeds size limit {} (given size {})",
        MAX_MODULE_SIZE,
        bytes.len()
    );
    result
}
```

**File:** third_party/move/move-bytecode-verifier/src/verifier.rs (L134-173)
```rust
pub fn verify_module_with_config(config: &VerifierConfig, module: &CompiledModule) -> VMResult<()> {
    if config.verify_nothing() {
        return Ok(());
    }
    let prev_state = move_core_types::state::set_state(VMState::VERIFIER);
    let result = std::panic::catch_unwind(|| {
        // Always needs to run bound checker first as subsequent passes depend on it
        BoundsChecker::verify_module(module).map_err(|e| {
            // We can't point the error at the module, because if bounds-checking
            // failed, we cannot safely index into module's handle to itself.
            e.finish(Location::Undefined)
        })?;
        FeatureVerifier::verify_module(config, module)?;
        LimitsVerifier::verify_module(config, module)?;
        DuplicationChecker::verify_module(module)?;

        signature_v2::verify_module(config, module)?;

        InstructionConsistency::verify_module(module)?;
        constants::verify_module(module)?;
        friends::verify_module(module)?;

        RecursiveStructDefChecker::verify_module(module)?;
        InstantiationLoopChecker::verify_module(module)?;
        CodeUnitVerifier::verify_module(config, module)?;

        // Add the failpoint injection to test the catch_unwind behavior.
        fail::fail_point!("verifier-failpoint-panic");

        script_signature::verify_module(module, no_additional_script_signature_checks)
    })
    .unwrap_or_else(|_| {
        Err(
            PartialVMError::new(StatusCode::VERIFIER_INVARIANT_VIOLATION)
                .finish(Location::Undefined),
        )
    });
    move_core_types::state::set_state(prev_state);
    result
}
```

**File:** third_party/move/move-bytecode-verifier/src/limits.rs (L19-35)
```rust
    pub fn verify_module(config: &VerifierConfig, module: &'a CompiledModule) -> VMResult<()> {
        Self::verify_module_impl(config, module)
            .map_err(|e| e.finish(Location::Module(module.self_id())))
    }

    fn verify_module_impl(
        config: &VerifierConfig,
        module: &'a CompiledModule,
    ) -> PartialVMResult<()> {
        let limit_check = Self {
            resolver: BinaryIndexedView::Module(module),
        };
        limit_check.verify_function_handles(config)?;
        limit_check.verify_struct_handles(config)?;
        limit_check.verify_type_nodes(config)?;
        limit_check.verify_definitions(config)
    }
```
