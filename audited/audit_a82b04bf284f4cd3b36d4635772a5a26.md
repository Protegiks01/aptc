# Audit Report

## Title
Byzantine Validators Can Evade Accountability Through Failed Proposer Recording Limit (Round Skip Attack)

## Summary
The `compute_failed_authors()` function in the proposal generator limits the number of recorded failed proposers to `max_failed_authors_to_store` (default: 10). When more than 10 consecutive rounds fail, only the last 10 failed proposers are recorded in block metadata and subsequently tracked in validator performance statistics. This allows Byzantine validators positioned early in a sequence of consecutive failures to completely evade accountability, maintaining their high reputation weight (1000x) for future leader selection despite deliberately skipping their proposer duties.

## Finding Description

The Aptos consensus protocol uses a deterministic proposer election mechanism where `get_valid_proposer()` calculates which validator should propose for each round. [1](#0-0) 

When rounds fail (due to proposers not broadcasting blocks), the protocol is designed to detect and record these failures through the `compute_failed_authors()` function. [2](#0-1) 

However, this function contains a critical limitation at line 895 that caps the recording window using `max_failed_authors_to_store`, which defaults to 10. [3](#0-2) 

**Attack Flow:**

1. **Setup**: Byzantine validators (< 1/3 of total) who happen to be positioned consecutively in the validator ordering coordinate to skip their proposer rounds simultaneously.

2. **Execution**: When 15+ consecutive rounds fail (each round timing out because the designated proposer refuses to broadcast a block), the `compute_failed_authors()` function calculates:
   - `start = max(previous_round + 1, end_round - 10)`
   - Only rounds from `start` to `end_round` are recorded

3. **Evasion**: The first 5+ Byzantine validators in the sequence have their failures completely omitted from the `failed_authors` vector returned by the function.

4. **Propagation**: The incomplete `failed_authors` list is converted to `failed_proposer_indices` and embedded in the next successful block's metadata. [4](#0-3) 

5. **Performance Tracking Bypass**: The `update_performance_statistics()` function only increments `failed_proposals` for validators whose indices appear in `failed_proposer_indices`. [5](#0-4) 

6. **Reputation System Manipulation**: The leader reputation system calculates weights where validators with failure rates > 10% receive `failed_weight = 1`, while others receive `active_weight = 1000`. [6](#0-5) 

7. **Sustained Attack**: Byzantine validators whose failures weren't recorded maintain their high reputation weight, giving them 1000x higher probability of being selected as future proposers despite their misbehavior.

**Broken Invariants:**
- **Staking Security**: Validator penalties are not calculated correctly (Invariant #6)
- **Consensus Liveness**: Byzantine validators can repeatedly delay block production without long-term consequences

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for "Significant protocol violations."

**Specific Impacts:**

1. **Degraded Reputation System**: The leader reputation mechanism, designed to penalize misbehaving validators, becomes ineffective when Byzantine validators can evade failure recording.

2. **Sustained Liveness Degradation**: Byzantine validators can repeatedly coordinate round-skip attacks without accumulating sufficient penalties to reduce their selection probability, leading to persistent delays in block production.

3. **Unfair Validator Selection**: Well-behaved validators are disadvantaged as Byzantine validators maintain disproportionately high selection weights despite poor performance.

4. **Economic Manipulation**: Byzantine validators avoid the intended economic disincentives (reduced rewards due to failed proposals) while continuing to earn rewards when they do propose.

While this does not constitute a consensus safety violation (no double-spending or chain splits), it represents a significant weakening of the protocol's defense mechanisms against Byzantine behavior, particularly affecting liveness guarantees.

## Likelihood Explanation

**Likelihood: Medium-High**

**Favorable Conditions for Attackers:**

1. **Validator Positioning**: With 100 validators and 30 Byzantine (approaching the 1/3 threshold), the probability of having 11+ consecutive Byzantine validators in the rotation is non-negligible due to:
   - Random distribution in validator ordering based on stake pool addresses
   - Ability for attackers to time validator joins to increase consecutive positioning
   - Stable ordering across epochs (new validators appended to end)

2. **No Technical Barriers**: The attack requires only:
   - Coordination among Byzantine validators (already assumed in BFT threat model)
   - Simply refusing to broadcast proposals (passive attack)
   - No cryptographic breaks or complex exploit chains required

3. **Detection Difficulty**: The protocol correctly identifies that proposers failed but doesn't record all failures, making the attack appear as "normal" timeout behavior rather than coordinated misbehavior.

**Mitigating Factors:**

- Requires multiple Byzantine validators to be positioned consecutively
- Each instance only allows a subset to evade accountability
- Repeated attacks may eventually be noticed by network operators

## Recommendation

**Immediate Fix**: Remove or significantly increase the `max_failed_authors_to_store` limit to ensure all failed proposers since the last successful block are recorded.

**Option 1 - Remove the Limit**:
```rust
pub fn compute_failed_authors(
    &self,
    round: Round,
    previous_round: Round,
    include_cur_round: bool,
    proposer_election: Arc<dyn ProposerElection>,
) -> Vec<(Round, Author)> {
    let end_round = round + u64::from(include_cur_round);
    let mut failed_authors = Vec::new();
    
    // Record ALL failures since last successful block
    let start = previous_round + 1;
    
    for i in start..end_round {
        failed_authors.push((i, proposer_election.get_valid_proposer(i)));
    }
    
    failed_authors
}
```

**Option 2 - Increase Limit with Safeguard**:
If storage/performance concerns exist, increase the limit substantially (e.g., to 100) and add monitoring:
- Set `max_failed_authors_to_store` to 100+ via on-chain config
- Add alerts when consecutive failures approach the limit
- Implement emergency response procedures for extended outages

**Additional Hardening**:
1. Add accumulative tracking across multiple blocks to catch validators who consistently avoid periods of their failures being recorded
2. Implement sliding window tracking that maintains failure history beyond single block boundaries
3. Add gossip-layer reputation tracking as a defense-in-depth measure

## Proof of Concept

**Scenario Setup:**
- Network with 100 validators
- 15 Byzantine validators positioned at indices 20-34 in rotation
- `max_failed_authors_to_store = 10` (default)
- `contiguous_rounds = 1` (each validator proposes once per rotation)

**Attack Execution:**

```rust
// Conceptual PoC (not compilable as standalone)

// Round 100: Last successful block by validator at index 19
// Rounds 101-115: Byzantine validators at indices 20-34 all refuse to propose

// When validator at index 35 proposes at round 116:
let previous_round = 100; // Last successful block
let current_round = 116;
let max_failed_authors = 10;

// compute_failed_authors calculation:
let start = max(previous_round + 1, current_round - max_failed_authors);
// start = max(101, 106) = 106

// Only rounds 106-115 are recorded (10 rounds)
// Validators at indices 25-34 get their failures recorded
// Validators at indices 20-24 (5 validators) ESCAPE recording

// Result in stake.move update_performance_statistics:
// - Validators 20-24: failed_proposals unchanged (VULNERABILITY)
// - Validators 25-34: failed_proposals incremented correctly

// Impact on leader_reputation.rs:
// - Validators 20-24 maintain active_weight = 1000
// - If they had been recorded, they'd receive failed_weight = 1 after 10% threshold
// - They have 1000x higher selection probability than they should
```

**Verification Steps:**
1. Deploy testnet with 15 consecutive Byzantine validators
2. Configure them to skip proposals for rounds 101-115
3. Observe block at round 116's `failed_proposer_indices`
4. Verify only 10 entries present (indices 25-34)
5. Check `ValidatorPerformance` resource to confirm validators 20-24 have no `failed_proposals` increment
6. Query leader reputation weights to confirm they maintain `active_weight`

## Notes

**Additional Context:**

1. The vulnerability affects ALL proposer election types (RotatingProposer, LeaderReputation, RoundProposer) because `compute_failed_authors()` is used universally for tracking failures regardless of election mechanism.

2. The default on-chain configuration uses LeaderReputation with ProposerAndVoterV2, making the reputation bypass particularly impactful since the entire system is designed around performance-based selection.

3. The limit was likely introduced for storage/gas optimization (limiting block metadata size), but the security implications were not fully considered. The comment at line 243 refers to "trimming" without discussing accountability concerns.

4. This vulnerability compounds with network conditions - during genuine network issues where multiple validators timeout, Byzantine validators can "hide" their intentional failures among legitimate ones.

5. While validator ordering is not directly manipulable, attackers controlling multiple validators can strategically time their joining to increase the probability of consecutive positioning, especially during periods of validator set expansion.

### Citations

**File:** consensus/src/liveness/rotating_proposer_election.rs (L36-39)
```rust
    fn get_valid_proposer(&self, round: Round) -> Author {
        self.proposers
            [((round / u64::from(self.contiguous_rounds)) % self.proposers.len() as u64) as usize]
    }
```

**File:** consensus/src/liveness/proposal_generator.rs (L882-902)
```rust
    /// Compute the list of consecutive proposers from the
    /// immediately preceeding rounds that didn't produce a successful block
    pub fn compute_failed_authors(
        &self,
        round: Round,
        previous_round: Round,
        include_cur_round: bool,
        proposer_election: Arc<dyn ProposerElection>,
    ) -> Vec<(Round, Author)> {
        let end_round = round + u64::from(include_cur_round);
        let mut failed_authors = Vec::new();
        let start = std::cmp::max(
            previous_round + 1,
            end_round.saturating_sub(self.max_failed_authors_to_store as u64),
        );
        for i in start..end_round {
            failed_authors.push((i, proposer_election.get_valid_proposer(i)));
        }

        failed_authors
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L487-487)
```rust
            max_failed_authors_to_store: 10,
```

**File:** aptos-move/framework/aptos-framework/sources/block.move (L154-199)
```text
    fun block_prologue_common(
        vm: &signer,
        hash: address,
        epoch: u64,
        round: u64,
        proposer: address,
        failed_proposer_indices: vector<u64>,
        previous_block_votes_bitvec: vector<u8>,
        timestamp: u64
    ): u64 acquires BlockResource, CommitHistory {
        // Operational constraint: can only be invoked by the VM.
        system_addresses::assert_vm(vm);

        // Blocks can only be produced by a valid proposer or by the VM itself for Nil blocks (no user txs).
        assert!(
            proposer == @vm_reserved || stake::is_current_epoch_validator(proposer),
            error::permission_denied(EINVALID_PROPOSER),
        );

        let proposer_index = option::none();
        if (proposer != @vm_reserved) {
            proposer_index = option::some(stake::get_validator_index(proposer));
        };

        let block_metadata_ref = borrow_global_mut<BlockResource>(@aptos_framework);
        block_metadata_ref.height = event::counter(&block_metadata_ref.new_block_events);

        let new_block_event = NewBlockEvent {
            hash,
            epoch,
            round,
            height: block_metadata_ref.height,
            previous_block_votes_bitvec,
            proposer,
            failed_proposer_indices,
            time_microseconds: timestamp,
        };
        emit_new_block_event(vm, &mut block_metadata_ref.new_block_events, new_block_event);

        // Performance scores have to be updated before the epoch transition as the transaction that triggers the
        // transition is the last block in the previous epoch.
        stake::update_performance_statistics(proposer_index, failed_proposer_indices);
        state_storage::on_new_block(reconfiguration::current_epoch());

        block_metadata_ref.epoch_interval
    }
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L1282-1332)
```text
    public(friend) fun update_performance_statistics(
        proposer_index: Option<u64>,
        failed_proposer_indices: vector<u64>
    ) acquires ValidatorPerformance {
        // Validator set cannot change until the end of the epoch, so the validator index in arguments should
        // match with those of the validators in ValidatorPerformance resource.
        let validator_perf = borrow_global_mut<ValidatorPerformance>(@aptos_framework);
        let validator_len = vector::length(&validator_perf.validators);

        spec {
            update ghost_valid_perf = validator_perf;
            update ghost_proposer_idx = proposer_index;
        };
        // proposer_index is an option because it can be missing (for NilBlocks)
        if (option::is_some(&proposer_index)) {
            let cur_proposer_index = option::extract(&mut proposer_index);
            // Here, and in all other vector::borrow, skip any validator indices that are out of bounds,
            // this ensures that this function doesn't abort if there are out of bounds errors.
            if (cur_proposer_index < validator_len) {
                let validator = vector::borrow_mut(&mut validator_perf.validators, cur_proposer_index);
                spec {
                    assume validator.successful_proposals + 1 <= MAX_U64;
                };
                validator.successful_proposals = validator.successful_proposals + 1;
            };
        };

        let f = 0;
        let f_len = vector::length(&failed_proposer_indices);
        while ({
            spec {
                invariant len(validator_perf.validators) == validator_len;
                invariant (option::is_some(ghost_proposer_idx) && option::borrow(
                    ghost_proposer_idx
                ) < validator_len) ==>
                    (validator_perf.validators[option::borrow(ghost_proposer_idx)].successful_proposals ==
                        ghost_valid_perf.validators[option::borrow(ghost_proposer_idx)].successful_proposals + 1);
            };
            f < f_len
        }) {
            let validator_index = *vector::borrow(&failed_proposer_indices, f);
            if (validator_index < validator_len) {
                let validator = vector::borrow_mut(&mut validator_perf.validators, validator_index);
                spec {
                    assume validator.failed_proposals + 1 <= MAX_U64;
                };
                validator.failed_proposals = validator.failed_proposals + 1;
            };
            f = f + 1;
        };
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L522-550)
```rust
    fn get_weights(
        &self,
        epoch: u64,
        epoch_to_candidates: &HashMap<u64, Vec<Author>>,
        history: &[NewBlockEvent],
    ) -> Vec<u64> {
        assert!(epoch_to_candidates.contains_key(&epoch));

        let (votes, proposals, failed_proposals) =
            self.aggregation
                .get_aggregated_metrics(epoch_to_candidates, history, &self.author);

        epoch_to_candidates[&epoch]
            .iter()
            .map(|author| {
                let cur_votes = *votes.get(author).unwrap_or(&0);
                let cur_proposals = *proposals.get(author).unwrap_or(&0);
                let cur_failed_proposals = *failed_proposals.get(author).unwrap_or(&0);

                if cur_failed_proposals * 100
                    > (cur_proposals + cur_failed_proposals) * self.failure_threshold_percent
                {
                    self.failed_weight
                } else if cur_proposals > 0 || cur_votes > 0 {
                    self.active_weight
                } else {
                    self.inactive_weight
                }
            })
```
