# Audit Report

## Title
Race Condition in Hot State LRU Eviction Causes Permanent Validator Node Crash

## Summary
A critical race condition exists between the hot state LRU eviction logic and the asynchronous hot state committer thread. When `maybe_evict()` executes with stale metadata while the committer thread modifies the underlying hash map, the `expect()` calls panic, causing the validator node to permanently crash via `process::exit(12)` with no recovery mechanism.

## Finding Description

The vulnerability stems from an unsynchronized read pattern in the hot state management system: [1](#0-0) 

When `get_committed()` is called, it returns:
1. A **cloned snapshot** of the State metadata (head, tail, num_items)
2. A **shared Arc pointer** to the live HotStateBase hash map

The committer thread asynchronously updates this shared hash map: [2](#0-1) 

The race occurs during execution when `State::update()` creates a `HotStateLRU`: [3](#0-2) 

The `HotStateLRU` is initialized with **stale metadata** but reads from the **live, concurrently-modified hash map**. When `maybe_evict()` executes: [4](#0-3) 

**Critical Failure Points:**

1. **Line 95**: `delete(&current).expect(...)` - If the committer has removed the entry between metadata read and access, this panics
2. **Line 99**: `slot.prev().expect(...)` - If the committer has modified the linked list structure, this panics

The execution path is: [5](#0-4) 

**Panic Recovery:**

When the panic occurs, the crash handler terminates the process: [6](#0-5) 

The handler calls `process::exit(12)` on line 57, causing **permanent node termination** with no automatic recovery. The node requires manual operator intervention to restart.

## Impact Explanation

**Critical Severity** - Meets "Total loss of liveness/network availability" criteria:

1. **Validator Node Crash**: The entire validator process terminates immediately upon panic
2. **No Recovery Mechanism**: The crash handler explicitly exits the process - there is no automatic restart
3. **Network Impact**: 
   - Crashed validators cannot participate in consensus
   - Multiple simultaneous crashes could reduce network below 2/3 threshold, halting the chain
   - Each crashed node requires manual diagnosis and restart by operators
4. **Deterministic Execution Broken**: Different validators may crash at different times based on race timing, causing state divergence

## Likelihood Explanation

**HIGH Likelihood** - This race condition occurs naturally during normal network operation:

1. **Frequency**: Triggered on every block execution that updates hot state (line 225 in state.rs)
2. **Race Window**: 
   - The committer uses a channel with `MAX_HOT_STATE_COMMIT_BACKLOG = 10` commits
   - Under high transaction load, commits queue up while execution continues
   - Parallel processing via rayon increases race probability
3. **No Synchronization**: The code has no locks or synchronization between metadata read and hash map access
4. **Triggering Conditions**:
   - High transaction throughput (normal for mainnet)
   - State updates that trigger eviction (`num_items > capacity`)
   - Concurrent commit processing (always active in background)

The race is **non-deterministic** but becomes **highly probable** under load, making it exploitable by simply increasing transaction volume.

## Recommendation

Implement atomic snapshot reads by holding the committed state lock while creating the `HotStateLRU`:

```rust
// In HotState::get_committed()
pub fn get_committed(&self) -> (Arc<dyn HotStateView>, State) {
    // Hold lock for entire snapshot to prevent committer from modifying during read
    let committed_guard = self.committed.lock();
    let state = committed_guard.clone();
    
    // Create a consistent snapshot of the base that matches the metadata
    let snapshot = self.base.create_snapshot(&state);
    drop(committed_guard);
    
    (Arc::new(snapshot), state)
}
```

Alternatively, use copy-on-write semantics:
1. Make `HotStateBase` use immutable data structures with copy-on-write
2. Each commit creates a new base version rather than mutating in place
3. Readers hold an Arc to their snapshot version

Or add validation before `expect()`:
```rust
// In maybe_evict()
let slot = match self.delete(&current) {
    Some(s) => s,
    None => {
        // Metadata became stale - abort eviction
        warn!("Hot state metadata stale during eviction");
        return evicted;
    }
};
```

## Proof of Concept

```rust
// Rust test demonstrating the race condition
#[test]
fn test_hot_state_race_condition() {
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    
    // Setup: Create hot state with items at capacity
    let config = HotStateConfig { max_items_per_shard: 10, ..Default::default() };
    let state = State::new_empty(config);
    let hot_state = Arc::new(HotState::new(state, config));
    
    // Populate with items
    let mut updates = HashMap::new();
    for i in 0..15 {
        let key = StateKey::raw(format!("key_{}", i).as_bytes());
        let slot = StateSlot::new_hot_occupied(/* ... */);
        updates.insert(key, slot);
    }
    
    // Execution thread - repeatedly calls get_committed and maybe_evict
    let hot_state_clone = hot_state.clone();
    let execution_thread = thread::spawn(move || {
        for _ in 0..1000 {
            let (persisted_view, persisted_state) = hot_state_clone.get_committed();
            
            // Create LRU with potentially stale metadata
            let mut lru = HotStateLRU::new(
                NonZeroUsize::new(10).unwrap(),
                persisted_view,
                &empty_overlay,
                persisted_state.latest_hot_key(0),
                persisted_state.oldest_hot_key(0),
                persisted_state.num_hot_items(0),
            );
            
            // This will panic if committer modified base between reads
            lru.maybe_evict(); // PANIC HERE
            
            thread::sleep(Duration::from_micros(1));
        }
    });
    
    // Committer thread - continuously commits new states
    let hot_state_clone = hot_state.clone();
    let commit_thread = thread::spawn(move || {
        for i in 0..1000 {
            let new_state = create_updated_state(i);
            hot_state_clone.enqueue_commit(new_state);
            thread::sleep(Duration::from_micros(1));
        }
    });
    
    // Race condition will cause panic in execution_thread
    execution_thread.join().expect("Should crash with panic");
    commit_thread.join().unwrap();
}
```

**Expected Result**: The test panics with message "There must be entries to evict when current size is above capacity" when the race condition triggers, demonstrating that the validator node would crash.

## Notes

This vulnerability is particularly severe because:
1. It affects **all validator nodes** running the code
2. No special permissions or malicious intent required - occurs naturally under load
3. The failure is **silent** until the panic occurs - no warnings or error logs beforehand
4. Recovery requires **manual operator intervention** - automated systems cannot self-heal
5. Could be **weaponized** by flooding the network with transactions to increase race probability

The root cause is a Time-Of-Check-Time-Of-Use (TOCTOU) vulnerability where metadata is read at one time but the underlying data structure is accessed later without verification that it hasn't changed.

### Citations

**File:** storage/aptosdb/src/state_store/hot_state.rs (L131-136)
```rust
    pub fn get_committed(&self) -> (Arc<dyn HotStateView>, State) {
        let state = self.committed.lock().clone();
        let base = self.base.clone();

        (base, state)
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L235-275)
```rust
    fn commit(&mut self, to_commit: &State) {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["hot_state_commit"]);

        let mut n_insert = 0;
        let mut n_update = 0;
        let mut n_evict = 0;

        let delta = to_commit.make_delta(&self.committed.lock());
        for shard_id in 0..NUM_STATE_SHARDS {
            for (key, slot) in delta.shards[shard_id].iter() {
                if slot.is_hot() {
                    let key_size = key.size();
                    self.total_key_bytes += key_size;
                    self.total_value_bytes += slot.size();
                    if let Some(old_slot) = self.base.shards[shard_id].insert(key, slot) {
                        self.total_key_bytes -= key_size;
                        self.total_value_bytes -= old_slot.size();
                        n_update += 1;
                    } else {
                        n_insert += 1;
                    }
                } else if let Some((key, old_slot)) = self.base.shards[shard_id].remove(&key) {
                    self.total_key_bytes -= key.size();
                    self.total_value_bytes -= old_slot.size();
                    n_evict += 1;
                }
            }
            self.heads[shard_id] = to_commit.latest_hot_key(shard_id);
            self.tails[shard_id] = to_commit.oldest_hot_key(shard_id);
            assert_eq!(
                self.base.shards[shard_id].len(),
                to_commit.num_hot_items(shard_id)
            );

            debug_assert!(self.validate_lru(shard_id).is_ok());
        }

        COUNTER.inc_with_by(&["hot_state_insert"], n_insert);
        COUNTER.inc_with_by(&["hot_state_update"], n_update);
        COUNTER.inc_with_by(&["hot_state_evict"], n_evict);
    }
```

**File:** storage/storage-interface/src/state_store/state.rs (L197-204)
```rust
                    let mut lru = HotStateLRU::new(
                        NonZeroUsize::new(self.hot_state_config.max_items_per_shard).unwrap(),
                        Arc::clone(&persisted_hot_state),
                        overlay,
                        hot_metadata.latest.clone(),
                        hot_metadata.oldest.clone(),
                        hot_metadata.num_items,
                    );
```

**File:** storage/storage-interface/src/state_store/hot_state.rs (L82-106)
```rust
    pub fn maybe_evict(&mut self) -> Vec<(StateKey, StateSlot)> {
        let mut current = match &self.tail {
            Some(tail) => tail.clone(),
            None => {
                assert_eq!(self.num_items, 0);
                return Vec::new();
            },
        };

        let mut evicted = Vec::new();
        while self.num_items > self.capacity.get() {
            let slot = self
                .delete(&current)
                .expect("There must be entries to evict when current size is above capacity.");
            let prev_key = slot
                .prev()
                .cloned()
                .expect("There must be at least one newer entry (num_items > capacity >= 1).");
            evicted.push((current.clone(), slot.clone()));
            self.pending.insert(current, slot.to_cold());
            current = prev_key;
            self.num_items -= 1;
        }
        evicted
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L420-425)
```rust
        let (result_state, hot_state_updates) = parent_state.update_with_memorized_reads(
            base_state_view.persisted_hot_state(),
            base_state_view.persisted_state(),
            to_commit.state_update_refs(),
            base_state_view.memorized_reads(),
        );
```

**File:** crates/crash-handler/src/lib.rs (L26-58)
```rust
pub fn setup_panic_handler() {
    panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}

// Formats and logs panic information
fn handle_panic(panic_info: &PanicHookInfo<'_>) {
    // The Display formatter for a PanicHookInfo contains the message, payload and location.
    let details = format!("{}", panic_info);
    let backtrace = format!("{:#?}", Backtrace::new());

    let info = CrashInfo { details, backtrace };
    let crash_info = toml::to_string_pretty(&info).unwrap();
    error!("{}", crash_info);
    // TODO / HACK ALARM: Write crash info synchronously via eprintln! to ensure it is written before the process exits which error! doesn't guarantee.
    // This is a workaround until https://github.com/aptos-labs/aptos-core/issues/2038 is resolved.
    eprintln!("{}", crash_info);

    // Wait till the logs have been flushed
    aptos_logger::flush();

    // Do not kill the process if the panics happened at move-bytecode-verifier.
    // This is safe because the `state::get_state()` uses a thread_local for storing states. Thus the state can only be mutated to VERIFIER by the thread that's running the bytecode verifier.
    //
    // TODO: once `can_unwind` is stable, we should assert it. See https://github.com/rust-lang/rust/issues/92988.
    if state::get_state() == VMState::VERIFIER || state::get_state() == VMState::DESERIALIZER {
        return;
    }

    // Kill the process
    process::exit(12);
}
```
