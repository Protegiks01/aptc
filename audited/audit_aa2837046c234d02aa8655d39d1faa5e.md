# Audit Report

## Title
Missing Shard ID Validation Allows Cross-Shard Transaction Execution Leading to Consensus Violation

## Summary
Executor shards do not validate that received `SubBlocksForShard` are intended for them, allowing transactions meant for ShardId N to be executed on ShardId M if misrouted. This violates the deterministic execution invariant and can cause consensus safety violations.

## Finding Description

The sharded block executor architecture partitions transactions across multiple shards for parallel execution. Each `SubBlocksForShard` structure contains a `shard_id` field identifying its intended executor. [1](#0-0) 

When the coordinator sends execution commands, it uses vector indexing to route sub_blocks to shards: [2](#0-1) 

The code assumes `sharded_txns[i].shard_id == i`, relying on correct partitioner output. However, when an executor shard receives an `ExecuteBlockCommand`, it **never validates** that the embedded `sub_blocks.shard_id` matches its own `shard_id`: [3](#0-2) 

The shard immediately extracts and executes the sub_blocks without checking the shard_id field. Similarly, the execution service stores its own shard_id but never uses it for validation: [4](#0-3) 

The actual execution in `ShardedExecutorService` also lacks validation: [5](#0-4) 

**Attack Scenarios:**

1. **Partitioner Bug**: If the partitioner has a bug that produces `sharded_txns` where indices don't match shard_ids, wrong transactions execute on wrong shards
2. **Memory Corruption**: Vector corruption could cause misalignment between indices and shard_ids
3. **Malicious Coordinator**: A compromised coordinator could deliberately send Shard 0's transactions to Shard 1
4. **Network Misconfiguration**: Channel mix-ups could route messages to wrong shards

**Invariant Violations:**

This breaks the **Deterministic Execution** invariant: if different validators receive different shard assignments (due to any bug or attack), they will execute different transactions and compute different state roots, causing a consensus safety violation and potential network split.

## Impact Explanation

**Critical Severity** - This qualifies for Critical severity under Aptos Bug Bounty criteria:

- **Consensus/Safety violations**: Different validators executing different transactions on the same shard would compute different state roots, violating BFT safety
- **Non-recoverable network partition**: If validators disagree on state roots due to misrouted transactions, this requires a hard fork to resolve
- **Deterministic execution failure**: The core invariant that all validators must produce identical outputs for identical inputs is violated

The impact is **consensus-breaking** because:
1. Validator A might execute Shard 0's transactions on Shard 0
2. Validator B (due to bug/attack) executes Shard 1's transactions on Shard 0  
3. Both produce different state roots for the same block
4. Network cannot reach consensus, causing liveness failure or chain split

## Likelihood Explanation

**Likelihood: Medium**

While the current partitioner implementation appears correct at creation time, this is a **latent vulnerability** - a missing defensive check that would allow exploitation if any of these conditions occur:

1. **Future partitioner bugs** (Medium likelihood): As the codebase evolves, bugs could be introduced in partitioning logic
2. **Memory safety issues** (Low likelihood): Rust provides memory safety, but unsafe code or logic bugs could corrupt vectors
3. **Malicious coordinator** (Low likelihood): Requires compromising validator infrastructure, but impact is critical if it occurs
4. **Integration bugs** (Medium likelihood): As new features are added, assumptions about vector ordering could be violated

The principle of **defense-in-depth** requires validating inputs even when believed correct. The absence of this validation means any upstream bug becomes a critical vulnerability rather than being caught early.

## Recommendation

Add shard ID validation in `RemoteCoordinatorClient::receive_execute_command()`:

```rust
impl CoordinatorClient<RemoteStateViewClient> for RemoteCoordinatorClient {
    fn receive_execute_command(&self) -> ExecutorShardCommand<RemoteStateViewClient> {
        match self.command_rx.recv() {
            Ok(message) => {
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
                match request {
                    RemoteExecutionRequest::ExecuteBlock(command) => {
                        // CRITICAL: Validate shard_id matches
                        assert_eq!(
                            command.sub_blocks.shard_id, 
                            self.shard_id,
                            "Shard ID mismatch: expected {}, received {}. \
                             This indicates a critical bug in transaction partitioning.",
                            self.shard_id,
                            command.sub_blocks.shard_id
                        );
                        
                        let state_keys = Self::extract_state_keys(&command);
                        self.state_view_client.init_for_block(state_keys);
                        let (sub_blocks, concurrency, onchain_config) = command.into();
                        ExecutorShardCommand::ExecuteSubBlocks(
                            self.state_view_client.clone(),
                            sub_blocks,
                            concurrency,
                            onchain_config,
                        )
                    },
                }
            },
            Err(_) => ExecutorShardCommand::Stop,
        }
    }
    // ... rest of implementation
}
```

Additionally, consider validating in the remote executor client that the vector index matches the shard_id:

```rust
for (index, sub_blocks) in sub_blocks.into_iter().enumerate() {
    assert_eq!(
        sub_blocks.shard_id, 
        index,
        "Shard ID mismatch in PartitionedTransactions: index {} has shard_id {}",
        index,
        sub_blocks.shard_id
    );
    // ... send to shard
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod shard_id_confusion_test {
    use super::*;
    use aptos_types::block_executor::partitioner::{ShardId, SubBlock, SubBlocksForShard};
    
    #[test]
    #[should_panic(expected = "Shard ID mismatch")]
    fn test_mismatched_shard_id_rejected() {
        // Create sub_blocks intended for Shard 0
        let sub_blocks_for_shard_0 = SubBlocksForShard::new(
            0, // shard_id = 0
            vec![SubBlock::empty()],
        );
        
        // Create ExecuteBlockCommand with Shard 0's transactions
        let command = ExecuteBlockCommand {
            sub_blocks: sub_blocks_for_shard_0,
            concurrency_level: 4,
            onchain_config: BlockExecutorConfigFromOnchain::default(),
        };
        
        // Simulate sending to Shard 1 (wrong shard)
        let coordinator_client = RemoteCoordinatorClient::new(
            1, // self.shard_id = 1 (but receiving shard_id=0 blocks)
            &mut controller,
            coordinator_address,
        );
        
        // This should panic with the validation check in place
        // Currently, it would execute without error - this is the vulnerability
        coordinator_client.receive_execute_command();
    }
    
    #[test]
    fn test_correct_shard_id_accepted() {
        // Create sub_blocks intended for Shard 1
        let sub_blocks_for_shard_1 = SubBlocksForShard::new(
            1, // shard_id = 1
            vec![SubBlock::empty()],
        );
        
        let command = ExecuteBlockCommand {
            sub_blocks: sub_blocks_for_shard_1,
            concurrency_level: 4,
            onchain_config: BlockExecutorConfigFromOnchain::default(),
        };
        
        // Sending to correct Shard 1
        let coordinator_client = RemoteCoordinatorClient::new(
            1, // self.shard_id = 1 (matches sub_blocks.shard_id)
            &mut controller,
            coordinator_address,
        );
        
        // This should succeed
        coordinator_client.receive_execute_command();
    }
}
```

## Notes

This vulnerability represents a **defense-in-depth failure** in the sharded execution architecture. While the current implementation may correctly maintain the `sharded_txns[i].shard_id == i` invariant at creation time, the complete absence of validation at the receiver means any future bug, memory corruption, or malicious behavior would go undetected and cause critical consensus violations. The fix is straightforward and should be implemented as a mandatory security hardening measure.

### Citations

**File:** types/src/block_executor/partitioner.rs (L302-307)
```rust
// A set of sub blocks assigned to a shard.
#[derive(Default, Clone, Debug, Serialize, Deserialize, Eq, PartialEq)]
pub struct SubBlocksForShard<T> {
    pub shard_id: ShardId,
    pub sub_blocks: Vec<SubBlock<T>>,
}
```

**File:** execution/executor-service/src/remote_executor_client.rs (L193-206)
```rust
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L80-113)
```rust
    fn receive_execute_command(&self) -> ExecutorShardCommand<RemoteStateViewClient> {
        match self.command_rx.recv() {
            Ok(message) => {
                let _rx_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx"])
                    .start_timer();
                let bcs_deser_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx_bcs_deser"])
                    .start_timer();
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
                drop(bcs_deser_timer);

                match request {
                    RemoteExecutionRequest::ExecuteBlock(command) => {
                        let init_prefetch_timer = REMOTE_EXECUTOR_TIMER
                            .with_label_values(&[&self.shard_id.to_string(), "init_prefetch"])
                            .start_timer();
                        let state_keys = Self::extract_state_keys(&command);
                        self.state_view_client.init_for_block(state_keys);
                        drop(init_prefetch_timer);

                        let (sub_blocks, concurrency, onchain_config) = command.into();
                        ExecutorShardCommand::ExecuteSubBlocks(
                            self.state_view_client.clone(),
                            sub_blocks,
                            concurrency,
                            onchain_config,
                        )
                    },
                }
            },
            Err(_) => ExecutorShardCommand::Stop,
        }
    }
```

**File:** execution/executor-service/src/remote_executor_service.rs (L15-24)
```rust
pub struct ExecutorService {
    shard_id: ShardId,
    controller: NetworkController,
    executor_service: Arc<ShardedExecutorService<RemoteStateViewClient>>,
}

impl ExecutorService {
    pub fn new(
        shard_id: ShardId,
        num_shards: usize,
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L185-213)
```rust
    fn execute_block(
        &self,
        transactions: SubBlocksForShard<AnalyzedTransaction>,
        state_view: &S,
        config: BlockExecutorConfig,
    ) -> Result<Vec<Vec<TransactionOutput>>, VMStatus> {
        let mut result = vec![];
        for (round, sub_block) in transactions.into_sub_blocks().into_iter().enumerate() {
            let _timer = SHARDED_BLOCK_EXECUTION_BY_ROUNDS_SECONDS
                .timer_with(&[&self.shard_id.to_string(), &round.to_string()]);
            SHARDED_BLOCK_EXECUTOR_TXN_COUNT.observe_with(
                &[&self.shard_id.to_string(), &round.to_string()],
                sub_block.transactions.len() as f64,
            );
            info!(
                "executing sub block for shard {} and round {}, number of txns {}",
                self.shard_id,
                round,
                sub_block.transactions.len()
            );
            result.push(self.execute_sub_block(sub_block, round, state_view, config.clone())?);
            trace!(
                "Finished executing sub block for shard {} and round {}",
                self.shard_id,
                round
            );
        }
        Ok(result)
    }
```
