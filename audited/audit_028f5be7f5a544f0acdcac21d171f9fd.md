# Audit Report

## Title
Memory Leak and State Corruption via Stale Block Insertion During Epoch Transitions in PendingBlocks

## Summary
The `pending_blocks` buffer shared across epoch boundaries lacks epoch-specific isolation, allowing verification tasks from a previous epoch to insert blocks into the new epoch's `PendingBlocks` after epoch transition completes. These cross-epoch blocks may have incompatible round numbers that evade garbage collection, causing gradual memory accumulation and potential state inconsistencies across multiple epoch transitions.

## Finding Description

The vulnerability exists in the epoch transition logic where `initiate_new_epoch()` clears `pending_blocks` without ensuring all message verification tasks from the previous epoch have completed. [1](#0-0) 

The race condition occurs as follows:

1. **Message Arrival (Epoch N)**: A consensus message arrives and passes epoch validation [2](#0-1) 

2. **Verification Task Spawning**: The message enters the `BoundedExecutor` for signature verification, capturing `pending_blocks.clone()` [3](#0-2) 

3. **Epoch Transition**: While verification is ongoing, `initiate_new_epoch()` is called, which:
   - Calls `shutdown_current_processor()` (waits for RoundManager/DAG but NOT BoundedExecutor tasks)
   - Replaces pending_blocks contents: `*self.pending_blocks.lock() = PendingBlocks::new()` [4](#0-3) 

4. **Cross-Epoch Insertion**: The old verification task completes and calls `forward_event()`, which inserts the epoch N block into the epoch N+1's `PendingBlocks` [5](#0-4) 

5. **Failed Garbage Collection**: The `PendingBlocks::gc()` method uses round-based cleanup [6](#0-5) 

Since `PendingBlocks` has no epoch tracking and only removes blocks with `round <= committed_round`, blocks from epoch N with high round numbers (e.g., round 1500) inserted into epoch N+1 (starting from round 1) will never be garbage collected until the new epoch reaches that round number. Across multiple rapid epoch transitions, these orphaned blocks accumulate. [7](#0-6) 

The shutdown mechanism fails to synchronize properly because `shutdown_current_processor()` drops channel senders but doesn't await BoundedExecutor task completion: [8](#0-7) 

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty criteria:

- **State Inconsistencies**: Cross-epoch blocks contaminate the new epoch's pending block buffer, causing blocks with incorrect epoch metadata to persist in memory
- **Resource Exhaustion**: Each epoch transition can leave multiple orphaned blocks (each potentially 100KB-1MB with payload) that accumulate over time
- **Validator Performance Degradation**: Gradual memory growth across frequent epoch transitions could lead to validator slowdowns or out-of-memory crashes

While not immediately catastrophic, this meets the "State inconsistencies requiring intervention" criterion for Medium severity. Over 100+ epoch transitions, memory leakage could reach hundreds of megabytes, impacting validator stability.

## Likelihood Explanation

**Likelihood: Medium-High**

- Epoch transitions occur regularly in Aptos (e.g., validator set updates, governance-triggered reconfigurations)
- The race window exists whenever messages arrive within ~100-500ms of an epoch transition (signature verification time)
- No special attacker privileges requiredâ€”normal network traffic triggers this naturally
- The vulnerability is deterministic given timing conditions; it will happen eventually during normal operation

The issue becomes more severe during periods of high network activity or rapid governance changes that trigger frequent epoch transitions.

## Recommendation

**Solution**: Add epoch tracking to `PendingBlocks` and implement proper synchronization for verification tasks during epoch transitions.

**Recommended Fix**:

1. **Add epoch field to PendingBlocks**:
```rust
pub struct PendingBlocks {
    epoch: u64,  // Add this field
    blocks_by_hash: HashMap<HashValue, Block>,
    blocks_by_round: BTreeMap<Round, Block>,
    opt_blocks_by_round: BTreeMap<Round, OptBlockData>,
    pending_request: Option<(TargetBlockRetrieval, oneshot::Sender<Block>)>,
}
```

2. **Validate epoch on insertion**:
```rust
pub fn insert_block(&mut self, block: Block) {
    // Reject blocks from wrong epoch
    if block.epoch() != self.epoch {
        warn!("Rejecting cross-epoch block: block_epoch={}, current_epoch={}", 
              block.epoch(), self.epoch);
        return;
    }
    // ... existing insertion logic
}
```

3. **Update initiate_new_epoch** to pass epoch:
```rust
async fn initiate_new_epoch(&mut self, proof: EpochChangeProof) -> anyhow::Result<()> {
    // ... existing code ...
    self.shutdown_current_processor().await;
    let new_epoch = ledger_info.ledger_info().next_block_epoch();
    *self.pending_blocks.lock() = PendingBlocks::new_with_epoch(new_epoch);
    // ... rest of function
}
```

4. **Alternatively, wait for BoundedExecutor tasks**: Implement a shutdown barrier that waits for all verification tasks to complete before epoch transition.

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[tokio::test]
async fn test_cross_epoch_block_insertion() {
    // Setup: Create EpochManager in epoch N
    let mut epoch_manager = setup_epoch_manager(epoch_n);
    
    // Step 1: Send consensus message during epoch N
    let proposal_msg = create_proposal_msg(epoch_n, round_1500);
    epoch_manager.process_message(peer_id, proposal_msg).await.unwrap();
    
    // Step 2: Immediately trigger epoch transition (while verification is in-flight)
    let epoch_change_proof = create_epoch_change_proof(epoch_n_plus_1);
    epoch_manager.check_epoch(peer_id, 
        ConsensusMsg::EpochChangeProof(Box::new(epoch_change_proof))).await.unwrap();
    
    // Step 3: Wait for verification task to complete
    tokio::time::sleep(Duration::from_millis(200)).await;
    
    // Step 4: Verify cross-epoch contamination
    let pending_blocks = epoch_manager.pending_blocks.lock();
    
    // Assertion: Block from epoch N (round 1500) exists in epoch N+1's PendingBlocks
    assert!(pending_blocks.blocks_by_round.contains_key(&1500));
    
    // Step 5: Trigger GC with new epoch's committed round (e.g., 50)
    pending_blocks.gc(50);
    
    // Assertion: Block at round 1500 was NOT garbage collected
    assert!(pending_blocks.blocks_by_round.contains_key(&1500), 
            "Memory leak: Old epoch block persists after GC");
}
```

**Expected Result**: The test demonstrates that blocks from epoch N with round 1500 persist in epoch N+1's PendingBlocks even after garbage collection with round 50, confirming the memory leak.

## Notes

This vulnerability is subtle because:
- The Arc reference counting mechanism works correctly (no dangling pointers)
- The Mutex locking is sound (no data races)
- The issue is **logical**: lack of epoch-based isolation in the shared buffer
- The comment at line 553 acknowledges race conditions but only addresses state sync, not verification tasks

The fix requires either epoch tracking in PendingBlocks or proper synchronization of all async tasks during epoch transitions.

### Citations

**File:** consensus/src/epoch_manager.rs (L544-569)
```rust
    async fn initiate_new_epoch(&mut self, proof: EpochChangeProof) -> anyhow::Result<()> {
        let ledger_info = proof
            .verify(self.epoch_state())
            .context("[EpochManager] Invalid EpochChangeProof")?;
        info!(
            LogSchema::new(LogEvent::NewEpoch).epoch(ledger_info.ledger_info().next_block_epoch()),
            "Received verified epoch change",
        );

        // shutdown existing processor first to avoid race condition with state sync.
        self.shutdown_current_processor().await;
        *self.pending_blocks.lock() = PendingBlocks::new();
        // make sure storage is on this ledger_info too, it should be no-op if it's already committed
        // panic if this doesn't succeed since the current processors are already shutdown.
        self.execution_client
            .sync_to_target(ledger_info.clone())
            .await
            .context(format!(
                "[EpochManager] State sync to new epoch {}",
                ledger_info
            ))
            .expect("Failed to sync to new epoch");

        monitor!("reconfig", self.await_reconfig_notification().await);
        Ok(())
    }
```

**File:** consensus/src/epoch_manager.rs (L637-683)
```rust
    async fn shutdown_current_processor(&mut self) {
        if let Some(close_tx) = self.round_manager_close_tx.take() {
            // Release the previous RoundManager, especially the SafetyRule client
            let (ack_tx, ack_rx) = oneshot::channel();
            close_tx
                .send(ack_tx)
                .expect("[EpochManager] Fail to drop round manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop round manager");
        }
        self.round_manager_tx = None;

        if let Some(close_tx) = self.dag_shutdown_tx.take() {
            // Release the previous RoundManager, especially the SafetyRule client
            let (ack_tx, ack_rx) = oneshot::channel();
            close_tx
                .send(ack_tx)
                .expect("[EpochManager] Fail to drop DAG bootstrapper");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop DAG bootstrapper");
        }
        self.dag_shutdown_tx = None;

        // Shutdown the previous rand manager
        self.rand_manager_msg_tx = None;

        // Shutdown the previous secret share manager
        self.secret_share_manager_tx = None;

        // Shutdown the previous buffer manager, to release the SafetyRule client
        self.execution_client.end_epoch().await;

        // Shutdown the block retrieval task by dropping the sender
        self.block_retrieval_tx = None;
        self.batch_retrieval_tx = None;

        if let Some(mut quorum_store_coordinator_tx) = self.quorum_store_coordinator_tx.take() {
            let (ack_tx, ack_rx) = oneshot::channel();
            quorum_store_coordinator_tx
                .send(CoordinatorCommand::Shutdown(ack_tx))
                .await
                .expect("Could not send shutdown indicator to QuorumStore");
            ack_rx.await.expect("Failed to stop QuorumStore");
        }
    }
```

**File:** consensus/src/epoch_manager.rs (L1586-1622)
```rust
            let pending_blocks = self.pending_blocks.clone();
            self.bounded_executor
                .spawn(async move {
                    match monitor!(
                        "verify_message",
                        unverified_event.clone().verify(
                            peer_id,
                            &epoch_state.verifier,
                            &proof_cache,
                            quorum_store_enabled,
                            peer_id == my_peer_id,
                            max_num_batches,
                            max_batch_expiry_gap_usecs,
                        )
                    ) {
                        Ok(verified_event) => {
                            Self::forward_event(
                                quorum_store_msg_tx,
                                round_manager_tx,
                                buffered_proposal_tx,
                                peer_id,
                                verified_event,
                                payload_manager,
                                pending_blocks,
                            );
                        },
                        Err(e) => {
                            error!(
                                SecurityEvent::ConsensusInvalidMessage,
                                remote_peer = peer_id,
                                error = ?e,
                                unverified_event = unverified_event
                            );
                        },
                    }
                })
                .await;
```

**File:** consensus/src/epoch_manager.rs (L1646-1647)
```rust
                if event.epoch()? == self.epoch() {
                    return Ok(Some(event));
```

**File:** consensus/src/epoch_manager.rs (L1773-1773)
```rust
                    pending_blocks.lock().insert_block(p.proposal().clone());
```

**File:** consensus/src/block_storage/pending_blocks.rs (L122-133)
```rust
    pub fn gc(&mut self, round: Round) {
        let mut to_remove = vec![];
        for (r, _) in self.blocks_by_round.range(..=round) {
            to_remove.push(*r);
        }
        for r in to_remove {
            self.opt_blocks_by_round.remove(&r);
            if let Some(block) = self.blocks_by_round.remove(&r) {
                self.blocks_by_hash.remove(&block.id());
            }
        }
    }
```

**File:** consensus/src/block_storage/block_store.rs (L334-336)
```rust
        self.pending_blocks
            .lock()
            .gc(finality_proof.commit_info().round());
```
