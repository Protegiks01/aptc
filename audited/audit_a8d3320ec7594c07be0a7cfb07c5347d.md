# Audit Report

## Title
Unbounded Memory Allocation Growth in Module ID Interner Can Cause Validator Node OOM Crashes

## Summary
The `InternedModuleIdPool` uses an interner with a doubling allocation strategy where `next_size` grows exponentially but is never reset after flushes. After multiple flush cycles triggered by interning many unique module IDs, `next_size` can grow to billions, causing subsequent `Vec::with_capacity()` allocations to request terabytes of memory, leading to validator node crashes.

## Finding Description
The module ID interner in the Move VM uses a buffer allocation strategy where each new buffer doubles in size. [1](#0-0) 

When the interner reaches 100,000 entries, it triggers a flush. [2](#0-1)  and [3](#0-2) 

The critical flaw is in the flush implementation: while it clears all data structures, it deliberately does NOT reset `next_size`. [4](#0-3) 

The developers' comment claims "Asymptotically, we are still using O(n) memory" - but this reasoning is flawed. While *used* memory is O(n), the *allocated* memory grows exponentially with the number of flushes because `next_size` keeps its value and continues doubling.

**Attack Path**:
1. Attacker deploys Move modules containing many struct handles referencing unique ModuleIds
2. Each struct handle causes a ModuleId to be interned during module loading [5](#0-4) 
3. After interning 100,000 unique ModuleIds, a flush is triggered, but `next_size` remains at its current value (e.g., 131,072)
4. Attacker repeats the process multiple times across many blocks
5. After k flush cycles: next_size = 2048 × 2^k
6. After 20 flushes (~2M module IDs processed): next_size ≈ 2.1 billion
7. Next buffer allocation attempts: `Vec::with_capacity(2.1 billion)` × 82 bytes/ModuleId ≈ 172 GB
8. This causes an allocation failure or OOM crash on the validator node

The RuntimeEnvironment (containing the interner) persists across blocks when environment configuration remains unchanged. [6](#0-5) 

Module IDs are interned both when loading modules and when resolving struct handles, providing multiple vectors for an attacker to force unique ModuleId interning.

## Impact Explanation
This vulnerability can cause **validator node crashes** through Out-Of-Memory (OOM) conditions, qualifying as **High Severity** per Aptos bug bounty criteria: "Validator node slowdowns" and "API crashes" both fall under High Severity (up to $50,000).

When `next_size` reaches dangerous levels (billions), the next `Vec::with_capacity()` call will attempt to allocate hundreds of gigabytes of contiguous memory, which will:
1. Fail on systems with insufficient memory
2. Cause OOM kills by the operating system
3. Crash the validator node, disrupting consensus participation
4. Potentially affect multiple validators if they all process the same malicious transactions

This breaks the **Move VM Safety** invariant: "Bytecode execution must respect gas limits and memory constraints" - the memory allocation is not properly bounded despite gas being charged for module deployment.

## Likelihood Explanation
**Likelihood: MEDIUM-LOW**

The attack requires significant resources but is technically feasible:

**Facilitating Factors**:
- No validation prevents modules from referencing many unique ModuleIds
- Struct handles can reference ModuleIds of non-existent modules
- Transaction size limit (64 KB) still allows hundreds of struct handles per module [7](#0-6) 
- RuntimeEnvironment persistence across blocks enables accumulation

**Mitigating Factors**:
- High gas costs for deploying thousands of modules
- Requires sustained attack over many blocks (20+ flushes = 2M module IDs)
- Environment recreation on config changes resets `next_size`
- Block gas limits constrain attack throughput

The attack is economically expensive but not impossible, especially for well-funded adversaries targeting network disruption.

## Recommendation
Reset `next_size` during flush to prevent unbounded growth, or implement a maximum cap:

```rust
fn flush(&mut self) {
    self.map.clear();
    self.vec.clear();
    self.buffer.clear();
    self.pool.clear();
    // FIX: Reset next_size to prevent unbounded growth
    self.next_size = INITIAL_SIZE * 2;
}
```

Alternatively, cap `next_size` at a reasonable maximum:

```rust
unsafe fn alloc(&mut self, val: T) -> &'static T {
    if self.buffer.len() >= self.buffer.capacity() {
        // Cap next_size at a reasonable maximum (e.g., 1M entries)
        const MAX_BUFFER_SIZE: usize = 1_048_576;
        let new_size = std::cmp::min(self.next_size, MAX_BUFFER_SIZE);
        let new_buffer = Vec::with_capacity(new_size);
        self.next_size = std::cmp::min(self.next_size * 2, MAX_BUFFER_SIZE);

        let old_buffer = std::mem::replace(&mut self.buffer, new_buffer);
        self.pool.push(old_buffer);
    }

    self.buffer.push(val);
    unsafe { &*(self.buffer.last().expect("last always exists") as *const T) }
}
```

## Proof of Concept
A complete PoC would require simulating multiple flush cycles, which is resource-intensive. Here's a conceptual Rust test demonstrating the unbounded growth:

```rust
#[test]
fn test_interner_unbounded_next_size_growth() {
    use move_vm_types::module_id_interner::InternedModuleIdPool;
    use move_core_types::language_storage::ModuleId;
    use move_core_types::account_address::AccountAddress;
    use move_core_types::identifier::Identifier;

    let pool = InternedModuleIdPool::new();
    
    // Simulate multiple flush cycles
    for cycle in 0..5 {
        // Intern 100,000 unique module IDs to trigger flush
        for i in 0..100_000 {
            let addr = AccountAddress::from_hex_literal(&format!("0x{:x}", i)).unwrap();
            let name = Identifier::new(format!("Module_{}", i)).unwrap();
            let module_id = ModuleId::new(addr, name);
            pool.intern(module_id);
        }
        
        println!("Cycle {}: {} module IDs interned", cycle, pool.len());
        
        // Flush would be triggered here in production
        pool.flush();
        
        // After flush, next_size has doubled but is not reset
        // On cycle 5, next_size would be dangerously large
    }
    
    // After 5 cycles, next_size ≈ 2048 * 2^5 = 65,536
    // After 20 cycles, next_size ≈ 2.1 billion (dangerous)
}
```

To demonstrate the actual OOM risk, one would need to force 20+ flush cycles by deploying modules with many struct handles across thousands of transactions, which would be expensive to execute in practice but proves the theoretical vulnerability.

## Notes
- The vulnerability exists in the core Move VM types used across the Aptos blockchain
- The flaw stems from a misunderstanding in the developer comment about asymptotic memory usage
- While economically expensive to exploit, the attack could be executed by well-funded adversaries
- The persistence of RuntimeEnvironment across blocks amplifies the issue
- A simple fix (resetting `next_size` on flush) would eliminate the vulnerability entirely

### Citations

**File:** third_party/move/move-vm/types/src/interner.rs (L59-71)
```rust
    /// Flushes the pool, clearing all interned values.
    ///
    /// Note that this specifically does not reset the current buffer size, nor the next size,
    /// as resetting gives no real benefit. Asymptotically, we are still using O(n) memory.
    ///
    /// Another way to think about this is that after a flush, we are starting with a larger
    /// initial size.
    fn flush(&mut self) {
        self.map.clear();
        self.vec.clear();
        self.buffer.clear();
        self.pool.clear();
    }
```

**File:** third_party/move/move-vm/types/src/interner.rs (L82-93)
```rust
    unsafe fn alloc(&mut self, val: T) -> &'static T {
        if self.buffer.len() >= self.buffer.capacity() {
            let new_buffer = Vec::with_capacity(self.next_size);
            self.next_size *= 2;

            let old_buffer = std::mem::replace(&mut self.buffer, new_buffer);
            self.pool.push(old_buffer);
        }

        self.buffer.push(val);
        unsafe { &*(self.buffer.last().expect("last always exists") as *const T) }
    }
```

**File:** aptos-move/block-executor/src/code_cache_global_manager.rs (L100-130)
```rust
        &mut self,
        storage_environment: AptosEnvironment,
        config: &BlockExecutorModuleCacheLocalConfig,
        transaction_slice_metadata: TransactionSliceMetadata,
    ) -> Result<(), VMStatus> {
        // If we execute non-consecutive sequence of transactions, we need to flush everything.
        if !transaction_slice_metadata.is_immediately_after(&self.transaction_slice_metadata) {
            self.module_cache.flush();
            self.environment = None;
        }
        // Record the new metadata for this slice of transactions.
        self.transaction_slice_metadata = transaction_slice_metadata;

        // Next, check the environment. If the current environment has not been set, or is
        // different, we reset it to the new one, and flush the module cache.
        let environment_requires_update = self.environment.as_ref() != Some(&storage_environment);
        if environment_requires_update {
            if storage_environment.gas_feature_version() >= RELEASE_V1_34 {
                let flush_verifier_cache = self.environment.as_ref().is_none_or(|e| {
                    e.verifier_config_bytes() != storage_environment.verifier_config_bytes()
                });
                if flush_verifier_cache {
                    // Additionally, if the verifier config changes, we flush static verifier cache
                    // as well.
                    RuntimeEnvironment::flush_verified_module_cache();
                }
            }

            self.environment = Some(storage_environment);
            self.module_cache.flush();
        }
```

**File:** aptos-move/block-executor/src/code_cache_global_manager.rs (L162-166)
```rust
        if num_interned_module_ids > config.max_interned_module_ids {
            runtime_environment.module_id_pool().flush();
            runtime_environment.struct_name_index_map().flush();
            self.module_cache.flush();
        }
```

**File:** types/src/block_executor/config.rs (L46-46)
```rust
            max_interned_module_ids: 100_000,
```

**File:** third_party/move/move-vm/runtime/src/loader/modules.rs (L194-203)
```rust
        // validate the correctness of struct handle references.
        for struct_handle in module.struct_handles() {
            let struct_name = module.identifier_at(struct_handle.name);
            let module_handle = module.module_handle_at(struct_handle.module);
            let module_id = module.module_id_for_handle(module_handle);
            let struct_name =
                StructIdentifier::new(module_id_pool, module_id, struct_name.to_owned());
            struct_idxs.push(struct_name_index_map.struct_name_to_idx(&struct_name)?);
            struct_names.push(struct_name)
        }
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L73-76)
```rust
            max_transaction_size_in_bytes: NumBytes,
            "max_transaction_size_in_bytes",
            64 * 1024
        ],
```
