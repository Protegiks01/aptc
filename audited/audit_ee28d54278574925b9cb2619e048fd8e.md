# Audit Report

## Title
Insufficient Fallback Peers for OptQuorumStore Batch Retrieval Causes Availability Delays

## Summary
When `process_optqs_payload()` is called with `additional_peers_to_request = None` for opt_batches, the function only uses 2 peers (block author and batch author) to fetch batches. If both peers are unresponsive, batch retrieval fails repeatedly, causing block execution delays and temporary consensus liveness degradation.

## Finding Description

The vulnerability exists in the peer selection logic for fetching opt_batches in OptQuorumStore payloads. [1](#0-0) 

When `additional_peers_to_request` is None, the function constructs a responders list containing only:
1. The block author (if it exists)
2. The batch author (returned by `summary.signers()`) [2](#0-1) 

For `BatchInfo` and `BatchInfoExt`, the `signers()` implementation returns only a single-element vector containing the batch author, unlike `ProofOfStore` which returns all multi-signature signers. [3](#0-2) 

This creates a critical disparity: proof_with_data batches get a quorum of peers, but opt_batches get only 2 peers.

The scenario occurs during block materialization when the QuorumCert hasn't arrived yet: [4](#0-3) 

The `tokio::select!` races between waiting for the QC (which provides `block_voters`) and immediately calling `get_transactions` with None. If the second branch completes first, opt_batches are fetched with only 2 responders.

The batch requester then cycles through these limited peers repeatedly: [5](#0-4) 

With default configuration requesting 5 peers but only having 2 available, the requester cycles through the same 2 peers for all retry attempts (default 10 retries). [6](#0-5) 

When batch fetching fails, block materialization retries indefinitely: [7](#0-6) 

**Attack Vector**: A malicious or faulty block proposer includes opt_batches from validators they know are offline or slow to respond. When combined with delayed QC arrival, this forces validators to repeatedly fail and retry batch fetching, degrading consensus liveness.

## Impact Explanation

This issue meets **Medium Severity** criteria under "state inconsistencies requiring intervention" as it causes:

1. **Temporary Liveness Degradation**: Block execution is delayed by repeated fetch failures (10 retries Ã— 500ms intervals = 5+ seconds minimum)
2. **Resource Consumption**: Unnecessary network requests and retry loops consume validator resources
3. **Cascading Delays**: If multiple blocks contain unavailable opt_batches, delays compound across rounds

While not causing permanent halt or safety violations, the issue impacts the AptosBFT liveness guarantee that blocks should be executed promptly when proposed. The system recovers when the QC arrives, but the delay window creates an exploitable availability gap.

## Likelihood Explanation

**High Likelihood** - This can occur through multiple scenarios:

1. **Natural Occurrence**: Network partitions, validator crashes, or maintenance periods naturally create situations where opt_batch authors are unreachable
2. **Race Condition**: The QC/transaction fetch race is inherent to the design and will frequently resolve with None when network latency varies
3. **Accidental Misconfiguration**: Proposers including batches from recently-offline validators without malicious intent
4. **Intentional Exploitation**: Malicious proposers deliberately selecting batches from offline validators to delay consensus

The default configuration (requesting 5 peers but only having 2) makes this issue likely to manifest whenever opt_batch authors are temporarily unreachable.

## Recommendation

Add additional fallback peer sources when `additional_peers_to_request` is None:

```rust
async fn process_optqs_payload<T: TDataInfo>(
    data_ptr: &BatchPointer<T>,
    batch_reader: Arc<dyn BatchReader>,
    block: &Block,
    ordered_authors: &[PeerId],
    additional_peers_to_request: Option<&BitVec>,
) -> ExecutorResult<Vec<SignedTransaction>> {
    let mut signers = Vec::new();
    if let Some(peers) = additional_peers_to_request {
        for i in peers.iter_ones() {
            if let Some(author) = ordered_authors.get(i) {
                signers.push(*author);
            }
        }
    } else {
        // MITIGATION: When additional_peers is None, add random validators as fallback
        // to ensure sufficient peer diversity for batch fetching
        let mut rng = rand::thread_rng();
        let num_fallback_peers = std::cmp::min(5, ordered_authors.len());
        let fallback_peers: Vec<_> = ordered_authors
            .choose_multiple(&mut rng, num_fallback_peers)
            .cloned()
            .collect();
        signers.extend(fallback_peers);
    }
    
    if let Some(author) = block.author() {
        signers.push(author);
    }

    // Rest of function unchanged...
}
```

Alternatively, always wait for the QC in `materialize_block` before fetching opt_batches, removing the race condition entirely.

## Proof of Concept

The issue can be reproduced by:

1. Creating an OptQuorumStore payload with opt_batches from validator V1
2. Having V1 go offline
3. Having the block author also be unresponsive
4. Calling `get_transactions(block, None)` before the QC arrives
5. Observing batch fetch failures and repeated retries in logs

Expected behavior: `RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT` counter increments repeatedly, `materialize_block` retries logged every 100ms until QC arrives with `block_voters` providing additional peers.

## Notes

The issue specifically affects **opt_batches** only. Proof batches (proof_with_data) are unaffected because `ProofOfStore::signers()` returns all multi-signature participants (a quorum), providing sufficient peer diversity. Inline batches are also unaffected as transactions are already embedded in the payload.

The vulnerability demonstrates a design inconsistency where different batch types have different availability guarantees despite being part of the same payload.

### Citations

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L601-637)
```rust
async fn process_optqs_payload<T: TDataInfo>(
    data_ptr: &BatchPointer<T>,
    batch_reader: Arc<dyn BatchReader>,
    block: &Block,
    ordered_authors: &[PeerId],
    additional_peers_to_request: Option<&BitVec>,
) -> ExecutorResult<Vec<SignedTransaction>> {
    let mut signers = Vec::new();
    if let Some(peers) = additional_peers_to_request {
        for i in peers.iter_ones() {
            if let Some(author) = ordered_authors.get(i) {
                signers.push(*author);
            }
        }
    }
    if let Some(author) = block.author() {
        signers.push(author);
    }

    let batches_and_responders = data_ptr
        .batch_summary
        .iter()
        .map(|summary| {
            let mut signers = signers.clone();
            signers.append(&mut summary.signers(ordered_authors));

            (summary.info().clone(), signers)
        })
        .collect();

    QuorumStorePayloadManager::request_and_wait_transactions(
        batches_and_responders,
        block.timestamp_usecs(),
        batch_reader,
    )
    .await
}
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L187-189)
```rust
    fn signers(&self, _ordered_authors: &[PeerId]) -> Vec<PeerId> {
        vec![self.author()]
    }
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L697-699)
```rust
    fn signers(&self, ordered_authors: &[PeerId]) -> Vec<PeerId> {
        self.shuffled_signers(ordered_authors)
    }
```

**File:** consensus/src/block_preparer.rs (L54-63)
```rust
        let (txns, max_txns_from_block_to_execute, block_gas_limit) = tokio::select! {
                // Poll the block qc future until a QC is received. Ignore None outcomes.
                Some(qc) = block_qc_fut => {
                    let block_voters = Some(qc.ledger_info().get_voters_bitvec().clone());
                    self.payload_manager.get_transactions(block, block_voters).await
                },
                result = self.payload_manager.get_transactions(block, None) => {
                   result
                }
        }?;
```

**File:** consensus/src/quorum_store/batch_requester.rs (L40-64)
```rust
    fn next_request_peers(&mut self, num_peers: usize) -> Option<Vec<PeerId>> {
        let signers = self.signers.lock();
        if self.num_retries == 0 {
            let mut rng = rand::thread_rng();
            // make sure nodes request from the different set of nodes
            self.next_index = rng.r#gen::<usize>() % signers.len();
            counters::SENT_BATCH_REQUEST_COUNT.inc_by(num_peers as u64);
        } else {
            counters::SENT_BATCH_REQUEST_RETRY_COUNT.inc_by(num_peers as u64);
        }
        if self.num_retries < self.retry_limit {
            self.num_retries += 1;
            let ret = signers
                .iter()
                .cycle()
                .skip(self.next_index)
                .take(num_peers)
                .cloned()
                .collect();
            self.next_index = (self.next_index + num_peers) % signers.len();
            Some(ret)
        } else {
            None
        }
    }
```

**File:** config/src/config/quorum_store_config.rs (L127-128)
```rust
            batch_request_num_peers: 5,
            batch_request_retry_limit: 10,
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L634-646)
```rust
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
```
