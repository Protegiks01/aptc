# Audit Report

## Title
Memory Exhaustion DoS via Unbounded `proposer_window_num_validators_multiplier` Leading to Network-Wide Validator Crashes

## Summary
The `proposer_window_num_validators_multiplier` field in the consensus configuration lacks validation, allowing a malicious or compromised governance proposal to set it to an extremely large value. This causes all validators to panic during epoch transitions when attempting to allocate enormous history buffers for leader reputation tracking, resulting in total network unavailability.

## Finding Description

The Aptos consensus uses leader reputation-based proposer election to prioritize validators with good historical performance. The `proposer_window_num_validators_multiplier` parameter controls how much history is considered when calculating reputation scores. [1](#0-0) 

During epoch initialization, this multiplier is used without validation to calculate the proposer window size: [2](#0-1) 

The calculated `window_size` is then passed to create an `AptosDBBackend` for fetching historical block metadata: [3](#0-2) 

When the leader reputation system is first invoked during consensus, it attempts to fetch historical events from the database: [4](#0-3) 

This triggers a database query that attempts to pre-allocate a vector with the calculated capacity: [5](#0-4) 

**Attack Path:**
1. Malicious governance proposal sets `proposer_window_num_validators_multiplier` to a very large value (e.g., `usize::MAX / 1000`)
2. With 100 validators: `proposer_window_size = 100 * (usize::MAX / 1000) ≈ 1.8 × 10^16`
3. During next epoch transition, `AptosDBBackend` is created with this enormous window size
4. When `get_block_metadata()` is called for proposer election, it invokes `refresh_db_result()`
5. This calls `get_latest_block_events(window_size + seek_len)` with the huge limit
6. `Vec::with_capacity(1.8 × 10^16)` is called, attempting to allocate ~1.8 × 10^19 bytes (18 exabytes)
7. This exceeds `isize::MAX` (9.2 × 10^18 bytes on 64-bit systems), causing an immediate panic
8. All validators crash simultaneously as they all process the same epoch change
9. Network consensus halts completely, requiring manual intervention and rollback

The on-chain config validation only checks that the config bytes are non-empty, with no bounds checking: [6](#0-5) 

**Additional Attack: Setting to 0**

Alternatively, setting `proposer_window_num_validators_multiplier = 0` eliminates reputation-based proposer exclusion entirely. With zero window size, all validators appear to have zero successful and failed proposals, making the reputation system unable to identify and deprioritize poorly-performing or malicious proposers. This weakens consensus liveness guarantees but doesn't cause immediate network failure.

## Impact Explanation

**Severity: Critical**

This vulnerability enables **total loss of liveness/network availability**, which qualifies for Critical severity ($1,000,000 maximum bounty) under Aptos bug bounty criteria.

**Scope of Impact:**
- All validator nodes crash simultaneously during epoch transition
- Network-wide consensus failure requiring emergency intervention
- Potential need for coordinated rollback/hardfork to recover
- No automatic recovery mechanism exists

**Broken Invariants:**
1. **Resource Limits**: "All operations must respect gas, storage, and computational limits" - The unbounded allocation violates memory resource limits
2. **Consensus Safety**: Network availability is compromised, breaking liveness guarantees essential for BFT consensus

## Likelihood Explanation

**Likelihood: Medium to High**

While this requires a malicious governance proposal to pass, several realistic scenarios enable exploitation:

1. **Compromised Governance**: A sophisticated attacker could craft a seemingly benign proposal with the malicious parameter buried in serialized config bytes
2. **Proposal Validation Gap**: No automated validation exists to detect out-of-range values before proposal execution
3. **Social Engineering**: Proposal could be disguised as a performance optimization ("increase history window for better leader selection")
4. **Implementation Bugs**: Errors in proposal generation tools could accidentally create malicious configs

The attack is deterministic once the malicious config is applied - it will trigger on all validators at the next epoch boundary, making it highly effective.

## Recommendation

Implement strict validation for `proposer_window_num_validators_multiplier` at multiple layers:

**1. Add Move-side validation:**

```move
// In consensus_config.move
const E_INVALID_WINDOW_MULTIPLIER: u64 = 2;
const MAX_WINDOW_MULTIPLIER: u64 = 1000; // Conservative upper bound

public fun validate_config_bytes(config_bytes: vector<u8>) {
    // Add native validation function that deserializes and checks bounds
    assert!(
        validate_consensus_config_internal(config_bytes),
        error::invalid_argument(E_INVALID_WINDOW_MULTIPLIER)
    );
}

native fun validate_consensus_config_internal(config_bytes: vector<u8>): bool;
```

**2. Add Rust-side validation during deserialization:**

```rust
// In types/src/on_chain_config/consensus_config.rs
impl ProposerAndVoterConfig {
    const MAX_WINDOW_MULTIPLIER: usize = 1000;
    const MIN_WINDOW_MULTIPLIER: usize = 1;
    
    pub fn validate(&self) -> Result<()> {
        ensure!(
            self.proposer_window_num_validators_multiplier >= Self::MIN_WINDOW_MULTIPLIER,
            "proposer_window_num_validators_multiplier must be at least {}",
            Self::MIN_WINDOW_MULTIPLIER
        );
        ensure!(
            self.proposer_window_num_validators_multiplier <= Self::MAX_WINDOW_MULTIPLIER,
            "proposer_window_num_validators_multiplier exceeds maximum of {}",
            Self::MAX_WINDOW_MULTIPLIER
        );
        ensure!(
            self.voter_window_num_validators_multiplier <= Self::MAX_WINDOW_MULTIPLIER,
            "voter_window_num_validators_multiplier exceeds maximum of {}",
            Self::MAX_WINDOW_MULTIPLIER
        );
        Ok(())
    }
}
```

**3. Add checked multiplication to prevent overflow:**

```rust
// In consensus/src/epoch_manager.rs
let proposer_window_size = proposers.len()
    .checked_mul(proposer_and_voter_config.proposer_window_num_validators_multiplier)
    .context("proposer_window_size calculation overflow")?;

let voter_window_size = proposers.len()
    .checked_mul(proposer_and_voter_config.voter_window_num_validators_multiplier)
    .context("voter_window_size calculation overflow")?;
```

**4. Add runtime validation before allocation:**

```rust
// In storage/aptosdb/src/db/aptosdb_reader.rs
fn get_latest_block_events(&self, num_events: usize) -> Result<Vec<EventWithVersion>> {
    const MAX_EVENTS_LIMIT: usize = 100_000; // Reasonable upper bound
    
    ensure!(
        num_events <= MAX_EVENTS_LIMIT,
        "Requested {} events exceeds maximum limit of {}",
        num_events,
        MAX_EVENTS_LIMIT
    );
    
    // ... existing allocation code
}
```

## Proof of Concept

```rust
// Rust test demonstrating the vulnerability
#[test]
#[should_panic(expected = "capacity overflow")]
fn test_malicious_window_multiplier_causes_panic() {
    use aptos_types::on_chain_config::{
        ProposerAndVoterConfig, ConsensusConfigV1, ProposerElectionType,
        LeaderReputationType, OnChainConsensusConfig,
    };
    
    // Create malicious config with huge multiplier
    let malicious_config = ProposerAndVoterConfig {
        active_weight: 1000,
        inactive_weight: 10,
        failed_weight: 1,
        failure_threshold_percent: 10,
        proposer_window_num_validators_multiplier: usize::MAX / 100, // Huge value
        voter_window_num_validators_multiplier: 1,
        weight_by_voting_power: true,
        use_history_from_previous_epoch_max_count: 5,
    };
    
    // Simulate what happens during epoch initialization
    let num_validators = 100;
    let proposer_window_size = num_validators * malicious_config.proposer_window_num_validators_multiplier;
    
    // This will overflow on 64-bit systems when multiplied by sizeof(EventWithVersion)
    // Attempting to allocate this will panic
    let _would_panic: Vec<u8> = Vec::with_capacity(proposer_window_size);
    
    // In reality, the panic occurs in get_latest_block_events() during epoch transition
}
```

**To exploit in production:**

1. Create governance proposal with serialized `OnChainConsensusConfig` containing `proposer_window_num_validators_multiplier: usize::MAX / 100`
2. Submit proposal through on-chain governance
3. If proposal passes and is executed, next epoch transition triggers the allocation panic across all validators
4. Network halts, requiring coordinated recovery effort

## Notes

This vulnerability exemplifies the critical importance of input validation for governance-controlled parameters, especially those affecting resource allocation. The lack of defense-in-depth (no validation at Move level, Rust deserialization, or allocation site) makes this a systemic issue rather than a simple coding error.

The secondary issue (setting to 0) eliminates reputation tracking but doesn't cause immediate network failure, representing a High severity protocol violation rather than Critical DoS.

### Citations

**File:** types/src/on_chain_config/consensus_config.rs (L552-575)
```rust
#[derive(Clone, Copy, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub struct ProposerAndVoterConfig {
    // Selection weight for active validators with proposer failures below threshold
    pub active_weight: u64,
    // Selection weight for inactive validators with proposer failures below threshold
    pub inactive_weight: u64,
    // Selection weight for validators with proposer failures above threshold
    pub failed_weight: u64,
    // Thresholed of failures in the rounds validator was selected to be proposer
    // integer values representing percentages, i.e. 12 is 12%.
    pub failure_threshold_percent: u32,
    // Window into history considered for proposer statistics, multiplier
    // on top of number of validators
    pub proposer_window_num_validators_multiplier: usize,
    // Window into history considered for votre statistics, multiplier
    // on top of number of validators
    pub voter_window_num_validators_multiplier: usize,
    // Flag whether to use voting power as multiplier to the weights
    pub weight_by_voting_power: bool,
    // Flag whether to use history from previous epoch (0 if not),
    // representing a number of historical epochs (beyond the current one)
    // to consider.
    pub use_history_from_previous_epoch_max_count: u32,
}
```

**File:** consensus/src/epoch_manager.rs (L314-317)
```rust
                        let proposer_window_size = proposers.len()
                            * proposer_and_voter_config.proposer_window_num_validators_multiplier;
                        let voter_window_size = proposers.len()
                            * proposer_and_voter_config.voter_window_num_validators_multiplier;
```

**File:** consensus/src/epoch_manager.rs (L342-346)
```rust
                let backend = Arc::new(AptosDBBackend::new(
                    window_size,
                    seek_len,
                    self.storage.aptos_db(),
                ));
```

**File:** consensus/src/liveness/leader_reputation.rs (L70-78)
```rust
    fn refresh_db_result(
        &self,
        locked: &mut MutexGuard<'_, Option<(Vec<VersionedNewBlockEvent>, u64, bool)>>,
        latest_db_version: u64,
    ) -> Result<(Vec<VersionedNewBlockEvent>, u64, bool)> {
        // assumes target round is not too far from latest commit
        let limit = self.window_size + self.seek_len;

        let events = self.aptos_db.get_latest_block_events(limit)?;
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L742-759)
```rust
    fn get_latest_block_events(&self, num_events: usize) -> Result<Vec<EventWithVersion>> {
        gauged_api("get_latest_block_events", || {
            let latest_version = self.get_synced_version()?;
            if !self.skip_index_and_usage {
                return self.get_events(
                    &new_block_event_key(),
                    u64::MAX,
                    Order::Descending,
                    num_events as u64,
                    latest_version.unwrap_or(0),
                );
            }

            let db = self.ledger_db.metadata_db_arc();
            let mut iter = db.rev_iter::<BlockInfoSchema>()?;
            iter.seek_to_last();

            let mut events = Vec::with_capacity(num_events);
```

**File:** aptos-move/framework/aptos-framework/sources/configs/consensus_config.move (L52-56)
```text
    public fun set_for_next_epoch(account: &signer, config: vector<u8>) {
        system_addresses::assert_aptos_framework(account);
        assert!(vector::length(&config) > 0, error::invalid_argument(EINVALID_CONFIG));
        std::config_buffer::upsert<ConsensusConfig>(ConsensusConfig {config});
    }
```
