# Audit Report

## Title
Consensus Observer Latency Cache Poisoning via Decoupled Ping and Message Delivery Performance

## Summary
The `get_latency_for_peer()` function in the consensus observer uses cached ping latency metrics (`average_ping_latency_secs`) to rank peers for subscription, but these metrics can be stale and don't reflect actual consensus message delivery performance. An attacker can maintain low ping response times while degrading actual message delivery, causing observers to remain subscribed to underperforming peers.

## Finding Description

The consensus observer system uses peer latency metrics to select optimal peers for subscriptions. The vulnerability exists in the decoupling of two measurement systems:

1. **Ping Latency Measurement**: The peer monitoring service measures latency by sending `LatencyPing` requests every 30 seconds and averaging the last 10 responses (5-minute sliding window). [1](#0-0) 

2. **Message Delivery Performance**: The consensus observer receives actual consensus messages (blocks, commits) from subscribed peers.

The `get_latency_for_peer()` function simply reads the cached `average_ping_latency_secs` from peer metadata without any staleness checks or validation: [2](#0-1) 

This cached value is used in `sort_peers_by_subscription_optimality()` to rank peers for subscription selection: [3](#0-2) 

**Attack Scenario:**

1. Attacker establishes connection and responds quickly to ping requests, building low latency history
2. Consensus observer subscribes to attacker based on optimal latency ranking
3. Attacker continues responding to pings quickly (maintaining low `average_ping_latency_secs`)
4. **BUT** attacker delays actual consensus message delivery by 10-14 seconds (below the 15-second timeout)
5. The observer's subscription health check sees the peer as "optimal" because ping latency is cached and stale: [4](#0-3) 

6. The observer doesn't switch peers and experiences degraded consensus tracking

The peer metadata is only updated every 5 seconds: [5](#0-4) 

But even with this update frequency, the underlying latency measurements have a 5-minute sliding window, making them inherently stale.

## Impact Explanation

**Medium Severity** - This qualifies under "State inconsistencies requiring intervention" in the Aptos bug bounty program.

**Impact:**
- Consensus observer nodes (VFNs/PFNs) experience 10-14 second delays in receiving consensus messages
- Observers remain subscribed to degraded peers due to stale latency cache
- Observer nodes fall behind consensus, impacting their ability to serve fresh data
- Multiple observers can be affected simultaneously if attacker is optimally positioned
- The optimality check only runs every 3 minutes by default: [6](#0-5) 

- Subscription timeout eventually detects the issue (15 seconds) but causes service degradation window

**Affected Systems:**
- All consensus observer nodes (primarily VFNs and PFNs)
- Does not affect core validator consensus safety
- Impacts network health and observer reliability

## Likelihood Explanation

**High Likelihood** - This attack is straightforward to execute:

1. **Low Complexity**: Attacker only needs to:
   - Connect as a peer to target nodes
   - Respond quickly to simple ping requests (cheap operation)
   - Delay consensus message forwarding (selective delay)

2. **No Special Privileges Required**: Any peer can perform this attack

3. **Difficult to Detect**: The peer appears "optimal" in metrics while degrading actual service

4. **Persistent Effect**: The 5-minute latency sliding window means the attack persists even after detection

5. **Multiple Targets**: Single attacker can affect multiple observer nodes simultaneously

## Recommendation

**Immediate Fix:**

1. **Add staleness checks to latency metrics:**
   - Track timestamp of last latency measurement update
   - Invalidate latency metrics older than a threshold (e.g., 2-3x ping interval)
   - Use maximum latency value for stale measurements

2. **Correlate ping latency with actual message delivery:**
   - Track actual consensus message delivery latency separately
   - Weight peer selection based on both ping latency AND message delivery performance
   - Detect discrepancies between ping and message latency as suspicious

3. **Enhanced peer optimality checks:**
   - Add message delivery latency to `PeerMonitoringMetadata`
   - Modify `sort_peers_by_subscription_optimality()` to consider message delivery metrics
   - Flag peers with significant ping/delivery latency discrepancy

**Code Fix Example:**

In `subscription_utils.rs`, enhance `get_latency_for_peer()`:

```rust
fn get_latency_for_peer(
    peer_network_id: &PeerNetworkId,
    peer_metadata: &PeerMetadata,
) -> Option<f64> {
    let peer_monitoring_metadata = peer_metadata.get_peer_monitoring_metadata();
    
    // Check if latency measurement is stale
    if let Some(latest_ping_time) = peer_monitoring_metadata.latest_ping_timestamp {
        let age = current_time() - latest_ping_time;
        let max_age = Duration::from_secs(90); // 3x ping interval
        
        if age > max_age {
            warn!("Latency metric for peer {:?} is stale (age: {:?})", 
                  peer_network_id, age);
            return Some(MAX_PING_LATENCY_SECS); // Use max penalty for stale data
        }
    }
    
    let latency = peer_monitoring_metadata.average_ping_latency_secs;
    
    if latency.is_none() {
        warn!("Unable to get latency for peer! Peer: {:?}", peer_network_id);
    }
    
    latency
}
```

4. **Add message delivery metrics to health checks:**
   - Track average consensus message delivery time per peer
   - Terminate subscriptions where message delivery significantly exceeds ping latency

## Proof of Concept

**Rust Integration Test:**

```rust
#[tokio::test]
async fn test_latency_cache_poisoning_attack() {
    // Setup: Create consensus observer with peer monitoring
    let observer_config = ConsensusObserverConfig::default();
    let (peers_and_metadata, observer_client, _) = create_test_environment();
    
    // Step 1: Attacker establishes connection with low latency
    let attacker_peer = PeerNetworkId::random();
    let low_latency = 0.05; // 50ms - very low
    create_peer_with_latency(
        peers_and_metadata.clone(),
        attacker_peer,
        low_latency,
    );
    
    // Step 2: Observer subscribes to attacker (due to low latency)
    let subscriptions = create_new_subscriptions(
        observer_config,
        observer_client.clone(),
        None,
        Arc::new(MockDatabaseReader::new()),
        TimeService::real(),
        peers_and_metadata.get_connected_peers_and_metadata().unwrap(),
        1, // Create 1 subscription
        vec![],
        vec![],
    )
    .await;
    
    assert_eq!(subscriptions.len(), 1);
    assert_eq!(subscriptions[0].get_peer_network_id(), attacker_peer);
    
    // Step 3: Attacker continues responding to pings quickly
    // (ping latency remains low at 50ms)
    
    // Step 4: Simulate attacker delaying actual consensus messages
    let message_delay = Duration::from_secs(12); // 12-second delay
    let start = Instant::now();
    
    // Send delayed message (but within 15-second timeout)
    tokio::time::sleep(message_delay).await;
    send_consensus_message_to_observer(attacker_peer, test_message());
    
    let actual_delivery_time = start.elapsed();
    assert!(actual_delivery_time >= message_delay);
    assert!(actual_delivery_time < Duration::from_secs(15)); // Below timeout
    
    // Step 5: Verify peer still considered optimal due to cached low latency
    let peer_metadata = peers_and_metadata
        .get_metadata_for_peer(attacker_peer)
        .unwrap();
    let cached_latency = get_latency_for_peer(&attacker_peer, &peer_metadata);
    
    // The cached latency is still low despite actual poor performance
    assert!(cached_latency.unwrap() < 0.1); // Still shows as 50ms
    
    // Step 6: Verify observer doesn't switch away from attacker
    let sorted_peers = sort_peers_by_subscription_optimality(
        &peers_and_metadata.get_connected_peers_and_metadata().unwrap()
    );
    
    // Attacker is still ranked as optimal peer
    assert_eq!(sorted_peers[0], attacker_peer);
    
    // Vulnerability confirmed: Observer stays subscribed to degraded peer
    println!("VULNERABILITY: Observer remains subscribed to peer with 12s message delays while cached latency shows 50ms");
}
```

**Notes:**
- This demonstrates decoupling between ping latency and actual message delivery
- Attacker maintains "optimal" status while degrading performance
- Observer cannot detect the attack through current latency metrics
- Requires adding timestamp tracking to peer monitoring metadata for complete fix

### Citations

**File:** config/src/config/peer_monitoring_config.rs (L21-35)
```rust
impl Default for PeerMonitoringServiceConfig {
    fn default() -> Self {
        Self {
            enable_peer_monitoring_client: true,
            latency_monitoring: LatencyMonitoringConfig::default(),
            max_concurrent_requests: 1000,
            max_network_channel_size: 1000,
            max_num_response_bytes: 100 * 1024, // 100 KB
            max_request_jitter_ms: 1000,        // Monitoring requests are very infrequent
            metadata_update_interval_ms: 5000,  // 5 seconds
            network_monitoring: NetworkMonitoringConfig::default(),
            node_monitoring: NodeMonitoringConfig::default(),
            peer_monitor_interval_usec: 1_000_000, // 1 second
        }
    }
```

**File:** config/src/config/peer_monitoring_config.rs (L47-56)
```rust
impl Default for LatencyMonitoringConfig {
    fn default() -> Self {
        Self {
            latency_ping_interval_ms: 30_000, // 30 seconds
            latency_ping_timeout_ms: 20_000,  // 20 seconds
            max_latency_ping_failures: 3,
            max_num_latency_pings_to_retain: 10,
        }
    }
}
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L221-240)
```rust
fn get_latency_for_peer(
    peer_network_id: &PeerNetworkId,
    peer_metadata: &PeerMetadata,
) -> Option<f64> {
    // Get the latency for the peer
    let peer_monitoring_metadata = peer_metadata.get_peer_monitoring_metadata();
    let latency = peer_monitoring_metadata.average_ping_latency_secs;

    // If the latency is missing, log a warning
    if latency.is_none() {
        warn!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Unable to get latency for peer! Peer: {:?}",
                peer_network_id
            ))
        );
    }

    latency
}
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L283-312)
```rust
pub fn sort_peers_by_subscription_optimality(
    peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) -> Vec<PeerNetworkId> {
    // Group peers and latencies by validator distance, i.e., distance -> [(peer, latency)]
    let mut unsupported_peers = Vec::new();
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for (peer_network_id, peer_metadata) in peers_and_metadata {
        // Verify that the peer supports consensus observer
        if !supports_consensus_observer(peer_metadata) {
            unsupported_peers.push(*peer_network_id);
            continue; // Skip the peer
        }

        // Get the distance and latency for the peer
        let distance = get_distance_for_peer(peer_network_id, peer_metadata);
        let latency = get_latency_for_peer(peer_network_id, peer_metadata);

        // If the distance is not found, use the maximum distance
        let distance =
            distance.unwrap_or(aptos_peer_monitoring_service_types::MAX_DISTANCE_FROM_VALIDATORS);

        // If the latency is not found, use a large latency
        let latency = latency.unwrap_or(MAX_PING_LATENCY_SECS);

        // Add the peer and latency to the distance group
        peers_and_latencies_by_distance
            .entry(distance)
            .or_insert_with(Vec::new)
            .push((*peer_network_id, OrderedFloat(latency)));
    }
```

**File:** consensus/src/consensus_observer/observer/subscription.rs (L100-162)
```rust
    fn check_subscription_peer_optimality(
        &mut self,
        peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
        skip_peer_optimality_check: bool,
    ) -> Result<(), Error> {
        // Get the last optimality check time and connected peers
        let (last_optimality_check_time, last_optimality_check_peers) =
            self.last_optimality_check_time_and_peers.clone();

        // If we're skipping the peer optimality check, update the last check time and return
        let time_now = self.time_service.now();
        if skip_peer_optimality_check {
            self.last_optimality_check_time_and_peers = (time_now, last_optimality_check_peers);
            return Ok(());
        }

        // Determine if enough time has elapsed to force a refresh
        let duration_since_last_check = time_now.duration_since(last_optimality_check_time);
        let refresh_interval = Duration::from_millis(
            self.consensus_observer_config
                .subscription_refresh_interval_ms,
        );
        let force_refresh = duration_since_last_check >= refresh_interval;

        // Determine if the peers have changed since the last check.
        // Note: we only check for peer changes periodically to avoid
        // excessive subscription churn due to peer connects/disconnects.
        let current_connected_peers = peers_and_metadata.keys().cloned().collect();
        let peer_check_interval = Duration::from_millis(
            self.consensus_observer_config
                .subscription_peer_change_interval_ms,
        );
        let peers_changed = duration_since_last_check >= peer_check_interval
            && current_connected_peers != last_optimality_check_peers;

        // Determine if we should perform the optimality check
        if !force_refresh && !peers_changed {
            return Ok(()); // We don't need to check optimality yet
        }

        // Otherwise, update the last peer optimality check time and peers
        self.last_optimality_check_time_and_peers = (time_now, current_connected_peers);

        // Sort the peers by subscription optimality
        let sorted_peers =
            subscription_utils::sort_peers_by_subscription_optimality(peers_and_metadata);

        // Verify that this peer is one of the most optimal peers
        let max_concurrent_subscriptions =
            self.consensus_observer_config.max_concurrent_subscriptions as usize;
        if !sorted_peers
            .iter()
            .take(max_concurrent_subscriptions)
            .any(|peer| peer == &self.peer_network_id)
        {
            return Err(Error::SubscriptionSuboptimal(format!(
                "Subscription to peer: {} is no longer optimal! New optimal peers: {:?}",
                self.peer_network_id, sorted_peers
            )));
        }

        Ok(())
    }
```

**File:** config/src/config/consensus_observer_config.rs (L63-84)
```rust
impl Default for ConsensusObserverConfig {
    fn default() -> Self {
        Self {
            observer_enabled: false,
            publisher_enabled: false,
            max_network_channel_size: 1000,
            max_parallel_serialization_tasks: num_cpus::get(), // Default to the number of CPUs
            network_request_timeout_ms: 5_000,                 // 5 seconds
            garbage_collection_interval_ms: 60_000,            // 60 seconds
            max_num_pending_blocks: 150, // 150 blocks (sufficient for existing production networks)
            progress_check_interval_ms: 5_000, // 5 seconds
            max_concurrent_subscriptions: 2, // 2 streams should be sufficient
            max_subscription_sync_timeout_ms: 15_000, // 15 seconds
            max_subscription_timeout_ms: 15_000, // 15 seconds
            subscription_peer_change_interval_ms: 180_000, // 3 minutes
            subscription_refresh_interval_ms: 600_000, // 10 minutes
            observer_fallback_duration_ms: 600_000, // 10 minutes
            observer_fallback_startup_period_ms: 60_000, // 60 seconds
            observer_fallback_progress_threshold_ms: 10_000, // 10 seconds
            observer_fallback_sync_lag_threshold_ms: 15_000, // 15 seconds
        }
    }
```
