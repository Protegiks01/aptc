# Audit Report

## Title
Indexer gRPC Data Service Trusts Upstream Server Without Cryptographic Verification - Byzantine Server Can Poison Indexer State

## Summary
The indexer-grpc-data-service-v2 fetches transaction data from an upstream gRPC server without performing any cryptographic verification of the data's authenticity or integrity. This trust model violation allows a Byzantine gRPC server to serve arbitrary, malicious transaction data that will be cached and served to downstream indexer clients, breaking the State Consistency invariant.

## Finding Description
The `fetch_and_update_cache()` function in the indexer data service retrieves transactions from an upstream gRPC server and stores them in the cache without verification. [1](#0-0) 

The function calls `data_client.fetch_transactions()` which makes a gRPC request: [2](#0-1) 

The data client only checks that the first transaction version matches the requested version, but performs **no cryptographic validation** of:
- Transaction hashes against signed ledger info
- State change hashes
- Event root hashes
- Merkle proofs
- Validator signatures

The transactions are then stored directly in the data manager: [3](#0-2) 

**Attack Path:**
1. Attacker operates a malicious gRPC server endpoint
2. Configure indexer data service to connect to the malicious server via `grpc_manager_addresses` config
3. Malicious server serves fabricated Transaction protobuf messages with:
   - Modified transaction data
   - Incorrect state changes
   - Fake events
   - Wrong gas amounts
4. Indexer caches and serves this poisoned data to all downstream clients
5. Applications relying on the indexer make decisions based on false blockchain state

**Broken Invariant:** This violates the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." The indexer cannot verify state consistency because it doesn't validate proofs.

**Contrast with Proper Verification:**
Aptos provides `TransactionListWithProof` with cryptographic verification: [4](#0-3) 

This verification ensures transactions are cryptographically proven against a validator-signed ledger info. The indexer data service does not use this mechanism at all.

## Impact Explanation
This is **High Severity** per Aptos bug bounty criteria for "Significant protocol violations" and "API crashes/data integrity issues."

**Impact:**
- **Data Integrity Breach**: Any application consuming indexer data receives unverified, potentially false blockchain state
- **User Fund Risk**: DApps making financial decisions (transfers, swaps, lending) based on false indexer data could lose funds
- **Ecosystem Trust**: Undermines confidence in Aptos indexer infrastructure
- **Cascading Failures**: Poisoned indexer state propagates to all downstream consumers (wallets, explorers, analytics platforms)

The vulnerability affects all instances of indexer-grpc-data-service-v2 in production that connect to potentially compromised or malicious upstream servers.

## Likelihood Explanation
**Likelihood: HIGH**

The attack is feasible because:
1. **Low Barrier**: Attacker only needs to run a gRPC server implementing the expected API
2. **No Authentication**: The data service doesn't validate the identity or trustworthiness of the upstream server
3. **Configuration Flexibility**: `grpc_manager_addresses` can point to arbitrary endpoints
4. **Operational Reality**: In distributed deployments, multiple gRPC servers may be used for load balancing, increasing attack surface

**Realistic Scenarios:**
- Compromised infrastructure provider
- Insider threat at indexer operator
- DNS/BGP hijacking to redirect to malicious server
- Misconfigured deployment pointing to untrusted endpoint

## Recommendation
Implement cryptographic verification of all transaction data received from the gRPC server:

1. **Use TransactionListWithProof**: Modify the gRPC API to return `TransactionListWithProof` instead of raw `Transaction` objects

2. **Maintain Trusted Ledger State**: Store and update a trusted `LedgerInfoWithSignatures` that has been verified with validator signatures: [5](#0-4) 

3. **Verify All Transactions**: Before caching, call the `verify()` method on received transaction lists:

```rust
// In fetch_and_update_cache()
async fn fetch_and_update_cache(
    data_client: Arc<DataClient>,
    data_manager: Arc<RwLock<DataManager>>,
    trusted_state: Arc<RwLock<TrustedState>>, // ADD THIS
    version: u64,
) -> usize {
    let transaction_list_with_proof = data_client.fetch_transactions_with_proof(version).await;
    
    // VERIFY BEFORE CACHING
    let trusted_state_guard = trusted_state.read().await;
    let ledger_info = trusted_state_guard.latest_ledger_info();
    
    transaction_list_with_proof.verify(
        ledger_info,
        Some(version)
    ).expect("Transaction proof verification failed");
    
    let transactions = transaction_list_with_proof.transactions;
    let len = transactions.len();
    
    if len > 0 {
        data_manager
            .write()
            .await
            .update_data(version, transactions);
    }
    
    len
}
```

4. **Update Trusted State**: Periodically fetch and verify new `LedgerInfoWithSignatures` from the upstream server and update the trusted state using `verify_and_ratchet()`: [6](#0-5) 

## Proof of Concept
**Setup:**
1. Deploy indexer-grpc-data-service-v2 with standard configuration
2. Create malicious gRPC server that implements `GetTransactions` RPC
3. Configure indexer to connect to malicious server

**Malicious Server (Rust pseudo-code):**
```rust
// Malicious gRPC server that serves fake transactions
impl DataService for MaliciousServer {
    async fn get_transactions(
        &self,
        request: Request<GetTransactionsRequest>,
    ) -> Result<Response<TransactionsResponse>, Status> {
        let fake_transaction = Transaction {
            version: request.starting_version.unwrap(),
            // FAKE DATA - transfer 1M APT from victim to attacker
            info: Some(TransactionInfo {
                hash: random_hash(),  // Wrong hash
                state_change_hash: random_hash(),  // Wrong hash
                // ... fake data
            }),
            txn_data: Some(UserTransaction {
                // Fabricated transfer transaction
            }),
            // ... more fake fields
        };
        
        Ok(Response::new(TransactionsResponse {
            transactions: vec![fake_transaction],
            chain_id: Some(1),
        }))
    }
}
```

**Verification:**
1. Query indexer data service for transaction at version X
2. Observe that fake transaction data is returned without any error
3. Compare with actual blockchain state from a validator node
4. Confirm discrepancy proves data was not verified

**Result:** The indexer serves false data to clients because it never verified the transaction authenticity against validator-signed proofs.

## Notes
This vulnerability is particularly concerning because:

1. **Architectural Issue**: The problem exists at the design level - the entire data flow assumes trust in the upstream server
2. **No Defense in Depth**: There are no compensating controls or validation layers
3. **Silent Failure**: Malicious data is served without any warning or error indication
4. **Production Deployment**: This code is actively used in Aptos indexer infrastructure

The proper solution requires redesigning the gRPC API to include cryptographic proofs and implementing verification logic throughout the data pipeline, similar to how the state-sync components operate with verified state proofs.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/fetch_manager.rs (L48-64)
```rust
    async fn fetch_and_update_cache(
        data_client: Arc<DataClient>,
        data_manager: Arc<RwLock<DataManager>>,
        version: u64,
    ) -> usize {
        let transactions = data_client.fetch_transactions(version).await;
        let len = transactions.len();

        if len > 0 {
            data_manager
                .write()
                .await
                .update_data(version, transactions);
        }

        len
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_client.rs (L18-43)
```rust
    pub(super) async fn fetch_transactions(&self, starting_version: u64) -> Vec<Transaction> {
        trace!("Fetching transactions from GrpcManager, start_version: {starting_version}.");

        let request = GetTransactionsRequest {
            starting_version: Some(starting_version),
            transactions_count: None,
            batch_size: None,
            transaction_filter: None,
        };
        loop {
            let mut client = self
                .connection_manager
                .get_grpc_manager_client_for_request();
            let response = client.get_transactions(request.clone()).await;
            if let Ok(response) = response {
                let transactions = response.into_inner().transactions;
                if transactions.is_empty() {
                    return vec![];
                }
                if transactions.first().unwrap().version == starting_version {
                    return transactions;
                }
            }
            // TODO(grao): Error handling.
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_manager.rs (L44-87)
```rust
    pub(super) fn update_data(&mut self, start_version: u64, transactions: Vec<Transaction>) {
        let end_version = start_version + transactions.len() as u64;

        trace!(
            "Updating data for {} transactions in range [{start_version}, {end_version}).",
            transactions.len(),
        );
        if start_version > self.end_version {
            error!(
                "The data is in the future, cache end_version: {}, data start_version: {start_version}.",
                self.end_version
            );
            COUNTER.with_label_values(&["data_too_new"]).inc();
            return;
        }

        if end_version <= self.start_version {
            warn!(
                "The data is too old, cache start_version: {}, data end_version: {end_version}.",
                self.start_version
            );
            COUNTER.with_label_values(&["data_too_old"]).inc();
            return;
        }

        let num_to_skip = self.start_version.saturating_sub(start_version);
        let start_version = start_version.max(self.start_version);

        let mut size_increased = 0;
        let mut size_decreased = 0;

        for (i, transaction) in transactions
            .into_iter()
            .enumerate()
            .skip(num_to_skip as usize)
        {
            let version = start_version + i as u64;
            let slot_index = version as usize % self.num_slots;
            if let Some(transaction) = self.data[slot_index].take() {
                size_decreased += transaction.encoded_len();
            }
            size_increased += transaction.encoded_len();
            self.data[version as usize % self.num_slots] = Some(Box::new(transaction));
        }
```

**File:** types/src/transaction/mod.rs (L2295-2354)
```rust
    pub fn verify(
        &self,
        ledger_info: &LedgerInfo,
        first_transaction_version: Option<Version>,
    ) -> Result<()> {
        // Verify the first transaction versions match
        ensure!(
            self.get_first_transaction_version() == first_transaction_version,
            "First transaction version ({:?}) doesn't match given version ({:?}).",
            self.get_first_transaction_version(),
            first_transaction_version,
        );

        // Verify the lengths of the transactions and transaction infos match
        ensure!(
            self.proof.transaction_infos.len() == self.get_num_transactions(),
            "The number of TransactionInfo objects ({}) does not match the number of \
             transactions ({}).",
            self.proof.transaction_infos.len(),
            self.get_num_transactions(),
        );

        // Verify the transaction hashes match those of the transaction infos
        self.transactions
            .par_iter()
            .zip_eq(self.proof.transaction_infos.par_iter())
            .map(|(txn, txn_info)| {
                let txn_hash = CryptoHash::hash(txn);
                ensure!(
                    txn_hash == txn_info.transaction_hash(),
                    "The hash of transaction does not match the transaction info in proof. \
                     Transaction hash: {:x}. Transaction hash in txn_info: {:x}.",
                    txn_hash,
                    txn_info.transaction_hash(),
                );
                Ok(())
            })
            .collect::<Result<Vec<_>>>()?;

        // Verify the transaction infos are proven by the ledger info.
        self.proof
            .verify(ledger_info, self.get_first_transaction_version())?;

        // Verify the events if they exist.
        if let Some(event_lists) = &self.events {
            ensure!(
                event_lists.len() == self.get_num_transactions(),
                "The length of event_lists ({}) does not match the number of transactions ({}).",
                event_lists.len(),
                self.get_num_transactions(),
            );
            event_lists
                .into_par_iter()
                .zip_eq(self.proof.transaction_infos.par_iter())
                .map(|(events, txn_info)| verify_events_against_root_hash(events, txn_info))
                .collect::<Result<Vec<_>>>()?;
        }

        Ok(())
    }
```

**File:** types/src/ledger_info.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

#[cfg(any(test, feature = "fuzzing"))]
use crate::validator_signer::ValidatorSigner;
use crate::{
    account_address::AccountAddress,
    block_info::{BlockInfo, Round},
    epoch_state::EpochState,
    on_chain_config::ValidatorSet,
    transaction::Version,
    validator_verifier::{ValidatorVerifier, VerifyError},
};
use aptos_crypto::{
    bls12381,
    hash::{CryptoHash, HashValue},
};
use aptos_crypto_derive::{BCSCryptoHash, CryptoHasher};
use derivative::Derivative;
#[cfg(any(test, feature = "fuzzing"))]
use proptest_derive::Arbitrary;
use serde::{Deserialize, Serialize};
use std::{
    collections::BTreeMap,
    fmt::{Display, Formatter},
    mem,
    ops::{Deref, DerefMut},
    sync::{
        atomic::{AtomicBool, Ordering},
        Arc,
    },
};

/// This structure serves a dual purpose.
///
/// First, if this structure is signed by 2f+1 validators it signifies the state of the ledger at
/// version `version` -- it contains the transaction accumulator at that version which commits to
/// all historical transactions. This structure may be expanded to include other information that
/// is derived from that accumulator (e.g. the current time according to the time contract) to
/// reduce the number of proofs a client must get.
///
/// Second, the structure contains a `consensus_data_hash` value. This is the hash of an internal
/// data structure that represents a block that is voted on in Consensus. If 2f+1 signatures are
/// gathered on the same ledger info that represents a Quorum Certificate (QC) on the consensus
/// data.
///
/// Combining these two concepts, when a validator votes on a block, B it votes for a
/// LedgerInfo with the `version` being the latest version that will be committed if B gets 2f+1
/// votes. It sets `consensus_data_hash` to represent B so that if those 2f+1 votes are gathered a
/// QC is formed on B.
```

**File:** types/src/trusted_state.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

#![allow(clippy::arc_with_non_send_sync)]

use crate::{
    epoch_change::{EpochChangeProof, Verifier},
    epoch_state::EpochState,
    ledger_info::{LedgerInfo, LedgerInfoWithSignatures},
    proof::TransactionAccumulatorSummary,
    state_proof::StateProof,
    transaction::Version,
    waypoint::Waypoint,
};
use anyhow::{bail, ensure, format_err, Result};
use aptos_crypto_derive::{BCSCryptoHash, CryptoHasher};
#[cfg(any(test, feature = "fuzzing"))]
use proptest_derive::Arbitrary;
use serde::{Deserialize, Serialize};

/// `TrustedState` keeps track of light clients' latest, trusted view of the
/// ledger state. Light clients can use proofs from a state proof to "ratchet"
/// their view forward to a newer state.
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize, CryptoHasher, BCSCryptoHash)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub enum TrustedState {
    /// The current trusted state is an epoch waypoint, which is a commitment to
    /// an epoch change ledger info. Most light clients will start here when
    /// syncing for the first time.
    EpochWaypoint(Waypoint),
    /// The current trusted state is inside a verified epoch (which includes the
    /// validator set inside that epoch).
    EpochState {
        /// The current trusted version and a commitment to a ledger info inside
        /// the current trusted epoch.
        waypoint: Waypoint,
        /// The current epoch and validator set inside that epoch.
        epoch_state: EpochState,
    },
}

/// `TrustedStateChange` is the result of attempting to ratchet to a new trusted
/// state. In order to reduce redundant error checking, `TrustedStateChange` also
/// contains references to relevant items used to ratchet us.
#[derive(Clone, Debug)]
pub enum TrustedStateChange<'a> {
    /// We have a newer `TrustedState` but it's still in the same epoch, so only
    /// the latest trusted version changed.
    Version { new_state: TrustedState },
    /// We have a newer `TrustedState` and there was at least one epoch change,
```
