# Audit Report

## Title
Silent Task Termination in Peer Monitoring Service Due to Missing Panic Recovery Mechanism

## Summary
The `start_peer_monitor()` function spawns a critical background task via `spawn_peer_metadata_updater()` but fails to store or monitor the returned `JoinHandle<()>`. If this task panics (e.g., due to RwLock poisoning), it terminates silently with no recovery mechanism, causing peer monitoring metadata to become permanently stale. [1](#0-0) 

## Finding Description

The peer monitoring service spawns a metadata updater task that continuously updates peer monitoring information used for peer selection across the network stack. However, the JoinHandle returned by `spawn_peer_metadata_updater()` is discarded rather than stored for monitoring. [2](#0-1) 

The spawned task runs an infinite loop that reads from shared state protected by `aptos_infallible::RwLock`: [3](#0-2) 

The critical vulnerability lies in the use of `aptos_infallible::RwLock`, which panics on poisoned locks rather than returning errors: [4](#0-3) 

**Panic Scenario:**

1. The main monitoring loop performs write operations on `peer_monitor_state.peer_states`: [5](#0-4) 

2. If any code panics while holding the write lock, the RwLock becomes poisoned.

3. The metadata updater task attempts to read from the poisoned lock: [6](#0-5) 

4. The `.read()` call panics, tokio catches it, and the task terminates silently.

5. With no JoinHandle monitoring, there is no detection or recovery mechanism.

**Security Impact:**

Stale peer monitoring metadata affects critical peer selection decisions in:
- State synchronization (aptos-data-client)
- Consensus observer subscriptions
- Network layer peer prioritization [7](#0-6) [8](#0-7) 

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program category "Validator node slowdowns" because:

1. **Permanent Service Degradation**: Once the metadata updater terminates, peer monitoring metadata freezes permanently until node restart.

2. **Peer Selection Degradation**: Stale metadata causes suboptimal peer selection, potentially connecting to slow or malicious peers, degrading synchronization and consensus performance.

3. **Difficult Diagnosis**: The failure is silentâ€”no logs indicate the task died, making debugging extremely challenging in production.

4. **Affects Validator Operations**: Validators rely on accurate peer metadata for efficient state sync and consensus observer functionality.

## Likelihood Explanation

**Likelihood: Medium**

While RwLock poisoning requires a panic while holding a write lock, several realistic scenarios exist:

1. **Memory Pressure Panics**: Allocation failures during HashMap operations under memory pressure.
2. **Dependency Panics**: Bugs in any dependency called during write lock hold (e.g., serialization, metrics).
3. **Concurrent Bug Manifestation**: Race conditions in the main monitoring loop could trigger panics during state updates.
4. **Upgrade Compatibility**: Code changes introducing panics in lock-protected sections.

The use of `aptos_infallible::RwLock` creates cascading panic scenarios where one panic poisons the lock, triggering panics in other tasks. This amplifies the likelihood beyond the initial triggering event.

## Recommendation

**Solution 1: Monitor JoinHandle with Restart Loop**

```rust
pub async fn start_peer_monitor(
    node_config: NodeConfig,
    network_client: NetworkClient<PeerMonitoringServiceMessage>,
    runtime: Option<Handle>,
) {
    let peer_monitoring_client = PeerMonitoringServiceClient::new(network_client);
    let peer_monitor_state = PeerMonitorState::new();
    let time_service = TimeService::real();

    // Store and monitor the metadata updater handle
    let mut metadata_updater_handle = spawn_peer_metadata_updater(
        node_config.peer_monitoring_service,
        peer_monitor_state.clone(),
        peer_monitoring_client.get_peers_and_metadata(),
        time_service.clone(),
        runtime.clone(),
    );

    // Spawn a task to monitor and restart the updater on panic
    let restart_config = node_config.peer_monitoring_service;
    let restart_state = peer_monitor_state.clone();
    let restart_peers = peer_monitoring_client.get_peers_and_metadata();
    let restart_time = time_service.clone();
    let restart_runtime = runtime.clone();
    
    tokio::spawn(async move {
        loop {
            match metadata_updater_handle.await {
                Ok(_) => {
                    warn!("Metadata updater exited unexpectedly, restarting...");
                },
                Err(e) if e.is_panic() => {
                    error!("Metadata updater panicked: {:?}, restarting...", e);
                },
                Err(e) => {
                    error!("Metadata updater error: {:?}, restarting...", e);
                }
            }
            
            // Restart the metadata updater
            metadata_updater_handle = spawn_peer_metadata_updater(
                restart_config,
                restart_state.clone(),
                restart_peers.clone(),
                restart_time.clone(),
                restart_runtime.clone(),
            );
        }
    });

    start_peer_monitor_with_state(
        node_config,
        peer_monitoring_client,
        peer_monitor_state,
        time_service,
        runtime,
    )
    .await
}
```

**Solution 2: Wrap Task in Panic Guard**

Modify `spawn_peer_metadata_updater` to catch and log panics internally:

```rust
pub(crate) fn spawn_peer_metadata_updater(...) -> JoinHandle<()> {
    let metadata_updater = async move {
        loop {
            let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
                // Existing loop body
            }));
            
            if let Err(panic_info) = result {
                error!("Metadata updater panicked: {:?}", panic_info);
                // Add backoff before retry
                tokio::time::sleep(Duration::from_secs(1)).await;
            }
        }
    };
    
    // Spawn with monitoring
    if let Some(runtime) = runtime {
        runtime.spawn(metadata_updater)
    } else {
        tokio::spawn(metadata_updater)
    }
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_metadata_updater_panic_handling() {
    use std::sync::Arc;
    use aptos_infallible::RwLock;
    use std::collections::HashMap;
    
    // Create a shared state that will be poisoned
    let shared_state = Arc::new(RwLock::new(HashMap::<u64, u64>::new()));
    let state_clone = shared_state.clone();
    
    // Spawn a task that panics while holding write lock
    let poisoning_task = tokio::spawn(async move {
        let mut guard = state_clone.write();
        guard.insert(1, 1);
        panic!("Intentional panic while holding write lock");
    });
    
    // Wait for poisoning
    let _ = poisoning_task.await;
    
    // Now simulate the metadata updater trying to read
    let reader_task = tokio::spawn(async move {
        // This will panic due to poisoned lock
        let _guard = shared_state.read();
    });
    
    // Verify the reader task panics
    let result = reader_task.await;
    assert!(result.is_err(), "Expected panic from poisoned lock");
    assert!(result.unwrap_err().is_panic(), "Expected panic error");
    
    // Demonstrate that without monitoring, this failure is silent
    // and the metadata updater would stop permanently
}
```

## Notes

This vulnerability demonstrates a critical gap in error handling for long-running background tasks. The combination of `aptos_infallible::RwLock`'s panic-on-poison behavior and the lack of task monitoring creates a silent failure mode that degrades node performance without detection. The fix requires either robust JoinHandle monitoring with restart logic or internal panic recovery within the task itself.

### Citations

**File:** peer-monitoring-service/client/src/lib.rs (L59-87)
```rust
pub async fn start_peer_monitor(
    node_config: NodeConfig,
    network_client: NetworkClient<PeerMonitoringServiceMessage>,
    runtime: Option<Handle>,
) {
    // Create a new monitoring client and peer monitor state
    let peer_monitoring_client = PeerMonitoringServiceClient::new(network_client);
    let peer_monitor_state = PeerMonitorState::new();

    // Spawn the peer metadata updater
    let time_service = TimeService::real();
    spawn_peer_metadata_updater(
        node_config.peer_monitoring_service,
        peer_monitor_state.clone(),
        peer_monitoring_client.get_peers_and_metadata(),
        time_service.clone(),
        runtime.clone(),
    );

    // Start the peer monitor
    start_peer_monitor_with_state(
        node_config,
        peer_monitoring_client,
        peer_monitor_state,
        time_service,
        runtime,
    )
    .await
}
```

**File:** peer-monitoring-service/client/src/lib.rs (L159-178)
```rust
/// Creates a new peer state for peers that don't yet have one
fn create_states_for_new_peers(
    node_config: &NodeConfig,
    peer_monitor_state: &PeerMonitorState,
    time_service: &TimeService,
    connected_peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) {
    for peer_network_id in connected_peers_and_metadata.keys() {
        let state_exists = peer_monitor_state
            .peer_states
            .read()
            .contains_key(peer_network_id);
        if !state_exists {
            peer_monitor_state.peer_states.write().insert(
                *peer_network_id,
                PeerState::new(node_config.clone(), time_service.clone()),
            );
        }
    }
}
```

**File:** peer-monitoring-service/client/src/lib.rs (L214-262)
```rust
    let metadata_updater = async move {
        // Create an interval ticker for the updater loop
        let metadata_update_loop_duration =
            Duration::from_millis(peer_monitoring_config.metadata_update_interval_ms);
        let metadata_update_loop_ticker = time_service.interval(metadata_update_loop_duration);
        futures::pin_mut!(metadata_update_loop_ticker);

        // Start the updater loop
        info!(LogSchema::new(LogEntry::MetadataUpdateLoop)
            .event(LogEvent::StartedMetadataUpdaterLoop)
            .message("Starting the peers and metadata updater!"));
        loop {
            // Wait for the next round before updating peers and metadata
            metadata_update_loop_ticker.next().await;

            // Get all peers
            let all_peers = peers_and_metadata.get_all_peers();

            // Update the latest peer monitoring metadata
            for peer_network_id in all_peers {
                let peer_monitoring_metadata =
                    match peer_monitor_state.peer_states.read().get(&peer_network_id) {
                        Some(peer_state) => {
                            peer_state
                                .extract_peer_monitoring_metadata()
                                .unwrap_or_else(|error| {
                                    // Log the error and return the default
                                    warn!(LogSchema::new(LogEntry::MetadataUpdateLoop)
                                        .event(LogEvent::UnexpectedErrorEncountered)
                                        .peer(&peer_network_id)
                                        .error(&error));
                                    PeerMonitoringMetadata::default()
                                })
                        },
                        None => PeerMonitoringMetadata::default(), // Use the default
                    };

                // Insert the latest peer monitoring metadata into peers and metadata
                if let Err(error) = peers_and_metadata
                    .update_peer_monitoring_metadata(peer_network_id, peer_monitoring_metadata)
                {
                    warn!(LogSchema::new(LogEntry::MetadataUpdateLoop)
                        .event(LogEvent::UnexpectedErrorEncountered)
                        .peer(&peer_network_id)
                        .error(&error.into()));
                }
            }
        }
    };
```

**File:** crates/aptos-infallible/src/rwlock.rs (L18-30)
```rust
    /// lock the rwlock in read mode
    pub fn read(&self) -> RwLockReadGuard<'_, T> {
        self.0
            .read()
            .expect("Cannot currently handle a poisoned lock")
    }

    /// lock the rwlock in write mode
    pub fn write(&self) -> RwLockWriteGuard<'_, T> {
        self.0
            .write()
            .expect("Cannot currently handle a poisoned lock")
    }
```

**File:** state-sync/aptos-data-client/src/utils.rs (L1-1)
```rust
// Copyright (c) Aptos Foundation
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L1-1)
```rust
// Copyright (c) Aptos Foundation
```
