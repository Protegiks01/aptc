# Audit Report

## Title
Resource Leak Vulnerability in Indexer-gRPC Cache Worker During Panic-Induced Process Termination

## Summary
The indexer-grpc-cache-worker's panic handler immediately terminates the process via `process::exit(12)` without executing Drop destructors, causing Redis connections, gRPC channels, and file store handles to leak. Repeated panics lead to resource accumulation on backend services (Redis server, fullnode gRPC server), potentially exhausting connection limits and causing service degradation.

## Finding Description

The indexer-grpc-cache-worker establishes critical connections to external services including Redis for caching, fullnode gRPC for transaction streaming, and file storage backends. The worker can panic in multiple scenarios: [1](#0-0) [2](#0-1) 

When any panic occurs, the global panic handler installed by the server framework immediately calls `process::exit(12)`: [3](#0-2) 

This abrupt termination prevents Rust's Drop trait from executing cleanup logic for:

1. **Redis ConnectionManager** - Created in the run loop, holds active TCP connections to Redis server: [4](#0-3) 

2. **gRPC Client** - Maintains HTTP/2 connection to fullnode: [5](#0-4) 

3. **FileStoreOperator** - Holds file/cloud storage handles: [6](#0-5) 

When the process exits without cleanup, these connections remain open from the server's perspective until TCP keepalive timeouts expire (typically 2+ hours). If the cache worker is restarted by an orchestration system (Kubernetes, systemd) and panics again, new connections accumulate while old connections haven't yet timed out.

**Attack Vector**: An attacker who can trigger conditions causing chain_id mismatches or transaction processing failures can induce repeated panics, causing resource exhaustion on Redis and fullnode servers.

## Impact Explanation

This vulnerability constitutes **Medium severity** per Aptos bug bounty criteria because:

1. **Resource Exhaustion**: Repeated panics cause accumulation of leaked connections, eventually exhausting:
   - Redis server's `maxclients` limit (default 10,000)
   - Fullnode's gRPC connection limits
   - System file descriptor limits
   - Network socket buffer memory

2. **Service Degradation**: When connection limits are reached:
   - New cache worker instances cannot connect
   - Other indexer services sharing Redis/fullnode are affected
   - Indexer infrastructure becomes unavailable

3. **State Inconsistencies Requiring Intervention**: The indexer system requires manual intervention to:
   - Kill stuck connections on Redis/fullnode
   - Restart affected services
   - Clear accumulated state

This does not reach Critical severity because it:
- Does not affect blockchain consensus
- Does not cause loss of funds
- Does not compromise validator nodes
- Requires external orchestration to repeatedly restart the process

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability is likely to manifest because:

1. **Explicit Panic Conditions Exist**: The code contains deliberate panic statements that can be triggered by:
   - Malformed or inconsistent data from fullnode
   - Redis connection/operation failures cascading to task errors
   - Network instability causing protocol violations

2. **Production Environment Factors**:
   - Container orchestration (Kubernetes) automatically restarts crashed pods
   - Network issues can cause transient failures triggering panics
   - Data inconsistencies between services can trigger chain_id mismatches

3. **No Rate Limiting**: The restart mechanism has no exponential backoff or panic rate limiting, allowing rapid accumulation if a persistent issue exists

However, likelihood is not High because:
- Requires actual error conditions (not normal operation)
- Most errors use `break` statements returning to the reconnection loop rather than panicking
- Production deployments typically have monitoring detecting crash loops

## Recommendation

Implement graceful shutdown with resource cleanup before process termination:

**Option 1: Replace panic! with graceful error handling**
```rust
// In worker.rs line 383 and 429, replace panic! with:
return Err(anyhow::anyhow!("Fatal error: {}", error_message));
```

**Option 2: Implement Drop handlers for cleanup**
```rust
impl Drop for Worker {
    fn drop(&mut self) {
        // Redis ConnectionManager automatically handles cleanup via its Drop
        // But we should log the cleanup
        tracing::warn!("Worker dropping, connections will be cleaned up");
    }
}
```

**Option 3: Catch panics at framework level**
```rust
// In lib.rs, modify handle_panic to attempt graceful cleanup:
fn handle_panic(panic_info: &PanicHookInfo<'_>) {
    let details = format!("{}", panic_info);
    let backtrace = format!("{:#?}", Backtrace::new());
    let info = CrashInfo { details, backtrace };
    let crash_info = toml::to_string_pretty(&info).unwrap();
    error!("{}", crash_info);
    eprintln!("{}", crash_info);
    
    // Give Drop handlers a chance to run by not calling process::exit
    // Instead, set a flag to terminate gracefully
    // Note: This requires refactoring to check termination flag in main loop
    std::process::abort(); // Or implement graceful termination
}
```

**Recommended Solution**: Remove panic statements and use Result-based error handling throughout the streaming pipeline, allowing the outer reconnection loop to handle failures gracefully.

## Proof of Concept

```rust
// Reproduction test demonstrating resource leak
#[tokio::test]
async fn test_panic_resource_leak() {
    use redis::Client;
    use std::sync::Arc;
    use std::sync::atomic::{AtomicUsize, Ordering};
    
    // Setup: Monitor Redis connection count
    let redis_url = "redis://127.0.0.1:6379";
    let connection_counter = Arc::new(AtomicUsize::new(0));
    
    // Simulate multiple panic-restart cycles
    for iteration in 0..10 {
        let counter = connection_counter.clone();
        
        // Spawn worker-like task that will panic
        let handle = tokio::spawn(async move {
            // Create Redis connection (not cleaned up on panic)
            let client = Client::open(redis_url).unwrap();
            let mut conn = client.get_tokio_connection_manager().await.unwrap();
            counter.fetch_add(1, Ordering::SeqCst);
            
            // Simulate cache worker operation then panic
            tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
            panic!("Simulating chain_id mismatch panic");
        });
        
        // Ignore panic result (simulates process restart)
        let _ = handle.await;
        
        // In production, orchestration would restart the process here
        // Connections accumulate because Drop isn't called
    }
    
    // Verify: Check Redis server connection count
    // Run: redis-cli CLIENT LIST | wc -l
    // Expected: 10+ connections remain open despite panics
    
    println!("Created {} connections that leaked on panic", 
             connection_counter.load(Ordering::SeqCst));
    
    // Wait to observe connections haven't closed
    tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
    
    // Connections remain until TCP keepalive timeout (hours later)
}
```

To manually verify:
1. Deploy cache worker with instrumented panic triggering
2. Monitor: `redis-cli CLIENT LIST | wc -l` before and during crashes
3. Observe: Connection count grows with each panic/restart cycle
4. Monitor fullnode: `netstat -an | grep :50051 | wc -l` for gRPC connections
5. Eventually: New connections fail with "max clients reached" error

## Notes

The vulnerability breaks the **Resource Limits** invariant (#9): "All operations must respect gas, storage, and computational limits." While not directly about blockchain resources, the indexer infrastructure's resource exhaustion impacts the availability of blockchain data access services.

The issue is specific to the panic handling strategy in `setup_panic_handler()` which prioritizes fail-fast behavior over graceful cleanup. This design choice is reasonable for preventing silent failures but creates resource leak vulnerabilities when panics occur frequently.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L112-116)
```rust
            let conn = self
                .redis_client
                .get_tokio_connection_manager()
                .await
                .context("Get redis connection failed.")?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L117-117)
```rust
            let mut rpc_client = create_grpc_client(self.fullnode_grpc_address.clone()).await;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L120-120)
```rust
            let file_store_operator: Box<dyn FileStoreOperator> = self.file_store.create();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L382-384)
```rust
        if received.chain_id as u64 != fullnode_chain_id as u64 {
            panic!("[Indexer Cache] Chain id mismatch happens during data streaming.");
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L418-430)
```rust
                    let result = join_all(tasks_to_run).await;
                    if result
                        .iter()
                        .any(|r| r.is_err() || r.as_ref().unwrap().is_err())
                    {
                        error!(
                            start_version = start_version,
                            num_of_transactions = num_of_transactions,
                            "[Indexer Cache] Process transactions from fullnode failed."
                        );
                        ERROR_COUNT.with_label_values(&["response_error"]).inc();
                        panic!("Error happens when processing transactions from fullnode.");
                    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L149-168)
```rust
pub fn setup_panic_handler() {
    std::panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}

// Formats and logs panic information
fn handle_panic(panic_info: &PanicHookInfo<'_>) {
    // The Display formatter for a PanicHookInfo contains the message, payload and location.
    let details = format!("{}", panic_info);
    let backtrace = format!("{:#?}", Backtrace::new());
    let info = CrashInfo { details, backtrace };
    let crash_info = toml::to_string_pretty(&info).unwrap();
    error!("{}", crash_info);
    // TODO / HACK ALARM: Write crash info synchronously via eprintln! to ensure it is written before the process exits which error! doesn't guarantee.
    // This is a workaround until https://github.com/aptos-labs/aptos-core/issues/2038 is resolved.
    eprintln!("{}", crash_info);
    // Kill the process
    process::exit(12);
}
```
