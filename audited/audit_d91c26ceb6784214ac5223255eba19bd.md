# Audit Report

## Title
Priority Inversion in BoundedExecutor Allows Low-Priority Verification Tasks to Block Critical Consensus Operations

## Summary
The consensus system uses a single shared `BoundedExecutor` with FIFO permit allocation and no priority mechanism. Low-priority message verification tasks can saturate all executor permits, blocking high-priority consensus operations like commit vote aggregation, causing validator slowdowns and consensus delays.

## Finding Description

The Aptos consensus implementation uses a single `BoundedExecutor` instance shared across multiple critical and non-critical components. The executor enforces a capacity limit (default 16 concurrent tasks) using a simple FIFO semaphore with no priority mechanism. [1](#0-0) 

The `spawn()` method blocks when at capacity, waiting for a permit: [2](#0-1) 

This single executor is shared across:

1. **RandManager verification tasks** - verifies incoming randomness generation RPC messages: [3](#0-2) 

2. **SecretShareManager verification tasks** - verifies incoming secret sharing RPC messages: [4](#0-3) 

3. **BufferManager commit message verification** - verifies incoming commit messages: [5](#0-4) 

4. **ReliableBroadcast aggregation tasks** - aggregates responses for consensus operations: [6](#0-5) 

The executor is initialized once and shared: [7](#0-6) 

**Attack Scenario:**

A malicious validator or coordinated group of validators can flood the node with RandGen or SecretShare RPC messages. Each message triggers a verification task spawn on the bounded executor. When all 16 permits are consumed by verification tasks, critical consensus operations like commit vote aggregation block indefinitely waiting for permits.

The commit vote broadcast uses the same executor: [8](#0-7) 

The ReliableBroadcast's aggregation spawns tasks that block if executor is at capacity: [9](#0-8) 

## Impact Explanation

This vulnerability causes **validator node slowdowns** (High Severity per Aptos Bug Bounty - up to $50,000).

**Specific impacts:**
- **Consensus delays**: Commit vote aggregation is delayed, slowing block finalization
- **Validator performance degradation**: The node appears slow or unresponsive 
- **Liveness concerns**: Prolonged saturation could cause the validator to fall behind
- **Network-wide impact**: If multiple validators are affected, consensus rounds take longer

The default executor capacity is only 16 tasks: [10](#0-9) 

## Likelihood Explanation

**Likelihood: Medium-High**

**Attack requirements:**
- Attacker must be able to send authenticated consensus RPC messages (requires validator credentials OR network access to flood messages)
- Network layer allows 100 concurrent inbound RPCs, sufficient to saturate the 16-task executor
- Per-key channels buffer 10 messages per validator with KLAST dropping policy: [11](#0-10) 

**Exploitation complexity:**
- Moderate - requires sustained message flooding
- Multiple validators can coordinate to amplify the attack
- Legitimate high load scenarios (epoch transitions, network issues) could trigger this naturally

## Recommendation

**Implement priority-based task scheduling in the BoundedExecutor:**

1. Add priority levels to the executor:
```rust
pub enum TaskPriority {
    Critical,   // Commit vote aggregation, block finalization
    High,       // Consensus core operations
    Normal,     // Message verification, background tasks
}
```

2. Replace the simple semaphore with a priority queue that preferentially allocates permits to higher-priority tasks.

3. Classify tasks at spawn time:
   - **Critical**: Commit vote aggregation in ReliableBroadcast
   - **Normal**: RandGen/SecretShare verification tasks

4. Consider separate executors for different task classes to provide strong isolation.

**Alternative mitigation:**
Use `try_spawn()` with fallback for non-critical verification tasks to prevent blocking: [12](#0-11) 

## Proof of Concept

```rust
// Rust test demonstrating the priority inversion
#[tokio::test]
async fn test_bounded_executor_priority_inversion() {
    let runtime = tokio::runtime::Runtime::new().unwrap();
    let executor = BoundedExecutor::new(2, runtime.handle().clone()); // Small capacity
    
    // Spawn 2 low-priority verification tasks that block
    let (tx1, rx1) = tokio::sync::oneshot::channel();
    let (tx2, rx2) = tokio::sync::oneshot::channel();
    
    let _handle1 = executor.spawn(async move {
        rx1.await.unwrap();
        "low priority 1"
    }).await;
    
    let _handle2 = executor.spawn(async move {
        rx2.await.unwrap();
        "low priority 2"
    }).await;
    
    // Now try to spawn critical consensus operation
    // This will block because all permits are consumed
    let start = std::time::Instant::now();
    
    let critical_task = tokio::spawn(async move {
        executor.spawn(async {
            "critical consensus operation"
        }).await
    });
    
    // Wait a bit to demonstrate blocking
    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
    
    // Critical task is still blocked
    assert!(!critical_task.is_finished());
    
    // Release one low-priority task
    tx1.send(()).unwrap();
    
    // Now critical task can proceed
    let result = critical_task.await.unwrap();
    assert_eq!(result.await.unwrap(), "critical consensus operation");
    
    println!("Critical task blocked for {:?}", start.elapsed());
}
```

## Notes

The vulnerability stems from a fundamental design choice: all consensus operations share a single bounded executor with no priority mechanism. While network-layer rate limiting and per-key queuing provide some protection, they are insufficient to prevent a determined attacker with validator credentials from causing validator slowdowns through executor saturation.

The FIFO permit allocation in the semaphore means arrival order determines execution, not task importance. This violates the principle that critical consensus operations should have guaranteed resources for timely completion.

### Citations

**File:** crates/bounded-executor/src/executor.rs (L16-35)
```rust
#[derive(Clone, Debug)]
pub struct BoundedExecutor {
    semaphore: Arc<Semaphore>,
    executor: Handle,
}

impl BoundedExecutor {
    /// Create a new `BoundedExecutor` from an existing tokio [`Handle`]
    /// with a maximum concurrent task capacity of `capacity`.
    pub fn new(capacity: usize, executor: Handle) -> Self {
        let semaphore = Arc::new(Semaphore::new(capacity));
        Self {
            semaphore,
            executor,
        }
    }

    async fn acquire_permit(&self) -> OwnedSemaphorePermit {
        self.semaphore.clone().acquire_owned().await.unwrap()
    }
```

**File:** crates/bounded-executor/src/executor.rs (L45-52)
```rust
    pub async fn spawn<F>(&self, future: F) -> JoinHandle<F::Output>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        let permit = self.acquire_permit().await;
        self.executor.spawn(future_with_permit(future, permit))
    }
```

**File:** crates/bounded-executor/src/executor.rs (L54-68)
```rust
    /// Try to spawn a [`Future`] on the `BoundedExecutor`. If the `BoundedExecutor`
    /// is at capacity, this will return an `Err(F)`, passing back the future the
    /// caller attempted to spawn. Otherwise, this will spawn the future on the
    /// executor and send back a [`JoinHandle`] that the caller can `.await` on
    /// for the results of the [`Future`].
    pub fn try_spawn<F>(&self, future: F) -> Result<JoinHandle<F::Output>, F>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        match self.try_acquire_permit() {
            Some(permit) => Ok(self.executor.spawn(future_with_permit(future, permit))),
            None => Err(future),
        }
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L221-261)
```rust
    async fn verification_task(
        epoch_state: Arc<EpochState>,
        mut incoming_rpc_request: aptos_channel::Receiver<Author, IncomingRandGenRequest>,
        verified_msg_tx: UnboundedSender<RpcRequest<S, D>>,
        rand_config: RandConfig,
        fast_rand_config: Option<RandConfig>,
        bounded_executor: BoundedExecutor,
    ) {
        while let Some(rand_gen_msg) = incoming_rpc_request.next().await {
            let tx = verified_msg_tx.clone();
            let epoch_state_clone = epoch_state.clone();
            let config_clone = rand_config.clone();
            let fast_config_clone = fast_rand_config.clone();
            bounded_executor
                .spawn(async move {
                    match bcs::from_bytes::<RandMessage<S, D>>(rand_gen_msg.req.data()) {
                        Ok(msg) => {
                            if msg
                                .verify(
                                    &epoch_state_clone,
                                    &config_clone,
                                    &fast_config_clone,
                                    rand_gen_msg.sender,
                                )
                                .is_ok()
                            {
                                let _ = tx.unbounded_send(RpcRequest {
                                    req: msg,
                                    protocol: rand_gen_msg.protocol,
                                    response_sender: rand_gen_msg.response_sender,
                                });
                            }
                        },
                        Err(e) => {
                            warn!("Invalid rand gen message: {}", e);
                        },
                    }
                })
                .await;
        }
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L205-235)
```rust
    async fn verification_task(
        epoch_state: Arc<EpochState>,
        mut incoming_rpc_request: aptos_channel::Receiver<Author, IncomingSecretShareRequest>,
        verified_msg_tx: UnboundedSender<SecretShareRpc>,
        config: SecretShareConfig,
        bounded_executor: BoundedExecutor,
    ) {
        while let Some(dec_msg) = incoming_rpc_request.next().await {
            let tx = verified_msg_tx.clone();
            let epoch_state_clone = epoch_state.clone();
            let config_clone = config.clone();
            bounded_executor
                .spawn(async move {
                    match bcs::from_bytes::<SecretShareMessage>(dec_msg.req.data()) {
                        Ok(msg) => {
                            if msg.verify(&epoch_state_clone, &config_clone).is_ok() {
                                let _ = tx.unbounded_send(SecretShareRpc {
                                    msg,
                                    protocol: dec_msg.protocol,
                                    response_sender: dec_msg.response_sender,
                                });
                            }
                        },
                        Err(e) => {
                            warn!("Invalid dec message: {}", e);
                        },
                    }
                })
                .await;
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L720-726)
```rust
                let signed_item_mut = signed_item.unwrap_signed_mut();
                let commit_vote = signed_item_mut.commit_vote.clone();
                let commit_vote = Self::generate_commit_message(commit_vote);
                signed_item_mut.rb_handle = self
                    .do_reliable_broadcast(commit_vote)
                    .map(|handle| (Instant::now(), handle));
                self.buffer.set(&current_cursor, signed_item);
```

**File:** consensus/src/pipeline/buffer_manager.rs (L918-930)
```rust
        let bounded_executor = self.bounded_executor.clone();
        spawn_named!("buffer manager verification", async move {
            while let Some((sender, commit_msg)) = commit_msg_rx.next().await {
                let tx = verified_commit_msg_tx.clone();
                let epoch_state_clone = epoch_state.clone();
                bounded_executor
                    .spawn(async move {
                        match commit_msg.req.verify(sender, &epoch_state_clone.verifier) {
                            Ok(_) => {
                                let _ = tx.unbounded_send(commit_msg);
                            },
                            Err(e) => warn!("Invalid commit message: {}", e),
                        }
```

**File:** crates/reliable-broadcast/src/lib.rs (L167-181)
```rust
            loop {
                tokio::select! {
                    Some((receiver, result)) = rpc_futures.next() => {
                        let aggregating = aggregating.clone();
                        let future = executor.spawn(async move {
                            (
                                    receiver,
                                    result
                                        .and_then(|msg| {
                                            msg.try_into().map_err(|e| anyhow::anyhow!("{:?}", e))
                                        })
                                        .and_then(|ack| aggregating.add(receiver, ack)),
                            )
                        }).await;
                        aggregate_futures.push(future);
```

**File:** consensus/src/consensus_provider.rs (L81-97)
```rust
    let bounded_executor = BoundedExecutor::new(
        node_config.consensus.num_bounded_executor_tasks as usize,
        runtime.handle().clone(),
    );
    let rand_storage = Arc::new(RandDb::new(node_config.storage.dir()));

    let execution_client = Arc::new(ExecutionProxyClient::new(
        node_config.consensus.clone(),
        Arc::new(execution_proxy),
        node_config.validator_network.as_ref().unwrap().peer_id(),
        self_sender.clone(),
        consensus_network_client.clone(),
        bounded_executor.clone(),
        rand_storage.clone(),
        node_config.consensus_observer,
        consensus_publisher.clone(),
    ));
```

**File:** config/src/config/consensus_config.rs (L242-242)
```rust
            internal_per_key_channel_size: 10,
```

**File:** config/src/config/consensus_config.rs (L379-379)
```rust
            num_bounded_executor_tasks: 16,
```
