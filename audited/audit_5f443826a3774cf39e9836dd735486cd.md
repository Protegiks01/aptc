# Audit Report

## Title
Unbounded Task Spawning in Storage Service Enables CPU and Memory Amplification Attack

## Summary
The storage service spawns an unbounded number of blocking tasks to handle incoming requests, with no per-peer or global rate limiting at the application layer. An attacker can send many small storage service requests (40-50 bytes each) that trigger expensive processing operations (database reads, serialization, compression of up to 40 MiB responses), causing sustained high CPU and memory consumption that degrades validator node performance.

## Finding Description
The storage service's main request handling loop spawns a new blocking task for every incoming network request without any concurrency limit: [1](#0-0) 

Each spawned task performs resource-intensive operations including:
1. Database reads of up to `max_chunk_size` items (e.g., 4000 state values or 3000 transactions)
2. Serialization of potentially large responses (up to 40 MiB for v2 requests)
3. Optional LZ4 compression of the entire response [2](#0-1) 

The request types that can trigger this amplification include:
- `StateValuesWithProofRequest`: Fetches state values with proof
- `GetTransactionDataWithProofRequest`: Fetches transactions with auxiliary data
- `GetTransactionOutputsWithProofRequest`: Fetches transaction outputs
- `EpochEndingLedgerInfoRequest`: Fetches epoch ending ledger infos [3](#0-2) 

While individual request and response sizes are capped, the system allows unlimited concurrent request processing: [4](#0-3) 

The only application-level protection is the request moderator, which only blocks peers after they send too many **invalid** requests (default: 500 invalid requests): [5](#0-4) 

This moderator does not limit valid requests, regardless of their resource consumption.

**Attack Path:**
1. Attacker establishes connections to the storage service (within HAProxy's 500 connection limit)
2. Sends many `StateValuesWithProofRequest` or `GetTransactionDataWithProofRequest` with varying parameters to bypass LRU cache
3. Each ~50 byte request triggers `spawn_blocking()`, creating a task that:
   - Reads up to 4000 items from the database
   - Serializes up to 40 MiB of data
   - Compresses the response if requested
4. Tokio's blocking thread pool (default 512 threads) becomes saturated
5. Tasks queue up in Tokio's unbounded task queue
6. Each queued task holds memory (closures, Arc references, cloned config)
7. Processing backlog causes sustained high CPU, memory, and I/O usage
8. Node performance degrades significantly, affecting state sync and potentially consensus participation

**Amplification Factor:**
- Request size: ~40-50 bytes
- Processing cost per request: 
  - Database I/O: Reading 4000 state values or 3000 transactions
  - CPU: Serialization + compression of up to 40 MiB
  - Memory: 40-100 MiB during processing per task
- Amplification: ~1,000,000x to 2,000,000x resource consumption

## Impact Explanation
This vulnerability enables an **amplification attack** that degrades validator node performance, qualifying as **High Severity** under the Aptos bug bounty program: "Validator node slowdowns."

The attack can cause:
- **CPU exhaustion**: Continuous serialization and compression operations
- **Memory pressure**: Hundreds of concurrent tasks, each holding 40-100 MiB during processing
- **Database I/O saturation**: Hundreds of concurrent read operations
- **Degraded state synchronization**: Legitimate peers cannot sync efficiently
- **Indirect consensus impact**: Validators may miss rounds due to slowdowns

While this does not directly break consensus safety, sustained performance degradation can prevent validators from participating effectively in consensus, potentially affecting network liveness if multiple validators are targeted simultaneously.

## Likelihood Explanation
This attack is **highly likely** to occur because:

1. **Low barrier to entry**: Any network peer can send storage service requests
2. **No authentication required**: Public fullnodes accept connections from arbitrary peers
3. **Simple to execute**: Standard storage service client code can be used
4. **Difficult to detect**: Requests appear valid and use legitimate protocol messages
5. **No automatic mitigation**: Only blocks invalid requests, not expensive valid ones

The attack is constrained by:
- Network bandwidth limits (50 MB/s per IP via HAProxy)
- Connection limits (500 connections, 300 connections/second rate)
- Geographic distribution of attackers (if rate limiting is per-IP)

However, these network-level protections are insufficient because:
- Requests are tiny (~50 bytes), so bandwidth limits don't prevent flooding
- The application layer spawns tasks immediately without checking concurrency
- Multiple attacking IPs can coordinate to amplify the effect

## Recommendation

Implement bounded concurrency controls at the application layer:

**Option 1: Use a Bounded Executor** (Recommended)
Replace direct `spawn_blocking` calls with a bounded executor that limits concurrent storage service tasks:

```rust
// In StorageServiceServer
pub struct StorageServiceServer<T> {
    // ... existing fields ...
    
    // Add bounded executor
    bounded_executor: Arc<BoundedExecutor>,
}

// In the start() method, replace spawn_blocking with bounded submission:
self.bounded_executor.spawn_blocking(move || {
    Handler::new(...)
        .process_request_and_respond(...);
});
```

**Option 2: Per-Peer Rate Limiting**
Add request rate limiting per peer in the request moderator:

```rust
// In RequestModerator
pub struct RequestModerator {
    // ... existing fields ...
    
    // Track requests per peer per time window
    peer_request_counts: Arc<DashMap<PeerNetworkId, RateLimitState>>,
    max_requests_per_peer_per_second: u64,
}
```

**Option 3: Global Concurrency Limit**
Add a semaphore to limit total concurrent storage service tasks:

```rust
// In StorageServiceServer
pub struct StorageServiceServer<T> {
    // ... existing fields ...
    
    // Limit concurrent requests
    request_semaphore: Arc<Semaphore>,
}

// In start() method:
let permit = self.request_semaphore.acquire().await;
self.runtime.spawn_blocking(move || {
    let _permit = permit; // Hold permit for task duration
    Handler::new(...)
        .process_request_and_respond(...);
});
```

**Recommended Configuration:**
- Max concurrent storage service tasks: 256 (per node)
- Max requests per peer per second: 10-20
- Max pending requests per peer: 3-5

## Proof of Concept

```rust
// Proof of concept showing the amplification attack
// This would be run as a separate attacking client

use aptos_storage_service_types::requests::{
    DataRequest, StateValuesWithProofRequest, StorageServiceRequest
};
use std::time::{Duration, Instant};
use tokio::time::sleep;

#[tokio::main]
async fn main() {
    println!("Storage Service Amplification Attack PoC");
    
    // Connect to target storage service endpoint
    let mut clients = vec![];
    for i in 0..100 {
        // Establish multiple connections
        let client = connect_to_storage_service("target_node:6180").await;
        clients.push(client);
    }
    
    println!("Established {} connections", clients.len());
    
    let start_time = Instant::now();
    let mut total_requests = 0;
    
    // Send many small requests that trigger large processing
    for round in 0..100 {
        for (i, client) in clients.iter_mut().enumerate() {
            // Each request is ~50 bytes but triggers processing of 4000 state values
            let request = StorageServiceRequest::new(
                DataRequest::GetStateValuesWithProof(
                    StateValuesWithProofRequest {
                        version: 1000000 + (round * 1000) + i,  // Vary to bypass cache
                        start_index: 0,
                        end_index: 3999,  // Request 4000 state values
                    }
                ),
                true,  // Request compression for extra CPU load
            );
            
            // Send without waiting for response
            client.send_request_async(request).await;
            total_requests += 1;
        }
        
        // Small delay to avoid overwhelming local network stack
        sleep(Duration::from_millis(10)).await;
    }
    
    let elapsed = start_time.elapsed();
    println!("Sent {} requests in {:?}", total_requests, elapsed);
    println!("Request rate: {} req/sec", total_requests as f64 / elapsed.as_secs_f64());
    println!("Average request size: ~50 bytes");
    println!("Expected processing per request: ~200 MiB read + serialize + compress");
    println!("Total amplification: ~{}x", (total_requests * 200_000_000) / (total_requests * 50));
    
    // Monitor target node's CPU and memory usage
    println!("Monitor target node for high CPU and memory usage...");
    sleep(Duration::from_secs(60)).await;
}
```

To demonstrate on a test network:
1. Deploy a test Aptos node with storage service enabled
2. Run this PoC client against the node's storage service endpoint
3. Observe node metrics showing:
   - CPU usage spike to 100% across cores
   - Memory usage increase from task queueing
   - Storage service request processing latency increase
   - Potential impact on consensus participation

## Notes

This vulnerability represents a **classic amplification attack** where minimal attacker resources (small requests) trigger disproportionate defender costs (large database reads, CPU-intensive compression). While network-level protections (bandwidth limits, connection limits) provide some defense, they are insufficient because:

1. The application layer spawns tasks immediately without checking system load
2. Request sizes are tiny relative to bandwidth limits
3. Tokio's blocking task queue is unbounded
4. No per-peer rate limiting for valid requests

The fix requires adding bounded concurrency controls at the application layer to complement network-level protections.

### Citations

**File:** state-sync/storage-service/server/src/lib.rs (L389-419)
```rust
        while let Some(network_request) = self.network_requests.next().await {
            // All handler methods are currently CPU-bound and synchronous
            // I/O-bound, so we want to spawn on the blocking thread pool to
            // avoid starving other async tasks on the same runtime.
            let storage = self.storage.clone();
            let config = self.storage_service_config;
            let cached_storage_server_summary = self.cached_storage_server_summary.clone();
            let optimistic_fetches = self.optimistic_fetches.clone();
            let subscriptions = self.subscriptions.clone();
            let lru_response_cache = self.lru_response_cache.clone();
            let request_moderator = self.request_moderator.clone();
            let time_service = self.time_service.clone();
            self.runtime.spawn_blocking(move || {
                Handler::new(
                    cached_storage_server_summary,
                    optimistic_fetches,
                    lru_response_cache,
                    request_moderator,
                    storage,
                    subscriptions,
                    time_service,
                )
                .process_request_and_respond(
                    config,
                    network_request.peer_network_id,
                    network_request.protocol_id,
                    network_request.storage_service_request,
                    network_request.response_sender,
                );
            });
        }
```

**File:** state-sync/storage-service/server/src/storage.rs (L900-987)
```rust
    fn get_state_value_chunk_with_proof_by_size(
        &self,
        version: u64,
        start_index: u64,
        end_index: u64,
        max_response_size: u64,
        use_size_and_time_aware_chunking: bool,
    ) -> Result<StateValueChunkWithProof, Error> {
        // Calculate the number of state values to fetch
        let expected_num_state_values = inclusive_range_len(start_index, end_index)?;
        let max_num_state_values = self.config.max_state_chunk_size;
        let num_state_values_to_fetch = min(expected_num_state_values, max_num_state_values);

        // If size and time-aware chunking are disabled, use the legacy implementation
        if !use_size_and_time_aware_chunking {
            return self.get_state_value_chunk_with_proof_by_size_legacy(
                version,
                start_index,
                end_index,
                num_state_values_to_fetch,
                max_response_size,
            );
        }

        // Get the state value chunk iterator
        let mut state_value_iterator = self.storage.get_state_value_chunk_iter(
            version,
            start_index as usize,
            num_state_values_to_fetch as usize,
        )?;

        // Initialize the fetched state values
        let mut state_values = vec![];

        // Create a response progress tracker
        let mut response_progress_tracker = ResponseDataProgressTracker::new(
            num_state_values_to_fetch,
            max_response_size,
            self.config.max_storage_read_wait_time_ms,
            self.time_service.clone(),
        );

        // Fetch as many state values as possible
        while !response_progress_tracker.is_response_complete() {
            match state_value_iterator.next() {
                Some(Ok(state_value)) => {
                    // Calculate the number of serialized bytes for the state value
                    let num_serialized_bytes = get_num_serialized_bytes(&state_value)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;

                    // Add the state value to the list
                    if response_progress_tracker
                        .data_items_fits_in_response(true, num_serialized_bytes)
                    {
                        state_values.push(state_value);
                        response_progress_tracker.add_data_item(num_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
                },
                Some(Err(error)) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
                None => {
                    // Log a warning that the iterator did not contain all the expected data
                    warn!(
                        "The state value iterator is missing data! Version: {:?}, \
                        start index: {:?}, end index: {:?}, num state values to fetch: {:?}",
                        version, start_index, end_index, num_state_values_to_fetch
                    );
                    break;
                },
            }
        }

        // Create the state value chunk with proof
        let state_value_chunk_with_proof = self.storage.get_state_value_chunk_proof(
            version,
            start_index as usize,
            state_values,
        )?;

        // Update the data truncation metrics
        response_progress_tracker
            .update_data_truncation_metrics(DataResponse::get_state_value_chunk_with_proof_label());

        Ok(state_value_chunk_with_proof)
    }
```

**File:** state-sync/storage-service/types/src/requests.rs (L34-56)
```rust
#[derive(Clone, Debug, Deserialize, Eq, Hash, PartialEq, Serialize)]
pub enum DataRequest {
    GetEpochEndingLedgerInfos(EpochEndingLedgerInfoRequest), // Fetches a list of epoch ending ledger infos
    GetNewTransactionOutputsWithProof(NewTransactionOutputsWithProofRequest), // Optimistically fetches new transaction outputs
    GetNewTransactionsWithProof(NewTransactionsWithProofRequest), // Optimistically fetches new transactions
    GetNumberOfStatesAtVersion(Version), // Fetches the number of states at the specified version
    GetServerProtocolVersion,            // Fetches the protocol version run by the server
    GetStateValuesWithProof(StateValuesWithProofRequest), // Fetches a list of states with a proof
    GetStorageServerSummary,             // Fetches a summary of the storage server state
    GetTransactionOutputsWithProof(TransactionOutputsWithProofRequest), // Fetches a list of transaction outputs with a proof
    GetTransactionsWithProof(TransactionsWithProofRequest), // Fetches a list of transactions with a proof
    GetNewTransactionsOrOutputsWithProof(NewTransactionsOrOutputsWithProofRequest), // Optimistically fetches new transactions or outputs
    GetTransactionsOrOutputsWithProof(TransactionsOrOutputsWithProofRequest), // Fetches a list of transactions or outputs with a proof
    SubscribeTransactionOutputsWithProof(SubscribeTransactionOutputsWithProofRequest), // Subscribes to transaction outputs with a proof
    SubscribeTransactionsOrOutputsWithProof(SubscribeTransactionsOrOutputsWithProofRequest), // Subscribes to transactions or outputs with a proof
    SubscribeTransactionsWithProof(SubscribeTransactionsWithProofRequest), // Subscribes to transactions with a proof

    // All the requests listed below are for transaction data v2 (i.e., transactions with auxiliary information).
    // TODO: eventually we should deprecate all the old request types.
    GetTransactionDataWithProof(GetTransactionDataWithProofRequest), // Fetches transaction data with a proof
    GetNewTransactionDataWithProof(GetNewTransactionDataWithProofRequest), // Optimistically fetches new transaction data with a proof
    SubscribeTransactionDataWithProof(SubscribeTransactionDataWithProofRequest), // Subscribes to transaction data with a proof
}
```

**File:** config/src/config/state_sync_config.rs (L19-27)
```rust
// The maximum message size per state sync message (for v2 data requests)
const CLIENT_MAX_MESSAGE_SIZE_V2: usize = 20 * 1024 * 1024; // 20 MiB (used for v2 data requests)
const SERVER_MAX_MESSAGE_SIZE_V2: usize = 40 * 1024 * 1024; // 40 MiB (used for v2 data requests)

// The maximum chunk sizes for data client requests and response
const MAX_EPOCH_CHUNK_SIZE: u64 = 200;
const MAX_STATE_CHUNK_SIZE: u64 = 4000;
const MAX_TRANSACTION_CHUNK_SIZE: u64 = 3000;
const MAX_TRANSACTION_OUTPUT_CHUNK_SIZE: u64 = 3000;
```

**File:** state-sync/storage-service/server/src/moderator.rs (L134-196)
```rust
    pub fn validate_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> Result<(), Error> {
        // Validate the request and time the operation
        let validate_request = || {
            // If the peer is being ignored, return an error
            if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
                if peer_state.is_ignored() {
                    return Err(Error::TooManyInvalidRequests(format!(
                        "Peer is temporarily ignored. Unable to handle request: {:?}",
                        request
                    )));
                }
            }

            // Get the latest storage server summary
            let storage_server_summary = self.cached_storage_server_summary.load();

            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }

            Ok(()) // The request is valid
        };
        utils::execute_and_time_duration(
            &metrics::STORAGE_REQUEST_VALIDATION_LATENCY,
            Some((peer_network_id, request)),
            None,
            validate_request,
            None,
        )
    }
```
