# Audit Report

## Title
TxnGuard Memory Leak in JWK Consensus Finished State Causing Transaction Pool Liveness Degradation

## Summary
The `ConsensusState::Finished` variant in the JWK consensus implementation can indefinitely hold a `TxnGuard` within a single epoch when observations stop flowing and on-chain state remains unchanged. This causes validator transactions to remain in the pool without being committed, leading to transaction pool bloat, memory leaks, and liveness degradation.

## Finding Description

The vulnerability exists in the state management logic of `IssuerLevelConsensusManager` (and `KeyLevelConsensusManager`). When JWK consensus completes, it transitions to `ConsensusState::Finished` and obtains a `TxnGuard` by adding the quorum-certified update to the validator transaction pool. [1](#0-0) 

The `TxnGuard` is an RAII guard that automatically removes the transaction from the pool when dropped: [2](#0-1) 

However, there are only three code paths that drop the `TxnGuard`:

**Path 1: New Observation** - When a new observation arrives that differs from on-chain state, it overwrites the `Finished` state: [3](#0-2) 

This path is **blocked** if the `JWKObserver` stops sending observations due to network failures, rate limiting, or OIDC provider downtime. The observer silently ignores errors: [4](#0-3) 

**Path 2: On-Chain State Update** - When `reset_with_on_chain_state` is called with a different on-chain state for the issuer: [5](#0-4) 

The critical issue is on lines 260-268: if the locally cached on-chain state **matches** the incoming on-chain state, the method performs a NO-OP and retains the existing `PerProviderState`, including the `Finished` consensus state with its `TxnGuard`. This path is **blocked** when the specific issuer's on-chain state doesn't change (transaction not committed or other issuers update instead).

**Path 3: Epoch End** - The entire manager is torn down: [6](#0-5) 

This only happens at epoch boundaries.

**Attack Scenario:**
1. Validator observes new JWKs for issuer A (v2) while on-chain shows v1
2. JWK consensus completes, creating `ConsensusState::Finished` with `TxnGuard`
3. Transaction added to validator pool with topic `JWK_CONSENSUS(issuer_A)`
4. `JWKObserver` stops sending observations (network partition, OIDC provider downtime, rate limiting)
5. Transaction either isn't pulled from pool or proposals containing it fail
6. Meanwhile, other issuers (B, C) successfully update on-chain
7. `reset_with_on_chain_state` called with new on-chain state
8. For issuer A: `locally_cached.on_chain` (v1) == `incoming.on_chain` (v1) â†’ **NO-OP**
9. `Finished` state with `TxnGuard` persists
10. This continues for hours until epoch ends

The `PerProviderState` structure shows the vulnerability: [7](#0-6) 

The `consensus_state` field holds the `Finished` variant indefinitely, which contains the full `ObservedUpdate` and `QuorumCertifiedUpdate` with BLS signatures: [8](#0-7) 

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:

1. **Validator node slowdowns**: Transaction pool bloat from stuck transactions degrades consensus performance. Multiple issuers can be affected simultaneously, compounding the impact.

2. **Significant protocol violations**: The validator transaction pool's liveness guarantees are violated. Transactions remain in the pool indefinitely without being committed or explicitly removed.

3. **Memory leak**: Each `Finished` state holds substantial data:
   - `ObservedUpdate` with full JWK set and BLS signature
   - `QuorumCertifiedUpdate` with aggregated multi-signatures from 2f+1 validators
   - These persist for the epoch duration (potentially hours)

4. **DOS vector**: An attacker controlling or influencing multiple OIDC providers could trigger this for many issuers simultaneously, exhausting pool resources and memory.

The vulnerability does not meet Critical severity because it:
- Does not cause consensus safety violations (different blocks)
- Does not result in loss of funds
- Is limited to epoch duration (eventually cleaned up)
- Requires specific conditions (observer failures + transaction not committed)

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability can be triggered through multiple realistic scenarios:

**Natural Failures (High probability):**
- Network partitions between validators and OIDC providers
- OIDC provider rate limiting (e.g., Google, Microsoft identity services have strict rate limits)
- OIDC provider downtime or maintenance windows
- Slow HTTP responses causing observation timeouts
- Validators in different geographic regions experiencing local network issues

**Malicious Triggers (Medium probability):**
- Attacker controlling an OIDC provider can stabilize JWKs after consensus completes
- Attacker with network-level access can selectively drop JWK observation traffic
- DOS attacks against OIDC providers to prevent observations

**Amplification factors:**
- Multiple issuers can be affected simultaneously
- Epochs can last hours, prolonging the leak
- No automatic retry or timeout mechanism exists
- The `observation_tx.push()` silently ignores errors (line 80 of observer)

The vulnerability is not theoretical - production validators regularly experience network issues, and OIDC providers are external dependencies outside validator control.

## Recommendation

Implement explicit timeout and cleanup logic for `ConsensusState::Finished`:

**Option 1: Add timestamp tracking and periodic cleanup**
```rust
pub struct PerProviderState {
    pub on_chain: Option<ProviderJWKs>,
    pub observed: Option<Vec<JWKMoveStruct>>,
    pub consensus_state: ConsensusState<ObservedUpdate>,
    pub consensus_finished_at: Option<Instant>, // Add timestamp
}

// In IssuerLevelConsensusManager, add periodic cleanup:
async fn periodic_cleanup(&mut self) {
    let timeout = Duration::from_secs(3600); // 1 hour
    let now = Instant::now();
    
    self.states_by_issuer.retain(|issuer, state| {
        match (&state.consensus_state, state.consensus_finished_at) {
            (ConsensusState::Finished { .. }, Some(finished_at)) 
                if now.duration_since(finished_at) > timeout => {
                warn!("Dropping stale Finished state for {:?}", issuer);
                false // Drop the state, which drops the TxnGuard
            },
            _ => true
        }
    });
}
```

**Option 2: Retry observation when in Finished state**
```rust
// In process_new_observation, always check if we should retry:
pub fn process_new_observation(&mut self, issuer: Issuer, jwks: Vec<JWKMoveStruct>) -> Result<()> {
    let state = self.states_by_issuer.entry(issuer.clone()).or_default();
    state.observed = Some(jwks.clone());
    
    // If in Finished state and observation still differs, restart consensus
    if matches!(state.consensus_state, ConsensusState::Finished { .. }) 
        && state.observed.as_ref() != state.on_chain.as_ref().map(ProviderJWKs::jwks) {
        // Drop old Finished state and start fresh
        state.consensus_state = ConsensusState::NotStarted;
    }
    
    // ... rest of existing logic
}
```

**Option 3: Drop Finished state when transaction is pulled but not committed (most robust)**

Track which transactions have been pulled and implement a callback mechanism to detect when proposals fail, allowing cleanup of the associated `Finished` state.

## Proof of Concept

```rust
#[cfg(test)]
mod txn_guard_leak_test {
    use super::*;
    use aptos_types::jwks::ProviderJWKs;
    
    #[tokio::test]
    async fn test_finished_state_persists_without_observations() {
        // Setup: Create manager with an issuer at version 1
        let mut manager = create_test_manager();
        let issuer = b"test_issuer".to_vec();
        
        // Simulate observation and consensus completion
        manager.process_new_observation(issuer.clone(), vec![test_jwk()]).unwrap();
        // Consensus runs...
        let qc_update = create_test_qc_update(&issuer, 2); // version 2
        manager.process_quorum_certified_update(qc_update).unwrap();
        
        // Verify Finished state with TxnGuard
        let state = manager.states_by_issuer.get(&issuer).unwrap();
        assert!(matches!(state.consensus_state, ConsensusState::Finished { .. }));
        
        // Transaction is in pool
        let pool_size_before = count_pool_transactions(&manager.vtxn_pool);
        assert_eq!(pool_size_before, 1);
        
        // Simulate reset_with_on_chain_state called multiple times
        // with unchanged on-chain state for this issuer
        for _ in 0..10 {
            let on_chain = create_on_chain_state(&issuer, 1); // Still version 1
            manager.reset_with_on_chain_state(on_chain).unwrap();
            
            // Finished state should persist (BUG!)
            let state = manager.states_by_issuer.get(&issuer).unwrap();
            assert!(matches!(state.consensus_state, ConsensusState::Finished { .. }));
            
            // Transaction still in pool (BUG!)
            let pool_size = count_pool_transactions(&manager.vtxn_pool);
            assert_eq!(pool_size, 1, "Transaction leaked in pool across reset");
        }
        
        // Memory leak: Finished state holds full QC with signatures
        // Transaction pool bloat: Transaction occupies slot indefinitely
        // This persists until epoch end!
    }
}
```

**Notes**

The vulnerability is confined to the JWK consensus subsystem and does not directly affect main consensus safety. However, it represents a significant liveness and resource management issue that can degrade validator performance over time. The root cause is the lack of explicit lifecycle management for the `Finished` state - the code assumes either new observations will always arrive or on-chain updates will always happen, but network failures violate these assumptions. The same vulnerability exists in both `IssuerLevelConsensusManager` and `KeyLevelConsensusManager` implementations.

### Citations

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L170-181)
```rust
    async fn tear_down(&mut self, ack_tx: Option<oneshot::Sender<()>>) -> Result<()> {
        self.stopped = true;
        let futures = std::mem::take(&mut self.jwk_observers)
            .into_iter()
            .map(JWKObserver::shutdown)
            .collect::<Vec<_>>();
        join_all(futures).await;
        if let Some(tx) = ack_tx {
            let _ = tx.send(());
        }
        Ok(())
    }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L183-228)
```rust
    /// Triggered by an observation thread periodically.
    pub fn process_new_observation(
        &mut self,
        issuer: Issuer,
        jwks: Vec<JWKMoveStruct>,
    ) -> Result<()> {
        debug!(
            epoch = self.epoch_state.epoch,
            issuer = String::from_utf8(issuer.clone()).ok(),
            "Processing new observation."
        );
        let state = self.states_by_issuer.entry(issuer.clone()).or_default();
        state.observed = Some(jwks.clone());
        if state.observed.as_ref() != state.on_chain.as_ref().map(ProviderJWKs::jwks) {
            let observed = ProviderJWKs {
                issuer: issuer.clone(),
                version: state.on_chain_version() + 1,
                jwks,
            };
            let signature = self
                .consensus_key
                .sign(&observed)
                .context("process_new_observation failed with signing error")?;
            let abort_handle = self
                .update_certifier
                .start_produce(
                    self.epoch_state.clone(),
                    observed.clone(),
                    self.qc_update_tx.clone(),
                )
                .context(
                    "process_new_observation failed with update_certifier.start_produce failure",
                )?;
            state.consensus_state = ConsensusState::InProgress {
                my_proposal: ObservedUpdate {
                    author: self.my_addr,
                    observed: observed.clone(),
                    signature,
                },
                abort_handle_wrapper: QuorumCertProcessGuard::new(abort_handle),
            };
            info!("[JWK] update observed, update={:?}", observed);
        }

        Ok(())
    }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L231-292)
```rust
    pub fn reset_with_on_chain_state(&mut self, on_chain_state: AllProvidersJWKs) -> Result<()> {
        info!(
            epoch = self.epoch_state.epoch,
            "reset_with_on_chain_state starting."
        );
        let onchain_issuer_set: HashSet<Issuer> = on_chain_state
            .entries
            .iter()
            .map(|entry| entry.issuer.clone())
            .collect();
        let local_issuer_set: HashSet<Issuer> = self.states_by_issuer.keys().cloned().collect();

        for issuer in local_issuer_set.difference(&onchain_issuer_set) {
            info!(
                epoch = self.epoch_state.epoch,
                op = "delete",
                issuer = issuer.clone(),
                "reset_with_on_chain_state"
            );
        }

        self.states_by_issuer
            .retain(|issuer, _| onchain_issuer_set.contains(issuer));
        for on_chain_provider_jwks in on_chain_state.entries {
            let issuer = on_chain_provider_jwks.issuer.clone();
            let locally_cached = self
                .states_by_issuer
                .get(&on_chain_provider_jwks.issuer)
                .and_then(|s| s.on_chain.as_ref());
            if locally_cached == Some(&on_chain_provider_jwks) {
                // The on-chain update did not touch this provider.
                // The corresponding local state does not have to be reset.
                info!(
                    epoch = self.epoch_state.epoch,
                    op = "no-op",
                    issuer = issuer,
                    "reset_with_on_chain_state"
                );
            } else {
                let old_value = self.states_by_issuer.insert(
                    on_chain_provider_jwks.issuer.clone(),
                    PerProviderState::new(on_chain_provider_jwks),
                );
                let op = if old_value.is_some() {
                    "update"
                } else {
                    "insert"
                };
                info!(
                    epoch = self.epoch_state.epoch,
                    op = op,
                    issuer = issuer,
                    "reset_with_on_chain_state"
                );
            }
        }
        info!(
            epoch = self.epoch_state.epoch,
            "reset_with_on_chain_state finished."
        );
        Ok(())
    }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L323-358)
```rust
    pub fn process_quorum_certified_update(&mut self, update: QuorumCertifiedUpdate) -> Result<()> {
        let issuer = update.update.issuer.clone();
        info!(
            epoch = self.epoch_state.epoch,
            issuer = String::from_utf8(issuer.clone()).ok(),
            version = update.update.version,
            "JWKManager processing certified update."
        );
        let state = self.states_by_issuer.entry(issuer.clone()).or_default();
        match &state.consensus_state {
            ConsensusState::InProgress { my_proposal, .. } => {
                //TODO: counters
                let txn = ValidatorTransaction::ObservedJWKUpdate(update.clone());
                let vtxn_guard =
                    self.vtxn_pool
                        .put(Topic::JWK_CONSENSUS(issuer.clone()), Arc::new(txn), None);
                state.consensus_state = ConsensusState::Finished {
                    vtxn_guard,
                    my_proposal: my_proposal.clone(),
                    quorum_certified: update.clone(),
                };
                info!(
                    epoch = self.epoch_state.epoch,
                    issuer = String::from_utf8(issuer).ok(),
                    version = update.update.version,
                    "certified update accepted."
                );
                Ok(())
            },
            _ => Err(anyhow!(
                "qc update not expected for issuer {:?} in state {}",
                String::from_utf8(issuer.clone()),
                state.consensus_state.name()
            )),
        }
    }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L361-382)
```rust
#[derive(Clone, Debug, Default, Eq, PartialEq)]
pub struct PerProviderState {
    pub on_chain: Option<ProviderJWKs>,
    pub observed: Option<Vec<JWKMoveStruct>>,
    pub consensus_state: ConsensusState<ObservedUpdate>,
}

impl PerProviderState {
    pub fn new(provider_jwks: ProviderJWKs) -> Self {
        Self {
            on_chain: Some(provider_jwks),
            observed: None,
            consensus_state: ConsensusState::NotStarted,
        }
    }

    pub fn on_chain_version(&self) -> u64 {
        self.on_chain
            .as_ref()
            .map_or(0, |provider_jwks| provider_jwks.version)
    }
}
```

**File:** crates/validator-transaction-pool/src/lib.rs (L202-206)
```rust
impl Drop for TxnGuard {
    fn drop(&mut self) {
        self.pool.lock().try_delete(self.seq_num);
    }
}
```

**File:** crates/aptos-jwk-consensus/src/jwk_observer.rs (L70-84)
```rust
        loop {
            tokio::select! {
                _ = interval.tick().fuse() => {
                    let timer = Instant::now();
                    let result = fetch_jwks(open_id_config_url.as_str(), my_addr).await;
                    debug!(issuer = issuer, "observe_result={:?}", result);
                    let secs = timer.elapsed().as_secs_f64();
                    if let Ok(mut jwks) = result {
                        OBSERVATION_SECONDS.with_label_values(&[issuer.as_str(), "ok"]).observe(secs);
                        jwks.sort();
                        let _ = observation_tx.push((), (issuer.as_bytes().to_vec(), jwks));
                    } else {
                        OBSERVATION_SECONDS.with_label_values(&[issuer.as_str(), "err"]).observe(secs);
                    }
                },
```

**File:** crates/aptos-jwk-consensus/src/types.rs (L104-115)
```rust
pub enum ConsensusState<T: Debug + Clone + Eq + PartialEq> {
    NotStarted,
    InProgress {
        my_proposal: T,
        abort_handle_wrapper: QuorumCertProcessGuard,
    },
    Finished {
        vtxn_guard: TxnGuard,
        my_proposal: T,
        quorum_certified: QuorumCertifiedUpdate,
    },
}
```
