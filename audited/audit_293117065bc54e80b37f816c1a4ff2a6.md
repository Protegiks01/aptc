# Audit Report

## Title
Race Condition in StateSnapshotCommitter Causes Node Crash When State Cache is Disabled

## Summary
A race condition exists between `StateSnapshotCommitter` and `StateMerkleBatchCommitter` threads that share `Arc<StateDb>`. When the node cache is disabled (or insufficient), `StateSnapshotCommitter` attempts to read a database version that hasn't been committed yet, causing a panic and node crash. This breaks state consistency guarantees and validator availability.

## Finding Description

The `StateSnapshotCommitter` and `StateMerkleBatchCommitter` run in separate threads and share access to the underlying database through `Arc<StateDb>` clones [1](#0-0) . 

The vulnerability occurs in this sequence:

1. **StateSnapshotCommitter processes snapshot N**: Updates `self.last_snapshot` to version N, computes merkle batches, and sends them to StateMerkleBatchCommitter [2](#0-1) .

2. **StateSnapshotCommitter immediately processes snapshot N+1**: Uses `base_version = self.last_snapshot.version()` (which is now N) and calls `get_shard_persisted_versions(base_version)` [3](#0-2)  and [4](#0-3) .

3. **Race condition**: The rendezvous channel (CHANNEL_SIZE = 0) [5](#0-4)  only ensures the message was received, not that StateMerkleBatchCommitter finished committing version N to disk. Meanwhile, `get_shard_persisted_versions` tries to read the root node at version N [6](#0-5) .

4. **Critical failure path**: If the root node doesn't exist yet (because commit hasn't completed), `get_node_with_tag` returns `AptosDbError::NotFound` [7](#0-6) , causing `merklize()` to fail with `.expect("Failed to compute JMT commit batch.")` [8](#0-7) , which **panics and crashes the StateSnapshotCommitter thread**.

**When does this occur?**

- **Guaranteed failure when caching is disabled**: When `max_num_nodes_per_lru_cache_shard = 0`, no nodes are cached [9](#0-8) , and reads go directly to the database, hitting the race condition.

- **The design acknowledges this risk**: The const_assert comment explicitly states "This is to ensure we cache nodes in memory from previous batches before they get committed to DB" [10](#0-9) , indicating the developers knew about the race but used caching as a mitigation rather than fixing the fundamental synchronization issue.

This breaks the **State Consistency** invariant because the state snapshot commit pipeline can crash mid-operation, leaving the database in an inconsistent state where new transactions are executed but state snapshots are not committed.

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria:

1. **Validator node crashes**: When triggered, the StateSnapshotCommitter thread panics, causing the entire validator node to crash or become unable to commit state snapshots.

2. **Loss of validator availability**: A crashed validator cannot participate in consensus, reducing network resilience. If multiple validators are misconfigured with caching disabled, this could impact network liveness.

3. **State DB divergence**: The state merkle database stops being updated while transaction execution continues, eventually causing the node to fall behind and requiring manual intervention to recover.

4. **Production reachability**: The default configuration has caching enabled [11](#0-10) , but operators can disable it for testing or debugging, unknowingly triggering this bug. Even with caching enabled, cache eviction or failures could expose the race condition under high load.

## Likelihood Explanation

**Likelihood: Medium to High**

- **Configuration-dependent**: The bug is guaranteed to trigger when `max_num_nodes_per_lru_cache_shard = 0`. Operators debugging performance issues or running custom configurations might set this value.

- **High transaction throughput increases risk**: Even with caching enabled, processing many snapshots rapidly increases the chance of cache misses or evictions hitting the race window.

- **No external attacker needed**: This is a design flaw that can be triggered by normal operations under specific configurations or load conditions.

- **Detection difficulty**: The issue manifests as intermittent crashes that might be attributed to other causes, making it hard to diagnose in production.

## Recommendation

**Fix the race condition with proper synchronization instead of relying on caching:**

1. **Option 1 - Synchronous commit**: Make `StateSnapshotCommitter` wait for the previous batch to be committed before processing the next snapshot. Modify the channel protocol to send an acknowledgment when commit completes.

2. **Option 2 - Read from persisted_state**: Instead of reading from `state_db` directly in `merklize()`, read the base version information from `persisted_state`, which is updated atomically after commits complete [12](#0-11) .

3. **Option 3 - Enforce cache invariant**: Add a runtime check that panics at startup if caching is disabled, making the dependency explicit:

```rust
pub fn new(state_db: Arc<StateDb>, ...) -> Self {
    assert!(
        state_db.state_merkle_db.cache_enabled(),
        "StateSnapshotCommitter requires caching to be enabled to prevent race conditions"
    );
    // ... rest of initialization
}
```

**Recommended approach**: Implement Option 1 or 2 to fix the root cause, and add Option 3 as defense-in-depth.

## Proof of Concept

**Reproduction steps:**

1. Configure a node with `max_num_nodes_per_lru_cache_shard: 0` in the storage config.

2. Start the node and begin processing transactions that generate state updates.

3. Ensure multiple state snapshots are created in quick succession (e.g., by lowering `buffered_state_target_items`).

4. Observe the node crash with the error: `"Failed to compute JMT commit batch."` originating from `state_snapshot_committer.rs:165`.

**Expected behavior**: The node panics because `get_node_with_tag` cannot find the root node at the base version, as it hasn't been committed to disk yet.

**Crash stack trace location**:
- [13](#0-12) 
- [4](#0-3) 
- [6](#0-5) 

## Notes

The `const_assert` at line 60-62 and its comment reveal that this race condition was partially understood during development, but the mitigation (caching) is insufficient because:

1. Caching can be disabled via configuration
2. Caches can fail or be evicted under memory pressure
3. The fundamental synchronization issue remains unaddressed

This is a systemic design issue where two threads share mutable database access through Arc without proper coordination of read-after-write dependencies.

### Citations

**File:** storage/aptosdb/src/state_store/state_snapshot_committer.rs (L51-51)
```rust
    const CHANNEL_SIZE: usize = 0;
```

**File:** storage/aptosdb/src/state_store/state_snapshot_committer.rs (L59-62)
```rust
        // Note: This is to ensure we cache nodes in memory from previous batches before they get committed to DB.
        const_assert!(
            StateSnapshotCommitter::CHANNEL_SIZE < VersionedNodeCache::NUM_VERSIONS_TO_CACHE
        );
```

**File:** storage/aptosdb/src/state_store/state_snapshot_committer.rs (L66-77)
```rust
        let arc_state_db = Arc::clone(&state_db);
        let join_handle = std::thread::Builder::new()
            .name("state_batch_committer".to_string())
            .spawn(move || {
                let committer = StateMerkleBatchCommitter::new(
                    arc_state_db,
                    state_merkle_batch_commit_receiver,
                    persisted_state.clone(),
                );
                committer.run();
            })
            .expect("Failed to spawn state merkle batch committer thread.");
```

**File:** storage/aptosdb/src/state_store/state_snapshot_committer.rs (L92-100)
```rust
                    let base_version = self.last_snapshot.version();
                    let previous_epoch_ending_version = self
                        .state_db
                        .ledger_db
                        .metadata_db()
                        .get_previous_epoch_ending(version)
                        .unwrap()
                        .map(|(v, _e)| v);
                    let min_version = self.last_snapshot.next_version();
```

**File:** storage/aptosdb/src/state_store/state_snapshot_committer.rs (L156-165)
```rust
                    let (state_merkle_batch, leaf_count) = Self::merklize(
                        &self.state_db.state_merkle_db,
                        base_version,
                        version,
                        &self.last_snapshot.summary().global_state_summary,
                        &snapshot.summary().global_state_summary,
                        all_updates.try_into().expect("Must be 16 shards."),
                        previous_epoch_ending_version,
                    )
                    .expect("Failed to compute JMT commit batch.");
```

**File:** storage/aptosdb/src/state_store/state_snapshot_committer.rs (L177-185)
```rust
                    self.last_snapshot = snapshot.clone();

                    self.state_merkle_batch_commit_sender
                        .send(CommitMessage::Data(StateMerkleCommit {
                            snapshot,
                            hot_batch: hot_state_merkle_batch_opt,
                            cold_batch: state_merkle_batch,
                        }))
                        .unwrap();
```

**File:** storage/aptosdb/src/state_store/state_snapshot_committer.rs (L212-212)
```rust
        let shard_persisted_versions = db.get_shard_persisted_versions(base_version)?;
```

**File:** storage/jellyfish-merkle/src/lib.rs (L126-129)
```rust
    fn get_node_with_tag(&self, node_key: &NodeKey, tag: &str) -> Result<Node<K>> {
        self.get_node_option(node_key, tag)?
            .ok_or_else(|| AptosDbError::NotFound(format!("Missing node at {:?}.", node_key)))
    }
```

**File:** storage/jellyfish-merkle/src/lib.rs (L467-468)
```rust
            let root_node_key = NodeKey::new_empty_path(root_persisted_version);
            let root_node = self.reader.get_node_with_tag(&root_node_key, "commit")?;
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L556-558)
```rust
    pub(crate) fn cache_enabled(&self) -> bool {
        self.lru_cache.is_some()
    }
```

**File:** config/src/config/storage_config.rs (L25-25)
```rust
pub const DEFAULT_MAX_NUM_NODES_PER_LRU_CACHE_SHARD: usize = 1 << 13;
```

**File:** storage/aptosdb/src/state_store/state_merkle_batch_committer.rs (L106-106)
```rust
                    self.persisted_state.set(snapshot);
```
