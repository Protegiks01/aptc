# Audit Report

## Title
Resource Exhaustion via Unvalidated Record Sizes in Backup Restore Process

## Summary
The `LoadedChunk::load()` function in the transaction backup restore process loads and deserializes all chunk data into memory before verifying cryptographic proofs. An attacker who can serve malicious backup files can exploit this ordering to cause denial-of-service by providing chunks with extremely large records (up to 4GB each) and invalid proofs, forcing nodes to exhaust memory and CPU resources before discovering the proof is invalid.

## Finding Description

The vulnerability exists in the backup restore workflow where proof verification occurs after complete data loading, violating the "Resource Limits" invariant that all operations must respect memory and computational constraints.

**Attack Flow:**

1. The `LoadedChunk::load()` function processes backup chunks in this order:
   - Opens the chunk file and begins reading records [1](#0-0) 
   - For each record, calls `read_record_bytes()` which reads a 4-byte size header (u32), allowing records up to 4GB [2](#0-1) 
   - Allocates a buffer of that size without validation [3](#0-2) 
   - Deserializes all transactions, auxiliary info, transaction infos, events, and write sets into memory vectors [4](#0-3) 
   - Only after loading ALL data does it load the proof [5](#0-4) 
   - Finally verifies the proof cryptographically [6](#0-5) 

2. An attacker who controls or compromises the backup storage source can craft malicious backup files with:
   - Manifests declaring normal version ranges
   - Chunk files containing records with inflated size headers (e.g., 500MB-4GB each)
   - Invalid or mismatched cryptographic proofs that will fail verification

3. When a node attempts to restore from this source:
   - It loads the manifest (which appears valid) [7](#0-6) 
   - For each chunk, spawns tasks to load them in parallel [8](#0-7) 
   - Each task allocates gigabytes of memory deserializing the malicious records
   - Only after consuming these resources does proof verification fail
   - The node may run out of memory (OOM kill), become unresponsive, or fail the restore operation

4. The manifest verification only checks version range continuity, not data sizes [9](#0-8) 

**Invariant Violation:**
This breaks the documented invariant #9: "Resource Limits: All operations must respect gas, storage, and computational limits." The restore operation performs unbounded memory allocation based on untrusted size headers before cryptographic validation.

## Impact Explanation

**Severity: Medium** (up to $10,000 per Aptos Bug Bounty)

This vulnerability enables denial-of-service attacks against nodes performing backup restoration operations. While it does not directly impact consensus, active validators, or cause fund loss, it has significant availability implications:

- **Prevents New Nodes from Joining**: If malicious backup sources are used, new validators or fullnodes cannot bootstrap successfully
- **Disrupts Disaster Recovery**: Existing nodes recovering from failures cannot restore state
- **Resource Exhaustion**: Can cause out-of-memory kills, system instability, or prolonged unresponsiveness during restore attempts
- **Repeated Exploitation**: The attack can be sustained across multiple restore attempts

This does not qualify as High severity because it:
- Does not affect running consensus nodes
- Requires the victim to configure and use a compromised backup source
- Does not cause permanent network disruption

It exceeds Low severity because the impact extends beyond minor information leaks and can prevent critical infrastructure operations.

## Likelihood Explanation

**Likelihood: Medium**

The attack requires specific preconditions but is feasible:

**Required Conditions:**
- Attacker must control or compromise a backup storage source (S3, GCS, Azure bucket, or local filesystem)
- Victim node must be configured to restore from this source
- Typical scenarios: compromised cloud credentials, malicious storage provider, supply chain attack on backup infrastructure

**Feasibility:**
- Cloud storage compromise is a known attack vector
- Backup authentication may rely solely on cloud provider credentials
- The attack is technically simple: craft files with large size headers and invalid proofs
- No specialized blockchain knowledge required beyond backup format structure

**Mitigation Factors:**
- Most production nodes use operator-controlled or Aptos Foundation backup sources
- Backup sources are typically considered semi-trusted infrastructure
- The cryptographic proof verification does eventually detect and reject malicious data

The balance of these factors results in Medium likelihood.

## Recommendation

Implement record size validation and consider incremental verification:

**Immediate Fix - Add Maximum Record Size Limit:**

Add a constant for maximum allowed record size in `read_record_bytes.rs`:
```rust
const MAX_RECORD_SIZE: usize = 128 * 1024 * 1024; // 128 MB, matching max_chunk_size default

async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
    // ... existing size reading code ...
    
    let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
    
    // Add validation
    ensure!(
        record_size <= MAX_RECORD_SIZE,
        "Record size {} exceeds maximum allowed size of {}",
        record_size,
        MAX_RECORD_SIZE
    );
    
    // ... rest of function ...
}
```

**Additional Hardening:**

1. Track cumulative memory allocation per chunk and enforce limits
2. Verify proof signature before loading full chunk data (if protocol allows)
3. Add configurable timeout for chunk loading operations
4. Implement streaming verification where feasible

**Defense in Depth:**
- Document that backup sources should be authenticated and integrity-checked
- Consider adding backup manifest signatures
- Implement monitoring/alerting for abnormal restore resource consumption

## Proof of Concept

```rust
// File: malicious_backup_creator.rs
// Creates a malicious backup chunk file that triggers the vulnerability

use std::io::Write;
use std::fs::File;

fn create_malicious_chunk(output_path: &str) {
    let mut file = File::create(output_path).unwrap();
    
    // Create 10 records, each claiming to be 500MB
    for _ in 0..10 {
        let malicious_size: u32 = 500 * 1024 * 1024; // 500 MB
        
        // Write the size header
        file.write_all(&malicious_size.to_be_bytes()).unwrap();
        
        // Write actual garbage data (smaller than claimed size to save disk space)
        // The read_record_bytes will still try to allocate 500MB buffer
        let garbage: Vec<u8> = vec![0xFF; 1024]; // Just 1KB of data
        file.write_all(&garbage).unwrap();
    }
    
    println!("Created malicious chunk at {}", output_path);
    println!("This chunk claims 5GB total but contains minimal actual data");
    println!("When LoadedChunk::load() processes this, it will:");
    println!("1. Allocate 500MB for first record");
    println!("2. Fail to read full 500MB (hit EOF)"); 
    println!("3. But the allocation already happened, consuming memory");
    println!("With proper padding, could force full allocation before proof verification");
}

// To demonstrate the attack in a full test environment:
// 1. Create malicious chunk file as above with proper record structure
// 2. Create a manifest pointing to this chunk
// 3. Create an invalid proof file
// 4. Configure a node to restore from this backup
// 5. Observe memory consumption spike before proof verification fails
// 6. With multiple concurrent chunks, can exhaust node memory
```

**Test Scenario:**
A production-ready PoC would require creating properly formatted but oversized BCS-serialized transaction records with invalid proofs. The node would consume gigabytes of memory across multiple parallel chunk loads [10](#0-9)  before any proof verification occurs, potentially causing OOM conditions.

## Notes

The vulnerability is confirmed present in the codebase. While backup sources are typically semi-trusted infrastructure, the principle of defense-in-depth suggests that resource validation should occur before cryptographic verification to prevent resource exhaustion attacks. The lack of size limits in `read_record_bytes()` combined with post-load proof verification creates an exploitable DoS vector against nodes performing restore operations.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L105-105)
```rust
        let mut file = BufReader::new(storage.open_for_read(&manifest.transactions).await?);
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L112-137)
```rust
        while let Some(record_bytes) = file.read_record_bytes().await? {
            let (txn, aux_info, txn_info, events, write_set): (
                _,
                PersistedAuxiliaryInfo,
                _,
                _,
                WriteSet,
            ) = match manifest.format {
                TransactionChunkFormat::V0 => {
                    let (txn, txn_info, events, write_set) = bcs::from_bytes(&record_bytes)?;
                    (
                        txn,
                        PersistedAuxiliaryInfo::None,
                        txn_info,
                        events,
                        write_set,
                    )
                },
                TransactionChunkFormat::V1 => bcs::from_bytes(&record_bytes)?,
            };
            txns.push(txn);
            persisted_aux_info.push(aux_info);
            txn_infos.push(txn_info);
            event_vecs.push(events);
            write_sets.push(write_set);
        }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L147-151)
```rust
        let (range_proof, ledger_info) = storage
            .load_bcs_file::<(TransactionAccumulatorRangeProof, LedgerInfoWithSignatures)>(
                &manifest.proof,
            )
            .await?;
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L167-167)
```rust
        txn_list_with_proof.verify(ledger_info.ledger_info(), Some(manifest.first_version))?;
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L347-353)
```rust
        let manifest_stream = manifest_handle_stream
            .map(move |hdl| {
                let storage = storage.clone();
                async move { storage.load_json_file(&hdl).await.err_notes(&hdl) }
            })
            .buffered_x(con * 3, con)
            .and_then(|m: TransactionBackup| future::ready(m.verify().map(|_| m)));
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L391-396)
```rust
                    tokio::task::spawn(async move {
                        LoadedChunk::load(chunk, &storage, epoch_history.as_ref()).await
                    })
                    .err_into::<anyhow::Error>()
                    .await
                })
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L398-398)
```rust
            .try_buffered_x(con * 2, con)
```

**File:** storage/backup/backup-cli/src/utils/read_record_bytes.rs (L54-54)
```rust
        let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
```

**File:** storage/backup/backup-cli/src/utils/read_record_bytes.rs (L60-60)
```rust
        let mut record_buf = BytesMut::with_capacity(record_size);
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/manifest.rs (L50-88)
```rust
    pub fn verify(&self) -> Result<()> {
        // check number of waypoints
        ensure!(
            self.first_version <= self.last_version,
            "Bad version range: [{}, {}]",
            self.first_version,
            self.last_version,
        );

        // check chunk ranges
        ensure!(!self.chunks.is_empty(), "No chunks.");

        let mut next_version = self.first_version;
        for chunk in &self.chunks {
            ensure!(
                chunk.first_version == next_version,
                "Chunk ranges not continuous. Expected first version: {}, actual: {}.",
                next_version,
                chunk.first_version,
            );
            ensure!(
                chunk.last_version >= chunk.first_version,
                "Chunk range invalid. [{}, {}]",
                chunk.first_version,
                chunk.last_version,
            );
            next_version = chunk.last_version + 1;
        }

        // check last version in chunk matches manifest
        ensure!(
            next_version - 1 == self.last_version, // okay to -1 because chunks is not empty.
            "Last version in chunks: {}, in manifest: {}",
            next_version - 1,
            self.last_version,
        );

        Ok(())
    }
```
