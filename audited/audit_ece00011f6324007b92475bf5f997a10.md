# Audit Report

## Title
Premature CommitNotification During State Sync Causes Quorum Store Timestamp Desynchronization

## Summary
The `sync_to_target` method sends a `CommitNotification` with a target block timestamp to the Quorum Store **before** the actual state synchronization completes. If state sync fails after this notification, the Quorum Store maintains an incorrect `latest_block_timestamp` that doesn't correspond to any locally committed block, causing premature batch expiration and degraded consensus participation.

## Finding Description

The vulnerability exists in the state synchronization flow within actively participating validator nodes. When a validator falls behind and receives sync info from peers, it triggers a synchronization attempt that prematurely updates the Quorum Store's timestamp before verifying sync success.

**Execution Path:**

1. An active validator receives a proposal or sync info with newer certificates from another validator [1](#0-0) 

2. RoundManager calls `add_certs`, which triggers `sync_to_highest_quorum_cert` [2](#0-1) 

3. This calls `fast_forward_sync`, which invokes `sync_to_target` on the execution client [3](#0-2) [4](#0-3) 

4. **Critical Issue**: `sync_to_target` calls `notify_commit` with the target timestamp **before** attempting the actual sync [5](#0-4) 

5. The actual state sync occurs after the notification [6](#0-5) 

6. A fail point can inject errors at this stage [7](#0-6) 

7. **Aggravating Factor**: The `latest_logical_time` is also updated **after** the sync operation but **before** checking for errors [8](#0-7) 

**Quorum Store Impact:**

When the Quorum Store receives the commit notification, it irreversibly updates its timestamp: [9](#0-8) 

The timestamp cannot decrease due to the check at lines 523-525. This timestamp is then used to expire batches: [10](#0-9) 

The same irreversible update occurs in the batch proof queue: [11](#0-10) 

**Error Handling:**

When sync fails, the error is logged but the RoundManager continues processing: [12](#0-11) 

The incomplete error handling is acknowledged in the codebase: [13](#0-12) 

**Protocol Violation:**

After a failed sync, the node has:
- `latest_logical_time` = target round (e.g., 1000)
- `latest_block_timestamp` = target timestamp  
- Actual committed state = original round (e.g., 500)

This violates the invariant that these timestamps correspond to locally committed blocks. The early return check prevents syncing to intermediate rounds: [14](#0-13) 

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty category "Validator Node Slowdowns - Significant performance degradation affecting consensus":

1. **Degraded Consensus Participation**: The validator's Quorum Store expires all batches with timestamps below the incorrectly advanced timestamp, leaving it unable to propose effectively when elected as leader.

2. **Protocol Invariant Violation**: The core invariant that `latest_block_timestamp` corresponds to committed blockchain state is broken, leading to inconsistent internal state.

3. **Persistent Until Recovery**: The timestamp can only move forward. The node remains in this degraded state until it successfully syncs to a round higher than the failed target, which may take multiple rounds depending on network conditions.

4. **Liveness Impact**: While the validator can still vote on other validators' proposals, it cannot fulfill its proposer duties effectively, reducing network throughput when this validator is elected as leader.

This affects validator liveness rather than consensus safety—the network continues to make progress, but with reduced efficiency and potential leader election failures.

## Likelihood Explanation

**High Likelihood** - This vulnerability can manifest through natural operational failures that occur regularly in distributed systems:

1. **Network Interruptions**: Temporary connectivity loss to state sync peers during the sync operation
2. **Storage Errors**: Disk I/O failures or database write errors during state application
3. **Resource Exhaustion**: Memory pressure or disk space limitations during sync
4. **Natural Sync Failures**: Any condition causing state sync to fail after the notification

The fail point mechanism confirms this scenario is anticipated: [15](#0-14) 

Every validator that experiences state sync failures during normal catch-up operations is susceptible. No malicious actor is required—the vulnerability manifests during routine operational challenges that validators face regularly.

## Recommendation

**Fix 1: Delay notification until sync success**

Move the `notify_commit` call to occur **after** successful state sync:

```rust
// In sync_to_target method:
// First perform the sync
let result = monitor!(
    "sync_to_target",
    self.state_sync_notifier.sync_to_target(target.clone()).await
);

// Only notify on success
if result.is_ok() {
    if let Some(inner) = self.state.read().as_ref() {
        let block_timestamp = target.commit_info().timestamp_usecs();
        inner.payload_manager.notify_commit(block_timestamp, Vec::new());
    }
}
```

**Fix 2: Add rollback mechanism**

Implement a rollback method in QuorumStore that can be called when sync fails to revert timestamp updates.

**Fix 3: Move latest_logical_time update**

Ensure `latest_logical_time` is only updated after confirming sync success:

```rust
let result = monitor!("sync_to_target", self.state_sync_notifier.sync_to_target(target).await);

// Only update on success
if result.is_ok() {
    *latest_logical_time = target_logical_time;
}
```

## Proof of Concept

The vulnerability can be demonstrated using the existing fail point mechanism:

```rust
// Enable the fail point to inject errors
fail::cfg("consensus::sync_to_target", "return").unwrap();

// Simulate validator falling behind and receiving sync info
// The sync will fail after notify_commit is called
// QuorumStore timestamp will be incorrectly advanced
// Validator continues running but cannot propose effectively
```

A complete integration test would:
1. Start validator nodes
2. Cause one validator to fall behind
3. Enable the fail point for that validator
4. Trigger sync via sync info from peers
5. Verify QuorumStore timestamp is advanced despite sync failure
6. Observe degraded proposal capability when validator is elected leader

## Notes

This vulnerability specifically affects validators during normal consensus participation, not just during recovery mode. The RoundManager's sync_up flow can trigger this during active consensus rounds when catching up to peers. The issue is compounded by the dual timestamp update (both `latest_logical_time` and Quorum Store timestamp), making recovery more complex.

### Citations

**File:** consensus/src/round_manager.rs (L878-906)
```rust
    async fn sync_up(&mut self, sync_info: &SyncInfo, author: Author) -> anyhow::Result<()> {
        let local_sync_info = self.block_store.sync_info();
        if sync_info.has_newer_certificates(&local_sync_info) {
            info!(
                self.new_log(LogEvent::ReceiveNewCertificate)
                    .remote_peer(author),
                "Local state {},\n remote state {}", local_sync_info, sync_info
            );
            // Some information in SyncInfo is ahead of what we have locally.
            // First verify the SyncInfo (didn't verify it in the yet).
            sync_info.verify(&self.epoch_state.verifier).map_err(|e| {
                error!(
                    SecurityEvent::InvalidSyncInfoMsg,
                    sync_info = sync_info,
                    remote_peer = author,
                    error = ?e,
                );
                VerifyError::from(e)
            })?;
            SYNC_INFO_RECEIVED_WITH_NEWER_CERT.inc();
            let result = self
                .block_store
                .add_certs(sync_info, self.create_block_retriever(author))
                .await;
            self.process_certificates().await?;
            result
        } else {
            Ok(())
        }
```

**File:** consensus/src/round_manager.rs (L2136-2142)
```rust
                        match result {
                            Ok(_) => trace!(RoundStateLogSchema::new(round_state)),
                            Err(e) => {
                                counters::ERROR_COUNT.inc();
                                warn!(kind = error_kind(&e), RoundStateLogSchema::new(round_state), "Error: {:#}", e);
                            }
                        }
```

**File:** consensus/src/block_storage/sync_manager.rs (L116-132)
```rust
    pub async fn add_certs(
        &self,
        sync_info: &SyncInfo,
        mut retriever: BlockRetriever,
    ) -> anyhow::Result<()> {
        // When the local ordered round is very old than the received sync_info, this function will
        // (1) resets the block store with highest commit cert = sync_info.highest_quorum_cert()
        // (2) insert all the blocks between (inclusive) highest_commit_cert.commit_info().id() to
        // highest_quorum_cert.certified_block().id() into the block store and storage
        // (3) insert the quorum cert for all the above blocks into the block store and storage
        // (4) executes all the blocks that are ordered while inserting the above quorum certs
        self.sync_to_highest_quorum_cert(
            sync_info.highest_quorum_cert().clone(),
            sync_info.highest_commit_cert().clone(),
            &mut retriever,
        )
        .await?;
```

**File:** consensus/src/block_storage/sync_manager.rs (L295-306)
```rust
        let (root, root_metadata, blocks, quorum_certs) = Self::fast_forward_sync(
            &highest_quorum_cert,
            &highest_commit_cert,
            retriever,
            self.storage.clone(),
            self.execution_client.clone(),
            self.payload_manager.clone(),
            self.order_vote_enabled,
            self.window_size,
            Some(self),
        )
        .await?
```

**File:** consensus/src/block_storage/sync_manager.rs (L512-514)
```rust
        execution_client
            .sync_to_target(highest_commit_cert.ledger_info().clone())
            .await?;
```

**File:** consensus/src/state_computer.rs (L187-194)
```rust
        // The pipeline phase already committed beyond the target block timestamp, just return.
        if *latest_logical_time >= target_logical_time {
            warn!(
                "State sync target {:?} is lower than already committed logical time {:?}",
                target_logical_time, *latest_logical_time
            );
            return Ok(());
        }
```

**File:** consensus/src/state_computer.rs (L196-204)
```rust
        // This is to update QuorumStore with the latest known commit in the system,
        // so it can set batches expiration accordingly.
        // Might be none if called in the recovery path, or between epoch stop and start.
        if let Some(inner) = self.state.read().as_ref() {
            let block_timestamp = target.commit_info().timestamp_usecs();
            inner
                .payload_manager
                .notify_commit(block_timestamp, Vec::new());
        }
```

**File:** consensus/src/state_computer.rs (L206-209)
```rust
        // Inject an error for fail point testing
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });
```

**File:** consensus/src/state_computer.rs (L216-219)
```rust
        let result = monitor!(
            "sync_to_target",
            self.state_sync_notifier.sync_to_target(target).await
        );
```

**File:** consensus/src/state_computer.rs (L221-222)
```rust
        // Update the latest logical time
        *latest_logical_time = target_logical_time;
```

**File:** consensus/src/quorum_store/batch_generator.rs (L517-526)
```rust
                        BatchGeneratorCommand::CommitNotification(block_timestamp, batches) => {
                            trace!(
                                "QS: got clean request from execution, block timestamp {}",
                                block_timestamp
                            );
                            // Block timestamp is updated asynchronously, so it may race when it enters state sync.
                            if self.latest_block_timestamp > block_timestamp {
                                continue;
                            }
                            self.latest_block_timestamp = block_timestamp;
```

**File:** consensus/src/quorum_store/batch_generator.rs (L534-550)
```rust
                            // Cleans up all batches that expire in timestamp <= block_timestamp. This is
                            // safe since clean request must occur only after execution result is certified.
                            for (author, batch_id) in self.batch_expirations.expire(block_timestamp) {
                                if let Some(batch_in_progress) = self.batches_in_progress.get(&(author, batch_id)) {
                                    // If there is an identical batch with higher expiry time, re-insert it.
                                    if batch_in_progress.expiry_time_usecs > block_timestamp {
                                        self.batch_expirations.add_item((author, batch_id), batch_in_progress.expiry_time_usecs);
                                        continue;
                                    }
                                }
                                if self.remove_batch_in_progress(author, batch_id) {
                                    counters::BATCH_IN_PROGRESS_EXPIRED.inc();
                                    debug!(
                                        "QS: logical time based expiration batch w. id {} from batches_in_progress, new size {}",
                                        batch_id,
                                        self.batches_in_progress.len(),
                                    );
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L716-729)
```rust
    pub(crate) fn handle_updated_block_timestamp(&mut self, block_timestamp: u64) {
        // tolerate asynchronous notification
        if self.latest_block_timestamp > block_timestamp {
            return;
        }
        let start = Instant::now();
        self.latest_block_timestamp = block_timestamp;
        if let Some(time_lag) = aptos_infallible::duration_since_epoch()
            .checked_sub(Duration::from_micros(block_timestamp))
        {
            counters::TIME_LAG_IN_BATCH_PROOF_QUEUE.observe_duration(time_lag);
        }

        let expired = self.expirations.expire(block_timestamp);
```

**File:** consensus/src/pipeline/execution_client.rs (L669-671)
```rust
        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
```
