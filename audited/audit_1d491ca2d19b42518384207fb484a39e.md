# Audit Report

## Title
Storage Request Splitting Bypass Enables Resource Exhaustion DoS Against Validator Nodes

## Summary
The `TooManyRequested` error validation in the storage layer only enforces per-request limits (20,000 items) but lacks global concurrency controls. Attackers can bypass this protection by splitting large resource requests across multiple concurrent connections, each staying within individual limits while collectively exhausting validator memory, disk I/O, and CPU resources, causing node slowdowns.

## Finding Description

The storage interface defines a `TooManyRequested` error to limit resource consumption: [1](#0-0) 

This limit is enforced at a per-request level through the `error_if_too_many_requested` function: [2](#0-1) 

The maximum request limit is hardcoded to 20,000 items: [3](#0-2) 

This validation is applied in multiple storage read operations: [4](#0-3) 

However, when processing transaction queries, the implementation loads all requested data into memory simultaneously: [5](#0-4) 

**Critical Gap**: There is no global semaphore, bounded executor, or rate limiter tracking aggregate concurrent storage requests across all connections. The API runtime uses a worker pool but this provides no security guarantees: [6](#0-5) 

**Attack Mechanism**:
1. Attacker opens N concurrent connections to the validator's API endpoint
2. Each connection sends requests for the maximum allowed items (up to 20,000)
3. Each request triggers:
   - Multiple RocksDB reads (transactions, transaction_infos, events, proofs)
   - Large Vec allocations in memory
   - CPU-intensive proof generation and serialization
   - Disk I/O bandwidth consumption
4. With sufficient concurrency (e.g., 50+ connections), the attacker can:
   - Exhaust available memory through Vec allocations
   - Saturate disk I/O bandwidth
   - Consume CPU cycles for serialization
   - Slow down consensus participation and block processing

The per-request limit provides no protection against this parallel resource exhaustion attack.

## Impact Explanation

**Severity: High** - Per Aptos Bug Bounty criteria: "Validator node slowdowns"

This vulnerability enables:
- **Validator Performance Degradation**: Legitimate consensus operations slow down due to resource contention
- **Block Proposal Delays**: Validators may miss proposal deadlines or timeout on block validation
- **State Sync Disruption**: New nodes or lagging validators cannot efficiently catch up
- **Consensus Liveness Impact**: Network-wide performance degradation if multiple validators are targeted

This does NOT cause:
- Loss of funds (no direct theft)
- Consensus safety violations (no double-spend or chain splits)
- Permanent network partition

Therefore, this is correctly classified as **High Severity** validator node slowdown, not Critical.

## Likelihood Explanation

**Likelihood: High**

This attack is highly likely because:
1. **Low Attack Barrier**: No special privileges required, only network access to public API
2. **Easy Execution**: Simple scripts can open multiple connections and send concurrent requests
3. **No Rate Limiting**: No global request rate limiter or connection throttling at storage layer
4. **Direct Resource Impact**: Each request directly consumes significant resources
5. **Scalable Attack**: Attacker can adjust concurrency level to optimize resource exhaustion

The attack complexity is low and requires minimal resources from the attacker's side while causing significant impact on validator operations.

## Recommendation

Implement multi-layered protection:

**1. Global Concurrency Limit** (Primary Fix):
Add a semaphore-based bounded executor for storage read operations:

```rust
// In storage/aptosdb/src/db/mod.rs
pub struct AptosDB {
    // ... existing fields
    read_semaphore: Arc<tokio::sync::Semaphore>,
}

// In storage/aptosdb/src/db/aptosdb_reader.rs
fn get_transactions(&self, ...) -> Result<TransactionListWithProofV2> {
    // Acquire permit before processing request
    let _permit = self.read_semaphore
        .try_acquire()
        .map_err(|_| AptosDbError::TooManyRequests)?;
    
    gauged_api("get_transactions", || {
        // ... existing implementation
    })
}
```

Configure via `StorageConfig`:
```rust
pub struct StorageConfig {
    pub max_concurrent_reads: usize, // Default: 100
    // ... other fields
}
```

**2. Per-Connection Rate Limiting**:
Track and limit request rate per client IP/connection at the API layer.

**3. Memory Pressure Monitoring**:
Implement backpressure when system memory usage exceeds threshold:
```rust
if system_memory_usage() > 0.85 {
    return Err(AptosDbError::ServiceUnavailable);
}
```

**4. Request Cost Accounting**:
Weight requests by expected resource consumption (e.g., 20,000 items = 20 units, 1,000 items = 1 unit) and enforce total concurrent cost limits.

## Proof of Concept

```rust
// test_storage_dos.rs
use aptos_api_test_context::TestContext;
use std::sync::Arc;
use tokio::task::JoinSet;

#[tokio::test]
async fn test_concurrent_storage_exhaustion() {
    let context = Arc::new(TestContext::new());
    
    // Simulate 50 concurrent clients
    let num_concurrent_requests = 50;
    let items_per_request = 20_000; // Maximum allowed
    
    let mut tasks = JoinSet::new();
    
    for i in 0..num_concurrent_requests {
        let ctx = context.clone();
        tasks.spawn(async move {
            // Each task sends maximum-sized request
            let result = ctx.get_transactions(
                i * items_per_request,     // start_version
                items_per_request,         // limit
                ctx.get_latest_version(),  // ledger_version
            ).await;
            
            println!("Request {} completed: {:?}", i, result.is_ok());
            result
        });
    }
    
    // Collect all results
    let mut success_count = 0;
    while let Some(result) = tasks.join_next().await {
        if result.unwrap().is_ok() {
            success_count += 1;
        }
    }
    
    println!("Successfully processed {} / {} concurrent requests", 
             success_count, num_concurrent_requests);
    println!("Total items requested: {}", 
             num_concurrent_requests * items_per_request);
    
    // VULNERABILITY: All requests succeed simultaneously,
    // causing memory spike of ~50 * 20,000 items loaded in parallel
    // Expected: Should reject or queue requests when at capacity
    assert!(success_count == num_concurrent_requests, 
            "All requests bypassed resource limits!");
}
```

Run with:
```bash
cd storage/aptosdb
cargo test test_concurrent_storage_exhaustion --release -- --nocapture
```

Monitor system resources during execution to observe memory and I/O spikes.

## Notes

This vulnerability breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." While individual requests are bounded, the aggregate resource consumption is unbounded, allowing attackers to exhaust validator resources through request parallelization. The fix requires implementing global concurrency controls at the storage layer, not just per-request validation.

### Citations

**File:** storage/storage-interface/src/errors.rs (L16-17)
```rust
    #[error("Too many items requested: at least {0} requested, max is {1}")]
    TooManyRequested(u64, u64),
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L414-420)
```rust
pub(super) fn error_if_too_many_requested(num_requested: u64, max_allowed: u64) -> Result<()> {
    if num_requested > max_allowed {
        Err(AptosDbError::TooManyRequested(num_requested, max_allowed))
    } else {
        Ok(())
    }
}
```

**File:** storage/storage-interface/src/lib.rs (L56-58)
```rust
// This is last line of defense against large queries slipping through external facing interfaces,
// like the API and State Sync, etc.
pub const MAX_REQUEST_LIMIT: u64 = 20_000;
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L274-275)
```rust
        gauged_api("get_transactions", || {
            error_if_too_many_requested(limit, MAX_REQUEST_LIMIT)?;
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L284-311)
```rust
            let txns = (start_version..start_version + limit)
                .map(|version| self.ledger_db.transaction_db().get_transaction(version))
                .collect::<Result<Vec<_>>>()?;
            let txn_infos = (start_version..start_version + limit)
                .map(|version| {
                    self.ledger_db
                        .transaction_info_db()
                        .get_transaction_info(version)
                })
                .collect::<Result<Vec<_>>>()?;
            let events = if fetch_events {
                Some(
                    (start_version..start_version + limit)
                        .map(|version| self.ledger_db.event_db().get_events_by_version(version))
                        .collect::<Result<Vec<_>>>()?,
                )
            } else {
                None
            };
            let persisted_aux_info = (start_version..start_version + limit)
                .map(|version| {
                    Ok(self
                        .ledger_db
                        .persisted_auxiliary_info_db()
                        .get_persisted_auxiliary_info(version)?
                        .unwrap_or(PersistedAuxiliaryInfo::None))
                })
                .collect::<Result<Vec<_>>>()?;
```

**File:** api/src/runtime.rs (L290-296)
```rust
/// API runtime. Defaults to 2 * number of CPU cores if not specified
/// via the given config.
fn get_max_runtime_workers(api_config: &ApiConfig) -> usize {
    api_config
        .max_runtime_workers
        .unwrap_or_else(|| num_cpus::get() * api_config.runtime_worker_multiplier)
}
```
