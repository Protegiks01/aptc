# Audit Report

## Title
Block Commit Pipeline Stall Due to DropHelper Thread Pool Exhaustion

## Summary
The execution pipeline can stall during block commits when the `DEFAULT_DROPPER` thread pool is exhausted. The `commit_ledger()` function calls `prune()` which synchronously blocks on `schedule_drop_with_waiter()` if the drop queue is full, preventing new blocks from being committed and causing consensus liveness issues.

## Finding Description

The vulnerability exists in the block pruning mechanism during ledger commits. When a block is committed, old blocks in the block tree must be pruned. This pruning operation schedules the old block tree for asynchronous dropping. [1](#0-0) 

The `prune()` function uses `DEFAULT_DROPPER.schedule_drop_with_waiter(old_root)` to schedule the old block for dropping: [2](#0-1) 

However, `schedule_drop_with_waiter()` internally calls `schedule_drop_impl()` which invokes `num_tasks_tracker.inc()`: [3](#0-2) 

The `inc()` function **blocks** the calling thread when the drop queue is at capacity: [4](#0-3) 

The `DEFAULT_DROPPER` is configured with only 32 maximum queued tasks and 8 worker threads: [5](#0-4) 

Each pruned block contains complex nested structures that trigger further drops:
- `Block` → `PartialStateComputeResult` → `StateCheckpointOutput` → `LedgerStateSummary` → `StateSummary` (2x) → `SparseMerkleTree` (4x total) [6](#0-5) 

The documentation explicitly acknowledges this blocking behavior: [7](#0-6) 

**Attack Scenario:**
1. Under heavy load, blocks are created and committed rapidly
2. Each commit triggers pruning of old blocks via `schedule_drop_with_waiter()`
3. If blocks contain large `SparseMerkleTree` structures, the 8 drop worker threads are busy for extended periods
4. The drop queue fills to capacity (32 tasks)
5. The 33rd commit attempt blocks in `prune()` waiting for drop queue capacity
6. The `commit_ledger()` function stalls, preventing new blocks from being committed
7. Consensus progress halts until drops complete

## Impact Explanation

This vulnerability causes **High Severity** impact per Aptos bug bounty criteria:

- **Validator node slowdowns**: Nodes become unable to commit blocks promptly, causing them to fall behind consensus
- **Consensus liveness degradation**: If multiple validators are affected simultaneously during high load, consensus round progression slows significantly
- **Chain stall risk**: In extreme cases where many validators are blocked, the chain could temporarily halt

This breaks the **Resource Limits** invariant (#9) - the drop operations should not block critical consensus paths. It also impacts the **Total loss of liveness** scenario if the condition persists.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can occur under normal operational conditions without requiring attacker action:

1. **Legitimate heavy load**: During periods of high transaction throughput, blocks are created and committed rapidly
2. **Large state trees**: Blocks with significant state changes contain large `SparseMerkleTree` structures that are slow to drop
3. **Cascading drops**: Each block drop triggers nested drops of multiple structures (4 SparseMerkleTrees per block)
4. **Limited capacity**: With only 32 queue slots and 8 threads, the system can saturate quickly

An attacker could potentially accelerate this condition by:
- Causing rapid block creation through transaction spam
- Creating transactions that result in large state trees
- Coordinating timing to maximize queue pressure during epoch transitions

The vulnerability requires no special privileges and can manifest during normal network operation.

## Recommendation

**Immediate Fix:**

Increase the `DEFAULT_DROPPER` capacity to handle burst load:

```rust
pub static DEFAULT_DROPPER: Lazy<AsyncConcurrentDropper> =
    Lazy::new(|| AsyncConcurrentDropper::new("default", 256, 16));
```

**Proper Long-term Solution:**

Make block pruning fully non-blocking by not waiting for queue capacity:

```rust
pub fn prune(&self, ledger_info: &LedgerInfo) -> Result<()> {
    // ... existing code to update root ...
    
    let old_root = std::mem::replace(&mut *self.root.lock(), root);
    
    // Schedule drop without blocking if queue is full
    // Option 1: Use try_schedule_drop that fails gracefully
    // Option 2: Use a separate unbounded channel for critical path drops
    // Option 3: Drop synchronously only if queue is full (fallback)
    
    if DEFAULT_DROPPER.try_schedule_drop(old_root).is_err() {
        // Queue full - log warning but don't block consensus
        warn!("Drop queue full, deferring block cleanup");
        // Consider: store reference and retry later
    }
    
    Ok(())
}
```

Add monitoring metrics for drop queue saturation to detect this condition before it causes outages.

## Proof of Concept

```rust
#[test]
fn test_commit_blocks_with_saturated_drop_queue() {
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    
    // Simulate slow drops by filling the DEFAULT_DROPPER queue
    let slow_items: Vec<_> = (0..32).map(|_| {
        Arc::new(vec![0u8; 1024 * 1024]) // 1MB items
    }).collect();
    
    // Saturate the drop queue
    for item in slow_items {
        DEFAULT_DROPPER.schedule_drop(item);
    }
    
    // Now attempt to commit blocks - this should block
    let start = std::time::Instant::now();
    
    // Create and commit a block which triggers prune()
    let executor = create_test_executor();
    let block = create_test_block();
    
    executor.execute_and_update_state(block, parent_id, config).unwrap();
    executor.ledger_update(block_id, parent_id).unwrap();
    
    // This commit will call prune() which blocks on schedule_drop_with_waiter
    let result = executor.commit_ledger(create_test_ledger_info());
    
    let elapsed = start.elapsed();
    
    // If the commit took significantly longer than expected,
    // it was blocked on drop queue
    assert!(elapsed > Duration::from_secs(1), 
        "Commit should block when drop queue is saturated");
}
```

## Notes

The codebase demonstrates awareness of this issue through the warning comment in `AsyncConcurrentDropper`, but the critical path still blocks on queue capacity. The nested drop protection via `IN_ANY_DROP_POOL` prevents deadlocks within drop threads but does not protect the commit path from blocking when scheduling initial drops.

This issue is exacerbated by the complex drop chain: each block contains multiple `SparseMerkleTree` instances that themselves use `SUBTREE_DROPPER` for nested subtree cleanup, multiplying the drop work required per block.

### Citations

**File:** execution/executor/src/block_executor/mod.rs (L392-392)
```rust
        self.block_tree.prune(ledger_info_with_sigs.ledger_info())?;
```

**File:** execution/executor/src/block_executor/block_tree/mod.rs (L264-267)
```rust
        let old_root = std::mem::replace(&mut *self.root.lock(), root);

        // send old root to async task to drop it
        Ok(DEFAULT_DROPPER.schedule_drop_with_waiter(old_root))
```

**File:** crates/aptos-drop-helper/src/async_concurrent_dropper.rs (L16-21)
```rust
/// A helper to send things to a thread pool for asynchronous dropping.
///
/// Be aware that there is a bounded number of concurrent drops, as a result:
///   1. when it's "out of capacity", `schedule_drop` will block until a slot to be available.
///   2. if the `Drop` implementation tries to lock things, there can be a potential deadlock due
///      to another thing being waiting for a slot to be available.
```

**File:** crates/aptos-drop-helper/src/async_concurrent_dropper.rs (L42-46)
```rust
    pub fn schedule_drop_with_waiter<V: Send + 'static>(&self, v: V) -> Receiver<()> {
        let (tx, rx) = channel();
        self.schedule_drop_impl(v, Some(tx));
        rx
    }
```

**File:** crates/aptos-drop-helper/src/async_concurrent_dropper.rs (L112-119)
```rust
    fn inc(&self) {
        let mut num_tasks = self.lock.lock();
        while *num_tasks >= self.max_tasks {
            num_tasks = self.cvar.wait(num_tasks).expect("lock poisoned.");
        }
        *num_tasks += 1;
        GAUGE.set_with(&[self.name, "num_tasks"], *num_tasks as i64);
    }
```

**File:** crates/aptos-drop-helper/src/lib.rs (L19-20)
```rust
pub static DEFAULT_DROPPER: Lazy<AsyncConcurrentDropper> =
    Lazy::new(|| AsyncConcurrentDropper::new("default", 32, 8));
```

**File:** execution/executor-types/src/state_checkpoint_output.rs (L41-45)
```rust
    fn new_impl(inner: Inner) -> Self {
        Self {
            inner: Arc::new(DropHelper::new(inner)),
        }
    }
```
