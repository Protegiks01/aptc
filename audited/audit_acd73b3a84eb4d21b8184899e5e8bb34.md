# Audit Report

## Title
Network Partition Risk Through Aggressive Peer Ignoring During State Divergence

## Summary
The default configuration `ignore_low_score_peers = true` combined with aggressive peer scoring creates a network partition risk when a node's local state diverges from the honest network. After only 3-4 proof verification failures per peer, all honest peers become ignored, causing the node to permanently lose the ability to sync from the honest network.

## Finding Description

The vulnerability exists in the peer scoring and selection mechanism of the state sync data client. The issue spans multiple components:

**1. Aggressive Peer Scoring**

The peer scoring system uses multiplicative penalties for errors. When proof verification fails, peers are marked as "malicious" with an 0.8 multiplier: [1](#0-0) 

Starting from a score of 50.0, after just 3-4 `ProofVerificationError` responses, a peer's score drops below the `IGNORE_PEER_THRESHOLD` of 25.0: [2](#0-1) 

**2. Default Peer Ignoring Enabled**

The data client configuration has `ignore_low_score_peers` set to `true` by default: [3](#0-2) 

**3. Ignored Peers Cannot Service Requests**

When a peer is ignored, it returns `None` from `get_storage_summary_if_not_ignored()`, which prevents it from servicing any requests: [4](#0-3) 

This propagates through the request selection logic: [5](#0-4) 

**4. Proof Failures Map to Malicious Errors**

When the state sync driver detects proof verification failures, it sends `PayloadProofFailed` feedback: [6](#0-5) 

This maps to `ResponseError::ProofVerificationError`, which is classified as `ErrorType::Malicious`: [7](#0-6) 

**5. Attack Scenario: State Divergence Causing Cascading Failures**

If a node's local state becomes corrupted or diverges from the honest network (e.g., during fast sync with a corrupted snapshot, epoch transition bugs, disk corruption, or software bugs), the following cascade occurs:

1. Node's state is incorrect (e.g., wrong epoch state, wrong Merkle root hash)
2. Node requests data from honest Peer A → valid proof from peer fails local verification → Peer A scored down
3. Node requests data from honest Peer B → valid proof fails → Peer B scored down
4. Node requests data from honest Peer C → valid proof fails → Peer C scored down
5. After 3-4 requests per peer, ALL honest peers are ignored (score < 25.0)
6. Node attempts to sync but `choose_peers_for_request()` returns `DataIsUnavailable`: [8](#0-7) 

7. Node is completely partitioned from the honest network with no automatic recovery

**6. Insufficient Recovery Mechanism**

While peers can theoretically recover their scores through successful storage summary polls, if the node's state remains divergent, any actual DATA requests will continue to fail proof verification, keeping peers permanently in the ignored state. The global summary excludes ignored peers, making the divergent state self-reinforcing: [9](#0-8) 

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: The node becomes partitioned from the honest network and cannot recover without manual intervention (peer reconnection or configuration changes)
- **Node-level liveness failure**: The affected node loses the ability to sync state, effectively rendering it non-functional
- **Limited scope**: Only affects the individual node with divergent state, not the broader network or consensus

The impact does not reach Critical/High severity because:
- No funds are lost or stolen
- No consensus safety violations occur
- No network-wide effects (other nodes continue functioning normally)
- The issue is self-contained to nodes with already-divergent state

However, this represents a significant operational risk as nodes can become permanently stuck without manual intervention.

## Likelihood Explanation

The likelihood is **Medium** because:

**Triggering Conditions (More likely than expected):**
- Fast sync with corrupted snapshots (network issues, disk corruption)
- Epoch transition bugs causing state mismatches
- Software bugs in state management leading to divergence
- Disk corruption affecting stored state
- Interrupted sync processes leaving inconsistent state

**Amplifying Factors:**
- Only requires 3-4 proof failures per peer (very low threshold)
- Default configuration has peer ignoring enabled
- No automatic fallback mechanism when all peers are ignored
- State divergence may not be immediately apparent to operators

**Real-world scenarios:**
- Nodes bootstrapping during network upgrades
- Nodes recovering from hardware failures
- Nodes with corrupted databases attempting to resync

## Recommendation

Implement a graduated peer ignoring strategy with safeguards:

**1. Add Fallback Logic for All-Peers-Ignored Scenario**

When all peers are ignored and requests fail with `DataIsUnavailable`, temporarily allow sampling from ignored peers as a last resort:

```rust
// In choose_peers_for_request()
let serviceable_peers = self.identify_serviceable(...);
if serviceable_peers.is_empty() && ignore_low_score_peers {
    // Last resort: sample from ignored peers
    warn!("All peers ignored, temporarily allowing low-score peers");
    let all_peers = self.get_all_connected_peers()?;
    return self.choose_random_peers_from_set(all_peers, num_peers_for_request);
}
```

**2. Increase Ignore Threshold or Reduce Malicious Penalty**

Either:
- Increase `IGNORE_PEER_THRESHOLD` from 25.0 to 10.0 (allowing more failures)
- Reduce `MALICIOUS_MULTIPLIER` from 0.8 to 0.9 (requiring ~13 failures instead of 3)

**3. Add Automatic Score Recovery for Ignored Peers**

Implement periodic score boost for ignored peers to allow natural recovery:

```rust
// Periodically boost ignored peers' scores
pub fn boost_ignored_peer_scores(&self) {
    for mut peer_state in self.peer_to_state.iter_mut() {
        if peer_state.is_ignored() {
            peer_state.score = f64::min(peer_state.score + 5.0, IGNORE_PEER_THRESHOLD + 1.0);
        }
    }
}
```

**4. Add Circuit Breaker**

If a node fails proof verification for N consecutive peers (e.g., 5), trigger a warning and disable peer ignoring temporarily, suggesting potential local state issues.

## Proof of Concept

```rust
// Reproduction test demonstrating the vulnerability
#[tokio::test]
async fn test_network_partition_via_state_divergence() {
    use crate::client::AptosDataClient;
    use aptos_config::config::AptosDataClientConfig;
    
    // Create data client with default config (ignore_low_score_peers = true)
    let config = AptosDataClientConfig::default();
    assert!(config.ignore_low_score_peers); // Confirm default
    
    // Create mock network with 5 honest peers
    let (mut mock_network, _, client, _) = 
        MockNetwork::new(Some(base_config), Some(config), None);
    
    let honest_peers = add_several_peers(&mut mock_network, 5, PeerPriority::HighPriority);
    
    // Advertise data from all peers
    for peer in &honest_peers {
        client.update_peer_storage_summary(*peer, create_storage_summary(1000));
    }
    client.update_global_summary_cache().unwrap();
    
    // Spawn handler that returns valid data
    tokio::spawn(async move {
        while let Some(request) = mock_network.next_request(network_id).await {
            send_valid_transaction_response(request);
        }
    });
    
    // Simulate state divergence: Node thinks all proofs are invalid
    // Send 4 requests per peer, marking all as proof verification failures
    for _ in 0..4 {
        for peer in &honest_peers {
            let result = client.get_transactions_with_proof(100, 0, 100, false, 5000).await;
            if let Ok(response) = result {
                // Simulate divergent state: all proofs appear invalid to this node
                response.context.response_callback.notify_bad_response(
                    ResponseError::ProofVerificationError
                );
            }
        }
    }
    
    // Now ALL honest peers should be ignored
    client.update_global_summary_cache().unwrap();
    let summary = client.get_global_data_summary();
    assert!(summary.advertised_data.transactions.is_empty()); // No peers advertising!
    
    // Node is now partitioned: cannot sync from any peer
    let result = client.get_transactions_with_proof(100, 0, 100, false, 5000).await;
    assert_matches!(result, Err(Error::DataIsUnavailable(_)));
    
    // This demonstrates permanent network partition with no automatic recovery
}
```

## Notes

The vulnerability is exacerbated by the lack of differentiation between "peer is Byzantine" and "my local state is wrong." When a node's state diverges, it cannot distinguish between:
- Honest peers sending valid data that fails verification due to local state issues
- Malicious peers sending invalid data

The current implementation assumes all proof failures indicate peer misbehavior, when they could equally indicate local state corruption. A more robust design would detect patterns suggesting local state issues (e.g., all peers failing simultaneously) and adjust behavior accordingly.

### Citations

**File:** state-sync/aptos-data-client/src/peer_states.rs (L33-43)
```rust
const MAX_SCORE: f64 = 100.0;
const MIN_SCORE: f64 = 0.0;
const STARTING_SCORE: f64 = 50.0;
/// Add this score on a successful response.
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
/// Not necessarily a malicious response, but not super useful.
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
/// Likely to be a malicious response.
const MALICIOUS_MULTIPLIER: f64 = 0.8;
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L54-62)
```rust
impl From<ResponseError> for ErrorType {
    fn from(error: ResponseError) -> Self {
        match error {
            ResponseError::InvalidData | ResponseError::InvalidPayloadDataType => {
                ErrorType::NotUseful
            },
            ResponseError::ProofVerificationError => ErrorType::Malicious,
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L142-160)
```rust
    /// Returns the storage summary iff the peer is not below the ignore threshold
    pub fn get_storage_summary_if_not_ignored(&self) -> Option<&StorageServerSummary> {
        if self.is_ignored() {
            None
        } else {
            self.storage_summary.as_ref()
        }
    }

    /// Returns true iff the peer is currently ignored
    fn is_ignored(&self) -> bool {
        // Only ignore peers if the config allows it
        if !self.data_client_config.ignore_low_score_peers {
            return false;
        }

        // Otherwise, ignore peers with a low score
        self.score <= IGNORE_PEER_THRESHOLD
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L168-174)
```rust
    fn update_score_error(&mut self, error: ErrorType) {
        let multiplier = match error {
            ErrorType::NotUseful => NOT_USEFUL_MULTIPLIER,
            ErrorType::Malicious => MALICIOUS_MULTIPLIER,
        };
        self.score = f64::max(self.score * multiplier, MIN_SCORE);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L200-227)
```rust
    pub fn can_service_request(
        &self,
        peer: &PeerNetworkId,
        time_service: TimeService,
        request: &StorageServiceRequest,
    ) -> bool {
        // Storage services can always respond to data advertisement requests.
        // We need this outer check, since we need to be able to send data summary
        // requests to new peers (who don't have a peer state yet).
        if request.data_request.is_storage_summary_request()
            || request.data_request.is_protocol_version_request()
        {
            return true;
        }

        // Check if the peer can service the request
        if let Some(peer_state) = self.peer_to_state.get(peer) {
            return match peer_state.get_storage_summary_if_not_ignored() {
                Some(storage_summary) => {
                    storage_summary.can_service(&self.data_client_config, time_service, request)
                },
                None => false, // The peer is temporarily ignored
            };
        }

        // Otherwise, the request cannot be serviced
        false
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L338-350)
```rust
    /// Calculates a global data summary using all known storage summaries
    pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
        // Gather all storage summaries, but exclude peers that are ignored
        let storage_summaries: Vec<StorageServerSummary> = self
            .peer_to_state
            .iter()
            .filter_map(|peer_state| {
                peer_state
                    .value()
                    .get_storage_summary_if_not_ignored()
                    .cloned()
            })
            .collect();
```

**File:** config/src/config/state_sync_config.rs (L460-466)
```rust
impl Default for AptosDataClientConfig {
    fn default() -> Self {
        Self {
            enable_transaction_data_v2: true,
            data_poller_config: AptosDataPollerConfig::default(),
            data_multi_fetch_config: AptosDataMultiFetchConfig::default(),
            ignore_low_score_peers: true,
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L1383-1394)
```rust
fn extract_response_error(
    notification_feedback: &NotificationFeedback,
) -> Result<ResponseError, Error> {
    match notification_feedback {
        NotificationFeedback::InvalidPayloadData => Ok(ResponseError::InvalidData),
        NotificationFeedback::PayloadTypeIsIncorrect => Ok(ResponseError::InvalidPayloadDataType),
        NotificationFeedback::PayloadProofFailed => Ok(ResponseError::ProofVerificationError),
        _ => Err(Error::UnexpectedErrorEncountered(format!(
            "Invalid notification feedback given: {:?}",
            notification_feedback
        ))),
    }
```

**File:** state-sync/aptos-data-client/src/client.rs (L322-328)
```rust
        // Verify that we have at least one peer to service the request
        if num_peers_for_request == 0 {
            return Err(Error::DataIsUnavailable(format!(
                "No peers are available to service the given request: {:?}",
                request
            )));
        }
```
