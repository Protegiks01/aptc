# Audit Report

## Title
Unbounded Concurrent Requests DoS Attack on Fullnode Backup Service

## Summary
The `concurrent_data_requests` parameter in `GlobalBackupOpt` lacks validation, allowing an attacker to set arbitrarily high values (e.g., 10000) that overwhelm the fullnode backup service with concurrent HTTP requests and database operations, causing denial of service for legitimate backup operations and potentially degrading fullnode performance.

## Finding Description

The backup CLI allows users to specify the number of concurrent requests via the `--concurrent-data-requests` parameter, which is defined as an unbounded `usize` type with no validation. [1](#0-0) 

This parameter directly controls concurrency in the backup process through `try_buffered_x(concurrency * 2, concurrency)`, creating up to `concurrency` parallel HTTP requests to the backup service: [2](#0-1) [3](#0-2) 

The backup service itself has no rate limiting, connection pooling, or concurrency controls. It uses a basic warp HTTP server: [4](#0-3) 

Each concurrent request creates a `JellyfishMerkleIterator` and performs database I/O operations: [5](#0-4) [6](#0-5) 

In production deployments, the backup service is exposed on all interfaces (`0.0.0.0:6186`), not just localhost: [7](#0-6) 

**Attack Path:**
1. Attacker identifies a fullnode with exposed backup service on port 6186
2. Attacker runs: `aptos-db-tool backup oneoff --backup-service-address http://<target>:6186 --concurrent-data-requests 10000 state-snapshot --state-snapshot-epoch 1 --local-fs-dir /tmp/backup`
3. This creates 10,000 concurrent HTTP connections and database iterators
4. The fullnode's backup service becomes overwhelmed:
   - Excessive memory consumption from 10,000 iterators
   - Database I/O saturation from concurrent state tree traversals
   - HTTP connection exhaustion
5. Legitimate backup operations fail or timeout
6. Fullnode performance may degrade due to resource contention

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per Aptos bug bounty criteria because it causes:

1. **Denial of Service**: Legitimate backup operations become unavailable or significantly degraded
2. **Resource Exhaustion**: Can consume excessive CPU, memory, and I/O resources on the fullnode
3. **Potential Fullnode Impact**: Since the backup service shares resources with the fullnode, extreme resource exhaustion could affect normal node operations

While this doesn't directly compromise consensus safety or cause fund loss, it violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The backup service has no such limits.

## Likelihood Explanation

**Likelihood: High**

1. **Easy to Exploit**: Requires only CLI access and network connectivity to port 6186
2. **No Authentication**: The backup service has no authentication or authorization mechanisms
3. **Public Exposure**: Production fullnodes expose backup service on `0.0.0.0:6186`
4. **No Detection**: No rate limiting or monitoring to detect abuse
5. **Low Cost**: Attacker only needs to run a single CLI command

The only barrier is that port 6186 must be accessible, but since it's configured to bind to all interfaces in production deployments, this is commonly the case.

## Recommendation

Implement multiple layers of defense:

**1. Add validation to `GlobalBackupOpt`:**
```rust
#[derive(Clone, Parser)]
pub struct GlobalBackupOpt {
    #[clap(
        long,
        default_value_t = 8,
        help = "..."
    )]
    #[clap(validator = validate_concurrent_requests)]
    pub concurrent_data_requests: usize,
}

fn validate_concurrent_requests(v: &str) -> Result<(), String> {
    let val: usize = v.parse().map_err(|_| "Invalid number")?;
    if val > 100 {
        return Err(format!("concurrent_data_requests cannot exceed 100, got {}", val));
    }
    Ok(())
}
```

**2. Implement server-side rate limiting** using connection pools or semaphores in the backup service to limit concurrent request processing.

**3. Add authentication/authorization** to the backup service to prevent unauthorized access.

**4. Document security best practices** recommending operators keep backup service on localhost-only (`127.0.0.1:6186`) unless explicitly needed.

## Proof of Concept

```bash
# Terminal 1: Start a fullnode with backup service exposed
# (This would be a production fullnode configuration)

# Terminal 2: Launch DoS attack
aptos-db-tool backup oneoff \
  --backup-service-address http://<fullnode-ip>:6186 \
  --concurrent-data-requests 10000 \
  --max-chunk-size 134217728 \
  state-snapshot \
  --state-snapshot-epoch 1 \
  --local-fs-dir /tmp/attack_backup

# Observe:
# 1. 10,000 HTTP connections created to the backup service
# 2. Fullnode memory usage spikes
# 3. Database I/O saturates
# 4. Legitimate backup attempts fail or timeout
# 5. Monitor with: netstat -an | grep 6186 | wc -l (shows connection count)
```

To demonstrate the lack of validation:
```rust
// Test showing unbounded parameter acceptance
#[test]
fn test_unbounded_concurrent_requests() {
    let opt = GlobalBackupOpt {
        max_chunk_size: 134217728,
        concurrent_data_requests: 999999, // No validation prevents this
    };
    // This compiles and runs without error
    assert_eq!(opt.concurrent_data_requests, 999999);
}
```

## Notes

The vulnerability stems from a design assumption that only trusted operators would use the backup CLI against their own nodes. However, with production deployments exposing the backup service on all interfaces without authentication, this becomes an exploitable attack vector. The lack of resource limits violates defense-in-depth principles and the documented invariant requiring all operations to respect computational limits.

### Citations

**File:** storage/backup/backup-cli/src/utils/mod.rs (L49-65)
```rust
#[derive(Clone, Parser)]
pub struct GlobalBackupOpt {
    // Defaults to 128MB, so concurrent chunk downloads won't take up too much memory.
    #[clap(
        long = "max-chunk-size",
        default_value_t = 134217728,
        help = "Maximum chunk file size in bytes."
    )]
    pub max_chunk_size: usize,
    #[clap(
        long,
        default_value_t = 8,
        help = "When applicable (currently only for state snapshot backups), the number of \
        concurrent requests to the fullnode backup service. "
    )]
    pub concurrent_data_requests: usize,
}
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L242-242)
```rust
        let record_stream = Box::pin(self.record_stream(self.concurrent_data_requests).await?);
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L311-313)
```rust
        Ok(record_stream_stream
            .try_buffered_x(concurrency * 2, concurrency)
            .try_flatten())
```

**File:** storage/backup/backup-service/src/lib.rs (L26-26)
```rust
    let server = warp::serve(routes).bind(address);
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L72-79)
```rust
    let state_snapshot_chunk = warp::path!(Version / usize / usize)
        .map(move |version, start_idx, limit| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT_CHUNK, move |bh, sender| {
                bh.get_state_item_iter(version, start_idx, limit)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L145-162)
```rust
    pub fn get_state_item_iter(
        &self,
        version: Version,
        start_idx: usize,
        limit: usize,
    ) -> Result<impl Iterator<Item = Result<(StateKey, StateValue)>> + Send + use<>> {
        let iterator = self
            .state_store
            .get_state_key_and_value_iter(version, start_idx)?
            .take(limit)
            .enumerate()
            .map(move |(idx, res)| {
                BACKUP_STATE_SNAPSHOT_VERSION.set(version as i64);
                BACKUP_STATE_SNAPSHOT_LEAF_IDX.set((start_idx + idx) as i64);
                res
            });
        Ok(Box::new(iterator))
    }
```

**File:** terraform/helm/fullnode/files/fullnode-base.yaml (L68-68)
```yaml
  backup_service_address: "0.0.0.0:6186"
```
