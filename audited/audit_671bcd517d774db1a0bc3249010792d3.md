# Audit Report

## Title
Transient Network Failures Cause Permanent Message Loss and Node Hang in Remote Executor Communication

## Summary
The `send_message` function in the secure network layer panics on any gRPC error instead of implementing retry logic, causing permanent message loss and indefinite hangs in the remote sharded block executor. This affects node liveness when remote execution is configured.

## Finding Description

The vulnerability exists in the network communication layer used by the remote executor service. [1](#0-0) 

The `send_message` function explicitly acknowledges the missing retry logic with a TODO comment but instead implements a panic on any gRPC failure. This panic occurs within an async task spawned by the `OutboundHandler`. [2](#0-1) 

**Attack Flow:**

1. The `RemoteExecutorClient` is used to coordinate sharded block execution across remote nodes. [3](#0-2) 

2. During block execution, the coordinator sends execution commands to remote shards via crossbeam channels. [4](#0-3) 

3. These messages are processed by the `OutboundHandler::process_one_outgoing_message` loop, which calls `send_message` on the gRPC client. [5](#0-4) 

4. When any transient network failure occurs (timeout, connection reset, DNS failure, etc.), `send_message` panics, crashing the entire outbound handler task.

5. All subsequent messages in the channel are permanently lost.

6. The coordinator waits indefinitely for execution results from shards that will never arrive. [6](#0-5) 

7. The node cannot complete block execution, causing a liveness failure.

The remote executor is activated when remote addresses are configured in the execution pipeline. [7](#0-6) 

## Impact Explanation

This vulnerability meets **HIGH severity** criteria per the Aptos bug bounty program:

- **Validator node slowdowns**: When remote execution is configured and transient network failures occur, the node hangs indefinitely waiting for results, preventing block execution progress.

- **Significant protocol violations**: The system violates its liveness guarantee by permanently failing to complete block execution due to unrecoverable message loss.

- **Availability impact**: Affected nodes cannot participate in block execution, reducing the validator set's effective capacity.

The impact is severe because:
- No recovery mechanism exists - the outbound task remains crashed
- Message loss is permanent - there's no retry or redelivery
- The issue cascades - one network failure breaks all subsequent executions
- Transient failures become permanent failures

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability is highly likely to occur in production because:

1. **Common trigger conditions**: Network timeouts, connection resets, DNS failures, TLS handshake failures, and temporary service unavailability are routine in distributed systems.

2. **No attacker required**: Natural network conditions trigger this vulnerability without any malicious action.

3. **No complexity**: Simply requires a transient network issue during message transmission.

4. **Production usage**: The remote executor is designed for production use when scaling execution across multiple machines.

5. **Long-running operations**: Block execution involves multiple network round-trips, increasing exposure to transient failures.

6. **No mitigation**: There is no automatic recovery, health check, or task restart mechanism.

## Recommendation

Implement the retry logic with exponential backoff as indicated by the TODO comment:

```rust
pub async fn send_message(
    &mut self,
    sender_addr: SocketAddr,
    message: Message,
    mt: &MessageType,
) {
    let request = tonic::Request::new(NetworkMessage {
        message: message.data,
        message_type: mt.get_type(),
    });
    
    // Implement retry with exponential backoff
    let max_retries = 5;
    let mut retry_delay = Duration::from_millis(100);
    
    for attempt in 0..max_retries {
        match self.remote_channel.simple_msg_exchange(request.clone()).await {
            Ok(_) => return,
            Err(e) => {
                if attempt == max_retries - 1 {
                    error!(
                        "Failed to send message to {} after {} retries: {}",
                        self.remote_addr, max_retries, e
                    );
                    // Consider returning an error instead of panicking
                    panic!(
                        "Error '{}' sending message to {} on node {:?}",
                        e, self.remote_addr, sender_addr
                    );
                }
                warn!(
                    "Retry {}/{} for sending message to {}: {}",
                    attempt + 1, max_retries, self.remote_addr, e
                );
                tokio::time::sleep(retry_delay).await;
                retry_delay *= 2; // Exponential backoff
            }
        }
    }
}
```

Additionally:
- Add health checks for the outbound handler task
- Implement task restart mechanisms on panic
- Add metrics to track retry attempts and failures
- Consider circuit breaker patterns for persistent failures

## Proof of Concept

```rust
// Reproduction test demonstrating the vulnerability
#[tokio::test]
async fn test_transient_network_failure_causes_hang() {
    use std::net::{IpAddr, Ipv4Addr, SocketAddr};
    use std::time::Duration;
    
    // Setup: Create a coordinator and a remote shard address
    let coordinator_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 52200);
    // Use an address that will fail (e.g., non-existent port)
    let bad_shard_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 1);
    
    // Create a RemoteExecutorClient with the bad shard address
    let mut controller = NetworkController::new(
        "test-coordinator".to_string(),
        coordinator_addr,
        1000, // 1 second timeout
    );
    
    // Register outbound channel to bad address
    let message_tx = controller.create_outbound_channel(
        bad_shard_addr,
        "execute_command_0".to_string(),
    );
    
    controller.start();
    
    // Simulate sending an execution command
    let test_message = Message::new(vec![1, 2, 3, 4]);
    
    // This will cause the outbound handler to panic when it tries to send
    message_tx.send(test_message).unwrap();
    
    // The outbound handler will crash, and this message will be lost
    // In production, the RemoteExecutorClient would wait indefinitely for a result
    
    // Wait to observe the panic
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // Subsequent messages will never be delivered because the outbound task crashed
    let another_message = Message::new(vec![5, 6, 7, 8]);
    message_tx.send(another_message).unwrap();
    
    // This demonstrates permanent message loss - messages queue up but are never sent
    tokio::time::sleep(Duration::from_secs(1)).await;
}
```

**To reproduce in a real environment:**
1. Configure remote executor addresses in the execution pipeline
2. Start the coordinator and remote shards
3. Temporarily disconnect network between coordinator and one shard (e.g., using `iptables` or network namespace manipulation)
4. Trigger block execution
5. Observe the panic in logs and indefinite hang in block execution
6. The node becomes unable to execute blocks even after network is restored

## Notes

This vulnerability is particularly concerning because:

1. **Silent failure mode**: The panic occurs in a background async task, making it difficult to detect and diagnose in production.

2. **Cascading failures**: Once the outbound handler crashes, all subsequent blocks requiring remote execution will fail.

3. **No automatic recovery**: Manual node restart is required to restore functionality.

4. **Design flaw acknowledgment**: The TODO comment shows this was a known design gap, but the chosen implementation (panic) is worse than doing nothing.

The issue affects the remote executor service used for distributed block execution, which is critical for scaling Aptos nodes horizontally.

### Citations

**File:** secure/net/src/grpc_network_service/mod.rs (L140-160)
```rust
    pub async fn send_message(
        &mut self,
        sender_addr: SocketAddr,
        message: Message,
        mt: &MessageType,
    ) {
        let request = tonic::Request::new(NetworkMessage {
            message: message.data,
            message_type: mt.get_type(),
        });
        // TODO: Retry with exponential backoff on failures
        match self.remote_channel.simple_msg_exchange(request).await {
            Ok(_) => {},
            Err(e) => {
                panic!(
                    "Error '{}' sending message to {} on node {:?}",
                    e, self.remote_addr, sender_addr
                );
            },
        }
    }
```

**File:** secure/net/src/network_controller/outbound_handler.rs (L89-99)
```rust
        rt.spawn(async move {
            info!("Starting outbound handler at {}", address.to_string());
            Self::process_one_outgoing_message(
                outbound_handlers,
                &address,
                inbound_handler.clone(),
                &mut grpc_clients,
            )
            .await;
            info!("Stopping outbound handler at {}", address.to_string());
        });
```

**File:** secure/net/src/network_controller/outbound_handler.rs (L155-160)
```rust
                grpc_clients
                    .get_mut(remote_addr)
                    .unwrap()
                    .send_message(*socket_addr, msg, message_type)
                    .await;
            }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L180-212)
```rust
    fn execute_block(
        &self,
        state_view: Arc<S>,
        transactions: PartitionedTransactions,
        concurrency_level_per_shard: usize,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<ShardedExecutionOutput, VMStatus> {
        trace!("RemoteExecutorClient Sending block to shards");
        self.state_view_service.set_state_view(state_view);
        let (sub_blocks, global_txns) = transactions.into();
        if !global_txns.is_empty() {
            panic!("Global transactions are not supported yet");
        }
        for (shard_id, sub_blocks) in sub_blocks.into_iter().enumerate() {
            let senders = self.command_txs.clone();
            let execution_request = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
                sub_blocks,
                concurrency_level: concurrency_level_per_shard,
                onchain_config: onchain_config.clone(),
            });

            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }

        let execution_results = self.get_output_from_shards()?;

        self.state_view_service.drop_state_view();
        Ok(ShardedExecutionOutput::new(execution_results, vec![]))
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L261-267)
```rust
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
```
