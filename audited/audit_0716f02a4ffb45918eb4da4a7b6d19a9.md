# Audit Report

## Title
Non-Atomic Commit of TransactionInfo and PersistedAuxiliaryInfo Breaks Storage Invariant

## Summary
The parallel commit of `TransactionInfo` and `PersistedAuxiliaryInfo` in separate, non-atomic database writes can result in a state where `TransactionInfo.auxiliary_info_hash` references non-existent `PersistedAuxiliaryInfo` data, violating the cryptographic commitment invariant. This occurs because crash recovery does not truncate orphaned `TransactionInfo` entries.

## Finding Description
The Aptos storage layer maintains a critical invariant: if `TransactionInfo.auxiliary_info_hash` is set to `Some(hash)`, then the corresponding `PersistedAuxiliaryInfo` at that version must exist in storage and hash to that value.

This invariant is violated due to non-atomic writes during transaction commit. In the commit flow, two separate parallel tasks write to different databases: [1](#0-0) 

These writes use separate `SchemaBatch` instances that are committed independently: [2](#0-1) [3](#0-2) 

When storage sharding is enabled, these write to completely separate RocksDB instances. Even without sharding, they use different column families in separate batches, making them non-atomic.

The developers acknowledge this issue with an explicit TODO comment: [4](#0-3) 

The critical failure is in crash recovery. During startup, `sync_commit_progress` truncates databases to the last committed version: [5](#0-4) 

However, the truncation logic deletes `TransactionInfoSchema` but **NOT** `PersistedAuxiliaryInfoSchema`: [6](#0-5) 

Confirming this, no reference to `PersistedAuxiliaryInfo` exists in the truncation helper: [7](#0-6) 

When `PersistedAuxiliaryInfo` is missing, reads default to `PersistedAuxiliaryInfo::None`: [8](#0-7) 

The verification function would detect this mismatch: [9](#0-8) 

But it's only called during state sync and backup/restore operations, not during normal operation or startup recovery.

## Impact Explanation
This is a **HIGH severity** vulnerability per Aptos bug bounty criteria for "Significant protocol violations" and "State inconsistencies requiring intervention."

**Specific Impacts:**
1. **State Consistency Violation**: Breaks the fundamental invariant that `TransactionInfo` cryptographically commits to auxiliary data
2. **State Sync Failure**: Nodes attempting to sync from an affected validator will fail verification when the mismatch is detected
3. **Backup/Restore Failure**: Backup operations will fail when verifying auxiliary info consistency
4. **Silent Data Loss**: API queries return `PersistedAuxiliaryInfo::None` despite the hash indicating data should exist
5. **Transaction Accumulator Corruption**: Since `TransactionInfo` is part of the Merkle accumulator, the inconsistency compromises the integrity of the committed state

## Likelihood Explanation
**HIGH likelihood** - This can occur in common operational scenarios:

1. **System Crashes**: Power failures, kernel panics, or OS crashes during block commit
2. **Disk Failures**: I/O errors causing one write to fail while the other succeeds  
3. **Resource Exhaustion**: Out-of-disk-space errors affecting one database but not the other
4. **Process Termination**: SIGKILL or OOM-killer during commit window

The parallel execution with `.unwrap()` error handling means either commit succeeding and the other panicking leaves inconsistent state. The commit window spans the entire parallel scope execution, creating a realistic attack surface.

## Recommendation
Implement atomic commit across both schemas:

1. **Short-term Fix**: Add `PersistedAuxiliaryInfo` truncation to recovery:

```rust
// In delete_per_version_data()
delete_per_version_data_impl::<PersistedAuxiliaryInfoSchema>(
    ledger_db.persisted_auxiliary_info_db_raw(),
    start_version,
    &mut batch.persisted_auxiliary_info_db_batches,
)?;
```

2. **Long-term Fix**: Implement write progress tracking per database as mentioned in the TODO, and ensure all writes complete before updating overall commit progress. Use a single atomic batch for related writes when possible.

3. **Validation**: Add startup consistency check to detect and report (or auto-repair) auxiliary info mismatches.

## Proof of Concept

```rust
// Reproduction steps for controlled environment
use std::sync::Arc;
use aptos_storage_interface::DbWriter;

#[test]
fn test_auxiliary_info_inconsistency_on_crash() {
    // 1. Setup AptosDB with sharding enabled
    let db = AptosDB::new_for_test_with_sharding(temp_dir, true);
    
    // 2. Prepare chunk with transactions having auxiliary info
    let chunk = create_test_chunk_with_auxiliary_info();
    
    // 3. Begin commit
    db.pre_commit_ledger(chunk, false).unwrap();
    
    // 4. Simulate crash by force-killing one of the parallel writes
    //    (In practice, this happens via SIGKILL, power loss, etc.)
    //    The parallel scope at line 271-319 has no transaction guarantee
    
    // 5. Restart database
    drop(db);
    let db = AptosDB::open(temp_dir, rocksdb_config).unwrap();
    
    // 6. Verify inconsistency
    let version = chunk.first_version;
    let txn_info = db.get_transaction_info(version).unwrap();
    let aux_info = db.get_persisted_auxiliary_info_by_version(version).unwrap();
    
    // BUG: txn_info.auxiliary_info_hash() is Some(hash)
    // but aux_info is None, violating the invariant
    assert!(txn_info.auxiliary_info_hash().is_some());
    assert!(matches!(aux_info, PersistedAuxiliaryInfo::None));
}
```

To trigger in production: Force a system crash (power failure, SIGKILL) during the commit window when `calculate_and_commit_ledger_and_state_kv` is executing. Recovery will leave inconsistent state detectable via state sync or backup verification failures.

**Notes:**
- The vulnerability is confirmed by explicit TODO comments acknowledging the issue
- No current mechanism prevents or detects this inconsistency at startup  
- The missing truncation of `PersistedAuxiliaryInfo` in recovery guarantees persistence of the inconsistency
- Verification only occurs in specific sync/backup paths, not during normal operation

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L272-275)
```rust
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L300-313)
```rust
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L500-520)
```rust
    pub(super) fn commit_transaction_infos(
        &self,
        first_version: Version,
        txn_infos: &[TransactionInfo],
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_transaction_infos"]);

        let mut batch = SchemaBatch::new();
        txn_infos
            .iter()
            .enumerate()
            .try_for_each(|(i, txn_info)| -> Result<()> {
                let version = first_version + i as u64;
                TransactionInfoDb::put_transaction_info(version, txn_info, &mut batch)?;

                Ok(())
            })?;

        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_transaction_infos___commit"]);
        self.ledger_db.transaction_info_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/ledger_db/persisted_auxiliary_info_db.rs (L91-110)
```rust
    pub(crate) fn commit_auxiliary_info(
        &self,
        first_version: Version,
        persisted_auxiliary_info: &[PersistedAuxiliaryInfo],
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_auxiliary_info"]);

        let mut batch = SchemaBatch::new();
        persisted_auxiliary_info.iter().enumerate().try_for_each(
            |(i, aux_info)| -> Result<()> {
                let version = first_version + i as u64;
                Self::put_persisted_auxiliary_info(version, aux_info, &mut batch)
            },
        )?;

        {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_auxiliary_info___commit"]);
            self.write_schemas(batch)
        }
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-450)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L1-550)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

#![allow(dead_code)]

use crate::{
    ledger_db::{
        ledger_metadata_db::LedgerMetadataDb, transaction_db::TransactionDb, LedgerDb,
        LedgerDbSchemaBatches,
    },
    schema::{
        db_metadata::{DbMetadataKey, DbMetadataSchema, DbMetadataValue},
        epoch_by_version::EpochByVersionSchema,
        jellyfish_merkle_node::JellyfishMerkleNodeSchema,
        ledger_info::LedgerInfoSchema,
        stale_node_index::StaleNodeIndexSchema,
        stale_node_index_cross_epoch::StaleNodeIndexCrossEpochSchema,
        stale_state_value_index::StaleStateValueIndexSchema,
        stale_state_value_index_by_key_hash::StaleStateValueIndexByKeyHashSchema,
        state_value::StateValueSchema,
        state_value_by_key_hash::StateValueByKeyHashSchema,
        transaction::TransactionSchema,
        transaction_accumulator::TransactionAccumulatorSchema,
        transaction_accumulator_root_hash::TransactionAccumulatorRootHashSchema,
        transaction_info::TransactionInfoSchema,
        transaction_summaries_by_account::TransactionSummariesByAccountSchema,
        version_data::VersionDataSchema,
        write_set::WriteSetSchema,
    },
    state_kv_db::StateKvDb,
    state_merkle_db::StateMerkleDb,
    state_store::MAX_COMMIT_PROGRESS_DIFFERENCE,
    transaction_store::TransactionStore,
    utils::get_progress,
};
use aptos_crypto::hash::CryptoHash;
use aptos_jellyfish_merkle::{node_type::NodeKey, StaleNodeIndex};
use aptos_logger::info;
use aptos_schemadb::{
    batch::SchemaBatch,
    schema::{Schema, SeekKeyCodec},
    DB,
};
use aptos_storage_interface::Result;
use aptos_types::{proof::position::Position, transaction::Version};
use claims::assert_ge;
use rayon::prelude::*;
use status_line::StatusLine;
use std::{
    fmt::{Display, Formatter},
    sync::{
        atomic::{AtomicU64, Ordering},
        Arc,
    },
};

pub(crate) fn get_state_kv_commit_progress(state_kv_db: &StateKvDb) -> Result<Option<Version>> {
    get_progress(
        state_kv_db.metadata_db(),
        &DbMetadataKey::StateKvCommitProgress,
    )
}

pub(crate) fn get_state_merkle_commit_progress(
    state_merkle_db: &StateMerkleDb,
) -> Result<Option<Version>> {
    get_progress(
        state_merkle_db.metadata_db(),
        &DbMetadataKey::StateMerkleCommitProgress,
    )
}

pub(crate) fn truncate_ledger_db(ledger_db: Arc<LedgerDb>, target_version: Version) -> Result<()> {
    let transaction_store = TransactionStore::new(Arc::clone(&ledger_db));

    let start_version = target_version + 1;
    truncate_ledger_db_single_batch(&ledger_db, &transaction_store, start_version)?;
    Ok(())
}

pub(crate) fn truncate_state_kv_db(
    state_kv_db: &StateKvDb,
    current_version: Version,
    target_version: Version,
    batch_size: usize,
) -> Result<()> {
    assert!(batch_size > 0);
    let status = StatusLine::new(Progress::new("Truncating State KV DB", target_version));
    status.set_current_version(current_version);

    let mut current_version = current_version;
    // current_version can be the same with target_version while there is data written to the db before
    // the progress is recorded -- we need to run the truncate for at least one batch
    loop {
        let target_version_for_this_batch = std::cmp::max(
            current_version.saturating_sub(batch_size as Version),
            target_version,
        );
        // By writing the progress first, we still maintain that it is less than or equal to the
        // actual progress per shard, even if it dies in the middle of truncation.
        state_kv_db.write_progress(target_version_for_this_batch)?;
        // the first batch can actually delete more versions than the target batch size because
        // we calculate the start version of this batch assuming the latest data is at
        // `current_version`. Otherwise, we need to seek all shards to determine the
        // actual latest version of data.
        truncate_state_kv_db_shards(state_kv_db, target_version_for_this_batch)?;
        current_version = target_version_for_this_batch;
        status.set_current_version(current_version);

        if current_version <= target_version {
            break;
        }
    }
    assert_eq!(current_version, target_version);
    Ok(())
}

pub(crate) fn truncate_state_kv_db_shards(
    state_kv_db: &StateKvDb,
    target_version: Version,
) -> Result<()> {
    (0..state_kv_db.hack_num_real_shards())
        .into_par_iter()
        .try_for_each(|shard_id| {
            truncate_state_kv_db_single_shard(state_kv_db, shard_id, target_version)
        })
}

pub(crate) fn truncate_state_kv_db_single_shard(
    state_kv_db: &StateKvDb,
    shard_id: usize,
    target_version: Version,
) -> Result<()> {
    let mut batch = SchemaBatch::new();
    delete_state_value_and_index(
        state_kv_db.db_shard(shard_id),
        target_version + 1,
        &mut batch,
        state_kv_db.enabled_sharding(),
    )?;
    state_kv_db.commit_single_shard(target_version, shard_id, batch)
}

pub(crate) fn truncate_state_merkle_db(
    state_merkle_db: &StateMerkleDb,
    target_version: Version,
) -> Result<()> {
    let status = StatusLine::new(Progress::new("Truncating State Merkle DB.", target_version));

    loop {
        let current_version = get_current_version_in_state_merkle_db(state_merkle_db)?
            .expect("Current version of state merkle db must exist.");
        status.set_current_version(current_version);
        assert_ge!(current_version, target_version);
        if current_version == target_version {
            break;
        }

        let version_before = find_closest_node_version_at_or_before(
            state_merkle_db.metadata_db(),
            current_version - 1,
        )?
        .expect("Must exist.");

        let mut top_levels_batch = SchemaBatch::new();

        delete_nodes_and_stale_indices_at_or_after_version(
            state_merkle_db.metadata_db(),
            current_version,
            None, // shard_id
            &mut top_levels_batch,
        )?;

        state_merkle_db.commit_top_levels(version_before, top_levels_batch)?;

        truncate_state_merkle_db_shards(state_merkle_db, version_before)?;
    }

    Ok(())
}

pub(crate) fn truncate_state_merkle_db_shards(
    state_merkle_db: &StateMerkleDb,
    target_version: Version,
) -> Result<()> {
    (0..state_merkle_db.hack_num_real_shards())
        .into_par_iter()
        .try_for_each(|shard_id| {
            truncate_state_merkle_db_single_shard(state_merkle_db, shard_id, target_version)
        })
}

pub(crate) fn truncate_state_merkle_db_single_shard(
    state_merkle_db: &StateMerkleDb,
    shard_id: usize,
    target_version: Version,
) -> Result<()> {
    let mut batch = SchemaBatch::new();
    delete_nodes_and_stale_indices_at_or_after_version(
        state_merkle_db.db_shard(shard_id),
        target_version + 1,
        Some(shard_id),
        &mut batch,
    )?;
    state_merkle_db.db_shard(shard_id).write_schemas(batch)
}

pub(crate) fn find_tree_root_at_or_before(
    ledger_metadata_db: &LedgerMetadataDb,
    state_merkle_db: &StateMerkleDb,
    version: Version,
) -> Result<Option<Version>> {
    if let Some(closest_version) =
        find_closest_node_version_at_or_before(state_merkle_db.metadata_db(), version)?
    {
        if root_exists_at_version(state_merkle_db, closest_version)? {
            return Ok(Some(closest_version));
        }

        // It's possible that it's a partial commit when sharding is not enabled,
        // look again for the previous version:
        if version == 0 {
            return Ok(None);
        }
        if let Some(closest_version) =
            find_closest_node_version_at_or_before(state_merkle_db.metadata_db(), version - 1)?
        {
            if root_exists_at_version(state_merkle_db, closest_version)? {
                return Ok(Some(closest_version));
            }

            // Now we are probably looking at a pruned version in this epoch, look for the previous
            // epoch ending:
            let mut iter = ledger_metadata_db.db().iter::<EpochByVersionSchema>()?;
            iter.seek_for_prev(&version)?;
            if let Some((closest_epoch_version, _)) = iter.next().transpose()? {
                if root_exists_at_version(state_merkle_db, closest_epoch_version)? {
                    return Ok(Some(closest_epoch_version));
                }
            }
        }
    }

    Ok(None)
}

pub(crate) fn root_exists_at_version(
    state_merkle_db: &StateMerkleDb,
    version: Version,
) -> Result<bool> {
    Ok(state_merkle_db
        .metadata_db()
        .get::<JellyfishMerkleNodeSchema>(&NodeKey::new_empty_path(version))?
        .is_some())
}

pub(crate) fn get_current_version_in_state_merkle_db(
    state_merkle_db: &StateMerkleDb,
) -> Result<Option<Version>> {
    find_closest_node_version_at_or_before(state_merkle_db.metadata_db(), Version::MAX)
}

pub(crate) fn get_max_version_in_state_merkle_db(
    state_merkle_db: &StateMerkleDb,
) -> Result<Option<Version>> {
    let mut version = get_current_version_in_state_merkle_db(state_merkle_db)?;
    let num_real_shards = state_merkle_db.hack_num_real_shards();
    if num_real_shards > 1 {
        for shard_id in 0..num_real_shards {
            let shard_version = find_closest_node_version_at_or_before(
                state_merkle_db.db_shard(shard_id),
                Version::MAX,
            )?;
            if version.is_none() {
                version = shard_version;
            } else if let Some(shard_version) = shard_version {
                if shard_version > version.unwrap() {
                    version = Some(shard_version);
                }
            }
        }
    }
    Ok(version)
}

pub(crate) fn find_closest_node_version_at_or_before(
    db: &DB,
    version: Version,
) -> Result<Option<Version>> {
    let mut iter = db.rev_iter::<JellyfishMerkleNodeSchema>()?;
    iter.seek_for_prev(&NodeKey::new_empty_path(version))?;
    Ok(iter.next().transpose()?.map(|item| item.0.version()))
}

pub(crate) fn num_frozen_nodes_in_accumulator(num_leaves: u64) -> u64 {
    2 * num_leaves - num_leaves.count_ones() as u64
}

fn truncate_transaction_accumulator(
    transaction_accumulator_db: &DB,
    start_version: Version,
    batch: &mut SchemaBatch,
) -> Result<()> {
    let mut iter = transaction_accumulator_db.iter::<TransactionAccumulatorSchema>()?;
    iter.seek_to_last();
    let (position, _) = iter.next().transpose()?.unwrap();
    let num_frozen_nodes = position.to_postorder_index() + 1;
    let num_frozen_nodes_after = num_frozen_nodes_in_accumulator(start_version);
    let mut num_nodes_to_delete = num_frozen_nodes - num_frozen_nodes_after;

    let start_position = Position::from_postorder_index(num_frozen_nodes_after)?;
    iter.seek(&start_position)?;

    for item in iter {
        let (position, _) = item?;
        batch.delete::<TransactionAccumulatorSchema>(&position)?;
        num_nodes_to_delete -= 1;
    }

    assert_eq!(num_nodes_to_delete, 0);

    Ok(())
}

fn truncate_ledger_db_single_batch(
    ledger_db: &LedgerDb,
    transaction_store: &TransactionStore,
    start_version: Version,
) -> Result<()> {
    let mut batch = LedgerDbSchemaBatches::new();

    delete_transaction_index_data(
        ledger_db,
        transaction_store,
        start_version,
        &mut batch.transaction_db_batches,
    )?;
    delete_per_epoch_data(
        &ledger_db.metadata_db_arc(),
        start_version,
        &mut batch.ledger_metadata_db_batches,
    )?;
    delete_per_version_data(ledger_db, start_version, &mut batch)?;

    delete_event_data(ledger_db, start_version, &mut batch.event_db_batches)?;

    truncate_transaction_accumulator(
        ledger_db.transaction_accumulator_db_raw(),
        start_version,
        &mut batch.transaction_accumulator_db_batches,
    )?;

    let mut progress_batch = SchemaBatch::new();
    progress_batch.put::<DbMetadataSchema>(
        &DbMetadataKey::LedgerCommitProgress,
        &DbMetadataValue::Version(start_version - 1),
    )?;
    ledger_db.metadata_db().write_schemas(progress_batch)?;

    ledger_db.write_schemas(batch)
}

fn delete_transaction_index_data(
    ledger_db: &LedgerDb,
    transaction_store: &TransactionStore,
    start_version: Version,
    batch: &mut SchemaBatch,
) -> Result<()> {
    let transactions = ledger_db
        .transaction_db()
        .get_transaction_iter(start_version, MAX_COMMIT_PROGRESS_DIFFERENCE as usize * 2)?
        .collect::<Result<Vec<_>>>()?;
    let num_txns = transactions.len();
    if num_txns > 0 {
        info!(
            start_version = start_version,
            latest_version = start_version + num_txns as u64 - 1,
            "Truncate transaction index data."
        );
        ledger_db
            .transaction_db()
            .prune_transaction_by_hash_indices(transactions.iter().map(|txn| txn.hash()), batch)?;

        let transactions = (start_version..=start_version + transactions.len() as u64 - 1)
            .zip(transactions)
            .collect::<Vec<_>>();
        transaction_store.prune_transaction_by_account(&transactions, batch)?;
        transaction_store.prune_transaction_summaries_by_account(&transactions, batch)?;
    }

    Ok(())
}

fn delete_per_epoch_data(
    ledger_db: &DB,
    start_version: Version,
    batch: &mut SchemaBatch,
) -> Result<()> {
    let mut iter = ledger_db.iter::<LedgerInfoSchema>()?;
    iter.seek_to_last();
    if let Some((epoch, ledger_info)) = iter.next().transpose()? {
        let version = ledger_info.commit_info().version();
        if version >= start_version {
            info!(
                version = version,
                epoch = epoch,
                "Truncate latest epoch data."
            );
            batch.delete::<LedgerInfoSchema>(&epoch)?;
        }
    }

    let mut iter = ledger_db.iter::<EpochByVersionSchema>()?;
    iter.seek(&start_version)?;

    for item in iter {
        let (version, epoch) = item?;
        info!(
            version = version,
            epoch = epoch,
            "Truncate epoch ending data."
        );
        batch.delete::<EpochByVersionSchema>(&version)?;
        batch.delete::<LedgerInfoSchema>(&epoch)?;
    }

    Ok(())
}

fn delete_per_version_data(
    ledger_db: &LedgerDb,
    start_version: Version,
    batch: &mut LedgerDbSchemaBatches,
) -> Result<()> {
    delete_per_version_data_impl::<TransactionAccumulatorRootHashSchema>(
        ledger_db.transaction_accumulator_db_raw(),
        start_version,
        &mut batch.transaction_accumulator_db_batches,
    )?;
    delete_per_version_data_impl::<TransactionInfoSchema>(
        ledger_db.transaction_info_db_raw(),
        start_version,
        &mut batch.transaction_info_db_batches,
    )?;
    delete_transactions_and_transaction_summary_data(
        ledger_db.transaction_db(),
        start_version,
        &mut batch.transaction_db_batches,
    )?;
    delete_per_version_data_impl::<VersionDataSchema>(
        &ledger_db.metadata_db_arc(),
        start_version,
        &mut batch.ledger_metadata_db_batches,
    )?;
    delete_per_version_data_impl::<WriteSetSchema>(
        ledger_db.write_set_db_raw(),
        start_version,
        &mut batch.write_set_db_batches,
    )?;

    Ok(())
}

fn delete_transactions_and_transaction_summary_data(
    transaction_db: &TransactionDb,
    start_version: Version,
    batch: &mut SchemaBatch,
) -> Result<()> {
    let mut iter = transaction_db.db().iter::<TransactionSchema>()?;
    iter.seek_to_last();
    if let Some((latest_version, _)) = iter.next().transpose()? {
        if latest_version >= start_version {
            info!(
                start_version = start_version,
                latest_version = latest_version,
                cf_name = TransactionSchema::COLUMN_FAMILY_NAME,
                "Truncate per version data."
            );
            for version in start_version..=latest_version {
                let transaction = transaction_db.get_transaction(version)?;
                batch.delete::<TransactionSchema>(&version)?;
                if let Some(signed_txn) = transaction.try_as_signed_user_txn() {
                    batch.delete::<TransactionSummariesByAccountSchema>(&(
                        signed_txn.sender(),
                        version,
                    ))?;
                }
            }
        }
    }
    Ok(())
}

fn delete_per_version_data_impl<S>(
    ledger_db: &DB,
    start_version: Version,
    batch: &mut SchemaBatch,
) -> Result<()>
where
    S: Schema<Key = Version>,
{
    let mut iter = ledger_db.iter::<S>()?;
    iter.seek_to_last();
    if let Some((latest_version, _)) = iter.next().transpose()? {
        if latest_version >= start_version {
            info!(
                start_version = start_version,
                latest_version = latest_version,
                cf_name = S::COLUMN_FAMILY_NAME,
                "Truncate per version data."
            );
            for version in start_version..=latest_version {
                batch.delete::<S>(&version)?;
            }
        }
    }
    Ok(())
}

fn delete_event_data(
    ledger_db: &LedgerDb,
    start_version: Version,
    batch: &mut SchemaBatch,
) -> Result<()> {
    if let Some(latest_version) = ledger_db.event_db().latest_version()? {
        if latest_version >= start_version {
            info!(
                start_version = start_version,
                latest_version = latest_version,
                "Truncate event data."
            );
            let num_events_per_version = ledger_db.event_db().prune_event_indices(
                start_version,
                latest_version + 1,
                // Assuming same data will be overwritten into indices, we don't bother to deal
                // with the existence or placement of indices
                // TODO: prune data from internal indices
                None,
            )?;
            ledger_db.event_db().prune_events(
                num_events_per_version,
                start_version,
                latest_version + 1,
                batch,
            )?;
        }
    }
    Ok(())
}

```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L117-122)
```rust
            Ok(self
                .ledger_db
                .persisted_auxiliary_info_db()
                .get_persisted_auxiliary_info(version)?
                .unwrap_or(PersistedAuxiliaryInfo::None))
        })
```

**File:** types/src/transaction/mod.rs (L2829-2837)
```rust
            match aux_info {
                PersistedAuxiliaryInfo::None
                | PersistedAuxiliaryInfo::TimestampNotYetAssignedV1 { .. } => {
                    ensure!(
                        txn_info.auxiliary_info_hash().is_none(),
                        "The transaction info has an auxiliary info hash: {:?}, \
                             but the persisted auxiliary info is None!",
                        txn_info.auxiliary_info_hash()
                    );
```
