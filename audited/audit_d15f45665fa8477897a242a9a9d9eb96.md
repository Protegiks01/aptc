# Audit Report

## Title
Unbounded Transaction Retry Mechanism Enables Sustained Bandwidth Exhaustion

## Summary
The mempool's transaction broadcast retry mechanism lacks limits on retry count and duration, allowing transactions to be repeatedly rebroadcast to peers with full mempools until transaction expiration, wasting network bandwidth and potentially degrading validator performance.

## Finding Description

The mempool network interface maintains a `retry_messages` set in `BroadcastInfo` to track broadcasts that received a retry signal from peers. When a peer's mempool is full, it responds with `MempoolStatusCode::MempoolIsFull`, causing the sender to add the message to `retry_messages` for later rebroadcast. [1](#0-0) [2](#0-1) 

The critical flaw is that there are no limits on:
1. **Retry count**: A message can be retried unlimited times
2. **Retry duration**: Messages remain in `retry_messages` until transactions expire or are committed
3. **Set size**: The `retry_messages` set can grow unbounded

The only cleanup mechanism filters out messages whose transactions have been committed, but valid uncommitted transactions remain in retry indefinitely: [3](#0-2) 

**Attack Scenario:**

1. An attacker or network condition causes multiple peers' mempools to become persistently full (via DoS, flash traffic, or legitimate high load)
2. Node A broadcasts transaction batch with message_id X to peer B
3. Peer B's mempool is full → returns `MempoolIsFull` → `retry=true`
4. Node A adds X to `retry_messages`
5. After backoff interval (30 seconds), Node A rebroadcasts X
6. Peer B's mempool is still full → `retry=true` again
7. Steps 5-6 repeat until transactions expire (default 600 seconds system TTL)

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." Network bandwidth is consumed without proper bounds.

## Impact Explanation

**Medium Severity** - This vulnerability enables resource exhaustion attacks:

- **Bandwidth Waste**: With default configuration (300 tx/batch, ~90KB per broadcast, 30s backoff interval), a single peer with persistent mempool congestion causes ~180KB/minute waste. With 10 peers experiencing congestion, this reaches ~108MB/hour per node.

- **Network Degradation**: Under high transaction volumes or during DoS attacks, multiple nodes simultaneously retrying can saturate network links, degrading validator communication and potentially impacting consensus performance.

- **Amplification**: An attacker submitting transactions with maximum expiration times (far future timestamps) can maximize retry duration, and targeting multiple peers multiplies the bandwidth waste.

While not causing direct fund loss or state corruption, this violates resource management principles and can degrade network performance, qualifying as resource exhaustion warranting intervention.

## Likelihood Explanation

**High Likelihood** - This condition occurs naturally:

1. **Legitimate Triggers**: Network congestion, flash transaction surges, or mempool capacity limits can cause persistent `MempoolIsFull` conditions
2. **Malicious Triggers**: Attackers can deliberately fill target mempools via transaction spam
3. **No Privilege Required**: Any network peer experiencing mempool pressure triggers the issue
4. **Persistent Nature**: Once mempools fill during high load, they may remain full for extended periods, sustaining retry loops

The issue activates automatically during normal network stress without requiring specific attacker capabilities.

## Recommendation

Implement bounded retry logic with exponential backoff and maximum retry limits:

```rust
// In BroadcastInfo struct (types.rs)
pub struct BroadcastInfo {
    pub sent_messages: BTreeMap<MempoolMessageId, SystemTime>,
    pub retry_messages: BTreeMap<MempoolMessageId, RetryMetadata>, // Changed from BTreeSet
    pub backoff_mode: bool,
}

pub struct RetryMetadata {
    pub retry_count: u32,
    pub first_retry_time: SystemTime,
    pub last_retry_time: SystemTime,
}

// In MempoolConfig (mempool_config.rs)
pub struct MempoolConfig {
    // ... existing fields ...
    /// Maximum number of retry attempts per message
    pub max_retry_attempts: u32,  // Default: 5
    /// Maximum time a message can remain in retry queue
    pub max_retry_duration_secs: u64,  // Default: 300 (5 minutes)
}

// In determine_broadcast_batch (network.rs)
// Add retry limit checks:
if let Some(retry_metadata) = state.broadcast_info.retry_messages.get(&message_id) {
    // Check retry count limit
    if retry_metadata.retry_count >= self.mempool_config.max_retry_attempts {
        state.broadcast_info.retry_messages.remove(&message_id);
        continue;
    }
    
    // Check retry duration limit
    let retry_duration = SystemTime::now()
        .duration_since(retry_metadata.first_retry_time)
        .unwrap_or_default();
    if retry_duration.as_secs() >= self.mempool_config.max_retry_duration_secs {
        state.broadcast_info.retry_messages.remove(&message_id);
        continue;
    }
}
```

Additionally, implement exponential backoff to reduce retry frequency over time.

## Proof of Concept

```rust
// Add to mempool/src/tests/shared_mempool_test.rs

#[tokio::test]
async fn test_unbounded_retry_bandwidth_waste() {
    use crate::mocks::MockSharedMempool;
    use std::time::{SystemTime, Duration};
    
    // Setup: Create two nodes A (sender) and B (receiver with full mempool)
    let mut node_a = MockSharedMempool::new();
    let mut node_b = MockSharedMempool::new();
    
    // Fill node B's mempool to capacity
    for _ in 0..node_b.mempool_config.capacity {
        node_b.add_transaction(create_test_transaction());
    }
    
    // Node A broadcasts transactions to node B
    let message_id = create_test_message_id();
    let txns = create_test_transaction_batch(300);
    
    let start_time = SystemTime::now();
    let mut retry_count = 0;
    let mut total_bytes_sent = 0;
    
    // Simulate retry loop until transaction expiration (10 minutes)
    while SystemTime::now().duration_since(start_time).unwrap().as_secs() < 600 {
        // Broadcast attempt
        let result = node_a.broadcast_to_peer(node_b.peer_id(), message_id.clone(), txns.clone()).await;
        
        // Node B responds with MempoolIsFull
        assert!(matches!(result, BroadcastResult::MempoolFull));
        
        // Node A adds to retry_messages
        node_a.add_to_retry(message_id.clone());
        
        retry_count += 1;
        total_bytes_sent += estimate_batch_size(&txns);
        
        // Wait for backoff interval
        tokio::time::sleep(Duration::from_secs(30)).await;
    }
    
    // Assert: Significant bandwidth waste occurred
    assert!(retry_count > 10, "Expected multiple retries");
    assert!(total_bytes_sent > 1_000_000, "Expected >1MB bandwidth waste");
    
    // Assert: No limit prevented unbounded retries
    let retry_messages = node_a.get_retry_messages(node_b.peer_id());
    assert!(retry_messages.contains(&message_id), "Message still in retry queue");
}
```

**Notes**

While transaction expiration provides an eventual upper bound (default 600 seconds), the lack of explicit retry limits allows sustained bandwidth waste during that window. The vulnerability is exacerbated when multiple peers experience congestion simultaneously or when attackers submit transactions with far-future expiration times. The recommended fix adds defense-in-depth by imposing retry count and duration limits independent of transaction expiration.

### Citations

**File:** mempool/src/shared_mempool/tasks.rs (L254-278)
```rust
fn gen_ack_response(
    message_id: MempoolMessageId,
    results: Vec<SubmissionStatusBundle>,
    peer: &PeerNetworkId,
) -> MempoolSyncMsg {
    let mut backoff_and_retry = false;
    for (_, (mempool_status, _)) in results.into_iter() {
        if mempool_status.code == MempoolStatusCode::MempoolIsFull {
            backoff_and_retry = true;
            break;
        }
    }

    update_ack_counter(
        peer,
        counters::SENT_LABEL,
        backoff_and_retry,
        backoff_and_retry,
    );
    MempoolSyncMsg::BroadcastTransactionsResponse {
        message_id,
        retry: backoff_and_retry,
        backoff: backoff_and_retry,
    }
}
```

**File:** mempool/src/shared_mempool/network.rs (L345-347)
```rust
        if retry {
            sync_state.broadcast_info.retry_messages.insert(message_id);
        }
```

**File:** mempool/src/shared_mempool/network.rs (L411-421)
```rust
        state.broadcast_info.retry_messages = state
            .broadcast_info
            .retry_messages
            .clone()
            .into_iter()
            .filter(|message_id| {
                !mempool
                    .timeline_range_of_message(message_id.decode())
                    .is_empty()
            })
            .collect::<BTreeSet<MempoolMessageId>>();
```
