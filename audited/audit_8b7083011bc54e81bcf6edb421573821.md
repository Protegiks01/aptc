# Audit Report

## Title
Single-Validator Network Indefinite Liveness Failure on Key Mismatch

## Summary
In a single-validator network, when the on-chain validator set does not include the running node's validator key (due to key mismatch or misconfiguration), the node continues running but enters an unrecoverable liveness failure state. The node indefinitely attempts to reach consensus but all self-generated votes are rejected as `UnknownAuthor`, preventing any blocks from being committed. There is no special handling or fail-fast behavior for this single-validator edge case.

## Finding Description

The vulnerability manifests through the following execution flow:

1. **Epoch Initialization with Key Mismatch**: During epoch transition, the `start_new_epoch` function creates a `ValidatorVerifier` from the on-chain `ValidatorSet`. [1](#0-0) 

2. **Permissive Key Loading**: The `load_consensus_key` function checks if the node's author exists in the validator set. When the author is not found (None case), instead of failing, it logs a warning and loads the default consensus key, returning `Ok`. [2](#0-1) 

3. **Consensus Start with Mismatched Key**: The node proceeds to initialize consensus and starts proposing blocks and voting, signing all messages with its local private key. [3](#0-2) 

4. **Vote Rejection**: When the node receives its own votes, the `insert_vote` function checks the author's voting power in the validator verifier. Since the author is not in the validator set, `get_voting_power` returns `None`. [4](#0-3) 

5. **Unknown Author Result**: The vote is rejected with `VoteReceptionResult::UnknownAuthor`, which is then converted to an error in `process_vote_reception_result`. [5](#0-4) 

6. **Indefinite Liveness Failure**: Since the node cannot accumulate any voting power (always 0) and the required quorum is 1, it can never form a QuorumCertificate. The node waits indefinitely for votes that will never count, and even timeout votes face the same rejection, leaving the network permanently halted.

This breaks the **Consensus Liveness** invariant - the network should either make progress or fail fast, not enter an unrecoverable state where it appears operational but cannot commit any blocks.

## Impact Explanation

This is a **Medium severity** vulnerability under the Aptos bug bounty program, qualifying as "State inconsistencies requiring intervention."

**Impact Scope:**
- **Total liveness loss** for single-validator networks (devnet, testnet, or recovery scenarios)
- **No block commits**: Network cannot process any transactions
- **No automatic recovery**: Requires manual intervention to update the on-chain validator set or fix key configuration
- **Funds effectively frozen**: Users cannot perform transfers or interact with smart contracts during the outage

**Affected Deployments:**
- Development networks with single validators
- Test networks during initialization or recovery
- Emergency recovery scenarios after network partition
- Misconfigured validator nodes during key rotation

While single-validator production networks are uncommon, the lack of proper error handling creates operational risks and violates the fail-fast principle that safety-critical systems should follow.

## Likelihood Explanation

**Likelihood: Medium**

This issue can occur through several realistic scenarios:

1. **Governance Attack**: An attacker with sufficient governance voting power could propose changing the active validator set to exclude or replace the current validator's key, causing immediate liveness failure in single-validator scenarios.

2. **Operator Misconfiguration**: During network setup, genesis file initialization, or key rotation, operators may inadvertently create a mismatch between on-chain validator keys and local node configuration.

3. **State Sync Issues**: If a node syncs state from an incompatible chain or corrupted snapshot, the validator set may not match the local configuration.

4. **Recovery Scenarios**: After network partitions or failures requiring single-validator operation, key mismatches can occur during restoration.

The issue is **not** immediately exploitable by external attackers without governance power, but represents a critical operational vulnerability for single-validator deployments.

## Recommendation

Implement fail-fast behavior in the `load_consensus_key` function to detect and reject key mismatches for single-validator networks:

```rust
fn load_consensus_key(&self, vv: &ValidatorVerifier) -> anyhow::Result<PrivateKey> {
    match vv.get_public_key(&self.author) {
        Some(pk) => self
            .key_storage
            .consensus_sk_by_pk(pk)
            .map_err(|e| anyhow!("could not find sk by pk: {:?}", e)),
        None => {
            // For single-validator networks, this is fatal - fail immediately
            if vv.len() == 1 {
                bail!(
                    "FATAL: Single-validator network but validator {} not in validator set. \
                    Expected validator: {:?}. This indicates a critical configuration mismatch.",
                    self.author,
                    vv.get_ordered_account_addresses()
                );
            }
            warn!("could not find my pk in validator set, loading default sk!");
            self.key_storage
                .default_consensus_sk()
                .map_err(|e| anyhow!("could not load default sk: {e}"))
        },
    }
}
```

Additionally, enhance the `find_key_mismatches` function in the discovery layer to block epoch transitions when critical mismatches are detected: [6](#0-5) 

## Proof of Concept

The following Rust integration test demonstrates the vulnerability:

```rust
#[tokio::test]
async fn test_single_validator_key_mismatch_liveness_failure() {
    use aptos_types::validator_verifier::{ValidatorVerifier, ValidatorConsensusInfo};
    use aptos_crypto::bls12381::PrivateKey;
    use aptos_types::account_address::AccountAddress;
    
    // Setup: Create a single-validator network
    let validator_a = AccountAddress::random();
    let key_a = PrivateKey::generate_for_testing();
    let pk_a = key_a.public_key();
    
    // Simulate key mismatch: on-chain has different validator
    let validator_b = AccountAddress::random();
    let key_b = PrivateKey::generate_for_testing();
    let pk_b = key_b.public_key();
    
    // ValidatorVerifier has validator_b with pk_b
    let validator_info = ValidatorConsensusInfo::new(validator_b, pk_b, 1);
    let verifier = ValidatorVerifier::new(vec![validator_info]);
    
    // Node is configured as validator_a with key_a
    // This simulates the key mismatch scenario
    
    // When validator_a tries to participate:
    assert_eq!(verifier.get_public_key(&validator_a), None);
    assert_eq!(verifier.get_voting_power(&validator_a), None);
    
    // Quorum voting power is 1 (single validator)
    assert_eq!(verifier.quorum_voting_power(), 1);
    assert_eq!(verifier.total_voting_power(), 1);
    
    // Validator A cannot contribute to quorum (voting power = 0)
    // Network will be stuck indefinitely trying to reach quorum = 1
    
    println!("Single-validator network with key mismatch:");
    println!("- Required quorum: {}", verifier.quorum_voting_power());
    println!("- Validator A voting power: {:?}", verifier.get_voting_power(&validator_a));
    println!("- Result: Indefinite liveness failure - cannot form QC");
}
```

To observe the full liveness failure in a running consensus system, configure a single-validator node with a genesis file containing a different validator key, then observe that the node continues running but never commits blocks, with logs showing repeated `UnknownAuthor` vote rejections.

### Citations

**File:** consensus/src/epoch_manager.rs (L1164-1174)
```rust
    async fn start_new_epoch(&mut self, payload: OnChainConfigPayload<P>) {
        let validator_set: ValidatorSet = payload
            .get()
            .expect("failed to get ValidatorSet from payload");
        let mut verifier: ValidatorVerifier = (&validator_set).into();
        verifier.set_optimistic_sig_verification_flag(self.config.optimistic_sig_verification);

        let epoch_state = Arc::new(EpochState {
            epoch: payload.epoch(),
            verifier: verifier.into(),
        });
```

**File:** consensus/src/epoch_manager.rs (L1228-1233)
```rust
        let loaded_consensus_key = match self.load_consensus_key(&epoch_state.verifier) {
            Ok(k) => Arc::new(k),
            Err(e) => {
                panic!("load_consensus_key failed: {e}");
            },
        };
```

**File:** consensus/src/epoch_manager.rs (L1971-1983)
```rust
    fn load_consensus_key(&self, vv: &ValidatorVerifier) -> anyhow::Result<PrivateKey> {
        match vv.get_public_key(&self.author) {
            Some(pk) => self
                .key_storage
                .consensus_sk_by_pk(pk)
                .map_err(|e| anyhow!("could not find sk by pk: {:?}", e)),
            None => {
                warn!("could not find my pk in validator set, loading default sk!");
                self.key_storage
                    .default_consensus_sk()
                    .map_err(|e| anyhow!("could not load default sk: {e}"))
            },
        }
```

**File:** consensus/src/pending_votes.rs (L331-336)
```rust
        let validator_voting_power = validator_verifier.get_voting_power(&vote.author());

        if validator_voting_power.is_none() {
            warn!("Received vote from an unknown author: {}", vote.author());
            return VoteReceptionResult::UnknownAuthor(vote.author());
        }
```

**File:** consensus/src/round_manager.rs (L1774-1830)
```rust
    async fn process_vote_reception_result(
        &mut self,
        vote: &Vote,
        result: VoteReceptionResult,
    ) -> anyhow::Result<()> {
        let round = vote.vote_data().proposed().round();
        match result {
            VoteReceptionResult::NewQuorumCertificate(qc) => {
                if !vote.is_timeout() {
                    observe_block(
                        qc.certified_block().timestamp_usecs(),
                        BlockStage::QC_AGGREGATED,
                    );
                }
                QC_AGGREGATED_FROM_VOTES.inc();
                self.new_qc_aggregated(qc.clone(), vote.author())
                    .await
                    .context(format!(
                        "[RoundManager] Unable to process the created QC {:?}",
                        qc
                    ))?;
                if self.onchain_config.order_vote_enabled() {
                    // This check is already done in safety rules. As printing the "failed to broadcast order vote"
                    // in humio logs could sometimes look scary, we are doing the same check again here.
                    if let Some(last_sent_vote) = self.round_state.vote_sent() {
                        if let Some((two_chain_timeout, _)) = last_sent_vote.two_chain_timeout() {
                            if round <= two_chain_timeout.round() {
                                return Ok(());
                            }
                        }
                    }
                    // Broadcast order vote if the QC is successfully aggregated
                    // Even if broadcast order vote fails, the function will return Ok
                    if let Err(e) = self.broadcast_order_vote(vote, qc.clone()).await {
                        warn!(
                            "Failed to broadcast order vote for QC {:?}. Error: {:?}",
                            qc, e
                        );
                    } else {
                        self.broadcast_fast_shares(qc.certified_block()).await;
                    }
                }
                Ok(())
            },
            VoteReceptionResult::New2ChainTimeoutCertificate(tc) => {
                self.new_2chain_tc_aggregated(tc).await
            },
            VoteReceptionResult::EchoTimeout(_) if !self.round_state.is_timeout_sent() => {
                self.process_local_timeout(round).await
            },
            VoteReceptionResult::VoteAdded(_) => {
                PROPOSAL_VOTE_ADDED.inc();
                Ok(())
            },
            VoteReceptionResult::EchoTimeout(_) | VoteReceptionResult::DuplicateVote => Ok(()),
            e => Err(anyhow::anyhow!("{:?}", e)),
        }
```

**File:** network/discovery/src/validator_set.rs (L44-66)
```rust
    fn find_key_mismatches(&self, onchain_keys: Option<&HashSet<x25519::PublicKey>>) {
        let mismatch = onchain_keys.map_or(0, |pubkeys| {
            if !pubkeys.contains(&self.expected_pubkey) {
                error!(
                    NetworkSchema::new(&self.network_context),
                    "Onchain pubkey {:?} differs from local pubkey {}",
                    pubkeys,
                    self.expected_pubkey
                );
                1
            } else {
                0
            }
        });

        NETWORK_KEY_MISMATCH
            .with_label_values(&[
                self.network_context.role().as_str(),
                self.network_context.network_id().as_str(),
                self.network_context.peer_id().short_str().as_str(),
            ])
            .set(mismatch);
    }
```
