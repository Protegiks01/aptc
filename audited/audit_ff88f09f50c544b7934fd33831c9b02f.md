# Audit Report

## Title
Memory Leak in Batch Store Subscribe Mechanism Leading to Unbounded Memory Growth

## Summary
The `subscribe()` function in the batch store creates oneshot channels for batch persistence notifications but fails to clean up channel senders when batch requests fail, causing unbounded memory accumulation in the `persist_subscribers` map. [1](#0-0) 

## Finding Description

The vulnerability exists in the batch subscription mechanism used during consensus batch fetching. When a validator node needs to fetch a batch that doesn't exist locally, it calls `subscribe()` which creates a oneshot channel and adds the sender to the `persist_subscribers` DashMap. [2](#0-1) 

The race condition check attempts to notify subscribers if the batch already exists locally, but if the batch doesn't exist, no notification occurs at this point. [3](#0-2) 

Subscribers are only cleaned up in two scenarios:

1. **Successful notification** via `notify_subscribers()` when a batch is successfully persisted [4](#0-3) 

2. **Batch expiration** via `clear_expired_payload()` when a batch that exists in cache expires [5](#0-4) 

However, when batch fetching fails in `get_or_fetch_batch()`, the error propagates without persisting the batch, leaving the subscriber permanently leaked. [6](#0-5) 

The `request_batch()` function can fail due to timeouts, network errors, or expired batches, returning errors without any cleanup of subscribers. [7](#0-6) 

**Attack Scenario:**
1. Consensus requests a batch that doesn't exist locally
2. `subscribe(digest)` is called, adding a sender to `persist_subscribers`
3. `request_batch()` times out or receives invalid responses
4. Error propagates, batch is never persisted
5. Sender remains in `persist_subscribers` indefinitely
6. Repeated failures across many batches cause unbounded memory growth

This breaks the **Resource Limits** invariant - consensus nodes should respect memory constraints, but leaked subscribers accumulate without bound within an epoch.

## Impact Explanation

**HIGH SEVERITY** - This qualifies as "Validator node slowdowns" per the Aptos bug bounty program.

The memory leak has cascading effects on validator node stability:
- Each leaked subscriber contains a `oneshot::Sender<PersistedValue<BatchInfoExt>>` 
- In high-throughput networks with frequent batch request failures (network issues, malicious peers), leaks accumulate rapidly
- Memory pressure causes garbage collection overhead and eventual slowdowns
- Severe cases can lead to out-of-memory crashes, forcing validator restarts
- Impacts consensus liveness when multiple validators experience degraded performance

While the leak is cleared at epoch boundaries (when new `BatchStore` is created), epochs can last hours, allowing significant accumulation.

## Likelihood Explanation

**HIGH LIKELIHOOD** - This occurs naturally in distributed consensus without requiring sophisticated attacks:

**Natural Triggers:**
- Network partitions causing batch request timeouts
- Transient connectivity issues between validators
- Load spikes causing batch fetch failures
- Normal Byzantine fault scenarios

**Attack Amplification:**
- Malicious peers can deliberately send invalid batch responses
- Attackers can create proposals referencing non-existent batches
- Network-level disruption can be used to maximize batch fetch failures

The vulnerability is triggered during normal consensus operation whenever batch fetching fails, making it highly likely to occur even without malicious intent.

## Recommendation

Implement proper cleanup of orphaned subscribers when batch fetching fails. Add a cleanup mechanism in the error path:

**Option 1: Cleanup on subscriber drop**
Wrap the subscriber receiver in a guard that removes the sender when dropped without receiving a value:

```rust
struct SubscriberGuard {
    digest: HashValue,
    receiver: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
    store: Arc<BatchStore>,
}

impl Drop for SubscriberGuard {
    fn drop(&mut self) {
        // Clean up sender if receiver is dropped without receiving
        self.store.cleanup_subscriber(&self.digest);
    }
}

fn cleanup_subscriber(&self, digest: &HashValue) {
    // Remove the specific subscriber that was never notified
    self.persist_subscribers.remove(digest);
}
```

**Option 2: Timeout-based cleanup**
Add a background task that periodically cleans up old subscribers that were never notified:

```rust
fn cleanup_stale_subscribers(&self, max_age: Duration) {
    let cutoff = SystemTime::now() - max_age;
    self.persist_subscribers.retain(|digest, subscribers| {
        // Check if digest exists in cache or if subscribers are too old
        self.db_cache.contains_key(digest) || subscribers.creation_time > cutoff
    });
}
```

**Option 3: Explicit cleanup in error path**
Modify `get_or_fetch_batch()` to explicitly clean up subscribers on error:

```rust
let subscriber_rx = batch_store.subscribe(*batch_info.digest());
let result = requester.request_batch(
    batch_digest,
    batch_info.expiration(),
    responders,
    subscriber_rx,
).await;

match result {
    Ok(payload) => {
        batch_store.persist(vec![PersistedValue::new(
            batch_info.into(),
            Some(payload.clone()),
        )]);
        Ok(payload)
    },
    Err(e) => {
        // Clean up leaked subscriber on failure
        batch_store.cleanup_subscriber(&batch_digest);
        Err(e)
    }
}
```

**Recommended: Combination approach** - Use Option 3 for immediate cleanup with Option 2 as a safety net for any edge cases.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    
    #[tokio::test]
    async fn test_subscriber_leak_on_failed_batch_request() {
        // Setup: Create a batch store
        let db = Arc::new(MockQuorumStoreStorage::new());
        let validator_signer = ValidatorSigner::random();
        let batch_store = Arc::new(BatchStore::new(
            1, // epoch
            true, // is_new_epoch
            0, // last_certified_time
            db,
            1_000_000, // memory_quota
            10_000_000, // db_quota
            1000, // batch_quota
            validator_signer,
            60_000_000, // expiration_buffer_usecs
        ));
        
        // Simulate batch subscription without successful fetch
        let mut leaked_digests = vec![];
        
        for i in 0..100 {
            let digest = HashValue::random();
            
            // Call subscribe - this adds a sender to persist_subscribers
            let _rx = batch_store.subscribe(digest);
            
            // Simulate failed batch request by just dropping the receiver
            // without ever calling notify_subscribers
            drop(_rx);
            
            leaked_digests.push(digest);
        }
        
        // Verify memory leak: subscribers remain in persist_subscribers
        for digest in leaked_digests {
            assert!(batch_store.persist_subscribers.contains_key(&digest),
                "Subscriber should remain in map after failed request");
        }
        
        // This demonstrates unbounded growth - in production, this would
        // happen over many consensus rounds with failed batch fetches
        assert_eq!(batch_store.persist_subscribers.len(), 100,
            "All 100 failed subscriptions leaked");
    }
}
```

## Notes

This vulnerability specifically affects consensus nodes during batch synchronization. The leak persists for the duration of an epoch, which can be several hours in production environments. While individual leaks are small, high-frequency batch request failures (common in networks with connectivity issues or under Byzantine attacks) can accumulate thousands of leaked subscribers, leading to measurable memory pressure and performance degradation. The issue is particularly concerning because it can be triggered without any malicious intent - normal network conditions are sufficient to cause gradual memory exhaustion.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L457-457)
```rust
                        self.persist_subscribers.remove(entry.get().digest());
```

**File:** consensus/src/quorum_store/batch_store.rs (L591-602)
```rust
    fn subscribe(&self, digest: HashValue) -> oneshot::Receiver<PersistedValue<BatchInfoExt>> {
        let (tx, rx) = oneshot::channel();
        self.persist_subscribers.entry(digest).or_default().push(tx);

        // This is to account for the race where this subscribe call happens after the
        // persist call.
        if let Ok(value) = self.get_batch_from_local(&digest) {
            self.notify_subscribers(value)
        }

        rx
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L604-610)
```rust
    fn notify_subscribers(&self, value: PersistedValue<BatchInfoExt>) {
        if let Some((_, subscribers)) = self.persist_subscribers.remove(value.digest()) {
            for subscriber in subscribers {
                subscriber.send(value.clone()).ok();
            }
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L684-709)
```rust
                let fut = async move {
                    let batch_digest = *batch_info.digest();
                    defer!({
                        inflight_requests_clone.lock().remove(&batch_digest);
                    });
                    // TODO(ibalajiarun): Support V2 batch
                    if let Ok(mut value) = batch_store.get_batch_from_local(&batch_digest) {
                        Ok(value.take_payload().expect("Must have payload"))
                    } else {
                        // Quorum store metrics
                        counters::MISSED_BATCHES_COUNT.inc();
                        let subscriber_rx = batch_store.subscribe(*batch_info.digest());
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
                            .await?;
                        batch_store.persist(vec![PersistedValue::new(
                            batch_info.into(),
                            Some(payload.clone()),
                        )]);
                        Ok(payload)
                    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L176-179)
```rust
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
        })
```
