# Audit Report

## Title
Consensus Safety Violation via Selective Block Discarding with Non-Deterministic Execution Failures

## Summary
The `discard_failed_blocks` configuration option enables validators to discard all transactions in a block when execution fails, producing a state root hash equal to the parent block. Since this is a per-validator local configuration (not enforced on-chain), validators with different configurations can vote on different state root hashes for the same block when execution failures occur non-deterministically, violating the fundamental consensus safety invariant.

## Finding Description

The `discard_failed_blocks` field in `ExecutionConfig` is a local, per-validator configuration that defaults to `false`. [1](#0-0) [2](#0-1) 

When enabled and both parallel and sequential execution fail with `FatalBlockExecutorError` or `FatalVMError`, the block executor discards ALL transactions (including BlockMetadata and validator transactions) by returning outputs with `Discard` status. [3](#0-2) 

These discarded transactions produce empty VMOutput with no state changes. [4](#0-3) 

Critically, discarded transactions are filtered out and NOT included in the `to_commit` set during execution output processing. [5](#0-4) 

When computing the ledger update, only transactions in `to_commit` contribute to the transaction accumulator root hash. [6](#0-5)  If ALL transactions are discarded, `to_commit` is empty, and the transaction accumulator root hash remains identical to the parent block's hash.

This root hash becomes the `executed_state_id` in `BlockInfo` [7](#0-6) , which is included in the `LedgerInfo` that validators vote on. [8](#0-7) 

During consensus voting, votes are grouped by `LedgerInfo` hash. [9](#0-8)  Votes with different `executed_state_id` values will have different `LedgerInfo` hashes and be placed in separate vote buckets, preventing quorum formation or causing validators to form different QCs.

**Attack Scenario:**

1. A block with legitimate transactions is proposed
2. Validator A (with `discard_failed_blocks=false`) successfully executes the block → votes on state root hash `H1`
3. Validator B (with `discard_failed_blocks=true`) experiences an execution failure (due to non-deterministic factors like resource limits, timing, or environmental conditions) → discards all transactions → votes on parent state root hash `H0`
4. `H0 ≠ H1`, so votes are in different buckets
5. **Consensus safety violation**: Validators disagree on the execution result of the same block

The execution failures (`FatalBlockExecutorError`, `FatalVMError`) are supposed to be deterministic, but can exhibit non-determinism due to:
- Resource exhaustion (memory limits, stack overflow)
- Timing-dependent race conditions in parallel execution
- Platform-specific behavior differences
- Environmental configuration differences

## Impact Explanation

This vulnerability represents a **Critical Severity** consensus safety violation. According to the Aptos bug bounty program, "Consensus/Safety violations" qualify for up to $1,000,000.

The impact includes:

1. **Consensus Safety Break**: Violates the fundamental invariant that "all validators must produce identical state roots for identical blocks"
2. **Chain Fork Risk**: Different validators voting on different state roots can lead to non-recoverable chain splits
3. **Transaction Censorship**: Validators can selectively censor transactions by claiming execution failures
4. **Liveness Failure**: If no vote bucket reaches quorum, consensus stalls
5. **No Detection Mechanism**: There is no cross-validator verification that execution results match before voting

This breaks Critical Invariant #1 (Deterministic Execution) and Critical Invariant #2 (Consensus Safety).

## Likelihood Explanation

**Likelihood: Medium-High**

While `discard_failed_blocks` defaults to `false`, validators may enable it for operational resilience against VM bugs. The likelihood increases when:

1. **Configuration Heterogeneity**: Some validators enable `discard_failed_blocks` while others don't
2. **Non-Deterministic Execution**: Environmental factors cause different execution outcomes:
   - Resource constraints varying across validator hardware
   - Timing-dependent parallel execution behavior
   - Platform-specific differences in memory management or stack limits
3. **Complex Transactions**: Blocks containing transactions that push execution limits are more likely to trigger non-deterministic failures

The vulnerability does not require Byzantine behavior—honest validators with different configurations can inadvertently cause consensus divergence.

## Recommendation

**Immediate Fix:**

1. **Make `discard_failed_blocks` a consensus parameter** enforced on-chain, not a local configuration. All validators in an epoch must have the same setting.

2. **Add execution result verification** before voting. Implement a mechanism where validators cross-check execution results (similar to how other BFT systems handle this):
   - Include execution result commitments in block proposals
   - Validators verify their execution matches the proposer's before voting
   - Reject blocks where execution results diverge

3. **Improved Fallback Logic**: As noted in the TODO comment, implement better fallback that attempts to execute just the BlockMetadata transaction before discarding everything. [10](#0-9) 

4. **Determinism Enforcement**: Add runtime checks to ensure execution failures are truly deterministic across validators before allowing discarding.

**Code Fix Example (Conceptual):**

```rust
// In ExecutionConfig - make this an on-chain parameter
pub struct OnChainExecutionConfig {
    pub discard_failed_blocks: bool, // Must be same for all validators
}

// In consensus voting - verify execution result matches expected
pub fn insert_vote(
    &mut self,
    vote: &Vote,
    validator_verifier: &ValidatorVerifier,
    expected_state_root: HashValue, // From block proposal
) -> VoteReceptionResult {
    // Verify vote's state root matches expected
    if vote.ledger_info().transaction_accumulator_hash() != expected_state_root {
        return VoteReceptionResult::ErrorAddingVote(
            VerifyError::InconsistentExecutionResult
        );
    }
    // ... existing vote processing
}
```

## Proof of Concept

**Setup:**
1. Configure a network with 4 validators (A, B, C, D)
2. Set `discard_failed_blocks=true` on validators A and B
3. Set `discard_failed_blocks=false` on validators C and D

**Execution:**

```rust
// Scenario: Block execution fails on A and B due to resource limits,
// but succeeds on C and D

// Validator A (discard_failed_blocks=true):
// - Parallel execution fails with FatalBlockExecutorError
// - Sequential execution fails with FatalBlockExecutorError
// - Returns: BlockOutput with all transactions having Discard status
// - to_commit is empty
// - State root = parent_hash (e.g., 0xAAA)
// - Votes on LedgerInfo with executed_state_id = 0xAAA

// Validator C (discard_failed_blocks=false):
// - Execution succeeds
// - to_commit contains all transactions
// - State root = new_hash (e.g., 0xBBB)
// - Votes on LedgerInfo with executed_state_id = 0xBBB

// Vote Aggregation:
// - li_digest_A = hash(LedgerInfo with 0xAAA)
// - li_digest_C = hash(LedgerInfo with 0xBBB)
// - li_digest_A ≠ li_digest_C
// - Votes split into two buckets
// - If both A,B and C,D have 50% voting power each:
//   * Neither bucket reaches 2f+1 quorum
//   * Consensus stalls
// - If A,B have >66% voting power:
//   * QC forms on 0xAAA (discarded state)
//   * C,D reject the QC as invalid (different execution result)
//   * Chain fork occurs
```

This demonstrates how the local configuration combined with non-deterministic execution failures causes consensus safety violations without requiring malicious actors.

**Notes**

The vulnerability exists because `discard_failed_blocks` is a local validator configuration rather than a consensus-enforced parameter. While the feature appears designed for operational resilience against VM bugs, its current implementation violates the fundamental BFT assumption that all honest replicas produce identical outputs for identical inputs. The lack of cross-validator execution result verification before voting allows this configuration heterogeneity to break consensus safety.

### Citations

**File:** config/src/config/execution_config.rs (L46-46)
```rust
    pub discard_failed_blocks: bool,
```

**File:** config/src/config/execution_config.rs (L88-88)
```rust
            discard_failed_blocks: false,
```

**File:** aptos-move/block-executor/src/executor.rs (L2648-2662)
```rust
        if self.config.local.discard_failed_blocks {
            // We cannot execute block, discard everything (including block metadata and validator transactions)
            // (TODO: maybe we should add fallback here to first try BlockMetadataTransaction alone)
            let error_code = match sequential_error {
                BlockExecutionError::FatalBlockExecutorError(_) => {
                    StatusCode::DELAYED_FIELD_OR_BLOCKSTM_CODE_INVARIANT_ERROR
                },
                BlockExecutionError::FatalVMError(_) => {
                    StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR
                },
            };
            let ret = (0..signature_verified_block.num_txns())
                .map(|_| E::Output::discard_output(error_code))
                .collect();
            return Ok(BlockOutput::new(ret, None));
```

**File:** aptos-move/aptos-vm-types/src/output.rs (L62-70)
```rust
    pub fn empty_with_status(status: TransactionStatus) -> Self {
        Self {
            change_set: VMChangeSet::empty(),
            module_write_set: ModuleWriteSet::empty(),
            fee_statement: FeeStatement::zero(),
            status,
            trace: Trace::empty(),
        }
    }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L460-517)
```rust
    fn extract_retries_and_discards(
        transactions: &mut Vec<Transaction>,
        transaction_outputs: &mut Vec<TransactionOutput>,
        persisted_auxiliary_infos: &mut Vec<PersistedAuxiliaryInfo>,
    ) -> (TransactionsWithOutput, TransactionsWithOutput) {
        let _timer = OTHER_TIMERS.timer_with(&["parse_raw_output__retries_and_discards"]);

        let mut to_discard = TransactionsWithOutput::new_empty();
        let mut to_retry = TransactionsWithOutput::new_empty();

        let mut num_keep_txns = 0;

        for idx in 0..transactions.len() {
            match transaction_outputs[idx].status() {
                TransactionStatus::Keep(_) => {
                    if num_keep_txns != idx {
                        transactions[num_keep_txns] = transactions[idx].clone();
                        transaction_outputs[num_keep_txns] = transaction_outputs[idx].clone();
                        persisted_auxiliary_infos[num_keep_txns] = persisted_auxiliary_infos[idx];
                    }
                    num_keep_txns += 1;
                },
                TransactionStatus::Retry => to_retry.push(
                    transactions[idx].clone(),
                    transaction_outputs[idx].clone(),
                    persisted_auxiliary_infos[idx],
                ),
                TransactionStatus::Discard(_) => to_discard.push(
                    transactions[idx].clone(),
                    transaction_outputs[idx].clone(),
                    persisted_auxiliary_infos[idx],
                ),
            }
        }

        transactions.truncate(num_keep_txns);
        transaction_outputs.truncate(num_keep_txns);
        persisted_auxiliary_infos.truncate(num_keep_txns);

        // Sanity check transactions with the Discard status:
        to_discard.iter().for_each(|(t, o, _)| {
            // In case a new status other than Retry, Keep and Discard is added:
            if !matches!(o.status(), TransactionStatus::Discard(_)) {
                error!("Status other than Retry, Keep or Discard; Transaction discarded.");
            }
            // VM shouldn't have output anything for discarded transactions, log if it did.
            if !o.write_set().is_empty() || !o.events().is_empty() {
                error!(
                    "Discarded transaction has non-empty write set or events. \
                        Transaction: {:?}. Status: {:?}.",
                    t,
                    o.status(),
                );
                EXECUTOR_ERRORS.inc();
            }
        });

        (to_retry, to_discard)
```

**File:** execution/executor/src/workflow/do_ledger_update.rs (L47-93)
```rust
    fn assemble_transaction_infos(
        to_commit: &TransactionsWithOutput,
        state_checkpoint_hashes: Vec<Option<HashValue>>,
    ) -> (Vec<TransactionInfo>, Vec<HashValue>) {
        let _timer = OTHER_TIMERS.timer_with(&["assemble_transaction_infos"]);

        (0..to_commit.len())
            .into_par_iter()
            .with_min_len(optimal_min_len(to_commit.len(), 64))
            .map(|i| {
                let txn = &to_commit.transactions[i];
                let txn_output = &to_commit.transaction_outputs[i];
                let persisted_auxiliary_info = &to_commit.persisted_auxiliary_infos[i];
                // Use the auxiliary info hash directly from the persisted info
                let auxiliary_info_hash = match persisted_auxiliary_info {
                    PersistedAuxiliaryInfo::None => None,
                    PersistedAuxiliaryInfo::V1 { .. } => {
                        Some(CryptoHash::hash(persisted_auxiliary_info))
                    },
                    PersistedAuxiliaryInfo::TimestampNotYetAssignedV1 { .. } => None,
                };
                let state_checkpoint_hash = state_checkpoint_hashes[i];
                let event_hashes = txn_output
                    .events()
                    .iter()
                    .map(CryptoHash::hash)
                    .collect::<Vec<_>>();
                let event_root_hash =
                    InMemoryEventAccumulator::from_leaves(&event_hashes).root_hash();
                let write_set_hash = CryptoHash::hash(txn_output.write_set());
                let txn_info = TransactionInfo::new(
                    txn.hash(),
                    write_set_hash,
                    event_root_hash,
                    state_checkpoint_hash,
                    txn_output.gas_used(),
                    txn_output
                        .status()
                        .as_kept_status()
                        .expect("Already sorted."),
                    auxiliary_info_hash,
                );
                let txn_info_hash = txn_info.hash();
                (txn_info, txn_info_hash)
            })
            .unzip()
    }
```

**File:** types/src/block_info.rs (L36-37)
```rust
    /// The accumulator root hash after executing this block.
    executed_state_id: HashValue,
```

**File:** types/src/ledger_info.rs (L129-131)
```rust
    pub fn transaction_accumulator_hash(&self) -> HashValue {
        self.commit_info.executed_state_id()
    }
```

**File:** consensus/src/pending_votes.rs (L281-329)
```rust
        let li_digest = vote.ledger_info().hash();

        //
        // 1. Has the author already voted for this round?
        //

        if let Some((previously_seen_vote, previous_li_digest)) =
            self.author_to_vote.get(&vote.author())
        {
            // is it the same vote?
            if &li_digest == previous_li_digest {
                // we've already seen an equivalent vote before
                let new_timeout_vote = vote.is_timeout() && !previously_seen_vote.is_timeout();
                if !new_timeout_vote {
                    // it's not a new timeout vote
                    return VoteReceptionResult::DuplicateVote;
                }
            } else {
                // we have seen a different vote for the same round
                error!(
                    SecurityEvent::ConsensusEquivocatingVote,
                    remote_peer = vote.author(),
                    vote = vote,
                    previous_vote = previously_seen_vote
                );

                return VoteReceptionResult::EquivocateVote;
            }
        }

        //
        // 2. Store new vote (or update, in case it's a new timeout vote)
        //

        self.author_to_vote
            .insert(vote.author(), (vote.clone(), li_digest));

        //
        // 3. Let's check if we can create a QC
        //

        let len = self.li_digest_to_votes.len() + 1;
        // obtain the ledger info with signatures associated to the vote's ledger info
        let (hash_index, status) = self.li_digest_to_votes.entry(li_digest).or_insert_with(|| {
            (
                len,
                VoteStatus::NotEnoughVotes(SignatureAggregator::new(vote.ledger_info().clone())),
            )
        });
```
