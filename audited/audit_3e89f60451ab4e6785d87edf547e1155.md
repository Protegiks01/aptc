# Audit Report

## Title
Stale Layout Cache Poisoning via Module Republishing Race Condition Causes Consensus Divergence

## Summary
A race condition in the Move VM's layout cache mechanism allows stale type layouts to be cached after module republishing, leading to non-deterministic transaction execution and potential consensus divergence across validators. When a transaction speculatively computes and caches a struct layout based on an old module version, then another transaction publishes a new module version and flushes the cache, the first transaction can still insert its stale layout into the now-empty cache. Subsequent transactions will use this incorrect layout with the new module definition, causing consensus violations.

## Finding Description

The vulnerability exists in the interaction between three components:

1. **Layout cache storage** in `GlobalModuleCache` uses a concurrent `DashMap` without version tracking [1](#0-0) 

2. **Layout cache flushing** occurs when modules are published during block execution [2](#0-1) 

3. **Layout computation and storage** happens speculatively during transaction execution [3](#0-2) 

The `StructKey` used for caching contains only the struct name index and type arguments, with no module version or hash: [4](#0-3) 

**Attack Sequence:**

1. Transaction T1 (index 100) begins execution, reads module M version 1, computes layout L1 for struct S based on M v1
2. Transaction T2 (index 50) publishes module M version 2 with a different struct S definition
3. T2 calls `publish_module_write_set`, which marks M v1 as overridden and flushes the layout cache
4. T1 (still executing, before validation) calls `store_layout_to_cache` with stale layout L1
5. The `DashMap::entry()` check sees the entry is vacant (cache was just flushed) and inserts L1
6. T1's validation fails (module M was overridden), T1 is aborted and re-executed
7. **Critical bug:** When T1 or other transactions re-execute, they hit the cached layout L1 in `load_layout_from_cache`
8. The cache load charges for module M and gets M v2, but uses layout L1 (computed from M v1) [5](#0-4) 

9. Validation succeeds (the transaction read M v2), but execution uses the wrong layout!

The layout cache loading mechanism only ensures that module reads are captured for validation purposes, but does not verify that the cached layout was actually computed from the current module version. The comment indicates the intent was to invalidate transactions when modules change, but this fails when stale layouts are inserted after cache flushes.

## Impact Explanation

This vulnerability represents a **Critical Severity** consensus violation:

**Deterministic Execution Violation:** The Aptos blockchain requires that all validators produce identical state roots for identical blocks (Critical Invariant #1). This vulnerability breaks that guarantee because:
- Different validators executing the same block at different times may have different cached layouts
- The race condition outcome depends on precise timing of parallel execution
- Validators could compute different state roots for the same transaction sequence

**Consensus Safety Violation:** Under Byzantine fault tolerance assumptions, honest validators should always agree on committed state. This bug allows consensus divergence without requiring Byzantine actors:
- Validator V1 executes a block and hits a stale layout (race occurred)
- Validator V2 executes the same block with correct timing (race didn't occur)  
- Both validators sign different state roots for the same block
- Network cannot reach consensus or splits into conflicting forks

**State Corruption:** Using incorrect layouts for serialization/deserialization operations leads to:
- Wrong field sizes read from storage (e.g., reading u64 as u128)
- Memory corruption and potential crashes
- Incorrect state transitions persisted to storage
- Non-recoverable state inconsistencies requiring hard fork

This meets the **Critical Severity** criteria: "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)".

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability requires specific timing conditions but is highly likely to occur in production:

**Triggering Conditions:**
1. Parallel block execution with BlockSTM (always enabled in Aptos)
2. Module publishing transaction in the same block as transactions using that module's types
3. Race condition between layout computation and cache flush (timing-dependent)

**Why this is likely:**
- Module upgrades are common on Aptos (framework updates, dApp deployments)
- Each module upgrade flushes the entire layout cache
- High transaction throughput increases parallel execution and race probability
- The vulnerability window spans the entire transaction execution time (can be milliseconds to seconds)
- No synchronization exists between layout caching and cache flushes

**Real-world scenarios:**
- Framework upgrades affecting core types (e.g., `coin::Coin`, `account::Account`)
- Popular dApp module upgrades during high traffic
- Governance proposals executing module upgrades
- Any block containing both a module publish and regular transactions using that module's structs

The race is **silent** - no errors are raised when stale layouts are inserted, making this difficult to detect in testing.

## Recommendation

**Immediate Fix:** Add module version tracking to layout cache entries:

1. Include module hash/version in `StructKey`:
```rust
pub struct StructKey {
    pub idx: StructNameIndex,
    pub ty_args_id: TypeVecId,
    pub module_hash: HashValue, // Add this field
}
```

2. When storing layouts, include the hash of each defining module:
```rust
let cache_entry = LayoutCacheEntry::new(
    layout.clone(), 
    modules,
    compute_module_hashes(&modules) // New parameter
);
```

3. When loading from cache, verify module hashes match:
```rust
fn load_layout_from_cache(&self, key: &StructKey) -> Option<...> {
    let entry = self.module_storage.get_struct_layout(key)?;
    // Verify all module hashes still match
    for (module_id, expected_hash) in entry.module_hashes() {
        let current_hash = self.get_module_hash(module_id)?;
        if current_hash != expected_hash {
            return None; // Cache miss - module changed
        }
    }
    // ... rest of logic
}
```

**Alternative Fix:** Disable layout caching entirely until proper versioning is implemented:
```rust
// In VMConfig
enable_layout_caches: false  // Temporary safety measure
```

**Long-term Solution:** 
- Implement transactional layout cache with rollback on validation failure
- Use per-block layout caches instead of global cache
- Add cache invalidation on module override (not just flush)

## Proof of Concept

The following scenario demonstrates the vulnerability (pseudocode due to complexity of setting up full parallel execution):

```rust
// Step 1: Initial state - Module M v1 with struct S { field: u64 }
let module_v1 = compile_module("
    module 0x1::M {
        struct S { field: u64 }
    }
");

// Step 2: Start transaction T1 that needs layout for S
let t1_execution = async {
    // T1 reads module M v1
    let module = load_module(0x1, "M");  // Gets v1
    // T1 computes layout based on v1
    let layout = compute_layout("0x1::M::S");  // Layout: struct { u64 }
    // ... T1 continues execution ...
    thread::sleep(Duration::from_millis(10)); // Simulate execution time
    // T1 stores layout to cache (after T2 has flushed!)
    store_layout_to_cache(struct_key, layout);
};

// Step 3: Transaction T2 publishes M v2 with struct S { field: u128, new_field: u64 }
let t2_execution = async {
    thread::sleep(Duration::from_millis(5)); // T2 commits first
    let module_v2 = compile_module("
        module 0x1::M {
            struct S { 
                field: u128,    // Changed from u64!
                new_field: u64  // New field!
            }
        }
    ");
    publish_module(module_v2);
    mark_overridden(0x1, "M");
    flush_layout_cache();  // Clears all layouts
};

// Step 4: Transaction T3 uses the stale layout
let t3_execution = async {
    thread::sleep(Duration::from_millis(15)); // After T1 cached stale layout
    let cached_layout = load_layout_from_cache("0x1::M::S");
    // Gets stale layout: struct { u64 }
    // But module is now v2: struct { u128, u64 }
    
    let value = deserialize_with_layout(storage_bytes, cached_layout);
    // WRONG! Reads 8 bytes as u64, but should read 16 bytes as u128
    // Remaining bytes interpreted as garbage
    // State corruption!
};

// Execute in parallel
tokio::join!(t1_execution, t2_execution, t3_execution);

// Result: T3 executes with wrong layout, different validators
// may or may not hit this race, causing consensus divergence
```

**Expected Behavior:** T3 should compute fresh layout based on M v2

**Actual Behavior:** T3 uses stale layout based on M v1, leading to incorrect deserialization and state corruption

**Consensus Impact:** Different validators executing this block will produce different state roots depending on whether they hit the race condition.

## Notes

This vulnerability is particularly insidious because:

1. **Silent failure:** No errors are raised when stale layouts are used
2. **Timing-dependent:** May only manifest under high load or specific execution ordering
3. **Non-deterministic:** Same transaction sequence can produce different results across validators
4. **Hard to debug:** The cached layout looks valid, only the module version mismatch reveals the bug

The root cause is that layout caching was designed for performance optimization but lacks proper invalidation semantics when modules change during parallel execution. The cache flush mechanism assumes synchronous execution, but BlockSTM's parallel execution model allows race conditions.

### Citations

**File:** aptos-move/block-executor/src/code_cache_global.rs (L181-190)
```rust
    pub(crate) fn store_struct_layout_entry(
        &self,
        key: &StructKey,
        entry: LayoutCacheEntry,
    ) -> PartialVMResult<()> {
        if let dashmap::Entry::Vacant(e) = self.struct_layouts.entry(*key) {
            e.insert(entry);
        }
        Ok(())
    }
```

**File:** aptos-move/block-executor/src/txn_last_input_output.rs (L572-575)
```rust
        if published {
            // Record validation requirements after the modules are published.
            global_module_cache.flush_layout_cache();
            scheduler.record_validation_requirements(txn_idx, module_ids_for_v2)?;
```

**File:** third_party/move/move-vm/runtime/src/storage/ty_layout_converter.rs (L116-129)
```rust

                // Otherwise a cache miss, compute the result and store it.
                let mut modules = DefiningModules::new();
                let layout = self.type_to_type_layout_with_delayed_fields_impl::<false>(
                    gas_meter,
                    traversal_context,
                    &mut modules,
                    ty,
                    check_option_type,
                )?;
                let cache_entry = LayoutCacheEntry::new(layout.clone(), modules);
                self.struct_definition_loader
                    .store_layout_to_cache(&key, cache_entry)?;
                return Ok(layout);
```

**File:** third_party/move/move-vm/runtime/src/storage/layout_cache.rs (L79-83)
```rust
#[derive(Debug, Copy, Clone, Eq, PartialEq, Hash)]
pub struct StructKey {
    pub idx: StructNameIndex,
    pub ty_args_id: TypeVecId,
}
```

**File:** third_party/move/move-vm/runtime/src/storage/loader/lazy.rs (L203-221)
```rust
    fn load_layout_from_cache(
        &self,
        gas_meter: &mut impl DependencyGasMeter,
        traversal_context: &mut TraversalContext,
        key: &StructKey,
    ) -> Option<PartialVMResult<LayoutWithDelayedFields>> {
        let entry = self.module_storage.get_struct_layout(key)?;
        let (layout, modules) = entry.unpack();
        for module_id in modules.iter() {
            // Re-read all modules for this layout, so that transaction gets invalidated
            // on module publish. Also, we re-read them in exactly the same way as they
            // were traversed during layout construction, so gas charging should be exactly
            // the same as on the cache miss.
            if let Err(err) = self.charge_module(gas_meter, traversal_context, module_id) {
                return Some(Err(err));
            }
        }
        Some(Ok(layout))
    }
```
