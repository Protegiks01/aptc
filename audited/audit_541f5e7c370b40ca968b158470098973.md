# Audit Report

## Title
Database Consistency Verification Tool Fails to Detect Cross-Database Corruption Due to Non-Atomic Parallel Writes

## Summary
The `check_range_proof` debugging tool can incorrectly report verification success when ledger databases are in an inconsistent state due to partial writes during crashes. The tool only validates a user-specified range of versions, failing to detect corruption at higher versions when metadata_db indicates a later ledger version exists. This creates false confidence in database health, potentially allowing validators to continue operating with corrupt state.

## Finding Description

The Aptos storage system uses multiple sharded databases (transaction_info_db, transaction_accumulator_db, ledger_metadata_db, etc.) that are written in parallel without cross-database atomicity guarantees. [1](#0-0) 

The TODO comment explicitly acknowledges this issue: "Write progress for each of the following databases, and handle the inconsistency at the startup time." Each database is written by a separate thread, and writes are committed sequentially: [2](#0-1) 

**Vulnerability Scenario:**

1. During transaction commit (versions 100-110), parallel writes begin
2. `transaction_accumulator_db` successfully writes versions 100-110  
3. `ledger_metadata_db` successfully writes LedgerCommitProgress = 110
4. **System crashes** before `transaction_info_db` completes its write
5. After restart, `transaction_info_db` only contains versions 0-99

When an operator runs the diagnostic tool: [3](#0-2) 

If they check versions 0-99:
- The tool successfully retrieves transaction infos 0-99 from transaction_info_db
- The tool successfully gets range proofs from transaction_accumulator_db  
- The proofs verify correctly against the accumulator root hash
- **Tool reports SUCCESS** despite versions 100-110 being corrupted

The tool does not automatically verify all versions up to the latest ledger info indicated by metadata_db. It only validates the user-specified range, leaving a gap where corruption remains undetected.

**Broken Invariant:**

This violates the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." The parallel writes without atomicity create a window for partial state commits that break consistency across database shards. [4](#0-3) 

## Impact Explanation

This issue constitutes **Medium Severity** under the Aptos bug bounty program ("State inconsistencies requiring intervention").

**Impact:**
- Validators operating with undetected database corruption may serve incorrect data
- False confidence from diagnostic tools delays detection and remediation  
- Corrupted state could cause consensus divergence if validators disagree on transaction history
- Manual intervention required to restore from backup or resync from network
- In worst case, could contribute to network partition if multiple validators have different corrupted states

While not directly exploitable by external attackers, the consequence of running with corrupt database state on critical blockchain infrastructure justifies Medium-to-High severity.

## Likelihood Explanation

**Likelihood: Medium**

This scenario occurs when:
1. System experiences crash or hardware failure during transaction commit (realistic in production)
2. Crash timing falls within the parallel write window (narrow but non-zero probability)  
3. Operator uses diagnostic tool to verify database health (standard operational procedure)
4. Operator checks only a subset of versions rather than all versions up to latest (likely if they don't know the full extent)

The explicit TODO comments indicate developers are aware of this risk but have not yet implemented recovery mechanisms.

## Recommendation

**Immediate Fix:**

Enhance the `check_range_proof` tool to automatically detect and validate the complete database state:

```rust
pub fn run(self) -> Result<()> {
    let ledger_db = Arc::new(self.db_dir.open_ledger_db()?);
    let ledger_metadata_db = ledger_db.metadata_db();
    let ledger_info = ledger_metadata_db.get_latest_ledger_info()?;
    let latest_version = ledger_info.ledger_info().version();
    
    println!("Latest LedgerInfo: {:?}", ledger_info);
    println!("Latest version in metadata_db: {}", latest_version);
    
    // ADDED: Verify user range doesn't exceed available data
    let last_checked_version = self.start_version + self.num_versions as u64 - 1;
    
    // ADDED: Warn if not checking all versions
    if last_checked_version < latest_version {
        eprintln!(
            "WARNING: Only checking up to version {}, but metadata_db indicates version {}. \
             Corruption may exist in unchecked range [{}, {}]",
            last_checked_version, latest_version, last_checked_version + 1, latest_version
        );
    }
    
    // ADDED: Verify transaction_info_db has all versions up to latest
    for v in self.start_version..=last_checked_version {
        ledger_db.transaction_info_db().get_transaction_info(v)
            .map_err(|e| {
                AptosDbError::Other(format!(
                    "Missing transaction_info at version {}: {}. Database may be corrupted.", 
                    v, e
                ))
            })?;
    }
    
    // Original validation logic continues...
}
```

**Long-term Solution:**

Implement atomic cross-database writes or recovery mechanisms as indicated by the TODO:

1. Add write progress tracking per database (as suggested in TODO)
2. On startup, detect inconsistencies between databases by comparing progress markers
3. Implement rollback of partially-committed transactions  
4. Add automatic consistency verification on node startup

## Proof of Concept

**Reproduction Steps:**

1. Set up Aptos node with sharded storage enabled
2. Begin transaction commit sequence  
3. Use fault injection to crash process after transaction_accumulator_db write completes but before transaction_info_db write completes:
   ```rust
   // In aptosdb_writer.rs, add fault injection after line 316
   if std::env::var("INJECT_CRASH_AFTER_ACCUMULATOR").is_ok() {
       std::process::abort();
   }
   ```
4. Restart node
5. Run diagnostic tool:
   ```bash
   cargo run --bin aptos-db-tool -- check-range-proof \
     --db-dir /path/to/db \
     --start-version 0 \
     --num-versions 100
   ```
6. Observe tool reports SUCCESS despite database having versions 100+ in accumulator but not in transaction_info_db
7. Verify corruption:
   ```bash
   cargo run --bin aptos-db-tool -- check-txn-info-hashes \
     --db-dir /path/to/db \
     --start-version 100 \
     --num-versions 11
   ```
8. This second check will FAIL, confirming the corruption that the range proof tool missed

**Expected vs Actual:**
- **Expected:** Diagnostic tool detects all database inconsistencies
- **Actual:** Tool reports success when checking versions before corruption point, giving false confidence

## Notes

This finding highlights a broader architectural concern: the non-atomic parallel writes across sharded databases create a persistent risk of partial state commits. While the immediate tool limitation can be addressed, the underlying architectural issue requires comprehensive recovery mechanisms on startup, as acknowledged in the TODO comments throughout the codebase.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L271-319)
```rust
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L281-281)
```rust
        // TODO(grao): Handle data inconsistency.
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L531-548)
```rust
    pub fn write_schemas(&self, schemas: LedgerDbSchemaBatches) -> Result<()> {
        self.write_set_db
            .write_schemas(schemas.write_set_db_batches)?;
        self.transaction_info_db
            .write_schemas(schemas.transaction_info_db_batches)?;
        self.transaction_db
            .write_schemas(schemas.transaction_db_batches)?;
        self.persisted_auxiliary_info_db
            .write_schemas(schemas.persisted_auxiliary_info_db_batches)?;
        self.event_db.write_schemas(schemas.event_db_batches)?;
        self.transaction_accumulator_db
            .write_schemas(schemas.transaction_accumulator_db_batches)?;
        self.transaction_auxiliary_data_db
            .write_schemas(schemas.transaction_auxiliary_data_db_batches)?;
        // TODO: remove this after sharding migration
        self.ledger_metadata_db
            .write_schemas(schemas.ledger_metadata_db_batches)
    }
```

**File:** storage/aptosdb/src/db_debugger/ledger/check_range_proof.rs (L25-70)
```rust
    pub fn run(self) -> Result<()> {
        let ledger_db = Arc::new(self.db_dir.open_ledger_db()?);
        let ledger_metadata_db = ledger_db.metadata_db();
        let ledger_info = ledger_metadata_db.get_latest_ledger_info()?;
        println!("Latest LedgerInfo: {:?}", ledger_info);

        println!("Checking Range proof...");

        let txn_infos: Vec<_> = ledger_db
            .transaction_info_db()
            .get_transaction_info_iter(self.start_version, self.num_versions)?
            .collect::<Result<_>>()?;
        ensure!(
            txn_infos.len() == self.num_versions,
            "expecting {} txns, got {}",
            self.num_versions,
            txn_infos.len(),
        );
        let txn_info_hashes: Vec<_> = txn_infos.iter().map(CryptoHash::hash).collect();

        let last_version = self.start_version + self.num_versions as u64 - 1;
        let last_version_epoch = ledger_metadata_db.get_epoch(last_version)?;
        for epoch in last_version_epoch..=ledger_info.ledger_info().epoch() {
            println!("Check against epoch {} LedgerInfo.", epoch);
            let li = ledger_metadata_db.get_latest_ledger_info_in_epoch(epoch)?;
            println!(
                "    Root hash: {:?}",
                li.ledger_info().transaction_accumulator_hash()
            );
            let range_proof = ledger_db
                .transaction_accumulator_db()
                .get_transaction_range_proof(
                    Some(self.start_version),
                    self.num_versions as u64,
                    li.ledger_info().version(),
                )?;
            range_proof.verify(
                li.ledger_info().transaction_accumulator_hash(),
                Some(self.start_version),
                &txn_info_hashes,
            )?;
        }

        println!("Done.");
        Ok(())
    }
```
