# Audit Report

## Title
Transaction Propagation Partition During Validator Set Reconfiguration Due to Unsynchronized Mempool Broadcast Flag and Network Peer Updates

## Summary
During validator set updates (epoch transitions), the mempool's `broadcast_within_validator_network` flag update and the network layer's peer connection updates occur asynchronously and independently. This creates a race condition window where different validators have inconsistent views of which peers to broadcast to and whether to broadcast at all, causing a partition in transaction propagation across the validator network. [1](#0-0) 

## Finding Description

When a reconfiguration event occurs (e.g., epoch change with Quorum Store enablement), two independent asynchronous processes are triggered:

**Process 1: Mempool broadcast flag update** [2](#0-1) 

The `handle_mempool_reconfig_event` spawns a `process_config_update` task that updates the `broadcast_within_validator_network` flag: [3](#0-2) 

**Process 2: Network peer connection updates**

The network layer's `ConnectivityManager` receives validator set updates and schedules connection updates, but stale connection closure happens only during periodic `check_connectivity` ticks: [4](#0-3) [5](#0-4) 

**The Race Condition:**

These processes execute with unpredictable timing:

**Validator A (fast flag update, slow network update):**
- T+5ms: Flag updated to `false` (stops broadcasting)
- T+100ms: Network closes old connections, establishes new ones
- **During T+5 to T+100ms**: Not broadcasting but still connected to old validator set

**Validator B (slow flag update, fast network update):**
- T+10ms: Network updates to new validator set
- T+50ms: Flag updated to `false`
- **During T+10 to T+50ms**: Broadcasting to new validator set

**Validator C (slow both):**
- T+80ms: Flag updated to `false`
- T+120ms: Network updates
- **During T+0 to T+80ms**: Broadcasting to old validator set

**The Partition:**

When incoming transactions are received by validators with `flag=false`, they are marked as `NonQualified` and never rebroadcast: [6](#0-5) [7](#0-6) 

Transactions marked as `NonQualified` are never added to the timeline_index for broadcasting. This creates a partition where:
- Transactions broadcast to old validator set don't reach new validators
- Transactions broadcast to new validators are rejected (marked `NonQualified`) by validators with updated flags
- Transactions submitted to validators with updated flags are not broadcast at all

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty program:

1. **Significant Protocol Violations**: The mempool broadcast protocol's fundamental assumption—that all validators propagate transactions consistently—is violated during epoch transitions.

2. **Transaction Propagation Failures**: Transactions may fail to reach all validators, causing:
   - Delayed transaction inclusion (validators without the transaction can't propose blocks containing it)
   - Potential liveness degradation (if many transactions are affected)
   - Inconsistent mempool state across validators

3. **Consensus Impact**: While this doesn't directly break consensus safety, it can affect block proposal efficiency and transaction fairness, as some validators may never see certain transactions during the race window.

The race window duration depends on:
- `connectivity_check_interval` (network layer)
- Bounded executor queue depth and processing time (mempool layer)
- Network latency between validators

In a production environment with multiple validators, different timing across nodes could create a window of several seconds.

## Likelihood Explanation

**Likelihood: High**

This vulnerability triggers automatically during every epoch transition when Quorum Store is enabled/disabled:
- No attacker action required to trigger the race
- Happens naturally due to asynchronous task execution
- Window duration is system-dependent but likely measurable in seconds
- Affects all validators simultaneously (but with different timing)
- Any user submitting transactions during the window is affected

The race is exacerbated by:
- High system load (delays bounded_executor task execution)
- Network latency variations
- Differences in node hardware/performance

## Recommendation

**Solution: Synchronize network peer updates with broadcast flag updates**

Introduce a coordination mechanism to ensure the mempool broadcast flag is only updated after network connections have been updated (or vice versa):

```rust
// In coordinator.rs handle_mempool_reconfig_event
async fn handle_mempool_reconfig_event<NetworkClient, TransactionValidator, ConfigProvider>(
    smp: &mut SharedMempool<NetworkClient, TransactionValidator>,
    bounded_executor: &BoundedExecutor,
    config_update: OnChainConfigPayload<ConfigProvider>,
) where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg> + 'static,
    TransactionValidator: TransactionValidation + 'static,
    ConfigProvider: OnChainConfigProvider,
{
    // First, wait for network to signal it has updated peers
    // (requires adding a notification mechanism from network layer)
    
    // Then update the broadcast flag
    bounded_executor
        .spawn(tasks::process_config_update(
            config_update,
            smp.validator.clone(),
            smp.broadcast_within_validator_network.clone(),
        ))
        .await;
}
```

**Alternative: Add grace period**

Maintain both old and new behavior for a grace period:
- Continue accepting broadcasts even after flag is updated
- Gradually phase out old behavior
- Requires more complex state management but provides smoother transition

## Proof of Concept

```rust
// Integration test demonstrating the race condition
#[tokio::test]
async fn test_reconfig_broadcast_partition() {
    // Setup: 3 validators with mempool and network layers
    let (validator_a, validator_b, validator_c) = setup_three_validators().await;
    
    // Initial state: all validators broadcasting, connected to each other
    assert!(validator_a.broadcast_within_validator_network());
    assert!(validator_b.broadcast_within_validator_network());
    assert!(validator_c.broadcast_within_validator_network());
    
    // Trigger reconfiguration (Quorum Store enabled)
    let reconfig_notification = create_reconfig_with_quorum_store_enabled();
    
    // Simulate different processing times:
    // Validator A: fast flag update (5ms), slow network (100ms)
    tokio::spawn(async move {
        tokio::time::sleep(Duration::from_millis(5)).await;
        validator_a.process_config_update(reconfig_notification.clone()).await;
        tokio::time::sleep(Duration::from_millis(95)).await;
        validator_a.update_network_peers().await;
    });
    
    // Validator B: fast network (10ms), slow flag (50ms)
    tokio::spawn(async move {
        tokio::time::sleep(Duration::from_millis(10)).await;
        validator_b.update_network_peers().await;
        tokio::time::sleep(Duration::from_millis(40)).await;
        validator_b.process_config_update(reconfig_notification.clone()).await;
    });
    
    // Submit transaction at T+20ms (during race window)
    tokio::time::sleep(Duration::from_millis(20)).await;
    let txn = create_test_transaction();
    validator_b.submit_transaction(txn.clone()).await;
    
    // Validator B should broadcast to new validator set
    // Validator A should reject it (flag=false)
    // Validator C should not receive it (not in new set yet)
    
    tokio::time::sleep(Duration::from_millis(200)).await;
    
    // Assert: Transaction not in all validators' mempools
    assert!(validator_b.has_transaction(&txn));
    assert!(!validator_a.has_transaction(&txn) || 
            validator_a.is_nonqualified(&txn)); // Received but marked NonQualified
    // If C was in old set but not new set, it won't have the transaction
}
```

## Notes

The vulnerability is rooted in the lack of coordination between two independent subsystems (mempool and network layer) during epoch transitions. The `RwLock::write()` itself is not the direct cause—it correctly synchronizes access to the boolean flag. However, the *timing* of when `write()` is called relative to network peer updates creates the race condition.

The comment in the codebase acknowledges transition issues for false→true transitions but doesn't address the true→false case: [8](#0-7)

### Citations

**File:** crates/aptos-infallible/src/rwlock.rs (L26-30)
```rust
    pub fn write(&self) -> RwLockWriteGuard<'_, T> {
        self.0
            .write()
            .expect("Cannot currently handle a poisoned lock")
    }
```

**File:** mempool/src/shared_mempool/coordinator.rs (L268-291)
```rust
async fn handle_mempool_reconfig_event<NetworkClient, TransactionValidator, ConfigProvider>(
    smp: &mut SharedMempool<NetworkClient, TransactionValidator>,
    bounded_executor: &BoundedExecutor,
    config_update: OnChainConfigPayload<ConfigProvider>,
) where
    NetworkClient: NetworkClientInterface<MempoolSyncMsg> + 'static,
    TransactionValidator: TransactionValidation + 'static,
    ConfigProvider: OnChainConfigProvider,
{
    info!(LogSchema::event_log(
        LogEntry::ReconfigUpdate,
        LogEvent::Received
    ));
    let _timer =
        counters::task_spawn_latency_timer(counters::RECONFIG_EVENT_LABEL, counters::SPAWN_LABEL);

    bounded_executor
        .spawn(tasks::process_config_update(
            config_update,
            smp.validator.clone(),
            smp.broadcast_within_validator_network.clone(),
        ))
        .await;
}
```

**File:** mempool/src/shared_mempool/coordinator.rs (L312-319)
```rust
    let ineligible_for_broadcast = (smp.network_interface.is_validator()
        && !smp.broadcast_within_validator_network())
        || smp.network_interface.is_upstream_peer(&peer, None);
    let timeline_state = if ineligible_for_broadcast {
        TimelineState::NonQualified
    } else {
        TimelineState::NotReady
    };
```

**File:** mempool/src/shared_mempool/tasks.rs (L140-146)
```rust
    let ineligible_for_broadcast =
        smp.network_interface.is_validator() && !smp.broadcast_within_validator_network();
    let timeline_state = if ineligible_for_broadcast {
        TimelineState::NonQualified
    } else {
        TimelineState::NotReady
    };
```

**File:** mempool/src/shared_mempool/tasks.rs (L762-794)
```rust
pub(crate) async fn process_config_update<V, P>(
    config_update: OnChainConfigPayload<P>,
    validator: Arc<RwLock<V>>,
    broadcast_within_validator_network: Arc<RwLock<bool>>,
) where
    V: TransactionValidation,
    P: OnChainConfigProvider,
{
    info!(LogSchema::event_log(
        LogEntry::ReconfigUpdate,
        LogEvent::Process
    ));

    if let Err(e) = validator.write().restart() {
        counters::VM_RECONFIG_UPDATE_FAIL_COUNT.inc();
        error!(LogSchema::event_log(LogEntry::ReconfigUpdate, LogEvent::VMUpdateFail).error(&e));
    }

    let consensus_config: anyhow::Result<OnChainConsensusConfig> = config_update.get();
    match consensus_config {
        Ok(consensus_config) => {
            *broadcast_within_validator_network.write() =
                !consensus_config.quorum_store_enabled() && !consensus_config.is_dag_enabled()
        },
        Err(e) => {
            error!(
                "Failed to read on-chain consensus config, keeping value broadcast_within_validator_network={}: {}",
                *broadcast_within_validator_network.read(),
                e
            );
        },
    }
}
```

**File:** network/framework/src/connectivity_manager/mod.rs (L484-531)
```rust
    async fn close_stale_connections(&mut self) {
        if let Some(trusted_peers) = self.get_trusted_peers() {
            // Identify stale peer connections
            let stale_peers = self
                .connected
                .iter()
                .filter(|(peer_id, _)| !trusted_peers.contains_key(peer_id))
                .filter_map(|(peer_id, metadata)| {
                    // If we're using server only auth, we need to not evict unknown peers
                    // TODO: We should prevent `Unknown` from discovery sources
                    if !self.mutual_authentication
                        && metadata.origin == ConnectionOrigin::Inbound
                        && (metadata.role == PeerRole::ValidatorFullNode
                            || metadata.role == PeerRole::Unknown)
                    {
                        None
                    } else {
                        Some(*peer_id) // The peer is stale
                    }
                });

            // Close existing connections to stale peers
            for stale_peer in stale_peers {
                info!(
                    NetworkSchema::new(&self.network_context).remote_peer(&stale_peer),
                    "{} Closing stale connection to peer {}",
                    self.network_context,
                    stale_peer.short_str()
                );

                if let Err(disconnect_error) = self
                    .connection_reqs_tx
                    .disconnect_peer(stale_peer, DisconnectReason::StaleConnection)
                    .await
                {
                    info!(
                        NetworkSchema::new(&self.network_context)
                            .remote_peer(&stale_peer),
                        error = %disconnect_error,
                        "{} Failed to close stale connection to peer {}, error: {}",
                        self.network_context,
                        stale_peer.short_str(),
                        disconnect_error
                    );
                }
            }
        }
    }
```

**File:** network/framework/src/connectivity_manager/mod.rs (L807-832)
```rust
    async fn check_connectivity<'a>(
        &'a mut self,
        pending_dials: &'a mut FuturesUnordered<BoxFuture<'static, PeerId>>,
    ) {
        trace!(
            NetworkSchema::new(&self.network_context),
            "{} Checking connectivity",
            self.network_context
        );

        // Log the eligible peers with addresses from discovery
        sample!(SampleRate::Duration(Duration::from_secs(60)), {
            info!(
                NetworkSchema::new(&self.network_context),
                discovered_peers = ?self.discovered_peers,
                "Active discovered peers"
            )
        });

        // Cancel dials to peers that are no longer eligible.
        self.cancel_stale_dials().await;
        // Disconnect from connected peers that are no longer eligible.
        self.close_stale_connections().await;
        // Dial peers which are eligible but are neither connected nor queued for dialing in the
        // future.
        self.dial_eligible_peers(pending_dials).await;
```

**File:** mempool/src/shared_mempool/types.rs (L95-103)
```rust
    pub fn broadcast_within_validator_network(&self) -> bool {
        // This value will be changed true -> false via onchain config when quorum store is enabled.
        // On the transition from true -> false, all transactions in mempool will be eligible for
        // at least one of mempool broadcast or quorum store batch.
        // A transition from false -> true is unexpected -- it would only be triggered if quorum
        // store needs an emergency rollback. In this case, some transactions may not be propagated,
        // they will neither go through a mempool broadcast or quorum store batch.
        *self.broadcast_within_validator_network.read()
    }
```
