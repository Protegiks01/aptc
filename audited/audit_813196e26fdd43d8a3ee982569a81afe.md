# Audit Report

## Title
JWK Consensus State Loss on Validator Crash Causes Protocol Divergence and Liveness Failures

## Summary
The JWK consensus runtime maintains all state (in-progress proposals, quorum-certified updates) purely in memory with no persistence mechanism. When a validator crashes and restarts, all JWK consensus state is irrecoverably lost, potentially causing validators to diverge on state and creating liveness failures for JWK updates. [1](#0-0) 

## Finding Description

The `start_jwk_consensus_runtime()` function creates a fresh `EpochManager` with no state recovery mechanism. The `EpochManager` maintains JWK consensus state in memory-only data structures: [2](#0-1) 

The consensus managers (`IssuerLevelConsensusManager` and `KeyLevelConsensusManager`) store critical state in HashMaps: [3](#0-2) 

This state includes:
1. **In-progress consensus proposals** with signatures and abort handles
2. **Quorum-certified updates** held by `TxnGuard` in the validator transaction pool
3. **Observed JWK data** from external OIDC providers [4](#0-3) 

The `PersistentSafetyStorage` used by JWK consensus only stores consensus private keys, NOT JWK consensus state: [5](#0-4) 

When a validator crashes, the in-memory `TxnGuard` holding quorum-certified updates is lost. On restart, a fresh `VTxnPoolState` is created: [6](#0-5) [7](#0-6) 

The `TxnGuard` Drop implementation automatically removes transactions from the pool: [8](#0-7) 

**Attack Scenario:**
1. All validators observe OIDC provider JWKs and reach quorum for version N+1 with content J1
2. Validator A crashes after quorum but before on-chain commitment
3. Validator A's in-memory state (including vtxn pool) is lost
4. Validator A restarts with fresh state, sees on-chain version is still N
5. OIDC provider has rotated keys between crash and restart (common during key rotation)
6. Validator A re-observes and gets JWKs J2 (different from J1)
7. Validator A creates NEW proposal for version N+1 with J2
8. Other validators have version N+1 with J1 in their pools
9. Reliable broadcast aggregation fails due to mismatched views: [9](#0-8) 

10. Neither proposal can reach quorum, causing liveness failure for JWK updates

JWK observations are NOT guaranteed deterministic, as the `JWKObserver` fetches from external OIDC providers that can change at any time: [10](#0-9) 

The on-chain validation ensures only one version can be committed, but doesn't prevent divergent proposals: [11](#0-10) 

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: Validators perform redundant consensus work when divergent proposals exist
2. **Significant Protocol Violations**: Validators maintain divergent state in their transaction pools, violating the invariant that validators should have consistent views of pending consensus
3. **JWK Update Liveness Failures**: Critical for keyless account functionality - if JWK updates stall, keyless accounts cannot authenticate, impacting user availability

This breaks the **State Consistency** invariant: crash recovery should maintain atomic state transitions, but JWK consensus has no persistence mechanism.

While not Critical (no fund loss, main consensus unaffected), it constitutes a significant protocol violation affecting validator operations and user-facing features.

## Likelihood Explanation

**Medium to High Likelihood:**

1. **Validator crashes are common**: Due to bugs, OOM, hardware failures, or maintenance restarts
2. **Window of vulnerability**: Any crash between quorum achievement and on-chain commitment
3. **OIDC key rotation is normal**: Major providers (Google, Apple) rotate keys regularly, with TTLs of hours to days
4. **No manual intervention prevents it**: Automatic crash recovery is the norm; operators won't manually restore state

The issue occurs whenever:
- A validator crashes during active JWK consensus
- The external OIDC provider serves different JWKs post-restart (timing-dependent but common during rotation)
- The quorum-certified update hasn't been committed yet

## Recommendation

Implement persistent storage for JWK consensus state, similar to how main consensus uses `PersistentSafetyStorage`:

**1. Create a `JWKConsensusStorage` trait:**
```rust
pub struct JWKConsensusStorage {
    internal_store: Storage,
}

impl JWKConsensusStorage {
    pub fn save_state(&mut self, epoch: u64, issuer: Issuer, state: PerProviderState) -> Result<()>;
    pub fn load_states(&self, epoch: u64) -> Result<HashMap<Issuer, PerProviderState>>;
    pub fn clear_epoch(&mut self, epoch: u64) -> Result<()>;
}
```

**2. Modify `EpochManager::new()` to accept storage config:**
```rust
pub fn new(
    my_addr: AccountAddress,
    safety_rules_config: &SafetyRulesConfig,
    jwk_consensus_storage: JWKConsensusStorage,  // Add this
    // ... other params
) -> Self
```

**3. Implement state recovery in `start_new_epoch()`:**
```rust
// After creating epoch_state, attempt to load persisted state
if let Ok(persisted_states) = jwk_consensus_storage.load_states(epoch_state.epoch) {
    jwk_consensus_manager.restore_states(persisted_states);
}
```

**4. Persist state on critical transitions:**
- After signing proposals
- After reaching quorum
- After placing in vtxn pool

**5. For vtxn pool, implement persistent backing:**
Either persist the pool itself, or ensure quorum-certified updates can be reconstructed from persistent state.

## Proof of Concept

```rust
// Reproduce the vulnerability
#[tokio::test]
async fn test_jwk_consensus_state_loss_on_crash() {
    // Setup: Create JWK consensus runtime
    let (network_client, network_events) = create_test_network();
    let (reconfig_tx, reconfig_rx) = create_reconfig_channel();
    let (jwk_event_tx, jwk_event_rx) = create_jwk_event_channel();
    let vtxn_pool = VTxnPoolState::default();
    
    let runtime = start_jwk_consensus_runtime(
        test_validator_addr(),
        &safety_rules_config(),
        network_client,
        network_events,
        reconfig_rx,
        jwk_event_rx,
        vtxn_pool.clone(),
    );
    
    // Simulate: Trigger epoch start
    reconfig_tx.send(create_reconfig_notification()).await;
    
    // Simulate: Observe JWKs and reach quorum
    let issuer = b"https://accounts.google.com".to_vec();
    let jwks_v1 = vec![test_jwk_1()];
    
    // All validators observe and sign
    simulate_quorum_for_jwk_update(issuer.clone(), 11, jwks_v1.clone()).await;
    
    // Verify: Update is in vtxn pool
    let pool_items = vtxn_pool.pull(Instant::now(), 10, 1000, TransactionFilter::empty());
    assert_eq!(pool_items.len(), 1);
    
    // CRASH: Drop runtime (simulates validator crash)
    drop(runtime);
    drop(vtxn_pool);
    
    // RESTART: Create new runtime with fresh state
    let vtxn_pool_new = VTxnPoolState::default();
    let runtime_new = start_jwk_consensus_runtime(
        test_validator_addr(),
        &safety_rules_config(),
        network_client,
        network_events,
        reconfig_rx,
        jwk_event_rx,
        vtxn_pool_new.clone(),
    );
    
    // Verify: State is LOST
    let pool_items_after = vtxn_pool_new.pull(Instant::now(), 10, 1000, TransactionFilter::empty());
    assert_eq!(pool_items_after.len(), 0); // ‚ùå Update lost!
    
    // Simulate: OIDC provider returns DIFFERENT JWKs after restart
    let jwks_v2 = vec![test_jwk_2()]; // Different content
    
    // Validator re-observes and creates NEW proposal for same version
    simulate_local_observation(issuer.clone(), jwks_v2.clone()).await;
    
    // Result: Two different proposals for version 11 exist across validators
    // Neither can reach quorum due to view mismatch
    // JWK consensus STALLED
}
```

**Notes:**
- This vulnerability is particularly severe during OIDC provider key rotation periods
- The lack of any recovery mechanism makes this a systematic failure mode
- Comparison: Main consensus uses `PersistentSafetyStorage` to survive crashes; JWK consensus should have equivalent protection
- The issue requires architectural changes to properly fix, not just a simple patch

### Citations

**File:** crates/aptos-jwk-consensus/src/lib.rs (L25-50)
```rust
pub fn start_jwk_consensus_runtime(
    my_addr: AccountAddress,
    safety_rules_config: &SafetyRulesConfig,
    network_client: NetworkClient<JWKConsensusMsg>,
    network_service_events: NetworkServiceEvents<JWKConsensusMsg>,
    reconfig_events: ReconfigNotificationListener<DbBackedOnChainConfig>,
    jwk_updated_events: EventNotificationListener,
    vtxn_pool_writer: VTxnPoolState,
) -> Runtime {
    let runtime = aptos_runtimes::spawn_named_runtime("jwk".into(), Some(4));
    let (self_sender, self_receiver) = aptos_channels::new(1_024, &counters::PENDING_SELF_MESSAGES);
    let jwk_consensus_network_client = JWKConsensusNetworkClient::new(network_client);
    let epoch_manager = EpochManager::new(
        my_addr,
        safety_rules_config,
        reconfig_events,
        jwk_updated_events,
        self_sender,
        jwk_consensus_network_client,
        vtxn_pool_writer,
    );
    let (network_task, network_receiver) = NetworkTask::new(network_service_events, self_receiver);
    runtime.spawn(network_task.start());
    runtime.spawn(epoch_manager.start(network_receiver));
    runtime
}
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L42-66)
```rust
pub struct EpochManager<P: OnChainConfigProvider> {
    // some useful metadata
    my_addr: AccountAddress,
    epoch_state: Option<Arc<EpochState>>,

    // credential
    key_storage: PersistentSafetyStorage,

    // events we subscribe
    reconfig_events: ReconfigNotificationListener<P>,
    jwk_updated_events: EventNotificationListener,

    // message channels to JWK manager
    jwk_updated_event_txs: Option<aptos_channel::Sender<(), ObservedJWKsUpdated>>,
    jwk_rpc_msg_tx:
        Option<aptos_channel::Sender<AccountAddress, (AccountAddress, IncomingRpcRequest)>>,
    jwk_manager_close_tx: Option<oneshot::Sender<oneshot::Sender<()>>>,

    // network utils
    self_sender: aptos_channels::Sender<Event<JWKConsensusMsg>>,
    network_sender: JWKConsensusNetworkClient<NetworkClient<JWKConsensusMsg>>,

    // vtxn pool handle
    vtxn_pool: VTxnPoolState,
}
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L39-62)
```rust
pub struct IssuerLevelConsensusManager {
    /// Some useful metadata.
    my_addr: AccountAddress,
    epoch_state: Arc<EpochState>,

    /// Used to sign JWK observations before sharing them with peers.
    consensus_key: Arc<PrivateKey>,

    /// The sub-process that collects JWK updates from peers and aggregate them into a quorum-certified JWK update.
    update_certifier: Arc<dyn TUpdateCertifier<PerIssuerMode>>,

    /// When a quorum-certified JWK update is available, use this to put it into the validator transaction pool.
    vtxn_pool: VTxnPoolState,

    /// The JWK consensus states of all the issuers.
    states_by_issuer: HashMap<Issuer, PerProviderState>,

    /// Whether a CLOSE command has been received.
    stopped: bool,

    qc_update_tx: aptos_channel::Sender<Issuer, QuorumCertifiedUpdate>,
    qc_update_rx: aptos_channel::Receiver<Issuer, QuorumCertifiedUpdate>,
    jwk_observers: Vec<JWKObserver>,
}
```

**File:** crates/aptos-jwk-consensus/src/types.rs (L103-115)
```rust
#[derive(Debug, Clone)]
pub enum ConsensusState<T: Debug + Clone + Eq + PartialEq> {
    NotStarted,
    InProgress {
        my_proposal: T,
        abort_handle_wrapper: QuorumCertProcessGuard,
    },
    Finished {
        vtxn_guard: TxnGuard,
        my_proposal: T,
        quorum_certified: QuorumCertifiedUpdate,
    },
}
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L24-28)
```rust
pub struct PersistentSafetyStorage {
    enable_cached_safety_data: bool,
    cached_safety_data: Option<SafetyData>,
    internal_store: Storage,
}
```

**File:** crates/validator-transaction-pool/src/lib.rs (L48-54)
```rust
impl Default for VTxnPoolState {
    fn default() -> Self {
        Self {
            inner: Arc::new(Mutex::new(PoolStateInner::default())),
        }
    }
}
```

**File:** crates/validator-transaction-pool/src/lib.rs (L202-206)
```rust
impl Drop for TxnGuard {
    fn drop(&mut self) {
        self.pool.lock().try_delete(self.seq_num);
    }
}
```

**File:** aptos-node/src/consensus.rs (L76-76)
```rust
    let vtxn_pool = VTxnPoolState::default();
```

**File:** crates/aptos-jwk-consensus/src/observation_aggregation/mod.rs (L80-84)
```rust

        ensure!(
            self.local_view == peer_view,
            "adding peer observation failed with mismatched view"
        );
```

**File:** crates/aptos-jwk-consensus/src/jwk_observer.rs (L102-110)
```rust
async fn fetch_jwks(open_id_config_url: &str, my_addr: Option<AccountAddress>) -> Result<Vec<JWK>> {
    let jwks_uri = fetch_jwks_uri_from_openid_config(open_id_config_url)
        .await
        .map_err(|e| anyhow!("fetch_jwks failed with open-id config request: {e}"))?;
    let jwks = fetch_jwks_from_jwks_uri(my_addr, jwks_uri.as_str())
        .await
        .map_err(|e| anyhow!("fetch_jwks failed with jwks uri request: {e}"))?;
    Ok(jwks)
}
```

**File:** aptos-move/aptos-vm/src/validator_txns/jwk.rs (L127-130)
```rust
        // Check version.
        if on_chain.version + 1 != observed.version {
            return Err(Expected(IncorrectVersion));
        }
```
