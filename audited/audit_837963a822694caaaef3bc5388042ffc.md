# Audit Report

## Title
Silent Consensus Failure Due to Unmonitored Critical Runtime Task Exits

## Summary
Critical consensus components (NetworkTask, EpochManager) are spawned without storing or monitoring their JoinHandles. When these tasks exit unexpectedly—for example, if network event streams close due to bugs, resource exhaustion, or shutdown races—the validator silently loses consensus participation capability without immediate detection, logging, or alerting.

## Finding Description

The Aptos consensus implementation spawns critical asynchronous tasks using `Runtime::spawn()` but discards the returned `JoinHandle`, eliminating any mechanism to detect task termination.

**Affected code locations:**

1. **Main Consensus** - The core AptosBFT consensus spawns NetworkTask and EpochManager without monitoring: [1](#0-0) 

2. **DKG Runtime** - Distributed Key Generation for randomness spawns critical tasks unmonitored: [2](#0-1) 

3. **JWK Consensus** - JSON Web Key consensus spawns epoch manager and network task without monitoring: [3](#0-2) 

4. **Consensus Observer/Publisher** - Observer network handler and publisher spawns are unmonitored: [4](#0-3) [5](#0-4) 

**How tasks can exit silently:**

The `NetworkTask::start()` implementation processes network events in a loop that exits when the stream completes: [6](#0-5) 

The loop body processes messages, but when `self.all_events.next().await` returns `None` (stream closure), the function simply returns—no panic, no error log: [7](#0-6) 

**Detection gap:**

While a panic handler exists that catches task panics and exits the process: [8](#0-7) 

This only helps if tasks panic. Tasks that exit normally (stream closure, early return from bugs) go completely undetected.

The consensus health check only indirectly detects this issue after the node falls behind and switches to state sync: [9](#0-8) 

This gauge is updated by the state sync driver based on whether blocks are being executed from consensus or state sync: [10](#0-9) 

**Failure scenario:**
1. Network event stream closes due to network layer bug, resource exhaustion, or edge case
2. `NetworkTask::start()` loop exits normally (no panic)
3. JoinHandle was discarded at spawn, so exit goes undetected
4. Consensus messages stop being processed
5. EpochManager continues running but receives no messages
6. Validator misses rounds and eventually falls behind
7. Node switches to state sync mode (health check eventually fails)
8. Root cause (NetworkTask exit) is never logged or diagnosed

## Impact Explanation

**Severity: High** per Aptos bug bounty criteria:
- **Validator node slowdowns/failures**: Validators cannot participate in consensus, missing all rounds
- **Significant protocol violations**: Breaks the liveness guarantee that properly functioning validators participate in consensus
- **Validator slashing risk**: Missing consensus rounds can trigger validator penalties
- **Network liveness impact**: If multiple validators experience this issue, network liveness degrades

This violates the operational invariant that critical consensus components must remain running and failures must be immediately detectable for rapid operator response.

## Likelihood Explanation

**Likelihood: Medium**

While not trivially exploitable by external attackers, this can occur through:
- **Network layer bugs**: Edge cases in the networking stack causing stream closure
- **Resource exhaustion**: Memory pressure, file descriptor limits, or runtime issues
- **Shutdown race conditions**: Improper shutdown sequencing closing streams before tasks complete
- **Implementation bugs**: Future changes to networking code introducing stream closure paths

The codebase shows that stream completion is an expected possibility: [11](#0-10) 

The fact that multiple critical subsystems (consensus, DKG, JWK) all exhibit this pattern increases the likelihood that at least one will experience silent failure in production.

## Recommendation

**1. Store and monitor JoinHandles for critical tasks:**

```rust
// In consensus_provider.rs
pub fn start_consensus(...) -> (Runtime, Arc<StorageWriteProxy>, Arc<QuorumStoreDB>) {
    let runtime = aptos_runtimes::spawn_named_runtime("consensus".into(), None);
    // ... setup code ...
    
    let (network_task, network_receiver) = NetworkTask::new(network_service_events, self_receiver);
    
    // Store handles and spawn monitoring task
    let network_handle = runtime.spawn(network_task.start());
    let epoch_mgr_handle = runtime.spawn(epoch_mgr.start(timeout_receiver, network_receiver));
    
    // Spawn monitoring task
    runtime.spawn(monitor_critical_tasks(
        vec![
            ("NetworkTask", network_handle),
            ("EpochManager", epoch_mgr_handle),
        ]
    ));
    
    (runtime, storage, quorum_store_db)
}

async fn monitor_critical_tasks(tasks: Vec<(&'static str, JoinHandle<()>)>) {
    for (name, handle) in tasks {
        tokio::spawn(async move {
            match handle.await {
                Ok(_) => {
                    error!("Critical task {} exited unexpectedly", name);
                    // Exit process to trigger restart
                    std::process::exit(13);
                }
                Err(e) if e.is_panic() => {
                    error!("Critical task {} panicked: {:?}", name, e);
                    std::process::exit(13);
                }
                Err(e) => {
                    error!("Critical task {} join error: {:?}", name, e);
                    std::process::exit(13);
                }
            }
        });
    }
}
```

**2. Add explicit error logging in NetworkTask when stream closes:**

```rust
pub async fn start(mut self) {
    while let Some(message) = self.all_events.next().await {
        // ... process message ...
    }
    // This should never happen - log and panic
    error!("NetworkTask stream closed unexpectedly! This indicates a critical failure.");
    panic!("NetworkTask stream closed - consensus cannot function");
}
```

**3. Implement active liveness monitoring:**
- Add periodic heartbeat metrics from each critical task
- Monitor these metrics in the health check endpoint
- Alert operators when heartbeats stop

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tokio::sync::mpsc;
    
    #[tokio::test]
    async fn test_undetected_network_task_exit() {
        // Create a channel that we'll close to simulate stream closure
        let (tx, rx) = mpsc::unbounded_channel();
        
        // Wrap in a stream
        let stream = tokio_stream::wrappers::UnboundedReceiverStream::new(rx);
        let mut fused_stream = stream.fuse();
        
        // Simulate NetworkTask behavior
        let task_handle = tokio::spawn(async move {
            while let Some(_msg) = fused_stream.next().await {
                // Process message
            }
            // Task exits silently when stream closes
            println!("NetworkTask exited");
        });
        
        // Close the stream by dropping sender
        drop(tx);
        
        // Wait for task to complete
        let result = task_handle.await;
        
        // Task completed successfully (no panic), but this is a critical failure
        assert!(result.is_ok(), "Task should complete without panic");
        
        // In production, no one would know the task exited because JoinHandle was discarded
        println!("VULNERABILITY: Critical NetworkTask exited silently - no detection, no alerting!");
    }
    
    #[tokio::test] 
    async fn test_proper_monitoring() {
        let (tx, rx) = mpsc::unbounded_channel();
        let stream = tokio_stream::wrappers::UnboundedReceiverStream::new(rx);
        let mut fused_stream = stream.fuse();
        
        // Proper implementation: store and monitor JoinHandle
        let task_handle = tokio::spawn(async move {
            while let Some(_msg) = fused_stream.next().await {
                // Process message
            }
            panic!("NetworkTask stream closed - this should never happen");
        });
        
        // Monitor the handle
        let monitor_handle = tokio::spawn(async move {
            match task_handle.await {
                Ok(_) => panic!("Task exited without error - should have panicked"),
                Err(e) if e.is_panic() => {
                    println!("DETECTED: Task panic caught and can be handled");
                }
                Err(e) => panic!("Unexpected join error: {:?}", e),
            }
        });
        
        drop(tx);
        
        // Monitoring detects the failure
        let result = monitor_handle.await;
        assert!(result.is_ok(), "Monitor should detect task failure");
    }
}
```

## Notes

This vulnerability exists because Tokio's default behavior is to silently catch panics in spawned tasks. While the crash handler mitigates panics, tasks that exit normally remain undetected. The security question specifically addresses error handling around runtime spawns, and this finding demonstrates that critical consensus components lack proper spawn failure detection and monitoring, constituting a significant robustness and operational security issue.

### Citations

**File:** consensus/src/consensus_provider.rs (L119-120)
```rust
    runtime.spawn(network_task.start());
    runtime.spawn(epoch_mgr.start(timeout_receiver, network_receiver));
```

**File:** dkg/src/lib.rs (L53-54)
```rust
    runtime.spawn(network_task.start());
    runtime.spawn(dkg_epoch_manager.start(network_receiver));
```

**File:** crates/aptos-jwk-consensus/src/lib.rs (L47-48)
```rust
    runtime.spawn(network_task.start());
    runtime.spawn(epoch_manager.start(network_receiver));
```

**File:** aptos-node/src/consensus.rs (L260-260)
```rust
    runtime.spawn(
```

**File:** aptos-node/src/consensus.rs (L297-297)
```rust
    consensus_observer_runtime.spawn(consensus_observer_network_handler.start());
```

**File:** consensus/src/network.rs (L771-782)
```rust
        // Verify the network events have been constructed correctly
        let network_and_events = network_service_events.into_network_and_events();
        if (network_and_events.values().len() != 1)
            || !network_and_events.contains_key(&NetworkId::Validator)
        {
            panic!("The network has not been setup correctly for consensus!");
        }

        // Collect all the network events into a single stream
        let network_events: Vec<_> = network_and_events.into_values().collect();
        let network_events = select_all(network_events).fuse();
        let all_events = Box::new(select(network_events, self_receiver));
```

**File:** consensus/src/network.rs (L815-816)
```rust
    pub async fn start(mut self) {
        while let Some(message) = self.all_events.next().await {
```

**File:** consensus/src/network.rs (L1028-1029)
```rust
        }
    }
```

**File:** crates/crash-handler/src/lib.rs (L26-30)
```rust
pub fn setup_panic_handler() {
    panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}
```

**File:** crates/aptos-inspection-service/src/server/metrics.rs (L30-40)
```rust
    // Check the value of the consensus execution gauge
    let metrics = utils::get_all_metrics();
    if let Some(gauge_value) = metrics.get(CONSENSUS_EXECUTION_GAUGE) {
        if gauge_value == "1" {
            return (
                StatusCode::OK,
                Body::from("Consensus health check passed!"),
                CONTENT_TYPE_TEXT.into(),
            );
        }
    }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L744-748)
```rust
        if executing_component == ExecutingComponent::Consensus {
            metrics::CONSENSUS_EXECUTING_GAUGE.set(1);
        } else {
            metrics::CONSENSUS_EXECUTING_GAUGE.set(0);
        }
```
