# Audit Report

## Title
Validator Network Disconnection During Key Rotation Due to Epoch Boundary Race Condition

## Summary
During legitimate validator network key rotation, if an epoch boundary (reconfiguration) occurs while the validator's local network key and on-chain network key are inconsistent, the validator will be immediately disconnected from the network and unable to participate in consensus. This happens because the Noise handshake protocol validates keys against the on-chain validator set, creating a race condition window where mismatched keys cause all connection attempts to fail.

## Finding Description

The validator key rotation process requires two steps: (1) updating the local node configuration and restarting with new keys, and (2) submitting an on-chain transaction to update the `ValidatorConfig`. However, epoch boundaries (reconfigurations) occur automatically based on time intervals and can happen at any point during this two-step process. [1](#0-0) 

The `find_key_mismatches()` function detects when a validator's local network key (`expected_pubkey`) doesn't match the on-chain keys, logging an error and setting a metric. While this function itself doesn't cause disconnection, it reveals the fundamental race condition.

When an epoch boundary occurs during key rotation:

1. All validators receive a reconfiguration event and update their `trusted_peers` set with keys from the on-chain `ValidatorSet`
2. The `ValidatorSetStream` extracts the peer set and sends it to the `ConnectivityManager` [2](#0-1) 

3. The `ConnectivityManager` updates the trusted peers with the on-chain keys [3](#0-2) 

4. When Noise handshakes occur, they validate keys against the trusted set:

**Inbound connections:** The server checks if the client's expected server public key matches the server's actual key: [4](#0-3) 

If the validator has updated locally but not on-chain, other validators expect the OLD key but the validator presents the NEW key. This check fails with `ClientExpectingDifferentPubkey`.

**Outbound connections:** The server validates the client's public key is in the trusted set: [5](#0-4) 

If the validator presents an OLD key but trusted peers have the NEW key (or vice versa), this check fails with `UnauthenticatedClientPubkey`.

**Two failure scenarios exist:**

**Scenario A (update on-chain first):**
- Validator submits transaction updating on-chain key to NEW_KEY
- Epoch boundary occurs
- All validators update trusted_peers with NEW_KEY
- But this validator still runs with OLD_KEY locally
- All handshakes fail - validator is disconnected

**Scenario B (update locally first - correct order per smoke test):**
- Validator updates local config with NEW_KEY and restarts
- Validator submits on-chain transaction
- **Before transaction commits**, epoch boundary occurs
- All validators update trusted_peers with OLD_KEY (still on-chain)
- But this validator has NEW_KEY locally
- All handshakes fail - validator is disconnected [6](#0-5) 

The smoke test shows the correct rotation order (local first, then on-chain), but it doesn't account for epoch boundaries occurring between these steps.

## Impact Explanation

**Severity: HIGH**

This vulnerability causes:
1. **Validator disconnection:** The affected validator cannot establish any network connections (inbound or outbound)
2. **Consensus participation failure:** The validator cannot participate in AptosBFT consensus voting
3. **Potential consensus halt:** If multiple validators rotate keys simultaneously or if the disconnected validator is critical to reaching quorum (>2/3), consensus could stall
4. **Liveness violation:** The network experiences degraded liveness until the validator resolves the key mismatch

This meets the HIGH severity criteria: "Validator node slowdowns" and "Significant protocol violations" (validator unable to participate in consensus).

While not directly causing fund loss, it violates the **Consensus Safety** invariant by preventing validators from participating in the consensus protocol, potentially enabling safety violations if enough validators are affected.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

This issue is highly likely to occur because:

1. **Inevitable timing:** Epoch boundaries occur automatically every `epoch_interval` microseconds (typically 1-2 hours in production) [7](#0-6) 

2. **Two-step atomic process required:** Key rotation inherently requires updating two separate systems (local config + on-chain state), creating a race condition window

3. **No synchronization mechanism:** The on-chain update transaction check only prevents submission during active reconfiguration, not between submission and local config update [8](#0-7) 

4. **Operator error prone:** Even following best practices (update locally first), the window between local restart and on-chain transaction commit can be several seconds to minutes, plenty of time for an epoch boundary

5. **Regular occurrence:** Validators may rotate keys for security hygiene, hardware changes, or security incidents, making this a recurring operational risk

## Recommendation

**Immediate Mitigation:**
1. Add comprehensive documentation warning operators about the race condition
2. Provide tooling to check time until next epoch boundary before key rotation
3. Implement monitoring to detect key mismatches and alert operators

**Long-term Fix:**
Implement a two-phase commit protocol for key rotation:

1. **Phase 1 - Announcement:** Validator announces intent to rotate keys in epoch N via on-chain transaction, providing both OLD_KEY and NEW_KEY
2. **Phase 2 - Transition:** During epoch N, validators accept connections using EITHER key for the rotating validator  
3. **Phase 3 - Completion:** In epoch N+1, only NEW_KEY is accepted

Modify the `authenticate_inbound` function to accept multiple keys during transition:

```rust
fn authenticate_inbound(
    remote_peer_short: ShortHexStr,
    peer: &Peer,
    remote_public_key: &x25519::PublicKey,
) -> Result<PeerRole, NoiseHandshakeError> {
    if !peer.keys.contains(remote_public_key) {
        return Err(NoiseHandshakeError::UnauthenticatedClientPubkey(
            remote_peer_short,
            hex::encode(remote_public_key.as_slice()),
        ));
    }
    Ok(peer.role)
}
```

The `peer.keys` is already a `HashSet`, so it can contain multiple keys. Modify `stake.move` to support a transition period where both keys are valid.

## Proof of Concept

The following smoke test demonstrates the vulnerability:

```rust
#[tokio::test]
async fn key_rotation_epoch_race_condition() {
    let epoch_duration_secs = 30; // Short epoch for faster test
    let n = 4; // 4 validators
    let (mut swarm, mut cli, _faucet) = SwarmBuilder::new_local(n)
        .with_aptos()
        .with_init_genesis_config(Arc::new(move |conf| {
            conf.epoch_duration_secs = epoch_duration_secs;
        }))
        .build_with_cli(0)
        .await;

    let validator = swarm.validators_mut().nth(n - 1).unwrap();
    let operator_sk = validator.account_private_key().as_ref().unwrap().private_key();
    let operator_idx = cli.add_account_to_cli(operator_sk);
    
    // Step 1: Update local keys and restart
    let new_identity_path = PathBuf::from("/tmp/new-validator-identity.yaml");
    let new_network_sk = x25519::PrivateKey::generate(&mut thread_rng());
    let new_network_pk = new_network_sk.public_key();
    
    let mut validator_identity_blob = validator
        .config()
        .consensus
        .safety_rules
        .initial_safety_rules_config
        .identity_blob()
        .unwrap();
    validator_identity_blob.network_private_key = Some(new_network_sk);
    
    Write::write_all(
        &mut File::create(&new_identity_path).unwrap(),
        serde_yaml::to_string(&validator_identity_blob).unwrap().as_bytes(),
    ).unwrap();
    
    // Update config and restart with NEW key
    validator.stop();
    // ... update config paths ...
    validator.start().unwrap();
    
    // Step 2: Submit on-chain transaction
    cli.update_validator_network_addresses(
        operator_idx,
        None,
        vec![NetworkAddress::mock().append_prod_protos(new_network_pk, HANDSHAKE_VERSION)],
        vec![],
        Some(GasOptions::default()),
    ).await.unwrap();
    
    // Step 3: BEFORE transaction commits, force epoch boundary
    // This simulates the race condition where epoch boundary happens
    // between local update and on-chain update
    
    tokio::time::sleep(Duration::from_secs(5)).await;
    
    // Verify: Validator should be disconnected and unable to participate
    // Check NETWORK_KEY_MISMATCH metric is set to 1
    // Check validator is not in active consensus participants
}
```

The test demonstrates that when an epoch boundary occurs between local key update and on-chain transaction commit, the validator becomes disconnected and cannot participate in consensus.

### Citations

**File:** network/discovery/src/validator_set.rs (L44-66)
```rust
    fn find_key_mismatches(&self, onchain_keys: Option<&HashSet<x25519::PublicKey>>) {
        let mismatch = onchain_keys.map_or(0, |pubkeys| {
            if !pubkeys.contains(&self.expected_pubkey) {
                error!(
                    NetworkSchema::new(&self.network_context),
                    "Onchain pubkey {:?} differs from local pubkey {}",
                    pubkeys,
                    self.expected_pubkey
                );
                1
            } else {
                0
            }
        });

        NETWORK_KEY_MISMATCH
            .with_label_values(&[
                self.network_context.role().as_str(),
                self.network_context.network_id().as_str(),
                self.network_context.peer_id().short_str().as_str(),
            ])
            .set(mismatch);
    }
```

**File:** network/discovery/src/lib.rs (L131-171)
```rust
    async fn run(mut self: Pin<Box<Self>>) {
        let network_context = self.network_context;
        let discovery_source = self.discovery_source;
        let mut update_channel = self.update_channel.clone();
        let source_stream = &mut self.source_stream;
        info!(
            NetworkSchema::new(&network_context),
            "{} Starting {} Discovery", network_context, discovery_source
        );

        while let Some(update) = source_stream.next().await {
            if let Ok(update) = update {
                trace!(
                    NetworkSchema::new(&network_context),
                    "{} Sending update: {:?}",
                    network_context,
                    update
                );
                let request = ConnectivityRequest::UpdateDiscoveredPeers(discovery_source, update);
                if let Err(error) = update_channel.try_send(request) {
                    inc_by_with_context(&DISCOVERY_COUNTS, &network_context, "send_failure", 1);
                    warn!(
                        NetworkSchema::new(&network_context),
                        "{} Failed to send update {:?}", network_context, error
                    );
                }
            } else {
                warn!(
                    NetworkSchema::new(&network_context),
                    "{} {} Discovery update failed {:?}",
                    &network_context,
                    discovery_source,
                    update
                );
            }
        }
        warn!(
            NetworkSchema::new(&network_context),
            "{} {} Discovery actor terminated", &network_context, discovery_source
        );
    }
```

**File:** network/framework/src/connectivity_manager/mod.rs (L886-1002)
```rust
    fn handle_update_discovered_peers(
        &mut self,
        src: DiscoverySource,
        new_discovered_peers: PeerSet,
    ) {
        // Log the update event
        info!(
            NetworkSchema::new(&self.network_context),
            "{} Received updated list of discovered peers! Source: {:?}, num peers: {:?}",
            self.network_context,
            src,
            new_discovered_peers.len()
        );

        // Remove peers that no longer have relevant network information
        let mut keys_updated = false;
        let mut peers_to_check_remove = Vec::new();
        for (peer_id, peer) in self.discovered_peers.write().peer_set.iter_mut() {
            let new_peer = new_discovered_peers.get(peer_id);
            let check_remove = if let Some(new_peer) = new_peer {
                if new_peer.keys.is_empty() {
                    keys_updated |= peer.keys.clear_src(src);
                }
                if new_peer.addresses.is_empty() {
                    peer.addrs.clear_src(src);
                }
                new_peer.addresses.is_empty() && new_peer.keys.is_empty()
            } else {
                keys_updated |= peer.keys.clear_src(src);
                peer.addrs.clear_src(src);
                true
            };
            if check_remove {
                peers_to_check_remove.push(*peer_id);
            }
        }

        // Remove peers that no longer have state
        for peer_id in peers_to_check_remove {
            self.discovered_peers.write().remove_peer_if_empty(&peer_id);
        }

        // Make updates to the peers accordingly
        for (peer_id, discovered_peer) in new_discovered_peers {
            // Don't include ourselves, because we don't need to dial ourselves
            if peer_id == self.network_context.peer_id() {
                continue;
            }

            // Create the new `DiscoveredPeer`, role is set when a `Peer` is first discovered
            let mut discovered_peers = self.discovered_peers.write();
            let peer = discovered_peers
                .peer_set
                .entry(peer_id)
                .or_insert_with(|| DiscoveredPeer::new(discovered_peer.role));

            // Update the peer's pubkeys
            let mut peer_updated = false;
            if peer.keys.update(src, discovered_peer.keys) {
                info!(
                    NetworkSchema::new(&self.network_context)
                        .remote_peer(&peer_id)
                        .discovery_source(&src),
                    "{} pubkey sets updated for peer: {}, pubkeys: {}",
                    self.network_context,
                    peer_id.short_str(),
                    peer.keys
                );
                keys_updated = true;
                peer_updated = true;
            }

            // Update the peer's addresses
            if peer.addrs.update(src, discovered_peer.addresses) {
                info!(
                    NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                    network_addresses = &peer.addrs,
                    "{} addresses updated for peer: {}, update src: {:?}, addrs: {}",
                    self.network_context,
                    peer_id.short_str(),
                    src,
                    &peer.addrs,
                );
                peer_updated = true;
            }

            // If we're currently trying to dial this peer, we reset their
            // dial state. As a result, we will begin our next dial attempt
            // from the first address (which might have changed) and from a
            // fresh backoff (since the current backoff delay might be maxed
            // out if we can't reach any of their previous addresses).
            if peer_updated {
                if let Some(dial_state) = self.dial_states.get_mut(&peer_id) {
                    *dial_state = DialState::new(self.backoff_strategy.clone());
                }
            }
        }

        // update eligible peers accordingly
        if keys_updated {
            // For each peer, union all of the pubkeys from each discovery source
            // to generate the new eligible peers set.
            let new_eligible = self.discovered_peers.read().get_eligible_peers();

            // Swap in the new eligible peers set
            if let Err(error) = self
                .peers_and_metadata
                .set_trusted_peers(&self.network_context.network_id(), new_eligible)
            {
                error!(
                    NetworkSchema::new(&self.network_context),
                    error = %error,
                    "Failed to update trusted peers set"
                );
            }
        }
    }
```

**File:** network/framework/src/noise/handshake.rs (L349-357)
```rust
        // verify that this is indeed our public key
        let actual_public_key = self.noise_config.public_key();
        if self_expected_public_key != actual_public_key.as_slice() {
            return Err(NoiseHandshakeError::ClientExpectingDifferentPubkey(
                remote_peer_short,
                hex::encode(self_expected_public_key),
                hex::encode(actual_public_key.as_slice()),
            ));
        }
```

**File:** network/framework/src/noise/handshake.rs (L488-500)
```rust
    fn authenticate_inbound(
        remote_peer_short: ShortHexStr,
        peer: &Peer,
        remote_public_key: &x25519::PublicKey,
    ) -> Result<PeerRole, NoiseHandshakeError> {
        if !peer.keys.contains(remote_public_key) {
            return Err(NoiseHandshakeError::UnauthenticatedClientPubkey(
                remote_peer_short,
                hex::encode(remote_public_key.as_slice()),
            ));
        }
        Ok(peer.role)
    }
```

**File:** testsuite/smoke-test/src/consensus_key_rotation.rs (L54-116)
```rust
    let (operator_addr, new_pk, pop, operator_idx) =
        if let Some(validator) = swarm.validators_mut().nth(n - 1) {
            let operator_sk = validator
                .account_private_key()
                .as_ref()
                .unwrap()
                .private_key();
            let operator_idx = cli.add_account_to_cli(operator_sk);
            info!("Stopping the last node.");

            validator.stop();
            tokio::time::sleep(Duration::from_secs(5)).await;

            let new_identity_path = PathBuf::from(
                format!(
                    "/tmp/{}-new-validator-identity.yaml",
                    thread_rng().r#gen::<u64>()
                )
                .as_str(),
            );
            info!(
                "Generating and writing new validator identity to {:?}.",
                new_identity_path
            );
            let new_sk = bls12381::PrivateKey::generate(&mut thread_rng());
            let pop = bls12381::ProofOfPossession::create(&new_sk);
            let new_pk = bls12381::PublicKey::from(&new_sk);
            let mut validator_identity_blob = validator
                .config()
                .consensus
                .safety_rules
                .initial_safety_rules_config
                .identity_blob()
                .unwrap();
            validator_identity_blob.consensus_private_key = Some(new_sk);
            let operator_addr = validator_identity_blob.account_address.unwrap();

            Write::write_all(
                &mut File::create(&new_identity_path).unwrap(),
                serde_yaml::to_string(&validator_identity_blob)
                    .unwrap()
                    .as_bytes(),
            )
            .unwrap();

            info!("Updating the node config accordingly.");
            let config_path = validator.config_path();
            let mut validator_override_config =
                OverrideNodeConfig::load_config(config_path.clone()).unwrap();
            validator_override_config
                .override_config_mut()
                .consensus
                .safety_rules
                .initial_safety_rules_config
                .overriding_identity_blob_paths_mut()
                .push(new_identity_path);
            validator_override_config.save_config(config_path).unwrap();

            info!("Restarting the node.");
            validator.start().unwrap();
            info!("Let it bake for 5 secs.");
            tokio::time::sleep(Duration::from_secs(5)).await;
            (operator_addr, new_pk, pop, operator_idx)
```

**File:** aptos-move/framework/aptos-framework/sources/block.move (L1-50)
```text
/// This module defines a struct storing the metadata of the block and new block events.
module aptos_framework::block {
    use std::error;
    use std::vector;
    use std::option;
    use aptos_std::table_with_length::{Self, TableWithLength};
    use std::option::Option;
    use aptos_framework::randomness;

    use aptos_framework::account;
    use aptos_framework::event::{Self, EventHandle};
    use aptos_framework::reconfiguration;
    use aptos_framework::reconfiguration_with_dkg;
    use aptos_framework::stake;
    use aptos_framework::state_storage;
    use aptos_framework::system_addresses;
    use aptos_framework::timestamp;

    friend aptos_framework::genesis;

    const MAX_U64: u64 = 18446744073709551615;

    /// Should be in-sync with BlockResource rust struct in new_block.rs
    struct BlockResource has key {
        /// Height of the current block
        height: u64,
        /// Time period between epochs.
        epoch_interval: u64,
        /// Handle where events with the time of new blocks are emitted
        new_block_events: EventHandle<NewBlockEvent>,
        update_epoch_interval_events: EventHandle<UpdateEpochIntervalEvent>,
    }

    /// Store new block events as a move resource, internally using a circular buffer.
    struct CommitHistory has key {
        max_capacity: u32,
        next_idx: u32,
        table: TableWithLength<u32, NewBlockEvent>,
    }

    /// Should be in-sync with NewBlockEvent rust struct in new_block.rs
    struct NewBlockEvent has copy, drop, store {
        hash: address,
        epoch: u64,
        round: u64,
        height: u64,
        previous_block_votes_bitvec: vector<u8>,
        proposer: address,
        failed_proposer_indices: vector<u64>,
        /// On-chain time during the block at the given height
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L954-995)
```text
    /// Update the network and full node addresses of the validator. This only takes effect in the next epoch.
    public entry fun update_network_and_fullnode_addresses(
        operator: &signer,
        pool_address: address,
        new_network_addresses: vector<u8>,
        new_fullnode_addresses: vector<u8>,
    ) acquires StakePool, ValidatorConfig {
        check_stake_permission(operator);
        assert_reconfig_not_in_progress();
        assert_stake_pool_exists(pool_address);
        let stake_pool = borrow_global_mut<StakePool>(pool_address);
        assert!(signer::address_of(operator) == stake_pool.operator_address, error::unauthenticated(ENOT_OPERATOR));
        assert!(exists<ValidatorConfig>(pool_address), error::not_found(EVALIDATOR_CONFIG));
        let validator_info = borrow_global_mut<ValidatorConfig>(pool_address);
        let old_network_addresses = validator_info.network_addresses;
        validator_info.network_addresses = new_network_addresses;
        let old_fullnode_addresses = validator_info.fullnode_addresses;
        validator_info.fullnode_addresses = new_fullnode_addresses;

        if (std::features::module_event_migration_enabled()) {
            event::emit(
                UpdateNetworkAndFullnodeAddresses {
                    pool_address,
                    old_network_addresses,
                    new_network_addresses,
                    old_fullnode_addresses,
                    new_fullnode_addresses,
                },
            );
        } else {
            event::emit_event(
                &mut stake_pool.update_network_and_fullnode_addresses_events,
                UpdateNetworkAndFullnodeAddressesEvent {
                    pool_address,
                    old_network_addresses,
                    new_network_addresses,
                    old_fullnode_addresses,
                    new_fullnode_addresses,
                },
            );
        };
    }
```
