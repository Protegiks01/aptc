# Audit Report

## Title
Network Partition via Inconsistent window_size Configuration During Validator Recovery

## Summary
The `find_root()` function routes to different recovery code paths based on `window_size`, causing validators with inconsistent configurations to calculate different root blocks during recovery. This can lead to consensus divergence and network partition requiring a hardfork to resolve.

## Finding Description

The vulnerability exists in the consensus recovery mechanism where validators reconstruct their block tree state after a restart. The `find_root()` function at lines 290-295 performs a match on `window_size`: [1](#0-0) 

This routing causes fundamentally different recovery behaviors:

**When window_size = None:** The `find_root_without_window()` function uses the committed block directly as the root: [2](#0-1) 

**When window_size = Some(value):** The `find_root_with_window()` function calculates a window_start_block by walking backwards from the commit block: [3](#0-2) 

The critical divergence occurs in `RecoveryData::new()` where different root_ids are used for block tree construction: [4](#0-3) 

Validators with `window_size = None` use `commit_root_block.id()` as their tree root, while validators with `window_size = Some(100)` use a block 100 rounds earlier. This causes `find_blocks_to_prune()` to retain different block sets: [5](#0-4) 

**How Inconsistency Occurs:**

The `window_size` value comes from `OnChainConsensusConfig` fetched during epoch initialization. If config reading fails, validators silently fall back to default: [6](#0-5) 

The default window_size is None: [7](#0-6) 

**Attack Scenario:**

1. Network upgrades from config V4 to V5 with execution pool enabled (window_size = Some(100))
2. Due to validator software version mismatch during staged rollout:
   - Validators with updated software: successfully deserialize V5 config, get window_size = Some(100)
   - Validators with old software: fail to deserialize V5 config, fall back to default window_size = None
3. Both groups participate in consensus during epoch N+1
4. Several validators from both groups restart during the epoch (round ~1100)
5. Recovery divergence:
   - Group A (window_size = None): sets tree root to block 1100, prunes all blocks before 1100
   - Group B (window_size = Some(100)): sets tree root to block 1001, retains blocks 1001-1100
6. When proposals reference blocks in range 1001-1100:
   - Group A validators cannot validate (blocks pruned)
   - Group B validators validate successfully
7. Neither group can form a 2/3 quorum, causing network partition

## Impact Explanation

This vulnerability meets **Critical Severity** criteria per Aptos bug bounty:

- **Non-recoverable network partition (requires hardfork):** Once validators diverge on their block tree roots, they cannot reconcile without manual intervention. Different groups will build on incompatible chain views, splitting the network into separate consensus partitions that cannot merge.

- **Consensus Safety violation:** Breaks the fundamental invariant that "All validators must produce identical state roots for identical blocks." Validators with different block trees may commit conflicting transactions on incompatible forks.

- **Total loss of liveness:** If validator distribution is roughly even between configurations, neither group can achieve the 2/3+ quorum required for consensus, halting the entire network.

The impact is network-wide and affects all transactions and users. Recovery requires coordinated hardfork with manual validator intervention.

## Likelihood Explanation

**Likelihood: Medium to High during network upgrades**

The vulnerability is most likely to manifest during:

1. **Staged validator upgrades:** When on-chain config format changes (V4â†’V5), validators upgrade gradually. During the transition window (which could be hours or days), inconsistent config deserialization is expected operational behavior.

2. **Config schema evolution:** Any time the consensus config schema changes incompatibly, older validators cannot deserialize the new format.

3. **Validator restarts:** Common during routine maintenance, software updates, or infrastructure issues. If multiple validators restart during the inconsistent config period, divergence occurs.

The vulnerability does NOT require:
- Malicious validator behavior
- Attacker-controlled infrastructure  
- Byzantine nodes or collusion

It can occur naturally during legitimate network operations, making it a design flaw rather than requiring active exploitation.

## Recommendation

**Immediate Fix: Make config deserialization failure fatal**

Replace the silent fallback with explicit validation:

```rust
// In epoch_manager.rs start_new_epoch()
let consensus_config = onchain_consensus_config
    .expect("FATAL: Cannot deserialize OnChainConsensusConfig. Validator software may be incompatible with network version.");
```

**Long-term Fixes:**

1. **Config version compatibility enforcement:**
   - Add minimum software version field to on-chain config
   - Reject validators running incompatible versions from participating
   - Enforce version checks before epoch transition

2. **Deterministic recovery roots:**
   - Store the window_size value used during block persistence alongside the blocks
   - During recovery, use the historical window_size rather than current config
   - This ensures recovery uses the same parameters as original execution

3. **Recovery consistency checks:**
   - Add validation that all validators agree on root_id before participating in consensus
   - Broadcast root_id hashes during recovery and verify 2/3+ agreement
   - Halt and alert if inconsistency detected

## Proof of Concept

```rust
// Simulation demonstrating divergent recovery paths
// To run: place in consensus/src/persistent_liveness_storage.rs tests

#[cfg(test)]
mod divergence_test {
    use super::*;
    
    #[test]
    fn test_window_size_recovery_divergence() {
        // Setup: Create ledger recovery data at round 1100
        let storage_ledger = create_test_ledger_info(1100);
        let ledger_recovery = LedgerRecoveryData::new(storage_ledger);
        
        // Create blocks from round 1000 to 1110
        let mut blocks_a = create_test_blocks(1000, 1110);
        let mut blocks_b = blocks_a.clone();
        let mut qcs_a = create_test_qcs(&blocks_a);
        let mut qcs_b = qcs_a.clone();
        
        // Validator A: window_size = None (old config/fallback)
        let root_a = ledger_recovery.find_root(
            &mut blocks_a,
            &mut qcs_a,
            true,
            None, // window_size = None
        ).expect("Recovery A failed");
        
        // Validator B: window_size = Some(100) (new config)
        let root_b = ledger_recovery.find_root(
            &mut blocks_b,
            &mut qcs_b,
            true,
            Some(100), // window_size = Some(100)
        ).expect("Recovery B failed");
        
        // Verify divergence
        let root_id_a = match root_a.window_root_block {
            None => root_a.commit_root_block.id(),
            Some(ref w) => w.id(),
        };
        
        let root_id_b = match root_b.window_root_block {
            None => root_b.commit_root_block.id(),
            Some(ref w) => w.id(),
        };
        
        // Critical assertion: roots are DIFFERENT
        assert_ne!(
            root_id_a, 
            root_id_b,
            "Validators with different window_size should have different roots"
        );
        
        // Verify blocks_a has fewer retained blocks than blocks_b
        assert!(
            blocks_a.len() < blocks_b.len(),
            "Validator A (window_size=None) should prune more blocks than B (window_size=Some(100))"
        );
        
        println!("VULNERABILITY CONFIRMED:");
        println!("  Validator A root: {:?} with {} blocks", root_id_a, blocks_a.len());
        println!("  Validator B root: {:?} with {} blocks", root_id_b, blocks_b.len());
        println!("  Network partition risk: HIGH");
    }
}
```

## Notes

This vulnerability represents a **critical protocol design flaw** in the recovery mechanism. While the trigger requires validators to have inconsistent configurations (typically during upgrades), this is a **realistic operational scenario** rather than a theoretical attack. 

The silent fallback to default configuration (unwrap_or_default) masks the inconsistency, allowing validators to participate in consensus with incompatible states. This violates the deterministic execution invariant and creates a non-Byzantine path to network partition.

The fix must ensure that configuration inconsistency is detected and prevented before validators can diverge, rather than allowing silent failures that compromise consensus safety.

### Citations

**File:** consensus/src/persistent_liveness_storage.rs (L165-187)
```rust
        let window_start_round = calculate_window_start_round(commit_block.round(), window_size);
        let mut id_to_blocks = HashMap::new();
        blocks.iter().for_each(|block| {
            id_to_blocks.insert(block.id(), block);
        });

        let mut current_block = &commit_block;
        while !current_block.is_genesis_block()
            && current_block.quorum_cert().certified_block().round() >= window_start_round
        {
            if let Some(parent_block) = id_to_blocks.get(&current_block.parent_id()) {
                current_block = *parent_block;
            } else {
                bail!("Parent block not found for block {}", current_block.id());
            }
        }
        let window_start_id = current_block.id();

        let window_start_idx = blocks
            .iter()
            .position(|block| block.id() == window_start_id)
            .ok_or_else(|| format_err!("unable to find window root: {}", window_start_id))?;
        let window_start_block = blocks.remove(window_start_idx);
```

**File:** consensus/src/persistent_liveness_storage.rs (L233-237)
```rust
        let root_idx = blocks
            .iter()
            .position(|block| block.id() == root_id)
            .ok_or_else(|| format_err!("unable to find root: {}", root_id))?;
        let root_block = blocks.remove(root_idx);
```

**File:** consensus/src/persistent_liveness_storage.rs (L290-295)
```rust
        match window_size {
            None => self.find_root_without_window(blocks, quorum_certs, order_vote_enabled),
            Some(window_size) => {
                self.find_root_with_window(blocks, quorum_certs, order_vote_enabled, window_size)
            },
        }
```

**File:** consensus/src/persistent_liveness_storage.rs (L386-402)
```rust
        let (root_id, epoch) = match &root.window_root_block {
            None => {
                let commit_root_id = root.commit_root_block.id();
                let epoch = root.commit_root_block.epoch();
                (commit_root_id, epoch)
            },
            Some(window_root_block) => {
                let window_start_id = window_root_block.id();
                let epoch = window_root_block.epoch();
                (window_start_id, epoch)
            },
        };
        let blocks_to_prune = Some(Self::find_blocks_to_prune(
            root_id,
            &mut blocks,
            &mut quorum_certs,
        ));
```

**File:** consensus/src/persistent_liveness_storage.rs (L448-476)
```rust
    fn find_blocks_to_prune(
        root_id: HashValue,
        blocks: &mut Vec<Block>,
        quorum_certs: &mut Vec<QuorumCert>,
    ) -> Vec<HashValue> {
        // prune all the blocks that don't have root as ancestor
        let mut tree = HashSet::new();
        let mut to_remove = HashSet::new();
        tree.insert(root_id);
        // assume blocks are sorted by round already
        blocks.retain(|block| {
            if tree.contains(&block.parent_id()) {
                tree.insert(block.id());
                true
            } else {
                to_remove.insert(block.id());
                false
            }
        });
        quorum_certs.retain(|qc| {
            if tree.contains(&qc.certified_block().id()) {
                true
            } else {
                to_remove.insert(qc.certified_block().id());
                false
            }
        });
        to_remove.into_iter().collect()
    }
```

**File:** consensus/src/epoch_manager.rs (L1178-1201)
```rust
        let onchain_consensus_config: anyhow::Result<OnChainConsensusConfig> = payload.get();
        let onchain_execution_config: anyhow::Result<OnChainExecutionConfig> = payload.get();
        let onchain_randomness_config_seq_num: anyhow::Result<RandomnessConfigSeqNum> =
            payload.get();
        let randomness_config_move_struct: anyhow::Result<RandomnessConfigMoveStruct> =
            payload.get();
        let onchain_jwk_consensus_config: anyhow::Result<OnChainJWKConsensusConfig> = payload.get();
        let dkg_state = payload.get::<DKGState>();

        if let Err(error) = &onchain_consensus_config {
            warn!("Failed to read on-chain consensus config {}", error);
        }

        if let Err(error) = &onchain_execution_config {
            warn!("Failed to read on-chain execution config {}", error);
        }

        if let Err(error) = &randomness_config_move_struct {
            warn!("Failed to read on-chain randomness config {}", error);
        }

        self.epoch_state = Some(epoch_state.clone());

        let consensus_config = onchain_consensus_config.unwrap_or_default();
```

**File:** types/src/on_chain_config/consensus_config.rs (L443-451)
```rust
impl Default for OnChainConsensusConfig {
    fn default() -> Self {
        OnChainConsensusConfig::V4 {
            alg: ConsensusAlgorithmConfig::default_if_missing(),
            vtxn: ValidatorTxnConfig::default_if_missing(),
            window_size: DEFAULT_WINDOW_SIZE,
        }
    }
}
```
