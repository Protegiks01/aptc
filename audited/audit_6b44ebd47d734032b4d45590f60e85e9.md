# Audit Report

## Title
Missing Root Hash Verification in State Snapshot Restoration Allows Acceptance of Corrupted State Trees

## Summary
The `StateSnapshotReceiver::finish()` function completes state snapshot restoration without verifying that the final computed Merkle root hash matches the `expected_root_hash`. This allows an attacker to provide partial or corrupted state data and have it accepted as valid, breaking the fundamental state consistency invariant of the Aptos blockchain.

## Finding Description

The state snapshot restoration process in Aptos uses the Jellyfish Merkle Tree to restore state from chunks. The restoration process involves:

1. Initialization with an `expected_root_hash` that represents the correct final state [1](#0-0) 

2. Adding chunks of state data with proofs via `add_chunk_impl()` [2](#0-1) 

3. Finalizing the restoration via `finish_impl()` [3](#0-2) 

**The Critical Vulnerability:**

The `finish_impl()` function performs the following operations:
- Waits for async commits to complete
- Handles special cases (single leaf or null tree)
- Calls `freeze(0)` to freeze all remaining partial nodes
- Writes the frozen nodes to storage via `write_node_batch()`
- Returns `Ok(())`

**Crucially, there is NO verification that the final root node's hash matches `expected_root_hash`.**

The only verification that occurs is during `add_chunk_impl()`, which calls `verify()` to check that the chunks added SO FAR are consistent with a POSSIBLE path to the expected root hash using range proofs. [4](#0-3) 

However, this proof-based verification only confirms that the partial state received is consistent with the expected root - it does NOT verify that ALL necessary state has been received.

**Attack Scenario:**

1. Attacker initiates state snapshot restoration with a legitimate `expected_root_hash` (e.g., hash for a state tree with 10,000 accounts)
2. Attacker sends only 5,000 accounts worth of state chunks with valid proofs
3. Each chunk passes `verify()` because the proofs are valid for a potential path to the root
4. Attacker calls `finish()` prematurely
5. `finish_impl()` freezes the incomplete tree structure, computes hashes for only the 5,000 accounts, and writes this corrupted tree to storage
6. The function returns `Ok(())` without detecting that the actual root hash differs from `expected_root_hash`
7. The incomplete/corrupted state tree is now persisted and may be used by the node

The implementation delegates to `StateSnapshotRestore::finish()`: [5](#0-4) 

This subsequently calls `JellyfishMerkleRestore::finish_impl()`, which performs no root hash verification.

The state sync driver calls this without any subsequent verification: [6](#0-5) 

And the subsequent `finalize_state_snapshot()` also does not verify the root hash: [7](#0-6) 

## Impact Explanation

This vulnerability has **CRITICAL** severity impact:

**1. State Consistency Violation (Critical Invariant #4)**
- The Aptos blockchain requires that "State transitions must be atomic and verifiable via Merkle proofs"
- Accepting an incomplete/corrupted state tree violates this fundamental invariant
- The state is no longer cryptographically verifiable against the expected root hash

**2. Consensus Safety Risk**
- Different nodes could accept different incomplete state trees if they receive different subsets of chunks
- This breaks deterministic execution (Critical Invariant #1) - validators would have different state roots for the same version
- Could lead to chain splits and consensus failure

**3. State Corruption**
- Missing account data means queries will return incorrect results (accounts appear to not exist)
- Transaction execution will fail or produce incorrect results when accessing missing state
- The node operates with an invalid, unverifiable state

**4. Non-Recoverable Without Hard Fork**
- Once the corrupted state is committed and the node continues operation, the state divergence propagates
- Requires manual intervention or hard fork to recover
- Meets the "Non-recoverable network partition (requires hardfork)" critical severity category

This qualifies for **Critical Severity** (up to $1,000,000) under the Aptos Bug Bounty program as it enables:
- Consensus/Safety violations
- Non-recoverable network partition scenarios
- State corruption affecting all network participants

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

**Attacker Requirements:**
- Ability to act as a state sync peer (low barrier - any network participant)
- Ability to provide valid Merkle range proofs for partial data (straightforward if attacker has access to any valid state snapshot)
- No validator access or insider privileges required

**Feasibility:**
- The attack is technically straightforward - simply stop sending chunks early and call finish()
- The vulnerability is in the core state restoration logic, affecting all nodes during state sync
- State sync is a common operation (new nodes joining, nodes recovering from crashes)

**Detection Difficulty:**
- The bug is subtle - proofs verify correctly for partial data
- No error or warning is generated when finish() accepts corrupted state
- The only detection point is in `JellyfishMerkleRestore::new()` on SUBSEQUENT initialization, which checks if a previous restore has the correct root hash - but this is too late

**Mitigating Factors:**
- Honest nodes would reject serving incomplete snapshots
- In practice, state sync peers are somewhat trusted
- However, this is a defense-in-depth failure - the code should verify correctness regardless of peer trust

## Recommendation

**Add explicit root hash verification in `finish_impl()` before writing nodes to storage:**

The fix should compute the actual root hash after freezing all nodes and verify it matches `expected_root_hash`. The verification should occur BEFORE writing to storage:

```rust
pub fn finish_impl(mut self) -> Result<()> {
    self.wait_for_async_commit()?;
    
    // ... existing special case handling ...
    
    self.freeze(0);
    
    // NEW: Verify the root hash before writing to storage
    let root_key = NodeKey::new_empty_path(self.version);
    let root_node = self.frozen_nodes.get(&root_key)
        .ok_or_else(|| anyhow!("Root node not found after freezing"))?;
    
    let actual_root_hash = root_node.hash();
    ensure!(
        actual_root_hash == self.expected_root_hash,
        "State snapshot restore failed: actual root hash {} does not match expected {}",
        actual_root_hash,
        self.expected_root_hash
    );
    
    // Only write if verification passes
    self.store.write_node_batch(&self.frozen_nodes)?;
    Ok(())
}
```

**Additional Recommendations:**
1. Add metrics/logging when finish() is called to track restoration completion attempts
2. Consider adding a progress check to ensure minimum expected nodes were received
3. Add integration tests that specifically test premature finish() calls with partial data

## Proof of Concept

**Rust Test Demonstrating the Vulnerability:**

```rust
#[test]
fn test_premature_finish_accepts_incomplete_tree() {
    use aptos_jellyfish_merkle::JellyfishMerkleTree;
    use aptos_crypto::hash::CryptoHash;
    
    // Create a complete tree with 100 accounts
    let mut complete_kvs = BTreeMap::new();
    for i in 0..100 {
        let key = format!("account_{}", i);
        let value = format!("balance_{}", i * 1000);
        complete_kvs.insert(
            CryptoHash::hash(key.as_bytes()),
            (key.as_bytes().to_vec(), value.as_bytes().to_vec())
        );
    }
    
    let (source_db, version) = init_mock_store(&complete_kvs);
    let source_tree = JellyfishMerkleTree::new(&source_db);
    let expected_root_hash = source_tree.get_root_hash(version).unwrap();
    
    // Attacker's incomplete restoration with only 50 accounts
    let target_db = Arc::new(MockSnapshotStore::default());
    let mut restore = StateSnapshotRestore::new(
        &target_db,
        &target_db,
        version,
        expected_root_hash, // Claiming to restore to this hash
        false,
        StateSnapshotRestoreMode::Default,
    ).unwrap();
    
    // Send only HALF the accounts
    let partial_kvs: Vec<_> = complete_kvs.iter().take(50).collect();
    for (hashed_key, (k, v)) in partial_kvs {
        let proof = source_tree.get_range_proof(*hashed_key, version).unwrap();
        restore.add_chunk(vec![(k.clone(), v.clone())], proof).unwrap();
    }
    
    // Call finish() prematurely - THIS SHOULD FAIL BUT DOESN'T
    let result = restore.finish();
    
    // BUG: finish() returns Ok even though tree is incomplete
    assert!(result.is_ok(), "finish() incorrectly accepted incomplete tree");
    
    // Verify the actual root hash differs from expected
    let actual_tree = JellyfishMerkleTree::new(&target_db);
    let actual_root_hash = actual_tree.get_root_hash(version).unwrap();
    
    // Demonstrate the vulnerability: hashes don't match but finish() succeeded
    assert_ne!(
        actual_root_hash, 
        expected_root_hash,
        "VULNERABILITY: Corrupted tree with wrong root hash was accepted"
    );
}
```

**Attack Steps for Real Network:**
1. Deploy a malicious state sync peer
2. Respond to state sync requests with valid proofs but only partial data (e.g., 50% of accounts)
3. When the target node calls finish(), it will accept the corrupted tree
4. The target node now has invalid state that doesn't match the expected root hash
5. Queries for missing accounts will fail, causing operational issues
6. If this affects multiple nodes, consensus divergence occurs

## Notes

The vulnerability exists because the proof-based verification in `verify()` is insufficient. Range proofs can only verify that "IF the root hash is X, THEN this partial data is consistent with it." They cannot verify that "ALL data for root hash X has been received."

The proper check - comparing the final computed root hash with `expected_root_hash` - is conspicuously absent from `finish_impl()`, representing a critical gap in the state restoration security model.

While `JellyfishMerkleRestore::new()` does verify the root hash when checking if a PREVIOUS restore completed, this only helps detect the issue on the next node restart, which is far too late - the corrupted data has already been written and potentially used.

### Citations

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L189-235)
```rust
    pub fn new<D: 'static + TreeReader<K> + TreeWriter<K>>(
        store: Arc<D>,
        version: Version,
        expected_root_hash: HashValue,
        async_commit: bool,
    ) -> Result<Self> {
        let tree_reader = Arc::clone(&store);
        let (finished, partial_nodes, previous_leaf) = if let Some(root_node) =
            tree_reader.get_node_option(&NodeKey::new_empty_path(version), "restore")?
        {
            info!("Previous restore is complete, checking root hash.");
            ensure!(
                root_node.hash() == expected_root_hash,
                "Previous completed restore has root hash {}, expecting {}",
                root_node.hash(),
                expected_root_hash,
            );
            (true, vec![], None)
        } else if let Some((node_key, leaf_node)) = tree_reader.get_rightmost_leaf(version)? {
            // If the system crashed in the middle of the previous restoration attempt, we need
            // to recover the partial nodes to the state right before the crash.
            (
                false,
                Self::recover_partial_nodes(tree_reader.as_ref(), version, node_key)?,
                Some(leaf_node),
            )
        } else {
            (
                false,
                vec![InternalInfo::new_empty(NodeKey::new_empty_path(version))],
                None,
            )
        };

        Ok(Self {
            store,
            version,
            partial_nodes,
            frozen_nodes: HashMap::new(),
            previous_leaf,
            num_keys_received: 0,
            expected_root_hash,
            finished,
            async_commit,
            async_commit_result: None,
        })
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L339-413)
```rust
    pub fn add_chunk_impl(
        &mut self,
        mut chunk: Vec<(&K, HashValue)>,
        proof: SparseMerkleRangeProof,
    ) -> Result<()> {
        if self.finished {
            info!("State snapshot restore already finished, ignoring entire chunk.");
            return Ok(());
        }

        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }

        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }

        Ok(())
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L628-696)
```rust
    fn verify(&self, proof: SparseMerkleRangeProof) -> Result<()> {
        let previous_leaf = self
            .previous_leaf
            .as_ref()
            .expect("The previous leaf must exist.");

        let previous_key = previous_leaf.account_key();
        // If we have all siblings on the path from root to `previous_key`, we should be able to
        // compute the root hash. The siblings on the right are already in the proof. Now we
        // compute the siblings on the left side, which represent all the states that have ever
        // been added.
        let mut left_siblings = vec![];

        // The following process might add some extra placeholder siblings on the left, but it is
        // nontrivial to determine when the loop should stop. So instead we just add these
        // siblings for now and get rid of them in the next step.
        let mut num_visited_right_siblings = 0;
        for (i, bit) in previous_key.iter_bits().enumerate() {
            if bit {
                // This node is a right child and there should be a sibling on the left.
                let sibling = if i >= self.partial_nodes.len() * 4 {
                    *SPARSE_MERKLE_PLACEHOLDER_HASH
                } else {
                    Self::compute_left_sibling(
                        &self.partial_nodes[i / 4],
                        previous_key.get_nibble(i / 4),
                        (3 - i % 4) as u8,
                    )
                };
                left_siblings.push(sibling);
            } else {
                // This node is a left child and there should be a sibling on the right.
                num_visited_right_siblings += 1;
            }
        }
        ensure!(
            num_visited_right_siblings >= proof.right_siblings().len(),
            "Too many right siblings in the proof.",
        );

        // Now we remove any extra placeholder siblings at the bottom. We keep removing the last
        // sibling if 1) it's a placeholder 2) it's a sibling on the left.
        for bit in previous_key.iter_bits().rev() {
            if bit {
                if *left_siblings.last().expect("This sibling must exist.")
                    == *SPARSE_MERKLE_PLACEHOLDER_HASH
                {
                    left_siblings.pop();
                } else {
                    break;
                }
            } else if num_visited_right_siblings > proof.right_siblings().len() {
                num_visited_right_siblings -= 1;
            } else {
                break;
            }
        }

        // Left siblings must use the same ordering as the right siblings in the proof
        left_siblings.reverse();

        // Verify the proof now that we have all the siblings
        proof
            .verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(*previous_key, previous_leaf.value_hash()),
                left_siblings,
            )
            .map_err(Into::into)
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L260-273)
```rust
    fn finish(self) -> Result<()> {
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => self.kv_restore.lock().take().unwrap().finish()?,
            StateSnapshotRestoreMode::TreeOnly => {
                self.tree_restore.lock().take().unwrap().finish_impl()?
            },
            StateSnapshotRestoreMode::Default => {
                // for tree only mode, we also need to write the usage to DB
                self.kv_restore.lock().take().unwrap().finish()?;
                self.tree_restore.lock().take().unwrap().finish_impl()?
            },
        }
        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L1122-1128)
```rust
    // Finalize the state snapshot
    state_snapshot_receiver.finish_box().map_err(|error| {
        format!(
            "Failed to finish the state value synchronization! Error: {:?}",
            error
        )
    })?;
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-240)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
            restore_utils::save_ledger_infos(
                self.ledger_db.metadata_db(),
                ledger_infos,
                Some(&mut ledger_db_batch.ledger_metadata_db_batches),
            )?;

            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::LedgerCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;
            ledger_db_batch
                .ledger_metadata_db_batches
                .put::<DbMetadataSchema>(
                    &DbMetadataKey::OverallCommitProgress,
                    &DbMetadataValue::Version(version),
                )?;

            // Apply the change set writes to the database (atomically) and update in-memory state
            //
            // state kv and SMT should use shared way of committing.
            self.ledger_db.write_schemas(ledger_db_batch)?;

            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;

            restore_utils::update_latest_ledger_info(self.ledger_db.metadata_db(), ledger_infos)?;
            self.state_store.reset();

            Ok(())
        })
```
