# Audit Report

## Title
Memory Exhaustion Vulnerability in Transaction Accumulator Pruner Catch-Up Phase

## Summary
The `TransactionAccumulatorPruner::new()` function performs unbounded catch-up pruning during initialization, accumulating millions of delete operations in a single `SchemaBatch` when the sub-pruner progress lags significantly behind metadata progress. This can cause out-of-memory errors or initialization timeouts, preventing validator nodes from restarting after crashes.

## Finding Description

During normal pruning operations, the `LedgerPruner` respects a configurable batch size (default 5,000 versions) to limit memory consumption. [1](#0-0) 

However, during the initialization catch-up phase, all sub-pruners bypass this batching mechanism. The `TransactionAccumulatorPruner::new()` function calls `prune(progress, metadata_progress)` for the entire gap without any batching: [2](#0-1) 

This triggers `TransactionAccumulatorDb::prune()`, which loops through potentially millions of versions, accumulating all delete operations in a single `SchemaBatch`: [3](#0-2) 

The `SchemaBatch` structure stores all operations in an unbounded `HashMap` with no memory limits: [4](#0-3) 

**Scenario causing large gaps:**
1. During normal operation, the `LedgerPruner` advances metadata pruner and sub-pruners together in batches
2. Each sub-pruner atomically commits its progress with the prune operation: [5](#0-4) 
3. If a crash/restart occurs after metadata pruner commits but before a sub-pruner commits, a gap emerges
4. On restart, `LedgerPruner::new()` initializes all sub-pruners with the current metadata progress: [6](#0-5) 
5. The sub-pruner attempts to catch up by pruning the entire gap in one operation

**Memory impact calculation:**
For a 10 million version gap:
- Each version: 1 root hash delete + (for odd versions) ~log₂(version) ≈ 23 accumulator node deletes
- Total: ~10M + (5M × 23) = ~125 million delete operations
- Memory: ~125M operations × 20-30 bytes per key ≈ 3-5 GB in the `SchemaBatch` HashMap alone

This violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation

This issue qualifies as **Medium Severity** under "State inconsistencies requiring intervention":

1. **Validator Availability Impact**: When a validator node crashes during pruning operations and attempts to restart with a large gap, the initialization will fail due to OOM or timeout, preventing the node from rejoining the network.

2. **Manual Intervention Required**: Recovery requires either:
   - Restoring from a recent database backup
   - Manually manipulating database metadata to reduce the gap
   - Waiting for system resource exhaustion to complete (potentially hours)

3. **Network Impact**: While a single validator failure doesn't break consensus, if multiple validators experience this issue (e.g., after a network-wide incident), it could degrade network liveness and increase block times.

4. **No Direct Exploitation**: This is not directly exploitable by external attackers but represents a failure mode that affects validator reliability and network availability.

## Likelihood Explanation

**Medium-High Likelihood** in production environments:

1. **Natural Occurrence**: The vulnerability triggers naturally through crash timing, without requiring attacker intervention
2. **Common Scenario**: Database crashes during write operations are common in distributed systems
3. **Gap Accumulation**: The longer a validator runs, the larger potential gaps can become if crashes occur at inopportune times
4. **Affected Components**: The same pattern exists in multiple sub-pruners (EventStorePruner, TransactionInfoPruner, etc.), increasing the attack surface: [7](#0-6) 

## Recommendation

Implement batched catch-up pruning during initialization. Modify the sub-pruner `new()` functions to respect the same batch size used during normal operations:

```rust
pub(in crate::pruner) fn new(
    ledger_db: Arc<LedgerDb>,
    metadata_progress: Version,
) -> Result<Self> {
    let progress = get_or_initialize_subpruner_progress(
        ledger_db.transaction_accumulator_db_raw(),
        &DbMetadataKey::TransactionAccumulatorPrunerProgress,
        metadata_progress,
    )?;

    let myself = TransactionAccumulatorPruner { ledger_db };

    info!(
        progress = progress,
        metadata_progress = metadata_progress,
        "Catching up TransactionAccumulatorPruner."
    );
    
    // Perform batched catch-up instead of single large prune
    const CATCHUP_BATCH_SIZE: u64 = 5_000; // Use same as normal batch size
    let mut current = progress;
    while current < metadata_progress {
        let target = std::cmp::min(current + CATCHUP_BATCH_SIZE, metadata_progress);
        myself.prune(current, target)?;
        current = target;
    }

    Ok(myself)
}
```

Apply the same fix to all sub-pruners: EventStorePruner, TransactionInfoPruner, TransactionPruner, WriteSetPruner, and PersistedAuxiliaryInfoPruner.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_temppath::TempPath;
    
    #[test]
    fn test_catchup_memory_exhaustion() {
        // Setup: Create a ledger DB with metadata progress far ahead
        let tmpdir = TempPath::new();
        let db = LedgerDb::new_for_test(&tmpdir);
        
        // Simulate metadata pruner at version 1,000,000
        let metadata_progress = 1_000_000;
        
        // Simulate transaction accumulator pruner at version 0
        // (by not initializing its progress in the DB)
        
        // Attempt to create pruner - this will try to prune 1M versions at once
        let result = TransactionAccumulatorPruner::new(
            Arc::new(db),
            metadata_progress,
        );
        
        // This should either:
        // 1. Consume excessive memory (multiple GB for large gaps)
        // 2. Take excessive time (minutes to hours)
        // 3. Potentially OOM on systems with limited memory
        
        // With the fix, this should complete quickly in batches
        assert!(result.is_ok());
    }
}
```

**Notes**

This vulnerability represents a design flaw in the pruner initialization logic rather than an externally exploitable attack vector. However, it has real availability implications for validator operators and can naturally occur through normal system failures. The issue affects multiple sub-pruners systemically, suggesting it was an oversight in the original implementation that prioritized simplicity over robustness during the catch-up phase. The fix is straightforward and aligns the catch-up behavior with the already-proven batching logic used during normal operations.

### Citations

**File:** config/src/config/storage_config.rs (L387-395)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_accumulator_pruner.rs (L25-35)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        TransactionAccumulatorDb::prune(current_progress, target_version, &mut batch)?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionAccumulatorPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_db
            .transaction_accumulator_db()
            .write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_accumulator_pruner.rs (L39-59)
```rust
    pub(in crate::pruner) fn new(
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.transaction_accumulator_db_raw(),
            &DbMetadataKey::TransactionAccumulatorPrunerProgress,
            metadata_progress,
        )?;

        let myself = TransactionAccumulatorPruner { ledger_db };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up TransactionAccumulatorPruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_accumulator_db.rs (L149-172)
```rust
    pub(crate) fn prune(begin: Version, end: Version, db_batch: &mut SchemaBatch) -> Result<()> {
        for version_to_delete in begin..end {
            db_batch.delete::<TransactionAccumulatorRootHashSchema>(&version_to_delete)?;
            // The even version will be pruned in the iteration of version + 1.
            if version_to_delete % 2 == 0 {
                continue;
            }

            let first_ancestor_that_is_a_left_child =
                Self::find_first_ancestor_that_is_a_left_child(version_to_delete);

            // This assertion is true because we skip the leaf nodes with address which is a
            // a multiple of 2.
            assert!(!first_ancestor_that_is_a_left_child.is_leaf());

            let mut current = first_ancestor_that_is_a_left_child;
            while !current.is_leaf() {
                db_batch.delete::<TransactionAccumulatorSchema>(&current.left_child())?;
                db_batch.delete::<TransactionAccumulatorSchema>(&current.right_child())?;
                current = current.right_child();
            }
        }
        Ok(())
    }
```

**File:** storage/schemadb/src/batch.rs (L129-133)
```rust
#[derive(Debug, Default)]
pub struct SchemaBatch {
    rows: DropHelper<HashMap<ColumnFamilyName, Vec<WriteOp>>>,
    stats: SampledBatchStats,
}
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L147-150)
```rust
        let transaction_accumulator_pruner = Box::new(TransactionAccumulatorPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L85-109)
```rust
    pub(in crate::pruner) fn new(
        ledger_db: Arc<LedgerDb>,
        metadata_progress: Version,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> Result<Self> {
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;

        let myself = EventStorePruner {
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up EventStorePruner."
        );
        myself.prune(progress, metadata_progress)?;

        Ok(myself)
    }
```
