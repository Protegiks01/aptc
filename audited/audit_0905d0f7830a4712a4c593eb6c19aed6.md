# Audit Report

## Title
Verified Module Cache Exhaustion Enables Sustained Validator Performance Degradation

## Summary
An attacker can deploy 100,000+ unique Move modules to fill the `VERIFIED_MODULES_CACHE` LRU cache to capacity, evicting critical Aptos framework module hashes. This forces expensive bytecode re-verification of system modules like `transaction_validation` on every subsequent block execution, causing sustained validator performance degradation.

## Finding Description

The `VERIFIED_MODULES_CACHE` is a global static LRU cache with a hardcoded limit of 100,000 entries that stores module hashes to skip expensive bytecode verification during module loading. [1](#0-0) 

When modules are loaded for execution, the cache is checked first. If a module hash is not found, the expensive `move_bytecode_verifier::verify_module_with_config` operation executes. [2](#0-1) 

**Attack Mechanism:**

1. An attacker deploys 100,000+ unique Move modules, each with an `init_module` function. During module publishing, `init_module` execution causes each module to be loaded and verified, adding its hash to the cache.

2. Since the cache uses LRU eviction and treats all modules equally (no protection for system modules), older entries including Aptos framework modules at address `0x1` (like `transaction_validation`, `account`, `coin`) are evicted as the cache fills.

3. On the next block, the framework prefetching mechanism attempts to load `transaction_validation` and its dependencies. [3](#0-2) 

4. Because framework module hashes are no longer in the cache, they undergo full bytecode verification (CFG construction, stack safety, type safety, resource safety, reference safety checks), which is documented as expensive.

5. Every transaction requires system modules for prologue/epilogue execution, so cache misses impact transaction processing speed across all validators.

6. The attacker can sustain this by periodically deploying additional batches of unique modules to continuously evict re-added system modules.

**Why System Modules Are Vulnerable:**

The cache has no special handling for frequently-used system modules. It only flushes when the verifier configuration changes, not based on usage patterns or module importance. [4](#0-3) 

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty program's "Validator node slowdowns" category. 

The attack causes:
- Expensive bytecode verification overhead on every block that accesses evicted system modules
- Degraded transaction processing performance affecting all validators network-wide  
- Sustained performance impact as long as the attacker continues module deployment
- No permanent damage, but significant operational disruption

The attack does not cause consensus failures, fund loss, or network partition, but measurably degrades validator performance, meeting the Medium severity threshold.

## Likelihood Explanation

**High likelihood** for a determined attacker:

- **No special privileges required**: Any account can deploy modules
- **Deterministic success**: If attacker has sufficient funds, attack will succeed
- **No rate limiting**: Only gas and storage costs constrain deployment
- **Cost**: Approximately 290 million internal gas units for 100k minimal modules (~2,900 gas units each) plus storage fees. While expensive (thousands of APT), this is feasible for well-funded attackers targeting network stability
- **Sustainability**: Attack can be maintained by periodically deploying additional modules

## Recommendation

Implement a two-tier caching strategy with protection for system modules:

1. **Priority Cache Segment**: Reserve a portion of the verification cache (e.g., 10,000 entries) exclusively for framework modules at special addresses (`0x1` through `0xa`). These entries cannot be evicted by user-deployed modules.

2. **Module Classification**: Tag modules during verification as either "system" (from reserved addresses) or "user" (from regular accounts).

3. **Modified LRU Eviction**: When cache reaches capacity, only evict from the user module segment. System modules remain pinned.

4. **Configuration**: Make cache sizes configurable per segment:

```rust
pub struct VerifiedModuleCache {
    // Protected cache for system modules (cannot be evicted by user modules)
    system_cache: Mutex<lru::LruCache<[u8; 32], ()>>,
    // Regular LRU cache for user modules
    user_cache: Mutex<lru::LruCache<[u8; 32], ()>>,
}

impl VerifiedModuleCache {
    const SYSTEM_CACHE_SIZE: NonZeroUsize = NonZeroUsize::new(10_000).unwrap();
    const USER_CACHE_SIZE: NonZeroUsize = NonZeroUsize::new(90_000).unwrap();
    
    pub(crate) fn put(&self, module_hash: [u8; 32], is_system_module: bool) {
        if is_system_module {
            self.system_cache.lock().put(module_hash, ());
        } else {
            self.user_cache.lock().put(module_hash, ());
        }
    }
    
    pub(crate) fn contains(&self, module_hash: &[u8; 32]) -> bool {
        self.system_cache.lock().contains(module_hash) || 
        self.user_cache.lock().contains(module_hash)
    }
}
```

## Proof of Concept

```rust
// Test demonstrating cache exhaustion attack
#[test]
fn test_verified_cache_exhaustion_attack() {
    use move_binary_format::CompiledModule;
    use move_core_types::account_address::AccountAddress;
    
    // 1. Deploy attacker modules to fill cache
    let attacker_addr = AccountAddress::random();
    let mut attacker_modules = vec![];
    
    for i in 0..100_001 {
        // Create unique module with init_module function
        let module = create_minimal_module_with_init(
            attacker_addr, 
            format!("Module{}", i)
        );
        attacker_modules.push(module);
    }
    
    // 2. Publish all modules (triggers init_module execution)
    for module in attacker_modules {
        publish_module_with_init(&module);
        // Each publish loads module for init_module execution
        // Adding to VERIFIED_MODULES_CACHE
    }
    
    // 3. Verify cache is full (100,000 entries)
    assert_eq!(VERIFIED_MODULES_CACHE.size(), 100_000);
    
    // 4. Check if framework module hash is still in cache
    let framework_module_hash = get_module_hash(
        &AccountAddress::ONE, 
        "transaction_validation"
    );
    assert!(!VERIFIED_MODULES_CACHE.contains(&framework_module_hash));
    
    // 5. Execute transaction requiring framework module
    let start = Instant::now();
    execute_transaction(); // Will trigger re-verification
    let duration = start.elapsed();
    
    // 6. Verify re-verification occurred (slower execution)
    assert!(duration > expected_cached_duration);
    
    // 7. Verify module was re-added to cache
    assert!(VERIFIED_MODULES_CACHE.contains(&framework_module_hash));
}

fn create_minimal_module_with_init(addr: AccountAddress, name: String) -> CompiledModule {
    // Create minimal valid module with init_module function
    // Each module must have unique bytecode for unique hash
    // init_module ensures it gets loaded during publish
}
```

## Notes

The verified module cache serves a critical performance optimization by avoiding repeated bytecode verification. However, the current implementation lacks any protection mechanism for high-value system modules that are accessed on every transaction. The LRU eviction policy, while simple and fair, creates a vulnerability where an attacker with sufficient capital can weaponize the cache's fixed capacity to degrade network-wide validator performance.

Alternative mitigations could include:
- Dynamic cache resizing based on system load
- Weighted LRU that deprioritizes eviction of frequently-accessed modules  
- Separate verification fast-path for framework modules
- Rate limiting on module deployment per account/epoch

### Citations

**File:** third_party/move/move-vm/runtime/src/storage/verified_module_cache.rs (L16-17)
```rust
    /// Maximum size of the cache. When modules are cached, they can skip re-verification.
    const VERIFIED_CACHE_SIZE: NonZeroUsize = NonZeroUsize::new(100_000).unwrap();
```

**File:** third_party/move/move-vm/runtime/src/storage/environment.rs (L184-197)
```rust
        if !VERIFIED_MODULES_CACHE.contains(module_hash) {
            let _timer =
                VM_TIMER.timer_with_label("move_bytecode_verifier::verify_module_with_config");

            // For regular execution, we cache already verified modules. Note that this even caches
            // verification for the published modules. This should be ok because as long as the
            // hash is the same, the deployed bytecode and any dependencies are the same, and so
            // the cached verification result can be used.
            move_bytecode_verifier::verify_module_with_config(
                &self.vm_config().verifier_config,
                compiled_module.as_ref(),
            )?;
            check_natives(compiled_module.as_ref())?;
            VERIFIED_MODULES_CACHE.put(*module_hash);
```

**File:** aptos-move/block-executor/src/code_cache_global_manager.rs (L116-130)
```rust
        if environment_requires_update {
            if storage_environment.gas_feature_version() >= RELEASE_V1_34 {
                let flush_verifier_cache = self.environment.as_ref().is_none_or(|e| {
                    e.verifier_config_bytes() != storage_environment.verifier_config_bytes()
                });
                if flush_verifier_cache {
                    // Additionally, if the verifier config changes, we flush static verifier cache
                    // as well.
                    RuntimeEnvironment::flush_verified_module_cache();
                }
            }

            self.environment = Some(storage_environment);
            self.module_cache.flush();
        }
```

**File:** aptos-move/block-executor/src/code_cache_global_manager.rs (L345-379)
```rust
fn prefetch_aptos_framework(
    state_view: &impl StateView,
    guard: &mut AptosModuleCacheManagerGuard,
) -> Result<(), PanicError> {
    let code_storage = state_view.as_aptos_code_storage(guard.environment());

    // INVARIANT:
    //   If framework code exists in storage, the transitive closure will be verified and cached to
    //   avoid cold starts. From metering perspective, all modules are at special addresses, so we
    //   do not need to meter anything.
    cfg_if! {
        if #[cfg(fuzzing)] {
            let maybe_loaded = code_storage
                .unmetered_get_module_skip_verification(&AccountAddress::ONE, ident_str!("transaction_validation"))
                .map_err(|err| {
                    PanicError::CodeInvariantError(format!("Unable to fetch Aptos framework: {:?}", err))
                })?;
        } else {
            let maybe_loaded = code_storage
                .unmetered_get_eagerly_verified_module(&AccountAddress::ONE, ident_str!("transaction_validation"))
                .map_err(|err| {
                    PanicError::CodeInvariantError(format!("Unable to fetch Aptos framework: {:?}", err))
                })?;
        }
    }

    if maybe_loaded.is_some() {
        // Framework must have been loaded. Drain verified modules from local cache into
        // global cache.
        let verified_module_code_iter = code_storage.into_verified_module_code_iter()?;
        guard
            .module_cache_mut()
            .insert_verified(verified_module_code_iter)?;
    }
    Ok(())
```
