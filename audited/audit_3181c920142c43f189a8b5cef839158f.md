# Audit Report

## Title
Cross-Epoch Finality Proof Acceptance Without Validator Set Verification During Recovery

## Summary
The `send_for_execution()` function and recovery process accept finality proofs (WrappedLedgerInfo) from previous epochs without verifying their signatures against the current epoch's validator set. During node recovery after an epoch change, old-epoch quorum certificates persisted in storage are loaded and sent for execution without epoch validation, potentially causing consensus inconsistencies.

## Finding Description

During node recovery, the consensus system loads blocks and quorum certificates from persistent storage to reconstruct the block tree. However, the recovery process fails to filter these artifacts by epoch, allowing quorum certificates from epoch N to be loaded and processed when the system is in epoch N+1 with a completely different validator set. [1](#0-0) 

The `RecoveryData::new()` function filters `last_vote` and `highest_2chain_timeout_certificate` by epoch (lines 405-408, 414-417), but critically does NOT filter `blocks` and `quorum_certs` by epoch. These unfiltered artifacts are then inserted into the BlockStore. [2](#0-1) 

During BlockStore initialization, `try_send_for_execution()` is automatically called, which retrieves ALL quorum certificates with commit info from the block tree, regardless of their epoch: [3](#0-2) [4](#0-3) 

The `send_for_execution()` function then converts these QCs to WrappedLedgerInfo and sends them to the execution client WITHOUT verifying the signatures: [5](#0-4) 

The finality proof is forwarded to the buffer manager, which also does NOT verify it against the current epoch state: [6](#0-5) 

When the buffer manager processes these ordered blocks and advances them through the execution pipeline, it uses the current epoch's validator verifier but the old epoch's ordered_proof: [7](#0-6) [8](#0-7) 

The ordered_proof from epoch N (containing signatures from epoch N validators) is used to generate commit ledger info, while the system is operating in epoch N+1 with epoch N+1 validators.

**Attack Scenario:**
1. System operates in epoch N with validator set V_N
2. Block B at round 100 receives a QC signed by V_N, stored persistently
3. Reconfiguration occurs at round 150, transitioning to epoch N+1 with validator set V_{N+1}
4. Node restarts; recovery loads QC from round 100 (epoch N) alongside epoch N+1 blocks
5. Since round 100 may be higher than the new epoch's initial ordered root round, the QC passes the round check
6. The epoch N QC is sent for execution without signature verification
7. The unverified proof is used in the consensus pipeline with epoch N+1 validator context

## Impact Explanation

**Critical Severity** - This vulnerability breaks fundamental consensus safety guarantees:

1. **Cryptographic Correctness Violation**: Finality proofs containing BLS signatures from old epoch validators bypass verification, violating the requirement that all signatures must be verified against the current validator set.

2. **Consensus Safety Risk**: Different nodes may have different sets of old-epoch QCs in storage depending on when they restarted relative to the epoch change, leading to divergent execution paths and potential chain splits.

3. **Validator Set Authority Bypass**: Signatures from validators who are no longer in the active set (and may even be malicious actors removed during reconfiguration) are accepted without verification.

This qualifies for Critical severity under "Consensus/Safety violations" as it allows finality proofs to be accepted without proper cryptographic verification, fundamentally undermining the security model of AptosBFT.

## Likelihood Explanation

**High Likelihood:**
- Triggers automatically during normal recovery operations after any epoch change
- Requires no attacker action - occurs through normal node restart patterns
- Every node that persists QCs before an epoch change and restarts afterward is affected
- Epochs change regularly through on-chain governance reconfigurations

## Recommendation

Add epoch validation to the recovery and execution flow:

1. **Filter recovery data by epoch:**
```rust
// In RecoveryData::new()
let epoch = match &root.window_root_block {
    None => root.commit_root_block.epoch(),
    Some(window_root_block) => window_root_block.epoch(),
};

// Filter blocks by epoch
let blocks: Vec<Block> = blocks.into_iter()
    .filter(|b| b.epoch() == epoch)
    .collect();

// Filter QCs by epoch  
let quorum_certs: Vec<QuorumCert> = quorum_certs.into_iter()
    .filter(|qc| qc.ledger_info().ledger_info().commit_info().epoch() == epoch)
    .collect();
```

2. **Verify finality proofs in send_for_execution:**
```rust
pub async fn send_for_execution(
    &self,
    finality_proof: WrappedLedgerInfo,
) -> anyhow::Result<()> {
    // Verify proof epoch matches current epoch
    let current_epoch = self.commit_root().epoch();
    ensure!(
        finality_proof.epoch() == current_epoch,
        "Finality proof epoch {} does not match current epoch {}",
        finality_proof.epoch(),
        current_epoch
    );
    
    // ... rest of function
}
```

3. **Add epoch verification in buffer_manager:**
```rust
async fn process_ordered_blocks(&mut self, ordered_blocks: OrderedBlocks) {
    let OrderedBlocks {
        ordered_blocks,
        ordered_proof,
    } = ordered_blocks;
    
    // Verify proof epoch
    ensure!(
        ordered_proof.ledger_info().commit_info().epoch() == self.epoch_state.epoch,
        "Ordered proof epoch mismatch"
    );
    
    // ... rest of function
}
```

## Proof of Concept

```rust
// Scenario: Node recovery after epoch change
// 
// Setup:
// 1. Create QC at epoch 1, round 100, persist to storage
// 2. Trigger epoch change to epoch 2 at round 150
// 3. Set new ordered root at epoch 2, round 1
// 4. Restart node, triggering recovery
//
// Expected: Epoch 1 QC should be rejected
// Actual: Epoch 1 QC passes round check (100 > 1) and is sent for execution
//
// To reproduce:
// 1. Run consensus node in epoch 1
// 2. Create and persist blocks with QCs at rounds 50-100  
// 3. Execute reconfiguration transaction triggering epoch 2
// 4. Kill and restart the consensus node
// 5. Observe in logs: "trying to commit to round 100" despite being in epoch 2
// 6. The epoch 1 QC's signatures from old validators are never verified
```

**Notes**

The vulnerability specifically violates the security invariant that all cryptographic signatures must be verified before acceptance. While the execution itself may be deterministic, the acceptance of unverified finality proofs from incorrect validator sets creates a consensus safety risk. The `EpochState` contains the authoritative validator set for signature verification [9](#0-8) , but this validation is bypassed during recovery. The fix requires ensuring that all finality proofs loaded during recovery are filtered by epoch and that `send_for_execution()` validates the proof's epoch against the current consensus state before forwarding to the execution pipeline.

### Citations

**File:** consensus/src/persistent_liveness_storage.rs (L348-419)
```rust
    pub fn new(
        last_vote: Option<Vote>,
        ledger_recovery_data: LedgerRecoveryData,
        mut blocks: Vec<Block>,
        root_metadata: RootMetadata,
        mut quorum_certs: Vec<QuorumCert>,
        highest_2chain_timeout_cert: Option<TwoChainTimeoutCertificate>,
        order_vote_enabled: bool,
        window_size: Option<u64>,
    ) -> Result<Self> {
        let root = ledger_recovery_data
            .find_root(
                &mut blocks,
                &mut quorum_certs,
                order_vote_enabled,
                window_size,
            )
            .with_context(|| {
                // for better readability
                blocks.sort_by_key(|block| block.round());
                quorum_certs.sort_by_key(|qc| qc.certified_block().round());
                format!(
                    "\nRoot: {}\nBlocks in db: {}\nQuorum Certs in db: {}\n",
                    ledger_recovery_data.storage_ledger.ledger_info(),
                    blocks
                        .iter()
                        .map(|b| format!("\n{}", b))
                        .collect::<Vec<String>>()
                        .concat(),
                    quorum_certs
                        .iter()
                        .map(|qc| format!("\n{}", qc))
                        .collect::<Vec<String>>()
                        .concat(),
                )
            })?;

        // If execution pool is enabled, use the window_root, else use the commit_root
        let (root_id, epoch) = match &root.window_root_block {
            None => {
                let commit_root_id = root.commit_root_block.id();
                let epoch = root.commit_root_block.epoch();
                (commit_root_id, epoch)
            },
            Some(window_root_block) => {
                let window_start_id = window_root_block.id();
                let epoch = window_root_block.epoch();
                (window_start_id, epoch)
            },
        };
        let blocks_to_prune = Some(Self::find_blocks_to_prune(
            root_id,
            &mut blocks,
            &mut quorum_certs,
        ));

        Ok(RecoveryData {
            last_vote: match last_vote {
                Some(v) if v.epoch() == epoch => Some(v),
                _ => None,
            },
            root,
            root_metadata,
            blocks,
            quorum_certs,
            blocks_to_prune,
            highest_2chain_timeout_certificate: match highest_2chain_timeout_cert {
                Some(tc) if tc.epoch() == epoch => Some(tc),
                _ => None,
            },
        })
    }
```

**File:** consensus/src/block_storage/block_store.rs (L144-161)
```rust
    async fn try_send_for_execution(&self) {
        // reproduce the same batches (important for the commit phase)
        let mut certs = self.inner.read().get_all_quorum_certs_with_commit_info();
        certs.sort_unstable_by_key(|qc| qc.commit_info().round());
        for qc in certs {
            if qc.commit_info().round() > self.commit_root().round() {
                info!(
                    "trying to commit to round {} with ledger info {}",
                    qc.commit_info().round(),
                    qc.ledger_info()
                );

                if let Err(e) = self.send_for_execution(qc.into_wrapped_ledger_info()).await {
                    error!("Error in try-committing blocks. {}", e.to_string());
                }
            }
        }
    }
```

**File:** consensus/src/block_storage/block_store.rs (L282-305)
```rust
        for block in blocks {
            if block.round() <= root_block_round {
                block_store
                    .insert_committed_block(block)
                    .await
                    .unwrap_or_else(|e| {
                        panic!(
                            "[BlockStore] failed to insert committed block during build {:?}",
                            e
                        )
                    });
            } else {
                block_store.insert_block(block).await.unwrap_or_else(|e| {
                    panic!("[BlockStore] failed to insert block during build {:?}", e)
                });
            }
        }
        for qc in quorum_certs {
            block_store
                .insert_single_quorum_cert(qc)
                .unwrap_or_else(|e| {
                    panic!("[BlockStore] failed to insert quorum during build{:?}", e)
                });
        }
```

**File:** consensus/src/block_storage/block_store.rs (L311-350)
```rust
    /// Send an ordered block id with the proof for execution, returns () on success or error
    pub async fn send_for_execution(
        &self,
        finality_proof: WrappedLedgerInfo,
    ) -> anyhow::Result<()> {
        let block_id_to_commit = finality_proof.commit_info().id();
        let block_to_commit = self
            .get_block(block_id_to_commit)
            .ok_or_else(|| format_err!("Committed block id not found"))?;

        // First make sure that this commit is new.
        ensure!(
            block_to_commit.round() > self.ordered_root().round(),
            "Committed block round lower than root"
        );

        let blocks_to_commit = self
            .path_from_ordered_root(block_id_to_commit)
            .unwrap_or_default();

        assert!(!blocks_to_commit.is_empty());

        let finality_proof_clone = finality_proof.clone();
        self.pending_blocks
            .lock()
            .gc(finality_proof.commit_info().round());

        self.inner.write().update_ordered_root(block_to_commit.id());
        self.inner
            .write()
            .insert_ordered_cert(finality_proof_clone.clone());
        update_counters_for_ordered_blocks(&blocks_to_commit);

        self.execution_client
            .finalize_order(blocks_to_commit, finality_proof.clone())
            .await
            .expect("Failed to persist commit");

        Ok(())
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L161-167)
```rust
    pub fn get_all_quorum_certs_with_commit_info(&self) -> Vec<QuorumCert> {
        self.id_to_quorum_cert
            .values()
            .filter(|qc| qc.commit_info() != &BlockInfo::empty())
            .map(|qc| (**qc).clone())
            .collect::<Vec<QuorumCert>>()
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L382-424)
```rust
    async fn process_ordered_blocks(&mut self, ordered_blocks: OrderedBlocks) {
        let OrderedBlocks {
            ordered_blocks,
            ordered_proof,
        } = ordered_blocks;

        info!(
            "Receive {} ordered block ends with [epoch: {}, round: {}, id: {}], the queue size is {}",
            ordered_blocks.len(),
            ordered_proof.commit_info().epoch(),
            ordered_proof.commit_info().round(),
            ordered_proof.commit_info().id(),
            self.buffer.len() + 1,
        );

        let request = self.create_new_request(ExecutionRequest {
            ordered_blocks: ordered_blocks.clone(),
        });
        if let Some(consensus_publisher) = &self.consensus_publisher {
            let message = ConsensusObserverMessage::new_ordered_block_message(
                ordered_blocks.clone(),
                ordered_proof.clone(),
            );
            consensus_publisher.publish_message(message);
        }
        self.execution_schedule_phase_tx
            .send(request)
            .await
            .expect("Failed to send execution schedule request");

        let mut unverified_votes = HashMap::new();
        if let Some(block) = ordered_blocks.last() {
            if let Some(votes) = self.pending_commit_votes.remove(&block.round()) {
                for (_, vote) in votes {
                    if vote.commit_info().id() == block.id() {
                        unverified_votes.insert(vote.author(), vote);
                    }
                }
            }
        }
        let item = BufferItem::new_ordered(ordered_blocks, ordered_proof, unverified_votes);
        self.buffer.push_back(item);
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L659-666)
```rust
        let item = self.buffer.take(&current_cursor);
        let round = item.round();
        let mut new_item = item.advance_to_executed_or_aggregated(
            executed_blocks,
            &self.epoch_state.verifier,
            self.end_epoch_timestamp.get().cloned(),
            self.order_vote_enabled,
        );
```

**File:** consensus/src/pipeline/buffer_item.rs (L159-163)
```rust
                    let commit_ledger_info = generate_commit_ledger_info(
                        &commit_info,
                        &ordered_proof,
                        order_vote_enabled,
                    );
```

**File:** types/src/epoch_state.rs (L41-50)
```rust
    fn verify(&self, ledger_info: &LedgerInfoWithSignatures) -> anyhow::Result<()> {
        ensure!(
            self.epoch == ledger_info.ledger_info().epoch(),
            "LedgerInfo has unexpected epoch {}, expected {}",
            ledger_info.ledger_info().epoch(),
            self.epoch
        );
        ledger_info.verify_signatures(&self.verifier)?;
        Ok(())
    }
```
