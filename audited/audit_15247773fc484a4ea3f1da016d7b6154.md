# Audit Report

## Title
Secret Sharing Queue Incorrectly Marks Blocks as Ready When Channel Send Fails, Causing Pipeline Execution Failures

## Summary
The `set_secret_shared_key()` function in the secret sharing block queue unconditionally removes rounds from `pending_secret_key_rounds` even when the channel send operation fails. This causes blocks to be incorrectly marked as ready for execution when their decryption keys were never successfully delivered to the pipeline, leading to task panics and potential consensus inconsistencies.

## Finding Description

The vulnerability exists in the secret sharing block queue's key delivery mechanism: [1](#0-0) 

The function attempts to send the reconstructed secret key to the block's decryption pipeline via a oneshot channel. However, the result of `send()` is discarded via `.map()` without checking for errors. The function then unconditionally removes the round from `pending_secret_key_rounds`, regardless of whether the send succeeded.

In Rust, `oneshot::Sender::send()` returns `Result<(), T>` and fails with `Err(T)` if the receiver has been dropped. This can occur when:

1. The block's pipeline is aborted via `abort_pipeline()` (e.g., during state sync) [2](#0-1) 

2. The pipeline abort cancels the decryption task, dropping the receiver [3](#0-2) 

When the send fails but the round is still removed from pending, the `is_fully_secret_shared()` check incorrectly returns true: [4](#0-3) 

This causes blocks to be dequeued as "ready" and sent downstream for execution: [5](#0-4) 

Meanwhile, the decryption task is still waiting (or has panicked) because it never received the key: [6](#0-5) 

The `.expect()` call will panic with "decryption key should be available" when the receiver gets an error, causing a `TaskError::JoinError` to propagate through the execution pipeline.

## Impact Explanation

**Severity: High** (potentially Critical)

This vulnerability breaks the **Deterministic Execution** invariant. The impact includes:

1. **Execution Pipeline Failures**: Blocks marked as ready but lacking decryption keys will cause the execution pipeline to fail when awaiting the decryption future, resulting in `TaskError::JoinError` propagation.

2. **State Inconsistency Risk**: Different validators may experience different timing for pipeline aborts and key reconstruction. This creates a race condition where:
   - Some validators successfully decrypt and execute blocks
   - Others fail due to missing keys
   - This could lead to state divergence if error handling differs

3. **Liveness Impact**: Blocks with encrypted transactions that fail decryption may block the execution pipeline, preventing subsequent blocks from being processed.

4. **Validator Slowdowns**: Repeated failures and retries in the execution pipeline degrade performance across the network.

This qualifies as **High Severity** per the Aptos bug bounty criteria due to validator slowdowns and significant protocol violations. It approaches **Critical Severity** if it can be demonstrated to cause consensus splits in practice.

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability can occur during normal operations without requiring attacker intervention:

1. **State Sync Scenarios**: When nodes perform state sync, `abort_pipeline_for_state_sync()` is called, aborting all block pipelines [7](#0-6) 

2. **Block Pruning**: Blocks may be pruned from the block store while still present in the secret sharing queue

3. **Epoch Transitions**: Pipeline resets during epoch changes could trigger this condition

The timing window exists whenever:
- Blocks enter the secret sharing queue
- Secret key reconstruction is in progress
- Pipeline aborts occur before key delivery

Given the asynchronous nature of secret sharing and the potential for network delays, this scenario is realistic in production environments, especially during state sync or network disruptions.

## Recommendation

The fix should check the result of the channel send operation and only remove the round from pending if the send succeeds:

```rust
pub fn set_secret_shared_key(&mut self, round: Round, key: SecretSharedKey) {
    let offset = self.offset(round);
    if self.pending_secret_key_rounds.contains(&round) {
        observe_block(
            self.blocks()[offset].timestamp_usecs(),
            BlockStage::SECRET_SHARING_ADD_DECISION,
        );
        let block = &self.blocks_mut()[offset];
        let mut should_remove = false;
        if let Some(tx) = block.pipeline_tx().lock().as_mut() {
            if let Some(sender) = tx.secret_shared_key_tx.take() {
                // Only mark as complete if send succeeds
                should_remove = sender.send(Some(key)).is_ok();
            }
        }
        // Only remove from pending if we successfully sent the key
        if should_remove {
            self.pending_secret_key_rounds.remove(&round);
        }
    }
}
```

Additionally, improve error handling in the decryption pipeline to gracefully handle missing keys instead of panicking: [6](#0-5) 

## Proof of Concept

Due to the complexity of the secret sharing and pipeline systems, a full PoC requires extensive setup. However, the vulnerability can be demonstrated through the following test scenario:

```rust
#[tokio::test]
async fn test_secret_key_send_failure() {
    // 1. Create a QueueItem with a block that has pending secret key rounds
    // 2. Build the block's pipeline
    // 3. Abort the pipeline to drop the receiver
    // 4. Call set_secret_shared_key() with a reconstructed key
    // 5. Assert that pending_secret_key_rounds is empty (vulnerability)
    // 6. Assert that is_fully_secret_shared() returns true (incorrect)
    // 7. Dequeue the block as "ready"
    // 8. Attempt to await the decryption future
    // 9. Observe TaskError::JoinError due to panic
    
    // This demonstrates that blocks are marked ready
    // when they actually lack decryption keys
}
```

The core issue can be reproduced by:
1. Creating a oneshot channel
2. Dropping the receiver
3. Calling `send()` on the sender
4. Observing that it returns `Err`
5. Demonstrating that the current code ignores this error

**Notes**

This vulnerability highlights a broader issue in the codebase where channel send operations are not properly validated. Similar patterns exist in other pipeline coordination code (e.g., `set_qc()`), though they may not have the same critical impact on consensus correctness. A comprehensive audit of all oneshot channel usage in the pipeline system is recommended.

### Citations

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L60-62)
```rust
    pub fn is_fully_secret_shared(&self) -> bool {
        self.pending_secret_key_rounds.is_empty()
    }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L64-77)
```rust
    pub fn set_secret_shared_key(&mut self, round: Round, key: SecretSharedKey) {
        let offset = self.offset(round);
        if self.pending_secret_key_rounds.contains(&round) {
            observe_block(
                self.blocks()[offset].timestamp_usecs(),
                BlockStage::SECRET_SHARING_ADD_DECISION,
            );
            let block = &self.blocks_mut()[offset];
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
                tx.secret_shared_key_tx.take().map(|tx| tx.send(Some(key)));
            }
            self.pending_secret_key_rounds.remove(&round);
        }
    }
```

**File:** consensus/src/rand/secret_sharing/block_queue.rs (L112-127)
```rust
    pub fn dequeue_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.is_fully_secret_shared() {
                let (_, item) = self.queue.pop_first().expect("First key must exist");
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::SECRET_SHARING_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        ready_prefix
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L528-547)
```rust
    pub fn abort_pipeline(&self) -> Option<PipelineFutures> {
        if let Some(abort_handles) = self.pipeline_abort_handle.lock().take() {
            let mut aborted = false;
            for handle in abort_handles {
                if !handle.is_finished() {
                    handle.abort();
                    aborted = true;
                }
            }
            if aborted {
                info!(
                    "[Pipeline] Aborting pipeline for block {} {} {}",
                    self.id(),
                    self.epoch(),
                    self.round()
                );
            }
        }
        self.pipeline_futs.lock().take()
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L461-471)
```rust
        let decryption_fut = spawn_shared_fut(
            Self::decrypt_encrypted_txns(
                materialize_fut,
                block.clone(),
                self.signer.author(),
                self.secret_share_config.clone(),
                derived_self_key_share_tx,
                secret_shared_key_rx,
            ),
            Some(&mut abort_handles),
        );
```

**File:** consensus/src/pipeline/decryption_pipeline_builder.rs (L115-119)
```rust
        let maybe_decryption_key = secret_shared_key_rx
            .await
            .expect("decryption key should be available");
        // TODO(ibalajiarun): account for the case where decryption key is not available
        let decryption_key = maybe_decryption_key.expect("decryption key should be available");
```

**File:** consensus/src/block_storage/block_store.rs (L617-627)
```rust
    pub async fn abort_pipeline_for_state_sync(&self) {
        let blocks = self.inner.read().get_all_blocks();
        // the blocks are not ordered by round here, so we need to abort all then wait
        let futs: Vec<_> = blocks
            .into_iter()
            .filter_map(|b| b.abort_pipeline())
            .collect();
        for f in futs {
            f.wait_until_finishes().await;
        }
    }
```
