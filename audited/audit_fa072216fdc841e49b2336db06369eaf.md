# Audit Report

## Title
Indexer Timestamp Desynchronization in Token V1 Tables Breaking Time-Based Queries

## Summary
The `insert_current_token_ownerships` function fails to update the `last_transaction_timestamp` field during upsert operations, causing it to remain frozen at the initial insertion value while `last_transaction_version` advances. This creates a permanent desynchronization between version and timestamp data, breaking time-based queries and violating data consistency guarantees in the indexer.

## Finding Description

The Aptos indexer maintains "current" state tables that track the latest state of tokens. The `current_token_ownerships` table includes two fields that should remain synchronized:
- `last_transaction_version`: The version number of the last transaction affecting this token ownership
- `last_transaction_timestamp`: The timestamp of that transaction [1](#0-0) 

When the indexer processes token ownership changes, it performs upsert operations via the `insert_current_token_ownerships` function: [2](#0-1) 

The critical bug is in the `do_update().set()` clause, which updates `last_transaction_version` but **omits** `last_transaction_timestamp` from the list of fields being updated. This causes:

1. **Initial Insert**: Both fields set correctly (e.g., version=100, timestamp=T100)
2. **Subsequent Updates**: Only version updates (e.g., version=200, timestamp=T100 ← WRONG)
3. **Further Updates**: Version continues advancing while timestamp stays frozen (version=300, timestamp=T100)

The WHERE clause at line 406 ensures updates only occur when the new version is greater than or equal to the existing version, but this doesn't help - the timestamp still doesn't update.

**Invariant Violation**: The semantic contract that `last_transaction_timestamp` represents the timestamp of the transaction identified by `last_transaction_version` is permanently broken after the first update.

**Comparison with Correct Implementation**: The V2 token tables and coin balance tables correctly update the timestamp: [3](#0-2) [4](#0-3) 

**Systemic Issue**: The same bug affects all three V1 token tables:
- `current_token_ownerships` 
- `current_token_datas` (missing timestamp update at line 448)
- `current_collection_datas` (missing timestamp update at line 482) [5](#0-4) [6](#0-5) 

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty criteria for "State inconsistencies requiring intervention":

1. **Data Integrity Violation**: The indexer database contains systematically incorrect timestamp data, violating user expectations about data consistency
2. **Broken Time-Based Queries**: Any application querying tokens by time ranges, sorting by last update time, or displaying "last modified" timestamps will receive incorrect results
3. **Cascading Effects**: NFT marketplaces, analytics dashboards, and wallet applications relying on indexer data will show stale "last updated" times indefinitely
4. **Intervention Required**: Fixing this requires both code changes AND a complete indexer database rebuild to correct historical data
5. **Silent Failure**: The bug produces no errors or warnings; data appears valid but is subtly incorrect

**Scope Limitation**: This is an indexer bug, not a core blockchain vulnerability. It does not:
- Affect consensus or validator operations
- Enable theft or minting of funds
- Compromise blockchain state or security
- Impact the actual on-chain token data (which remains correct)

The indexer is an off-chain query layer; bugs here affect data availability and correctness for applications, but not blockchain security itself.

## Likelihood Explanation

**Likelihood: Certain (100%)**

This bug triggers automatically on every token ownership update after the initial creation:
1. Any token transfer, amount change, or property update will exhibit this behavior
2. No attacker action required - normal token operations trigger it
3. Affects all V1 tokens on the Aptos blockchain
4. Has likely been occurring since the `last_transaction_timestamp` field was added in migration 2022-10-04

The bug is deterministic and systematic, not dependent on timing, race conditions, or specific transaction ordering.

## Recommendation

**Code Fix**: Add `last_transaction_timestamp` to the update set clause for all three affected functions:

```rust
// In insert_current_token_ownerships (line 395-405)
.set((
    creator_address.eq(excluded(creator_address)),
    collection_name.eq(excluded(collection_name)),
    name.eq(excluded(name)),
    amount.eq(excluded(amount)),
    token_properties.eq(excluded(token_properties)),
    last_transaction_version.eq(excluded(last_transaction_version)),
    last_transaction_timestamp.eq(excluded(last_transaction_timestamp)), // ADD THIS LINE
    collection_data_id_hash.eq(excluded(collection_data_id_hash)),
    table_type.eq(excluded(table_type)),
    inserted_at.eq(excluded(inserted_at)),
))
```

Apply the same fix to:
- `insert_current_token_datas` (around line 427)
- `insert_current_collection_datas` (around line 470)

**Data Recovery**: After deploying the fix, the indexer database must be rebuilt from genesis to correct all historical timestamp data.

## Proof of Concept

**Setup**: Deploy a token and transfer it multiple times, then query the indexer.

**Expected Behavior**:
- Transaction 1 (v100, T100): Create token ownership → DB shows version=100, timestamp=T100 ✓
- Transaction 2 (v200, T200): Transfer token → DB shows version=200, timestamp=T200 ✓
- Transaction 3 (v300, T300): Transfer again → DB shows version=300, timestamp=T300 ✓

**Actual Behavior**:
- Transaction 1 (v100, T100): Create token ownership → DB shows version=100, timestamp=T100 ✓
- Transaction 2 (v200, T200): Transfer token → DB shows version=200, timestamp=T100 ✗
- Transaction 3 (v300, T300): Transfer again → DB shows version=300, timestamp=T100 ✗

**Database Query Proof**:
```sql
-- Query a frequently-traded token
SELECT 
    token_data_id_hash,
    owner_address,
    last_transaction_version,
    last_transaction_timestamp
FROM current_token_ownerships 
WHERE token_data_id_hash = '<some active token>'
ORDER BY last_transaction_version DESC 
LIMIT 1;

-- Result will show a recent version number but an old timestamp
-- e.g., version=15000000, timestamp='2022-10-05' (months apart)
```

## Notes

**Important Context**:
1. This bug was introduced when the `last_transaction_timestamp` field was added to token tables via database migration in 2022-10-04, but the corresponding update logic was not added to the upsert operations
2. The V2 token implementations (added later in 2023-04-28) correctly update the timestamp field, suggesting the developers learned from this mistake
3. The coin balance indexer (added in the same 2022-10-04 migration) was implemented correctly from the start
4. This is an **indexer-specific** issue affecting off-chain query infrastructure, not the core Aptos blockchain or consensus layer
5. The actual on-chain token state remains correct; only the indexer's cached view is incorrect

### Citations

**File:** crates/indexer/src/models/token_models/token_ownerships.rs (L56-59)
```rust
    pub last_transaction_version: i64,
    pub collection_data_id_hash: String,
    pub table_type: String,
    pub last_transaction_timestamp: chrono::NaiveDateTime,
```

**File:** crates/indexer/src/processors/token_processor.rs (L380-410)
```rust
fn insert_current_token_ownerships(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentTokenOwnership],
) -> Result<(), diesel::result::Error> {
    use schema::current_token_ownerships::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentTokenOwnership::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_token_ownerships::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((token_data_id_hash, property_version, owner_address))
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    name.eq(excluded(name)),
                    amount.eq(excluded(amount)),
                    token_properties.eq(excluded(token_properties)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    collection_data_id_hash.eq(excluded(collection_data_id_hash)),
                    table_type.eq(excluded(table_type)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_token_ownerships.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/token_processor.rs (L412-453)
```rust
fn insert_current_token_datas(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentTokenData],
) -> Result<(), diesel::result::Error> {
    use schema::current_token_datas::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentTokenData::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_token_datas::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict(token_data_id_hash)
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    name.eq(excluded(name)),
                    maximum.eq(excluded(maximum)),
                    supply.eq(excluded(supply)),
                    largest_property_version.eq(excluded(largest_property_version)),
                    metadata_uri.eq(excluded(metadata_uri)),
                    payee_address.eq(excluded(payee_address)),
                    royalty_points_numerator.eq(excluded(royalty_points_numerator)),
                    royalty_points_denominator.eq(excluded(royalty_points_denominator)),
                    maximum_mutable.eq(excluded(maximum_mutable)),
                    uri_mutable.eq(excluded(uri_mutable)),
                    description_mutable.eq(excluded(description_mutable)),
                    properties_mutable.eq(excluded(properties_mutable)),
                    royalty_mutable.eq(excluded(royalty_mutable)),
                    default_properties.eq(excluded(default_properties)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    collection_data_id_hash.eq(excluded(collection_data_id_hash)),
                    description.eq(excluded(description)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_token_datas.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/token_processor.rs (L455-488)
```rust
fn insert_current_collection_datas(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentCollectionData],
) -> Result<(), diesel::result::Error> {
    use schema::current_collection_datas::dsl::*;

    let chunks = get_chunks(items_to_insert.len(), CurrentCollectionData::field_count());

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_collection_datas::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict(collection_data_id_hash)
                .do_update()
                .set((
                    creator_address.eq(excluded(creator_address)),
                    collection_name.eq(excluded(collection_name)),
                    description.eq(excluded(description)),
                    metadata_uri.eq(excluded(metadata_uri)),
                    supply.eq(excluded(supply)),
                    maximum.eq(excluded(maximum)),
                    maximum_mutable.eq(excluded(maximum_mutable)),
                    uri_mutable.eq(excluded(uri_mutable)),
                    description_mutable.eq(excluded(description_mutable)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    table_handle.eq(excluded(table_handle)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
            Some(" WHERE current_collection_datas.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/token_processor.rs (L762-796)
```rust
fn insert_current_token_ownerships_v2(
    conn: &mut PgConnection,
    items_to_insert: &[CurrentTokenOwnershipV2],
) -> Result<(), diesel::result::Error> {
    use schema::current_token_ownerships_v2::dsl::*;

    let chunks = get_chunks(
        items_to_insert.len(),
        CurrentTokenOwnershipV2::field_count(),
    );

    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_token_ownerships_v2::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((token_data_id, property_version_v1, owner_address, storage_id))
                .do_update()
                .set((
                    amount.eq(excluded(amount)),
                    table_type_v1.eq(excluded(table_type_v1)),
                    token_properties_mutated_v1.eq(excluded(token_properties_mutated_v1)),
                    is_soulbound_v2.eq(excluded(is_soulbound_v2)),
                    token_standard.eq(excluded(token_standard)),
                    is_fungible_v2.eq(excluded(is_fungible_v2)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    last_transaction_timestamp.eq(excluded(last_transaction_timestamp)),
                    inserted_at.eq(excluded(inserted_at)),
                    non_transferrable_by_owner.eq(excluded(non_transferrable_by_owner)),
                )),
            Some(" WHERE current_token_ownerships_v2.last_transaction_version <= excluded.last_transaction_version "),
        )?;
    }
    Ok(())
}
```

**File:** crates/indexer/src/processors/coin_processor.rs (L201-225)
```rust
fn insert_current_coin_balances(
    conn: &mut PgConnection,
    item_to_insert: &[CurrentCoinBalance],
) -> Result<(), diesel::result::Error> {
    use schema::current_coin_balances::dsl::*;

    let chunks = get_chunks(item_to_insert.len(), CurrentCoinBalance::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::current_coin_balances::table)
                .values(&item_to_insert[start_ind..end_ind])
                .on_conflict((owner_address, coin_type_hash))
                .do_update()
                .set((
                    amount.eq(excluded(amount)),
                    last_transaction_version.eq(excluded(last_transaction_version)),
                    last_transaction_timestamp.eq(excluded(last_transaction_timestamp)),
                    inserted_at.eq(excluded(inserted_at)),
                )),
                Some(" WHERE current_coin_balances.last_transaction_version <= excluded.last_transaction_version "),
            )?;
    }
    Ok(())
}
```
