# Audit Report

## Title
BlockStore Read-After-Write Race Condition Causes Inconsistent State Observations in Consensus Voting Decisions

## Summary
The BlockStore implementation uses separate RwLock acquisitions for each BlockReader method call, allowing write operations to interleave between sequential reads. This creates "phantom" state observations that never actually existed atomically in the system, potentially causing validators to make incorrect voting and backpressure decisions.

## Finding Description

The BlockStore stores its internal state in `Arc<RwLock<BlockTree>>` and implements the BlockReader trait. [1](#0-0) 

Each BlockReader method acquires the read lock, accesses data, and immediately releases it: [2](#0-1) 

The critical vulnerability occurs in `vote_back_pressure()`, which makes two separate BlockReader calls: [3](#0-2) 

Between lines 698 and 699, another thread can execute `send_for_execution()` which updates the ordered_root: [4](#0-3) 

Or execute `commit_callback()` which updates both commit_root and ordered_root: [5](#0-4) 

**Attack Scenario:**

1. Thread 1 (consensus voting): Calls `vote_back_pressure()`
   - Reads `commit_root().round()` = 100 (acquires/releases lock)
   
2. Thread 2 (execution): Calls `send_for_execution()`  
   - Updates `ordered_root` from round 150 → 160 (write lock)
   - Updates `commit_root` from round 100 → 110 (write lock)

3. Thread 1 (continues):
   - Reads `ordered_root().round()` = 160 (acquires/releases lock)
   - Calculates gap = 160 - 100 = 60
   - If `vote_back_pressure_limit` = 50, triggers backpressure

**Reality Check:**
- Actual old state: ordered=150, commit=100, gap=50 → No backpressure
- Actual new state: ordered=160, commit=110, gap=50 → No backpressure  
- Observed phantom state: ordered=160, commit=100, gap=60 → **FALSE backpressure triggered**

This phantom state causes the validator to refuse voting when it should vote, called from: [6](#0-5) 

Similar race conditions exist in `pipeline_pending_latency()` where `ordered_root()` is called multiple times: [7](#0-6) 

And in `need_sync_for_ledger_info()` with three separate lock acquisitions: [8](#0-7) 

## Impact Explanation

**Severity: High** (per Aptos Bug Bounty criteria: "Validator node slowdowns" and "Significant protocol violations")

The race condition can cause:

1. **False Positive Backpressure**: Validators incorrectly refuse to vote, potentially preventing quorum formation and causing consensus liveness failures
2. **False Negative Backpressure**: Validators vote when they shouldn't, potentially overloading the execution pipeline  
3. **Incorrect Proposal Parameters**: `pipeline_pending_latency()` calculates wrong latency values, affecting proposal generation at: [9](#0-8) 
4. **Inconsistent Sync Decisions**: Validators may unnecessarily trigger state sync or fail to sync when needed

While this doesn't directly cause consensus safety violations (the BFT protocol maintains safety with < 1/3 Byzantine), it represents a **significant protocol violation** that affects network health and liveness.

## Likelihood Explanation

**Likelihood: High**

This race condition occurs during normal consensus operation without requiring any attacker action:

- Consensus threads regularly call `vote_back_pressure()` during block voting
- Execution completion threads regularly call `send_for_execution()` 
- The window between reads is large enough (hash map lookups, Arc cloning) for interleaving
- High transaction throughput increases race probability
- Affects all validators equally during normal operation

## Recommendation

**Solution: Atomic Multi-Field Reads**

Modify BlockStore to support atomic snapshots for operations requiring multiple related fields:

```rust
// Add new method to BlockTree
pub(super) fn get_backpressure_state(&self) -> (Round, Round) {
    (self.commit_root().round(), self.ordered_root().round())
}

// In BlockStore
fn vote_back_pressure(&self) -> bool {
    #[cfg(any(test, feature = "fuzzing"))]
    {
        if self.back_pressure_for_test.load(Ordering::Relaxed) {
            return true;
        }
    }
    
    // Single lock acquisition gets both values atomically
    let (commit_round, ordered_round) = self.inner.read().get_backpressure_state();
    
    counters::OP_COUNTERS
        .gauge("back_pressure")
        .set((ordered_round - commit_round) as i64);
    ordered_round > self.vote_back_pressure_limit + commit_round
}
```

Similarly, fix `pipeline_pending_latency()`:

```rust
pub(super) fn get_pipeline_state(&self) -> (Arc<PipelinedBlock>, Arc<PipelinedBlock>, HashValue) {
    let ordered = self.ordered_root();
    let commit = self.commit_root();
    let ordered_id = self.ordered_root_id;
    (ordered, commit, ordered_id)
}

fn pipeline_pending_latency(&self, proposal_timestamp: Duration) -> Duration {
    // Single atomic read of all three values
    let (ordered_root, commit_root, ordered_root_id) = 
        self.inner.read().get_pipeline_state();
    
    let pending_path = self
        .path_from_commit_root(ordered_root_id)  // Use captured ID
        .unwrap_or_default();
    // ... rest of implementation
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_vote_backpressure_race_condition() {
    use std::sync::Arc;
    use std::sync::atomic::{AtomicBool, Ordering};
    
    // Setup BlockStore with vote_back_pressure_limit = 50
    let block_store = /* initialize BlockStore */;
    
    // Set initial state: commit_root=100, ordered_root=150 (gap=50, no backpressure)
    
    let store_clone = block_store.clone();
    let race_detected = Arc::new(AtomicBool::new(false));
    let race_clone = race_detected.clone();
    
    // Thread 1: Repeatedly check vote_backpressure
    let handle1 = tokio::spawn(async move {
        for _ in 0..1000 {
            let has_backpressure = store_clone.vote_back_pressure();
            if has_backpressure {
                // Detected false positive backpressure!
                race_clone.store(true, Ordering::SeqCst);
                break;
            }
        }
    });
    
    // Thread 2: Update roots to maintain gap=50
    let handle2 = tokio::spawn(async move {
        for i in 0..1000 {
            // Update ordered_root to 160 and commit_root to 110 (maintaining gap=50)
            let finality_proof = /* create proof for round 110 */;
            block_store.send_for_execution(finality_proof).await.unwrap();
        }
    });
    
    handle1.await.unwrap();
    handle2.await.unwrap();
    
    // If race_detected is true, we observed the phantom state
    assert!(race_detected.load(Ordering::SeqCst), 
            "Race condition allowed false backpressure detection");
}
```

## Notes

The vulnerability exists in production code and affects consensus operation during normal execution. While the BFT protocol's safety properties prevent actual consensus divergence (different final states), the race condition causes **consensus liveness degradation** and violates the implicit assumption that state observations within a single operation should be consistent. This represents a significant protocol violation qualifying for High severity under the Aptos Bug Bounty program.

### Citations

**File:** consensus/src/block_storage/block_store.rs (L85-104)
```rust
pub struct BlockStore {
    inner: Arc<RwLock<BlockTree>>,
    execution_client: Arc<dyn TExecutionClient>,
    /// The persistent storage backing up the in-memory data structure, every write should go
    /// through this before in-memory tree.
    storage: Arc<dyn PersistentLivenessStorage>,
    /// Used to ensure that any block stored will have a timestamp < the local time
    time_service: Arc<dyn TimeService>,
    // consistent with round type
    vote_back_pressure_limit: Round,
    payload_manager: Arc<dyn TPayloadManager>,
    #[cfg(any(test, feature = "fuzzing"))]
    back_pressure_for_test: AtomicBool,
    order_vote_enabled: bool,
    /// Window Size for Execution Pool
    window_size: Option<u64>,
    pending_blocks: Arc<Mutex<PendingBlocks>>,
    pipeline_builder: Option<PipelineBuilder>,
    pre_commit_status: Option<Arc<Mutex<PreCommitStatus>>>,
}
```

**File:** consensus/src/block_storage/block_store.rs (L338-341)
```rust
        self.inner.write().update_ordered_root(block_to_commit.id());
        self.inner
            .write()
            .insert_ordered_cert(finality_proof_clone.clone());
```

**File:** consensus/src/block_storage/block_store.rs (L630-646)
```rust
impl BlockReader for BlockStore {
    fn block_exists(&self, block_id: HashValue) -> bool {
        self.inner.read().block_exists(&block_id)
    }

    fn get_block(&self, block_id: HashValue) -> Option<Arc<PipelinedBlock>> {
        self.inner.read().get_block(&block_id)
    }

    fn ordered_root(&self) -> Arc<PipelinedBlock> {
        self.inner.read().ordered_root()
    }

    fn commit_root(&self) -> Arc<PipelinedBlock> {
        self.inner.read().commit_root()
    }

```

**File:** consensus/src/block_storage/block_store.rs (L691-704)
```rust
    fn vote_back_pressure(&self) -> bool {
        #[cfg(any(test, feature = "fuzzing"))]
        {
            if self.back_pressure_for_test.load(Ordering::Relaxed) {
                return true;
            }
        }
        let commit_round = self.commit_root().round();
        let ordered_round = self.ordered_root().round();
        counters::OP_COUNTERS
            .gauge("back_pressure")
            .set((ordered_round - commit_round) as i64);
        ordered_round > self.vote_back_pressure_limit + commit_round
    }
```

**File:** consensus/src/block_storage/block_store.rs (L706-711)
```rust
    fn pipeline_pending_latency(&self, proposal_timestamp: Duration) -> Duration {
        let ordered_root = self.ordered_root();
        let commit_root = self.commit_root();
        let pending_path = self
            .path_from_commit_root(self.ordered_root().id())
            .unwrap_or_default();
```

**File:** consensus/src/block_storage/block_tree.rs (L341-346)
```rust
    fn update_highest_commit_cert(&mut self, new_commit_cert: WrappedLedgerInfo) {
        if new_commit_cert.commit_info().round() > self.highest_commit_cert.commit_info().round() {
            self.highest_commit_cert = Arc::new(new_commit_cert);
            self.update_commit_root(self.highest_commit_cert.commit_info().id());
        }
    }
```

**File:** consensus/src/round_manager.rs (L1514-1517)
```rust
        ensure!(
            !self.sync_only(),
            "[RoundManager] sync_only flag is set, stop voting"
        );
```

**File:** consensus/src/block_storage/sync_manager.rs (L67-73)
```rust
        let block_not_exist = self.ordered_root().round() < li.commit_info().round()
            && !self.block_exists(li.commit_info().id());
        // TODO move min gap to fallback (30) to config, and if configurable make sure the value is
        // larger than buffer manager MAX_BACKLOG (20)
        let max_commit_gap = 30.max(2 * self.vote_back_pressure_limit);
        let min_commit_round = li.commit_info().round().saturating_sub(max_commit_gap);
        let current_commit_round = self.commit_root().round();
```

**File:** consensus/src/liveness/proposal_generator.rs (L766-778)
```rust
        let pipeline_pending_latency = self.block_store.pipeline_pending_latency(timestamp);
        let pipeline_backpressure = self
            .pipeline_backpressure_config
            .get_backoff(pipeline_pending_latency);
        if let Some(value) = pipeline_backpressure {
            values_max_block_txns_after_filtering
                .push(value.max_sending_block_txns_after_filtering_override);
            values_max_block.push(
                self.max_block_txns
                    .compute_with_bytes(value.max_sending_block_bytes_override),
            );
            values_proposal_delay.push(Duration::from_millis(value.backpressure_proposal_delay_ms));
            PIPELINE_BACKPRESSURE_ON_PROPOSAL_TRIGGERED.observe(1.0);
```
