# Audit Report

## Title
Race Condition in Indexer Snapshot Creation and Backup Leading to Corrupted Backups

## Summary
A race condition exists between the snapshot creation process during epoch transitions and the concurrent backup loop in the indexer-grpc table info service. This can result in incomplete or corrupted snapshots being backed up to Google Cloud Storage, causing restoration failures and potential API unavailability when attempting to recover from these backups.

## Finding Description

The `snapshot_folder_name()` function generates directory names for epoch-based snapshots. [1](#0-0) 

During epoch transitions, the main processing loop creates snapshots by calling `snapshot_indexer_async_v2()`. [2](#0-1) 

This function creates a checkpoint at a directory path. [3](#0-2) 

The checkpoint creation first removes any existing directory, then calls RocksDB's checkpoint operation. [4](#0-3) 

Concurrently, a separate async task runs every 5 seconds to scan for and backup available snapshots. [5](#0-4) 

The backup loop scans the data directory for snapshot folders. [6](#0-5) 

**Race Condition Scenarios:**

1. **Incomplete Backup**: The backup loop finds a snapshot directory while `create_checkpoint()` is still writing files. The directory exists but is incomplete. The backup process compresses and uploads this incomplete snapshot. [7](#0-6) 

2. **Directory Deletion During Write**: If the backup metadata shows an epoch is already backed up, the backup process deletes the snapshot directory. [8](#0-7)  This can occur while `create_checkpoint()` is still writing to that directory, causing corruption or failure.

**Confirmation of Issue**: The codebase contains an explicit TODO comment acknowledging this concurrency problem but it remains unaddressed. [9](#0-8) 

There is no synchronization mechanism (mutex, file lock, or completion marker) between the snapshot creation and backup processes to prevent concurrent access to the same snapshot directory.

## Impact Explanation

**Severity: Medium (State inconsistencies requiring intervention)**

This vulnerability causes corrupted backups in the indexer table info service. When a node attempts to restore from a corrupted backup:
- Restoration will fail or provide incomplete table metadata [10](#0-9) 
- API queries dependent on table info will return incorrect results or fail
- Manual intervention is required to restore proper service

This falls under **Medium severity** per the Aptos bug bounty program: "State inconsistencies requiring intervention."

**Important Note**: This affects the indexer-grpc auxiliary service, NOT the core consensus mechanism. It does not compromise blockchain consensus, validator operation, or fund safety. The impact is limited to API availability and disaster recovery scenarios.

## Likelihood Explanation

**Likelihood: High during epoch transitions**

Epoch transitions occur regularly (~every 2 hours on mainnet). The backup loop runs every 5 seconds. [11](#0-10) 

During each epoch transition:
1. A snapshot is created (non-atomic RocksDB checkpoint operation)
2. The backup loop independently scans for snapshots
3. No synchronization exists between these operations

The race window exists from when the snapshot directory is created until the checkpoint operation completes. Given the frequency of epoch transitions and backup scans, this race condition will eventually manifest in production environments.

## Recommendation

Implement proper synchronization between snapshot creation and backup processes:

**Solution 1: Atomic Directory Naming**
- Create snapshots in a temporary directory with `.tmp` suffix
- After checkpoint completes, atomically rename to final name
- Backup loop should ignore `.tmp` directories

**Solution 2: File-based Locking**
- Create a `.lock` file when starting snapshot creation
- Backup loop checks for lock file and skips locked snapshots
- Remove lock file after checkpoint completes

**Solution 3: Coordination Flag**
- Use an `Arc<AtomicBool>` or `Arc<Mutex<HashSet<u64>>>` to track epochs currently being snapshotted
- Check this flag in both snapshot creation and backup processes

**Recommended Implementation (Solution 1):**

In `db_v2.rs`:
```rust
pub fn create_checkpoint(&self, path: &PathBuf) -> Result<()> {
    let temp_path = PathBuf::from(format!("{}.tmp", path.display()));
    fs::remove_dir_all(&temp_path).unwrap_or(());
    fs::remove_dir_all(path).unwrap_or(());
    self.db.create_checkpoint(&temp_path)?;
    fs::rename(&temp_path, path)?;
    Ok(())
}
```

In `table_info_service.rs`, update the backup scan to filter `.tmp` directories:
```rust
if path.is_dir()
    && file_name.starts_with(&target_snapshot_directory_prefix)
    && !file_name.ends_with(".tmp")  // Already exists
{
    // Process snapshot
}
```

## Proof of Concept

```rust
// Test to demonstrate the race condition
#[tokio::test]
async fn test_snapshot_backup_race_condition() {
    use std::sync::Arc;
    use tokio::time::Duration;
    
    // Setup: Create a mock indexer and backup operator
    let temp_dir = tempfile::TempDir::new().unwrap();
    let snapshot_dir = temp_dir.path().join("snapshot_chain_1_epoch_10");
    
    // Simulate snapshot creation in one task
    let snapshot_path = snapshot_dir.clone();
    let create_task = tokio::spawn(async move {
        // Simulate slow checkpoint creation
        std::fs::create_dir_all(&snapshot_path).unwrap();
        tokio::time::sleep(Duration::from_secs(2)).await;
        // Write some files to simulate checkpoint
        std::fs::write(snapshot_path.join("data"), b"incomplete").unwrap();
    });
    
    // Simulate backup scan in another task (after 100ms)
    let backup_path = snapshot_dir.clone();
    let backup_task = tokio::spawn(async move {
        tokio::time::sleep(Duration::from_millis(100)).await;
        // Backup finds the directory while it's being created
        if backup_path.exists() {
            println!("RACE CONDITION: Found snapshot while still being created!");
            // In real code, this would compress and upload incomplete data
            let contents = std::fs::read_dir(&backup_path).unwrap();
            println!("Files found: {:?}", contents.count());
        }
    });
    
    create_task.await.unwrap();
    backup_task.await.unwrap();
}
```

## Notes

- This vulnerability is specific to the indexer-grpc table-info service and does not affect core blockchain consensus or validator operation
- The explicit TODO comment at line 599 confirms developers are aware of the concurrency issue but have not yet implemented a solution
- The impact is limited to disaster recovery scenarios where corrupted backups are used for restoration
- While this is a genuine data integrity issue, it does not meet Critical or High severity thresholds as it does not affect consensus, fund safety, or core network operation

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/lib.rs (L16-18)
```rust
pub fn snapshot_folder_name(chain_id: u64, epoch: u64) -> String {
    format!("snapshot_chain_{}_epoch_{}", chain_id, epoch)
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L28-28)
```rust
const TABLE_INFO_SNAPSHOT_CHECK_INTERVAL_IN_SECS: u64 = 5;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L86-98)
```rust
                let _task = tokio::spawn(async move {
                    loop {
                        aptos_logger::info!("[Table Info] Checking for snapshots to backup.");
                        Self::backup_snapshot_if_present(
                            context.clone(),
                            backup_restore_operator.clone(),
                        )
                        .await;
                        tokio::time::sleep(Duration::from_secs(
                            TABLE_INFO_SNAPSHOT_CHECK_INTERVAL_IN_SECS,
                        ))
                        .await;
                    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L136-142)
```rust
                    Self::snapshot_indexer_async_v2(
                        self.context.clone(),
                        self.indexer_async_v2.clone(),
                        previous_epoch,
                    )
                    .await
                    .expect("Failed to snapshot indexer async v2");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L429-435)
```rust
            .join(snapshot_folder_name(chain_id as u64, epoch));
        // rocksdb will create a checkpoint to take a snapshot of full db and then save it to snapshot_path
        indexer_async_v2
            .create_checkpoint(&snapshot_dir)
            .context(format!("DB checkpoint failed at epoch {}", epoch))?;

        Ok(())
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L447-463)
```rust
        let target_snapshot_directory_prefix =
            snapshot_folder_prefix(context.chain_id().id() as u64);
        // Scan the data directory to find the latest epoch to upload.
        let mut epochs_to_backup = vec![];
        for entry in std::fs::read_dir(context.node_config.get_data_dir()).unwrap() {
            let entry = entry.unwrap();
            let path = entry.path();
            let file_name = path.file_name().unwrap().to_string_lossy();
            if path.is_dir()
                && file_name.starts_with(&target_snapshot_directory_prefix)
                && !file_name.ends_with(".tmp")
            {
                let epoch = file_name.replace(&target_snapshot_directory_prefix, "");
                let epoch = epoch.parse::<u64>().unwrap();
                epochs_to_backup.push(epoch);
            }
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L576-584)
```rust
        if metadata.epoch >= epoch {
            aptos_logger::info!(
                epoch = epoch,
                snapshot_folder_name = snapshot_folder_name,
                "[Table Info] Snapshot already backed up. Skipping the backup."
            );
            // Remove the snapshot directory.
            std::fs::remove_dir_all(snapshot_dir).unwrap();
            return;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L599-599)
```rust
    // TODO: add checks to handle concurrent backup jobs.
```

**File:** storage/indexer/src/db_v2.rs (L193-196)
```rust
    pub fn create_checkpoint(&self, path: &PathBuf) -> Result<()> {
        fs::remove_dir_all(path).unwrap_or(());
        self.db.create_checkpoint(path)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L190-205)
```rust
        let tar_file = task::spawn_blocking(move || {
            aptos_logger::info!(
                snapshot_tar_file_name = snapshot_tar_file_name.as_str(),
                "[Table Info] Compressing the folder."
            );
            let result = create_tar_gz(snapshot_path_closure.clone(), &snapshot_tar_file_name);
            aptos_logger::info!(
                snapshot_tar_file_name = snapshot_tar_file_name.as_str(),
                result = result.is_ok(),
                "[Table Info] Compressed the folder."
            );
            result
        })
        .await
        .context("Failed to spawn task to create snapshot backup file.")?
        .context("Failed to create tar.gz file in blocking task")?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L264-314)
```rust
    pub async fn restore_db_snapshot(
        &self,
        chain_id: u64,
        metadata: BackupRestoreMetadata,
        db_path: PathBuf,
        base_path: PathBuf,
    ) -> anyhow::Result<()> {
        assert!(metadata.chain_id == chain_id, "Chain ID mismatch.");

        let epoch = metadata.epoch;
        let epoch_based_filename = generate_blob_name(chain_id, epoch);

        match self
            .gcs_client
            .download_streamed_object(
                &GetObjectRequest {
                    bucket: self.bucket_name.clone(),
                    object: epoch_based_filename.clone(),
                    ..Default::default()
                },
                &Range::default(),
            )
            .await
        {
            Ok(mut stream) => {
                // Create a temporary file and write the stream to it directly
                let temp_file_name = "snapshot.tar.gz";
                let temp_file_path = base_path.join(temp_file_name);
                let temp_file_path_clone = temp_file_path.clone();
                let mut temp_file = File::create(&temp_file_path_clone).await?;
                while let Some(chunk) = stream.next().await {
                    match chunk {
                        Ok(data) => temp_file.write_all(&data).await?,
                        Err(e) => return Err(anyhow::Error::new(e)),
                    }
                }
                temp_file.sync_all().await?;

                // Spawn blocking a thread to synchronously unpack gzipped tar file without blocking the async thread
                task::spawn_blocking(move || unpack_tar_gz(&temp_file_path_clone, &db_path))
                    .await?
                    .expect("Failed to unpack gzipped tar file");

                fs::remove_file(&temp_file_path)
                    .await
                    .context("Failed to remove temporary file after unpacking")?;
                Ok(())
            },
            Err(e) => Err(anyhow::Error::new(e)),
        }
    }
```
