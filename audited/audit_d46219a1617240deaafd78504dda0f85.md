# Audit Report

## Title
DKG Participation Divergence Due to Unsynchronized `randomness_override_seq_num` Configuration

## Summary
Validators with different `randomness_override_seq_num` configuration values can make inconsistent decisions about DKG participation during epoch initialization, potentially causing insufficient validator participation and DKG failure when the participating subset falls below the quorum threshold.

## Finding Description

The `randomness_override_seq_num` parameter is loaded independently from each validator's local node configuration file with no cross-validator validation. [1](#0-0) 

During epoch initialization, each validator independently compares its local `randomness_override_seq_num` against the on-chain `RandomnessConfigSeqNum.seq_num` to determine the effective randomness configuration. [2](#0-1) 

The critical logic in `OnChainRandomnessConfig::from_configs` force-disables randomness when the local override value exceeds the on-chain value. [3](#0-2) 

This creates a configuration divergence where validators with `local_seqnum > onchain_seqnum` will skip DKG manager initialization, while others with `local_seqnum <= onchain_seqnum` will participate. [4](#0-3) 

**Attack Scenario:**
1. On-chain `RandomnessConfigSeqNum.seq_num = 1` (normal operation)
2. 65% of validators have `randomness_override_seq_num = 0` (default, will participate)
3. 35% of validators have `randomness_override_seq_num = 2` (misconfigured, will not participate)
4. Only 65% of validators spawn DKG managers and participate in DKG
5. DKG requires ~67% quorum for transcript aggregation (reconstruction threshold) [5](#0-4) 
6. 65% < 67%, so quorum cannot be reached
7. DKG fails to complete, no `last_completed` session exists
8. Consensus epoch manager detects missing DKG completion and disables randomness for next epoch [6](#0-5) 

## Impact Explanation

This issue qualifies as **High Severity** under Aptos bug bounty criteria for "Significant protocol violations" and "Validator node slowdowns":

1. **Randomness Feature Unavailability**: When DKG fails due to insufficient participation, randomness becomes unavailable in subsequent epochs, breaking any on-chain functionality that depends on randomness.

2. **Silent Failure**: The system provides no validation, warning, or detection mechanism for configuration divergence across validators. Operators have no visibility into whether other validators have different settings.

3. **Recovery Complexity**: Once randomness is disabled due to failed DKG, all validators must coordinate to either fix their configurations or deliberately disable randomness via the override mechanism. [7](#0-6) 

## Likelihood Explanation

**Likelihood: Medium-High**

While this requires validator operator action rather than external attacker exploitation, the likelihood is elevated because:

1. **Intended Use Case**: The `randomness_override_seq_num` field is documented as a recovery mechanism [8](#0-7) , so operators may legitimately set different values during recovery procedures.

2. **No Coordination Protocol**: The documentation suggests operators should set this value during stalls, but provides no protocol ensuring all operators set it to the same value simultaneously.

3. **Configuration Drift**: In distributed systems, configuration files can drift across nodes over time, especially after manual interventions or partial rollbacks.

4. **No Runtime Validation**: The node configuration is loaded at startup with no checks against peer configurations. [9](#0-8) 

## Recommendation

Implement multi-layered validation to prevent configuration divergence:

**1. Startup Validation:**
Add a configuration consistency check during epoch initialization that validates critical consensus parameters match across validators before proceeding with DKG:

```rust
// In dkg/src/epoch_manager.rs, start_new_epoch function
fn validate_randomness_config_consistency(
    &self,
    local_override: u64,
    onchain_seq_num: u64,
) -> Result<()> {
    // Broadcast local override value to peers
    // Collect peer values
    // Verify all validators have consistent override values
    // Return error if divergence detected
}
```

**2. On-Chain Override State:**
Instead of using local configuration, store the override sequence number on-chain through governance, ensuring all validators observe the same value:

```rust
// Create new on-chain config: RandomnessOverrideSeqNum
// Load during epoch initialization alongside RandomnessConfigSeqNum
// All validators guaranteed to see identical value
```

**3. Configuration File Warnings:**
Add explicit warnings in the configuration file template and documentation that `randomness_override_seq_num` must be coordinated across ALL validators and should only be modified during coordinated recovery procedures.

**4. Metrics and Alerting:**
Expose metrics comparing local override value with on-chain value and emit alerts when they diverge, allowing operators to detect potential issues before they cause DKG failures.

## Proof of Concept

```rust
// Reproduction steps:
// 1. Setup a local testnet with 4 validators
// 2. Configure 3 validators with randomness_override_seq_num = 0
// 3. Configure 1 validator with randomness_override_seq_num = 100
// 4. Set on-chain RandomnessConfigSeqNum.seq_num = 50
// 5. Enable randomness with reconstruction_threshold = 67%
// 6. Start epoch transition

// Expected behavior:
// - 3 validators (75%) will participate in DKG
// - 1 validator (25%) will NOT participate
// - DKG completes successfully (75% > 67%)

// Now configure 2 validators with randomness_override_seq_num = 100

// Expected behavior:
// - 2 validators (50%) will participate in DKG
// - 2 validators (50%) will NOT participate  
// - DKG FAILS (50% < 67%)
// - Randomness disabled in next epoch

// Test implementation in testsuite/smoke-test/src/randomness/:
#[tokio::test]
async fn test_dkg_participation_divergence() {
    let (mut swarm, _cli, _faucet) = SwarmBuilder::new_local(4)
        .with_init_config(Arc::new(|idx, conf, _| {
            // Set divergent override values
            conf.randomness_override_seq_num = if idx < 2 { 0 } else { 100 };
        }))
        .with_init_genesis_config(Arc::new(|conf| {
            conf.consensus_config.enable_validator_txns();
            conf.randomness_config_override = Some(
                OnChainRandomnessConfig::default_enabled()
            );
        }))
        .build_with_cli(0)
        .await;

    // Set on-chain seq_num to 50 via governance
    // ... governance transaction code ...

    // Wait for epoch transition
    swarm.wait_for_all_nodes_to_catchup_to_epoch(2, Duration::from_secs(30))
        .await
        .expect("Epoch 2 should arrive");

    // Verify DKG failed due to insufficient participation
    let dkg_state = get_on_chain_resource::<DKGState>(&rest_client, account_address).await;
    assert!(dkg_state.last_completed.is_none(), "DKG should have failed");
}
```

## Notes

**Important Limitation:** This vulnerability requires validator operators to configure their nodes with different `randomness_override_seq_num` values. Since validator operators are considered trusted actors in the Aptos threat model, this is more accurately classified as a **robustness issue** or **configuration validation gap** rather than an externally exploitable vulnerability.

The issue becomes exploitable in scenarios where:
1. Operators independently misconfigure during recovery procedures
2. Configuration management tools have bugs leading to divergent deployments
3. Partial rollouts or staged updates create temporary divergence

The primary security concern is the **lack of defensive validation** - the system should detect and prevent configuration divergence for critical consensus parameters rather than silently allowing it to break DKG quorum requirements.

### Citations

**File:** config/src/config/node_config.rs (L78-81)
```rust
    /// In a randomness stall, set this to be on-chain `RandomnessConfigSeqNum` + 1.
    /// Once enough nodes restarted with the new value, the chain should unblock with randomness disabled.
    #[serde(default)]
    pub randomness_override_seq_num: u64,
```

**File:** dkg/src/epoch_manager.rs (L176-190)
```rust
        info!(
            epoch = epoch_state.epoch,
            local = self.randomness_override_seq_num,
            onchain = onchain_randomness_config_seq_num.seq_num,
            "Checking randomness config override."
        );
        if self.randomness_override_seq_num > onchain_randomness_config_seq_num.seq_num {
            warn!("Randomness will be force-disabled by local config!");
        }

        let onchain_randomness_config = OnChainRandomnessConfig::from_configs(
            self.randomness_override_seq_num,
            onchain_randomness_config_seq_num.seq_num,
            randomness_config_move_struct.ok(),
        );
```

**File:** dkg/src/epoch_manager.rs (L199-259)
```rust
        let randomness_enabled =
            consensus_config.is_vtxn_enabled() && onchain_randomness_config.randomness_enabled();
        if let (true, Some(my_index)) = (randomness_enabled, my_index) {
            let DKGState {
                in_progress: in_progress_session,
                ..
            } = payload.get::<DKGState>().unwrap_or_default();

            let network_sender = self.create_network_sender();
            let rb = ReliableBroadcast::new(
                self.my_addr,
                epoch_state.verifier.get_ordered_account_addresses(),
                Arc::new(network_sender),
                ExponentialBackoff::from_millis(self.rb_config.backoff_policy_base_ms)
                    .factor(self.rb_config.backoff_policy_factor)
                    .max_delay(Duration::from_millis(
                        self.rb_config.backoff_policy_max_delay_ms,
                    )),
                aptos_time_service::TimeService::real(),
                Duration::from_millis(self.rb_config.rpc_timeout_ms),
                BoundedExecutor::new(8, tokio::runtime::Handle::current()),
            );
            let agg_trx_producer = AggTranscriptProducer::new(rb);

            let (dkg_start_event_tx, dkg_start_event_rx) =
                aptos_channel::new(QueueStyle::KLAST, 1, None);
            self.dkg_start_event_tx = Some(dkg_start_event_tx);

            let (dkg_rpc_msg_tx, dkg_rpc_msg_rx) = aptos_channel::new::<
                AccountAddress,
                (AccountAddress, IncomingRpcRequest),
            >(QueueStyle::FIFO, 100, None);
            self.dkg_rpc_msg_tx = Some(dkg_rpc_msg_tx);
            let (dkg_manager_close_tx, dkg_manager_close_rx) = oneshot::channel();
            self.dkg_manager_close_tx = Some(dkg_manager_close_tx);
            let my_pk = epoch_state
                .verifier
                .get_public_key(&self.my_addr)
                .ok_or_else(|| anyhow!("my pk not found in validator set"))?;
            let dealer_sk = self
                .key_storage
                .consensus_sk_by_pk(my_pk.clone())
                .map_err(|e| {
                    anyhow!("dkg new epoch handling failed with consensus sk lookup err: {e}")
                })?;
            let dkg_manager = DKGManager::<DefaultDKG>::new(
                Arc::new(dealer_sk),
                Arc::new(my_pk),
                my_index,
                self.my_addr,
                epoch_state,
                Arc::new(agg_trx_producer),
                self.vtxn_pool.clone(),
            );
            tokio::spawn(dkg_manager.run(
                in_progress_session,
                dkg_start_event_rx,
                dkg_rpc_msg_rx,
                dkg_manager_close_rx,
            ));
        };
```

**File:** types/src/on_chain_config/randomness_config.rs (L52-65)
```rust
impl Default for ConfigV2 {
    fn default() -> Self {
        Self {
            secrecy_threshold: FixedPoint64MoveStruct::from_u64f64(
                U64F64::from_num(1) / U64F64::from_num(2),
            ),
            reconstruction_threshold: FixedPoint64MoveStruct::from_u64f64(
                U64F64::from_num(2) / U64F64::from_num(3),
            ),
            fast_path_secrecy_threshold: FixedPoint64MoveStruct::from_u64f64(
                U64F64::from_num(2) / U64F64::from_num(3),
            ),
        }
    }
```

**File:** types/src/on_chain_config/randomness_config.rs (L139-151)
```rust
    pub fn from_configs(
        local_seqnum: u64,
        onchain_seqnum: u64,
        onchain_raw_config: Option<RandomnessConfigMoveStruct>,
    ) -> Self {
        if local_seqnum > onchain_seqnum {
            Self::default_disabled()
        } else {
            onchain_raw_config
                .and_then(|onchain_raw| OnChainRandomnessConfig::try_from(onchain_raw).ok())
                .unwrap_or_else(OnChainRandomnessConfig::default_if_missing)
        }
    }
```

**File:** consensus/src/epoch_manager.rs (L1039-1044)
```rust
        let dkg_state = maybe_dkg_state.map_err(NoRandomnessReason::DKGStateResourceMissing)?;
        let dkg_session = dkg_state
            .last_completed
            .ok_or_else(|| NoRandomnessReason::DKGCompletedSessionResourceMissing)?;
        if dkg_session.metadata.dealer_epoch + 1 != new_epoch_state.epoch {
            return Err(NoRandomnessReason::CompletedSessionTooOld);
```

**File:** testsuite/smoke-test/src/randomness/randomness_stall_recovery.rs (L64-84)
```rust
    info!("Hot-fixing all validators.");
    for (idx, validator) in swarm.validators_mut().enumerate() {
        info!("Stopping validator {}.", idx);
        validator.stop();
        let config_path = validator.config_path();
        let mut validator_override_config =
            OverrideNodeConfig::load_config(config_path.clone()).unwrap();
        validator_override_config
            .override_config_mut()
            .randomness_override_seq_num = 1;
        validator_override_config
            .override_config_mut()
            .consensus
            .sync_only = false;
        info!("Updating validator {} config.", idx);
        validator_override_config.save_config(config_path).unwrap();
        info!("Restarting validator {}.", idx);
        validator.start().unwrap();
        info!("Let validator {} bake for 5 secs.", idx);
        tokio::time::sleep(Duration::from_secs(5)).await;
    }
```

**File:** aptos-node/src/consensus.rs (L87-97)
```rust
            let dkg_runtime = start_dkg_runtime(
                my_addr,
                &node_config.consensus.safety_rules,
                network_client,
                network_service_events,
                reconfig_events,
                dkg_start_events,
                vtxn_pool.clone(),
                rb_config,
                node_config.randomness_override_seq_num,
            );
```
