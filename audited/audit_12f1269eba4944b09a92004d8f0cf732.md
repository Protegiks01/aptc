# Audit Report

## Title
Indexer Backfiller Data Loss Due to Strict 1000-Transaction Batch Requirement

## Summary
The indexer-grpc-file-store-backfiller worker tasks enforce an exact 1000-transaction batch size requirement, but the gRPC server can legitimately send partial batches (< 1000 transactions) when approaching the ending_version boundary. This causes valid transactions to be buffered but never processed, resulting in incomplete indexer data and backfill job crashes.

## Finding Description

The backfiller's worker tasks enforce a strict validation check requiring exactly 1000 transactions per batch: [1](#0-0) 

However, the gRPC server (`IndexerStreamCoordinator`) explicitly adjusts batch sizes when approaching the end of the requested range: [2](#0-1) 

This creates a critical mismatch:

1. **Server-side behavior**: When `ending_version` is reached, the coordinator sends a final batch containing fewer than 1000 transactions (e.g., if ending_version is 10500, the last batch contains only 500 transactions from versions 10000-10499).

2. **Client-side buffering**: The backfiller accumulates received transactions in a `transactions_buffer` and only processes them when at least 1000 transactions are available: [3](#0-2) 

3. **Data loss scenario**: When the final partial batch (< 1000 transactions) is received:
   - Transactions are added to the buffer
   - The `while transactions_buffer.len() >= 1000` condition fails
   - These transactions remain in the buffer, never sent to worker tasks
   - The stream ends and the code panics at the unwrap: [4](#0-3) 

4. **Validation failure**: Even if the stream handling were fixed, any partial batch sent to workers would fail the strict size validation, causing the job to crash.

**Concrete Example:**
- Backfill from version 0 with `transactions_count = 1500` (ending_version = 1500)
- Batch 1: versions 0-999 (1000 txns) → buffered → sent to workers → processed ✓
- Batch 2: versions 1000-1499 (500 txns) → buffered → never sent (buffer only has 500 < 1000) → lost
- Stream ends → panic at line 267
- Result: Transactions 1000-1499 permanently missing from file store

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty criteria:
- **State inconsistencies requiring intervention**: The indexer file store will be permanently incomplete, missing the final transactions in any backfill range not aligned to 1000-transaction boundaries
- **Service unavailability**: The backfill job crashes and cannot complete, preventing historical data population
- **Data integrity violation**: Applications relying on complete historical indexer data will receive incomplete results

While this affects indexer infrastructure rather than core blockchain consensus/execution, it impacts the ecosystem's data availability guarantees.

## Likelihood Explanation

**HIGH** - This bug will trigger in all of the following common scenarios:
1. Any backfill job where `transactions_count` is not a multiple of 1000
2. Backfilling to the current ledger tip when the latest version is not aligned to 1000
3. Any bounded backfill range (e.g., "backfill epoch 5") where boundaries don't align to 1000

The issue is deterministic and will occur 100% of the time for these inputs. Given that transaction version numbers are sequential and arbitrary, most practical backfill operations will be affected.

## Recommendation

Modify the buffering logic to drain all remaining transactions when the stream ends, regardless of batch size. Additionally, relax the worker validation to accept partial batches at range boundaries:

```rust
// In backfill() function, replace the infinite loop with:
loop {
    let item = grpc_stream.next().await;
    let response = match item {
        Some(Ok(r)) => r,
        Some(Err(e)) => {
            tracing::error!("Stream error: {:?}", e);
            panic!("Stream error: {:?}", e);
        },
        None => {
            // Stream ended - drain remaining buffer
            if !transactions_buffer.is_empty() {
                let mut remaining = Vec::new();
                while let Some((_, txn)) = transactions_buffer.pop_first() {
                    remaining.push(txn);
                }
                if !remaining.is_empty() {
                    sender.send(remaining).await?;
                }
            }
            break;
        }
    };
    // ... rest of processing
}

// In worker task, replace the strict check:
// Old: ensure!(transactions.len() == 1000, "Unexpected transaction count");
// New:
ensure!(
    transactions.len() == 1000 || 
    (transactions.len() < 1000 && transactions[0].version % 1000 == 0),
    "Invalid transaction batch size: expected 1000 or partial batch at boundary, got {}",
    transactions.len()
);
```

## Proof of Concept

```rust
// Reproduction steps:
// 1. Deploy indexer-grpc-fullnode with test data up to version 1500
// 2. Run backfiller with:
//    starting_version = 0
//    transactions_count = 1500
// 3. Observe logs showing:
//    - Batch 0-999 processed successfully
//    - Batch 1000-1499 buffered but never sent
//    - Panic: "called `Option::unwrap()` on a `None` value"
// 4. Check file store - missing transactions 1000-1499

// Alternative PoC: Modify processor_test.rs (if it exists) to test partial batch handling:
#[tokio::test]
async fn test_partial_batch_at_end() {
    // Set up mock stream that sends:
    // - Init signal
    // - 1000 transactions (versions 0-999)
    // - BatchEnd signal
    // - 500 transactions (versions 1000-1499)
    // - BatchEnd signal
    // - Stream closes
    
    // Expected: All 1500 transactions uploaded
    // Actual: Only 1000 uploaded, panic on stream close
}
```

## Notes

This vulnerability is specific to the indexer-grpc-file-store-backfiller infrastructure component and does not affect blockchain consensus, execution, or validator operations. However, it represents a critical operational bug that prevents reliable historical data backfilling, which is essential for ecosystem tools and applications that rely on complete transaction history.

The root cause is an architectural mismatch between the server's flexible batch sizing (which correctly adapts to range boundaries) and the client's rigid expectation of 1000-transaction batches. Both the buffering logic and worker validation need to be updated to handle partial batches at range boundaries.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L189-189)
```rust
                        ensure!(transactions.len() == 1000, "Unexpected transaction count");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L266-267)
```rust
            let item = grpc_stream.next().await;
            let item = item.unwrap();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store-backfiller/src/processor.rs (L290-299)
```rust
                    while transactions_buffer.len() >= 1000 {
                        // Take the first 1000 transactions.
                        let mut transactions = Vec::new();
                        // Pop the first 1000 transactions from buffer.
                        for _ in 0..1000 {
                            let (_, txn) = transactions_buffer.pop_first().unwrap();
                            transactions.push(txn);
                        }
                        sender.send(transactions).await?;
                    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L304-307)
```rust
            let num_transactions_to_fetch = std::cmp::min(
                self.processor_batch_size as u64,
                end_version - starting_version,
            ) as u16;
```
