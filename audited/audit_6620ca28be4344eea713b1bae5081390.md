# Audit Report

## Title
Peer Connection Inconsistent State Due to Incomplete Error Handling in PeerManager::add_peer()

## Summary
When `PeerManager::add_peer()` fails during metadata insertion at line 684-687, the peer is left in an inconsistent state: the Peer actor remains running and the peer stays in `active_peers`, but it's not registered in `peers_and_metadata` and no connection notifications are sent. This creates a "ghost peer" that is invisible to critical network components while still consuming resources and processing messages. [1](#0-0) 

## Finding Description

The vulnerability exists in the error handling flow of `PeerManager::add_peer()`. The function performs three critical operations in sequence:

1. **Spawns the Peer actor** (line 679) - The actor immediately starts running in the background
2. **Inserts into active_peers** (lines 682-683) - Makes the peer visible to PeerManager
3. **Inserts into peers_and_metadata** (lines 684-687) - Registers the peer globally with `?` error propagation [1](#0-0) 

When step 3 fails (e.g., if the `network_id` is not registered in `PeersAndMetadata`), the function returns early via the `?` operator. The caller only logs a warning and continues: [2](#0-1) 

This leaves the system in an inconsistent state where:
- The Peer actor is running and processing messages
- The peer exists in `active_peers` (PeerManager's view)
- The peer does NOT exist in `peers_and_metadata` (global view)
- No `NewPeer` notification is sent to `connection_event_handlers`
- No broadcast event is sent to `PeersAndMetadata` subscribers

The failure condition in `insert_connection_metadata()` occurs when `get_peer_metadata_for_network()` cannot find the network_id: [3](#0-2) 

Critical components that depend on connection notifications include:

**ConnectivityManager** - Won't see the peer as connected: [4](#0-3) 

**Application Subscribers** - Consensus, mempool, and state sync won't receive NewPeer events via the subscription mechanism: [5](#0-4) 

## Impact Explanation

This qualifies as **High Severity** under the bug bounty program's "Significant protocol violations" category because:

1. **Network View Inconsistency**: Different components have conflicting views of network connectivity. PeerManager thinks the peer is connected, but ConnectivityManager and application layers don't see it.

2. **Consensus Impact**: If this affects validator connections, consensus participants may have inconsistent views of which validators are available, potentially impacting voting and block propagation.

3. **Resource Leak**: The Peer actor continues running indefinitely without proper tracking or cleanup, consuming system resources.

4. **Connection Management Failure**: ConnectivityManager cannot properly manage connections it doesn't know about, breaking automatic reconnection and connection balancing logic.

5. **Metrics Corruption**: Connection metrics become inaccurate, making monitoring and debugging difficult.

When the peer eventually disconnects, `remove_peer_from_metadata()` will fail but only log a warning, leaving the inconsistency unresolved: [6](#0-5) 

## Likelihood Explanation

While this requires a configuration mismatch between the `PeerManager`'s network_context and the networks registered in `PeersAndMetadata`, this can occur in several scenarios:

1. **Configuration Errors**: Deployment with mismatched network configurations
2. **Dynamic Network Changes**: Runtime network reconfiguration without proper synchronization
3. **Race Conditions**: Edge cases during network initialization or epoch changes
4. **Software Updates**: Version mismatches during rolling updates

The likelihood is **Medium** because while it requires specific conditions, the consequences are severe and the error handling is fundamentally broken regardless of how rarely it occurs.

## Recommendation

Implement proper cleanup in `add_peer()` when `insert_connection_metadata()` fails:

```rust
// After spawning peer and before inserting metadata
self.executor.spawn(peer.start());

// Save PeerRequest sender to `active_peers`.
self.active_peers
    .insert(peer_id, (conn_meta.clone(), peer_reqs_tx));

// Try to insert metadata - if this fails, clean up
if let Err(error) = self.peers_and_metadata.insert_connection_metadata(
    PeerNetworkId::new(self.network_context.network_id(), peer_id),
    conn_meta.clone(),
) {
    // Cleanup: remove from active_peers to prevent inconsistent state
    if let Some((_, peer_handle)) = self.active_peers.remove(&peer_id) {
        // Drop the peer handle to trigger actor shutdown
        drop(peer_handle);
        warn!(
            NetworkSchema::new(&self.network_context),
            "Failed to insert peer metadata, cleaned up active peer. Error: {:?}",
            error
        );
    }
    return Err(error);
}

// Send NewPeer notification only after successful metadata insertion
if send_new_peer_notification {
    let notif = ConnectionNotification::NewPeer(conn_meta, self.network_context.network_id());
    self.send_conn_notification(peer_id, notif);
}

Ok(())
```

## Proof of Concept

```rust
#[test]
fn test_add_peer_metadata_insertion_failure_leaves_inconsistent_state() {
    use tokio::runtime::Runtime;
    
    let rt = Runtime::new().unwrap();
    rt.block_on(async {
        // Create PeersAndMetadata with only NetworkId::Validator registered
        let peers_and_metadata = PeersAndMetadata::new(&[NetworkId::Validator]);
        
        // Create PeerManager with a DIFFERENT network_id that's NOT registered
        let network_context = NetworkContext::new(
            NetworkId::Public, // NOT registered in peers_and_metadata!
            RoleType::Validator,
            PeerId::random(),
        );
        
        let (connection_reqs_tx, connection_reqs_rx) = 
            aptos_channel::new(QueueStyle::FIFO, 10, None);
        let (requests_tx, requests_rx) = 
            aptos_channel::new(QueueStyle::FIFO, 10, None);
            
        let mut peer_manager = PeerManager::new(
            rt.handle().clone(),
            TimeService::mock(),
            transport,
            network_context,
            listen_addr,
            peers_and_metadata.clone(),
            requests_rx,
            connection_reqs_rx,
            HashMap::new(),
            vec![],
            10,
            4096,
            65536,
            100,
        );
        
        // Create a test connection
        let peer_id = PeerId::random();
        let connection = Connection::mock(peer_id);
        
        // Call add_peer - this should fail at insert_connection_metadata()
        let result = peer_manager.add_peer(connection);
        
        // Verify the error occurred
        assert!(result.is_err());
        
        // BUG: Peer is still in active_peers despite the error!
        assert!(peer_manager.active_peers.contains_key(&peer_id));
        
        // BUG: Peer is NOT in peers_and_metadata
        assert!(peers_and_metadata
            .get_metadata_for_peer(
                PeerNetworkId::new(NetworkId::Public, peer_id)
            )
            .is_err()
        );
        
        // BUG: Peer actor is running but invisible to the system
        // This creates a "ghost peer" that consumes resources and processes
        // messages but is invisible to consensus, mempool, and other components
    });
}
```

**Notes**

The core issue is that `add_peer()` performs non-atomic operations (spawn actor, insert into active_peers, insert into metadata) without proper cleanup on failure. This violates the **State Consistency** invariant that "State transitions must be atomic". The incomplete error handling creates observable inconsistencies that can impact consensus coordination, connection management, and network reliability. While the trigger condition may be rare (configuration mismatch), the fundamental bug in error handling represents a significant protocol violation that merits High severity classification.

### Citations

**File:** network/framework/src/peer_manager/mod.rs (L398-403)
```rust
        if let Err(error) = self.add_peer(conn) {
            warn!(
                NetworkSchema::new(&self.network_context),
                "Failed to add peer. Error: {:?}", error
            )
        }
```

**File:** network/framework/src/peer_manager/mod.rs (L407-420)
```rust
    fn remove_peer_from_metadata(&mut self, peer_id: AccountAddress, connection_id: ConnectionId) {
        let peer_network_id = PeerNetworkId::new(self.network_context.network_id(), peer_id);
        if let Err(error) = self
            .peers_and_metadata
            .remove_peer_metadata(peer_network_id, connection_id)
        {
            warn!(
                NetworkSchema::new(&self.network_context),
                "Failed to remove peer from peers and metadata. Peer: {:?}, error: {:?}",
                peer_network_id,
                error
            );
        }
    }
```

**File:** network/framework/src/peer_manager/mod.rs (L679-687)
```rust
        self.executor.spawn(peer.start());

        // Save PeerRequest sender to `active_peers`.
        self.active_peers
            .insert(peer_id, (conn_meta.clone(), peer_reqs_tx));
        self.peers_and_metadata.insert_connection_metadata(
            PeerNetworkId::new(self.network_context.network_id(), peer_id),
            conn_meta.clone(),
        )?;
```

**File:** network/framework/src/application/storage.rs (L186-214)
```rust
    pub fn insert_connection_metadata(
        &self,
        peer_network_id: PeerNetworkId,
        connection_metadata: ConnectionMetadata,
    ) -> Result<(), Error> {
        // Grab the write lock for the peer metadata
        let mut peers_and_metadata = self.peers_and_metadata.write();

        // Fetch the peer metadata for the given network
        let peer_metadata_for_network =
            get_peer_metadata_for_network(&peer_network_id, &mut peers_and_metadata)?;

        // Update the metadata for the peer or insert a new entry
        peer_metadata_for_network
            .entry(peer_network_id.peer_id())
            .and_modify(|peer_metadata| {
                peer_metadata.connection_metadata = connection_metadata.clone()
            })
            .or_insert_with(|| PeerMetadata::new(connection_metadata.clone()));

        // Update the cached peers and metadata
        self.set_cached_peers_and_metadata(peers_and_metadata.clone());

        let event =
            ConnectionNotification::NewPeer(connection_metadata, peer_network_id.network_id());
        self.broadcast(event);

        Ok(())
    }
```

**File:** network/framework/src/application/storage.rs (L397-419)
```rust
    /// subscribe() returns a channel for receiving NewPeer/LostPeer events.
    /// subscribe() immediately sends all* current connections as NewPeer events.
    /// (* capped at NOTIFICATION_BACKLOG, currently 1000, use get_connected_peers() to be sure)
    pub fn subscribe(&self) -> tokio::sync::mpsc::Receiver<ConnectionNotification> {
        let (sender, receiver) = tokio::sync::mpsc::channel(NOTIFICATION_BACKLOG);
        let peers_and_metadata = self.peers_and_metadata.read();
        'outer: for (network_id, network_peers_and_metadata) in peers_and_metadata.iter() {
            for (_addr, peer_metadata) in network_peers_and_metadata.iter() {
                let event = ConnectionNotification::NewPeer(
                    peer_metadata.connection_metadata.clone(),
                    *network_id,
                );
                if let Err(err) = sender.try_send(event) {
                    warn!("could not send initial NewPeer on subscribe(): {:?}", err);
                    break 'outer;
                }
            }
        }
        // I expect the peers_and_metadata read lock to still be in effect until after listeners.push() below
        let mut listeners = self.subscribers.lock();
        listeners.push(sender);
        receiver
    }
```

**File:** network/framework/src/connectivity_manager/mod.rs (L1011-1019)
```rust
            peer_manager::ConnectionNotification::NewPeer(metadata, _network_id) => {
                let peer_id = metadata.remote_peer_id;
                counters::peer_connected(&self.network_context, &peer_id, 1);
                self.connected.insert(peer_id, metadata);

                // Cancel possible queued dial to this peer.
                self.dial_states.remove(&peer_id);
                self.dial_queue.remove(&peer_id);
            },
```
