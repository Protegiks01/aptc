# Audit Report

## Title
State Restore Atomicity Violation: KV and Merkle Tree Divergence via Parallel Execution Failure

## Summary
The state snapshot restoration process executes KV store writes and Merkle tree updates in parallel without atomicity guarantees. When proof verification fails, the KV data is already permanently committed while the Merkle tree writes are aborted, leaving the database in an inconsistent state. The vulnerability is exacerbated by in-memory state corruption in `JellyfishMerkleRestore` that causes subsequent chunks to skip the failed range, making the corruption permanent.

## Finding Description

The vulnerability exists in the parallel execution pattern where KV and tree operations execute without transactional coordination: [1](#0-0) 

The `IO_POOL.join(kv_fn, tree_fn)` executes two database operations in parallel. The `kv_fn` calls `write_kv_batch()` which commits directly to RocksDB: [2](#0-1) 

This commit is permanent and durable via `state_kv_db.commit()`: [3](#0-2) 

Meanwhile, `tree_fn` processes the chunk and verifies the proof BEFORE writing any nodes: [4](#0-3) 

**Critical Flaw:** At lines 381-385, `self.previous_leaf` is updated in memory for each key in the chunk. At line 391, `verify(proof)?` is called. If verification fails, the function returns an error BUT `previous_leaf` is already updated to point to the last key (F) even though no tree nodes were written to storage.

**Exploitation Scenario:**
1. Malicious peer sends chunk [D, E, F] with valid KV data but invalid proof
2. Both operations execute in parallel
3. `kv_fn` succeeds: writes D-F to KV database, commits progress showing "completed up to F"
4. `tree_fn` fails: updates `previous_leaf` to F in memory (lines 381-385), then verify() fails at line 391, no nodes written
5. Error propagates but KV commit is permanent
6. State sync reuses the same `StateSnapshotRestore` instance for next chunk: [5](#0-4) 

7. When next chunk [G, H, I] arrives, `JellyfishMerkleRestore` skips D-F based on corrupted `previous_leaf` (F): [6](#0-5) 

8. **Result:** KV has D-F and G-I, but Merkle tree only has G-I - permanent state corruption.

The error handling does not recreate the receiver instance: [7](#0-6) 

Progress tracking cannot recover because KV progress shows D-F completed while tree progress is corrupted in memory.

## Impact Explanation

**Severity: Critical**

This vulnerability causes permanent state corruption where the KV store and Jellyfish Merkle tree become permanently desynchronized:

1. **Permanent State Corruption:** Affected keys exist in KV storage but have no corresponding Merkle tree nodes, making it impossible to generate valid Merkle proofs for those keys.

2. **Consensus Divergence Risk:** If multiple nodes are attacked at different key ranges during state sync, they will have different corrupted states, potentially causing consensus failures when computing state root hashes.

3. **Unrecoverable Without Manual Intervention:** Recovery requires complete database reset and resync from genesis, or manual database surgery to identify and remove orphaned KV entries.

This meets the Critical severity criterion of "Non-recoverable network partition" and "Permanent state corruption" per Aptos Bug Bounty guidelines.

## Likelihood Explanation

**Likelihood: High**

The attack requires only:
- Ability to act as a state sync peer (no privileges required - anyone can join the network)
- Crafting chunks with invalid Merkle proofs (trivial - simply provide wrong sibling hashes or mismatched root hash)
- No cryptographic breaks or validator compromise needed

The vulnerability is triggered during:
- Regular state sync operations for new/syncing nodes
- State snapshot restoration during recovery
- Fast sync modes where nodes bootstrap from snapshots

State sync is a continuous network operation, making this attack surface constantly available. The parallel execution runs by default in `StateSnapshotRestoreMode::Default`.

## Recommendation

Implement one of the following fixes:

**Option 1: Atomic Transaction Wrapper**
Wrap both KV and tree operations in a single database transaction that commits or rolls back atomically.

**Option 2: Two-Phase Commit**
- Phase 1: Verify proof and prepare both batches without committing
- Phase 2: Only if both succeed, commit both atomically

**Option 3: Rollback In-Memory State**
In `JellyfishMerkleRestore::add_chunk_impl()`, save the initial state of `previous_leaf` and restore it if `verify()` fails:

```rust
pub fn add_chunk_impl(...) -> Result<()> {
    let initial_previous_leaf = self.previous_leaf.clone();
    
    for (key, value_hash) in chunk {
        // ... process chunk
    }
    
    // Verify - if this fails, restore initial state
    if let Err(e) = self.verify(proof) {
        self.previous_leaf = initial_previous_leaf;
        self.frozen_nodes.clear();
        return Err(e);
    }
    
    // Write nodes ...
}
```

**Option 4: Recreate Receiver on Error**
In state sync, create a fresh `StateSnapshotRestore` instance after any `add_chunk()` failure to ensure clean in-memory state.

## Proof of Concept

The vulnerability can be demonstrated by:
1. Creating a `StateSnapshotRestore` instance
2. Calling `add_chunk()` with valid KV data but an invalid `SparseMerkleRangeProof`
3. Observing that KV data is committed but tree has no nodes
4. Calling `add_chunk()` again with the next chunk
5. Observing that the tree skips the failed range due to corrupted `previous_leaf`

The existing test suite does not cover this failure scenario: [8](#0-7) 

This test simulates interruption but always uses valid proofs, so it doesn't catch the in-memory state corruption issue.

## Notes

The vulnerability combines two issues:
1. Lack of atomic transaction coordination between parallel KV and tree writes
2. In-memory state corruption in `JellyfishMerkleRestore` when proof verification fails after updating `previous_leaf`

While the code has a `previous_key_hash()` method that returns the MIN of both progresses, this doesn't help because the tree's in-memory `previous_leaf` is corrupted and doesn't match its on-disk state. The recovery mechanism fails because the state sync reuses the corrupted instance rather than reconstructing from storage.

### Citations

**File:** storage/aptosdb/src/state_restore/mod.rs (L228-258)
```rust
    fn add_chunk(&mut self, chunk: Vec<(K, V)>, proof: SparseMerkleRangeProof) -> Result<()> {
        let kv_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_add_chunk"]);
            self.kv_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk(chunk.clone())
        };

        let tree_fn = || {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["jmt_add_chunk"]);
            self.tree_restore
                .lock()
                .as_mut()
                .unwrap()
                .add_chunk_impl(chunk.iter().map(|(k, v)| (k, v.hash())).collect(), proof)
        };
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => kv_fn()?,
            StateSnapshotRestoreMode::TreeOnly => tree_fn()?,
            StateSnapshotRestoreMode::Default => {
                // We run kv_fn with TreeOnly to restore the usage of DB
                let (r1, r2) = IO_POOL.join(kv_fn, tree_fn);
                r1?;
                r2?;
            },
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1244-1279)
```rust
    fn write_kv_batch(
        &self,
        version: Version,
        node_batch: &StateValueBatch,
        progress: StateSnapshotProgress,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_writer_write_chunk"]);
        let mut batch = SchemaBatch::new();
        let mut sharded_schema_batch = self.state_kv_db.new_sharded_native_batches();

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateSnapshotKvRestoreProgress(version),
            &DbMetadataValue::StateSnapshotProgress(progress),
        )?;

        if self.internal_indexer_db.is_some()
            && self
                .internal_indexer_db
                .as_ref()
                .unwrap()
                .statekeys_enabled()
        {
            let keys = node_batch.keys().map(|key| key.0.clone()).collect();
            self.internal_indexer_db
                .as_ref()
                .unwrap()
                .write_keys_to_indexer_db(&keys, version, progress)?;
        }
        self.shard_state_value_batch(
            &mut sharded_schema_batch,
            node_batch,
            self.state_kv_db.enabled_sharding(),
        )?;
        self.state_kv_db
            .commit(version, Some(batch), sharded_schema_batch)
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L177-208)
```rust
    pub(crate) fn commit(
        &self,
        version: Version,
        state_kv_metadata_batch: Option<SchemaBatch>,
        sharded_state_kv_batches: ShardedStateKvSchemaBatch,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit"]);
        {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_shards"]);
            THREAD_MANAGER.get_io_pool().scope(|s| {
                let mut batches = sharded_state_kv_batches.into_iter();
                for shard_id in 0..NUM_STATE_SHARDS {
                    let state_kv_batch = batches
                        .next()
                        .expect("Not sufficient number of sharded state kv batches");
                    s.spawn(move |_| {
                        // TODO(grao): Consider propagating the error instead of panic, if necessary.
                        self.commit_single_shard(version, shard_id, state_kv_batch)
                            .unwrap_or_else(|err| {
                                panic!("Failed to commit shard {shard_id}: {err}.")
                            });
                    });
                }
            });
        }
        if let Some(batch) = state_kv_metadata_batch {
            let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_db__commit_metadata"]);
            self.state_kv_metadata_db.write_schemas(batch)?;
        }

        self.write_progress(version)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L349-371)
```rust
        if let Some(prev_leaf) = &self.previous_leaf {
            let skip_until = chunk
                .iter()
                .find_position(|(key, _hash)| key.hash() > *prev_leaf.account_key());
            chunk = match skip_until {
                None => {
                    info!("Skipping entire chunk.");
                    return Ok(());
                },
                Some((0, _)) => chunk,
                Some((num_to_skip, next_leaf)) => {
                    info!(
                        num_to_skip = num_to_skip,
                        next_leaf = next_leaf,
                        "Skipping leaves."
                    );
                    chunk.split_off(num_to_skip)
                },
            }
        };
        if chunk.is_empty() {
            return Ok(());
        }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L373-413)
```rust
        for (key, value_hash) in chunk {
            let hashed_key = key.hash();
            if let Some(ref prev_leaf) = self.previous_leaf {
                ensure!(
                    &hashed_key > prev_leaf.account_key(),
                    "State keys must come in increasing order.",
                )
            }
            self.previous_leaf.replace(LeafNode::new(
                hashed_key,
                value_hash,
                (key.clone(), self.version),
            ));
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

        // Write the frozen nodes to storage.
        if self.async_commit {
            self.wait_for_async_commit()?;
            let (tx, rx) = channel();
            self.async_commit_result = Some(rx);

            let mut frozen_nodes = HashMap::new();
            std::mem::swap(&mut frozen_nodes, &mut self.frozen_nodes);
            let store = self.store.clone();

            IO_POOL.spawn(move || {
                let res = store.write_node_batch(&frozen_nodes);
                tx.send(res).unwrap();
            });
        } else {
            self.store.write_node_batch(&self.frozen_nodes)?;
            self.frozen_nodes.clear();
        }

        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L857-881)
```rust
        let mut state_snapshot_receiver = storage
            .writer
            .get_state_snapshot_receiver(version, expected_root_hash)
            .expect("Failed to initialize the state snapshot receiver!");

        // Handle state value chunks
        while let Some(storage_data_chunk) = state_snapshot_listener.next().await {
            // Start the snapshot timer for the state value chunk
            let _timer = metrics::start_timer(
                &metrics::STORAGE_SYNCHRONIZER_LATENCIES,
                metrics::STORAGE_SYNCHRONIZER_STATE_VALUE_CHUNK,
            );

            // Commit the state value chunk
            match storage_data_chunk {
                StorageDataChunk::States(notification_id, states_with_proof) => {
                    // Commit the state value chunk
                    let all_states_synced = states_with_proof.is_last_chunk();
                    let last_committed_state_index = states_with_proof.last_index;
                    let num_state_values = states_with_proof.raw_values.len();

                    let result = state_snapshot_receiver.add_chunk(
                        states_with_proof.raw_values,
                        states_with_proof.proof.clone(),
                    );
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L956-965)
```rust
                        Err(error) => {
                            let error =
                                format!("Failed to commit state value chunk! Error: {:?}", error);
                            send_storage_synchronizer_error(
                                error_notification_sender.clone(),
                                notification_id,
                                error,
                            )
                            .await;
                        },
```

**File:** storage/aptosdb/src/state_restore/restore_test.rs (L170-217)
```rust
    #[test]
    fn test_restore_with_interruption(
        (all, batch1_size, overlap_size) in arb_btree_map(2)
            .prop_flat_map(|btree| {
                let len = btree.len();
                (Just(btree), 1..len)
            })
            .prop_flat_map(|(btree, batch1_size)| {
                // n.b. overlap needs to be at least 1, because the last leaf is not frozen
                (Just(btree), Just(batch1_size), (1..=batch1_size))
            })
    ) {
        let (db, version) = init_mock_store(&all.clone().into_values().collect());
        let tree = JellyfishMerkleTree::new(&db);
        let expected_root_hash = tree.get_root_hash(version).unwrap();
        let batch1: Vec<_> = all.clone().into_iter().take(batch1_size).collect();

        let restore_db = Arc::new(MockSnapshotStore::default());
        {
            let mut restore =
                StateSnapshotRestore::new(&restore_db, &restore_db,  version, expected_root_hash, true /* async_commit */, StateSnapshotRestoreMode::Default).unwrap();
            let proof = tree
                .get_range_proof(batch1.last().map(|(key, _value)| *key).unwrap(), version)
                .unwrap();
            restore.add_chunk(batch1.into_iter().map(|(_, kv)| kv).collect(), proof).unwrap();
            // Do not call `finish`.
        }

        {
            let remaining_accounts: Vec<_> = all.clone().into_iter().skip(batch1_size - overlap_size).collect();

            let mut restore =
                StateSnapshotRestore::new(&restore_db, &restore_db,  version, expected_root_hash, true /* async commit */, StateSnapshotRestoreMode::Default ).unwrap();
            let proof = tree
                .get_range_proof(
                    remaining_accounts.last().map(|(h, _)| *h).unwrap(),
                    version,
                )
                .unwrap();
            restore.add_chunk(remaining_accounts.into_iter().
                map(|(_, kv)| kv)
                              .collect()
                              , proof).unwrap();
            restore.finish().unwrap();
        }

        assert_success(&restore_db, expected_root_hash, &all, version);
    }
```
