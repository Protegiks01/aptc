# Audit Report

## Title
Validator Discovery Service Panic on Missing ValidatorSet Violates Error Handling Contract

## Summary
The `ValidatorSetStream::extract_updates()` function uses `.expect()` to retrieve ValidatorSet from the on-chain config payload, causing validator nodes to panic and crash instead of gracefully handling errors. This violates the documented requirement that reconfig subscribers must handle missing configs and creates inconsistency with other discovery streams.

## Finding Description

The discovery service defines three stream implementations (ValidatorSetStream, FileStream, RestStream) that all implement the `Stream` trait with `type Item = Result<PeerSet, DiscoveryError>`. However, their error handling is inconsistent:

**ValidatorSetStream** panics on errors: [1](#0-0) 

The `.expect()` call will panic if ValidatorSet is missing or deserialization fails. Additionally, the result is unconditionally wrapped in `Ok()`: [2](#0-1) 

In contrast, **FileStream** properly returns errors: [3](#0-2) 

**RestStream** also properly returns errors: [4](#0-3) 

The `DiscoveryError` enum exists specifically for error propagation: [5](#0-4) 

The error handling contract expects errors to be logged but not crash the node: [6](#0-5) 

Most critically, the reconfiguration notification system explicitly documents that subscribers must handle missing configs: [7](#0-6) 

The `OnChainConfigProvider::get()` can fail when configs are missing: [8](#0-7) 

## Impact Explanation

**Severity: Medium**

This issue causes validator node crashes in edge cases:

1. **State Sync Race Conditions**: During fast-sync or state catchup, a reconfiguration event may arrive before the ValidatorSet state is fully synced to the database
2. **Database Read Errors**: Temporary I/O errors or corruption could cause ValidatorSet retrieval to fail
3. **Deserialization Failures**: Version mismatches or corrupted bytes could cause deserialization to fail

When triggered, the validator node panics and crashes, requiring restart. This affects:
- **Validator Availability**: Crashed validators cannot participate in consensus
- **Network Liveness**: Multiple simultaneous crashes could impact consensus if enough validators are affected
- **Inconsistent Behavior**: FileStream and RestStream handle similar errors gracefully

This qualifies as **Medium severity** per the bug bounty criteria: "State inconsistencies requiring intervention" and potential for "Validator node slowdowns" (crashes require manual intervention).

## Likelihood Explanation

**Likelihood: Low to Medium**

The vulnerability requires specific timing conditions:
- Race between reconfig event processing and state synchronization
- Database corruption or read failures
- Version incompatibilities during upgrades

While these are edge cases under normal operation, they become more likely during:
- Network partitions or sync issues
- State sync from genesis or snapshots
- Software upgrades with config format changes
- Hardware failures affecting database reads

The documented requirement that subscribers "must be able to handle" missing configs suggests the developers anticipated these scenarios, making the panic behavior unintentional.

## Recommendation

Modify `extract_updates` to return `Result<PeerSet, DiscoveryError>` and properly handle errors:

```rust
fn extract_updates(&mut self, payload: OnChainConfigPayload<P>) -> Result<PeerSet, DiscoveryError> {
    let _process_timer = EVENT_PROCESSING_LOOP_BUSY_DURATION_S.start_timer();

    let node_set: ValidatorSet = payload
        .get()
        .map_err(|e| DiscoveryError::Parsing(format!("Failed to get ValidatorSet: {}", e)))?;

    let peer_set = extract_validator_set_updates(self.network_context, node_set);
    
    self.find_key_mismatches(
        peer_set
            .get(&self.network_context.peer_id())
            .map(|peer| &peer.keys),
    );

    inc_by_with_context(
        &DISCOVERY_COUNTS,
        &self.network_context,
        "new_nodes",
        peer_set.len() as u64,
    );

    Ok(peer_set)
}
```

Update `poll_next` to propagate the error:

```rust
fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
    Pin::new(&mut self.reconfig_events)
        .poll_next(cx)
        .map(|maybe_notification| {
            maybe_notification
                .map(|notification| self.extract_updates(notification.on_chain_configs))
        })
}
```

This makes ValidatorSetStream consistent with FileStream and RestStream, allowing the discovery system to log errors and continue operating instead of crashing.

## Proof of Concept

This vulnerability can be triggered by simulating a missing ValidatorSet in the test environment:

```rust
#[tokio::test]
async fn test_missing_validator_set_causes_panic() {
    use aptos_channels::aptos_channel;
    use aptos_config::network_id::NetworkContext;
    use aptos_crypto::x25519::PrivateKey;
    use aptos_event_notifications::{ReconfigNotification, ReconfigNotificationListener};
    use aptos_types::on_chain_config::InMemoryOnChainConfig;
    use std::collections::HashMap;

    let (mut reconfig_sender, reconfig_events) = aptos_channel::new(QueueStyle::LIFO, 1, None);
    let reconfig_listener = ReconfigNotificationListener {
        notification_receiver: reconfig_events,
    };
    
    let private_key = PrivateKey::generate(&mut rand::thread_rng());
    let pubkey = private_key.public_key();
    
    let mut stream = ValidatorSetStream::new(
        NetworkContext::mock(),
        pubkey,
        reconfig_listener,
    );

    // Create payload WITHOUT ValidatorSet
    let empty_configs = HashMap::new();
    let payload = OnChainConfigPayload::new(1, InMemoryOnChainConfig::new(empty_configs));
    
    reconfig_sender
        .push((), ReconfigNotification {
            version: 1,
            on_chain_configs: payload,
        })
        .unwrap();

    // This will panic with "failed to get ValidatorSet from payload"
    let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
        futures::executor::block_on(async {
            use futures::StreamExt;
            stream.next().await
        })
    }));

    assert!(result.is_err(), "Expected panic but operation succeeded");
}
```

The test demonstrates that when ValidatorSet is missing from the payload (as can happen during state sync race conditions), the node panics instead of returning `Err(DiscoveryError)`.

## Notes

This finding reveals a defensive programming violation rather than a direct security exploit. The documented contract explicitly requires subscribers to handle missing configs gracefully, but the implementation uses `.expect()` which violates this requirement. The inconsistency with FileStream and RestStream error handling patterns suggests this was unintentional rather than a deliberate design choice.

### Citations

**File:** network/discovery/src/validator_set.rs (L68-91)
```rust
    fn extract_updates(&mut self, payload: OnChainConfigPayload<P>) -> PeerSet {
        let _process_timer = EVENT_PROCESSING_LOOP_BUSY_DURATION_S.start_timer();

        let node_set: ValidatorSet = payload
            .get()
            .expect("failed to get ValidatorSet from payload");

        let peer_set = extract_validator_set_updates(self.network_context, node_set);
        // Ensure that the public key matches what's onchain for this peer
        self.find_key_mismatches(
            peer_set
                .get(&self.network_context.peer_id())
                .map(|peer| &peer.keys),
        );

        inc_by_with_context(
            &DISCOVERY_COUNTS,
            &self.network_context,
            "new_nodes",
            peer_set.len() as u64,
        );

        peer_set
    }
```

**File:** network/discovery/src/validator_set.rs (L97-104)
```rust
    fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        Pin::new(&mut self.reconfig_events)
            .poll_next(cx)
            .map(|maybe_notification| {
                maybe_notification
                    .map(|notification| Ok(self.extract_updates(notification.on_chain_configs)))
            })
    }
```

**File:** network/discovery/src/file.rs (L38-46)
```rust
    fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        // Wait for delay, or add the delay for next call
        futures::ready!(self.interval.as_mut().poll_next(cx));

        Poll::Ready(Some(match load_file(self.file_path.as_path()) {
            Ok(peers) => Ok(peers),
            Err(error) => Err(error),
        }))
    }
```

**File:** network/discovery/src/rest.rs (L42-68)
```rust
    fn poll_next(mut self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        // Wait for delay, or add the delay for next call
        futures::ready!(self.interval.as_mut().poll_next(cx));

        // Retrieve the onchain resource at the interval
        // TODO there should be a better way than converting this to a blocking call
        let response = block_on(self.rest_client.get_account_resource_bcs::<ValidatorSet>(
            AccountAddress::ONE,
            "0x1::stake::ValidatorSet",
        ));
        Poll::Ready(match response {
            Ok(inner) => {
                let validator_set = inner.into_inner();
                Some(Ok(extract_validator_set_updates(
                    self.network_context,
                    validator_set,
                )))
            },
            Err(err) => {
                info!(
                    "Failed to retrieve validator set by REST discovery {:?}",
                    err
                );
                Some(Err(DiscoveryError::Rest(err)))
            },
        })
    }
```

**File:** network/discovery/src/lib.rs (L33-38)
```rust
#[derive(Debug)]
pub enum DiscoveryError {
    IO(std::io::Error),
    Parsing(String),
    Rest(aptos_rest_client::error::RestError),
}
```

**File:** network/discovery/src/lib.rs (L141-166)
```rust
        while let Some(update) = source_stream.next().await {
            if let Ok(update) = update {
                trace!(
                    NetworkSchema::new(&network_context),
                    "{} Sending update: {:?}",
                    network_context,
                    update
                );
                let request = ConnectivityRequest::UpdateDiscoveredPeers(discovery_source, update);
                if let Err(error) = update_channel.try_send(request) {
                    inc_by_with_context(&DISCOVERY_COUNTS, &network_context, "send_failure", 1);
                    warn!(
                        NetworkSchema::new(&network_context),
                        "{} Failed to send update {:?}", network_context, error
                    );
                }
            } else {
                warn!(
                    NetworkSchema::new(&network_context),
                    "{} {} Discovery update failed {:?}",
                    &network_context,
                    discovery_source,
                    update
                );
            }
        }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L277-284)
```rust
    /// Fetches the configs on-chain at the specified version.
    /// Note: We cannot assume that all configs will exist on-chain. As such, we
    /// must fetch each resource one at a time. Reconfig subscribers must be able
    /// to handle on-chain configs not existing in a reconfiguration notification.
    fn read_on_chain_configs(
        &self,
        version: Version,
    ) -> Result<OnChainConfigPayload<DbBackedOnChainConfig>, Error> {
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L397-412)
```rust
impl OnChainConfigProvider for DbBackedOnChainConfig {
    fn get<T: OnChainConfig>(&self) -> Result<T> {
        let bytes = self
            .reader
            .get_state_value_by_version(&StateKey::on_chain_config::<T>()?, self.version)?
            .ok_or_else(|| {
                anyhow!(
                    "no config {} found in aptos root account state",
                    T::CONFIG_ID
                )
            })?
            .bytes()
            .clone();

        T::deserialize_into_config(&bytes)
    }
```
