# Audit Report

## Title
Storage Format Mismatch Vulnerability in LocalFileStoreOperator Allows Data Corruption and Service Crashes

## Summary
The `LocalFileStoreOperator` in the Aptos indexer-grpc system fails to validate that its initialized `storage_format` matches the `storage_format` stored in the on-disk metadata. This allows operators to be restarted with a different compression setting, causing file path mismatches, deserialization failures, and data unavailability.

## Finding Description

The `LocalFileStoreOperator` stores its `storage_format` as an immutable field set during initialization based on the `enable_compression` configuration parameter. [1](#0-0) 

This storage format determines two critical aspects of file operations:

1. **File path patterns**: Different formats use different directory structures and file extensions [2](#0-1) 

2. **Encoding/decoding methods**: Files are encoded and decoded differently based on the format [3](#0-2) 

The vulnerability exists because `update_file_store_metadata_with_timeout()` only validates the `chain_id` but **NOT** the `storage_format`: [4](#0-3) 

When reading transactions, the operator uses its own `self.storage_format()` to construct file paths and decode data: [5](#0-4) 

**Attack Scenario:**

1. Operator A runs with `enable_compression: true` â†’ stores files in `compressed_files/lz4/{hash}_{version}.bin` with Lz4 compression
2. Metadata is written with `storage_format: "Lz4CompressedProto"`
3. Administrator changes configuration to `enable_compression: false` and restarts
4. Operator B initializes with `storage_format: JsonBase64UncompressedProto`
5. Operator B passes validation (only checks chain_id)
6. When reading files:
   - Operator B looks for `files/{version}.json`
   - But files are actually at `compressed_files/lz4/...`
   - Result: File not found errors OR if wrong file is read, deserialization panics with "Lz4 decompression failed" or "json deserialization failed"

Notably, the **GCS operator implementation correctly validates storage format**, demonstrating this is a known requirement that was implemented for GCS but missed for Local: [6](#0-5) 

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program ("API crashes, Significant protocol violations"):

1. **Service Crashes**: Deserialization failures cause panics in the transaction decoding thread, crashing the indexer service
2. **Data Unavailability**: Historical transaction data becomes inaccessible to all downstream consumers (wallets, explorers, analytics)
3. **Cascading Failures**: Dependent services that rely on the indexer API will experience failures
4. **Data Corruption Risk**: If the operator writes new files in a different format while old files exist in another format, the storage becomes inconsistent with metadata pointing to the wrong format

While this doesn't affect blockchain consensus directly, the indexer-grpc is critical infrastructure that provides transaction data to the entire Aptos ecosystem.

## Likelihood Explanation

**High Likelihood** - This can occur through:

1. **Configuration Error**: Administrator accidentally changes `enable_compression` setting
2. **Migration**: Intentional format migration without proper validation
3. **Configuration Management**: Using different configs across environments (dev/staging/prod)
4. **Rollback**: Reverting to an older configuration after files were written in a new format

No malicious intent is required - this is a configuration consistency bug that will inevitably occur in production deployments.

## Recommendation

Add storage format validation to `LocalFileStoreOperator::update_file_store_metadata_with_timeout()` matching the GCS implementation:

```rust
async fn update_file_store_metadata_with_timeout(
    &mut self,
    expected_chain_id: u64,
    _version: u64,
) -> anyhow::Result<()> {
    let metadata_path = self.path.join(METADATA_FILE_NAME);
    match tokio::fs::read(metadata_path).await {
        Ok(metadata) => {
            let metadata: FileStoreMetadata =
                serde_json::from_slice(&metadata).expect("Expected metadata to be valid JSON.");
            anyhow::ensure!(metadata.chain_id == expected_chain_id, "Chain ID mismatch.");
            // ADD THIS VALIDATION:
            anyhow::ensure!(
                metadata.storage_format == self.storage_format,
                "Storage format mismatch: metadata has {:?} but operator initialized with {:?}",
                metadata.storage_format,
                self.storage_format
            );
            Ok(())
        },
        // ... rest of the function
    }
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod storage_format_mismatch_test {
    use super::*;
    use aptos_protos::transaction::v1::Transaction;
    use tempfile::TempDir;
    
    #[tokio::test]
    async fn test_storage_format_mismatch_vulnerability() {
        let temp_dir = TempDir::new().unwrap();
        let path = temp_dir.path().to_path_buf();
        
        // Step 1: Create operator with compression enabled
        let mut operator_compressed = LocalFileStoreOperator::new(path.clone(), true);
        assert_eq!(operator_compressed.storage_format(), StorageFormat::Lz4CompressedProto);
        
        // Step 2: Write metadata and files with Lz4 format
        operator_compressed
            .update_file_store_metadata_with_timeout(1, 0)
            .await
            .unwrap();
        
        let transactions: Vec<Transaction> = (0..1000)
            .map(|version| Transaction {
                version,
                ..Transaction::default()
            })
            .collect();
        
        operator_compressed
            .upload_transaction_batch(1, transactions)
            .await
            .unwrap();
        
        // Step 3: Create new operator with compression DISABLED (format mismatch!)
        let mut operator_uncompressed = LocalFileStoreOperator::new(path.clone(), false);
        assert_eq!(operator_uncompressed.storage_format(), StorageFormat::JsonBase64UncompressedProto);
        
        // Step 4: Vulnerability - validation passes even though formats don't match!
        let validation_result = operator_uncompressed
            .update_file_store_metadata_with_timeout(1, 0)
            .await;
        
        // This SHOULD fail but currently passes!
        assert!(validation_result.is_ok(), "Validation incorrectly passed with mismatched storage format!");
        
        // Step 5: Demonstrate the corruption - reading files will fail
        let read_result = operator_uncompressed.get_transactions(0, 3).await;
        
        // This will fail because operator looks for files/0.json but file is at compressed_files/lz4/...
        assert!(read_result.is_err(), "Read should fail due to file path mismatch");
        
        println!("VULNERABILITY CONFIRMED: Storage format mismatch not detected!");
    }
}
```

## Notes

This vulnerability exists specifically in the `LocalFileStoreOperator` implementation. The `GcsFileStoreOperator` correctly implements the storage format validation, indicating this is a known requirement that was overlooked in the local implementation. The fix should be backported from the GCS implementation to ensure consistency across all file store operators.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L24-35)
```rust
    pub fn new(path: PathBuf, enable_compression: bool) -> Self {
        let storage_format = if enable_compression {
            StorageFormat::Lz4CompressedProto
        } else {
            StorageFormat::JsonBase64UncompressedProto
        };
        Self {
            path,
            latest_metadata_update_timestamp: None,
            storage_format,
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L94-124)
```rust
    async fn update_file_store_metadata_with_timeout(
        &mut self,
        expected_chain_id: u64,
        _version: u64,
    ) -> anyhow::Result<()> {
        let metadata_path = self.path.join(METADATA_FILE_NAME);
        match tokio::fs::read(metadata_path).await {
            Ok(metadata) => {
                let metadata: FileStoreMetadata =
                    serde_json::from_slice(&metadata).expect("Expected metadata to be valid JSON.");
                anyhow::ensure!(metadata.chain_id == expected_chain_id, "Chain ID mismatch.");
                Ok(())
            },
            Err(err) => {
                if err.kind() == std::io::ErrorKind::NotFound {
                    // If the metadata is not found, it means the file store is empty.
                    info!("File store is empty. Creating metadata file.");
                    self.update_file_store_metadata_internal(expected_chain_id, 0)
                        .await
                        .expect("[Indexer File] Update metadata failed.");
                    Ok(())
                } else {
                    // If not in write mode, the metadata must exist.
                    Err(anyhow::Error::msg(format!(
                        "Metadata not found or file store operator is not in write mode. {}",
                        err
                    )))
                }
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L240-260)
```rust
    pub fn build_key(version: u64, storage_format: StorageFormat) -> String {
        let starting_version =
            version / FILE_ENTRY_TRANSACTION_COUNT * FILE_ENTRY_TRANSACTION_COUNT;
        let mut hasher = Ripemd128::new();
        hasher.update(starting_version.to_string());
        let file_prefix = format!("{:x}", hasher.finalize());
        match storage_format {
            StorageFormat::Lz4CompressedProto => {
                format!(
                    "compressed_files/lz4/{}_{}.bin",
                    file_prefix, starting_version
                )
            },
            StorageFormat::JsonBase64UncompressedProto => {
                format!("files/{}.json", starting_version)
            },
            StorageFormat::Base64UncompressedProto => {
                panic!("Base64UncompressedProto is not supported.")
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L262-292)
```rust
    pub fn into_transactions_in_storage(self) -> TransactionsInStorage {
        match self {
            FileEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                TransactionsInStorage::decode(decompressed.as_slice())
                    .expect("proto deserialization failed.")
            },
            FileEntry::JsonBase64UncompressedProto(bytes) => {
                let file: TransactionsLegacyFile =
                    serde_json::from_slice(bytes.as_slice()).expect("json deserialization failed.");
                let transactions = file
                    .transactions_in_base64
                    .into_iter()
                    .map(|base64| {
                        let bytes: Vec<u8> =
                            base64::decode(base64).expect("base64 decoding failed.");
                        Transaction::decode(bytes.as_slice())
                            .expect("proto deserialization failed.")
                    })
                    .collect::<Vec<Transaction>>();
                TransactionsInStorage {
                    starting_version: Some(file.starting_version),
                    transactions,
                }
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/mod.rs (L59-86)
```rust
    async fn get_transactions_with_durations(
        &self,
        version: u64,
        retries: u8,
    ) -> Result<(Vec<Transaction>, f64, f64)> {
        let io_start_time = std::time::Instant::now();
        let bytes = self.get_raw_file_with_retries(version, retries).await?;
        let io_duration = io_start_time.elapsed().as_secs_f64();
        let decoding_start_time = std::time::Instant::now();
        let storage_format = self.storage_format();

        let transactions_in_storage = tokio::task::spawn_blocking(move || {
            FileEntry::new(bytes, storage_format).into_transactions_in_storage()
        })
        .await
        .context("Converting storage bytes to FileEntry transactions thread panicked")?;

        let decoding_duration = decoding_start_time.elapsed().as_secs_f64();
        Ok((
            transactions_in_storage
                .transactions
                .into_iter()
                .skip((version % FILE_ENTRY_TRANSACTION_COUNT) as usize)
                .collect(),
            io_duration,
            decoding_duration,
        ))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/gcs.rs (L162-182)
```rust
    async fn update_file_store_metadata_with_timeout(
        &mut self,
        expected_chain_id: u64,
        version: u64,
    ) -> anyhow::Result<()> {
        if let Some(metadata) = self.get_file_store_metadata().await {
            assert_eq!(metadata.chain_id, expected_chain_id, "Chain ID mismatch.");
            assert_eq!(
                metadata.storage_format, self.storage_format,
                "Storage format mismatch."
            );
        }
        if self.file_store_metadata_last_updated.elapsed().as_millis()
            < FILE_STORE_METADATA_TIMEOUT_MILLIS
        {
            bail!("File store metadata is updated too frequently.")
        }
        self.update_file_store_metadata_internal(expected_chain_id, version)
            .await?;
        Ok(())
    }
```
