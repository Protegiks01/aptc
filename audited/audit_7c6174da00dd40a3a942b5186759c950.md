# Audit Report

## Title
Randomness Confusion Vulnerability After Block Reorganization Due to Uncancelled Aggregation Tasks

## Summary
During a block reorganization, the `block_queue` is cleared and `rand_store` entries are removed, but ongoing randomness aggregation tasks continue executing in background threads. When these tasks complete, they deliver randomness decisions based on round numbers only (without block IDs), which can be incorrectly applied to new blocks at the same rounds after the reorg, breaking the fundamental randomness unpredictability guarantee.

## Finding Description

The vulnerability exists in the interaction between three components during block reorganization:

1. **Randomness is keyed by (epoch, round) only**: The `Randomness` struct contains only `RandMetadata` (epoch + round) without any block identifier. [1](#0-0) 

2. **Aggregation tasks spawn independently**: When randomness shares reach threshold, `ShareAggregator::try_aggregate` spawns a blocking task using `tokio::task::spawn_blocking` to compute the aggregated randomness. [2](#0-1) 

3. **Reset clears state but not spawned tasks**: When `process_reset` is called during reorganization, it creates a new empty `BlockQueue` and clears `rand_store` entries >= target_round. [3](#0-2) 

4. **Randomness lookup uses round only**: The `process_randomness` method looks up blocks by round number only via `block_queue.item_mut(randomness.round())`, with no validation that the randomness was generated for the specific block. [4](#0-3) 

5. **No validation in set_randomness**: The `PipelinedBlock::set_randomness` method simply stores the randomness value without any validation. [5](#0-4) 

**Attack Scenario:**
1. Validator processes Block A at round 100 (block_id = 0xAAA)
2. Randomness aggregation task spawns for (epoch, round=100)
3. Block reorganization occurs via state sync: `reset(100)` is called [6](#0-5) 
4. Block B arrives at round 100 (block_id = 0xBBB, different content)
5. The old aggregation task completes and sends `Randomness{epoch, round=100}` to `decision_rx`
6. `process_randomness` receives it and applies it to Block B via `block_queue.item_mut(100)`
7. Block B now has randomness that was generated for Block A

This breaks the **Cryptographic Correctness** invariant: randomness generated for one block's context is applied to a completely different block.

## Impact Explanation

**High Severity** - Significant Protocol Violation

This vulnerability breaks the fundamental security property of the randomness beacon:
- **Unpredictability**: An attacker who can influence reorganization timing may predict randomness values for new blocks
- **Consensus Safety**: Different validators may compute different randomness if they experience reorgs at different times
- **Leader Selection**: If randomness is used for leader election, this could enable manipulation
- **On-Chain Applications**: Any Move contracts relying on on-chain randomness would receive predictable values

While not immediately exploitable for direct fund theft, this represents a significant protocol-level vulnerability that undermines the security guarantees of the randomness system, fitting the "Significant protocol violations" category for High Severity.

## Likelihood Explanation

**Medium-to-High Likelihood**

This can occur during:
- **State sync operations**: When validators fall behind and sync to the canonical chain
- **Network partitions**: Temporary forks that get resolved via reorganization  
- **Epoch boundaries**: When validators transition between epochs

The vulnerability requires:
1. A block reorganization to occur (common during state sync)
2. Randomness aggregation task to complete after the reset but before new blocks are fully processed (timing-dependent race condition)
3. New blocks to arrive at the same round numbers as old blocks (typical in state sync scenarios)

While requiring specific timing, these conditions occur naturally during normal network operations, especially during validator catch-up scenarios.

## Recommendation

**Fix 1: Include block_id in Randomness validation**

Modify `Randomness` to include `block_id` and validate it in `process_randomness`:

```rust
// In process_randomness
fn process_randomness(&mut self, randomness: Randomness) {
    if let Some(item) = self.block_queue.item_mut(randomness.round()) {
        // Validate block_id matches before setting
        let block = &item.blocks()[item.offset(randomness.round())];
        if block.id() == randomness.block_id() {
            item.set_randomness(randomness.round(), randomness);
        } else {
            warn!("Randomness block_id mismatch, ignoring stale randomness");
        }
    }
}
```

**Fix 2: Cancel aggregation tasks during reset**

Store `AbortHandle` for aggregation tasks and cancel them during reset:

```rust
// In rand_store.rs, store abort handles
pub struct RandStore<S> {
    // ... existing fields
    aggregation_handles: HashMap<Round, AbortHandle>,
}

// Cancel tasks during reset
pub fn reset(&mut self, round: u64) {
    self.update_highest_known_round(round);
    let removed_entries = self.rand_map.split_off(&round);
    // Cancel all aggregation tasks for removed rounds
    for round in removed_entries.keys() {
        if let Some(handle) = self.aggregation_handles.remove(round) {
            handle.abort();
        }
    }
    // ... existing code
}
```

**Recommended Approach**: Implement both fixes for defense in depth.

## Proof of Concept

```rust
#[tokio::test]
async fn test_randomness_confusion_after_reorg() {
    // Setup: Create RandManager with test configuration
    let (block_tx, block_rx) = unbounded();
    let (reset_tx, reset_rx) = unbounded();
    let rand_manager = /* initialize RandManager */;
    
    // Step 1: Send Block A at round 100
    let block_a = create_test_block(100, HashValue::from_u64(0xAAA));
    let blocks_a = OrderedBlocks {
        ordered_blocks: vec![block_a.clone()],
        ordered_proof: create_test_proof(100),
    };
    block_tx.unbounded_send(blocks_a).unwrap();
    
    // Step 2: Simulate shares being aggregated (trigger spawn_blocking)
    // Allow time for aggregation to start
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Step 3: Trigger reset (simulating state sync/reorg)
    let (ack_tx, ack_rx) = oneshot::channel();
    reset_tx.unbounded_send(ResetRequest {
        tx: ack_tx,
        signal: ResetSignal::TargetRound(100),
    }).unwrap();
    ack_rx.await.unwrap();
    
    // Step 4: Send Block B at same round 100 (different block_id)
    let block_b = create_test_block(100, HashValue::from_u64(0xBBB));
    let blocks_b = OrderedBlocks {
        ordered_blocks: vec![block_b.clone()],
        ordered_proof: create_test_proof(100),
    };
    block_tx.unbounded_send(blocks_b).unwrap();
    
    // Step 5: Wait for old aggregation task to complete
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    // Step 6: Verify Block B received randomness (it shouldn't have!)
    // If Block B has randomness, the vulnerability is confirmed
    assert!(block_b.randomness.get().is_none(), 
            "Block B should not have randomness from Block A!");
}
```

## Notes

This vulnerability specifically affects the randomness generation subsystem during reorganization events. The root cause is the architectural decision to key randomness by (epoch, round) rather than by block identity, combined with the asynchronous nature of aggregation tasks that aren't lifecycle-managed relative to the block queue state. Similar issues may exist in the secret sharing manager (`consensus/src/rand/secret_sharing/`), which uses an identical pattern.

### Citations

**File:** types/src/randomness.rs (L23-27)
```rust
#[derive(Clone, Serialize, Deserialize, Debug, Default, PartialEq, Eq, Hash)]
pub struct RandMetadata {
    pub epoch: u64,
    pub round: Round,
}
```

**File:** consensus/src/rand/rand_gen/rand_store.rs (L69-87)
```rust
        tokio::task::spawn_blocking(move || {
            let maybe_randomness = S::aggregate(
                self.shares.values(),
                &rand_config,
                rand_metadata.metadata.clone(),
            );
            match maybe_randomness {
                Ok(randomness) => {
                    let _ = decision_tx.unbounded_send(randomness);
                },
                Err(e) => {
                    warn!(
                        epoch = rand_metadata.metadata.epoch,
                        round = rand_metadata.metadata.round,
                        "Aggregation error: {e}"
                    );
                },
            }
        });
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L184-194)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.rand_store.lock().reset(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L196-206)
```rust
    fn process_randomness(&mut self, randomness: Randomness) {
        let rand = hex::encode(randomness.randomness());
        info!(
            metadata = randomness.metadata(),
            rand = rand,
            "Processing decisioned randomness."
        );
        if let Some(block) = self.block_queue.item_mut(randomness.round()) {
            block.set_randomness(randomness.round(), randomness);
        }
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L332-334)
```rust
    pub fn set_randomness(&self, randomness: Randomness) {
        assert!(self.randomness.set(randomness.clone()).is_ok());
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```
