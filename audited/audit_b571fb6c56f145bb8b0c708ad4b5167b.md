# Audit Report

## Title
Block Partitioner Breaks Sender Transaction Ordering Leading to Sequence Number Violations

## Summary
The ConnectedComponentPartitioner can separate transactions from the same sender into different conflicting sets when early transactions have no writes, causing PrePartitionedTxnIdx reordering that violates sequence number requirements. This allows later transactions to be accepted while earlier ones are discarded, breaking Aptos's transaction ordering invariant and causing execution failures.

## Finding Description

The vulnerability exists in the interaction between the pre-partitioning phase and the cross-shard dependency removal phase of the block partitioner.

**Root Cause in Pre-Partitioning:**

In the ConnectedComponentPartitioner, the union-find algorithm only considers write operations to group conflicting transactions: [1](#0-0) 

This creates a critical flaw: if a sender's first transaction has no writes (read-only or non-storage transaction), it won't union the sender with any keys. When subsequent transactions from the same sender do have writes, they union the sender with those keys, but the earlier transaction remains in a separate set.

**How Sets Are Assigned:** [2](#0-1) 

Each transaction is assigned to a set based on `uf.find(sender_idx)`. Since the union-find state changes after write operations, transactions from the same sender can end up in different sets.

**Reordering During Materialization:**

When groups are materialized and assigned to shards, the shard iteration order determines PrePartitionedTxnIdx values: [3](#0-2) 

This assigns sequential PrePartitionedTxnIdx values based on shard order, NOT original transaction order.

**Incorrect Ordering Check:**

The discarding logic uses PrePartitionedTxnIdx to preserve sender ordering: [4](#0-3) [5](#0-4) 

The check `if txn_idx < min_discarded` compares PrePartitionedTxnIdx values, assuming they respect original ordering. This assumption is violated when transactions are split across sets.

**Attack Scenario:**

1. Sender A submits:
   - txn-0 (seq=0): Read-only transaction with NO write_hints
   - txn-1 (seq=1): Transaction that writes to key Y

2. Union-find groups:
   - txn-0: No writes → sender A remains in singleton set
   - txn-1: Writes to Y → unions A with Y, creating {A, Y}

3. Set assignment:
   - set-0 contains txn-0 (uf.find(A) before union)
   - set-1 contains txn-1 (uf.find(A) after union with Y)

4. Load balancing assigns:
   - set-0 → shard-1
   - set-1 → shard-0

5. PrePartitionedTxnIdx assignment (shard-0 processed first):
   - txn-1: PrePartitionedTxnIdx = 0
   - txn-0: PrePartitionedTxnIdx = 1

6. During discarding round:
   - txn-0 (PrePartitionedTxnIdx=1) has cross-shard conflict → discarded
   - min_discard_table[A] = 1
   - txn-1 (PrePartitionedTxnIdx=0) is checked: `if 0 < 1` → TRUE → ACCEPTED

7. Result: txn-1 (seq=1) is accepted while txn-0 (seq=0) is discarded, violating sequence number ordering.

**Execution Failure:**

At execution time, transaction validation enforces strict sequence number ordering: [6](#0-5) 

When txn-1 executes without txn-0, the account's sequence number will be 0, but txn-1 expects sequence number 1, causing `PROLOGUE_ESEQUENCE_NUMBER_TOO_NEW` error.

## Impact Explanation

**Severity: Critical**

This vulnerability breaks multiple critical invariants:

1. **Transaction Validation Invariant Violation**: The prologue enforces that transaction sequence numbers must exactly match the account's current sequence number. This partitioner bug allows transactions to be ordered incorrectly, causing validation failures.

2. **Deterministic Execution Violation**: If different validators partition the same block differently (even with slight timing variations affecting union-find), they could accept different transaction subsets, leading to different state roots and consensus failure.

3. **Transaction Failures**: Users submitting mixed read-only and write transactions will experience unexpected transaction failures, as their transactions get reordered and rejected during execution.

4. **Potential Consensus Issues**: While the partitioner is designed to be deterministic, any non-determinism in the union-find path compression or race conditions could cause different validators to produce different partitioning results, leading to state divergence.

This meets **Critical Severity** criteria as it directly impacts consensus safety and can cause transaction execution failures affecting all users.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability triggers when:
1. A sender submits multiple transactions in a block
2. Earlier transactions have empty write_hints (read-only queries, balance checks, view functions)
3. Later transactions have writes
4. The transactions end up in different shards during load balancing

Read-only transactions are common in Aptos:
- Balance queries
- NFT metadata reads  
- Smart contract view function calls
- Multi-signature approval checks

The existing test suite doesn't catch this because all test transactions (P2P transfers) have writes: [7](#0-6) 

This test only creates P2P transactions which always write to sender and receiver coin stores, missing the read-only transaction case.

## Recommendation

**Solution 1: Force Same-Sender Grouping in Union-Find**

Modify the union-find to explicitly group all transactions from the same sender, regardless of writes:

```rust
// In connected_component/mod.rs, after line 56:
for txn_idx in 0..state.num_txns() {
    let sender_idx = state.sender_idx(txn_idx);
    let write_set = state.write_sets[txn_idx].read().unwrap();
    for &key_idx in write_set.iter() {
        let key_idx_in_uf = num_senders + key_idx;
        uf.union(key_idx_in_uf, sender_idx);
    }
    
    // NEW: If transaction has no writes, union sender with a sentinel
    // to ensure all transactions from same sender stay in same set
    if write_set.is_empty() {
        // Union with sender itself ensures it's tracked
        // This is a no-op but ensures find() returns consistent value
        uf.union(sender_idx, sender_idx);
    }
}
```

**Solution 2: Use OriginalTxnIdx for Ordering Checks (Preferred)**

Modify the discarding logic to track and compare OriginalTxnIdx instead of PrePartitionedTxnIdx: [4](#0-3) 

Change to:
```rust
// Track minimum discarded OriginalTxnIdx per sender instead
let min_discard_table: DashMap<SenderIdx, AtomicUsize> = 
    DashMap::with_shard_amount(state.dashmap_num_shards);
```

And update the comparison logic (line 128-134):
```rust
if in_round_conflict_detected {
    let sender = state.sender_idx(ori_txn_idx);
    min_discard_table
        .entry(sender)
        .or_insert_with(|| AtomicUsize::new(usize::MAX))
        .fetch_min(ori_txn_idx, Ordering::SeqCst);  // Use ori_txn_idx not txn_idx
    discarded[shard_id].write().unwrap().push(txn_idx);
}
```

And update the acceptance check (line 155-166):
```rust
let min_discarded = min_discard_table
    .get(&sender_idx)
    .map(|kv| kv.load(Ordering::SeqCst))
    .unwrap_or(usize::MAX);
if ori_txn_idx < min_discarded {  // Compare OriginalTxnIdx
    state.update_trackers_on_accepting(txn_idx, round_id, shard_id);
    finally_accepted[shard_id].write().unwrap().push(txn_idx);
} else {
    discarded[shard_id].write().unwrap().push(txn_idx);
}
```

## Proof of Concept

```rust
#[test]
fn test_sender_ordering_with_read_only_transactions() {
    use crate::test_utils::{generate_test_account, verify_partitioner_output};
    use crate::v2::config::PartitionerV2Config;
    use aptos_types::transaction::{
        analyzed_transaction::AnalyzedTransaction,
        analyzed_transaction::StorageLocation,
        signature_verified_transaction::SignatureVerifiedTransaction,
        Transaction,
    };
    
    let mut sender = generate_test_account();
    
    // Create a read-only transaction (no writes)
    let read_only_txn = create_read_only_transaction(&mut sender);
    
    // Create a write transaction
    let write_txn = create_write_transaction(&mut sender);
    
    let transactions = vec![
        read_only_txn,  // seq=0, no writes
        write_txn,      // seq=1, has writes
    ];
    
    let partitioner = PartitionerV2Config::default().build();
    let partitioned = partitioner.partition(transactions.clone(), 2);
    
    // Flatten and check sequence numbers
    let (sub_blocks, _) = partitioned.into();
    let flat = SubBlocksForShard::flatten(sub_blocks);
    
    let mut prev_seq = None;
    for txn_with_deps in flat.iter() {
        let (sender_addr, seq_num) = get_account_seq_number(
            txn_with_deps.transaction().expect_valid()
        );
        if sender_addr == sender.account_address {
            if let Some(prev) = prev_seq {
                assert!(seq_num > prev, 
                    "Sequence numbers out of order: {} followed by {}", prev, seq_num);
            }
            prev_seq = Some(seq_num);
        }
    }
}

fn create_read_only_transaction(sender: &mut TestAccount) -> AnalyzedTransaction {
    // Create a transaction with empty write_hints
    let txn = create_transaction_with_hints(
        sender,
        vec![],  // empty write_hints
        vec![StorageLocation::Specific(StateKey::from_random())], // read_hints
    );
    txn
}
```

This test will fail with the current implementation when the read-only transaction and write transaction end up in different shards, demonstrating the ordering violation.

## Notes

- The vulnerability requires specific transaction patterns but is realistic given the prevalence of read-only operations in blockchain applications
- The existing test suite `test_relative_ordering_for_sender` only tests P2P transfers which always have writes, missing this edge case
- The comment at line 94-95 in connected_component/mod.rs acknowledges ordering concerns but doesn't address this specific scenario
- Solution 2 (using OriginalTxnIdx) is preferred as it's more robust and maintains the correct invariant regardless of how pre-partitioning reorders transactions

### Citations

**File:** execution/block-partitioner/src/pre_partition/connected_component/mod.rs (L49-56)
```rust
        for txn_idx in 0..state.num_txns() {
            let sender_idx = state.sender_idx(txn_idx);
            let write_set = state.write_sets[txn_idx].read().unwrap();
            for &key_idx in write_set.iter() {
                let key_idx_in_uf = num_senders + key_idx;
                uf.union(key_idx_in_uf, sender_idx);
            }
        }
```

**File:** execution/block-partitioner/src/pre_partition/connected_component/mod.rs (L78-86)
```rust
        for ori_txn_idx in 0..state.num_txns() {
            let sender_idx = state.sender_idx(ori_txn_idx);
            let uf_set_idx = uf.find(sender_idx);
            let set_idx = set_idx_registry.entry(uf_set_idx).or_insert_with(|| {
                txns_by_set.push(VecDeque::new());
                set_idx_counter.fetch_add(1, Ordering::SeqCst)
            });
            txns_by_set[*set_idx].push_back(ori_txn_idx);
        }
```

**File:** execution/block-partitioner/src/pre_partition/connected_component/mod.rs (L122-144)
```rust
        let mut ori_txns_idxs_by_shard: Vec<Vec<OriginalTxnIdx>> =
            vec![vec![]; state.num_executor_shards];
        for (shard_id, group_ids) in groups_by_shard.into_iter().enumerate() {
            for group_id in group_ids.into_iter() {
                let (set_id, amount) = group_metadata[group_id];
                for _ in 0..amount {
                    let ori_txn_idx = txns_by_set[set_id].pop_front().unwrap();
                    ori_txns_idxs_by_shard[shard_id].push(ori_txn_idx);
                }
            }
        }

        // Prepare `ori_txn_idxs` and `start_txn_idxs_by_shard`.
        let mut start_txn_idxs_by_shard = vec![0; state.num_executor_shards];
        let mut ori_txn_idxs = vec![0; state.num_txns()];
        let mut pre_partitioned_txn_idx = 0;
        for (shard_id, txn_idxs) in ori_txns_idxs_by_shard.iter().enumerate() {
            start_txn_idxs_by_shard[shard_id] = pre_partitioned_txn_idx;
            for &i0 in txn_idxs {
                ori_txn_idxs[pre_partitioned_txn_idx] = i0;
                pre_partitioned_txn_idx += 1;
            }
        }
```

**File:** execution/block-partitioner/src/v2/partition_to_matrix.rs (L102-104)
```rust
        // Initialize a table to keep track of the minimum discarded PrePartitionedTxnIdx.
        let min_discard_table: DashMap<SenderIdx, AtomicUsize> =
            DashMap::with_shard_amount(state.dashmap_num_shards);
```

**File:** execution/block-partitioner/src/v2/partition_to_matrix.rs (L144-166)
```rust
            // Additional discarding to preserve relative txn order for the same sender.
            tentatively_accepted
                .into_iter()
                .enumerate()
                .collect::<Vec<_>>()
                .into_par_iter()
                .for_each(|(shard_id, txn_idxs)| {
                    let txn_idxs = mem::take(&mut *txn_idxs.write().unwrap());
                    txn_idxs.into_par_iter().for_each(|txn_idx| {
                        let ori_txn_idx = state.ori_idxs_by_pre_partitioned[txn_idx];
                        let sender_idx = state.sender_idx(ori_txn_idx);
                        let min_discarded = min_discard_table
                            .get(&sender_idx)
                            .map(|kv| kv.load(Ordering::SeqCst))
                            .unwrap_or(usize::MAX);
                        if txn_idx < min_discarded {
                            state.update_trackers_on_accepting(txn_idx, round_id, shard_id);
                            finally_accepted[shard_id].write().unwrap().push(txn_idx);
                        } else {
                            discarded[shard_id].write().unwrap().push(txn_idx);
                        }
                    });
                });
```

**File:** aptos-move/framework/aptos-framework/sources/transaction_validation.move (L63-64)
```text
    const PROLOGUE_ESEQUENCE_NUMBER_TOO_OLD: u64 = 1002;
    const PROLOGUE_ESEQUENCE_NUMBER_TOO_NEW: u64 = 1003;
```

**File:** execution/block-partitioner/src/tests.rs (L49-89)
```rust
#[test]
// Ensures that transactions from the same sender are not reordered.
fn test_relative_ordering_for_sender() {
    let mut rng = OsRng;
    let num_shards = 8;
    let num_accounts = 50;
    let num_txns = 500;
    let mut accounts = Vec::new();
    for _ in 0..num_accounts {
        accounts.push(Mutex::new(generate_test_account()));
    }
    let mut transactions = Vec::new();

    for _ in 0..num_txns {
        let indices = rand::seq::index::sample(&mut rng, num_accounts, 2);
        let sender = &mut accounts[indices.index(0)].lock().unwrap();
        let receiver = &accounts[indices.index(1)].lock().unwrap();
        let txn = create_signed_p2p_transaction(sender, vec![receiver]).remove(0);
        transactions.push(txn.clone());
        transactions.push(create_signed_p2p_transaction(sender, vec![receiver]).remove(0));
    }

    let partitioner = PartitionerV2Config::default().build();
    let (sub_blocks, _) = partitioner
        .partition(transactions.clone(), num_shards)
        .into();

    let mut account_to_expected_seq_number: HashMap<AccountAddress, u64> = HashMap::new();
    SubBlocksForShard::flatten(sub_blocks)
        .iter()
        .for_each(|txn| {
            let (sender, seq_number) = get_account_seq_number(txn.transaction().expect_valid());
            if account_to_expected_seq_number.contains_key(&sender) {
                assert_eq!(
                    account_to_expected_seq_number.get(&sender).unwrap(),
                    &seq_number
                );
            }
            account_to_expected_seq_number.insert(sender, seq_number + 1);
        });
}
```
