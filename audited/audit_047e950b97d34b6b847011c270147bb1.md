# Audit Report

## Title
Silent Thread Panic in Remote Executor Service Causes Distributed Execution Hang

## Summary
The `ExecutorService::start()` function spawns a thread to run the shard executor service but does not handle thread panics. If the spawned thread panics during execution, it terminates silently while the coordinator continues to believe the shard is operational, causing the entire distributed block execution system to hang indefinitely. [1](#0-0) 

## Finding Description

The vulnerability exists in the thread spawning logic of `ExecutorService::start()`. The function spawns a detached thread that calls `executor_service_clone.start()`, but only validates that the thread spawns successfully via `.expect("Failed to spawn thread")`. This validation does **not** catch panics that occur after the thread has started.

The spawned thread runs `ShardedExecutorService::start()`, which contains a loop that:
1. Receives execution commands from the coordinator
2. Executes sub-blocks
3. Sends results back to the coordinator [2](#0-1) 

Multiple panic points exist in the execution path:

**Deserialization Panic:** [3](#0-2) 

**Serialization and Send Panics:** [4](#0-3) 

**Execution Path Panics:** [5](#0-4) [6](#0-5) 

When any of these panics occur, the thread terminates, but the coordinator has no mechanism to detect this failure. The coordinator will send execution commands and block waiting for results that will never arrive: [7](#0-6) 

The blocking `rx.recv().unwrap()` call will either hang indefinitely (if the channel remains open but the shard is dead) or panic itself (if the channel closes), cascading the failure to the coordinator.

## Impact Explanation

**Severity: High** (up to $50,000 per Aptos Bug Bounty)

This qualifies as High Severity under the following categories:
- **Validator node slowdowns**: The distributed execution system hangs, blocking all block processing
- **Significant protocol violations**: Breaks the liveness guarantee of the execution layer

The impact includes:
- Complete stall of distributed block execution
- Coordinator hangs waiting for results from dead shard
- No recovery mechanism without process restart
- Silent failure mode where the system appears operational but is actually broken
- Affects all validators using remote sharded execution

This breaks **Critical Invariant #4 (State Consistency)** and the liveness guarantee of the consensus system, as blocks cannot be executed and committed.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability can be triggered by:
1. **Network corruption**: Malformed messages causing BCS deserialization failures
2. **Internal bugs**: Any unhandled error in the execution path with `.unwrap()` calls
3. **Channel disconnection**: Coordinator process failures causing send/receive errors
4. **Resource exhaustion**: Thread pool saturation or memory issues

While it requires specific error conditions, these can occur naturally in production environments with:
- Unreliable networks
- High load scenarios
- Software bugs in the execution pipeline
- Infrastructure failures

The likelihood increases with network instability and system complexity.

## Recommendation

Implement proper panic handling for the spawned thread:

```rust
pub fn start(&mut self) {
    self.controller.start();
    let thread_name = format!("ExecutorService-{}", self.shard_id);
    let builder = thread::Builder::new().name(thread_name);
    let executor_service_clone = self.executor_service.clone();
    let shard_id = self.shard_id;
    
    let join_handle = builder
        .spawn(move || {
            // Set up panic handler
            let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
                executor_service_clone.start();
            }));
            
            if let Err(panic_info) = result {
                error!(
                    "ExecutorService thread for shard {} panicked: {:?}",
                    shard_id, panic_info
                );
                // Could also notify coordinator here
            }
        })
        .expect("Failed to spawn thread");
    
    // Store join_handle to monitor thread health
    self.join_handle = Some(join_handle);
}
```

Additional recommendations:
1. **Replace `.unwrap()` calls** with proper error handling and logging throughout the execution path
2. **Implement health checks** to detect dead shards
3. **Add timeout mechanisms** in the coordinator's `get_output_from_shards()` to prevent indefinite hangs
4. **Use Result types** instead of panicking for recoverable errors
5. **Implement automatic shard restart** on failure detection

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Duration;
    use std::thread;

    #[test]
    fn test_thread_panic_silent_failure() {
        // Simulate a coordinator that expects a result from a shard
        let (result_tx, result_rx) = crossbeam_channel::unbounded::<String>();
        
        // Spawn a thread similar to ExecutorService::start()
        let builder = thread::Builder::new().name("test-executor".to_string());
        
        builder
            .spawn(move || {
                // Simulate the executor service panicking
                panic!("Simulated panic in executor_service.start()");
                
                // This line is never reached
                result_tx.send("Success".to_string()).unwrap();
            })
            .expect("Failed to spawn thread");
        
        // Coordinator tries to receive result with timeout
        let result = result_rx.recv_timeout(Duration::from_secs(2));
        
        // Demonstrates the hang: recv_timeout expires because thread panicked
        assert!(result.is_err(), "Should timeout waiting for dead thread");
        
        println!("PoC: Thread panicked silently, coordinator hung waiting for result");
    }
    
    #[test]
    fn test_proper_panic_handling() {
        let (result_tx, result_rx) = crossbeam_channel::unbounded::<String>();
        
        let builder = thread::Builder::new().name("test-executor".to_string());
        
        let join_handle = builder
            .spawn(move || {
                // Proper panic handling
                let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
                    panic!("Simulated panic");
                }));
                
                if result.is_err() {
                    // Notify coordinator of failure
                    result_tx.send("PANIC_DETECTED".to_string()).unwrap();
                }
            })
            .expect("Failed to spawn thread");
        
        // Coordinator can now detect the failure
        let result = result_rx.recv_timeout(Duration::from_secs(2));
        assert_eq!(result.unwrap(), "PANIC_DETECTED");
        
        join_handle.join().ok();
        println!("PoC: Panic properly caught and reported to coordinator");
    }
}
```

## Notes

The vulnerability is particularly severe in production environments where:
- Multiple remote shards are coordinating distributed execution
- Network reliability varies across data centers
- System load can trigger edge cases in the execution path

The silent failure mode is especially problematic because:
- No error logs indicate the shard has failed
- Monitoring systems may not detect the hang immediately
- Manual intervention is required to identify and restart failed shards
- The coordinator has no recovery strategy built-in

This issue affects the **Remote Execution Service** specifically and would impact validators using the sharded execution feature for performance optimization. The local execution path has similar issues but uses in-process communication which may behave differently on panic.

### Citations

**File:** execution/executor-service/src/remote_executor_service.rs (L57-67)
```rust
    pub fn start(&mut self) {
        self.controller.start();
        let thread_name = format!("ExecutorService-{}", self.shard_id);
        let builder = thread::Builder::new().name(thread_name);
        let executor_service_clone = self.executor_service.clone();
        builder
            .spawn(move || {
                executor_service_clone.start();
            })
            .expect("Failed to spawn thread");
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L174-174)
```rust
                callback.send(ret).unwrap();
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L182-182)
```rust
        block_on(callback_receiver).unwrap()
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-260)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L89-89)
```rust
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L117-118)
```rust
        let output_message = bcs::to_bytes(&remote_execution_result).unwrap();
        self.result_tx.send(Message::new(output_message)).unwrap();
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```
