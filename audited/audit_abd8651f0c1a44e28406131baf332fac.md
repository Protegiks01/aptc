# Audit Report

## Title
Transaction Starvation Due to Race Condition Between Broadcast Retry and Peer Priority Updates

## Summary
A race condition exists between mempool broadcast retries and peer priority updates that causes transactions to be silently dropped when sender bucket assignments change. When a broadcast times out and needs to be retried, if peer priorities have been updated in the interim, the retry mechanism fails to fetch transactions for sender buckets that are no longer assigned to that peer, resulting in those transactions never being forwarded to that peer.

## Finding Description

The vulnerability exists in the broadcast retry mechanism for fullnodes in the shared mempool system. The issue occurs across two critical functions:

The `get_sender_bucket_priority_for_peer()` function performs a simple lookup to retrieve the broadcast priority for a specific peer and sender bucket combination. [1](#0-0) 

However, the `peer_to_sender_buckets` HashMap that this function queries is completely cleared and rebuilt during every peer priority update. [2](#0-1) 

The vulnerability manifests when retrying expired or failed broadcasts. When a broadcast needs to be retried, the code attempts to fetch transactions from the original sender buckets that were in the message. For each sender bucket, it calls `get_sender_bucket_priority_for_peer()`. If this lookup fails (returns `None`), it uses `map_or_else(Vec::new, ...)` which returns an empty vector, effectively dropping all transactions from that sender bucket for this retry. [3](#0-2) 

**Attack Sequence:**

1. **T0**: Peer A is assigned sender bucket 2 with Primary priority
2. **T1**: Fresh broadcast to Peer A includes transactions from bucket 2, creating `message_id_A` encoding bucket 2
3. **T2**: Network latency prevents ACK from arriving within timeout window
4. **T3**: Peer priority update executes (occurs every 600 seconds by default), completely clearing and rebuilding `peer_to_sender_buckets`. Peer A now has different bucket assignments and no longer has bucket 2. [4](#0-3) 
5. **T4**: Broadcast ACK times out (2000ms default), retry is triggered [5](#0-4) 
6. **T5**: Retry mechanism decodes `message_id_A` to get bucket 2, calls `get_sender_bucket_priority_for_peer(&peer_A, 2)`, receives `None`, returns empty vector
7. **T6**: Transactions from bucket 2 are silently dropped from this retry
8. **T7**: Subsequent fresh broadcasts to Peer A only include current bucket assignments (no longer bucket 2), so these transactions never reach Peer A

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program based on "Significant protocol violations" criteria:

1. **Transaction Liveness Violation**: Transactions fail to propagate to peers they were originally targeted for, violating the mempool's guarantee that transactions should reach all connected peers

2. **Uneven Transaction Distribution**: Different peers may receive different subsets of transactions based on when priority updates occur relative to their broadcasts, causing network-wide inconsistency in mempool state

3. **Validator Impact**: If validator fullnodes (VFNs) experience this issue, validators may not receive certain transactions, potentially causing slower block proposal and reduced network throughput

4. **Silent Failure Mode**: The bug causes silent transaction drops without errors or warnings, making it difficult to detect and debug in production

The issue does not reach Critical severity as it does not cause permanent fund loss, consensus safety violations, or complete network partition. However, it significantly degrades protocol performance and reliability.

## Likelihood Explanation

This vulnerability has **MEDIUM-HIGH likelihood** of occurrence in production:

**Factors Increasing Likelihood:**
- Peer priority updates occur every 10 minutes by default on all fullnodes
- ACK timeout is only 2 seconds, creating a 0.33% probability window per broadcast
- Network latency, packet loss, or peer processing delays can extend ACK times beyond 2 seconds
- Fullnode peer churn and load balancing changes trigger more frequent priority updates
- No special attacker action required - occurs during normal operation

**Frequency Calculation:**
- Each fullnode broadcasts to peers every 10ms (default tick interval)
- With typical network latency of 100-500ms, most ACKs arrive within timeout
- However, during network congestion or peer issues, ACK times can exceed 2 seconds
- If priority update occurs during any pending broadcast's ACK window, transaction loss occurs
- Expected occurrence: Multiple times per day on busy fullnodes

## Recommendation

**Root Cause**: The retry mechanism uses stale sender bucket assignments (encoded in `message_id`) instead of current assignments.

**Fix Strategy**: When retrying broadcasts, use the current sender bucket assignments for the peer instead of the original assignments from the message_id. This ensures retries use up-to-date routing information.

**Recommended Code Changes**:

In `mempool/src/shared_mempool/network.rs`, modify the retry broadcast logic to:

1. **Option A (Preferred)**: For retry/expired broadcasts, fetch transactions using the peer's CURRENT bucket assignments instead of the original message_id's buckets. This aligns retry behavior with fresh broadcast behavior.

2. **Option B**: Maintain a stable snapshot of bucket assignments per broadcast and use that snapshot for retries. However, this adds memory overhead.

3. **Option C**: If a bucket lookup fails during retry, log a warning and treat it as a transient failure, allowing the normal retry mechanism to re-attempt later when assignments may have stabilized.

**Specific Fix for Option A**:

Instead of decoding the message_id and using those buckets directly, first check if the peer still has those buckets assigned. If not, skip that bucket for this retry and rely on fresh broadcasts to propagate those transactions using current assignments.

Add explicit error handling: When `get_sender_bucket_priority_for_peer()` returns `None`, log this as a warning (not silent failure) and allow the system to rely on fresh broadcasts to handle propagation.

## Proof of Concept

```rust
#[test]
fn test_transaction_starvation_on_priority_update() {
    use crate::shared_mempool::priority::PrioritizedPeersState;
    use crate::shared_mempool::network::MempoolNetworkInterface;
    use aptos_config::config::{MempoolConfig, NodeType};
    use aptos_time_service::TimeService;
    
    // Setup
    let mempool_config = MempoolConfig::default();
    let time_service = TimeService::mock();
    let mut priority_state = PrioritizedPeersState::new(
        mempool_config.clone(),
        NodeType::PublicFullnode,
        time_service.clone(),
    );
    
    // Create test peers
    let peer_a = create_test_peer();
    let peer_b = create_test_peer();
    
    // Initial priority update - Peer A gets bucket 2
    let peers_and_metadata = vec![
        (peer_a, Some(&create_test_metadata(0.1))),
        (peer_b, Some(&create_test_metadata(0.2))),
    ];
    priority_state.update_prioritized_peers(peers_and_metadata.clone(), 1000, 1000);
    
    // Verify Peer A has bucket 2
    let bucket_priority = priority_state.get_sender_bucket_priority_for_peer(&peer_a, 2);
    assert!(bucket_priority.is_some(), "Peer A should have bucket 2");
    
    // Simulate time passing and traffic change
    time_service.clone().into_mock().advance_secs(700);
    
    // Second priority update with different load - bucket assignments change
    priority_state.update_prioritized_peers(peers_and_metadata, 5000, 5000);
    
    // Critical: Check if Peer A lost bucket 2
    let bucket_priority_after = priority_state.get_sender_bucket_priority_for_peer(&peer_a, 2);
    
    // If bucket 2 moved to Peer B, this demonstrates the vulnerability:
    // Any pending retry for Peer A with bucket 2 will now fail to fetch transactions
    if bucket_priority_after.is_none() {
        println!("VULNERABILITY CONFIRMED: Peer A lost bucket 2 assignment");
        println!("Any retry broadcast containing bucket 2 will drop those transactions");
        
        // This simulates what happens in network.rs line 476:
        // .map_or_else(Vec::new, |priority| { ... })
        // Returns empty Vec when bucket_priority_after is None
        let transactions_in_retry = bucket_priority_after
            .map_or_else(Vec::new, |_priority| {
                vec!["tx1", "tx2", "tx3"] // Would normally fetch from mempool
            });
        
        assert!(transactions_in_retry.is_empty(), 
            "Transactions silently dropped due to bucket reassignment");
    }
}

// Helper functions
fn create_test_peer() -> PeerNetworkId {
    use aptos_types::PeerId;
    use aptos_config::network_id::NetworkId;
    PeerNetworkId::new(NetworkId::Public, PeerId::random())
}

fn create_test_metadata(latency: f64) -> PeerMonitoringMetadata {
    use aptos_peer_monitoring_service_types::PeerMonitoringMetadata;
    PeerMonitoringMetadata::new(Some(latency), None, None, None, None)
}
```

This test demonstrates that when peer priorities update and bucket assignments change, the lookup for the old bucket assignment fails, returning `None`, which causes transactions to be dropped via the `map_or_else(Vec::new, ...)` pattern.

## Notes

This vulnerability specifically affects fullnodes (VFNs and PFNs) but not validators, as validators bypass the priority-based bucket assignment system. [6](#0-5) 

The issue is exacerbated by the fact that fresh broadcasts use current bucket assignments, while retries use stale assignments from the message_id encoding. [7](#0-6)  This asymmetry creates an inconsistent retry mechanism.

### Citations

**File:** mempool/src/shared_mempool/priority.rs (L205-213)
```rust
    pub fn get_sender_bucket_priority_for_peer(
        &self,
        peer: &PeerNetworkId,
        sender_bucket: MempoolSenderBucket,
    ) -> Option<BroadcastPeerPriority> {
        self.peer_to_sender_buckets
            .get(peer)
            .and_then(|buckets| buckets.get(&sender_bucket).cloned())
    }
```

**File:** mempool/src/shared_mempool/priority.rs (L399-431)
```rust
        self.peer_to_sender_buckets = HashMap::new();
        if !self.prioritized_peers.read().is_empty() {
            // Assign sender buckets with Primary priority
            let mut peer_index = 0;
            for bucket_index in 0..self.mempool_config.num_sender_buckets {
                self.peer_to_sender_buckets
                    .entry(*top_peers.get(peer_index).unwrap())
                    .or_default()
                    .insert(bucket_index, BroadcastPeerPriority::Primary);
                peer_index = (peer_index + 1) % top_peers.len();
            }

            // Assign sender buckets with Failover priority. Use Round Robin.
            peer_index = 0;
            let num_prioritized_peers = self.prioritized_peers.read().len();
            for _ in 0..self.mempool_config.default_failovers {
                for bucket_index in 0..self.mempool_config.num_sender_buckets {
                    // Find the first peer that already doesn't have the sender bucket, and add the bucket
                    for _ in 0..num_prioritized_peers {
                        let peer = self.prioritized_peers.read()[peer_index];
                        let sender_bucket_list =
                            self.peer_to_sender_buckets.entry(peer).or_default();
                        if let std::collections::hash_map::Entry::Vacant(e) =
                            sender_bucket_list.entry(bucket_index)
                        {
                            e.insert(BroadcastPeerPriority::Failover);
                            break;
                        }
                        peer_index = (peer_index + 1) % num_prioritized_peers;
                    }
                }
            }
        }
```

**File:** mempool/src/shared_mempool/network.rs (L461-487)
```rust
                    let txns = message_id
                        .decode()
                        .into_iter()
                        .flat_map(|(sender_bucket, start_end_pairs)| {
                            if self.node_type.is_validator() {
                                mempool
                                    .timeline_range(sender_bucket, start_end_pairs)
                                    .into_iter()
                                    .map(|(txn, ready_time)| {
                                        (txn, ready_time, BroadcastPeerPriority::Primary)
                                    })
                                    .collect::<Vec<_>>()
                            } else {
                                self.prioritized_peers_state
                                    .get_sender_bucket_priority_for_peer(&peer, sender_bucket)
                                    .map_or_else(Vec::new, |priority| {
                                        mempool
                                            .timeline_range(sender_bucket, start_end_pairs)
                                            .into_iter()
                                            .map(|(txn, ready_time)| {
                                                (txn, ready_time, priority.clone())
                                            })
                                            .collect::<Vec<_>>()
                                    })
                            }
                        })
                        .collect::<Vec<_>>();
```

**File:** mempool/src/shared_mempool/network.rs (L502-513)
```rust
                            self.prioritized_peers_state
                                .get_sender_buckets_for_peer(&peer)
                                .ok_or_else(|| {
                                    BroadcastError::PeerNotPrioritized(
                                        peer,
                                        self.prioritized_peers_state.get_peer_priority(&peer),
                                    )
                                })?
                                .clone()
                                .into_iter()
                                .collect()
                        };
```

**File:** config/src/config/mempool_config.rs (L115-115)
```rust
            shared_mempool_ack_timeout_ms: 2_000,
```

**File:** config/src/config/mempool_config.rs (L127-127)
```rust
            shared_mempool_priority_update_interval_secs: 600, // 10 minutes (frequent reprioritization is expensive)
```
