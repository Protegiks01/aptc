# Audit Report

## Title
Insecure File Permission Preservation During Database Backup Restoration Exposes Blockchain State Metadata

## Summary
The `unpack_tar_gz()` function in the indexer-grpc-table-info backup/restore system preserves file permissions from tar archives without any hardening, allowing database files to be restored with overly permissive permissions (e.g., world-readable 0644). This exposes blockchain state metadata to unauthorized local processes. [1](#0-0) 

## Finding Description
The vulnerability exists in the database backup and restore flow for the indexer-grpc-table-info service:

1. **Backup Creation**: When `create_tar_gz()` creates a backup, it uses `tar_builder.append_dir_all()` which preserves the original file permissions from the source RocksDB database directory. [2](#0-1) 

2. **Permission Propagation**: RocksDB creates database files using the process's umask. On standard Linux systems with umask 0022, this results in files with 0644 permissions (owner read/write, group read, world read). [3](#0-2) 

3. **Restoration Without Hardening**: When `unpack_tar_gz()` restores the database, it uses `archive.unpack()` which preserves the permissions stored in the tar archive. There is NO subsequent code to set secure permissions (e.g., 0600) on the extracted files. [1](#0-0) 

4. **Exposure**: The restored database files remain world-readable, allowing any local user or process on the system to read the indexer database contents, which includes table schema metadata (mappings of TableHandle to TableInfo containing key_type and value_type TypeTags). [4](#0-3) 

**Attack Path:**
- Attacker gains local unprivileged access to a validator or fullnode system (through compromised service, container escape, or other means)
- Database was previously backed up and restored using the GCS backup/restore functionality
- Database files retain world-readable permissions (0644) from the restore process
- Attacker reads database files: `cat /path/to/storage/index_indexer_async_v2_db/*.sst`
- Attacker obtains table schema information for all on-chain tables

This breaks the **Access Control** security principle - database files containing blockchain state should only be readable by the database process owner, not by arbitrary local users.

## Impact Explanation
This vulnerability constitutes a **Low to Medium Severity** information disclosure issue:

**Low Severity Factors:**
- Requires local access to the system (not remotely exploitable)
- Only exposes table schema metadata, not sensitive data like private keys or transaction details
- Does not directly impact funds, consensus, or network availability

**Medium Severity Consideration:**
- Exposes blockchain state information that should be protected
- Violates defense-in-depth security principles
- Could enable reconnaissance for more sophisticated attacks
- Affects any system using the backup/restore functionality

Per Aptos bug bounty criteria, this most closely aligns with **"Minor information leaks"** (Low Severity), though the exposure of blockchain state metadata could be argued as requiring intervention to fix on affected systems (Medium Severity).

## Likelihood Explanation
**HIGH Likelihood:**

1. **Default Behavior**: RocksDB uses the process umask for file creation, and the standard Linux umask of 0022 results in world-readable files by default
2. **No Mitigation**: There is no code to explicitly set secure permissions after database restoration
3. **Production Use**: The backup/restore functionality is documented and intended for production use [5](#0-4) 
4. **Operator Assumptions**: Operators are unlikely to manually verify and fix file permissions after every restore operation

## Recommendation
Implement explicit permission hardening after tar extraction:

```rust
pub fn unpack_tar_gz(temp_file_path: &PathBuf, target_db_path: &PathBuf) -> anyhow::Result<()> {
    let temp_dir_path = target_db_path.with_extension("tmp");
    fs::create_dir(&temp_dir_path)?;

    let file = File::open(temp_file_path)?;
    let gz_decoder = GzDecoder::new(file);
    let mut archive = Archive::new(gz_decoder);
    
    // Disable automatic permission preservation
    archive.set_preserve_permissions(false);
    archive.unpack(&temp_dir_path)?;

    // Explicitly set secure permissions on extracted directory and contents
    set_secure_permissions(&temp_dir_path)?;

    fs::remove_dir_all(target_db_path).unwrap_or(());
    fs::rename(&temp_dir_path, target_db_path)?;
    Ok(())
}

fn set_secure_permissions(dir_path: &PathBuf) -> anyhow::Result<()> {
    use std::os::unix::fs::PermissionsExt;
    
    // Set directory to 0700 (owner-only access)
    let dir_perms = fs::Permissions::from_mode(0o700);
    fs::set_permissions(dir_path, dir_perms)?;
    
    // Recursively set all files to 0600 (owner read/write only)
    for entry in fs::read_dir(dir_path)? {
        let entry = entry?;
        let path = entry.path();
        let file_perms = fs::Permissions::from_mode(0o600);
        fs::set_permissions(&path, file_perms)?;
        
        if path.is_dir() {
            set_secure_permissions(&path)?;
        }
    }
    Ok(())
}
```

Additionally, consider setting a restrictive umask (0077) when opening RocksDB databases to ensure files are created with secure permissions from the start.

## Proof of Concept

```rust
#[test]
fn test_insecure_permissions_preserved() -> anyhow::Result<()> {
    use std::os::unix::fs::PermissionsExt;
    use tempfile::tempdir;
    
    // Create a source directory with overly permissive files (simulating umask 0022)
    let source_dir = tempdir()?;
    let test_file = source_dir.path().join("test.db");
    std::fs::write(&test_file, b"sensitive data")?;
    
    // Set world-readable permissions (0644)
    let insecure_perms = std::fs::Permissions::from_mode(0o644);
    std::fs::set_permissions(&test_file, insecure_perms)?;
    
    // Create backup
    let backup_path = create_tar_gz(source_dir.path().to_path_buf(), "test_backup")?;
    
    // Restore to new location
    let restore_dir = tempdir()?;
    unpack_tar_gz(&backup_path, &restore_dir.path().to_path_buf())?;
    
    // Check if insecure permissions were preserved
    let restored_file = restore_dir.path().join("test.db");
    let metadata = std::fs::metadata(&restored_file)?;
    let mode = metadata.permissions().mode();
    
    // Assert that world-readable bit is set (vulnerability confirmed)
    assert_eq!(mode & 0o004, 0o004, "File is world-readable - VULNERABILITY!");
    
    println!("VULNERABILITY CONFIRMED: Restored file has mode {:o}, which is world-readable", mode);
    
    Ok(())
}
```

## Notes
- This vulnerability affects the indexer-grpc-table-info service specifically, not the main AptosDB or consensus-critical databases
- The issue is inherent in the design - the Rust `tar` crate's default behavior is to preserve permissions
- While the exposed data (table schema metadata) is not as sensitive as private keys, it still represents blockchain state that should be protected under the principle of least privilege
- Mitigation should include both fixing the restore process and ensuring databases are created with secure permissions initially (via umask or RocksDB configuration)

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/fs_ops.rs (L56-98)
```rust
pub fn create_tar_gz(dir_path: PathBuf, backup_file_name: &str) -> Result<PathBuf, anyhow::Error> {
    // Create a buffer to write the tar.gz archive.
    let gz_encoder = GzEncoder::new(Vec::new(), Compression::fast());
    let tar_data = BufWriter::new(gz_encoder);
    let mut tar_builder = Builder::new(tar_data);
    aptos_logger::info!(
        dir_path = dir_path.to_str(),
        backup_file_name = backup_file_name,
        "[Table Info] Creating a tar.gz archive from the db snapshot directory"
    );
    tar_builder
        .append_dir_all(".", &dir_path)
        .context("Tar building failed.")?;
    aptos_logger::info!("[Table Info] Directory contents appended to the tar.gz archive");
    // Finish writing the tar archive and get the compressed GzEncoder back
    let tar_data = tar_builder
        .into_inner()
        .context("Unwrap the tar builder failed.")?;
    let gz_encoder = tar_data
        .into_inner()
        .context("Failed to get the compressed buffer.")?;

    // Finish the compression process
    let compressed_data = gz_encoder
        .finish()
        .context("Failed to build the compressed bytes.")?;

    let tar_file_name = format!("{}.tar.gz", backup_file_name);
    let tar_file_path = dir_path.join(&tar_file_name);
    aptos_logger::info!(
        dir_path = dir_path.to_str(),
        backup_file_name = backup_file_name,
        tar_file_path = tar_file_path.to_str(),
        tar_file_name = tar_file_name,
        "[Table Info] Prepare to compress the db snapshot directory"
    );
    // Write the tar.gz archive to a file
    std::fs::write(&tar_file_path, compressed_data)
        .context("Failed to write the compressed data.")?;
    aptos_logger::info!("[Table Info] Tar.gz archive created successfully");

    Ok(tar_file_path)
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/fs_ops.rs (L101-113)
```rust
pub fn unpack_tar_gz(temp_file_path: &PathBuf, target_db_path: &PathBuf) -> anyhow::Result<()> {
    let temp_dir_path = target_db_path.with_extension("tmp");
    fs::create_dir(&temp_dir_path)?;

    let file = File::open(temp_file_path)?;
    let gz_decoder = GzDecoder::new(file);
    let mut archive = Archive::new(gz_decoder);
    archive.unpack(&temp_dir_path)?;

    fs::remove_dir_all(target_db_path).unwrap_or(());
    fs::rename(&temp_dir_path, target_db_path)?; // Atomically replace the directory
    Ok(())
}
```

**File:** storage/indexer/src/db_ops.rs (L14-35)
```rust
pub fn open_db<P: AsRef<Path>>(
    db_path: P,
    rocksdb_config: &RocksdbConfig,
    readonly: bool,
) -> Result<DB> {
    let env = None;
    if readonly {
        Ok(DB::open_readonly(
            db_path,
            TABLE_INFO_DB_NAME,
            column_families(),
            &gen_rocksdb_options(rocksdb_config, env, readonly),
        )?)
    } else {
        Ok(DB::open(
            db_path,
            TABLE_INFO_DB_NAME,
            column_families(),
            &gen_rocksdb_options(rocksdb_config, env, readonly),
        )?)
    }
}
```

**File:** storage/indexer/src/db_v2.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

/// This file is a copy of the file storage/indexer/src/lib.rs.
/// At the end of the migration to migrate table info mapping
/// from storage critical path to indexer, the other file will be removed
/// and this file will be moved to /ecosystem/indexer-grpc/indexer-grpc-table-info.
use aptos_db_indexer_schemas::{
    metadata::{MetadataKey, MetadataValue},
    schema::{indexer_metadata::IndexerMetadataSchema, table_info::TableInfoSchema},
};
use aptos_logger::{info, sample, sample::SampleRate};
use aptos_resource_viewer::{AptosValueAnnotator, MoveTableInfo};
use aptos_schemadb::{batch::SchemaBatch, DB};
use aptos_storage_interface::{
    db_other_bail as bail, state_store::state_view::db_state_view::DbStateViewAtVersion,
    AptosDbError, DbReader, Result,
};
use aptos_types::{
    access_path::Path,
    state_store::{
        state_key::{inner::StateKeyInner, StateKey},
        table::{TableHandle, TableInfo},
        StateView,
    },
    transaction::Version,
    write_set::{WriteOp, WriteSet},
};
use bytes::Bytes;
use dashmap::{DashMap, DashSet};
use move_core_types::language_storage::{StructTag, TypeTag};
use std::{
    collections::{BTreeMap, HashMap},
    fs,
    path::PathBuf,
    sync::{
        atomic::{AtomicU64, Ordering},
        Arc,
    },
    time::Duration,
};

const TABLE_INFO_RETRY_TIME_MILLIS: u64 = 10;

#[derive(Debug)]
pub struct IndexerAsyncV2 {
    pub db: DB,
    // Next version to be processed
    next_version: AtomicU64,
    // It is used in the context of processing write ops and extracting table information.
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L264-314)
```rust
    pub async fn restore_db_snapshot(
        &self,
        chain_id: u64,
        metadata: BackupRestoreMetadata,
        db_path: PathBuf,
        base_path: PathBuf,
    ) -> anyhow::Result<()> {
        assert!(metadata.chain_id == chain_id, "Chain ID mismatch.");

        let epoch = metadata.epoch;
        let epoch_based_filename = generate_blob_name(chain_id, epoch);

        match self
            .gcs_client
            .download_streamed_object(
                &GetObjectRequest {
                    bucket: self.bucket_name.clone(),
                    object: epoch_based_filename.clone(),
                    ..Default::default()
                },
                &Range::default(),
            )
            .await
        {
            Ok(mut stream) => {
                // Create a temporary file and write the stream to it directly
                let temp_file_name = "snapshot.tar.gz";
                let temp_file_path = base_path.join(temp_file_name);
                let temp_file_path_clone = temp_file_path.clone();
                let mut temp_file = File::create(&temp_file_path_clone).await?;
                while let Some(chunk) = stream.next().await {
                    match chunk {
                        Ok(data) => temp_file.write_all(&data).await?,
                        Err(e) => return Err(anyhow::Error::new(e)),
                    }
                }
                temp_file.sync_all().await?;

                // Spawn blocking a thread to synchronously unpack gzipped tar file without blocking the async thread
                task::spawn_blocking(move || unpack_tar_gz(&temp_file_path_clone, &db_path))
                    .await?
                    .expect("Failed to unpack gzipped tar file");

                fs::remove_file(&temp_file_path)
                    .await
                    .context("Failed to remove temporary file after unpacking")?;
                Ok(())
            },
            Err(e) => Err(anyhow::Error::new(e)),
        }
    }
```
