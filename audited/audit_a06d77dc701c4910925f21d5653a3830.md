# Audit Report

## Title
Consensus Key and SafetyData Backup Inconsistency Leading to Validator Liveness Failure and Potential Equivocation

## Summary
Restoring a validator node from a backup containing stale `OnDiskStorage` data (stored in `secure-data.json`) can cause the validator to either fail to start due to consensus key mismatch with the current epoch, or attempt equivocation due to rolled-back `SafetyData` within the same epoch. This occurs because the `OnDiskStorage.set()` function persists both consensus keys and safety-critical voting state to a single file that validators may include in disaster recovery backups.

## Finding Description

The vulnerability stems from the design of `OnDiskStorage` which stores all secure data in a single JSON file at `file_path` (typically `/opt/aptos/data/secure-data.json` in production). [1](#0-0) 

This file contains both:
1. **Consensus private keys** - stored with key `CONSENSUS_KEY` 
2. **SafetyData** - containing `epoch`, `last_voted_round`, `preferred_round`, and `last_vote` [2](#0-1) 

When validators perform disaster recovery backups of their data directory, this file is typically included. Restoring from an old backup creates two distinct vulnerability scenarios:

**Scenario A: Consensus Key Mismatch After Rotation (Liveness Failure)**

1. Validator rotates consensus key in epoch N via `stake::rotate_consensus_key()`
2. New key is stored locally and rotation takes effect at epoch N+1
3. Backup taken contains the old key from epoch N
4. Validator later restores from this backup in epoch N+2 or beyond
5. On startup, `EpochManager::load_consensus_key()` attempts to load the key expected by current epoch's `ValidatorVerifier` [3](#0-2) 

6. The function fails to find the new key in storage and falls back to default key
7. The default key (from backup) doesn't match the expected public key [4](#0-3) 

8. Returns `Error::SecureStorageMissingDataError`
9. Node panics and cannot start [5](#0-4) 

**Scenario B: SafetyData Rollback Within Same Epoch (Safety Violation)**

1. Validator votes in rounds R1...Rn during epoch E, with `last_voted_round = Rn`
2. Backup taken earlier has `last_voted_round = R1` (where R1 < Rn)
3. Validator restarts and restores from old backup
4. `guarded_initialize()` is called but SafetyData is NOT reset (same epoch) [6](#0-5) 

5. Validator now has stale `last_voted_round = R1` instead of actual Rn
6. When asked to vote in round Rk where R1 < Rk â‰¤ Rn, the check passes [7](#0-6) 

7. Validator creates a potentially different vote for round Rk (equivocation)
8. Network-level detection catches this, but local SafetyRules has failed [8](#0-7) 

## Impact Explanation

**Scenario A Impact: High Severity (Liveness)**
- Validator cannot participate in consensus until manual intervention
- Requires validator operator to manually restore correct key or rotate on-chain
- If multiple validators experience this simultaneously, network liveness degrades
- Meets "Validator node slowdowns" under High severity ($50,000) per bug bounty criteria

**Scenario B Impact: High Severity (Safety, Mitigated)**
- Validator attempts to violate consensus safety by double-voting
- Local SafetyRules protection is bypassed by stale data
- Network-level equivocation detection provides mitigation but indicates Byzantine behavior
- SecurityEvent logs are triggered, potentially affecting validator reputation
- Violates "Consensus Safety: AptosBFT must prevent double-spending" invariant
- While mitigated, represents significant protocol violation under High severity

## Likelihood Explanation

**Moderate to High Likelihood:**

1. **Validator backups are standard practice** - Production validators regularly backup their data directories for disaster recovery
2. **Config files place OnDiskStorage in data directory** - Default configuration stores `secure-data.json` in `/opt/aptos/data/` [9](#0-8) 

3. **Key rotation is recommended practice** - Validators periodically rotate consensus keys for security
4. **Restores happen during migrations/disasters** - Validators may restore from backups during hardware failures, cloud migrations, or disaster recovery scenarios
5. **No warning mechanism** - The system does not warn operators that restoring from old backups is dangerous

## Recommendation

**Immediate Mitigations:**

1. **Separate storage paths** - Store consensus keys and SafetyData in separate files to allow independent backup/restore strategies
2. **Add version/timestamp checks** - Include timestamps in SafetyData and reject stale data on load
3. **Explicit backup warnings** - Document that `secure-data.json` should NOT be restored from old backups
4. **Epoch validation on restore** - Add explicit checks that refuse to load SafetyData if epoch has regressed

**Long-term Fix:**

Implement a versioned storage format with monotonic counters:

```rust
// In SafetyData
pub struct SafetyData {
    pub epoch: u64,
    pub last_voted_round: Round,
    pub preferred_round: Round,
    pub highest_timeout_round: Round,
    pub one_chain_round: Round,
    pub last_vote: Option<Vote>,
    pub storage_version: u64,  // NEW: Monotonically increasing
    pub last_persist_timestamp: u64,  // NEW: Detect stale data
}

// In guarded_initialize
if restored_safety_data.storage_version < expected_minimum_version {
    return Err(Error::StaleBackupDetected);
}
```

Additionally, implement a separate key rotation storage that maintains historical keys indexed by epoch, allowing validators to safely restore while preserving the ability to participate in current epoch consensus.

## Proof of Concept

The existing `consensus_key_rotation` test demonstrates the key rotation mechanism: [10](#0-9) 

To demonstrate the vulnerability, extend this test:

```rust
// After key rotation and epoch transition (line 164)
// Simulate backup restore by copying old secure-data.json
let old_storage_path = backup_secure_data_json(&validator); 
validator.stop();
restore_secure_data_json(&validator, &old_storage_path);
validator.start(); // This will panic due to key mismatch

// Alternatively for Scenario B:
// Backup SafetyData within same epoch, vote, restore, and observe equivocation attempt
```

The test would show that after restoring from a backup with an old consensus key, the validator fails to start with the panic message from `load_consensus_key()`.

---

## Notes

This vulnerability is conditional on validator operational practices (backup/restore procedures) but represents a significant design flaw. The system should be resilient against operator mistakes that are likely to occur in production environments. The lack of protective mechanisms against stale data restoration violates the principle of defense-in-depth for consensus-critical components.

While Scenario B (equivocation) is mitigated by network-level detection, it indicates a fundamental problem: the local SafetyRules system can be bypassed through operational actions, converting an honest validator into a Byzantine actor. This is particularly concerning for validators operating in adversarial environments where backup systems might be compromised.

### Citations

**File:** secure/storage/src/on_disk.rs (L85-93)
```rust
    fn set<V: Serialize>(&mut self, key: &str, value: V) -> Result<(), Error> {
        let now = self.time_service.now_secs();
        let mut data = self.read()?;
        data.insert(
            key.to_string(),
            serde_json::to_value(GetResponse::new(value, now))?,
        );
        self.write(&data)
    }
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L98-104)
```rust
    pub fn default_consensus_sk(
        &self,
    ) -> Result<bls12381::PrivateKey, aptos_secure_storage::Error> {
        self.internal_store
            .get::<bls12381::PrivateKey>(CONSENSUS_KEY)
            .map(|v| v.value)
    }
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L125-130)
```rust
        if key.public_key() != pk {
            return Err(Error::SecureStorageMissingDataError(format!(
                "Incorrect sk saved for {:?} the expected pk",
                pk
            )));
        }
```

**File:** consensus/src/epoch_manager.rs (L1228-1233)
```rust
        let loaded_consensus_key = match self.load_consensus_key(&epoch_state.verifier) {
            Ok(k) => Arc::new(k),
            Err(e) => {
                panic!("load_consensus_key failed: {e}");
            },
        };
```

**File:** consensus/src/epoch_manager.rs (L1971-1984)
```rust
    fn load_consensus_key(&self, vv: &ValidatorVerifier) -> anyhow::Result<PrivateKey> {
        match vv.get_public_key(&self.author) {
            Some(pk) => self
                .key_storage
                .consensus_sk_by_pk(pk)
                .map_err(|e| anyhow!("could not find sk by pk: {:?}", e)),
            None => {
                warn!("could not find my pk in validator set, loading default sk!");
                self.key_storage
                    .default_consensus_sk()
                    .map_err(|e| anyhow!("could not load default sk: {e}"))
            },
        }
    }
```

**File:** consensus/safety-rules/src/safety_rules.rs (L218-223)
```rust
        if round <= safety_data.last_voted_round {
            return Err(Error::IncorrectLastVotedRound(
                round,
                safety_data.last_voted_round,
            ));
        }
```

**File:** consensus/safety-rules/src/safety_rules.rs (L284-309)
```rust
        match current_epoch.cmp(&epoch_state.epoch) {
            Ordering::Greater => {
                // waypoint is not up to the current epoch.
                return Err(Error::WaypointOutOfDate(
                    waypoint.version(),
                    new_waypoint.version(),
                    current_epoch,
                    epoch_state.epoch,
                ));
            },
            Ordering::Less => {
                // start new epoch
                self.persistent_storage.set_safety_data(SafetyData::new(
                    epoch_state.epoch,
                    0,
                    0,
                    0,
                    None,
                    0,
                ))?;

                info!(SafetyLogSchema::new(LogEntry::Epoch, LogEvent::Update)
                    .epoch(epoch_state.epoch));
            },
            Ordering::Equal => (),
        };
```

**File:** consensus/src/pending_votes.rs (L287-308)
```rust
        if let Some((previously_seen_vote, previous_li_digest)) =
            self.author_to_vote.get(&vote.author())
        {
            // is it the same vote?
            if &li_digest == previous_li_digest {
                // we've already seen an equivalent vote before
                let new_timeout_vote = vote.is_timeout() && !previously_seen_vote.is_timeout();
                if !new_timeout_vote {
                    // it's not a new timeout vote
                    return VoteReceptionResult::DuplicateVote;
                }
            } else {
                // we have seen a different vote for the same round
                error!(
                    SecurityEvent::ConsensusEquivocatingVote,
                    remote_peer = vote.author(),
                    vote = vote,
                    previous_vote = previously_seen_vote
                );

                return VoteReceptionResult::EquivocateVote;
            }
```

**File:** docker/compose/aptos-node/validator.yaml (L3-13)
```yaml
  data_dir: "/opt/aptos/data"
  waypoint:
    from_file: "/opt/aptos/genesis/waypoint.txt"

consensus:
  safety_rules:
    service:
      type: "local"
    backend:
      type: "on_disk_storage"
      path: secure-data.json
```

**File:** testsuite/smoke-test/src/consensus_key_rotation.rs (L54-119)
```rust
    let (operator_addr, new_pk, pop, operator_idx) =
        if let Some(validator) = swarm.validators_mut().nth(n - 1) {
            let operator_sk = validator
                .account_private_key()
                .as_ref()
                .unwrap()
                .private_key();
            let operator_idx = cli.add_account_to_cli(operator_sk);
            info!("Stopping the last node.");

            validator.stop();
            tokio::time::sleep(Duration::from_secs(5)).await;

            let new_identity_path = PathBuf::from(
                format!(
                    "/tmp/{}-new-validator-identity.yaml",
                    thread_rng().r#gen::<u64>()
                )
                .as_str(),
            );
            info!(
                "Generating and writing new validator identity to {:?}.",
                new_identity_path
            );
            let new_sk = bls12381::PrivateKey::generate(&mut thread_rng());
            let pop = bls12381::ProofOfPossession::create(&new_sk);
            let new_pk = bls12381::PublicKey::from(&new_sk);
            let mut validator_identity_blob = validator
                .config()
                .consensus
                .safety_rules
                .initial_safety_rules_config
                .identity_blob()
                .unwrap();
            validator_identity_blob.consensus_private_key = Some(new_sk);
            let operator_addr = validator_identity_blob.account_address.unwrap();

            Write::write_all(
                &mut File::create(&new_identity_path).unwrap(),
                serde_yaml::to_string(&validator_identity_blob)
                    .unwrap()
                    .as_bytes(),
            )
            .unwrap();

            info!("Updating the node config accordingly.");
            let config_path = validator.config_path();
            let mut validator_override_config =
                OverrideNodeConfig::load_config(config_path.clone()).unwrap();
            validator_override_config
                .override_config_mut()
                .consensus
                .safety_rules
                .initial_safety_rules_config
                .overriding_identity_blob_paths_mut()
                .push(new_identity_path);
            validator_override_config.save_config(config_path).unwrap();

            info!("Restarting the node.");
            validator.start().unwrap();
            info!("Let it bake for 5 secs.");
            tokio::time::sleep(Duration::from_secs(5)).await;
            (operator_addr, new_pk, pop, operator_idx)
        } else {
            unreachable!()
        };
```
