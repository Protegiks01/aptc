# Audit Report

## Title
Cross-Executor Circular Dependency Deadlock in BoundedExecutor concurrent_map

## Summary
The `concurrent_map` function in the bounded-executor crate does not prevent mapper functions from spawning work on different BoundedExecutors, creating a potential for circular dependency deadlocks where both executors wait indefinitely for permits from each other, causing permanent consensus liveness failure.

## Finding Description

The `concurrent_map` function accepts a user-provided mapper closure that returns a Future, which is then spawned on a BoundedExecutor. [1](#0-0) 

The BoundedExecutor uses a semaphore to limit concurrent task execution. When `spawn()` is called, it blocks until a permit is acquired from the semaphore, then spawns the task which holds the permit until completion. [2](#0-1) 

The permits are released only when the spawned future completes. [3](#0-2) 

**The Vulnerability:**

If a mapper function spawns work on Executor B, and that work attempts to spawn back on Executor A (the original executor used by concurrent_map), a circular dependency deadlock occurs:

1. Executor A (capacity N) processes stream items via concurrent_map
2. Each item acquires a permit from A and spawns a future F_A
3. F_A spawns work on Executor B, acquiring permit from B, spawning future F_B
4. F_A waits for F_B to complete (holding permit from A)
5. F_B attempts to spawn work on Executor A
6. When A is at capacity, F_B blocks waiting for a permit from A
7. A won't release permits until F_A completes
8. F_A won't complete until F_B completes
9. F_B won't complete until it gets a permit from A
10. **Circular dependency deadlock â†’ permanent liveness failure**

**Current Codebase Analysis:**

In the consensus DAG handler, `concurrent_map` is used with one executor, and a separate executor is created for processing. [4](#0-3) 

While the current implementation doesn't exhibit the vulnerable pattern, both executors exist in the same scope and are accessible. The architecture creates conditions where future code modifications could accidentally introduce cross-executor dependencies, especially given that:

1. Multiple BoundedExecutors are passed to various components [5](#0-4) 
2. The same executor instance is cloned and shared across managers [6](#0-5) 
3. Components that use concurrent_map have access to other executors in scope

## Impact Explanation

**High Severity** - This vulnerability causes permanent consensus liveness failure:

1. **Total Loss of Liveness**: Once the deadlock occurs, affected validator nodes cannot process new blocks, leading to consensus stalling
2. **Non-Recoverable Without Restart**: The deadlock is permanent and requires node restart to recover
3. **Consensus Protocol Impact**: If enough validators experience this deadlock, the network cannot reach quorum for block commits
4. **No Automatic Recovery**: Unlike transient issues, deadlocked threads will wait indefinitely with no timeout mechanism

This meets the **High Severity** criteria per Aptos Bug Bounty: "Validator node slowdowns" and "Significant protocol violations" (permanent liveness loss).

## Likelihood Explanation

**Current Likelihood: Low** - The vulnerable code pattern does not currently exist in the production codebase. The current usage in dag_handler.rs keeps the executors separate.

**Future Likelihood: Medium-High** - The conditions for accidental introduction are present:
- Multiple executors exist in the same scope in consensus-critical code
- No compile-time or runtime safeguards prevent the pattern
- The async/await syntax makes it easy to accidentally nest executor spawns
- Code refactoring or feature additions could introduce the pattern unknowingly
- The library provides no documentation warning about this hazard

## Recommendation

**Immediate Fix**: Add runtime detection and prevention of cross-executor dependencies:

```rust
// In executor.rs
pub struct BoundedExecutor {
    semaphore: Arc<Semaphore>,
    executor: Handle,
    executor_id: Arc<usize>, // Add unique ID
}

static EXECUTOR_ID_COUNTER: AtomicUsize = AtomicUsize::new(0);

impl BoundedExecutor {
    pub fn new(capacity: usize, executor: Handle) -> Self {
        let semaphore = Arc::new(Semaphore::new(capacity));
        let executor_id = Arc::new(EXECUTOR_ID_COUNTER.fetch_add(1, Ordering::SeqCst));
        Self {
            semaphore,
            executor,
            executor_id,
        }
    }

    pub async fn spawn<F>(&self, future: F) -> JoinHandle<F::Output>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        // Check if we're already inside a spawn from a different executor
        CURRENT_EXECUTOR_ID.with(|current| {
            if let Some(current_id) = *current.borrow() {
                if current_id != *self.executor_id {
                    panic!("Cross-executor spawn detected: spawning on executor {} while inside executor {}", 
                           *self.executor_id, current_id);
                }
            }
        });

        let permit = self.acquire_permit().await;
        let executor_id = self.executor_id.clone();
        self.executor.spawn(async move {
            CURRENT_EXECUTOR_ID.with(|current| {
                *current.borrow_mut() = Some(*executor_id);
            });
            let result = future.await;
            CURRENT_EXECUTOR_ID.with(|current| {
                *current.borrow_mut() = None;
            });
            drop(permit);
            result
        })
    }
}

thread_local! {
    static CURRENT_EXECUTOR_ID: RefCell<Option<usize>> = RefCell::new(None);
}
```

**Alternative Fix**: Document the hazard and add linting rules to detect the pattern during code review.

**Long-term Fix**: Re-architect to use a single shared executor pool with priority queues instead of multiple independent executors.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use tokio::runtime::Runtime;
    use futures::stream;
    
    #[test]
    #[should_panic(timeout = "5s")]
    fn test_cross_executor_deadlock() {
        let rt = Runtime::new().unwrap();
        let handle = rt.handle().clone();
        
        // Create two executors with capacity 2
        let executor_a = BoundedExecutor::new(2, handle.clone());
        let executor_b = BoundedExecutor::new(2, handle.clone());
        
        let executor_a_clone = executor_a.clone();
        let executor_b_clone = executor_b.clone();
        
        rt.block_on(async move {
            // Create a stream with 3 items
            let stream = stream::iter(vec![1, 2, 3]);
            
            // Use concurrent_map with executor_a
            // Mapper spawns work on executor_b, which spawns back on executor_a
            let result = concurrent_map(stream, executor_a_clone, move |_item| {
                let exec_b = executor_b_clone.clone();
                let exec_a = executor_a.clone();
                async move {
                    // Spawn on executor B
                    exec_b.spawn(async move {
                        // Try to spawn back on executor A
                        exec_a.spawn(async move {
                            // This causes deadlock when A is full
                            tokio::time::sleep(Duration::from_millis(100)).await;
                        }).await
                    }).await
                }
            })
            .collect::<Vec<_>>()
            .await;
            
            // This will never complete - deadlock occurs
            println!("Result: {:?}", result);
        });
    }
}
```

**Expected Behavior**: Test times out after 5 seconds, demonstrating permanent deadlock.

**Notes**

While I've identified this as a genuine architectural vulnerability in the BoundedExecutor design, the current production code does not contain an exploitable instance of this pattern. The vulnerability is **latent** - it exists as a design flaw that could be accidentally introduced through future code changes, especially given that multiple executors are used in consensus-critical paths with overlapping scopes.

The risk is real because:
1. The library provides no safeguards against this pattern
2. Multiple executors exist in consensus code where this could be introduced
3. The async/await syntax makes nested spawns easy to write accidentally
4. The consequence (permanent liveness failure) is severe

However, this finding does not meet the strict validation criterion of being "exploitable by unprivileged attacker" in the current codebase state, as there is no present code path where an external attacker can trigger the deadlock pattern.

### Citations

**File:** crates/bounded-executor/src/concurrent_stream.rs (L10-35)
```rust
pub fn concurrent_map<St, Fut, F>(
    stream: St,
    executor: BoundedExecutor,
    mut mapper: F,
) -> impl FusedStream<Item = Fut::Output>
where
    St: Stream,
    F: FnMut(St::Item) -> Fut + Send,
    Fut: Future + Send + 'static,
    Fut::Output: Send + 'static,
{
    stream
        .flat_map_unordered(None, move |item| {
            let future = mapper(item);
            let executor = executor.clone();
            stream::once(
                #[allow(clippy::async_yields_async)]
                async move { executor.spawn(future).await }.boxed(),
            )
            .boxed()
        })
        .flat_map_unordered(None, |handle| {
            stream::once(async move { handle.await.expect("result") }.boxed()).boxed()
        })
        .fuse()
}
```

**File:** crates/bounded-executor/src/executor.rs (L41-52)
```rust
    /// Spawn a [`Future`] on the `BoundedExecutor`. This function is async and
    /// will block if the executor is at capacity until one of the other spawned
    /// futures completes. This function returns a [`JoinHandle`] that the caller
    /// can `.await` on for the results of the [`Future`].
    pub async fn spawn<F>(&self, future: F) -> JoinHandle<F::Output>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        let permit = self.acquire_permit().await;
        self.executor.spawn(future_with_permit(future, permit))
    }
```

**File:** crates/bounded-executor/src/executor.rs (L98-109)
```rust
/// Wrap a `Future` so it releases the spawn permit back to the semaphore when
/// it completes.
fn future_with_permit<F>(future: F, permit: OwnedSemaphorePermit) -> impl Future<Output = F::Output>
where
    F: Future + Send + 'static,
    F::Output: Send + 'static,
{
    future.map(move |ret| {
        drop(permit);
        ret
    })
}
```

**File:** consensus/src/dag/dag_handler.rs (L88-127)
```rust
        // TODO: feed in the executor based on verification Runtime
        let mut verified_msg_stream = concurrent_map(
            dag_rpc_rx,
            executor.clone(),
            move |rpc_request: IncomingDAGRequest| {
                let epoch_state = epoch_state.clone();
                async move {
                    let epoch = rpc_request.req.epoch();
                    let result = rpc_request
                        .req
                        .try_into()
                        .and_then(|dag_message: DAGMessage| {
                            monitor!(
                                "dag_message_verify",
                                dag_message.verify(rpc_request.sender, &epoch_state.verifier)
                            )?;
                            Ok(dag_message)
                        });
                    (result, epoch, rpc_request.sender, rpc_request.responder)
                }
            },
        );

        let dag_driver_clone = dag_driver.clone();
        let node_receiver_clone = node_receiver.clone();
        let handle = tokio::spawn(async move {
            while let Some(new_round) = new_round_event.recv().await {
                monitor!("dag_on_new_round_event", {
                    dag_driver_clone.enter_new_round(new_round).await;
                    node_receiver_clone.gc();
                });
            }
        });
        defer!(handle.abort());

        let mut futures = FuturesUnordered::new();
        // A separate executor to ensure the message verification sender (above) and receiver (below) are
        // not blocking each other.
        // TODO: make this configurable
        let executor = BoundedExecutor::new(8, Handle::current());
```

**File:** consensus/src/pipeline/execution_client.rs (L249-257)
```rust
            self.bounded_executor.clone(),
            &self.consensus_config.rand_rb_config,
        );

        tokio::spawn(rand_manager.start(
            ordered_block_rx,
            rand_msg_rx,
            reset_rand_manager_rx,
            self.bounded_executor.clone(),
```

**File:** consensus/src/pipeline/execution_client.rs (L292-300)
```rust
            self.bounded_executor.clone(),
            &self.consensus_config.rand_rb_config,
        );

        tokio::spawn(secret_share_manager.start(
            ordered_block_rx,
            secret_sharing_msg_rx,
            reset_secret_share_manager_rx,
            self.bounded_executor.clone(),
```
