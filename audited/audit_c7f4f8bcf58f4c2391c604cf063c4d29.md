# Audit Report

## Title
Permanent Peer Monitoring Deadlock Due to Uncompleted Request State Machine Transition

## Summary
The peer monitoring service's `RequestTracker` state machine can enter a permanent deadlock state when `request_completed()` is never called after `request_started()`, preventing all future monitoring requests to the affected peer. This occurs because spawned async tasks that handle `request_completed()` can fail to complete due to panics, runtime shutdown, or task cancellation, while the `JoinHandle` is immediately dropped without being awaited.

## Finding Description

The peer monitoring service tracks request states using a `RequestTracker` that maintains an `in_flight_request` boolean flag. The state machine has a critical flaw in its transition management: [1](#0-0) 

The `new_request_required()` method enforces that no new request can be sent if one is already in-flight: [2](#0-1) 

The vulnerability exists in how requests are initiated and completed:

**Step 1**: `request_started()` is called synchronously BEFORE spawning the async task: [3](#0-2) 

**Step 2**: An async task is spawned that will eventually call `request_completed()`: [4](#0-3) 

**Step 3**: The returned `JoinHandle` is immediately dropped (not awaited): [5](#0-4) 

This creates several failure scenarios:

1. **Task Panic Before Completion**: If any code between lines 101-120 in the async task panics (network timeout panic, time service panic, ID generator overflow, etc.), the task aborts and `request_completed()` at line 121 never executes.

2. **Runtime Shutdown**: If the tokio runtime shuts down during node restart or crash while tasks are in-flight, all pending tasks are cancelled and `request_completed()` never executes.

3. **RwLock Poisoning Cascade**: If a panic occurs while holding the write lock on `request_tracker` (e.g., during `request_completed()` call), the `aptos_infallible::RwLock` becomes poisoned: [6](#0-5) 

All subsequent access attempts will panic, breaking the entire peer monitoring loop for ALL peers.

There is no timeout mechanism or cleanup for stuck `in_flight_request` states. The only recovery is peer disconnection, which triggers garbage collection: [7](#0-6) 

## Impact Explanation

This vulnerability meets **High Severity** criteria per the Aptos bug bounty program for the following reasons:

**Validator Node Slowdowns**: Affected validator nodes gradually lose accurate peer health information. Stale latency data, network info, and node info degrade the quality of peer selection decisions, impacting:
- Consensus message routing efficiency
- State synchronization peer selection
- Network connection management

**Significant Protocol Violations**: The peer monitoring service is designed to continuously refresh peer states. Breaking this invariant means nodes operate with incomplete or outdated network topology information, potentially affecting:
- Consensus participation efficiency
- State sync reliability
- Network resilience to peer failures

**Cumulative Degradation**: Unlike a single-point failure, this issue can affect multiple peers over time as tasks fail for various reasons (network issues, transient panics, node restarts), progressively degrading the node's network view.

**RwLock Poisoning Escalation**: In the worst case, if a panic occurs while holding the `request_tracker` write lock, the entire peer monitoring system crashes for all peers, constituting a complete failure of the monitoring subsystem.

## Likelihood Explanation

This vulnerability has **MODERATE to HIGH** likelihood:

**Common Triggers**:
- Node restarts during in-flight requests (very common in production)
- Network timeouts or disconnections causing task panics
- Transient panics in time service, ID generation, or network client code
- Runtime shutdown during graceful/ungraceful node termination

**Cumulative Risk**: Even if individual task failure rate is low (e.g., 0.1%), over thousands of monitoring cycles across dozens of peers, the probability of at least one deadlock approaches certainty over time.

**No Recovery Mechanism**: Once deadlocked, the state persists until peer disconnection, which may not happen for hours or days on stable connections.

**Observable Impact**: Operators would notice stale peer metrics but might attribute it to normal network variability rather than a code bug, allowing the issue to persist undetected.

## Recommendation

Implement proper task lifecycle management and recovery mechanisms:

**Solution 1: Ensure request_completed() is Always Called**

Wrap the task execution in a guard that ensures cleanup:
```rust
pub fn refresh_peer_state_key(...) -> Result<JoinHandle<()>, Error> {
    let request_tracker = self.get_request_tracker(peer_state_key)?;
    request_tracker.write().request_started();
    
    let request_tracker_cleanup = request_tracker.clone();
    let request_task = async move {
        // Ensure cleanup happens even on panic
        let _cleanup_guard = scopeguard::guard((), |_| {
            request_tracker_cleanup.write().request_completed();
        });
        
        sleep(Duration::from_millis(request_jitter_ms)).await;
        let start_time = time_service.now();
        
        let request_id = request_id_generator.next();
        let monitoring_service_response = network::send_request_to_peer(...).await;
        
        let request_duration_secs = start_time.elapsed().as_secs_f64();
        
        // Process response...
    };
    
    // ... spawn task
}
```

**Solution 2: Add Timeout-Based State Reset**

Add automatic timeout recovery in `new_request_required()`:
```rust
pub fn new_request_required(&self) -> bool {
    if self.in_flight_request() {
        // Check if request has been in-flight too long
        if let Some(last_request_time) = self.last_request_time {
            let timeout_duration = Duration::from_secs(300); // 5 minutes
            if self.time_service.now() > last_request_time.add(timeout_duration) {
                // Request timed out, allow new request
                return true;
            }
        }
        return false;
    }
    // ... rest of logic
}
```

**Solution 3: Await JoinHandle or Store for Cleanup**

Instead of dropping the JoinHandle, either await it or store it for later cleanup:
```rust
// Store join handles for cleanup
pub struct PeerState {
    state_entries: Arc<RwLock<HashMap<PeerStateKey, Arc<RwLock<PeerStateValue>>>>>,
    in_flight_tasks: Arc<RwLock<Vec<JoinHandle<()>>>>,
}

// Periodically clean up completed tasks
async fn cleanup_completed_tasks(peer_state: &PeerState) {
    let mut tasks = peer_state.in_flight_tasks.write();
    tasks.retain(|handle| !handle.is_finished());
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_time_service::TimeService;
    use std::sync::Arc;
    use std::time::Duration;
    
    #[tokio::test]
    async fn test_request_tracker_deadlock_on_panic() {
        // Create request tracker
        let time_service = TimeService::mock();
        let request_tracker = Arc::new(RwLock::new(
            RequestTracker::new(100, time_service.clone())
        ));
        
        // Initial state: new request required
        assert!(request_tracker.read().new_request_required());
        
        // Start a request
        request_tracker.write().request_started();
        
        // Verify in-flight request blocks new requests
        assert!(!request_tracker.read().new_request_required());
        
        // Simulate async task that panics before calling request_completed()
        let tracker_clone = request_tracker.clone();
        let handle = tokio::spawn(async move {
            tokio::time::sleep(Duration::from_millis(10)).await;
            // Simulate panic before request_completed() is called
            panic!("Simulated network error");
            // This line never executes:
            // tracker_clone.write().request_completed();
        });
        
        // Task panics and is aborted
        let _ = handle.await;
        
        // Advance time beyond normal request interval
        time_service.into_mock().advance(Duration::from_millis(200));
        
        // BUG: new_request_required() still returns false!
        // The state machine is permanently deadlocked
        assert!(!request_tracker.read().new_request_required(), 
                "VULNERABILITY: Request tracker is deadlocked after task panic");
        
        // Only way to recover is to drop and recreate the tracker
        // (equivalent to peer disconnect/reconnect)
    }
    
    #[tokio::test]
    async fn test_request_tracker_normal_flow() {
        // Show that normal flow works correctly
        let time_service = TimeService::mock();
        let request_tracker = Arc::new(RwLock::new(
            RequestTracker::new(100, time_service.clone())
        ));
        
        assert!(request_tracker.read().new_request_required());
        
        request_tracker.write().request_started();
        assert!(!request_tracker.read().new_request_required());
        
        // Simulate successful task completion
        let tracker_clone = request_tracker.clone();
        let handle = tokio::spawn(async move {
            tokio::time::sleep(Duration::from_millis(10)).await;
            tracker_clone.write().request_completed();
        });
        
        handle.await.unwrap();
        
        // After completion, time advancement allows new requests
        time_service.into_mock().advance(Duration::from_millis(200));
        assert!(request_tracker.read().new_request_required());
    }
}
```

## Notes

This vulnerability demonstrates a critical gap in asynchronous task lifecycle management. The peer monitoring service's design assumes tasks always complete successfully, but real-world scenarios involving panics, runtime shutdowns, and network failures violate this assumption. The lack of defensive programming (timeouts, cleanup guards, or join handle management) creates a permanently unrecoverable state that degrades validator node functionality over time.

The severity is compounded by the fact that the issue is **silent and cumulative** - affected nodes continue operating but with progressively degrading network visibility, making diagnosis difficult and allowing the issue to persist in production environments.

### Citations

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L61-72)
```rust
    pub fn request_started(&mut self) {
        // Mark the request as in-flight
        self.in_flight_request = true;

        // Update the last request time
        self.last_request_time = Some(self.time_service.now());
    }

    /// Updates the state to mark a request as having completed
    pub fn request_completed(&mut self) {
        self.in_flight_request = false;
    }
```

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L76-90)
```rust
    pub fn new_request_required(&self) -> bool {
        // There's already an in-flight request. A new one should not be sent.
        if self.in_flight_request() {
            return false;
        }

        // Otherwise, check the last request time for freshness
        match self.last_request_time {
            Some(last_request_time) => {
                self.time_service.now()
                    > last_request_time.add(Duration::from_micros(self.request_interval_usec))
            },
            None => true, // A request should be sent immediately
        }
    }
```

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L80-83)
```rust
        // Mark the request as having started. We do this here to prevent
        // the monitor loop from selecting the same peer state key concurrently.
        let request_tracker = self.get_request_tracker(peer_state_key)?;
        request_tracker.write().request_started();
```

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L98-121)
```rust
        let request_task = async move {
            // Add some amount of jitter before sending the request.
            // This helps to prevent requests from becoming too bursty.
            sleep(Duration::from_millis(request_jitter_ms)).await;

            // Start the request timer
            let start_time = time_service.now();

            // Send the request to the peer and wait for a response
            let request_id = request_id_generator.next();
            let monitoring_service_response = network::send_request_to_peer(
                peer_monitoring_client,
                &peer_network_id,
                request_id,
                monitoring_service_request.clone(),
                request_timeout_ms,
            )
            .await;

            // Stop the timer and calculate the duration
            let request_duration_secs = start_time.elapsed().as_secs_f64();

            // Mark the in-flight request as now complete
            request_tracker.write().request_completed();
```

**File:** peer-monitoring-service/client/src/peer_states/mod.rs (L56-68)
```rust
            let should_refresh_peer_state_key = request_tracker.read().new_request_required();
            if should_refresh_peer_state_key {
                peer_state.refresh_peer_state_key(
                    monitoring_service_config,
                    &peer_state_key,
                    peer_monitoring_client.clone(),
                    *peer_network_id,
                    peer_metadata.clone(),
                    peer_monitor_state.request_id_generator.clone(),
                    time_service.clone(),
                    runtime.clone(),
                )?;
            }
```

**File:** crates/aptos-infallible/src/rwlock.rs (L25-30)
```rust
    /// lock the rwlock in write mode
    pub fn write(&self) -> RwLockWriteGuard<'_, T> {
        self.0
            .write()
            .expect("Cannot currently handle a poisoned lock")
    }
```

**File:** peer-monitoring-service/client/src/lib.rs (L180-202)
```rust
/// Garbage collects peer states for peers that are no longer connected
fn garbage_collect_peer_states(
    peer_monitor_state: &PeerMonitorState,
    connected_peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) {
    // Get the set of peers with existing states
    let peers_with_existing_states: Vec<PeerNetworkId> = peer_monitor_state
        .peer_states
        .read()
        .keys()
        .cloned()
        .collect();

    // Remove the states for disconnected peers
    for peer_network_id in peers_with_existing_states {
        if !connected_peers_and_metadata.contains_key(&peer_network_id) {
            peer_monitor_state
                .peer_states
                .write()
                .remove(&peer_network_id);
        }
    }
}
```
