# Audit Report

## Title
Race Condition in sync_info() Causes Invalid Certificate Round Ordering Leading to Consensus Synchronization Failures

## Summary
The `BlockStore::sync_info()` method reads three certificates through separate, non-atomic read lock acquisitions, allowing concurrent updates to create an inconsistent `SyncInfo` that violates round ordering invariants and causes peer synchronization failures.

## Finding Description
The AptosBFT consensus protocol requires strict ordering between certificate rounds (HQC.round >= HOC.round >= HCC.round). The `SyncInfo` struct enforces this through validation that explicitly checks these invariants. [1](#0-0) 

However, `BlockStore::sync_info()` violates atomicity by calling three separate methods, each acquiring and releasing the read lock independently. [2](#0-1) 

Each certificate accessor method acquires its own read lock: [3](#0-2) 

**Race Condition Scenario:**

1. Thread A (generating consensus message) calls `sync_info()` and reads `highest_quorum_cert()` returning round 10, then releases the read lock
2. Thread B (pipeline execution) calls `insert_quorum_cert()` with a QC certifying round 12 that commits round 11, acquiring a write lock [4](#0-3) 
3. Thread B updates `highest_quorum_cert` to round 12 and `highest_ordered_cert` to round 11 atomically under the write lock, then releases it
4. Thread A continues and reads `highest_ordered_cert()` returning round 11, then reads `highest_commit_cert()`
5. Thread A constructs `SyncInfo` with HQC.round=10, HOC.round=11, violating the invariant 10 >= 11

The asynchronous pipeline callback mechanism enables this race. [5](#0-4) 

When this invalid `SyncInfo` is broadcast in consensus messages, peers reject it during verification with "HQC has lower round than HOC", causing synchronization failures.

## Impact Explanation
This vulnerability qualifies as **HIGH severity** per Aptos bug bounty criteria:

- **Validator Node Slowdowns**: Affected nodes repeatedly construct invalid `SyncInfo`, triggering continuous peer rejections and retry cycles, degrading consensus performance
- **Significant Protocol Violations**: Breaks fundamental consensus synchronization invariants required for AptosBFT operation
- **Liveness Degradation**: Nodes experiencing this race cannot effectively synchronize with peers, reducing the network's active validator participation

The impact is limited to liveness rather than safetyâ€”invalid `SyncInfo` is rejected by peers rather than causing state divergence. However, in a production network processing high transaction throughput, repeated synchronization failures constitute a significant availability issue affecting consensus progression.

## Likelihood Explanation
**Likelihood: Medium to High**

The race condition occurs naturally during normal network operation when:
- Nodes generate proposals, votes, or timeout messages (calling `sync_info()`)
- Concurrently, the execution pipeline processes block commitments (calling certificate update methods)
- The narrow time window between the three read lock acquisitions allows interleaving with write operations

**Triggering Factors:**
- High transaction throughput increases pipeline callback frequency
- Multiple validators proposing simultaneously increases `sync_info()` call frequency
- Multi-core validator systems enable true concurrent execution
- Larger gaps between certificate rounds make violations more observable

The race window is measured in microseconds, but in production networks processing thousands of TPS with aggressive pipelining and multiple CPU cores, this condition will occur regularly during peak load periods.

## Recommendation
Acquire a single read lock and atomically read all three certificates within that lock scope:

```rust
fn sync_info(&self) -> SyncInfo {
    let inner = self.inner.read();
    SyncInfo::new_decoupled(
        inner.highest_quorum_cert().as_ref().clone(),
        inner.highest_ordered_cert().as_ref().clone(),
        inner.highest_commit_cert().as_ref().clone(),
        inner.highest_2chain_timeout_cert()
            .map(|tc| tc.as_ref().clone()),
    )
}
```

This ensures all three certificate reads form a consistent snapshot by holding the lock throughout the entire operation, preventing interleaving with concurrent write operations.

## Proof of Concept
While a full concurrent PoC requires intricate timing control, the vulnerability is demonstrable through code inspection:

1. The non-atomic implementation is visible at [2](#0-1) 
2. Concurrent updates occur via [6](#0-5) 
3. The validation that rejects inconsistent state is at [7](#0-6) 

Under load testing with multiple concurrent proposal generations and pipeline commits, invalid `SyncInfo` instances would be observable in consensus message rejections and synchronization failure logs.

## Notes
This is a classic atomicity violation bug where multiple related state variables are read without maintaining lock consistency across all reads. The vulnerability exists in the current implementation and can manifest during normal high-throughput operation without requiring any attacker action. The self-correcting nature (subsequent `sync_info()` calls may succeed) does not eliminate the impact, as repeated failures still degrade consensus liveness and validator participation.

### Citations

**File:** consensus/consensus-types/src/sync_info.rs (L152-165)
```rust
        ensure!(
            self.highest_quorum_cert.certified_block().round()
                >= self.highest_ordered_cert().commit_info().round(),
            "HQC has lower round than HOC"
        );

        ensure!(
            self.highest_ordered_round() >= self.highest_commit_round(),
            format!(
                "HOC {} has lower round than HLI {}",
                self.highest_ordered_cert(),
                self.highest_commit_cert()
            )
        );
```

**File:** consensus/src/block_storage/block_store.rs (L475-489)
```rust
            let callback = Box::new(
                move |finality_proof: WrappedLedgerInfo,
                      commit_decision: LedgerInfoWithSignatures| {
                    if let Some(tree) = block_tree.upgrade() {
                        tree.write().commit_callback(
                            storage,
                            id,
                            round,
                            finality_proof,
                            commit_decision,
                            window_size,
                        );
                    }
                },
            );
```

**File:** consensus/src/block_storage/block_store.rs (L664-674)
```rust
    fn highest_quorum_cert(&self) -> Arc<QuorumCert> {
        self.inner.read().highest_quorum_cert()
    }

    fn highest_ordered_cert(&self) -> Arc<WrappedLedgerInfo> {
        self.inner.read().highest_ordered_cert()
    }

    fn highest_commit_cert(&self) -> Arc<WrappedLedgerInfo> {
        self.inner.read().highest_commit_cert()
    }
```

**File:** consensus/src/block_storage/block_store.rs (L680-688)
```rust
    fn sync_info(&self) -> SyncInfo {
        SyncInfo::new_decoupled(
            self.highest_quorum_cert().as_ref().clone(),
            self.highest_ordered_cert().as_ref().clone(),
            self.highest_commit_cert().as_ref().clone(),
            self.highest_2chain_timeout_cert()
                .map(|tc| tc.as_ref().clone()),
        )
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L349-386)
```rust
    pub(super) fn insert_quorum_cert(&mut self, qc: QuorumCert) -> anyhow::Result<()> {
        let block_id = qc.certified_block().id();
        let qc = Arc::new(qc);

        // Safety invariant: For any two quorum certificates qc1, qc2 in the block store,
        // qc1 == qc2 || qc1.round != qc2.round
        // The invariant is quadratic but can be maintained in linear time by the check
        // below.
        precondition!({
            let qc_round = qc.certified_block().round();
            self.id_to_quorum_cert.values().all(|x| {
                (*(*x).ledger_info()).ledger_info().consensus_data_hash()
                    == (*(*qc).ledger_info()).ledger_info().consensus_data_hash()
                    || x.certified_block().round() != qc_round
            })
        });

        match self.get_block(&block_id) {
            Some(block) => {
                if block.round() > self.highest_certified_block().round() {
                    self.highest_certified_block_id = block.id();
                    self.highest_quorum_cert = Arc::clone(&qc);
                }
            },
            None => bail!("Block {} not found", block_id),
        }

        self.id_to_quorum_cert
            .entry(block_id)
            .or_insert_with(|| Arc::clone(&qc));

        if self.highest_ordered_cert.commit_info().round() < qc.commit_info().round() {
            // Question: We are updating highest_ordered_cert but not highest_ordered_root. Is that fine?
            self.highest_ordered_cert = Arc::new(qc.into_wrapped_ledger_info());
        }

        Ok(())
    }
```
