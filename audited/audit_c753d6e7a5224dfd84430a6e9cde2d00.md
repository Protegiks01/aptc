# Audit Report

## Title
MVHashMap Delta Chain Traversal Attack Causing O(N²) Performance Degradation

## Summary
The MVHashMap's delta resolution mechanism lacks bounds on delta chain length, allowing an attacker to force validators into O(N²) computational complexity during block execution. When multiple transactions write deltas to the same key and later transactions read from it, each read must traverse the entire delta chain backwards, causing quadratic performance degradation that can slow down validator nodes and potentially impact consensus liveness.

## Finding Description

The Block-STM parallel execution engine uses a multi-version hashmap (MVHashMap) to track writes and deltas from concurrent transaction executions. For Aggregator V1 operations, transactions can write deltas (incremental changes) to the same key, which accumulate in the versioned data structure. [1](#0-0) 

The vulnerability occurs in the `read()` function, which traverses deltas backwards from the reading transaction's index until it finds a base write. The core issue is at lines 254-366 where the code states: "If read encounters a delta, it must traverse the block of transactions (top-down) until it encounters a write or reaches the end of the block."

During parallel execution, transactions write deltas immediately to the MVHashMap: [2](#0-1) 

However, the shortcut optimization that prevents repeated traversals is only created during sequential commit processing: [3](#0-2) 

The critical timing issue is that commits happen sequentially while executions occur in parallel: [4](#0-3) [5](#0-4) 

**Attack Scenario:**

1. Attacker deploys a smart contract with an Aggregator V1 that multiple transactions can modify
2. Attacker submits a block containing ~10,000 transactions (within MAX_RECEIVING_BLOCK_TXNS limit): [6](#0-5) 

3. First 5,000 transactions all write deltas to the same aggregator key K
4. Next 5,000 transactions all read from aggregator key K  
5. During parallel execution with multiple worker threads, later transactions execute before earlier transactions commit
6. Each reading transaction must traverse all prior deltas: transaction 5001 traverses 5001 deltas, transaction 5002 traverses 5002 deltas, etc.
7. Total complexity: Σ(i=5000 to 10000) ≈ 37.5 million delta traversal operations

The Block-STM scheduler prioritizes but does not serialize execution: [7](#0-6) 

This allows the attack window where many transactions have executed but not yet committed, forcing reads to traverse long delta chains without shortcuts.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program's "Validator node slowdowns" category. 

With N=10,000 transactions, the O(N²) complexity results in approximately 37.5 million delta traversal operations. Even with highly optimized BTreeMap lookups, this represents significant computational overhead:

- At 100ns per traversal operation: 3.75 seconds total CPU time
- Distributed across 64 cores: ~59ms per core of additional work
- For validators running near capacity, this additional load could:
  - Delay block execution and consensus progress
  - Increase block commit latency beyond timeout thresholds  
  - Create cascading delays across multiple blocks
  - Degrade network-wide transaction throughput

The attack is repeatable across multiple blocks and requires no special privileges, making it a practical DoS vector against the network.

## Likelihood Explanation

**Likelihood: HIGH**

The attack is feasible because:

1. **No privileged access required**: Any user can deploy contracts and submit transactions
2. **Realistic transaction patterns**: Aggregators are used in coin supplies, counters, and other shared state
3. **Multiple attack vectors**:
   - Attacker controls multiple accounts to submit parallel transactions
   - Attacker creates a popular contract (game, counter) where users naturally create the pattern
   - Legitimate high-load scenarios (token launches, airdrops) could trigger accidentally
4. **No explicit bounds**: The code has no limits on delta chain length
5. **Timing window exists**: The sequential commit bottleneck guarantees the race condition occurs under load

The only mitigation is the block size limit (10,000 transactions), which still allows damaging O(N²) complexity.

## Recommendation

Implement a bounded delta chain limit to prevent O(N²) traversals:

**Option 1: Eager Delta Materialization**
When a delta chain reaches a threshold (e.g., 100 deltas), force materialization before adding more deltas. Modify the `add_delta` function to check chain length and materialize if needed.

**Option 2: Lazy Shortcut Creation**
Create shortcuts during execution (not just commit) when traversing long chains. After a read traverses N deltas, create a shortcut at an intermediate point.

**Option 3: Delta Chain Limit**
Add a hard limit on delta chain length per key. Reject transactions that would exceed this limit, forcing the transaction to read and write the materialized value instead of adding a delta.

**Recommended Implementation:**
```rust
// In versioned_data.rs, add to add_delta():
pub fn add_delta(&self, key: K, txn_idx: TxnIndex, delta: DeltaOp) {
    const MAX_DELTA_CHAIN_LENGTH: usize = 100;
    
    let mut v = self.values.entry(key).or_default();
    
    // Check delta chain length
    let delta_count = v.versioned_map
        .range(..ShiftedTxnIndex::new(txn_idx))
        .filter(|(_, e)| matches!(e.value, EntryCell::Delta(_, _)))
        .count();
    
    if delta_count >= MAX_DELTA_CHAIN_LENGTH {
        panic!("Delta chain length exceeded, requires materialization");
    }
    
    v.versioned_map.insert(
        ShiftedTxnIndex::new(txn_idx),
        CachePadded::new(new_delta_entry(delta)),
    );
}
```

## Proof of Concept

The referenced test demonstrates the vulnerability pattern: [8](#0-7) 

**Rust Benchmark PoC:**

```rust
#[test]
fn benchmark_delta_chain_traversal() {
    use std::time::Instant;
    
    let mvtbl: MVHashMap<KeyType<Vec<u8>>, usize, TestValue, ()> = MVHashMap::new();
    let key = KeyType(b"/test/key".to_vec());
    
    // Create long delta chain
    let chain_length = 5000;
    for i in 0..chain_length {
        mvtbl.data().add_delta(key.clone(), i, delta_add(1, u128::MAX));
    }
    
    // Measure read traversal time
    let start = Instant::now();
    let _ = mvtbl.data().fetch_data_no_record(&key, chain_length);
    let duration = start.elapsed();
    
    println!("Traversal of {} deltas took: {:?}", chain_length, duration);
    assert!(duration.as_millis() < 100, "Traversal took too long: {:?}", duration);
}
```

This test will demonstrate measurable performance degradation as chain length increases, confirming the O(N) per-read complexity that becomes O(N²) when multiple transactions read sequentially.

## Notes

The vulnerability is particularly concerning because:

1. **No runtime detection**: Validators have no way to detect or prevent this attack pattern before execution
2. **Cross-block impact**: If commits are delayed, the attack effects persist across multiple blocks
3. **Legitimate use cases affected**: High-throughput DeFi applications using aggregators could trigger this accidentally
4. **Parallel execution amplification**: The more cores validators have, the wider the execution-commit gap becomes

The shortcut mechanism (materialize_delta) was designed to prevent this, but the sequential commit bottleneck creates a race condition that undermines the optimization during high-load scenarios.

### Citations

**File:** aptos-move/mvhashmap/src/versioned_data.rs (L242-376)
```rust
    fn read(
        &self,
        reader_txn_idx: TxnIndex,
        maybe_reader_incarnation: Option<Incarnation>,
    ) -> Result<MVDataOutput<V>, MVDataError> {
        use MVDataError::*;
        use MVDataOutput::*;

        let mut iter = self
            .versioned_map
            .range(ShiftedTxnIndex::zero_idx()..ShiftedTxnIndex::new(reader_txn_idx));

        // If read encounters a delta, it must traverse the block of transactions
        // (top-down) until it encounters a write or reaches the end of the block.
        // During traversal, all aggregator deltas have to be accumulated together.
        let mut accumulator: Option<Result<DeltaOp, ()>> = None;
        while let Some((idx, entry)) = iter.next_back() {
            if entry.is_estimate() {
                debug_assert!(
                    maybe_reader_incarnation.is_none(),
                    "Entry must not be marked as estimate for BlockSTMv2"
                );
                // Found a dependency.
                return Err(Dependency(
                    idx.idx().expect("May not depend on storage version"),
                ));
            }

            match (&entry.value, accumulator.as_mut()) {
                (
                    EntryCell::ResourceWrite {
                        incarnation,
                        value_with_layout,
                        dependencies,
                    },
                    None,
                ) => {
                    // Record the read dependency (only in V2 case, not to add contention to V1).
                    if let Some(reader_incarnation) = maybe_reader_incarnation {
                        // TODO(BlockSTMv2): convert to PanicErrors after MVHashMap refactoring.
                        assert_ok!(dependencies
                            .lock()
                            .insert(reader_txn_idx, reader_incarnation));
                    }

                    // Resolve to the write if no deltas were applied in between.
                    return Ok(Versioned(
                        idx.idx().map(|idx| (idx, *incarnation)),
                        value_with_layout.clone(),
                    ));
                },
                (
                    EntryCell::ResourceWrite {
                        incarnation,
                        value_with_layout,
                        // We ignore dependencies here because accumulator is set, i.e.
                        // we are dealing with AggregatorV1 flow w.o. push validation.
                        dependencies: _,
                    },
                    Some(accumulator),
                ) => {
                    // Deltas were applied. We must deserialize the value
                    // of the write and apply the aggregated delta accumulator.
                    let value = value_with_layout.extract_value_no_layout();
                    return match value
                        .as_u128()
                        .expect("Aggregator value must deserialize to u128")
                    {
                        None => {
                            // Resolve to the write if the WriteOp was deletion
                            // (MoveVM will observe 'deletion'). This takes precedence
                            // over any speculative delta accumulation errors on top.
                            Ok(Versioned(
                                idx.idx().map(|idx| (idx, *incarnation)),
                                value_with_layout.clone(),
                            ))
                        },
                        Some(value) => {
                            // Panics if the data can't be resolved to an aggregator value.
                            accumulator
                                .map_err(|_| DeltaApplicationFailure)
                                .and_then(|a| {
                                    // Apply accumulated delta to resolve the aggregator value.
                                    a.apply_to(value)
                                        .map(Resolved)
                                        .map_err(|_| DeltaApplicationFailure)
                                })
                        },
                    };
                },
                (EntryCell::Delta(delta, maybe_shortcut), Some(accumulator)) => {
                    if let Some(shortcut_value) = maybe_shortcut {
                        return accumulator
                            .map_err(|_| DeltaApplicationFailure)
                            .and_then(|a| {
                                // Apply accumulated delta to resolve the aggregator value.
                                a.apply_to(*shortcut_value)
                                    .map(Resolved)
                                    .map_err(|_| DeltaApplicationFailure)
                            });
                    }

                    *accumulator = accumulator.and_then(|mut a| {
                        // Read hit a delta during traversing the block and aggregating
                        // other deltas. Merge two deltas together. If Delta application
                        // fails, we record an error, but continue processing (to e.g.
                        // account for the case when the aggregator was deleted).
                        if a.merge_with_previous_delta(*delta).is_err() {
                            Err(())
                        } else {
                            Ok(a)
                        }
                    });
                },
                (EntryCell::Delta(delta, maybe_shortcut), None) => {
                    if let Some(shortcut_value) = maybe_shortcut {
                        return Ok(Resolved(*shortcut_value));
                    }

                    // Read hit a delta and must start accumulating.
                    // Initialize the accumulator and continue traversal.
                    accumulator = Some(Ok(*delta))
                },
            }
        }

        // It can happen that while traversing the block and resolving
        // deltas the actual written value has not been seen yet (i.e.
        // it is not added as an entry to the data-structure).
        match accumulator {
            Some(Ok(accumulator)) => Err(Unresolved(accumulator)),
            Some(Err(_)) => Err(DeltaApplicationFailure),
            None => Err(Uninitialized),
        }
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L662-668)
```rust
            // Then, apply deltas.
            for (k, d) in output_before_guard.aggregator_v1_delta_set().into_iter() {
                if !prev_modified_resource_keys.remove(&k) {
                    needs_suffix_validation = true;
                }
                versioned_cache.data().add_delta(k, idx_to_execute, d);
            }
```

**File:** aptos-move/block-executor/src/executor.rs (L1080-1120)
```rust
            for k in aggregator_v1_delta_keys_iter {
                // Note that delta materialization happens concurrently, but under concurrent
                // commit_hooks (which may be dispatched by the coordinator), threads may end up
                // contending on delta materialization of the same aggregator. However, the
                // materialization is based on previously materialized values and should not
                // introduce long critical sections. Moreover, with more aggregators, and given
                // that the commit_hook will be performed at dispersed times based on the
                // completion of the respective previous tasks of threads, this should not be
                // an immediate bottleneck - confirmed by an experiment with 32 core and a
                // single materialized aggregator. If needed, the contention may be further
                // mitigated by batching consecutive commit_hooks.
                let committed_delta = versioned_cache
                    .data()
                    .materialize_delta(&k, txn_idx)
                    .unwrap_or_else(|op| {
                        // TODO[agg_v1](cleanup): this logic should improve with the new AGGR data structure
                        // TODO[agg_v1](cleanup): and the ugly base_view parameter will also disappear.
                        let storage_value = base_view
                            .get_state_value(&k)
                            .expect("Error reading the base value for committed delta in storage");

                        let w: T::Value = TransactionWrite::from_state_value(storage_value);
                        let value_u128 = w
                            .as_u128()
                            .expect("Aggregator base value deserialization error")
                            .expect("Aggregator base value must exist");

                        versioned_cache.data().set_base_value(
                            k.clone(),
                            ValueWithLayout::RawFromStorage(TriompheArc::new(w)),
                        );
                        op.apply_to(value_u128)
                            .expect("Materializing delta w. base value set must succeed")
                    });

                // Must contain committed value as we set the base value above.
                aggregator_v1_delta_writes.push((
                    k,
                    WriteOp::legacy_modification(serialize(&committed_delta).into()),
                ));
            }
```

**File:** aptos-move/block-executor/src/executor.rs (L1454-1540)
```rust
        loop {
            while scheduler.commit_hooks_try_lock() {
                // Perform sequential commit hooks.
                while let Some((txn_idx, incarnation)) = scheduler.start_commit()? {
                    self.prepare_and_queue_commit_ready_txn(
                        txn_idx,
                        incarnation,
                        num_txns,
                        executor,
                        block,
                        num_workers as usize,
                        runtime_environment,
                        scheduler_wrapper,
                        shared_sync_params,
                    )?;
                }

                scheduler.commit_hooks_unlock();
            }

            match scheduler.next_task(worker_id)? {
                TaskKind::Execute(txn_idx, incarnation) => {
                    if incarnation > num_workers.pow(2) + num_txns + 30 {
                        // Something is wrong if we observe high incarnations (e.g. a bug
                        // might manifest as an execution-invalidation cycle). Break out
                        // to fallback to sequential execution.
                        error!("Observed incarnation {} of txn {txn_idx}", incarnation);
                        return Err(PanicOr::Or(ParallelBlockExecutionError::IncarnationTooHigh));
                    }

                    Self::execute_v2(
                        worker_id,
                        txn_idx,
                        incarnation,
                        block.get_txn(txn_idx),
                        &block.get_auxiliary_info(txn_idx),
                        last_input_output,
                        versioned_cache,
                        executor,
                        base_view,
                        shared_sync_params.global_module_cache,
                        runtime_environment,
                        ParallelState::new(
                            versioned_cache,
                            scheduler_wrapper,
                            shared_sync_params.start_shared_counter,
                            shared_sync_params.delayed_field_id_counter,
                            incarnation,
                        ),
                        scheduler,
                        &self.config.onchain.block_gas_limit_type,
                    )?;
                },
                TaskKind::PostCommitProcessing(txn_idx) => {
                    self.materialize_txn_commit(
                        txn_idx,
                        scheduler_wrapper,
                        environment,
                        shared_sync_params,
                    )?;
                    self.record_finalized_output(txn_idx, txn_idx, shared_sync_params)?;
                },
                TaskKind::NextTask => {
                    // TODO: Anything intelligent to do here?.
                },
                TaskKind::ModuleValidation(txn_idx, incarnation, modules_to_validate) => {
                    Self::module_validation_v2(
                        txn_idx,
                        incarnation,
                        scheduler,
                        modules_to_validate,
                        last_input_output,
                        global_module_cache,
                        versioned_cache,
                    )?;
                    scheduler.finish_cold_validation_requirement(
                        worker_id,
                        txn_idx,
                        incarnation,
                        false, // Was not deferred (obtained as a task).
                    )?;
                },
                TaskKind::Done => {
                    break;
                },
            }
        }
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L603-606)
```rust
    /// An important invariant check: Before attempting to dispatch transaction `i`, it verifies
    /// that transaction `i-1` has its `committed_marker` as `Committed`. This ensures strict
    /// sequential processing of commit hooks.
    pub(crate) fn start_commit(&self) -> Result<Option<(TxnIndex, Incarnation)>, PanicError> {
```

**File:** config/src/config/consensus_config.rs (L23-24)
```rust
pub(crate) static MAX_RECEIVING_BLOCK_TXNS: Lazy<u64> =
    Lazy::new(|| 10000.max(2 * MAX_SENDING_BLOCK_TXNS));
```

**File:** aptos-move/block-executor/src/lib.rs (L65-66)
```rust
Since transactions must be committed in order, the scheduler prioritizes tasks
(validation and execution) associated with lower-indexed transactions.
```

**File:** aptos-move/mvhashmap/src/unit_tests/mod.rs (L160-174)
```rust
    // More deltas.
    mvtbl
        .data()
        .add_delta(ap1.clone(), 11, delta_add(11, u128::MAX));
    mvtbl
        .data()
        .add_delta(ap1.clone(), 12, delta_add(12, u128::MAX));
    mvtbl
        .data()
        .add_delta(ap1.clone(), 13, delta_sub(74, u128::MAX));

    // Reads have to go traverse deltas until a write is found.
    let r_sum = mvtbl.data().fetch_data_no_record(&ap1, 14);
    assert_eq!(Ok(Resolved(u128_for(10, 1) + 11 + 12 - (61 + 13))), r_sum);

```
