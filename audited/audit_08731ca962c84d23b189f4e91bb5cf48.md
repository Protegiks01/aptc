# Audit Report

## Title
Transaction Chunk Gap Detection Bypass Allows Silent Skipping of Versions During Restore

## Summary
The `loaded_chunk_stream()` function in transaction restore fails to validate that the first chunk begins at the expected version when `first_version` is explicitly provided. The continuity check only validates gaps between consecutive chunks but skips validation for the initial chunk, allowing silent omission of transaction ranges from the beginning of the restore window.

## Finding Description

The vulnerability exists in the transaction backup restore path where two interacting flaws enable silent version skipping:

**Flaw 1: Permissive Filter Logic**

The filter permits any chunk where `last_version >= first_version`, regardless of where the chunk actually starts: [1](#0-0) 

If `first_version = 100` and a chunk spans versions 150-200 with `target_version = 300`, the filter evaluates `150 <= 300 && 200 >= 100` which passes, despite the chunk starting 50 versions after the expected start.

**Flaw 2: First Chunk Validation Bypass**

The scan operation initializes gap detection state to 0 and only validates when `*last_chunk_last_version != 0`: [2](#0-1) 

For the first chunk, the condition `*last_chunk_last_version != 0` is false, so gap detection is completely skipped.

**Exploitation Scenario:**

1. Victim has database with versions 0-99 (`db_next_version = 100`)
2. Attacker provides backup with chunks: 150-200, 201-300 (missing 100-149)
3. During restore, `first_version = Some(100)` is set based on database state: [3](#0-2) 

4. In `run_impl()`, the code skips `confirm_or_save_frozen_subtrees()` which would peek at the first chunk: [4](#0-3) 

5. The draining logic checks if `global_first_version > first_version`. With `global_first_version = 100` and `chunk.first_version = 150`, the condition is false: [5](#0-4) 

6. Transactions are saved starting at version 150 via `save_transactions()`: [6](#0-5) 

7. The database writer performs no version continuity validation: [7](#0-6) 

**Result:** Database contains versions 0-99, then gap, then 150-300, causing:
- Incorrect state root hash computation
- Consensus failures when syncing with honest nodes
- Potential network partition if multiple nodes use the same malicious backup
- Violation of state consistency and deterministic execution invariants

## Impact Explanation

**Severity: Critical**

This meets Aptos Critical severity criteria:

1. **Non-recoverable Network Partition**: If multiple validators restore from the same incomplete backup, they form a partition with incorrect state requiring hardfork resolution.

2. **State Consistency Violation**: The Jellyfish Merkle tree becomes inconsistent with transaction history, violating the invariant that identical transaction sequences produce identical state across all nodes.

3. **Consensus/Safety Violations**: Restored nodes compute different state roots than honest nodes, breaking consensus safety guarantees and preventing block validation.

The vulnerability is particularly severe because:
- Restore completes "successfully" without errors
- Corruption is silent until consensus participation
- Multiple affected nodes amplify impact
- Recovery requires gap identification and complete re-restore

## Likelihood Explanation

**Likelihood: Medium-High**

Exploitable in realistic scenarios:

1. **Malicious Backup Provider**: Attacker operates backup service providing incomplete files
2. **Compromised Backup Storage**: Attacker modifies files in S3/GCS/cloud storage
3. **Accidental Corruption**: Backup deletion creates undetected gaps
4. **Bootstrap Operations**: New validators commonly restore from public/third-party backups

Attack requirements:
- Backup file access (Medium difficulty - achievable via service operation or storage compromise)
- No validator privileges needed (Low barrier)
- No cryptographic breaks required (chunks are cryptographically valid)
- Simple execution once backup access obtained (Low complexity)

The two-phase restore architecture explicitly sets `first_version` in production scenarios: [8](#0-7) 

## Recommendation

Implement first chunk validation when `first_version` is provided:

```rust
.scan(first_version.unwrap_or(0), |last_chunk_last_version, chunk_res| {
    let res = match &chunk_res {
        Ok(chunk) => {
            if *last_chunk_last_version != 0 
                && chunk.first_version != *last_chunk_last_version + 1
            {
                Some(Err(anyhow!(
                    "Chunk range not consecutive. expecting {}, got {}",
                    *last_chunk_last_version + 1,
                    chunk.first_version
                )))
            } else {
                *last_chunk_last_version = chunk.last_version;
                Some(chunk_res)
            }
        },
        Err(_) => Some(chunk_res),
    };
    future::ready(res)
});
```

Additionally, validate in `save_before_replay_version()` that chunks start at the expected version.

## Proof of Concept

A complete PoC would require:
1. Creating a test AptosDB instance with transactions 0-99
2. Generating backup manifests with chunks 150-200, 201-300
3. Invoking `TransactionRestoreBatchController` with `first_version = Some(100)`
4. Verifying database contains gap at versions 100-149
5. Demonstrating state root mismatch with complete restore

The technical analysis demonstrates the vulnerability exists through code inspection of the restore pipeline, filter logic, scan operation, and database write path.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L306-312)
```rust
        // If first_version is None, we confirm and save frozen substrees to create a baseline
        // When first version is not None, it only happens when we already finish first phase of db restore and
        // we don't need to confirm and save frozen subtrees again.
        let first_version = self.first_version.unwrap_or(
            self.confirm_or_save_frozen_subtrees(&mut loaded_chunk_stream)
                .await?,
        );
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L360-362)
```rust
            .try_filter(move |c| {
                future::ready(c.first_version <= target_version && c.last_version >= first_version)
            })
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L363-382)
```rust
            .scan(0, |last_chunk_last_version, chunk_res| {
                let res = match &chunk_res {
                    Ok(chunk) => {
                        if *last_chunk_last_version != 0
                            && chunk.first_version != *last_chunk_last_version + 1
                        {
                            Some(Err(anyhow!(
                                "Chunk range not consecutive. expecting {}, got {}",
                                *last_chunk_last_version + 1,
                                chunk.first_version
                            )))
                        } else {
                            *last_chunk_last_version = chunk.last_version;
                            Some(chunk_res)
                        }
                    },
                    Err(_) => Some(chunk_res),
                };
                future::ready(res)
            });
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L485-495)
```rust
                    // remove the txns that are before the global_first_version
                    if global_first_version > first_version {
                        let num_to_remove = (global_first_version - first_version) as usize;

                        txns.drain(..num_to_remove);
                        persisted_aux_info.drain(..num_to_remove);
                        txn_infos.drain(..num_to_remove);
                        event_vecs.drain(..num_to_remove);
                        write_sets.drain(..num_to_remove);
                        first_version = global_first_version;
                    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L507-517)
```rust
                        tokio::task::spawn_blocking(move || {
                            restore_handler.save_transactions(
                                first_version,
                                &txns_to_save,
                                &persisted_aux_info_to_save,
                                &txn_infos_to_save,
                                &event_vecs_to_save,
                                write_sets_to_save,
                            )
                        })
                        .await??;
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L289-294)
```rust
            TransactionRestoreBatchController::new(
                transaction_restore_opt,
                Arc::clone(&self.storage),
                txn_manifests,
                Some(db_next_version),
                Some((kv_replay_version, true /* only replay KV */)),
```

**File:** storage/backup/backup-cli/src/coordinators/restore.rs (L307-307)
```rust
            let first_version = (db_next_version > 0).then_some(db_next_version);
```

**File:** storage/aptosdb/src/backup/restore_utils.rs (L193-244)
```rust
pub(crate) fn save_transactions_impl(
    state_store: Arc<StateStore>,
    ledger_db: Arc<LedgerDb>,
    first_version: Version,
    txns: &[Transaction],
    persisted_aux_info: &[PersistedAuxiliaryInfo],
    txn_infos: &[TransactionInfo],
    events: &[Vec<ContractEvent>],
    write_sets: &[WriteSet],
    ledger_db_batch: &mut LedgerDbSchemaBatches,
    state_kv_batches: &mut ShardedStateKvSchemaBatch,
    kv_replay: bool,
) -> Result<()> {
    for (idx, txn) in txns.iter().enumerate() {
        ledger_db.transaction_db().put_transaction(
            first_version + idx as Version,
            txn,
            /*skip_index=*/ false,
            &mut ledger_db_batch.transaction_db_batches,
        )?;
    }

    for (idx, aux_info) in persisted_aux_info.iter().enumerate() {
        PersistedAuxiliaryInfoDb::put_persisted_auxiliary_info(
            first_version + idx as Version,
            aux_info,
            &mut ledger_db_batch.persisted_auxiliary_info_db_batches,
        )?;
    }

    for (idx, txn_info) in txn_infos.iter().enumerate() {
        TransactionInfoDb::put_transaction_info(
            first_version + idx as Version,
            txn_info,
            &mut ledger_db_batch.transaction_info_db_batches,
        )?;
    }

    ledger_db
        .transaction_accumulator_db()
        .put_transaction_accumulator(
            first_version,
            txn_infos,
            &mut ledger_db_batch.transaction_accumulator_db_batches,
        )?;

    ledger_db.event_db().put_events_multiple_versions(
        first_version,
        events,
        &mut ledger_db_batch.event_db_batches,
    )?;

```
