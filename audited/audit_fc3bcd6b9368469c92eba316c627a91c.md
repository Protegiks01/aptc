# Audit Report

## Title
Race Condition in GCS Backup Operations Allowing Data Corruption and Loss in Table Info Indexer

## Summary
The table info indexer's GCS backup mechanism lacks distributed coordination, allowing multiple nodes to concurrently backup to the same GCS bucket. This creates a race condition where concurrent uploads can corrupt snapshot data and metadata, potentially causing data loss and restore failures.

## Finding Description

The vulnerability exists in the backup flow initiated in `runtime.rs` where `backup_restore_operator` is created and passed to `TableInfoService`. [1](#0-0) 

The `TableInfoService` spawns a continuous backup loop that periodically scans for snapshots to upload. [2](#0-1) 

The critical race condition occurs in `backup_the_snapshot_and_cleanup`, which is explicitly marked as "Not thread safe" and contains a TODO acknowledging the missing concurrent backup handling. [3](#0-2) [4](#0-3) 

The function performs a classic Time-Of-Check-Time-Of-Use (TOCTOU) sequence:
1. Reads metadata from GCS [5](#0-4) 
2. Checks if epoch is already backed up [6](#0-5) 
3. Uploads snapshot and updates metadata [7](#0-6) 

The `backup_db_snapshot_and_update_metadata` function uploads to a deterministic GCS object name based only on chain_id and epoch, with no versioning or conflict detection. [8](#0-7) [9](#0-8) 

The metadata update itself has no atomic conditional write protection and simply overwrites the existing metadata. [10](#0-9) 

**Exploitation Scenario:**
1. Deploy two indexer nodes with `TableInfoServiceMode::Backup(bucket_name)` pointing to the same GCS bucket [11](#0-10) 
2. Both nodes reach epoch boundary and create snapshots locally
3. Both nodes' backup loops detect the snapshot simultaneously
4. Node A reads metadata (epoch=5), sees 6 > 5, proceeds with upload
5. Node B reads metadata (epoch=5), sees 6 > 5, proceeds with upload  
6. Both upload to the same GCS object name concurrently
7. Node B's upload overwrites Node A's (partial or complete)
8. Both update metadata to epoch=6
9. The final snapshot may be corrupted, incomplete, or from the wrong node

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program's criteria for "API crashes" and "Significant protocol violations":

1. **Data Corruption**: Concurrent uploads to the same GCS object can result in corrupted snapshot archives that fail integrity checks during restore operations
2. **Data Loss**: If different nodes produce snapshots representing different local states, the overwrite loses one node's data permanently  
3. **Service Unavailability**: Corrupted backups render the restore mechanism inoperable, preventing recovery from the backup when needed
4. **Metadata Inconsistency**: Race conditions in metadata updates could leave the backup system in an inconsistent state where metadata points to non-existent or corrupted snapshots

While this doesn't directly affect consensus or on-chain funds, it compromises critical indexer infrastructure that applications depend on for querying blockchain state. A corrupted backup system means inability to recover from failures, qualifying this as a significant protocol violation of the backup/restore invariant.

## Likelihood Explanation

**Likelihood: Medium to High**

This will occur whenever:
1. Multiple indexer nodes are configured in Backup mode with the same GCS bucket (legitimate high-availability deployment)
2. Nodes reach epoch boundaries within the 5-second backup check interval
3. Both detect the new snapshot simultaneously

This is not a theoretical edge case - it's an expected scenario in production deployments where multiple indexer nodes run for redundancy. The 5-second polling interval makes collision highly probable during epoch transitions when all nodes process the same ledger state.

## Recommendation

Implement distributed coordination using one of these approaches:

**Option 1: GCS Generation-Based Conditional Writes**
```rust
// In update_metadata function, add generation matching:
let current_metadata = self.download_metadata_object().await.ok();
let expected_generation = current_metadata
    .as_ref()
    .and_then(|m| m.generation);

// Use if_generation_match in UploadObjectRequest
let request = UploadObjectRequest {
    bucket: self.bucket_name.clone(),
    if_generation_match: expected_generation,
    ..Default::default()
};

// Handle precondition failure as indication of concurrent update
match self.gcs_client.upload_object(&request, ...).await {
    Err(Error::Response(err)) if err.code == 412 => {
        // Another node updated, skip this backup
        return Ok(());
    },
    ...
}
```

**Option 2: Distributed Lock via GCS Object Creation**
```rust
// Before backup, attempt to create a lock object with if_generation_match=0
// This succeeds only if object doesn't exist (atomic create)
let lock_name = format!("locks/epoch_{}_lock", epoch);
match create_lock_object(&lock_name).await {
    Ok(_) => {
        // We got the lock, proceed with backup
        backup_db_snapshot_and_update_metadata(...).await?;
        delete_lock_object(&lock_name).await?;
    },
    Err(_) => {
        // Another node holds lock, skip backup
        return Ok(());
    }
}
```

**Option 3: Leader Election**
Designate a single backup leader using etcd/Consul/similar distributed coordination service.

## Proof of Concept

```rust
// Integration test demonstrating the race condition
#[tokio::test]
async fn test_concurrent_backup_race_condition() {
    // Setup: Create two indexer nodes with same GCS bucket
    let bucket_name = "test-backup-bucket";
    let chain_id = 1;
    let epoch = 100;
    
    let operator1 = Arc::new(GcsBackupRestoreOperator::new(bucket_name.to_string()).await);
    let operator2 = Arc::new(GcsBackupRestoreOperator::new(bucket_name.to_string()).await);
    
    // Initialize with epoch 99
    operator1.update_metadata(chain_id, 99).await.unwrap();
    
    // Create two snapshot directories
    let snapshot1 = create_test_snapshot("snapshot1", chain_id, epoch);
    let snapshot2 = create_test_snapshot("snapshot2", chain_id, epoch);
    
    // Simulate concurrent backup from both nodes
    let handle1 = tokio::spawn({
        let op = operator1.clone();
        let path = snapshot1.clone();
        async move {
            op.backup_db_snapshot_and_update_metadata(chain_id, epoch, path)
                .await
        }
    });
    
    let handle2 = tokio::spawn({
        let op = operator2.clone();
        let path = snapshot2.clone();
        async move {
            op.backup_db_snapshot_and_update_metadata(chain_id, epoch, path)
                .await
        }
    });
    
    // Both should complete without error
    let (result1, result2) = tokio::join!(handle1, handle2);
    assert!(result1.is_ok());
    assert!(result2.is_ok());
    
    // Verify: Download the snapshot and check for corruption
    let downloaded = operator1.restore_db_snapshot(
        chain_id,
        BackupRestoreMetadata::new(chain_id, epoch),
        temp_dir(),
        temp_dir(),
    ).await;
    
    // Expected: Either corruption or data from only one node
    // Actual behavior: May fail unpredictably depending on timing
    match downloaded {
        Ok(_) => {
            // If successful, verify it's consistent with one of the sources
            // In practice, may be corrupted mix of both
        },
        Err(_) => {
            // Expected: Corruption leads to restore failure
            println!("Race condition caused snapshot corruption");
        }
    }
}
```

## Notes

This vulnerability specifically affects the indexer-grpc table-info component's backup mechanism. While not directly impacting consensus or on-chain state, it represents a critical failure in the backup/restore invariant for indexer infrastructure. The developers' TODO comment explicitly acknowledges the need for concurrent backup handling, confirming awareness of the issue but no implemented solution. Production deployments running multiple backup-enabled indexer nodes for high availability are at immediate risk of backup corruption.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/runtime.rs (L93-98)
```rust
        let backup_restore_operator = match node_config.indexer_table_info.table_info_service_mode {
            TableInfoServiceMode::Backup(gcs_bucket_name) => Some(Arc::new(
                GcsBackupRestoreOperator::new(gcs_bucket_name).await,
            )),
            _ => None,
        };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L86-99)
```rust
                let _task = tokio::spawn(async move {
                    loop {
                        aptos_logger::info!("[Table Info] Checking for snapshots to backup.");
                        Self::backup_snapshot_if_present(
                            context.clone(),
                            backup_restore_operator.clone(),
                        )
                        .await;
                        tokio::time::sleep(Duration::from_secs(
                            TABLE_INFO_SNAPSHOT_CHECK_INTERVAL_IN_SECS,
                        ))
                        .await;
                    }
                });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L441-442)
```rust
    /// Not thread safe.
    /// TODO(larry): improve the error handling.
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L544-544)
```rust
    let backup_metadata = backup_restore_operator.get_metadata().await;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L576-585)
```rust
        if metadata.epoch >= epoch {
            aptos_logger::info!(
                epoch = epoch,
                snapshot_folder_name = snapshot_folder_name,
                "[Table Info] Snapshot already backed up. Skipping the backup."
            );
            // Remove the snapshot directory.
            std::fs::remove_dir_all(snapshot_dir).unwrap();
            return;
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L599-603)
```rust
    // TODO: add checks to handle concurrent backup jobs.
    backup_restore_operator
        .backup_db_snapshot_and_update_metadata(ledger_chain_id as u64, epoch, snapshot_dir.clone())
        .await
        .expect("Failed to upload snapshot in table info service");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L124-162)
```rust
    pub async fn update_metadata(&self, chain_id: u64, epoch: u64) -> anyhow::Result<()> {
        let metadata = BackupRestoreMetadata::new(chain_id, epoch);
        loop {
            match self
                .gcs_client
                .upload_object(
                    &UploadObjectRequest {
                        bucket: self.bucket_name.clone(),
                        ..Default::default()
                    },
                    serde_json::to_vec(&metadata).unwrap(),
                    &UploadType::Simple(Media {
                        name: Borrowed(METADATA_FILE_NAME),
                        content_type: Borrowed(JSON_FILE_TYPE),
                        content_length: None,
                    }),
                )
                .await
            {
                Ok(_) => {
                    aptos_logger::info!(
                        "[Table Info] Successfully updated metadata to GCS bucket: {}",
                        METADATA_FILE_NAME
                    );
                    return Ok(());
                },
                // https://cloud.google.com/storage/quotas
                // add retry logic due to: "Maximum rate of writes to the same object name: One write per second"
                Err(Error::Response(err)) if (err.is_retriable() && err.code == 429) => {
                    info!("Retried with rateLimitExceeded on gcs single object at epoch {} when updating the metadata", epoch);
                    tokio::time::sleep(Duration::from_millis(500)).await;
                    continue;
                },
                Err(err) => {
                    anyhow::bail!("Failed to update metadata: {}", err);
                },
            }
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L217-217)
```rust
        let filename = generate_blob_name(chain_id, epoch);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/mod.rs (L14-20)
```rust
#[inline]
pub fn generate_blob_name(chain_id: u64, epoch: u64) -> String {
    format!(
        "{}/chain_id_{}_epoch_{}.tar.gz",
        FILE_FOLDER_NAME, chain_id, epoch
    )
}
```

**File:** config/src/config/indexer_table_info_config.rs (L12-19)
```rust
pub enum TableInfoServiceMode {
    /// Backup service mode with GCS bucket name.
    Backup(String),
    /// Restore service mode with GCS bucket name.
    Restore(String),
    IndexingOnly,
    Disabled,
}
```
