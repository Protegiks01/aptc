# Audit Report

## Title
Resource Exhaustion via Concurrent Epoch Ending Ledger Info Requests with Large Validator Sets

## Summary
The `get_epoch_ending_ledger_infos_impl` function in AptosDB uses `.collect()` to load up to 100 epoch ending ledger infos into memory simultaneously, with each ledger info containing the full validator set information. When multiple concurrent requests are made, this can cause significant memory pressure on validator nodes, especially as validator sets grow larger.

## Finding Description

The vulnerability exists in the database reader implementation where epoch ending ledger infos are fetched. [1](#0-0) 

This constant limits each request to 100 epoch ending ledger infos. However, the implementation loads all of them into memory at once: [2](#0-1) 

Each epoch ending ledger info contains a `LedgerInfoWithSignatures` structure: [3](#0-2) 

The critical component is the `EpochState` embedded in epoch ending ledger infos: [4](#0-3) 

This `EpochState` contains a `ValidatorVerifier` with the full validator set: [5](#0-4) 

Each validator's information (address, public key, voting power) is stored, totaling approximately 88 bytes per validator: [6](#0-5) 

The maximum validator set size is defined as: [7](#0-6) 

**Attack Path:**

1. Attacker leverages the network's RPC concurrency limit: [8](#0-7) 

2. Opens 100 concurrent connections and sends `get_epoch_ending_ledger_infos` requests

3. Each request triggers memory allocation for 100 epoch ending ledger infos before any size validation occurs

4. With current mainnet validator set (~129 validators), each ledger info is approximately 12 KB, resulting in ~1.2 MB per request

5. With 100 concurrent requests: ~120 MB total allocation

6. If validator set grows to 1,000 validators: ~87 KB per ledger info → ~870 MB for 100 concurrent requests

7. If validator set approaches 10,000 validators: ~870 KB per ledger info → ~8.7 GB for 100 concurrent requests

The storage service attempts to enforce size limits, but the underlying database implementation allocates memory first: [9](#0-8) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Node Slowdowns**: As the validator set grows, concurrent requests can consume hundreds of megabytes to several gigabytes of memory, causing garbage collection pressure and performance degradation.

2. **Potential OOM Conditions**: With validator sets of 10,000+ validators, sustained attacks could trigger out-of-memory conditions, causing validator crashes.

3. **Scalability Risk**: The vulnerability worsens as the network grows, threatening the protocol's ability to scale to larger validator sets.

While the current impact with ~129 validators is moderate (~120 MB for 100 concurrent requests), the vulnerability fundamentally violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The lack of memory-aware limiting means the system cannot guarantee safe operation as validator sets scale.

## Likelihood Explanation

**Likelihood: Medium**

The attack is feasible but requires:
- Multiple concurrent connections (up to 100)
- Coordination to maintain sustained request volume
- Target validator must be accessible from attacker-controlled IPs

However, the attack is straightforward:
- No authentication bypass required
- Standard RPC protocol usage
- No rate limiting for valid requests (only invalid requests are rate limited)
- Public validator endpoints are typically accessible

The current impact is limited with ~129 validators, but becomes severe as the validator set grows beyond 1,000 validators - a realistic future scenario for a successful L1 blockchain.

## Recommendation

Implement memory-aware limiting at the database layer before collection:

```rust
pub(super) fn get_epoch_ending_ledger_infos_impl(
    &self,
    start_epoch: u64,
    end_epoch: u64,
    limit: usize,
    max_bytes: Option<usize>,
) -> Result<(Vec<LedgerInfoWithSignatures>, bool)> {
    self.check_epoch_ending_ledger_infos_request(start_epoch, end_epoch)?;

    let (paging_epoch, more) = if end_epoch - start_epoch > limit as u64 {
        (start_epoch + limit as u64, true)
    } else {
        (end_epoch, false)
    };

    let iter = self
        .ledger_db
        .metadata_db()
        .get_epoch_ending_ledger_info_iter(start_epoch, paging_epoch)?;

    // Stream and size-check instead of collecting all at once
    let mut lis = Vec::new();
    let mut total_bytes = 0usize;
    let mut truncated = false;
    
    for result in iter {
        let li = result?;
        
        if let Some(max) = max_bytes {
            // Estimate size before adding
            let estimated_size = bcs::serialized_size(&li)?;
            if total_bytes + estimated_size > max && !lis.is_empty() {
                truncated = true;
                break;
            }
            total_bytes += estimated_size;
        }
        
        lis.push(li);
    }

    ensure!(
        !lis.is_empty() || truncated,
        "DB corruption: missing epoch ending ledger info"
    );
    
    Ok((lis, more || truncated))
}
```

Additionally, consider:
1. Reducing `MAX_NUM_EPOCH_ENDING_LEDGER_INFO` to 50 or implementing dynamic limits based on validator set size
2. Adding per-peer rate limiting for all requests, not just invalid ones
3. Implementing memory pressure monitoring with automatic backpressure

## Proof of Concept

```rust
// Integration test demonstrating concurrent memory allocation
#[tokio::test]
async fn test_concurrent_epoch_ending_ledger_info_memory_pressure() {
    use futures::future::join_all;
    use std::sync::Arc;
    
    // Setup: Create validator with large validator set
    let num_validators = 1000; // Simulating future growth
    let num_epochs = 100;
    let num_concurrent_requests = 100;
    
    let db = Arc::new(create_test_db_with_epochs(num_validators, num_epochs));
    
    // Track memory before attack
    let initial_memory = get_process_memory_mb();
    
    // Launch concurrent requests
    let mut handles = vec![];
    for _ in 0..num_concurrent_requests {
        let db_clone = db.clone();
        handles.push(tokio::spawn(async move {
            // Each request fetches 100 epoch ending ledger infos
            db_clone.get_epoch_ending_ledger_infos(0, 100).await
        }));
    }
    
    // Wait for all requests to complete
    join_all(handles).await;
    
    let peak_memory = get_process_memory_mb();
    let memory_increase = peak_memory - initial_memory;
    
    // With 1000 validators:
    // Expected memory per ledger info: ~87 KB
    // Per request (100 infos): ~8.7 MB
    // 100 concurrent requests: ~870 MB
    assert!(memory_increase > 800, 
        "Memory increase {} MB indicates vulnerability", memory_increase);
}
```

**Notes**

The vulnerability is most severe in future scenarios where the validator set grows beyond 1,000 validators. The core issue is the use of `.collect()` which eagerly allocates memory for all 100 ledger infos before any size checking can occur. This design pattern violates the principle that resource limits should be enforced at the allocation point, not after allocation.

The fix requires streaming processing with incremental size checking, allowing the system to respect memory budgets throughout the fetching process rather than discovering size violations only after full allocation.

### Citations

**File:** storage/aptosdb/src/common.rs (L9-9)
```rust
pub(crate) const MAX_NUM_EPOCH_ENDING_LEDGER_INFO: usize = 100;
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L1050-1054)
```rust
        let lis = self
            .ledger_db
            .metadata_db()
            .get_epoch_ending_ledger_info_iter(start_epoch, paging_epoch)?
            .collect::<Result<Vec<_>>>()?;
```

**File:** types/src/ledger_info.rs (L240-246)
```rust
#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize)]
pub struct LedgerInfoWithV0 {
    ledger_info: LedgerInfo,
    /// Aggregated BLS signature of all the validators that signed the message. The bitmask in the
    /// aggregated signature can be used to find out the individual validators signing the message
    signatures: AggregateSignature,
}
```

**File:** types/src/block_info.rs (L29-44)
```rust
pub struct BlockInfo {
    /// The epoch to which the block belongs.
    epoch: u64,
    /// The consensus protocol is executed in rounds, which monotonically increase per epoch.
    round: Round,
    /// The identifier (hash) of the block.
    id: HashValue,
    /// The accumulator root hash after executing this block.
    executed_state_id: HashValue,
    /// The version of the latest transaction after executing this block.
    version: Version,
    /// The timestamp this block was proposed by a proposer.
    timestamp_usecs: u64,
    /// An optional field containing the next epoch info
    next_epoch_state: Option<EpochState>,
}
```

**File:** types/src/validator_verifier.rs (L70-76)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct ValidatorConsensusInfo {
    pub address: AccountAddress,
    pub public_key: PublicKey,
    pub voting_power: u64,
}
```

**File:** types/src/validator_verifier.rs (L135-149)
```rust
#[derive(Debug, Derivative, Serialize)]
#[derivative(PartialEq, Eq)]
pub struct ValidatorVerifier {
    /// A vector of each validator's on-chain account address to its pubkeys and voting power.
    pub validator_infos: Vec<ValidatorConsensusInfo>,
    /// The minimum voting power required to achieve a quorum
    #[serde(skip)]
    quorum_voting_power: u128,
    /// Total voting power of all validators (cached from address_to_validator_info)
    #[serde(skip)]
    total_voting_power: u128,
    /// In-memory index of account address to its index in the vector, does not go through serde.
    #[serde(skip)]
    address_to_validator_index: HashMap<AccountAddress, usize>,
    /// With optimistic signature verification, we aggregate all the votes on a message and verify at once.
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L98-100)
```text
    /// Limit the maximum size to u16::max, it's the current limit of the bitvec
    /// https://github.com/aptos-labs/aptos-core/blob/main/crates/aptos-bitvec/src/lib.rs#L20
    const MAX_VALIDATOR_SET_SIZE: u64 = 65536;
```

**File:** network/framework/src/constants.rs (L14-15)
```rust
/// Limit on concurrent Inbound RPC requests before backpressure is applied
pub const MAX_CONCURRENT_INBOUND_RPCS: u32 = 100;
```

**File:** state-sync/storage-service/server/src/storage.rs (L1092-1103)
```rust
    fn get_epoch_ending_ledger_infos(
        &self,
        start_epoch: u64,
        expected_end_epoch: u64,
    ) -> aptos_storage_service_types::Result<EpochChangeProof, Error> {
        self.get_epoch_ending_ledger_infos_by_size(
            start_epoch,
            expected_end_epoch,
            self.config.max_network_chunk_bytes,
            self.config.enable_size_and_time_aware_chunking,
        )
    }
```
