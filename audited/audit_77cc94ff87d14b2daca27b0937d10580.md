# Audit Report

## Title
Wrapped Range Logic Error in Cross-Shard Conflict Detection Allows Acceptance of Conflicting Transactions

## Summary
The `key_owned_by_another_shard` function contains a critical logic error when `shard_id < anchor_shard_id`. The wrapped range calculation incorrectly checks `[anchor_start, MAX) ∪ [0, current_start)` instead of properly accounting for intermediate shards, causing the conflict detection logic to miss writes in intermediate shards. This allows transactions with cross-shard dependencies to be incorrectly accepted in the same round, violating the partitioner's core invariant.

## Finding Description

The block partitioner's `discarding_round()` function eliminates cross-shard dependencies within each non-final round by checking whether accessed keys are "owned by another shard" using the `key_owned_by_another_shard` function. [1](#0-0) 

The `key_owned_by_another_shard` function computes a range to check for conflicting writes: [2](#0-1) 

When `shard_id < anchor_shard_id`, `range_start > range_end`, triggering wrapped range logic in `has_write_in_range`: [3](#0-2) 

**The Bug:** For a 3-shard system with `start_txn_idxs_by_shard = [0, 10, 20]`, when checking a transaction in shard 0 accessing a key anchored to shard 2:
- `range_start = start_txn_idxs_by_shard[2] = 20`
- `range_end = start_txn_idxs_by_shard[0] = 0`
- Wrapped range evaluates to `[20, MAX) ∪ [0, 0)` = `[20, MAX)`
- This checks only shard 2 (indices 20+), **missing shard 1 (indices 10-19)**

**Concrete Exploit Scenario:**
1. Transaction T5 (index 5, shard 0) reads key K (anchored to shard 2)
2. Transaction T15 (index 15, shard 1) writes to key K
3. No transaction in shard 2 writes to key K

Processing results:
- T5 checks `[20, MAX)` for writes to K → T15 not detected → **INCORRECTLY ACCEPTED**
- T15 checks `[20, MAX) ∪ [0, 10)` for writes to K → finds none (T5 only reads) → **ACCEPTED**

Both transactions are accepted in the same round, creating an undetected read-write dependency that violates the critical invariant.

The system explicitly validates this invariant in test verification code: [4](#0-3) 

The anchor shard assignment is deterministic via hashing: [5](#0-4) 

## Impact Explanation

This vulnerability enables **non-deterministic execution** across validator nodes. The sharded executor relies on the partitioner's guarantee that non-final rounds contain no in-round cross-shard dependencies. When this guarantee is violated:

1. Transactions T5 (reading K) and T15 (writing K) execute concurrently in different shards
2. Execution order is undefined - some validators may execute T15 before T5, others after
3. Different validators observe different read values
4. State root hash computation becomes non-deterministic
5. Consensus participants may produce conflicting state commitments

**Severity: MEDIUM** (per Aptos bug bounty criteria)
- **State inconsistencies requiring manual intervention**: Different validators may compute different state roots for the same block
- Not immediate consensus failure, but degrades deterministic execution guarantees
- Requires specific transaction patterns with 3+ shards and particular anchor shard assignments

## Likelihood Explanation

**Likelihood: Medium-High**

**Requirements:**
- Block partitioner must use ≥3 executor shards (realistic in production environments)
- Transactions must access storage keys with specific anchor shard assignments (deterministic via hashing, predictable)
- Attacker must submit multiple transactions accessing the same storage location

**Complexity: Low**
- No validator collusion required
- No special privileges needed
- Attacker only needs to craft transactions accessing common storage locations
- Natural transaction patterns can trigger this condition

**Real-world scenarios:**
- High-throughput DeFi applications with shared liquidity pools
- Token transfers involving popular coins with many concurrent transactions
- NFT marketplace operations on shared collection state

The bug is **deterministically exploitable** once the conditions are met - no timing manipulation required.

## Recommendation

Fix the range calculation to properly check intermediate shards when `shard_id < anchor_shard_id`. The function should check the range from `min(shard_id, anchor_shard_id)` to `max(shard_id, anchor_shard_id)`, excluding the current shard:

```rust
pub(crate) fn key_owned_by_another_shard(&self, shard_id: ShardId, key: StorageKeyIdx) -> bool {
    let tracker_ref = self.trackers.get(&key).unwrap();
    let tracker = tracker_ref.read().unwrap();
    
    let (range_start, range_end) = if shard_id < tracker.anchor_shard_id {
        // Check from current shard to anchor shard (exclusive of current)
        (self.start_txn_idxs_by_shard[shard_id], self.start_txn_idxs_by_shard[tracker.anchor_shard_id])
    } else {
        // Original logic for non-wrapped case
        (self.start_txn_idxs_by_shard[tracker.anchor_shard_id], self.start_txn_idxs_by_shard[shard_id])
    };
    
    // Check if there are writes in the range, excluding the current shard's own range
    tracker.has_write_in_range(range_start, range_end)
}
```

This ensures intermediate shards are properly checked in both directions.

## Proof of Concept

The vulnerability can be demonstrated by creating a test case with 3 shards where:
1. A transaction in shard 0 reads a key anchored to shard 2
2. A transaction in shard 1 writes to the same key
3. No transaction in shard 2 writes to the key

Both transactions will be incorrectly accepted in the same round, violating the invariant checked at: [6](#0-5)

### Citations

**File:** execution/block-partitioner/src/v2/partition_to_matrix.rs (L116-126)
```rust
                    txn_idxs.into_par_iter().for_each(|txn_idx| {
                        let ori_txn_idx = state.ori_idxs_by_pre_partitioned[txn_idx];
                        let mut in_round_conflict_detected = false;
                        let write_set = state.write_sets[ori_txn_idx].read().unwrap();
                        let read_set = state.read_sets[ori_txn_idx].read().unwrap();
                        for &key_idx in write_set.iter().chain(read_set.iter()) {
                            if state.key_owned_by_another_shard(shard_id, key_idx) {
                                in_round_conflict_detected = true;
                                break;
                            }
                        }
```

**File:** execution/block-partitioner/src/v2/state.rs (L211-217)
```rust
    pub(crate) fn key_owned_by_another_shard(&self, shard_id: ShardId, key: StorageKeyIdx) -> bool {
        let tracker_ref = self.trackers.get(&key).unwrap();
        let tracker = tracker_ref.read().unwrap();
        let range_start = self.start_txn_idxs_by_shard[tracker.anchor_shard_id];
        let range_end = self.start_txn_idxs_by_shard[shard_id];
        tracker.has_write_in_range(range_start, range_end)
    }
```

**File:** execution/block-partitioner/src/v2/conflicting_txn_tracker.rs (L70-84)
```rust
    pub fn has_write_in_range(
        &self,
        start_txn_id: PrePartitionedTxnIdx,
        end_txn_id: PrePartitionedTxnIdx,
    ) -> bool {
        if start_txn_id <= end_txn_id {
            self.pending_writes
                .range(start_txn_id..end_txn_id)
                .next()
                .is_some()
        } else {
            self.pending_writes.range(start_txn_id..).next().is_some()
                || self.pending_writes.range(..end_txn_id).next().is_some()
        }
    }
```

**File:** execution/block-partitioner/src/test_utils.rs (L150-151)
```rust
/// - Before the last round, there is no in-round cross-shard dependency.
///
```

**File:** execution/block-partitioner/src/test_utils.rs (L222-224)
```rust
                    if round_id != num_rounds - 1 {
                        assert_ne!(src_txn_idx.round_id, round_id);
                    }
```

**File:** execution/block-partitioner/src/lib.rs (L39-43)
```rust
fn get_anchor_shard_id(storage_location: &StorageLocation, num_shards: usize) -> ShardId {
    let mut hasher = DefaultHasher::new();
    storage_location.hash(&mut hasher);
    (hasher.finish() % num_shards as u64) as usize
}
```
