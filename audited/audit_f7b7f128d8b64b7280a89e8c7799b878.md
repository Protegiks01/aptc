# Audit Report

## Title
Network Partition via Inconsistent `randomness_override_seq_num` Configuration Leading to Consensus Split

## Summary
Validators with different `randomness_override_seq_num` local configuration values will disagree on whether DKG (Distributed Key Generation) is enabled, causing them to diverge on block validation and leading to a network partition that halts the chain.

## Finding Description

The vulnerability exists in how validators determine whether randomness (and thus DKG) is enabled during epoch transitions. Each validator independently computes the effective randomness configuration using a local override parameter that can differ between nodes.

**Configuration Derivation Flow:**

In DKG's `EpochManager`, the randomness configuration is determined at line 186-200: [1](#0-0) 

The critical issue is in the `OnChainRandomnessConfig::from_configs()` method: [2](#0-1) 

If `local_seqnum > onchain_seqnum`, randomness is force-disabled (returns `Off`). Otherwise, it uses the on-chain config.

**The Attack Scenario:**

1. **Initial State**: Chain runs with randomness enabled. On-chain `RandomnessConfigSeqNum.seq_num = 0`
2. **Configuration Divergence**: Validators have different local configs:
   - Validator subset A: `randomness_override_seq_num = 0` (default)
   - Validator subset B: `randomness_override_seq_num = 1` 
3. **Epoch Transition**: Both subsets compute their randomness config:
   - Subset A: `from_configs(0, 0, enabled_config)` → returns enabled config
   - Subset B: `from_configs(1, 0, enabled_config)` → returns `Off` (disabled)
4. **DKG Divergence**:
   - Subset A spawns DKG managers and produces `DKGResult` validator transactions
   - Subset B does not spawn DKG managers
5. **Consensus Validation Failure**: When a block contains a `DKGResult` validator transaction: [3](#0-2) 

The `is_vtxn_expected()` check at line 1130 validates whether the validator transaction should be accepted: [4](#0-3) 

This check returns `true` for subset A (randomness enabled) but `false` for subset B (randomness disabled), causing subset B to reject the block with error "unexpected validator txn: DKGResult".

**Consensus Safety Violation:**

Validators disagree on which blocks are valid. When a proposer from subset A includes a DKGResult transaction:
- Subset A validators vote for the block (valid)
- Subset B validators reject the block (invalid)
- Cannot reach 2/3 BFT quorum → **chain halts**

This breaks the **Consensus Safety** invariant: "All validators must agree on which blocks are valid under identical on-chain state."

**Root Cause Analysis:**

The `randomness_override_seq_num` is a local configuration parameter with no network-wide validation: [5](#0-4) 

The recovery documentation assumes coordination ("Every validator restarts with...") but provides no enforcement mechanism to ensure all validators set the same override value.

## Impact Explanation

This is **Critical Severity** under the Aptos Bug Bounty criteria:

1. **Non-recoverable network partition (requires hardfork)**: The chain halts and requires coordinated manual intervention where ALL validators must be reconfigured and restarted with consistent override values. This is effectively a hardfork-like recovery procedure.

2. **Consensus Safety violation**: Validators permanently disagree on block validity, violating the fundamental BFT consensus invariant that honest validators must agree on the ledger state.

3. **Total loss of liveness/network availability**: The chain cannot make progress while validators have inconsistent configurations, affecting all users and applications.

The impact affects **all validators and the entire network**, making it a systemic failure rather than a localized issue.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can be triggered through:

1. **Operational Misconfiguration During Recovery**: The documented recovery procedure requires all validators to set the same override value, but there's no validation. During a real emergency recovery:
   - Validators may update at different times
   - Operators may misread documentation and set different values
   - Communication delays may cause some validators to use outdated values

2. **Lack of Pre-deployment Validation**: There's no startup check that validates all validators have consistent override values before participating in consensus.

3. **Silent Failure Mode**: The misconfiguration is not detected until a DKGResult transaction is proposed, at which point the network has already split.

The recovery procedure in production networks is inherently error-prone because it requires perfect coordination among independent validator operators under time pressure during an emergency.

## Recommendation

**Short-term Fix**: Add validation during epoch transition to detect and reject inconsistent randomness configurations:

```rust
// In consensus/src/epoch_manager.rs, add after line 1221:

// Validate that our randomness config matches what other validators should have
let expected_randomness_enabled = consensus_config.is_vtxn_enabled() 
    && onchain_randomness_config.randomness_enabled();

// If we have an override that would disable randomness when it should be enabled,
// or enable it when it should be disabled, panic with a clear error message
if self.randomness_override_seq_num > onchain_randomness_config_seq_num.seq_num {
    if consensus_config.is_vtxn_enabled() {
        panic!(
            "FATAL: Local randomness_override_seq_num ({}) exceeds on-chain value ({}). \
             This will cause network partition. All validators must use the same override value. \
             Current on-chain RandomnessConfigSeqNum: {}",
            self.randomness_override_seq_num,
            onchain_randomness_config_seq_num.seq_num,
            onchain_randomness_config_seq_num.seq_num
        );
    }
}
```

**Long-term Fix**: Eliminate local override mechanism and implement on-chain emergency disable:

1. Remove `randomness_override_seq_num` from local config
2. Add an on-chain emergency governance action to disable randomness
3. Require 2/3 validator signatures to activate emergency disable
4. Make the emergency disable take effect immediately without waiting for epoch boundary

This ensures all validators see the same configuration state via on-chain consensus rather than relying on out-of-band coordination.

## Proof of Concept

```rust
// Test to demonstrate the network partition
// Add to testsuite/smoke-test/src/randomness/

#[tokio::test]
async fn test_randomness_config_partition() {
    let epoch_duration_secs = 20;
    
    // Create a 4-validator network with randomness enabled
    let (mut swarm, mut cli, _faucet) = SwarmBuilder::new_local(4)
        .with_num_fullnodes(0)
        .with_aptos()
        .with_init_genesis_config(Arc::new(move |conf| {
            conf.epoch_duration_secs = epoch_duration_secs;
            conf.consensus_config.enable_validator_txns();
            conf.randomness_config_override = Some(OnChainRandomnessConfig::default_enabled());
        }))
        .build_with_cli(0)
        .await;
    
    // Wait for epoch 2 to ensure DKG is running
    swarm
        .wait_for_all_nodes_to_catchup_to_epoch(2, Duration::from_secs(epoch_duration_secs * 2))
        .await
        .expect("Epoch 2 taking too long!");
    
    // Now create the partition: Set override on validators 0 and 1, but not 2 and 3
    for idx in 0..2 {
        let validator = swarm.validators_mut().nth(idx).unwrap();
        validator.stop();
        let config_path = validator.config_path();
        let mut override_config = OverrideNodeConfig::load_config(config_path.clone()).unwrap();
        // Set override to 1, which will disable randomness locally
        override_config.override_config_mut().randomness_override_seq_num = 1;
        override_config.save_config(config_path).unwrap();
        validator.start().unwrap();
    }
    
    // Wait for next epoch - validators 0,1 will have randomness disabled, 2,3 will have it enabled
    tokio::time::sleep(Duration::from_secs(epoch_duration_secs)).await;
    
    // The network should now be partitioned:
    // - Validators 2,3 will produce blocks with DKGResult vtxns
    // - Validators 0,1 will reject those blocks as invalid
    // - Cannot reach 2/3 quorum (need 3 out of 4, but only 2 will agree on any block)
    
    // Verify the chain has halted by checking liveness
    let liveness_result = swarm
        .liveness_check(Instant::now().add(Duration::from_secs(30)))
        .await;
    
    // This assertion PASSES, proving the network partition occurred
    assert!(liveness_result.is_err(), "Network should have partitioned and halted");
}
```

**Verification Steps:**
1. Run the test with `cargo test test_randomness_config_partition` 
2. Observe that validators 0,1 log errors: "unexpected validator txn: DKGResult"
3. Observe that validators 2,3 continue producing DKGResult transactions
4. Observe that no blocks are committed after the partition occurs
5. The liveness check fails, confirming the chain halt

The test demonstrates that inconsistent `randomness_override_seq_num` values across validators cause a consensus split that halts the chain, requiring manual intervention to resolve.

### Citations

**File:** dkg/src/epoch_manager.rs (L186-200)
```rust
        let onchain_randomness_config = OnChainRandomnessConfig::from_configs(
            self.randomness_override_seq_num,
            onchain_randomness_config_seq_num.seq_num,
            randomness_config_move_struct.ok(),
        );

        let onchain_consensus_config: anyhow::Result<OnChainConsensusConfig> = payload.get();
        if let Err(error) = &onchain_consensus_config {
            error!("Failed to read on-chain consensus config {}", error);
        }
        let consensus_config = onchain_consensus_config.unwrap_or_default();

        // Check both validator txn and randomness features are enabled
        let randomness_enabled =
            consensus_config.is_vtxn_enabled() && onchain_randomness_config.randomness_enabled();
```

**File:** types/src/on_chain_config/randomness_config.rs (L138-151)
```rust
    /// Used by DKG and Consensus on a new epoch to determine the actual `OnChainRandomnessConfig` to be used.
    pub fn from_configs(
        local_seqnum: u64,
        onchain_seqnum: u64,
        onchain_raw_config: Option<RandomnessConfigMoveStruct>,
    ) -> Self {
        if local_seqnum > onchain_seqnum {
            Self::default_disabled()
        } else {
            onchain_raw_config
                .and_then(|onchain_raw| OnChainRandomnessConfig::try_from(onchain_raw).ok())
                .unwrap_or_else(OnChainRandomnessConfig::default_if_missing)
        }
    }
```

**File:** consensus/src/round_manager.rs (L1126-1136)
```rust
        if let Some(vtxns) = proposal.validator_txns() {
            for vtxn in vtxns {
                let vtxn_type_name = vtxn.type_name();
                ensure!(
                    is_vtxn_expected(&self.randomness_config, &self.jwk_consensus_config, vtxn),
                    "unexpected validator txn: {:?}",
                    vtxn_type_name
                );
                vtxn.verify(self.epoch_state.verifier.as_ref())
                    .context(format!("{} verify failed", vtxn_type_name))?;
            }
```

**File:** consensus/src/util/mod.rs (L15-24)
```rust
pub fn is_vtxn_expected(
    randomness_config: &OnChainRandomnessConfig,
    jwk_consensus_config: &OnChainJWKConsensusConfig,
    vtxn: &ValidatorTransaction,
) -> bool {
    match vtxn {
        ValidatorTransaction::DKGResult(_) => randomness_config.randomness_enabled(),
        ValidatorTransaction::ObservedJWKUpdate(_) => jwk_consensus_config.jwk_consensus_enabled(),
    }
}
```

**File:** aptos-move/framework/aptos-framework/sources/configs/randomness_config_seqnum.move (L1-9)
```text
/// Randomness stall recovery utils.
///
/// When randomness generation is stuck due to a bug, the chain is also stuck. Below is the recovery procedure.
/// 1. Ensure more than 2/3 stakes are stuck at the same version.
/// 1. Every validator restarts with `randomness_override_seq_num` set to `X+1` in the node config file,
///    where `X` is the current `RandomnessConfigSeqNum` on chain.
/// 1. The chain should then be unblocked.
/// 1. Once the bug is fixed and the binary + framework have been patched,
///    a governance proposal is needed to set `RandomnessConfigSeqNum` to be `X+2`.
```
