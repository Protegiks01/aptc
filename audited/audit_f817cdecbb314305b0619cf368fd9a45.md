# Audit Report

## Title
Health Checker Bounded Channel Can Silently Drop Ping Requests Leading to Peer Disconnections

## Summary
The HealthChecker's inbound message channel is bounded with a LIFO eviction policy that silently drops messages when full. A slow health checker (due to blocking operations or high load) can cause legitimate ping requests to be dropped without notification, leading remote peers to disconnect due to timeout, degrading the node's peer connectivity and potentially affecting validator participation.

## Finding Description

The `HealthCheckerNetworkEvents` channel uses a bounded `aptos_channel` with a capacity of 1024 messages per peer. [1](#0-0) 

The channel is configured with LIFO queue style and bounded capacity: [2](#0-1) 

When the channel is full, the oldest messages are dropped rather than blocking the sender: [3](#0-2) 

The critical issue is that when a `ReceivedMessage` is dropped from the queue, its associated `oneshot::Sender` for the RPC response is also dropped. The `Peer` actor receives no error notification that the message was dropped: [4](#0-3) 

The dropped message's response channel receives a `Canceled` error, preventing any response from being sent to the remote peer: [5](#0-4) 

**Attack/Failure Scenarios:**

1. **Sequential Disconnect Blocking**: When the health checker processes multiple ping failures requiring disconnection, each `disconnect_peer()` call can block for up to 50ms: [6](#0-5) 

   During this blocking period, the main event loop cannot process new incoming pings: [7](#0-6) 

2. **High Peer Count**: With many connected peers (100+) sending periodic pings, legitimate pings can accumulate in the queue during any processing delay.

3. **No Rate Limiting**: There is no protection against rapid ping requests from peers, allowing queues to fill up more easily under load.

## Impact Explanation

This qualifies as **Medium Severity** under Aptos bug bounty criteria as it can cause "Validator node slowdowns":

- When ping responses are dropped, remote peers timeout waiting for pong responses
- Remote peers increment failure counters and eventually disconnect from the affected node
- Loss of peer connections degrades the node's ability to:
  - Participate effectively in consensus (reduced message propagation)
  - Relay transactions across the network
  - Maintain validator network health
  
This does not directly compromise consensus safety or cause fund loss, but degrades node operational reliability and network connectivity, which can indirectly impact validator performance and participation.

## Likelihood Explanation

**Moderate likelihood** under high-load conditions:

- Each peer has a 1024 message queue capacity, providing significant buffering
- Ping messages are lightweight (single u32 nonce) and fast to process
- However, blocking operations (disconnect with 50ms timeout) can delay processing
- The issue is more likely to manifest during:
  - Network partitions or issues causing many simultaneous disconnect operations
  - High validator counts with many active peers
  - Resource contention on the validator node (CPU, I/O)
  - DoS attempts where malicious actors send rapid pings

The silent nature of message drops (no alerts or errors) makes this issue difficult to detect until peer disconnections occur.

## Recommendation

Implement one or more of the following mitigations:

1. **Add Feedback Mechanism**: Use `push_with_feedback()` to detect when messages are dropped and log alerts:
   ```rust
   let (status_tx, status_rx) = oneshot::channel();
   if let Err(err) = peer_notifs_tx.push_with_feedback((peer_id, protocol_id), request, Some(status_tx)) {
       // Handle error
   }
   // Monitor status_rx for ElementStatus::Dropped
   ```

2. **Increase Channel Capacity**: Increase `NETWORK_CHANNEL_SIZE` for health checker to provide more buffering during high load periods.

3. **Add Rate Limiting**: Implement per-peer rate limiting on inbound ping requests to prevent queue saturation from rapid requests.

4. **Non-blocking Disconnect**: Spawn disconnect operations in separate tasks rather than awaiting them inline, allowing the event loop to continue processing:
   ```rust
   tokio::spawn(async move {
       let _ = network_interface.disconnect_peer(peer_network_id, reason).await;
   });
   ```

5. **Monitor Queue Depth**: Add monitoring for the `PENDING_HEALTH_CHECKER_NETWORK_EVENTS` counter's "dropped" label to alert operators when messages are being dropped.

## Proof of Concept

```rust
// Test demonstrating ping drop under slow processing
#[tokio::test]
async fn test_health_checker_ping_drops_under_load() {
    let (mut harness, mut health_checker) = TestHarness::new_strict();
    
    // Connect a peer
    let peer_id = PeerId::random();
    harness.connect_peer(peer_id).await;
    
    // Send 1025 rapid pings (exceeding queue capacity of 1024)
    for i in 0..1025 {
        let ping = HealthCheckerMsg::Ping(Ping(i));
        let rpc_request = RpcRequest {
            request_id: i as u32,
            protocol_id: HealthCheckerRpc,
            priority: 0,
            raw_request: bcs::to_bytes(&ping).unwrap(),
        };
        let message = NetworkMessage::RpcRequest(rpc_request);
        let received_msg = ReceivedMessage::new(message, 
            PeerNetworkId::new(NetworkId::Validator, peer_id));
        
        // Push without waiting for health checker to process
        harness.peer_mgr_notifs_tx.push((peer_id, HealthCheckerRpc), received_msg).unwrap();
    }
    
    // Process some events but not enough to drain queue
    for _ in 0..100 {
        tokio::time::sleep(Duration::from_millis(1)).await;
    }
    
    // Verify that some messages were dropped (counter would show "dropped" state)
    // The oldest ~25 pings should have been dropped due to LIFO eviction
}
```

## Notes

The vulnerability exists at the intersection of channel design (bounded with silent dropping) and health checker's event processing model (sequential with potential blocking). While unlikely under normal operation, it represents a failure mode that could manifest during high load, network issues, or malicious activity, degrading validator network connectivity.

The issue is exacerbated by the lack of visibility into message drops and the absence of rate limiting on inbound health check requests, making it difficult to detect and mitigate when it occurs in production environments.

### Citations

**File:** network/framework/src/constants.rs (L19-19)
```rust
pub const NETWORK_CHANNEL_SIZE: usize = 1024;
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L71-78)
```rust
    let network_service_config = NetworkServiceConfig::new(
        direct_send_protocols,
        rpc_protocols,
        aptos_channel::Config::new(NETWORK_CHANNEL_SIZE)
            .queue_style(QueueStyle::LIFO)
            .counters(&counters::PENDING_HEALTH_CHECKER_NETWORK_EVENTS),
    );
    NetworkApplicationConfig::new(network_client_config, network_service_config)
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L265-268)
```rust
                res = tick_handlers.select_next_some() => {
                    let (peer_id, round, nonce, ping_result) = res;
                    self.handle_ping_response(peer_id, round, nonce, ping_result).await;
                }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L373-391)
```rust
                    if let Err(err) = timeout(
                        Duration::from_millis(50),
                        self.network_interface.disconnect_peer(
                            peer_network_id,
                            DisconnectReason::NetworkHealthCheckFailure,
                        ),
                    )
                    .await
                    {
                        warn!(
                            NetworkSchema::new(&self.network_context)
                                .remote_peer(&peer_id),
                            error = ?err,
                            "{} Failed to disconnect from peer: {} with error: {:?}",
                            self.network_context,
                            peer_id.short_str(),
                            err
                        );
                    }
```

**File:** crates/channel/src/message_queues.rs (L138-147)
```rust
            match self.queue_style {
                // Drop the newest message for FIFO
                QueueStyle::FIFO => Some(message),
                // Drop the oldest message for LIFO
                QueueStyle::LIFO | QueueStyle::KLAST => {
                    let oldest = key_message_queue.pop_front();
                    key_message_queue.push_back(message);
                    oldest
                },
            }
```

**File:** network/framework/src/protocols/rpc/mod.rs (L249-253)
```rust
        if let Err(err) = peer_notifs_tx.push((peer_id, protocol_id), request) {
            counters::rpc_messages(network_context, REQUEST_LABEL, INBOUND_LABEL, FAILED_LABEL)
                .inc();
            return Err(err.into());
        }
```

**File:** network/framework/src/protocols/rpc/mod.rs (L256-281)
```rust
        let inbound_rpc_task = self
            .time_service
            .timeout(self.inbound_rpc_timeout, response_rx)
            .map(move |result| {
                // Flatten the errors
                let maybe_response = match result {
                    Ok(Ok(Ok(response_bytes))) => {
                        let rpc_response = RpcResponse {
                            request_id,
                            priority,
                            raw_response: Vec::from(response_bytes.as_ref()),
                        };
                        Ok((rpc_response, protocol_id))
                    },
                    Ok(Ok(Err(err))) => Err(err),
                    Ok(Err(oneshot::Canceled)) => Err(RpcError::UnexpectedResponseChannelCancel),
                    Err(timeout::Elapsed) => Err(RpcError::TimedOut),
                };
                // Only record latency of successful requests
                match maybe_response {
                    Ok(_) => timer.stop_and_record(),
                    Err(_) => timer.stop_and_discard(),
                };
                maybe_response
            })
            .boxed();
```
