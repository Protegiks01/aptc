# Audit Report

## Title
TokenDataId Hash Collision via Delimiter Injection in Indexer

## Summary
The `TokenDataId` struct allows `collection` and `name` fields to contain the delimiter characters `::`, which are also used by the indexer to format these fields for hashing. This enables attackers to craft distinct tokens that produce identical hashes, causing database conflicts, data corruption, and API failures in the indexer infrastructure.

## Finding Description

The vulnerability exists across three layers of the system:

**Layer 1: Move Framework - Insufficient Validation**

The Move framework validates only the length of `collection` and `name` strings, not their content. The `create_token_data_id` function permits any valid UTF-8 characters, including `::`. [1](#0-0) 

This validation only checks `collection.length() <= 128` and `name.length() <= 128`, but does not restrict the character set. Since Move's `String` type is defined as a UTF-8 encoded byte vector, the colon character (`:`, ASCII 58) is valid. [2](#0-1) 

**Layer 2: Rust Event Deserialization - No Content Validation**

The Rust side deserializes `TokenDataId` from events without any content validation, only using BCS deserialization. [3](#0-2) [4](#0-3) 

**Layer 3: Indexer Hash Computation - Ambiguous Format**

The indexer computes a hash based on the string representation using a `creator::collection::name` format, where `::` is used as a delimiter. [5](#0-4) [6](#0-5) 

This creates an ambiguity: if `collection` or `name` fields contain `::`, the formatted string becomes ambiguous.

**Attack Scenario:**

1. Attacker creates Token A:
   - creator: `0x1234`
   - collection: `"mycollection"`
   - name: `"token::data"`
   - Formatted: `"0x1234::mycollection::token::data"`
   - Hash: `SHA256("0x1234::mycollection::token::data")`

2. Attacker creates Token B:
   - creator: `0x1234`
   - collection: `"mycollection::token"`
   - name: `"data"`
   - Formatted: `"0x1234::mycollection::token::data"`
   - Hash: `SHA256("0x1234::mycollection::token::data")`

Both produce **identical hashes** despite being different tokens on-chain.

**Database Impact:**

The indexer uses `token_data_id_hash` as part of primary keys and lookup keys across multiple tables: [7](#0-6) 

When hash collisions occur, the indexer may:
- Overwrite one token's data with another's
- Fail to insert records due to constraint violations  
- Return incorrect balances or ownership data through APIs
- Crash or exhibit undefined behavior

The indexer attempts insertion without sanitization first, only cleaning data on retry: [8](#0-7) 

However, this cleanup only removes null bytes, not delimiter characters.

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty criteria:

**High Severity Impacts Demonstrated:**
- **API crashes**: The indexer may crash or return errors when encountering hash collisions in database operations
- **Significant protocol violations**: The indexer is critical infrastructure that wallets, explorers, and dApps rely on for token data

**Concrete User Impact:**
- Users see incorrect token balances in wallets and explorers
- Token ownership tracking becomes corrupted
- NFT marketplaces display wrong ownership information
- APIs return incorrect or inconsistent data

**Note:** This does NOT affect consensus or on-chain state, as the Move VM uses the `TokenDataId` struct directly (not a hash) for state management. The vulnerability is isolated to off-chain indexing infrastructure.

## Likelihood Explanation

**Likelihood: High**

1. **Easy to Execute**: Any user can create tokens with `::` in collection or name fields through standard token creation functions
2. **No Special Privileges Required**: Attackers need no validator access or special permissions
3. **Difficult to Detect**: The collision is subtle and may not be immediately obvious to indexer operators
4. **Natural Occurrence Possible**: Even non-malicious users might unknowingly create collections/tokens with `::` in names, causing accidental collisions

The attack requires only:
- Creating two collections or tokens with carefully chosen names
- Standard Move token creation transactions
- No gas cost beyond normal token creation

## Recommendation

Implement content validation at the Move framework level to prevent delimiter characters in token identifiers:

```move
public fun create_token_data_id(
    creator: address,
    collection: String,
    name: String,
): TokenDataId {
    assert!(collection.length() <= MAX_COLLECTION_NAME_LENGTH, error::invalid_argument(ECOLLECTION_NAME_TOO_LONG));
    assert!(name.length() <= MAX_NFT_NAME_LENGTH, error::invalid_argument(ENFT_NAME_TOO_LONG));
    
    // NEW: Validate that collection and name don't contain delimiter characters
    assert!(!string_contains(&collection, b"::"), error::invalid_argument(EINVALID_TOKEN_NAME_CHARACTERS));
    assert!(!string_contains(&name, b"::"), error::invalid_argument(EINVALID_TOKEN_NAME_CHARACTERS));
    
    TokenDataId { creator, collection, name }
}

// Helper function to check for substring
fun string_contains(s: &String, pattern: &vector<u8>): bool {
    // Implementation to check if string contains pattern
    // ...
}
```

**Alternative Solution**: Change the indexer hash computation to use a non-ambiguous format, such as:
- Use BCS serialization of the struct instead of string formatting
- Use length-prefixed format: `creator::<len>:<collection>:<len>:<name>`
- Use a different delimiter that's validated to be forbidden in the strings

## Proof of Concept

```move
#[test(creator = @0x123)]
fun test_hash_collision_attack(creator: &signer) {
    use std::string;
    
    // Create collection A
    let collection_a = string::utf8(b"mycollection");
    create_collection(creator, collection_a, ...);
    
    // Create Token 1: collection="mycollection", name="token::data"
    let name_1 = string::utf8(b"token::data");
    create_token_data(creator, collection_a, name_1, ...);
    
    // Create collection B  
    let collection_b = string::utf8(b"mycollection::token");
    create_collection(creator, collection_b, ...);
    
    // Create Token 2: collection="mycollection::token", name="data"
    let name_2 = string::utf8(b"data");
    create_token_data(creator, collection_b, name_2, ...);
    
    // Both tokens are valid on-chain but will produce:
    // Hash of: "0x123::mycollection::token::data"
    // This causes indexer hash collision
}
```

**Indexer Verification:**
```rust
// Demonstrate hash collision in Rust
let token_a = TokenDataIdType {
    creator: "0x123".to_string(),
    collection: "mycollection".to_string(),
    name: "token::data".to_string(),
};

let token_b = TokenDataIdType {
    creator: "0x123".to_string(),
    collection: "mycollection::token".to_string(),
    name: "data".to_string(),
};

assert_eq!(token_a.to_hash(), token_b.to_hash()); // Same hash!
assert_ne!(token_a, token_b); // But different tokens!
```

## Notes

**Additional Concerns Beyond Delimiter Injection:**

1. **Null Bytes**: Strings can contain null bytes (`\0`), which PostgreSQL rejects. The indexer handles this via retry logic with `clean_data_for_db`, but this is inefficient and could cause initial insertion failures. [9](#0-8) 

2. **Control Characters**: Newlines, tabs, and other control characters are not validated and could cause log injection or display issues in downstream UIs.

3. **Unicode Exploits**: No validation exists for homoglyphs, zero-width characters, or right-to-left override characters, which could enable phishing attacks where tokens appear to have trusted names.

All of these stem from the same root cause: **insufficient content validation at the Move framework level**.

### Citations

**File:** aptos-move/framework/aptos-token/sources/token.move (L1538-1546)
```text
    public fun create_token_data_id(
        creator: address,
        collection: String,
        name: String,
    ): TokenDataId {
        assert!(collection.length() <= MAX_COLLECTION_NAME_LENGTH, error::invalid_argument(ECOLLECTION_NAME_TOO_LONG));
        assert!(name.length() <= MAX_NFT_NAME_LENGTH, error::invalid_argument(ENFT_NAME_TOO_LONG));
        TokenDataId { creator, collection, name }
    }
```

**File:** aptos-move/framework/move-stdlib/sources/string.move (L11-14)
```text
    /// A `String` holds a sequence of bytes which is guaranteed to be in utf8 format.
    struct String has copy, drop, store {
        bytes: vector<u8>,
    }
```

**File:** types/src/account_config/events/token_deposit.rs (L29-34)
```rust
#[derive(Debug, Clone, Deserialize, Serialize)]
pub struct TokenDataId {
    creator: AccountAddress,
    collection: String,
    name: String,
}
```

**File:** types/src/account_config/events/token_deposit.rs (L45-47)
```rust
    pub fn try_from_bytes(bytes: &[u8]) -> Result<Self> {
        bcs::from_bytes(bytes).map_err(Into::into)
    }
```

**File:** crates/indexer/src/models/token_models/token_utils.rs (L46-48)
```rust
    pub fn to_hash(&self) -> String {
        hash_str(&self.to_string())
    }
```

**File:** crates/indexer/src/models/token_models/token_utils.rs (L67-76)
```rust
impl fmt::Display for TokenDataIdType {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        write!(
            f,
            "{}::{}::{}",
            standardize_address(self.creator.as_str()),
            self.collection,
            self.name
        )
    }
```

**File:** crates/indexer/src/schema.rs (L859-876)
```rust
    tokens (token_data_id_hash, property_version, transaction_version) {
        #[max_length = 64]
        token_data_id_hash -> Varchar,
        property_version -> Numeric,
        transaction_version -> Int8,
        #[max_length = 66]
        creator_address -> Varchar,
        #[max_length = 128]
        collection_name -> Varchar,
        #[max_length = 128]
        name -> Varchar,
        token_properties -> Jsonb,
        inserted_at -> Timestamp,
        #[max_length = 64]
        collection_data_id_hash -> Varchar,
        transaction_timestamp -> Timestamp,
    }
}
```

**File:** crates/indexer/src/processors/token_processor.rs (L229-243)
```rust
        Err(_) => conn
            .build_transaction()
            .read_write()
            .run::<_, Error, _>(|pg_conn| {
                let tokens = clean_data_for_db(tokens, true);
                let token_datas = clean_data_for_db(token_datas, true);
                let token_ownerships = clean_data_for_db(token_ownerships, true);
                let collection_datas = clean_data_for_db(collection_datas, true);
                let current_token_ownerships = clean_data_for_db(current_token_ownerships, true);
                let current_token_datas = clean_data_for_db(current_token_datas, true);
                let current_collection_datas = clean_data_for_db(current_collection_datas, true);
                let token_activities = clean_data_for_db(token_activities, true);
                let current_token_claims = clean_data_for_db(current_token_claims, true);
                let current_ans_lookups = clean_data_for_db(current_ans_lookups, true);
                let nft_points = clean_data_for_db(nft_points, true);
```

**File:** crates/indexer/src/util.rs (L95-97)
```rust
fn string_null_byte_replacement(value: &mut str) -> String {
    value.replace('\u{0000}', "").replace("\\u0000", "")
}
```
