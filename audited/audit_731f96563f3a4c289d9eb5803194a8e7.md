Based on my deep technical validation against the Aptos Core codebase, I have determined that this report describes a **VALID vulnerability**.

# Audit Report

## Title
Consensus Observer Liveness Failure Due to Pending Block Garbage Collection

## Summary
The consensus observer's pending block store garbage collects blocks that exceed the maximum capacity (150 by default), but if the payload for a garbage-collected block arrives later, the block cannot be recovered. This breaks the sequential block processing requirement and causes liveness failure until fallback mode is triggered after 10+ seconds.

## Finding Description
The consensus observer separates block ordering information (`OrderedBlock`) from transaction payloads (`BlockPayload`). When `OrderedBlock` messages arrive without payloads, they are stored in `PendingBlockStore` to await payload arrival. [1](#0-0) 

The pending block store enforces a maximum capacity via `max_num_pending_blocks` (default 150). [2](#0-1) [3](#0-2) 

When this limit is reached, garbage collection automatically removes the oldest pending blocks: [4](#0-3) 

The vulnerability occurs when:
1. Network delays cause 150+ `OrderedBlock` messages to arrive without payloads, filling the store
2. Oldest pending blocks (e.g., round 101) are garbage collected to make room for newer blocks
3. The payload for the garbage-collected block arrives later and is stored in the payload store
4. When `process_block_payload_message` attempts to process ready blocks, `remove_ready_pending_block` returns `None` because the `OrderedBlock` was already garbage collected [5](#0-4) [6](#0-5) 

The consensus observer requires blocks to form a sequential parent-child chain. Blocks are only processed if they extend from the last ordered block: [7](#0-6) 

Without the missing block (101), all subsequent blocks (102, 103, etc.) are rejected because their parent check fails at line 776, even if their payloads exist.

The node cannot make consensus progress. After 10 seconds without version increases, the fallback manager detects the stall: [8](#0-7) 

The system enters fallback mode, clearing all state and using state sync to recover: [9](#0-8) [10](#0-9) 

## Impact Explanation
This meets **High severity** criteria per the Aptos bug bounty program as "Validator Node Slowdowns":

1. **Validator Fullnode Impact**: Consensus observers are enabled by default on Validator Fullnodes (VFNs) [11](#0-10) . VFNs are critical infrastructure serving consensus data to downstream clients and public fullnodes.

2. **Temporary Liveness Failure**: Affected nodes experience 10+ seconds of inability to process consensus updates, qualifying as "significant performance degradation affecting consensus."

3. **Service Degradation**: During liveness failure, the VFN cannot serve clients with current consensus state, degrading reliability of dependent services.

4. **Repeated Occurrence**: In high block rate environments, the condition can recur repeatedly. Test networks use increased pending block limits (300) specifically to accommodate higher block rates [12](#0-11) , indicating this issue is anticipated during high throughput periods.

5. **Fallback Overhead**: Recovery requires falling back to state sync for 10 minutes [13](#0-12) , which is slower and more resource-intensive than direct consensus observation.

## Likelihood Explanation
This has **Medium-to-High** likelihood:

1. **Natural Network Conditions**: Message reordering and delays are common in distributed systems. Normal network variability can trigger this without malicious actors.

2. **No Recovery Mechanism**: Once a pending block is garbage collected from both `blocks_without_payloads` and `blocks_without_payloads_by_hash`, there's no mechanism to re-request the `OrderedBlock` message. The block is permanently lost from the observer's perspective.

3. **Cascading Effect**: A single garbage-collected block breaks the entire chain of subsequent blocks due to the sequential parent-child validation requirement.

## Recommendation
Implement one or more of the following mitigations:

1. **Prioritized Garbage Collection**: Instead of removing oldest blocks indiscriminately, prioritize removal of blocks whose payloads have not arrived within a reasonable timeout, while preserving blocks that form a continuous chain.

2. **Payload-First Processing**: When payloads arrive for missing `OrderedBlock` entries, temporarily cache them and implement a re-request mechanism for the corresponding `OrderedBlock` from peers.

3. **Dynamic Capacity Adjustment**: Increase `max_num_pending_blocks` dynamically based on observed network conditions and block production rate.

4. **Gap Detection**: Detect gaps in the pending block sequence and enter fallback mode proactively rather than waiting for complete liveness failure.

## Proof of Concept
The vulnerability can be demonstrated by:
1. Configuring a VFN with `max_num_pending_blocks = 150`
2. Simulating network delays where `OrderedBlock` messages arrive first, filling the pending store to capacity
3. Causing the oldest block (e.g., round 101) to be garbage collected when block 251 arrives
4. Delivering the `BlockPayload` for round 101 after garbage collection
5. Observing that block 101 cannot be processed, breaking the chain for all subsequent blocks
6. Confirming the 10+ second liveness failure until fallback mode is triggered

---

**Notes**:
This is a protocol-level liveness bug, not a network DoS attack. It occurs through natural network conditions without requiring malicious actor involvement. The fallback mechanism provides recovery but at significant performance cost. The vulnerability is particularly concerning for VFNs that serve as critical infrastructure for downstream clients.

### Citations

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L60-73)
```rust
/// A simple struct to hold blocks that are waiting for payloads
pub struct PendingBlockStore {
    // The configuration of the consensus observer
    consensus_observer_config: ConsensusObserverConfig,

    // A map of ordered blocks that are without payloads. The key is
    // the (epoch, round) of the first block in the ordered block.
    blocks_without_payloads: BTreeMap<(u64, Round), Arc<PendingBlockWithMetadata>>,

    // A map of ordered blocks that are without payloads. The key is
    // the hash of the first block in the ordered block.
    // Note: this is the same as blocks_without_payloads, but with a different key.
    blocks_without_payloads_by_hash: BTreeMap<HashValue, Arc<PendingBlockWithMetadata>>,
}
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L156-195)
```rust
    /// Garbage collects the pending blocks store by removing
    /// the oldest blocks if the store is too large.
    fn garbage_collect_pending_blocks(&mut self) {
        // Verify that both stores have the same number of entries.
        // If not, log an error as this should never happen.
        let num_pending_blocks = self.blocks_without_payloads.len() as u64;
        let num_pending_blocks_by_hash = self.blocks_without_payloads_by_hash.len() as u64;
        if num_pending_blocks != num_pending_blocks_by_hash {
            error!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "The pending block stores have different numbers of entries: {} and {} (by hash)",
                    num_pending_blocks, num_pending_blocks_by_hash
                ))
            );
        }

        // Calculate the number of blocks to remove
        let max_pending_blocks = self.consensus_observer_config.max_num_pending_blocks;
        let num_blocks_to_remove = num_pending_blocks.saturating_sub(max_pending_blocks);

        // Remove the oldest blocks if the store is too large
        for _ in 0..num_blocks_to_remove {
            if let Some((oldest_epoch_round, pending_block)) =
                self.blocks_without_payloads.pop_first()
            {
                // Log a warning message for the removed block
                warn!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "The pending block store is too large: {:?} blocks. Removing the block for the oldest epoch and round: {:?}",
                        num_pending_blocks, oldest_epoch_round
                    ))
                );

                // Remove the block from the hash store
                let first_block = pending_block.ordered_block().first_block();
                self.blocks_without_payloads_by_hash
                    .remove(&first_block.id());
            }
        }
    }
```

**File:** config/src/config/consensus_observer_config.rs (L16-17)
```rust
// Maximum number of pending blocks for test networks (e.g., devnet)
const MAX_NUM_PENDING_BLOCKS_FOR_TEST_NETWORKS: u64 = 300;
```

**File:** config/src/config/consensus_observer_config.rs (L36-37)
```rust
    /// Maximum number of blocks to keep in memory (e.g., pending blocks, ordered blocks, etc.)
    pub max_num_pending_blocks: u64,
```

**File:** config/src/config/consensus_observer_config.rs (L72-72)
```rust
            max_num_pending_blocks: 150, // 150 blocks (sufficient for existing production networks)
```

**File:** config/src/config/consensus_observer_config.rs (L79-79)
```rust
            observer_fallback_duration_ms: 600_000, // 10 minutes
```

**File:** config/src/config/consensus_observer_config.rs (L119-128)
```rust
            NodeType::ValidatorFullnode => {
                if ENABLE_ON_VALIDATOR_FULLNODES
                    && !observer_manually_set
                    && !publisher_manually_set
                {
                    // Enable both the observer and the publisher for VFNs
                    consensus_observer_config.observer_enabled = true;
                    consensus_observer_config.publisher_enabled = true;
                    modified_config = true;
                }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L236-246)
```rust
    /// Enters fallback mode for consensus observer by invoking state sync
    async fn enter_fallback_mode(&mut self) {
        // Terminate all active subscriptions (to ensure we don't process any more messages)
        self.subscription_manager.terminate_all_subscriptions();

        // Clear all the pending block state
        self.clear_pending_block_state().await;

        // Start syncing for the fallback
        self.state_sync_manager.sync_for_fallback();
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L435-438)
```rust
        if verified_payload {
            self.order_ready_pending_block(block_epoch, block_round)
                .await;
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L773-800)
```rust
        // The block was verified correctly. If the block is a child of our
        // last block, we can insert it into the ordered block store.
        let last_ordered_block = self.observer_block_data.lock().get_last_ordered_block();
        if last_ordered_block.id() == ordered_block.first_block().parent_id() {
            // Update the latency metrics for ordered block processing
            update_message_processing_latency_metrics(
                message_received_time,
                &peer_network_id,
                metrics::ORDERED_BLOCK_LABEL,
            );

            // Insert the ordered block into the pending blocks
            self.observer_block_data
                .lock()
                .insert_ordered_block(observed_ordered_block.clone());

            // If state sync is not syncing to a commit, finalize the ordered blocks
            if !self.state_sync_manager.is_syncing_to_commit() {
                self.finalize_ordered_block(ordered_block).await;
            }
        } else {
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Parent block for ordered block is missing! Ignoring: {:?}",
                    ordered_block.proof_block_info()
                ))
            );
        }
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L93-105)
```rust
    pub fn clear_block_data(&mut self) -> LedgerInfoWithSignatures {
        // Clear the payload store
        self.block_payload_store.clear_all_payloads();

        // Clear the ordered blocks
        self.ordered_block_store.clear_all_ordered_blocks();

        // Clear the pending blocks
        self.pending_block_store.clear_missing_blocks();

        // Return the root ledger info
        self.root()
    }
```

**File:** consensus/src/consensus_observer/observer/block_data.rs (L244-254)
```rust
    pub fn remove_ready_pending_block(
        &mut self,
        received_payload_epoch: u64,
        received_payload_round: Round,
    ) -> Option<Arc<PendingBlockWithMetadata>> {
        self.pending_block_store.remove_ready_block(
            received_payload_epoch,
            received_payload_round,
            &mut self.block_payload_store,
        )
    }
```

**File:** consensus/src/consensus_observer/observer/fallback_manager.rs (L89-116)
```rust
    fn verify_increasing_sync_versions(
        &mut self,
        latest_ledger_info_version: Version,
        time_now: Instant,
    ) -> Result<(), Error> {
        // Verify that the synced version is increasing appropriately
        let (highest_synced_version, highest_version_timestamp) =
            self.highest_synced_version_and_time;
        if latest_ledger_info_version <= highest_synced_version {
            // The synced version hasn't increased. Check if we should enter fallback mode.
            let duration_since_highest_seen = time_now.duration_since(highest_version_timestamp);
            let fallback_threshold = Duration::from_millis(
                self.consensus_observer_config
                    .observer_fallback_progress_threshold_ms,
            );
            if duration_since_highest_seen > fallback_threshold {
                Err(Error::ObserverProgressStopped(format!(
                    "Consensus observer is not making progress! Highest synced version: {}, elapsed: {:?}",
                    highest_synced_version, duration_since_highest_seen
                )))
            } else {
                Ok(()) // We haven't passed the fallback threshold yet
            }
        } else {
            // The synced version has increased. Update the highest synced version and time.
            self.highest_synced_version_and_time = (latest_ledger_info_version, time_now);
            Ok(())
        }
```
