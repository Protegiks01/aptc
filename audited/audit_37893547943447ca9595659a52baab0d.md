# Audit Report

## Title
Race Condition in Pruner Reset During State Snapshot Finalization Causes Data Loss

## Summary
A race condition exists in `finalize_state_snapshot()` where updating `min_readable_version` for all pruners does not synchronize with ongoing pruner worker threads. This allows pruner workers to continue deleting historical data beyond the newly set minimum readable version, causing premature data deletion and state inconsistency.

## Finding Description

The vulnerability occurs at lines 225-234 in `finalize_state_snapshot()` where `save_min_readable_version()` is called sequentially for all four pruners (ledger, state_merkle, epoch_snapshot, state_kv): [1](#0-0) 

The `save_min_readable_version()` method only updates the atomic `min_readable_version` field and persists it to disk, but crucially **does NOT update the pruner worker's `target_version`**: [2](#0-1) 

In contrast, the normal pruning path uses `set_pruner_target_db_version()` which updates BOTH the `min_readable_version` AND the pruner worker's `target_version`: [3](#0-2) 

The pruner worker thread continuously executes its `prune()` method using only the `target_version`, never checking `min_readable_version`: [4](#0-3) 

**Attack Scenario:**

1. Node is operating normally with latest_version = 10000, prune_window = 1000
2. Normal pruning sets: min_readable_version = 9000, pruner worker target_version = 9000
3. Pruner worker is actively deleting versions (e.g., progress = 8500)
4. State sync restore calls `finalize_state_snapshot(8700)` to restore from a backup
5. All pruners' `save_min_readable_version(8700)` executes, setting atomic min_readable_version = 8700
6. **But pruner worker's target_version remains 9000**
7. Pruner continues executing, deleting versions 8700-8999
8. Subsequent read requests for versions in [8700, 9000) will pass the validation check but fail to retrieve data

The validation in `error_if_ledger_pruned()` reads the updated `min_readable_version` and incorrectly assumes data exists: [5](#0-4) 

This breaks the **State Consistency** invariant - the system guarantees that data >= min_readable_version should be readable, but the pruner has deleted it.

## Impact Explanation

This is a **High Severity** vulnerability per the Aptos bug bounty criteria due to:

1. **Significant Protocol Violation**: The min_readable_version represents a critical storage invariant that data at or above this version should be available. Breaking this causes state inconsistency.

2. **Data Loss**: Historical ledger data, state merkle tree nodes, epoch snapshots, and state KV pairs can be prematurely deleted despite the system claiming they should exist.

3. **Node Failures**: When queries request data in the deleted range, the `error_if_ledger_pruned()` check passes but data retrieval fails, potentially causing node crashes or incorrect responses to state sync requests.

4. **State Sync Corruption**: Nodes restoring from state snapshots may have their data immediately pruned, making the restore operation ineffective and requiring manual intervention or node restarts.

While this doesn't directly cause loss of funds or consensus violations, it represents a **significant protocol violation** affecting storage layer integrity and could require manual intervention to recover, meeting the High severity threshold.

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability can occur in several realistic scenarios:

1. **State Sync During Active Operations**: When a node performs state sync restore while pruning is enabled, which is the default configuration
2. **Node Recovery**: During node restarts or failover scenarios where snapshot finalization overlaps with pruner initialization
3. **Fast Sync Mode**: During bootstrap operations that use `finalize_state_snapshot()`

The vulnerability requires:
- Pruners to be enabled (default configuration)
- `finalize_state_snapshot()` to be called with a version lower than the current pruner target
- Timing overlap where pruner worker is actively running

While not trivially exploitable by external attackers, this is a natural race condition that can occur during normal operations, particularly during state sync and node recovery scenarios that are common in production deployments.

## Recommendation

Introduce proper synchronization between `save_min_readable_version()` and the pruner worker's target version. The fix should ensure that when `min_readable_version` is updated during snapshot finalization, the pruner worker is either:

1. **Stopped and restarted** with the new minimum version, OR
2. **Target version updated** to match the new minimum version

**Recommended Fix:**

Add a method to update both the min_readable_version and pruner worker target atomically:

```rust
// In PrunerManager trait
fn reset_min_readable_version(&self, version: Version) -> Result<()>;

// In LedgerPrunerManager implementation
fn reset_min_readable_version(&self, version: Version) -> Result<()> {
    self.min_readable_version.store(version, Ordering::SeqCst);
    
    PRUNER_VERSIONS
        .with_label_values(&["ledger_pruner", "min_readable"])
        .set(version as i64);
    
    // Update pruner worker target to prevent deletion below min_readable
    if let Some(worker) = &self.pruner_worker {
        worker.set_target_db_version(version);
    }
    
    self.ledger_db.write_pruner_progress(version)
}
```

Then in `finalize_state_snapshot()`, replace `save_min_readable_version()` calls with `reset_min_readable_version()`:

```rust
self.ledger_pruner.reset_min_readable_version(version)?;
self.state_store.state_merkle_pruner.reset_min_readable_version(version)?;
self.state_store.epoch_snapshot_pruner.reset_min_readable_version(version)?;
self.state_store.state_kv_pruner.reset_min_readable_version(version)?;
```

Alternatively, add explicit locking or stop pruner workers before updating min_readable_version during snapshot finalization.

## Proof of Concept

```rust
#[test]
fn test_pruner_race_during_snapshot_finalization() {
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;
    
    // Setup: Create AptosDB with pruning enabled (prune_window = 1000)
    let tmpdir = aptos_temppath::TempPath::new();
    let db = Arc::new(AptosDB::new_for_test_with_pruner(
        tmpdir.path(),
        LedgerPrunerConfig {
            enable: true,
            prune_window: 1000,
            batch_size: 500,
            user_pruning_window_offset: 0,
        },
    ));
    
    // Commit transactions up to version 10000
    for i in 0..10000 {
        // ... commit transaction at version i ...
    }
    
    // Trigger normal pruning - sets min_readable = 9000, target = 9000
    db.ledger_pruner.maybe_set_pruner_target_db_version(10000);
    
    // Wait for pruner to start (progress at ~8500)
    thread::sleep(Duration::from_millis(100));
    
    // Simulate state sync restore at version 8700
    // This calls save_min_readable_version(8700) 
    let snapshot_version = 8700;
    db.ledger_pruner.save_min_readable_version(snapshot_version).unwrap();
    
    // Check: min_readable_version is now 8700
    assert_eq!(db.ledger_pruner.get_min_readable_version(), 8700);
    
    // Wait for pruner to continue (it still has target = 9000)
    thread::sleep(Duration::from_millis(500));
    
    // BUG: Try to read version 8800 (within [8700, 9000) range)
    // error_if_ledger_pruned passes (8800 >= 8700) 
    // but data retrieval fails because pruner deleted it!
    let result = db.get_transaction_by_version(8800, false);
    
    // This should succeed per min_readable_version = 8700
    // but will fail because pruner deleted up to 9000
    match result {
        Err(e) => {
            // Vulnerability confirmed: data expected but missing
            println!("Race condition confirmed: {:?}", e);
            assert!(e.to_string().contains("pruned") || 
                    e.to_string().contains("not found"));
        },
        Ok(_) => panic!("Expected data to be missing due to race condition"),
    }
}
```

## Notes

This vulnerability affects all four pruner types (ledger_pruner, state_merkle_pruner, epoch_snapshot_pruner, state_kv_pruner) as they all use the same pattern where `save_min_readable_version()` doesn't synchronize with the pruner worker's `target_version`. The issue is particularly concerning because `finalize_state_snapshot()` is called during critical state sync operations where data integrity is paramount.

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L225-234)
```rust
            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L80-89)
```rust
    fn save_min_readable_version(&self, min_readable_version: Version) -> Result<()> {
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.ledger_db.write_pruner_progress(min_readable_version)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L162-176)
```rust
    fn set_pruner_target_db_version(&self, latest_version: Version) {
        assert!(self.pruner_worker.is_some());
        let min_readable_version = latest_version.saturating_sub(self.prune_window);
        self.min_readable_version
            .store(min_readable_version, Ordering::SeqCst);

        PRUNER_VERSIONS
            .with_label_values(&["ledger_pruner", "min_readable"])
            .set(min_readable_version as i64);

        self.pruner_worker
            .as_ref()
            .unwrap()
            .set_target_db_version(min_readable_version);
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L261-271)
```rust
    pub(super) fn error_if_ledger_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.ledger_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```
