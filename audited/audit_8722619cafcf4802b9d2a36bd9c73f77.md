# Audit Report

## Title
EpochRetrievalRequest Message Replay Enables Resource Exhaustion Attack on Validators

## Summary
The `EpochRetrievalRequest` message handler lacks replay protection, rate limiting, and response caching. An attacker can repeatedly send identical requests to force validators to perform expensive database queries and proof generation operations, causing CPU, I/O, and network resource exhaustion. [1](#0-0) 

## Finding Description

The `EpochRetrievalRequest` structure contains only `start_epoch` and `end_epoch` fields with no nonce, timestamp, or other replay protection mechanism. [2](#0-1) 

The `process_epoch_retrieval` function processes every received request by:
1. Querying the database via `get_epoch_ending_ledger_infos()` to retrieve up to 100 epoch ending ledger infos
2. Constructing an `EpochChangeProof` with validator signatures
3. Sending the response over the network [3](#0-2) 

The only validation is checking that `request.end_epoch <= self.epoch()`, which does not prevent replay attacks. [4](#0-3) 

The database query can retrieve up to `MAX_NUM_EPOCH_ENDING_LEDGER_INFO` (100) entries per request. [5](#0-4) 

**Deduplication Analysis:** [6](#0-5) 

Messages are queued in an `aptos_channel` with key `(peer_id, discriminant(message_type))`. The discriminant is identical for all `EpochRetrievalRequest` messages regardless of epoch parameters. [7](#0-6) 

The `PerKeyQueue` only prevents multiple messages from being queued simultaneously per key. Once a message is dequeued and processed, an identical message can be immediately requeued and reprocessed. There is no persistent deduplication or rate limiting.

**Attack Path:**
1. Attacker connects as a network peer
2. Sends `EpochRetrievalRequest(start_epoch: 1, end_epoch: 100)`
3. Validator processes request, performs database query, constructs proof, sends response
4. Message is dequeued from channel
5. Attacker immediately sends identical request
6. Validator processes it again with full database query and proof generation
7. Repeat continuously

## Impact Explanation

This vulnerability causes **validator node slowdowns** through resource exhaustion:

- **CPU**: Repeated serialization of large `LedgerInfoWithSignatures` objects containing validator signatures
- **I/O**: Repeated database reads from epoch metadata storage
- **Network**: Repeated transmission of potentially large `EpochChangeProof` responses

This qualifies as **Medium Severity** ($10,000) under the Aptos bug bounty program. While it degrades validator performance, it does not directly cause consensus failure, fund loss, or total liveness failure. The attack is limited to a specific message type and requires sustained effort to significantly impact operations.

## Likelihood Explanation

**High likelihood** of exploitation:
- **Low complexity**: Simply send the same message repeatedly
- **No authentication**: Only requires network peer connectivity
- **No special privileges**: Any peer can send consensus messages
- **Scalable**: Single attacker can target all validators simultaneously
- **Detection difficulty**: Requests appear legitimate and comply with protocol

The attack is immediately exploitable by any malicious peer with no special setup required.

## Recommendation

Implement multi-layered protection:

**1. Request Deduplication Cache:**
```rust
// Add to EpochManager struct
request_cache: Arc<Mutex<LruCache<(u64, u64), Instant>>>,

fn process_epoch_retrieval(
    &mut self,
    request: EpochRetrievalRequest,
    peer_id: AccountAddress,
) -> anyhow::Result<()> {
    let key = (request.start_epoch, request.end_epoch);
    let now = Instant::now();
    
    // Check if we recently processed this request
    let mut cache = self.request_cache.lock();
    if let Some(last_processed) = cache.get(&key) {
        if now.duration_since(*last_processed) < Duration::from_secs(5) {
            debug!("Ignoring duplicate epoch retrieval request within 5s window");
            return Ok(());
        }
    }
    cache.insert(key, now);
    drop(cache);
    
    // ... existing processing logic
}
```

**2. Per-Peer Rate Limiting:**
```rust
// Track requests per peer
peer_request_tracker: Arc<Mutex<HashMap<AccountAddress, RateLimiter>>>,

// Before processing, check rate limit
if !self.peer_request_tracker.lock()
    .entry(peer_id)
    .or_insert_with(|| RateLimiter::new(5, Duration::from_secs(60)))
    .check_allow() 
{
    warn!("Rate limit exceeded for epoch retrieval from {}", peer_id);
    return Ok(());
}
```

**3. Response Caching:**
Cache constructed `EpochChangeProof` responses to avoid regenerating identical proofs.

## Proof of Concept

```rust
#[tokio::test]
async fn test_epoch_retrieval_replay_attack() {
    // Setup validator node with epoch manager
    let (mut epoch_manager, network_sender) = setup_epoch_manager_for_test();
    
    let attacker_peer = AccountAddress::random();
    let request = EpochRetrievalRequest {
        start_epoch: 1,
        end_epoch: 50,
    };
    
    // Send same request repeatedly
    for i in 0..100 {
        let start = Instant::now();
        
        // Send request
        epoch_manager.process_epoch_retrieval(
            request.clone(),
            attacker_peer,
        ).unwrap();
        
        let duration = start.elapsed();
        println!("Request {} took {:?}", i, duration);
        
        // Verify each request triggers full database query
        // by measuring consistent processing time
        assert!(duration > Duration::from_millis(10));
    }
    
    // Verify resource consumption
    // - Database query count should be 100
    // - CPU time should be substantial
    // - Network bandwidth consumed
}
```

## Notes

Network-level bandwidth rate limiting exists but operates on bytes per second, not message count. Small request messages can bypass this protection while still causing expensive processing. The vulnerability exists at the application protocol layer where message-level semantics should enforce additional protections beyond raw bandwidth limits.

### Citations

**File:** consensus/consensus-types/src/epoch_retrieval.rs (L7-12)
```rust
/// Request to get a EpochChangeProof from current_epoch to target_epoch
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct EpochRetrievalRequest {
    pub start_epoch: u64,
    pub end_epoch: u64,
}
```

**File:** consensus/src/epoch_manager.rs (L451-476)
```rust
    fn process_epoch_retrieval(
        &mut self,
        request: EpochRetrievalRequest,
        peer_id: AccountAddress,
    ) -> anyhow::Result<()> {
        debug!(
            LogSchema::new(LogEvent::ReceiveEpochRetrieval)
                .remote_peer(peer_id)
                .epoch(self.epoch()),
            "[EpochManager] receive {}", request,
        );
        let proof = self
            .storage
            .aptos_db()
            .get_epoch_ending_ledger_infos(request.start_epoch, request.end_epoch)
            .map_err(DbError::from)
            .context("[EpochManager] Failed to get epoch proof")?;
        let msg = ConsensusMsg::EpochChangeProof(Box::new(proof));
        if let Err(err) = self.network_sender.send_to(peer_id, msg) {
            warn!(
                "[EpochManager] Failed to send epoch proof to {}, with error: {:?}",
                peer_id, err,
            );
        }
        Ok(())
    }
```

**File:** consensus/src/epoch_manager.rs (L1677-1686)
```rust
            ConsensusMsg::EpochRetrievalRequest(request) => {
                ensure!(
                    request.end_epoch <= self.epoch(),
                    "[EpochManager] Received EpochRetrievalRequest beyond what we have locally"
                );
                monitor!(
                    "process_epoch_retrieval",
                    self.process_epoch_retrieval(*request, peer_id)
                )?;
            },
```

**File:** storage/aptosdb/src/common.rs (L9-9)
```rust
pub(crate) const MAX_NUM_EPOCH_ENDING_LEDGER_INFO: usize = 100;
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L995-1064)
```rust
    fn get_epoch_ending_ledger_infos(
        &self,
        start_epoch: u64,
        end_epoch: u64,
    ) -> Result<(Vec<LedgerInfoWithSignatures>, bool)> {
        self.get_epoch_ending_ledger_infos_impl(
            start_epoch,
            end_epoch,
            MAX_NUM_EPOCH_ENDING_LEDGER_INFO,
        )
    }

    fn check_epoch_ending_ledger_infos_request(
        &self,
        start_epoch: u64,
        end_epoch: u64,
    ) -> Result<()> {
        ensure!(
            start_epoch <= end_epoch,
            "Bad epoch range [{}, {})",
            start_epoch,
            end_epoch,
        );
        // Note that the latest epoch can be the same with the current epoch (in most cases), or
        // current_epoch + 1 (when the latest ledger_info carries next validator set)

        let latest_epoch = self
            .ledger_db
            .metadata_db()
            .get_latest_ledger_info()?
            .ledger_info()
            .next_block_epoch();
        ensure!(
            end_epoch <= latest_epoch,
            "Unable to provide epoch change ledger info for still open epoch. asked upper bound: {}, last sealed epoch: {}",
            end_epoch,
            latest_epoch - 1,  // okay to -1 because genesis LedgerInfo has .next_block_epoch() == 1
        );
        Ok(())
    }

    pub(super) fn get_epoch_ending_ledger_infos_impl(
        &self,
        start_epoch: u64,
        end_epoch: u64,
        limit: usize,
    ) -> Result<(Vec<LedgerInfoWithSignatures>, bool)> {
        self.check_epoch_ending_ledger_infos_request(start_epoch, end_epoch)?;

        let (paging_epoch, more) = if end_epoch - start_epoch > limit as u64 {
            (start_epoch + limit as u64, true)
        } else {
            (end_epoch, false)
        };

        let lis = self
            .ledger_db
            .metadata_db()
            .get_epoch_ending_ledger_info_iter(start_epoch, paging_epoch)?
            .collect::<Result<Vec<_>>>()?;

        ensure!(
            lis.len() == (paging_epoch - start_epoch) as usize,
            "DB corruption: missing epoch ending ledger info for epoch {}",
            lis.last()
                .map(|li| li.ledger_info().next_block_epoch() - 1)
                .unwrap_or(start_epoch),
        );
        Ok((lis, more))
    }
```

**File:** consensus/src/network.rs (L757-761)
```rust
        let (consensus_messages_tx, consensus_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            10,
            Some(&counters::CONSENSUS_CHANNEL_MSGS),
        );
```

**File:** crates/channel/src/message_queues.rs (L112-152)
```rust
    pub(crate) fn push(&mut self, key: K, message: T) -> Option<T> {
        if let Some(c) = self.counters.as_ref() {
            c.with_label_values(&["enqueued"]).inc();
        }

        let key_message_queue = self
            .per_key_queue
            .entry(key.clone())
            // Only allocate a small initial queue for a new key. Previously, we
            // allocated a queue with all `max_queue_size_per_key` entries;
            // however, this breaks down when we have lots of transient peers.
            // For example, many of our queues have a max capacity of 1024. To
            // handle a single rpc from a transient peer, we would end up
            // allocating ~ 96 b * 1024 ~ 64 Kib per queue.
            .or_insert_with(|| VecDeque::with_capacity(1));

        // Add the key to our round-robin queue if it's not already there
        if key_message_queue.is_empty() {
            self.round_robin_queue.push_back(key);
        }

        // Push the message to the actual key message queue
        if key_message_queue.len() >= self.max_queue_size.get() {
            if let Some(c) = self.counters.as_ref() {
                c.with_label_values(&["dropped"]).inc();
            }
            match self.queue_style {
                // Drop the newest message for FIFO
                QueueStyle::FIFO => Some(message),
                // Drop the oldest message for LIFO
                QueueStyle::LIFO | QueueStyle::KLAST => {
                    let oldest = key_message_queue.pop_front();
                    key_message_queue.push_back(message);
                    oldest
                },
            }
        } else {
            key_message_queue.push_back(message);
            None
        }
    }
```
