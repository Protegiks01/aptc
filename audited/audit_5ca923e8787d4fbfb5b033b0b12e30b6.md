# Audit Report

## Title
TOCTOU Race Condition in PeersAndMetadata::subscribe() Causes Subscribers to Miss Peer Connection Events

## Summary
The `subscribe()` function in `PeersAndMetadata` contains a Time-of-Check-Time-of-Use (TOCTOU) race condition where the read lock on peer state is released before the subscriber is added to the broadcast list, creating a window where peer connection events can be lost.

## Finding Description

In the `PeersAndMetadata::subscribe()` function, there is a critical race condition between reading the current peer state and registering for future updates. [1](#0-0) 

The vulnerable sequence is:

1. **Line 402**: The function acquires a read lock on `peers_and_metadata` and reads current peer state
2. **Lines 403-414**: It iterates through all peers and sends `NewPeer` events to the new subscriber  
3. **After line 414**: The `peers_and_metadata` variable goes out of scope, **releasing the read lock**
4. **Line 416**: Only then does it acquire the `subscribers` lock to add the sender

The comment on line 415 reveals the developer's incorrect assumption: `"I expect the peers_and_metadata read lock to still be in effect until after listeners.push() below"`. However, this is false—the read lock is released when the variable goes out of scope after the loop.

This creates a race window where:
- Thread A (subscriber): Reads peer state {X, Y}, sends initial events
- Thread A: **Releases read lock** (peers_and_metadata out of scope)
- Thread B (connection handler): Acquires write lock, adds peer Z, updates cache
- Thread B: Calls `broadcast(NewPeer(Z))` via `insert_connection_metadata()` [2](#0-1) 
- Thread B: Releases write lock  
- Thread A: Acquires subscribers lock and adds sender to list
- **Result**: New subscriber missed the `NewPeer(Z)` event

This violates the documented contract in the function comment: *"subscribe() immediately sends all* current connections as NewPeer events"* and ensures subscribers receive ongoing events. [3](#0-2) 

## Impact Explanation

This breaks the **State Consistency** invariant, causing subscribers to maintain an incorrect view of connected peers. The primary victim is the health checker, which uses `subscribe()` to track peer connections: [4](#0-3) 

When the health checker misses a `NewPeer` event:
- The peer is not added to its internal `health_check_data` map [5](#0-4) 
- The `connected_peers()` function returns an incomplete peer list [6](#0-5) 
- That peer is never pinged for health checks
- Validator health monitoring becomes inconsistent with actual network state

This qualifies as **Medium Severity** per the Aptos bug bounty: *"State inconsistencies requiring intervention"*. While not immediately breaking consensus, this creates divergent state where network monitoring systems have an incorrect view of peer connectivity.

## Likelihood Explanation

**High likelihood** under normal operation:
- Every time a new network component calls `subscribe()` (health checker startup, network reconfigurations)
- Concurrent with active peer connections being established
- No special attacker capabilities required—it's a natural timing race in multi-threaded environments
- More likely during network startup or epoch transitions when many peers connect simultaneously

## Recommendation

Fix the race by ensuring atomic registration. Acquire the `subscribers` lock **before** releasing the `peers_and_metadata` read lock:

```rust
pub fn subscribe(&self) -> tokio::sync::mpsc::Receiver<ConnectionNotification> {
    let (sender, receiver) = tokio::sync::mpsc::channel(NOTIFICATION_BACKLOG);
    
    // Acquire subscribers lock FIRST to ensure atomicity
    let mut listeners = self.subscribers.lock();
    
    // Then read peer state while still holding subscribers lock
    let peers_and_metadata = self.peers_and_metadata.read();
    'outer: for (network_id, network_peers_and_metadata) in peers_and_metadata.iter() {
        for (_addr, peer_metadata) in network_peers_and_metadata.iter() {
            let event = ConnectionNotification::NewPeer(
                peer_metadata.connection_metadata.clone(),
                *network_id,
            );
            if let Err(err) = sender.try_send(event) {
                warn!("could not send initial NewPeer on subscribe(): {:?}", err);
                break 'outer;
            }
        }
    }
    
    // Add to subscribers while still holding both locks
    listeners.push(sender);
    // Both locks released here atomically
    
    receiver
}
```

This ensures that once a subscriber reads the peer state, no `broadcast()` calls can occur before the subscriber is registered, eliminating the race window.

## Proof of Concept

```rust
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_subscribe_race_condition() {
    use std::sync::Arc;
    use tokio::sync::Barrier;
    
    let peers_and_metadata = PeersAndMetadata::new(&[NetworkId::Validator]);
    let peers_and_metadata = Arc::new(peers_and_metadata);
    let barrier = Arc::new(Barrier::new(2));
    
    // Thread 1: Subscribe and count initial events
    let pm1 = peers_and_metadata.clone();
    let b1 = barrier.clone();
    let subscriber = tokio::spawn(async move {
        b1.wait().await;
        let mut rx = pm1.subscribe();
        let mut count = 0;
        // Drain all immediately available messages
        while rx.try_recv().is_ok() {
            count += 1;
        }
        count
    });
    
    // Thread 2: Add peer during subscription
    let pm2 = peers_and_metadata.clone();
    let b2 = barrier.clone();
    let inserter = tokio::spawn(async move {
        b2.wait().await;
        // Give subscribe() time to read state but not register
        tokio::time::sleep(Duration::from_micros(10)).await;
        
        let peer_id = PeerId::random();
        let conn_meta = ConnectionMetadata::mock(peer_id);
        pm2.insert_connection_metadata(
            PeerNetworkId::new(NetworkId::Validator, peer_id),
            conn_meta
        ).unwrap();
    });
    
    subscriber.await.unwrap();
    inserter.await.unwrap();
    
    // If no race: subscriber should have 1 event (the peer that was inserted)
    // If race occurs: subscriber might have 0 events (missed the broadcast)
    // Run this test repeatedly to trigger the race condition
}
```

## Notes

The vulnerability exists because Rust's RAII lock guards are dropped at scope boundaries, not at statement boundaries. The developer's comment indicates awareness of the need for atomicity but incorrect understanding of when the lock is released. The fix requires carefully ordering lock acquisitions to maintain the atomicity guarantee that subscribers expect.

### Citations

**File:** network/framework/src/application/storage.rs (L209-212)
```rust
        let event =
            ConnectionNotification::NewPeer(connection_metadata, peer_network_id.network_id());
        self.broadcast(event);

```

**File:** network/framework/src/application/storage.rs (L397-399)
```rust
    /// subscribe() returns a channel for receiving NewPeer/LostPeer events.
    /// subscribe() immediately sends all* current connections as NewPeer events.
    /// (* capped at NOTIFICATION_BACKLOG, currently 1000, use get_connected_peers() to be sure)
```

**File:** network/framework/src/application/storage.rs (L400-419)
```rust
    pub fn subscribe(&self) -> tokio::sync::mpsc::Receiver<ConnectionNotification> {
        let (sender, receiver) = tokio::sync::mpsc::channel(NOTIFICATION_BACKLOG);
        let peers_and_metadata = self.peers_and_metadata.read();
        'outer: for (network_id, network_peers_and_metadata) in peers_and_metadata.iter() {
            for (_addr, peer_metadata) in network_peers_and_metadata.iter() {
                let event = ConnectionNotification::NewPeer(
                    peer_metadata.connection_metadata.clone(),
                    *network_id,
                );
                if let Err(err) = sender.try_send(event) {
                    warn!("could not send initial NewPeer on subscribe(): {:?}", err);
                    break 'outer;
                }
            }
        }
        // I expect the peers_and_metadata read lock to still be in effect until after listeners.push() below
        let mut listeners = self.subscribers.lock();
        listeners.push(sender);
        receiver
    }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L209-227)
```rust
                conn_event = connection_events.select_next_some() => {
                    match conn_event {
                        ConnectionNotification::NewPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.create_peer_and_health_data(
                                    metadata.remote_peer_id, self.round
                                );
                            }
                        }
                        ConnectionNotification::LostPeer(metadata, network_id) => {
                            // PeersAndMetadata is a global singleton across all networks; filter connect/disconnect events to the NetworkId that this HealthChecker instance is watching
                            if network_id == self_network_id {
                                self.network_interface.remove_peer_and_health_data(
                                    &metadata.remote_peer_id
                                );
                            }
                        }
                    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L59-61)
```rust
    pub fn connected_peers(&self) -> Vec<PeerId> {
        self.health_check_data.read().keys().cloned().collect()
    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L95-101)
```rust
    pub fn create_peer_and_health_data(&mut self, peer_id: PeerId, round: u64) {
        self.health_check_data
            .write()
            .entry(peer_id)
            .and_modify(|health_check_data| health_check_data.round = round)
            .or_insert_with(|| HealthCheckData::new(round));
    }
```
