# Audit Report

## Title
Unbounded Async Task Spawning and Infinite Retry Loop in Indexer gRPC Data Service Enables Denial of Service

## Summary
The `fetch_transactions()` function in the indexer-grpc-data-service-v2 contains an infinite retry loop without timeout, backoff, or iteration limits. When upstream gRPC calls fail or return mismatched transaction versions, the function loops indefinitely while holding async task resources. Multiple concurrent client requests can spawn numerous such infinite loops, exhausting system resources and preventing legitimate requests from being processed.

## Finding Description
The vulnerability exists in the data fetching logic where each client request can trigger an unbounded infinite loop: [1](#0-0) 

The loop only exits if the gRPC call succeeds AND returns either empty transactions OR transactions with the correct starting version. Otherwise, it continues forever with no sleep, backoff, or retry limit. The TODO comment acknowledges missing error handling.

The attack path is:

1. **Entry Point**: Unauthenticated clients send `GetTransactionsRequest` to the gRPC API [2](#0-1) 

2. **Task Spawning**: Each request spawns a new async task without concurrency limits [3](#0-2) 

3. **Cache Miss Triggers Fetch**: When requested data is not in cache, `fetch_past_data` is called [4](#0-3) 

4. **Infinite Loop Triggered**: `fetch_transactions` is invoked and can loop forever [5](#0-4) 

An attacker can trigger the infinite loop by:
- Causing upstream gRPC failures (network errors, service unavailability)
- Requesting versions that cause version mismatches in responses
- Exploiting race conditions where upstream data is inconsistent

This violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits."

## Impact Explanation
This is **High Severity** per Aptos bug bounty criteria for "API crashes" - the indexer gRPC API becomes unresponsive or severely degraded when resources are exhausted. While not affecting consensus directly, the indexer is critical infrastructure for clients querying blockchain data.

Each stuck task:
- Consumes memory for task state and stack
- May make rapid gRPC calls in tight loops if errors are immediate
- Holds file descriptors for network connections
- Never releases resources until service restart

With enough concurrent malicious requests (hundreds to thousands depending on system resources), the service becomes unable to handle legitimate queries, requiring manual intervention to restore availability.

## Likelihood Explanation
**High Likelihood**. The attack requires:
- No authentication (gRPC endpoint is publicly accessible)
- No special privileges or insider access
- Simple concurrent requests with specific version parameters
- Upstream service errors or inconsistencies (which occur naturally during network issues or service upgrades)

The service configuration shows no concurrency limits on request handling, and the default Tokio runtime has no hard task cap, making exploitation straightforward. [6](#0-5) 

## Recommendation
Implement multiple defensive layers:

1. **Add timeout and retry limits** to `fetch_transactions()`:
```rust
pub(super) async fn fetch_transactions(&self, starting_version: u64) -> Vec<Transaction> {
    const MAX_RETRIES: u32 = 5;
    const RETRY_DELAY_MS: u64 = 500;
    const REQUEST_TIMEOUT: Duration = Duration::from_secs(30);
    
    for attempt in 0..MAX_RETRIES {
        let request = GetTransactionsRequest { /* ... */ };
        
        match tokio::time::timeout(REQUEST_TIMEOUT, async {
            let mut client = self.connection_manager.get_grpc_manager_client_for_request();
            client.get_transactions(request.clone()).await
        }).await {
            Ok(Ok(response)) => {
                let transactions = response.into_inner().transactions;
                if transactions.is_empty() {
                    return vec![];
                }
                if transactions.first().unwrap().version == starting_version {
                    return transactions;
                }
                // Version mismatch - log and retry with backoff
                warn!("Version mismatch: expected {}, got {}", 
                      starting_version, transactions.first().unwrap().version);
            }
            Ok(Err(e)) => {
                error!("gRPC error: {:?}", e);
            }
            Err(_) => {
                error!("Request timeout after {:?}", REQUEST_TIMEOUT);
            }
        }
        
        if attempt < MAX_RETRIES - 1 {
            tokio::time::sleep(Duration::from_millis(RETRY_DELAY_MS * (1 << attempt))).await;
        }
    }
    
    // Return empty after exhausting retries
    vec![]
}
```

2. **Add concurrency limits** using a semaphore in the request handler to cap concurrent `fetch_transactions` calls

3. **Implement request deduplication** to prevent multiple concurrent fetches for the same version

4. **Add monitoring and alerts** for stuck tasks and excessive retry rates

## Proof of Concept
```rust
#[tokio::test]
async fn test_dos_via_concurrent_failing_requests() {
    use tokio::time::Duration;
    use std::sync::Arc;
    
    // Setup: Start indexer service with mocked upstream that returns errors
    // or mismatched versions
    
    // Attack: Spawn many concurrent requests
    let mut handles = vec![];
    for i in 0..100 {
        let handle = tokio::spawn(async move {
            let request = GetTransactionsRequest {
                starting_version: Some(i * 1000),
                transactions_count: None,
                batch_size: None,
                transaction_filter: None,
            };
            // This will trigger fetch_transactions() which loops forever
            // if upstream returns errors
            client.get_transactions(request).await
        });
        handles.push(handle);
    }
    
    // Observation: After spawning 100 tasks that hit the infinite loop,
    // new legitimate requests timeout or fail to be processed
    tokio::time::sleep(Duration::from_secs(10)).await;
    
    // Legitimate request should fail or timeout
    let legitimate_request = GetTransactionsRequest {
        starting_version: Some(0),
        transactions_count: Some(10),
        batch_size: None,
        transaction_filter: None,
    };
    
    let result = tokio::time::timeout(
        Duration::from_secs(5),
        client.get_transactions(legitimate_request)
    ).await;
    
    assert!(result.is_err(), "Legitimate request should timeout due to resource exhaustion");
}
```

## Notes
While this vulnerability affects the indexer API service rather than core consensus, it represents a significant availability issue for the Aptos ecosystem. The indexer is critical infrastructure that clients rely on for querying blockchain state, and its unavailability impacts user experience and dependent applications. The missing error handling (noted by the TODO comment) combined with unlimited task spawning creates a classic resource exhaustion vulnerability.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_client.rs (L27-42)
```rust
        loop {
            let mut client = self
                .connection_manager
                .get_grpc_manager_client_for_request();
            let response = client.get_transactions(request.clone()).await;
            if let Ok(response) = response {
                let transactions = response.into_inner().transactions;
                if transactions.is_empty() {
                    return vec![];
                }
                if transactions.first().unwrap().version == starting_version {
                    return transactions;
                }
            }
            // TODO(grao): Error handling.
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/service.rs (L138-149)
```rust
    async fn get_transactions(
        &self,
        req: Request<GetTransactionsRequest>,
    ) -> Result<Response<Self::GetTransactionsStream>, Status> {
        let (tx, rx) = channel(self.data_service_response_channel_size);
        self.handler_tx.send((req, tx)).await.unwrap();

        let output_stream = ReceiverStream::new(rx);
        let response = Response::new(Box::pin(output_stream) as Self::GetTransactionsStream);

        Ok(response)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L127-139)
```rust
                scope.spawn(async move {
                    self.start_streaming(
                        id,
                        starting_version,
                        ending_version,
                        max_num_transactions_per_batch,
                        MAX_BYTES_PER_BATCH,
                        filter,
                        request_metadata,
                        response_sender,
                    )
                    .await
                });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/in_memory_cache.rs (L73-76)
```rust
            if data_manager.get_data(starting_version).is_none() {
                drop(data_manager);
                self.fetch_manager.fetch_past_data(starting_version).await;
                continue;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/fetch_manager.rs (L48-64)
```rust
    async fn fetch_and_update_cache(
        data_client: Arc<DataClient>,
        data_manager: Arc<RwLock<DataManager>>,
        version: u64,
    ) -> usize {
        let transactions = data_client.fetch_transactions(version).await;
        let len = transactions.len();

        if len > 0 {
            data_manager
                .write()
                .await
                .update_data(version, transactions);
        }

        len
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/main.rs (L13-17)
```rust
#[tokio::main]
async fn main() -> Result<()> {
    let args = ServerArgs::parse();
    args.run::<IndexerGrpcDataServiceConfig>().await
}
```
