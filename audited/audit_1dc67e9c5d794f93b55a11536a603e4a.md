# Audit Report

## Title
Readiness Probe Bypass in Indexer-gRPC Server Framework Allows Kubernetes to Route Traffic to Deadlocked Services

## Summary
The `/readiness` endpoint in the indexer-grpc server framework unconditionally returns HTTP 200 "ready" status without checking the actual health of the main indexer service. Since the health check server and main service run in independent tokio tasks, Kubernetes will continue routing traffic to pods where the indexer service is deadlocked, corrupted, or otherwise unable to process requests. [1](#0-0) 

## Finding Description
The `register_probes_and_metrics_handler()` function creates a readiness probe that returns success without any health validation. The health check HTTP server spawns in a separate tokio task from the main service: [2](#0-1) [3](#0-2) 

This architectural separation means the health check remains responsive even when the main service encounters critical failures.

**Concrete Deadlock Scenarios:**

1. **Channel Saturation Deadlock**: The data service uses a bounded channel (capacity 10) to communicate between the gRPC API layer and the processing layer: [4](#0-3) 

When requests are sent to this channel: [5](#0-4) 

If the receiver (LiveDataService::run) stops processing due to deadlock, the channel fills up and all subsequent gRPC requests hang indefinitely, while the readiness probe still returns "ready".

2. **Cache Lock Deadlock**: The LiveDataService uses an in-memory cache with async locks: [6](#0-5) 

Multiple tasks accessing the cache could deadlock with the fetch manager updating cache state: [7](#0-6) 

3. **Blocking Receive Loop Hang**: The main processing loop runs in a blocking context: [8](#0-7) 

Any spawned task that hangs within the tokio_scoped scope causes the entire run method to stall.

**Contrast with Proper Health Checking:**

The codebase already implements sophisticated health checks for development/testing environments that actually verify service responsiveness: [9](#0-8) 

This proper implementation creates a gRPC client, makes an actual request, and verifies a response with timeout protection. However, this is NOT used in the production readiness probe.

The ready_server pattern shows how to properly aggregate health checks: [10](#0-9) 

This checks all services with a 3-second timeout and returns HTTP 503 if any service is unhealthy.

## Impact Explanation
This vulnerability meets **High Severity** criteria per Aptos bug bounty program:

- **API crashes**: Users receive timeouts and errors when routed to deadlocked pods
- **Significant protocol violations**: Violates the Kubernetes health check contract, which is foundational to cluster orchestration
- **Service degradation**: Indexer infrastructure becomes unreliable, affecting all applications querying blockchain data

The indexer-grpc services are critical infrastructure for the Aptos ecosystem. They provide the transaction streaming and historical data APIs that wallets, explorers, and dApps depend on. When these services falsely report readiness:

1. Kubernetes continues routing user traffic to unhealthy pods
2. Users experience request timeouts instead of being routed to healthy replicas
3. The service appears "flapping" with intermittent failures
4. No automatic remediation occurs (pod stays in rotation)
5. Manual intervention required to detect and restart affected pods

## Likelihood Explanation
**Likelihood: MEDIUM-HIGH**

Deadlocks and service hangs are common failure modes in concurrent systems, especially those involving:
- Multiple async tasks sharing state
- Channel-based communication patterns
- Complex lock hierarchies
- Connection management to upstream services

The specific architecture (separate health check and service tasks) guarantees that any deadlock in the main service will manifest this vulnerability. No attacker action is required - normal operational bugs will trigger false readiness reports.

Production environments commonly experience:
- Resource exhaustion causing hangs
- Connection pool saturation
- Lock contention under high load
- Bugs in error handling causing infinite retry loops

The vulnerability is **automatically exploitable** whenever the service enters any degraded state that doesn't immediately crash the process.

## Recommendation
Implement proper health checking in the readiness probe that validates the main service is actually processing requests. The fix should:

1. **Create a health check mechanism** that the main service can update to indicate its liveness
2. **Check actual service state** in the readiness handler, not just HTTP server availability
3. **Use timeouts** to prevent the health check itself from hanging
4. **Return HTTP 503** when the service is unhealthy

**Recommended Implementation:**

```rust
// Add to GenericConfig or RunnableConfig trait
use std::sync::Arc;
use tokio::sync::RwLock;

#[derive(Clone)]
pub struct HealthStatus {
    is_ready: Arc<RwLock<bool>>,
    last_update: Arc<RwLock<std::time::Instant>>,
}

impl HealthStatus {
    pub fn new() -> Self {
        Self {
            is_ready: Arc::new(RwLock::new(false)),
            last_update: Arc::new(RwLock::new(std::time::Instant::now())),
        }
    }
    
    pub async fn mark_ready(&self) {
        *self.is_ready.write().await = true;
        *self.last_update.write().await = std::time::Instant::now();
    }
    
    pub async fn check(&self) -> bool {
        let is_ready = *self.is_ready.read().await;
        let last_update = *self.last_update.read().await;
        let staleness = std::time::Instant::now() - last_update;
        
        // Consider unhealthy if no update in last 30 seconds
        is_ready && staleness < std::time::Duration::from_secs(30)
    }
}

// Update register_probes_and_metrics_handler
async fn register_probes_and_metrics_handler<C>(
    config: GenericConfig<C>, 
    port: u16,
    health_status: HealthStatus, // Add parameter
) where C: RunnableConfig {
    let health_status_clone = health_status.clone();
    let readiness = warp::path("readiness").and_then(move || {
        let health_status = health_status_clone.clone();
        async move {
            match tokio::time::timeout(
                std::time::Duration::from_secs(3),
                health_status.check()
            ).await {
                Ok(true) => Ok::<_, warp::Rejection>(
                    warp::reply::with_status("ready", warp::http::StatusCode::OK)
                ),
                _ => Ok(warp::reply::with_status(
                    "not ready", 
                    warp::http::StatusCode::SERVICE_UNAVAILABLE
                )),
            }
        }
    });
    // ... rest of implementation
}
```

Each service implementation should periodically call `health_status.mark_ready()` to indicate it's processing normally. The readiness probe checks this status with timeout protection.

## Proof of Concept

```rust
#[cfg(test)]
mod readiness_bypass_poc {
    use super::*;
    use std::time::Duration;
    use tokio::time::sleep;
    
    #[derive(Clone, Debug, Deserialize, Serialize)]
    pub struct DeadlockedServiceConfig {
        pub should_deadlock: bool,
    }
    
    #[async_trait::async_trait]
    impl RunnableConfig for DeadlockedServiceConfig {
        async fn run(&self) -> Result<()> {
            if self.should_deadlock {
                // Simulate a deadlocked service that never returns
                loop {
                    sleep(Duration::from_secs(1)).await;
                }
            }
            Ok(())
        }
        
        fn get_server_name(&self) -> String {
            "deadlocked_test_service".to_string()
        }
    }
    
    #[tokio::test]
    async fn test_readiness_probe_reports_ready_when_service_deadlocked() {
        let config = GenericConfig {
            health_check_port: 18888,
            server_config: DeadlockedServiceConfig {
                should_deadlock: true,
            },
        };
        
        // Start the server (which will deadlock)
        let server_handle = tokio::spawn(async move {
            run_server_with_config(config).await
        });
        
        // Give it time to start the health server
        sleep(Duration::from_secs(2)).await;
        
        // The readiness probe should be accessible
        let response = reqwest::get("http://127.0.0.1:18888/readiness")
            .await
            .expect("Health check should respond");
        
        // VULNERABILITY: Reports ready even though main service is deadlocked
        assert_eq!(response.status(), 200);
        assert_eq!(response.text().await.unwrap(), "ready");
        
        // Prove the main service is actually deadlocked by checking it never completes
        let result = tokio::time::timeout(
            Duration::from_secs(5),
            server_handle
        ).await;
        
        assert!(result.is_err(), "Service should timeout because it's deadlocked");
        
        println!("âœ“ PoC Confirmed: Readiness probe returns 'ready' while service is deadlocked");
    }
}
```

**Reproduction Steps:**
1. Deploy an indexer-grpc service to Kubernetes with readiness probe configured to check `/readiness`
2. Introduce a deadlock in the service (e.g., via load testing that triggers a concurrency bug)
3. Observe that the pod remains in "Ready" state in Kubernetes
4. Observe that user requests to the pod timeout or fail
5. Observe that Kubernetes continues routing traffic to the broken pod

## Notes

This vulnerability affects ALL services built on the `indexer-grpc-server-framework`, including:
- indexer-grpc-data-service-v2
- indexer-grpc-cache-worker  
- indexer-grpc-file-store
- indexer-grpc-file-store-backfiller
- indexer-grpc-manager
- indexer-grpc-gateway

The framework is designed to be reusable, so fixing it at the framework level will protect all dependent services. The proper health checking infrastructure already exists in the codebase (as shown in `HealthChecker::DataServiceGrpc`), it simply needs to be integrated into the production readiness probe.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L53-56)
```rust
    let task_handler = tokio::spawn(async move {
        register_probes_and_metrics_handler(config_clone, health_port).await;
        anyhow::Ok(())
    });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L57-58)
```rust
    let main_task_handler =
        tokio::spawn(async move { config.run().await.expect("task should exit with Ok.") });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L200-201)
```rust
    let readiness = warp::path("readiness")
        .map(move || warp::reply::with_status("ready", warp::http::StatusCode::OK));
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L123-123)
```rust
        let (handler_tx, handler_rx) = tokio::sync::mpsc::channel(10);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/service.rs (L143-143)
```rust
        self.handler_tx.send((req, tx)).await.unwrap();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L67-72)
```rust
            scope.spawn(async move {
                let _ = self
                    .in_memory_cache
                    .fetch_manager
                    .continuously_fetch_latest_data()
                    .await;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L74-74)
```rust
            while let Some((request, response_sender)) = handler_rx.blocking_recv() {
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L144-145)
```rust
    pub(crate) async fn get_min_servable_version(&self) -> u64 {
        self.in_memory_cache.data_manager.read().await.start_version
```

**File:** crates/aptos-localnet/src/health_checker.rs (L58-78)
```rust
            HealthChecker::DataServiceGrpc(url) => {
                let mut client = aptos_indexer_grpc_utils::create_data_service_grpc_client(
                    url.clone(),
                    Some(Duration::from_secs(5)),
                )
                .await?;
                let request = tonic::Request::new(GetTransactionsRequest {
                    starting_version: Some(0),
                    ..Default::default()
                });
                // Make sure we can stream the first message from the stream.
                client
                    .get_transactions(request)
                    .await
                    .context("GRPC connection error")?
                    .into_inner()
                    .next()
                    .await
                    .context("Did not receive init signal from data service GRPC stream")?
                    .context("Error processing first message from GRPC stream")?;
                Ok(())
```

**File:** crates/aptos/src/node/local_testnet/ready_server.rs (L114-124)
```rust
    for health_checker in &health_checkers.health_checkers {
        // Use timeout since some of these checks can take quite a while if the
        // underlying service is not ready. This is best effort of course, see the docs
        // for tokio::time::timeout for more information.
        match timeout(Duration::from_secs(3), health_checker.check()).await {
            Ok(Ok(())) => ready.push(health_checker.clone()),
            _ => {
                not_ready.push(health_checker.clone());
            },
        }
    }
```
