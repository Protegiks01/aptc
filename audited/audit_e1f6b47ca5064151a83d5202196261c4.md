# Audit Report

## Title
Backup Verification Metrics Not Persisted - Failed Verifications Can Be Hidden Through Process Restarts

## Summary
The backup verification system tracks completion status using in-memory Prometheus metrics that are not persisted to disk. If the verification process restarts before completing successfully, all metrics reset to zero, permanently hiding any prior failures. This creates a monitoring blind spot where operators cannot reliably determine if backups have been successfully verified.

## Finding Description

The `VerifyCoordinator` tracks verification status through three Prometheus `IntGauge` metrics defined in [1](#0-0) . These metrics are set during the verification lifecycle in [2](#0-1) .

The critical issue is that `RestoreRunMode::Verify` uses a `MockStore` implementation that does not persist any state [3](#0-2) . When the verification process restarts, the run mode always reports no in-progress state [4](#0-3)  and resets to version 0 [5](#0-4) .

The dashboard monitoring system queries these metrics using `last_over_time(...[2d])` [6](#0-5) , meaning that if the process hasn't run within 2 days, no status information is available.

**Attack Scenario:**
1. Operator starts backup verification for a potentially corrupted backup
2. Verification fails or detects issues partway through
3. Operator (maliciously or accidentally) restarts the verification process
4. All metrics reset to 0; `VERIFY_COORDINATOR_FAIL_TS` is lost
5. Dashboard shows "no data" or only the new start timestamp
6. Failed verification is permanently hidden from monitoring systems
7. Organization may later attempt disaster recovery with unverified/corrupted backups

Unlike `RestoreRunMode::Restore` which persists state through a `RestoreHandler` [7](#0-6) , the Verify mode has no persistence mechanism whatsoever.

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria: "State inconsistencies requiring intervention."

The vulnerability creates a state inconsistency where the actual verification status diverges from observable metrics. This could lead to:

- **Operational Risk**: Reliance on unverified backups during disaster recovery, potentially causing extended downtime if restoration fails
- **Compliance Issues**: Inability to prove backup verification completed successfully
- **Hidden Failures**: Repeated verification failures could go undetected if the process keeps restarting

While this does not directly impact consensus, funds, or blockchain execution, it undermines backup integrity guarantees which are critical for blockchain disaster recovery capabilities.

## Likelihood Explanation

**Likelihood: High**

Process restarts are common in production environments due to:
- Crashes (OOM, panics, hardware failures)
- Deployments and upgrades
- Manual operator intervention
- Container orchestration systems (Kubernetes pod evictions)

The 2-day lookback window in the dashboard configuration suggests verification jobs are expected to run for extended periods, increasing the probability of interruptions. An operator managing a failing verification job may be tempted to "try again" by restarting, inadvertently or deliberately hiding the failure.

## Recommendation

Implement persistent state tracking for verification completion:

**Option 1 - Status File Persistence:**
Create a verification status file that records start time, completion status, and any errors. Write this atomically on verification completion/failure:

```rust
// In VerifyCoordinator::run()
pub async fn run(self) -> Result<()> {
    info!("Verify coordinator started.");
    VERIFY_COORDINATOR_START_TS.set(unix_timestamp_sec());
    
    // Write start status to persistent file
    self.write_status_file(VerificationStatus::Running)?;
    
    let ret = self.run_impl().await;
    
    if let Err(e) = &ret {
        error!(error = ?e, "Verify coordinator failed.");
        VERIFY_COORDINATOR_FAIL_TS.set(unix_timestamp_sec());
        self.write_status_file(VerificationStatus::Failed(e.to_string()))?;
    } else {
        info!("Verify coordinator exiting with success.");
        VERIFY_COORDINATOR_SUCC_TS.set(unix_timestamp_sec());
        self.write_status_file(VerificationStatus::Succeeded)?;
    }
    ret
}
```

**Option 2 - Database Metadata:**
Store verification metadata in the backup storage metadata cache alongside backup manifests. This would survive process restarts and provide historical verification records.

**Option 3 - Prometheus Pushgateway:**
Push metrics to a Prometheus Pushgateway before process exit, ensuring metrics persist beyond process lifetime.

The status file should be checked on startup and exposed as a separate metric (`VERIFY_LAST_KNOWN_STATUS`) independent of the process lifetime.

## Proof of Concept

**Demonstration Steps:**

1. Start backup verification:
```bash
aptos-db-tool backup verify \
  --metadata-cache-dir /tmp/metadata \
  --storage-backend local \
  --local-path /backup/storage \
  --start-version 0 \
  --end-version 1000000
```

2. While verification is running, query metrics:
```bash
curl http://localhost:9101/metrics | grep verify_coordinator
# Shows: aptos_db_backup_verify_coordinator_start_timestamp_s = <timestamp>
```

3. Kill the process:
```bash
kill -9 <verify_pid>
```

4. Query metrics again:
```bash
curl http://localhost:9101/metrics | grep verify_coordinator
# Shows: No data (metrics server not running)
```

5. Restart verification:
```bash
aptos-db-tool backup verify ... # same command
curl http://localhost:9101/metrics | grep verify_coordinator
# Shows: NEW start_timestamp, no record of previous failure
```

6. Check Grafana dashboard after 2+ days:
   - Previous verification status is completely lost
   - Only the most recent attempt is visible
   - No way to determine if verification ever completed successfully

**Expected Behavior:**
A persistent status file should exist showing all verification attempts and their outcomes, regardless of process restarts.

**Actual Behavior:**
All verification history is lost on process restart, allowing failed verifications to be hidden.

---

**Notes:**
This vulnerability is specific to the `VerifyCoordinator` in verify mode. The `ReplayVerifyCoordinator` and `RestoreCoordinator` use `RestoreRunMode::Restore` which does have persistence through the database, allowing them to resume after restarts. However, the lightweight verification mode sacrifices this capability for the convenience of not requiring a database, creating this monitoring gap.

### Citations

**File:** storage/backup/backup-cli/src/metrics/verify.rs (L55-77)
```rust
pub static VERIFY_COORDINATOR_START_TS: Lazy<IntGauge> = Lazy::new(|| {
    register_int_gauge!(
        "aptos_db_backup_verify_coordinator_start_timestamp_s",
        "Timestamp when the verify coordinator starts."
    )
    .unwrap()
});

pub static VERIFY_COORDINATOR_SUCC_TS: Lazy<IntGauge> = Lazy::new(|| {
    register_int_gauge!(
        "aptos_db_backup_verify_coordinator_succeed_timestamp_s",
        "Timestamp when the verify coordinator fails."
    )
    .unwrap()
});

pub static VERIFY_COORDINATOR_FAIL_TS: Lazy<IntGauge> = Lazy::new(|| {
    register_int_gauge!(
        "aptos_db_backup_verify_coordinator_fail_timestamp_s",
        "Timestamp when the verify coordinator fails."
    )
    .unwrap()
});
```

**File:** storage/backup/backup-cli/src/coordinators/verify.rs (L65-82)
```rust
    pub async fn run(self) -> Result<()> {
        info!("Verify coordinator started.");
        VERIFY_COORDINATOR_START_TS.set(unix_timestamp_sec());

        let ret = self.run_impl().await;

        if let Err(e) = &ret {
            error!(
                error = ?e,
                "Verify coordinator failed."
            );
            VERIFY_COORDINATOR_FAIL_TS.set(unix_timestamp_sec());
        } else {
            info!("Verify coordinator exiting with success.");
            VERIFY_COORDINATOR_SUCC_TS.set(unix_timestamp_sec());
        }
        ret
    }
```

**File:** storage/backup/backup-cli/src/utils/mod.rs (L169-172)
```rust
pub enum RestoreRunMode {
    Restore { restore_handler: RestoreHandler },
    Verify,
}
```

**File:** storage/backup/backup-cli/src/utils/mod.rs (L174-199)
```rust
struct MockStore;

impl TreeWriter<StateKey> for MockStore {
    fn write_node_batch(&self, _node_batch: &NodeBatch<StateKey>) -> Result<()> {
        Ok(())
    }
}

impl StateValueWriter<StateKey, StateValue> for MockStore {
    fn write_kv_batch(
        &self,
        _version: Version,
        _kv_batch: &StateValueBatch<StateKey, Option<StateValue>>,
        _progress: StateSnapshotProgress,
    ) -> Result<()> {
        Ok(())
    }

    fn kv_finish(&self, _version: Version, _usage: StateStorageUsage) -> Result<()> {
        Ok(())
    }

    fn get_progress(&self, _version: Version) -> Result<Option<StateSnapshotProgress>> {
        Ok(None)
    }
}
```

**File:** storage/backup/backup-cli/src/utils/mod.rs (L250-260)
```rust
    pub fn get_next_expected_transaction_version(&self) -> Result<Version> {
        match self {
            RestoreRunMode::Restore { restore_handler } => {
                restore_handler.get_next_expected_transaction_version()
            },
            RestoreRunMode::Verify => {
                info!("This is a dry run. Assuming resuming point at version 0.");
                Ok(0)
            },
        }
    }
```

**File:** storage/backup/backup-cli/src/utils/mod.rs (L271-278)
```rust
    pub fn get_in_progress_state_kv_snapshot(&self) -> Result<Option<Version>> {
        match self {
            RestoreRunMode::Restore { restore_handler } => {
                restore_handler.get_in_progress_state_kv_snapshot_version()
            },
            RestoreRunMode::Verify => Ok(None),
        }
    }
```

**File:** dashboards/storage-backup-and-restore.json (L1215-1215)
```json
              "expr": "sort_desc(last_over_time(aptos_db_backup_verify_coordinator_start_timestamp_s{chain_name=~\"$chain_name\", cluster=~\"$cluster\", metrics_source=~\"$metrics_source\", namespace=~\"$namespace\", kubernetes_pod_name=~\"$kubernetes_pod_name\"}[2d]) * 1000\r\nunless last_over_time(aptos_db_backup_verify_coordinator_succeed_timestamp_s[2d]) >  last_over_time(aptos_db_backup_verify_coordinator_start_timestamp_s[2d])\r\nunless last_over_time(aptos_db_backup_verify_coordinator_fail_timestamp_s[2d]) >  last_over_time(aptos_db_backup_verify_coordinator_start_timestamp_s[2d]))",
```
