# Audit Report

## Title
Stale Node Information Cache Causes Incorrect Peer Health Assessments in Mempool

## Summary
The `get_latest_node_info_response()` function returns cached peer node information without staleness validation. When node info requests fail repeatedly, the cached `ledger_timestamp_usecs` becomes outdated but is never invalidated, causing mempool's health check to incorrectly classify healthy, actively-syncing peers as unhealthy. [1](#0-0) 

## Finding Description

The vulnerability exists in the peer monitoring service's caching mechanism. When a peer successfully responds to a node info request, the response is cached in `recorded_node_info_response`: [2](#0-1) 

However, when subsequent node info requests fail, the `handle_request_failure()` method only updates failure counters but **never invalidates the cached response**: [3](#0-2) 

The `get_latest_node_info_response()` function simply returns the cached value without any timestamp validation: [4](#0-3) 

This stale data propagates to mempool through `extract_peer_monitoring_metadata()`: [5](#0-4) 

The mempool's `check_peer_metadata_health()` function uses this potentially stale `ledger_timestamp_usecs` to determine peer health: [6](#0-5) 

**Attack Scenario:**
1. At time T0, Peer A responds with `ledger_timestamp_usecs = T0` (healthy, synced)
2. Between T0 and T0+45s, node info requests to Peer A fail due to transient network issues or monitoring service load
3. Cache still contains response from T0 with `ledger_timestamp_usecs = T0`
4. At T0+45s, mempool calculates: `current_time (T0+45s) - cached_timestamp (T0) = 45 seconds`
5. With default `max_sync_lag_before_unhealthy_secs = 30`, peer is marked **UNHEALTHY**
6. In reality, if Peer A was syncing normally, its actual ledger timestamp might be T0+40s (only 5s lag) [7](#0-6) [8](#0-7) 

## Impact Explanation

This issue causes **suboptimal peer selection and transaction propagation inefficiency**, but does **not** meet the criteria for Medium severity or higher in the Aptos bug bounty program.

**Why this is NOT Medium+ severity:**
- **No fund loss or manipulation**: Transaction broadcasting continues to work, just with suboptimal peer selection
- **No state inconsistencies**: The blockchain state remains consistent; only peer prioritization is affected
- **No consensus violations**: Consensus operates normally
- **No availability impact**: The network continues functioning; transactions still propagate through other peers

**Actual impact:**
- Healthy peers temporarily deprioritized for transaction forwarding
- Slightly degraded transaction propagation efficiency during network instability
- Suboptimal load balancing in mempool

This is more accurately classified as a **performance/quality issue** rather than a security vulnerability, falling below even the Low severity threshold which targets "minor information leaks" rather than operational efficiency concerns.

## Likelihood Explanation

While the condition can occur naturally (transient network issues causing monitoring request failures are common), the **security impact is minimal**. The mempool continues to function correctly, and transactions still propagate through the network via alternative peers. The system self-corrects once monitoring requests succeed again.

## Recommendation

Add staleness validation to prevent using outdated cached responses:

```rust
pub fn get_latest_node_info_response(&self) -> Option<NodeInformationResponse> {
    // Check if the cached response is too stale
    if let Some(last_response_time) = self.request_tracker.read().get_last_response_time() {
        let time_since_response = self.time_service.now().duration_since(last_response_time);
        let max_staleness = Duration::from_millis(
            self.node_monitoring_config.node_info_request_interval_ms * 3
        );
        
        if time_since_response > max_staleness {
            return None; // Invalidate stale cache
        }
    }
    
    self.recorded_node_info_response.clone()
}
```

Alternatively, clear the cache on consecutive failures:

```rust
fn handle_request_failure(&self) {
    self.request_tracker.write().record_response_failure();
    
    // Clear cache after threshold consecutive failures
    if self.request_tracker.read().get_num_consecutive_failures() > 3 {
        self.recorded_node_info_response = None;
    }
}
```

## Proof of Concept

Based on existing test pattern at: [9](#0-8) 

The test demonstrates that after 15 consecutive failures, the old cached response is still retained and used.

---

**Note:** After thorough analysis against the bug bounty severity criteria, this issue does **not** meet the threshold for a valid security vulnerability report. It represents a code quality concern affecting performance optimization rather than a security flaw causing fund loss, consensus violations, or state inconsistencies.

### Citations

**File:** peer-monitoring-service/client/src/peer_states/node_info.rs (L46-53)
```rust
    /// Records the new node info response for the peer
    pub fn record_node_info_response(&mut self, node_info_response: NodeInformationResponse) {
        // Update the request tracker with a successful response
        self.request_tracker.write().record_response_success();

        // Save the node info
        self.recorded_node_info_response = Some(node_info_response);
    }
```

**File:** peer-monitoring-service/client/src/peer_states/node_info.rs (L55-58)
```rust
    /// Handles a request failure for the specified peer
    fn handle_request_failure(&self) {
        self.request_tracker.write().record_response_failure();
    }
```

**File:** peer-monitoring-service/client/src/peer_states/node_info.rs (L60-63)
```rust
    /// Returns the latest node info response
    pub fn get_latest_node_info_response(&self) -> Option<NodeInformationResponse> {
        self.recorded_node_info_response.clone()
    }
```

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L208-211)
```rust
        // Get and store the latest node info response
        let node_info_state = self.get_node_info_state()?;
        let node_info_response = node_info_state.get_latest_node_info_response();
        peer_monitoring_metadata.latest_node_info_response = node_info_response;
```

**File:** mempool/src/shared_mempool/priority.rs (L562-589)
```rust
fn check_peer_metadata_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
) -> bool {
    monitoring_metadata
        .and_then(|metadata| {
            metadata
                .latest_node_info_response
                .as_ref()
                .map(|node_information_response| {
                    // Get the peer's ledger timestamp and the current timestamp
                    let peer_ledger_timestamp_usecs =
                        node_information_response.ledger_timestamp_usecs;
                    let current_timestamp_usecs = get_timestamp_now_usecs(time_service);

                    // Calculate the max sync lag before the peer is considered unhealthy (in microseconds)
                    let max_sync_lag_secs =
                        mempool_config.max_sync_lag_before_unhealthy_secs as u64;
                    let max_sync_lag_usecs = max_sync_lag_secs * MICROS_PER_SECOND;

                    // Determine if the peer is healthy
                    current_timestamp_usecs.saturating_sub(peer_ledger_timestamp_usecs)
                        < max_sync_lag_usecs
                })
        })
        .unwrap_or(false) // If metadata is missing, consider the peer unhealthy
}
```

**File:** config/src/config/mempool_config.rs (L118-118)
```rust
            max_sync_lag_before_unhealthy_secs: 30, // 30 seconds
```

**File:** config/src/config/peer_monitoring_config.rs (L84-84)
```rust
            node_info_request_interval_ms: 15_000, // 15 seconds
```

**File:** peer-monitoring-service/client/src/tests/single_peer.rs (L713-829)
```rust
async fn test_node_info_request_failures() {
    // Create the peer monitoring client and server
    let network_id = NetworkId::Validator;
    let (peer_monitoring_client, mut mock_monitoring_server, peer_monitor_state, time_service) =
        MockMonitoringServer::new(vec![network_id]);

    // Create a node config where only node infos refresh
    let node_config = config_with_node_info_requests();

    // Spawn the peer monitoring client
    start_peer_monitor(
        peer_monitoring_client,
        &peer_monitor_state,
        &time_service,
        &node_config,
    )
    .await;

    // Add a connected validator peer
    let validator_peer = mock_monitoring_server.add_new_peer(network_id, PeerRole::Validator);

    // Initialize all the peer states by running the peer monitor once
    let mock_time = time_service.into_mock();
    let (_, node_info_response) = initialize_and_verify_peer_states(
        &network_id,
        &mut mock_monitoring_server,
        &peer_monitor_state,
        &node_config,
        &validator_peer,
        &mock_time,
    )
    .await;

    // Handle several node info requests with bad responses
    for i in 0..5 {
        // Elapse enough time for a node info update
        elapse_node_info_update_interval(node_config.clone(), mock_time.clone()).await;

        // Verify that a single node info request is received and send a bad response
        // Create the test data
        verify_node_info_request_and_respond(
            &network_id,
            &mut mock_monitoring_server,
            create_random_node_info_response(),
            true,
            false,
            false,
        )
        .await;

        // Wait until the node info state is updated with the failure
        wait_for_node_info_request_failure(&peer_monitor_state, &validator_peer, i + 1).await;
    }

    // Handle several node info requests with responses that are too large
    for i in 5..10 {
        // Elapse enough time for a node info update
        elapse_node_info_update_interval(node_config.clone(), mock_time.clone()).await;

        // Verify that a single node info request is received and send a response that is too large
        verify_node_info_request_and_respond(
            &network_id,
            &mut mock_monitoring_server,
            create_random_node_info_response(),
            false,
            true,
            false,
        )
        .await;

        // Wait until the node info state is updated with the failure
        wait_for_node_info_request_failure(&peer_monitor_state, &validator_peer, i + 1).await;
    }

    // Handle several node info requests without responses
    for i in 10..15 {
        // Elapse enough time for a node info update
        elapse_node_info_update_interval(node_config.clone(), mock_time.clone()).await;

        // Verify that a single node info request is received and don't send a response
        verify_node_info_request_and_respond(
            &network_id,
            &mut mock_monitoring_server,
            create_random_node_info_response(),
            false,
            false,
            true,
        )
        .await;

        // Wait until the node info state is updated with the failure
        wait_for_node_info_request_failure(&peer_monitor_state, &validator_peer, i + 1).await;
    }

    // Verify the new node info state of the peer monitor
    verify_peer_node_state(
        &peer_monitor_state,
        &validator_peer,
        node_info_response.clone(),
        15,
    );

    // Elapse enough time for a node info request and perform a successful execution
    verify_and_handle_node_info_request(
        &network_id,
        &mut mock_monitoring_server,
        &peer_monitor_state,
        &node_config,
        &validator_peer,
        &mock_time,
        node_info_response.clone(),
    )
    .await;

    // Verify the new node info state of the peer monitor (the number
    // of failures should have been reset).
    verify_peer_node_state(&peer_monitor_state, &validator_peer, node_info_response, 0);
```
