# Audit Report

## Title
Unbounded URL Array in NFT Metadata Crawler Asset Uploader API Enables Service Denial

## Summary
The `BatchUploadRequest` struct in the NFT metadata crawler's asset uploader API contains an unbounded `Vec<Url>` field with no validation on the number of URLs submitted. An attacker can submit tens of thousands of short URLs within the default 2MB body limit, causing excessive memory allocation, database query strain, and API service degradation or crash.

## Finding Description
The vulnerability exists in the batch upload endpoint handling flow. The `BatchUploadRequest` struct accepts an unbounded vector of URLs [1](#0-0)  with no size validation at the API layer.

When a request is received, the `handle_upload_batch` function directly processes the entire URL vector [2](#0-1)  without checking the array length. The `upload_batch` function then iterates over all URLs [3](#0-2)  to create request status objects and execute database operations.

**Attack Path:**
1. Attacker crafts a POST request to `/upload` endpoint with ~20,000-40,000 short URLs (e.g., 50-100 bytes each, totaling ~2MB)
2. Axum's default 2MB body limit accepts the request
3. JSON deserialization allocates memory for the large vector
4. `get_existing_rows` executes a SQL query with a massive IN clause containing all URLs [4](#0-3) 
5. The loop allocates ~20,000-40,000 `AssetUploaderRequestStatuses` objects in memory
6. `insert_request_statuses` attempts a bulk INSERT with tens of thousands of rows [5](#0-4) 
7. Database connection is held during expensive queries, exhausting connection pool
8. Repeated requests cause cumulative memory exhaustion and service crash

The router configuration shows no request size or rate limiting middleware applied beyond axum's defaults [6](#0-5) .

## Impact Explanation
This qualifies as **High Severity** under the Aptos Bug Bounty program's "API crashes" category. While the NFT metadata crawler is an ecosystem component rather than core consensus infrastructure, it provides critical NFT metadata services. Exploitation causes:

- **API Service Degradation**: Database connection pool exhaustion prevents legitimate requests
- **Memory Exhaustion**: Repeated attacks can crash the service completely  
- **Resource Starvation**: Database server experiences high load from massive queries
- **Service Unavailability**: NFT metadata becomes inaccessible to users and applications

This does NOT directly affect consensus, validator operations, or blockchain state, but significantly impacts the availability of an important ecosystem service.

## Likelihood Explanation
**Likelihood: High**

The attack is trivially easy to execute:
- Requires only HTTP access to a public API endpoint
- No authentication or special privileges needed (assuming endpoint is public)
- Simple to craft (basic script generating short URLs)
- Low technical barrier for attacker
- Immediate impact with single request
- Can be automated for sustained attack

The vulnerability is deterministic and requires no race conditions or complex timing.

## Recommendation
Implement explicit validation on the `urls` array size before processing:

```rust
const MAX_URLS_PER_BATCH: usize = 100; // or appropriate limit

#[derive(Debug, Deserialize)]
struct BatchUploadRequest {
    #[serde(flatten)]
    idempotency_tuple: IdempotencyTuple,
    #[serde(deserialize_with = "deserialize_bounded_urls")]
    urls: Vec<Url>,
}

fn deserialize_bounded_urls<'de, D>(deserializer: D) -> Result<Vec<Url>, D::Error>
where
    D: serde::Deserializer<'de>,
{
    let urls: Vec<Url> = Vec::deserialize(deserializer)?;
    if urls.len() > MAX_URLS_PER_BATCH {
        return Err(serde::de::Error::custom(format!(
            "URL count {} exceeds maximum of {}",
            urls.len(),
            MAX_URLS_PER_BATCH
        )));
    }
    if urls.is_empty() {
        return Err(serde::de::Error::custom("URL list cannot be empty"));
    }
    Ok(urls)
}
```

Additionally, consider implementing:
- Rate limiting middleware per client IP
- Database query batching with smaller chunks
- Connection pool monitoring and circuit breakers
- Request timeout configuration

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[cfg(test)]
mod tests {
    use super::*;
    use axum::body::Body;
    use axum::http::{Request, StatusCode};
    use tower::ServiceExt;

    #[tokio::test]
    async fn test_unbounded_url_dos() {
        // Create test context
        let pool = create_test_pool(); // Helper to create test DB pool
        let context = AssetUploaderApiContext::new(pool);
        let app = context.build_router();

        // Generate 30,000 short URLs (~2MB payload)
        let urls: Vec<String> = (0..30_000)
            .map(|i| format!("https://example.com/nft/{}", i))
            .collect();

        let payload = serde_json::json!({
            "idempotency_key": "test_key",
            "application_id": "test_app",
            "urls": urls
        });

        // Send malicious request
        let request = Request::builder()
            .method("POST")
            .uri("/upload")
            .header("content-type", "application/json")
            .body(Body::from(serde_json::to_vec(&payload).unwrap()))
            .unwrap();

        // This should either:
        // 1. Return 413 Payload Too Large (if fixed)
        // 2. Cause timeout/crash (vulnerability)
        let response = app.oneshot(request).await.unwrap();
        
        // Without fix: service crashes or times out
        // With fix: returns 400 Bad Request with validation error
        assert_eq!(response.status(), StatusCode::BAD_REQUEST);
    }
}

// Command-line exploitation example:
// python3 -c "import json; print(json.dumps({'idempotency_key':'attack','application_id':'test','urls':['https://ex.co/'+str(i) for i in range(30000)]}))" | \
// curl -X POST http://target:port/upload \
//   -H "Content-Type: application/json" \
//   -d @-
```

## Notes
- This vulnerability is in an ecosystem component (NFT metadata crawler), not core consensus/validator infrastructure
- While it qualifies as High severity under "API crashes," it does not directly affect blockchain operation
- The 2MB default body limit in axum 0.7.5 [7](#0-6)  provides some mitigation but is insufficient
- Similar validation should be applied to other array/collection fields in API endpoints
- Consider implementing centralized request validation middleware for all ecosystem APIs

### Citations

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/api/mod.rs (L37-42)
```rust
#[derive(Debug, Deserialize)]
struct BatchUploadRequest {
    #[serde(flatten)]
    idempotency_tuple: IdempotencyTuple,
    urls: Vec<Url>,
}
```

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/api/mod.rs (L87-106)
```rust
    async fn handle_upload_batch(
        Extension(context): Extension<Arc<AssetUploaderApiContext>>,
        Json(request): Json<BatchUploadRequest>,
    ) -> impl IntoResponse {
        match upload_batch(context.pool.clone(), &request) {
            Ok(idempotency_tuple) => (
                StatusCode::OK,
                Json(BatchUploadResponse::Success { idempotency_tuple }),
            ),
            Err(e) => {
                error!(error = ?e, "Error uploading asset");
                (
                    StatusCode::INTERNAL_SERVER_ERROR,
                    Json(BatchUploadResponse::Error {
                        error: format!("Error uploading asset: {}", e),
                    }),
                )
            },
        }
    }
```

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/api/mod.rs (L138-149)
```rust
impl Server for AssetUploaderApiContext {
    fn build_router(&self) -> axum::Router {
        let self_arc = Arc::new(self.clone());
        axum::Router::new()
            .route("/upload", post(Self::handle_upload_batch))
            .route(
                "/status/:application_id/:idempotency_key",
                get(Self::handle_get_status),
            )
            .layer(Extension(self_arc.clone()))
    }
}
```

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/api/upload_batch.rs (L27-40)
```rust
    for url in &request.urls {
        if let Some(cdn_image_uri) = existing_rows.get(url.as_str()) {
            request_statuses.push(AssetUploaderRequestStatuses::new_completed(
                &request.idempotency_tuple,
                url.as_str(),
                cdn_image_uri.as_deref().unwrap(), // Safe to unwrap because we checked for existence when querying
            ));
        } else {
            request_statuses.push(AssetUploaderRequestStatuses::new(
                &request.idempotency_tuple,
                url.as_str(),
            ));
        }
    }
```

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/api/upload_batch.rs (L46-64)
```rust
fn get_existing_rows(
    conn: &mut PooledConnection<ConnectionManager<PgConnection>>,
    urls: &[Url],
) -> anyhow::Result<AHashMap<String, Option<String>>> {
    use schema::nft_metadata_crawler::parsed_asset_uris::dsl::*;

    let query = parsed_asset_uris
        .filter(
            asset_uri
                .eq_any(urls.iter().map(Url::as_str))
                .and(cdn_image_uri.is_not_null()),
        )
        .select((asset_uri, cdn_image_uri));

    let debug_query = diesel::debug_query::<diesel::pg::Pg, _>(&query).to_string();
    debug!("Executing Query: {}", debug_query);
    let rows = query.load(conn)?;
    Ok(AHashMap::from_iter(rows))
}
```

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/api/upload_batch.rs (L66-81)
```rust
fn insert_request_statuses(
    conn: &mut PooledConnection<ConnectionManager<PgConnection>>,
    request_statuses: &[AssetUploaderRequestStatuses],
) -> anyhow::Result<usize> {
    use schema::nft_metadata_crawler::asset_uploader_request_statuses::dsl::*;

    let query =
        diesel::insert_into(schema::nft_metadata_crawler::asset_uploader_request_statuses::table)
            .values(request_statuses)
            .on_conflict((idempotency_key, application_id, asset_uri))
            .do_nothing();

    let debug_query = diesel::debug_query::<diesel::pg::Pg, _>(&query).to_string();
    debug!("Executing Query: {}", debug_query);
    query.execute(conn).context(debug_query)
}
```

**File:** Cargo.toml (L527-527)
```text
axum = "0.7.5"
```
