# Audit Report

## Title
Permanent Permit Consumption in BoundedExecutor Due to Never-Completing Futures

## Summary
The `BoundedExecutor` used throughout Aptos consensus components can permanently lose semaphore permits if spawned futures never complete. The buffer manager spawns commit message verification tasks without timeout protection, abort handles, or task tracking, allowing permits to be consumed indefinitely when verification hangs, eventually causing total validator liveness failure.

## Finding Description

The `BoundedExecutor` releases semaphore permits **only** when spawned futures complete. The permit is wrapped with the future and dropped inside a `.map()` closure that executes upon future completion. [1](#0-0) 

The buffer manager spawns commit message verification tasks on the bounded executor without any defensive mechanisms. A long-lived task processes incoming commit messages, and for each message, spawns a verification task using `bounded_executor.spawn()`. The returned `JoinHandle` is immediately dropped without being stored or awaited, creating fire-and-forget tasks with no abort capability or timeout enforcement. [2](#0-1) 

The verification tasks are **not tracked** in the `ongoing_tasks` counter. Only requests wrapped with `CountedRequest` increment this atomic counter. [3](#0-2)  The buffer manager's `create_new_request()` helper wraps requests with `CountedRequest` [4](#0-3) , but the verification tasks spawned on lines 923-932 are spawned directly without this wrapper, meaning they bypass task tracking entirely.

The commit message verification has **no timeout protection**. The `verify()` method calls into cryptographic signature verification operations without any timeout wrapper. [5](#0-4) [6](#0-5) 

At epoch boundaries, the `end_epoch()` cleanup process sends reset signals to managers and waits for acknowledgment. [7](#0-6)  The buffer manager's `reset()` method waits for `ongoing_tasks` to reach zero before completing. [8](#0-7)  However, since verification tasks are **not counted** in `ongoing_tasks`, hung verification tasks from previous epochs persist indefinitely and continue consuming permits.

The bounded executor is created once at node startup and shared across all consensus components, persisting across epoch boundaries. [9](#0-8) 

**Attack Path:**
1. Attacker identifies the bounded executor capacity (typically 32-64 permits)
2. Attacker sends messages that trigger verification to hang:
   - Malformed signatures triggering edge cases in cryptographic verification
   - Messages causing race conditions or deadlocks in verification logic
   - High volume during epoch transitions causing runtime starvation
3. Each hung verification task permanently consumes one permit
4. Over time or multiple epochs, all permits are exhausted
5. New verification tasks block indefinitely waiting for permits
6. Buffer manager cannot process commit messages
7. Consensus halts - validator cannot participate in block agreement

This breaks the **Consensus Liveness** invariant (validators must be able to process consensus messages) and the **Resource Limits** invariant (system must recover from resource exhaustion).

## Impact Explanation

**HIGH Severity** per Aptos bug bounty criteria - "Validator node slowdowns" escalating to functional liveness failure.

When all permits are exhausted:
- Buffer manager cannot verify incoming commit messages
- Reliable broadcast cannot aggregate responses
- RandManager and SecretShareManager (which also use the shared bounded executor) cannot process messages
- Validator effectively stops participating in consensus

Unlike temporary DoS, permit exhaustion is **permanent** until node restart. If the triggering condition persists (e.g., a specific malformed message pattern or verification bug), the issue recurs immediately after restart.

A single affected validator loses consensus participation. If multiple validators are simultaneously affected through sustained attack or persistent bugs, network liveness degrades proportionally.

## Likelihood Explanation

**MEDIUM Likelihood**

**Triggering Factors:**
- Bugs in verification logic causing hangs (low probability but non-zero)
- Tokio runtime edge cases under high load
- Race conditions in async task scheduling
- Cryptographic verification library edge cases with malformed inputs

**Exploitation Requirements:**
- Attacker needs network access to send messages to validators (standard for any peer)
- No special validator privileges required
- Attack is cumulative - even rare hangs eventually exhaust permits
- Repeatable across epochs

**Mitigating Factors:**
- Normal verification operations complete quickly
- Requires either a latent bug or very specific timing/load conditions to trigger hangs

However, the **complete absence** of defensive mechanisms (no timeouts, no abort handles, no task limits per epoch, no tracking in ongoing_tasks counter) means that even extremely rare edge cases will eventually cause permit exhaustion over time.

## Recommendation

Implement multiple layers of defense:

1. **Add timeout protection to verification tasks:**
```rust
bounded_executor.spawn(async move {
    match tokio::time::timeout(
        Duration::from_secs(5),
        async move {
            commit_msg.req.verify(sender, &epoch_state_clone.verifier)
        }
    ).await {
        Ok(Ok(_)) => { let _ = tx.unbounded_send(commit_msg); },
        Ok(Err(e)) => warn!("Invalid commit message: {}", e),
        Err(_) => warn!("Commit message verification timed out"),
    }
}).await;
```

2. **Track verification tasks in ongoing_tasks counter** by wrapping them with a lightweight guard mechanism similar to CountedRequest.

3. **Store verification task AbortHandles** and cancel them during epoch cleanup:
```rust
let abort_handle = bounded_executor.spawn(...).await.abort_handle();
// Store abort_handle in a collection
// In reset(): abort_handle.abort();
```

4. **Implement per-epoch task limits** to prevent accumulation across epochs, resetting the counter at each epoch boundary.

5. **Add monitoring** for bounded executor permit usage to detect exhaustion before complete failure.

## Proof of Concept

While a complete PoC requires triggering a verification hang (which depends on finding a specific bug in cryptographic verification or runtime edge case), the vulnerability mechanism can be demonstrated conceptually:

```rust
// Simulated scenario showing permit exhaustion
#[tokio::test]
async fn test_permit_exhaustion_simulation() {
    let rt = Runtime::new().unwrap();
    let executor = BoundedExecutor::new(3, rt.handle().clone()); // Small capacity
    
    // Spawn tasks that never complete (simulating hung verification)
    for i in 0..3 {
        executor.spawn(async move {
            println!("Task {} started, will never complete", i);
            std::future::pending::<()>().await; // Hangs forever
        }).await;
    }
    
    // All permits now consumed
    // Next spawn will block indefinitely
    let result = tokio::time::timeout(
        Duration::from_secs(1),
        executor.spawn(async { println!("This will never execute"); })
    ).await;
    
    assert!(result.is_err()); // Times out because no permits available
}
```

The actual vulnerability would manifest when specific malformed messages or runtime conditions cause the `verify()` call to hang, permanently consuming permits without any cleanup mechanism.

### Citations

**File:** crates/bounded-executor/src/executor.rs (L100-109)
```rust
fn future_with_permit<F>(future: F, permit: OwnedSemaphorePermit) -> impl Future<Output = F::Output>
where
    F: Future + Send + 'static,
    F::Output: Send + 'static,
{
    future.map(move |ret| {
        drop(permit);
        ret
    })
}
```

**File:** consensus/src/pipeline/buffer_manager.rs (L289-291)
```rust
    fn create_new_request<Request>(&self, req: Request) -> CountedRequest<Request> {
        CountedRequest::new(req, self.ongoing_tasks.clone())
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L543-576)
```rust
    /// Reset any request in buffer manager, this is important to avoid race condition with state sync.
    /// Internal requests are managed with ongoing_tasks.
    /// Incoming ordered blocks are pulled, it should only have existing blocks but no new blocks until reset finishes.
    async fn reset(&mut self) {
        while let Some((_, block)) = self.pending_commit_blocks.pop_first() {
            // Those blocks don't have any dependencies, should be able to finish commit_ledger.
            // Abort them can cause error on epoch boundary.
            block.wait_for_commit_ledger().await;
        }
        while let Some(item) = self.buffer.pop_front() {
            for b in item.get_blocks() {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
        self.buffer = Buffer::new();
        self.execution_root = None;
        self.signing_root = None;
        self.previous_commit_time = Instant::now();
        self.commit_proof_rb_handle.take();
        // purge the incoming blocks queue
        while let Ok(Some(blocks)) = self.block_rx.try_next() {
            for b in blocks.ordered_blocks {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
        // Wait for ongoing tasks to finish before sending back ack.
        while self.ongoing_tasks.load(Ordering::SeqCst) > 0 {
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L919-934)
```rust
        spawn_named!("buffer manager verification", async move {
            while let Some((sender, commit_msg)) = commit_msg_rx.next().await {
                let tx = verified_commit_msg_tx.clone();
                let epoch_state_clone = epoch_state.clone();
                bounded_executor
                    .spawn(async move {
                        match commit_msg.req.verify(sender, &epoch_state_clone.verifier) {
                            Ok(_) => {
                                let _ = tx.unbounded_send(commit_msg);
                            },
                            Err(e) => warn!("Invalid commit message: {}", e),
                        }
                    })
                    .await;
            }
        });
```

**File:** consensus/src/pipeline/pipeline_phase.rs (L47-64)
```rust
pub struct CountedRequest<Request> {
    req: Request,
    guard: TaskGuard,
}

impl<Request> CountedRequest<Request> {
    pub fn new(req: Request, counter: Arc<AtomicU64>) -> Self {
        let guard = TaskGuard::new(counter);
        Self { req, guard }
    }

    pub fn spawn<OtherRequest>(&self, other_req: OtherRequest) -> CountedRequest<OtherRequest> {
        CountedRequest {
            req: other_req,
            guard: self.guard.spawn(),
        }
    }
}
```

**File:** consensus/src/pipeline/commit_reliable_broadcast.rs (L37-54)
```rust
    pub fn verify(&self, sender: Author, verifier: &ValidatorVerifier) -> anyhow::Result<()> {
        match self {
            CommitMessage::Vote(vote) => {
                let _timer = counters::VERIFY_MSG
                    .with_label_values(&["commit_vote"])
                    .start_timer();
                vote.verify(sender, verifier)
            },
            CommitMessage::Decision(decision) => {
                let _timer = counters::VERIFY_MSG
                    .with_label_values(&["commit_decision"])
                    .start_timer();
                decision.verify(verifier)
            },
            CommitMessage::Ack(_) => bail!("Unexpected ack in incoming commit message"),
            CommitMessage::Nack => bail!("Unexpected NACK in incoming commit message"),
        }
    }
```

**File:** consensus/consensus-types/src/pipeline/commit_vote.rs (L103-113)
```rust
    pub fn verify(&self, sender: Author, validator: &ValidatorVerifier) -> anyhow::Result<()> {
        ensure!(
            self.author() == sender,
            "Commit vote author {:?} doesn't match with the sender {:?}",
            self.author(),
            sender
        );
        validator
            .optimistic_verify(self.author(), &self.ledger_info, &self.signature)
            .context("Failed to verify Commit Vote")
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L711-760)
```rust
    async fn end_epoch(&self) {
        let (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        ) = {
            let mut handle = self.handle.write();
            handle.reset()
        };

        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }

        if let Some(mut tx) = reset_tx_to_buffer_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop buffer manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop buffer manager");
        }
        self.execution_proxy.end_epoch();
    }
```

**File:** consensus/src/consensus_provider.rs (L81-84)
```rust
    let bounded_executor = BoundedExecutor::new(
        node_config.consensus.num_bounded_executor_tasks as usize,
        runtime.handle().clone(),
    );
```
