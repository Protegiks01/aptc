# Audit Report

## Title
ExecutorError Cascading Failure Leading to Consensus Deadlock and Denial of Service

## Summary
The consensus pipeline's `BufferManager` fails to implement proper retry logic for `ExecutorError` failures, causing blocks to become permanently stuck in the "Ordered" state. This leads to buffer exhaustion, backpressure activation, and complete consensus halt when execution errors occur.

## Finding Description

The vulnerability exists in the consensus pipeline's error handling mechanism. When an `ExecutorError` occurs during block execution, the affected block remains stuck in the "Ordered" state without progressing through the pipeline (Ordered → Executed → Signed → Aggregated). 

The critical flaw is in how `advance_execution_root()` handles retry signaling. This method returns `Option<HashValue>` to indicate when a block needs retry, but **all three call sites completely discard this return value**. [1](#0-0) 

When `ExecutorError` occurs in `process_execution_response()`, the error is logged but the block state is never updated: [2](#0-1) 

The `advance_execution_root()` method is called in three places in the event loop, but the return value (which signals retry needed) is ignored in all cases: [3](#0-2) 

This creates a cascading failure scenario:
1. A single `ExecutorError::BlockNotFound` occurs (e.g., due to race condition with block pruning)
2. The block remains in "Ordered" state indefinitely without retry
3. All subsequent blocks cannot execute because they're queued behind the stuck block
4. After 20 uncommitted rounds accumulate, backpressure activates: [4](#0-3) 

5. No new blocks can enter the pipeline (line 938: `if !self.need_back_pressure()`)
6. Consensus completely halts - validator cannot participate in block production or voting

The attack can be triggered by:
- Natural race conditions during block tree pruning
- Malicious timing of block submissions to trigger `BlockNotFound` errors
- Parent-child block dependency violations causing cascading `BlockNotFound` errors across multiple blocks

## Impact Explanation

**High Severity** - This qualifies as "Validator node slowdowns" and "Significant protocol violations" per the Aptos bug bounty program:

- **Consensus Liveness Failure**: Affected validators stop participating in consensus, degrading network performance
- **Validator Penalties**: Stuck validators miss block proposals and votes, incurring performance penalties
- **Network Degradation**: If multiple validators are affected simultaneously, consensus quality degrades significantly
- **Recovery Requires Restart**: The only recovery is node restart or manual reset, as there's no automatic recovery mechanism

While not causing permanent network partition or complete network halt (Critical severity), this significantly impairs validator operation and network health.

## Likelihood Explanation

**High Likelihood** of occurrence:

1. **Natural Triggers**: `ExecutorError::BlockNotFound` can occur legitimately during normal operation when:
   - Block tree pruning happens concurrently with execution attempts
   - Parent blocks are not yet available in the execution block tree
   - Fork resolution causes some blocks to be dropped

2. **No Special Privileges Required**: Any validator can encounter this during normal consensus operation without malicious activity

3. **Amplification Effect**: Once one block fails, ALL subsequent blocks in the pipeline become blocked, rapidly filling the 20-block buffer

4. **No Recovery Mechanism**: The code contains no timeout, retry, or automatic recovery logic for stuck blocks

The vulnerability is present in production code and can manifest under normal network conditions, making it a high-likelihood issue.

## Recommendation

Implement proper retry logic for execution failures by actually using the return value from `advance_execution_root()`. Add a retry scheduling mechanism with exponential backoff:

```rust
async fn process_execution_response(&mut self, response: ExecutionResponse) {
    let ExecutionResponse { block_id, inner } = response;
    let current_cursor = self.buffer.find_elem_by_key(self.execution_root, block_id);
    if current_cursor.is_none() {
        return;
    }

    let executed_blocks = match inner {
        Ok(result) => result,
        Err(e) => {
            log_executor_error_occurred(
                e,
                &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                block_id,
            );
            
            // ADD RETRY LOGIC HERE
            // Schedule retry with exponential backoff after a delay
            let retry_duration = Duration::from_millis(100); // Start with 100ms
            Self::spawn_retry_request(
                self.execution_schedule_phase_tx.clone(),
                ExecutionRequest { ordered_blocks: /* retrieve from buffer */ },
                retry_duration,
            );
            return;
        },
    };
    // ... rest of method
}

// In event loop, use the return value:
if let Some(block_id_to_retry) = self.advance_execution_root() {
    // Schedule retry for the stuck block
    let retry_duration = Duration::from_millis(100);
    if let Some(item) = self.buffer.get_by_key(block_id_to_retry) {
        Self::spawn_retry_request(
            self.execution_schedule_phase_tx.clone(),
            ExecutionRequest { ordered_blocks: item.get_blocks() },
            retry_duration,
        );
    }
}
```

Additionally, implement a timeout mechanism to reset stuck blocks after a maximum retry period, and add telemetry to detect when blocks are stuck for extended periods.

## Proof of Concept

The following Rust test demonstrates the vulnerability:

```rust
#[tokio::test]
async fn test_executor_error_causes_deadlock() {
    // Setup: Create a BufferManager with a mock executor that returns ExecutorError
    let (execution_phase_tx, mut execution_phase_rx) = create_channel();
    let (buffer_manager_tx, buffer_manager_rx) = create_channel();
    
    // Simulate normal block flow
    for i in 0..5 {
        let block = create_test_block(i);
        buffer_manager_tx.send(OrderedBlocks {
            ordered_blocks: vec![block],
            ordered_proof: create_test_ledger_info(i),
        }).await.unwrap();
    }
    
    // Inject ExecutorError for block 3
    execution_phase_rx.next().await; // blocks 0,1,2 succeed
    execution_phase_rx.next().await;
    execution_phase_rx.next().await;
    
    // Block 3 fails with ExecutorError::BlockNotFound
    let failed_response = ExecutionResponse {
        block_id: block_3_id,
        inner: Err(ExecutorError::BlockNotFound(parent_block_id)),
    };
    
    // Process the error
    buffer_manager.process_execution_response(failed_response).await;
    
    // Attempt to continue: blocks 4+ should be sent for execution
    buffer_manager.advance_execution_root(); // This will return Some(block_3_id) but it's ignored!
    
    // Verify: Block 3 is still in "Ordered" state
    assert!(buffer.get(block_3_cursor).is_ordered());
    
    // Verify: Blocks 4+ cannot be executed because block 3 is blocking
    assert!(buffer.get(block_4_cursor).is_ordered());
    
    // Add more blocks until backpressure kicks in
    for i in 5..25 {
        buffer_manager_tx.send(create_ordered_blocks(i)).await.unwrap();
    }
    
    // Verify: need_back_pressure() now returns true
    assert!(buffer_manager.need_back_pressure());
    
    // Verify: No new blocks can be accepted
    // This simulates complete consensus halt for this validator
}
```

The test shows how a single `ExecutorError` causes the entire consensus pipeline to deadlock, with no automatic recovery mechanism.

## Notes

The vulnerability is exacerbated by the fact that `ExecutorError::BlockNotFound` can legitimately occur due to:
- Block tree pruning in the execution layer removing blocks that consensus still references
- Race conditions between consensus ordering and execution availability
- Parent block dependencies where children fail when parents aren't available

The lack of retry logic means these transient failures become permanent deadlocks, requiring node restart to recover.

### Citations

**File:** consensus/src/pipeline/buffer_manager.rs (L429-452)
```rust
    fn advance_execution_root(&mut self) -> Option<HashValue> {
        let cursor = self.execution_root;
        self.execution_root = self
            .buffer
            .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
                item.is_ordered()
            });
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
        } else {
            sample!(
                SampleRate::Frequency(2),
                info!(
                    "Advance execution root from {:?} to {:?}",
                    cursor, self.execution_root
                )
            );
            // Otherwise do nothing, because the execution wait phase is driven by the response of
            // the execution schedule phase, which is in turn fed as soon as the ordered blocks
            // come in.
            None
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L609-627)
```rust
    async fn process_execution_response(&mut self, response: ExecutionResponse) {
        let ExecutionResponse { block_id, inner } = response;
        // find the corresponding item, may not exist if a reset or aggregated happened
        let current_cursor = self.buffer.find_elem_by_key(self.execution_root, block_id);
        if current_cursor.is_none() {
            return;
        }

        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
        };
```

**File:** consensus/src/pipeline/buffer_manager.rs (L906-910)
```rust
    fn need_back_pressure(&self) -> bool {
        const MAX_BACKLOG: Round = 20;

        self.back_pressure_enabled && self.highest_committed_round + MAX_BACKLOG < self.latest_round
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L938-960)
```rust
                Some(blocks) = self.block_rx.next(), if !self.need_back_pressure() => {
                    self.latest_round = blocks.latest_round();
                    monitor!("buffer_manager_process_ordered", {
                    self.process_ordered_blocks(blocks).await;
                    if self.execution_root.is_none() {
                        self.advance_execution_root();
                    }});
                },
                Some(reset_event) = self.reset_rx.next() => {
                    monitor!("buffer_manager_process_reset",
                    self.process_reset_request(reset_event).await);
                },
                Some(response) = self.execution_schedule_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_schedule_response", {
                    self.process_execution_schedule_response(response).await;
                })},
                Some(response) = self.execution_wait_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_wait_response", {
                    self.process_execution_response(response).await;
                    self.advance_execution_root();
                    if self.signing_root.is_none() {
                        self.advance_signing_root().await;
                    }});
```
