# Audit Report

## Title
Silent Validator Exclusion from DKG Due to Address Mismatch Without Error Logging

## Summary
The DKG epoch manager silently excludes validators from participating in Distributed Key Generation (DKG) when their configured network address (`my_addr`) does not match their on-chain validator set address, without logging any error or warning. This can cause legitimate validators to be excluded from DKG, potentially preventing randomness generation and affecting network liveness.

## Finding Description
In the DKG epoch manager's `start_new_epoch()` function, the validator lookup uses `my_addr` (derived from the node's x25519 network identity key) to find the validator's index in the validator set: [1](#0-0) 

When `my_index` is `None` (validator not found in the set), the code silently skips DKG participation: [2](#0-1) 

The critical issue is that `my_addr` comes from the validator's network configuration (peer_id derived from x25519 key): [3](#0-2) 

However, the validator set uses the stake pool's account address: [4](#0-3) 

There is a known inconsistency in how addresses are derived - the codebase explicitly documents this: [5](#0-4) 

**Attack Scenario:**
1. A validator operator registers their validator on-chain with account address `A` (stake pool address)
2. The operator configures their node with a network identity key that derives to peer_id `B` (where `B ≠ A`)
3. During epoch transition, the DKG manager looks up `B` in the validator set (which contains `A`)
4. The lookup fails (`my_index = None`), and the validator silently doesn't participate in DKG
5. No error or warning is logged, so the operator has no visibility into this misconfiguration
6. If multiple validators have this issue, DKG may fail to reach quorum, breaking randomness generation

## Impact Explanation
**High Severity** - This qualifies as a "Significant protocol violation" and can cause "Validator node issues":

1. **DKG Failure**: If enough validators are silently excluded, DKG cannot generate the randomness beacon, affecting consensus
2. **Silent Failures**: The complete absence of error logging means operators cannot detect or debug this issue
3. **Liveness Risk**: Randomness generation is critical for consensus; failures can cause network stalls
4. **Operator Visibility**: Unlike the public key check (which does error), the index lookup failure is completely silent

Compare with the JWK consensus manager which has the same pattern: [6](#0-5) 

Both systems suffer from this silent failure mode.

## Likelihood Explanation
**Medium-High Likelihood**:

1. **Configuration Complexity**: Validators have multiple addresses (owner, operator, pool) and multiple keys (consensus, network), creating confusion
2. **Address Derivation Inconsistency**: The codebase itself notes the inconsistency between Ed25519-derived and x25519-derived addresses
3. **Key Rotation**: Operators rotating network keys without updating on-chain configuration could trigger this
4. **No Validation**: There's no startup-time validation that `my_addr` matches the on-chain validator address
5. **Silent Nature**: Without logging, this could persist undetected across multiple epochs

## Recommendation
Add explicit error logging when `my_index` lookup fails for a node that expects to be a validator:

```rust
async fn start_new_epoch(&mut self, payload: OnChainConfigPayload<P>) -> Result<()> {
    let validator_set: ValidatorSet = payload
        .get()
        .expect("failed to get ValidatorSet from payload");

    let epoch_state = Arc::new(EpochState::new(payload.epoch(), (&validator_set).into()));
    self.epoch_state = Some(epoch_state.clone());
    let my_index = epoch_state
        .verifier
        .address_to_validator_index()
        .get(&self.my_addr)
        .copied();

    // ADD ERROR LOGGING HERE
    if my_index.is_none() {
        warn!(
            my_addr = %self.my_addr,
            epoch = epoch_state.epoch,
            "This node's configured address was not found in the validator set. \
             DKG will not run. Verify that the network peer_id matches the on-chain \
             validator account address."
        );
    }

    // ... rest of the function
}
```

Additionally, consider adding a startup-time validation that checks if the configured `my_addr` matches the expected validator address in the genesis or latest validator set.

## Proof of Concept
**Reproduction Steps:**

1. Set up a test network with 4 validators
2. Configure validator node V1 with:
   - On-chain stake pool at address `0xA` 
   - Network configuration with x25519 key deriving to peer_id `0xB` (where `B ≠ A`)
3. Trigger an epoch transition that enables DKG
4. Observe V1's logs - no error or warning about DKG exclusion
5. Check DKG state - V1 has not contributed any DKG shares
6. If <2/3 of validators participate, DKG fails without clear indication of which validators were excluded

**Expected vs Actual Behavior:**
- **Expected**: Error logged indicating address mismatch and DKG exclusion
- **Actual**: Silent exclusion with no logging, operator has no visibility

This can be tested by modifying a validator's network configuration to use a different identity key than what derives to their registered stake pool address.

---

**Notes:**
The vulnerability stems from the architectural decision to use network-derived addresses (peer_id from x25519) for node identity while the validator set uses stake pool addresses. While these are expected to match, there's no enforcement or validation, and failures are silent. This affects both DKG and JWK consensus systems which use identical lookup patterns.

### Citations

**File:** dkg/src/epoch_manager.rs (L164-168)
```rust
        let my_index = epoch_state
            .verifier
            .address_to_validator_index()
            .get(&self.my_addr)
            .copied();
```

**File:** dkg/src/epoch_manager.rs (L199-201)
```rust
        let randomness_enabled =
            consensus_config.is_vtxn_enabled() && onchain_randomness_config.randomness_enabled();
        if let (true, Some(my_index)) = (randomness_enabled, my_index) {
```

**File:** aptos-node/src/consensus.rs (L85-85)
```rust
            let my_addr = node_config.validator_network.as_ref().unwrap().peer_id();
```

**File:** types/src/validator_info.rs (L21-24)
```rust
    // The validator's account address. AccountAddresses are initially derived from the account
    // auth pubkey; however, the auth key can be rotated, so one should not rely on this
    // initial property.
    pub account_address: AccountAddress,
```

**File:** types/src/account_address.rs (L135-146)
```rust
// Note: This is inconsistent with current types because AccountAddress is derived
// from consensus key which is of type Ed25519PublicKey. Since AccountAddress does
// not mean anything in a setting without remote authentication, we use the network
// public key to generate a peer_id for the peer.
// See this issue for potential improvements: https://github.com/aptos-labs/aptos-core/issues/3960
pub fn from_identity_public_key(identity_public_key: x25519::PublicKey) -> AccountAddress {
    let mut array = [0u8; AccountAddress::LENGTH];
    let pubkey_slice = identity_public_key.as_slice();
    // keep only the last 16 bytes
    array.copy_from_slice(&pubkey_slice[x25519::PUBLIC_KEY_SIZE - AccountAddress::LENGTH..]);
    AccountAddress::new(array)
}
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L161-165)
```rust
        let my_index = epoch_state
            .verifier
            .address_to_validator_index()
            .get(&self.my_addr)
            .copied();
```
