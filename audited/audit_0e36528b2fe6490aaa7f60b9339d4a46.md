# Audit Report

## Title
Admin Service Binds to All Network Interfaces by Default Without Validation, Risking Public Exposure of Sensitive Debug Endpoints

## Summary
The Admin Service configuration lacks validation to prevent binding to public IP addresses. The default configuration binds to `0.0.0.0` (all network interfaces), and there is no check in the `ConfigSanitizer` to warn operators or enforce localhost-only binding. This allows operators to accidentally expose sensitive debugging endpoints (consensus database dumps, transaction data, mempool state, profiling tools) to the internet, particularly on non-mainnet networks where authentication is not enforced.

## Finding Description
The `AdminServiceConfig` structure defines the admin service binding address with a default value of `"0.0.0.0"`. [1](#0-0) 

The `ConfigSanitizer` implementation for `AdminServiceConfig` only validates that authentication is configured on mainnet if the service is enabled, but does NOT validate the binding address to ensure localhost-only binding. [2](#0-1) 

The admin service exposes highly sensitive debugging endpoints including:
- `/debug/consensus/consensusdb` - dumps consensus state including votes, blocks, and quorum certificates
- `/debug/consensus/quorumstoredb` - dumps quorum store batches  
- `/debug/consensus/block` - extracts user transactions from blocks
- `/debug/mempool/parking-lot/addresses` - dumps mempool parking lot state
- `/profilez`, `/threadz`, `/malloc/*` - CPU profiling, thread dumps, and memory statistics [3](#0-2) 

The consensus database dump endpoint reveals internal consensus state that should never be publicly accessible. [4](#0-3) 

**Attack Scenario:**

1. **Non-Mainnet Networks (Testnet/Devnet):** The `ConfigOptimizer` automatically enables the admin service on non-mainnet chains. [5](#0-4)  On these networks, authentication is NOT required (only mainnet enforces authentication), meaning the endpoints are completely open if exposed.

2. **Production Deployment:** When deployed via Kubernetes with `enableAdminPort: true` in Helm values, the admin service port is exposed through a LoadBalancer. [6](#0-5) [7](#0-6) 

3. **Default Binding:** The service binds to the configured address without validation. [8](#0-7) 

An operator who:
- Deploys a testnet/devnet validator
- Sets `enableAdminPort: true` (or uses defaults that expose the port)
- Does not explicitly configure `address: "127.0.0.1"`

Will have their admin endpoints exposed to the internet with NO authentication, allowing any attacker to:
- Dump consensus state and analyze block production patterns
- Extract transaction data from blocks before public commitment
- Profile validator CPU/memory usage for targeted attacks
- Monitor mempool state to understand transaction flow

Even on mainnet with authentication enabled, the passcode is transmitted in plaintext query parameters, making it vulnerable to interception if TLS is not properly configured. [9](#0-8) 

## Impact Explanation
This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria for the following reasons:

1. **Significant Protocol Violations:** Exposing internal consensus state, transaction data, and validator profiling information to unauthorized parties represents a significant security violation of the validator's trusted operation.

2. **Information Disclosure Enabling Further Attacks:** The exposed information (consensus voting patterns, block timing, transaction ordering, mempool state) can be leveraged to:
   - Conduct timing attacks on consensus
   - Predict validator behavior for targeted attacks
   - Extract sensitive transaction data
   - Map validator network topology and performance characteristics

3. **Wide Attack Surface on Non-Mainnet:** Testnet and devnet validators are particularly vulnerable as they have NO authentication requirement while often being used for production-like testing with real operational patterns.

While not directly causing fund loss or consensus failure, this creates a significant attack vector that undermines validator security posture and could be combined with other vulnerabilities for more severe impacts.

## Likelihood Explanation
**High Likelihood** due to:

1. **Default Insecure Configuration:** The default binding to `0.0.0.0` means operators must explicitly change it to be secure, violating the principle of "secure by default."

2. **No Warning or Validation:** The absence of validation means operators receive no warning that they may be exposing sensitive endpoints publicly.

3. **Kubernetes Deployment Patterns:** Production deployments commonly use Kubernetes with LoadBalancers, which naturally expose configured ports to the internet unless explicitly restricted.

4. **Non-Mainnet Auto-Enable:** On testnet/devnet, the service is automatically enabled without authentication, maximizing the attack window.

5. **Documentation Gap:** Example configuration files do not include admin service configuration, suggesting operators may not even be aware of this security consideration. [10](#0-9) 

## Recommendation
Implement the following security controls:

1. **Change Default to Localhost:**
```rust
impl Default for AdminServiceConfig {
    fn default() -> Self {
        Self {
            enabled: None,
            address: "127.0.0.1".to_string(),  // Changed from "0.0.0.0"
            port: 9102,
            authentication_configs: vec![],
            malloc_stats_max_len: 2 * 1024 * 1024,
        }
    }
}
```

2. **Add Address Validation in Sanitizer:**
```rust
impl ConfigSanitizer for AdminServiceConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        if node_config.admin_service.enabled == Some(true) {
            // Validate address is localhost
            let addr = &node_config.admin_service.address;
            if addr != "127.0.0.1" && addr != "localhost" && addr != "::1" {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    format!(
                        "Admin service must bind to localhost only (127.0.0.1, ::1, or localhost), not '{}'",
                        addr
                    ),
                ));
            }

            // Existing authentication check for mainnet
            if let Some(chain_id) = chain_id {
                if chain_id.is_mainnet()
                    && node_config.admin_service.authentication_configs.is_empty()
                {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "Must enable authentication for AdminService on mainnet.".into(),
                    ));
                }
            }
        }

        Ok(())
    }
}
```

3. **Add Configuration Warning:** Log a prominent warning when the admin service is enabled, reminding operators of security implications.

4. **Update Documentation:** Clearly document that admin service should only be bound to localhost and provide secure configuration examples.

## Proof of Concept

**Reproduction Steps:**

1. Deploy an Aptos validator/fullnode on testnet using default configuration
2. Ensure `enableAdminPort: true` in Helm values
3. Observe admin service binds to `0.0.0.0:9102`
4. From external network, access: `http://<validator-ip>:9102/debug/consensus/consensusdb`
5. Successfully retrieve consensus database dump without authentication

**Expected Behavior:** Request should be rejected (connection refused) because service should be bound to localhost only.

**Actual Behavior:** Request succeeds, returning sensitive consensus state information to unauthorized external party.

**Test Configuration:**
```yaml
admin_service:
  enabled: true
  address: "0.0.0.0"  # Insecure default
  port: 9102
  authentication_configs: []  # No auth on non-mainnet
```

This configuration is vulnerable to public exposure when deployed with Kubernetes LoadBalancer or any cloud environment with public IP assignment.

### Citations

**File:** config/src/config/admin_service_config.rs (L41-50)
```rust
impl Default for AdminServiceConfig {
    fn default() -> Self {
        Self {
            enabled: None,
            address: "0.0.0.0".to_string(),
            port: 9102,
            authentication_configs: vec![],
            malloc_stats_max_len: 2 * 1024 * 1024,
        }
    }
```

**File:** config/src/config/admin_service_config.rs (L59-82)
```rust
impl ConfigSanitizer for AdminServiceConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        if node_config.admin_service.enabled == Some(true) {
            if let Some(chain_id) = chain_id {
                if chain_id.is_mainnet()
                    && node_config.admin_service.authentication_configs.is_empty()
                {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "Must enable authentication for AdminService on mainnet.".into(),
                    ));
                }
            }
        }

        Ok(())
    }
}
```

**File:** config/src/config/admin_service_config.rs (L84-107)
```rust
impl ConfigOptimizer for AdminServiceConfig {
    fn optimize(
        node_config: &mut NodeConfig,
        _local_config_yaml: &Value,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<bool, Error> {
        let mut modified_config = false;

        if node_config.admin_service.enabled.is_none() {
            // Only enable the admin service if the chain is not mainnet
            let admin_service_enabled = if let Some(chain_id) = chain_id {
                !chain_id.is_mainnet()
            } else {
                false // We cannot determine the chain ID, so we disable the admin service
            };
            node_config.admin_service.enabled = Some(admin_service_enabled);

            modified_config = true; // The config was modified
        }

        Ok(modified_config)
    }
}
```

**File:** crates/aptos-admin-service/src/server/mod.rs (L71-87)
```rust
    pub fn new(node_config: &NodeConfig) -> Self {
        let config = node_config.admin_service.clone();
        // Fetch the service port and address
        let service_port = config.port;
        let service_address = config.address.clone();

        // Create the admin service socket address
        let address: SocketAddr = (service_address.as_str(), service_port)
            .to_socket_addrs()
            .unwrap_or_else(|_| {
                panic!(
                    "Failed to parse {}:{} as address",
                    service_address, service_port
                )
            })
            .next()
            .unwrap();
```

**File:** crates/aptos-admin-service/src/server/mod.rs (L160-173)
```rust
                    AuthenticationConfig::PasscodeSha256(passcode_sha256) => {
                        let query = req.uri().query().unwrap_or("");
                        let query_pairs: HashMap<_, _> =
                            url::form_urlencoded::parse(query.as_bytes()).collect();
                        let passcode: Option<String> =
                            query_pairs.get("passcode").map(|p| p.to_string());
                        if let Some(passcode) = passcode {
                            if sha256::digest(passcode) == *passcode_sha256 {
                                authenticated = true;
                            }
                        }
                    },
                }
            }
```

**File:** crates/aptos-admin-service/src/server/mod.rs (L183-243)
```rust
        match (req.method().clone(), req.uri().path()) {
            #[cfg(target_os = "linux")]
            (hyper::Method::GET, "/profilez") => handle_cpu_profiling_request(req).await,
            #[cfg(target_os = "linux")]
            (hyper::Method::GET, "/threadz") => handle_thread_dump_request(req).await,
            #[cfg(unix)]
            (hyper::Method::GET, "/malloc/stats") => {
                malloc::handle_malloc_stats_request(context.config.malloc_stats_max_len)
            },
            #[cfg(unix)]
            (hyper::Method::GET, "/malloc/dump_profile") => malloc::handle_dump_profile_request(),
            (hyper::Method::GET, "/debug/consensus/consensusdb") => {
                let consensus_db = context.consensus_db.read().clone();
                if let Some(consensus_db) = consensus_db {
                    consensus::handle_dump_consensus_db_request(req, consensus_db).await
                } else {
                    Ok(reply_with_status(
                        StatusCode::NOT_FOUND,
                        "Consensus db is not available.",
                    ))
                }
            },
            (hyper::Method::GET, "/debug/consensus/quorumstoredb") => {
                let quorum_store_db = context.quorum_store_db.read().clone();
                if let Some(quorum_store_db) = quorum_store_db {
                    consensus::handle_dump_quorum_store_db_request(req, quorum_store_db).await
                } else {
                    Ok(reply_with_status(
                        StatusCode::NOT_FOUND,
                        "Quorum store db is not available.",
                    ))
                }
            },
            (hyper::Method::GET, "/debug/consensus/block") => {
                let consensus_db = context.consensus_db.read().clone();
                let quorum_store_db = context.quorum_store_db.read().clone();
                if let Some(consensus_db) = consensus_db
                    && let Some(quorum_store_db) = quorum_store_db
                {
                    consensus::handle_dump_block_request(req, consensus_db, quorum_store_db).await
                } else {
                    Ok(reply_with_status(
                        StatusCode::NOT_FOUND,
                        "Consensus db and/or quorum store db is not available.",
                    ))
                }
            },
            (hyper::Method::GET, "/debug/mempool/parking-lot/addresses") => {
                let mempool_client_sender = context.mempool_client_sender.read().clone();
                if let Some(mempool_client_sender) = mempool_client_sender {
                    mempool::mempool_handle_parking_lot_address_request(req, mempool_client_sender)
                        .await
                } else {
                    Ok(reply_with_status(
                        StatusCode::NOT_FOUND,
                        "Mempool parking lot is not available.",
                    ))
                }
            },
            _ => Ok(reply_with_status(StatusCode::NOT_FOUND, "Not found.")),
        }
```

**File:** crates/aptos-admin-service/src/server/consensus/mod.rs (L130-156)
```rust
fn dump_consensus_db(consensus_db: &dyn PersistentLivenessStorage) -> anyhow::Result<String> {
    let mut body = String::new();

    let (last_vote, highest_tc, consensus_blocks, consensus_qcs) =
        consensus_db.consensus_db().get_data()?;

    body.push_str(&format!("Last vote: \n{last_vote:?}\n\n"));
    body.push_str(&format!("Highest tc: \n{highest_tc:?}\n\n"));
    body.push_str("Blocks: \n");
    for block in consensus_blocks {
        body.push_str(&format!(
            "[id: {:?}, author: {:?}, epoch: {}, round: {:02}, parent_id: {:?}, timestamp: {}, payload: {:?}]\n\n",
            block.id(),
            block.author(),
            block.epoch(),
            block.round(),
            block.parent_id(),
            block.timestamp_usecs(),
            block.payload(),
        ));
    }
    body.push_str("QCs: \n");
    for qc in consensus_qcs {
        body.push_str(&format!("{qc:?}\n\n"));
    }
    Ok(body)
}
```

**File:** terraform/helm/aptos-node/templates/haproxy.yaml (L44-48)
```yaml
  {{- if $.Values.service.validator.enableAdminPort }}
  - name: admin
    port: 9102
    targetPort: 9202
  {{- end }}
```

**File:** terraform/helm/aptos-node/values.yaml (L158-159)
```yaml
    # -- Enable the admin port on the validator
    enableAdminPort: false
```

**File:** config/src/config/test_data/validator.yaml (L1-81)
```yaml
base:
    data_dir: "/opt/aptos/data"
    role: "validator"
    waypoint:
        from_storage:
            type: "vault"
            server: "https://127.0.0.1:8200"
            ca_certificate: "/full/path/to/certificate"
            token:
                from_disk: "/full/path/to/token"

consensus:
    safety_rules:
        service:
            type: process
            server_address: "/ip4/127.0.0.1/tcp/5555"

execution:
    genesis_file_location: "relative/path/to/genesis"

# For validator node we setup two networks, validator_network to allow validator connect to each other,
# and full_node_networks to allow fullnode connects to validator.

full_node_networks:
    - listen_address: "/ip4/0.0.0.0/tcp/6181"
      max_outbound_connections: 0
      identity:
          type: "from_storage"
          key_name: "fullnode_network"
          peer_id_name: "owner_account"
          backend:
              type: "vault"
              server: "https://127.0.0.1:8200"
              ca_certificate: "/full/path/to/certificate"
              token:
                  from_disk: "/full/path/to/token"
      network_id:
          private: "vfn"

validator_network:
    discovery_method: "onchain"
    listen_address: "/ip4/0.0.0.0/tcp/6180"
    identity:
        type: "from_storage"
        key_name: "validator_network"
        peer_id_name: "owner_account"
        backend:
            type: "vault"
            server: "https://127.0.0.1:8200"
            ca_certificate: "/full/path/to/certificate"
            token:
                from_disk: "/full/path/to/token"
    network_id: "validator"
    ### Load keys from file
    # identity:
    #     type: "from_file"
    #     path: /full/path/to/private-keys.yml
    #
    ### Load keys from secure storage service like vault:
    #
    # identity:
    #     type: "from_storage"
    #     key_name: "validator_network"
    #     peer_id_name: "owner_account"
    #     backend:
    #         type: "vault"
    #         server: "https://127.0.0.1:8200"
    #         ca_certificate: "/full/path/to/certificate"
    #         token:
    #             from_disk: "/full/path/to/token"
    #
    ### Load keys directly from config
    #
    # identity:
    #     type: "from_config"
    #     key: "b0f405a3e75516763c43a2ae1d70423699f34cd68fa9f8c6bb2d67aa87d0af69"
    #     peer_id: "00000000000000000000000000000000d58bc7bb154b38039bc9096ce04e1237"
    mutual_authentication: true
    max_frame_size: 4194304 # 4 MiB
api:
    enabled: true
```
