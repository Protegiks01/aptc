# Audit Report

## Title
Liveness Vulnerability: Inconsistent Payload Availability Semantics Enable Infinite Retry Loops

## Summary
The `check_payload_availability` method in QuorumStore has inconsistent semantics between payload types - `InQuorumStore` uses optimistic checking (always returns `Ok()`), while `OptQuorumStore` uses pessimistic checking (verifies local batch availability). This inconsistency allows validators to vote on blocks before confirming data availability, leading to infinite retry loops during execution when batches cannot be fetched, causing permanent liveness failure.

## Finding Description

The vulnerability stems from inconsistent availability checking across QuorumStore payload types combined with an unbounded retry loop in the execution pipeline.

**Inconsistent Availability Semantics:** [1](#0-0) 

For `InQuorumStore` and `QuorumStoreInlineHybrid` payloads, the method returns `Ok()` optimistically, assuming network delivery will succeed: [2](#0-1) 

However, for `OptQuorumStore` payloads, it checks local batch availability and returns `Err(missing_authors)` if batches are missing: [3](#0-2) 

**Voting Without Data Availability:**

Validators call `check_payload_availability` before voting. If it returns `Ok()`, they vote immediately: [4](#0-3) 

**Infinite Retry Loop:**

When a block enters execution, the `materialize` phase attempts to fetch transaction data. If `get_transactions` fails (e.g., `ExecutorError::CouldNotGetData` when batches cannot be fetched), it retries indefinitely with no timeout: [5](#0-4) 

**Batch Fetching Failure:**

When batches aren't available locally, the system attempts network fetching with limited retries. After exhausting retries, it returns an error: [6](#0-5) 

**Attack Scenario:**

1. A Byzantine leader creates a block with `InQuorumStore` payload containing batches distributed to only 2f+1 validators
2. All validators' `check_payload_availability` returns `Ok()` (optimistic)
3. All validators vote on the block, which achieves QC
4. During execution, f honest validators without the batches attempt to fetch them from the network
5. Byzantine peers refuse to respond to batch requests
6. After retry exhaustion, `batch_reader.get_batch` returns `ExecutorError::CouldNotGetData`
7. The `materialize` function catches this error and enters infinite retry loop (100ms intervals)
8. These f validators are permanently stuck and cannot progress

This violates the critical invariant: **"Validators should only vote on blocks they can execute."**

## Impact Explanation

**Critical Severity - Total Loss of Liveness:**

This vulnerability enables a Byzantine leader (controlling < 1/3 Byzantine validators) to permanently halt consensus progress by causing f honest validators to enter infinite retry loops. Once stuck, these validators cannot:
- Process subsequent proposals
- Participate in voting
- Advance execution root
- Commit blocks

The network loses liveness even though the Byzantine threshold isn't exceeded, as the stuck validators effectively become unavailable. Recovery requires manual intervention (node restart or state sync), meeting the "Non-recoverable network partition" criteria. [7](#0-6) 

## Likelihood Explanation

**High Likelihood:**

The attack requires:
1. Byzantine leader proposes block with `InQuorumStore` payload
2. Selective batch distribution to subset of validators  
3. Byzantine peers refuse serving batches to targeted validators

In BFT systems, Byzantine behavior (up to f validators) is the expected threat model. The attack is straightforward to execute for any malicious leader, requires no cryptographic breaks, and exploits intended protocol behavior (optimistic availability checking).

The vulnerability is deterministic - once validators vote on a block with unavailable data, they will invariably enter the infinite retry loop. Network partitions or transient failures could also trigger this condition without malicious intent.

## Recommendation

**Enforce Consistent Pessimistic Availability Checking:**

All payload types should verify local batch availability before allowing votes. Remove optimistic checking for `InQuorumStore` payloads:

```rust
fn check_payload_availability(&self, block: &Block) -> Result<(), BitVec> {
    let Some(payload) = block.payload() else {
        return Ok(());
    };

    match payload {
        Payload::InQuorumStore(proof_with_data) | 
        Payload::InQuorumStoreWithLimit(proof_with_data_limit) => {
            // Change from optimistic Ok() to pessimistic batch checking
            let proofs = match payload {
                Payload::InQuorumStore(pwd) => &pwd.proofs,
                Payload::InQuorumStoreWithLimit(pwdl) => &pwdl.proof_with_data.proofs,
                _ => unreachable!(),
            };
            
            let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
            for proof in proofs {
                if self.batch_reader.exists(proof.digest()).is_none() {
                    let index = *self.address_to_validator_index
                        .get(&proof.author())
                        .expect("Proof author should have been verified");
                    missing_authors.set(index as u16);
                }
            }
            
            if missing_authors.all_zeros() {
                Ok(())
            } else {
                Err(missing_authors)
            }
        },
        // ... rest of payload types
    }
}
```

**Add Timeout to Materialize Retry Loop:** [8](#0-7) 

Add a deadline parameter and timeout mechanism:

```rust
async fn materialize(
    preparer: Arc<BlockPreparer>,
    block: Arc<Block>,
    qc_rx: oneshot::Receiver<Arc<QuorumCert>>,
    deadline: Duration,
) -> TaskResult<MaterializeResult> {
    let start = Instant::now();
    let result = loop {
        match preparer.materialize_block(&block, qc_rx.clone()).await {
            Ok(input_txns) => break input_txns,
            Err(e) => {
                if start.elapsed() > deadline {
                    return Err(anyhow::anyhow!("Materialize timeout for block {}", block.id()));
                }
                warn!("[BlockPreparer] failed to prepare block {}, retrying: {}", block.id(), e);
                tokio::time::sleep(Duration::from_millis(100)).await;
            },
        }
    };
    Ok(result)
}
```

## Proof of Concept

**Setup:** Deploy Aptos testnet with 4 validators (f=1), configure QuorumStore enabled.

**Attack Steps:**

1. **Byzantine Leader Creates Malicious Proposal:**
```rust
// Leader at round R creates InQuorumStore payload
let batches = create_batches(transactions);
// Distribute batches to only 3 out of 4 validators
distribute_batches_selectively(batches, vec![validator1, validator2, byzantine_validator]);
let payload = Payload::InQuorumStore(ProofWithData { proofs: batch_proofs });
let proposal = create_proposal(round, payload);
broadcast_proposal(proposal);
```

2. **All Validators Vote Optimistically:**
```rust
// All validators process proposal
// check_payload_availability returns Ok() for InQuorumStore (optimistic)
// validator4 doesn't have batches but votes anyway
vote = create_vote(proposal);
broadcast_vote(vote);
```

3. **Block Gets QC and Enters Execution:**
```rust
// Block achieves 2f+1 votes, gets QC
// Execution pipeline processes block
materialize_block(block) {
    get_transactions(block) {
        // validator4 tries to fetch missing batches
        batch_reader.get_batch(batch_info, responders) {
            // Retry loop with Byzantine peers not responding
            request_batch() // fails after retries
            -> ExecutorError::CouldNotGetData
        }
    }
    // Error caught, retry indefinitely at 100ms intervals
    loop { /* stuck here forever */ }
}
```

4. **Verify Liveness Failure:**
```bash
# Monitor validator4 logs - shows repeated batch request failures
# Check consensus progress - validator4 frozen at block B
# Network cannot progress past block B due to validator4 stuck
# Manual intervention (restart/state-sync) required for recovery
```

**Expected Result:** validator4 enters infinite retry loop, consensus halts, demonstrating critical liveness vulnerability from inconsistent availability semantics.

## Notes

The vulnerability is particularly dangerous because:
1. It manifests within normal BFT threat model (f Byzantine validators)
2. The infinite retry has no timeout or circuit breaker mechanism
3. Recovery requires manual intervention (violates non-recoverable criterion)
4. The inconsistency between payload types makes the attack non-obvious

The pessimistic checking in `OptQuorumStore` demonstrates the correct approach - validators should only vote after confirming they can execute the block. Extending this invariant to all payload types closes the vulnerability.

### Citations

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L349-444)
```rust
    fn check_payload_availability(&self, block: &Block) -> Result<(), BitVec> {
        let Some(payload) = block.payload() else {
            return Ok(());
        };

        match payload {
            Payload::DirectMempool(_) => {
                unreachable!("QuorumStore doesn't support DirectMempool payload")
            },
            Payload::InQuorumStore(_) => Ok(()),
            Payload::InQuorumStoreWithLimit(_) => Ok(()),
            Payload::QuorumStoreInlineHybrid(inline_batches, proofs, _)
            | Payload::QuorumStoreInlineHybridV2(inline_batches, proofs, _) => {
                fn update_availability_metrics<'a>(
                    batch_reader: &Arc<dyn BatchReader>,
                    is_proof_label: &str,
                    batch_infos: impl Iterator<Item = &'a BatchInfo>,
                ) {
                    for (author, chunk) in &batch_infos.chunk_by(|info| info.author()) {
                        let (available_count, missing_count) = chunk
                            .map(|info| batch_reader.exists(info.digest()))
                            .fold((0, 0), |(available_count, missing_count), item| {
                                if item.is_some() {
                                    (available_count + 1, missing_count)
                                } else {
                                    (available_count, missing_count + 1)
                                }
                            });
                        counters::CONSENSUS_PROPOSAL_PAYLOAD_BATCH_AVAILABILITY_IN_QS
                            .with_label_values(&[
                                &author.to_hex_literal(),
                                is_proof_label,
                                "available",
                            ])
                            .inc_by(available_count as u64);
                        counters::CONSENSUS_PROPOSAL_PAYLOAD_BATCH_AVAILABILITY_IN_QS
                            .with_label_values(&[
                                &author.to_hex_literal(),
                                is_proof_label,
                                "missing",
                            ])
                            .inc_by(missing_count as u64);
                    }
                }

                update_availability_metrics(
                    &self.batch_reader,
                    "false",
                    inline_batches.iter().map(|(batch_info, _)| batch_info),
                );
                update_availability_metrics(
                    &self.batch_reader,
                    "true",
                    proofs.proofs.iter().map(|proof| proof.info()),
                );

                // The payload is considered available because it contains only proofs that guarantee network availabiliy
                // or inlined transactions.
                Ok(())
            },
            Payload::OptQuorumStore(OptQuorumStorePayload::V1(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
            },
            Payload::OptQuorumStore(OptQuorumStorePayload::V2(p)) => {
                let mut missing_authors = BitVec::with_num_bits(self.ordered_authors.len() as u16);
                for batch in p.opt_batches().deref() {
                    if self.batch_reader.exists(batch.digest()).is_none() {
                        let index = *self
                            .address_to_validator_index
                            .get(&batch.author())
                            .expect("Payload author should have been verified");
                        missing_authors.set(index as u16);
                    }
                }
                if missing_authors.all_zeros() {
                    Ok(())
                } else {
                    Err(missing_authors)
                }
            },
        }
    }
```

**File:** consensus/src/round_manager.rs (L1262-1285)
```rust
        if block_store.check_payload(&proposal).is_err() {
            debug!("Payload not available locally for block: {}", proposal.id());
            counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
                .with_label_values(&["missing"])
                .inc();
            let start_time = Instant::now();
            let deadline = self.round_state.current_round_deadline();
            let future = async move {
                (
                    block_store.wait_for_payload(&proposal, deadline).await,
                    proposal,
                    start_time,
                )
            }
            .boxed();
            self.futures.push(future);
            return Ok(());
        }

        counters::CONSENSUS_PROPOSAL_PAYLOAD_AVAILABILITY
            .with_label_values(&["available"])
            .inc();

        self.check_backpressure_and_process_proposal(proposal).await
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L613-648)
```rust
    /// Precondition: Block is inserted into block tree (all ancestors are available)
    /// What it does: Wait for all data becomes available and verify transaction signatures
    async fn materialize(
        preparer: Arc<BlockPreparer>,
        block: Arc<Block>,
        qc_rx: oneshot::Receiver<Arc<QuorumCert>>,
    ) -> TaskResult<MaterializeResult> {
        let mut tracker = Tracker::start_waiting("materialize", &block);
        tracker.start_working();

        let qc_rx = async {
            match qc_rx.await {
                Ok(qc) => Some(qc),
                Err(_) => {
                    warn!("[BlockPreparer] qc tx cancelled for block {}", block.id());
                    None
                },
            }
        }
        .shared();
        // the loop can only be abort by the caller
        let result = loop {
            match preparer.materialize_block(&block, qc_rx.clone()).await {
                Ok(input_txns) => break input_txns,
                Err(e) => {
                    warn!(
                        "[BlockPreparer] failed to prepare block {}, retrying: {}",
                        block.id(),
                        e
                    );
                    tokio::time::sleep(Duration::from_millis(100)).await;
                },
            }
        };
        Ok(result)
    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L101-180)
```rust
    pub(crate) async fn request_batch(
        &self,
        digest: HashValue,
        expiration: u64,
        responders: Arc<Mutex<BTreeSet<PeerId>>>,
        mut subscriber_rx: oneshot::Receiver<PersistedValue<BatchInfoExt>>,
    ) -> ExecutorResult<Vec<SignedTransaction>> {
        let validator_verifier = self.validator_verifier.clone();
        let mut request_state = BatchRequesterState::new(responders, self.retry_limit);
        let network_sender = self.network_sender.clone();
        let request_num_peers = self.request_num_peers;
        let my_peer_id = self.my_peer_id;
        let epoch = self.epoch;
        let retry_interval = Duration::from_millis(self.retry_interval_ms as u64);
        let rpc_timeout = Duration::from_millis(self.rpc_timeout_ms as u64);

        monitor!("batch_request", {
            let mut interval = time::interval(retry_interval);
            let mut futures = FuturesUnordered::new();
            let request = BatchRequest::new(my_peer_id, epoch, digest);
            loop {
                tokio::select! {
                    _ = interval.tick() => {
                        // send batch request to a set of peers of size request_num_peers
                        if let Some(request_peers) = request_state.next_request_peers(request_num_peers) {
                            for peer in request_peers {
                                futures.push(network_sender.request_batch(request.clone(), peer, rpc_timeout));
                            }
                        } else if futures.is_empty() {
                            // end the loop when the futures are drained
                            break;
                        }
                    },
                    Some(response) = futures.next() => {
                        match response {
                            Ok(BatchResponse::Batch(batch)) => {
                                counters::RECEIVED_BATCH_RESPONSE_COUNT.inc();
                                let payload = batch.into_transactions();
                                return Ok(payload);
                            }
                            // Short-circuit if the chain has moved beyond expiration
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
                            }
                            Ok(BatchResponse::BatchV2(_)) => {
                                error!("Batch V2 response is not supported");
                            }
                            Err(e) => {
                                counters::RECEIVED_BATCH_RESPONSE_ERROR_COUNT.inc();
                                debug!("QS: batch request error, digest:{}, error:{:?}", digest, e);
                            }
                        }
                    },
                    result = &mut subscriber_rx => {
                        match result {
                            Ok(persisted_value) => {
                                counters::RECEIVED_BATCH_FROM_SUBSCRIPTION_COUNT.inc();
                                let (_, maybe_payload) = persisted_value.unpack();
                                return Ok(maybe_payload.expect("persisted value must exist"));
                            }
                            Err(err) => {
                                debug!("channel closed: {}", err);
                            }
                        };
                    },
                }
            }
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
        })
    }
```

**File:** consensus/src/counters.rs (L1184-1212)
```rust
pub fn log_executor_error_occurred(
    e: ExecutorError,
    counter: &Lazy<IntCounterVec>,
    block_id: HashValue,
) {
    match e {
        ExecutorError::CouldNotGetData => {
            counter.with_label_values(&["CouldNotGetData"]).inc();
            warn!(
                block_id = block_id,
                "Execution error - CouldNotGetData {}", block_id
            );
        },
        ExecutorError::BlockNotFound(block_id) => {
            counter.with_label_values(&["BlockNotFound"]).inc();
            warn!(
                block_id = block_id,
                "Execution error BlockNotFound {}", block_id
            );
        },
        e => {
            counter.with_label_values(&["UnexpectedError"]).inc();
            warn!(
                block_id = block_id,
                "Execution error {:?} for {}", e, block_id
            );
        },
    }
}
```
