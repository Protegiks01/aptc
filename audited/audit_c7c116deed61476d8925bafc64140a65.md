# Audit Report

## Title
Hot State Merkle Tree Node Accumulation Due to Missing Pruning Mechanism

## Summary
Hot state operations create Merkle tree nodes that are persisted to disk but never pruned, leading to unbounded disk space growth until node restart. While the filtering in `put_stale_state_value_index_for_shard()` correctly excludes hot state operations from cold state stale indices, there is no corresponding pruning mechanism for hot state Merkle tree nodes, causing orphaned data accumulation.

## Finding Description

The Aptos storage system maintains two separate state trees: cold state (global state) and hot state (frequently accessed items). When processing state updates in `put_stale_state_value_index_for_shard()`, the code filters operations using `is_value_write_op()` to exclude `MakeHot` operations from creating cold state stale indices. [1](#0-0) [2](#0-1) 

This filtering is correct for cold state - `MakeHot` operations don't create state values in cold storage and shouldn't generate cold state stale indices. However, these operations DO create hot state Merkle tree nodes that are persisted to disk: [3](#0-2) [4](#0-3) 

The critical issue is that there is **no pruning mechanism** for hot state Merkle tree nodes: [5](#0-4) 

Every hot state change (promotion via `MakeHot`, refresh, modification, or eviction) creates new Merkle tree nodes in `hot_state_merkle_db`, but old nodes are never cleaned up during runtime. These orphaned nodes accumulate indefinitely.

**Attack Path:**
1. Attacker sends transactions that read many different state keys
2. The read-write summary generates `MakeHot` operations for accessed keys
3. Each `MakeHot` operation creates hot state Merkle nodes
4. As the hot state LRU evicts old entries, more Merkle nodes are created for evictions
5. None of these nodes are ever pruned - they accumulate in `hot_state_merkle_db`
6. Over time, disk space is exhausted, potentially causing node failure

## Impact Explanation

This qualifies as **High Severity** under Aptos bug bounty criteria:
- **Validator node slowdowns**: Accumulated Merkle nodes degrade disk I/O performance
- **Potential node unavailability**: Disk exhaustion can crash the validator node

While not Critical severity (no consensus violation or fund loss), this creates a genuine operational risk. The mitigation of restarting nodes to clear hot state is documented but requiring periodic restarts to avoid disk exhaustion is unacceptable for production blockchain infrastructure. [6](#0-5) 

## Likelihood Explanation

**High likelihood** - This occurs naturally during normal blockchain operation:
- Every transaction execution generates hot state updates
- High transaction throughput accelerates accumulation
- No special attacker permissions required - anyone can send transactions
- Disk space growth is inevitable and continuous

An attacker can accelerate the attack by:
- Sending transactions that read many unique state keys (generating `MakeHot` operations)
- Operating at gas cost, making this economically feasible for motivated attackers

## Recommendation

Implement hot state Merkle tree pruning similar to cold state pruning:

1. Create a `StaleHotNodeIndexSchema` to track stale hot state Merkle nodes
2. Track hot state node versions when committing hot state updates
3. Enable `state_merkle_pruner` to handle `hot_state_merkle_db` pruning
4. Set pruning targets for hot state alongside cold state:

```rust
// In state_merkle_batch_committer.rs, replace TODO with:
if let Some(hot_db) = &self.state_db.hot_state_merkle_db {
    // Create hot state pruner manager if needed
    self.state_db
        .hot_state_merkle_pruner  
        .maybe_set_pruner_target_db_version(current_version);
}
```

5. Update `put_stale_state_value_index_for_shard()` to also create stale indices for hot state Merkle nodes (separate from cold state indices)

## Proof of Concept

```rust
// Reproduction steps (requires running validator node):

// 1. Start a fresh validator node with hot state enabled
// 2. Monitor hot_state_merkle_db directory size:
//    du -sh <data_path>/hot_state_merkle_db/

// 3. Execute high-volume transactions reading diverse state keys:
use aptos_sdk::types::transaction::Script;

for i in 0..100000 {
    // Transaction reading unique state keys
    let state_key = StateKey::access_path(AccessPath::new(
        AccountAddress::random(),
        random_resource_tag()
    ));
    // Read operations generate MakeHot operations
    vm.read_state(state_key);
    vm.execute_transaction(...);
}

// 4. Observe continuous growth of hot_state_merkle_db:
//    - Initial size: ~10MB
//    - After 100k transactions: ~500MB+
//    - Growth continues unbounded until restart
//    - No pruning occurs despite old nodes being stale

// 5. Verify pruning absence:
//    Check logs for "hot state pruning" - none exist
//    Verify state_merkle_pruner only targets cold state
//    Confirm hot_state_merkle_db contains nodes from early versions

// 6. Restart node:
//    hot_state_merkle_db is deleted (delete_on_restart=true)
//    Size resets to ~10MB
//    Confirms nodes were orphaned, not actively used
```

**Notes:**
This vulnerability represents a documented limitation (per the TODO comment) that creates real operational risk. While mitigation exists (restart clears hot state), the accumulation of orphaned data constitutes a genuine issue warranting attention and resolution through proper hot state pruning implementation.

### Citations

**File:** storage/aptosdb/src/state_store/mod.rs (L940-944)
```rust
            let ver_iter = iter
                .take_while_ref(|(_k, u)| u.version == version)
                // ignore hot state only ops
                // TODO(HotState): revisit
                .filter(|(_key, update)| update.state_op.is_value_write_op());
```

**File:** types/src/write_set.rs (L117-124)
```rust
    pub fn is_value_write_op(&self) -> bool {
        use BaseStateOp::*;

        match self {
            Creation(_) | Modification(_) | Deletion(_) => true,
            MakeHot => false,
        }
    }
```

**File:** storage/aptosdb/src/state_store/state_snapshot_committer.rs (L109-122)
```rust
                            let mut hot_updates = Vec::new();
                            let mut all_updates = Vec::new();
                            for (key, slot) in updates.iter() {
                                if slot.is_hot() {
                                    hot_updates.push((
                                        CryptoHash::hash(&key),
                                        Some((
                                            HotStateValueRef::from_slot(&slot).hash(),
                                            key.clone(),
                                        )),
                                    ));
                                } else {
                                    hot_updates.push((CryptoHash::hash(&key), None));
                                }
```

**File:** storage/aptosdb/src/state_store/state_merkle_batch_committer.rs (L69-79)
```rust
                    if let Some(hot_state_merkle_batch) = hot_batch {
                        self.commit(
                            self.state_db
                                .hot_state_merkle_db
                                .as_ref()
                                .expect("Hot state merkle db must exist."),
                            current_version,
                            hot_state_merkle_batch,
                        )
                        .expect("Hot state merkle nodes commit failed.");
                    }
```

**File:** storage/aptosdb/src/state_store/state_merkle_batch_committer.rs (L91-98)
```rust
                    // TODO(HotState): no pruning for hot state right now, since we always reset it
                    // upon restart.
                    self.state_db
                        .state_merkle_pruner
                        .maybe_set_pruner_target_db_version(current_version);
                    self.state_db
                        .epoch_snapshot_pruner
                        .maybe_set_pruner_target_db_version(current_version);
```

**File:** storage/aptosdb/src/state_merkle_db.rs (L717-721)
```rust
        if delete_on_restart {
            ensure!(!readonly, "Should not reset DB in read-only mode.");
            info!("delete_on_restart is true. Removing {path:?} entirely.");
            std::fs::remove_dir_all(&path).unwrap_or(());
        }
```
