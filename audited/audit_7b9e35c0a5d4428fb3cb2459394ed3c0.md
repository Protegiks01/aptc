# Audit Report

## Title
Race Condition in CommandAdapter Allows Backup Corruption When Multiple Validators Share Storage

## Summary
The `CommandAdapter` backup storage implementation lacks race condition protection when multiple validator nodes use shared storage. Unlike the `LocalFs` implementation which uses atomic file creation (`create_new(true)`), the `CommandAdapter` blindly executes shell commands that can overwrite existing files, leading to backup corruption and metadata inconsistency. [1](#0-0) 

## Finding Description

The backup system makes a critical assumption that only a single `BackupCoordinator` writes to the shared storage, as documented in the codebase: [2](#0-1) [3](#0-2) 

However, when multiple validator nodes are configured to use `CommandAdapter` with shared cloud storage (S3, GCS, Azure), concurrent operations cause race conditions:

**The Vulnerability Chain:**

1. Multiple nodes run `BackupCoordinator` independently, backing up the same epoch/version ranges
2. Each node creates separate backup directories with random suffixes via `create_backup_with_random_suffix()` [4](#0-3) 

3. Both nodes write chunk and manifest files to their respective directories successfully
4. Both nodes attempt to write metadata files with identical names (e.g., `epoch_ending_0-5.meta`) [5](#0-4) 

5. The `save_metadata_lines` command in `CommandAdapter` uses cloud storage commands (`aws s3 cp`, `gsutil cp`) that **overwrite existing files without atomic checks**: [6](#0-5) [7](#0-6) 

**Contrast with LocalFs Protection:**

The `LocalFs` implementation uses `create_new(true)` which atomically fails if the file exists, and handles `AlreadyExists` errors gracefully: [8](#0-7) 

**Race Condition Outcomes:**

- **Metadata Overwrite**: Node B's metadata silently replaces Node A's, causing Node A's backup files to become orphaned (unreferenced in metadata)
- **Concurrent Write Corruption**: If both nodes write simultaneously, the gzipped streams can be interleaved, producing a corrupted file that cannot be decompressed
- **Inconsistent State**: Partial writes during failures leave the metadata in an inconsistent state

## Impact Explanation

This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria:

- **Significant protocol violation**: Violates the backup system's core assumption of single-coordinator consistency
- **State inconsistencies requiring intervention**: Backup corruption requires manual recovery procedures, potentially involving database reconstruction from incomplete backups
- **Data loss risk**: Orphaned backup files waste storage and create confusion about which backups are valid
- **Silent failure**: Unlike `LocalFs`, there's no error detection—the overwrite succeeds silently

While this doesn't directly affect consensus or live validator operations, backup integrity is critical for disaster recovery scenarios. A corrupted backup discovered during an emergency restoration could be catastrophic.

## Likelihood Explanation

**High likelihood** in multi-validator deployments:

1. **Common deployment pattern**: Operators may configure multiple validators to back up to the same S3/GCS bucket for centralization and cost efficiency
2. **Automatic trigger**: The race condition occurs naturally without any attacker action—merely running multiple coordinators triggers it
3. **No documentation warning**: The README provides configuration examples but doesn't warn against shared storage use [9](#0-8) 

4. **Implicit assumption**: The single-coordinator assumption is buried in implementation comments, not enforced by design

## Recommendation

**Immediate Fix**: Add atomic write protection to `CommandAdapter.save_metadata_lines()`:

1. Implement existence check before writing (similar to `LocalFs`)
2. Use cloud-provider-specific atomic operations:
   - AWS S3: Use conditional PutObject with `If-None-Match: *` header
   - GCS: Use precondition `ifGenerationMatch: 0`
   - Azure: Use `If-None-Match: *` header

3. Handle `AlreadyExists` errors gracefully (log and skip, like `LocalFs`)

**Additional Mitigations**:

1. **Add distributed locking**: Implement a lock file mechanism (e.g., `metadata/.lock`) that coordinators must acquire before writing
2. **Unique metadata naming**: Include node identity in metadata filenames to avoid collisions
3. **Documentation**: Clearly document that only one `BackupCoordinator` should write to a given storage location
4. **Configuration validation**: Add a node identity check at startup that fails if multiple nodes with the same identity write to shared storage

## Proof of Concept

```rust
// Simulation of race condition in CommandAdapter
// This demonstrates the vulnerability without requiring actual cloud storage setup

use std::sync::Arc;
use tokio::task::JoinSet;
use tempfile::TempDir;

async fn simulate_concurrent_backup_coordinators() {
    // Setup: Two validator nodes using shared CommandAdapter storage
    let shared_storage = Arc::new(setup_command_adapter_storage());
    
    let mut tasks = JoinSet::new();
    
    // Node A starts backing up epoch 0-5
    let storage_a = Arc::clone(&shared_storage);
    tasks.spawn(async move {
        let coordinator_a = BackupCoordinator::new(
            /* ... */,
            storage_a,
        );
        // This will create backup directory "epoch_ending_0-.abcd/"
        // and write metadata file "metadata/epoch_ending_0-5.meta"
        coordinator_a.backup_epoch_endings(0, 5).await
    });
    
    // Node B starts backing up the same epoch range
    let storage_b = Arc::clone(&shared_storage);
    tasks.spawn(async move {
        let coordinator_b = BackupCoordinator::new(
            /* ... */,
            storage_b,
        );
        // This will create backup directory "epoch_ending_0-.efgh/"
        // and attempt to write the SAME metadata file "metadata/epoch_ending_0-5.meta"
        coordinator_b.backup_epoch_endings(0, 5).await
    });
    
    // Wait for both to complete
    while let Some(result) = tasks.join_next().await {
        result.unwrap().unwrap();
    }
    
    // Verify corruption:
    let metadata_content = read_metadata_file("epoch_ending_0-5.meta").await;
    // Expected: metadata points to ONLY ONE backup directory
    // Actual: The last write wins (either .abcd or .efgh)
    // Result: The other backup directory's files are orphaned
    
    assert!(backup_files_exist("epoch_ending_0-.abcd/"));
    assert!(backup_files_exist("epoch_ending_0-.efgh/"));
    // But only ONE is referenced in metadata - the other is orphaned!
}
```

To reproduce with actual cloud storage:
1. Deploy two validator nodes with identical `CommandAdapterConfig` pointing to the same S3 bucket
2. Start both `BackupCoordinator` instances simultaneously
3. Monitor the metadata directory—observe files being overwritten
4. Verify orphaned backup directories accumulate over time

## Notes

This vulnerability is particularly insidious because:
1. The `LocalFs` implementation has proper protection, but `CommandAdapter` (likely the production deployment path) does not
2. The vulnerability manifests silently—no errors are logged when overwrites occur
3. The backup system's continuity assumptions break down, making restoration unreliable
4. The cloud storage commands in sample configs (`aws s3 cp`, `gsutil cp`) default to overwrite behavior without any atomic guarantees

### Citations

**File:** storage/backup/backup-cli/src/storage/command_adapter/mod.rs (L162-191)
```rust
    async fn save_metadata_lines(
        &self,
        name: &ShellSafeName,
        lines: &[TextLine],
    ) -> Result<FileHandle> {
        let mut child = self
            .cmd(&self.config.commands.save_metadata_line, vec![
                EnvVar::file_name(name.as_ref()),
            ])
            .spawn()?;
        let mut file_handle = FileHandle::new();
        child
            .stdout()
            .read_to_string(&mut file_handle)
            .await
            .err_notes(name)?;
        let content = lines
            .iter()
            .map(|e| e.as_ref())
            .collect::<Vec<&str>>()
            .join("");
        child
            .stdin()
            .write_all(content.as_bytes())
            .await
            .err_notes(name)?;
        child.join().await?;
        file_handle.truncate(file_handle.trim_end().len());
        Ok(file_handle)
    }
```

**File:** storage/backup/backup-cli/src/metadata/view.rs (L136-138)
```rust
    ) -> Result<Vec<TransactionBackupMeta>> {
        // This can be more flexible, but for now we assume and check backups are continuous in
        // range (which is always true when we backup from a single backup coordinator)
```

**File:** storage/backup/backup-cli/src/metadata/view.rs (L174-176)
```rust
    ) -> Result<Vec<EpochEndingBackupMeta>> {
        // This can be more flexible, but for now we assume and check backups are continuous in
        // range (which is always true when we backup from a single backup coordinator)
```

**File:** storage/backup/backup-cli/src/utils/storage_ext.rs (L39-42)
```rust
    async fn create_backup_with_random_suffix(&self, name: &str) -> Result<BackupHandle> {
        self.create_backup(&format!("{}.{:04x}", name, random::<u16>()).try_into()?)
            .await
    }
```

**File:** storage/backup/backup-cli/src/metadata/mod.rs (L152-160)
```rust
    pub fn name(&self) -> ShellSafeName {
        match self {
            Self::EpochEndingBackup(e) => {
                format!("epoch_ending_{}-{}.meta", e.first_epoch, e.last_epoch)
            },
            Self::StateSnapshotBackup(s) => format!("state_snapshot_ver_{}.meta", s.version),
            Self::TransactionBackup(t) => {
                format!("transaction_{}-{}.meta", t.first_version, t.last_version)
            },
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/sample_configs/s3.sample.yaml (L22-27)
```yaml
  save_metadata_line: |
    # save the line to a new file under the metadata folder
    FILE_HANDLE="metadata/$FILE_NAME"
    echo "$FILE_HANDLE"
    exec 1>&-
    gzip -c | aws s3 cp - "s3://$BUCKET/$SUB_DIR/$FILE_HANDLE"
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/sample_configs/gcp.sample.yaml (L22-26)
```yaml
  save_metadata_line: |
    FILE_HANDLE="metadata/$FILE_NAME"
    echo "$FILE_HANDLE"
    exec 1>&-
    gzip -c | gsutil -q cp - "gs://$BUCKET/$SUB_DIR/$FILE_HANDLE" > /dev/null
```

**File:** storage/backup/backup-cli/src/storage/local_fs/mod.rs (L162-176)
```rust
        let file = OpenOptions::new()
            .write(true)
            .create_new(true)
            .open(&path)
            .await;
        match file {
            Ok(mut f) => {
                f.write_all(content.as_bytes()).await.err_notes(&path)?;
                f.shutdown().await.err_notes(&path)?;
            },
            Err(e) if e.kind() == io::ErrorKind::AlreadyExists => {
                info!("File {} already exists, Skip", name.as_ref());
            },
            _ => bail!("Unexpected Error in saving metadata file {}", name.as_ref()),
        }
```

**File:** storage/README.md (L189-198)
```markdown
### Continuously backing up to a cloud storage

The backup coordinator runs continuously, talks to the backup service embedded
inside a Aptos Node and writes backup data automatically to a configured cloud
storage.

One can make a config file for a specific cloud storage position by updating
one of the examples here
https://github.com/aptos-labs/aptos-core/tree/main/storage/backup/backup-cli/src/storage/command_adapter/sample_configs/

```
