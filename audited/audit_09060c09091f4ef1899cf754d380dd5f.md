# Audit Report

## Title
Metadata Synchronization Race Condition Causes File Store Processor Crash and Data Gaps

## Summary
The indexer-grpc file store processor updates Redis metadata before committing GCS metadata, creating a non-atomic two-phase update. If the system crashes or GCS updates are delayed between these operations, the cache worker may evict transactions that the file store processor still needs to process, causing permanent data gaps and processor crashes.

## Finding Description

The file store processor violates atomic metadata consistency by performing two separate, non-atomic metadata updates: [1](#0-0) 

The processor first updates Redis `FILE_STORE_LATEST_VERSION` using a simple SET operation without atomicity guarantees: [2](#0-1) 

Then attempts to update GCS metadata with retries: [3](#0-2) 

**The vulnerability occurs when:**

1. File store processor uploads transaction batches (e.g., versions 0-49,999) to GCS successfully
2. Processor updates Redis `FILE_STORE_LATEST_VERSION` to 50,000 (fast, synchronous)
3. **System crashes or GCS experiences network delays before metadata commit completes**
4. On restart: Redis shows version 50,000, but GCS metadata still shows version 0
5. Cache worker reads Redis version 50,000 and believes file store is caught up: [4](#0-3) 

6. Cache worker continues advancing (e.g., to version 350,000+)
7. Cache worker actively evicts versions 0-49,999 (because 350,000 - 300,000 > 0): [5](#0-4) 

8. File store processor restarts and reads GCS metadata = version 0
9. Processor attempts to fetch versions 0-999 from cache
10. **Cache returns error because transactions were evicted**: [6](#0-5) 

11. **File store processor panics** because `get_transactions` is unwrapped: [7](#0-6) [8](#0-7) 

## Impact Explanation

**HIGH Severity** per Aptos bug bounty criteria:
- **API crashes**: The file store processor panics and becomes unavailable, requiring manual intervention
- **Significant protocol violations**: Metadata consistency invariant is violated - Redis and GCS metadata drift out of sync

**MEDIUM Severity** aspects:
- **State inconsistencies requiring intervention**: Permanent gap in file store requires manual backfill from fullnode to recover
- Data integrity compromise: File store becomes incomplete and unreliable for downstream consumers

The vulnerability breaks the **State Consistency** invariant: metadata state transitions must be atomic and consistent across storage layers. The non-atomic update creates a window where distributed state (Redis vs GCS) becomes inconsistent, violating the fundamental assumption that `FILE_STORE_LATEST_VERSION` accurately reflects committed file store progress.

## Likelihood Explanation

**MEDIUM to HIGH Likelihood:**

1. **Common trigger conditions:**
   - System crashes during processing (OOM, deployment, infrastructure failures)
   - GCS network latency or temporary unavailability
   - Rate limiting by GCS (note the 200ms minimum interval between updates)

2. **Extended vulnerability window:**
   - GCS metadata updates can retry multiple times with 500ms delays
   - Each retry extends the window where crash can cause inconsistency
   - The rate limit check can cause initial failures, extending the window further

3. **No recovery mechanism:**
   - The processor has no fallback to fetch from fullnode when cache misses occur
   - Manual intervention is required to backfill missing data
   - Production deployments are likely to experience this periodically

4. **Environmental factors:**
   - High transaction volume increases processing frequency
   - Cloud infrastructure instability amplifies the risk
   - GCS regional outages or throttling make this highly likely

## Recommendation

**Immediate Fix: Atomic Metadata Updates**

1. **Update Redis AFTER GCS metadata commits successfully**, not before:

```rust
// In processor.rs, reverse the order:
while self
    .file_store_operator
    .update_file_store_metadata_with_timeout(chain_id, batch_start_version)
    .await
    .is_err()
{
    tracing::error!(...);
    std::thread::sleep(std::time::Duration::from_millis(500));
    METADATA_UPLOAD_FAILURE_COUNT.inc();
}

// Only update Redis after GCS succeeds
self.cache_operator
    .update_file_store_latest_version(batch_start_version)
    .await?;
```

2. **Add atomic check-and-set for Redis updates**, similar to `update_cache_latest_version`:

```rust
pub async fn update_file_store_latest_version_atomic(
    &mut self,
    expected_current: u64,
    new_version: u64,
) -> anyhow::Result<()> {
    let script = redis::Script::new(r#"
        local current = redis.call("GET", KEYS[1])
        if current and tonumber(current) ~= tonumber(ARGV[1]) then
            return 0
        end
        redis.call("SET", KEYS[1], ARGV[2])
        return 1
    "#);
    
    let result: i32 = script
        .key(FILE_STORE_LATEST_VERSION)
        .arg(expected_current)
        .arg(new_version)
        .invoke_async(&mut self.conn)
        .await?;
        
    ensure!(result == 1, "File store version mismatch during update");
    Ok(())
}
```

3. **Add fallback fetch from fullnode** when cache misses occur in the processor, similar to the backfiller's approach.

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[tokio::test]
async fn test_metadata_race_causes_cache_eviction_and_crash() {
    // Setup: Initialize processor with mock Redis and GCS
    let mut processor = create_test_processor().await;
    
    // Step 1: Process first batch successfully
    process_batch_successfully(&mut processor, 0, 999).await;
    
    // Step 2: Simulate slow GCS by injecting delay
    let gcs_delay_injector = inject_gcs_delay(Duration::from_secs(10));
    
    // Step 3: Start processing next batch
    let process_handle = tokio::spawn(async move {
        processor.run().await
    });
    
    // Step 4: Wait for Redis update but before GCS commit
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Step 5: Verify Redis shows advanced version
    let redis_version = get_redis_file_store_version().await;
    assert_eq!(redis_version, 1000);
    
    // Step 6: Verify GCS metadata still shows old version
    let gcs_version = get_gcs_metadata_version().await;
    assert_eq!(gcs_version, 0);
    
    // Step 7: Simulate crash by dropping the process handle
    drop(process_handle);
    
    // Step 8: Simulate cache worker advancing and evicting
    advance_cache_worker_to_version(350_000).await;
    
    // Step 9: Restart processor - it reads GCS metadata = 0
    let mut processor_restarted = create_test_processor().await;
    
    // Step 10: Processor attempts to fetch version 0-999 from cache
    // This should panic because cache evicted these versions
    let result = processor_restarted.run().await;
    
    // Expected: Panic with "Failed to get all transactions from cache"
    assert!(result.is_err());
    assert!(result.unwrap_err().to_string().contains("Failed to get all transactions"));
}
```

## Notes

This vulnerability affects the indexer-grpc auxiliary infrastructure, not core consensus. However, it creates a **critical availability issue** and **data integrity problem** for the indexer service. The file store is relied upon by downstream consumers (data APIs, analytics platforms), making gaps in the file store a significant operational concern.

The root cause is violating atomicity in distributed state management - a classic distributed systems bug where multiple sources of truth (Redis and GCS) are updated non-atomically, allowing them to drift out of sync during failures.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L162-165)
```rust
                    let transactions = cache_operator_clone
                        .get_transactions(start_version, FILE_ENTRY_TRANSACTION_COUNT)
                        .await
                        .unwrap();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L243-243)
```rust
                    Err(err) => panic!("Error processing transaction batches: {:?}", err),
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L258-273)
```rust
            self.cache_operator
                .update_file_store_latest_version(batch_start_version)
                .await?;
            while self
                .file_store_operator
                .update_file_store_metadata_with_timeout(chain_id, batch_start_version)
                .await
                .is_err()
            {
                tracing::error!(
                    batch_start_version = batch_start_version,
                    "Failed to update file store metadata. Retrying."
                );
                std::thread::sleep(std::time::Duration::from_millis(500));
                METADATA_UPLOAD_FAILURE_COUNT.inc();
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L181-190)
```rust
    pub async fn update_file_store_latest_version(
        &mut self,
        latest_version: u64,
    ) -> anyhow::Result<()> {
        let _: () = self
            .conn
            .set(FILE_STORE_LATEST_VERSION, latest_version)
            .await?;
        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L282-289)
```rust
            if version >= CACHE_SIZE_EVICTION_LOWER_BOUND {
                let key = CacheEntry::build_key(
                    version - CACHE_SIZE_EVICTION_LOWER_BOUND,
                    self.storage_format,
                )
                .to_string();
                redis_pipeline.cmd("DEL").arg(key).ignore();
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L389-392)
```rust
        ensure!(
            transactions.len() == transaction_count as usize,
            "Failed to get all transactions from cache."
        );
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/gcs.rs (L162-182)
```rust
    async fn update_file_store_metadata_with_timeout(
        &mut self,
        expected_chain_id: u64,
        version: u64,
    ) -> anyhow::Result<()> {
        if let Some(metadata) = self.get_file_store_metadata().await {
            assert_eq!(metadata.chain_id, expected_chain_id, "Chain ID mismatch.");
            assert_eq!(
                metadata.storage_format, self.storage_format,
                "Storage format mismatch."
            );
        }
        if self.file_store_metadata_last_updated.elapsed().as_millis()
            < FILE_STORE_METADATA_TIMEOUT_MILLIS
        {
            bail!("File store metadata is updated too frequently.")
        }
        self.update_file_store_metadata_internal(expected_chain_id, version)
            .await?;
        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L479-499)
```rust
        loop {
            let file_store_version = cache_operator
                .get_file_store_latest_version()
                .await?
                .unwrap();
            if file_store_version + FILE_STORE_VERSIONS_RESERVED < current_version {
                tokio::time::sleep(std::time::Duration::from_millis(
                    CACHE_WORKER_WAIT_FOR_FILE_STORE_MS,
                ))
                .await;
                tracing::warn!(
                    current_version = current_version,
                    file_store_version = file_store_version,
                    "[Indexer Cache] File store version is behind current version too much."
                );
                WAIT_FOR_FILE_STORE_COUNTER.inc();
            } else {
                // File store is up to date, continue cache update.
                break;
            }
        }
```
