# Audit Report

## Title
Liveness Violation During Epoch Transition: Event Loop Blocking Prevents Timeout Processing and Consensus Participation

## Summary
During epoch transitions, the consensus `EpochManager` event loop becomes completely blocked while waiting for reconfiguration notifications, preventing all timeout processing, consensus message handling, and network participation. This creates a critical liveness gap where validators cannot fulfill their consensus duties.

## Finding Description

The vulnerability exists in the epoch transition flow within `EpochManager`. When a valid `EpochChangeProof` is received and processed, the following sequence occurs: [1](#0-0) 

The `initiate_new_epoch()` function is called and awaited in the main event loop, which blocks all event processing: [2](#0-1) 

During this blocking period:
1. The old `RoundManager` is shut down, setting `round_manager_tx` to `None`
2. The node syncs to the new epoch's ledger state
3. The node waits indefinitely for a reconfiguration notification via `await_reconfig_notification().await` [3](#0-2) 

While blocked, the main event loop cannot process any events: [4](#0-3) 

Most critically, local timeout events cannot be processed: [5](#0-4) 

The timeout processing requires an active `round_manager_tx`, which is `None` during the transition, causing timeouts to be silently discarded with only a warning.

Meanwhile, the old `RoundState` from the previous epoch still has active timeout tasks scheduled via the time service: [6](#0-5) 

These timeouts will fire and send messages to the `timeout_sender` channel, but the messages accumulate unprocessed because the event loop is blocked.

## Impact Explanation

This constitutes a **High Severity** vulnerability under the Aptos bug bounty program as it causes "Significant protocol violations" specifically:

1. **Liveness Guarantee Violation**: The AptosBFT consensus protocol assumes validators can always eventually make progress. During epoch transitions, affected validators cannot:
   - Process or respond to timeouts
   - Vote on proposals
   - Send timeout certificates
   - Participate in leader election
   - Handle sync info messages

2. **Network-Wide Impact**: If multiple validators experience delayed reconfiguration notifications simultaneously (e.g., due to state sync delays, network congestion, or resource constraints), the network's ability to reach quorum is compromised.

3. **Unbounded Blocking**: There is no timeout mechanism for `await_reconfig_notification()`. If the reconfiguration notification is delayed due to state sync issues, the validator remains frozen indefinitely. [7](#0-6) 

The reconfiguration channel size is 1, but if state sync fails to deliver the notification promptly, the validator cannot recover.

## Likelihood Explanation

**Likelihood: High**

This vulnerability triggers on every epoch transition, which occurs:
- During validator set updates
- During on-chain configuration changes
- During scheduled epoch boundaries

The severity depends on:
1. **State sync latency**: If `sync_to_target()` is slow (large state, network issues, disk I/O), the blocking period extends
2. **Notification delivery**: The reconfiguration notification depends on state sync completing and sending the notification
3. **Concurrent transitions**: If many validators transition simultaneously, network liveness degrades proportionally

The blocking is guaranteed to occur on every epoch change, making this a deterministic protocol-level issue rather than an edge case.

## Recommendation

Refactor the epoch transition to avoid blocking the main event loop:

1. **Spawn async task for epoch transition**: Move `initiate_new_epoch()` into a separate async task so the main event loop continues processing events.

2. **Implement graceful transition state**: Add a transition state where the node:
   - Continues processing timeouts with adjusted behavior
   - Buffers but doesn't process consensus messages for the new epoch
   - Responds to sync requests
   - Monitors transition progress

3. **Add timeout for reconfig notification**: Implement a deadline for waiting on the reconfiguration notification with fallback recovery:

```rust
async fn await_reconfig_notification(&mut self) {
    let timeout_duration = Duration::from_secs(30);
    match tokio::time::timeout(timeout_duration, self.reconfig_events.next()).await {
        Ok(Some(notification)) => {
            self.start_new_epoch(notification.on_chain_configs).await;
        },
        Ok(None) => {
            error!("Reconfig sender dropped");
            // Implement recovery logic
        },
        Err(_) => {
            error!("Timeout waiting for reconfig notification");
            // Implement fallback: maybe request state sync again
        }
    }
}
```

4. **Maintain minimal liveness during transition**: Keep a minimal event processing capability during transitions to handle critical messages like sync info.

## Proof of Concept

This vulnerability can be demonstrated by instrumenting the code to measure the blocking duration during epoch transitions:

```rust
// Add to epoch_manager.rs in initiate_new_epoch()
let start_time = Instant::now();
info!("Starting epoch transition, event loop will block");

// ... existing code ...

self.await_reconfig_notification().await;
let elapsed = start_time.elapsed();
warn!("Event loop was blocked for {:?} during epoch transition", elapsed);
```

During testing with delayed state sync or network issues, this will show blocking times of several seconds to minutes, during which:
- No timeout events are processed (can be verified by logging in `process_local_timeout`)
- No consensus messages are handled (can be verified by logging in `process_message`)
- The validator is effectively offline

A more comprehensive test would:
1. Set up a local testnet
2. Trigger an epoch change
3. Inject delays in state sync via the fail point at line 662 of `execution_client.rs`
4. Monitor timeout processing and consensus participation
5. Observe that the validator cannot participate in consensus during the entire blocking period

**Notes:**
- This issue affects all validators transitioning epochs, not just malicious or faulty nodes
- The blocking behavior is deterministic and reproducible on every epoch change
- The vulnerability compounds when multiple validators experience delays simultaneously
- Current implementation has no recovery mechanism if reconfiguration notification is lost or delayed beyond reasonable bounds

### Citations

**File:** consensus/src/epoch_manager.rs (L544-569)
```rust
    async fn initiate_new_epoch(&mut self, proof: EpochChangeProof) -> anyhow::Result<()> {
        let ledger_info = proof
            .verify(self.epoch_state())
            .context("[EpochManager] Invalid EpochChangeProof")?;
        info!(
            LogSchema::new(LogEvent::NewEpoch).epoch(ledger_info.ledger_info().next_block_epoch()),
            "Received verified epoch change",
        );

        // shutdown existing processor first to avoid race condition with state sync.
        self.shutdown_current_processor().await;
        *self.pending_blocks.lock() = PendingBlocks::new();
        // make sure storage is on this ledger_info too, it should be no-op if it's already committed
        // panic if this doesn't succeed since the current processors are already shutdown.
        self.execution_client
            .sync_to_target(ledger_info.clone())
            .await
            .context(format!(
                "[EpochManager] State sync to new epoch {}",
                ledger_info
            ))
            .expect("Failed to sync to new epoch");

        monitor!("reconfig", self.await_reconfig_notification().await);
        Ok(())
    }
```

**File:** consensus/src/epoch_manager.rs (L1655-1676)
```rust
            ConsensusMsg::EpochChangeProof(proof) => {
                let msg_epoch = proof.epoch()?;
                debug!(
                    LogSchema::new(LogEvent::ReceiveEpochChangeProof)
                        .remote_peer(peer_id)
                        .epoch(self.epoch()),
                    "Proof from epoch {}", msg_epoch,
                );
                if msg_epoch == self.epoch() {
                    monitor!("process_epoch_proof", self.initiate_new_epoch(*proof).await)?;
                } else {
                    info!(
                        remote_peer = peer_id,
                        "[EpochManager] Unexpected epoch proof from epoch {}, local epoch {}",
                        msg_epoch,
                        self.epoch()
                    );
                    counters::EPOCH_MANAGER_ISSUES_DETAILS
                        .with_label_values(&["epoch_proof_wrong_epoch"])
                        .inc();
                }
            },
```

**File:** consensus/src/epoch_manager.rs (L1896-1910)
```rust
    fn process_local_timeout(&mut self, round: u64) {
        let Some(sender) = self.round_manager_tx.as_mut() else {
            warn!(
                "Received local timeout for round {} without Round Manager",
                round
            );
            return;
        };

        let peer_id = self.author;
        let event = VerifiedEvent::LocalTimeout(round);
        if let Err(e) = sender.push((peer_id, discriminant(&event)), (peer_id, event)) {
            error!("Failed to send event to round manager {:?}", e);
        }
    }
```

**File:** consensus/src/epoch_manager.rs (L1912-1920)
```rust
    async fn await_reconfig_notification(&mut self) {
        let reconfig_notification = self
            .reconfig_events
            .next()
            .await
            .expect("Reconfig sender dropped, unable to start new epoch");
        self.start_new_epoch(reconfig_notification.on_chain_configs)
            .await;
    }
```

**File:** consensus/src/epoch_manager.rs (L1922-1960)
```rust
    pub async fn start(
        mut self,
        mut round_timeout_sender_rx: aptos_channels::Receiver<Round>,
        mut network_receivers: NetworkReceivers,
    ) {
        // initial start of the processor
        self.await_reconfig_notification().await;
        loop {
            tokio::select! {
                (peer, msg) = network_receivers.consensus_messages.select_next_some() => {
                    monitor!("epoch_manager_process_consensus_messages",
                    if let Err(e) = self.process_message(peer, msg).await {
                        error!(epoch = self.epoch(), error = ?e, kind = error_kind(&e));
                    });
                },
                (peer, msg) = network_receivers.quorum_store_messages.select_next_some() => {
                    monitor!("epoch_manager_process_quorum_store_messages",
                    if let Err(e) = self.process_message(peer, msg).await {
                        error!(epoch = self.epoch(), error = ?e, kind = error_kind(&e));
                    });
                },
                (peer, request) = network_receivers.rpc_rx.select_next_some() => {
                    monitor!("epoch_manager_process_rpc",
                    if let Err(e) = self.process_rpc_request(peer, request) {
                        error!(epoch = self.epoch(), error = ?e, kind = error_kind(&e));
                    });
                },
                round = round_timeout_sender_rx.select_next_some() => {
                    monitor!("epoch_manager_process_round_timeout",
                    self.process_local_timeout(round));
                },
            }
            // Continually capture the time of consensus process to ensure that clock skew between
            // validators is reasonable and to find any unusual (possibly byzantine) clock behavior.
            counters::OP_COUNTERS
                .gauge("time_since_epoch_ms")
                .set(duration_since_epoch().as_millis() as i64);
        }
    }
```

**File:** consensus/src/liveness/round_state.rs (L339-354)
```rust
    fn setup_timeout(&mut self, multiplier: u32) -> Duration {
        let timeout_sender = self.timeout_sender.clone();
        let timeout = self.setup_deadline(multiplier);
        trace!(
            "Scheduling timeout of {} ms for round {}",
            timeout.as_millis(),
            self.current_round
        );
        let abort_handle = self
            .time_service
            .run_after(timeout, SendTask::make(timeout_sender, self.current_round));
        if let Some(handle) = self.abort_handle.replace(abort_handle) {
            handle.abort();
        }
        timeout
    }
```

**File:** state-sync/inter-component/event-notifications/src/lib.rs (L40-40)
```rust
const RECONFIG_NOTIFICATION_CHANNEL_SIZE: usize = 1; // Note: this should be 1 to ensure only the latest reconfig is consumed
```
