# Audit Report

## Title
Incomplete Checkpoint Creation Leads to Silent State Corruption in AptosDB

## Summary
The checkpoint creation function in `db_debugger/checkpoint/mod.rs` does not properly handle disk space exhaustion or other I/O errors during the multi-phase checkpoint process. When checkpoint creation fails partway through, the partial checkpoint directory remains on disk without any completion marker or rollback. If this incomplete checkpoint is later used to initialize a database, it silently creates an inconsistent state where different sub-databases are at incompatible versions, violating state consistency invariants and potentially causing consensus divergence.

## Finding Description

The `run()` function in the checkpoint debugger creates AptosDB checkpoints by sequentially checkpointing multiple sub-databases. [1](#0-0) 

This function calls `AptosDB::create_checkpoint()`, which performs checkpoint operations in sequence without atomic guarantees. [2](#0-1) 

The checkpoint process consists of multiple sequential operations:
1. Creates LedgerDb checkpoint (metadata + 7 sub-databases if sharding enabled)
2. Creates StateKvDb checkpoint (metadata + 16 shards if sharding enabled)  
3. Creates hot StateMerkleDb checkpoint (metadata + 16 shards if sharding enabled)
4. Creates cold StateMerkleDb checkpoint (metadata + 16 shards if sharding enabled)

Each sub-operation creates its own directories and database files using RocksDB's checkpoint mechanism. [3](#0-2) 

**The Vulnerability:**

If disk space is exhausted (or any I/O error occurs) during checkpoint creation, the error propagates via the `?` operator. [4](#0-3) 

However, there is **no cleanup mechanism** to remove the partially created checkpoint. The `Cmd::run()` function only checks that the output directory doesn't exist before starting, but doesn't handle cleanup on failure. [1](#0-0) 

**Silent Corruption on Later Use:**

When someone later tries to open a database from an incomplete checkpoint directory, RocksDB's `open_cf_descriptors()` will create NEW EMPTY databases for any missing sub-databases in ReadWrite mode. [5](#0-4) 

This results in a critically inconsistent state where:
- LedgerDb: Contains data at checkpoint version V
- StateKvDb: Empty (newly created) 
- StateMerkleDb: Empty (newly created)

The database opens successfully with **no error**, but subsequent operations will encounter severe state inconsistencies. There is no cross-database consistency validation during `AptosDB::open_internal()`. [6](#0-5) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under Aptos bug bounty criteria for the following reasons:

1. **State Inconsistencies Requiring Intervention**: An incomplete checkpoint creates state corruption that cannot be automatically recovered. The node will operate with fundamentally inconsistent data across its storage components.

2. **Validator Node Malfunction**: If a validator node is initialized from an incomplete checkpoint, it will:
   - Produce incorrect state roots (different from other validators)
   - Fail to execute transactions correctly
   - Be unable to serve valid state queries
   - Potentially cause consensus divergence if participating in validation

3. **Significant Protocol Violations**: This breaks the fundamental **State Consistency** invariant - that state transitions must be atomic and verifiable. Different storage components being at incompatible versions violates deterministic execution guarantees.

4. **Silent Failure**: The most dangerous aspect is that the database opens successfully without any error or warning. Operators have no indication that they're running a corrupted node until consensus or execution failures occur.

## Likelihood Explanation

This vulnerability has **Medium-High Likelihood**:

**Triggering Conditions:**
- Disk space exhaustion during checkpoint creation (common operational scenario)
- I/O errors during checkpoint writes
- Process interruption during checkpoint creation
- Storage device failures

**Usage Scenario:**
- Checkpoints are commonly used for:
  - Backup/restore operations
  - Node initialization
  - Testing and benchmarking
  - Database migration

The checkpoint tool is part of the official `aptos-db-tool` and operators are likely to use it for production backup workflows. If a checkpoint fails due to disk space (a realistic operational issue), the partial checkpoint remains and could be unknowingly used months later for node restoration.

## Recommendation

Implement atomic checkpoint creation with proper cleanup and validation:

```rust
// In storage/aptosdb/src/db_debugger/checkpoint/mod.rs
impl Cmd {
    pub fn run(self) -> Result<()> {
        ensure!(!self.output_dir.exists(), "Output dir already exists.");
        fs::create_dir_all(&self.output_dir)?;
        
        let sharding_config = self.db_dir.sharding_config.clone();
        
        // Wrap in a guard that cleans up on failure
        let result = AptosDB::create_checkpoint(
            self.db_dir,
            &self.output_dir,
            sharding_config.enable_storage_sharding,
        );
        
        match result {
            Ok(_) => {
                // Write completion marker
                fs::write(self.output_dir.join(".checkpoint_complete"), "")?;
                Ok(())
            }
            Err(e) => {
                // Clean up partial checkpoint on failure
                warn!("Checkpoint creation failed, cleaning up: {}", e);
                fs::remove_dir_all(&self.output_dir).unwrap_or(());
                Err(e)
            }
        }
    }
}

// In storage/aptosdb/src/db/aptosdb_internal.rs
// Add validation during open_internal:
impl AptosDB {
    pub(super) fn open_internal(...) -> Result<Self> {
        // ... existing code ...
        
        // Validate checkpoint completion if opening from checkpoint
        let checkpoint_marker = db_paths.default_root_path().join(".checkpoint_complete");
        if checkpoint_marker.exists() {
            // This is a checkpoint directory, validate it
            ensure!(
                ledger_db.metadata_db().get_synced_version()?.is_some(),
                "Invalid checkpoint: LedgerDb has no data"
            );
        }
        
        // Validate cross-database consistency
        let ledger_version = ledger_db.metadata_db().get_synced_version()?;
        if rocksdb_configs.enable_storage_sharding && ledger_version.is_some() {
            let state_kv_progress = state_kv_db.metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)?;
            ensure!(
                state_kv_progress.is_some(),
                "Inconsistent checkpoint: StateKvDb is empty but LedgerDb has data"
            );
        }
        
        // ... rest of existing code ...
    }
}
```

## Proof of Concept

```rust
// Reproduction steps demonstrating the vulnerability:

use tempfile::TempDir;
use std::fs;
use aptos_db::AptosDB;

#[test]
fn test_incomplete_checkpoint_corruption() {
    // 1. Create a source database with data
    let source_dir = TempDir::new().unwrap();
    let db = AptosDB::new_for_test(&source_dir);
    // ... populate with test data at version 100 ...
    db.save_transactions(/* test txns */);
    
    // 2. Create checkpoint directory  
    let checkpoint_dir = TempDir::new().unwrap();
    fs::create_dir_all(&checkpoint_dir).unwrap();
    
    // 3. Simulate partial checkpoint by manually creating only LedgerDb
    let ledger_checkpoint = checkpoint_dir.path().join("ledger_db");
    AptosDB::create_checkpoint(
        source_dir.path().join("ledger_db"),
        &ledger_checkpoint,
        true
    ).unwrap();
    
    // 4. Intentionally skip StateKvDb and StateMerkleDb checkpoints
    // (simulating disk space exhaustion after LedgerDb succeeds)
    
    // 5. Try to open database from incomplete checkpoint
    let result = AptosDB::open(
        StorageDirPaths::from_path(&checkpoint_dir),
        false,
        PrunerConfig::default(),
        RocksdbConfigs::default(),
        false,
        0,
        0,
        None,
        HotStateConfig::default(),
    );
    
    // 6. Database opens successfully (BUG - should fail!)
    assert!(result.is_ok());
    let corrupt_db = result.unwrap();
    
    // 7. Verify inconsistent state
    let ledger_version = corrupt_db.get_synced_version().unwrap();
    assert!(ledger_version.is_some()); // LedgerDb has data
    
    // StateKvDb will be empty (newly created)
    // Any state query will fail or return incorrect data
    let state_result = corrupt_db.get_state_value_by_version(
        &StateKey::raw(b"test"),
        ledger_version.unwrap()
    );
    
    // This demonstrates the corruption - state is missing
    assert!(state_result.is_err() || state_result.unwrap().is_none());
}
```

**Notes:**

This vulnerability represents a critical flaw in checkpoint atomicity. The lack of rollback on failure and completion validation on restore creates a time-delayed corruption scenario where operational errors during backup can manifest as silent state corruption during restore operations. This violates the fundamental state consistency guarantees required for blockchain correctness and could lead to consensus divergence if corrupt nodes participate in validation.

### Citations

**File:** storage/aptosdb/src/db_debugger/checkpoint/mod.rs (L20-29)
```rust
    pub fn run(self) -> Result<()> {
        ensure!(!self.output_dir.exists(), "Output dir already exists.");
        fs::create_dir_all(&self.output_dir)?;
        let sharding_config = self.db_dir.sharding_config.clone();
        AptosDB::create_checkpoint(
            self.db_dir,
            self.output_dir,
            sharding_config.enable_storage_sharding,
        )
    }
```

**File:** storage/aptosdb/src/db/mod.rs (L172-205)
```rust
    pub fn create_checkpoint(
        db_path: impl AsRef<Path>,
        cp_path: impl AsRef<Path>,
        sharding: bool,
    ) -> Result<()> {
        let start = Instant::now();

        info!(sharding = sharding, "Creating checkpoint for AptosDB.");

        LedgerDb::create_checkpoint(db_path.as_ref(), cp_path.as_ref(), sharding)?;
        if sharding {
            StateKvDb::create_checkpoint(db_path.as_ref(), cp_path.as_ref())?;
            StateMerkleDb::create_checkpoint(
                db_path.as_ref(),
                cp_path.as_ref(),
                sharding,
                /* is_hot = */ true,
            )?;
        }
        StateMerkleDb::create_checkpoint(
            db_path.as_ref(),
            cp_path.as_ref(),
            sharding,
            /* is_hot = */ false,
        )?;

        info!(
            db_path = db_path.as_ref(),
            cp_path = cp_path.as_ref(),
            time_ms = %start.elapsed().as_millis(),
            "Made AptosDB checkpoint."
        );
        Ok(())
    }
```

**File:** storage/schemadb/src/lib.rs (L141-192)
```rust
    fn open_cf_impl(
        db_opts: &Options,
        path: impl AsRef<Path>,
        name: &str,
        cfds: Vec<ColumnFamilyDescriptor>,
        open_mode: OpenMode,
    ) -> DbResult<DB> {
        // ignore error, since it'll fail to list cfs on the first open
        let existing_cfs: HashSet<String> = rocksdb::DB::list_cf(db_opts, path.de_unc())
            .unwrap_or_default()
            .into_iter()
            .collect();
        let requested_cfs: HashSet<String> =
            cfds.iter().map(|cfd| cfd.name().to_string()).collect();
        let missing_cfs: HashSet<&str> = requested_cfs
            .difference(&existing_cfs)
            .map(|cf| {
                warn!("Missing CF: {}", cf);
                cf.as_ref()
            })
            .collect();
        let unrecognized_cfs = existing_cfs.difference(&requested_cfs);

        let all_cfds = cfds
            .into_iter()
            .chain(unrecognized_cfs.map(Self::cfd_for_unrecognized_cf));

        let inner = {
            use rocksdb::DB;
            use OpenMode::*;

            match open_mode {
                ReadWrite => DB::open_cf_descriptors(db_opts, path.de_unc(), all_cfds),
                ReadOnly => {
                    DB::open_cf_descriptors_read_only(
                        db_opts,
                        path.de_unc(),
                        all_cfds.filter(|cfd| !missing_cfs.contains(cfd.name())),
                        false, /* error_if_log_file_exist */
                    )
                },
                Secondary(secondary_path) => DB::open_cf_descriptors_as_secondary(
                    db_opts,
                    path.de_unc(),
                    secondary_path,
                    all_cfds,
                ),
            }
        }
        .into_db_res()?;

        Ok(Self::log_construct(name, open_mode, inner))
```

**File:** storage/schemadb/src/lib.rs (L355-362)
```rust
    /// Creates new physical DB checkpoint in directory specified by `path`.
    pub fn create_checkpoint<P: AsRef<Path>>(&self, path: P) -> DbResult<()> {
        rocksdb::checkpoint::Checkpoint::new(&self.inner)
            .into_db_res()?
            .create_checkpoint(path)
            .into_db_res()?;
        Ok(())
    }
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L311-370)
```rust
    pub(crate) fn create_checkpoint(
        db_root_path: impl AsRef<Path>,
        cp_root_path: impl AsRef<Path>,
        sharding: bool,
    ) -> Result<()> {
        let rocksdb_configs = RocksdbConfigs {
            enable_storage_sharding: sharding,
            ..Default::default()
        };
        let env = None;
        let block_cache = None;
        let ledger_db = Self::new(
            db_root_path,
            rocksdb_configs,
            env,
            block_cache,
            /*readonly=*/ false,
        )?;
        let cp_ledger_db_folder = cp_root_path.as_ref().join(LEDGER_DB_FOLDER_NAME);

        info!(
            sharding = sharding,
            "Creating ledger_db checkpoint at: {cp_ledger_db_folder:?}"
        );

        std::fs::remove_dir_all(&cp_ledger_db_folder).unwrap_or(());
        if sharding {
            std::fs::create_dir_all(&cp_ledger_db_folder).unwrap_or(());
        }

        ledger_db
            .metadata_db()
            .create_checkpoint(Self::metadata_db_path(cp_root_path.as_ref(), sharding))?;

        if sharding {
            ledger_db
                .event_db()
                .create_checkpoint(cp_ledger_db_folder.join(EVENT_DB_NAME))?;
            ledger_db
                .persisted_auxiliary_info_db()
                .create_checkpoint(cp_ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME))?;
            ledger_db
                .transaction_accumulator_db()
                .create_checkpoint(cp_ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME))?;
            ledger_db
                .transaction_auxiliary_data_db()
                .create_checkpoint(cp_ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME))?;
            ledger_db
                .transaction_db()
                .create_checkpoint(cp_ledger_db_folder.join(TRANSACTION_DB_NAME))?;
            ledger_db
                .transaction_info_db()
                .create_checkpoint(cp_ledger_db_folder.join(TRANSACTION_INFO_DB_NAME))?;
            ledger_db
                .write_set_db()
                .create_checkpoint(cp_ledger_db_folder.join(WRITE_SET_DB_NAME))?;
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L112-192)
```rust
    pub(super) fn open_internal(
        db_paths: &StorageDirPaths,
        readonly: bool,
        pruner_config: PrunerConfig,
        rocksdb_configs: RocksdbConfigs,
        enable_indexer: bool,
        buffered_state_target_items: usize,
        max_num_nodes_per_lru_cache_shard: usize,
        empty_buffered_state_for_restore: bool,
        internal_indexer_db: Option<InternalIndexerDB>,
        hot_state_config: HotStateConfig,
    ) -> Result<Self> {
        ensure!(
            pruner_config.eq(&NO_OP_STORAGE_PRUNER_CONFIG) || !readonly,
            "Do not set prune_window when opening readonly.",
        );

        let mut env =
            Env::new().map_err(|err| AptosDbError::OtherRocksDbError(err.into_string()))?;
        env.set_high_priority_background_threads(rocksdb_configs.high_priority_background_threads);
        env.set_low_priority_background_threads(rocksdb_configs.low_priority_background_threads);
        let block_cache = Cache::new_hyper_clock_cache(
            rocksdb_configs.shared_block_cache_size,
            /* estimated_entry_charge = */ 0,
        );

        let (ledger_db, hot_state_merkle_db, state_merkle_db, state_kv_db) = Self::open_dbs(
            db_paths,
            rocksdb_configs,
            Some(&env),
            Some(&block_cache),
            readonly,
            max_num_nodes_per_lru_cache_shard,
            hot_state_config.delete_on_restart,
        )?;

        let mut myself = Self::new_with_dbs(
            ledger_db,
            hot_state_merkle_db,
            state_merkle_db,
            state_kv_db,
            pruner_config,
            buffered_state_target_items,
            readonly,
            empty_buffered_state_for_restore,
            rocksdb_configs.enable_storage_sharding,
            internal_indexer_db,
            hot_state_config,
        );

        if !readonly {
            if let Some(version) = myself.get_synced_version()? {
                myself
                    .ledger_pruner
                    .maybe_set_pruner_target_db_version(version);
                myself
                    .state_store
                    .state_kv_pruner
                    .maybe_set_pruner_target_db_version(version);
            }
            if let Some(version) = myself.get_latest_state_checkpoint_version()? {
                myself
                    .state_store
                    .state_merkle_pruner
                    .maybe_set_pruner_target_db_version(version);
                myself
                    .state_store
                    .epoch_snapshot_pruner
                    .maybe_set_pruner_target_db_version(version);
            }
        }

        if !readonly && enable_indexer {
            myself.open_indexer(
                db_paths.default_root_path(),
                rocksdb_configs.index_db_config,
            )?;
        }

        Ok(myself)
    }
```
