# Audit Report

## Title
Epoch Boundary Attack: Stale Snapshot Target Enables State Manipulation After Epoch Transitions

## Summary
When a node restarts during state snapshot synchronization after an epoch change, it resumes syncing to a stale target from a previous epoch without validating the target against the current epoch's validator set. This allows attackers with compromised old validator keys to provide manipulated state that the node will accept as valid.

## Finding Description

The vulnerability exists in the state snapshot synchronization recovery mechanism. When a node performs fast sync (downloading state snapshots), it persists its progress including the target ledger info in `StateSnapshotProgress`. [1](#0-0) 

The target ledger info contains signatures from validators in a specific epoch. [2](#0-1) 

When the node restarts after an epoch change, the bootstrapper retrieves the previous snapshot sync target without any epoch validation. [3](#0-2) 

The bootstrapper then resumes syncing to this stale target if the sync is incomplete. [4](#0-3) 

The critical flaw is that `fetch_missing_state_values` accepts and uses the stale target without validating it against the current epoch state. [5](#0-4) 

The storage synchronizer initializes the state snapshot receiver with this unvalidated target. [6](#0-5) 

During finalization, the system saves the stale target alongside current epoch change proofs without verifying consistency between them. [7](#0-6) 

**Attack Scenario:**

1. Node starts fast-syncing to target at version 1000 in epoch 10 (50% complete)
2. Network transitions to epoch 11 with different validators
3. Node restarts and loads epoch 11 as current epoch state
4. Node retrieves stale epoch 10 target from metadata storage
5. Node resumes syncing WITHOUT validating target against epoch 11
6. Malicious actors with compromised epoch 10 validator keys provide manipulated state values matching the epoch 10 target's root hash
7. Node accepts the invalid state because it validates against the stale epoch 10 target
8. Node finalizes with inconsistent state: current epoch 11 proofs + stale epoch 10 target + manipulated state

The test suite even demonstrates this behavior without treating it as a security issue. [8](#0-7) 

## Impact Explanation

This is **Critical Severity** under the Aptos bug bounty program because it enables:

1. **Consensus/Safety Violations**: Different nodes restarting at different times could sync to different states if targeted with different malicious data, causing consensus splits.

2. **State Manipulation**: Attackers with old validator keys can inject invalid state into restarting nodes during epoch boundaries.

3. **Non-recoverable Network Partition**: If multiple nodes sync to inconsistent states, it could require manual intervention or a hardfork to resolve.

The vulnerability breaks the fundamental invariant that all nodes must agree on state at any given version. It allows state that was valid under an old validator set (which may have been compromised) to be injected into nodes that should only trust the current validator set.

## Likelihood Explanation

This vulnerability is **highly likely** to occur because:

1. **Frequent Conditions**: Node restarts during state sync are common (maintenance, crashes, upgrades)
2. **Regular Epoch Changes**: Aptos epochs change regularly as part of normal operation
3. **Timing Window**: Any node restarting within the window between starting a sync in epoch N and completing it after epoch N+1 begins is vulnerable
4. **Attractive Target**: Initial state sync is when nodes are most vulnerable and have no established view of correct state
5. **Low Attacker Requirements**: Only requires old validator keys (which may have been compromised through various means) rather than current validator access

## Recommendation

Add epoch validation when resuming from a previous snapshot sync target. The target must be verified against the current epoch state before resuming:

**Proposed Fix in `bootstrapper.rs`:**

Add validation in `fetch_missing_state_snapshot_data` before resuming to a previous target:

```rust
if let Some(target) = self.metadata_storage.previous_snapshot_sync_target()? {
    // Validate that the target is not stale relative to current epoch
    let target_epoch = target.ledger_info().epoch();
    let current_epoch = self.verified_epoch_states.latest_epoch_state.epoch;
    
    if target_epoch < current_epoch {
        warn!(
            "Previous snapshot sync target is from stale epoch {}. Current epoch is {}. \
             Discarding stale target and starting fresh sync.",
            target_epoch, current_epoch
        );
        // Start a new sync to the highest known ledger info
        self.fetch_missing_state_values(highest_known_ledger_info, false).await
    } else if target_epoch == current_epoch {
        // Verify the target against current epoch state
        self.verified_epoch_states.latest_epoch_state
            .verify(&target)
            .map_err(|error| {
                Error::VerificationError(format!(
                    "Previous snapshot target failed verification against current epoch: {:?}",
                    error
                ))
            })?;
        
        if self.metadata_storage.is_snapshot_sync_complete(&target)? {
            // ... existing completion logic ...
        } else {
            // Continue syncing to verified target
            self.fetch_missing_state_values(target, true).await
        }
    } else {
        // Target is from a future epoch - this shouldn't happen
        return Err(Error::UnexpectedError(format!(
            "Previous snapshot target is from future epoch {}. Current epoch is {}.",
            target_epoch, current_epoch
        )));
    }
} else {
    // No previous sync - start fresh
    self.fetch_missing_state_values(highest_known_ledger_info, false).await
}
```

This ensures that stale targets from previous epochs are discarded and nodes always sync to targets that are valid under the current validator set.

## Proof of Concept

The following Rust integration test demonstrates the vulnerability:

```rust
#[tokio::test]
async fn test_epoch_boundary_stale_target_vulnerability() {
    // Setup: Node starts syncing in epoch 1
    let synced_version = GENESIS_TRANSACTION_VERSION;
    let target_version = 1000;
    let target_epoch = 1;
    let current_epoch = 2; // Epoch has advanced
    let last_persisted_index = 500; // Partial progress
    
    // Create target from old epoch 1 (signed by epoch 1 validators)
    let stale_target = create_random_epoch_ending_ledger_info(target_version, target_epoch);
    
    // Create current highest known info from epoch 2 (signed by epoch 2 validators)
    let current_highest = create_random_epoch_ending_ledger_info(5000, current_epoch);
    
    // Setup mock metadata storage that returns the stale target
    let mut metadata_storage = MockMetadataStorage::new();
    metadata_storage
        .expect_previous_snapshot_sync_target()
        .returning(move || Ok(Some(stale_target.clone())));
    metadata_storage
        .expect_is_snapshot_sync_complete()
        .returning(|_| Ok(false)); // Sync not complete
    metadata_storage
        .expect_get_last_persisted_state_value_index()
        .returning(move |_| Ok(last_persisted_index));
    
    // Setup streaming client to expect sync to stale target
    let mut mock_streaming_client = create_mock_streaming_client();
    mock_streaming_client
        .expect_get_all_state_values()
        .times(1)
        .with(eq(target_version), eq(Some(last_persisted_index)))
        .return_once(move |_, _| Ok(create_data_stream_listener().1));
    
    // Create bootstrapper with epoch 2 as current
    let mut bootstrapper = create_bootstrapper_with_storage(
        create_full_node_driver_configuration(),
        mock_streaming_client,
        metadata_storage,
        None,
        synced_version,
        true,
    );
    
    // Insert epoch 2 into verified states (simulating current epoch)
    manipulate_verified_epoch_states(&mut bootstrapper, true, true, Some(5000));
    
    // Manually set transaction output to sync
    bootstrapper
        .get_state_value_syncer()
        .set_transaction_output_to_sync(create_output_list_with_proof());
    
    // Create global data summary with current epoch 2
    let mut global_data_summary = create_global_summary(current_epoch);
    global_data_summary.advertised_data.synced_ledger_infos = vec![current_highest];
    
    // Drive progress - this SHOULD reject the stale target but DOESN'T
    // The node will resume syncing to the epoch 1 target despite being in epoch 2
    let result = drive_progress(&mut bootstrapper, &global_data_summary, false).await;
    
    // VULNERABILITY: This succeeds when it should fail with epoch mismatch error
    assert!(result.is_ok(), "Node resumed syncing to stale epoch 1 target in epoch 2!");
}
```

This test demonstrates that a node in epoch 2 will resume syncing to a target from epoch 1 without validation, enabling the attack described above.

**Notes**

The vulnerability is particularly dangerous because:
1. It only manifests during the specific timing window of epoch transitions combined with node restarts
2. The existing test suite validates the behavior but doesn't recognize it as a security issue
3. The impact scales with how many nodes restart during epoch boundaries
4. Old validator keys are often considered less sensitive than current ones, making them more likely to be compromised

### Citations

**File:** state-sync/state-sync-driver/src/metadata_storage.rs (L195-199)
```rust
    fn previous_snapshot_sync_target(&self) -> Result<Option<LedgerInfoWithSignatures>, Error> {
        Ok(self
            .get_snapshot_progress()?
            .map(|snapshot_progress| snapshot_progress.target_ledger_info))
    }
```

**File:** state-sync/state-sync-driver/src/metadata_storage.rs (L231-236)
```rust
#[derive(Debug, Deserialize, Eq, PartialEq, Serialize)]
pub struct StateSnapshotProgress {
    pub target_ledger_info: LedgerInfoWithSignatures,
    pub last_persisted_state_value_index: u64,
    pub snapshot_sync_completed: bool,
}
```

**File:** types/src/epoch_state.rs (L40-50)
```rust
impl Verifier for EpochState {
    fn verify(&self, ledger_info: &LedgerInfoWithSignatures) -> anyhow::Result<()> {
        ensure!(
            self.epoch == ledger_info.ledger_info().epoch(),
            "LedgerInfo has unexpected epoch {}, expected {}",
            ledger_info.ledger_info().epoch(),
            self.epoch
        );
        ledger_info.verify_signatures(&self.verifier)?;
        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L540-543)
```rust
                } else {
                    // Continue snapshot syncing to the target
                    self.fetch_missing_state_values(target, true).await
                }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L663-684)
```rust
    async fn fetch_missing_state_values(
        &mut self,
        target_ledger_info: LedgerInfoWithSignatures,
        existing_snapshot_progress: bool,
    ) -> Result<(), Error> {
        // Initialize the target ledger info and verify it never changes
        if let Some(ledger_info_to_sync) = &self.state_value_syncer.ledger_info_to_sync {
            if ledger_info_to_sync != &target_ledger_info {
                return Err(Error::UnexpectedError(format!(
                    "Mismatch in ledger info to sync! Given target: {:?}, stored target: {:?}",
                    target_ledger_info, ledger_info_to_sync
                )));
            }
        } else {
            info!(LogSchema::new(LogEntry::Bootstrapper).message(&format!(
                "Setting the target ledger info for fast sync! Target: {:?}",
                target_ledger_info
            )));

            self.state_value_syncer
                .set_ledger_info_to_sync(target_ledger_info.clone());
        }
```

**File:** state-sync/state-sync-driver/src/storage_synchronizer.rs (L378-406)
```rust
    fn initialize_state_synchronizer(
        &mut self,
        epoch_change_proofs: Vec<LedgerInfoWithSignatures>,
        target_ledger_info: LedgerInfoWithSignatures,
        target_output_with_proof: TransactionOutputListWithProofV2,
    ) -> Result<JoinHandle<()>, Error> {
        // Create a channel to notify the state snapshot receiver when data chunks are ready
        let max_pending_data_chunks = self.driver_config.max_pending_data_chunks as usize;
        let (state_snapshot_notifier, state_snapshot_listener) =
            mpsc::channel(max_pending_data_chunks);

        // Spawn the state snapshot receiver that commits state values
        let receiver_handle = spawn_state_snapshot_receiver(
            self.chunk_executor.clone(),
            state_snapshot_listener,
            self.commit_notification_sender.clone(),
            self.error_notification_sender.clone(),
            self.pending_data_chunks.clone(),
            self.metadata_storage.clone(),
            self.storage.clone(),
            epoch_change_proofs,
            target_ledger_info,
            target_output_with_proof,
            self.runtime.clone(),
        );
        self.state_snapshot_notifier = Some(state_snapshot_notifier);

        Ok(receiver_handle)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L125-206)
```rust
    fn finalize_state_snapshot(
        &self,
        version: Version,
        output_with_proof: TransactionOutputListWithProofV2,
        ledger_infos: &[LedgerInfoWithSignatures],
    ) -> Result<()> {
        let (output_with_proof, persisted_aux_info) = output_with_proof.into_parts();
        gauged_api("finalize_state_snapshot", || {
            // Ensure the output with proof only contains a single transaction output and info
            let num_transaction_outputs = output_with_proof.get_num_outputs();
            let num_transaction_infos = output_with_proof.proof.transaction_infos.len();
            ensure!(
                num_transaction_outputs == 1,
                "Number of transaction outputs should == 1, but got: {}",
                num_transaction_outputs
            );
            ensure!(
                num_transaction_infos == 1,
                "Number of transaction infos should == 1, but got: {}",
                num_transaction_infos
            );

            // TODO(joshlind): include confirm_or_save_frozen_subtrees in the change set
            // bundle below.

            // Update the merkle accumulator using the given proof
            let frozen_subtrees = output_with_proof
                .proof
                .ledger_info_to_transaction_infos_proof
                .left_siblings();
            restore_utils::confirm_or_save_frozen_subtrees(
                self.ledger_db.transaction_accumulator_db_raw(),
                version,
                frozen_subtrees,
                None,
            )?;

            // Create a single change set for all further write operations
            let mut ledger_db_batch = LedgerDbSchemaBatches::new();
            let mut sharded_kv_batch = self.state_kv_db.new_sharded_native_batches();
            let mut state_kv_metadata_batch = SchemaBatch::new();
            // Save the target transactions, outputs, infos and events
            let (transactions, outputs): (Vec<Transaction>, Vec<TransactionOutput>) =
                output_with_proof
                    .transactions_and_outputs
                    .into_iter()
                    .unzip();
            let events = outputs
                .clone()
                .into_iter()
                .map(|output| output.events().to_vec())
                .collect::<Vec<_>>();
            let wsets: Vec<WriteSet> = outputs
                .into_iter()
                .map(|output| output.write_set().clone())
                .collect();
            let transaction_infos = output_with_proof.proof.transaction_infos;
            // We should not save the key value since the value is already recovered for this version
            restore_utils::save_transactions(
                self.state_store.clone(),
                self.ledger_db.clone(),
                version,
                &transactions,
                &persisted_aux_info,
                &transaction_infos,
                &events,
                wsets,
                Some((
                    &mut ledger_db_batch,
                    &mut sharded_kv_batch,
                    &mut state_kv_metadata_batch,
                )),
                false,
            )?;

            // Save the epoch ending ledger infos
            restore_utils::save_ledger_infos(
                self.ledger_db.metadata_db(),
                ledger_infos,
                Some(&mut ledger_db_batch.ledger_metadata_db_batches),
            )?;

```

**File:** state-sync/state-sync-driver/src/tests/bootstrapper.rs (L957-1019)
```rust
async fn test_snapshot_sync_epoch_change() {
    // Create test data
    let synced_version = GENESIS_TRANSACTION_VERSION; // Genesis is the highest synced
    let target_version = 1000;
    let highest_version = 5000;
    let last_persisted_index = 1030405;
    let target_ledger_info = create_random_epoch_ending_ledger_info(target_version, 1);
    let highest_ledger_info = create_random_epoch_ending_ledger_info(highest_version, 2);

    // Create a driver configuration with a genesis waypoint and state syncing
    let mut driver_configuration = create_full_node_driver_configuration();
    driver_configuration.config.bootstrapping_mode = BootstrappingMode::DownloadLatestStates;

    // Create the mock streaming client
    let mut mock_streaming_client = create_mock_streaming_client();
    let (_notification_sender_1, data_stream_listener_1) = create_data_stream_listener();
    mock_streaming_client
        .expect_get_all_state_values()
        .times(1)
        .with(eq(target_version), eq(Some(last_persisted_index)))
        .return_once(move |_, _| Ok(data_stream_listener_1));

    // Create the mock metadata storage
    let mut metadata_storage = MockMetadataStorage::new();
    let target_ledger_info_clone = target_ledger_info.clone();
    let last_persisted_index_clone = last_persisted_index;
    metadata_storage
        .expect_previous_snapshot_sync_target()
        .returning(move || Ok(Some(target_ledger_info_clone.clone())));
    metadata_storage
        .expect_is_snapshot_sync_complete()
        .returning(|_| Ok(false));
    metadata_storage
        .expect_get_last_persisted_state_value_index()
        .returning(move |_| Ok(last_persisted_index_clone));

    // Create the bootstrapper
    let mut bootstrapper = create_bootstrapper_with_storage(
        driver_configuration,
        mock_streaming_client,
        metadata_storage,
        None,
        synced_version,
        true,
    );

    // Insert an epoch ending ledger info into the verified states of the bootstrapper
    manipulate_verified_epoch_states(&mut bootstrapper, true, true, Some(highest_version));

    // Manually insert a transaction output to sync
    bootstrapper
        .get_state_value_syncer()
        .set_transaction_output_to_sync(create_output_list_with_proof());

    // Create a global data summary
    let mut global_data_summary = create_global_summary(1);
    global_data_summary.advertised_data.synced_ledger_infos = vec![highest_ledger_info.clone()];

    // Drive progress to start the state value stream
    drive_progress(&mut bootstrapper, &global_data_summary, false)
        .await
        .unwrap();
}
```
