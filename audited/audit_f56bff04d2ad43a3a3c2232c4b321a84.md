# Audit Report

## Title
Backup Service Lacks Audit Logging for Sensitive Data Access Enabling Undetectable Insider Data Exfiltration

## Summary
The Aptos backup service exposes complete blockchain state data (transactions, account balances, smart contract states) via HTTP endpoints without any authentication, authorization, or audit logging. While designed as an internal service, production deployments bind to all network interfaces (0.0.0.0:6186), making it accessible within the Kubernetes cluster. The complete absence of audit trails means malicious insiders or actors with compromised cluster credentials can exfiltrate the entire blockchain state without leaving any forensic evidence.

## Finding Description

The backup service endpoints defined in [1](#0-0)  expose nine endpoints that provide access to sensitive blockchain data including complete transaction history, account states, and smart contract data.

These endpoints have no authentication mechanism [2](#0-1)  and only implement metrics collection via `warp::log::custom` [3](#0-2)  which logs endpoint names and latency but not client identity, IP addresses, specific parameters accessed, or timestamps suitable for audit purposes.

The exposed data includes:
- Complete transaction history via `get_transaction_iter` [4](#0-3)  returning transactions, events, write sets, and auxiliary information
- Complete state snapshots via `get_state_item_iter` [5](#0-4)  exposing all account states and smart contract data

In production deployments, the service is configured to listen on all network interfaces [6](#0-5)  making it accessible to any pod within the Kubernetes cluster via an internal service [7](#0-6) .

**Attack Scenario:**
1. Malicious insider with cluster access OR compromised service account credentials
2. Direct HTTP GET requests to `http://<fullnode-service>:6186/state_snapshot/<version>` or `/transactions/<start>/<count>`
3. Complete blockchain state exfiltration without any audit trail
4. No forensic evidence of who accessed what data when

## Impact Explanation

This constitutes a **Medium severity** information disclosure vulnerability per the Aptos bug bounty program. While it doesn't directly lead to loss of funds or consensus violations, it enables:

- **Privacy violation**: Exfiltration of all account balances, transaction history, and private smart contract states
- **Regulatory compliance failure**: Inability to detect or prove unauthorized data access violates data protection regulations
- **Insider threat enablement**: No deterrent or detection mechanism for malicious operators
- **Forensic investigation failure**: Post-breach analysis impossible without audit logs

The impact is elevated from "minor information leak" to Medium because:
- Complete blockchain state is exposed (not partial data)
- Zero forensic capability to detect or investigate breaches
- Enables systematic data exfiltration for sale on dark markets as described in the security question

## Likelihood Explanation

**Likelihood: Medium-High** given the right circumstances:

**Required conditions:**
- Access to Kubernetes cluster (via compromised credentials, malicious insider, or vulnerable pod)
- Knowledge of internal service endpoints (documented in code)

**Factors increasing likelihood:**
- Service runs in all production fullnode deployments
- Default production config binds to 0.0.0.0 exposing to cluster
- No authentication barrier
- Simple HTTP GET requests (trivial to execute)
- Kubernetes cluster compromise is a common attack vector

**Factors decreasing likelihood:**
- Requires insider access or credential compromise (not publicly accessible)
- Kubernetes network policies may restrict access (though not evident in reviewed configs)

## Recommendation

Implement comprehensive audit logging for all backup service endpoints:

```rust
// In storage/backup/backup-service/src/handlers/mod.rs
use warp::filters::addr::remote;

pub(crate) fn get_routes(backup_handler: BackupHandler) -> BoxedFilter<(impl Reply,)> {
    // ... existing route definitions ...
    
    warp::get()
        .and(remote()) // Capture client address
        .and(routes)
        .map(|addr: Option<SocketAddr>, response| {
            // Log access details
            if let Some(client_addr) = addr {
                info!(
                    "Backup service access: client={}, endpoint={}, timestamp={}",
                    client_addr,
                    // extract endpoint from request path
                    chrono::Utc::now()
                );
            }
            response
        })
        .with(warp::log::custom(|info| {
            // Enhanced logging with client IP, full path, query params
            info!(
                "Backup access: method={}, path={}, remote={:?}, status={}, duration={:?}",
                info.method(),
                info.path(),
                info.remote_addr(),
                info.status(),
                info.elapsed()
            );
            
            LATENCY_HISTOGRAM.observe_with(
                &[endpoint, info.status().as_str()],
                info.elapsed().as_secs_f64(),
            )
        }))
        .boxed()
}
```

**Additional recommendations:**
1. Implement mTLS authentication for backup service access
2. Add authorization checks based on service account identity
3. Ship audit logs to a centralized, immutable logging system
4. Add structured audit events including: timestamp, client identity, endpoint, parameters (version/key ranges), response size
5. Consider adding rate limiting to detect anomalous access patterns
6. Document access patterns and set up alerts for unusual activity

## Proof of Concept

```bash
#!/bin/bash
# Demonstrates unaudited data exfiltration from backup service
# Assumes access to Kubernetes cluster with deployed fullnode

# Get the backup service endpoint
BACKUP_SERVICE="http://aptos-fullnode:6186"

# 1. Exfiltrate database state (no logging of WHO accessed this)
curl -s "${BACKUP_SERVICE}/db_state" > db_state.bin

# 2. Exfiltrate complete state snapshot at version 1000000
curl -s "${BACKUP_SERVICE}/state_snapshot/1000000" > state_snapshot.bin

# 3. Exfiltrate 1 million transactions starting from version 0
curl -s "${BACKUP_SERVICE}/transactions/0/1000000" > transactions.bin

# 4. Exfiltrate epoch ending ledger infos
curl -s "${BACKUP_SERVICE}/epoch_ending_ledger_infos/0/100" > epoch_infos.bin

echo "Data exfiltrated successfully"
echo "No audit trail left behind - check node logs:"
kubectl logs -l app.kubernetes.io/name=fullnode | grep -i "backup" | tail -20
# Will only show metrics, not WHO accessed WHAT data
```

**Verification:**
1. Deploy an Aptos fullnode with backup service enabled
2. From another pod in the same cluster, execute the above script
3. Observe that metrics are collected but no audit trail exists showing:
   - Client IP or service account identity
   - Specific versions/ranges accessed
   - Time-series of access patterns
   - Ability to correlate access to specific operators

**Notes**

This finding highlights a defense-in-depth gap where the security model relies solely on network isolation without audit logging as a secondary control. While the service is designed to be internal-only, the lack of audit capabilities means breaches cannot be detected or investigated. This is particularly concerning for:

- Compliance with data protection regulations (GDPR, CCPA) requiring access logging
- Insider threat detection and deterrence  
- Incident response and forensic investigations
- Proving the absence of unauthorized access

The issue is categorized as Medium severity because while it doesn't directly compromise consensus or enable fund theft, it creates an undetectable pathway for complete blockchain data exfiltration by insiders or attackers with cluster access.

### Citations

**File:** storage/backup/backup-service/src/handlers/mod.rs (L27-147)
```rust
pub(crate) fn get_routes(backup_handler: BackupHandler) -> BoxedFilter<(impl Reply,)> {
    // GET db_state
    let bh = backup_handler.clone();
    let db_state = warp::path::end()
        .map(move || reply_with_bcs_bytes(DB_STATE, &bh.get_db_state()?))
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_range_proof/<version>/<end_key>
    let bh = backup_handler.clone();
    let state_range_proof = warp::path!(Version / HashValue)
        .map(move |version, end_key| {
            reply_with_bcs_bytes(
                STATE_RANGE_PROOF,
                &bh.get_account_state_range_proof(end_key, version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot/<version>
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_item_count/<version>
    let bh = backup_handler.clone();
    let state_item_count = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(
                STATE_ITEM_COUNT,
                &(bh.get_state_item_count(version)? as u64),
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot_chunk/<version>/<start_idx>/<limit>
    let bh = backup_handler.clone();
    let state_snapshot_chunk = warp::path!(Version / usize / usize)
        .map(move |version, start_idx, limit| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT_CHUNK, move |bh, sender| {
                bh.get_state_item_iter(version, start_idx, limit)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_root_proof/<version>
    let bh = backup_handler.clone();
    let state_root_proof = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(STATE_ROOT_PROOF, &bh.get_state_root_proof(version)?)
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET epoch_ending_ledger_infos/<start_epoch>/<end_epoch>/
    let bh = backup_handler.clone();
    let epoch_ending_ledger_infos = warp::path!(u64 / u64)
        .map(move |start_epoch, end_epoch| {
            reply_with_bytes_sender(&bh, EPOCH_ENDING_LEDGER_INFOS, move |bh, sender| {
                bh.get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transactions/<start_version>/<num_transactions>
    let bh = backup_handler.clone();
    let transactions = warp::path!(Version / usize)
        .map(move |start_version, num_transactions| {
            reply_with_bytes_sender(&bh, TRANSACTIONS, move |bh, sender| {
                bh.get_transaction_iter(start_version, num_transactions)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transaction_range_proof/<first_version>/<last_version>
    let bh = backup_handler;
    let transaction_range_proof = warp::path!(Version / Version)
        .map(move |first_version, last_version| {
            reply_with_bcs_bytes(
                TRANSACTION_RANGE_PROOF,
                &bh.get_transaction_range_proof(first_version, last_version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // Route by endpoint name.
    let routes = warp::any()
        .and(warp::path(DB_STATE).and(db_state))
        .or(warp::path(STATE_RANGE_PROOF).and(state_range_proof))
        .or(warp::path(STATE_SNAPSHOT).and(state_snapshot))
        .or(warp::path(STATE_ITEM_COUNT).and(state_item_count))
        .or(warp::path(STATE_SNAPSHOT_CHUNK).and(state_snapshot_chunk))
        .or(warp::path(STATE_ROOT_PROOF).and(state_root_proof))
        .or(warp::path(EPOCH_ENDING_LEDGER_INFOS).and(epoch_ending_ledger_infos))
        .or(warp::path(TRANSACTIONS).and(transactions))
        .or(warp::path(TRANSACTION_RANGE_PROOF).and(transaction_range_proof));

    // Serve all routes for GET only.
    warp::get()
        .and(routes)
        .with(warp::log::custom(|info| {
            let endpoint = info.path().split('/').nth(1).unwrap_or("-");
            LATENCY_HISTOGRAM.observe_with(
                &[endpoint, info.status().as_str()],
                info.elapsed().as_secs_f64(),
            )
        }))
        .boxed()
}
```

**File:** storage/backup/backup-service/src/lib.rs (L12-30)
```rust
pub fn start_backup_service(address: SocketAddr, db: Arc<AptosDB>) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler);

    let runtime = aptos_runtimes::spawn_named_runtime("backup".into(), None);

    // Ensure that we actually bind to the socket first before spawning the
    // server tasks. This helps in tests to prevent races where a client attempts
    // to make a request before the server task is actually listening on the
    // socket.
    //
    // Note: we need to enter the runtime context first to actually bind, since
    //       tokio TcpListener can only be bound inside a tokio context.
    let _guard = runtime.enter();
    let server = warp::serve(routes).bind(address);
    runtime.handle().spawn(server);
    info!("Backup service spawned.");
    runtime
}
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L41-109)
```rust
    pub fn get_transaction_iter(
        &self,
        start_version: Version,
        num_transactions: usize,
    ) -> Result<
        impl Iterator<
                Item = Result<(
                    Transaction,
                    PersistedAuxiliaryInfo,
                    TransactionInfo,
                    Vec<ContractEvent>,
                    WriteSet,
                )>,
            > + '_,
    > {
        let txn_iter = self
            .ledger_db
            .transaction_db()
            .get_transaction_iter(start_version, num_transactions)?;
        let mut txn_info_iter = self
            .ledger_db
            .transaction_info_db()
            .get_transaction_info_iter(start_version, num_transactions)?;
        let mut event_vec_iter = self
            .ledger_db
            .event_db()
            .get_events_by_version_iter(start_version, num_transactions)?;
        let mut write_set_iter = self
            .ledger_db
            .write_set_db()
            .get_write_set_iter(start_version, num_transactions)?;
        let mut persisted_aux_info_iter = self
            .ledger_db
            .persisted_auxiliary_info_db()
            .get_persisted_auxiliary_info_iter(start_version, num_transactions)?;

        let zipped = txn_iter.enumerate().map(move |(idx, txn_res)| {
            let version = start_version + idx as u64; // overflow is impossible since it's check upon txn_iter construction.

            let txn = txn_res?;
            let txn_info = txn_info_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "TransactionInfo not found when Transaction exists, version {}",
                    version
                ))
            })??;
            let event_vec = event_vec_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "Events not found when Transaction exists., version {}",
                    version
                ))
            })??;
            let write_set = write_set_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "WriteSet not found when Transaction exists, version {}",
                    version
                ))
            })??;
            let persisted_aux_info = persisted_aux_info_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "PersistedAuxiliaryInfo not found when Transaction exists, version {}",
                    version
                ))
            })??;
            BACKUP_TXN_VERSION.set(version as i64);
            Ok((txn, persisted_aux_info, txn_info, event_vec, write_set))
        });
        Ok(zipped)
    }
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L145-150)
```rust
    pub fn get_state_item_iter(
        &self,
        version: Version,
        start_idx: usize,
        limit: usize,
    ) -> Result<impl Iterator<Item = Result<(StateKey, StateValue)>> + Send + use<>> {
```

**File:** terraform/helm/fullnode/files/fullnode-base.yaml (L67-68)
```yaml
storage:
  backup_service_address: "0.0.0.0:6186"
```

**File:** terraform/helm/fullnode/templates/service.yaml (L52-54)
```yaml
  ports:
  - name: backup
    port: 6186
```
