# Audit Report

## Title
Unbounded Memory Exhaustion in Indexer-GRPC Stream Coordinator Response Accumulation

## Summary
The `process_next_batch()` function in the indexer-grpc fullnode service accumulates all transaction responses in memory before sending them to the client. When processing large transactions, the `chunk_transactions()` function creates many small chunks (one transaction per chunk when transactions exceed the MESSAGE_SIZE_LIMIT), causing the responses vector to grow unboundedly and exhaust available memory before any data is transmitted. [1](#0-0) 

## Finding Description

The vulnerability exists in the nested loop structure that accumulates responses. The outer loop chunks transactions by `output_batch_size` (default 100), and the inner loop calls `chunk_transactions()` with a 15MB size limit. When transactions are large (approaching or exceeding the Aptos transaction limits of 10MB for write operations and 10MB for events), each transaction gets placed in its own chunk because its encoded protobuf representation exceeds MESSAGE_SIZE_LIMIT. [2](#0-1) 

The critical issue is that ALL responses are accumulated in memory across all spawned tasks before ANY responses are sent to the client: [3](#0-2) 

Attack path:
1. Attacker submits legitimate transactions with maximum allowed state changes and events (up to 10MB each per transaction limit)
2. These transactions are committed to the blockchain
3. A client requests transaction streaming via indexer-grpc
4. The `process_next_batch()` fetches up to 20,000 transactions (processor_task_count × processor_batch_size = 20 × 1000)
5. Each large transaction (20-30MB when encoded as protobuf) creates its own response chunk
6. All responses accumulate: 20,000 transactions × 25MB average = 500GB memory usage
7. Service crashes with OOM before sending any data [4](#0-3) 

The Aptos gas schedule allows transactions to have substantial state changes: [5](#0-4) 

## Impact Explanation

This is a **Medium Severity** vulnerability under the Aptos bug bounty program. While it causes API service disruption, it does not affect:
- Blockchain consensus or validator operations
- Core node functionality
- Fund safety or state consistency

The impact is limited to denial of service of the indexer-grpc data streaming service, preventing clients from querying historical transaction data. This breaks the **Resource Limits** invariant that "all operations must respect gas, storage, and computational limits."

The indexer-grpc service is an auxiliary component for data access, not a consensus-critical component. However, its unavailability disrupts ecosystem applications relying on transaction indexing.

## Likelihood Explanation

**Likelihood: High**

The vulnerability is easily exploitable:
- **No special permissions required**: Any user can submit large transactions
- **Legitimate transactions**: Transactions with max write ops/events are valid
- **Default configuration vulnerable**: Default settings (20 tasks × 1000 batch size) enable the attack
- **No rate limiting**: No bounds on response accumulation
- **Automatic trigger**: Simply requesting transaction streams triggers the accumulation

An attacker needs only to:
1. Submit ~20,000 transactions with maximum state changes (costs APT for gas/storage fees)
2. Request transaction streaming from an indexer-grpc endpoint
3. Wait for memory exhaustion

## Recommendation

Implement streaming response transmission instead of batch accumulation. Send responses to the client incrementally as they are generated, rather than accumulating all responses in memory first.

**Proposed fix:**

```rust
// In the spawned task (line 170-200), instead of accumulating responses:
// Send responses through a channel as they're generated
let (response_tx, mut response_rx) = mpsc::channel(100);

let task = tokio::task::spawn_blocking(move || {
    let raw_txns = batch;
    let api_txns = Self::convert_to_api_txns(context, raw_txns);
    let pb_txns = Self::convert_to_pb_txns(api_txns);
    // Apply filter if present
    let pb_txns = if let Some(ref filter) = filter {
        pb_txns.into_iter().filter(|txn| filter.matches(txn)).collect::<Vec<_>>()
    } else {
        pb_txns
    };
    
    // Stream responses instead of accumulating
    for chunk in pb_txns.chunks(output_batch_size as usize) {
        for chunk in chunk_transactions(chunk.to_vec(), MESSAGE_SIZE_LIMIT) {
            let item = TransactionsFromNodeResponse {
                response: Some(transactions_from_node_response::Response::Data(
                    TransactionsOutput { transactions: chunk },
                )),
                chain_id: ledger_chain_id as u32,
            };
            // Send immediately instead of accumulating
            if response_tx.blocking_send(item).is_err() {
                break; // Client disconnected
            }
        }
    }
});

// Receive and forward responses as they arrive
tokio::spawn(async move {
    while let Some(response) = response_rx.recv().await {
        if transactions_sender.send(Ok(response)).await.is_err() {
            break; // Client disconnected
        }
    }
});
```

Additionally, implement a maximum memory threshold or response count limit per batch.

## Proof of Concept

```rust
#[tokio::test]
async fn test_memory_exhaustion_on_large_transactions() {
    // Setup: Create test context with large transactions
    let context = create_test_context();
    
    // Create 1000 transactions, each with maximum allowed state changes
    let large_transactions = create_large_test_transactions(1000);
    populate_storage_with_transactions(&context, large_transactions);
    
    let (tx, mut rx) = mpsc::channel(10);
    
    let mut coordinator = IndexerStreamCoordinator::new(
        Arc::new(context),
        0,
        1000,
        1, // processor_task_count
        1000, // processor_batch_size  
        100, // output_batch_size
        tx,
        None,
        None,
    );
    
    // Monitor memory before processing
    let mem_before = get_process_memory();
    
    // Trigger the vulnerability
    coordinator.process_next_batch().await;
    
    // Memory should grow significantly (multiple GB for 1000 large txns)
    let mem_after = get_process_memory();
    let mem_growth = mem_after - mem_before;
    
    // With 1000 transactions @ ~25MB each = ~25GB growth expected
    assert!(mem_growth > 20_000_000_000, 
            "Memory grew by {}GB, expected >20GB", 
            mem_growth / 1_000_000_000);
    
    // Verify responses were accumulated before sending
    // (all would be sent at once, not streamed)
}

fn create_large_test_transactions(count: usize) -> Vec<Transaction> {
    (0..count).map(|i| {
        // Create transaction with max write ops (10MB) and events (10MB)
        create_transaction_with_large_state_changes(i as u64)
    }).collect()
}
```

## Notes

This vulnerability is specific to the indexer-grpc component and does not affect core blockchain operations. The root cause is the synchronous accumulation pattern that collects all responses before transmission, combined with the lack of bounds checking on total memory consumption. The fix requires restructuring the response generation to use streaming/incremental transmission rather than batch accumulation.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L183-198)
```rust
                let mut responses = vec![];
                // Wrap in stream response object and send to channel
                for chunk in pb_txns.chunks(output_batch_size as usize) {
                    for chunk in chunk_transactions(chunk.to_vec(), MESSAGE_SIZE_LIMIT) {
                        let item = TransactionsFromNodeResponse {
                            response: Some(transactions_from_node_response::Response::Data(
                                TransactionsOutput {
                                    transactions: chunk,
                                },
                            )),
                            chain_id: ledger_chain_id as u32,
                        };
                        responses.push(item);
                    }
                }
                responses
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L202-208)
```rust
        let responses = match futures::future::try_join_all(tasks).await {
            Ok(res) => res.into_iter().flatten().collect::<Vec<_>>(),
            Err(err) => panic!(
                "[Indexer Fullnode] Error processing transaction batches: {:?}",
                err
            ),
        };
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/constants.rs (L18-19)
```rust
// Limit the message size to 15MB. By default the downstream can receive up to 15MB.
pub const MESSAGE_SIZE_LIMIT: usize = 1024 * 1024 * 15;
```

**File:** config/src/config/indexer_grpc_config.rs (L17-18)
```rust
const DEFAULT_PROCESSOR_BATCH_SIZE: u16 = 1000;
const DEFAULT_OUTPUT_BATCH_SIZE: u16 = 100;
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L154-177)
```rust
            max_bytes_per_write_op: NumBytes,
            { 5.. => "max_bytes_per_write_op" },
            1 << 20, // a single state item is 1MB max
        ],
        [
            max_bytes_all_write_ops_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_write_ops_per_transaction" },
            10 << 20, // all write ops from a single transaction are 10MB max
        ],
        [
            max_bytes_per_event: NumBytes,
            { 5.. => "max_bytes_per_event" },
            1 << 20, // a single event is 1MB max
        ],
        [
            max_bytes_all_events_per_transaction: NumBytes,
            { 5.. => "max_bytes_all_events_per_transaction"},
            10 << 20, // all events from a single transaction are 10MB max
        ],
        [
            max_write_ops_per_transaction: NumSlots,
            { 11.. => "max_write_ops_per_transaction" },
            8192,
        ],
```
