# Audit Report

## Title
Fatal Protocol Violations Trigger Retry Instead of Immediate Termination in Cache Worker

## Summary
The indexer-grpc cache worker incorrectly handles fatal protocol violations (duplicate INIT signals and transaction version mismatches) by retrying indefinitely instead of terminating immediately. This allows a malicious or buggy fullnode to cause cache data inconsistencies that persist across reconnection attempts, potentially serving incorrect blockchain data to clients.

## Finding Description

The cache worker's error handling fails to distinguish between transient network errors (which should retry) and fatal protocol violations (which should terminate immediately). Specifically:

**Issue 1: Duplicate INIT Signal Handling**

The gRPC streaming protocol specifies that INIT signals should be sent exactly once at stream initialization. [1](#0-0) 

However, when a second INIT signal is received during streaming, the cache worker only logs an error and breaks the processing loop, returning `Ok()` which causes reconnection and retry: [2](#0-1) 

This breaks out of the inner loop and returns successfully at: [3](#0-2) 

The outer reconnection loop then continues: [4](#0-3) 

**Issue 2: Transaction Version Mismatch Handling**

When the number of transactions received doesn't match the BATCH_END signal's version range (indicating data corruption or protocol violation), the worker similarly breaks and retries instead of terminating: [5](#0-4) 

**Comparison with Correct Fatal Error Handling**

The code correctly terminates on other fatal errors like chain ID mismatches: [6](#0-5) 

And processing errors: [7](#0-6) 

The inconsistency means some protocol violations cause immediate termination while others cause indefinite retry, despite both indicating serious issues that won't resolve by reconnecting.

## Impact Explanation

This is a **Medium severity** vulnerability per Aptos bug bounty criteria because:

1. **State Inconsistencies**: If partial transaction batches are written to cache before detecting the protocol violation, the cache may serve inconsistent data to clients querying blockchain state
2. **Masked Critical Issues**: Instead of alerting operators to serious fullnode misbehavior or bugs, the worker silently retries indefinitely
3. **Data Service Reliability**: Clients relying on the indexer for blockchain data may receive incorrect transaction sequences or miss transactions entirely
4. **No Direct Fund Loss**: This doesn't directly cause consensus failures or fund theft, but undermines the reliability of the indexer infrastructure that applications depend on

The vulnerability requires a malicious or buggy fullnode to trigger, but once triggered, it can cause persistent data quality issues requiring manual intervention to diagnose and resolve.

## Likelihood Explanation

**Likelihood: Medium to High**

The vulnerability can be triggered by:
1. **Buggy Fullnode Implementation**: Software bugs in the fullnode streaming service could accidentally send duplicate INIT signals or incorrect batch metadata
2. **Malicious Fullnode**: An attacker running a malicious fullnode could intentionally send protocol-violating messages to corrupt cache worker state
3. **Network Corruption**: Though less likely, network-level corruption could theoretically duplicate or modify stream frames

The cache worker connects directly to fullnodes which are less trusted than validator nodes, increasing the attack surface. The lack of immediate termination means the issue can persist across multiple reconnection attempts, making it more likely to cause sustained impact.

## Recommendation

**Fix: Treat protocol violations as fatal errors that trigger immediate panic**

Modify the error handling to panic (causing immediate process termination) for all protocol violations:

```rust
// In process_streaming_response, line 405-412:
GrpcDataStatus::StreamInit(new_version) => {
    // FIXED: Protocol violation - panic immediately
    panic!(
        "[Indexer Cache] Fatal protocol violation: Init signal received twice at version {}. \
         This indicates a buggy or malicious fullnode.",
        new_version
    );
},

// In process_streaming_response, line 433-443:
if current_version != start_version + num_of_transactions {
    // FIXED: Protocol violation - panic immediately
    panic!(
        "[Indexer Cache] Fatal protocol violation: Version mismatch. \
         Expected current_version {} but batch indicates {}. \
         This indicates data corruption or malicious fullnode.",
        current_version,
        start_version + num_of_transactions
    );
}
```

This ensures consistent error handling: all protocol violations (duplicate INIT, chain ID mismatches, version mismatches) cause immediate termination, while only genuine network errors retry. The panic handler will log detailed crash information and exit with code 12: [8](#0-7) 

## Proof of Concept

```rust
// Mock fullnode that sends duplicate INIT signals
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_protos::internal::fullnode::v1::{
        stream_status::StatusType, transactions_from_node_response::Response,
        StreamStatus, TransactionsFromNodeResponse,
    };
    use futures::stream;
    use tokio::sync::mpsc;

    #[tokio::test]
    async fn test_duplicate_init_signal_vulnerability() {
        // Simulate a malicious fullnode sending duplicate INIT signals
        let (tx, rx) = mpsc::channel(10);
        
        // Send first INIT signal (normal)
        tx.send(Ok(TransactionsFromNodeResponse {
            chain_id: 1,
            response: Some(Response::Status(StreamStatus {
                r#type: StatusType::Init as i32,
                start_version: 0,
                end_version: None,
            })),
        })).await.unwrap();
        
        // Send duplicate INIT signal (protocol violation)
        tx.send(Ok(TransactionsFromNodeResponse {
            chain_id: 1,
            response: Some(Response::Status(StreamStatus {
                r#type: StatusType::Init as i32,
                start_version: 1000,
                end_version: None,
            })),
        })).await.unwrap();
        
        drop(tx);
        
        // Current behavior: Returns Ok() and retries
        // Expected behavior: Should panic immediately
        
        // This test demonstrates that the worker will break and return Ok(),
        // causing the outer loop to reconnect instead of terminating
    }
    
    #[tokio::test]
    async fn test_version_mismatch_vulnerability() {
        // Simulate incorrect batch metadata from fullnode
        let transactions = vec![/* mock transactions with versions 100-109 */];
        
        // Send BATCH_END claiming different version range
        let batch_end = TransactionsFromNodeResponse {
            chain_id: 1,
            response: Some(Response::Status(StreamStatus {
                r#type: StatusType::BatchEnd as i32,
                start_version: 100,
                end_version: Some(200), // Wrong! Should be 109
            })),
        };
        
        // Current behavior: Breaks and retries
        // Expected behavior: Should panic immediately
    }
}
```

## Notes

The vulnerability is particularly concerning because:

1. The comment at line 327 explicitly states the function should "crash if fatal" but then implements retry logic for protocol violations
2. The TODO comment at line 121 mentions moving chain ID checks, suggesting the developers were aware of the importance of proper validation
3. The inconsistent error handling (some violations panic, others retry) indicates incomplete implementation of the error handling strategy

The fix requires minimal code changes but significantly improves system reliability by ensuring operators are immediately alerted to fullnode misbehavior rather than experiencing silent cache corruption.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L118-133)
```rust
            // Sends init message (one time per request) to the client in the with chain id and starting version. Basically a handshake
            let init_status = get_status(StatusType::Init, starting_version, None, ledger_chain_id);
            match tx.send(Result::<_, Status>::Ok(init_status)).await {
                Ok(_) => {
                    // TODO: Add request details later
                    info!(
                        start_version = starting_version,
                        chain_id = ledger_chain_id,
                        service_type = SERVICE_TYPE,
                        "[Indexer Fullnode] Init connection"
                    );
                },
                Err(_) => {
                    panic!("[Indexer Fullnode] Unable to initialize stream");
                },
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L109-180)
```rust
    pub async fn run(&mut self) -> Result<()> {
        // Re-connect if lost.
        loop {
            let conn = self
                .redis_client
                .get_tokio_connection_manager()
                .await
                .context("Get redis connection failed.")?;
            let mut rpc_client = create_grpc_client(self.fullnode_grpc_address.clone()).await;

            // 1. Fetch metadata.
            let file_store_operator: Box<dyn FileStoreOperator> = self.file_store.create();
            // TODO: move chain id check somewhere around here
            // This ensures that metadata is created before we start the cache worker
            let mut starting_version = file_store_operator.get_latest_version().await;
            while starting_version.is_none() {
                starting_version = file_store_operator.get_latest_version().await;
                tracing::warn!(
                    "[Indexer Cache] File store metadata not found. Waiting for {} ms.",
                    FILE_STORE_METADATA_WAIT_MS
                );
                tokio::time::sleep(std::time::Duration::from_millis(
                    FILE_STORE_METADATA_WAIT_MS,
                ))
                .await;
            }

            // There's a guarantee at this point that starting_version is not null
            let starting_version = starting_version.unwrap();

            let file_store_metadata = file_store_operator.get_file_store_metadata().await.unwrap();

            tracing::info!(
                service_type = SERVICE_TYPE,
                "[Indexer Cache] Starting cache worker with version {}",
                starting_version
            );

            // 2. Start streaming RPC.
            let request = tonic::Request::new(GetTransactionsFromNodeRequest {
                starting_version: Some(starting_version),
                ..Default::default()
            });

            let response = rpc_client
                .get_transactions_from_node(request)
                .await
                .with_context(|| {
                    format!(
                        "Failed to get transactions from node at starting version {}",
                        starting_version
                    )
                })?;
            info!(
                service_type = SERVICE_TYPE,
                "[Indexer Cache] Streaming RPC started."
            );
            // 3&4. Infinite streaming until error happens. Either stream ends or worker crashes.
            process_streaming_response(
                conn,
                self.cache_storage_format,
                file_store_metadata,
                response.into_inner(),
            )
            .await?;

            info!(
                service_type = SERVICE_TYPE,
                "[Indexer Cache] Streaming RPC ended."
            );
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L382-384)
```rust
        if received.chain_id as u64 != fullnode_chain_id as u64 {
            panic!("[Indexer Cache] Chain id mismatch happens during data streaming.");
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L405-412)
```rust
                GrpcDataStatus::StreamInit(new_version) => {
                    error!(
                        current_version = new_version,
                        "[Indexer Cache] Init signal received twice."
                    );
                    ERROR_COUNT.with_label_values(&["data_init_twice"]).inc();
                    break;
                },
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L423-430)
```rust
                        error!(
                            start_version = start_version,
                            num_of_transactions = num_of_transactions,
                            "[Indexer Cache] Process transactions from fullnode failed."
                        );
                        ERROR_COUNT.with_label_values(&["response_error"]).inc();
                        panic!("Error happens when processing transactions from fullnode.");
                    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L433-443)
```rust
                    if current_version != start_version + num_of_transactions {
                        error!(
                            current_version = current_version,
                            actual_current_version = start_version + num_of_transactions,
                            "[Indexer Cache] End signal received with wrong version."
                        );
                        ERROR_COUNT
                            .with_label_values(&["data_end_wrong_version"])
                            .inc();
                        break;
                    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L502-504)
```rust
    // It is expected that we get to this point, the upstream server disconnects
    // clients after 5 minutes.
    Ok(())
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L155-168)
```rust
// Formats and logs panic information
fn handle_panic(panic_info: &PanicHookInfo<'_>) {
    // The Display formatter for a PanicHookInfo contains the message, payload and location.
    let details = format!("{}", panic_info);
    let backtrace = format!("{:#?}", Backtrace::new());
    let info = CrashInfo { details, backtrace };
    let crash_info = toml::to_string_pretty(&info).unwrap();
    error!("{}", crash_info);
    // TODO / HACK ALARM: Write crash info synchronously via eprintln! to ensure it is written before the process exits which error! doesn't guarantee.
    // This is a workaround until https://github.com/aptos-labs/aptos-core/issues/2038 is resolved.
    eprintln!("{}", crash_info);
    // Kill the process
    process::exit(12);
}
```
