# Audit Report

## Title
Byzantine Validator Can Cause Temporary Observer Liveness Failure Through Selective Message Publishing

## Summary
A Byzantine validator can selectively publish only `OrderedBlock` messages without corresponding `CommitDecision` messages, causing consensus observers to execute blocks that can never be committed to storage. This results in incomplete consensus state and temporary liveness failure for affected observers lasting up to 10 seconds until timeout mechanisms trigger recovery.

## Finding Description

The consensus observer architecture allows validators to publish three types of consensus updates to subscribed observers via the `publish_message()` function: [1](#0-0) 

These messages are published at different stages of consensus:
- **BlockPayload**: Published when block transaction payloads are ready [2](#0-1) 

- **OrderedBlock**: Published when blocks are ordered [3](#0-2) 

- **CommitDecision**: Published when blocks are committed [4](#0-3) 

A Byzantine validator controlling their `ConsensusPublisher` instance can selectively skip calling `publish_message()` for commit decisions while continuing to publish ordered blocks. This causes observers to:

1. **Receive and process ordered blocks** that are verified and sent to the execution pipeline [5](#0-4) 

2. **Execute blocks without commit decisions**, leaving them in a non-aggregated state in the buffer manager where they cannot advance to the persisting phase [6](#0-5) 

3. **Cannot persist blocks to storage** because the `advance_head()` function explicitly requires aggregated items with commit proofs [7](#0-6) 

4. **Database version remains unchanged**, creating observable incomplete consensus state where observers have executed blocks but no committed state progression.

The attack is eventually detected through built-in timeout mechanisms: [8](#0-7) 

After `max_subscription_sync_timeout_ms` (default 10 seconds), the subscription health check detects that the database version is not increasing and terminates the subscription, triggering recovery through either re-subscription to a different validator or fallback mode.

## Impact Explanation

This issue qualifies as **High Severity** under the Aptos bug bounty criteria ("Validator node slowdowns" and "Significant protocol violations"):

**Affected Components:**
- Consensus observers subscribed to Byzantine validators experience temporary liveness failure
- Multiple observers can be simultaneously affected if subscribed to the same malicious validator
- Resources are wasted executing blocks that are never committed

**Duration and Scope:**
- Up to 10 seconds of progress stalling per occurrence (configurable via `max_subscription_sync_timeout_ms`)
- Attack can be repeated if observers re-subscribe to the same Byzantine validator
- Limited to observers only; does not affect validator consensus or the main blockchain

**Defense Mechanisms:**
The protocol includes multiple layers of defense:
1. Subscription timeout detection [9](#0-8) 

2. Syncing progress verification that terminates unhealthy subscriptions
3. Fallback manager that monitors database progress [10](#0-9) 

## Likelihood Explanation

**Attack Complexity:** Low - Byzantine validator only needs to conditionally skip `publish_message()` calls for commit decisions in their modified codebase.

**Attacker Requirements:** Requires control of a validator node that observers choose to subscribe to. This aligns with the Byzantine Fault Tolerance assumption that up to 1/3 of validators may behave maliciously.

**Detection:** Automatic and guaranteed within 10 seconds through built-in timeout mechanisms. No manual intervention required for recovery.

**Practical Likelihood:** Medium - While technically feasible, the temporary and self-recovering nature of the attack limits its practical impact. Observers learn from the experience and avoid re-subscribing to malicious validators.

## Recommendation

The current implementation includes appropriate defense mechanisms (timeouts, health checks, fallback modes) that detect and recover from this attack pattern. However, several improvements could reduce the impact window:

1. **Reduce timeout values** - Consider lowering `max_subscription_sync_timeout_ms` from 10 seconds to 5 seconds to detect stalled progress faster.

2. **Implement message correlation checking** - Track expected message sequences and detect when commit decisions are missing for received ordered blocks:

```rust
// In ConsensusObserver
fn verify_message_sequence(&self, ordered_block: &OrderedBlock) {
    // Track that we expect a commit decision for this block
    self.expected_commit_decisions.insert(
        (ordered_block.epoch(), ordered_block.round()),
        Instant::now()
    );
    
    // Periodically check for missing commit decisions
    // If OrderedBlock received but CommitDecision not received within threshold,
    // proactively terminate subscription
}
```

3. **Reputation system** - Implement peer reputation tracking that penalizes validators who exhibit selective publishing behavior, reducing the likelihood of observers re-subscribing to known malicious validators.

4. **Parallel subscriptions** - Allow observers to maintain subscriptions to multiple validators simultaneously, accepting the first valid commit decision received, reducing dependency on any single validator.

## Proof of Concept

This vulnerability manifests through validator-level code modification and requires a Byzantine validator to implement selective publishing logic. The attack can be demonstrated conceptually:

```rust
// Byzantine validator's modified consensus_publisher.rs
pub fn publish_message(&self, message: ConsensusObserverDirectSend) {
    // Selective publishing attack: only publish OrderedBlock and BlockPayload
    // Skip CommitDecision messages
    match &message {
        ConsensusObserverDirectSend::CommitDecision(_) => {
            // Byzantine behavior: intentionally skip publishing commit decisions
            warn!("Byzantine: Skipping commit decision publication");
            return;
        },
        _ => {
            // Publish other message types normally
        }
    }
    
    // Original publish logic continues...
    let active_subscribers = self.get_active_subscribers();
    for peer_network_id in &active_subscribers {
        // ... send message
    }
}
```

Observable impact on affected observer:
1. Observer receives and executes ordered blocks
2. Database version remains unchanged (no commits)
3. After 10 seconds, subscription health check fails with `SubscriptionProgressStopped` error
4. Observer automatically switches to different validator or enters fallback mode

The attack is self-limiting due to built-in timeout mechanisms and does not cause permanent harm to the network or affected observers.

## Notes

This finding represents a **design limitation within acceptable Byzantine Fault Tolerance parameters** rather than a critical protocol flaw. The consensus observer protocol explicitly anticipates Byzantine validator behavior and includes appropriate defense mechanisms. The 10-second impact window represents a conscious trade-off between responsiveness and false-positive detection.

The key security invariant maintained: **Observers never commit invalid state**. They may experience temporary delays, but the protocol ensures that only properly certified blocks with valid commit proofs are persisted to storage.

### Citations

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L210-232)
```rust
    /// Publishes a direct send message to all active subscribers. Note: this method
    /// is non-blocking (to avoid blocking callers during publishing, e.g., consensus).
    pub fn publish_message(&self, message: ConsensusObserverDirectSend) {
        // Get the active subscribers
        let active_subscribers = self.get_active_subscribers();

        // Send the message to all active subscribers
        for peer_network_id in &active_subscribers {
            // Send the message to the outbound receiver for publishing
            let mut outbound_message_sender = self.outbound_message_sender.clone();
            if let Err(error) =
                outbound_message_sender.try_send((*peer_network_id, message.clone()))
            {
                // The message send failed
                warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::SendDirectSendMessage)
                        .message(&format!(
                            "Failed to send outbound message to the receiver for peer {:?}! Error: {:?}",
                            peer_network_id, error
                    )));
            }
        }
    }
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L551-557)
```rust
        if let Some(consensus_publisher) = &self.maybe_consensus_publisher {
            let message = ConsensusObserverMessage::new_block_payload_message(
                block.gen_block_info(HashValue::zero(), 0, None),
                transaction_payload.clone(),
            );
            consensus_publisher.publish_message(message);
        }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L400-406)
```rust
        if let Some(consensus_publisher) = &self.consensus_publisher {
            let message = ConsensusObserverMessage::new_ordered_block_message(
                ordered_blocks.clone(),
                ordered_proof.clone(),
            );
            consensus_publisher.publish_message(message);
        }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L503-504)
```rust
            if item.block_id() == target_block_id {
                let aggregated_item = item.unwrap_aggregated();
```

**File:** consensus/src/pipeline/buffer_manager.rs (L514-518)
```rust
                if let Some(consensus_publisher) = &self.consensus_publisher {
                    let message =
                        ConsensusObserverMessage::new_commit_decision_message(commit_proof.clone());
                    consensus_publisher.publish_message(message);
                }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L523-529)
```rust
                self.persisting_phase_tx
                    .send(self.create_new_request(PersistingRequest {
                        blocks: blocks_to_persist,
                        commit_ledger_info: aggregated_item.commit_proof,
                    }))
                    .await
                    .expect("Failed to send persist request");
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L784-792)
```rust
            // Insert the ordered block into the pending blocks
            self.observer_block_data
                .lock()
                .insert_ordered_block(observed_ordered_block.clone());

            // If state sync is not syncing to a commit, finalize the ordered blocks
            if !self.state_sync_manager.is_syncing_to_commit() {
                self.finalize_ordered_block(ordered_block).await;
            }
```

**File:** consensus/src/consensus_observer/observer/subscription.rs (L164-182)
```rust
    /// Verifies that the subscription has not timed out based
    /// on the last received message time.
    fn check_subscription_timeout(&self) -> Result<(), Error> {
        // Calculate the duration since the last message
        let time_now = self.time_service.now();
        let duration_since_last_message = time_now.duration_since(self.last_message_receive_time);

        // Check if the subscription has timed out
        if duration_since_last_message
            > Duration::from_millis(self.consensus_observer_config.max_subscription_timeout_ms)
        {
            return Err(Error::SubscriptionTimeout(format!(
                "Subscription to peer: {} has timed out! No message received for: {:?}",
                self.peer_network_id, duration_since_last_message
            )));
        }

        Ok(())
    }
```

**File:** consensus/src/consensus_observer/observer/subscription.rs (L184-222)
```rust
    /// Verifies that the DB is continuing to sync and commit new data
    fn check_syncing_progress(&mut self) -> Result<(), Error> {
        // Get the current time and synced version from storage
        let time_now = self.time_service.now();
        let current_synced_version =
            self.db_reader
                .get_latest_ledger_info_version()
                .map_err(|error| {
                    Error::UnexpectedError(format!(
                        "Failed to read highest synced version: {:?}",
                        error
                    ))
                })?;

        // Verify that the synced version is increasing appropriately
        let (highest_synced_version, highest_version_timestamp) =
            self.highest_synced_version_and_time;
        if current_synced_version <= highest_synced_version {
            // The synced version hasn't increased. Check if we should terminate
            // the subscription based on the last time the highest synced version was seen.
            let duration_since_highest_seen = time_now.duration_since(highest_version_timestamp);
            let timeout_duration = Duration::from_millis(
                self.consensus_observer_config
                    .max_subscription_sync_timeout_ms,
            );
            if duration_since_highest_seen > timeout_duration {
                return Err(Error::SubscriptionProgressStopped(format!(
                    "The DB is not making sync progress! Highest synced version: {}, elapsed: {:?}",
                    highest_synced_version, duration_since_highest_seen
                )));
            }
            return Ok(()); // We haven't timed out yet
        }

        // Update the highest synced version and time
        self.highest_synced_version_and_time = (current_synced_version, time_now);

        Ok(())
    }
```

**File:** consensus/src/consensus_observer/observer/fallback_manager.rs (L55-85)
```rust
    /// Verifies that the DB is continuing to sync and commit new data, and that
    /// the node has not fallen too far behind the rest of the network.
    /// If not, an error is returned, indicating that we should enter fallback mode.
    pub fn check_syncing_progress(&mut self) -> Result<(), Error> {
        // If we're still within the startup period, we don't need to verify progress
        let time_now = self.time_service.now();
        let startup_period = Duration::from_millis(
            self.consensus_observer_config
                .observer_fallback_startup_period_ms,
        );
        if time_now.duration_since(self.start_time) < startup_period {
            return Ok(()); // We're still in the startup period
        }

        // Fetch the synced ledger info version from storage
        let latest_ledger_info_version =
            self.db_reader
                .get_latest_ledger_info_version()
                .map_err(|error| {
                    Error::UnexpectedError(format!(
                        "Failed to read highest synced version: {:?}",
                        error
                    ))
                })?;

        // Verify that the synced version is increasing appropriately
        self.verify_increasing_sync_versions(latest_ledger_info_version, time_now)?;

        // Verify that the sync lag is within acceptable limits
        self.verify_sync_lag_health(latest_ledger_info_version)
    }
```
