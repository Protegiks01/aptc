# Audit Report

## Title
Transient Database Errors Cause Validator Panic During Startup, Bypassing Recovery Fallback Mechanism

## Summary
The `get_data()` function in ConsensusDB properly propagates database errors using `?` operators, but its caller in `StorageWriteProxy::start()` uses `.expect()` which causes a panic on any database error. This bypasses the designed `PartialRecoveryData` fallback mechanism intended to handle corrupted or missing consensus state, causing validators to fail startup on transient errors instead of gracefully recovering.

## Finding Description
The Aptos consensus system implements a two-tier recovery mechanism for validator startup:

1. **Full Recovery**: Read blocks and QCs from ConsensusDB and construct `RecoveryData`
2. **Partial Recovery (Fallback)**: If block tree is corrupted/incomplete, fall back to `PartialRecoveryData` using only ledger info, then sync blocks from peers via `RecoveryManager` [1](#0-0) 

However, the fallback mechanism is unreachable when database read errors occur. In `StorageWriteProxy::start()`, the code calls `get_data().expect()`: [2](#0-1) 

The `get_data()` function correctly uses `?` operators to propagate errors from multiple database operations: [3](#0-2) 

These operations can fail with various `AptosDbError` types including:
- `IoError` - File system I/O failures (transient)
- `RocksDbIncompleteResult` - RocksDB errors (potentially transient)
- `OtherRocksDbError` - RocksDB errors (potentially transient)
- `BcsError` - Deserialization errors (corruption) [4](#0-3) 

The fallback mechanism only handles errors from `RecoveryData::new()` (when block tree validation fails), not from `get_data()` (when database reads fail): [5](#0-4) 

**Attack Sequence:**
1. Validator node starts or restarts
2. `EpochManager::await_reconfig_notification()` triggers `start_new_epoch()` → `start_new_epoch_with_jolteon()` → `storage.start()`
3. During `get_data()`, a transient error occurs (I/O timeout, RocksDB lock contention, disk flake)
4. Error propagates via `?` but hits `.expect()` → **validator panics and crashes**
5. `PartialRecoveryData` fallback is never reached despite being designed for this scenario [6](#0-5) 

## Impact Explanation
This qualifies as **High Severity** per Aptos bug bounty criteria:

**Validator Node Failures**: Transient database errors (common in production environments with high I/O load, network-attached storage, or hardware flakes) cause complete validator node crashes instead of graceful recovery.

**Liveness Impact**: During coordinated restarts (e.g., after network partition recovery, software upgrades, or coordinated maintenance), multiple validators may experience transient errors simultaneously. If enough validators fail to start, the network loses consensus quorum, causing temporary loss of liveness.

**Availability Degradation**: Individual validators experiencing transient errors must wait for manual intervention or process restart rather than self-recovering via the designed fallback mechanism.

The bug bypasses a critical reliability feature (`PartialRecoveryData` + `RecoveryManager`) specifically designed to handle consensus database corruption/incompleteness.

## Likelihood Explanation
**Medium-High Likelihood:**

Transient database errors occur regularly in production environments:
- Disk I/O timeouts under load
- Network-attached storage transient failures
- RocksDB background compaction causing temporary lock contention
- Filesystem-level issues (NFS glitches, FUSE delays)
- Partial writes during unclean shutdowns

The bug manifests during validator startup, a critical operation that occurs:
- After software upgrades
- After crashes/restarts
- During coordinated network maintenance
- During recovery from network partitions

No attacker action is required—natural operational conditions trigger this bug. However, an attacker with ability to cause temporary filesystem disruption (disk filling, I/O storms) on validator nodes could weaponize this to prevent validator startup.

## Recommendation
Replace `.expect()` with proper error handling that allows fallback to `PartialRecoveryData`:

```rust
fn start(&self, order_vote_enabled: bool, window_size: Option<u64>) -> LivenessStorageData {
    info!("Start consensus recovery.");
    
    let raw_data = match self.db.get_data() {
        Ok(data) => data,
        Err(e) => {
            error!(error = ?e, "Failed to read consensus data from database, falling back to partial recovery");
            let latest_ledger_info = self
                .aptos_db
                .get_latest_ledger_info()
                .expect("Failed to get latest ledger info.");
            return LivenessStorageData::PartialRecoveryData(
                LedgerRecoveryData::new(latest_ledger_info)
            );
        }
    };
    
    // Rest of the function remains unchanged...
}
```

This allows the system to gracefully fall back to `PartialRecoveryData` mode when database reads fail, enabling the `RecoveryManager` to sync missing blocks from peers rather than crashing the validator.

Additionally, consider similar fixes for the `.expect()` calls on lines 528, 531, 552, 556, and 572 in the same function, though those are less critical as they occur after initial data retrieval.

## Proof of Concept
The following Rust test demonstrates the vulnerability:

```rust
#[test]
#[should_panic(expected = "unable to recover consensus data")]
fn test_transient_db_error_causes_panic() {
    use tempfile::TempDir;
    use std::sync::Arc;
    
    // Setup: Create a temporary ConsensusDB
    let temp_dir = TempDir::new().unwrap();
    let db = Arc::new(ConsensusDB::new(temp_dir.path()));
    
    // Simulate a transient I/O error by corrupting the database directory
    std::fs::remove_dir_all(temp_dir.path().join("consensus_db")).unwrap();
    
    // Create mock AptosDB
    let aptos_db = create_mock_aptos_db();
    
    let storage = StorageWriteProxy {
        db,
        aptos_db,
    };
    
    // This will panic instead of returning PartialRecoveryData
    let result = storage.start(false, None);
    
    // We should reach PartialRecoveryData but instead we panic
    match result {
        LivenessStorageData::PartialRecoveryData(_) => {
            println!("Correctly fell back to PartialRecoveryData");
        },
        LivenessStorageData::FullRecoveryData(_) => {
            panic!("Unexpected FullRecoveryData");
        }
    }
}
```

To trigger in a real environment:
1. Start a validator node
2. During startup, inject a transient I/O error (e.g., temporarily make RocksDB files unreadable, cause filesystem delays)
3. The validator will panic with "unable to recover consensus data" instead of entering recovery mode

## Notes
The vulnerability specifically affects the `?` operators on lines 88-89 (and also 91, 96) in `get_data()` as mentioned in the security question. While the `?` operators themselves correctly propagate errors, the `.expect()` in the caller at line 523 of `persistent_liveness_storage.rs` converts these errors into panics, preventing the graceful fallback mechanism from functioning. The designed `PartialRecoveryData` recovery path via `RecoveryManager` is well-implemented but unreachable due to this panic point.

### Citations

**File:** consensus/src/epoch_manager.rs (L126-129)
```rust
pub enum LivenessStorageData {
    FullRecoveryData(RecoveryData),
    PartialRecoveryData(LedgerRecoveryData),
}
```

**File:** consensus/src/epoch_manager.rs (L1383-1417)
```rust
        match self.storage.start(
            consensus_config.order_vote_enabled(),
            consensus_config.window_size(),
        ) {
            LivenessStorageData::FullRecoveryData(initial_data) => {
                self.recovery_mode = false;
                self.start_round_manager(
                    consensus_key,
                    initial_data,
                    epoch_state,
                    consensus_config,
                    execution_config,
                    onchain_randomness_config,
                    jwk_consensus_config,
                    Arc::new(network_sender),
                    payload_client,
                    payload_manager,
                    rand_config,
                    fast_rand_config,
                    rand_msg_rx,
                    secret_share_msg_rx,
                )
                .await
            },
            LivenessStorageData::PartialRecoveryData(ledger_data) => {
                self.recovery_mode = true;
                self.start_recovery_manager(
                    ledger_data,
                    consensus_config,
                    epoch_state,
                    Arc::new(network_sender),
                )
                .await
            },
        }
```

**File:** consensus/src/persistent_liveness_storage.rs (L521-524)
```rust
        let raw_data = self
            .db
            .get_data()
            .expect("unable to recover consensus data");
```

**File:** consensus/src/persistent_liveness_storage.rs (L559-595)
```rust
        match RecoveryData::new(
            last_vote,
            ledger_recovery_data.clone(),
            blocks,
            accumulator_summary.into(),
            quorum_certs,
            highest_2chain_timeout_cert,
            order_vote_enabled,
            window_size,
        ) {
            Ok(mut initial_data) => {
                (self as &dyn PersistentLivenessStorage)
                    .prune_tree(initial_data.take_blocks_to_prune())
                    .expect("unable to prune dangling blocks during restart");
                if initial_data.last_vote.is_none() {
                    self.db
                        .delete_last_vote_msg()
                        .expect("unable to cleanup last vote");
                }
                if initial_data.highest_2chain_timeout_certificate.is_none() {
                    self.db
                        .delete_highest_2chain_timeout_certificate()
                        .expect("unable to cleanup highest 2-chain timeout cert");
                }
                info!(
                    "Starting up the consensus state machine with recovery data - [last_vote {}], [highest timeout certificate: {}]",
                    initial_data.last_vote.as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                    initial_data.highest_2chain_timeout_certificate().as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                );

                LivenessStorageData::FullRecoveryData(initial_data)
            },
            Err(e) => {
                error!(error = ?e, "Failed to construct recovery data");
                LivenessStorageData::PartialRecoveryData(ledger_recovery_data)
            },
        }
```

**File:** consensus/src/consensusdb/mod.rs (L80-106)
```rust
    pub fn get_data(
        &self,
    ) -> Result<(
        Option<Vec<u8>>,
        Option<Vec<u8>>,
        Vec<Block>,
        Vec<QuorumCert>,
    )> {
        let last_vote = self.get_last_vote()?;
        let highest_2chain_timeout_certificate = self.get_highest_2chain_timeout_certificate()?;
        let consensus_blocks = self
            .get_all::<BlockSchema>()?
            .into_iter()
            .map(|(_, block)| block)
            .collect();
        let consensus_qcs = self
            .get_all::<QCSchema>()?
            .into_iter()
            .map(|(_, qc)| qc)
            .collect();
        Ok((
            last_vote,
            highest_2chain_timeout_certificate,
            consensus_blocks,
            consensus_qcs,
        ))
    }
```

**File:** storage/storage-interface/src/errors.rs (L11-37)
```rust
pub enum AptosDbError {
    /// A requested item is not found.
    #[error("{0} not found.")]
    NotFound(String),
    /// Requested too many items.
    #[error("Too many items requested: at least {0} requested, max is {1}")]
    TooManyRequested(u64, u64),
    #[error("Missing state root node at version {0}, probably pruned.")]
    MissingRootError(u64),
    /// Other non-classified error.
    #[error("AptosDB Other Error: {0}")]
    Other(String),
    #[error("AptosDB RocksDb Error: {0}")]
    RocksDbIncompleteResult(String),
    #[error("AptosDB RocksDB Error: {0}")]
    OtherRocksDbError(String),
    #[error("AptosDB bcs Error: {0}")]
    BcsError(String),
    #[error("AptosDB IO Error: {0}")]
    IoError(String),
    #[error("AptosDB Recv Error: {0}")]
    RecvError(String),
    #[error("AptosDB ParseInt Error: {0}")]
    ParseIntError(String),
    #[error("Hot state not configured properly")]
    HotStateError,
}
```
