# Audit Report

## Title
Race Condition in Cold Validation Requirements Causes Invariant Violation and Forced Sequential Execution Fallback

## Summary
A race condition exists in the `activate_pending_requirements()` function where concurrent `record_requirements()` calls can add new pending requirements after the dedicated worker has checked and found the pending queue empty, but before it accesses active requirements. This causes the worker to hit an "impossible" invariant error, triggering a forced fallback to sequential execution and degrading validator performance.

## Finding Description

The vulnerability exists in the interaction between `activate_pending_requirements()` and `record_requirements()` in the cold validation requirements system for BlockSTMv2. [1](#0-0) 

The race occurs in this specific sequence:

1. The designated worker (Worker 1) calls `get_validation_requirement_to_process()` which invokes `activate_pending_requirements()` at line 291 [2](#0-1) 

2. Worker 1 drains pending requirements at line 463, releases the lock, and processes them (lines 466-499)

3. If all drained transactions are in `Aborted` or `PendingScheduling` states (determined by `requires_module_validation()`), no active requirements are created [3](#0-2) 

4. **RACE WINDOW**: Between releasing the lock (line 464) and re-acquiring it (line 507), Worker 2 calls `record_requirements()` and adds new pending requirements [4](#0-3) 

5. Worker 1 re-locks at line 507, finds `pending_reqs_guard.is_empty()` returns false (due to Worker 2's additions), and returns `Ok(false)` at line 516 without resetting the dedicated worker

6. Worker 1 continues in `get_validation_requirement_to_process()` and tries to access `active_reqs.versions.first_key_value()` at line 302-303, but active requirements are empty

7. The code hits the invariant error at line 306, which the comment explicitly states "Should not be empty as dedicated worker was set in the beginning of the method"

8. This `PanicError` propagates through `next_task()` in the scheduler and is caught in the worker loop [5](#0-4) 

9. The error handler halts the scheduler and sets the error flag, forcing fallback to sequential execution [6](#0-5) 

This breaks the code's assumption that a designated worker will always have either pending or active requirements to process.

## Impact Explanation

**Severity: HIGH** - This qualifies as "Validator node slowdowns" per the Aptos bug bounty criteria.

When triggered, this race condition forces the block executor to:
1. Halt parallel execution mid-block
2. Fall back to sequential execution (with `allow_fallback: true` by default) [7](#0-6) 

Sequential execution is significantly slower than parallel execution for blocks with many transactions. If this race occurs repeatedly (in blocks with multiple module publishes and contentious workloads), it would:
- Degrade validator throughput substantially
- Increase block processing latency
- Potentially affect consensus timing if validators experience different race frequencies

While the fallback mechanism prevents incorrect execution (maintaining consensus safety), the performance impact on validators is severe enough to qualify as HIGH severity.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH** in blocks with module publishing activity.

The race requires:
1. Multiple transactions publishing modules in the same block
2. Transactions in the requirement range being in non-validatable states (Aborted/PendingScheduling) when processed
3. Precise timing for the second module publish to occur during the critical window

While the race window is narrow (nanoseconds between lock operations), it becomes increasingly likely in:
- Blocks with high module publishing activity
- Contentious workloads where transactions are frequently aborted
- Networks with high transaction throughput

In normal operation without module publishes, the race cannot occur. However, during protocol upgrades, framework updates, or smart contract deployments, module publishing activity increases significantly.

## Recommendation

Add a retry loop when active requirements are found empty instead of immediately triggering an invariant error. The designated worker should re-check for pending requirements before concluding no work exists:

**In `get_validation_requirement_to_process()` after line 295, replace the immediate panic with:**

```rust
// After activate_pending_requirements returns false, re-check if new pending
// requirements were added concurrently before accessing active requirements
loop {
    let active_reqs = self.active_requirements.dereference();
    
    if let Some((min_active_requirement_idx, (incarnation, is_executing))) =
        active_reqs.versions.first_key_value()
    {
        // Found active requirement, process it
        if *min_active_requirement_idx <= idx_threshold {
            return Ok(Some((
                *min_active_requirement_idx,
                *incarnation,
                ValidationRequirement::new(
                    self.active_requirements.dereference_mut(),
                    *is_executing,
                ),
            )));
        }
        return Ok(None);
    }
    
    // Active is empty, check if new pending requirements were added
    {
        let pending_guard = self.pending_requirements.lock();
        if pending_guard.is_empty() {
            // Truly no work, reset dedicated worker
            self.dedicated_worker_id.store(u32::MAX, Ordering::Relaxed);
            return Ok(None);
        }
        // New pending requirements exist, activate them
        drop(pending_guard);
    }
    
    // Re-activate pending requirements that were added concurrently
    if self.activate_pending_requirements(statuses)? {
        self.dedicated_worker_id.store(u32::MAX, Ordering::Relaxed);
        return Ok(None);
    }
    // Loop back to check active requirements again
}
```

## Proof of Concept

The race can be demonstrated with a Rust test in the block executor that:

1. Creates a block with transactions T1, T2-T10, T11, T12-T20
2. Has T1 commit and publish modules, recording requirements for T2-T10
3. Controls the execution so T2-T10 are all in Aborted state
4. Spawns Worker 1 as dedicated worker to process requirements
5. At the exact moment Worker 1 drains pending and finds no active requirements (all Aborted)
6. Injects T11 commit with module publish from Worker 2
7. Observes Worker 1 hitting the invariant error at line 306
8. Verifies the scheduler halts and falls back to sequential execution

```rust
// Test would be added to aptos-move/block-executor/src/cold_validation.rs tests
#[test]
fn test_concurrent_record_during_empty_activation_race() {
    // Setup: 20 transactions, Worker 1 designated
    let requirements = ColdValidationRequirements::<TestRequirement>::new(20);
    let statuses = create_execution_statuses_with_txns(
        20,
        // T2-T10 are all Aborted (won't create active requirements)
        (2..10).map(|i| (i, (SchedulingStatus::Aborted, 1))).collect(),
    );
    
    // T1 publishes modules, records requirements for T2-T10
    requirements.record_requirements(1, 1, 10, BTreeSet::from([100])).unwrap();
    
    // Simulate Worker 1 starting to process
    // At the point after draining but before re-locking,
    // Worker 2 records new requirements
    
    // This should trigger the race and hit invariant error
    let result = requirements.get_validation_requirement_to_process(1, 20, &statuses);
    
    // Expect invariant error
    assert!(result.is_err());
}
```

**Notes**

The vulnerability is implementation-specific to BlockSTMv2's cold validation requirements system and does not affect BlockSTMv1. The race is timing-dependent but becomes statistically significant in production scenarios with frequent module publishing. While fallback prevents consensus violations, the performance impact warrants addressing this race condition through proper synchronization or retry logic.

### Citations

**File:** aptos-move/block-executor/src/cold_validation.rs (L208-266)
```rust
    pub(crate) fn record_requirements(
        &self,
        worker_id: u32,
        calling_txn_idx: TxnIndex,
        min_never_scheduled_idx: TxnIndex,
        requirements: BTreeSet<R>,
    ) -> Result<(), PanicError> {
        if min_never_scheduled_idx > self.num_txns || min_never_scheduled_idx <= calling_txn_idx {
            return Err(code_invariant_error(format!(
                "Invalid min_never_scheduled_idx = {} for calling_txn_idx = {} and num_txns = {}",
                min_never_scheduled_idx, calling_txn_idx, self.num_txns
            )));
        }

        if calling_txn_idx + 1 == std::cmp::min(self.num_txns, min_never_scheduled_idx) {
            // Requirements are void, since it applies to txns before min_never_scheduled_idx.
            return Ok(());
        }

        if requirements.is_empty() {
            return Err(code_invariant_error(format!(
                "Empty requirements to record for calling_txn_idx = {}",
                calling_txn_idx
            )));
        }

        let mut pending_reqs = self.pending_requirements.lock();
        pending_reqs.push(PendingRequirement {
            requirements,
            from_idx: calling_txn_idx + 1,
            to_idx: min_never_scheduled_idx,
        });

        // Updates to atomic variables while recording pending requirements occur under the
        // pending_requirements lock to ensure atomicity versus draining to activate.
        // However, for simplicity and simpler invariants, all updates (including in
        // validation_requirement_processed) are under the same lock.
        let _ = self.dedicated_worker_id.compare_exchange(
            u32::MAX,
            worker_id,
            Ordering::Relaxed,
            Ordering::Relaxed,
        );
        let prev_min_idx = self
            .min_idx_with_unprocessed_validation_requirement
            .swap(calling_txn_idx + 1, Ordering::Relaxed);
        if prev_min_idx <= calling_txn_idx {
            // Record may not be called with a calling_txn_idx higher or equal to the
            // min_from_idx, as committing calling_txn_idx is impossible before the pending
            // requirements with lower min index are processed and any (lower or equal)
            // required validations are performed.
            return Err(code_invariant_error(format!(
                "Recording validation requirements, min idx = {} <= calling_txn_idx = {}",
                prev_min_idx, calling_txn_idx
            )));
        }

        Ok(())
    }
```

**File:** aptos-move/block-executor/src/cold_validation.rs (L281-323)
```rust
    pub(crate) fn get_validation_requirement_to_process<'a>(
        &self,
        worker_id: u32,
        idx_threshold: TxnIndex,
        statuses: &ExecutionStatuses,
    ) -> Result<Option<(TxnIndex, Incarnation, ValidationRequirement<'a, R>)>, PanicError> {
        if !self.is_dedicated_worker(worker_id) {
            return Ok(None);
        }

        if self.activate_pending_requirements(statuses)? {
            self.dedicated_worker_id.store(u32::MAX, Ordering::Relaxed);
            // If the worker id was reset, the worker can early return (no longer assigned).
            return Ok(None);
        }

        // After the drain, another worker may have concurrently added pending requirements,
        // reducing the min_idx_with_unprocessed_validation_requirement (to make sure it's blocked
        // from getting committed). Hence, when obtaining an active validation requirement, the
        // index should be based on the versions map in active_requirements.
        let active_reqs = self.active_requirements.dereference();
        let (min_active_requirement_idx, (incarnation, is_executing)) =
            active_reqs.versions.first_key_value().ok_or_else(|| {
                // Should not be empty as dedicated worker was set in the beginning of the method
                // and can only be reset by the worker itself.
                code_invariant_error(
                    "Empty active requirements in get_validation_requirement_to_process",
                )
            })?;

        if *min_active_requirement_idx <= idx_threshold {
            return Ok(Some((
                *min_active_requirement_idx,
                *incarnation,
                ValidationRequirement::new(
                    self.active_requirements.dereference_mut(),
                    *is_executing,
                ),
            )));
        }

        Ok(None)
    }
```

**File:** aptos-move/block-executor/src/cold_validation.rs (L453-516)
```rust
    fn activate_pending_requirements(
        &self,
        statuses: &ExecutionStatuses,
    ) -> Result<bool, PanicError> {
        let pending_reqs = {
            let mut guard = self.pending_requirements.lock();
            if guard.is_empty() {
                // No requirements to drain.
                return Ok(false);
            }
            std::mem::take(&mut *guard)
        };

        let starting_idx = pending_reqs
            .iter()
            .map(|req| req.from_idx)
            .min()
            .expect("Expected at least one requirement");
        let ending_idx = pending_reqs
            .iter()
            .map(|req| req.to_idx)
            .max()
            .expect("Expected at least one requirement");
        if starting_idx >= ending_idx || ending_idx > self.num_txns {
            return Err(code_invariant_error(format!(
                "Invariant broken, starting idx {} >= ending idx {} or ending idx > num_txns {}",
                starting_idx, ending_idx, self.num_txns
            )));
        }

        let new_versions: BTreeMap<TxnIndex, (Incarnation, bool)> = (starting_idx..ending_idx)
            .filter_map(|txn_idx| {
                statuses
                    .requires_module_validation(txn_idx)
                    .map(|(incarnation, is_executing)| (txn_idx, (incarnation, is_executing)))
            })
            .collect();
        let new_requirements = pending_reqs
            .into_iter()
            .fold(BTreeSet::new(), |mut acc, req| {
                acc.extend(req.requirements);
                acc
            });

        let active_reqs = self.active_requirements.dereference_mut();
        active_reqs.requirements.extend(new_requirements);
        active_reqs.versions.extend(new_versions);

        if active_reqs.versions.is_empty() {
            // It is possible that the active versions map was empty, and no pending
            // requirements needed to be activated (i.e. not executing or executed).
            // In this case, we may update min_idx_with_unprocessed_validation_requirement
            // as validation_requirement_processed does so only when the pending
            // requirements are empty.
            let pending_reqs_guard = self.pending_requirements.lock();
            if pending_reqs_guard.is_empty() {
                self.min_idx_with_unprocessed_validation_requirement
                    .store(u32::MAX, Ordering::Relaxed);
                return Ok(true);
            }
        }

        Ok(false)
    }
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L793-805)
```rust
    pub(crate) fn requires_module_validation(
        &self,
        txn_idx: TxnIndex,
    ) -> Option<(Incarnation, bool)> {
        let status = &self.statuses[txn_idx as usize];
        let status_guard = status.status_with_incarnation.lock();

        match status_guard.status {
            SchedulingStatus::Executing(_) => Some((status_guard.incarnation(), true)),
            SchedulingStatus::Executed => Some((status_guard.incarnation(), false)),
            SchedulingStatus::PendingScheduling | SchedulingStatus::Aborted => None,
        }
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L1778-1799)
```rust
                    if let Err(err) = self.worker_loop_v2(
                        &executor,
                        signature_verified_block,
                        environment,
                        *worker_id,
                        num_workers,
                        &scheduler,
                        &shared_sync_params,
                    ) {
                        // If there are multiple errors, they all get logged: FatalVMError is
                        // logged at construction, below we log CodeInvariantErrors.
                        if let PanicOr::CodeInvariantError(err_msg) = err {
                            alert!(
                                "[BlockSTMv2] worker loop: CodeInvariantError({:?})",
                                err_msg
                            );
                        }
                        shared_maybe_error.store(true, Ordering::SeqCst);

                        // Make sure to halt the scheduler if it hasn't already been halted.
                        scheduler.halt();
                    }
```

**File:** aptos-move/block-executor/src/executor.rs (L1809-1841)
```rust
        let (has_error, maybe_block_epilogue_txn) = if shared_maybe_error.load(Ordering::SeqCst) {
            (true, None)
        } else {
            match self.finalize_parallel_execution(
                maybe_executor.into_inner(),
                signature_verified_block,
                !scheduler.post_commit_processing_queue_is_empty(),
                transaction_slice_metadata,
                SchedulerWrapper::V2(&scheduler, 0),
                module_cache_manager_guard.environment(),
                &shared_sync_params,
            ) {
                Ok(maybe_block_epilogue_txn) => {
                    // Update state counters & insert verified modules into cache (safe after error check).
                    counters::update_state_counters(versioned_cache.stats(), true);
                    (
                        module_cache_manager_guard
                            .module_cache_mut()
                            .insert_verified(versioned_cache.take_modules_iter())
                            .is_err(),
                        maybe_block_epilogue_txn,
                    )
                },
                Err(_) => (true, None),
            }
        };

        // Explicit async drops even when there is an error.
        DEFAULT_DROPPER.schedule_drop((last_input_output, scheduler, versioned_cache));

        if has_error {
            return Err(());
        }
```

**File:** types/src/block_executor/config.rs (L71-79)
```rust
    pub fn default_with_concurrency_level(concurrency_level: usize) -> Self {
        Self {
            blockstm_v2: false,
            concurrency_level,
            allow_fallback: true,
            discard_failed_blocks: false,
            module_cache_config: BlockExecutorModuleCacheLocalConfig::default(),
        }
    }
```
