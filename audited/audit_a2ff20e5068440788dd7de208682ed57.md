# Audit Report

## Title
Permanent Permit Consumption in BoundedExecutor Due to Never-Completing Futures

## Summary
The `BoundedExecutor` used throughout Aptos consensus components can permanently lose permits if spawned futures never complete. The buffer manager and reliable broadcast verification tasks spawn futures without timeout protection or abort mechanisms, allowing permits to be consumed indefinitely in edge cases, eventually causing total validator liveness failure.

## Finding Description

The `BoundedExecutor` in [1](#0-0)  releases semaphore permits **only** when spawned futures complete. The permit is wrapped with the future and dropped inside a `.map()` closure that executes upon future completion.

Critical consensus components spawn verification tasks on this executor without defensive mechanisms:

**Buffer Manager Verification:** [2](#0-1)  spawns commit message verification tasks. The `JoinHandle` returned by `spawn().await` is immediately dropped without being stored or awaited. No `AbortHandle` is created, and no timeout is enforced on the verification task.

**Reliable Broadcast Aggregation:** [3](#0-2)  spawns aggregation tasks on the bounded executor. While these tasks are eventually awaited, if the aggregation never completes, permits remain consumed.

**Shared Executor Across Epochs:** [4](#0-3)  creates a single `BoundedExecutor` instance at node startup with capacity from `node_config.consensus.num_bounded_executor_tasks`. This executor is cloned and shared across all consensus components and persists across epoch boundaries.

**No Cleanup at Epoch End:** [5](#0-4)  shows that `end_epoch()` resets managers but does not cancel or track spawned verification tasks. Hung tasks from previous epochs continue consuming permits indefinitely.

**Attack Path:**
1. Attacker identifies the bounded executor capacity (configured value, typically 32-64)
2. Attacker triggers edge cases that cause verification or processing tasks to hang:
   - Malformed messages triggering verification bugs
   - Race conditions in channel operations
   - Tokio runtime starvation scenarios
   - Panics caught by tokio that leave tasks "alive" but non-functional
3. Each hung task permanently consumes one permit
4. Over multiple epochs or with sustained attack, all permits are exhausted
5. New legitimate verification tasks block indefinitely waiting for permits
6. Buffer manager stops processing commit messages
7. Consensus halts - validator cannot participate in block agreement

This breaks the **Consensus Liveness** invariant (validators must be able to process consensus messages) and the **Resource Limits** invariant (system must recover from resource exhaustion).

## Impact Explanation

**High Severity** per Aptos bug bounty criteria - "Validator node slowdowns" escalating to functional liveness failure.

If all permits are exhausted:
- Buffer manager cannot verify incoming commit messages
- Reliable broadcast cannot aggregate responses  
- RandManager and SecretShareManager cannot process messages
- Validator effectively stops participating in consensus

While this requires sustained exploitation or specific triggering conditions, the impact is severe: a single affected validator loses consensus participation. If multiple validators are affected simultaneously, network liveness degrades.

Unlike temporary DoS, permit exhaustion is **permanent** until node restart, and will recur if the underlying trigger conditions persist.

## Likelihood Explanation

**Medium Likelihood**:

**Triggering Factors:**
- Bugs in verification logic causing hangs (low but non-zero probability)
- Tokio runtime edge cases under high load
- Race conditions in async task scheduling
- Channel receiver drops causing send operations to behave unexpectedly

**Exploitation Requirements:**
- Attacker needs network access to send messages to validators (standard for any peer)
- No special validator privileges required
- Attack is repeatable and cumulative across epochs

**Mitigating Factors:**
- Normal verification operations complete quickly
- Requires either a bug to trigger hanging or very specific timing/load conditions

However, the lack of ANY defensive mechanisms (timeouts, task limits per epoch, abort handles) means that even rare edge cases will eventually cause permit exhaustion.

## Recommendation

Implement defense-in-depth mechanisms to prevent permanent permit consumption:

**1. Add Timeouts to Verification Tasks:**
```rust
// In buffer_manager.rs
bounded_executor
    .spawn(async move {
        let verification = async move {
            match commit_msg.req.verify(sender, &epoch_state_clone.verifier) {
                Ok(_) => { let _ = tx.unbounded_send(commit_msg); },
                Err(e) => warn!("Invalid commit message: {}", e),
            }
        };
        
        tokio::time::timeout(
            Duration::from_secs(5), // Reasonable timeout for crypto verification
            verification
        ).await.unwrap_or_else(|_| {
            warn!("Commit verification timed out - possible attack or bug");
        })
    })
    .await;
```

**2. Store and Track JoinHandles:**
Create a task registry that tracks spawned verification tasks per epoch and cancels them during epoch transitions.

**3. Add Permit Metrics:**
Expose metrics on available permits to detect exhaustion early:
```rust
pub fn available_permits(&self) -> usize {
    self.semaphore.available_permits()
}
```

**4. Implement Task Cancellation:**
Use `AbortHandle` pattern (already imported in buffer_manager.rs) to cancel pending verification tasks when they become irrelevant (e.g., during epoch changes or resets).

**5. Consider Per-Epoch Executors:**
Instead of sharing one executor across epochs, create new executors per epoch to automatically clean up hung tasks during epoch transitions.

## Proof of Concept

```rust
// Rust test demonstrating permit leak
#[tokio::test]
async fn test_permit_leak_on_hanging_future() {
    use aptos_bounded_executor::BoundedExecutor;
    use tokio::runtime::Runtime;
    use std::sync::Arc;
    use std::sync::atomic::{AtomicUsize, Ordering};
    
    let rt = Runtime::new().unwrap();
    let executor = BoundedExecutor::new(5, rt.handle().clone());
    let counter = Arc::new(AtomicUsize::new(0));
    
    // Spawn 5 futures that never complete
    for _ in 0..5 {
        let c = counter.clone();
        executor.spawn(async move {
            c.fetch_add(1, Ordering::SeqCst);
            // Simulate hanging - future never completes
            futures::future::pending::<()>().await;
        }).await;
    }
    
    // Wait for all tasks to be spawned
    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
    assert_eq!(counter.load(Ordering::SeqCst), 5);
    
    // Try to spawn a 6th task - this will hang forever waiting for a permit
    let timeout_result = tokio::time::timeout(
        tokio::time::Duration::from_secs(2),
        executor.spawn(async { println!("This will never execute"); })
    ).await;
    
    // Demonstrates permit exhaustion - timeout occurs because no permit available
    assert!(timeout_result.is_err(), "Should timeout - all permits consumed by hanging futures");
}
```

This test proves that hung futures permanently consume permits, blocking all subsequent spawn attempts indefinitely.

### Citations

**File:** crates/bounded-executor/src/executor.rs (L100-109)
```rust
fn future_with_permit<F>(future: F, permit: OwnedSemaphorePermit) -> impl Future<Output = F::Output>
where
    F: Future + Send + 'static,
    F::Output: Send + 'static,
{
    future.map(move |ret| {
        drop(permit);
        ret
    })
}
```

**File:** consensus/src/pipeline/buffer_manager.rs (L923-932)
```rust
                bounded_executor
                    .spawn(async move {
                        match commit_msg.req.verify(sender, &epoch_state_clone.verifier) {
                            Ok(_) => {
                                let _ = tx.unbounded_send(commit_msg);
                            },
                            Err(e) => warn!("Invalid commit message: {}", e),
                        }
                    })
                    .await;
```

**File:** crates/reliable-broadcast/src/lib.rs (L171-181)
```rust
                        let future = executor.spawn(async move {
                            (
                                    receiver,
                                    result
                                        .and_then(|msg| {
                                            msg.try_into().map_err(|e| anyhow::anyhow!("{:?}", e))
                                        })
                                        .and_then(|ack| aggregating.add(receiver, ack)),
                            )
                        }).await;
                        aggregate_futures.push(future);
```

**File:** consensus/src/consensus_provider.rs (L81-84)
```rust
    let bounded_executor = BoundedExecutor::new(
        node_config.consensus.num_bounded_executor_tasks as usize,
        runtime.handle().clone(),
    );
```

**File:** consensus/src/pipeline/execution_client.rs (L711-760)
```rust
    async fn end_epoch(&self) {
        let (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        ) = {
            let mut handle = self.handle.write();
            handle.reset()
        };

        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }

        if let Some(mut tx) = reset_tx_to_buffer_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop buffer manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop buffer manager");
        }
        self.execution_proxy.end_epoch();
    }
```
