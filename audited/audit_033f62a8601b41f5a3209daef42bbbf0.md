# Audit Report

## Title
AptosDB Lacks Application-Level File Locking - Reliance on RocksDB LOCK File Insufficient in Non-Standard Deployments

## Summary
AptosDB::open() does not implement application-level exclusive file locking beyond RocksDB's internal LOCK file mechanism. In environments where fcntl-based file locking is unreliable (NFS, certain containers, or after manual LOCK file deletion), multiple processes can simultaneously open the database in write mode, leading to RocksDB corruption and permanent blockchain state loss.

## Finding Description

AptosDB relies entirely on RocksDB's internal LOCK file mechanism to prevent concurrent write access from multiple processes. This is evident in the database opening flow: [1](#0-0) 

The `open()` function calls `open_internal()`, which ultimately opens RocksDB instances: [2](#0-1) 

These databases are opened through the schemadb wrapper: [3](#0-2) 

Which directly calls RocksDB's native open functions: [4](#0-3) 

**Critical Gap**: No application-level file locking (fs2::FileExt, flock, etc.) is implemented. The codebase DOES have file locking patterns in the Move package cache: [5](#0-4) 

But this pattern is **not used** for AptosDB.

**Internal Locks Are Process-Scoped**: AptosDB has Mutex locks to prevent concurrent commits within a process: [6](#0-5) [7](#0-6) 

These locks explicitly state they only detect concurrent calls (within the same process) and do **not** provide cross-process protection.

**Vulnerability Trigger**: The debugging tool in `db_debugger/watch/opened.rs` demonstrates the risk: [8](#0-7) 

This tool opens the database with `readonly=false` and holds it open indefinitely. If RocksDB's LOCK file fails (NFS mount, deleted lock, container misconfiguration), a validator node could also open the same database, causing:
1. Concurrent writes to RocksDB from multiple processes (undefined behavior per RocksDB docs)
2. Corruption of SST files, WAL logs, and MANIFEST
3. Jellyfish Merkle tree corruption
4. Non-recoverable blockchain state requiring full re-sync or hardfork

**Broken Invariant**: State Consistency - "State transitions must be atomic and verifiable via Merkle proofs" is violated when database corruption occurs.

## Impact Explanation

**Critical Severity** per Aptos Bug Bounty criteria:
- **Non-recoverable network partition (requires hardfork)**: Database corruption can render validators unable to sync, potentially requiring emergency intervention
- **Permanent freezing of funds (requires hardfork)**: Corrupted state could make funds inaccessible until state is restored

The severity depends on deployment environment:
- **NFS/Network Filesystems**: HIGH likelihood - fcntl locks frequently fail on NFS
- **Standard Local SSD**: LOW likelihood - RocksDB's LOCK file works correctly
- **Containerized Environments**: MEDIUM likelihood - depends on configuration

However, when it occurs, the impact is catastrophic - complete loss of blockchain state integrity.

## Likelihood Explanation

**Moderate to High** in production environments using:
- Network-attached storage (NFS, CIFS, cloud-mounted volumes)
- Docker/Kubernetes with certain volume configurations
- Shared hosting or multi-tenant environments
- Operational errors (accidental deletion of LOCK file during maintenance)

**Low** in standard validator deployments on dedicated hardware with local SSDs.

The lack of defense-in-depth is concerning - RocksDB's LOCK file is a single point of failure with no backup mechanism.

## Recommendation

Implement application-level file locking in `AptosDB::open_internal()` using the same pattern as the Move package cache:

```rust
// In storage/aptosdb/src/db/aptosdb_internal.rs
use fs2::FileExt;
use std::fs::File;

impl AptosDB {
    pub(super) fn open_internal(
        db_paths: &StorageDirPaths,
        readonly: bool,
        // ... other params
    ) -> Result<Self> {
        // Acquire application-level exclusive lock
        let lock_file_path = db_paths.default_root_path().join("APTOSDB.lock");
        let lock_file = if !readonly {
            let f = File::create(&lock_file_path)?;
            f.try_lock_exclusive().map_err(|e| {
                AptosDbError::Other(format!(
                    "Failed to acquire exclusive lock on database directory. \
                     Another process may be using this database. Error: {}", e
                ))
            })?;
            Some(f)
        } else {
            None
        };

        // Existing code continues...
        // Store lock_file in AptosDB struct to maintain lock until drop
    }
}
```

Store the lock file handle in `AptosDB` struct to maintain the lock for the lifetime of the database instance.

Additionally, add validation in deployment documentation warning against NFS/network filesystems for database directories.

## Proof of Concept

```rust
// Test demonstrating concurrent database opening vulnerability
// Requires NFS mount or manual LOCK file deletion to reproduce

use aptos_config::config::{RocksdbConfigs, StorageDirPaths};
use aptos_temppath::TempPath;
use std::process::{Command, Stdio};
use std::thread;
use std::time::Duration;

#[test]
#[ignore] // Requires special filesystem setup
fn test_concurrent_db_open_corruption() {
    let db_dir = TempPath::new();
    
    // Process 1: Open database and hold it
    let db_path = db_dir.path().to_path_buf();
    let handle1 = thread::spawn(move || {
        let _db = AptosDB::open(
            StorageDirPaths::from_path(db_path),
            false, // readonly=false
            Default::default(),
            RocksdbConfigs::default(),
            false,
            1000,
            16,
            None,
            Default::default(),
        ).expect("First open should succeed");
        
        thread::sleep(Duration::from_secs(60)); // Hold database open
    });
    
    thread::sleep(Duration::from_secs(1));
    
    // Simulate LOCK file deletion or NFS lock failure
    let lock_path = db_dir.path().join("db").join("ledger_db").join("LOCK");
    std::fs::remove_file(&lock_path).ok(); // Force unlock
    
    // Process 2: Attempt concurrent open (should fail but might succeed on NFS)
    let db2 = AptosDB::open(
        StorageDirPaths::from_path(db_dir.path()),
        false, // readonly=false
        Default::default(),
        RocksdbConfigs::default(),
        false,
        1000,
        16,
        None,
        Default::default(),
    );
    
    // On proper filesystems: db2 is Err
    // On NFS or after LOCK deletion: db2 is Ok -> CORRUPTION RISK
    assert!(db2.is_err(), "Concurrent write access should be prevented");
    
    handle1.join().unwrap();
}
```

### Citations

**File:** storage/aptosdb/src/db/mod.rs (L34-37)
```rust
    /// This is just to detect concurrent calls to `pre_commit_ledger()`
    pre_commit_lock: std::sync::Mutex<()>,
    /// This is just to detect concurrent calls to `commit_ledger()`
    commit_lock: std::sync::Mutex<()>,
```

**File:** storage/aptosdb/src/db/mod.rs (L57-80)
```rust
    pub fn open(
        db_paths: StorageDirPaths,
        readonly: bool,
        pruner_config: PrunerConfig,
        rocksdb_configs: RocksdbConfigs,
        enable_indexer: bool,
        buffered_state_target_items: usize,
        max_num_nodes_per_lru_cache_shard: usize,
        internal_indexer_db: Option<InternalIndexerDB>,
        hot_state_config: HotStateConfig,
    ) -> Result<Self> {
        Self::open_internal(
            &db_paths,
            readonly,
            pruner_config,
            rocksdb_configs,
            enable_indexer,
            buffered_state_target_items,
            max_num_nodes_per_lru_cache_shard,
            false,
            internal_indexer_db,
            hot_state_config,
        )
    }
```

**File:** storage/aptosdb/src/db/mod.rs (L106-156)
```rust
    pub fn open_dbs(
        db_paths: &StorageDirPaths,
        rocksdb_configs: RocksdbConfigs,
        env: Option<&Env>,
        block_cache: Option<&Cache>,
        readonly: bool,
        max_num_nodes_per_lru_cache_shard: usize,
        reset_hot_state: bool,
    ) -> Result<(LedgerDb, Option<StateMerkleDb>, StateMerkleDb, StateKvDb)> {
        let ledger_db = LedgerDb::new(
            db_paths.ledger_db_root_path(),
            rocksdb_configs,
            env,
            block_cache,
            readonly,
        )?;
        let state_kv_db = StateKvDb::new(
            db_paths,
            rocksdb_configs,
            env,
            block_cache,
            readonly,
            ledger_db.metadata_db_arc(),
        )?;
        let hot_state_merkle_db = if !readonly && rocksdb_configs.enable_storage_sharding {
            Some(StateMerkleDb::new(
                db_paths,
                rocksdb_configs,
                env,
                block_cache,
                readonly,
                max_num_nodes_per_lru_cache_shard,
                /* is_hot = */ true,
                reset_hot_state,
            )?)
        } else {
            None
        };
        let state_merkle_db = StateMerkleDb::new(
            db_paths,
            rocksdb_configs,
            env,
            block_cache,
            readonly,
            max_num_nodes_per_lru_cache_shard,
            /* is_hot = */ false,
            /* delete_on_restart = */ false,
        )?;

        Ok((ledger_db, hot_state_merkle_db, state_merkle_db, state_kv_db))
    }
```

**File:** storage/schemadb/src/lib.rs (L80-87)
```rust
    pub fn open_cf(
        db_opts: &Options,
        path: impl AsRef<Path>,
        name: &str,
        cfds: Vec<ColumnFamilyDescriptor>,
    ) -> DbResult<DB> {
        Self::open_cf_impl(db_opts, path, name, cfds, OpenMode::ReadWrite)
    }
```

**File:** storage/schemadb/src/lib.rs (L168-190)
```rust
        let inner = {
            use rocksdb::DB;
            use OpenMode::*;

            match open_mode {
                ReadWrite => DB::open_cf_descriptors(db_opts, path.de_unc(), all_cfds),
                ReadOnly => {
                    DB::open_cf_descriptors_read_only(
                        db_opts,
                        path.de_unc(),
                        all_cfds.filter(|cfd| !missing_cfs.contains(cfd.name())),
                        false, /* error_if_log_file_exist */
                    )
                },
                Secondary(secondary_path) => DB::open_cf_descriptors_as_secondary(
                    db_opts,
                    path.de_unc(),
                    secondary_path,
                    all_cfds,
                ),
            }
        }
        .into_db_res()?;
```

**File:** third_party/move/tools/move-package-cache/src/file_lock.rs (L15-66)
```rust
/// A file-based lock to ensure exclusive access to certain resources.
///
/// This is used by the package cache to ensure only one process can mutate a cached repo, checkout,
/// or on-chain package at a time.
pub struct FileLock {
    file: Option<File>,
    path: PathBuf,
}

impl FileLock {
    /// Attempts to acquire an exclusive `FileLock`, with an optional alert callback.
    ///
    /// If the lock cannot be acquired within `alert_timeout`, the `alert_on_wait` callback
    /// is executed to notify the caller.
    pub async fn lock_with_alert_on_wait<P, F>(
        lock_path: P,
        alert_timeout: Duration,
        alert_on_wait: F,
    ) -> Result<Self>
    where
        P: AsRef<Path>,
        F: FnOnce(),
    {
        let lock_path = lock_path.as_ref().to_owned();

        let lock_fut = {
            let lock_path = lock_path.clone();

            task::spawn_blocking(move || -> Result<File> {
                let lock_file = File::create(&lock_path)?;
                lock_file.lock_exclusive()?;
                Ok(lock_file)
            })
        };

        let timeout = tokio::time::sleep(alert_timeout).fuse();

        pin!(lock_fut, timeout);

        let lock_file = select! {
            _ = &mut timeout => {
                alert_on_wait();
                lock_fut.await??
            },
            res = &mut lock_fut => res??,
        };

        Ok(Self {
            file: Some(lock_file),
            path: lock_path,
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L44-53)
```rust
    fn pre_commit_ledger(&self, chunk: ChunkToCommit, sync_commit: bool) -> Result<()> {
        gauged_api("pre_commit_ledger", || {
            // Pre-committing and committing in concurrency is allowed but not pre-committing at the
            // same time from multiple threads, the same for committing.
            // Consensus and state sync must hand over to each other after all pending execution and
            // committing complete.
            let _lock = self
                .pre_commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
```

**File:** storage/aptosdb/src/db_debugger/watch/opened.rs (L21-46)
```rust
    pub fn run(self) -> Result<()> {
        let mut config = StorageConfig::default();
        config.set_data_dir(self.db_dir);
        config.rocksdb_configs.enable_storage_sharding =
            self.sharding_config.enable_storage_sharding;
        config.hot_state_config.delete_on_restart = false;

        let _db = AptosDB::open(
            config.get_dir_paths(),
            false, /* readonly */
            config.storage_pruner_config,
            config.rocksdb_configs,
            config.enable_indexer,
            config.buffered_state_target_items,
            config.max_num_nodes_per_lru_cache_shard,
            None,
            config.hot_state_config,
        )
        .expect("Failed to open AptosDB");

        println!("AptosDB opened. Kill to exit.");

        loop {
            std::thread::sleep(std::time::Duration::from_secs(1));
        }
    }
```
