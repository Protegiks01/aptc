# Audit Report

## Title
Storage Service Request Moderator Rate Limiting Bypass via Connection Reset

## Summary

The storage service request moderator's rate limiting mechanism can be completely bypassed by malicious Public Full Node (PFN) peers through disconnection and reconnection. When a peer accumulates too many invalid requests and gets marked as "ignored," they can disconnect from the network, causing their unhealthy state to be garbage collected within ~1 second, then reconnect with a clean slate to repeat the attack indefinitely.

## Finding Description

The storage service implements rate limiting to protect against abusive peers sending invalid requests. [1](#0-0) 

When a peer sends requests that cannot be serviced (e.g., requesting data beyond the available version range), the request moderator tracks invalid requests in an `UnhealthyPeerState` structure. [2](#0-1) 

Once a Public network peer reaches the configured limit (default 500 invalid requests), they are marked as "ignored" and subsequent requests return `TooManyInvalidRequests` errors without processing. [3](#0-2) 

However, the system implements automatic garbage collection of unhealthy peer states for disconnected peers. [4](#0-3) 

This garbage collection runs every 1 second by default. [5](#0-4) 

The garbage collection is triggered by a periodic refresh task. [6](#0-5) 

**Attack Flow:**
1. Attacker connects to a validator/fullnode's storage service on the Public network
2. Sends 500 invalid storage requests (e.g., requesting non-existent transaction versions)
3. Gets marked as "ignored" by the request moderator
4. Disconnects from the network
5. Waits ~1 second for the `refresh_unhealthy_peer_states()` to run and garbage collect their entry
6. Reconnects with a fresh `UnhealthyPeerState` (invalid_request_count = 0)
7. Repeats steps 2-6 indefinitely

The test suite explicitly confirms this behavior as a "feature" for cleaning up disconnected peer state. [7](#0-6) 

## Impact Explanation

This vulnerability falls under **High Severity** according to Aptos bug bounty criteria:

1. **Validator node slowdowns**: Each invalid request consumes CPU for validation against the storage server summary, memory for metrics tracking, and network bandwidth. By bypassing rate limiting, an attacker can sustain continuous resource consumption on validator and fullnode storage services.

2. **Significant protocol violations**: The rate limiting mechanism is a critical security control explicitly designed to protect nodes from abusive peers. Bypassing this control violates the protocol's intended security guarantees.

3. **State synchronization impact**: The storage service is critical for state synchronization between nodes. Degrading its performance can impact the network's ability to onboard new nodes or help existing nodes catch up after downtime.

The vulnerability specifically targets Public network peers, which is the attack surface where untrusted external actors connect, making this the primary vector for exploitation.

## Likelihood Explanation

**Likelihood: High**

- **Attack Complexity**: Trivial - requires only the ability to connect/disconnect and send storage service requests
- **Attacker Requirements**: Any peer on the Public network can exploit this
- **No Special Privileges**: Does not require validator access, staking, or any trusted role
- **Easily Automated**: The attack can be scripted and run continuously
- **No Cost Barrier**: Connection establishment has minimal cost beyond network connectivity
- **Detection Difficulty**: The attack appears as normal peer churn (disconnect/reconnect), making it hard to distinguish from legitimate behavior

The only protection is the `inbound_connection_limit` which limits concurrent unknown connections, but this doesn't prevent a single attacker from cycling through the exploit pattern. [8](#0-7) 

## Recommendation

**Option 1: Persistent Identity-Based Tracking (Recommended)**

Track rate limiting state based on `PeerId` (the peer's cryptographic identity) rather than connection state. Maintain unhealthy peer states across disconnections with time-based expiration rather than connection-based cleanup.

**Option 2: Exponential Backoff on Reconnection**

After a peer is marked as ignored, track this in a separate persistent structure and apply increasingly long connection cooldown periods for peers that repeatedly trigger rate limiting.

**Option 3: Connection-Level Rate Limiting**

Implement connection establishment rate limiting per `PeerId` to make rapid reconnection costly.

**Recommended Fix:**

Modify the garbage collection logic to only remove unhealthy peer states after the full ignore period has elapsed, regardless of connection state: [4](#0-3) 

Replace the connection-based cleanup with time-based expiration that respects the ignore duration plus a grace period, ensuring that disconnecting doesn't reset the rate limiting state prematurely.

## Proof of Concept

```rust
// Test demonstrating the vulnerability
#[tokio::test]
async fn test_rate_limiting_bypass_via_reconnection() {
    use aptos_config::{config::StorageServiceConfig, network_id::{NetworkId, PeerNetworkId}};
    use aptos_types::PeerId;
    
    // Create a storage service with low invalid request threshold for testing
    let max_invalid_requests_per_peer = 5;
    let storage_service_config = StorageServiceConfig {
        max_invalid_requests_per_peer,
        request_moderator_refresh_interval_ms: 100, // Fast refresh for testing
        ..Default::default()
    };
    
    // Setup storage client and server
    let (mut mock_client, mut service, _, time_service, peers_and_metadata) =
        MockClient::new(None, Some(storage_service_config));
    
    let peer_network_id = PeerNetworkId::new(NetworkId::Public, PeerId::random());
    
    // Connect the peer
    peers_and_metadata
        .insert_connection_metadata(
            peer_network_id,
            create_connection_metadata(peer_network_id.peer_id(), 0),
        )
        .unwrap();
    
    tokio::spawn(service.start());
    
    // Exploit the vulnerability multiple times
    for iteration in 0..3 {
        println!("Iteration {}: Sending {} invalid requests", iteration, max_invalid_requests_per_peer);
        
        // Send invalid requests up to the limit
        for _ in 0..max_invalid_requests_per_peer {
            send_invalid_request(&mut mock_client, peer_network_id).await.unwrap_err();
        }
        
        // Verify peer is now ignored
        let response = send_invalid_request(&mut mock_client, peer_network_id).await;
        assert!(matches!(response.unwrap_err(), StorageServiceError::TooManyInvalidRequests(_)));
        
        // EXPLOIT: Disconnect and wait for garbage collection
        peers_and_metadata
            .update_connection_state(peer_network_id, ConnectionState::Disconnecting)
            .unwrap();
        
        // Wait for moderator refresh to garbage collect
        time_service.advance_ms_async(200).await;
        tokio::time::sleep(Duration::from_millis(100)).await;
        
        // Reconnect with fresh state
        peers_and_metadata
            .update_connection_state(peer_network_id, ConnectionState::Connected)
            .unwrap();
        
        println!("Successfully bypassed rate limiting - can send another {} requests", max_invalid_requests_per_peer);
    }
    
    // Attacker successfully bypassed rate limiting 3 times in succession
    // In production, this could be repeated indefinitely to cause sustained resource exhaustion
}
```

This PoC demonstrates that an attacker can bypass rate limiting by simply disconnecting and reconnecting, allowing them to send batches of invalid requests indefinitely without respecting the intended 5-minute ignore period.

### Citations

**File:** state-sync/storage-service/server/src/handler.rs (L212-213)
```rust
        self.request_moderator
            .validate_request(peer_network_id, request)?;
```

**File:** state-sync/storage-service/server/src/moderator.rs (L50-69)
```rust
    pub fn increment_invalid_request_count(&mut self, peer_network_id: &PeerNetworkId) {
        // Increment the invalid request count
        self.invalid_request_count += 1;

        // If the peer is a PFN and has sent too many invalid requests, start ignoring it
        if self.ignore_start_time.is_none()
            && peer_network_id.network_id().is_public_network()
            && self.invalid_request_count >= self.max_invalid_requests
        {
            // TODO: at some point we'll want to terminate the connection entirely

            // Start ignoring the peer
            self.ignore_start_time = Some(self.time_service.now());

            // Log the fact that we're now ignoring the peer
            warn!(LogSchema::new(LogEntry::RequestModeratorIgnoredPeer)
                .peer_network_id(peer_network_id)
                .message("Ignoring peer due to too many invalid requests!"));
        }
    }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L213-228)
```rust
        self.unhealthy_peer_states
            .retain(|peer_network_id, unhealthy_peer_state| {
                if connected_peers_and_metadata.contains_key(peer_network_id) {
                    // Refresh the ignored peer state
                    unhealthy_peer_state.refresh_peer_state(peer_network_id);

                    // If the peer is ignored, increment the ignored peer count
                    if unhealthy_peer_state.is_ignored() {
                        num_ignored_peers += 1;
                    }

                    true // The peer is still connected, so we should keep it
                } else {
                    false // The peer is no longer connected, so we should remove it
                }
            });
```

**File:** config/src/config/state_sync_config.rs (L201-201)
```rust
            max_invalid_requests_per_peer: 500,
```

**File:** config/src/config/state_sync_config.rs (L214-214)
```rust
            request_moderator_refresh_interval_ms: 1000, // 1 second
```

**File:** state-sync/storage-service/server/src/lib.rs (L356-381)
```rust
    async fn spawn_moderator_peer_refresher(&mut self) {
        // Clone all required components for the task
        let config = self.storage_service_config;
        let request_moderator = self.request_moderator.clone();
        let time_service = self.time_service.clone();

        // Spawn the task
        self.runtime.spawn(async move {
            // Create a ticker for the refresh interval
            let duration = Duration::from_millis(config.request_moderator_refresh_interval_ms);
            let ticker = time_service.interval(duration);
            futures::pin_mut!(ticker);

            // Periodically refresh the peer states
            loop {
                ticker.next().await;

                // Refresh the unhealthy peer states
                if let Err(error) = request_moderator.refresh_unhealthy_peer_states() {
                    error!(LogSchema::new(LogEntry::RequestModeratorRefresh)
                        .error(&error)
                        .message("Failed to refresh the request moderator!"));
                }
            }
        });
    }
```

**File:** state-sync/storage-service/server/src/tests/request_moderator.rs (L209-354)
```rust
#[tokio::test]
async fn test_request_moderator_peer_garbage_collect() {
    // Create test data
    let highest_synced_version = 500;
    let highest_synced_epoch = 3;

    // Create a storage service config for testing
    let max_invalid_requests_per_peer = 3;
    let storage_service_config = StorageServiceConfig {
        max_invalid_requests_per_peer,
        ..Default::default()
    };

    // Create the storage client and server
    let (mut mock_client, mut service, _, time_service, peers_and_metadata) =
        MockClient::new(None, Some(storage_service_config));
    utils::update_storage_server_summary(
        &mut service,
        highest_synced_version,
        highest_synced_epoch,
    );

    // Get the request moderator and unhealthy peer states
    let request_moderator = service.get_request_moderator();
    let unhealthy_peer_states = request_moderator.get_unhealthy_peer_states();

    // Connect multiple peers
    let peer_network_ids = [
        PeerNetworkId::new(NetworkId::Validator, PeerId::random()),
        PeerNetworkId::new(NetworkId::Vfn, PeerId::random()),
        PeerNetworkId::new(NetworkId::Public, PeerId::random()),
    ];
    for (index, peer_network_id) in peer_network_ids.iter().enumerate() {
        peers_and_metadata
            .insert_connection_metadata(
                *peer_network_id,
                create_connection_metadata(peer_network_id.peer_id(), index as u32),
            )
            .unwrap();
    }

    // Spawn the server
    tokio::spawn(service.start());

    // Send an invalid request from the first two peers
    for peer_network_id in peer_network_ids.iter().take(2) {
        // Send the invalid request
        send_invalid_transaction_request(
            highest_synced_version,
            &mut mock_client,
            *peer_network_id,
        )
        .await
        .unwrap_err();

        // Verify the peer is now tracked as unhealthy
        assert!(unhealthy_peer_states.contains_key(peer_network_id));
    }

    // Verify that only the first two peers are being tracked
    assert_eq!(unhealthy_peer_states.len(), 2);

    // Disconnect the first peer
    peers_and_metadata
        .update_connection_state(peer_network_ids[0], ConnectionState::Disconnecting)
        .unwrap();

    // Elapse enough time for the peer monitor loop to garbage collect the peer
    wait_for_request_moderator_to_garbage_collect(
        unhealthy_peer_states.clone(),
        &time_service,
        &peer_network_ids[0],
    )
    .await;

    // Verify that only the second peer is being tracked
    assert_eq!(unhealthy_peer_states.len(), 1);

    // Disconnect the second peer
    peers_and_metadata
        .remove_peer_metadata(peer_network_ids[1], ConnectionId::from(1))
        .unwrap();

    // Elapse enough time for the peer monitor loop to garbage collect the peer
    wait_for_request_moderator_to_garbage_collect(
        unhealthy_peer_states.clone(),
        &time_service,
        &peer_network_ids[1],
    )
    .await;

    // Verify that no peer is being tracked
    assert!(unhealthy_peer_states.is_empty());

    // Reconnect the first peer
    peers_and_metadata
        .update_connection_state(peer_network_ids[0], ConnectionState::Connected)
        .unwrap();

    // Send an invalid request from the first peer
    send_invalid_transaction_request(
        highest_synced_version,
        &mut mock_client,
        peer_network_ids[0],
    )
    .await
    .unwrap_err();

    // Verify the peer is now tracked as unhealthy
    assert!(unhealthy_peer_states.contains_key(&peer_network_ids[0]));

    // Process enough invalid requests to ignore the third peer
    for _ in 0..max_invalid_requests_per_peer {
        send_invalid_transaction_request(
            highest_synced_version,
            &mut mock_client,
            peer_network_ids[2],
        )
        .await
        .unwrap_err();
    }

    // Verify the third peer is now tracked and blocked
    assert_eq!(unhealthy_peer_states.len(), 2);
    assert!(unhealthy_peer_states
        .get(&peer_network_ids[2])
        .unwrap()
        .is_ignored());

    // Disconnect the third peer
    peers_and_metadata
        .remove_peer_metadata(peer_network_ids[2], ConnectionId::from(2))
        .unwrap();

    // Elapse enough time for the peer monitor loop to garbage collect the peer
    wait_for_request_moderator_to_garbage_collect(
        unhealthy_peer_states.clone(),
        &time_service,
        &peer_network_ids[2],
    )
    .await;

    // Verify that the peer is no longer being tracked
    assert!(!unhealthy_peer_states.contains_key(&peer_network_ids[2]));
    assert_eq!(unhealthy_peer_states.len(), 1);
}
```

**File:** network/framework/src/peer_manager/mod.rs (L372-387)
```rust
                if !self
                    .active_peers
                    .contains_key(&conn.metadata.remote_peer_id)
                    && unknown_inbound_conns + 1 > self.inbound_connection_limit
                {
                    info!(
                        NetworkSchema::new(&self.network_context)
                            .connection_metadata_with_address(&conn.metadata),
                        "{} Connection rejected due to connection limit: {}",
                        self.network_context,
                        conn.metadata
                    );
                    counters::connections_rejected(&self.network_context, conn.metadata.origin)
                        .inc();
                    self.disconnect(conn);
                    return;
```
