# Audit Report

## Title
Unsorted Load Balancing Thresholds Cause Incorrect Mempool Peer Selection in PFN Nodes

## Summary
The `MempoolConfig` allows users to provide custom `load_balancing_thresholds` via YAML configuration without validation. The peer selection algorithm in `priority.rs` assumes these thresholds are sorted in ascending order by `avg_mempool_traffic_threshold_in_tps`. When unsorted thresholds are provided, the algorithm selects incorrect threshold configurations, leading to suboptimal peer selection with wrong numbers of upstream peers and incorrect latency parameters for PFN (Public Full Node) mempool transaction broadcasting.

## Finding Description

The default `load_balancing_thresholds` configuration is properly sorted in ascending order (500, 1000, 1500, 2500, 3500, 4500 TPS). [1](#0-0) 

However, the configuration struct uses serde deserialization allowing users to provide custom configurations via YAML files. [2](#0-1) 

The critical vulnerability exists in the `ConfigSanitizer` implementation which performs NO validation of the threshold ordering, as indicated by the TODO comment. [3](#0-2) 

The peer selection algorithm in `update_sender_bucket_for_peers` reverses the threshold iterator and finds the first match, explicitly assuming sorted order. [4](#0-3) 

**Attack Scenario:**

A node operator (or attacker with configuration access) provides unsorted thresholds:
```yaml
mempool:
  load_balancing_thresholds:
    - avg_mempool_traffic_threshold_in_tps: 2500
      latency_slack_between_top_upstream_peers: 100
      max_number_of_upstream_peers: 5
    - avg_mempool_traffic_threshold_in_tps: 500
      latency_slack_between_top_upstream_peers: 50
      max_number_of_upstream_peers: 2
    - avg_mempool_traffic_threshold_in_tps: 1500
      latency_slack_between_top_upstream_peers: 75
      max_number_of_upstream_peers: 4
```

When mempool traffic is 1200 TPS:
1. Algorithm reverses to: [1500, 500, 2500]
2. Searches for first threshold where `threshold <= 1200`
3. Finds 500 TPS threshold (incorrect - should be 1000 from sorted defaults)
4. Uses `max_number_of_upstream_peers: 2` instead of correct value
5. Uses `latency_slack_between_top_upstream_peers: 50ms` instead of correct value

This causes the PFN node to:
- Select wrong number of upstream FN peers for broadcasting
- Use incorrect latency parameters for peer filtering
- Experience suboptimal transaction propagation behavior

## Impact Explanation

This is a **Medium severity** vulnerability per Aptos bug bounty criteria. It causes:

1. **State Inconsistencies**: The mempool peer selection state becomes inconsistent with intended load balancing behavior, requiring operator intervention to diagnose and fix the misconfiguration.

2. **Network Performance Degradation**: PFN nodes with misconfigured thresholds will:
   - Send transactions to suboptimal number of upstream peers
   - Use incorrect latency thresholds for peer selection
   - Experience transaction broadcast delays or failures
   - Contribute to overall network inefficiency

3. **Operational Impact**: Node operators may experience unexplained performance issues that are difficult to diagnose without understanding this bug.

While this does not directly cause consensus violations or fund loss, it affects critical mempool infrastructure and can require intervention to resolve, fitting the Medium severity criteria of "state inconsistencies requiring intervention."

## Likelihood Explanation

**High Likelihood** of occurrence:

1. **Easy to Trigger**: Any node operator can modify configuration files. No special privileges required beyond normal node operation.

2. **Accidental Misconfiguration**: Operators editing threshold configurations may inadvertently reorder entries without realizing the sorting requirement is undocumented.

3. **No Validation**: The missing validation in `sanitize()` means misconfigured nodes will start successfully with silent failures in load balancing behavior.

4. **Difficult to Detect**: The bug causes suboptimal behavior rather than crashes, making it hard to identify without detailed performance analysis.

5. **Configuration as Attack Vector**: Attackers who compromise node configuration files can deliberately introduce this misconfiguration to degrade network performance.

## Recommendation

Implement validation in the `ConfigSanitizer` for `MempoolConfig` to ensure `load_balancing_thresholds` are sorted in ascending order by `avg_mempool_traffic_threshold_in_tps`:

```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        
        // Validate that load_balancing_thresholds are sorted in ascending order
        for window in self.load_balancing_thresholds.windows(2) {
            if window[0].avg_mempool_traffic_threshold_in_tps 
                >= window[1].avg_mempool_traffic_threshold_in_tps {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    format!(
                        "load_balancing_thresholds must be sorted in ascending order by avg_mempool_traffic_threshold_in_tps. Found {} >= {}",
                        window[0].avg_mempool_traffic_threshold_in_tps,
                        window[1].avg_mempool_traffic_threshold_in_tps
                    ),
                ));
            }
        }
        
        Ok(())
    }
}
```

This follows the validation pattern used in other config sanitizers. [5](#0-4) 

## Proof of Concept

Add this test to `config/src/config/mempool_config.rs`:

```rust
#[test]
fn test_unsorted_load_balancing_thresholds_rejected() {
    use crate::config::{NodeConfig, ConfigSanitizer, NodeType};
    use aptos_types::chain_id::ChainId;
    
    let mut node_config = NodeConfig::default();
    
    // Create unsorted thresholds
    node_config.mempool.load_balancing_thresholds = vec![
        LoadBalancingThresholdConfig {
            avg_mempool_traffic_threshold_in_tps: 2500,
            latency_slack_between_top_upstream_peers: 100,
            max_number_of_upstream_peers: 5,
        },
        LoadBalancingThresholdConfig {
            avg_mempool_traffic_threshold_in_tps: 500,
            latency_slack_between_top_upstream_peers: 50,
            max_number_of_upstream_peers: 2,
        },
    ];
    
    // Sanitization should fail
    let result = MempoolConfig::sanitize(
        &node_config,
        NodeType::PublicFullnode,
        Some(ChainId::testnet()),
    );
    
    assert!(result.is_err());
    assert!(result.unwrap_err().to_string().contains("sorted"));
}

#[test]
fn test_sorted_load_balancing_thresholds_accepted() {
    use crate::config::{NodeConfig, ConfigSanitizer, NodeType};
    use aptos_types::chain_id::ChainId;
    
    let node_config = NodeConfig::default(); // Uses default sorted thresholds
    
    // Sanitization should succeed
    let result = MempoolConfig::sanitize(
        &node_config,
        NodeType::PublicFullnode,
        Some(ChainId::testnet()),
    );
    
    assert!(result.is_ok());
}
```

## Notes

This vulnerability is specific to PFN (Public Full Node) mempool load balancing configuration. The comment in the configuration file indicates this feature is "used only by PFNs." [6](#0-5) 

The TODO comment in the sanitize function explicitly acknowledges that reasonable verifications need to be added, confirming this is a known gap in validation. [7](#0-6) 

Other configuration sanitizers in the codebase demonstrate proper validation patterns that should be applied here to prevent misconfigurations that can degrade network performance.

### Citations

**File:** config/src/config/mempool_config.rs (L39-41)
```rust
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct MempoolConfig {
```

**File:** config/src/config/mempool_config.rs (L98-99)
```rust
    /// Load balancing configuration for the mempool. This is used only by PFNs.
    pub load_balancing_thresholds: Vec<LoadBalancingThresholdConfig>,
```

**File:** config/src/config/mempool_config.rs (L138-169)
```rust
            load_balancing_thresholds: vec![
                LoadBalancingThresholdConfig {
                    avg_mempool_traffic_threshold_in_tps: 500,
                    latency_slack_between_top_upstream_peers: 50,
                    max_number_of_upstream_peers: 2,
                },
                LoadBalancingThresholdConfig {
                    avg_mempool_traffic_threshold_in_tps: 1000,
                    latency_slack_between_top_upstream_peers: 50,
                    max_number_of_upstream_peers: 3,
                },
                LoadBalancingThresholdConfig {
                    avg_mempool_traffic_threshold_in_tps: 1500,
                    latency_slack_between_top_upstream_peers: 75,
                    max_number_of_upstream_peers: 4,
                },
                LoadBalancingThresholdConfig {
                    avg_mempool_traffic_threshold_in_tps: 2500,
                    latency_slack_between_top_upstream_peers: 100,
                    max_number_of_upstream_peers: 5,
                },
                LoadBalancingThresholdConfig {
                    avg_mempool_traffic_threshold_in_tps: 3500,
                    latency_slack_between_top_upstream_peers: 125,
                    max_number_of_upstream_peers: 6,
                },
                LoadBalancingThresholdConfig {
                    avg_mempool_traffic_threshold_in_tps: 4500,
                    latency_slack_between_top_upstream_peers: 150,
                    max_number_of_upstream_peers: 7,
                },
            ],
```

**File:** config/src/config/mempool_config.rs (L176-184)
```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        Ok(()) // TODO: add reasonable verifications
    }
}
```

**File:** mempool/src/shared_mempool/priority.rs (L303-317)
```rust
        // Obtain the highest threshold from mempool_config.load_balancing_thresholds for which avg_mempool_traffic_threshold_in_tps exceeds average_mempool_traffic_observed
        let threshold_config = self
            .mempool_config
            .load_balancing_thresholds
            .clone()
            .into_iter()
            .rev()
            .find(|threshold_config| {
                threshold_config.avg_mempool_traffic_threshold_in_tps
                    <= max(
                        average_mempool_traffic_observed as u64,
                        average_committed_traffic_observed as u64,
                    )
            })
            .unwrap_or_default();
```

**File:** config/src/config/consensus_config.rs (L442-500)
```rust
    fn sanitize_batch_block_limits(
        sanitizer_name: &str,
        config: &ConsensusConfig,
    ) -> Result<(), Error> {
        // Note, we are strict here: receiver batch limits <= sender block limits
        let mut recv_batch_send_block_pairs = vec![
            (
                config.quorum_store.receiver_max_batch_txns as u64,
                config.max_sending_block_txns,
                "QS recv batch txns < max_sending_block_txns".to_string(),
            ),
            (
                config.quorum_store.receiver_max_batch_txns as u64,
                config.max_sending_block_txns_after_filtering,
                "QS recv batch txns < max_sending_block_txns_after_filtering ".to_string(),
            ),
            (
                config.quorum_store.receiver_max_batch_txns as u64,
                config.min_max_txns_in_block_after_filtering_from_backpressure,
                "QS recv batch txns < min_max_txns_in_block_after_filtering_from_backpressure"
                    .to_string(),
            ),
            (
                config.quorum_store.receiver_max_batch_bytes as u64,
                config.max_sending_block_bytes,
                "QS recv batch bytes < max_sending_block_bytes".to_string(),
            ),
        ];
        for backpressure_values in &config.pipeline_backpressure {
            recv_batch_send_block_pairs.push((
                config.quorum_store.receiver_max_batch_bytes as u64,
                backpressure_values.max_sending_block_bytes_override,
                format!(
                    "backpressure {} ms: QS recv batch bytes < max_sending_block_bytes_override",
                    backpressure_values.back_pressure_pipeline_latency_limit_ms,
                ),
            ));
        }
        for backoff_values in &config.chain_health_backoff {
            recv_batch_send_block_pairs.push((
                config.quorum_store.receiver_max_batch_bytes as u64,
                backoff_values.max_sending_block_bytes_override,
                format!(
                    "backoff {} %: bytes: QS recv batch bytes < max_sending_block_bytes_override",
                    backoff_values.backoff_if_below_participating_voting_power_percentage,
                ),
            ));
        }

        for (batch, block, label) in &recv_batch_send_block_pairs {
            if *batch > *block {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name.to_owned(),
                    format!("Failed {}: {} > {}", label, *batch, *block),
                ));
            }
        }
        Ok(())
    }
```
