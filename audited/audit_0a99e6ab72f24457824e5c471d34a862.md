# Audit Report

## Title
Inspection Service Outlives Node Shutdown Leading to Information Leak

## Summary
The node inspection service spawns a detached thread with its own runtime that is not tracked by the `AptosHandle` lifecycle manager. When the Aptos node shuts down gracefully, all managed services terminate, but the inspection service continues running indefinitely, exposing sensitive node information including network topology, peer states, metrics, and configuration to unauthorized parties.

## Finding Description

The vulnerability exists in the inspection service initialization flow: [1](#0-0) 

The `start_node_inspection_service()` function returns void (`()`), providing no handle to the spawned service. The underlying implementation spawns an untracked thread: [2](#0-1) 

The inspection service creates a tokio runtime on a detached thread that blocks on a hyper HTTP server. This thread is never joined or terminated during node shutdown.

During node initialization, the inspection service is started but not added to the `AptosHandle`: [3](#0-2) 

The `AptosHandle` struct manages all critical runtimes for proper lifecycle management: [4](#0-3) 

Notice there is no `_inspection_service_runtime` field. When the node shuts down and `AptosHandle` is dropped, all tracked runtimes terminate gracefully, but the inspection service thread continues executing.

The inspection service exposes sensitive operational data through multiple endpoints:

**Configuration Endpoint:** [5](#0-4) 

**Peer Information Endpoint:** [6](#0-5) 

This endpoint reveals network topology, peer IDs, connection states, trusted peer sets, state sync metadata, and internal client states.

**Attack Scenario:**
1. Node operator initiates graceful shutdown (maintenance, upgrade, or migration)
2. All major services shut down via `AptosHandle` Drop implementation
3. Inspection service thread remains running with stale but revealing data
4. Attacker queries inspection endpoints to:
   - Determine actual node state (shutdown vs. network unreachable)
   - Extract network topology and validator set information
   - Obtain peer connection metadata and states
   - Gather intelligence for future attacks

## Impact Explanation

This qualifies as **Low Severity** per Aptos bug bounty criteria: "Minor information leaks." The vulnerability enables:

- **Information Disclosure**: Post-shutdown exposure of network topology, peer relationships, and node configuration
- **Reconnaissance Value**: Attackers can distinguish between genuinely shutdown nodes and temporarily unreachable nodes
- **Stale Data Exposure**: Metrics and peer states remain accessible even though underlying services have terminated

The impact is limited because:
- No direct consensus, fund loss, or availability compromise
- No remote code execution or system takeover
- Many deployments firewall the inspection service or disable sensitive endpoints
- Requires network access to inspection service port

However, the information leak persists indefinitely until process termination, providing a window for intelligence gathering.

## Likelihood Explanation

**Likelihood: Medium**

This occurs deterministically on every graceful node shutdown event, which happens regularly during:
- Scheduled maintenance windows
- Software upgrades and rollouts
- Node migrations
- Configuration changes requiring restart

The exploit requires:
- Network accessibility to inspection service port (often port 9101)
- Knowledge that inspection service is enabled
- Timing coincidence with shutdown window

Many production deployments may mitigate this through firewall rules or by disabling the inspection service entirely, reducing real-world exploitation likelihood.

## Recommendation

Track the inspection service runtime in `AptosHandle` to ensure proper lifecycle management:

**In `crates/aptos-inspection-service/src/server/mod.rs`:**
```rust
// Return the runtime instead of spawning a detached thread
pub fn start_inspection_service(
    node_config: NodeConfig,
    aptos_data_client: AptosDataClient,
    peers_and_metadata: Arc<PeersAndMetadata>,
) -> Runtime {
    let service_port = node_config.inspection_service.port;
    let service_address = node_config.inspection_service.address.clone();
    let address: SocketAddr = (service_address.as_str(), service_port)
        .to_socket_addrs()
        .unwrap_or_else(|_| panic!("Failed to parse {}:{} as address", service_address, service_port))
        .next()
        .unwrap();

    let runtime = aptos_runtimes::spawn_named_runtime("inspection".into(), None);
    
    let make_service = make_service_fn(move |_conn| {
        let node_config = node_config.clone();
        let aptos_data_client = aptos_data_client.clone();
        let peers_and_metadata = peers_and_metadata.clone();
        async move {
            Ok::<_, Infallible>(service_fn(move |request| {
                serve_requests(request, node_config.clone(), aptos_data_client.clone(), peers_and_metadata.clone())
            }))
        }
    });

    runtime.spawn(async move {
        let server = Server::bind(&address).serve(make_service);
        if let Err(e) = server.await {
            eprintln!("Inspection service error: {}", e);
        }
    });

    runtime
}
```

**In `aptos-node/src/services.rs`:**
```rust
pub fn start_node_inspection_service(
    node_config: &NodeConfig,
    aptos_data_client: AptosDataClient,
    peers_and_metadata: Arc<PeersAndMetadata>,
) -> Runtime {
    aptos_inspection_service::start_inspection_service(
        node_config.clone(),
        aptos_data_client,
        peers_and_metadata,
    )
}
```

**In `aptos-node/src/lib.rs`:**
- Add `_inspection_service_runtime: Runtime` field to `AptosHandle` struct
- Capture the returned runtime when calling `start_node_inspection_service()`
- Include it in the `AptosHandle` construction

This ensures the inspection service runtime is properly dropped and shut down when the node terminates.

## Proof of Concept

**Rust Reproduction Steps:**

1. Start an Aptos node with inspection service enabled
2. Query `http://localhost:9101/peer_information` to verify service is running
3. Initiate node shutdown (send SIGTERM or call shutdown logic)
4. Wait for main node services to terminate (check logs for "Shutting down")
5. Query `http://localhost:9101/peer_information` again
6. Observe that the endpoint still responds with peer information despite node shutdown
7. Query `http://localhost:9101/metrics` to confirm prometheus metrics are still exposed
8. Service continues responding until process is forcefully killed (SIGKILL)

**Test Validation:**
```bash
# Terminal 1: Start node
cargo run --bin aptos-node -- --config node_config.yaml

# Terminal 2: Verify service running
curl http://localhost:9101/peer_information

# Terminal 1: Gracefully shutdown (Ctrl+C or SIGTERM)
# Wait for shutdown logs...

# Terminal 2: Service still responds
curl http://localhost:9101/peer_information  # Returns data
curl http://localhost:9101/metrics          # Returns metrics

# Service continues until process killed
```

The inspection service thread remains responsive post-shutdown because it's not tracked by `AptosHandle` and the detached thread continues executing independently.

---

**Notes:**
This is a valid Low severity information leak vulnerability as defined in the Aptos bug bounty program. While it doesn't pose immediate consensus or fund loss risks, it violates proper resource lifecycle management and enables reconnaissance attacks against supposedly shutdown nodes.

### Citations

**File:** aptos-node/src/services.rs (L212-222)
```rust
pub fn start_node_inspection_service(
    node_config: &NodeConfig,
    aptos_data_client: AptosDataClient,
    peers_and_metadata: Arc<PeersAndMetadata>,
) {
    aptos_inspection_service::start_inspection_service(
        node_config.clone(),
        aptos_data_client,
        peers_and_metadata,
    )
}
```

**File:** crates/aptos-inspection-service/src/server/mod.rs (L50-101)
```rust
pub fn start_inspection_service(
    node_config: NodeConfig,
    aptos_data_client: AptosDataClient,
    peers_and_metadata: Arc<PeersAndMetadata>,
) {
    // Fetch the service port and address
    let service_port = node_config.inspection_service.port;
    let service_address = node_config.inspection_service.address.clone();

    // Create the inspection service socket address
    let address: SocketAddr = (service_address.as_str(), service_port)
        .to_socket_addrs()
        .unwrap_or_else(|_| {
            panic!(
                "Failed to parse {}:{} as address",
                service_address, service_port
            )
        })
        .next()
        .unwrap();

    // Create a runtime for the inspection service
    let runtime = aptos_runtimes::spawn_named_runtime("inspection".into(), None);

    // Spawn the inspection service
    thread::spawn(move || {
        // Create the service function that handles the endpoint requests
        let make_service = make_service_fn(move |_conn| {
            let node_config = node_config.clone();
            let aptos_data_client = aptos_data_client.clone();
            let peers_and_metadata = peers_and_metadata.clone();
            async move {
                Ok::<_, Infallible>(service_fn(move |request| {
                    serve_requests(
                        request,
                        node_config.clone(),
                        aptos_data_client.clone(),
                        peers_and_metadata.clone(),
                    )
                }))
            }
        });

        // Start and block on the server
        runtime
            .block_on(async {
                let server = Server::bind(&address).serve(make_service);
                server.await
            })
            .unwrap();
    });
}
```

**File:** aptos-node/src/lib.rs (L197-215)
```rust
pub struct AptosHandle {
    _admin_service: AdminService,
    _api_runtime: Option<Runtime>,
    _backup_runtime: Option<Runtime>,
    _consensus_observer_runtime: Option<Runtime>,
    _consensus_publisher_runtime: Option<Runtime>,
    _consensus_runtime: Option<Runtime>,
    _dkg_runtime: Option<Runtime>,
    _indexer_grpc_runtime: Option<Runtime>,
    _indexer_runtime: Option<Runtime>,
    _indexer_table_info_runtime: Option<Runtime>,
    _jwk_consensus_runtime: Option<Runtime>,
    _mempool_runtime: Runtime,
    _network_runtimes: Vec<Runtime>,
    _peer_monitoring_service_runtime: Runtime,
    _state_sync_runtimes: StateSyncRuntimes,
    _telemetry_runtime: Option<Runtime>,
    _indexer_db_runtime: Option<Runtime>,
}
```

**File:** aptos-node/src/lib.rs (L772-776)
```rust
    services::start_node_inspection_service(
        &node_config,
        aptos_data_client,
        peers_and_metadata.clone(),
    );
```

**File:** crates/aptos-inspection-service/src/server/configuration.rs (L13-29)
```rust
pub fn handle_configuration_request(node_config: &NodeConfig) -> (StatusCode, Body, String) {
    // Only return configuration if the endpoint is enabled
    let (status_code, body) = if node_config.inspection_service.expose_configuration {
        // We format the configuration using debug formatting. This is important to
        // prevent secret/private keys from being serialized and leaked (i.e.,
        // all secret keys are marked with SilentDisplay and SilentDebug).
        let encoded_configuration = format!("{:?}", node_config);
        (StatusCode::OK, Body::from(encoded_configuration))
    } else {
        (
            StatusCode::FORBIDDEN,
            Body::from(CONFIGURATION_DISABLED_MESSAGE),
        )
    };

    (status_code, body, CONTENT_TYPE_TEXT.into())
}
```

**File:** crates/aptos-inspection-service/src/server/peer_information.rs (L21-106)
```rust
pub fn handle_peer_information_request(
    node_config: &NodeConfig,
    aptos_data_client: AptosDataClient,
    peers_and_metadata: Arc<PeersAndMetadata>,
) -> (StatusCode, Body, String) {
    // Only return peer information if the endpoint is enabled
    let (status_code, body) = if node_config.inspection_service.expose_peer_information {
        let peer_information = get_peer_information(aptos_data_client, peers_and_metadata);
        (StatusCode::OK, Body::from(peer_information))
    } else {
        (
            StatusCode::FORBIDDEN,
            Body::from(PEER_INFO_DISABLED_MESSAGE),
        )
    };

    (status_code, body, CONTENT_TYPE_TEXT.into())
}

/// Returns a simple text formatted string with peer and network information
fn get_peer_information(
    aptos_data_client: AptosDataClient,
    peers_and_metadata: Arc<PeersAndMetadata>,
) -> String {
    // Get all registered networks
    let registered_networks: Vec<NetworkId> =
        peers_and_metadata.get_registered_networks().collect();

    // Get all peers (sorted by peer ID)
    let mut all_peers = peers_and_metadata.get_all_peers();
    all_peers.sort();

    // Display a summary of all peers and networks
    let mut peer_information_output = Vec::<String>::new();
    display_peer_information_summary(
        &mut peer_information_output,
        &all_peers,
        &registered_networks,
    );
    peer_information_output.push("\n".into());

    // Display connection metadata for each peer
    display_peer_connection_metadata(
        &mut peer_information_output,
        &all_peers,
        peers_and_metadata.deref(),
    );
    peer_information_output.push("\n".into());

    // Display the entire set of trusted peers
    display_trusted_peers(
        &mut peer_information_output,
        registered_networks,
        peers_and_metadata.deref(),
    );
    peer_information_output.push("\n".into());

    // Display basic peer metadata for each peer
    display_peer_monitoring_metadata(
        &mut peer_information_output,
        &all_peers,
        peers_and_metadata.deref(),
    );
    peer_information_output.push("\n".into());

    // Display state sync metadata for each peer
    display_state_sync_metadata(&mut peer_information_output, &all_peers, aptos_data_client);
    peer_information_output.push("\n".into());

    // Display detailed peer metadata for each peer
    display_detailed_monitoring_metadata(
        &mut peer_information_output,
        &all_peers,
        peers_and_metadata.deref(),
    );
    peer_information_output.push("\n".into());

    // Display the internal client state for each peer
    display_internal_client_state(
        &mut peer_information_output,
        &all_peers,
        peers_and_metadata.deref(),
    );

    peer_information_output.join("\n") // Separate each entry with a newline to construct the output
}
```
