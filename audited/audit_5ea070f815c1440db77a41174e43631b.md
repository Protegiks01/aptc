# Audit Report

## Title
Snapshot Directory Cleanup Failure Causes Disk Space Exhaustion and Indexer Service DoS

## Summary
The indexer-grpc-table-info service contains a critical flaw in snapshot cleanup logic where filesystem errors during snapshot directory deletion cause a panic, terminating the backup service task. This prevents future snapshot cleanups, leading to accumulating snapshots that exhaust disk space and ultimately cause indexer service failure.

## Finding Description

The vulnerability exists in the snapshot backup and cleanup flow across two files: [1](#0-0) [2](#0-1) 

**Attack Flow:**

1. **Snapshot Creation**: Every epoch, a new snapshot directory is created via `snapshot_indexer_async_v2()`. [3](#0-2) 

2. **Backup Loop**: A separate tokio task runs continuously checking for snapshots to backup every 5 seconds. [4](#0-3) 

3. **Cleanup with Panic**: When a snapshot is successfully uploaded to GCS, the cleanup uses `.expect()` which panics if filesystem operations fail. [2](#0-1) 

4. **Task Termination**: The panic propagates through the outer `.expect()` call. [5](#0-4) 

5. **Service Degradation**: The backup task terminates permanently. New snapshots continue being created each epoch but are never cleaned up, as there are only two cleanup points in the entire codebase. [6](#0-5) 

**Failure Scenarios:**
- Permission errors (directory/file locked by another process)
- I/O errors (disk errors, filesystem corruption)
- Network filesystem timeouts
- Insufficient permissions after configuration changes
- Race conditions with concurrent access

## Impact Explanation

This qualifies as **Medium Severity** under Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: Once the backup task terminates, operator intervention is required to restart the service and manually clean up accumulated snapshots
- **Service Availability**: Indexer service becomes unavailable when disk space is exhausted, impacting the entire indexer infrastructure that downstream applications depend on
- **Resource Exhaustion**: Violates the "Resource Limits" invariant - the service fails to respect storage limits and allows unbounded disk consumption

While this doesn't directly impact consensus or funds, indexer availability is critical for ecosystem functionality, affecting dApp queries, explorers, and analytics platforms.

## Likelihood Explanation

**High Likelihood** - This will eventually occur in production environments:

1. **Common Triggers**: Filesystem errors occur regularly in production (I/O timeouts, permission changes, disk issues)
2. **No Recovery Mechanism**: The `.expect()` panic provides no recovery path
3. **Continuous Operation**: With epochs occurring regularly, the probability of encountering a filesystem error over time approaches certainty
4. **Silent Failure**: The backup task terminates silently without alerting operators until disk space is exhausted

The vulnerability is particularly concerning because the TODO comment at line 599 suggests the team is aware of concurrency issues but hasn't addressed the fundamental error handling problem.

## Recommendation

Replace panic-based error handling with proper error propagation and recovery:

**Fix for `gcs.rs` (lines 242-245):**
```rust
// Replace .expect() with proper error handling
if let Err(e) = fs::remove_file(&tar_file).await {
    error!("Failed to remove tar file {:?}: {}", tar_file, e);
}
if let Err(e) = fs::remove_dir_all(snapshot_path_clone).await {
    error!("Failed to remove snapshot directory {:?}: {}. Manual cleanup may be required.", 
           snapshot_path_clone, e);
    // Continue operation - don't panic
}
```

**Fix for `table_info_service.rs` (lines 600-603):**
```rust
// Replace .expect() with error logging
match backup_restore_operator
    .backup_db_snapshot_and_update_metadata(ledger_chain_id as u64, epoch, snapshot_dir.clone())
    .await
{
    Ok(_) => {
        info!(backup_epoch = epoch, "[Table Info] Snapshot backed up successfully");
    },
    Err(e) => {
        error!(epoch = epoch, error = ?e, "[Table Info] Failed to backup snapshot. Will retry on next iteration.");
        // Don't panic - allow retry in next loop iteration
    }
}
```

**Additional improvements:**
1. Implement exponential backoff for cleanup retries
2. Add metrics/alerts for cleanup failures
3. Implement a separate cleanup task that periodically scans for orphaned snapshots
4. Add disk space monitoring and alerting

## Proof of Concept

```rust
// Reproduction scenario for the vulnerability

use std::fs;
use std::path::PathBuf;
use std::os::unix::fs::PermissionsExt;

#[tokio::test]
async fn test_snapshot_cleanup_panic() {
    // 1. Create a snapshot directory
    let snapshot_dir = PathBuf::from("/tmp/test_snapshot");
    fs::create_dir_all(&snapshot_dir).unwrap();
    
    // 2. Create a file inside and make it read-only
    let protected_file = snapshot_dir.join("protected.db");
    fs::write(&protected_file, b"data").unwrap();
    let mut perms = fs::metadata(&protected_file).unwrap().permissions();
    perms.set_mode(0o000); // No permissions
    fs::set_permissions(&protected_file, perms).unwrap();
    
    // 3. Attempt cleanup (simulates the vulnerable code path)
    let result = fs::remove_dir_all(&snapshot_dir);
    
    // 4. Verify it fails
    assert!(result.is_err(), "Should fail due to permission error");
    
    // 5. In production code, this would panic with .expect()
    // result.expect("Failed to clean up after db snapshot upload");
    // ☝️ This panic terminates the backup task permanently
    
    // Cleanup for test
    fs::set_permissions(&protected_file, fs::Permissions::from_mode(0o644)).unwrap();
    fs::remove_dir_all(&snapshot_dir).unwrap();
}

// Simulation of accumulating snapshots over time:
#[tokio::test] 
async fn test_snapshot_accumulation() {
    let data_dir = PathBuf::from("/tmp/test_data");
    fs::create_dir_all(&data_dir).unwrap();
    
    // Simulate 100 epochs where cleanup fails
    for epoch in 0..100 {
        let snapshot_dir = data_dir.join(format!("snapshot_chain_1_epoch_{}", epoch));
        fs::create_dir_all(&snapshot_dir).unwrap();
        // Create 100MB of data per snapshot
        let data_file = snapshot_dir.join("data.db");
        fs::write(&data_file, vec![0u8; 100_000_000]).unwrap();
    }
    
    // Calculate accumulated size
    let total_size: u64 = fs::read_dir(&data_dir).unwrap()
        .filter_map(|e| e.ok())
        .filter_map(|e| e.metadata().ok())
        .map(|m| m.len())
        .sum();
    
    // With 100 epochs at ~100MB each, that's ~10GB of accumulated snapshots
    assert!(total_size > 5_000_000_000, "Should accumulate significant disk space");
    
    // Cleanup
    fs::remove_dir_all(&data_dir).unwrap();
}
```

**Notes:**
- This vulnerability requires no attacker interaction - it will naturally occur due to filesystem errors
- The current implementation prioritizes panic on error over graceful degradation
- The backup task runs in a spawned tokio task, so its panic won't crash the main service, but permanently disables backup/cleanup functionality
- Accumulated snapshots will grow at a rate of approximately one full database snapshot per epoch until disk exhaustion occurs

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L86-99)
```rust
                let _task = tokio::spawn(async move {
                    loop {
                        aptos_logger::info!("[Table Info] Checking for snapshots to backup.");
                        Self::backup_snapshot_if_present(
                            context.clone(),
                            backup_restore_operator.clone(),
                        )
                        .await;
                        tokio::time::sleep(Duration::from_secs(
                            TABLE_INFO_SNAPSHOT_CHECK_INTERVAL_IN_SECS,
                        ))
                        .await;
                    }
                });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L426-433)
```rust
        let snapshot_dir = context
            .node_config
            .get_data_dir()
            .join(snapshot_folder_name(chain_id as u64, epoch));
        // rocksdb will create a checkpoint to take a snapshot of full db and then save it to snapshot_path
        indexer_async_v2
            .create_checkpoint(&snapshot_dir)
            .context(format!("DB checkpoint failed at epoch {}", epoch))?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L531-611)
```rust
async fn backup_the_snapshot_and_cleanup(
    context: Arc<ApiContext>,
    backup_restore_operator: Arc<GcsBackupRestoreOperator>,
    epoch: u64,
) {
    let snapshot_folder_name = snapshot_folder_name(context.chain_id().id() as u64, epoch);
    aptos_logger::info!(
        epoch = epoch,
        snapshot_folder_name = snapshot_folder_name,
        "[Table Info] Backing up the snapshot and cleaning up the old snapshot."
    );
    let ledger_chain_id = context.chain_id().id();
    // Validate the runtime.
    let backup_metadata = backup_restore_operator.get_metadata().await;
    if let Some(metadata) = backup_metadata {
        if metadata.chain_id != (ledger_chain_id as u64) {
            panic!(
                "Table Info backup chain id does not match with current network. Expected: {}, found in backup: {}",
                context.chain_id().id(),
                metadata.chain_id
            );
        }
    } else {
        aptos_logger::warn!(
            epoch = epoch,
            snapshot_folder_name = snapshot_folder_name,
            "[Table Info] No backup metadata found. Skipping the backup."
        );
    }

    let start_time = std::time::Instant::now();
    // temporary path to store the snapshot
    let snapshot_dir = context
        .node_config
        .get_data_dir()
        .join(snapshot_folder_name.clone());
    // If the backup is for old epoch, clean up and return.
    if let Some(metadata) = backup_metadata {
        aptos_logger::info!(
            epoch = epoch,
            metadata_epoch = metadata.epoch,
            snapshot_folder_name = snapshot_folder_name,
            snapshot_dir = snapshot_dir.to_str(),
            "[Table Info] Checking the metadata before backup."
        );
        if metadata.epoch >= epoch {
            aptos_logger::info!(
                epoch = epoch,
                snapshot_folder_name = snapshot_folder_name,
                "[Table Info] Snapshot already backed up. Skipping the backup."
            );
            // Remove the snapshot directory.
            std::fs::remove_dir_all(snapshot_dir).unwrap();
            return;
        }
    } else {
        aptos_logger::warn!(
            epoch = epoch,
            snapshot_folder_name = snapshot_folder_name,
            "[Table Info] No backup metadata found."
        );
    }
    aptos_logger::info!(
        epoch = epoch,
        snapshot_folder_name = snapshot_folder_name,
        snapshot_dir = snapshot_dir.to_str(),
        "[Table Info] Backing up the snapshot."
    );
    // TODO: add checks to handle concurrent backup jobs.
    backup_restore_operator
        .backup_db_snapshot_and_update_metadata(ledger_chain_id as u64, epoch, snapshot_dir.clone())
        .await
        .expect("Failed to upload snapshot in table info service");

    // TODO: use log_grpc_step to log the backup step.
    info!(
        backup_epoch = epoch,
        backup_millis = start_time.elapsed().as_millis(),
        "[Table Info] Table info db backed up successfully"
    );
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/backup_restore/gcs.rs (L242-245)
```rust
                fs::remove_file(&tar_file)
                    .and_then(|_| fs::remove_dir_all(snapshot_path_clone))
                    .await
                    .expect("Failed to clean up after db snapshot upload");
```
