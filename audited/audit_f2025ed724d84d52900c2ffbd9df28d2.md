# Audit Report

## Title
Pipeline Backpressure Level Oscillation Attack: Lack of Hysteresis Enables Consensus Instability via Block Size Manipulation

## Summary
The pipeline backpressure mechanism in Aptos consensus lacks hysteresis in its 7-level threshold system, allowing attackers to trigger oscillations between backpressure levels by manipulating execution timing. This causes dramatic block size changes (up to 44% swings between 1800 and 1000 transactions) that destabilize consensus, degrade throughput, and create validator disagreement on appropriate block sizes.

## Finding Description

The Aptos consensus implements a 7-level pipeline backpressure mechanism to dynamically adjust block sizes based on execution pipeline latency. [1](#0-0) 

The backpressure levels are selected based on `pipeline_pending_latency`, which measures how long the oldest uncommitted block has been in the execution pipeline. [2](#0-1) 

**Critical Flaw #1: No Hysteresis**

The backpressure level selection uses a simple threshold comparison without hysteresis. [3](#0-2) 

The selection logic uses `range(..(pipeline_pending_latency)).last()`, meaning the exact same threshold applies for both increasing and decreasing transitions. For example:
- At 2500ms latency: Level 3 (1800 txns)
- At 2501ms latency: Level 4 (1000 txns)

This 1ms difference causes a 44% block size change with no buffer zone to prevent oscillations.

**Critical Flaw #2: Local Timing Calculations**

Each validator calculates `pipeline_pending_latency` independently using local wall-clock time (`Instant::now()`). [4](#0-3) 

The pipeline insertion time is set when blocks enter each validator's local pipeline. [5](#0-4) 

This means different validators will calculate different `pipeline_pending_latency` values for the same round, leading to inconsistent backpressure level decisions across the network.

**Critical Flaw #3: No Validation of Backpressure Levels**

When validators receive proposals, they only validate against static receiving limits (10,000 txns, 6MB), not the dynamically calculated backpressure-adjusted limits. [6](#0-5) 

This allows proposers to use any block size within the wide receiving limits without other validators being able to verify whether the correct backpressure level was applied.

**Attack Mechanism:**

1. **Phase 1**: Attacker monitors pipeline latency hovering near a threshold (e.g., ~2500ms)
2. **Phase 2**: When latency is below threshold, attacker submits gas-expensive transactions (within individual gas limits)
3. **Phase 3**: Execution slows down, pipeline latency crosses threshold (e.g., 2499ms → 2501ms)
4. **Phase 4**: Next proposer applies stricter backpressure level (1800 → 1000 txns)
5. **Phase 5**: Smaller blocks cause faster execution, latency decreases back below threshold
6. **Phase 6**: Next proposer reverts to less strict backpressure level (1000 → 1800 txns)
7. **Repeat**: The feedback loop sustains oscillations

**Feedback Loop Amplification:**

The vulnerability creates a self-sustaining feedback loop:
- Large blocks (1800 txns) → Slower execution → Higher latency → Next proposer uses small blocks
- Small blocks (1000 txns) → Faster execution → Lower latency → Next proposer uses large blocks

This oscillation is amplified by:
- Natural timing jitter across validators
- The large magnitude of block size changes (44% between levels 3 and 4)
- Absence of smoothing or dampening mechanisms

## Impact Explanation

**High Severity** per Aptos Bug Bounty criteria:

1. **Validator Node Slowdowns**: Constant recalculation of backpressure levels and oscillating block sizes reduce validator efficiency. Each level transition requires reconfiguring payload pull parameters and execution limits.

2. **Significant Protocol Violations**: The protocol assumes validators will converge on similar block sizes for efficient operation. Oscillations violate this assumption, causing:
   - Inconsistent validator behavior across rounds
   - Suboptimal resource utilization
   - Degraded consensus performance

3. **Consensus Instability**: While not a complete safety violation (blocks are still valid), the rapid block size changes reduce consensus efficiency and increase the risk of timing-related issues during high load.

4. **Throughput Degradation**: Oscillating between 1800 and 1000 txn blocks means the network operates at reduced average throughput compared to stable operation. During low-block phases, more rounds are needed to process the same transaction volume.

5. **Increased Transaction Latency**: Users experience higher and more variable transaction confirmation times as the network oscillates between efficient and inefficient operating points.

The attack requires minimal resources (just submitting valid transactions with high gas usage) and is difficult to detect or mitigate without protocol changes.

## Likelihood Explanation

**High Likelihood**:

1. **Low Attack Barrier**: Any transaction sender can submit gas-expensive transactions without special privileges. The attacker only needs to:
   - Monitor public metrics/logs to observe current pipeline latency
   - Submit compute-intensive transactions (within gas limits) to slow execution
   - Time submissions to maintain oscillations

2. **Natural Occurrence**: Even without malicious intent, normal load variations can cause oscillations when the network operates near threshold boundaries. The lack of hysteresis means any natural jitter around 2500ms, 3500ms, or 4500ms will trigger level transitions.

3. **Difficult to Distinguish**: The attack traffic looks like legitimate transaction load. Expensive transactions (e.g., complex DeFi operations, NFT minting with heavy computation) are valid and paid for via gas fees.

4. **No Built-in Mitigations**: The codebase contains no:
   - Hysteresis bands around thresholds
   - Smoothing or averaging of pipeline latency measurements
   - Rate limiting on level transitions
   - Validation that proposers used appropriate backpressure levels

5. **Multiple Attack Windows**: Seven threshold boundaries provide multiple opportunities for oscillation attacks at different latency ranges.

## Recommendation

Implement a multi-layered mitigation strategy:

**1. Add Hysteresis to Level Transitions**

Modify `PipelineBackpressureConfig::get_backoff` to use different thresholds for upward vs. downward transitions:

```rust
pub struct PipelineBackpressureConfig {
    backoffs: BTreeMap<Round, PipelineBackpressureValues>,
    execution: Option<ExecutionBackpressureConfig>,
    // Add hysteresis state
    current_level: Mutex<Option<u64>>,
    hysteresis_margin_ms: u64, // e.g., 200ms
}

pub fn get_backoff(
    &self,
    pipeline_pending_latency: Duration,
) -> Option<&PipelineBackpressureValues> {
    if self.backoffs.is_empty() {
        return None;
    }
    
    let latency_ms = pipeline_pending_latency.as_millis() as u64;
    let mut current = self.current_level.lock();
    
    // Apply hysteresis: different thresholds for increasing vs decreasing
    let effective_latency = if let Some(cur_threshold) = *current {
        if latency_ms > cur_threshold {
            // Increasing: use actual latency
            latency_ms
        } else {
            // Decreasing: require drop below (threshold - hysteresis)
            if latency_ms < cur_threshold.saturating_sub(self.hysteresis_margin_ms) {
                latency_ms
            } else {
                // Stay at current level
                cur_threshold
            }
        }
    } else {
        latency_ms
    };
    
    let result = self.backoffs
        .range(..effective_latency)
        .last()
        .map(|(threshold, v)| {
            *current = Some(*threshold);
            v
        });
    
    result
}
```

**2. Add Exponential Moving Average Smoothing**

Instead of using instantaneous `pipeline_pending_latency`, apply exponential smoothing:

```rust
pub struct PipelineBackpressureConfig {
    // ... existing fields ...
    smoothed_latency: Mutex<Option<Duration>>,
    smoothing_alpha: f64, // e.g., 0.3 for moderate smoothing
}

pub fn get_backoff_with_smoothing(
    &self,
    pipeline_pending_latency: Duration,
) -> Option<&PipelineBackpressureValues> {
    let mut smoothed = self.smoothed_latency.lock();
    
    let effective_latency = if let Some(prev) = *smoothed {
        // EMA: smoothed = alpha * new + (1 - alpha) * previous
        let new_ms = pipeline_pending_latency.as_millis() as f64;
        let prev_ms = prev.as_millis() as f64;
        let smoothed_ms = self.smoothing_alpha * new_ms + (1.0 - self.smoothing_alpha) * prev_ms;
        Duration::from_millis(smoothed_ms as u64)
    } else {
        pipeline_pending_latency
    };
    
    *smoothed = Some(effective_latency);
    self.get_backoff(effective_latency)
}
```

**3. Add Rate Limiting on Level Transitions**

Prevent rapid oscillations by limiting how frequently backpressure levels can change:

```rust
pub struct PipelineBackpressureConfig {
    // ... existing fields ...
    last_transition_time: Mutex<Instant>,
    min_transition_interval: Duration, // e.g., 5 seconds
}

// Only allow level change if sufficient time has passed since last transition
```

**4. Use Consensus-Agreed Timing** (Long-term Fix)

Replace local `Instant::now()` with block timestamps for pipeline timing calculations, ensuring all validators calculate identical backpressure levels. This requires protocol changes but provides the most robust solution.

## Proof of Concept

```rust
// Rust test demonstrating oscillation vulnerability
#[tokio::test]
async fn test_pipeline_backpressure_oscillation() {
    // Setup: Create consensus config with standard backpressure levels
    let mut config = ConsensusConfig::default();
    
    // Initial state: pipeline latency at 2499ms (just below Level 4 threshold)
    let initial_latency = Duration::from_millis(2499);
    let backpressure_config = PipelineBackpressureConfig::new(
        config.pipeline_backpressure.clone(),
        config.execution_backpressure.clone(),
    );
    
    // Round 1: At 2499ms, Level 3 applies (1800 txns)
    let level1 = backpressure_config.get_backoff(initial_latency);
    assert_eq!(
        level1.unwrap().max_sending_block_txns_after_filtering_override,
        1800
    );
    
    // Attack: Submit expensive transactions, latency increases by 2ms to 2501ms
    let increased_latency = Duration::from_millis(2501);
    
    // Round 2: At 2501ms, Level 4 applies (1000 txns)
    let level2 = backpressure_config.get_backoff(increased_latency);
    assert_eq!(
        level2.unwrap().max_sending_block_txns_after_filtering_override,
        1000
    );
    
    // Demonstrate oscillation: Smaller blocks execute faster, latency decreases
    let decreased_latency = Duration::from_millis(2499);
    
    // Round 3: Back to Level 3 (1800 txns)
    let level3 = backpressure_config.get_backoff(decreased_latency);
    assert_eq!(
        level3.unwrap().max_sending_block_txns_after_filtering_override,
        1800
    );
    
    // Oscillation confirmed: 1800 -> 1000 -> 1800
    // This is a 44% change in block size with only 2ms latency variation
}

// Move test demonstrating attack via expensive transactions
#[test_only]
module oscillation_attack::expensive_txn {
    use std::vector;
    
    // Submit transaction with maximum gas consumption to slow execution
    public entry fun consume_max_gas() {
        let i = 0;
        let sum = 0;
        // Loop to consume gas (within limits)
        while (i < 10000) {
            sum = sum + i;
            i = i + 1;
        };
    }
    
    // Attack pattern: Submit many expensive transactions when latency is below threshold
    public entry fun trigger_oscillation(account: &signer) {
        let i = 0;
        while (i < 100) {
            consume_max_gas();
            i = i + 1;
        };
    }
}
```

**Notes**

The vulnerability is particularly severe because:

1. **Timing Sensitivity**: The 1ms boundary between levels creates extreme sensitivity to minor timing variations
2. **Validator Disagreement**: Each validator calculates different latencies due to local timing, leading to inconsistent view of appropriate block sizes
3. **No Attestation**: Block proposals don't include attestation of which backpressure level was used, preventing validators from detecting deviations
4. **Compound Effect**: When multiple thresholds are crossed simultaneously, the impact compounds (e.g., if both pipeline and execution backpressure activate)

The fix requires careful balancing: too much hysteresis/smoothing reduces responsiveness to genuine overload, while too little leaves the system vulnerable to oscillations. Recommended values: 200-300ms hysteresis margin and 0.2-0.3 smoothing alpha.

### Citations

**File:** config/src/config/consensus_config.rs (L263-319)
```rust
            pipeline_backpressure: vec![
                PipelineBackpressureValues {
                    // pipeline_latency looks how long has the oldest block still in pipeline
                    // been in the pipeline.
                    // Block enters the pipeline after consensus orders it, and leaves the
                    // pipeline once quorum on execution result among validators has been reached
                    // (so-(badly)-called "commit certificate"), meaning 2f+1 validators have finished execution.
                    back_pressure_pipeline_latency_limit_ms: 1200,
                    max_sending_block_txns_after_filtering_override:
                        MAX_SENDING_BLOCK_TXNS_AFTER_FILTERING,
                    max_sending_block_bytes_override: 5 * 1024 * 1024,
                    backpressure_proposal_delay_ms: 50,
                },
                PipelineBackpressureValues {
                    back_pressure_pipeline_latency_limit_ms: 1500,
                    max_sending_block_txns_after_filtering_override:
                        MAX_SENDING_BLOCK_TXNS_AFTER_FILTERING,
                    max_sending_block_bytes_override: 5 * 1024 * 1024,
                    backpressure_proposal_delay_ms: 100,
                },
                PipelineBackpressureValues {
                    back_pressure_pipeline_latency_limit_ms: 1900,
                    max_sending_block_txns_after_filtering_override:
                        MAX_SENDING_BLOCK_TXNS_AFTER_FILTERING,
                    max_sending_block_bytes_override: 5 * 1024 * 1024,
                    backpressure_proposal_delay_ms: 200,
                },
                // with execution backpressure, only later start reducing block size
                PipelineBackpressureValues {
                    back_pressure_pipeline_latency_limit_ms: 2500,
                    max_sending_block_txns_after_filtering_override: 1000,
                    max_sending_block_bytes_override: MIN_BLOCK_BYTES_OVERRIDE,
                    backpressure_proposal_delay_ms: 300,
                },
                PipelineBackpressureValues {
                    back_pressure_pipeline_latency_limit_ms: 3500,
                    max_sending_block_txns_after_filtering_override: 200,
                    max_sending_block_bytes_override: MIN_BLOCK_BYTES_OVERRIDE,
                    backpressure_proposal_delay_ms: 300,
                },
                PipelineBackpressureValues {
                    back_pressure_pipeline_latency_limit_ms: 4500,
                    max_sending_block_txns_after_filtering_override: 30,
                    max_sending_block_bytes_override: MIN_BLOCK_BYTES_OVERRIDE,
                    backpressure_proposal_delay_ms: 300,
                },
                PipelineBackpressureValues {
                    back_pressure_pipeline_latency_limit_ms: 6000,
                    // in practice, latencies and delay make it such that ~2 blocks/s is max,
                    // meaning that most aggressively we limit to ~10 TPS
                    // For transactions that are more expensive than that, we should
                    // instead rely on max gas per block to limit latency.
                    max_sending_block_txns_after_filtering_override: 5,
                    max_sending_block_bytes_override: MIN_BLOCK_BYTES_OVERRIDE,
                    backpressure_proposal_delay_ms: 300,
                },
            ],
```

**File:** consensus/src/block_storage/block_store.rs (L706-776)
```rust
    fn pipeline_pending_latency(&self, proposal_timestamp: Duration) -> Duration {
        let ordered_root = self.ordered_root();
        let commit_root = self.commit_root();
        let pending_path = self
            .path_from_commit_root(self.ordered_root().id())
            .unwrap_or_default();
        let pending_rounds = pending_path.len();
        let oldest_not_committed = pending_path.into_iter().min_by_key(|b| b.round());

        let oldest_not_committed_spent_in_pipeline = oldest_not_committed
            .as_ref()
            .and_then(|b| b.elapsed_in_pipeline())
            .unwrap_or(Duration::ZERO);

        let ordered_round = ordered_root.round();
        let oldest_not_committed_round = oldest_not_committed.as_ref().map_or(0, |b| b.round());
        let commit_round = commit_root.round();
        let ordered_timestamp = Duration::from_micros(ordered_root.timestamp_usecs());
        let oldest_not_committed_timestamp = oldest_not_committed
            .as_ref()
            .map(|b| Duration::from_micros(b.timestamp_usecs()))
            .unwrap_or(Duration::ZERO);
        let committed_timestamp = Duration::from_micros(commit_root.timestamp_usecs());
        let commit_cert_timestamp =
            Duration::from_micros(self.highest_commit_cert().commit_info().timestamp_usecs());

        fn latency_from_proposal(proposal_timestamp: Duration, timestamp: Duration) -> Duration {
            if timestamp.is_zero() {
                // latency not known without non-genesis blocks
                Duration::ZERO
            } else {
                proposal_timestamp.saturating_sub(timestamp)
            }
        }

        let latency_to_committed = latency_from_proposal(proposal_timestamp, committed_timestamp);
        let latency_to_oldest_not_committed =
            latency_from_proposal(proposal_timestamp, oldest_not_committed_timestamp);
        let latency_to_ordered = latency_from_proposal(proposal_timestamp, ordered_timestamp);

        info!(
            pending_rounds = pending_rounds,
            ordered_round = ordered_round,
            oldest_not_committed_round = oldest_not_committed_round,
            commit_round = commit_round,
            oldest_not_committed_spent_in_pipeline =
                oldest_not_committed_spent_in_pipeline.as_millis() as u64,
            latency_to_ordered_ms = latency_to_ordered.as_millis() as u64,
            latency_to_oldest_not_committed = latency_to_oldest_not_committed.as_millis() as u64,
            latency_to_committed_ms = latency_to_committed.as_millis() as u64,
            latency_to_commit_cert_ms =
                latency_from_proposal(proposal_timestamp, commit_cert_timestamp).as_millis() as u64,
            "Pipeline pending latency on proposal creation",
        );

        counters::CONSENSUS_PROPOSAL_PENDING_ROUNDS.observe(pending_rounds as f64);
        counters::CONSENSUS_PROPOSAL_PENDING_DURATION
            .observe_duration(oldest_not_committed_spent_in_pipeline);

        if pending_rounds > 1 {
            // TODO cleanup
            // previous logic was using difference between committed and ordered.
            // keeping it until we test out the new logic.
            // latency_to_oldest_not_committed
            //     .saturating_sub(latency_to_ordered.min(MAX_ORDERING_PIPELINE_LATENCY_REDUCTION))

            oldest_not_committed_spent_in_pipeline
        } else {
            Duration::ZERO
        }
    }
```

**File:** consensus/src/liveness/proposal_generator.rs (L137-159)
```rust
    pub fn get_backoff(
        &self,
        pipeline_pending_latency: Duration,
    ) -> Option<&PipelineBackpressureValues> {
        if self.backoffs.is_empty() {
            return None;
        }

        self.backoffs
            .range(..(pipeline_pending_latency.as_millis() as u64))
            .last()
            .map(|(_, v)| {
                sample!(
                    SampleRate::Duration(Duration::from_secs(10)),
                    warn!(
                        "Using consensus backpressure config for {}ms pending duration: {:?}",
                        pipeline_pending_latency.as_millis(),
                        v
                    )
                );
                v
            })
    }
```

**File:** consensus/consensus-types/src/pipelined_block.rs (L336-338)
```rust
    pub fn set_insertion_time(&self) {
        assert!(self.pipeline_insertion_time.set(Instant::now()).is_ok());
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L604-606)
```rust
        for block in &blocks {
            block.set_insertion_time();
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
```

**File:** consensus/src/round_manager.rs (L1178-1193)
```rust
        let payload_len = proposal.payload().map_or(0, |payload| payload.len());
        let payload_size = proposal.payload().map_or(0, |payload| payload.size());
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```
