# Audit Report

## Title
Unauthenticated Consensus State Information Disclosure via Inspection Service Health Check Endpoint

## Summary
The `/consensus_health_check` endpoint in the Aptos inspection service exposes real-time consensus participation state of validator nodes without any authentication or access control, enabling network reconnaissance and targeted attack planning against offline or struggling validators. [1](#0-0) 

## Finding Description

The `handle_consensus_health_check()` function provides an HTTP endpoint that reveals whether a validator is actively participating in consensus by checking the `aptos_state_sync_consensus_executing_gauge` metric. The endpoint returns:
- **200 OK** if the node is a validator AND consensus is executing (gauge = 1)
- **500 INTERNAL_SERVER_ERROR** if the node is a validator but NOT executing consensus
- **400 BAD_REQUEST** if the node is not a validator [2](#0-1) 

The consensus execution gauge is updated by the state sync driver based on whether consensus is actively running: [3](#0-2) 

**Critical Security Issues:**

1. **No Authentication**: The endpoint is exposed without any authentication mechanism on the inspection service (default port 9101). Unlike other sensitive endpoints like configuration, identity information, and peer information which have corresponding `expose_*` configuration flags, the consensus health check has no disable mechanism. [4](#0-3) 

2. **Public Exposure by Default**: The inspection service binds to `0.0.0.0` by default, making it accessible from any network interface: [5](#0-4) 

3. **No Disable Flag**: Unlike other potentially sensitive endpoints (configuration, identity, peer info, system info) which have boolean flags to disable them, the consensus health check endpoint is always enabled and cannot be disabled through configuration. [6](#0-5) 

**Attack Scenario:**

1. Attacker scans all known validator IP addresses (obtainable from on-chain validator set data)
2. For each validator, queries `http://<validator-ip>:9101/consensus_health_check` or `http://<validator-ip>:9102/consensus_health_check` (via HAProxy)
3. Builds real-time map of which validators are:
   - Actively participating in consensus (200 OK)
   - Online but not participating (500 ERROR)
   - Completely offline (connection timeout/refused)
4. Uses this intelligence to:
   - Identify weak validators for targeted infrastructure attacks
   - Time attacks to coincide with periods of low validator participation
   - Monitor validator recovery patterns to identify persistent weaknesses
   - Coordinate multi-vector attacks against validators already experiencing issues

## Impact Explanation

This vulnerability falls into the **High Severity** category per Aptos bug bounty criteria for the following reasons:

**Primary Impact - Network Reconnaissance Enabling Validator Attacks:**
- Provides real-time operational intelligence about validator health across the entire network
- Enables attackers to identify and target validators during vulnerable periods
- Facilitates strategic attack planning by revealing which validators are offline or struggling

**Security Invariant Violation:**
- Violates the principle of **least privilege** by exposing sensitive operational state to untrusted parties
- Breaks the expectation that consensus participation details should only be observable through legitimate on-chain mechanisms (block signatures)
- Provides more granular and real-time information than what's available through on-chain observation

**Comparison to Similar Issues:**
While block signatures reveal which validators signed specific blocks, this endpoint provides:
- Real-time status (no need to wait for block production)
- Binary participation state (easier to monitor and act on)
- Direct HTTP access (no need to run a full node or parse blockchain data)

While the direct impact is information disclosure, it qualifies as **High Severity** because it enables "Validator node slowdowns" and facilitates attacks against critical network infrastructure by providing actionable attack planning intelligence.

## Likelihood Explanation

**Likelihood: High**

The vulnerability is trivially exploitable:
- **No special access required**: Any attacker with network connectivity can query the endpoint
- **No authentication**: No credentials or authorization checks
- **Simple exploitation**: Single HTTP GET request
- **Low detection risk**: Endpoint queries appear as normal monitoring traffic
- **Deployed by default**: All validators expose this endpoint unless custom firewall rules are in place

The only mitigation in production environments is HAProxy-based IP filtering, which:
- Requires manual configuration of `blocked.ips` file
- Is not enabled by default
- Can be bypassed if the attacker's IPs are not blocked
- May not be properly configured in all validator deployments [7](#0-6) 

## Recommendation

**Immediate Mitigations:**

1. **Add Configuration Flag**: Introduce an `expose_consensus_health_check` boolean flag in `InspectionServiceConfig` (default: false for mainnet validators):

```rust
pub struct InspectionServiceConfig {
    pub address: String,
    pub port: u16,
    pub expose_configuration: bool,
    pub expose_consensus_health_check: bool,  // NEW
    pub expose_identity_information: bool,
    pub expose_peer_information: bool,
    pub expose_system_information: bool,
}
```

2. **Add Access Control Check**: Modify the endpoint handler to check the configuration flag:

```rust
pub async fn handle_consensus_health_check(node_config: &NodeConfig) -> (StatusCode, Body, String) {
    // Check if the endpoint is disabled
    if !node_config.inspection_service.expose_consensus_health_check {
        return (
            StatusCode::FORBIDDEN,
            Body::from("Consensus health check is disabled!"),
            CONTENT_TYPE_TEXT.into(),
        );
    }
    
    // Verify the node is a validator...
    // [rest of existing implementation]
}
```

3. **Add Sanitizer Rule**: Add a config sanitizer to prevent mainnet validators from exposing this endpoint:

```rust
// In ConfigSanitizer implementation
if node_type.is_validator()
    && chain_id.is_mainnet()
    && inspection_service_config.expose_consensus_health_check
{
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "Mainnet validators should not expose consensus health check!".to_string(),
    ));
}
```

4. **Use Internal-Only Port**: Consider moving health check probes to a separate internal-only port that is never exposed via HAProxy.

## Proof of Concept

**Step 1: Query a Validator's Consensus Status**

```bash
# Target a validator node (replace with actual validator IP)
VALIDATOR_IP="<validator-ip>"

# Query the consensus health check endpoint
curl -v "http://${VALIDATOR_IP}:9101/consensus_health_check"

# Expected responses:
# - HTTP 200: "Consensus health check passed!" (validator is participating)
# - HTTP 500: "Consensus health check failed! Consensus is not executing!" (validator online but not in consensus)
# - Connection refused/timeout: Validator is offline
```

**Step 2: Monitor Multiple Validators**

```bash
#!/bin/bash
# Reconnaissance script to monitor validator consensus status

VALIDATORS=(
    "validator1.example.com:9101"
    "validator2.example.com:9101"
    "validator3.example.com:9101"
)

echo "Monitoring validator consensus status..."
while true; do
    for validator in "${VALIDATORS[@]}"; do
        response=$(curl -s -o /dev/null -w "%{http_code}" "http://${validator}/consensus_health_check" --max-time 3)
        
        if [ "$response" = "200" ]; then
            echo "[$(date)] ${validator}: ACTIVE (participating in consensus)"
        elif [ "$response" = "500" ]; then
            echo "[$(date)] ${validator}: DEGRADED (online but not participating)"
        else
            echo "[$(date)] ${validator}: OFFLINE (no response)"
        fi
    done
    sleep 10
done
```

**Step 3: Verify the Gauge is Exposed via Metrics Endpoint**

```bash
# The same information is also available via the general metrics endpoint
curl -s "http://${VALIDATOR_IP}:9101/metrics" | grep "aptos_state_sync_consensus_executing_gauge"

# Output example:
# aptos_state_sync_consensus_executing_gauge 1  (participating)
# aptos_state_sync_consensus_executing_gauge 0  (not participating)
```

## Notes

**Comparison with Other Endpoints:**
All other potentially sensitive endpoints in the inspection service have disable flags and are protected for mainnet validators:
- `expose_configuration` - disabled by default for mainnet validators
- `expose_identity_information` - has toggle flag  
- `expose_peer_information` - has toggle flag
- `expose_system_information` - has toggle flag

The consensus health check endpoint lacks these protections despite revealing operationally sensitive information.

**Deployment Context:**
The endpoint is used as a Kubernetes `startupProbe`, which requires it to be accessible to the K8s control plane. However, this does not necessitate public internet exposure - the probe can access cluster-internal ports that are not exposed externally. [8](#0-7)

### Citations

**File:** crates/aptos-inspection-service/src/server/metrics.rs (L16-48)
```rust
/// Handles a consensus health check request. This method returns
/// 200 if the node is currently participating in consensus.
///
/// Note: we assume that this endpoint will only be used every few seconds.
pub async fn handle_consensus_health_check(node_config: &NodeConfig) -> (StatusCode, Body, String) {
    // Verify the node is a validator. If not, return an error.
    if !node_config.base.role.is_validator() {
        return (
            StatusCode::BAD_REQUEST,
            Body::from("This node is not a validator!"),
            CONTENT_TYPE_TEXT.into(),
        );
    }

    // Check the value of the consensus execution gauge
    let metrics = utils::get_all_metrics();
    if let Some(gauge_value) = metrics.get(CONSENSUS_EXECUTION_GAUGE) {
        if gauge_value == "1" {
            return (
                StatusCode::OK,
                Body::from("Consensus health check passed!"),
                CONTENT_TYPE_TEXT.into(),
            );
        }
    }

    // Otherwise, consensus is not executing
    (
        StatusCode::INTERNAL_SERVER_ERROR,
        Body::from("Consensus health check failed! Consensus is not executing!"),
        CONTENT_TYPE_TEXT.into(),
    )
}
```

**File:** state-sync/state-sync-driver/src/driver.rs (L722-749)
```rust
    /// Updates the executing component metrics for the driver
    fn update_executing_component_metrics(&self) {
        // Determine the executing component
        let executing_component = if self.check_if_consensus_or_observer_executing() {
            if self.driver_configuration.role.is_validator() {
                ExecutingComponent::Consensus
            } else {
                ExecutingComponent::ConsensusObserver
            }
        } else if self.bootstrapper.is_bootstrapped() {
            ExecutingComponent::ContinuousSyncer
        } else {
            ExecutingComponent::Bootstrapper
        };

        // Increment the executing component counter
        metrics::increment_counter(
            &metrics::EXECUTING_COMPONENT,
            executing_component.get_label(),
        );

        // Set the consensus executing gauge
        if executing_component == ExecutingComponent::Consensus {
            metrics::CONSENSUS_EXECUTING_GAUGE.set(1);
        } else {
            metrics::CONSENSUS_EXECUTING_GAUGE.set(0);
        }
    }
```

**File:** crates/aptos-inspection-service/src/server/mod.rs (L117-121)
```rust
        CONSENSUS_HEALTH_CHECK_PATH => {
            // /consensus_health_check
            // Exposes the consensus health check
            metrics::handle_consensus_health_check(&node_config).await
        },
```

**File:** config/src/config/inspection_service_config.rs (L15-24)
```rust
#[derive(Clone, Debug, Deserialize, PartialEq, Eq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct InspectionServiceConfig {
    pub address: String,
    pub port: u16,
    pub expose_configuration: bool,
    pub expose_identity_information: bool,
    pub expose_peer_information: bool,
    pub expose_system_information: bool,
}
```

**File:** config/src/config/inspection_service_config.rs (L26-37)
```rust
impl Default for InspectionServiceConfig {
    fn default() -> InspectionServiceConfig {
        InspectionServiceConfig {
            address: "0.0.0.0".to_string(),
            port: 9101,
            expose_configuration: false,
            expose_identity_information: true,
            expose_peer_information: true,
            expose_system_information: true,
        }
    }
}
```

**File:** terraform/helm/aptos-node/files/haproxy.cfg (L93-108)
```text
frontend validator-metrics
    mode http
    option httplog
    bind :9102
    default_backend validator-metrics

    # Deny requests from blocked IPs
    tcp-request connection reject if { src -n -f /usr/local/etc/haproxy/blocked.ips }

    ## Add the forwarded header
    http-request add-header Forwarded "for=%ci"

## Specify the validator metrics backend
backend validator-metrics
    mode http
    server {{ include "aptos-validator.fullname" $ }}-{{ $.Values.i }}-validator {{ include "aptos-validator.fullname" $ }}-{{ $.Values.i }}-validator:9101
```

**File:** terraform/helm/aptos-node/templates/validator.yaml (L137-147)
```yaml
        {{- if $.Values.validator.useConsensusHealthCheckAsStartupProbe }}
        startupProbe:
          httpGet:
            path: /consensus_health_check
            port: 9101
            scheme: HTTP
          failureThreshold: 2147483647 # set it to the max value since we don't want to restart the pod automatically even if it can't participate in consensus
          periodSeconds: 1
          successThreshold: 1
          timeoutSeconds: 3
        {{- end }}
```
