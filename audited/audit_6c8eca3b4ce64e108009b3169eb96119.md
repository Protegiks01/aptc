# Audit Report

## Title
Resource Leak During Epoch Transitions: Batch Data Persists When Garbage Collection Task Fails

## Summary
When a validator node starts a new epoch after an epoch gap (e.g., following network downtime), the asynchronous garbage collection task that deletes old batch data may fail silently, causing batch data from intermediate epochs to permanently leak in the database. This leads to unbounded disk space growth over time, eventually causing node failures.

## Finding Description

The vulnerability exists in the batch cleanup mechanism during epoch transitions. When a node starts for a new epoch, two separate cleanup operations occur:

1. **Batch ID Cleanup** (synchronous): The `clean_and_get_batch_id()` function successfully deletes batch ID metadata entries for old epochs [1](#0-0) 

2. **Batch Data Cleanup** (asynchronous): The `BatchStore::new()` spawns a fire-and-forget task to delete actual batch data [2](#0-1) 

The critical flaw is in the garbage collection functions that use `.expect()` for error handling: [3](#0-2) 

**Attack Scenario:**
When the current epoch jumps from 100 to 105 (skipping epochs 101-104):
1. The database contains batches from epochs 100-104 (accumulated from peer synchronization or previous operation)
2. Node initializes BatchStore with `is_new_epoch=true` for epoch 105
3. An async task is spawned to delete all batches with `epoch < 105`
4. If the database deletion operation fails (disk I/O error, permissions issue, database corruption), the task panics via `.expect()`
5. The panic only crashes the spawned task, not the main initialization flow
6. Meanwhile, `clean_and_get_batch_id(105)` successfully removes batch ID entries
7. **Result**: Batch metadata is gone, but actual batch data (potentially gigabytes) remains in the database forever

The batches are never cleaned up because:
- When `is_new_epoch=true`, old batches are NOT loaded into the in-memory cache [4](#0-3) 
- The time-based expiration cleanup only processes cached entries [5](#0-4) 
- There is no retry mechanism for failed garbage collection

With epoch gaps, this vulnerability is amplified because multiple epochs' worth of batch data accumulates.

## Impact Explanation

**Severity: Medium** (up to $10,000 per Aptos Bug Bounty)

This qualifies as "State inconsistencies requiring intervention" because:

1. **Disk Space Exhaustion**: Each batch can contain hundreds of transactions, consuming megabytes of storage. Over multiple epoch transitions with gaps, leaked batches accumulate, eventually filling the validator's disk
2. **Validator Node Degradation**: As disk space diminishes, the node experiences:
   - Slowdowns due to reduced disk I/O performance
   - Eventual crash when disk is full
   - Database corruption risks
3. **Network Health Impact**: Multiple validators experiencing this issue could reduce network liveness and consensus participation
4. **Manual Intervention Required**: Operators must manually identify and delete orphaned batch data from the database

While this doesn't cause immediate consensus violations or fund loss, it creates a gradual degradation path that can lead to High severity impacts (validator node slowdowns/crashes).

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability is likely to manifest because:

1. **Database Errors are Common**: Production environments experience transient database failures from:
   - Disk I/O errors
   - File system issues
   - Resource exhaustion during high load
   - Database locks or conflicts

2. **Epoch Gaps are Expected**: The security question explicitly mentions "after network downtime," which is a realistic scenario for:
   - Validator maintenance windows
   - Network partitions
   - Node crashes requiring restoration from backup
   - State sync operations

3. **Silent Failure**: The fire-and-forget task design means operators receive no alerts when garbage collection fails

4. **Accumulation Over Time**: Each failed cleanup compounds the problem, making subsequent cleanups more likely to fail (larger datasets to delete)

## Recommendation

Implement proper error handling and monitoring for batch garbage collection:

```rust
// In batch_store.rs, BatchStore::new()
if is_new_epoch {
    let gc_task = tokio::task::spawn_blocking(move || {
        if let Err(e) = Self::gc_previous_epoch_batches_from_db_v1(db_clone.clone(), epoch) {
            error!("Failed to GC V1 batches: {:?}", e);
            counters::BATCH_GC_FAILURE_COUNT.inc();
            return Err(e);
        }
        if let Err(e) = Self::gc_previous_epoch_batches_from_db_v2(db_clone, epoch) {
            error!("Failed to GC V2 batches: {:?}", e);
            counters::BATCH_GC_FAILURE_COUNT.inc();
            return Err(e);
        }
        Ok(())
    });
    
    // Monitor GC task completion and retry on failure
    tokio::spawn(async move {
        match gc_task.await {
            Ok(Ok(())) => {
                info!("Batch GC completed successfully for epoch {}", epoch);
            }
            Ok(Err(e)) | Err(e) => {
                error!("Batch GC failed for epoch {}: {:?}. Scheduling retry...", epoch, e);
                counters::BATCH_GC_FAILURE_COUNT.inc();
                // Implement exponential backoff retry mechanism
            }
        }
    });
}

// Update gc_previous_epoch_batches_from_db_v1 and v2 signatures
fn gc_previous_epoch_batches_from_db_v1(
    db: Arc<dyn QuorumStoreStorage>, 
    current_epoch: u64
) -> Result<(), DbError> {
    let db_content = db.get_all_batches()?;
    // ... existing logic ...
    db.delete_batches(expired_keys)?;
    Ok(())
}
```

Additional mitigations:
1. Add periodic background task to detect and clean orphaned batches
2. Implement metrics/alerts for failed GC operations
3. Add database integrity checks on startup
4. Document manual cleanup procedures for operators

## Proof of Concept

```rust
// Test case demonstrating the vulnerability
#[tokio::test]
async fn test_batch_gc_failure_causes_leak() {
    // Setup: Create mock DB that fails on delete_batches
    struct FailingQuorumStoreDB {
        batches: HashMap<HashValue, PersistedValue<BatchInfo>>,
        fail_delete: Arc<AtomicBool>,
    }
    
    impl QuorumStoreStorage for FailingQuorumStoreDB {
        fn delete_batches(&self, _digests: Vec<HashValue>) -> Result<(), DbError> {
            if self.fail_delete.load(Ordering::Relaxed) {
                Err(DbError::Other("Simulated I/O error".into()))
            } else {
                Ok(())
            }
        }
        
        fn get_all_batches(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfo>>> {
            Ok(self.batches.clone())
        }
        
        // ... other trait methods ...
    }
    
    let db = Arc::new(FailingQuorumStoreDB {
        batches: create_test_batches_for_epochs(100..105), // Epochs 100-104
        fail_delete: Arc::new(AtomicBool::new(true)),
    });
    
    // Create BatchStore for epoch 105 with is_new_epoch=true
    let batch_store = BatchStore::new(
        105, // current_epoch
        true, // is_new_epoch - triggers async GC
        0,
        db.clone(),
        1000,
        1000,
        100,
        validator_signer,
        Duration::from_secs(60).as_micros() as u64,
    );
    
    // Wait for GC task to complete/fail
    tokio::time::sleep(Duration::from_secs(1)).await;
    
    // Verify: Old batches still exist in database despite new epoch
    let remaining_batches = db.get_all_batches().unwrap();
    assert_eq!(remaining_batches.len(), 5, "Batches from epochs 100-104 should leak");
    
    // Verify: Batch IDs were cleaned up in clean_and_get_batch_id
    let batch_id = db.clean_and_get_batch_id(105).unwrap();
    assert!(batch_id.is_none(), "Batch ID metadata should be cleaned");
    
    // This demonstrates the inconsistent state: metadata gone, data persists
}
```

**Notes:**
- This vulnerability is specific to epoch transitions with `is_new_epoch=true`, which occurs when the ledger info indicates an epoch boundary
- The severity is amplified when there are epoch gaps because more batch data accumulates from intermediate epochs
- The fire-and-forget design pattern used here violates Rust's error handling best practices and creates an unmonitored failure path
- While the question asks specifically about "intermediate epochs leak," the root cause affects all epoch transitions, with gaps making the impact worse

### Citations

**File:** consensus/src/quorum_store/quorum_store_db.rs (L163-179)
```rust
    fn clean_and_get_batch_id(&self, current_epoch: u64) -> Result<Option<BatchId>, DbError> {
        let mut iter = self.db.iter::<BatchIdSchema>()?;
        iter.seek_to_first();
        let epoch_batch_id = iter
            .map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<u64, BatchId>>>()?;
        let mut ret = None;
        for (epoch, batch_id) in epoch_batch_id {
            assert!(current_epoch >= epoch);
            if epoch < current_epoch {
                self.delete_batch_id(epoch)?;
            } else {
                ret = Some(batch_id);
            }
        }
        Ok(ret)
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L156-176)
```rust
        if is_new_epoch {
            tokio::task::spawn_blocking(move || {
                Self::gc_previous_epoch_batches_from_db_v1(db_clone.clone(), epoch);
                Self::gc_previous_epoch_batches_from_db_v2(db_clone, epoch);
            });
        } else {
            Self::populate_cache_and_gc_expired_batches_v1(
                db_clone.clone(),
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
            Self::populate_cache_and_gc_expired_batches_v2(
                db_clone,
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L208-209)
```rust
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
```

**File:** consensus/src/quorum_store/batch_store.rs (L443-472)
```rust
    pub(crate) fn clear_expired_payload(&self, certified_time: u64) -> Vec<HashValue> {
        // To help slow nodes catch up via execution without going to state sync we keep the blocks for 60 extra seconds
        // after the expiration time. This will help remote peers fetch batches that just expired but are within their
        // execution window.
        let expiration_time = certified_time.saturating_sub(self.expiration_buffer_usecs);
        let expired_digests = self.expirations.lock().expire(expiration_time);
        let mut ret = Vec::new();
        for h in expired_digests {
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
                    } else {
                        None
                    }
                },
                Vacant(_) => unreachable!("Expired entry not in cache"),
            };
            // No longer holding the lock on db_cache entry.
            if let Some(value) = removed_value {
                self.free_quota(value);
                ret.push(h);
            }
        }
        ret
    }
```
