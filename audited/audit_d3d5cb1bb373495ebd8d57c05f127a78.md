# Audit Report

## Title
Memory Exhaustion in Parallel Indexer Processing via Large Struct Layouts

## Summary
The `FatType::from_layout_slice()` function in the move-resource-viewer lacks proper upfront memory limit checks, allowing an attacker to publish Move modules with structs containing thousands of fields. When multiple indexer workers process transactions containing these resources in parallel, each worker allocates large vectors without adequate limiter charging, potentially exhausting memory and causing denial of service on indexer infrastructure.

## Finding Description

The vulnerability exists in the resource annotation pipeline used by Aptos indexers. The attack flow is:

**1. Vulnerable Code Path:** [1](#0-0) 

The indexer spawns multiple tokio tasks to process transactions in parallel. Each task processes write sets that may contain resources with complex type layouts.

**2. Resource Processing:** [2](#0-1) 

For each resource, the annotator's `collect_table_info` is called, which builds type layouts and eventually invokes: [3](#0-2) 

**3. Inadequate Limiter Charging:**

The `from_layout_slice` function allocates a `Vec<FatType>` via `.collect()` **before** any limiter checks occur. Additionally, `from_runtime_layout` does not charge the limiter for primitive types: [4](#0-3) 

For primitive types (Bool, U8, U16, etc.), no `limit.charge()` call is made, bypassing memory accounting.

**4. No Struct Field Limit:** [5](#0-4) 

Production configuration sets `max_fields_in_struct: None`, allowing modules to define structs with arbitrarily many fields (limited only by module size constraints).

**5. Limiter Per Worker:** [6](#0-5) 

Each worker creates its own `Limiter` with a 100MB default limit: [7](#0-6) 

**Attack Scenario:**

1. Attacker publishes a Move module (within 60KB size limit) containing a struct with thousands of primitive fields or deeply nested struct fields
2. Attacker creates transactions that write resources of this type to state
3. When the indexer processes these transactions, multiple parallel workers simultaneously call `from_layout_slice` with large layout arrays
4. Each worker allocates large `Vec<FatType>` without proper upfront checks
5. For primitive types, no limiter charges occur at all
6. Aggregate memory usage across all parallel workers exhausts available memory, crashing the indexer process

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty program criteria:
- **API crashes**: Indexer crashes prevent API endpoints from serving data
- **Validator node slowdowns**: While validators are not directly affected by indexer crashes, integrated validator+indexer nodes experience degraded service

The impact is limited to indexer infrastructure (off-chain) and does not affect consensus, validator operations, or on-chain state. However, indexers are critical infrastructure for ecosystem functionality (block explorers, wallets, dApps), making their availability important.

## Likelihood Explanation

**Likelihood: Medium to High**

**Attacker Requirements:**
- Publish a malicious Move module (costs gas but is permissionless)
- Create transactions storing resources of the malicious type
- No special privileges or validator access required

**Feasibility:**
- Module size limits (60KB) still allow structs with thousands of fields
- Within a 60KB budget, an attacker could create structs with ~5,000-6,000 primitive fields
- Nested structures amplify the effect (e.g., 100 structs Ã— 1,000 fields each = 100,000 total fields)
- Multiple parallel workers (typically 10-20) amplify memory usage
- Attack is repeatable and can target specific indexer nodes

**Constraints:**
- Module publishing costs gas (but one-time cost)
- Attack requires indexer to process attacker's transactions
- Some indexers may have additional memory safeguards

## Recommendation

**Immediate Fix:**

1. Add upfront capacity checks in `from_layout_slice`:

```rust
fn from_layout_slice(
    layouts: &[MoveTypeLayout],
    limit: &mut Limiter,
) -> PartialVMResult<Vec<FatType>> {
    // Charge upfront for the slice length to prevent large allocations
    let estimated_size = layouts.len() * std::mem::size_of::<FatType>();
    limit.charge(estimated_size)?;
    
    layouts
        .iter()
        .map(|l| Self::from_runtime_layout(l, limit))
        .collect()
}
```

2. Add limiter charges for primitive types in `from_runtime_layout`:

```rust
pub(crate) fn from_runtime_layout(
    layout: &MoveTypeLayout,
    limit: &mut Limiter,
) -> PartialVMResult<FatType> {
    use MoveTypeLayout::*;
    
    // Charge for each type node processed
    limit.charge(std::mem::size_of::<FatType>())?;
    
    Ok(match layout {
        Bool => FatType::Bool,
        U8 => FatType::U8,
        // ... rest of implementation
    })
}
```

3. Consider enforcing `max_fields_in_struct` limit in production config to bound worst-case struct complexity.

**Long-term Solutions:**
- Implement streaming/lazy evaluation for large type layouts
- Add per-struct complexity limits enforced at module publishing time
- Implement global memory limits across all indexer workers
- Add monitoring and alerting for excessive memory usage during indexing

## Proof of Concept

```move
// malicious_module.move - Module with large struct
module attacker::dos {
    struct LargeStruct has key, store {
        f0: u8, f1: u8, f2: u8, f3: u8, f4: u8,
        // ... repeat for thousands of fields ...
        // Within 60KB module limit, can fit ~5000 primitive fields
        f4999: u8
    }
    
    public entry fun publish_large_resource(account: &signer) {
        move_to(account, LargeStruct {
            f0: 0, f1: 0, /* ... */ f4999: 0
        });
    }
}
```

**Reproduction Steps:**

1. Compile and publish the malicious module
2. Execute multiple transactions calling `publish_large_resource` from different accounts
3. Observe indexer memory usage spike as parallel workers process these transactions
4. With sufficient transaction volume and struct complexity, indexer will crash due to memory exhaustion
5. Monitor indexer logs for "Query exceeds size limit" errors or OOM crashes

**Expected Result:** Indexer process crashes or becomes unresponsive when processing transactions containing large struct layouts, especially when multiple workers handle such transactions simultaneously.

## Notes

- This vulnerability affects only indexer infrastructure, not consensus-critical validator operations
- The "parallel conversion" mentioned in the security question refers to parallel processing at the worker/task level, not within the `from_layout_slice` function itself
- Different indexer implementations may have varying susceptibility based on worker pool size and available memory
- The 100MB per-worker limit provides some protection but is insufficient when multiple workers operate in parallel
- Attack complexity is low once a malicious module is published, as subsequent exploitation only requires sending transactions

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-table-info/src/table_info_service.rs (L239-274)
```rust
    /// Fans out a bunch of threads and processes write sets from transactions in parallel.
    /// Pushes results in parallel to the stream, but only return that the batch is
    /// fully completed if every job in the batch is successful and no pending on items
    /// Processing transactions in 2 stages:
    /// 1. Fetch transactions from ledger db
    /// 2. Get write sets from transactions and parse write sets to get handle -> key,value type mapping, write the mapping to the rocksdb
    async fn process_transactions_in_parallel(
        &self,
        indexer_async_v2: Arc<IndexerAsyncV2>,
        transactions: Vec<TransactionOnChainData>,
    ) -> Vec<EndVersion> {
        let mut tasks = vec![];
        let context = self.context.clone();
        let last_version = transactions
            .last()
            .map(|txn| txn.version)
            .unwrap_or_default();

        let transactions = Arc::new(transactions);
        for (chunk_idx, batch_size) in transactions
            .chunks(self.parser_batch_size as usize)
            .enumerate()
            .map(|(idx, chunk)| (idx, chunk.len()))
        {
            let start = chunk_idx * self.parser_batch_size as usize;
            let end = start + batch_size;

            let transactions = transactions.clone();
            let context = context.clone();
            let indexer_async_v2 = indexer_async_v2.clone();
            let task = tokio::spawn(async move {
                Self::process_transactions(context, indexer_async_v2, &transactions[start..end])
                    .await
            });
            tasks.push(task);
        }
```

**File:** storage/indexer/src/db_v2.rs (L258-268)
```rust
    fn collect_table_info_from_struct(
        &mut self,
        struct_tag: StructTag,
        bytes: &Bytes,
    ) -> Result<()> {
        let ty_tag = TypeTag::Struct(Box::new(struct_tag));
        let mut infos = vec![];
        self.annotator
            .collect_table_info(&ty_tag, bytes, &mut infos)?;
        self.process_table_infos(infos)
    }
```

**File:** third_party/move/tools/move-resource-viewer/src/fat_type.rs (L469-518)
```rust
    pub(crate) fn from_runtime_layout(
        layout: &MoveTypeLayout,
        limit: &mut Limiter,
    ) -> PartialVMResult<FatType> {
        use MoveTypeLayout::*;
        Ok(match layout {
            Bool => FatType::Bool,
            U8 => FatType::U8,
            U16 => FatType::U16,
            U32 => FatType::U32,
            U64 => FatType::U64,
            U128 => FatType::U128,
            U256 => FatType::U256,
            I8 => FatType::I8,
            I16 => FatType::I16,
            I32 => FatType::I32,
            I64 => FatType::I64,
            I128 => FatType::I128,
            I256 => FatType::I256,
            Address => FatType::Address,
            Signer => FatType::Signer,
            Vector(ty) => FatType::Vector(Box::new(Self::from_runtime_layout(ty, limit)?)),
            Struct(MoveStructLayout::Runtime(tys)) => {
                FatType::Runtime(Self::from_layout_slice(tys, limit)?)
            },
            Struct(MoveStructLayout::RuntimeVariants(vars)) => FatType::RuntimeVariants(
                vars.iter()
                    .map(|tys| Self::from_layout_slice(tys, limit))
                    .collect::<PartialVMResult<Vec<Vec<_>>>>()?,
            ),
            Function => {
                // We cannot derive the actual type from layout, however, a dummy
                // function type will do since annotation of closures is not depending
                // actually on their type, but only their (hidden) captured arguments.
                // Currently, `from_runtime_layout` is only used to annotate captured arguments
                // of closures.
                FatType::Function(Box::new(FatFunctionType {
                    args: vec![],
                    results: vec![],
                    abilities: AbilitySet::EMPTY,
                }))
            },
            Native(..) | Struct(_) => {
                return Err(PartialVMError::new_invariant_violation(format!(
                    "cannot derive fat type for {:?}",
                    layout
                )))
            },
        })
    }
```

**File:** third_party/move/tools/move-resource-viewer/src/fat_type.rs (L520-528)
```rust
    fn from_layout_slice(
        layouts: &[MoveTypeLayout],
        limit: &mut Limiter,
    ) -> PartialVMResult<Vec<FatType>> {
        layouts
            .iter()
            .map(|l| Self::from_runtime_layout(l, limit))
            .collect()
    }
```

**File:** aptos-move/aptos-vm-environment/src/prod_configs.rs (L170-170)
```rust
        max_fields_in_struct: None,
```

**File:** third_party/move/tools/move-resource-viewer/src/lib.rs (L685-688)
```rust
    pub fn view_value(&self, ty_tag: &TypeTag, blob: &[u8]) -> anyhow::Result<AnnotatedMoveValue> {
        let mut limit = Limiter::default();
        let ty = self.resolve_type_impl(ty_tag, &mut limit)?;
        self.view_value_by_fat_type(&ty, blob, &mut limit)
```

**File:** third_party/move/tools/move-resource-viewer/src/limit.rs (L7-8)
```rust
// Default limit set to 100mb per query.
const DEFAULT_LIMIT: usize = 100_000_000;
```
