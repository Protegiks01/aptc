# Audit Report

## Title
Cache Interference Causes False Negatives in Consensus Timeout Monitoring

## Summary
The `ConsensusTimeoutsChecker::check()` function in the node-checker service can produce incorrect health assessments when multiple checks run concurrently. Due to a shared cache without proper concurrency controls, parallel checker invocations can interfere with each other's time-based measurements, causing consensus timeout issues to go undetected.

## Finding Description

The vulnerability exists in how the `OutputCache` is shared between concurrent checker invocations. The `ConsensusTimeoutsChecker` is designed to measure consensus timeout increases over a fixed time period (default 4 seconds) by:

1. Fetching metrics at time T₀ [1](#0-0) 
2. Sleeping for `check_delay` seconds [2](#0-1) 
3. Fetching metrics at time T₀ + delay [3](#0-2) 
4. Comparing timeout counts [4](#0-3) 

All checkers share a single `MetricsProvider` instance with a common `OutputCache` [5](#0-4) , and checkers run concurrently [6](#0-5) .

The `OutputCache` implementation has a critical TOCTOU (Time-of-Check-Time-of-Use) race condition [7](#0-6)  where the cache validity check and data read use separate lock acquisitions, allowing inconsistent states.

**Attack Scenario:**
With default configuration (`cache_ttl_ms=1000`, `check_delay_secs=4`):

- **T₀**: Checker-A fetches Scrape-1 (timeouts=10), caches it, begins 4-second sleep
- **T₀ + 3.5s**: Checker-B starts, cache expired (3500ms > 1000ms), fetches Scrape-2 (timeouts=15), caches with timestamp T₀+3500ms
- **T₀ + 4s**: Checker-A wakes, cache age = 500ms < 1000ms → **Cache hit!**
- Checker-A compares Scrape-1 (T₀) vs Scrape-2 (T₀+3500ms)
- **Time delta is 3.5s instead of 4s** (12.5% timing error)
- Timeout rate appears lower: 5 timeouts/3.5s = 1.43/s vs actual 5/4s = 1.25/s
- If threshold is 1 timeout per 4-second window, check may incorrectly pass when it should fail

Additionally, the cache has a thundering herd problem where multiple threads seeing an expired cache will all proceed to fetch [8](#0-7) , causing redundant fetches and potential timestamp skew.

## Impact Explanation

**Severity Assessment: Medium (Does Not Meet Bug Bounty Criteria)**

While this is a legitimate concurrency bug, it affects the **node-checker operational tooling**, not the core blockchain protocol. The node-checker is a monitoring/diagnostic service that runs separately from validator nodes and does not participate in:
- Consensus protocol execution
- Transaction validation or execution  
- State management or persistence
- On-chain governance or staking

False negatives in health monitoring could delay detection of degraded nodes, but this does not:
- Compromise funds or allow theft/minting
- Break consensus safety or cause chain splits
- Cause network partition or liveness failures
- Corrupt blockchain state or validator operations

Per the Aptos bug bounty criteria, this would not qualify for a reward as it doesn't meet Critical, High, or Medium severity thresholds for blockchain security impacts.

## Likelihood Explanation

The bug occurs naturally during normal operations when:
1. Multiple checker instances run concurrently (standard in production monitoring)
2. Checker start times are offset by specific intervals (3-4 seconds with default config)
3. Default configuration is used (cache_ttl=1s, check_delay=4s)

Probability: **High** in production environments with continuous monitoring, but impact is limited to operational visibility rather than blockchain security.

## Recommendation

Implement double-checked locking in the cache to prevent TOCTOU races and cache interference:

```rust
pub async fn get(
    &self,
    func: impl Future<Output = Result<T, ProviderError>>,
) -> Result<T, ProviderError> {
    // Fast path: check if cache is valid
    if self.last_run.read().await.elapsed() < self.cache_ttl {
        if let Some(last_output) = &*self.last_output.read().await {
            return Ok(last_output.clone());
        }
    }

    // Slow path: acquire write lock and re-check
    let mut last_output = self.last_output.write().await;
    let mut last_run = self.last_run.write().await;
    
    // Double-check after acquiring write lock
    if last_run.elapsed() < self.cache_ttl {
        if let Some(cached) = &*last_output {
            return Ok(cached.clone());
        }
    }
    
    let new_output = func.await?;
    *last_output = Some(new_output.clone());
    *last_run = Instant::now();
    Ok(new_output)
}
```

Alternatively, use a single `RwLock<Option<(Instant, T)>>` to ensure atomic reads of timestamp and data.

## Proof of Concept

```rust
// Test demonstrating cache interference
#[tokio::test]
async fn test_concurrent_checker_interference() {
    use std::sync::Arc;
    use std::time::Duration;
    use tokio::time::sleep;
    
    // Simulate shared cache with 1s TTL
    let cache = Arc::new(OutputCache::new(Duration::from_millis(1000)));
    
    // Checker-A starts
    let cache_a = cache.clone();
    let handle_a = tokio::spawn(async move {
        // Fetch at T=0
        let scrape1 = cache_a.get(async { Ok(10u64) }).await.unwrap();
        let time1 = std::time::Instant::now();
        
        // Sleep 4 seconds
        sleep(Duration::from_secs(4)).await;
        
        // Fetch at T=4s (expects new data)
        let scrape2 = cache_a.get(async { Ok(20u64) }).await.unwrap();
        let time2 = std::time::Instant::now();
        
        (scrape1, scrape2, time2.duration_since(time1))
    });
    
    // Checker-B starts at T=3.5s
    let cache_b = cache.clone();
    let handle_b = tokio::spawn(async move {
        sleep(Duration::from_millis(3500)).await;
        
        // This updates cache at T=3.5s
        cache_b.get(async { Ok(15u64) }).await.unwrap();
    });
    
    handle_b.await.unwrap();
    let (s1, s2, delta) = handle_a.await.unwrap();
    
    // Bug: Checker-A gets cached value from Checker-B
    // Expected: s1=10, s2=20, delta≈4s
    // Actual: s1=10, s2=15, delta≈4s but values are inconsistent
    assert_eq!(s1, 10);
    assert_eq!(s2, 15); // Got Checker-B's cached value!
    println!("Time delta: {:?}", delta);
}
```

---

## Notes

**This vulnerability does not meet the Aptos bug bounty severity criteria** because the node-checker is an operational monitoring tool, not a consensus-critical component. While the concurrency bugs are real and should be fixed for operational reliability, they do not compromise blockchain security, consensus safety, fund security, or state integrity. This would be classified as a **code quality issue** rather than a security vulnerability in the context of blockchain security auditing.

### Citations

**File:** ecosystem/node-checker/src/checker/consensus_timeouts.rs (L105-117)
```rust
        let first_scrape = match target_metrics_provider.provide().await {
            Ok(scrape) => scrape,
            Err(e) => {
                return Ok(vec![Self::build_result(
                    "Failed to check consensus timeouts".to_string(),
                    0,
                    format!(
                        "Failed to scrape metrics from your node (1st time): {:#}",
                        e
                    ),
                )])
            },
        };
```

**File:** ecosystem/node-checker/src/checker/consensus_timeouts.rs (L119-119)
```rust
        tokio::time::sleep(target_metrics_provider.config.common.check_delay()).await;
```

**File:** ecosystem/node-checker/src/checker/consensus_timeouts.rs (L121-133)
```rust
        let second_scrape = match target_metrics_provider.provide().await {
            Ok(scrape) => scrape,
            Err(e) => {
                return Ok(vec![Self::build_result(
                    "Failed to check consensus timeouts".to_string(),
                    0,
                    format!(
                        "Failed to scrape metrics from your node (2nd time): {:#}",
                        e
                    ),
                )])
            },
        };
```

**File:** ecosystem/node-checker/src/checker/consensus_timeouts.rs (L149-152)
```rust
        Ok(vec![self.build_check_result(
            previous_round.unwrap(),
            latest_round.unwrap(),
        )])
```

**File:** ecosystem/node-checker/src/runner/sync_runner.rs (L106-111)
```rust
            provider_collection.target_metrics_provider = Some(MetricsProvider::new(
                self.provider_configs.metrics.clone(),
                metrics_client.clone(),
                target_node_address.url.clone(),
                target_node_address.get_metrics_port().unwrap(),
            ));
```

**File:** ecosystem/node-checker/src/runner/sync_runner.rs (L156-163)
```rust
        let mut futures = Vec::new();
        for checker in &self.checkers {
            futures.push(self.call_check(checker, &provider_collection));
        }

        // Run all the Checkers concurrently and collect their results.
        let check_results: Vec<CheckResult> =
            try_join_all(futures).await?.into_iter().flatten().collect();
```

**File:** ecosystem/node-checker/src/provider/cache.rs (L40-44)
```rust
        if self.last_run.read().await.elapsed() < self.cache_ttl {
            if let Some(last_output) = &*self.last_output.read().await {
                return Ok(last_output.clone());
            }
        }
```

**File:** ecosystem/node-checker/src/provider/cache.rs (L46-53)
```rust
        // Otherwise fetch the value and update the cache. We take the locks while
        // fetching the new value so we don't waste effort fetching it multiple times.
        let mut last_output = self.last_output.write().await;
        let mut last_run = self.last_run.write().await;
        let new_output = func.await?;
        *last_output = Some(new_output.clone());
        *last_run = Instant::now();
        Ok(new_output)
```
