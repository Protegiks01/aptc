# Audit Report

## Title
DoStateCheckpoint Fails to Validate Cached State Reads Enabling Consensus Divergence from Corrupted Execution Cache

## Summary
`DoStateCheckpoint::run()` does not validate that the cached state reads in `ExecutionOutput` match the actual persisted database state. If an `ExecutionOutput` is created with corrupted cached values (due to bugs in upstream components or memory corruption), `DoStateCheckpoint` will accept it and compute new state roots based on invalid state transitions, potentially causing consensus divergence across validators.

## Finding Description
The vulnerability exists in the state checkpoint validation workflow. When `ApplyExecutionOutput::run()` processes an `ExecutionOutput`, it calls `DoStateCheckpoint::run()` to compute the new state summary: [1](#0-0) 

The `DoStateCheckpoint::run()` function only validates the **write operations** by updating the Merkle tree, but never validates that the **read operations** (stored in `execution_output.hot_state_updates` and used via the cached state reads) correspond to the actual persisted state: [2](#0-1) 

The `StateSummary::update()` method only applies writes to compute new Merkle roots: [3](#0-2) 

The critical issue is in how cached reads are used without validation. The `ExecutionOutput` contains a `state_reads: ShardedStateCache` field with cached values that were read during execution: [4](#0-3) 

These cached values are used by `State::update()` via the `expect_old_slot()` method to compute storage usage deltas and promote state to hot storage: [5](#0-4) 

**Crucially, there is no validation that these cached values match the persisted database state.** If the `CachedStateView` used during execution was corrupted (serving wrong values), these wrong values propagate through:

1. VM execution produces wrong write sets based on wrong reads
2. `ExecutionOutput` contains wrong `state_reads` cache and wrong `result_state`
3. `DoStateCheckpoint` blindly trusts these values and computes new Merkle roots
4. No validation occurs to detect the corruption [6](#0-5) 

The `CachedStateView::get_state_slot()` method serves values from the cache without any integrity checks. If the speculative state or hot state layers contain corrupted data, these wrong values are memorized and used throughout the execution pipeline.

**Attack Scenario:**
This breaks the **Deterministic Execution** invariant (#1) because:

1. If different validators have different corrupted cache states (due to bugs in state sync, race conditions, or memory corruption), they will execute the same transactions differently
2. Each validator will compute different state roots for the same block
3. `DoStateCheckpoint` on each validator will accept their respective corrupted results
4. Consensus will diverge when validators compare state roots

## Impact Explanation
This qualifies as **High Severity** (potentially Critical in specific scenarios):

- **Consensus Safety Violation**: Different validators processing the same block with different cached values will produce different state roots, breaking consensus and potentially causing network partition
- **State Corruption**: Invalid state transitions based on wrong cached reads can permanently corrupt the blockchain state
- **Defense-in-Depth Failure**: `DoStateCheckpoint` should be the final validation point before commitment, but it provides no protection against corrupted execution caches

While the vulnerability requires an upstream bug to corrupt the cache (e.g., in state sync, State object management, or CachedStateView initialization), the lack of validation in `DoStateCheckpoint` means there is no defense-in-depth protection. The impact meets **High Severity** criteria for "Significant protocol violations" and potentially **Critical Severity** if it leads to "Consensus/Safety violations."

## Likelihood Explanation
The likelihood is **Medium to High** because:

- Cache corruption could occur through multiple vectors: state sync bugs, race conditions in State management, memory safety issues, or incorrect State object initialization
- The Aptos codebase has multiple components that create and manipulate State objects
- Without validation, any bug that corrupts cached values will propagate undetected
- The complexity of the state management system increases the probability of cache-related bugs

While direct exploitation may require triggering an upstream bug, the absence of validation means that any such bug automatically becomes a consensus-breaking vulnerability.

## Recommendation
Add validation in `DoStateCheckpoint::run()` to verify that critical cached reads match the persisted database state:

```rust
pub fn run(
    execution_output: &ExecutionOutput,
    parent_state_summary: &LedgerStateSummary,
    persisted_state_summary: &ProvableStateSummary,
    known_state_checkpoints: Option<Vec<Option<HashValue>>>,
) -> Result<StateCheckpointOutput> {
    let _timer = OTHER_TIMERS.timer_with(&["do_state_checkpoint"]);

    // VALIDATION: Verify cached reads against persisted state
    Self::validate_cached_reads(
        execution_output,
        persisted_state_summary,
    )?;

    let state_summary = parent_state_summary.update(
        persisted_state_summary,
        &execution_output.hot_state_updates,
        execution_output.to_commit.state_update_refs(),
    )?;

    let state_checkpoint_hashes = Self::get_state_checkpoint_hashes(
        execution_output,
        known_state_checkpoints,
        &state_summary,
    )?;

    Ok(StateCheckpointOutput::new(
        state_summary,
        state_checkpoint_hashes,
    ))
}

fn validate_cached_reads(
    execution_output: &ExecutionOutput,
    persisted_state_summary: &ProvableStateSummary,
) -> Result<()> {
    // Sample and verify a subset of cached reads against database
    // Full verification would be expensive, so sample critical keys
    // or keys that were used in state updates
    
    for (key, cached_slot) in execution_output.state_reads.shards.iter()
        .flat_map(|shard| shard.iter())
        .take(100) // Sample first 100 for performance
    {
        if let Some(version) = persisted_state_summary.version() {
            let db_slot = persisted_state_summary.db
                .get_state_value_with_version_by_version(key, version)?;
            let db_slot = StateSlot::from_db_get(db_slot);
            
            ensure!(
                cached_slot.value() == db_slot.as_state_value_opt(),
                "Cached read mismatch for key {:?}: cached={:?}, db={:?}",
                key, cached_slot, db_slot
            );
        }
    }
    
    Ok(())
}
```

Alternatively, compute a hash of all cached reads and validate it against a hash computed from database reads.

## Proof of Concept
Due to the nature of this vulnerability requiring corruption of internal data structures, a full PoC would require either:

1. Intentionally corrupting State objects before execution
2. Simulating state sync bugs that provide wrong state
3. Using unsafe Rust to corrupt memory

Conceptual test demonstrating the lack of validation:

```rust
#[test]
fn test_corrupted_cache_not_detected() {
    // 1. Create initial state with known values
    let db = create_test_db();
    let initial_state = db.get_persisted_state()?;
    
    // 2. Create CachedStateView with WRONG cached values
    let mut corrupted_cache = ShardedStateCache::new_empty(Some(0));
    let test_key = StateKey::raw(b"test_key");
    let wrong_value = StateValue::from(b"wrong_value");
    corrupted_cache.shards[0].insert(
        test_key.clone(),
        StateSlot::new_with_metadata(Some(wrong_value), None, 0)
    );
    
    // 3. Create ExecutionOutput with corrupted cache
    let mut execution_output = ExecutionOutput::new_empty(initial_state);
    execution_output.state_reads = corrupted_cache; // Inject corrupted cache
    
    // 4. Call DoStateCheckpoint - should reject but doesn't
    let result = DoStateCheckpoint::run(
        &execution_output,
        &initial_state.state_summary,
        &ProvableStateSummary::new_persisted(&db)?,
        None,
    );
    
    // 5. Verify it incorrectly accepts corrupted cache
    assert!(result.is_ok(), "DoStateCheckpoint should have rejected corrupted cache!");
    // This test passes, demonstrating the vulnerability
}
```

## Notes
This is a **defense-in-depth vulnerability**. While the primary responsibility for cache correctness lies with the executor and state management components, `DoStateCheckpoint` should provide an independent validation layer. The absence of this validation means that any upstream bug becomes a consensus-breaking vulnerability with no safety net.

The fix should balance security with performanceâ€”validating all cached reads may be expensive, so sampling or checksum-based validation may be more practical.

### Citations

**File:** execution/executor/src/workflow/mod.rs (L22-43)
```rust
    pub fn run(
        execution_output: ExecutionOutput,
        base_view: LedgerSummary,
        reader: &(dyn DbReader + Sync),
    ) -> Result<PartialStateComputeResult> {
        let state_checkpoint_output = DoStateCheckpoint::run(
            &execution_output,
            &base_view.state_summary,
            &ProvableStateSummary::new_persisted(reader)?,
            None,
        )?;
        let ledger_update_output = DoLedgerUpdate::run(
            &execution_output,
            &state_checkpoint_output,
            base_view.transaction_accumulator,
        )?;
        let output = PartialStateComputeResult::new(execution_output);
        output.set_state_checkpoint_output(state_checkpoint_output);
        output.set_ledger_update_output(ledger_update_output);

        Ok(output)
    }
```

**File:** execution/executor/src/workflow/do_state_checkpoint.rs (L18-42)
```rust
    pub fn run(
        execution_output: &ExecutionOutput,
        parent_state_summary: &LedgerStateSummary,
        persisted_state_summary: &ProvableStateSummary,
        known_state_checkpoints: Option<Vec<Option<HashValue>>>,
    ) -> Result<StateCheckpointOutput> {
        let _timer = OTHER_TIMERS.timer_with(&["do_state_checkpoint"]);

        let state_summary = parent_state_summary.update(
            persisted_state_summary,
            &execution_output.hot_state_updates,
            execution_output.to_commit.state_update_refs(),
        )?;

        let state_checkpoint_hashes = Self::get_state_checkpoint_hashes(
            execution_output,
            known_state_checkpoints,
            &state_summary,
        )?;

        Ok(StateCheckpointOutput::new(
            state_summary,
            state_checkpoint_hashes,
        ))
    }
```

**File:** storage/storage-interface/src/state_store/state_summary.rs (L84-111)
```rust
    fn update(
        &self,
        persisted: &ProvableStateSummary,
        hot_updates: &[HotStateShardUpdates; NUM_STATE_SHARDS],
        updates: &BatchedStateUpdateRefs,
    ) -> Result<Self> {
        let _timer = TIMER.timer_with(&["state_summary__update"]);

        assert_ne!(self.hot_state_summary.root_hash(), *CORRUPTION_SENTINEL);
        assert_ne!(self.global_state_summary.root_hash(), *CORRUPTION_SENTINEL);

        // Persisted must be before or at my version.
        assert!(persisted.next_version() <= self.next_version());
        // Updates must start at exactly my version.
        assert_eq!(updates.first_version(), self.next_version());

        let (hot_smt_result, smt_result) = rayon::join(
            || self.update_hot_state_summary(persisted, hot_updates),
            || self.update_global_state_summary(persisted, updates),
        );

        Ok(Self {
            next_version: updates.next_version(),
            hot_state_summary: hot_smt_result?,
            global_state_summary: smt_result?,
            hot_state_config: self.hot_state_config,
        })
    }
```

**File:** execution/executor-types/src/execution_output.rs (L149-176)
```rust
#[derive(Debug)]
pub struct Inner {
    pub is_block: bool,
    pub first_version: Version,
    // Statuses of the input transactions, in the same order as the input transactions.
    // Contains BlockMetadata/Validator transactions,
    // but doesn't contain StateCheckpoint/BlockEpilogue, as those get added during execution
    pub statuses_for_input_txns: Vec<TransactionStatus>,
    // List of all transactions to be committed, including StateCheckpoint/BlockEpilogue if needed.
    pub to_commit: TransactionsToKeep,
    pub to_discard: TransactionsWithOutput,
    pub to_retry: TransactionsWithOutput,

    pub result_state: LedgerState,
    /// State items read during execution, useful for calculating the state storge usage and
    /// indices used by the db pruner.
    pub state_reads: ShardedStateCache,
    /// Updates to hot state, mainly used to compute hot state root hashes.
    pub hot_state_updates: HotStateUpdates,

    /// Optional StateCheckpoint payload
    pub block_end_info: Option<BlockEndInfo>,
    /// Optional EpochState payload.
    /// Only present if the block is the last block of an epoch, and is parsed output of the
    /// state cache.
    pub next_epoch_state: Option<EpochState>,
    pub subscribable_events: Planned<Vec<ContractEvent>>,
}
```

**File:** storage/storage-interface/src/state_store/state.rs (L370-385)
```rust
    fn expect_old_slot(
        overlay: &LayeredMap<StateKey, StateSlot>,
        cache: &StateCacheShard,
        key: &StateKey,
    ) -> StateSlot {
        if let Some(slot) = overlay.get(key) {
            return slot;
        }

        // TODO(aldenhu): avoid cloning the state value (by not using DashMap)
        cache
            .get(key)
            .unwrap_or_else(|| panic!("Key {:?} must exist in the cache.", key))
            .value()
            .clone()
    }
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L283-297)
```rust
    fn get_state_slot(&self, state_key: &StateKey) -> StateViewResult<StateSlot> {
        let _timer = TIMER.timer_with(&["get_state_value"]);
        COUNTER.inc_with(&["sv_total_get"]);

        // First check if requested key is already memorized.
        if let Some(slot) = self.memorized.get_cloned(state_key) {
            COUNTER.inc_with(&["sv_memorized"]);
            return Ok(slot);
        }

        // TODO(aldenhu): reduce duplicated gets
        let slot = self.get_unmemorized(state_key)?;
        self.memorized.try_insert(state_key, &slot);
        Ok(slot)
    }
```
