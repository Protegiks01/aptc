# Audit Report

## Title
Runtime Handle Leakage via Reference Passing Allows Unbounded Task Spawning in Consensus Observer

## Summary
Passing `consensus_observer_runtime` by reference to `start_consensus_observer()` does not properly restrict task spawning. The function clones the runtime handle and stores it in components that subsequently spawn unlimited tasks using `tokio::spawn()`, completely bypassing the `BoundedExecutor`'s 32-task limit designed to prevent resource exhaustion.

## Finding Description

The vulnerability exists in how the consensus observer runtime is managed during initialization. The architecture introduces a resource limit bypass through the following chain: [1](#0-0) 

The `create_consensus_observer()` function receives the runtime by reference and passes it to `start_consensus_observer()`: [2](#0-1) 

Inside `start_consensus_observer()`, the runtime handle is cloned and stored in a `BoundedExecutor` with a capacity limit of 32: [3](#0-2) 

This `BoundedExecutor` is then stored in `ExecutionProxyClient` and passed to various consensus components. However, when these components spawn tasks, they use `tokio::spawn()` instead of the bounded executor: [4](#0-3) [5](#0-4) [6](#0-5) 

Since `tokio::spawn()` spawns on the current runtime context (which is `consensus_observer_runtime` due to the initial spawn at line 202 in consensus_provider.rs), these tasks bypass the BoundedExecutor's 32-task limit entirely. Each epoch change spawns at least 7 new tasks (rand_manager, secret_share_manager, and 5 pipeline phases), with no mechanism to prevent accumulation over multiple epoch transitions.

## Impact Explanation

**Medium Severity** - This violates resource limit invariants and could lead to node slowdown or degraded consensus performance. While it doesn't directly cause consensus safety violations or fund loss, it creates a resource exhaustion vector that affects validator node availability and responsiveness, qualifying as "Validator node slowdowns" under High Severity or "State inconsistencies requiring intervention" under Medium Severity.

The impact is limited because:
- Requires repeated epoch transitions (natural rate limiting via consensus protocol)
- Affects node performance rather than consensus safety directly
- Observable through monitoring and can be mitigated by node restart

## Likelihood Explanation

**Low-Medium Likelihood** - The vulnerability is triggered automatically during normal consensus observer operation whenever `start_epoch()` is called. While epoch transitions are infrequent (hours to days), each transition spawns unbounded tasks. A malicious peer could potentially craft messages to trigger repeated subscription changes or fallback mode transitions, increasing the spawn rate, though this requires specific network positioning.

## Recommendation

Replace `tokio::spawn()` calls with bounded executor spawning to enforce the intended resource limits:

```rust
// In execution_client.rs, replace tokio::spawn with:
self.bounded_executor.spawn(rand_manager.start(...)).await;
self.bounded_executor.spawn(secret_share_manager.start(...)).await;
self.bounded_executor.spawn(execution_schedule_phase.start()).await;
// ... etc for all pipeline phases
```

Alternatively, if different task types require different limits, create separate bounded executors with appropriate capacities for pipeline phases vs. verification tasks, and pass the runtime by value or use a more restrictive API that prevents handle cloning.

## Proof of Concept

```rust
// Rust integration test demonstrating unbounded spawning
#[tokio::test]
async fn test_consensus_observer_unbounded_spawning() {
    use tokio::runtime::Runtime;
    use std::sync::Arc;
    use std::sync::atomic::{AtomicUsize, Ordering};
    
    // Create a runtime similar to consensus_observer_runtime
    let runtime = Runtime::new().unwrap();
    let spawn_counter = Arc::new(AtomicUsize::new(0));
    
    // Simulate multiple epoch transitions
    for _epoch in 0..10 {
        let counter_clone = spawn_counter.clone();
        runtime.spawn(async move {
            // Simulate what start_consensus_observer does
            // Each epoch spawns multiple tasks via tokio::spawn
            for _i in 0..7 {  // 7 tasks per epoch
                let counter = counter_clone.clone();
                tokio::spawn(async move {
                    counter.fetch_add(1, Ordering::SeqCst);
                    tokio::time::sleep(tokio::time::Duration::from_secs(10)).await;
                });
            }
        });
    }
    
    tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;
    
    // Without proper bounding, this will show 70 tasks spawned
    // instead of being limited to 32
    let spawned = spawn_counter.load(Ordering::SeqCst);
    assert!(spawned > 32, "Spawned {} tasks, exceeding 32-task limit", spawned);
}
```

### Citations

**File:** aptos-node/src/consensus.rs (L207-237)
```rust
fn create_consensus_observer(
    node_config: &NodeConfig,
    consensus_observer_runtime: &Runtime,
    consensus_observer_client: Arc<
        ConsensusObserverClient<NetworkClient<ConsensusObserverMessage>>,
    >,
    consensus_observer_message_receiver: Receiver<(), ConsensusObserverNetworkMessage>,
    consensus_publisher: Option<Arc<ConsensusPublisher>>,
    state_sync_notifier: ConsensusNotifier,
    consensus_to_mempool_sender: Sender<QuorumStoreRequest>,
    db_rw: DbReaderWriter,
    observer_reconfig_subscription: Option<ReconfigNotificationListener<DbBackedOnChainConfig>>,
) {
    // If the observer is not enabled, return early
    if !node_config.consensus_observer.observer_enabled {
        return;
    }

    // Create the consensus observer
    start_consensus_observer(
        node_config,
        consensus_observer_runtime,
        consensus_observer_client,
        consensus_observer_message_receiver,
        consensus_publisher,
        Arc::new(state_sync_notifier),
        consensus_to_mempool_sender,
        db_rw,
        observer_reconfig_subscription,
    );
}
```

**File:** consensus/src/consensus_provider.rs (L127-139)
```rust
pub fn start_consensus_observer(
    node_config: &NodeConfig,
    consensus_observer_runtime: &Runtime,
    consensus_observer_client: Arc<
        ConsensusObserverClient<NetworkClient<ConsensusObserverMessage>>,
    >,
    consensus_observer_message_receiver: Receiver<(), ConsensusObserverNetworkMessage>,
    consensus_publisher: Option<Arc<ConsensusPublisher>>,
    state_sync_notifier: Arc<dyn ConsensusNotificationSender>,
    consensus_to_mempool_sender: mpsc::Sender<QuorumStoreRequest>,
    aptos_db: DbReaderWriter,
    reconfig_events: Option<ReconfigNotificationListener<DbBackedOnChainConfig>>,
) {
```

**File:** consensus/src/consensus_provider.rs (L168-169)
```rust
        let bounded_executor =
            BoundedExecutor::new(32, consensus_observer_runtime.handle().clone());
```

**File:** consensus/src/pipeline/execution_client.rs (L253-259)
```rust
        tokio::spawn(rand_manager.start(
            ordered_block_rx,
            rand_msg_rx,
            reset_rand_manager_rx,
            self.bounded_executor.clone(),
            highest_committed_round,
        ));
```

**File:** consensus/src/pipeline/execution_client.rs (L296-300)
```rust
        tokio::spawn(secret_share_manager.start(
            ordered_block_rx,
            secret_sharing_msg_rx,
            reset_secret_share_manager_rx,
            self.bounded_executor.clone(),
```

**File:** consensus/src/pipeline/execution_client.rs (L512-516)
```rust
        tokio::spawn(execution_schedule_phase.start());
        tokio::spawn(execution_wait_phase.start());
        tokio::spawn(signing_phase.start());
        tokio::spawn(persisting_phase.start());
        tokio::spawn(buffer_manager.start());
```
