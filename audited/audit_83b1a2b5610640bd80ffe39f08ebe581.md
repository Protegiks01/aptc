# Audit Report

## Title
Race Condition in sync_for_duration() Allows BlockExecutor Reset During Active Pipeline Execution

## Summary
The `sync_for_duration()` path in `ExecutionClient` calls `executor.reset()` before aborting in-flight pipeline tasks, creating a race condition where `BlockExecutorInner` is initialized with a stale database snapshot that doesn't include blocks currently being executed by the consensus pipeline.

## Finding Description
The vulnerability stems from improper ordering of operations in the state synchronization flow. In `sync_for_duration()`, the execution proxy sync (which resets the executor) occurs BEFORE pipeline task abortion: [1](#0-0) 

This ordering is reversed compared to `sync_to_target()`, which correctly aborts pipeline tasks FIRST: [2](#0-1) 

The race condition occurs when:

1. Pipeline task executes `execute_and_update_state()`, adding a block to the in-memory `BlockTree`
2. The task releases `execution_lock` and returns (but `ledger_update()` hasn't been called yet)
3. `sync_for_duration()` is triggered, acquiring `write_mutex`
4. `executor.finish()` sets `inner` to `None`, then `executor.reset()` creates a new `BlockExecutorInner`: [3](#0-2) 

5. The new `BlockExecutorInner` reads from the database via `BlockTree::new()`: [4](#0-3) [5](#0-4) [6](#0-5) 

6. The database doesn't contain the block that was just executed (not yet committed)
7. Pipeline task tries to call `ledger_update()` with the block ID, but the new `BlockExecutorInner` doesn't have it: [7](#0-6) [8](#0-7) 

The synchronization locks don't prevent this race because:
- `execution_lock` only protects `execute_and_update_state()` calls, not the gap between execution and ledger_update: [9](#0-8) 

- `write_mutex` in `ExecutionProxy` doesn't coordinate with pipeline execution
- Pipeline tasks spawn asynchronously without acquiring `write_mutex`: [10](#0-9) [11](#0-10) 

## Impact Explanation
**Severity: Medium**

This vulnerability causes state inconsistencies requiring intervention. When the race condition is triggered:

1. `ledger_update()` fails with `BlockNotFound` error, causing pipeline task failures
2. The error is logged and the block is dropped from the pipeline: [12](#0-11) 

3. The validator may enter an inconsistent state where executed blocks are lost
4. Breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable"

While this doesn't directly lead to consensus violations or fund loss, it can cause validator nodes to fail or require manual intervention to recover, meeting the Medium severity criteria of "State inconsistencies requiring intervention" per Aptos bug bounty guidelines.

## Likelihood Explanation
**Likelihood: Medium**

This race condition can occur during normal operations:
- Validators regularly fall behind and trigger state sync
- High transaction throughput increases the window for the race
- No attacker control required - it's a natural timing-dependent bug

However, it requires precise timing where:
- A block has completed `execute_and_update_state()` but not yet called `ledger_update()`
- State sync is triggered in this narrow window

The window is small but non-zero, especially under load when multiple blocks are in the pipeline simultaneously.

## Recommendation
Reorder the operations in `sync_for_duration()` to match `sync_to_target()`:

```rust
async fn sync_for_duration(
    &self,
    duration: Duration,
) -> Result<LedgerInfoWithSignatures, StateSyncError> {
    // Reset the rand and buffer managers FIRST to abort pipeline tasks
    let result = self.execution_proxy.sync_for_duration(duration).await;
    
    if let Ok(latest_synced_ledger_info) = &result {
        self.reset(latest_synced_ledger_info).await?;
    }
    
    result
}
```

Actually, the correct fix is to call `self.reset()` BEFORE `execution_proxy.sync_for_duration()`, similar to `sync_to_target()`.

## Proof of Concept
The race can be demonstrated by:
1. Setting up a validator under load with multiple blocks in the pipeline
2. Triggering `sync_for_duration()` while blocks are being executed
3. Observing `BlockNotFound` errors in validator logs when `ledger_update()` is called on blocks that were executed before the reset but haven't completed the full pipeline

## Notes
This is a reliability/availability issue that affects single validator operation. While it doesn't directly compromise consensus or cause fund loss across the network, it meets the Aptos bug bounty Medium severity criteria for "state inconsistencies requiring intervention." The fact that `sync_to_target()` has the correct ordering suggests this is an oversight in `sync_for_duration()` rather than intentional design.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L642-659)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Sync for the specified duration
        let result = self.execution_proxy.sync_for_duration(duration).await;

        // Reset the rand and buffer managers to the new synced round
        if let Ok(latest_synced_ledger_info) = &result {
            self.reset(latest_synced_ledger_info).await?;
        }

        result
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L661-672)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/state_computer.rs (L132-174)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        // Grab the logical time lock
        let mut latest_logical_time = self.write_mutex.lock().await;

        // Before state synchronization, we have to call finish() to free the
        // in-memory SMT held by the BlockExecutor to prevent a memory leak.
        self.executor.finish();

        // Inject an error for fail point testing
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Invoke state sync to synchronize for the specified duration. Here, the
        // ChunkExecutor will process chunks and commit to storage. However, after
        // block execution and commits, the internal state of the ChunkExecutor may
        // not be up to date. So, it is required to reset the cache of the
        // ChunkExecutor in state sync when requested to sync.
        let result = monitor!(
            "sync_for_duration",
            self.state_sync_notifier.sync_for_duration(duration).await
        );

        // Update the latest logical time
        if let Ok(latest_synced_ledger_info) = &result {
            let ledger_info = latest_synced_ledger_info.ledger_info();
            let synced_logical_time = LogicalTime::new(ledger_info.epoch(), ledger_info.round());
            *latest_logical_time = synced_logical_time;
        }

        // Similarly, after state synchronization, we have to reset the cache of
        // the BlockExecutor to guarantee the latest committed state is up to date.
        self.executor.reset()?;

        // Return the result
        result.map_err(|error| {
            let anyhow_error: anyhow::Error = error.into();
            anyhow_error.into()
        })
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L90-95)
```rust
    fn reset(&self) -> Result<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "reset"]);

        *self.inner.write() = Some(BlockExecutorInner::new(self.db.clone())?);
        Ok(())
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L97-113)
```rust
    fn execute_and_update_state(
        &self,
        block: ExecutableBlock,
        parent_block_id: HashValue,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> ExecutorResult<()> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "execute_and_state_checkpoint"]);

        self.maybe_initialize()?;
        // guarantee only one block being executed at a time
        let _guard = self.execution_lock.lock();
        self.inner
            .read()
            .as_ref()
            .expect("BlockExecutor is not reset")
            .execute_and_update_state(block, parent_block_id, onchain_config)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L115-129)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["block", "ledger_update"]);

        self.inner
            .read()
            .as_ref()
            .ok_or_else(|| ExecutorError::InternalError {
                error: "BlockExecutor is not reset".into(),
            })?
            .ledger_update(block_id, parent_block_id)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L173-180)
```rust
    pub fn new(db: DbReaderWriter) -> Result<Self> {
        let block_tree = BlockTree::new(&db.reader)?;
        Ok(Self {
            db,
            block_tree,
            block_executor: V::new(),
        })
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L260-285)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _timer = UPDATE_LEDGER.start_timer();
        info!(
            LogSchema::new(LogEntry::BlockExecutor).block_id(block_id),
            "ledger_update"
        );
        let committed_block_id = self.committed_block_id();
        let mut block_vec = self
            .block_tree
            .get_blocks_opt(&[block_id, parent_block_id])?;
        let parent_block = block_vec
            .pop()
            .expect("Must exist.")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
        // At this point of time two things must happen
        // 1. The block tree must also have the current block id with or without the ledger update output.
        // 2. We must have the ledger update output of the parent block.
        // Above is not ture if the block is on a forked branch.
        let block = block_vec
            .pop()
            .expect("Must exist")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
```

**File:** execution/executor/src/block_executor/block_tree/mod.rs (L179-184)
```rust
    pub fn new(db: &Arc<dyn DbReader>) -> Result<Self> {
        let block_lookup = Arc::new(BlockLookup::new());
        let root = Mutex::new(Self::root_from_db(&block_lookup, db)?);

        Ok(Self { root, block_lookup })
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L857-869)
```rust
        tokio::task::spawn_blocking(move || {
            executor
                .execute_and_update_state(
                    (block.id(), txns, auxiliary_info).into(),
                    block.parent_id(),
                    onchain_execution_config,
                )
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
        Ok(start.elapsed())
    }
```

**File:** consensus/src/pipeline/pipeline_builder.rs (L887-893)
```rust
        let result = tokio::task::spawn_blocking(move || {
            executor
                .ledger_update(block_clone.id(), block_clone.parent_id())
                .map_err(anyhow::Error::from)
        })
        .await
        .expect("spawn blocking failed")?;
```

**File:** consensus/src/pipeline/buffer_manager.rs (L617-626)
```rust
        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
```
