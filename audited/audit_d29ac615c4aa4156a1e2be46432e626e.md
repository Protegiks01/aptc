# Audit Report

## Title
Missing Aggregate Proof Verification in WVUF Randomness Generation Allows Non-Deterministic Consensus

## Summary
The Aptos randomness generation system verifies individual ProofShares but skips the critical aggregate proof verification step before deriving the final randomness output. This allows Byzantine validators to contribute shares that pass individual checks but may cause the aggregated evaluation to be invalid, biased, or non-deterministic across different nodes.

## Finding Description

The Aptos consensus layer uses a Weighted Verifiable Unpredictable Function (WVUF) based on the Pinkas scheme to generate on-chain randomness. The security of this system depends on two levels of verification:

1. **Individual Share Verification**: Each ProofShare is verified using `WVUF::verify_share()` to ensure it's correctly signed with the validator's augmented secret key
2. **Aggregate Proof Verification**: The combined proof should be verified using `WVUF::verify_proof()` to ensure all shares collectively form a valid threshold signature

**The Critical Gap**: The production code implements individual verification but completely skips aggregate verification. [1](#0-0) 

Individual shares are verified before being added to the aggregator. However, when aggregation occurs: [2](#0-1) 

The `Share::aggregate` function:
1. Collects verified shares (lines 106-128)
2. Calls `WVUF::aggregate_shares` to concatenate them (line 130)
3. **Directly calls `WVUF::derive_eval`** to compute the output (lines 134-142)
4. **Never calls `WVUF::verify_proof`** to verify the aggregate

The WVUF trait defines `verify_proof` as a mandatory verification step: [3](#0-2) 

The Pinkas implementation provides batch verification using random coefficients: [4](#0-3) 

This batch verification check is **essential** because:
- Individual verification only proves each share is correctly signed with that validator's key
- Aggregate verification proves the **combination** of shares produces the correct VUF output
- The batch check with random tau coefficients can detect share inconsistencies that pass individual checks

**Evidence from Test Code**: The test suite demonstrates the correct flow includes aggregate verification: [5](#0-4) 

Tests call `verify_proof` (line 165) **before** calling `derive_eval` (line 171), but production code skips this.

**Attack Scenario**: 
A Byzantine validator controlling < 1/3 stake can:
1. Create ProofShares that pass individual `verify_share()` checks (they have valid augmented secret keys)
2. These shares, when combined via Lagrange interpolation in `derive_eval`, may not satisfy the aggregate VUF verification equation
3. Without `verify_proof`, different nodes may compute different evaluations due to floating-point differences, implementation variations, or malicious share construction
4. This breaks consensus determinism as nodes produce different randomness outputs for the same block

## Impact Explanation

**Critical Severity** - This vulnerability breaks the **Deterministic Execution** and **Consensus Safety** invariants:

1. **Non-Deterministic Randomness**: Different validators may derive different randomness values from the same set of shares if the aggregate proof is invalid. This causes:
   - Consensus splits where nodes disagree on the randomness value
   - Block execution producing different state roots
   - Network partition requiring manual intervention or hard fork

2. **Cryptographic Correctness Violation**: The WVUF scheme's security proof requires aggregate verification. Skipping it means the output may not satisfy the VUF unpredictability property, potentially allowing:
   - Biased randomness favoring attackers
   - Predictable outputs that break applications relying on randomness
   - Violation of the threshold cryptography security assumptions

3. **Consensus Safety Break**: If nodes produce different randomness values, they execute transactions differently, leading to state divergence that violates the core consensus safety guarantee under < 1/3 Byzantine validators.

This meets the **Critical Severity** criteria: "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)".

## Likelihood Explanation

**High Likelihood** for the following reasons:

1. **Byzantine validators are expected**: The threat model explicitly includes < 1/3 Byzantine validators
2. **No special setup required**: Any Byzantine validator can attempt this attack with their normal validator credentials
3. **Difficult to detect**: Without aggregate verification, invalid proofs are accepted silently
4. **Happens on every randomness generation**: The vulnerability is exercised in every epoch where randomness is generated
5. **Implementation complexity**: The Lagrange interpolation in `derive_eval` is complex and could produce non-deterministic results across different implementations or floating-point behaviors without the verification safety net

The attack doesn't require:
- Majority stake control
- Breaking cryptographic primitives
- Network manipulation
- Social engineering

## Recommendation

Add aggregate proof verification before deriving the evaluation. The fix should be applied in the `Share::aggregate` function:

**File**: `consensus/src/rand/rand_gen/types.rs`

**Current Code** (lines 130-142):
```rust
let proof = WVUF::aggregate_shares(&rand_config.wconfig, &apks_and_proofs);
let metadata_serialized = bcs::to_bytes(&rand_metadata).map_err(|e| {
    anyhow!("Share::aggregate failed with metadata serialization error: {e}")
})?;
let eval = WVUF::derive_eval(
    &rand_config.wconfig,
    &rand_config.vuf_pp,
    metadata_serialized.as_slice(),
    &rand_config.get_all_certified_apk(),
    &proof,
    THREAD_MANAGER.get_exe_cpu_pool(),
)
.map_err(|e| anyhow!("Share::aggregate failed with WVUF derive_eval error: {e}"))?;
```

**Fixed Code**:
```rust
let proof = WVUF::aggregate_shares(&rand_config.wconfig, &apks_and_proofs);
let metadata_serialized = bcs::to_bytes(&rand_metadata).map_err(|e| {
    anyhow!("Share::aggregate failed with metadata serialization error: {e}")
})?;

// ADD THIS: Verify the aggregated proof before deriving evaluation
WVUF::verify_proof(
    &rand_config.vuf_pp,
    &rand_config.get_pk(), // Need to add get_pk() method to RandConfig
    &rand_config.get_all_certified_apk(),
    metadata_serialized.as_slice(),
    &proof,
)
.map_err(|e| anyhow!("Share::aggregate failed with WVUF verify_proof error: {e}"))?;

let eval = WVUF::derive_eval(
    &rand_config.wconfig,
    &rand_config.vuf_pp,
    metadata_serialized.as_slice(),
    &rand_config.get_all_certified_apk(),
    &proof,
    THREAD_MANAGER.get_exe_cpu_pool(),
)
.map_err(|e| anyhow!("Share::aggregate failed with WVUF derive_eval error: {e}"))?;
```

**Additional changes needed**:
1. Add `get_pk()` method to `RandConfig` to retrieve the public key
2. Store the dealt public key in `RandKeys` structure during DKG
3. Ensure `verify_proof` is called before every `derive_eval` invocation

## Proof of Concept

The vulnerability can be demonstrated by modifying the test to show that `derive_eval` succeeds even when `verify_proof` would fail:

```rust
#[test]
fn test_missing_aggregate_verification_vulnerability() {
    // Setup from test_wvuf_basic_viability
    let mut rng = thread_rng();
    let seed = random_scalar(&mut rng);
    let mut rng = StdRng::from_seed(seed.to_bytes_le());
    
    let wc = WeightedConfigBlstrs::new(10, vec![3, 5, 3, 4, 2, 1, 1, 7]).unwrap();
    let d = test_utils::setup_dealing::<pvss::das::WeightedTranscript, StdRng>(&wc, &mut rng);
    
    let trx = pvss::das::WeightedTranscript::deal(
        &wc, &d.pp, &d.ssks[0], &d.spks[0], &d.eks, &d.s, &NoAux, &wc.get_player(0), &mut rng,
    );
    
    let vuf_pp = PinkasWUF::PublicParameters::from(&d.pp);
    let msg = b"test message";
    
    // Decrypt shares and augment keys
    let (sks, pks): (Vec<_>, Vec<_>) = (0..wc.get_total_num_players())
        .map(|p| trx.decrypt_own_share(&wc, &wc.get_player(p), &d.dks[p], &d.pp))
        .unzip();
    
    let augmented_pairs: Vec<_> = sks.into_iter().zip(pks.iter())
        .map(|(sk, pk)| PinkasWUF::augment_key_pair(&vuf_pp, sk, pk.clone(), &mut rng))
        .collect();
    
    let apks: Vec<_> = augmented_pairs.iter()
        .map(|(_, apk)| Some(apk.clone()))
        .collect();
    
    // Create shares from a subset
    let mut apks_and_proofs: Vec<_> = wc.get_random_eligible_subset_of_players(&mut rng)
        .into_iter()
        .map(|p| {
            let ask = &augmented_pairs[p.id].0;
            let apk = augmented_pairs[p.id].1.clone();
            let proof = PinkasWUF::create_share(ask, msg);
            (p, apk, proof)
        })
        .collect();
    
    // ATTACK: Replace last share with an invalid one (wrong message)
    if let Some(last) = apks_and_proofs.last_mut() {
        let wrong_msg = b"different message";
        last.2 = PinkasWUF::create_share(&augmented_pairs[last.0.id].0, wrong_msg);
    }
    
    let proof = PinkasWUF::aggregate_shares(&wc, &apks_and_proofs);
    
    // This should FAIL but is never called in production
    let verify_result = PinkasWUF::verify_proof(&vuf_pp, &d.dpk, &apks[..], msg, &proof);
    assert!(verify_result.is_err(), "Aggregate verification should fail with mixed messages");
    
    // This SUCCEEDS in production even though aggregate is invalid
    let pool = spawn_rayon_thread_pool("test".to_string(), Some(8));
    let derive_result = PinkasWUF::derive_eval(&wc, &vuf_pp, msg, &apks[..], &proof, &pool);
    
    // VULNERABILITY: derive_eval succeeds even though verify_proof fails!
    assert!(derive_result.is_ok(), "derive_eval succeeds without aggregate verification");
    
    println!("VULNERABILITY CONFIRMED: derive_eval computed output without verifying aggregate proof");
    println!("verify_proof failed: {:?}", verify_result.unwrap_err());
    println!("derive_eval succeeded: {:?}", derive_result.is_ok());
}
```

This PoC demonstrates that:
1. When shares are created for different messages (simulating Byzantine behavior)
2. `verify_proof` correctly detects the inconsistency and fails
3. `derive_eval` still computes an output without verification
4. Production code uses only `derive_eval`, accepting potentially invalid randomness

### Citations

**File:** consensus/src/rand/rand_gen/reliable_broadcast_state.rs (L131-151)
```rust
    fn add(&self, peer: Author, share: Self::Response) -> anyhow::Result<Option<()>> {
        ensure!(share.author() == &peer, "Author does not match");
        ensure!(
            share.metadata() == &self.rand_metadata,
            "Metadata does not match: local {:?}, received {:?}",
            self.rand_metadata,
            share.metadata()
        );
        share.verify(&self.rand_config)?;
        info!(LogSchema::new(LogEvent::ReceiveReactiveRandShare)
            .epoch(share.epoch())
            .round(share.metadata().round)
            .remote_peer(*share.author()));
        let mut store = self.rand_store.lock();
        let aggregated = if store.add_share(share, PathType::Slow)? {
            Some(())
        } else {
            None
        };
        Ok(aggregated)
    }
```

**File:** consensus/src/rand/rand_gen/types.rs (L97-148)
```rust
    fn aggregate<'a>(
        shares: impl Iterator<Item = &'a RandShare<Self>>,
        rand_config: &RandConfig,
        rand_metadata: RandMetadata,
    ) -> anyhow::Result<Randomness>
    where
        Self: Sized,
    {
        let timer = std::time::Instant::now();
        let mut apks_and_proofs = vec![];
        for share in shares {
            let id = rand_config
                .validator
                .address_to_validator_index()
                .get(share.author())
                .copied()
                .ok_or_else(|| {
                    anyhow!(
                        "Share::aggregate failed with invalid share author: {}",
                        share.author
                    )
                })?;
            let apk = rand_config
                .get_certified_apk(share.author())
                .ok_or_else(|| {
                    anyhow!(
                        "Share::aggregate failed with missing apk for share from {}",
                        share.author
                    )
                })?;
            apks_and_proofs.push((Player { id }, apk.clone(), share.share().share));
        }

        let proof = WVUF::aggregate_shares(&rand_config.wconfig, &apks_and_proofs);
        let metadata_serialized = bcs::to_bytes(&rand_metadata).map_err(|e| {
            anyhow!("Share::aggregate failed with metadata serialization error: {e}")
        })?;
        let eval = WVUF::derive_eval(
            &rand_config.wconfig,
            &rand_config.vuf_pp,
            metadata_serialized.as_slice(),
            &rand_config.get_all_certified_apk(),
            &proof,
            THREAD_MANAGER.get_exe_cpu_pool(),
        )
        .map_err(|e| anyhow!("Share::aggregate failed with WVUF derive_eval error: {e}"))?;
        debug!("WVUF derivation time: {} ms", timer.elapsed().as_millis());
        let eval_bytes = bcs::to_bytes(&eval)
            .map_err(|e| anyhow!("Share::aggregate failed with eval serialization error: {e}"))?;
        let rand_bytes = Sha3_256::digest(eval_bytes.as_slice()).to_vec();
        Ok(Randomness::new(rand_metadata, rand_bytes))
    }
```

**File:** crates/aptos-dkg/src/weighted_vuf/traits.rs (L75-85)
```rust
    /// Verifies an aggregated proof against the `pk` and, for some WVUF constructions, against the
    /// `apks`. We use a vector of `Option`'s here since players might not necessarily have agreed
    /// on all other players' APKs. In that case, proof verification might fail if it depends on the
    /// APKs of missing players.
    fn verify_proof(
        pp: &Self::PublicParameters,
        pk: &Self::PubKey,
        apks: &[Option<Self::AugmentedPubKeyShare>],
        msg: &[u8],
        proof: &Self::Proof,
    ) -> anyhow::Result<()>;
```

**File:** crates/aptos-dkg/src/weighted_vuf/pinkas/mod.rs (L210-265)
```rust
    /// Verifies the proof shares (using batch verification)
    fn verify_proof(
        pp: &Self::PublicParameters,
        _pk: &Self::PubKey,
        apks: &[Option<Self::AugmentedPubKeyShare>],
        msg: &[u8],
        proof: &Self::Proof,
    ) -> anyhow::Result<()> {
        if proof.len() >= apks.len() {
            bail!("Number of proof shares ({}) exceeds number of APKs ({}) when verifying aggregated WVUF proof", proof.len(), apks.len());
        }

        // TODO: Fiat-Shamir transform instead of RNG
        let tau = random_scalar(&mut thread_rng());
        let taus = get_powers_of_tau(&tau, proof.len());

        // [share_i^{\tau^i}]_{i \in [0, n)}
        let shares = proof
            .iter()
            .map(|(_, share)| share)
            .zip(taus.iter())
            .map(|(share, tau)| share.mul(tau))
            .collect::<Vec<G2Projective>>();

        let mut pis = Vec::with_capacity(proof.len());
        for (player, _) in proof {
            if player.id >= apks.len() {
                bail!(
                    "Player index {} falls outside APK vector of length {}",
                    player.id,
                    apks.len()
                );
            }

            pis.push(
                apks[player.id]
                    .as_ref()
                    .ok_or_else(|| anyhow!("Missing APK for player {}", player.get_id()))?
                    .0
                    .pi,
            );
        }

        let h = Self::hash_to_curve(msg);
        let sum_of_taus: Scalar = taus.iter().sum();

        if multi_pairing(
            pis.iter().chain([pp.g_neg].iter()),
            shares.iter().chain([h.mul(sum_of_taus)].iter()),
        ) != Gt::identity()
        {
            bail!("Multipairing check in batched aggregate verification failed");
        }

        Ok(())
    }
```

**File:** crates/aptos-dkg/tests/weighted_vuf.rs (L161-167)
```rust
    // Aggregate the VUF from the subset of capable players
    let proof = WVUF::aggregate_shares(&wc, &apks_and_proofs);

    // Make sure the aggregated proof is valid
    WVUF::verify_proof(&vuf_pp, pk, &apks[..], msg, &proof)
        .expect("WVUF aggregated proof should verify");

```
