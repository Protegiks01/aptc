# Audit Report

## Title
Unbounded EpochChangeProof Verification Enables Resource Exhaustion Attack on Validators

## Summary
The `EpochChangeProof::verify()` function lacks a size limit check before verification, allowing an attacker to send a proof containing up to 200 valid epoch ending ledger infos. Each ledger info requires expensive BLS signature verification executed sequentially in the consensus critical path, enabling resource exhaustion and validator slowdown attacks.

## Finding Description

The vulnerability exists in the `EpochChangeProof::verify()` method which iterates through all ledger infos in a proof and performs BLS signature verification on each one without validating the proof size beforehand. [1](#0-0) 

When a validator receives a `ConsensusMsg::EpochChangeProof` message from a network peer, it processes the message in the epoch manager: [2](#0-1) 

The `initiate_new_epoch()` method then calls `proof.verify()` without any size validation: [3](#0-2) 

**Attack Path:**

1. **Attacker collects legitimate epoch ending ledger infos**: The attacker queries the blockchain and collects a sequence of contiguous, properly-signed epoch ending ledger infos (e.g., epochs 100-299).

2. **Attacker crafts oversized proof**: The attacker constructs an `EpochChangeProof` containing up to 200 ledger infos (limited by `MAX_EPOCH_CHUNK_SIZE`). [4](#0-3) 

3. **Attacker sends to target validator**: The attacker sends the proof as a `ConsensusMsg::EpochChangeProof` to a validator at epoch 100.

4. **Validator verifies all ledger infos**: The validator's epoch manager checks that `msg_epoch == self.epoch()` (both 100), then calls `initiate_new_epoch()` which calls `verify()`. The `skip_while` clause only skips ledger infos where `ledger_info.epoch() < self.epoch`, so no ledger infos are skipped since all are >= 100. [5](#0-4) 

5. **Expensive BLS verification**: Each ledger info undergoes BLS signature verification which involves public key aggregation and cryptographic operations: [6](#0-5) 

6. **Resource exhaustion**: With 200 ledger infos, each requiring milliseconds of BLS verification, the total processing time becomes significant (multiple seconds), blocking the validator's consensus progress.

## Impact Explanation

This vulnerability qualifies as **High Severity** per the Aptos bug bounty criteria:
- **Validator node slowdowns**: The attack directly causes validators to spend excessive CPU time on BLS signature verification in the consensus critical path
- **Resource exhaustion**: Repeated or concurrent attacks can overwhelm validator resources
- **Liveness impact**: During epoch transitions, validators become unresponsive for extended periods

The attack affects the validator's ability to participate in consensus during epoch transitions, potentially degrading network liveness if multiple validators are targeted simultaneously.

## Likelihood Explanation

This attack is **highly likely** to occur because:

1. **No authentication required**: Any network peer can send `ConsensusMsg::EpochChangeProof` messages
2. **Easy to exploit**: Attackers only need to collect legitimate historical epoch ending ledger infos from the blockchain
3. **Minimal resources needed**: The attack doesn't require validator privileges or significant computational resources from the attacker
4. **Critical path exposure**: The verification happens synchronously in `initiate_new_epoch()` which blocks epoch transitions
5. **No rate limiting**: There's no apparent rate limiting on epoch change proof messages before verification

The attack complexity is LOW and the impact is MEDIUM-HIGH, making this a realistic threat.

## Recommendation

Implement a maximum size check for `EpochChangeProof` before verification:

```rust
pub fn verify(&self, verifier: &dyn Verifier) -> Result<&LedgerInfoWithSignatures> {
    const MAX_EPOCH_PROOF_SIZE: usize = 10; // Reasonable limit for epoch jumps
    
    ensure!(
        !self.ledger_info_with_sigs.is_empty(),
        "The EpochChangeProof is empty"
    );
    
    // NEW: Enforce maximum proof size after accounting for stale entries
    let non_stale_count = self
        .ledger_info_with_sigs
        .iter()
        .filter(|li| !verifier.is_ledger_info_stale(li.ledger_info()))
        .count();
    
    ensure!(
        non_stale_count <= MAX_EPOCH_PROOF_SIZE,
        "EpochChangeProof contains too many non-stale ledger infos: {}, max allowed: {}",
        non_stale_count,
        MAX_EPOCH_PROOF_SIZE
    );
    
    ensure!(
        !verifier
            .is_ledger_info_stale(self.ledger_info_with_sigs.last().unwrap().ledger_info()),
        "The EpochChangeProof is stale as our verifier is already ahead \
         of the entire EpochChangeProof"
    );
    
    // ... rest of verification logic
}
```

Additionally, consider implementing rate limiting on `ConsensusMsg::EpochChangeProof` messages at the network layer to prevent spam attacks.

## Proof of Concept

```rust
#[cfg(test)]
mod resource_exhaustion_test {
    use super::*;
    use crate::{
        aggregate_signature::PartialSignatures,
        block_info::BlockInfo,
        epoch_state::EpochState,
        ledger_info::LedgerInfo,
        validator_verifier::random_validator_verifier,
    };
    use aptos_crypto::hash::HashValue;
    use std::{sync::Arc, time::Instant};

    #[test]
    fn test_epoch_change_proof_resource_exhaustion() {
        // Create 200 valid epoch ending ledger infos
        let num_epochs = 200;
        let mut valid_ledger_infos = vec![];
        let (mut current_signers, current_verifier) = 
            random_validator_verifier(4, None, true);
        let mut current_verifier = Arc::new(current_verifier);
        let mut current_version = 1000;

        for epoch in 1..=num_epochs {
            let (next_signers, next_verifier) = 
                random_validator_verifier(4, None, true);
            let next_verifier = Arc::new(next_verifier);
            let epoch_state = EpochState {
                epoch: epoch + 1,
                verifier: next_verifier.clone(),
            };
            
            let ledger_info = LedgerInfo::new(
                BlockInfo::new(
                    epoch,
                    0,
                    HashValue::zero(),
                    HashValue::zero(),
                    current_version,
                    0,
                    Some(epoch_state),
                ),
                HashValue::zero(),
            );
            
            let partial_signatures = PartialSignatures::new(
                current_signers
                    .iter()
                    .map(|s| (s.author(), s.sign(&ledger_info).unwrap()))
                    .collect(),
            );
            
            let aggregated_signature = current_verifier
                .aggregate_signatures(partial_signatures.signatures_iter())
                .unwrap();
            
            valid_ledger_infos.push(LedgerInfoWithSignatures::new(
                ledger_info,
                aggregated_signature,
            ));
            
            current_signers = next_signers;
            current_verifier = next_verifier;
            current_version += 100;
        }

        // Create proof with all 200 ledger infos
        let proof = EpochChangeProof::new(valid_ledger_infos.clone(), false);
        
        // Measure verification time
        let (initial_signers, initial_verifier) = 
            random_validator_verifier(4, None, true);
        let start_epoch_state = EpochState {
            epoch: 1,
            verifier: Arc::new(initial_verifier),
        };
        
        let start_time = Instant::now();
        let result = proof.verify(&start_epoch_state);
        let elapsed = start_time.elapsed();
        
        // Verification should succeed but take significant time
        assert!(result.is_ok());
        println!("Verified {} epochs in {:?}", num_epochs, elapsed);
        
        // With 200 BLS signature verifications, this should take multiple seconds
        // demonstrating the resource exhaustion vulnerability
    }
}
```

**Notes:**
- The vulnerability is confirmed in the codebase without any size limit checks on `EpochChangeProof` before verification
- The attack is realistic because legitimate epoch ending ledger infos can be collected from blockchain history
- BLS signature verification is computationally expensive, making even 100-200 verifications a significant burden
- The vulnerability exists in the consensus critical path during epoch transitions
- Network-level message size limits (10-40 MiB) and `MAX_EPOCH_CHUNK_SIZE` (200) constrain but do not prevent the attack

### Citations

**File:** types/src/epoch_change.rs (L66-118)
```rust
    pub fn verify(&self, verifier: &dyn Verifier) -> Result<&LedgerInfoWithSignatures> {
        ensure!(
            !self.ledger_info_with_sigs.is_empty(),
            "The EpochChangeProof is empty"
        );
        ensure!(
            !verifier
                .is_ledger_info_stale(self.ledger_info_with_sigs.last().unwrap().ledger_info()),
            "The EpochChangeProof is stale as our verifier is already ahead \
             of the entire EpochChangeProof"
        );
        let mut verifier_ref = verifier;

        for ledger_info_with_sigs in self
            .ledger_info_with_sigs
            .iter()
            // Skip any stale ledger infos in the proof prefix. Note that with
            // the assertion above, we are guaranteed there is at least one
            // non-stale ledger info in the proof.
            //
            // It's useful to skip these stale ledger infos to better allow for
            // concurrent client requests.
            //
            // For example, suppose the following:
            //
            // 1. My current trusted state is at epoch 5.
            // 2. I make two concurrent requests to two validators A and B, who
            //    live at epochs 9 and 11 respectively.
            //
            // If A's response returns first, I will ratchet my trusted state
            // to epoch 9. When B's response returns, I will still be able to
            // ratchet forward to 11 even though B's EpochChangeProof
            // includes a bunch of stale ledger infos (for epochs 5, 6, 7, 8).
            //
            // Of course, if B's response returns first, we will reject A's
            // response as it's completely stale.
            .skip_while(|&ledger_info_with_sigs| {
                verifier.is_ledger_info_stale(ledger_info_with_sigs.ledger_info())
            })
        {
            // Try to verify each (epoch -> epoch + 1) jump in the EpochChangeProof.
            verifier_ref.verify(ledger_info_with_sigs)?;
            // While the original verification could've been via waypoints,
            // all the next epoch changes are verified using the (already
            // trusted) validator sets.
            verifier_ref = ledger_info_with_sigs
                .ledger_info()
                .next_epoch_state()
                .ok_or_else(|| format_err!("LedgerInfo doesn't carry a ValidatorSet"))?;
        }

        Ok(self.ledger_info_with_sigs.last().unwrap())
    }
```

**File:** consensus/src/epoch_manager.rs (L544-569)
```rust
    async fn initiate_new_epoch(&mut self, proof: EpochChangeProof) -> anyhow::Result<()> {
        let ledger_info = proof
            .verify(self.epoch_state())
            .context("[EpochManager] Invalid EpochChangeProof")?;
        info!(
            LogSchema::new(LogEvent::NewEpoch).epoch(ledger_info.ledger_info().next_block_epoch()),
            "Received verified epoch change",
        );

        // shutdown existing processor first to avoid race condition with state sync.
        self.shutdown_current_processor().await;
        *self.pending_blocks.lock() = PendingBlocks::new();
        // make sure storage is on this ledger_info too, it should be no-op if it's already committed
        // panic if this doesn't succeed since the current processors are already shutdown.
        self.execution_client
            .sync_to_target(ledger_info.clone())
            .await
            .context(format!(
                "[EpochManager] State sync to new epoch {}",
                ledger_info
            ))
            .expect("Failed to sync to new epoch");

        monitor!("reconfig", self.await_reconfig_notification().await);
        Ok(())
    }
```

**File:** consensus/src/epoch_manager.rs (L1655-1676)
```rust
            ConsensusMsg::EpochChangeProof(proof) => {
                let msg_epoch = proof.epoch()?;
                debug!(
                    LogSchema::new(LogEvent::ReceiveEpochChangeProof)
                        .remote_peer(peer_id)
                        .epoch(self.epoch()),
                    "Proof from epoch {}", msg_epoch,
                );
                if msg_epoch == self.epoch() {
                    monitor!("process_epoch_proof", self.initiate_new_epoch(*proof).await)?;
                } else {
                    info!(
                        remote_peer = peer_id,
                        "[EpochManager] Unexpected epoch proof from epoch {}, local epoch {}",
                        msg_epoch,
                        self.epoch()
                    );
                    counters::EPOCH_MANAGER_ISSUES_DETAILS
                        .with_label_values(&["epoch_proof_wrong_epoch"])
                        .inc();
                }
            },
```

**File:** config/src/config/state_sync_config.rs (L24-24)
```rust
const MAX_EPOCH_CHUNK_SIZE: u64 = 200;
```

**File:** types/src/epoch_state.rs (L56-58)
```rust
    fn is_ledger_info_stale(&self, ledger_info: &LedgerInfo) -> bool {
        ledger_info.epoch() < self.epoch
    }
```

**File:** types/src/validator_verifier.rs (L345-386)
```rust
    pub fn verify_multi_signatures<T: CryptoHash + Serialize>(
        &self,
        message: &T,
        multi_signature: &AggregateSignature,
    ) -> std::result::Result<(), VerifyError> {
        // Verify the number of signature is not greater than expected.
        Self::check_num_of_voters(self.len() as u16, multi_signature.get_signers_bitvec())?;
        let mut pub_keys = vec![];
        let mut authors = vec![];
        for index in multi_signature.get_signers_bitvec().iter_ones() {
            let validator = self
                .validator_infos
                .get(index)
                .ok_or(VerifyError::UnknownAuthor)?;
            authors.push(validator.address);
            pub_keys.push(validator.public_key());
        }
        // Verify the quorum voting power of the authors
        self.check_voting_power(authors.iter(), true)?;
        #[cfg(any(test, feature = "fuzzing"))]
        {
            if self.quorum_voting_power == 0 {
                // This should happen only in case of tests.
                // TODO(skedia): Clean up the test behaviors to not rely on empty signature
                // verification
                return Ok(());
            }
        }
        // Verify empty multi signature
        let multi_sig = multi_signature
            .sig()
            .as_ref()
            .ok_or(VerifyError::EmptySignature)?;
        // Verify the optimistically aggregated signature.
        let aggregated_key =
            PublicKey::aggregate(pub_keys).map_err(|_| VerifyError::FailedToAggregatePubKey)?;

        multi_sig
            .verify(message, &aggregated_key)
            .map_err(|_| VerifyError::InvalidMultiSignature)?;
        Ok(())
    }
```
