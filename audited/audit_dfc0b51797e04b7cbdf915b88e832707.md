# Audit Report

## Title
Non-Deterministic Leader Election in LeaderReputation Breaks Consensus Safety

## Summary
The `ProposerElection` trait allows implementations to return inconsistent results for the same `(author, round)` pair across different nodes. The `LeaderReputation` implementation with `use_root_hash=true` (default in V2) computes proposer selection using a seed that includes the local database's accumulator root hash, which can differ across nodes due to sync delays. This causes validators to disagree on the valid proposer for a given round, breaking consensus safety.

## Finding Description

The `ProposerElection` trait defines the interface for leader election in the consensus protocol. [1](#0-0) 

The `is_valid_proposer()` function has a default implementation that delegates to `get_valid_proposer()`, which is left as an abstract method for implementations to provide. [2](#0-1) 

The `LeaderReputation` implementation is the default proposer election mechanism. In its `get_valid_proposer_and_voting_power_participation_ratio()` method, it fetches block metadata from the local database backend and uses it to compute the proposer. [3](#0-2) 

**The Critical Vulnerability:** When `use_root_hash` is `true` (which is the default for `ProposerAndVoterV2`), the seed for random proposer selection includes the `root_hash` from the database. [4](#0-3) 

The `root_hash` is computed from the local database's accumulator root at a specific version. [5](#0-4) 

**The Problem:** The `AptosDBBackend` implementation can return different root hashes on different nodes because it depends on the local database state. Nodes that are behind in syncing, missing blocks, or have database inconsistencies will compute different `max_version` values and therefore different root hashes. [6](#0-5) 

The code explicitly acknowledges this issue with a warning message: "Elected proposers are unlikely to match!!" when a node's local history is insufficient. [7](#0-6) 

The default consensus configuration uses `ProposerAndVoterV2` which sets `use_root_hash_for_seed()` to `true`. [8](#0-7) 

**Attack Scenario:**
1. Node A is fully synced with events up to round R-40
2. Node B is behind and missing recent blocks
3. Both nodes need to elect a proposer for round R
4. Both compute `target_round = R - 40` (with default `exclude_round=40`)
5. Node A fetches complete history and computes `root_hash_A`
6. Node B has incomplete history and computes `root_hash_B ≠ root_hash_A`
7. Nodes use different seeds: `[root_hash_A, epoch, round]` vs `[root_hash_B, epoch, round]`
8. They elect different proposers: Node A expects Validator X, Node B expects Validator Y
9. When Validator X proposes, Node A votes but Node B rejects
10. If enough nodes are out of sync (>f), quorum cannot be reached
11. Consensus safety is violated because honest nodes disagree on protocol rules

## Impact Explanation

This is a **Critical Severity** vulnerability under the Aptos bug bounty program because it constitutes a "Consensus/Safety violation". 

The vulnerability breaks the fundamental BFT invariant that all honest nodes must execute the same deterministic protocol. When nodes disagree on who the valid proposer is, they cannot reach consensus:

1. **Liveness Failure**: If >f nodes are out of sync, proposals from the correct proposer will be rejected by out-of-sync nodes, preventing quorum and halting the chain
2. **Consensus Confusion**: Validators waste resources voting for different proposals in the same round
3. **Potential Safety Violation**: In edge cases with complex network partitions, this could lead to chain forks if different quorums form with different views of the valid proposer

The explicit warning in the code confirms that the developers are aware this causes proposer election mismatches, which is a critical consensus bug.

## Likelihood Explanation

**High Likelihood** - This vulnerability occurs naturally in production scenarios:

1. **Network Delays**: Validators in geographically distributed locations naturally experience sync delays
2. **Node Recovery**: Validators restarting after downtime need time to sync, during which they have stale state
3. **State Sync Issues**: Any bugs or delays in state synchronization directly trigger this vulnerability
4. **Database Lag**: Even under normal operation, nodes may temporarily have different database states

The `exclude_round` parameter (default 40) provides some buffer, but it's insufficient if nodes are >40 rounds behind, which can happen during network issues, database problems, or validator restarts. The vulnerability requires no attacker action—it occurs naturally in adversarial network conditions.

## Recommendation

**Fix 1: Use Only Deterministic Inputs**
Remove `root_hash` from the seed and use only `epoch` and `round`, as in V1 (revert to `use_root_hash=false`). This ensures all nodes compute the same seed regardless of their database state. [9](#0-8) 

Modify the implementation to always use the deterministic path:
```rust
let state = [
    self.epoch.to_le_bytes().to_vec(),
    round.to_le_bytes().to_vec(),
]
.concat();
```

**Fix 2: Use Committed State Root**
Instead of using the local database's accumulator root, use the state root from the last committed QuorumCertificate, which all honest nodes agree on by definition. This requires passing the QC state root through the proposer election interface.

**Fix 3: Strict Sync Requirements**
Enforce that nodes must be fully synced before participating in consensus. Reject proposer election queries from nodes that don't have sufficient history, forcing them to sync first.

## Proof of Concept

```rust
// Test demonstrating non-deterministic proposer election
#[test]
fn test_inconsistent_proposer_election() {
    // Setup two nodes with different database states
    let mut node_a_events = vec![/* full history */];
    let mut node_b_events = vec![/* partial history */];
    
    let backend_a = Arc::new(create_backend_with_events(node_a_events));
    let backend_b = Arc::new(create_backend_with_events(node_b_events));
    
    let heuristic_a = Box::new(create_heuristic());
    let heuristic_b = Box::new(create_heuristic());
    
    // Create LeaderReputation instances with use_root_hash=true
    let proposer_election_a = LeaderReputation::new(
        epoch, epoch_to_proposers.clone(), voting_powers.clone(),
        backend_a, heuristic_a, 40, true, 100
    );
    
    let proposer_election_b = LeaderReputation::new(
        epoch, epoch_to_proposers.clone(), voting_powers.clone(),
        backend_b, heuristic_b, 40, true, 100
    );
    
    // Both nodes elect proposer for the same round
    let round = 100;
    let proposer_a = proposer_election_a.get_valid_proposer(round);
    let proposer_b = proposer_election_b.get_valid_proposer(round);
    
    // Assert: Different proposers elected (demonstrates vulnerability)
    assert_ne!(proposer_a, proposer_b, 
        "Nodes with different database states elected different proposers!");
    
    // Validate that is_valid_proposer returns inconsistent results
    assert!(proposer_election_a.is_valid_proposer(proposer_a, round));
    assert!(!proposer_election_b.is_valid_proposer(proposer_a, round));
}
```

## Notes

This vulnerability is explicitly acknowledged in the codebase with warning messages but remains unaddressed. The V2 configuration was likely introduced to make leader election "unpredictable" (as noted in comments), but this comes at the cost of breaking determinism—a fundamental requirement for BFT consensus. The trait design itself is correct; the issue lies in the implementation choice to depend on non-deterministic local state.

### Citations

**File:** consensus/src/liveness/proposer_election.rs (L10-21)
```rust
pub trait ProposerElection {
    /// If a given author is a valid candidate for being a proposer, generate the info,
    /// otherwise return None.
    /// Note that this function is synchronous.
    fn is_valid_proposer(&self, author: Author, round: Round) -> bool {
        self.get_valid_proposer(round) == author
    }

    /// Return the valid proposer for a given round (this information can be
    /// used by e.g., voters for choosing the destinations for sending their votes to).
    fn get_valid_proposer(&self, round: Round) -> Author;

```

**File:** consensus/src/liveness/leader_reputation.rs (L103-165)
```rust
    fn get_from_db_result(
        &self,
        target_epoch: u64,
        target_round: Round,
        events: &Vec<VersionedNewBlockEvent>,
        hit_end: bool,
    ) -> (Vec<NewBlockEvent>, HashValue) {
        // Do not warn when round==0, because check will always be unsure of whether we have
        // all events from the previous epoch. If there is an actual issue, next round will log it.
        if target_round != 0 {
            let has_larger = events.first().is_some_and(|e| {
                (e.event.epoch(), e.event.round()) >= (target_epoch, target_round)
            });
            if !has_larger {
                // error, and not a fatal, in an unlikely scenario that we have many failed consecutive rounds,
                // and nobody has any newer successful blocks.
                warn!(
                    "Local history is too old, asking for {} epoch and {} round, and latest from db is {} epoch and {} round! Elected proposers are unlikely to match!!",
                    target_epoch, target_round, events.first().map_or(0, |e| e.event.epoch()), events.first().map_or(0, |e| e.event.round()))
            }
        }

        let mut max_version = 0;
        let mut result = vec![];
        for event in events {
            if (event.event.epoch(), event.event.round()) <= (target_epoch, target_round)
                && result.len() < self.window_size
            {
                max_version = std::cmp::max(max_version, event.version);
                result.push(event.event.clone());
            }
        }

        if result.len() < self.window_size && !hit_end {
            error!(
                "We are not fetching far enough in history, we filtered from {} to {}, but asked for {}. Target ({}, {}), received from {:?} to {:?}.",
                events.len(),
                result.len(),
                self.window_size,
                target_epoch,
                target_round,
                events.last().map_or((0, 0), |e| (e.event.epoch(), e.event.round())),
                events.first().map_or((0, 0), |e| (e.event.epoch(), e.event.round())),
            );
        }

        if result.is_empty() {
            warn!("No events in the requested window could be found");
            (result, HashValue::zero())
        } else {
            let root_hash = self
                .aptos_db
                .get_accumulator_root_hash(max_version)
                .unwrap_or_else(|_| {
                    error!(
                        "We couldn't fetch accumulator hash for the {} version, for {} epoch, {} round",
                        max_version, target_epoch, target_round,
                    );
                    HashValue::zero()
                });
            (result, root_hash)
        }
    }
```

**File:** consensus/src/liveness/leader_reputation.rs (L696-734)
```rust
    fn get_valid_proposer_and_voting_power_participation_ratio(
        &self,
        round: Round,
    ) -> (Author, VotingPowerRatio) {
        let target_round = round.saturating_sub(self.exclude_round);
        let (sliding_window, root_hash) = self.backend.get_block_metadata(self.epoch, target_round);
        let voting_power_participation_ratio =
            self.compute_chain_health_and_add_metrics(&sliding_window, round);
        let mut weights =
            self.heuristic
                .get_weights(self.epoch, &self.epoch_to_proposers, &sliding_window);
        let proposers = &self.epoch_to_proposers[&self.epoch];
        assert_eq!(weights.len(), proposers.len());

        // Multiply weights by voting power:
        let stake_weights: Vec<u128> = weights
            .iter_mut()
            .enumerate()
            .map(|(i, w)| *w as u128 * self.voting_powers[i] as u128)
            .collect();

        let state = if self.use_root_hash {
            [
                root_hash.to_vec(),
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        } else {
            [
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        };

        let chosen_index = choose_index(stake_weights, state);
        (proposers[chosen_index], voting_power_participation_ratio)
    }
```

**File:** types/src/on_chain_config/consensus_config.rs (L540-544)
```rust
impl LeaderReputationType {
    pub fn use_root_hash_for_seed(&self) -> bool {
        // all versions after V1 should use root hash
        !matches!(self, Self::ProposerAndVoter(_))
    }
```
