# Audit Report

## Title
Critical Epoch-Ending State Snapshot Premature Pruning Vulnerability Preventing Validator Recovery and Network Growth

## Summary
During epoch reconfiguration, state merkle tree nodes created at the current epoch-ending version are incorrectly stored for short-term pruning (1M version window) instead of long-term epoch snapshot pruning (80M version window). This causes fast sync failures when new validators attempt to join or existing validators recover from downtime after the short prune window expires, leading to potential network partition and consensus unavailability.

## Finding Description

The vulnerability exists in the state merkle tree node pruning classification logic. When a state snapshot is created at an epoch-ending version V, the system must decide whether each stale JMT node should be stored in `StaleNodeIndexCrossEpochSchema` (pruned after 80M versions) or `StaleNodeIndexSchema` (pruned after 1M versions). [1](#0-0) 

The classification logic uses `previous_epoch_ending_version` to determine which schema to use. This value is obtained by calling `get_previous_epoch_ending(version)`: [2](#0-1) 

The `get_previous_epoch_ending` function explicitly returns the PREVIOUS epoch ending, not the current one, by seeking to `version - 1`: [3](#0-2) 

**The Bug:** When creating a snapshot at epoch-ending version V (epoch N → N+1):
- `previous_epoch_ending_version` = epoch N-1's ending version
- New stale nodes at version V fail the check: `V <= previous_epoch_ending_version` (since V > epoch N-1 ending)
- These nodes are stored in `StaleNodeIndexSchema` (1M window) instead of `StaleNodeIndexCrossEpochSchema` (80M window)

**The Impact:** When state sync attempts to read epoch-ending state after 1M versions, the `error_if_state_merkle_pruned` check passes because the version is within the epoch snapshot pruner's 80M window AND is marked as epoch ending: [4](#0-3) 

However, the actual JMT nodes were already pruned by state_merkle_pruner (1M window), causing proof generation to fail when `get_state_value_chunk_with_proof` attempts to traverse the missing nodes.

## Impact Explanation

**Critical Severity** - This vulnerability causes multiple critical availability failures aligning with Aptos bug bounty categories:

1. **Non-recoverable Network Partition (Critical)**: New validators cannot join the network if they need to fast sync to an epoch boundary older than the state_merkle_pruner window (~55 hours at 5000 TPS). This permanently prevents network growth and decentralization, requiring a hardfork to resolve.

2. **Total Loss of Liveness/Network Availability (Critical)**: If multiple validators experience downtime exceeding the prune window simultaneously, they cannot recover and rejoin consensus. With sufficient validators offline, the network loses BFT consensus quorum (< 2/3 validators available), halting the network.

3. **State Consistency Violation**: The system provides false guarantees - `error_if_state_merkle_pruned` indicates data is available for 80M versions when it's actually pruned after 1M versions, violating availability guarantees for state sync.

Default configuration values confirm the prune windows: [5](#0-4) [6](#0-5) 

## Likelihood Explanation

**High Likelihood** on production networks:

- On mainnet-scale networks (5000+ TPS): 1M versions ≈ 55 hours
- Epoch reconfigurations occur regularly (every ~2 hours on Aptos mainnet)
- New validators joining after 55+ hours will encounter this issue
- Validators experiencing 55+ hours of downtime cannot recover
- No attacker action required - this is a deterministic bug triggered by normal operations and time

The vulnerability is guaranteed to manifest in any network operating at moderate-to-high transaction throughput for extended periods.

## Recommendation

The fix requires updating the classification logic to use the CURRENT epoch-ending version instead of the PREVIOUS one. When committing a snapshot at epoch-ending version V, the code should recognize that V itself is an epoch ending and store all stale nodes at version V in `StaleNodeIndexCrossEpochSchema`.

**Recommended Fix:**

In `storage/aptosdb/src/state_store/state_snapshot_committer.rs`, check if the current version is an epoch ending and use it as the threshold:

```rust
let previous_epoch_ending_version = if self.state_db.ledger_db.metadata_db().is_epoch_ending(version)? {
    Some(version)
} else {
    self.state_db.ledger_db.metadata_db().get_previous_epoch_ending(version)?.map(|(v, _e)| v)
};
```

This ensures that stale nodes created at epoch-ending version V are correctly classified for long-term retention (80M window) rather than short-term pruning (1M window).

## Proof of Concept

Evidence of this issue exists in the test suite. The property test `test_state_merkle_pruning` is currently ignored with a TODO comment indicating awareness of the problem: [7](#0-6) 

The test implementation validates that epoch-ending snapshots retain their JMT nodes for the full epoch_snapshot prune window: [8](#0-7) 

The test generates blocks with epoch endings (via `BlockGen` which creates epoch-ending blocks with 25% probability) and verifies all referenced JMT nodes are present within the pruning windows. The fact that this test is ignored suggests it currently fails due to this bug.

## Notes

The ignored test with "TODO: Fix this" comment suggests the Aptos development team may be aware of this issue internally. However, given its critical impact on network availability and validator operations, it warrants immediate attention. The bug fundamentally breaks the assumption that epoch-ending state snapshots are available for the full 80M version window, which is critical for fast sync operations and network resilience.

### Citations

**File:** storage/aptosdb/src/state_merkle_db.rs (L376-386)
```rust
        stale_node_index_batch.iter().try_for_each(|row| {
            ensure!(row.node_key.get_shard_id() == shard_id, "shard_id mismatch");
            if previous_epoch_ending_version.is_some()
                && row.node_key.version() <= previous_epoch_ending_version.unwrap()
            {
                batch.put::<StaleNodeIndexCrossEpochSchema>(row, &())
            } else {
                // These are processed by the state merkle pruner.
                batch.put::<StaleNodeIndexSchema>(row, &())
            }
        })?;
```

**File:** storage/aptosdb/src/state_store/state_snapshot_committer.rs (L93-99)
```rust
                    let previous_epoch_ending_version = self
                        .state_db
                        .ledger_db
                        .metadata_db()
                        .get_previous_epoch_ending(version)
                        .unwrap()
                        .map(|(v, _e)| v);
```

**File:** storage/aptosdb/src/ledger_db/ledger_metadata_db.rs (L246-259)
```rust
    pub(crate) fn get_previous_epoch_ending(
        &self,
        version: Version,
    ) -> Result<Option<(u64, Version)>> {
        if version == 0 {
            return Ok(None);
        }
        let prev_version = version - 1;

        let mut iter = self.db.iter::<EpochByVersionSchema>()?;
        // Search for the end of the previous epoch.
        iter.seek_for_prev(&prev_version)?;
        iter.next().transpose()
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L273-303)
```rust
    pub(super) fn error_if_state_merkle_pruned(
        &self,
        data_type: &str,
        version: Version,
    ) -> Result<()> {
        let min_readable_version = self
            .state_store
            .state_db
            .state_merkle_pruner
            .get_min_readable_version();
        if version >= min_readable_version {
            return Ok(());
        }

        let min_readable_epoch_snapshot_version = self
            .state_store
            .state_db
            .epoch_snapshot_pruner
            .get_min_readable_version();
        if version >= min_readable_epoch_snapshot_version {
            self.ledger_db.metadata_db().ensure_epoch_ending(version)
        } else {
            bail!(
                "{} at version {} is pruned. snapshots are available at >= {}, epoch snapshots are available at >= {}",
                data_type,
                version,
                min_readable_version,
                min_readable_epoch_snapshot_version,
            )
        }
    }
```

**File:** config/src/config/storage_config.rs (L398-412)
```rust
impl Default for StateMerklePrunerConfig {
    fn default() -> Self {
        StateMerklePrunerConfig {
            enable: true,
            // This allows a block / chunk being executed to have access to a non-latest state tree.
            // It needs to be greater than the number of versions the state committing thread is
            // able to commit during the execution of the block / chunk. If the bad case indeed
            // happens due to this being too small, a node restart should recover it.
            // Still, defaulting to 1M to be super safe.
            prune_window: 1_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
```

**File:** config/src/config/storage_config.rs (L415-431)
```rust
impl Default for EpochSnapshotPrunerConfig {
    fn default() -> Self {
        Self {
            enable: true,
            // This is based on ~5K TPS * 2h/epoch * 2 epochs. -- epoch ending snapshots are used
            // by state sync in fast sync mode.
            // The setting is in versions, not epochs, because this makes it behave more like other
            // pruners: a slower network will have longer history in db with the same pruner
            // settings, but the disk space take will be similar.
            // settings.
            prune_window: 80_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
}
```

**File:** storage/aptosdb/src/db/aptosdb_test.rs (L228-325)
```rust
pub fn test_state_merkle_pruning_impl(
    input: Vec<(Vec<TransactionToCommit>, LedgerInfoWithSignatures)>,
) {
    // set up DB with state prune window 5 and epoch ending state prune window 10
    let tmp_dir = TempPath::new();
    let db = AptosDB::open(
        StorageDirPaths::from_path(tmp_dir),
        /*readonly=*/ false,
        PrunerConfig {
            ledger_pruner_config: LedgerPrunerConfig {
                enable: true,
                prune_window: 10,
                batch_size: 1,
                user_pruning_window_offset: 0,
            },
            state_merkle_pruner_config: StateMerklePrunerConfig {
                enable: true,
                prune_window: 5,
                batch_size: 1,
            },
            epoch_snapshot_pruner_config: EpochSnapshotPrunerConfig {
                enable: true,
                prune_window: 10,
                batch_size: 1,
            },
        },
        RocksdbConfigs::default(),
        false, /* enable_indexer */
        BUFFERED_STATE_TARGET_ITEMS_FOR_TEST,
        DEFAULT_MAX_NUM_NODES_PER_LRU_CACHE_SHARD,
        None,
        HotStateConfig::default(),
    )
    .unwrap();

    // augment DB in blocks
    let mut next_ver: Version = 0;
    let mut snapshot_versions = vec![];
    for (txns_to_commit, ledger_info_with_sigs) in input.iter() {
        db.save_transactions_for_test(
            txns_to_commit,
            next_ver, /* first_version */
            Some(ledger_info_with_sigs),
            true, /* sync_commit */
        )
        .unwrap();

        next_ver += txns_to_commit.len() as u64;

        let last_version = next_ver - 1;
        let is_epoch_ending = ledger_info_with_sigs.ledger_info().ends_epoch();
        snapshot_versions.push((last_version, is_epoch_ending));

        let state_merkle_min_readable = last_version.saturating_sub(5);
        let epoch_snapshot_min_readable = last_version.saturating_sub(10);
        let snapshots: Vec<_> = snapshot_versions
            .iter()
            .filter(|(v, _is_epoch_ending)| *v >= state_merkle_min_readable)
            .map(|(v, _)| *v)
            .collect();
        let epoch_snapshots: Vec<_> = snapshot_versions
            .iter()
            .filter(|(v, is_epoch_ending)| *is_epoch_ending && *v >= epoch_snapshot_min_readable)
            .map(|(v, _)| *v)
            .collect();

        // Prune till the oldest snapshot readable.
        let pruner = &db.state_store.state_db.state_merkle_pruner;
        let epoch_snapshot_pruner = &db.state_store.state_db.epoch_snapshot_pruner;
        pruner.set_worker_target_version(*snapshots.first().unwrap());
        epoch_snapshot_pruner.set_worker_target_version(std::cmp::min(
            *snapshots.first().unwrap(),
            *epoch_snapshots.first().unwrap_or(&Version::MAX),
        ));
        pruner.wait_for_pruner().unwrap();
        epoch_snapshot_pruner.wait_for_pruner().unwrap();

        // Check strictly that all trees in the window accessible and all those nodes not needed
        // must be gone.
        let non_pruned_versions: HashSet<_> = snapshots
            .into_iter()
            .chain(epoch_snapshots.into_iter())
            .collect();

        let expected_nodes: HashSet<_> = non_pruned_versions
            .iter()
            .flat_map(|v| db.state_store.get_all_jmt_nodes_referenced(*v).unwrap())
            .collect();
        let all_nodes: HashSet<_> = db
            .state_store
            .get_all_jmt_nodes()
            .unwrap()
            .into_iter()
            .collect();

        assert_eq!(expected_nodes, all_nodes);
    }
}
```

**File:** storage/aptosdb/src/db/aptosdb_test.rs (L330-336)
```rust
    #[test]
    #[ignore]
    // TODO(grao): Fix this.
    fn test_state_merkle_pruning(input in arb_blocks_to_commit()) {
        aptos_logger::Logger::new().init();
        test_state_merkle_pruning_impl(input);
    }
```
