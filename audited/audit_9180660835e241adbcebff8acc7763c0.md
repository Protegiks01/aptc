# Audit Report

## Title
LZ4 Decompression Bomb Vulnerability in Indexer File Store Due to Missing Size Limits

## Summary
The indexer-grpc file store decompression logic lacks size validation when decompressing LZ4-compressed transaction data, allowing an attacker who can modify stored files to cause memory exhaustion and crash indexer services through compression bomb attacks.

## Finding Description
The indexer-grpc system uses LZ4 compression to store transaction batches in file storage (GCS or local filesystem). When decompressing these files, the code in `compression_util.rs` directly calls `read_to_end()` without any size limits, violating the Resource Limits invariant that requires all operations to respect memory constraints. [1](#0-0) 

In contrast, the main Aptos compression library implements proper size validation before decompression: [2](#0-1) [3](#0-2) 

The vulnerability is exploitable when:
1. An attacker gains write access to the file store through misconfigured cloud storage permissions, stolen service account credentials, or other means
2. The attacker replaces legitimate compressed transaction files with malicious files containing compression bombs
3. When the indexer-grpc data service reads these files, it calls `get_transactions()` which triggers decompression [4](#0-3) 

The decompression occurs in a blocking task but still causes memory exhaustion on the indexer service: [5](#0-4) 

## Impact Explanation
This is a **High Severity** vulnerability per Aptos bug bounty criteria as it enables API crashes. An attacker who compromises file store access can:
- Cause indexer-grpc data service crashes through memory exhaustion
- Disrupt transaction indexing and querying capabilities
- Force service restarts and potential data inconsistencies

The impact is limited to indexer availability and does not directly affect consensus, validator operations, or blockchain state. However, it significantly degrades the ecosystem infrastructure that applications depend on.

## Likelihood Explanation
The likelihood is **Medium to High** because:
- Cloud storage misconfigurations (e.g., overly permissive GCS bucket policies) are common
- Service account credential theft is a realistic attack vector
- The attack requires no specialized blockchain knowledge, only file system access
- Once access is gained, exploitation is trivial (craft one malicious file)

While this requires some level of infrastructure access, it does not require validator privileges or consensus participation, making it exploitable by external attackers through standard cloud security vulnerabilities.

## Recommendation
Implement size validation in the indexer decompression logic following the pattern established in `crates/aptos-compression/src/lib.rs`. Add a maximum decompressed size check before allocating memory:

```rust
// In compression_util.rs, add before decompression:
const MAX_DECOMPRESSED_SIZE: usize = 100 * 1024 * 1024; // 100 MB reasonable limit

pub fn into_transactions_in_storage(self) -> TransactionsInStorage {
    match self {
        FileEntry::Lz4CompressionProto(bytes) => {
            // Extract and validate decompressed size from LZ4 header
            let decompressed_size = get_decompressed_size(&bytes, MAX_DECOMPRESSED_SIZE)
                .expect("Invalid decompressed size");
            
            let mut decompressor = Decoder::new(&bytes[..])
                .expect("Lz4 decompression failed.");
            let mut decompressed = Vec::with_capacity(decompressed_size);
            decompressor
                .read_to_end(&mut decompressed)
                .expect("Lz4 decompression failed.");
            
            TransactionsInStorage::decode(decompressed.as_slice())
                .expect("proto deserialization failed.")
        },
        // ... rest of implementation
    }
}

// Add helper function similar to crates/aptos-compression/src/lib.rs
fn get_decompressed_size(compressed_data: &[u8], max_size: usize) -> Result<usize, String> {
    if compressed_data.len() < 4 {
        return Err("Compressed data too short".to_string());
    }
    
    let size = i32::from_le_bytes([
        compressed_data[0],
        compressed_data[1], 
        compressed_data[2],
        compressed_data[3]
    ]);
    
    if size < 0 {
        return Err("Negative decompressed size".to_string());
    }
    
    let size = size as usize;
    if size > max_size {
        return Err(format!("Decompressed size {} exceeds limit {}", size, max_size));
    }
    
    Ok(size)
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod compression_bomb_test {
    use super::*;
    use lz4::EncoderBuilder;
    use std::io::Write;

    #[test]
    #[should_panic(expected = "memory allocation")]
    fn test_compression_bomb_attack() {
        // Create a compression bomb: 1 MB of zeros compresses to ~1 KB
        // but claims to decompress to 1 GB
        let malicious_data = vec![0u8; 1_000_000];
        
        // Compress with LZ4
        let mut compressed = EncoderBuilder::new()
            .level(0)
            .build(Vec::new())
            .unwrap();
        compressed.write_all(&malicious_data).unwrap();
        let compressed_bytes = compressed.finish().0;
        
        // Manually modify the size prefix to claim 1 GB decompressed size
        let mut malicious_compressed = compressed_bytes.clone();
        let fake_size: i32 = 1_000_000_000;
        malicious_compressed[0] = (fake_size & 0xFF) as u8;
        malicious_compressed[1] = ((fake_size >> 8) & 0xFF) as u8;
        malicious_compressed[2] = ((fake_size >> 16) & 0xFF) as u8;
        malicious_compressed[3] = ((fake_size >> 24) & 0xFF) as u8;
        
        // Attempt decompression - this will try to allocate 1 GB
        let file_entry = FileEntry::Lz4CompressionProto(malicious_compressed);
        let _ = file_entry.into_transactions_in_storage();
        // This should panic or cause OOM without size checks
    }
}
```

## Notes
This vulnerability exists because the indexer-grpc system uses a different LZ4 decompression implementation than the main Aptos compression library. While the main library has explicit size validation, the indexer uses the raw `lz4::Decoder` directly without checks. The attack requires file store compromise but does not require blockchain-level transaction manipulation, as the vulnerability is in the file reading/decompression path, not the transaction upload path.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L264-271)
```rust
            FileEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                TransactionsInStorage::decode(decompressed.as_slice())
                    .expect("proto deserialization failed.")
```

**File:** crates/aptos-compression/src/lib.rs (L100-108)
```rust
    // Check size of the data and initialize raw_data
    let decompressed_size = match get_decompressed_size(compressed_data, max_size) {
        Ok(size) => size,
        Err(error) => {
            let error_string = format!("Failed to get decompressed size: {}", error);
            return create_decompression_error(&client, error_string);
        },
    };
    let mut raw_data = vec![0u8; decompressed_size];
```

**File:** crates/aptos-compression/src/lib.rs (L150-184)
```rust
fn get_decompressed_size(
    compressed_data: &CompressedData,
    max_size: usize,
) -> Result<usize, Error> {
    // Ensure that the compressed data is at least 4 bytes long
    if compressed_data.len() < 4 {
        return Err(DecompressionError(format!(
            "Compressed data must be at least 4 bytes long! Got: {}",
            compressed_data.len()
        )));
    }

    // Parse the size prefix
    let size = (compressed_data[0] as i32)
        | ((compressed_data[1] as i32) << 8)
        | ((compressed_data[2] as i32) << 16)
        | ((compressed_data[3] as i32) << 24);
    if size < 0 {
        return Err(DecompressionError(format!(
            "Parsed size prefix in buffer must not be negative! Got: {}",
            size
        )));
    }

    // Ensure that the size is not greater than the max size limit
    let size = size as usize;
    if size > max_size {
        return Err(DecompressionError(format!(
            "Parsed size prefix in buffer is too big: {} > {}",
            size, max_size
        )));
    }

    Ok(size)
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/mod.rs (L59-86)
```rust
    async fn get_transactions_with_durations(
        &self,
        version: u64,
        retries: u8,
    ) -> Result<(Vec<Transaction>, f64, f64)> {
        let io_start_time = std::time::Instant::now();
        let bytes = self.get_raw_file_with_retries(version, retries).await?;
        let io_duration = io_start_time.elapsed().as_secs_f64();
        let decoding_start_time = std::time::Instant::now();
        let storage_format = self.storage_format();

        let transactions_in_storage = tokio::task::spawn_blocking(move || {
            FileEntry::new(bytes, storage_format).into_transactions_in_storage()
        })
        .await
        .context("Converting storage bytes to FileEntry transactions thread panicked")?;

        let decoding_duration = decoding_start_time.elapsed().as_secs_f64();
        Ok((
            transactions_in_storage
                .transactions
                .into_iter()
                .skip((version % FILE_ENTRY_TRANSACTION_COUNT) as usize)
                .collect(),
            io_duration,
            decoding_duration,
        ))
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L234-237)
```rust
        let transactions_in_storage = tokio::task::spawn_blocking(move || {
            FileEntry::new(bytes, StorageFormat::Lz4CompressedProto).into_transactions_in_storage()
        })
        .await?;
```
