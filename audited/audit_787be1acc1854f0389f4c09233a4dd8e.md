# Audit Report

## Title
Unauthenticated Backup Service Exposure Allows Complete Blockchain State Access via Network Misconfiguration

## Summary
The Aptos backup service (port 6186) lacks any authentication mechanism and binds to all network interfaces (`0.0.0.0:6186`) in production configurations. While Kubernetes deployments use ClusterIP by default (internal access only), common misconfigurations or alternative deployment methods can expose this service externally, allowing any attacker to download the complete blockchain state, including all transactions, account data, and state snapshots without authorization.

## Finding Description

The backup service implementation has no authentication layer protecting its HTTP endpoints. [1](#0-0) 

The service exposes nine sensitive endpoints that provide complete blockchain data access without any access control: [2](#0-1) 

Production Helm configurations explicitly bind the service to all network interfaces (`0.0.0.0:6186`): [3](#0-2) [4](#0-3) 

While the default code configuration uses localhost only (`127.0.0.1:6186`): [5](#0-4) 

The Kubernetes deployment exposes port 6186 as a ClusterIP service (internal by default), but the container itself binds to all interfaces: [6](#0-5) [7](#0-6) 

**Attack Scenarios:**

1. **Kubernetes Service Misconfiguration**: Changing the service type from ClusterIP to LoadBalancer or creating an Ingress rule for port 6186 exposes the service externally
2. **Docker/VM Deployments**: Using production config files as reference but publishing port 6186 (`docker run -p 6186:6186`) exposes the service
3. **Compromised Cluster Network**: Any pod within the Kubernetes cluster can access the backup service via ClusterIP
4. **Network Policy Failures**: Without proper NetworkPolicy enforcement, lateral movement within the cluster reaches the backup service
5. **Cloud Misconfiguration**: Security group or firewall rules allowing ingress to port 6186

**Exploitation Steps:**
1. Identify exposed Aptos node with port 6186 accessible
2. Query `/db_state` endpoint for database metadata
3. Query `/state_snapshot/{version}` to download complete state at any blockchain version
4. Query `/transactions/{start}/{count}` to download all historical transactions
5. Query `/epoch_ending_ledger_infos/{start}/{end}` for consensus metadata

## Impact Explanation

This vulnerability qualifies as **HIGH severity** per Aptos bug bounty criteria:

- **"Validator node slowdowns"**: An attacker can repeatedly request large state snapshots, exhausting node resources and degrading performance
- **"Significant protocol violations"**: Unauthorized access to complete blockchain state violates confidentiality guarantees and enables reconnaissance for further attacks
- **"API crashes"**: Resource exhaustion from malicious backup requests can crash the backup service or destabilize the node

**Specific Impacts:**
1. **Complete Privacy Violation**: All account balances, transaction history, and state data exposed
2. **Reconnaissance for Attacks**: Attackers gain full blockchain visibility to identify valuable targets or vulnerabilities
3. **Performance Degradation**: Backup operations are resource-intensive; malicious actors can trigger DoS conditions
4. **Competitive Intelligence**: Competitors gain unauthorized access to transaction patterns and network activity

While the default Kubernetes deployment mitigates exposure via ClusterIP, the fundamental security flaw—lack of authentication combined with binding to `0.0.0.0`—creates a dangerous condition that is easily triggered through common misconfigurations or alternative deployment methods.

## Likelihood Explanation

**Likelihood: MEDIUM-HIGH**

**Factors increasing likelihood:**
1. **Production configs use `0.0.0.0` by default**: Official Helm charts ship with this configuration, encouraging deployments to bind to all interfaces
2. **No documentation warnings**: The README mentions localhost binding for security but doesn't explicitly warn about the `0.0.0.0` production configs
3. **Common misconfiguration patterns**: 
   - Operators changing service types without understanding implications
   - Copy-pasting Docker commands with `-p 6186:6186`
   - Reusing production configs for test deployments without firewall protection
4. **No defense-in-depth**: Zero authentication means any exposure is immediately exploitable
5. **Network policy disabled by default**: [8](#0-7) 

**Factors decreasing likelihood:**
1. Kubernetes default (ClusterIP) provides baseline protection
2. Requires explicit action to expose externally
3. Responsible operators should review service configurations

However, the combination of unauthenticated access + binding to all interfaces + production configs encouraging this pattern creates a realistic attack surface.

## Recommendation

**Immediate Fix:**

1. **Implement Authentication**: Add bearer token or mutual TLS authentication to the backup service [1](#0-0) 

```rust
pub fn start_backup_service(
    address: SocketAddr, 
    db: Arc<AptosDB>,
    auth_token: Option<String>, // Add authentication token
) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler, auth_token); // Pass auth to routes
    
    // ... existing code
}
```

2. **Add Authentication Middleware**: Protect all endpoints with token validation

```rust
// In handlers/mod.rs
pub(crate) fn get_routes(
    backup_handler: BackupHandler,
    auth_token: Option<String>,
) -> BoxedFilter<(impl Reply,)> {
    let auth_filter = warp::header::optional::<String>("authorization")
        .and_then(move |auth: Option<String>| {
            let expected = auth_token.clone();
            async move {
                match (auth, expected) {
                    (Some(provided), Some(expected)) if provided == format!("Bearer {}", expected) => {
                        Ok(())
                    }
                    (None, None) => Ok(()), // Allow unauthenticated for localhost-only deployments
                    _ => Err(warp::reject::custom(UnauthorizedError))
                }
            }
        });
    
    // Apply auth_filter to all routes
    warp::get()
        .and(auth_filter)
        .and(routes)
        .boxed()
}
```

3. **Change Production Default to Localhost**: Update Helm charts to bind to `127.0.0.1:6186` by default [3](#0-2) 

Change to:
```yaml
storage:
  backup_service_address: "127.0.0.1:6186"
```

4. **Add Configuration Documentation**: Clearly document the security implications and authentication requirements

5. **Add Network Policy Template**: Provide a NetworkPolicy template that restricts access to the backup port

## Proof of Concept

**Scenario: Docker Deployment with Exposed Port**

1. Deploy an Aptos fullnode using Docker with the production config:

```bash
# Using production config that binds to 0.0.0.0:6186
docker run -d \
  -p 6186:6186 \
  -p 8080:8080 \
  -v $(pwd)/fullnode.yaml:/opt/aptos/etc/fullnode.yaml \
  -v $(pwd)/data:/opt/aptos/data \
  aptoslabs/validator:mainnet \
  /usr/local/bin/aptos-node -f /opt/aptos/etc/fullnode.yaml
```

2. Exploit from external machine:

```bash
# Get database state (no authentication required)
curl http://target-node-ip:6186/db_state

# Download complete state snapshot at version 1000000
curl http://target-node-ip:6186/state_snapshot/1000000 -o state_snapshot.bin

# Download 1000 transactions starting from version 0
curl http://target-node-ip:6186/transactions/0/1000 -o transactions.bin

# Get state root proof
curl http://target-node-ip:6186/state_root_proof/1000000 -o state_proof.bin
```

3. All requests succeed without any authentication, exposing complete blockchain data.

**Scenario: Kubernetes Service Type Change**

```bash
# Change service type to LoadBalancer
kubectl patch service aptos-fullnode \
  -p '{"spec":{"type":"LoadBalancer","ports":[{"name":"backup","port":6186,"targetPort":6186}]}}'

# Wait for external IP assignment
kubectl get service aptos-fullnode

# Access from internet
EXTERNAL_IP=$(kubectl get service aptos-fullnode -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
curl http://$EXTERNAL_IP:6186/db_state
```

Both scenarios demonstrate complete blockchain state access without authentication, confirming the vulnerability.

### Citations

**File:** storage/backup/backup-service/src/lib.rs (L12-30)
```rust
pub fn start_backup_service(address: SocketAddr, db: Arc<AptosDB>) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler);

    let runtime = aptos_runtimes::spawn_named_runtime("backup".into(), None);

    // Ensure that we actually bind to the socket first before spawning the
    // server tasks. This helps in tests to prevent races where a client attempts
    // to make a request before the server task is actually listening on the
    // socket.
    //
    // Note: we need to enter the runtime context first to actually bind, since
    //       tokio TcpListener can only be bound inside a tokio context.
    let _guard = runtime.enter();
    let server = warp::serve(routes).bind(address);
    runtime.handle().spawn(server);
    info!("Backup service spawned.");
    runtime
}
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L27-147)
```rust
pub(crate) fn get_routes(backup_handler: BackupHandler) -> BoxedFilter<(impl Reply,)> {
    // GET db_state
    let bh = backup_handler.clone();
    let db_state = warp::path::end()
        .map(move || reply_with_bcs_bytes(DB_STATE, &bh.get_db_state()?))
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_range_proof/<version>/<end_key>
    let bh = backup_handler.clone();
    let state_range_proof = warp::path!(Version / HashValue)
        .map(move |version, end_key| {
            reply_with_bcs_bytes(
                STATE_RANGE_PROOF,
                &bh.get_account_state_range_proof(end_key, version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot/<version>
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_item_count/<version>
    let bh = backup_handler.clone();
    let state_item_count = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(
                STATE_ITEM_COUNT,
                &(bh.get_state_item_count(version)? as u64),
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET state_snapshot_chunk/<version>/<start_idx>/<limit>
    let bh = backup_handler.clone();
    let state_snapshot_chunk = warp::path!(Version / usize / usize)
        .map(move |version, start_idx, limit| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT_CHUNK, move |bh, sender| {
                bh.get_state_item_iter(version, start_idx, limit)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET state_root_proof/<version>
    let bh = backup_handler.clone();
    let state_root_proof = warp::path!(Version)
        .map(move |version| {
            reply_with_bcs_bytes(STATE_ROOT_PROOF, &bh.get_state_root_proof(version)?)
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // GET epoch_ending_ledger_infos/<start_epoch>/<end_epoch>/
    let bh = backup_handler.clone();
    let epoch_ending_ledger_infos = warp::path!(u64 / u64)
        .map(move |start_epoch, end_epoch| {
            reply_with_bytes_sender(&bh, EPOCH_ENDING_LEDGER_INFOS, move |bh, sender| {
                bh.get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transactions/<start_version>/<num_transactions>
    let bh = backup_handler.clone();
    let transactions = warp::path!(Version / usize)
        .map(move |start_version, num_transactions| {
            reply_with_bytes_sender(&bh, TRANSACTIONS, move |bh, sender| {
                bh.get_transaction_iter(start_version, num_transactions)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);

    // GET transaction_range_proof/<first_version>/<last_version>
    let bh = backup_handler;
    let transaction_range_proof = warp::path!(Version / Version)
        .map(move |first_version, last_version| {
            reply_with_bcs_bytes(
                TRANSACTION_RANGE_PROOF,
                &bh.get_transaction_range_proof(first_version, last_version)?,
            )
        })
        .map(unwrap_or_500)
        .recover(handle_rejection);

    // Route by endpoint name.
    let routes = warp::any()
        .and(warp::path(DB_STATE).and(db_state))
        .or(warp::path(STATE_RANGE_PROOF).and(state_range_proof))
        .or(warp::path(STATE_SNAPSHOT).and(state_snapshot))
        .or(warp::path(STATE_ITEM_COUNT).and(state_item_count))
        .or(warp::path(STATE_SNAPSHOT_CHUNK).and(state_snapshot_chunk))
        .or(warp::path(STATE_ROOT_PROOF).and(state_root_proof))
        .or(warp::path(EPOCH_ENDING_LEDGER_INFOS).and(epoch_ending_ledger_infos))
        .or(warp::path(TRANSACTIONS).and(transactions))
        .or(warp::path(TRANSACTION_RANGE_PROOF).and(transaction_range_proof));

    // Serve all routes for GET only.
    warp::get()
        .and(routes)
        .with(warp::log::custom(|info| {
            let endpoint = info.path().split('/').nth(1).unwrap_or("-");
            LATENCY_HISTOGRAM.observe_with(
                &[endpoint, info.status().as_str()],
                info.elapsed().as_secs_f64(),
            )
        }))
        .boxed()
}
```

**File:** terraform/helm/fullnode/files/fullnode-base.yaml (L67-68)
```yaml
storage:
  backup_service_address: "0.0.0.0:6186"
```

**File:** terraform/helm/aptos-node/files/configs/fullnode-base.yaml (L13-16)
```yaml
storage:
  rocksdb_configs:
    enable_storage_sharding: true
  backup_service_address: "0.0.0.0:6186"
```

**File:** config/src/config/storage_config.rs (L433-456)
```rust
impl Default for StorageConfig {
    fn default() -> StorageConfig {
        StorageConfig {
            backup_service_address: SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 6186),
            dir: PathBuf::from("db"),
            hot_state_config: HotStateConfig::default(),
            // The prune window must at least out live a RPC request because its sub requests are
            // to return a consistent view of the DB at exactly same version. Considering a few
            // thousand TPS we are potentially going to achieve, and a few minutes a consistent view
            // of the DB might require, 10k (TPS)  * 100 (seconds)  =  1 Million might be a
            // conservatively safe minimal prune window. It'll take a few Gigabytes of disk space
            // depending on the size of an average account blob.
            storage_pruner_config: PrunerConfig::default(),
            data_dir: PathBuf::from("/opt/aptos/data"),
            rocksdb_configs: RocksdbConfigs::default(),
            enable_indexer: false,
            db_path_overrides: None,
            buffered_state_target_items: BUFFERED_STATE_TARGET_ITEMS,
            max_num_nodes_per_lru_cache_shard: DEFAULT_MAX_NUM_NODES_PER_LRU_CACHE_SHARD,
            ensure_rlimit_nofile: 0,
            assert_rlimit_nofile: false,
        }
    }
}
```

**File:** terraform/helm/fullnode/templates/fullnode.yaml (L152-154)
```yaml
        ports:
        - containerPort: 6182
        - containerPort: 6186
```

**File:** terraform/helm/fullnode/templates/service.yaml (L42-56)
```yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "aptos-fullnode.fullname" . }}
  labels:
    {{- include "aptos-fullnode.labels" . | nindent 4 }}
spec:
  selector:
    {{- include "aptos-fullnode.selectorLabels" . | nindent 4 }}
    app.kubernetes.io/name: fullnode
  ports:
  - name: backup
    port: 6186
  - name: metrics
    port: 9101
```

**File:** terraform/helm/aptos-node/README.md (L86-86)
```markdown
| validator.enableNetworkPolicy | bool | `false` | Lock down network ingress and egress with Kubernetes NetworkPolicy |
```
