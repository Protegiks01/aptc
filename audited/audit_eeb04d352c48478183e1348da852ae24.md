# Audit Report

## Title
Critical Cache-Database Desynchronization in Quorum Store Batch Persistence Leading to Consensus Liveness Failures

## Summary
The batch persistence mechanism in the Aptos consensus quorum store contains a critical atomicity violation where successful cache updates can be followed by database write failures that panic without rollback. This creates an inconsistent state where batches exist in the cache but not in persistent storage, causing consensus liveness failures when nodes attempt to retrieve batch payloads.

## Finding Description

The vulnerability exists in the `persist_inner()` function's two-phase commit pattern that lacks atomicity guarantees: [1](#0-0) 

The execution flow follows this sequence:

1. **Phase 1 - Cache Update**: The `save()` method calls `insert_to_cache()`, which modifies multiple in-memory data structures: [2](#0-1) [3](#0-2) 

The `insert_to_cache()` function performs three state-modifying operations:
   - Updates `db_cache` (DashMap) with the batch entry
   - Consumes quota from `peer_quota` (DashMap) for the batch author
   - Adds expiration tracking in `expirations` (Mutex)

Critically, depending on available quota, the batch may be stored in one of two modes defined in the `StorageMode` enum: [4](#0-3) [5](#0-4) 

When memory quota is exceeded, only metadata (no payload transactions) is stored in cache with `StorageMode::PersistedOnly`, requiring database retrieval for the actual payload.

2. **Phase 2 - Database Write with Panic on Failure**: After successful cache modification, if the database write fails, the code uses `.expect()` which unconditionally panics: [6](#0-5) 

The database implementation uses relaxed writes that can fail due to disk errors, disk full conditions, or I/O failures: [7](#0-6) 

3. **Task Termination Without Rollback**: The panic terminates the tokio task (batch_generator spawned here): [8](#0-7) 

The `spawn_named!` macro simply wraps `tokio::spawn()`: [9](#0-8) 

When a tokio task panics, it terminates silently without crashing the node process, leaving the node running with inconsistent state.

4. **Retrieval Failure for PersistedOnly Batches**: When other components attempt to retrieve a batch that was stored in `PersistedOnly` mode, the following retrieval path executes: [10](#0-9) [11](#0-10) 

The `get_batch_from_local()` function finds the batch metadata in cache, identifies it as `PersistedOnly`, attempts to fetch the payload from the database, and fails because the database write never succeeded.

**Invariants Broken:**
- **State Consistency**: The system violates the atomic state transition invariant - cache and database must remain synchronized
- **Consensus Safety/Liveness**: Batches required for block execution cannot be retrieved, potentially blocking consensus progress

## Impact Explanation

This vulnerability qualifies as **Critical Severity** under the Aptos bug bounty program for the following reasons:

1. **Consensus Liveness Failures**: When batches stored in `PersistedOnly` mode cannot be retrieved, block execution stalls. The `exists()` method reports the batch is available (it's in cache), but retrieval fails, causing indefinite waiting or retries that block consensus progress.

2. **Resource Exhaustion Amplification**: Each failed database write consumes quota permanently without persisting data. This amplifies resource exhaustion attacks - an attacker who can trigger disk failures (e.g., by filling disk with other operations) can cause quota leaks that cascade into denial of service.

3. **Network Bandwidth Waste**: Remote peers repeatedly request batches that appear to exist but cannot be delivered, wasting network bandwidth and creating backpressure throughout the system.

4. **Silent Degradation**: The task panic is silent - there's no automatic recovery, alerting, or rollback. Operators may not detect the issue until consensus completely stalls.

5. **Permanent Desynchronization Within Epoch**: While a full node restart rebuilds the cache from the database (eliminating desynchronization), within a single epoch run, the desynchronization is permanent. The batch generator task doesn't restart, so new batches cannot be generated, and existing broken batches remain broken until epoch transition or node restart.

This meets the Critical Severity threshold: "Total loss of liveness/network availability" or "Non-recoverable network partition (requires hardfork)" if enough validators experience this simultaneously.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can be triggered by:

1. **Disk Space Exhaustion**: Validators with insufficient disk monitoring can naturally hit disk full conditions during high transaction volume periods.

2. **I/O Hardware Failures**: Transient disk I/O errors, RAID controller issues, or network storage timeouts can cause database write failures.

3. **Resource Contention**: Under heavy load, filesystem-level write failures can occur due to inode exhaustion or filesystem corruption.

4. **Attacker-Induced Conditions**: A malicious actor can:
   - Submit large numbers of transactions to fill disk space
   - Exploit any other disk-writing operations to exhaust storage
   - Trigger the vulnerability when memory quota is low (forcing `PersistedOnly` mode)

The vulnerability doesn't require validator collusion or privileged access - any condition causing database write failures triggers it. Given that validators run continuously under varying load conditions, the probability of encountering disk/I/O issues over time is non-negligible.

## Recommendation

Implement atomic two-phase commit semantics with proper rollback on database write failure:

```rust
pub(crate) fn insert_to_cache(
    &self,
    value: &PersistedValue<BatchInfoExt>,
) -> anyhow::Result<bool> {
    // ... existing code ...
    
    // Return the value to be inserted for potential rollback
    Ok((true, value_to_be_stored, digest, author, expiration_time))
}

fn persist_inner(
    &self,
    batch_info: BatchInfoExt,
    persist_request: PersistedValue<BatchInfoExt>,
) -> Option<SignedBatchInfo<BatchInfoExt>> {
    assert!(
        &batch_info == persist_request.batch_info(),
        "Provided batch info doesn't match persist request batch info"
    );
    
    // Phase 1: Insert to cache and capture rollback information
    let (needs_db, inserted_value, digest, author, expiration) = match self.save_with_rollback(&persist_request) {
        Ok(result) => result,
        Err(e) => {
            debug!("QS: failed to store to cache {:?}", e);
            return None;
        }
    };
    
    if needs_db {
        // Phase 2: Attempt database write
        let db_result = if !batch_info.is_v2() {
            let persist_request = persist_request.try_into().expect("Must be a V1 batch");
            self.db.save_batch(persist_request)
        } else {
            self.db.save_batch_v2(persist_request)
        };
        
        // Phase 3: Rollback cache on database failure
        if let Err(e) = db_result {
            error!("QS: Database write failed, rolling back cache: {:?}", e);
            self.rollback_cache_insert(digest, author, inserted_value);
            return None;
        }
    }
    
    // Generate signature only after successful persistence
    trace!("QS: sign digest {}", persist_request.digest());
    if !batch_info.is_v2() {
        self.generate_signed_batch_info(batch_info.info().clone())
            .ok()
            .map(|inner| inner.into())
    } else {
        self.generate_signed_batch_info(batch_info).ok()
    }
}

fn rollback_cache_insert(
    &self,
    digest: HashValue,
    author: PeerId,
    value: PersistedValue<BatchInfoExt>,
) {
    // Remove from cache
    if let Some((_, removed_value)) = self.db_cache.remove(&digest) {
        // Free quota
        self.free_quota(removed_value);
    }
    
    // Remove from expirations
    self.expirations.lock().remove_item(&digest);
    
    // Log for observability
    warn!("QS: Rolled back cache insertion for digest {} due to DB write failure", digest);
}
```

**Alternative Approach**: Use a write-ahead log or staging area where batches are validated before cache insertion, or implement a background task that periodically reconciles cache and database state.

**Additional Safeguards**:
1. Add monitoring and alerting for database write failures
2. Implement automatic task restart for the batch_generator on panic
3. Add health checks that detect cache-DB desynchronization
4. Consider using transactions or atomic batch operations in the database layer

## Proof of Concept

```rust
#[cfg(test)]
mod cache_db_desync_test {
    use super::*;
    use aptos_consensus_types::proof_of_store::BatchInfo;
    use aptos_crypto::HashValue;
    use std::sync::{Arc, atomic::{AtomicBool, Ordering}};
    
    struct FailingQuorumStoreDB {
        should_fail: Arc<AtomicBool>,
        inner: MockQuorumStoreDB,
    }
    
    impl QuorumStoreStorage for FailingQuorumStoreDB {
        fn save_batch(&self, _batch: PersistedValue<BatchInfo>) -> Result<(), DbError> {
            if self.should_fail.load(Ordering::Relaxed) {
                Err(DbError::Other(anyhow::anyhow!("Simulated disk failure")))
            } else {
                self.inner.save_batch(_batch)
            }
        }
        
        fn get_batch(&self, digest: &HashValue) -> Result<Option<PersistedValue<BatchInfo>>, DbError> {
            self.inner.get_batch(digest)
        }
        
        // ... implement other methods delegating to inner
    }
    
    #[test]
    fn test_cache_db_desynchronization() {
        let should_fail = Arc::new(AtomicBool::new(false));
        let db = Arc::new(FailingQuorumStoreDB {
            should_fail: should_fail.clone(),
            inner: MockQuorumStoreDB::new(),
        });
        
        let batch_store = BatchStore::new(
            1, // epoch
            true, // is_new_epoch
            0, // last_certified_time
            db.clone(),
            1024 * 1024, // memory_quota - 1MB
            1024 * 1024 * 10, // db_quota - 10MB
            100, // batch_quota
            validator_signer,
            60_000_000, // expiration_buffer
        );
        
        // Create a large batch that exceeds memory quota, forcing PersistedOnly mode
        let large_batch = create_large_batch_info(); // > 1MB
        let persist_request = PersistedValue::new(large_batch.clone(), Some(vec![/* txns */]));
        
        // Enable database failure
        should_fail.store(true, Ordering::Relaxed);
        
        // Attempt to persist - this will panic in current implementation
        let result = std::panic::catch_unwind(|| {
            batch_store.persist(vec![persist_request.clone()]);
        });
        
        assert!(result.is_err(), "Expected panic on DB write failure");
        
        // Verify desynchronization:
        // 1. Batch exists in cache
        let cache_exists = batch_store.exists(large_batch.digest());
        assert!(cache_exists.is_some(), "Batch should exist in cache");
        
        // 2. Batch payload cannot be retrieved (would fail when accessing DB)
        let retrieval_result = batch_store.get_batch_from_local(large_batch.digest());
        assert!(retrieval_result.is_err(), "Batch retrieval should fail - DB doesn't have it");
        
        // 3. Database doesn't have the batch
        let db_result = db.get_batch(large_batch.digest());
        assert!(db_result.unwrap().is_none(), "Database should not have the batch");
        
        // This demonstrates the desynchronization: cache says batch exists,
        // but attempting to retrieve payload fails because DB write never succeeded
    }
}
```

**Notes:**
- The vulnerability is most severe when batches are stored in `PersistedOnly` mode, which occurs when memory quota is exceeded
- The desynchronization persists for the lifetime of the node process within an epoch
- Full node restarts rebuild the cache from database, eliminating the desynchronization, but this requires manual intervention
- The panic terminates the batch_generator task silently, preventing generation of new batches until node restart
- This issue represents a fundamental violation of atomicity guarantees in a distributed consensus system

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L358-417)
```rust
    pub(crate) fn insert_to_cache(
        &self,
        value: &PersistedValue<BatchInfoExt>,
    ) -> anyhow::Result<bool> {
        let digest = *value.digest();
        let author = value.author();
        let expiration_time = value.expiration();

        {
            // Acquire dashmap internal lock on the entry corresponding to the digest.
            let cache_entry = self.db_cache.entry(digest);

            if let Occupied(entry) = &cache_entry {
                match entry.get().expiration().cmp(&expiration_time) {
                    std::cmp::Ordering::Equal => return Ok(false),
                    std::cmp::Ordering::Greater => {
                        debug!(
                            "QS: already have the digest with higher expiration {}",
                            digest
                        );
                        return Ok(false);
                    },
                    std::cmp::Ordering::Less => {},
                }
            };
            let value_to_be_stored = if self
                .peer_quota
                .entry(author)
                .or_insert(QuotaManager::new(
                    self.db_quota,
                    self.memory_quota,
                    self.batch_quota,
                ))
                .update_quota(value.num_bytes() as usize)?
                == StorageMode::PersistedOnly
            {
                PersistedValue::new(value.batch_info().clone(), None)
            } else {
                value.clone()
            };

            match cache_entry {
                Occupied(entry) => {
                    let (k, prev_value) = entry.replace_entry(value_to_be_stored);
                    debug_assert!(k == digest);
                    self.free_quota(prev_value);
                },
                Vacant(slot) => {
                    slot.insert(value_to_be_stored);
                },
            }
        }

        // Add expiration for the inserted entry, no need to be atomic w. insertion.
        #[allow(clippy::unwrap_used)]
        {
            self.expirations.lock().add_item(digest, expiration_time);
        }
        Ok(true)
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L419-439)
```rust
    pub(crate) fn save(&self, value: &PersistedValue<BatchInfoExt>) -> anyhow::Result<bool> {
        let last_certified_time = self.last_certified_time();
        if value.expiration() > last_certified_time {
            fail_point!("quorum_store::save", |_| {
                // Skip caching and storing value to the db
                Ok(false)
            });
            counters::GAP_BETWEEN_BATCH_EXPIRATION_AND_CURRENT_TIME_WHEN_SAVE.observe(
                Duration::from_micros(value.expiration() - last_certified_time).as_secs_f64(),
            );

            return self.insert_to_cache(value);
        }
        counters::NUM_BATCH_EXPIRED_WHEN_SAVE.inc();
        bail!(
            "Incorrect expiration {} in epoch {}, last committed timestamp {}",
            value.expiration(),
            self.epoch(),
            last_certified_time,
        );
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L488-528)
```rust
    fn persist_inner(
        &self,
        batch_info: BatchInfoExt,
        persist_request: PersistedValue<BatchInfoExt>,
    ) -> Option<SignedBatchInfo<BatchInfoExt>> {
        assert!(
            &batch_info == persist_request.batch_info(),
            "Provided batch info doesn't match persist request batch info"
        );
        match self.save(&persist_request) {
            Ok(needs_db) => {
                trace!("QS: sign digest {}", persist_request.digest());
                if needs_db {
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
                }
                if !batch_info.is_v2() {
                    self.generate_signed_batch_info(batch_info.info().clone())
                        .ok()
                        .map(|inner| inner.into())
                } else {
                    self.generate_signed_batch_info(batch_info).ok()
                }
            },
            Err(e) => {
                debug!("QS: failed to store to cache {:?}", e);
                None
            },
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L545-569)
```rust
    fn get_batch_from_db(
        &self,
        digest: &HashValue,
        is_v2: bool,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        counters::GET_BATCH_FROM_DB_COUNT.inc();

        if is_v2 {
            match self.db.get_batch_v2(digest) {
                Ok(Some(value)) => Ok(value),
                Ok(None) | Err(_) => {
                    warn!("Could not get batch from db");
                    Err(ExecutorError::CouldNotGetData)
                },
            }
        } else {
            match self.db.get_batch(digest) {
                Ok(Some(value)) => Ok(value.into()),
                Ok(None) | Err(_) => {
                    warn!("Could not get batch from db");
                    Err(ExecutorError::CouldNotGetData)
                },
            }
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L571-585)
```rust
    pub(crate) fn get_batch_from_local(
        &self,
        digest: &HashValue,
    ) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
        if let Some(value) = self.db_cache.get(digest) {
            if value.payload_storage_mode() == StorageMode::PersistedOnly {
                self.get_batch_from_db(digest, value.batch_info().is_v2())
            } else {
                // Available in memory.
                Ok(value.clone())
            }
        } else {
            Err(ExecutorError::CouldNotGetData)
        }
    }
```

**File:** consensus/src/quorum_store/types.rs (L27-31)
```rust
#[derive(PartialEq, Debug)]
pub(crate) enum StorageMode {
    PersistedOnly,
    MemoryAndPersisted,
}
```

**File:** consensus/src/quorum_store/types.rs (L41-46)
```rust
    pub(crate) fn payload_storage_mode(&self) -> StorageMode {
        match self.maybe_payload {
            Some(_) => StorageMode::MemoryAndPersisted,
            None => StorageMode::PersistedOnly,
        }
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L110-117)
```rust
    fn save_batch(&self, batch: PersistedValue<BatchInfo>) -> Result<(), DbError> {
        trace!(
            "QS: db persists digest {} expiration {:?}",
            batch.digest(),
            batch.expiration()
        );
        self.put::<BatchSchema>(batch.digest(), &batch)
    }
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L311-319)
```rust
        spawn_named!(
            "batch_generator",
            batch_generator.start(
                self.network_sender.clone(),
                batch_generator_cmd_rx,
                back_pressure_rx,
                interval
            )
        );
```

**File:** crates/aptos-logger/src/macros.rs (L7-8)
```rust
macro_rules! spawn_named {
      ($name:expr, $func:expr) => { tokio::spawn($func); };
```
