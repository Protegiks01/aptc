# Audit Report

## Title
Unhandled Database Errors in Remote State View Service Cause Complete Validator Node Crash

## Summary
The `RemoteStateViewService::handle_message()` function contains an `unwrap()` call on `get_state_value()` at line 104 that will panic on any database error (I/O failures, corruption, missing state). Due to the global panic handler, this panic terminates the entire validator node process with exit code 12, causing complete loss of availability. [1](#0-0) 

## Finding Description

The vulnerability exists in the remote state view service which handles state value requests from remote executor shards during distributed block execution. The problematic code path is:

1. **Service Architecture**: The `RemoteStateViewService` runs in a dedicated thread and spawns tasks in a rayon thread pool to handle incoming state value requests. [2](#0-1) 

2. **Vulnerable unwrap()**: When handling messages, the code calls `get_state_value(&state_key).unwrap()` without error handling, which will panic if the underlying state view returns any error. [3](#0-2) 

3. **Error Sources**: The `get_state_value()` method returns `StateViewResult<Option<StateValue>>`, which can fail with `StateViewError` for multiple reasons: [4](#0-3) [5](#0-4) 

4. **Database Error Propagation**: The underlying database implementation can return errors from:
   - RocksDB I/O operations during seek/iteration
   - BCS deserialization failures
   - Missing state due to pruning
   - Database corruption [6](#0-5) [7](#0-6) 

5. **Panic Handler Termination**: The global panic handler installed at node startup catches all panics and terminates the process with exit code 12, unless the panic occurs during bytecode verification. [8](#0-7) [9](#0-8) 

6. **Usage Context**: This service is used when remote sharded block execution is enabled, which is a configuration option for distributed execution. [10](#0-9) 

**Exploitation Path**:
- During block execution, remote executor shards request state values from the coordinator
- If any state key triggers a database error (I/O failure, corruption, missing/pruned state), the `unwrap()` panics
- The panic handler logs the error and calls `process::exit(12)`, killing the entire validator node
- The node stops participating in consensus until manually restarted

## Impact Explanation

This vulnerability qualifies as **HIGH severity** according to Aptos bug bounty criteria:

- **Validator node crashes**: The entire process terminates, not just a single thread or component
- **API crashes**: All node services become unavailable immediately
- **Availability Impact**: The validator cannot participate in consensus, propose blocks, or process transactions
- **Recovery Requirement**: Manual node restart required; no automatic recovery mechanism

While this doesn't cause permanent network partition or fund loss, it creates a **significant protocol violation** by removing a validator from the active set until operator intervention. Multiple nodes experiencing similar database issues could severely degrade network liveness.

The vulnerability breaks the **Resource Limits** and implicit **Availability** invariants - operations should gracefully handle resource failures rather than crashing the entire process.

## Likelihood Explanation

**MEDIUM-HIGH likelihood**:

**Natural Occurrence**:
- Database I/O errors during disk failures or high load
- State pruning causing missing state for older versions
- Database corruption from hardware failures or power loss
- BCS deserialization errors from corrupted on-disk data

**Induced Conditions**:
- Disk space exhaustion causing RocksDB write/read failures
- Memory pressure causing page cache evictions and I/O errors
- Concurrent database access patterns causing lock contention

**Frequency**: Any validator running for extended periods will likely encounter transient I/O errors. With aggressive state pruning enabled, requests for pruned state become more common.

**Attacker Requirements**: None - this occurs naturally. An attacker could induce it through resource exhaustion DoS, but the vulnerability manifests without malicious intent.

## Recommendation

Replace the `unwrap()` with proper error handling that logs the error and returns a failure response instead of panicking:

```rust
fn handle_message(
    message: Message,
    state_view: Arc<RwLock<Option<Arc<S>>>>,
    kv_tx: Arc<Vec<Sender<Message>>>,
) {
    let _timer = REMOTE_EXECUTOR_TIMER
        .with_label_values(&["0", "kv_requests"])
        .start_timer();
    let bcs_deser_timer = REMOTE_EXECUTOR_TIMER
        .with_label_values(&["0", "kv_req_deser"])
        .start_timer();
    
    let req: RemoteKVRequest = match bcs::from_bytes(&message.data) {
        Ok(req) => req,
        Err(e) => {
            error!("Failed to deserialize RemoteKVRequest: {:?}", e);
            return;
        }
    };
    drop(bcs_deser_timer);

    let (shard_id, state_keys) = req.into();
    trace!(
        "remote state view service - received request for shard {} with {} keys",
        shard_id,
        state_keys.len()
    );
    
    let state_view_lock = match state_view.read() {
        Ok(lock) => lock,
        Err(e) => {
            error!("Failed to acquire state view lock: {:?}", e);
            return;
        }
    };
    
    let state_view_ref = match state_view_lock.as_ref() {
        Some(sv) => sv,
        None => {
            error!("State view not set");
            return;
        }
    };
    
    let resp: Vec<(StateKey, Option<StateValue>)> = state_keys
        .into_iter()
        .map(|state_key| {
            match state_view_ref.get_state_value(&state_key) {
                Ok(state_value) => (state_key, state_value),
                Err(e) => {
                    error!("Failed to get state value for key {:?}: {:?}", state_key, e);
                    (state_key, None)
                }
            }
        })
        .collect();
        
    // Rest of the function remains the same...
}
```

Additionally, consider implementing a retry mechanism for transient errors and adding metrics to track state fetch failures.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_types::state_store::{StateView, StateViewResult, TStateView};
    use aptos_types::state_store::state_key::StateKey;
    use aptos_types::state_store::state_value::StateValue;
    use aptos_types::state_store::state_storage_usage::StateStorageUsage;
    use aptos_types::state_store::errors::StateViewError;
    use std::sync::{Arc, RwLock};
    
    // Mock StateView that returns errors
    struct FailingStateView;
    
    impl TStateView for FailingStateView {
        type Key = StateKey;
        
        fn get_state_value(&self, _state_key: &Self::Key) -> StateViewResult<Option<StateValue>> {
            // Simulate database I/O error
            Err(StateViewError::Other("RocksDB I/O error: disk read failed".to_string()))
        }
        
        fn get_usage(&self) -> StateViewResult<StateStorageUsage> {
            Ok(StateStorageUsage::new_untracked())
        }
    }
    
    impl StateView for FailingStateView {}
    
    #[test]
    #[should_panic(expected = "RocksDB I/O error")]
    fn test_state_value_fetch_panic() {
        // This test demonstrates that unwrap() on get_state_value() will panic
        // when the database returns an error
        
        let failing_view = Arc::new(FailingStateView);
        let state_key = StateKey::raw(vec![1, 2, 3]);
        
        // This will panic, just like line 104 in handle_message()
        let _value = failing_view.get_state_value(&state_key).unwrap();
    }
}
```

**Notes**:
- The vulnerability is in production code, not test code
- The panic occurs in the rayon thread pool, but the global panic handler catches it
- All three `unwrap()` calls in the chain (lines 100, 102, 104) are problematic
- The issue affects any deployment using remote sharded block execution
- Database errors are a realistic operational concern for long-running validators

### Citations

**File:** execution/executor-service/src/remote_state_view_service.rs (L64-72)
```rust
    pub fn start(&self) {
        while let Ok(message) = self.kv_rx.recv() {
            let state_view = self.state_view.clone();
            let kv_txs = self.kv_tx.clone();
            self.thread_pool.spawn(move || {
                Self::handle_message(message, state_view, kv_txs);
            });
        }
    }
```

**File:** execution/executor-service/src/remote_state_view_service.rs (L95-107)
```rust
        let resp = state_keys
            .into_iter()
            .map(|state_key| {
                let state_value = state_view
                    .read()
                    .unwrap()
                    .as_ref()
                    .unwrap()
                    .get_state_value(&state_key)
                    .unwrap();
                (state_key, state_value)
            })
            .collect_vec();
```

**File:** types/src/state_store/mod.rs (L29-29)
```rust
pub type StateViewResult<T, E = StateViewError> = std::result::Result<T, E>;
```

**File:** types/src/state_store/errors.rs (L6-15)
```rust
#[derive(Debug, Error)]
pub enum StateViewError {
    #[error("{0} not found.")]
    NotFound(String),
    /// Other non-classified error.
    #[error("{0}")]
    Other(String),
    #[error(transparent)]
    BcsError(#[from] bcs::Error),
}
```

**File:** storage/aptosdb/src/state_kv_db.rs (L374-402)
```rust
    pub(crate) fn get_state_value_with_version_by_version(
        &self,
        state_key: &StateKey,
        version: Version,
    ) -> Result<Option<(Version, StateValue)>> {
        let mut read_opts = ReadOptions::default();

        // We want `None` if the state_key changes in iteration.
        read_opts.set_prefix_same_as_start(true);
        if !self.enabled_sharding() {
            let mut iter = self
                .db_shard(state_key.get_shard_id())
                .iter_with_opts::<StateValueSchema>(read_opts)?;
            iter.seek(&(state_key.clone(), version))?;
            Ok(iter
                .next()
                .transpose()?
                .and_then(|((_, version), value_opt)| value_opt.map(|value| (version, value))))
        } else {
            let mut iter = self
                .db_shard(state_key.get_shard_id())
                .iter_with_opts::<StateValueByKeyHashSchema>(read_opts)?;
            iter.seek(&(state_key.hash(), version))?;
            Ok(iter
                .next()
                .transpose()?
                .and_then(|((_, version), value_opt)| value_opt.map(|value| (version, value))))
        }
    }
```

**File:** storage/storage-interface/src/errors.rs (L9-37)
```rust
/// This enum defines errors commonly used among `AptosDB` APIs.
#[derive(Clone, Debug, Error)]
pub enum AptosDbError {
    /// A requested item is not found.
    #[error("{0} not found.")]
    NotFound(String),
    /// Requested too many items.
    #[error("Too many items requested: at least {0} requested, max is {1}")]
    TooManyRequested(u64, u64),
    #[error("Missing state root node at version {0}, probably pruned.")]
    MissingRootError(u64),
    /// Other non-classified error.
    #[error("AptosDB Other Error: {0}")]
    Other(String),
    #[error("AptosDB RocksDb Error: {0}")]
    RocksDbIncompleteResult(String),
    #[error("AptosDB RocksDB Error: {0}")]
    OtherRocksDbError(String),
    #[error("AptosDB bcs Error: {0}")]
    BcsError(String),
    #[error("AptosDB IO Error: {0}")]
    IoError(String),
    #[error("AptosDB Recv Error: {0}")]
    RecvError(String),
    #[error("AptosDB ParseInt Error: {0}")]
    ParseIntError(String),
    #[error("Hot state not configured properly")]
    HotStateError,
}
```

**File:** crates/crash-handler/src/lib.rs (L26-58)
```rust
pub fn setup_panic_handler() {
    panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}

// Formats and logs panic information
fn handle_panic(panic_info: &PanicHookInfo<'_>) {
    // The Display formatter for a PanicHookInfo contains the message, payload and location.
    let details = format!("{}", panic_info);
    let backtrace = format!("{:#?}", Backtrace::new());

    let info = CrashInfo { details, backtrace };
    let crash_info = toml::to_string_pretty(&info).unwrap();
    error!("{}", crash_info);
    // TODO / HACK ALARM: Write crash info synchronously via eprintln! to ensure it is written before the process exits which error! doesn't guarantee.
    // This is a workaround until https://github.com/aptos-labs/aptos-core/issues/2038 is resolved.
    eprintln!("{}", crash_info);

    // Wait till the logs have been flushed
    aptos_logger::flush();

    // Do not kill the process if the panics happened at move-bytecode-verifier.
    // This is safe because the `state::get_state()` uses a thread_local for storing states. Thus the state can only be mutated to VERIFIER by the thread that's running the bytecode verifier.
    //
    // TODO: once `can_unwind` is stable, we should assert it. See https://github.com/rust-lang/rust/issues/92988.
    if state::get_state() == VMState::VERIFIER || state::get_state() == VMState::DESERIALIZER {
        return;
    }

    // Kill the process
    process::exit(12);
}
```

**File:** aptos-node/src/lib.rs (L233-234)
```rust
    // Setup panic handler
    aptos_crash_handler::setup_panic_handler();
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```
