# Audit Report

## Title
Remote Executor Service Channel Disconnection Causes Panics and Deadlocks During Block Execution

## Summary
The remote executor service does not gracefully handle network channel disconnections during block execution. When channels disconnect mid-execution, the system uses `.unwrap()` on channel send/receive operations causing thread panics and coordinator deadlocks, resulting in validator node hangs and execution failures.

## Finding Description

The `ExecutorService` and its associated remote execution infrastructure fail to handle network channel disconnections gracefully. The vulnerability manifests across multiple layers of the remote execution system:

**Critical Failure Points:**

1. **Shard-to-Coordinator Result Transmission** - When a shard completes block execution and attempts to send results back to the coordinator, if the network channel has disconnected, the operation panics instead of failing gracefully. [1](#0-0) 

2. **Cross-Shard Message Transmission** - During multi-round sharded execution, shards communicate via cross-shard messages. Channel disconnections cause immediate panics. [2](#0-1) 

3. **Cross-Shard Message Reception** - When receiving cross-shard messages, disconnections cause panics. [3](#0-2) 

4. **Coordinator Result Reception** - The coordinator blocks indefinitely waiting for shard results. If a shard crashes or its channel disconnects after receiving a command, the coordinator hangs forever with no timeout. [4](#0-3) 

5. **Coordinator Command Transmission** - When sending execution commands to shards, channel disconnections cause panics. [5](#0-4) 

6. **GRPC Layer Failures** - Even at the underlying gRPC network layer, send failures explicitly panic rather than retry or fail gracefully. The code includes a TODO comment acknowledging missing retry logic. [6](#0-5) 

**Execution Flow Vulnerability:**

The remote sharded executor service runs in a continuous loop receiving and executing commands: [7](#0-6) 

When a network partition or node failure occurs during this loop:
- Commands may be received successfully but results cannot be sent back
- Cross-shard communications fail mid-execution
- The coordinator waits indefinitely with no timeout mechanism

**Production Usage Confirmed:**

This code path is actively used in production when remote shard addresses are configured: [8](#0-7) 

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program criteria: "Validator node slowdowns, API crashes, Significant protocol violations."

**Specific Impacts:**

1. **Validator Node Hangs** - When the coordinator deadlocks waiting for results, the entire validator node's execution pipeline stalls. Block execution cannot proceed, affecting consensus participation.

2. **Executor Service Crashes** - Thread panics cause executor shard processes to crash, requiring manual intervention to restart. This reduces validator availability and reliability.

3. **Liveness Impact** - During network partitions or node restarts (common operational scenarios), the remote execution system fails catastrophically rather than degrading gracefully or retrying.

4. **No Error Recovery** - The system provides no mechanism to detect, report, or recover from channel failures. Operations simply panic or hang indefinitely.

5. **Cascading Failures** - A single shard failure or network issue can deadlock the entire coordinator, affecting all concurrent block executions.

The vulnerability breaks the **Liveness** invariant - the system should remain available and make progress even under partial network failures. It also violates **Graceful Degradation** - the system should detect failures and fail safely rather than panicking or deadlocking.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability will trigger in common operational scenarios:

1. **Network Partitions** - Temporary network disruptions between coordinator and shards are common in distributed systems, especially cloud deployments across availability zones.

2. **Node Restarts** - During validator software upgrades, node restarts, or process crashes (e.g., OOM), channels disconnect and trigger the vulnerability.

3. **Process Crashes** - If a shard process crashes due to any reason (bugs, resource exhaustion), the coordinator deadlocks waiting for results.

4. **Configuration Changes** - Reconfiguring shard addresses or coordinator addresses while the system is running causes channel disconnections.

5. **No Special Attack Required** - This is not an intentional attack vector but a reliability bug that triggers during normal failure scenarios. No malicious actor is needed - ordinary distributed system failures are sufficient.

The vulnerability is deterministic: whenever a channel disconnects during execution, the system will panic or deadlock. There are no timeouts, retries, or graceful degradation mechanisms in place.

## Recommendation

Implement proper error handling and timeout mechanisms throughout the remote execution stack:

**1. Replace `.unwrap()` with proper error handling:**

```rust
// In remote_cordinator_client.rs
fn send_execution_result(&self, result: Result<Vec<Vec<TransactionOutput>>, VMStatus>) {
    let remote_execution_result = RemoteExecutionResult::new(result);
    let output_message = bcs::to_bytes(&remote_execution_result).unwrap();
    
    // Replace unwrap with proper error handling
    if let Err(e) = self.result_tx.send(Message::new(output_message)) {
        error!("Failed to send execution result: {:?}. Channel disconnected.", e);
        // Signal shutdown or retry logic here
    }
}
```

**2. Add timeouts to receive operations:**

```rust
// In remote_executor_client.rs
fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
    let mut results = vec![];
    let timeout = Duration::from_secs(60); // Configurable timeout
    
    for rx in self.result_rxs.iter() {
        match rx.recv_timeout(timeout) {
            Ok(message) => {
                let received_bytes = message.to_bytes();
                let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes)?;
                results.push(result.inner?);
            },
            Err(RecvTimeoutError::Timeout) => {
                return Err(VMStatus::Error(StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR));
            },
            Err(RecvTimeoutError::Disconnected) => {
                return Err(VMStatus::Error(StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR));
            }
        }
    }
    Ok(results)
}
```

**3. Implement retry logic in GRPC layer:**

```rust
// In grpc_network_service.rs
pub async fn send_message(
    &mut self,
    sender_addr: SocketAddr,
    message: Message,
    mt: &MessageType,
) -> Result<(), tonic::Status> {
    let request = tonic::Request::new(NetworkMessage {
        message: message.data,
        message_type: mt.get_type(),
    });
    
    // Retry with exponential backoff
    let max_retries = 3;
    let mut retry_delay = Duration::from_millis(100);
    
    for attempt in 0..max_retries {
        match self.remote_channel.simple_msg_exchange(request.clone()).await {
            Ok(response) => return Ok(()),
            Err(e) if attempt < max_retries - 1 => {
                warn!("Send attempt {} failed: {}. Retrying...", attempt + 1, e);
                tokio::time::sleep(retry_delay).await;
                retry_delay *= 2;
            },
            Err(e) => {
                error!("Failed to send message after {} attempts: {}", max_retries, e);
                return Err(e);
            }
        }
    }
    unreachable!()
}
```

**4. Add health checks and connection monitoring:**

Implement periodic health checks on channels to detect disconnections early and reconnect or fail fast rather than discovering failures during critical operations.

**5. Add circuit breaker pattern:**

If a shard repeatedly fails, temporarily mark it as unavailable and route work around it rather than repeatedly attempting operations that will fail.

## Proof of Concept

```rust
// Reproduction scenario (conceptual test):
// This demonstrates how channel disconnection causes panic

#[test]
fn test_channel_disconnection_panic() {
    use crossbeam_channel::unbounded;
    use std::thread;
    use std::time::Duration;
    
    // Simulate coordinator-shard communication
    let (cmd_tx, cmd_rx) = unbounded();
    let (result_tx, result_rx) = unbounded();
    
    // Spawn shard thread
    let shard = thread::spawn(move || {
        // Receive command
        let _cmd = cmd_rx.recv().unwrap();
        
        // Simulate execution
        thread::sleep(Duration::from_millis(100));
        
        // Try to send result - THIS WILL PANIC if result_rx is dropped
        result_tx.send("result").unwrap(); // PANIC HERE!
    });
    
    // Coordinator sends command
    cmd_tx.send("execute").unwrap();
    
    // Simulate coordinator crash/disconnection - drop result receiver
    drop(result_rx);
    
    // Wait for shard thread - it will panic when trying to send
    let result = shard.join();
    assert!(result.is_err()); // Thread panicked
}

// To reproduce in actual system:
// 1. Start remote executor service with coordinator and 2 shards
// 2. Send block execution request
// 3. Kill coordinator process mid-execution (or disconnect network)
// 4. Observe shard panic when trying to send results:
//    "thread 'ExecutorService-0' panicked at 'called `Result::unwrap()` 
//     on an `Err` value: SendError { .. }'"
// 5. If coordinator survives, it deadlocks waiting for results
```

**Steps to Reproduce in Real Environment:**

1. Configure remote sharded execution with coordinator and multiple shard processes
2. Start block execution on coordinator
3. During execution (before results are sent):
   - Option A: Kill a shard process (`kill -9 <shard_pid>`)
   - Option B: Use network simulation tools to drop packets between coordinator and shard
   - Option C: Restart coordinator process
4. Observe:
   - Shard threads panic with `.unwrap()` errors
   - Coordinator deadlocks in `get_output_from_shards()` with no timeout
   - Block execution never completes
   - System requires manual intervention to recover

## Notes

- The vulnerability exists across the entire remote execution infrastructure, not just in a single location
- The root cause is the use of `.unwrap()` on fallible channel operations throughout the codebase
- The TODO comment at line 150 of `grpc_network_service.rs` explicitly acknowledges this missing error handling
- No timeout mechanisms exist at any layer (crossbeam channels, executor workflow, or coordinator client)
- The issue affects production deployments when remote executor addresses are configured
- This is a reliability/availability issue rather than a security exploit, but meets HIGH severity criteria for validator node impacts

### Citations

**File:** execution/executor-service/src/remote_cordinator_client.rs (L115-119)
```rust
    fn send_execution_result(&self, result: Result<Vec<Vec<TransactionOutput>>, VMStatus>) {
        let remote_execution_result = RemoteExecutionResult::new(result);
        let output_message = bcs::to_bytes(&remote_execution_result).unwrap();
        self.result_tx.send(Message::new(output_message)).unwrap();
    }
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L55-59)
```rust
    fn send_cross_shard_msg(&self, shard_id: ShardId, round: RoundId, msg: CrossShardMsg) {
        let input_message = bcs::to_bytes(&msg).unwrap();
        let tx = self.message_txs[shard_id][round].lock().unwrap();
        tx.send(Message::new(input_message)).unwrap();
    }
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L61-66)
```rust
    fn receive_cross_shard_msg(&self, current_round: RoundId) -> CrossShardMsg {
        let rx = self.message_rxs[current_round].lock().unwrap();
        let message = rx.recv().unwrap();
        let msg: CrossShardMsg = bcs::from_bytes(&message.to_bytes()).unwrap();
        msg
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor-service/src/remote_executor_client.rs (L201-206)
```rust
            senders[shard_id]
                .lock()
                .unwrap()
                .send(Message::new(bcs::to_bytes(&execution_request).unwrap()))
                .unwrap();
        }
```

**File:** secure/net/src/grpc_network_service/mod.rs (L150-160)
```rust
        // TODO: Retry with exponential backoff on failures
        match self.remote_channel.simple_msg_exchange(request).await {
            Ok(_) => {},
            Err(e) => {
                panic!(
                    "Error '{}' sending message to {} on node {:?}",
                    e, self.remote_addr, sender_addr
                );
            },
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-260)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```
