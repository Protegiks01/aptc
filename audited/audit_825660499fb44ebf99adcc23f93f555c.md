# Audit Report

## Title
Race Condition in Peer State Garbage Collection Allows Malicious Peers to Avoid Penalty for Invalid Proofs

## Summary
A race condition exists between the peer state garbage collection process and in-flight request handling in the Aptos data client. When garbage collection removes a peer's state while responses from that peer are being processed, score updates are silently dropped, allowing malicious peers to evade penalties for sending invalid proofs or bad data.

## Finding Description

The vulnerability exists in the state-sync data client's peer management system. The core issue manifests in the following execution flow:

**Thread 1 (Request/Response Handling):**
1. A peer is selected to fulfill a data request based on its storage summary [1](#0-0) 

2. The request is sent to the selected peer [2](#0-1) 

3. When a response is received, the peer's score should be updated [3](#0-2) 

**Thread 2 (Poller Thread):**
1. The poller periodically calls garbage collection to clean up disconnected peers [4](#0-3) 

2. Garbage collection removes all peer states not in the connected peers set [5](#0-4) 

**The Race Condition:**
If a peer disconnects after sending a response but before the response is processed, garbage collection can remove the peer's state. When the response processing attempts to update the peer's score, the update silently fails because the state no longer exists: [6](#0-5) [7](#0-6) 

**Exploitation Scenario:**

A malicious peer can exploit this by:
1. Sending responses with invalid proofs (which should trigger `ResponseError::ProofVerificationError`) [8](#0-7) 

2. Disconnecting immediately after sending the malicious response
3. The response is delivered and proof verification fails
4. The callback attempts to penalize the peer with `ErrorType::Malicious` (80% score multiplier) [9](#0-8) 

5. But if garbage collection ran during this window, the penalty is lost
6. When the peer reconnects, a fresh state is created with default score 50.0 [10](#0-9) 

This allows the peer to repeatedly send invalid proofs without accumulating penalties that would eventually cause it to be ignored (score below 25.0): [11](#0-10) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Node Slowdowns**: Malicious peers can persistently send invalid data without being properly penalized and ignored. This forces nodes to repeatedly waste resources verifying invalid proofs and retrying requests, slowing down state synchronization.

2. **Significant Protocol Violations**: The peer scoring and reputation system is a critical component of the state-sync protocol. Bypassing this mechanism violates the protocol's design assumption that malicious peers will be identified and deprioritized.

While invalid proofs are still detected and rejected (preventing state corruption), the inability to effectively penalize bad actors means:
- State sync operations take longer as bad peers remain in the active pool
- Network resources are wasted on peers known to provide invalid data
- The quality-of-service degradation affects all nodes attempting to sync state

## Likelihood Explanation

The likelihood of this vulnerability being exploited is **Medium to High**:

**Feasibility:**
- The race window exists naturally due to network latency between response delivery and processing
- An attacker can artificially widen this window by controlling disconnect timing
- The poller runs garbage collection every poll cycle (configurable, typically milliseconds)
- No special privileges are required - any network peer can attempt this

**Attack Complexity:**
- Moderate: Requires precise timing but can be attempted repeatedly
- The attacker needs to observe/estimate when their response is delivered
- Multiple malicious peers coordinating this attack can amplify the impact

**Detection:**
- The attack is difficult to distinguish from normal network instability
- No explicit error logging occurs when score updates are silently dropped
- Metrics might show peers with unexpectedly stable scores despite bad behavior

## Recommendation

Implement atomic handling of peer state during request/response processing to prevent garbage collection from removing states with in-flight requests:

**Option 1: Reference Counting**
```rust
// Add to PeerState
in_flight_requests: Arc<AtomicU64>,

// Increment when sending request
pub fn increment_in_flight_requests(&self, peer: PeerNetworkId) {
    if let Some(peer_state) = self.peer_to_state.get(&peer) {
        peer_state.in_flight_requests.fetch_add(1, Ordering::SeqCst);
    }
}

// Decrement when processing response
pub fn decrement_in_flight_requests(&self, peer: PeerNetworkId) {
    if let Some(peer_state) = self.peer_to_state.get(&peer) {
        peer_state.in_flight_requests.fetch_sub(1, Ordering::SeqCst);
    }
}

// Modified garbage collection
pub fn garbage_collect_peer_states(&self, connected_peers: HashSet<PeerNetworkId>) {
    self.peer_to_state.retain(|peer_network_id, peer_state| {
        // Keep if connected OR has in-flight requests
        connected_peers.contains(peer_network_id) || 
        peer_state.in_flight_requests.load(Ordering::SeqCst) > 0
    });
}
```

**Option 2: Grace Period**
```rust
// Add to PeerState
last_request_time: Arc<Mutex<Option<Instant>>>,

// Modified garbage collection with grace period
pub fn garbage_collect_peer_states(&self, connected_peers: HashSet<PeerNetworkId>) {
    let grace_period = Duration::from_secs(30);
    let now = Instant::now();
    
    self.peer_to_state.retain(|peer_network_id, peer_state| {
        if connected_peers.contains(peer_network_id) {
            return true;
        }
        
        // Keep peer state for grace period after last request
        if let Some(last_req) = peer_state.last_request_time.lock().as_ref() {
            now.duration_since(*last_req) < grace_period
        } else {
            false
        }
    });
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_race_condition_peer_state_garbage_collection() {
    use std::sync::Arc;
    use tokio::time::{sleep, Duration};
    use aptos_config::config::AptosDataClientConfig;
    use aptos_config::network_id::{NetworkId, PeerNetworkId};
    use aptos_crypto::HashValue;
    
    // Setup
    let config = Arc::new(AptosDataClientConfig::default());
    let peer_states = Arc::new(PeerStates::new(config));
    let peer = PeerNetworkId::new(NetworkId::Validator, AccountAddress::random());
    
    // Add peer with storage summary (peer is "connected")
    peer_states.update_summary(peer, StorageServerSummary::default());
    
    // Verify peer state exists
    assert!(peer_states.get_peer_to_states().contains_key(&peer));
    
    // Simulate sending request (in real code, this happens in send_request_to_peer)
    peer_states.increment_sent_request_counter(peer, &StorageServiceRequest::default());
    
    // Spawn task to simulate response processing after delay
    let peer_states_clone = peer_states.clone();
    let response_task = tokio::spawn(async move {
        sleep(Duration::from_millis(100)).await;
        // Simulate response received - try to update score
        peer_states_clone.update_score_error(peer, ErrorType::Malicious);
    });
    
    // Simulate peer disconnect and garbage collection
    sleep(Duration::from_millis(50)).await;
    let empty_connected_peers = HashSet::new(); // Peer disconnected
    peer_states.garbage_collect_peer_states(empty_connected_peers);
    
    // Wait for response processing
    response_task.await.unwrap();
    
    // BUG: Peer state was removed, so score update was lost
    // If peer reconnects, it gets fresh state with default score
    peer_states.update_summary(peer, StorageServerSummary::default());
    
    // Verify peer has default score instead of penalized score
    if let Some(peer_state) = peer_states.get_peer_to_states().get(&peer) {
        let score = peer_state.get_score();
        // Score should be heavily penalized (50.0 * 0.8 = 40.0)
        // But due to race condition, it's back to default 50.0
        assert_eq!(score, 50.0); // This demonstrates the bug
        // Expected: score should be 40.0 or lower
    }
}
```

## Notes

This vulnerability specifically affects the state-sync data client's ability to maintain accurate peer reputation scores. While it doesn't directly compromise consensus safety or data integrity (invalid proofs are still detected), it undermines the network's ability to efficiently identify and deprioritize malicious peers, leading to degraded state synchronization performance across the network.

### Citations

**File:** state-sync/aptos-data-client/src/client.rs (L639-639)
```rust
        let peers = self.choose_peers_for_request(&request)?;
```

**File:** state-sync/aptos-data-client/src/client.rs (L769-797)
```rust
    async fn send_request_to_peer(
        &self,
        peer: PeerNetworkId,
        request: StorageServiceRequest,
        request_timeout_ms: u64,
    ) -> crate::error::Result<Response<StorageServiceResponse>, Error> {
        // Generate a unique id for the request
        let id = self.response_id_generator.next();

        // Update the sent request metrics
        trace!(
            (LogSchema::new(LogEntry::StorageServiceRequest)
                .event(LogEvent::SendRequest)
                .request_type(&request.get_label())
                .request_id(id)
                .peer(&peer)
                .request_data(&request))
        );
        self.update_sent_request_metrics(peer, &request);

        // Send the request and process the result
        let result = self
            .storage_service_client
            .send_request(
                peer,
                Duration::from_millis(request_timeout_ms),
                request.clone(),
            )
            .await;
```

**File:** state-sync/aptos-data-client/src/client.rs (L817-817)
```rust
                self.peer_states.update_score_success(peer);
```

**File:** state-sync/aptos-data-client/src/poller.rs (L293-293)
```rust
        if let Err(error) = poller.data_client.update_global_summary_cache() {
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L42-43)
```rust
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L45-52)
```rust
pub enum ErrorType {
    /// A response or error that's not actively malicious but also doesn't help
    /// us make progress, e.g., timeouts, remote errors, invalid data, etc...
    NotUseful,
    /// A response or error that appears to be actively hindering progress or
    /// attempting to deceive us, e.g., invalid proof.
    Malicious,
}
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L281-299)
```rust
    pub fn update_score_success(&self, peer: PeerNetworkId) {
        if let Some(mut entry) = self.peer_to_state.get_mut(&peer) {
            // Get the peer's old score
            let old_score = entry.score;

            // Update the peer's score with a successful operation
            entry.update_score_success();

            // Log if the peer is no longer ignored
            let new_score = entry.score;
            if old_score <= IGNORE_PEER_THRESHOLD && new_score > IGNORE_PEER_THRESHOLD {
                info!(
                    (LogSchema::new(LogEntry::PeerStates)
                        .event(LogEvent::PeerNoLongerIgnored)
                        .message("Peer will no longer be ignored")
                        .peer(&peer))
                );
            }
        }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L303-322)
```rust
    pub fn update_score_error(&self, peer: PeerNetworkId, error: ErrorType) {
        if let Some(mut entry) = self.peer_to_state.get_mut(&peer) {
            // Get the peer's old score
            let old_score = entry.score;

            // Update the peer's score with an error
            entry.update_score_error(error);

            // Log if the peer is now ignored
            let new_score = entry.score;
            if old_score > IGNORE_PEER_THRESHOLD && new_score <= IGNORE_PEER_THRESHOLD {
                info!(
                    (LogSchema::new(LogEntry::PeerStates)
                        .event(LogEvent::PeerIgnored)
                        .message("Peer will be ignored")
                        .peer(&peer))
                );
            }
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L325-330)
```rust
    pub fn update_summary(&self, peer: PeerNetworkId, storage_summary: StorageServerSummary) {
        self.peer_to_state
            .entry(peer)
            .or_insert(PeerState::new(self.data_client_config.clone()))
            .update_storage_summary(storage_summary);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L333-336)
```rust
    pub fn garbage_collect_peer_states(&self, connected_peers: HashSet<PeerNetworkId>) {
        self.peer_to_state
            .retain(|peer_network_id, _| connected_peers.contains(peer_network_id));
    }
```

**File:** state-sync/aptos-data-client/src/interface.rs (L183-187)
```rust
pub enum ResponseError {
    InvalidData,
    InvalidPayloadDataType,
    ProofVerificationError,
}
```
