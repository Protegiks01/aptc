# Audit Report

## Title
Indexer File Store Data Loss Due to Cancellation After Buffer Clear

## Summary
The `dump_transactions_to_file()` function in the indexer file store operator contains a cancellation-unsafe await point at lines 80-81. If the async task is cancelled at this point, transaction data is permanently lost because the buffer has already been cleared and the data manager has already incremented its version counter, potentially leading to garbage collection of the data from cache before it can be recovered.

## Finding Description

The vulnerability exists in the transaction buffering and file storage pipeline of the Aptos indexer-grpc system. The issue occurs in the following sequence: [1](#0-0) 

**Critical State Modifications Before Await:**

1. The buffer is cleared via `std::mem::take(&mut self.buffer)` at line 71, moving transactions into a local variable
2. The batch metadata is updated with the new file metadata at lines 73-77
3. The buffer size is reset to 0 at line 78
4. **Then** the channel send occurs at lines 80-81 with `.await`

**The Data Loss Scenario:**

When `buffer_and_maybe_dump_transactions_to_file()` is called: [2](#0-1) 

The version counter is incremented at line 58 **before** calling `dump_transactions_to_file()`.

Meanwhile, in the data manager, when transactions are fetched from cache: [3](#0-2) 

The `file_store_version` atomic counter is incremented **optimistically** at lines 129-133 when `update_file_store_version=true`, which happens **before** the upload completes.

**If cancellation occurs at the await point (lines 80-81):**

1. The local `transactions` variable is dropped (data lost)
2. The buffer is already empty (cleared at line 71)
3. The file_store_operator's version counter has advanced (line 58)
4. The data manager's `file_store_version` has been incremented (line 131)
5. The channel receiver never receives the transactions
6. The transactions are never uploaded to file storage

**Recovery Failure:**

The cache's garbage collection mechanism: [4](#0-3) 

Will garbage collect transactions up to `file_store_version` (line 68). Since `file_store_version` was already incremented, these transactions may be GC'd from cache, making them unrecoverable.

Upon restart, the recovery process: [5](#0-4) 

Will read the latest version from file store, but the lost transactions were never written there and may have been GC'd from cache, resulting in permanent data loss.

## Impact Explanation

**Severity Assessment: Medium**

This issue represents a state inconsistency requiring intervention, which falls under the **Medium Severity** category of the Aptos bug bounty program. However, there are important considerations:

**Why Not Critical:**
- This does not affect consensus safety or liveness
- This does not affect validator node operations  
- This does not result in fund loss or manipulation
- This is not a consensus protocol violation

**Why Not High:**
- This affects indexer infrastructure, not validator nodes
- The indexer API continues functioning (doesn't crash), it just serves incomplete data
- This is not a core protocol violation

**Why Medium:**
- Results in state inconsistencies (missing transaction data in indexer)
- Requires manual intervention to recover/reprocess lost transactions
- Breaks data completeness guarantees for the indexer service
- Affects applications and users relying on complete historical data

**Important Limitation:**
The indexer is infrastructure for querying historical data, **not** a core consensus component. This vulnerability does not affect the blockchain's consensus, execution, or state commitment. It only affects the ability to query complete historical transaction data through the indexer API.

## Likelihood Explanation

**Likelihood: Medium**

This vulnerability will occur whenever:
- The indexer process is terminated during operation (SIGTERM, SIGKILL)
- The tokio runtime shuts down while tasks are in-flight
- Resource exhaustion causes task cancellation
- Timeouts occur on long-running operations
- System crashes or restarts occur

The vulnerability is **not** exploitable by an attacker - there is no malicious input or attack vector. It is a race condition inherent in the async task structure that occurs during exceptional conditions or normal shutdown sequences.

## Recommendation

Implement cancellation-safe transaction dumping by ensuring all state modifications happen **atomically** with the channel send, or use a two-phase commit pattern:

```rust
async fn dump_transactions_to_file(
    &mut self,
    end_batch: bool,
    tx: Sender<(Vec<Transaction>, BatchMetadata, bool)>,
) -> Result<()> {
    // Clone the buffer instead of taking it
    let transactions = self.buffer.clone();
    let first_version = transactions.first().unwrap().version;
    
    // Prepare metadata update but don't apply yet
    let file_metadata = FileMetadata {
        first_version,
        last_version: first_version + transactions.len() as u64,
        size_bytes: self.buffer_size_in_bytes,
    };
    
    // Send through channel FIRST (cancellation point)
    let mut batch_metadata_to_send = self.buffer_batch_metadata.clone();
    batch_metadata_to_send.files.push(file_metadata.clone());
    
    tx.send((transactions, batch_metadata_to_send, end_batch))
        .await
        .map_err(anyhow::Error::msg)?;
    
    // Only after successful send, modify state
    self.buffer.clear();
    self.buffer_size_in_bytes = 0;
    self.buffer_batch_metadata.files.push(file_metadata);
    
    if end_batch {
        self.buffer_batch_metadata = BatchMetadata::default();
    }

    Ok(())
}
```

Alternatively, use Tokio's `AbortHandle` and `Abortable` pattern to ensure cleanup on cancellation, or wrap the critical section in a cancellation guard.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use tokio::sync::mpsc::channel;
    use tokio::time::{timeout, Duration};

    #[tokio::test]
    async fn test_cancellation_data_loss() {
        let (tx, mut rx) = channel::<(Vec<Transaction>, BatchMetadata, bool)>(10);
        
        let mut operator = FileStoreOperatorV2::new(
            1000,
            100,
            0,
            BatchMetadata::default(),
        );
        
        // Add a transaction to buffer
        let transaction = Transaction {
            version: 0,
            ..Default::default()
        };
        
        operator.buffer.push(transaction.clone());
        operator.buffer_size_in_bytes = transaction.encoded_len();
        operator.version = 1;
        
        // Simulate cancellation by dropping the receiver
        // This will cause the send to fail immediately
        drop(rx);
        
        // Attempt to dump - this will fail but state is already modified
        let result = operator.dump_transactions_to_file(false, tx).await;
        
        // Verify data loss:
        // 1. Buffer is empty (cleared at line 71)
        assert_eq!(operator.buffer.len(), 0);
        // 2. Buffer size is 0 (reset at line 78)
        assert_eq!(operator.buffer_size_in_bytes, 0);
        // 3. Batch metadata was updated (lines 73-77)
        assert_eq!(operator.buffer_batch_metadata.files.len(), 1);
        // 4. But the transaction was never actually sent (receiver dropped)
        // 5. The transaction is permanently lost
        
        assert!(result.is_err());
    }
}
```

## Notes

**Important Context:**

This vulnerability affects the **indexer infrastructure**, not the core blockchain consensus or validator operations. The Aptos indexer-grpc system is a separate service that:
- Reads committed transaction data from the blockchain
- Stores it in file storage for historical queries
- Provides APIs for applications to query historical data

**Impact Scope:**
- Does NOT affect consensus safety, liveness, or validator operations
- Does NOT affect fund security or transaction execution
- DOES affect data completeness of the indexer service
- DOES require manual intervention to recover lost transaction data

Given the strict validation criteria and focus on consensus/execution/storage/governance/staking components, this finding may be **out of scope** for the core Aptos bug bounty program, as it affects infrastructure rather than core blockchain security. However, it represents a real data integrity issue that should be addressed to maintain indexer reliability.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_operator.rs (L43-64)
```rust
    pub async fn buffer_and_maybe_dump_transactions_to_file(
        &mut self,
        transaction: Transaction,
        tx: Sender<(Vec<Transaction>, BatchMetadata, bool)>,
    ) -> Result<()> {
        let end_batch = (transaction.version + 1) % self.num_txns_per_folder == 0;
        let size_bytes = transaction.encoded_len();
        ensure!(
            self.version == transaction.version,
            "Gap is found when buffering transaction, expected: {}, actual: {}",
            self.version,
            transaction.version,
        );
        self.buffer.push(transaction);
        self.buffer_size_in_bytes += size_bytes;
        self.version += 1;
        if self.buffer_size_in_bytes >= self.max_size_per_file || end_batch {
            self.dump_transactions_to_file(end_batch, tx).await?;
        }

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_operator.rs (L66-89)
```rust
    async fn dump_transactions_to_file(
        &mut self,
        end_batch: bool,
        tx: Sender<(Vec<Transaction>, BatchMetadata, bool)>,
    ) -> Result<()> {
        let transactions = std::mem::take(&mut self.buffer);
        let first_version = transactions.first().unwrap().version;
        self.buffer_batch_metadata.files.push(FileMetadata {
            first_version,
            last_version: first_version + transactions.len() as u64,
            size_bytes: self.buffer_size_in_bytes,
        });
        self.buffer_size_in_bytes = 0;

        tx.send((transactions, self.buffer_batch_metadata.clone(), end_batch))
            .await
            .map_err(anyhow::Error::msg)?;

        if end_batch {
            self.buffer_batch_metadata = BatchMetadata::default();
        }

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L62-80)
```rust
    // NOTE: This will only gc data up to the file store version.
    fn maybe_gc(&mut self) -> bool {
        if self.cache_size <= self.max_cache_size {
            return true;
        }

        while self.start_version < self.file_store_version.load(Ordering::SeqCst)
            && self.cache_size > self.target_cache_size
        {
            let transaction = self.transactions.pop_front().unwrap();
            self.cache_size -= transaction.encoded_len();
            self.start_version += 1;
        }

        CACHE_SIZE.set(self.cache_size as i64);
        CACHE_START_VERSION.set(self.start_version as i64);

        self.cache_size <= self.max_cache_size
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L92-143)
```rust
    fn get_transactions(
        &self,
        start_version: u64,
        max_size_bytes: usize,
        update_file_store_version: bool,
    ) -> Vec<Transaction> {
        if !update_file_store_version {
            trace!(
            "Requesting version {start_version} from cache, update_file_store_version = {update_file_store_version}.",
        );
            trace!(
                "Current data range in cache: [{}, {}).",
                self.start_version,
                self.start_version + self.transactions.len() as u64
            );
        }
        if start_version < self.start_version {
            return vec![];
        }

        let mut transactions = vec![];
        let mut size_bytes = 0;
        for transaction in self
            .transactions
            .iter()
            .skip((start_version - self.start_version) as usize)
        {
            size_bytes += transaction.encoded_len();
            transactions.push(transaction.clone());
            if size_bytes > max_size_bytes {
                // Note: We choose to not pop the last transaction here, so the size could be
                // slightly larger than the `max_size_bytes`. This is fine.
                break;
            }
        }
        if update_file_store_version {
            if !transactions.is_empty() {
                let old_version = self
                    .file_store_version
                    .fetch_add(transactions.len() as u64, Ordering::SeqCst);
                let new_version = old_version + transactions.len() as u64;
                FILE_STORE_VERSION_IN_CACHE.set(new_version as i64);
                info!("Updated file_store_version in cache to {new_version}.");
            }
        } else {
            trace!(
                "Returned {} transactions from Cache, total {size_bytes} bytes.",
                transactions.len()
            );
        }
        transactions
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L86-118)
```rust
    /// Recovers the batch metadata in memory buffer for the unfinished batch from file store.
    async fn recover(&self) -> Result<(u64, BatchMetadata)> {
        let _timer = TIMER.with_label_values(&["recover"]).start_timer();

        let mut version = self
            .reader
            .get_latest_version()
            .await
            .expect("Latest version must exist.");
        info!("Starting recovering process, current version in storage: {version}.");
        let mut num_folders_checked = 0;
        let mut buffered_batch_metadata_to_recover = BatchMetadata::default();
        while let Some(batch_metadata) = self.reader.get_batch_metadata(version).await {
            let batch_last_version = batch_metadata.files.last().unwrap().last_version;
            version = batch_last_version;
            if version % NUM_TXNS_PER_FOLDER != 0 {
                buffered_batch_metadata_to_recover = batch_metadata;
                break;
            }
            num_folders_checked += 1;
            if num_folders_checked >= MAX_NUM_FOLDERS_TO_CHECK_FOR_RECOVERY {
                panic!(
                    "File store metadata is way behind batch metadata, data might be corrupted."
                );
            }
        }

        self.update_file_store_metadata(version).await?;

        info!("Finished recovering process, recovered at version: {version}.");

        Ok((version, buffered_batch_metadata_to_recover))
    }
```
