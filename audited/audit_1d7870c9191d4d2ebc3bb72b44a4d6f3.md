# Audit Report

## Title
Unhandled Thread Panic in Executor Service Causes Coordinator Deadlock and Validator Unavailability

## Summary
The executor service spawns worker threads without storing their `JoinHandle`, causing panics to be silently swallowed. When a shard executor thread panics during block execution, the coordinator deadlocks waiting for results that will never arrive, leading to validator node unavailability and potential state inconsistency across the network.

## Finding Description

The vulnerability exists in how `ExecutorService::start()` spawns executor worker threads without proper panic handling. [1](#0-0) 

The spawned thread's `JoinHandle` is immediately dropped, meaning:
1. If the thread panics, the parent thread is never notified
2. The panic is silently swallowed by Rust's panic handler
3. The coordinator continues waiting for results from a dead shard

The executor thread runs in an infinite loop processing commands: [2](#0-1) 

Multiple panic points exist in this execution path:

**Panic Point 1**: BCS deserialization of incoming commands: [3](#0-2) 

**Panic Point 2**: Sending execution results back to coordinator: [4](#0-3) 

**Panic Point 3**: Metrics collection on shutdown: [5](#0-4) 

**Panic Point 4**: Internal callback channel: [6](#0-5) 

When a shard panics, the coordinator blocks indefinitely: [7](#0-6) 

The `recv()` call on line 167 blocks waiting for a message from a dead shard, causing the entire validator node to hang. If the channel is closed (sender dropped due to panic), the `.unwrap()` will panic the coordinator as well, but only after potentially significant delay.

This issue affects both test and production deployments:

**Production usage** via `ProcessExecutorService`: [8](#0-7) 

**Test usage** via `ThreadExecutorService`: [9](#0-8) 

### Breaking Critical Invariants

This breaks multiple Aptos invariants:

1. **Deterministic Execution**: If validators experience panics at different times or on different shards, they may produce divergent state
2. **State Consistency**: Partial shard execution before panic leads to inconsistent in-memory state that may not be properly cleaned up
3. **Validator Availability**: Coordinator deadlock prevents the validator from processing subsequent blocks

## Impact Explanation

This qualifies as **HIGH severity** per Aptos bug bounty criteria:

- **"Validator node slowdowns"**: The coordinator deadlock causes complete validator node hang (worse than slowdown)
- **"API crashes"**: The validator becomes unresponsive, equivalent to a crash from an external perspective

Thread panics can be triggered by:
1. **VM execution bugs**: Malicious transactions exploiting bugs in Move VM that cause assertion failures or panics
2. **Resource exhaustion**: Transactions causing out-of-memory conditions triggering panics
3. **Network issues**: Channel disconnections causing `.unwrap()` panics in network I/O code
4. **Software bugs**: Any unhandled panic in the execution path (division by zero, index out of bounds, etc.)

When multiple validators in a network experience this simultaneously (e.g., from the same malicious transaction), it causes:
- **Liveness failure**: Insufficient validators available to form consensus
- **State divergence**: Validators that panicked vs. those that didn't may have different states
- **Network partition risk**: Subsets of validators may become unable to progress

## Likelihood Explanation

**MEDIUM-HIGH Likelihood**:

The likelihood is elevated because:
1. Thread panics are inevitable in production systems (bugs, OOM, hardware failures)
2. Sharded execution increases surface area - N shards means N opportunities for panic per block
3. No graceful degradation or recovery mechanism exists
4. An attacker could deliberately trigger conditions causing panics (malicious transactions, resource exhaustion)

The impact is immediate and deterministic once a panic occurs - there is no probabilistic element to the coordinator deadlock.

## Recommendation

**Immediate Fix**: Store and monitor `JoinHandle` to detect panics:

```rust
pub struct ExecutorService {
    shard_id: ShardId,
    controller: NetworkController,
    executor_service: Arc<ShardedExecutorService<RemoteStateViewClient>>,
    executor_thread: Option<thread::JoinHandle<()>>, // ADD THIS
}

pub fn start(&mut self) {
    self.controller.start();
    let thread_name = format!("ExecutorService-{}", self.shard_id);
    let builder = thread::Builder::new().name(thread_name);
    let executor_service_clone = self.executor_service.clone();
    let join_handle = builder
        .spawn(move || {
            executor_service_clone.start();
        })
        .expect("Failed to spawn thread");
    self.executor_thread = Some(join_handle); // STORE THE HANDLE
}

pub fn check_health(&self) -> bool {
    if let Some(handle) = &self.executor_thread {
        !handle.is_finished()
    } else {
        false
    }
}
```

**Additional measures**:

1. **Panic handler**: Wrap the executor loop in a panic handler that sends error status to coordinator before thread termination
2. **Timeout mechanism**: Add timeout to `recv()` calls with periodic health checks
3. **Graceful degradation**: Allow coordinator to proceed with partial results after detecting shard failure
4. **Monitoring**: Add metrics for shard thread health and panic detection

## Proof of Concept

```rust
#[cfg(test)]
mod panic_propagation_test {
    use super::*;
    use std::{sync::Arc, thread, time::Duration};
    
    #[test]
    #[should_panic(expected = "Coordinator deadlock detected")]
    fn test_executor_thread_panic_causes_coordinator_deadlock() {
        // Setup: Create a mock executor service that will panic
        let (command_tx, command_rx) = crossbeam_channel::unbounded();
        let (result_tx, result_rx) = crossbeam_channel::unbounded();
        
        // Spawn executor thread that panics immediately
        let executor_thread = thread::spawn(move || {
            // Simulate panic in executor thread
            panic!("Simulated executor panic during block execution");
        });
        
        // Don't store the join handle (mimicking the vulnerability)
        drop(executor_thread);
        
        // Coordinator attempts to receive result
        // This will block indefinitely because the sender was dropped due to panic
        thread::spawn(move || {
            thread::sleep(Duration::from_secs(5));
            panic!("Coordinator deadlock detected - no result received after 5 seconds");
        });
        
        // This recv() will block forever waiting for a result from the dead thread
        let _result = result_rx.recv().unwrap();
    }
    
    #[test]
    fn test_panic_in_execute_block_path() {
        // Demonstrate panic in actual execution path
        // by triggering one of the .unwrap() calls with an error condition
        
        // Example: Closed channel causes send to fail and panic
        let (command_tx, command_rx) = crossbeam_channel::unbounded();
        let (result_tx, result_rx) = crossbeam_channel::unbounded();
        
        // Drop receiver to close channel
        drop(result_rx);
        
        // Attempt to send will panic due to .unwrap() on closed channel
        // This simulates what happens in send_execution_result()
        let panic_result = std::panic::catch_unwind(|| {
            result_tx.send("test").unwrap(); // This panics
        });
        
        assert!(panic_result.is_err(), "Expected panic from send on closed channel");
    }
}
```

## Notes

This vulnerability is present in both the test infrastructure (`ThreadExecutorService`) and production code (`ProcessExecutorService`), affecting actual validator deployments. The local executor correctly stores `JoinHandle` instances [10](#0-9) , but the remote executor service used in distributed deployments does not, creating an availability risk in production environments.

### Citations

**File:** execution/executor-service/src/remote_executor_service.rs (L57-67)
```rust
    pub fn start(&mut self) {
        self.controller.start();
        let thread_name = format!("ExecutorService-{}", self.shard_id);
        let builder = thread::Builder::new().name(thread_name);
        let executor_service_clone = self.executor_service.clone();
        builder
            .spawn(move || {
                executor_service_clone.start();
            })
            .expect("Failed to spawn thread");
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L174-174)
```rust
                callback.send(ret).unwrap();
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L215-260)
```rust
    pub fn start(&self) {
        trace!(
            "Shard starting, shard_id={}, num_shards={}.",
            self.shard_id,
            self.num_shards
        );
        let mut num_txns = 0;
        loop {
            let command = self.coordinator_client.receive_execute_command();
            match command {
                ExecutorShardCommand::ExecuteSubBlocks(
                    state_view,
                    transactions,
                    concurrency_level_per_shard,
                    onchain_config,
                ) => {
                    num_txns += transactions.num_txns();
                    trace!(
                        "Shard {} received ExecuteBlock command of block size {} ",
                        self.shard_id,
                        num_txns
                    );
                    let exe_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "execute_block"]);
                    let ret = self.execute_block(
                        transactions,
                        state_view.as_ref(),
                        BlockExecutorConfig {
                            local: BlockExecutorLocalConfig::default_with_concurrency_level(
                                concurrency_level_per_shard,
                            ),
                            onchain: onchain_config,
                        },
                    );
                    drop(state_view);
                    drop(exe_timer);

                    let _result_tx_timer = SHARDED_EXECUTOR_SERVICE_SECONDS
                        .timer_with(&[&self.shard_id.to_string(), "result_tx"]);
                    self.coordinator_client.send_execution_result(ret);
                },
                ExecutorShardCommand::Stop => {
                    break;
                },
            }
        }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_executor_service.rs (L261-264)
```rust
        let exe_time = SHARDED_EXECUTOR_SERVICE_SECONDS
            .get_metric_with_label_values(&[&self.shard_id.to_string(), "execute_block"])
            .unwrap()
            .get_sample_sum();
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L89-89)
```rust
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L117-118)
```rust
        let output_message = bcs::to_bytes(&remote_execution_result).unwrap();
        self.result_tx.send(Message::new(output_message)).unwrap();
```

**File:** execution/executor-service/src/remote_executor_client.rs (L163-172)
```rust
    fn get_output_from_shards(&self) -> Result<Vec<Vec<Vec<TransactionOutput>>>, VMStatus> {
        trace!("RemoteExecutorClient Waiting for results");
        let mut results = vec![];
        for rx in self.result_rxs.iter() {
            let received_bytes = rx.recv().unwrap().to_bytes();
            let result: RemoteExecutionResult = bcs::from_bytes(&received_bytes).unwrap();
            results.push(result.inner?);
        }
        Ok(results)
    }
```

**File:** execution/executor-service/src/process_executor_service.rs (L35-44)
```rust
        let mut executor_service = ExecutorService::new(
            shard_id,
            num_shards,
            num_threads,
            self_address,
            coordinator_address,
            remote_shard_addresses,
        );
        executor_service.start();
        Self { executor_service }
```

**File:** execution/executor-service/src/thread_executor_service.rs (L15-36)
```rust
    pub fn new(
        shard_id: ShardId,
        num_shards: usize,
        num_threads: usize,
        coordinator_address: SocketAddr,
        remote_shard_addresses: Vec<SocketAddr>,
    ) -> Self {
        let self_address = remote_shard_addresses[shard_id];
        let mut executor_service = ExecutorService::new(
            shard_id,
            num_shards,
            num_threads,
            self_address,
            coordinator_address,
            remote_shard_addresses,
        );
        executor_service.start();
        Self {
            _self_address: self_address,
            executor_service,
        }
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/local_executor_shard.rs (L55-63)
```rust
        let join_handle = thread::Builder::new()
            .name(format!("executor-shard-{}", shard_id))
            .spawn(move || executor_service.start())
            .unwrap();
        Self {
            join_handle: Some(join_handle),
            phantom: std::marker::PhantomData,
        }
    }
```
