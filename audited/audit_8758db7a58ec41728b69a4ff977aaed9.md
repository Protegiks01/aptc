# Audit Report

## Title
Health Check Bypass via Inbound Ping Exploitation Allows Unresponsive Peers to Evade Disconnection

## Summary
The health checker's `handle_ping_request()` function unconditionally resets peer failure counters when processing inbound pings, allowing malicious or unresponsive peers to bypass health check disconnection logic by periodically sending pings while never responding to outbound health checks. This creates an asymmetric health tracking vulnerability that can be exploited for connection slot exhaustion and network manipulation.

## Finding Description

The health checker protocol implements bidirectional ping/pong messaging to monitor peer liveness. When a node sends outbound pings and receives no response, it increments a failure counter. After exceeding `ping_failures_tolerated` failures, the peer should be disconnected. [1](#0-0) 

However, the implementation has a critical asymmetry in failure tracking. When processing INBOUND pings (pings received FROM a peer), the code unconditionally resets that peer's failure counter: [2](#0-1) 

The vulnerability manifests in the state management functions: [3](#0-2) 

**Attack Scenario:**

1. Malicious peer M connects to honest node H
2. H periodically sends health check pings to M (default: every 10 seconds)
3. M deliberately never responds to H's pings, causing failures to accumulate
4. Before reaching the disconnect threshold (default: 3 failures = ~30 seconds), M sends a single ping to H
5. H's `handle_ping_request()` executes and calls `reset_peer_failures(M)`, resetting M's failure count to 0
6. M repeats this pattern indefinitely, staying connected despite being unresponsive to H's health checks

The code comment acknowledges this is "Future Work" to properly implement: [4](#0-3) 

The current implementation resets failures as a side effect without proper bidirectional health tracking, creating an exploitable bypass.

## Impact Explanation

This vulnerability enables **Medium Severity** impacts per the Aptos bug bounty criteria:

1. **Connection Slot Exhaustion**: With default `MAX_INBOUND_CONNECTIONS = 100`, attackers can occupy connection slots with unresponsive peers that game the health checker, preventing honest peers from connecting. [5](#0-4) 

2. **Network Health Degradation**: Validators waste resources attempting to communicate with unresponsive peers, leading to "Validator node slowdowns" (High severity category).

3. **Asymmetric Network Views**: Malicious peers can selectively respond to some nodes but not others, creating inconsistent network topologies that could be leveraged in coordination with other attacks.

4. **State Inconsistencies Requiring Intervention**: If enough validators have degraded connections, this could require manual intervention to restore network health (Medium severity category).

The vulnerability does not directly cause consensus breaks or fund loss, but degrades network availability and performance, qualifying as Medium severity under "State inconsistencies requiring intervention."

## Likelihood Explanation

**Likelihood: HIGH**

The attack is trivial to execute:
- Requires only peer-level network access (no validator privileges)
- Default configuration provides a 30-second window to reset failures (3 failures Ã— 10-second intervals)
- Attacker needs to send just one ping every ~25 seconds to maintain connection
- No rate limiting on inbound ping processing in the health checker layer
- Exploitable against any Aptos node accepting peer connections

The attack can be executed at scale: a single attacker controlling multiple peer identities can occupy significant connection capacity across the network.

## Recommendation

Implement proper bidirectional health tracking with the following changes:

1. **Track inbound ping timestamps separately** rather than resetting failure counters:

```rust
pub struct HealthCheckData {
    pub round: u64,
    pub failures: u64,
    pub last_inbound_ping: Option<SystemTime>,  // NEW
}
```

2. **Modify failure reset logic** to require BOTH successful outbound pings AND recent inbound activity:

```rust
fn handle_ping_request(&mut self, peer_id: PeerId, ...) {
    // Record inbound ping timestamp instead of resetting failures
    self.network_interface.record_inbound_ping(peer_id, self.time_service.now());
    
    // Only reset failures if peer is responsive to BOTH directions
    let should_reset = self.network_interface.is_bidirectionally_healthy(
        peer_id, 
        self.ping_interval * 2  // Allow 2 intervals grace period
    );
    
    if should_reset {
        self.network_interface.reset_peer_failures(peer_id);
    }
}
```

3. **Add rate limiting** on failure resets to prevent rapid reset abuse:

```rust
pub struct HealthCheckData {
    pub round: u64,
    pub failures: u64,
    pub last_inbound_ping: Option<SystemTime>,
    pub last_failure_reset: Option<SystemTime>,  // NEW
}

pub fn reset_peer_failures(&mut self, peer_id: PeerId) {
    if let Some(health_check_data) = self.health_check_data.write().get_mut(&peer_id) {
        // Only allow resets once per interval to prevent gaming
        if let Some(last_reset) = health_check_data.last_failure_reset {
            if now - last_reset < MIN_RESET_INTERVAL {
                return;  // Too frequent, ignore
            }
        }
        health_check_data.failures = 0;
        health_check_data.last_failure_reset = Some(now);
    }
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_inbound_ping_bypass_vulnerability() {
    let ping_failures_tolerated = 3;
    let (mut harness, health_checker) = TestHarness::new_permissive(ping_failures_tolerated);

    let test = async move {
        let peer_id = PeerId::new([0x42; PeerId::LENGTH]);
        harness.send_new_peer_notification(peer_id).await;

        // Malicious peer M never responds to outbound pings
        // Trigger 3 outbound pings that all fail
        for i in 0..3 {
            harness.trigger_ping().await;
            harness.expect_ping_send_not_ok().await;
            
            // Before accumulating enough failures to disconnect,
            // M sends an inbound ping to reset the counter
            if i == 2 {  // Just before disconnect threshold
                let _res_rx = harness.send_inbound_ping(peer_id, 999).await;
                // Failure counter is now reset to 0!
            }
        }

        // Peer should have been disconnected after 3 failures,
        // but the inbound ping reset the counter
        // So we can continue failing indefinitely:
        for _ in 0..10 {
            harness.trigger_ping().await;
            harness.expect_ping_send_not_ok().await;
            
            // Send periodic inbound pings to keep resetting failures
            let _res_rx = harness.send_inbound_ping(peer_id, 999).await;
        }

        // Peer is STILL connected despite 13 failed outbound pings!
        // In normal operation, should have disconnected after 3 failures.
        
        // This demonstrates the vulnerability: malicious peer stays
        // connected indefinitely by gaming the health checker.
    };
    
    future::join(health_checker.start(), test).await;
}
```

The PoC demonstrates that a peer can avoid disconnection by sending strategic inbound pings to reset failure counters, despite never responding to outbound health checks. This bypasses the intended health check disconnection mechanism and allows unresponsive peers to maintain connections indefinitely.

**Notes:**

The vulnerability stems from incomplete implementation of bidirectional health checking. While the code intentionally resets failures on inbound pings (as noted in the comment), the absence of proper tracking, rate limiting, and bidirectional validation creates an exploitable bypass. The "Future Work" comment indicates the developers recognized the need for proper bidirectional health tracking but the current implementation creates a security gap that should be addressed.

### Citations

**File:** network/framework/src/protocols/health_checker/mod.rs (L14-19)
```rust
//! Future Work
//! -----------
//! We can make a few other improvements to the health checker. These are:
//! - Make the policy for interpreting ping failures pluggable
//! - Use successful inbound pings as a sign of remote note being healthy
//! - Ping a peer only in periods of no application-level communication with the peer
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L277-306)
```rust
    fn handle_ping_request(
        &mut self,
        peer_id: PeerId,
        ping: Ping,
        protocol: ProtocolId,
        res_tx: oneshot::Sender<Result<Bytes, RpcError>>,
    ) {
        let message = match protocol.to_bytes(&HealthCheckerMsg::Pong(Pong(ping.0))) {
            Ok(msg) => msg,
            Err(e) => {
                warn!(
                    NetworkSchema::new(&self.network_context),
                    error = ?e,
                    "{} Unable to serialize pong response: {}", self.network_context, e
                );
                return;
            },
        };
        trace!(
            NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
            "{} Sending Pong response to peer: {} with nonce: {}",
            self.network_context,
            peer_id.short_str(),
            ping.0,
        );
        // Record Ingress HC here and reset failures.
        self.network_interface.reset_peer_failures(peer_id);

        let _ = res_tx.send(Ok(message.into()));
    }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L308-395)
```rust
    async fn handle_ping_response(
        &mut self,
        peer_id: PeerId,
        round: u64,
        req_nonce: u32,
        ping_result: Result<Pong, RpcError>,
    ) {
        match ping_result {
            Ok(pong) => {
                if pong.0 == req_nonce {
                    trace!(
                        NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                        rount = round,
                        "{} Ping successful for peer: {} round: {}",
                        self.network_context,
                        peer_id.short_str(),
                        round
                    );
                    // Update last successful ping to current round.
                    // If it's not in storage, don't bother updating it
                    self.network_interface
                        .reset_peer_round_state(peer_id, round);
                } else {
                    warn!(
                        SecurityEvent::InvalidHealthCheckerMsg,
                        NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                        "{} Pong nonce doesn't match Ping nonce. Round: {}, Pong: {}, Ping: {}",
                        self.network_context,
                        round,
                        pong.0,
                        req_nonce
                    );
                    debug_assert!(false, "Pong nonce doesn't match our challenge Ping nonce");
                }
            },
            Err(err) => {
                warn!(
                    NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                    round = round,
                    "{} Ping failed for peer: {} round: {} with error: {:#}",
                    self.network_context,
                    peer_id.short_str(),
                    round,
                    err
                );
                self.network_interface
                    .increment_peer_round_failure(peer_id, round);

                // If the ping failures are now more than
                // `self.ping_failures_tolerated`, we disconnect from the node.
                // The HealthChecker only performs the disconnect. It relies on
                // ConnectivityManager or the remote peer to re-establish the connection.
                let failures = self
                    .network_interface
                    .get_peer_failures(peer_id)
                    .unwrap_or(0);
                if failures > self.ping_failures_tolerated {
                    info!(
                        NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                        "{} Disconnecting from peer: {}",
                        self.network_context,
                        peer_id.short_str()
                    );
                    let peer_network_id =
                        PeerNetworkId::new(self.network_context.network_id(), peer_id);
                    if let Err(err) = timeout(
                        Duration::from_millis(50),
                        self.network_interface.disconnect_peer(
                            peer_network_id,
                            DisconnectReason::NetworkHealthCheckFailure,
                        ),
                    )
                    .await
                    {
                        warn!(
                            NetworkSchema::new(&self.network_context)
                                .remote_peer(&peer_id),
                            error = ?err,
                            "{} Failed to disconnect from peer: {} with error: {:?}",
                            self.network_context,
                            peer_id.short_str(),
                            err
                        );
                    }
                }
            },
        }
    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L118-124)
```rust
    /// Resets the number of peer failures for the given peer.
    /// If the peer is not found, nothing is done.
    pub fn reset_peer_failures(&mut self, peer_id: PeerId) {
        if let Some(health_check_data) = self.health_check_data.write().get_mut(&peer_id) {
            health_check_data.failures = 0;
        }
    }
```

**File:** config/src/config/network_config.rs (L38-44)
```rust
pub const PING_INTERVAL_MS: u64 = 10_000;
pub const PING_TIMEOUT_MS: u64 = 20_000;
pub const PING_FAILURES_TOLERATED: u64 = 3;
pub const CONNECTIVITY_CHECK_INTERVAL_MS: u64 = 5000;
pub const MAX_CONNECTION_DELAY_MS: u64 = 60_000; /* 1 minute */
pub const MAX_FULLNODE_OUTBOUND_CONNECTIONS: usize = 6;
pub const MAX_INBOUND_CONNECTIONS: usize = 100;
```
