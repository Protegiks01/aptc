# Audit Report

## Title
Transaction Broadcast Batch Size Violation and Priority Ordering Failure in Mempool Multi-Bucket Accumulation

## Summary
The `determine_broadcast_batch()` function in mempool's network layer fails to enforce the configured batch size limit when accumulating transactions from multiple sender buckets, allowing broadcasts up to 4x larger than intended. Additionally, it violates global transaction priority ordering by grouping transactions by sender bucket rather than gas price, causing high-priority transactions to be delayed behind lower-priority ones during propagation.

## Finding Description

The vulnerability exists in the fresh broadcast batch creation logic within `determine_broadcast_batch()`. [1](#0-0) 

The code sets `max_txns` to `shared_mempool_batch_size` (default 300) once before the loop, then calls `read_timeline()` for each sender bucket with the same `max_txns` value without decrementing it. With the default configuration of 4 sender buckets for PFNs, this allows accumulation of up to 1,200 transactions (4 × 300) instead of the intended 300-transaction limit.

The sender buckets are sorted by peer priority (Primary vs. Failover), not by transaction gas price. [2](#0-1)  This means all transactions from sender_bucket 0 are added to the output first, then all from sender_bucket 1, etc., regardless of their relative gas prices.

According to the mempool specification, transactions should be "ordered by gas price" to ensure clients willing to pay more can enter consensus earlier. [3](#0-2)  The current implementation violates this guarantee during broadcast by prioritizing sender bucket grouping over global gas price ordering.

**Exploitation Scenario:**

An attacker can exploit this by:
1. Creating accounts that hash to different sender buckets (determined by last byte of address modulo num_sender_buckets) [4](#0-3) 
2. Submitting 300 transactions from accounts in each sender bucket
3. The resulting broadcast will contain up to 1,200 transactions, potentially exceeding network message size limits
4. Legitimate high-gas transactions in later-processed buckets will be delayed behind lower-gas transactions from earlier buckets

## Impact Explanation

**Severity: Medium**

This vulnerability meets Medium severity criteria for the following reasons:

1. **Batch Size Protocol Violation**: Broadcasts can exceed the configured `shared_mempool_batch_size` by up to 4x, potentially violating the `MAX_APPLICATION_MESSAGE_SIZE` network limit (approximately 62 MiB). [5](#0-4)  With 4 sender buckets each returning max_batch_bytes worth of data, total size could reach ~248 MiB, far exceeding the 64 MiB maximum message size.

2. **Network Resource Exhaustion**: Oversized broadcasts can cause network congestion, peer overload, and broadcast failures when messages exceed size limits, degrading network performance.

3. **Transaction Priority Ordering Violation**: High-gas transactions may be propagated significantly later than low-gas transactions, violating fairness guarantees and user expectations about gas-based prioritization.

4. **Affected Nodes**: Primarily impacts Public Full Nodes (PFNs) with default `num_sender_buckets = 4`. [6](#0-5)  Validators and VFNs use `num_sender_buckets = 1` and are less affected. [7](#0-6) 

This does not constitute Critical or High severity because it does not affect consensus safety, cause loss of funds, or compromise validator operations directly.

## Likelihood Explanation

**Likelihood: High**

This issue occurs automatically during normal mempool operations on PFNs whenever:
- The mempool contains transactions distributed across multiple sender buckets
- A broadcast is triggered to an upstream peer
- No special attacker actions are required

The issue is triggered more severely when:
- Network traffic is high and multiple sender buckets have pending transactions
- An attacker deliberately submits transactions across all sender buckets

The likelihood is high because PFNs routinely operate with multiple sender buckets for load balancing, making this a recurring operational issue rather than an edge case.

## Recommendation

The batch size should be enforced globally across all sender buckets, not per-bucket. Modify the loop to track total transactions accumulated and stop when the limit is reached:

```rust
let max_txns = self.mempool_config.shared_mempool_batch_size;
let mut output_txns = vec![];
let mut output_updates = vec![];
let mut remaining_capacity = max_txns;

for (sender_bucket, peer_priority) in sender_buckets {
    if remaining_capacity == 0 {
        break; // Batch is full
    }
    
    let before = match peer_priority {
        BroadcastPeerPriority::Primary => None,
        BroadcastPeerPriority::Failover => Some(
            Instant::now() - Duration::from_millis(
                self.mempool_config.shared_mempool_failover_delay_ms,
            ),
        ),
    };
    
    let old_timeline_id = state.timelines.get(&sender_bucket).unwrap();
    let (txns, new_timeline_id) = mempool.read_timeline(
        sender_bucket,
        old_timeline_id,
        remaining_capacity, // Use remaining capacity, not max_txns
        before,
        peer_priority.clone(),
    );
    
    remaining_capacity -= txns.len(); // Decrement by actual txns added
    
    output_txns.extend(
        txns.into_iter()
            .map(|(txn, ready_time)| {
                (txn, ready_time, peer_priority.clone())
            })
            .collect::<Vec<_>>(),
    );
    output_updates.push((sender_bucket, (old_timeline_id.clone(), new_timeline_id)));
}
```

For the priority ordering issue, consider either:
1. Merging results from all sender buckets and re-sorting by gas price globally, or
2. Using a min-heap to interleave transactions from multiple buckets in priority order, or
3. Documenting that broadcast ordering is bucket-first, gas-price-second as an intentional trade-off for load balancing

## Proof of Concept

```rust
#[test]
fn test_broadcast_batch_accumulation_across_sender_buckets() {
    use aptos_config::config::MempoolConfig;
    
    // Create a PFN configuration with 4 sender buckets and batch size 100
    let mut config = MempoolConfig::default();
    config.num_sender_buckets = 4;
    config.shared_mempool_batch_size = 100;
    
    // Create test harness with multiple validators
    let (mut harness, nodes, _runtime) = 
        TestHarness::bootstrap_validator_network(2, Some(config));
    
    // Create 400 transactions (100 per sender bucket) with varying gas prices
    // Ensure transactions are distributed across all 4 sender buckets
    let mut txns = vec![];
    for bucket_idx in 0..4 {
        for seq in 0..100 {
            // Create account that hashes to specific sender bucket
            let account = create_account_for_bucket(bucket_idx);
            let gas_price = 100 - (bucket_idx * 25) + seq; // Varying gas prices
            let txn = create_test_transaction(account, seq, gas_price);
            txns.push(txn);
        }
    }
    
    let node_a = nodes.first().unwrap();
    harness.add_txns(node_a, txns);
    
    // Trigger broadcast and capture the batch
    let broadcast = harness.broadcast_txns(
        node_a,
        NetworkId::Validator,
        1,
        None,
        None,
        true,
        true,
        true,
    );
    
    // Verify the batch contains more than 100 transactions
    // Expected: should be capped at 100
    // Actual: will contain up to 400 (100 from each of 4 buckets)
    assert!(
        broadcast.len() > 100,
        "Batch size exceeds configured limit: {} > 100",
        broadcast.len()
    );
    
    // Verify priority ordering violation
    // Check if transactions from bucket 1 (lower gas) come before bucket 0 (higher gas)
    let bucket_0_txns: Vec<_> = broadcast.iter()
        .take(100)
        .collect();
    let bucket_1_txns: Vec<_> = broadcast.iter()
        .skip(100)
        .take(100)
        .collect();
    
    // Some transactions in bucket_1 should have higher gas than some in bucket_0
    // due to gas price distribution, but they appear later in output
    assert!(
        has_priority_inversion(&bucket_0_txns, &bucket_1_txns),
        "Priority ordering violated across sender buckets"
    );
}
```

**Notes**

The vulnerability primarily affects Public Full Nodes (PFNs) operating with the default `num_sender_buckets = 4` configuration. Validators and Validator Full Nodes (VFNs) use `num_sender_buckets = 1` and are largely unaffected by the batch accumulation issue, though they still experience the lack of global priority ordering if their configuration is modified.

The byte-based limit (`max_batch_bytes`) in `read_timeline()` provides partial mitigation per individual sender bucket, but does not prevent the total accumulated batch from exceeding intended limits when summed across all buckets. [8](#0-7) 

This issue represents a clear violation of the mempool's documented behavior that transactions should be "ordered by gas price" for propagation efficiency and fairness. [9](#0-8)

### Citations

**File:** mempool/src/shared_mempool/network.rs (L514-523)
```rust
                    // Sort sender_buckets based on priority. Primary peer should be first.
                    sender_buckets.sort_by(|(_, priority_a), (_, priority_b)| {
                        if priority_a == priority_b {
                            std::cmp::Ordering::Equal
                        } else if *priority_a == BroadcastPeerPriority::Primary {
                            std::cmp::Ordering::Less
                        } else {
                            std::cmp::Ordering::Greater
                        }
                    });
```

**File:** mempool/src/shared_mempool/network.rs (L524-556)
```rust
                    let max_txns = self.mempool_config.shared_mempool_batch_size;
                    let mut output_txns = vec![];
                    let mut output_updates = vec![];
                    for (sender_bucket, peer_priority) in sender_buckets {
                        let before = match peer_priority {
                            BroadcastPeerPriority::Primary => None,
                            BroadcastPeerPriority::Failover => Some(
                                Instant::now()
                                    - Duration::from_millis(
                                        self.mempool_config.shared_mempool_failover_delay_ms,
                                    ),
                            ),
                        };
                        if max_txns > 0 {
                            let old_timeline_id = state.timelines.get(&sender_bucket).unwrap();
                            let (txns, new_timeline_id) = mempool.read_timeline(
                                sender_bucket,
                                old_timeline_id,
                                max_txns,
                                before,
                                peer_priority.clone(),
                            );
                            output_txns.extend(
                                txns.into_iter()
                                    .map(|(txn, ready_time)| {
                                        (txn, ready_time, peer_priority.clone())
                                    })
                                    .collect::<Vec<_>>(),
                            );
                            output_updates
                                .push((sender_bucket, (old_timeline_id.clone(), new_timeline_id)));
                        }
                    }
```

**File:** mempool/README.md (L22-25)
```markdown
* Mempool can continue ordering transactions based on gas; and
* Consensus can allow transactions to build up in the mempool.

This allows transactions to be grouped into a single consensus block, and prioritized by gas price.
```

**File:** mempool/README.md (L36-36)
```markdown
The main index - PriorityIndex is an ordered queue of transactions that are “consensus-ready” (i.e., they have a sequence number which is sequential to the current sequence number for the account). This queue is ordered by gas price so that if a client is willing to pay more (than other clients) per unit of execution, then they can enter consensus earlier.
```

**File:** mempool/src/core_mempool/transaction_store.rs (L42-47)
```rust
pub fn sender_bucket(
    address: &AccountAddress,
    num_sender_buckets: MempoolSenderBucket,
) -> MempoolSenderBucket {
    address.as_ref()[address.as_ref().len() - 1] as MempoolSenderBucket % num_sender_buckets
}
```

**File:** mempool/src/core_mempool/transaction_store.rs (L805-806)
```rust
                    if batch_total_bytes.saturating_add(transaction_bytes) > self.max_batch_bytes {
                        break; // The batch is full
```

**File:** config/src/config/network_config.rs (L47-48)
```rust
pub const MAX_APPLICATION_MESSAGE_SIZE: usize =
    (MAX_MESSAGE_SIZE - MAX_MESSAGE_METADATA_SIZE) - MESSAGE_PADDING_SIZE; /* The message size that applications should check against */
```

**File:** config/src/config/mempool_config.rs (L137-137)
```rust
            num_sender_buckets: 4,
```

**File:** config/src/config/mempool_config.rs (L211-211)
```rust
                mempool_config.num_sender_buckets = 1;
```
