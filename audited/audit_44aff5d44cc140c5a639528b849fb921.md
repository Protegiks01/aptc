# Audit Report

## Title
Quorum Store Batch Persistence Race Condition: Subscribers Notified Before Durable Storage Guarantees

## Summary
The `BatchStore::persist()` function notifies subscribers immediately after non-durable database writes complete, creating a race condition where subscribers receive and act on batch data that may not survive a machine crash. This violates the durability guarantee implied by the "persist" operation and can lead to state inconsistencies during crash recovery.

## Finding Description

The vulnerability exists in the batch persistence flow within the Quorum Store component of Aptos consensus: [1](#0-0) 

The `persist()` function calls `persist_inner()`, which performs database writes using **relaxed writes** (no fsync): [2](#0-1) 

These relaxed writes are implemented through the QuorumStoreDB: [3](#0-2) 

The critical issue is that `write_schemas_relaxed()` explicitly does NOT guarantee durability: [4](#0-3) 

**The Race Condition:**

1. `persist_inner()` writes batch data to disk using relaxed writes (data goes to OS page cache, not physical disk)
2. The write operation returns success immediately
3. `notify_subscribers()` is called at line 622, sending the batch to all subscribers
4. Subscribers receive a `PersistedValue<BatchInfoExt>` and act on it immediately [5](#0-4) 

Subscribers, such as `BatchRequester`, immediately return the batch payload for consensus use: [6](#0-5) 

**The Critical Window:**

Between notification and actual fsync to disk (which may take milliseconds to seconds depending on OS write-back policy), if a machine crash occurs:
- The batch data is lost from the QuorumStore DB
- But subscribers have already acted on it (returned transactions, potentially voted on blocks)
- On restart, the node cannot reconstruct its previous state

This breaks the fundamental invariant: **"State transitions must be atomic and verifiable via Merkle proofs"** - nodes cannot verify transactions they previously acted upon.

## Impact Explanation

**Severity: HIGH**

This meets the HIGH severity criteria for "Significant protocol violations" because:

1. **State Inconsistency After Crash**: Nodes can vote on or validate blocks containing batches they subsequently lose, creating divergent states across validators after crash recovery.

2. **Liveness Degradation**: Validators that crash become unable to serve batch requests from peers, slowing down the network. If multiple validators crash simultaneously (datacenter power failure, network partition followed by cascading failures), batches can be permanently lost.

3. **Durability Contract Violation**: The function name `persist()`, return type `PersistedValue`, and subscriber notification all imply that data is durable when it may only be in volatile memory (OS page cache).

4. **Consensus Protocol Impact**: Validators may sign ProofOfStore certificates for batches they cannot subsequently access, breaking the assumption that signed batches remain available for block reconstruction.

While this does not directly cause funds loss or consensus safety violations (different blocks committed), it causes **state inconsistencies requiring manual intervention** (MEDIUM) and **validator node degradation** (HIGH).

## Likelihood Explanation

**Likelihood: MEDIUM**

This issue occurs with moderate frequency:

1. **Machine Crashes Are Common**: Datacenter operators experience regular hardware failures, power outages, and OS crashes. Typical mean time between failures (MTBF) for servers is 3-5 years, meaning large validator sets see crashes weekly.

2. **Default OS Behavior**: Linux default `vm.dirty_writeback_centisecs` is 500 (5 seconds), creating a 5-second window where relaxed writes are vulnerable. Under heavy I/O load, this can extend to 30+ seconds.

3. **No Explicit Mitigation**: The codebase provides no documented recovery mechanism for this scenario. Subscribers assume `PersistedValue` means durable persistence.

4. **Amplification Through Coordination**: The subscription mechanism exists to coordinate multiple flows, meaning multiple components may act on non-durable data simultaneously, amplifying the impact.

The issue is NOT easily exploitable by external attackers (requires causing crashes, which is out of scope), but naturally occurs during normal operations, making it a design flaw rather than an attack vector.

## Recommendation

**Option 1: Use Synchronous Writes for Critical Batches (Recommended)**

Replace relaxed writes with synchronous writes for batches that have subscribers:

```rust
fn persist_inner(
    &self,
    batch_info: BatchInfoExt,
    persist_request: PersistedValue<BatchInfoExt>,
) -> Option<SignedBatchInfo<BatchInfoExt>> {
    assert!(
        &batch_info == persist_request.batch_info(),
        "Provided batch info doesn't match persist request batch info"
    );
    match self.save(&persist_request) {
        Ok(needs_db) => {
            if needs_db {
                // Use sync writes instead of relaxed writes
                if !batch_info.is_v2() {
                    let persist_request =
                        persist_request.try_into().expect("Must be a V1 batch");
                    self.db
                        .save_batch_sync(persist_request)  // NEW: sync variant
                        .expect("Could not write to DB");
                } else {
                    self.db
                        .save_batch_v2_sync(persist_request)  // NEW: sync variant
                        .expect("Could not write to DB")
                }
            }
            // NOW subscribers get durable data
            if !batch_info.is_v2() {
                self.generate_signed_batch_info(batch_info.info().clone())
                    .ok()
                    .map(|inner| inner.into())
            } else {
                self.generate_signed_batch_info(batch_info).ok()
            }
        },
        Err(e) => {
            debug!("QS: failed to store to cache {:?}", e);
            None
        },
    }
}
```

Add sync write methods to `QuorumStoreStorage`:
```rust
fn save_batch_sync(&self, batch: PersistedValue<BatchInfo>) -> Result<(), DbError>;
fn save_batch_v2_sync(&self, batch: PersistedValue<BatchInfoExt>) -> Result<(), DbError>;
```

**Option 2: Defer Notification Until After Fsync**

Move `notify_subscribers()` outside the hot path and only call it after explicit fsync:

```rust
pub fn persist(&self, persist_requests: Vec<PersistedValue<BatchInfoExt>>) 
    -> Vec<SignedBatchInfo<BatchInfoExt>> {
    let mut signed_infos = vec![];
    let mut to_notify = vec![];
    
    for persist_request in persist_requests.into_iter() {
        let batch_info = persist_request.batch_info().clone();
        if let Some(signed_info) = self.persist_inner(batch_info, persist_request.clone()) {
            to_notify.push(persist_request);
            signed_infos.push(signed_info);
        }
    }
    
    // Explicit fsync before notification
    self.db.flush().expect("Could not fsync DB");
    
    for persist_request in to_notify {
        self.notify_subscribers(persist_request);
    }
    
    signed_infos
}
```

**Option 3: Document and Accept Risk**

If performance is critical, document that `PersistedValue` is not durable across machine crashes and add recovery logic to re-fetch lost batches from peers on restart.

## Proof of Concept

```rust
#[tokio::test]
async fn test_batch_notification_before_fsync() {
    // Setup batch store with actual disk persistence
    let temp_dir = TempPath::new();
    let db = Arc::new(QuorumStoreDB::new(temp_dir.path()));
    let batch_store = Arc::new(BatchStore::new(
        1, // epoch
        true, // is_new_epoch
        0, // last_certified_time
        db.clone(),
        1024 * 1024, // memory_quota
        10 * 1024 * 1024, // db_quota
        100, // batch_quota
        ValidatorSigner::random(None),
        60_000_000, // expiration_buffer_usecs
    ));
    
    // Subscribe to batch
    let digest = HashValue::random();
    let rx = batch_store.subscribe(digest);
    
    // Create and persist batch
    let transactions = vec![create_signed_transaction()];
    let batch_info = create_batch_info(digest);
    let persist_request = PersistedValue::new(batch_info.into(), Some(transactions.clone()));
    
    // Persist - this triggers notification immediately after relaxed write
    let signed_infos = batch_store.persist(vec![persist_request]);
    assert!(!signed_infos.is_empty());
    
    // Subscriber receives notification
    let received = rx.await.unwrap();
    assert_eq!(received.digest(), &digest);
    
    // Simulate machine crash BEFORE fsync by killing process
    // and checking DB state
    drop(batch_store);
    
    // Reopen DB - in real crash, page cache is lost
    let db2 = Arc::new(QuorumStoreDB::new(temp_dir.path()));
    
    // Batch may or may not be present depending on OS fsync timing
    // In worst case, batch is lost despite notification being sent
    match db2.get_batch_v2(&digest) {
        Ok(Some(_)) => println!("Lucky: fsync happened before crash"),
        Ok(None) => println!("VULNERABILITY: batch lost but subscribers were notified"),
        Err(_) => println!("VULNERABILITY: batch lost but subscribers were notified"),
    }
}
```

To reliably trigger the vulnerability, use `fail` crate injection points or mock the DB layer to simulate the race condition between relaxed write completion and actual fsync.

## Notes

- The use of relaxed writes appears intentional for performance optimization
- The impact is amplified when multiple validators crash simultaneously (correlated failures in datacenters)
- The vulnerability manifests during normal operations (crashes), not requiring attacker actions
- Recovery requires manual intervention or automatic peer re-fetching mechanisms not currently implemented
- This issue affects both V1 and V2 batch persistence flows

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L488-528)
```rust
    fn persist_inner(
        &self,
        batch_info: BatchInfoExt,
        persist_request: PersistedValue<BatchInfoExt>,
    ) -> Option<SignedBatchInfo<BatchInfoExt>> {
        assert!(
            &batch_info == persist_request.batch_info(),
            "Provided batch info doesn't match persist request batch info"
        );
        match self.save(&persist_request) {
            Ok(needs_db) => {
                trace!("QS: sign digest {}", persist_request.digest());
                if needs_db {
                    if !batch_info.is_v2() {
                        let persist_request =
                            persist_request.try_into().expect("Must be a V1 batch");
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch(persist_request)
                            .expect("Could not write to DB");
                    } else {
                        #[allow(clippy::unwrap_in_result)]
                        self.db
                            .save_batch_v2(persist_request)
                            .expect("Could not write to DB")
                    }
                }
                if !batch_info.is_v2() {
                    self.generate_signed_batch_info(batch_info.info().clone())
                        .ok()
                        .map(|inner| inner.into())
                } else {
                    self.generate_signed_batch_info(batch_info).ok()
                }
            },
            Err(e) => {
                debug!("QS: failed to store to cache {:?}", e);
                None
            },
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L604-610)
```rust
    fn notify_subscribers(&self, value: PersistedValue<BatchInfoExt>) {
        if let Some((_, subscribers)) = self.persist_subscribers.remove(value.digest()) {
            for subscriber in subscribers {
                subscriber.send(value.clone()).ok();
            }
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L614-627)
```rust
    fn persist(
        &self,
        persist_requests: Vec<PersistedValue<BatchInfoExt>>,
    ) -> Vec<SignedBatchInfo<BatchInfoExt>> {
        let mut signed_infos = vec![];
        for persist_request in persist_requests.into_iter() {
            let batch_info = persist_request.batch_info().clone();
            if let Some(signed_info) = self.persist_inner(batch_info, persist_request.clone()) {
                self.notify_subscribers(persist_request);
                signed_infos.push(signed_info);
            }
        }
        signed_infos
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L82-89)
```rust
    /// Relaxed writes instead of sync writes.
    pub fn put<S: Schema>(&self, key: &S::Key, value: &S::Value) -> Result<(), DbError> {
        // Not necessary to use a batch, but we'd like a central place to bump counters.
        let mut batch = self.db.new_native_batch();
        batch.put::<S>(key, value)?;
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** storage/schemadb/src/lib.rs (L311-318)
```rust
    /// Writes without sync flag in write option.
    /// If this flag is false, and the machine crashes, some recent
    /// writes may be lost.  Note that if it is just the process that
    /// crashes (i.e., the machine does not reboot), no writes will be
    /// lost even if sync==false.
    pub fn write_schemas_relaxed(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &WriteOptions::default())
    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L162-173)
```rust
                    result = &mut subscriber_rx => {
                        match result {
                            Ok(persisted_value) => {
                                counters::RECEIVED_BATCH_FROM_SUBSCRIPTION_COUNT.inc();
                                let (_, maybe_payload) = persisted_value.unpack();
                                return Ok(maybe_payload.expect("persisted value must exist"));
                            }
                            Err(err) => {
                                debug!("channel closed: {}", err);
                            }
                        };
                    },
```
