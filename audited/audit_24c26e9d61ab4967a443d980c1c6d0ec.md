# Audit Report

## Title
Write Set Verification Bypass in KV-Only Replay Allows Arbitrary State Manipulation

## Summary
The `replay_kv()` function bypasses write set integrity verification during database restoration, allowing an attacker who controls backup storage to inject malicious state changes that would be rejected by normal Move VM execution. This completely circumvents all access control checks including `@aptos_framework` and `@core_resources` protections.

## Finding Description

The vulnerability exists in the backup restoration pipeline when using the `--kv-only-replay` option. The attack flow is:

**1. Backup File Structure**

Each transaction backup record contains a tuple: `(Transaction, PersistedAuxiliaryInfo, TransactionInfo, Events, WriteSet)`. The `TransactionInfo` includes a `state_change_hash` field which should be `CryptoHash::hash(WriteSet)`. [1](#0-0) 

**2. Missing Verification in LoadedChunk::load()**

When loading backup data, the code constructs a `TransactionListWithProof` for verification, but critically **excludes the write sets** from this verification: [2](#0-1) 

The write sets are loaded separately and never verified: [3](#0-2) 

**3. TransactionListWithProof Verification Gap**

The `TransactionListWithProof::verify()` method only verifies:
- Transaction hashes match `TransactionInfo.transaction_hash`
- Transaction infos are proven by the `LedgerInfo` signature
- Events match `TransactionInfo.event_root_hash`

But it **does NOT verify** that write sets match `TransactionInfo.state_change_hash`: [4](#0-3) 

**4. Direct State Application Without Validation**

The `replay_kv()` function directly applies these unverified write sets to the database state: [5](#0-4) 

This calls `save_transactions_and_replay_kv()` which applies write sets without any verification: [6](#0-5) 

The underlying `save_transactions_impl()` with `kv_replay=true` directly applies the write sets to state: [7](#0-6) 

**5. Comparison with Normal Replay**

In contrast, the normal `replay_transactions()` path re-executes transactions through the VM and verifies outputs match: [8](#0-7) 

This verification uses `ensure_match_transaction_info()` which **does verify** the write set hash: [9](#0-8) 

**Attack Scenario:**

1. Attacker compromises backup storage or operates a malicious backup service
2. Attacker modifies backup files:
   - Keeps `Transaction`, `TransactionInfo`, `Events` unchanged
   - Replaces `WriteSet` with malicious state changes (e.g., minting coins to attacker address, modifying validator set, changing governance parameters)
3. Node operator restores from compromised backup using `--kv-only-replay`
4. Malicious write sets are applied directly to state without verification
5. All Move VM access controls (like `assert_aptos_framework()`) are bypassed

## Impact Explanation

**CRITICAL Severity** - This vulnerability allows complete state manipulation:

- **Loss of Funds**: Attacker can mint unlimited APT coins to arbitrary addresses by crafting write sets that modify `CoinStore` resources
- **Consensus Safety Violation**: If a validator node is compromised, it will have different state than other validators, causing consensus divergence
- **Validator Set Manipulation**: Attacker can add/remove validators, modify stake amounts, manipulate validator rewards
- **Governance Bypass**: Attacker can modify voting power, manipulate proposal outcomes, change governance parameters
- **System Address Compromise**: Attacker can modify state at `@aptos_framework` and `@core_resources` addresses without any authorization checks

This directly violates invariants:
- **Access Control**: System addresses are no longer protected
- **State Consistency**: State transitions are not verifiable since write set hashes don't match `TransactionInfo`
- **Deterministic Execution**: Different nodes will have different state if some restore from malicious backups

## Likelihood Explanation

**MEDIUM to HIGH Likelihood**:

**Attack Requirements:**
- Attacker must compromise backup storage OR be a malicious backup service provider
- Node operator must use `--kv-only-replay` option (commonly used for fast restoration)
- No other technical barriers exist

**Realistic Scenarios:**
1. **Supply Chain Attack**: Malicious or compromised backup service provider
2. **Compromised Infrastructure**: Attacker gains access to S3/GCS backup buckets
3. **Insider Threat**: Malicious backup administrator
4. **Man-in-the-Middle**: If backup transfer is not properly secured

The `--kv-only-replay` feature is specifically designed for performance optimization during restoration, making it a commonly used code path.

## Recommendation

**Immediate Fix**: Add write set verification in `LoadedChunk::load()`:

```rust
// After line 167 in restore.rs, add write set verification:
for (write_set, txn_info) in write_sets.iter().zip(txn_infos.iter()) {
    let write_set_hash = CryptoHash::hash(write_set);
    ensure!(
        write_set_hash == txn_info.state_change_hash(),
        "Write set hash mismatch at version {}. Expected: {:?}, Got: {:?}",
        manifest.first_version,
        txn_info.state_change_hash(),
        write_set_hash
    );
}
```

**Alternative Approach**: Always verify write sets before applying them in `save_transactions_impl()` when `kv_replay=true`:

```rust
// In restore_utils.rs, before line 269, add:
if kv_replay {
    for (idx, (ws, txn_info)) in write_sets.iter().zip(txn_infos.iter()).enumerate() {
        let write_set_hash = CryptoHash::hash(ws);
        ensure!(
            write_set_hash == txn_info.state_change_hash(),
            "Write set verification failed at version {}: expected {:?}, got {:?}",
            first_version + idx as u64,
            txn_info.state_change_hash(),
            write_set_hash
        );
    }
}
```

**Defense in Depth**: 
1. Add cryptographic signatures to backup manifests
2. Implement backup integrity checking before restoration
3. Log warning when using `--kv-only-replay` about trust assumptions
4. Consider deprecating `--kv-only-replay` in favor of always re-executing transactions

## Proof of Concept

```rust
// Test demonstrating the vulnerability
#[tokio::test]
async fn test_malicious_write_set_bypass() {
    // Setup: Create a legitimate backup
    let (backup_storage, manifest) = create_test_backup().await;
    
    // Attack: Modify write sets in backup file
    let malicious_write_set = create_malicious_write_set(
        // Mint 1 million APT to attacker address
        attacker_address,
        1_000_000_000_000, // 1M APT in octas
    );
    
    // Load the backup file and inject malicious write set
    let backup_file = backup_storage.open_for_write(&manifest.transactions).await.unwrap();
    let modified_records = modify_backup_with_malicious_writeset(
        original_records,
        malicious_write_set,
    );
    write_records_to_backup(backup_file, modified_records).await;
    
    // Restore using kv-only-replay
    let restore_opt = TransactionRestoreOpt {
        manifest_handle: manifest,
        replay_from_version: Some(0),
        kv_only_replay: Some(true), // Triggers vulnerable path
    };
    
    let controller = TransactionRestoreController::new(
        restore_opt,
        global_opt,
        storage,
        epoch_history,
        VerifyExecutionMode::NoVerify,
    );
    
    // BUG: This succeeds even though write set doesn't match state_change_hash
    controller.run().await.expect("Restore should succeed");
    
    // Verify attacker now has 1M APT (state was manipulated)
    let attacker_balance = get_coin_balance(db, attacker_address).await;
    assert_eq!(attacker_balance, 1_000_000_000_000);
    
    // This demonstrates complete bypass of access controls
}
```

## Notes

This vulnerability represents a fundamental flaw in the backup/restore trust model. The `--kv-only-replay` optimization assumes backup data integrity without cryptographic verification. While the `TransactionInfo` objects are properly signed and verified through the validator quorum, the actual write sets are never validated against their committed hashes.

This is particularly severe because:
1. It bypasses ALL Move VM security checks including prologue/epilogue validation
2. It allows modification of system addresses without proper authorization
3. It can cause permanent consensus divergence if validators restore from different backups
4. The attack is silent - no error is raised during restoration

The fix is straightforward but critical: always verify `hash(WriteSet) == TransactionInfo.state_change_hash` before applying write sets to state.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L112-137)
```rust
        while let Some(record_bytes) = file.read_record_bytes().await? {
            let (txn, aux_info, txn_info, events, write_set): (
                _,
                PersistedAuxiliaryInfo,
                _,
                _,
                WriteSet,
            ) = match manifest.format {
                TransactionChunkFormat::V0 => {
                    let (txn, txn_info, events, write_set) = bcs::from_bytes(&record_bytes)?;
                    (
                        txn,
                        PersistedAuxiliaryInfo::None,
                        txn_info,
                        events,
                        write_set,
                    )
                },
                TransactionChunkFormat::V1 => bcs::from_bytes(&record_bytes)?,
            };
            txns.push(txn);
            persisted_aux_info.push(aux_info);
            txn_infos.push(txn_info);
            event_vecs.push(events);
            write_sets.push(write_set);
        }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L156-167)
```rust
        // make a `TransactionListWithProof` to reuse its verification code.
        let txn_list_with_proof =
            TransactionListWithProofV2::new(TransactionListWithAuxiliaryInfos::new(
                TransactionListWithProof::new(
                    txns,
                    Some(event_vecs),
                    Some(manifest.first_version),
                    TransactionInfoListWithProof::new(range_proof, txn_infos),
                ),
                persisted_aux_info,
            ));
        txn_list_with_proof.verify(ledger_info.ledger_info(), Some(manifest.first_version))?;
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L177-186)
```rust
        Ok(Self {
            manifest,
            txns,
            persisted_aux_info,
            txn_infos,
            event_vecs,
            range_proof,
            write_sets,
        })
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L554-637)
```rust
    async fn replay_kv(
        &self,
        restore_handler: &RestoreHandler,
        txns_to_execute_stream: impl Stream<
            Item = Result<(
                Transaction,
                PersistedAuxiliaryInfo,
                TransactionInfo,
                WriteSet,
                Vec<ContractEvent>,
            )>,
        >,
    ) -> Result<()> {
        let (first_version, _) = self.replay_from_version.unwrap();
        restore_handler.force_state_version_for_kv_restore(first_version.checked_sub(1))?;

        let mut base_version = first_version;
        let mut offset = 0u64;
        let replay_start = Instant::now();
        let arc_restore_handler = Arc::new(restore_handler.clone());

        let db_commit_stream = txns_to_execute_stream
            .try_chunks(BATCH_SIZE)
            .err_into::<anyhow::Error>()
            .map_ok(|chunk| {
                let (txns, persisted_aux_info, txn_infos, write_sets, events): (
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                    Vec<_>,
                ) = chunk.into_iter().multiunzip();
                let handler = arc_restore_handler.clone();
                base_version += offset;
                offset = txns.len() as u64;
                async move {
                    let _timer = OTHER_TIMERS_SECONDS.timer_with(&["replay_txn_chunk_kv_only"]);
                    tokio::task::spawn_blocking(move || {
                        // we directly save transaction and kvs to DB without involving chunk executor
                        handler.save_transactions_and_replay_kv(
                            base_version,
                            &txns,
                            &persisted_aux_info,
                            &txn_infos,
                            &events,
                            write_sets,
                        )?;
                        // return the last version after the replaying
                        Ok(base_version + offset - 1)
                    })
                    .err_into::<anyhow::Error>()
                    .await
                }
            })
            .try_buffered_x(self.global_opt.concurrent_downloads, 1)
            .and_then(future::ready);

        let total_replayed = db_commit_stream
            .and_then(|version| async move {
                let _timer = OTHER_TIMERS_SECONDS.timer_with(&["commit_txn_chunk_kv_only"]);
                tokio::task::spawn_blocking(move || {
                    // version is the latest version finishing the KV replaying
                    let total_replayed = version - first_version;
                    TRANSACTION_REPLAY_VERSION.set(version as i64);
                    info!(
                        version = version,
                        accumulative_tps =
                            (total_replayed as f64 / replay_start.elapsed().as_secs_f64()) as u64,
                        "KV replayed."
                    );
                    Ok(version)
                })
                .await?
            })
            .try_fold(0, |_total, total| future::ok(total))
            .await?;
        info!(
            total_replayed = total_replayed,
            accumulative_tps =
                (total_replayed as f64 / replay_start.elapsed().as_secs_f64()) as u64,
            "KV Replay finished."
        );
        Ok(())
    }
```

**File:** types/src/transaction/mod.rs (L1869-1928)
```rust
    pub fn ensure_match_transaction_info(
        &self,
        version: Version,
        txn_info: &TransactionInfo,
        expected_write_set: Option<&WriteSet>,
        expected_events: Option<&[ContractEvent]>,
    ) -> Result<()> {
        const ERR_MSG: &str = "TransactionOutput does not match TransactionInfo";

        let expected_txn_status: TransactionStatus = txn_info.status().clone().into();
        ensure!(
            self.status() == &expected_txn_status,
            "{}: version:{}, status:{:?}, auxiliary data:{:?}, expected:{:?}",
            ERR_MSG,
            version,
            self.status(),
            self.auxiliary_data(),
            expected_txn_status,
        );

        ensure!(
            self.gas_used() == txn_info.gas_used(),
            "{}: version:{}, gas_used:{:?}, expected:{:?}",
            ERR_MSG,
            version,
            self.gas_used(),
            txn_info.gas_used(),
        );

        let write_set_hash = CryptoHash::hash(self.write_set());
        ensure!(
            write_set_hash == txn_info.state_change_hash(),
            "{}: version:{}, write_set_hash:{:?}, expected:{:?}, write_set: {:?}, expected(if known): {:?}",
            ERR_MSG,
            version,
            write_set_hash,
            txn_info.state_change_hash(),
            self.write_set,
            expected_write_set,
        );

        let event_hashes = self
            .events()
            .iter()
            .map(CryptoHash::hash)
            .collect::<Vec<_>>();
        let event_root_hash = InMemoryEventAccumulator::from_leaves(&event_hashes).root_hash;
        ensure!(
            event_root_hash == txn_info.event_root_hash(),
            "{}: version:{}, event_root_hash:{:?}, expected:{:?}, events: {:?}, expected(if known): {:?}",
            ERR_MSG,
            version,
            event_root_hash,
            txn_info.event_root_hash(),
            self.events(),
            expected_events,
        );

        Ok(())
    }
```

**File:** types/src/transaction/mod.rs (L2290-2354)
```rust
    /// 1. All transactions exist on the given `ledger_info`.
    /// 2. All transactions in the list have consecutive versions.
    /// 3. If `first_transaction_version` is None, the transaction list is empty.
    ///    Otherwise, the transaction list starts at `first_transaction_version`.
    /// 4. If events exist, they match the expected event root hashes in the proof.
    pub fn verify(
        &self,
        ledger_info: &LedgerInfo,
        first_transaction_version: Option<Version>,
    ) -> Result<()> {
        // Verify the first transaction versions match
        ensure!(
            self.get_first_transaction_version() == first_transaction_version,
            "First transaction version ({:?}) doesn't match given version ({:?}).",
            self.get_first_transaction_version(),
            first_transaction_version,
        );

        // Verify the lengths of the transactions and transaction infos match
        ensure!(
            self.proof.transaction_infos.len() == self.get_num_transactions(),
            "The number of TransactionInfo objects ({}) does not match the number of \
             transactions ({}).",
            self.proof.transaction_infos.len(),
            self.get_num_transactions(),
        );

        // Verify the transaction hashes match those of the transaction infos
        self.transactions
            .par_iter()
            .zip_eq(self.proof.transaction_infos.par_iter())
            .map(|(txn, txn_info)| {
                let txn_hash = CryptoHash::hash(txn);
                ensure!(
                    txn_hash == txn_info.transaction_hash(),
                    "The hash of transaction does not match the transaction info in proof. \
                     Transaction hash: {:x}. Transaction hash in txn_info: {:x}.",
                    txn_hash,
                    txn_info.transaction_hash(),
                );
                Ok(())
            })
            .collect::<Result<Vec<_>>>()?;

        // Verify the transaction infos are proven by the ledger info.
        self.proof
            .verify(ledger_info, self.get_first_transaction_version())?;

        // Verify the events if they exist.
        if let Some(event_lists) = &self.events {
            ensure!(
                event_lists.len() == self.get_num_transactions(),
                "The length of event_lists ({}) does not match the number of transactions ({}).",
                event_lists.len(),
                self.get_num_transactions(),
            );
            event_lists
                .into_par_iter()
                .zip_eq(self.proof.transaction_infos.par_iter())
                .map(|(events, txn_info)| verify_events_against_root_hash(events, txn_info))
                .collect::<Result<Vec<_>>>()?;
        }

        Ok(())
    }
```

**File:** storage/aptosdb/src/backup/restore_handler.rs (L105-126)
```rust
    pub fn save_transactions_and_replay_kv(
        &self,
        first_version: Version,
        txns: &[Transaction],
        persisted_aux_info: &[PersistedAuxiliaryInfo],
        txn_infos: &[TransactionInfo],
        events: &[Vec<ContractEvent>],
        write_sets: Vec<WriteSet>,
    ) -> Result<()> {
        restore_utils::save_transactions(
            self.state_store.clone(),
            self.ledger_db.clone(),
            first_version,
            txns,
            persisted_aux_info,
            txn_infos,
            events,
            write_sets,
            None,
            true,
        )
    }
```

**File:** storage/aptosdb/src/backup/restore_utils.rs (L193-277)
```rust
pub(crate) fn save_transactions_impl(
    state_store: Arc<StateStore>,
    ledger_db: Arc<LedgerDb>,
    first_version: Version,
    txns: &[Transaction],
    persisted_aux_info: &[PersistedAuxiliaryInfo],
    txn_infos: &[TransactionInfo],
    events: &[Vec<ContractEvent>],
    write_sets: &[WriteSet],
    ledger_db_batch: &mut LedgerDbSchemaBatches,
    state_kv_batches: &mut ShardedStateKvSchemaBatch,
    kv_replay: bool,
) -> Result<()> {
    for (idx, txn) in txns.iter().enumerate() {
        ledger_db.transaction_db().put_transaction(
            first_version + idx as Version,
            txn,
            /*skip_index=*/ false,
            &mut ledger_db_batch.transaction_db_batches,
        )?;
    }

    for (idx, aux_info) in persisted_aux_info.iter().enumerate() {
        PersistedAuxiliaryInfoDb::put_persisted_auxiliary_info(
            first_version + idx as Version,
            aux_info,
            &mut ledger_db_batch.persisted_auxiliary_info_db_batches,
        )?;
    }

    for (idx, txn_info) in txn_infos.iter().enumerate() {
        TransactionInfoDb::put_transaction_info(
            first_version + idx as Version,
            txn_info,
            &mut ledger_db_batch.transaction_info_db_batches,
        )?;
    }

    ledger_db
        .transaction_accumulator_db()
        .put_transaction_accumulator(
            first_version,
            txn_infos,
            &mut ledger_db_batch.transaction_accumulator_db_batches,
        )?;

    ledger_db.event_db().put_events_multiple_versions(
        first_version,
        events,
        &mut ledger_db_batch.event_db_batches,
    )?;

    if ledger_db.enable_storage_sharding() {
        for (idx, txn_events) in events.iter().enumerate() {
            for event in txn_events {
                if let Some(event_key) = event.event_key() {
                    if *event_key == new_block_event_key() {
                        LedgerMetadataDb::put_block_info(
                            first_version + idx as Version,
                            event,
                            &mut ledger_db_batch.ledger_metadata_db_batches,
                        )?;
                    }
                }
            }
        }
    }
    // insert changes in write set schema batch
    for (idx, ws) in write_sets.iter().enumerate() {
        WriteSetDb::put_write_set(
            first_version + idx as Version,
            ws,
            &mut ledger_db_batch.write_set_db_batches,
        )?;
    }

    if kv_replay && first_version > 0 && state_store.get_usage(Some(first_version - 1)).is_ok() {
        let (ledger_state, _hot_state_updates) = state_store.calculate_state_and_put_updates(
            &StateUpdateRefs::index_write_sets(first_version, write_sets, write_sets.len(), vec![]),
            &mut ledger_db_batch.ledger_metadata_db_batches, // used for storing the storage usage
            state_kv_batches,
        )?;
        // n.b. ideally this is set after the batches are committed
        state_store.set_state_ignoring_summary(ledger_state);
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L630-650)
```rust
            begin_version..end_version,
            &execution_output.to_commit.transaction_outputs,
            transaction_infos.iter(),
            write_sets.iter(),
            event_vecs.iter(),
        )) {
            if let Err(err) = txn_out.ensure_match_transaction_info(
                version,
                txn_info,
                Some(write_set),
                Some(events),
            ) {
                return if verify_execution_mode.is_lazy_quit() {
                    error!("(Not quitting right away.) {}", err);
                    verify_execution_mode.mark_seen_error();
                    Ok(version + 1)
                } else {
                    Err(err)
                };
            }
        }
```
