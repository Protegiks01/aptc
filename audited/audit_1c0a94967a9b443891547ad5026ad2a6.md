# Audit Report

## Title
QuorumStoreDB Lacks fsync: Coordinated Power Failure Can Cause Permanent Consensus Halt

## Summary
The QuorumStoreDB uses `write_schemas_relaxed()` which does not perform fsync or fdatasync, creating a critical durability vulnerability. During a coordinated power failure affecting multiple validators, transaction batch data can be permanently lost from all nodes, causing blocks with valid quorum certificates to become unexecutable and halting consensus permanently.

## Finding Description

**The Vulnerability:**

QuorumStoreDB stores transaction batches using `write_schemas_relaxed()`, which explicitly does NOT perform fsync operations: [1](#0-0) [2](#0-1) 

This is in direct contrast to AptosDB, which uses `write_schemas()` WITH fsync for committed transactions: [3](#0-2) 

**The Attack Scenario:**

1. **Batch Creation Phase**: A validator creates batch X containing transactions and persists it to QuorumStoreDB using `write_schemas_relaxed()`. The data is written to the OS page cache but NOT synced to disk.

2. **Proof of Store Phase**: A quorum of validators receive and persist batch X to their QuorumStoreDBs (also without fsync). They sign a ProofOfStore for batch X.

3. **Block Consensus Phase**: Block B is proposed containing the ProofOfStore for batch X. Validators vote on block B by verifying the ProofOfStore signatures (NOT by verifying batch data availability). Block B receives a valid quorum certificate.

4. **Critical Window - Power Failure**: BEFORE block B is executed and its transactions are committed to AptosDB, a coordinated power failure (e.g., datacenter-wide outage) affects multiple validators simultaneously. The OS page cache is lost, and batch X disappears from ALL affected validators' QuorumStoreDBs.

5. **Recovery Failure**: After power restoration, validators attempt to execute block B. The execution path requires fetching batch X: [4](#0-3) 

When the batch cannot be found locally (line 690), the node attempts to fetch it from peers using BatchRequester (lines 696-703). However, if ALL nodes lost the batch due to the coordinated failure, the request will exhaust all retries and return `ExecutorError::CouldNotGetData`: [5](#0-4) 

6. **Consensus Halt**: Block B has a valid quorum certificate but cannot be executed. The blockchain cannot progress past block B because:
   - Blocks form a linear chain that cannot be skipped
   - The block has cryptographic proof of quorum acceptance
   - But the transaction data needed for execution is permanently lost
   - New validators cannot sync past this block

**Why This Breaks Consensus Invariants:**

This violates the **Consensus Safety** invariant: "AptosBFT must prevent double-spending and chain splits under < 1/3 Byzantine." A block with a valid QC becomes permanently unexecutable, halting the network without any Byzantine behavior—just a coordinated power failure.

It also violates **State Consistency**: "State transitions must be atomic and verifiable via Merkle proofs." The state cannot transition past block B despite having quorum agreement.

## Impact Explanation

**Critical Severity** - This vulnerability meets multiple Critical impact criteria per the Aptos bug bounty program:

1. **Non-recoverable network partition (requires hardfork)**: Once all nodes lose a batch referenced by a committed quorum certificate, the blockchain cannot progress. Recovery requires either:
   - A hardfork to skip the problematic block
   - Manual checkpoint restoration
   - Re-execution from genesis with modified state

2. **Total loss of liveness/network availability**: The network completely halts—no new transactions can be processed, no blocks can be committed. This is not a temporary slowdown but a permanent halt until manual intervention.

3. **Consensus/Safety violation**: A block with a valid quorum certificate cannot be executed, breaking the fundamental guarantee that quorum agreement leads to finality.

The impact affects the ENTIRE network, not just individual nodes. New validators attempting to join cannot sync past the problematic block.

## Likelihood Explanation

**Likelihood: Medium to High in certain deployment scenarios**

**Required Conditions:**
1. Coordinated power failure affecting multiple validators BEFORE batch data is committed to AptosDB
2. The timing window is between batch creation and transaction commitment (typically seconds to minutes)
3. OS page cache must not have flushed to disk (can be 5-30 seconds or more depending on OS settings)

**Why This Is Realistic:**

- **Datacenter deployments**: Multiple validators often run in the same datacenter or availability zone. Datacenter-wide power failures, while rare, do occur due to:
  - Power distribution equipment failure
  - UPS system failures  
  - Grid-level outages
  - Generator failover failures

- **OS write buffering**: Modern operating systems can buffer writes for many seconds. The comment in the code explicitly acknowledges this: [6](#0-5) 

- **Large transaction batches**: Batches can contain significant transaction sets that may take time to process through consensus, expanding the vulnerability window.

- **Network partitions during recovery**: If nodes restart in a staggered fashion after power restoration, early-restarting nodes may garbage collect batches before late-restarting nodes can replicate them.

Historical blockchain incidents (e.g., AWS outages affecting multiple validators, datacenter cooling failures) demonstrate that coordinated failures are not theoretical.

## Recommendation

**Immediate Fix: Add fsync to critical batch persistence operations**

Modify QuorumStoreDB to use `write_schemas()` instead of `write_schemas_relaxed()` for batch data that has been included in ProofOfStore certificates:

```rust
// In consensus/src/quorum_store/quorum_store_db.rs

pub fn put<S: Schema>(&self, key: &S::Key, value: &S::Value) -> Result<(), DbError> {
    let mut batch = self.db.new_native_batch();
    batch.put::<S>(key, value)?;
    // CRITICAL FIX: Use write_schemas() for durability
    self.db.write_schemas(batch)?;  // Changed from write_schemas_relaxed
    Ok(())
}
```

**Alternative Optimized Fix: Selective fsync for committed batches**

If performance is critical, implement a two-tier system:
1. Use `write_schemas_relaxed()` for speculative batch storage
2. Use `write_schemas()` with fsync when a batch's ProofOfStore is included in a block that receives a quorum certificate
3. This ensures durability only for batches that the network has committed to processing

**Additional Hardening:**
1. Implement batch recovery protocol where nodes can reconstruct batches from committed transaction data in AptosDB
2. Add monitoring/alerting for `ExecutorError::CouldNotGetData` errors during block execution
3. Implement graceful degradation: if a batch is missing but the block is old enough, allow skipping with network-wide governance approval

## Proof of Concept

**Reproduction Steps:**

1. **Setup**: Deploy a test network with 4 validators on the same system (simulating datacenter)

2. **Create batch with transactions**:
```rust
// Validator creates and persists batch
let batch = create_test_batch_with_transactions();
quorum_store_db.save_batch(batch.into()); // Uses write_schemas_relaxed
```

3. **Generate ProofOfStore**: Have quorum sign the batch, creating a valid ProofOfStore

4. **Propose block**: Include ProofOfStore in block B, obtain quorum certificate

5. **Simulate power failure BEFORE AptosDB commit**:
```bash
# Before block B transactions are committed to AptosDB:
# Abruptly kill all validator processes
killall -9 aptos-node

# Clear OS page cache to simulate power loss
echo 3 > /proc/sys/vm/drop_caches

# Restart validators
```

6. **Observe consensus halt**: Validators attempt to execute block B, fail with `ExecutorError::CouldNotGetData`, and cannot progress

7. **Verify unrecoverability**: Confirm no validator has batch data, and `BatchRequester` exhausts retries returning error

**Expected Result**: Consensus permanently halts at block B. Network requires manual intervention (hardfork or checkpoint) to recover.

**Test Code Structure**:
```rust
#[tokio::test]
async fn test_coordinated_power_failure_consensus_halt() {
    // 1. Create batch and persist with write_schemas_relaxed
    // 2. Generate ProofOfStore with quorum signatures  
    // 3. Create block with ProofOfStore, obtain QC
    // 4. Simulate power failure: drop all in-memory state
    // 5. Attempt block execution
    // 6. Assert: execution fails with CouldNotGetData
    // 7. Assert: consensus cannot progress
}
```

## Notes

This vulnerability exists because QuorumStoreDB is treated as a temporary cache with the assumption that batch data can always be recovered from peers. However, the lack of fsync creates a scenario where ALL nodes can simultaneously lose data, violating this assumption. The fix requires recognizing that once a batch is referenced by a quorum certificate, it transitions from "temporary cache" to "consensus-critical data" that requires durability guarantees.

### Citations

**File:** storage/schemadb/src/lib.rs (L311-318)
```rust
    /// Writes without sync flag in write option.
    /// If this flag is false, and the machine crashes, some recent
    /// writes may be lost.  Note that if it is just the process that
    /// crashes (i.e., the machine does not reboot), no writes will be
    /// lost even if sync==false.
    pub fn write_schemas_relaxed(&self, batch: impl IntoRawBatch) -> DbResult<()> {
        self.write_schemas_inner(batch, &WriteOptions::default())
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L82-89)
```rust
    /// Relaxed writes instead of sync writes.
    pub fn put<S: Schema>(&self, key: &S::Key, value: &S::Value) -> Result<(), DbError> {
        // Not necessary to use a batch, but we'd like a central place to bump counters.
        let mut batch = self.db.new_native_batch();
        batch.put::<S>(key, value)?;
        self.db.write_schemas_relaxed(batch)?;
        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L107-107)
```rust
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;
```

**File:** consensus/src/quorum_store/batch_store.rs (L684-709)
```rust
                let fut = async move {
                    let batch_digest = *batch_info.digest();
                    defer!({
                        inflight_requests_clone.lock().remove(&batch_digest);
                    });
                    // TODO(ibalajiarun): Support V2 batch
                    if let Ok(mut value) = batch_store.get_batch_from_local(&batch_digest) {
                        Ok(value.take_payload().expect("Must have payload"))
                    } else {
                        // Quorum store metrics
                        counters::MISSED_BATCHES_COUNT.inc();
                        let subscriber_rx = batch_store.subscribe(*batch_info.digest());
                        let payload = requester
                            .request_batch(
                                batch_digest,
                                batch_info.expiration(),
                                responders,
                                subscriber_rx,
                            )
                            .await?;
                        batch_store.persist(vec![PersistedValue::new(
                            batch_info.into(),
                            Some(payload.clone()),
                        )]);
                        Ok(payload)
                    }
```

**File:** consensus/src/quorum_store/batch_requester.rs (L129-178)
```rust
                        } else if futures.is_empty() {
                            // end the loop when the futures are drained
                            break;
                        }
                    },
                    Some(response) = futures.next() => {
                        match response {
                            Ok(BatchResponse::Batch(batch)) => {
                                counters::RECEIVED_BATCH_RESPONSE_COUNT.inc();
                                let payload = batch.into_transactions();
                                return Ok(payload);
                            }
                            // Short-circuit if the chain has moved beyond expiration
                            Ok(BatchResponse::NotFound(ledger_info)) => {
                                counters::RECEIVED_BATCH_NOT_FOUND_COUNT.inc();
                                if ledger_info.commit_info().epoch() == epoch
                                    && ledger_info.commit_info().timestamp_usecs() > expiration
                                    && ledger_info.verify_signatures(&validator_verifier).is_ok()
                                {
                                    counters::RECEIVED_BATCH_EXPIRED_COUNT.inc();
                                    debug!("QS: batch request expired, digest:{}", digest);
                                    return Err(ExecutorError::CouldNotGetData);
                                }
                            }
                            Ok(BatchResponse::BatchV2(_)) => {
                                error!("Batch V2 response is not supported");
                            }
                            Err(e) => {
                                counters::RECEIVED_BATCH_RESPONSE_ERROR_COUNT.inc();
                                debug!("QS: batch request error, digest:{}, error:{:?}", digest, e);
                            }
                        }
                    },
                    result = &mut subscriber_rx => {
                        match result {
                            Ok(persisted_value) => {
                                counters::RECEIVED_BATCH_FROM_SUBSCRIPTION_COUNT.inc();
                                let (_, maybe_payload) = persisted_value.unpack();
                                return Ok(maybe_payload.expect("persisted value must exist"));
                            }
                            Err(err) => {
                                debug!("channel closed: {}", err);
                            }
                        };
                    },
                }
            }
            counters::RECEIVED_BATCH_REQUEST_TIMEOUT_COUNT.inc();
            debug!("QS: batch request timed out, digest:{}", digest);
            Err(ExecutorError::CouldNotGetData)
```
