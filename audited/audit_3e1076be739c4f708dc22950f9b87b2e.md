# Audit Report

## Title
Atomicity Violation in `remove_peer_metadata()` Leads to Persistent Cache Inconsistency

## Summary
The `remove_peer_metadata()` function in `network/framework/src/application/storage.rs` contains an atomicity violation where a panic during the `broadcast()` call leaves the system in an inconsistent state. The peer is removed from the authoritative data structure but remains in the cached copy, causing all read operations to return stale peer information. [1](#0-0) 

## Finding Description

The vulnerability manifests in the following execution sequence in `remove_peer_metadata()`:

1. **Line 225**: Acquire write lock on `peers_and_metadata`
2. **Line 240**: Call `entry.remove()` - peer is removed from the authoritative HashMap
3. **Lines 241-245**: Create `ConnectionNotification::LostPeer` event and call `self.broadcast(event)`
4. **Line 259**: Call `self.set_cached_peers_and_metadata()` - update cache
5. **Line 261**: Return `Ok(peer_metadata)`

The critical issue occurs when `broadcast()` panics between steps 2 and 4. The `broadcast()` function uses `aptos_infallible::Mutex` which panics if the mutex is poisoned: [2](#0-1) [3](#0-2) 

When the mutex is poisoned (due to a previous panic while holding the lock), the `lock()` call panics with `"Cannot currently handle a poisoned lock"`. This causes:

- The peer remains removed from `peers_and_metadata` (authoritative data)
- The cache (`cached_peers_and_metadata`) is **NOT** updated
- The write lock is dropped during panic unwinding
- **Result**: Persistent state inconsistency

All read operations use the cached data exclusively: [4](#0-3) 

This means disconnected peers appear as connected, affecting:

1. **Consensus operations** that use `sort_peers_by_latency()` for peer selection: [5](#0-4) 

2. **Network broadcast operations** that query peer metadata for message routing: [6](#0-5) 

3. **Peer monitoring services** that rely on accurate connection state

This breaks the **State Consistency** invariant: state transitions must be atomic and verifiable.

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria for the following reasons:

1. **Validator Node Performance Degradation**: Consensus messages are sent to disconnected peers, causing:
   - Message send failures and timeouts
   - Wasted network resources on retry attempts
   - Delayed consensus message propagation
   - Potential quorum formation delays

2. **State Inconsistency**: The cache-authoritative data mismatch creates:
   - Incorrect peer lists returned to applications
   - Conflicting views between notification-based and query-based peer discovery
   - Memory leaks as disconnected peers never get cleaned from cache

3. **Persistent Impact**: The inconsistency persists until another peer event triggers a full cache update, which in stable networks with few connection changes could be indefinite.

While this does not directly cause consensus safety violations or fund loss, it significantly impairs validator node operations and network protocol correctness, fitting the "Validator node slowdowns" and "Significant protocol violations" criteria for High severity.

## Likelihood Explanation

**Likelihood: Medium**

The vulnerability requires mutex poisoning to occur, which happens when:
1. A thread panics while holding the `subscribers` mutex
2. Subsequent calls to `remove_peer_metadata()` attempt to acquire the poisoned lock

While mutex poisoning is not common in well-tested code, it can occur due to:
- Bugs in subscriber handling code
- Resource exhaustion causing panics
- Logic errors in notification processing
- Race conditions in concurrent operations

Once triggered, the impact is **immediate and persistent** - every read operation returns stale data until a cache refresh occurs naturally.

The code pattern in `insert_connection_metadata()` is safer because it updates the cache **before** broadcasting: [7](#0-6) 

## Recommendation

**Fix**: Reorder operations to update the cache **before** calling `broadcast()`, matching the safer pattern in `insert_connection_metadata()`:

```rust
pub fn remove_peer_metadata(
    &self,
    peer_network_id: PeerNetworkId,
    connection_id: ConnectionId,
) -> Result<PeerMetadata, Error> {
    let mut peers_and_metadata = self.peers_and_metadata.write();
    let peer_metadata_for_network =
        get_peer_metadata_for_network(&peer_network_id, &mut peers_and_metadata)?;

    let peer_metadata = if let Entry::Occupied(entry) =
        peer_metadata_for_network.entry(peer_network_id.peer_id())
    {
        let active_connection_id = entry.get().connection_metadata.connection_id;
        if active_connection_id == connection_id {
            let peer_metadata = entry.remove();
            
            // UPDATE CACHE FIRST - before any operation that can panic
            self.set_cached_peers_and_metadata(peers_and_metadata.clone());
            
            // THEN broadcast (if this panics, cache is already consistent)
            let event = ConnectionNotification::LostPeer(
                peer_metadata.connection_metadata.clone(),
                peer_network_id.network_id(),
            );
            self.broadcast(event);
            
            peer_metadata
        } else {
            return Err(Error::UnexpectedError(format!(
                "The peer connection id did not match! Given: {:?}, found: {:?}.",
                connection_id, active_connection_id
            )));
        }
    } else {
        return Err(missing_peer_metadata_error(&peer_network_id));
    };

    Ok(peer_metadata)
}
```

This ensures atomicity: either both the authoritative data and cache are updated, or neither is (if the removal fails).

**Alternative**: Wrap the broadcast call in a panic-safe block or use `std::panic::catch_unwind()`, though the reordering approach is cleaner and more idiomatic.

## Proof of Concept

```rust
#[cfg(test)]
mod test_atomicity_violation {
    use super::*;
    use std::sync::Arc;
    use std::thread;
    use std::panic;

    #[test]
    fn test_cache_inconsistency_on_broadcast_panic() {
        // Create PeersAndMetadata
        let peers_and_metadata = PeersAndMetadata::new(&[NetworkId::Validator]);
        
        // Add a peer
        let peer_id = PeerId::random();
        let peer_network_id = PeerNetworkId::new(NetworkId::Validator, peer_id);
        let connection_metadata = ConnectionMetadata::mock(peer_id);
        let connection_id = connection_metadata.connection_id;
        
        peers_and_metadata
            .insert_connection_metadata(peer_network_id, connection_metadata.clone())
            .unwrap();
        
        // Verify peer is in both authoritative and cached data
        assert!(peers_and_metadata
            .get_metadata_for_peer(peer_network_id)
            .is_ok());
        
        // Poison the subscribers mutex by causing a panic while holding it
        let peers_clone = Arc::new(peers_and_metadata);
        let peers_for_poison = peers_clone.clone();
        
        // Spawn thread that will panic while holding subscribers lock
        let handle = thread::spawn(move || {
            let mut listeners = peers_for_poison.subscribers.lock();
            panic!("Intentional panic to poison mutex");
        });
        
        // Wait for thread to panic (ignore the panic)
        let _ = handle.join();
        
        // Now try to remove the peer - this should panic due to poisoned mutex
        // but the peer will be removed from authoritative data before the panic
        let result = panic::catch_unwind(panic::AssertUnwindSafe(|| {
            peers_clone
                .remove_peer_metadata(peer_network_id, connection_id)
        }));
        
        // The removal panicked (due to poisoned mutex)
        assert!(result.is_err());
        
        // BUG: The cache still contains the peer (stale data)
        // but the authoritative data doesn't (was removed before panic)
        // This proves the atomicity violation
        let cached_peers = peers_clone.get_metadata_for_peer(peer_network_id);
        
        // EXPECTED: Should return error (peer removed)
        // ACTUAL: Returns Ok (peer still in cache)
        assert!(cached_peers.is_ok(), "Cache inconsistency detected!");
    }
}
```

## Notes

This atomicity violation represents a subtle but significant bug in the network layer's peer management subsystem. While not directly exploitable by external attackers, it can manifest under error conditions and cause persistent operational issues affecting consensus message delivery and validator performance. The fix is straightforward and aligns with the existing safe pattern used in `insert_connection_metadata()`.

### Citations

**File:** network/framework/src/application/storage.rs (L92-125)
```rust
    pub fn get_all_peers(&self) -> Vec<PeerNetworkId> {
        // Get the cached peers and metadata
        let cached_peers_and_metadata = self.cached_peers_and_metadata.load();

        // Collect all peers
        let mut all_peers = Vec::new();
        for (network_id, peers_and_metadata) in cached_peers_and_metadata.iter() {
            for (peer_id, _) in peers_and_metadata.iter() {
                let peer_network_id = PeerNetworkId::new(*network_id, *peer_id);
                all_peers.push(peer_network_id);
            }
        }
        all_peers
    }

    /// Returns metadata for all peers currently connected to the node
    pub fn get_connected_peers_and_metadata(
        &self,
    ) -> Result<HashMap<PeerNetworkId, PeerMetadata>, Error> {
        // Get the cached peers and metadata
        let cached_peers_and_metadata = self.cached_peers_and_metadata.load();

        // Collect all connected peers
        let mut connected_peers_and_metadata = HashMap::new();
        for (network_id, peers_and_metadata) in cached_peers_and_metadata.iter() {
            for (peer_id, peer_metadata) in peers_and_metadata.iter() {
                if peer_metadata.is_connected() {
                    let peer_network_id = PeerNetworkId::new(*network_id, *peer_id);
                    connected_peers_and_metadata.insert(peer_network_id, peer_metadata.clone());
                }
            }
        }
        Ok(connected_peers_and_metadata)
    }
```

**File:** network/framework/src/application/storage.rs (L186-214)
```rust
    pub fn insert_connection_metadata(
        &self,
        peer_network_id: PeerNetworkId,
        connection_metadata: ConnectionMetadata,
    ) -> Result<(), Error> {
        // Grab the write lock for the peer metadata
        let mut peers_and_metadata = self.peers_and_metadata.write();

        // Fetch the peer metadata for the given network
        let peer_metadata_for_network =
            get_peer_metadata_for_network(&peer_network_id, &mut peers_and_metadata)?;

        // Update the metadata for the peer or insert a new entry
        peer_metadata_for_network
            .entry(peer_network_id.peer_id())
            .and_modify(|peer_metadata| {
                peer_metadata.connection_metadata = connection_metadata.clone()
            })
            .or_insert_with(|| PeerMetadata::new(connection_metadata.clone()));

        // Update the cached peers and metadata
        self.set_cached_peers_and_metadata(peers_and_metadata.clone());

        let event =
            ConnectionNotification::NewPeer(connection_metadata, peer_network_id.network_id());
        self.broadcast(event);

        Ok(())
    }
```

**File:** network/framework/src/application/storage.rs (L219-262)
```rust
    pub fn remove_peer_metadata(
        &self,
        peer_network_id: PeerNetworkId,
        connection_id: ConnectionId,
    ) -> Result<PeerMetadata, Error> {
        // Grab the write lock for the peer metadata
        let mut peers_and_metadata = self.peers_and_metadata.write();

        // Fetch the peer metadata for the given network
        let peer_metadata_for_network =
            get_peer_metadata_for_network(&peer_network_id, &mut peers_and_metadata)?;

        // Remove the peer metadata for the peer
        let peer_metadata = if let Entry::Occupied(entry) =
            peer_metadata_for_network.entry(peer_network_id.peer_id())
        {
            // Don't remove the peer if the connection doesn't match!
            // For now, remove the peer entirely, we could in the future
            // have multiple connections for a peer
            let active_connection_id = entry.get().connection_metadata.connection_id;
            if active_connection_id == connection_id {
                let peer_metadata = entry.remove();
                let event = ConnectionNotification::LostPeer(
                    peer_metadata.connection_metadata.clone(),
                    peer_network_id.network_id(),
                );
                self.broadcast(event);
                peer_metadata
            } else {
                return Err(Error::UnexpectedError(format!(
                    "The peer connection id did not match! Given: {:?}, found: {:?}.",
                    connection_id, active_connection_id
                )));
            }
        } else {
            // Unable to find the peer metadata for the given peer
            return Err(missing_peer_metadata_error(&peer_network_id));
        };

        // Update the cached peers and metadata
        self.set_cached_peers_and_metadata(peers_and_metadata.clone());

        Ok(peer_metadata)
    }
```

**File:** network/framework/src/application/storage.rs (L371-395)
```rust
    fn broadcast(&self, event: ConnectionNotification) {
        let mut listeners = self.subscribers.lock();
        let mut to_del = vec![];
        for i in 0..listeners.len() {
            let dest = listeners.get_mut(i).unwrap();
            if let Err(err) = dest.try_send(event.clone()) {
                match err {
                    TrySendError::Full(_) => {
                        // Tried to send to an app, but the app isn't handling its messages fast enough.
                        // Drop message. Maybe increment a metrics counter?
                        sample!(
                            SampleRate::Duration(Duration::from_secs(1)),
                            warn!("PeersAndMetadata.broadcast() failed, some app is slow"),
                        );
                    },
                    TrySendError::Closed(_) => {
                        to_del.push(i);
                    },
                }
            }
        }
        for evict in to_del.into_iter() {
            listeners.swap_remove(evict);
        }
    }
```

**File:** network/framework/src/application/storage.rs (L445-470)
```rust
    pub fn sort_peers_by_latency(&self, network_id: NetworkId, peers: &mut [PeerId]) {
        let _timer = counters::OP_MEASURE
            .with_label_values(&["sort_peers"])
            .start_timer();

        let cached_peers_and_metadata = self.cached_peers_and_metadata.load();

        peers.sort_unstable_by(|peer_network_a, peer_network_b| {
            let get_latency = |&network_id, peer| -> f64 {
                cached_peers_and_metadata
                    .get(&network_id)
                    .and_then(|peers| peers.get(peer))
                    .and_then(|peer| {
                        peer.get_peer_monitoring_metadata()
                            .average_ping_latency_secs
                    })
                    .unwrap_or_default()
            };

            let a_latency = get_latency(&network_id, peer_network_a);
            let b_latency = get_latency(&network_id, peer_network_b);
            b_latency
                .partial_cmp(&a_latency)
                .expect("latency is never NaN")
        })
    }
```

**File:** crates/aptos-infallible/src/mutex.rs (L19-23)
```rust
    pub fn lock(&self) -> MutexGuard<'_, T> {
        self.0
            .lock()
            .expect("Cannot currently handle a poisoned lock")
    }
```

**File:** consensus/src/network.rs (L396-408)
```rust
        self.sort_peers_by_latency(&mut other_validators);

        counters::CONSENSUS_SENT_MSGS
            .with_label_values(&[msg.name()])
            .inc_by(other_validators.len() as u64);
        // Broadcast message over direct-send to all other validators.
        if let Err(err) = self
            .consensus_network_client
            .send_to_many(other_validators, msg)
        {
            warn!(error = ?err, "Error broadcasting message");
        }
    }
```
