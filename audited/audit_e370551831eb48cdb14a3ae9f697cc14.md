# Audit Report

## Title
Indexer-gRPC File Store Lacks Cryptographic Integrity Verification Enabling Post-Compromise Data Injection

## Summary
The indexer-gRPC file store system deserializes transaction data from local filesystem or cloud storage without any cryptographic integrity verification (checksums, signatures, or MACs). Files are treated as trusted input, allowing an attacker with filesystem or storage bucket access to inject malicious compressed data, triggering denial-of-service through decompression bombs or serving fabricated transaction data to downstream clients. [1](#0-0) 

## Finding Description
The `FileEntry::into_transactions_in_storage()` function deserializes transaction data read from storage without validating its authenticity or integrity. The system supports two storage formats:

1. **Lz4CompressedProto**: LZ4 decompression followed by protobuf deserialization
2. **JsonBase64UncompressedProto**: JSON parsing, base64 decoding, and protobuf deserialization

All deserialization operations use `.expect()` causing panics on invalid input, with no bounds checking on decompressed data size.

**Attack Path**:
1. Attacker gains write access to file store (local filesystem or GCS bucket) through separate compromise
2. Attacker identifies target transaction file via `FileEntry::build_key()` pattern
3. Attacker replaces legitimate file with malicious payload:
   - **LZ4 Decompression Bomb**: Small compressed file expanding to gigabytes
   - **Malformed Protobuf**: Invalid data causing panic in `Transaction::decode()`
   - **Fabricated Transactions**: Valid protobuf with fake transaction data [2](#0-1) 

The local file operator reads files directly from disk with `tokio::fs::read()` without verification. [3](#0-2) 

The GCS operator downloads files without signature validation.

When the data service fetches evicted transactions from file store, malicious data propagates to clients: [4](#0-3) 

## Impact Explanation
This issue achieves **High Severity** under Aptos Bug Bounty criteria:

- **API Crashes**: Malformed protobuf or JSON causes indexer service to panic, crashing the gRPC streaming endpoint
- **Validator Node Slowdowns**: LZ4 decompression bombs consume excessive memory/CPU when indexer runs on validator nodes
- **Data Integrity Violation**: Fabricated transaction data served to wallets, explorers, and indexer processors creates incorrect chain views

The indexer-gRPC service runs as part of node infrastructure and serves critical data to the ecosystem: [5](#0-4) 

However, this does NOT affect consensus safety as the indexer reads committed data after consensus finalization.

## Likelihood Explanation
**Likelihood: MEDIUM-LOW**

This vulnerability requires the attacker to first gain write access to:
- Local filesystem where indexer files are stored (`LocalFileStoreOperator`), OR
- GCS bucket containing transaction files (`GcsFileStoreOperator`)

Such access typically requires:
- Compromised node operator credentials
- Container escape vulnerability
- Insider threat (malicious operator)
- Misconfigured cloud IAM permissions

Once filesystem access is obtained, exploitation is trivial as no integrity checks exist.

**CRITICAL LIMITATION**: This vulnerability is **NOT exploitable by unprivileged external attackers** without prior system compromise. There is no public API accepting file uploads to the indexer storage.

## Recommendation
Implement cryptographic integrity verification for all stored files:

```rust
// Add to FileEntry
pub fn into_transactions_in_storage_verified(
    self,
    expected_hash: [u8; 32],
) -> anyhow::Result<TransactionsInStorage> {
    let bytes = match &self {
        FileEntry::Lz4CompressionProto(bytes) => bytes,
        FileEntry::JsonBase64UncompressedProto(bytes) => bytes,
    };
    
    // Verify SHA256 hash before deserialization
    let actual_hash = sha2::Sha256::digest(bytes);
    anyhow::ensure!(
        actual_hash.as_slice() == expected_hash,
        "File integrity check failed: hash mismatch"
    );
    
    // Add decompression size limits
    const MAX_DECOMPRESSED_SIZE: usize = 100 * 1024 * 1024; // 100MB
    
    match self {
        FileEntry::Lz4CompressionProto(bytes) => {
            let mut decompressor = Decoder::new(&bytes[..])?;
            let mut decompressed = Vec::new();
            let mut limited_reader = decompressor.take(MAX_DECOMPRESSED_SIZE as u64);
            limited_reader.read_to_end(&mut decompressed)?;
            
            TransactionsInStorage::decode(decompressed.as_slice())?
        }
        // ... handle other formats with validation
    }
}
```

Store file hashes in metadata with digital signatures from the uploader node.

## Proof of Concept

```rust
// Create malicious LZ4 decompression bomb
use lz4::EncoderBuilder;
use std::io::Write;

fn create_compression_bomb() -> Vec<u8> {
    let bomb_data = vec![0u8; 1_000_000_000]; // 1GB of zeros
    let mut encoder = EncoderBuilder::new()
        .level(9)
        .build(Vec::new())
        .unwrap();
    encoder.write_all(&bomb_data).unwrap();
    encoder.finish().0 // Returns small compressed file (~1MB)
}

// Replace legitimate file on filesystem
fn exploit_file_store() {
    let malicious_file = create_compression_bomb();
    let target_path = "/data/indexer/compressed_files/lz4/3d1bff1ba654ca5fdb6ac1370533d876_0.bin";
    std::fs::write(target_path, malicious_file).unwrap();
    
    // When indexer reads this file, decompression consumes excessive memory
    // causing OOM kill or node slowdown
}
```

---

**Notes**:
This vulnerability represents a **defense-in-depth** weakness rather than a direct exploit path. While the lack of integrity verification is a legitimate security concern, it fails the critical validation criterion: "Exploitable by unprivileged attacker (no validator insider access required)."

The security question explicitly asks about "filesystem-level attacks," which inherently requires elevated access to the storage backend. This is an **insider threat** or **post-compromise** scenario, not an attack vector available to external unprivileged users.

For production deployment, implementing cryptographic file integrity verification would provide important defense-in-depth against supply chain attacks, compromised operators, or container escape vulnerabilities.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L262-292)
```rust
    pub fn into_transactions_in_storage(self) -> TransactionsInStorage {
        match self {
            FileEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                TransactionsInStorage::decode(decompressed.as_slice())
                    .expect("proto deserialization failed.")
            },
            FileEntry::JsonBase64UncompressedProto(bytes) => {
                let file: TransactionsLegacyFile =
                    serde_json::from_slice(bytes.as_slice()).expect("json deserialization failed.");
                let transactions = file
                    .transactions_in_base64
                    .into_iter()
                    .map(|base64| {
                        let bytes: Vec<u8> =
                            base64::decode(base64).expect("base64 decoding failed.");
                        Transaction::decode(bytes.as_slice())
                            .expect("proto deserialization failed.")
                    })
                    .collect::<Vec<Transaction>>();
                TransactionsInStorage {
                    starting_version: Some(file.starting_version),
                    transactions,
                }
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L58-74)
```rust
    async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
        let file_entry_key = FileEntry::build_key(version, self.storage_format).to_string();
        let file_path = self.path.join(file_entry_key);
        match tokio::fs::read(file_path).await {
            Ok(file) => Ok(file),
            Err(err) => {
                if err.kind() == std::io::ErrorKind::NotFound {
                    anyhow::bail!("[Indexer File] Transactions file not found. Gap might happen between cache and file store. {}", err)
                } else {
                    anyhow::bail!(
                        "[Indexer File] Error happens when transaction file. {}",
                        err
                    );
                }
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/gcs.rs (L103-124)
```rust
    async fn get_raw_file(&self, version: u64) -> anyhow::Result<Vec<u8>> {
        let file_entry_key_path = self.get_file_entry_key_path(version);
        match Object::download(&self.bucket_name, file_entry_key_path.as_str()).await {
            Ok(file) => Ok(file),
            Err(cloud_storage::Error::Other(err)) => {
                if err.contains("No such object: ") {
                    anyhow::bail!("[Indexer File] Transactions file not found. Gap might happen between cache and file store. {}", err)
                } else {
                    anyhow::bail!(
                        "[Indexer File] Error happens when downloading transaction file. {}",
                        err
                    );
                }
            },
            Err(err) => {
                anyhow::bail!(
                    "[Indexer File] Error happens when transaction file. {}",
                    err
                );
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L779-820)
```rust
async fn data_fetch_from_filestore(
    starting_version: u64,
    file_store_operator: Arc<Box<dyn FileStoreOperator>>,
    request_metadata: Arc<IndexerGrpcRequestMetadata>,
) -> anyhow::Result<Vec<Transaction>> {
    // Data is evicted from the cache. Fetch from file store.
    let (transactions, io_duration, decoding_duration) = file_store_operator
        .get_transactions_with_durations(starting_version, NUM_DATA_FETCH_RETRIES)
        .await?;
    let size_in_bytes = transactions
        .iter()
        .map(|transaction| transaction.encoded_len())
        .sum::<usize>();
    let num_of_transactions = transactions.len();
    let start_version_timestamp = transactions.first().unwrap().timestamp.as_ref();
    let end_version_timestamp = transactions.last().unwrap().timestamp.as_ref();
    log_grpc_step(
        SERVICE_TYPE,
        IndexerGrpcStep::DataServiceDataFetchedFilestore,
        Some(starting_version as i64),
        Some(starting_version as i64 + num_of_transactions as i64 - 1),
        start_version_timestamp,
        end_version_timestamp,
        Some(io_duration),
        Some(size_in_bytes),
        Some(num_of_transactions as i64),
        Some(&request_metadata),
    );
    log_grpc_step(
        SERVICE_TYPE,
        IndexerGrpcStep::DataServiceTxnsDecoded,
        Some(starting_version as i64),
        Some(starting_version as i64 + num_of_transactions as i64 - 1),
        start_version_timestamp,
        end_version_timestamp,
        Some(decoding_duration),
        Some(size_in_bytes),
        Some(num_of_transactions as i64),
        Some(&request_metadata),
    );
    Ok(transactions)
}
```

**File:** aptos-node/src/services.rs (L113-121)
```rust
    // Creates the indexer grpc runtime
    let indexer_grpc = bootstrap_indexer_grpc(
        node_config,
        chain_id,
        db_rw.reader.clone(),
        mempool_client_sender.clone(),
        indexer_reader,
        indexer_grpc_port_tx,
    );
```
