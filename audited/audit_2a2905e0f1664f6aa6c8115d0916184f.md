# Audit Report

## Title
Consensus Observer Liveness Failure Due to Incomplete Execution Pool Implementation

## Summary
The consensus observer implementation contains an incomplete feature for execution pool windows that causes complete liveness failure for observer nodes if the execution pool configuration is enabled via on-chain governance. Observer nodes will reject all `OrderedBlock` messages when execution pool is enabled, while publishers only send `OrderedBlock` messages (never `OrderedBlockWithWindow`), resulting in observers being unable to process any consensus updates.

## Finding Description

The consensus observer system has two message types for ordered blocks:
- `OrderedBlock` - Standard ordered block messages
- `OrderedBlockWithWindow` - Ordered blocks with execution pool window information [1](#0-0) 

The execution pool feature is controlled by an on-chain configuration parameter `window_size` that observers read during epoch start: [2](#0-1) 

**The Critical Flaw:**

When execution pool is **enabled** (window_size is Some(x)), the observer's message handler explicitly REJECTS `OrderedBlock` messages: [3](#0-2) 

However, consensus publishers ONLY send `OrderedBlock` messages, as shown in the buffer manager: [4](#0-3) 

There is NO code in the entire codebase that creates or sends `OrderedBlockWithWindow` messages from publishers. Even if such messages were sent, the observer would accept them but immediately drop them due to an unimplemented TODO: [5](#0-4) 

**Attack Scenario (Mixed State):**

If governance enables the execution pool feature or during a transition period where different validator versions coexist:

1. Observer reads `window_size = Some(x)` from on-chain config
2. Observer sets `execution_pool_window_size = Some(x)` 
3. Publishers (all versions) send `OrderedBlock` messages
4. Observer rejects ALL `OrderedBlock` messages (line 646-656)
5. Observer receives no valid consensus updates
6. After 10 seconds without progress, observer enters fallback mode and uses expensive state sync
7. After fallback completes, cycle repeats - creating a perpetual fallback loop

This violates the **Consensus Liveness** invariant - observers cannot maintain consensus state through normal consensus observer mechanisms.

## Impact Explanation

This qualifies as **HIGH severity** per Aptos bug bounty criteria:

1. **Validator node slowdowns**: Observer nodes (including validator fullnodes) cannot use the efficient consensus observer mechanism and must repeatedly fall back to state sync, which is significantly slower and more resource-intensive. [6](#0-5) 

2. **Significant protocol violations**: The consensus observer protocol completely breaks when this feature flag is enabled. All observer nodes in the network are affected simultaneously if governance enables the feature.

3. **Availability degradation**: While not total liveness failure (state sync provides fallback), the repeated fallback cycles every 10 minutes create severe performance degradation. [7](#0-6) 

The default fallback threshold is 10 seconds with fallback duration of 10 minutes, causing continuous cycling between failed consensus observer mode and state sync.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability will trigger automatically if:
1. On-chain governance sets `window_size` to a non-None value in `OnChainConsensusConfig`
2. The execution pool feature is enabled for testing or deployment

The default value is disabled (`DEFAULT_WINDOW_SIZE: Option<u64> = None`), but governance can enable it at any time: [8](#0-7) 

No attacker action is required - the bug triggers automatically when the feature flag is enabled. The "mixed state" scenario would occur during any attempted rollout of this feature, where:
- Some validators might upgrade code expecting to send `OrderedBlockWithWindow` 
- Others remain on current code sending `OrderedBlock`
- Observers cannot handle either type correctly

## Recommendation

**Immediate Fix:** Complete the implementation of `OrderedBlockWithWindow` message handling before enabling the execution pool feature.

**Required Changes:**

1. **Implement `process_ordered_block_with_window_message` handler** - Remove the TODO and properly process these messages similar to how `process_ordered_block_message` works, but with window validation.

2. **Add publisher logic to send correct message type** - Modify buffer_manager.rs to check execution pool configuration and send `OrderedBlockWithWindow` when enabled:

```rust
// In buffer_manager.rs process_ordered_blocks()
if let Some(consensus_publisher) = &self.consensus_publisher {
    let message = if let Some(window_size) = self.get_execution_pool_window_size() {
        // Create execution pool window
        let window = create_execution_pool_window(/* params */);
        ConsensusObserverMessage::new_ordered_block_with_window_message(
            ordered_blocks.clone(),
            ordered_proof.clone(),
            window
        )
    } else {
        ConsensusObserverMessage::new_ordered_block_message(
            ordered_blocks.clone(),
            ordered_proof.clone(),
        )
    };
    consensus_publisher.publish_message(message);
}
```

3. **Make observers tolerant to mixed messages** - During transition periods, observers should accept both message types and process whichever arrives first for a given block, rather than strictly rejecting based on configuration.

4. **Add feature flag guard** - Prevent enabling execution pool via governance until implementation is complete.

## Proof of Concept

**Reproduction Steps:**

1. Deploy a consensus observer node with default configuration
2. Use governance to modify `OnChainConsensusConfig` to set `window_size = Some(1)` 
3. Wait for next epoch to start - observer reads new config
4. Observer will now reject all incoming `OrderedBlock` messages from publishers
5. Observe logs showing repeated invalid message counters and fallback mode entries
6. Monitor metrics showing observer continuously cycling between fallback and normal mode
7. Verify DB sync progress stalls except during state sync fallback periods

**Verification:**
```rust
// In a test harness, simulate the scenario:
let mut observer = create_consensus_observer();
observer.observer_epoch_state.execution_pool_window_size = Some(1); // Enable execution pool

// Publishers send OrderedBlock (current implementation)
let ordered_block_msg = ConsensusObserverDirectSend::OrderedBlock(create_test_ordered_block());

// Observer will reject this message
observer.process_network_message(network_msg).await;

// Verify message was rejected, not processed
assert!(observer.observer_block_data.lock().get_last_ordered_block().round() == initial_round);
```

**Expected behavior:** Observer rejects all messages and enters perpetual fallback loop.
**Actual secure behavior:** Observer should process messages regardless of execution pool config, or publishers should send correct message type.

## Notes

This vulnerability demonstrates a dangerous incomplete feature implementation where:
- The protocol definition includes both message types
- Configuration mechanisms exist to enable the feature
- Observer validation logic enforces the feature flag
- But publisher implementation never generates the new message type
- And observer handling for the new message type is unimplemented (TODO)

The mixed state scenario mentioned in the security question is particularly concerning during any attempted deployment of this feature, as observers cannot reliably sync from ANY publisher (new or old versions) when the feature flag is enabled.

### Citations

**File:** consensus/src/consensus_observer/network/observer_message.rs (L129-135)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub enum ConsensusObserverDirectSend {
    OrderedBlock(OrderedBlock),
    CommitDecision(CommitDecision),
    BlockPayload(BlockPayload),
    OrderedBlockWithWindow(OrderedBlockWithWindow),
}
```

**File:** consensus/src/consensus_observer/observer/epoch_state.rs (L99-108)
```rust
        // Update the local epoch state and quorum store config
        self.epoch_state = Some(epoch_state.clone());
        self.execution_pool_window_size = consensus_config.window_size();
        self.quorum_store_enabled = consensus_config.quorum_store_enabled();
        info!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "New epoch started: {:?}. Execution pool window: {:?}. Quorum store enabled: {:?}",
                epoch_state.epoch, self.execution_pool_window_size, self.quorum_store_enabled,
            ))
        );
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L645-656)
```rust
        // If execution pool is enabled, ignore the message
        if self.get_execution_pool_window_size().is_some() {
            // Log the failure and update the invalid message counter
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Received ordered block message from peer: {:?}, but execution pool is enabled! Ignoring: {:?}",
                    peer_network_id, ordered_block.proof_block_info()
                ))
            );
            increment_invalid_message_counter(&peer_network_id, metrics::ORDERED_BLOCK_LABEL);
            return;
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L895-896)
```rust
        // TODO: process the ordered block with window message (instead of just dropping it!)
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L397-406)
```rust
        let request = self.create_new_request(ExecutionRequest {
            ordered_blocks: ordered_blocks.clone(),
        });
        if let Some(consensus_publisher) = &self.consensus_publisher {
            let message = ConsensusObserverMessage::new_ordered_block_message(
                ordered_blocks.clone(),
                ordered_proof.clone(),
            );
            consensus_publisher.publish_message(message);
        }
```

**File:** consensus/src/consensus_observer/observer/fallback_manager.rs (L100-108)
```rust
            let fallback_threshold = Duration::from_millis(
                self.consensus_observer_config
                    .observer_fallback_progress_threshold_ms,
            );
            if duration_since_highest_seen > fallback_threshold {
                Err(Error::ObserverProgressStopped(format!(
                    "Consensus observer is not making progress! Highest synced version: {}, elapsed: {:?}",
                    highest_synced_version, duration_since_highest_seen
                )))
```

**File:** config/src/config/consensus_observer_config.rs (L79-82)
```rust
            observer_fallback_duration_ms: 600_000, // 10 minutes
            observer_fallback_startup_period_ms: 60_000, // 60 seconds
            observer_fallback_progress_threshold_ms: 10_000, // 10 seconds
            observer_fallback_sync_lag_threshold_ms: 15_000, // 15 seconds
```

**File:** types/src/on_chain_config/consensus_config.rs (L10-13)
```rust
/// Default Window Size for Execution Pool.
/// This describes the number of blocks in the Execution Pool Window
pub const DEFAULT_WINDOW_SIZE: Option<u64> = None;
pub const DEFAULT_ENABLED_WINDOW_SIZE: Option<u64> = Some(1);
```
