# Audit Report

## Title
Network Partition Creates Asymmetric Batch State in Quorum Store Leading to Liveness Degradation

## Summary
The `BatchCoordinator` in the quorum store consensus mechanism uses a fire-and-forget network send when returning signed batch signatures to batch authors. If this network delivery fails, recipient validators persist and process batches locally while the author may fail to collect sufficient signatures to form a ProofOfStore, creating asymmetric state and potential liveness issues.

## Finding Description

The vulnerability exists in the batch signature collection flow where validators sign batches from other validators but network failures are silently ignored.

**Attack Flow:**

1. **Batch Creation & Broadcast**: Validator A creates batches and broadcasts them to all validators [1](#0-0) 

2. **Batch Reception & Signing**: Validator B receives batches from Validator A and processes them in `BatchCoordinator::handle_batches_msg` [2](#0-1) 

3. **Critical Vulnerability - Fire-and-Forget Send**: The `persist_and_send_digests` function persists batches, signs them, and attempts to send signatures back to the author, but **does not verify send success** [3](#0-2) 

4. **Unconditional Local State Update**: Regardless of whether the network send succeeded, batches are sent to the local proof_manager [4](#0-3) 

5. **Network Layer Swallows Errors**: The `NetworkSender::send` method logs errors but does not propagate them to the caller [5](#0-4) 

6. **Asymmetric State Creation**: Batches are inserted into Validator B's `BatchProofQueue` with `proof: None` [6](#0-5) 

**Invariant Violation:**

This breaks the **State Consistency** invariant - validators should have consistent views of which batches have been signed and are progressing toward consensus inclusion. The design assumption is that if a validator signs a batch, that signature should reliably reach the author.

## Impact Explanation

**High Severity** - Significant protocol violations leading to liveness degradation:

1. **Liveness Degradation**: If sufficient validators (but < 2f+1) experience network send failures, the batch author cannot form a ProofOfStore. Transactions remain unprocessed until batches expire. This violates the liveness guarantee that transactions will eventually be included.

2. **Resource Consumption**: Validators accumulate batches without proofs in memory until expiration, consuming resources for batches that will never reach consensus [7](#0-6) 

3. **Network Partition Amplification**: Transient network issues create persistent state divergence lasting until batch expiration (configured timeout period), amplifying the impact of temporary failures.

4. **Exploitability**: A Byzantine validator controlling network routing (e.g., via BGP manipulation or network-layer attacks within scope) could selectively drop signed_batch_info messages while appearing to participate honestly, degrading system liveness without being detected.

This meets **High Severity** criteria: "Validator node slowdowns" and "Significant protocol violations" per the Aptos bug bounty program.

## Likelihood Explanation

**High Likelihood** in production environments:

1. **Network Unreliability**: Internet-scale P2P networks routinely experience packet loss, routing failures, and transient partitions. The absence of retry logic makes every network hiccup a potential state divergence event.

2. **No Detection Mechanism**: The system logs warnings but has no alerting or recovery mechanism [8](#0-7) 

3. **Byzantine Attack Surface**: Malicious validators can intentionally trigger this by selectively dropping messages while maintaining enough honest behavior to avoid detection.

4. **Production Evidence**: The existence of garbage collection logic specifically for "expired_batch_without_proof" suggests this scenario occurs in practice [9](#0-8) 

## Recommendation

Implement reliable delivery with acknowledgments and retries:

```rust
async fn persist_and_send_digests(
    &self,
    persist_requests: Vec<PersistedValue<BatchInfoExt>>,
    approx_created_ts_usecs: u64,
) {
    // ... existing code ...
    
    let signed_batch_infos = batch_store.persist(persist_requests);
    if !signed_batch_infos.is_empty() {
        // FIXED: Check send result and retry on failure
        let max_retries = 3;
        let mut retry_count = 0;
        
        loop {
            match network_sender
                .send_signed_batch_info_msg_v2(signed_batch_infos.clone(), vec![peer_id])
                .await 
            {
                Ok(_) => {
                    // Only send to proof_manager after successful network send
                    let _ = sender_to_proof_manager
                        .send(ProofManagerCommand::ReceiveBatches(batches))
                        .await;
                    break;
                },
                Err(e) if retry_count < max_retries => {
                    warn!("Failed to send signed_batch_info to {}, retrying: {:?}", peer_id, e);
                    retry_count += 1;
                    tokio::time::sleep(Duration::from_millis(100 * retry_count)).await;
                },
                Err(e) => {
                    error!("Failed to send signed_batch_info to {} after {} retries: {:?}", 
                           peer_id, max_retries, e);
                    // Don't add to local proof_manager if we can't notify the author
                    break;
                }
            }
        }
    }
}
```

Additionally, modify `NetworkSender::send` to return `Result<(), NetworkError>` instead of being fire-and-forget.

## Proof of Concept

```rust
#[tokio::test]
async fn test_network_failure_creates_asymmetric_state() {
    // Setup: Create two validators A and B
    let (validator_a, validator_b) = setup_test_validators().await;
    
    // Step 1: Validator A creates and broadcasts batches
    let batch = validator_a.create_test_batch(vec![create_test_txn()]);
    validator_a.broadcast_batch(batch.clone()).await;
    
    // Step 2: Inject network failure for signed_batch_info send
    // Using fail_point at consensus::send::signed_batch_info
    fail::cfg("consensus::send::signed_batch_info", "return").unwrap();
    
    // Step 3: Validator B receives batch
    validator_b.handle_batch_message(batch.clone()).await;
    
    // Step 4: Verify asymmetric state
    
    // Validator A should NOT have signature from B
    let proof_state_a = validator_a.get_proof_coordinator_state(&batch.batch_info());
    assert!(proof_state_a.is_none() || !proof_state_a.unwrap().has_signature_from(validator_b.peer_id()));
    
    // Validator B should have batch in proof_manager without proof
    let batch_queue_b = validator_b.get_batch_proof_queue_state();
    let batch_item = batch_queue_b.get_batch(&batch.batch_info());
    assert!(batch_item.is_some());
    assert!(batch_item.unwrap().proof.is_none()); // ASYMMETRIC STATE
    assert!(batch_item.unwrap().txn_summaries.is_some());
    
    // Step 5: Validator A cannot form ProofOfStore (assuming 2f+1 requires B's signature)
    tokio::time::sleep(Duration::from_secs(1)).await; // Wait for proof formation timeout
    
    let proof_a = validator_a.get_formed_proof(&batch.batch_info());
    assert!(proof_a.is_none()); // LIVENESS FAILURE
    
    // Step 6: Batch eventually expires on B, consuming resources until then
    assert_eq!(batch_queue_b.num_batches_without_proof(), 1); // RESOURCE CONSUMPTION
    
    fail::cfg("consensus::send::signed_batch_info", "off").unwrap();
}
```

**Notes:**

The vulnerability stems from the architectural decision to use fire-and-forget messaging for critical consensus coordination. While BFT systems tolerate Byzantine failures, the lack of delivery confirmation for signature messages creates unnecessary fragility. The fix requires making network sends synchronous with retry logic, or implementing an acknowledgment protocol where batch authors explicitly request missing signatures from non-responding validators.

### Citations

**File:** consensus/src/quorum_store/batch_generator.rs (L494-501)
```rust
                            if self.config.enable_batch_v2 {
                                network_sender.broadcast_batch_msg_v2(batches).await;
                            } else {
                                let batches = batches.into_iter().map(|batch| {
                                    batch.try_into().expect("Cannot send V2 batch with flag disabled")
                                }).collect();
                                network_sender.broadcast_batch_msg(batches).await;
                            }
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L102-110)
```rust
            if persist_requests[0].batch_info().is_v2() {
                let signed_batch_infos = batch_store.persist(persist_requests);
                if !signed_batch_infos.is_empty() {
                    if approx_created_ts_usecs > 0 {
                        observe_batch(approx_created_ts_usecs, peer_id, BatchStage::SIGNED);
                    }
                    network_sender
                        .send_signed_batch_info_msg_v2(signed_batch_infos, vec![peer_id])
                        .await;
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L131-133)
```rust
            let _ = sender_to_proof_manager
                .send(ProofManagerCommand::ReceiveBatches(batches))
                .await;
```

**File:** consensus/src/quorum_store/batch_coordinator.rs (L173-245)
```rust
    pub(crate) async fn handle_batches_msg(
        &mut self,
        author: PeerId,
        batches: Vec<Batch<BatchInfoExt>>,
    ) {
        if let Err(e) = self.ensure_max_limits(&batches) {
            error!("Batch from {}: {}", author, e);
            counters::RECEIVED_BATCH_MAX_LIMIT_FAILED.inc();
            return;
        }

        let Some(batch) = batches.first() else {
            error!("Empty batch received from {}", author.short_str().as_str());
            return;
        };

        // Filter the transactions in the batches. If any transaction is rejected,
        // the message will be dropped, and all batches will be rejected.
        if self.transaction_filter_config.is_enabled() {
            let transaction_filter = &self.transaction_filter_config.batch_transaction_filter();
            for batch in batches.iter() {
                for transaction in batch.txns() {
                    if !transaction_filter.allows_transaction(
                        batch.batch_info().batch_id(),
                        batch.author(),
                        batch.digest(),
                        transaction,
                    ) {
                        error!(
                            "Transaction {}, in batch {}, from {}, was rejected by the filter. Dropping {} batches!",
                            transaction.committed_hash(),
                            batch.batch_info().batch_id(),
                            author.short_str().as_str(),
                            batches.len()
                        );
                        counters::RECEIVED_BATCH_REJECTED_BY_FILTER.inc();
                        return;
                    }
                }
            }
        }

        let approx_created_ts_usecs = batch
            .info()
            .expiration()
            .saturating_sub(self.batch_expiry_gap_when_init_usecs);

        if approx_created_ts_usecs > 0 {
            observe_batch(
                approx_created_ts_usecs,
                batch.author(),
                BatchStage::RECEIVED,
            );
        }

        let mut persist_requests = vec![];
        for batch in batches.into_iter() {
            // TODO: maybe don't message batch generator if the persist is unsuccessful?
            if let Err(e) = self
                .sender_to_batch_generator
                .send(BatchGeneratorCommand::RemoteBatch(batch.clone()))
                .await
            {
                warn!("Failed to send batch to batch generator: {}", e);
            }
            persist_requests.push(batch.into());
        }
        counters::RECEIVED_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        if author != self.my_peer_id {
            counters::RECEIVED_REMOTE_BATCH_COUNT.inc_by(persist_requests.len() as u64);
        }
        self.persist_and_send_digests(persist_requests, approx_created_ts_usecs);
    }
```

**File:** consensus/src/network.rs (L426-431)
```rust
            if let Err(e) = network_sender.send_to(peer, msg.clone()) {
                warn!(
                    remote_peer = peer,
                    error = ?e, "Failed to send a msg {:?} to peer", msg
                );
            }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L300-312)
```rust
            match self.items.entry(batch_key) {
                Entry::Occupied(mut entry) => {
                    entry.get_mut().txn_summaries = Some(txn_summaries);
                },
                Entry::Vacant(entry) => {
                    entry.insert(QueueItem {
                        info: batch_info,
                        proof: None,
                        proof_insertion_time: None,
                        txn_summaries: Some(txn_summaries),
                    });
                },
            }
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L324-338)
```rust
    fn gc_expired_batch_summaries_without_proofs(&mut self) {
        let timestamp = aptos_infallible::duration_since_epoch().as_micros() as u64;
        self.items.retain(|_, item| {
            if item.is_committed() || item.proof.is_some() || item.info.expiration() > timestamp {
                true
            } else {
                self.author_to_batches
                    .get_mut(&item.info.author())
                    .map(|queue| queue.remove(&BatchSortKey::from_info(&item.info)));
                counters::GARBAGE_COLLECTED_IN_PROOF_QUEUE_COUNTER
                    .with_label_values(&["expired_batch_without_proof"])
                    .inc();
                false
            }
        });
```
