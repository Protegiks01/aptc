# Audit Report

## Title
CPU Exhaustion via Busy Loop in Indexer Data Client Transaction Fetching

## Summary
The `fetch_transactions()` function in the indexer-grpc data service contains an infinite retry loop without sleep or backoff, leading to 100% CPU consumption when gRPC calls fail or return unexpected data.

## Finding Description

The indexer-grpc data service implements a multi-layered infinite loop structure for continuously fetching blockchain transactions. While the design intent is to maintain a persistent connection to the blockchain, a critical implementation flaw exists in the innermost loop. [1](#0-0) 

The `fetch_transactions()` function contains a retry loop that attempts to fetch transactions from the GrpcManager. When errors occur (network failures, timeouts, or version mismatches), the loop retries **immediately without any sleep or backoff delay**. This creates a busy loop consuming 100% CPU.

The problematic flow is:

1. `continuously_fetch_latest_data()` calls `fetch_latest_data()` in an infinite loop [2](#0-1) 

2. `fetch_latest_data()` calls `fetch_and_update_cache()` with 200ms sleep between successful fetches [3](#0-2) 

3. `fetch_and_update_cache()` calls `data_client.fetch_transactions()` [4](#0-3) 

The 200ms sleep only occurs AFTER `fetch_transactions()` completes. While executing, if `fetch_transactions()` encounters errors, it spins in a tight loop with no delay.

The code even acknowledges missing error handling with a TODO comment on line 41 of data_client.rs, indicating this is a known gap in implementation.

## Impact Explanation

**Severity: Low (as marked in the security question)**

This vulnerability affects the **indexer-grpc data service**, which is auxiliary infrastructure for blockchain data indexing, not a core blockchain component. The impact is limited to:

- **CPU exhaustion** on indexer service nodes during network instability
- **Degraded indexer performance** and responsiveness
- **Potential service disruption** of indexer APIs

This does **NOT** affect:
- Consensus safety or validator operations
- Blockchain state or transaction processing
- User funds or on-chain assets
- Core protocol security

Per Aptos bug bounty criteria, this qualifies as **Low Severity**: "Non-critical implementation bugs" that cause resource consumption in auxiliary services without affecting the core blockchain.

## Likelihood Explanation

**Likelihood: High under specific conditions**

This issue manifests under common operational scenarios:

1. **Network instability**: Transient connection failures to GrpcManager trigger immediate retries
2. **Server errors**: GrpcManager returning errors causes continuous retry attempts
3. **Version mismatches**: Incorrect transaction version responses lead to retry loops
4. **Service degradation**: During GrpcManager overload or maintenance

The busy loop is **not** an intentional attack surface but a production bug that naturally occurs during normal operational issues. No attacker action is requiredâ€”standard network unreliability is sufficient.

## Recommendation

Implement proper error handling with exponential backoff in `fetch_transactions()`:

```rust
pub(super) async fn fetch_transactions(&self, starting_version: u64) -> Vec<Transaction> {
    trace!("Fetching transactions from GrpcManager, start_version: {starting_version}.");
    
    let request = GetTransactionsRequest {
        starting_version: Some(starting_version),
        transactions_count: None,
        batch_size: None,
        transaction_filter: None,
    };
    
    let mut retry_delay_ms = 100;
    const MAX_RETRY_DELAY_MS: u64 = 5000;
    const MAX_RETRIES: usize = 10;
    let mut retry_count = 0;
    
    loop {
        let mut client = self
            .connection_manager
            .get_grpc_manager_client_for_request();
        let response = client.get_transactions(request.clone()).await;
        
        if let Ok(response) = response {
            let transactions = response.into_inner().transactions;
            if transactions.is_empty() {
                return vec![];
            }
            if transactions.first().unwrap().version == starting_version {
                return transactions;
            }
            warn!("Version mismatch: expected {}, got {}", 
                  starting_version, transactions.first().unwrap().version);
        } else {
            warn!("Failed to fetch transactions: {:?}", response.err());
        }
        
        retry_count += 1;
        if retry_count >= MAX_RETRIES {
            error!("Exhausted retries fetching transactions from version {}", starting_version);
            return vec![];
        }
        
        tokio::time::sleep(Duration::from_millis(retry_delay_ms)).await;
        retry_delay_ms = (retry_delay_ms * 2).min(MAX_RETRY_DELAY_MS);
    }
}
```

This adds:
- Exponential backoff starting at 100ms, capping at 5 seconds
- Maximum retry limit to prevent infinite loops
- Proper logging for debugging
- Graceful failure after exhausting retries

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::atomic::{AtomicUsize, Ordering};
    use std::sync::Arc;
    
    #[tokio::test]
    async fn test_busy_loop_on_persistent_errors() {
        // Mock ConnectionManager that always returns errors
        let error_count = Arc::new(AtomicUsize::new(0));
        let error_count_clone = error_count.clone();
        
        // Simulate the current buggy behavior
        let start = std::time::Instant::now();
        let handle = tokio::spawn(async move {
            loop {
                // Simulate failed gRPC call
                error_count_clone.fetch_add(1, Ordering::SeqCst);
                // NO SLEEP HERE - this is the bug
                if error_count_clone.load(Ordering::SeqCst) > 10000 {
                    break;
                }
            }
        });
        
        // Let it run for 100ms
        tokio::time::sleep(Duration::from_millis(100)).await;
        handle.abort();
        
        let elapsed = start.elapsed();
        let errors = error_count.load(Ordering::SeqCst);
        
        // In 100ms, a busy loop can execute thousands of iterations
        // This demonstrates the CPU exhaustion issue
        assert!(errors > 1000, 
                "Busy loop executed {} iterations in {:?} - demonstrates 100% CPU usage",
                errors, elapsed);
    }
}
```

Run with: `cargo test test_busy_loop_on_persistent_errors --package indexer-grpc-data-service-v2`

## Notes

While this is a legitimate production bug that violates the Resource Limits invariant (all operations must respect computational limits), it does not meet the **Critical, High, or Medium** severity thresholds required by the validation checklist. The issue is correctly categorized as **Low severity** per the original security question, affecting only auxiliary indexer infrastructure rather than core blockchain consensus, state, or fund security.

The TODO comment in the code acknowledges the missing error handling, suggesting this may already be a known issue pending resolution. The recommended fix follows patterns used elsewhere in the codebase for transaction fetching with retry logic and exponential backoff.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/data_client.rs (L27-42)
```rust
        loop {
            let mut client = self
                .connection_manager
                .get_grpc_manager_client_for_request();
            let response = client.get_transactions(request.clone()).await;
            if let Ok(response) = response {
                let transactions = response.into_inner().transactions;
                if transactions.is_empty() {
                    return vec![];
                }
                if transactions.first().unwrap().version == starting_version {
                    return transactions;
                }
            }
            // TODO(grao): Error handling.
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/fetch_manager.rs (L40-46)
```rust
    pub(super) async fn continuously_fetch_latest_data(&'a self) {
        loop {
            let task = self.fetch_latest_data().boxed().shared();
            *self.fetching_latest_data_task.write().await = Some(task.clone());
            let _ = task.await;
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/fetch_manager.rs (L48-64)
```rust
    async fn fetch_and_update_cache(
        data_client: Arc<DataClient>,
        data_manager: Arc<RwLock<DataManager>>,
        version: u64,
    ) -> usize {
        let transactions = data_client.fetch_transactions(version).await;
        let len = transactions.len();

        if len > 0 {
            data_manager
                .write()
                .await
                .update_data(version, transactions);
        }

        len
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service-v2/src/live_data_service/fetch_manager.rs (L66-87)
```rust
    async fn fetch_latest_data(&'a self) -> usize {
        let version = self.data_manager.read().await.end_version;
        info!("Fetching latest data starting from version {version}.");
        loop {
            let num_transactions = {
                let _timer = TIMER
                    .with_label_values(&["fetch_latest_data"])
                    .start_timer();
                Self::fetch_and_update_cache(
                    self.data_client.clone(),
                    self.data_manager.clone(),
                    version,
                )
                .await
            };
            if num_transactions != 0 {
                info!("Finished fetching latest data, got {num_transactions} num_transactions starting from version {version}.");
                return num_transactions;
            }
            tokio::time::sleep(Duration::from_millis(200)).await;
        }
    }
```
