# Audit Report

## Title
Backup Process Panic Due to Unvalidated Empty Epoch Range from Malicious or Buggy Backup Service

## Summary
The epoch ending backup process in `storage/backup/backup-cli/src/backup_types/epoch_ending/backup.rs` contains an assertion at line 111 that panics if no ledger infos are returned from the backup service. A malicious backup service or edge case conditions (such as `start_epoch == end_epoch`) can cause the backup process to crash instead of returning a graceful error. [1](#0-0) 

## Finding Description

The `EpochEndingBackupController::run_impl()` function fetches epoch-ending ledger infos from a configurable backup service and processes them into chunks. The function contains two critical assertions after the processing loop: [2](#0-1) 

The vulnerability occurs when the backup service returns zero records. This can happen in several scenarios:

**Scenario 1: Malicious Backup Service**
- The backup service address is user-configurable via the `--backup-service-address` CLI parameter
- A malicious backup service can return an empty stream for the `epoch_ending_ledger_infos/{start_epoch}/{end_epoch}` endpoint
- The while loop at line 90 never executes, leaving `chunk_bytes` empty (initialized at line 81)
- The assertion at line 111 fails, causing a panic

**Scenario 2: Edge Case - Equal Epochs**
- If `start_epoch == end_epoch`, the iterator legitimately returns zero records
- The `EpochEndingLedgerInfoIter::next_impl()` function returns `None` immediately when `next_epoch >= end_epoch` [3](#0-2) 

**Scenario 3: Backup Service Bug**
- A buggy backup service might fail to return records for a valid epoch range
- Database inconsistencies or timing issues could result in empty responses

**Missing Validation:**
Unlike the `AptosDB::get_epoch_ending_ledger_info_iterator()` method which validates the epoch range, the backup handler path bypasses this validation: [4](#0-3) 

The backup handler directly calls the metadata database without validation: [5](#0-4) 

This validation gap allows invalid epoch ranges to reach the backup CLI and cause panics.

## Impact Explanation

**Severity: Medium** - This qualifies as Medium severity per the Aptos bug bounty criteria for the following reasons:

1. **Availability Impact**: The backup process is critical for disaster recovery and node restoration. A malicious backup service can prevent all backup operations from completing, creating a denial-of-service condition for backup infrastructure.

2. **State Inconsistencies**: Inability to create backups can lead to situations where node operators cannot recover from failures, potentially requiring manual intervention to restore nodes.

3. **No Direct Blockchain Impact**: This does not affect consensus, validator operations, or on-chain security. The blockchain continues to operate normally, and this only impacts the backup CLI tool.

4. **Attack Requirements**: While the attacker needs the user to configure a malicious backup service address, this is feasible through:
   - Social engineering (tricking operators to use a malicious service)
   - Compromising the legitimate backup service
   - Exploiting misconfigurations

The issue falls under "State inconsistencies requiring intervention" in the Medium severity category, as backup failures can lead to operational issues requiring manual recovery procedures.

## Likelihood Explanation

**Likelihood: Medium-High**

1. **User Error Scenarios**: Operators may accidentally specify `start_epoch == end_epoch` in scripts or manual commands, triggering the panic.

2. **Service Availability**: Temporary issues with the backup service (network problems, database issues) could result in empty responses.

3. **Configuration Risks**: The backup service address is user-configurable, making it vulnerable to misconfiguration or social engineering attacks.

4. **No Input Validation**: The lack of validation on the client side means any invalid response triggers the panic without warning.

5. **Production Impact**: Backup operations typically run in automated scripts. A panic would cause script failures and potentially go unnoticed until backups are actually needed.

## Recommendation

Replace the assertion with proper error handling that returns a descriptive error instead of panicking:

**Fix for `backup.rs`:**
```rust
// At line 111, replace:
assert!(!chunk_bytes.is_empty());

// With:
ensure!(
    !chunk_bytes.is_empty(),
    "No epoch-ending ledger infos found in range [{}, {}). Verify the epoch range is valid and the backup service is functioning correctly.",
    self.start_epoch,
    self.end_epoch
);
```

**Additional Validation (recommended):**
Add upfront validation similar to the DbReader implementation:
```rust
// At the beginning of run_impl(), after line 77:
ensure!(
    self.start_epoch < self.end_epoch,
    "Invalid epoch range: start_epoch ({}) must be less than end_epoch ({})",
    self.start_epoch,
    self.end_epoch
);
```

This ensures the backup process fails gracefully with a clear error message that can be logged and handled by automation scripts, rather than crashing with a panic.

## Proof of Concept

**Rust Test Case:**

```rust
#[tokio::test]
async fn test_backup_panic_on_empty_epoch_range() {
    use crate::backup_types::epoch_ending::backup::{
        EpochEndingBackupController, EpochEndingBackupOpt
    };
    use crate::utils::GlobalBackupOpt;
    
    // Mock a backup service that returns empty stream
    let mock_backup_service = /* mock implementation returning empty stream */;
    let mock_storage = /* mock storage */;
    
    let opt = EpochEndingBackupOpt {
        start_epoch: 5,
        end_epoch: 5, // Equal epochs - will return zero records
    };
    
    let global_opt = GlobalBackupOpt {
        max_chunk_size: 1024 * 1024,
    };
    
    let controller = EpochEndingBackupController::new(
        opt,
        global_opt,
        Arc::new(mock_backup_service),
        Arc::new(mock_storage),
    );
    
    // This should panic at the assertion on line 111
    let result = controller.run().await;
    
    // With the fix, this should return an error instead of panicking
    assert!(result.is_err());
}
```

**Manual Reproduction:**
```bash
# Run the backup CLI with equal start and end epochs
./aptos-backup-cli epoch-ending \
  --start-epoch 10 \
  --end-epoch 10 \
  --backup-service-address http://localhost:6186
  
# Expected: Panic with "assertion failed: !chunk_bytes.is_empty()"
# With fix: Error message about invalid epoch range
```

## Notes

This vulnerability demonstrates a common pattern where production code uses `assert!` instead of proper error handling with `ensure!` or `Result`. While assertions are useful for invariants that should never be violated, network I/O and external service responses should always be validated gracefully. The missing validation chain (backup handler bypasses the DbReader validation) compounds the issue by allowing invalid inputs to reach the assertion point.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/epoch_ending/backup.rs (L90-112)
```rust
        while let Some(record_bytes) = ledger_infos_file.read_record_bytes().await? {
            if should_cut_chunk(&chunk_bytes, &record_bytes, self.max_chunk_size) {
                let chunk = self
                    .write_chunk(
                        &backup_handle,
                        &chunk_bytes,
                        chunk_first_epoch,
                        current_epoch - 1,
                    )
                    .await?;
                chunks.push(chunk);
                chunk_bytes = vec![];
                chunk_first_epoch = current_epoch;
            }

            waypoints.push(Self::get_waypoint(&record_bytes, current_epoch)?);
            chunk_bytes.extend((record_bytes.len() as u32).to_be_bytes());
            chunk_bytes.extend(&record_bytes);
            current_epoch += 1;
        }

        assert!(!chunk_bytes.is_empty());
        assert_eq!(current_epoch, self.end_epoch);
```

**File:** storage/aptosdb/src/utils/iterators.rs (L209-212)
```rust
    fn next_impl(&mut self) -> Result<Option<LedgerInfoWithSignatures>> {
        if self.next_epoch >= self.end_epoch {
            return Ok(None);
        }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L1007-1034)
```rust
    fn check_epoch_ending_ledger_infos_request(
        &self,
        start_epoch: u64,
        end_epoch: u64,
    ) -> Result<()> {
        ensure!(
            start_epoch <= end_epoch,
            "Bad epoch range [{}, {})",
            start_epoch,
            end_epoch,
        );
        // Note that the latest epoch can be the same with the current epoch (in most cases), or
        // current_epoch + 1 (when the latest ledger_info carries next validator set)

        let latest_epoch = self
            .ledger_db
            .metadata_db()
            .get_latest_ledger_info()?
            .ledger_info()
            .next_block_epoch();
        ensure!(
            end_epoch <= latest_epoch,
            "Unable to provide epoch change ledger info for still open epoch. asked upper bound: {}, last sealed epoch: {}",
            end_epoch,
            latest_epoch - 1,  // okay to -1 because genesis LedgerInfo has .next_block_epoch() == 1
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L207-221)
```rust
    pub fn get_epoch_ending_ledger_info_iter(
        &self,
        start_epoch: u64,
        end_epoch: u64,
    ) -> Result<impl Iterator<Item = Result<LedgerInfoWithSignatures>> + '_> {
        Ok(self
            .ledger_db
            .metadata_db()
            .get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
            .enumerate()
            .map(move |(idx, li)| {
                BACKUP_EPOCH_ENDING_EPOCH.set((start_epoch + idx as u64) as i64);
                li
            }))
    }
```
