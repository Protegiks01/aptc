# Audit Report

## Title
Silent Pruner Initialization Failure with Corrupted Internal Indexer Database

## Summary
When `internal_indexer_db` is configured but corrupted, `LedgerPruner::new()` can succeed during initialization without detecting the corruption if sub-pruners are already caught up. Subsequently, the `PrunerWorker` fails silently during pruning operations, logging errors but continuing execution, which leaves the ledger pruner stuck in an inconsistent state where it cannot make progress.

## Finding Description

The vulnerability exists in the initialization and error handling flow of the ledger pruner with an optional internal indexer database dependency.

During initialization in `init_pruner()`, the code calls `LedgerPruner::new()` with an `.expect()` panic handler: [1](#0-0) 

Inside `LedgerPruner::new()`, it creates sub-pruners including `EventStorePruner` and `TransactionPruner`, passing the `internal_indexer_db`: [2](#0-1) 

Each sub-pruner calls `prune(progress, metadata_progress)` during initialization to "catch up": [3](#0-2) 

**Critical Issue**: If `progress == metadata_progress` (already caught up), the parent `LedgerPruner::prune()` method performs no work: [4](#0-3) 

The `while progress < target_version` loop (line 66) never executes if already caught up, so no write operations to the `internal_indexer_db` occur during initialization. This means a corrupted indexer database is never accessed and the corruption goes undetected.

Later, during normal pruning operations, the `PrunerWorker` catches errors but does not propagate them: [5](#0-4) 

When `prune()` fails due to the corrupted indexer DB (during `write_schemas()` calls), the error is logged but the worker continues looping (line 63). The pruner progress is never advanced because the prune operation failed, leaving the pruner stuck indefinitely.

**Attack Scenario:**
1. Node restarts with `internal_indexer_db` configured but corrupted (disk full, permissions issue, schema mismatch)
2. Sub-pruners are already caught up (`progress == metadata_progress`)
3. Initialization succeeds without ever writing to the corrupted indexer DB
4. During normal operation, pruning attempts fail when writing to the indexer DB
5. `PrunerWorker` logs errors but continues running without crashing
6. Pruner remains stuck, unable to advance `min_readable_version`
7. Old ledger data accumulates, causing disk space exhaustion
8. Indexer DB becomes increasingly out of sync with ledger DB
9. Queries depending on the indexer may fail or return stale data

## Impact Explanation

This is a **Medium Severity** issue according to Aptos bug bounty criteria:
- **State inconsistencies requiring intervention**: The ledger DB and indexer DB become desynchronized
- **Operational degradation**: Disk space exhaustion from unpruned data
- **Silent failure**: The node appears healthy but core functionality is broken
- Does not directly affect consensus or transaction processing
- Requires manual operator intervention to detect and resolve

The vulnerability breaks the **State Consistency** invariant (#4): "State transitions must be atomic and verifiable." The pruner's state becomes inconsistent - it believes pruning is ongoing, but actually no progress is made.

## Likelihood Explanation

**Likelihood: Medium to High** in production environments:

- Disk corruption, permissions changes, or filesystem issues are common operational scenarios
- The internal indexer DB may become corrupted due to incomplete writes during crashes
- Schema mismatches can occur during upgrades if migration is incomplete
- The error is logged but may be missed in high-volume production logs
- No automatic recovery mechanism exists - requires manual operator intervention
- Once stuck, the pruner remains stuck indefinitely until manually resolved

The vulnerability is particularly likely because:
1. The initialization check is insufficient (no actual write validation)
2. Error handling in the worker thread is too permissive (continues on errors)
3. No health check monitors pruner progress advancement

## Recommendation

**Immediate Fix**: Add explicit validation of the internal indexer DB during initialization:

```rust
fn init_pruner(
    ledger_db: Arc<LedgerDb>,
    ledger_pruner_config: LedgerPrunerConfig,
    internal_indexer_db: Option<InternalIndexerDB>,
) -> PrunerWorker {
    // Validate indexer DB is writable if provided
    if let Some(ref indexer_db) = internal_indexer_db {
        let mut test_batch = SchemaBatch::new();
        test_batch.put::<InternalIndexerMetadataSchema>(
            &MetadataKey::LedgerPrunerValidation,
            &MetadataValue::Version(0),
        ).expect("Failed to create validation batch");
        
        indexer_db.get_inner_db_ref()
            .write_schemas(test_batch)
            .expect("Internal indexer DB validation failed - DB may be corrupted");
    }
    
    let pruner = Arc::new(
        LedgerPruner::new(ledger_db, internal_indexer_db)
            .expect("Failed to create ledger pruner."),
    );
    // ... rest of initialization
}
```

**Long-term Fix**: Improve error handling in `PrunerWorker`:

```rust
fn work(&self) {
    let mut consecutive_failures = 0;
    const MAX_CONSECUTIVE_FAILURES: u32 = 100;
    
    while !self.quit_worker.load(Ordering::SeqCst) {
        let pruner_result = self.pruner.prune(self.batch_size);
        if pruner_result.is_err() {
            consecutive_failures += 1;
            sample!(
                SampleRate::Duration(Duration::from_secs(1)),
                error!(
                    error = ?pruner_result.err().unwrap(),
                    consecutive_failures = consecutive_failures,
                    "Pruner has error."
                )
            );
            
            if consecutive_failures >= MAX_CONSECUTIVE_FAILURES {
                panic!("Pruner failed {} consecutive times, likely corrupted database", 
                       MAX_CONSECUTIVE_FAILURES);
            }
            
            sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            continue;
        }
        consecutive_failures = 0;  // Reset on success
        // ... rest of work loop
    }
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_temppath::TempPath;
    use std::sync::Arc;
    
    #[test]
    #[should_panic(expected = "Internal indexer DB validation failed")]
    fn test_corrupted_indexer_db_detection() {
        // Setup: Create a ledger DB and a corrupted/read-only indexer DB
        let tmpdir = TempPath::new();
        let ledger_db = Arc::new(LedgerDb::new_for_test(tmpdir.path()));
        
        // Create indexer DB and corrupt it (e.g., make it read-only)
        let indexer_path = TempPath::new();
        let indexer_db = create_test_indexer_db(indexer_path.path());
        
        // Make the DB directory read-only to simulate corruption
        std::fs::set_permissions(
            indexer_path.path(),
            std::fs::Permissions::from_mode(0o444)
        ).unwrap();
        
        let config = LedgerPrunerConfig {
            enable: true,
            prune_window: 1000,
            batch_size: 100,
            user_pruning_window_offset: 0,
        };
        
        // This should panic when trying to validate the corrupted indexer DB
        let _manager = LedgerPrunerManager::new(
            ledger_db,
            config,
            Some(indexer_db),
        );
    }
    
    #[test]
    fn test_pruner_stuck_on_corrupted_indexer() {
        // Setup: Create normal DBs, then corrupt indexer after initialization
        let tmpdir = TempPath::new();
        let ledger_db = Arc::new(LedgerDb::new_for_test(tmpdir.path()));
        let indexer_db = create_test_indexer_db(TempPath::new().path());
        
        // Initialize with healthy DB - this succeeds
        let manager = LedgerPrunerManager::new(
            Arc::clone(&ledger_db),
            LedgerPrunerConfig {
                enable: true,
                prune_window: 100,
                batch_size: 10,
                user_pruning_window_offset: 0,
            },
            Some(indexer_db.clone()),
        );
        
        // Now corrupt the indexer DB (simulate disk full)
        corrupt_indexer_db(&indexer_db);
        
        // Add new versions to ledger DB
        for v in 0..200 {
            add_test_transaction(&ledger_db, v);
        }
        
        // Trigger pruning - this should fail but worker continues
        manager.maybe_set_pruner_target_db_version(200);
        
        // Sleep to allow worker to attempt pruning
        std::thread::sleep(Duration::from_secs(2));
        
        // Assert: min_readable_version hasn't advanced due to stuck pruner
        assert_eq!(manager.get_min_readable_version(), 0, 
                   "Pruner should be stuck at version 0");
        
        // Assert: Ledger DB has unpruned old data
        assert!(ledger_db.transaction_exists(0).unwrap(),
                "Old transactions should still exist (not pruned)");
    }
}
```

## Notes

This vulnerability represents a failure in defensive programming - the initialization assumes the indexer DB is healthy without validation, and the worker's error handling is too lenient for persistent failures. The silent degradation is particularly concerning because operators may not notice until disk space is exhausted or queries start failing.

The fix requires both immediate validation during initialization and improved error handling with failure limits to detect persistent corruption early and fail fast rather than silently degrading.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L146-149)
```rust
        let pruner = Arc::new(
            LedgerPruner::new(ledger_db, internal_indexer_db)
                .expect("Failed to create ledger pruner."),
        );
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L138-166)
```rust
        let event_store_pruner = Box::new(EventStorePruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db.clone(),
        )?);
        let persisted_auxiliary_info_pruner = Box::new(PersistedAuxiliaryInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
        let transaction_accumulator_pruner = Box::new(TransactionAccumulatorPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let transaction_auxiliary_data_pruner = Box::new(TransactionAuxiliaryDataPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);

        let transaction_info_pruner = Box::new(TransactionInfoPruner::new(
            Arc::clone(&ledger_db),
            metadata_progress,
        )?);
        let transaction_pruner = Box::new(TransactionPruner::new(
            Arc::clone(&transaction_store),
            Arc::clone(&ledger_db),
            metadata_progress,
            internal_indexer_db,
        )?);
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L90-107)
```rust
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;

        let myself = EventStorePruner {
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up EventStorePruner."
        );
        myself.prune(progress, metadata_progress)?;

```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```
