# Audit Report

## Title
Silent Message Dropping in Bounded Consensus Channels Causes Temporary Liveness Loss Under High Load

## Summary
Critical consensus messages (proposals, votes, round timeouts, sync info) can be silently dropped when per-validator message queues reach capacity during high load, leading to temporary loss of consensus liveness. The `aptos_channel::Sender::push()` method returns `Ok(())` even when messages are dropped, providing no notification to the application layer for retry or recovery.

## Finding Description

The Aptos consensus layer uses bounded per-key message queues implemented in `PerKeyQueue` with a default capacity of only 10 messages per validator. When a validator's queue fills during high load or slow processing, additional messages are silently dropped according to the queue style policy.

**Critical Flow:**

1. **Message Queue Implementation** - When the per-key queue is full, messages are dropped based on `QueueStyle`: [1](#0-0) 

2. **Silent Drop at Channel Layer** - The `aptos_channel` wrapper returns `Ok(())` even when messages are dropped, only notifying via an optional status channel that is NOT used by consensus: [2](#0-1) 

3. **No Error Detection in Network Layer** - The consensus network task only checks for channel closure, not message drops: [3](#0-2) 

4. **Critical Messages Affected** - The following consensus-critical message types use these bounded queues: [4](#0-3) 

5. **Low Default Capacity** - The internal channel size is only 10 messages per validator: [5](#0-4) 

6. **Consensus Channel Configuration** - The main consensus message channel uses FIFO with capacity 10: [6](#0-5) 

**Attack Scenario:**
During legitimate high load or slow processing on Validator B:
- Validator A sends 10+ messages (proposals, votes, timeouts) to Validator B
- Validator B's per-key queue fills to capacity (10 messages)
- Messages 11+ are silently dropped with no error returned
- If dropped messages include votes, quorum certificates cannot be formed
- If dropped messages include proposals, validators cannot participate in consensus
- If dropped messages include timeout messages, round advancement is delayed
- Consensus experiences temporary liveness loss until queue drains and new messages get through

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria for the following reasons:

1. **Validator Node Slowdowns**: The silent message dropping directly causes validator nodes to slow down or stall during high load, fitting the High Severity category explicitly.

2. **Temporary Loss of Liveness**: When critical consensus messages are lost:
   - Dropped votes prevent quorum certificate formation, blocking consensus progress
   - Dropped proposals prevent validators from voting on new blocks
   - Dropped round timeout messages delay round advancement
   - Dropped sync info messages prevent state synchronization

3. **No Recovery Mechanism**: Unlike normal network failures that trigger retries, these drops are silent at the application level, providing no opportunity for automatic recovery.

4. **Realistic Occurrence**: This is not a theoretical attack but a real design limitation that manifests during:
   - Network congestion or delays
   - CPU/memory saturation on validators  
   - I/O bottlenecks during state operations
   - High transaction throughput periods

The metrics system tracks drops [7](#0-6) , but metrics are for observability onlyâ€”they don't prevent liveness loss.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability will manifest whenever:
1. A validator processes messages slower than they arrive (common during load spikes)
2. The 10-message per-validator queue fills up
3. Additional messages from that validator are dropped

This is highly likely because:
- **Low Queue Capacity**: Only 10 messages per validator is insufficient during burst traffic
- **No Backpressure**: Senders have no feedback that queues are full, continuing to send
- **Natural Load Variations**: Validators routinely experience processing delays due to execution backpressure, network latency, or resource contention
- **Multiple Message Types**: Each consensus round generates multiple message types (proposals, votes, timeouts), quickly filling small queues

The FIFO queue style for consensus messages exacerbates the issue by dropping newest messages, potentially discarding the most time-critical information.

## Recommendation

**Immediate Mitigation:**
1. Increase `internal_per_key_channel_size` from 10 to at least 100 to provide more buffering headroom
2. Modify `Sender::push()` to return `Result<(), MessageDropped>` when messages are dropped, allowing callers to implement retry logic
3. Add application-level monitoring and alerting when drop counters exceed thresholds

**Long-term Fix:**
Implement a priority-based message queue with:
- Critical messages (votes, proposals) never dropped - use unbounded queue or apply backpressure to sender
- Non-critical messages can be dropped with explicit notification
- Sender-side backpressure signaling to slow message production when queues are near capacity

**Code Fix Example:**
```rust
// In aptos_channel.rs
pub enum PushError<M> {
    ChannelClosed,
    MessageDropped(M), // Return dropped message to caller
}

pub fn push(&self, key: K, message: M) -> Result<(), PushError<M>> {
    let mut shared_state = self.shared_state.lock();
    ensure!(!shared_state.receiver_dropped, PushError::ChannelClosed);
    
    let dropped = shared_state.internal_queue.push(key, (message, None));
    if let Some((dropped_val, _)) = dropped {
        return Err(PushError::MessageDropped(dropped_val));
    }
    
    if let Some(w) = shared_state.waker.take() {
        w.wake();
    }
    Ok(())
}
```

## Proof of Concept

```rust
// Rust test demonstrating message dropping
#[tokio::test]
async fn test_consensus_message_dropping() {
    use aptos_channels::aptos_channel;
    use aptos_channels::message_queues::QueueStyle;
    use std::mem::discriminant;
    
    // Create channel with capacity 10 (same as consensus config)
    let (tx, mut rx) = aptos_channel::new::<
        (AccountAddress, Discriminant<ConsensusMsg>), 
        (AccountAddress, ConsensusMsg)
    >(
        QueueStyle::FIFO,
        10,
        None,
    );
    
    let validator_addr = AccountAddress::random();
    
    // Send 15 messages from same validator
    for i in 0..15 {
        let msg = ConsensusMsg::VoteMsg(Box::new(create_test_vote(i)));
        let key = (validator_addr, discriminant(&msg));
        
        // Push returns Ok even when messages are dropped!
        let result = tx.push(key, (validator_addr, msg));
        assert!(result.is_ok()); // No error returned
    }
    
    // Only first 10 messages are in queue, last 5 were silently dropped
    let mut received = 0;
    while let Ok(Some(_)) = tokio::time::timeout(
        Duration::from_millis(10),
        rx.next()
    ).await {
        received += 1;
    }
    
    // Only 10 messages received, 5 critical votes lost!
    assert_eq!(received, 10);
    // The 5 dropped votes could have prevented quorum formation
}
```

## Notes

This vulnerability represents a fundamental design limitation in the message queue architecture where bounded queues are necessary for memory safety, but the current implementation provides no graceful degradation or recovery mechanism when capacity is exceeded. The silent nature of message drops (returning `Ok(())`) violates the principle of explicit error handling for critical operations and prevents the consensus layer from implementing appropriate retry or backpressure mechanisms.

While message drops are tracked in Prometheus metrics, metrics are insufficient for preventing liveness loss in real-time. The consensus protocol requires reliable delivery of critical messages to maintain liveness guarantees, and the current bounded queue implementation with silent drops cannot provide this guarantee under load.

### Citations

**File:** crates/channel/src/message_queues.rs (L134-147)
```rust
        if key_message_queue.len() >= self.max_queue_size.get() {
            if let Some(c) = self.counters.as_ref() {
                c.with_label_values(&["dropped"]).inc();
            }
            match self.queue_style {
                // Drop the newest message for FIFO
                QueueStyle::FIFO => Some(message),
                // Drop the oldest message for LIFO
                QueueStyle::LIFO | QueueStyle::KLAST => {
                    let oldest = key_message_queue.pop_front();
                    key_message_queue.push_back(message);
                    oldest
                },
            }
```

**File:** crates/channel/src/aptos_channel.rs (L96-112)
```rust
    ) -> Result<()> {
        let mut shared_state = self.shared_state.lock();
        ensure!(!shared_state.receiver_dropped, "Channel is closed");
        debug_assert!(shared_state.num_senders > 0);

        let dropped = shared_state.internal_queue.push(key, (message, status_ch));
        // If this or an existing message had to be dropped because of the queue being full, we
        // notify the corresponding status channel if it was registered.
        if let Some((dropped_val, Some(dropped_status_ch))) = dropped {
            // Ignore errors.
            let _err = dropped_status_ch.send(ElementStatus::Dropped(dropped_val));
        }
        if let Some(w) = shared_state.waker.take() {
            w.wake();
        }
        Ok(())
    }
```

**File:** consensus/src/network.rs (L757-761)
```rust
        let (consensus_messages_tx, consensus_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            10,
            Some(&counters::CONSENSUS_CHANNEL_MSGS),
        );
```

**File:** consensus/src/network.rs (L807-812)
```rust
        if let Err(e) = tx.push((peer_id, discriminant(&msg)), (peer_id, msg)) {
            warn!(
                remote_peer = peer_id,
                error = ?e, "Error pushing consensus msg",
            );
        }
```

**File:** consensus/src/network.rs (L863-870)
```rust
                        consensus_msg @ (ConsensusMsg::ProposalMsg(_)
                        | ConsensusMsg::OptProposalMsg(_)
                        | ConsensusMsg::VoteMsg(_)
                        | ConsensusMsg::RoundTimeoutMsg(_)
                        | ConsensusMsg::OrderVoteMsg(_)
                        | ConsensusMsg::SyncInfo(_)
                        | ConsensusMsg::EpochRetrievalRequest(_)
                        | ConsensusMsg::EpochChangeProof(_)) => {
```

**File:** config/src/config/consensus_config.rs (L242-242)
```rust
            internal_per_key_channel_size: 10,
```

**File:** consensus/src/counters.rs (L1068-1075)
```rust
pub static CONSENSUS_CHANNEL_MSGS: Lazy<IntCounterVec> = Lazy::new(|| {
    register_int_counter_vec!(
        "aptos_consensus_channel_msgs_count",
        "Counters(queued,dequeued,dropped) related to consensus channel",
        &["state"]
    )
    .unwrap()
});
```
