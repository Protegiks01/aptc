# Audit Report

## Title
Non-Consensus-Critical Block Partitioner Configuration Could Cause Determinism Violations in Future Sharded Execution Deployments

## Summary
The `load_imbalance_tolerance` configuration parameter (default 2.0) in the ConnectedComponentPartitioner is a local, per-node setting that directly affects transaction partitioning and ordering. If sharded execution is deployed in production consensus without making this configuration consensus-critical, different validators could partition and execute the same block in different orders, violating the deterministic execution invariant and causing consensus failures.

## Finding Description

The `ConnectedComponentPartitioner` uses `load_imbalance_tolerance` to calculate a group size limit that determines how large transaction conflict groups are broken into smaller chunks: [1](#0-0) 

This value is used during partitioning to compute the maximum group size: [2](#0-1) 

Different `load_imbalance_tolerance` values result in different group size limits, which cause connected components to be split differently. This affects which transactions are assigned to which shards via the longest-processing-time-first scheduling algorithm: [3](#0-2) 

The sharded execution path exists in the production executor workflow: [4](#0-3) 

When sharded execution is used, the final transaction order is determined by flattening the partitioned structure in round-shard-transaction order: [5](#0-4) 

The execution outputs are aggregated in this same round-shard order: [6](#0-5) 

Critically, the block executor configuration is separated into local (per-node) and on-chain components, and the partitioner configuration is NOT part of the on-chain configuration: [7](#0-6) 

**The Attack Scenario:**

If sharded execution were deployed in production without making `load_imbalance_tolerance` consensus-critical:

1. Consensus proposes a block with transactions [T1, T2, T3, ..., T100]
2. Validator A has `load_imbalance_tolerance = 2.0` (default)
3. Validator B has `load_imbalance_tolerance = 3.0` (after an upgrade/config change)
4. Validator A partitions transactions into groups of max size 20 (assuming 100 txns, 10 shards)
5. Validator B partitions into groups of max size 30
6. Different group sizes lead to different LPT scheduling results
7. Transactions are executed in different orders: A gets [T1,T3,T2,T4,...], B gets [T1,T2,T4,T3,...]
8. Even if individual transaction results are identical, the final state depends on execution order for events, gas charges, and state updates
9. Validators produce different state roots, breaking consensus

## Impact Explanation

**Severity: Critical** (Consensus/Safety Violation)

This violates the fundamental **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks." Different transaction orderings lead to:

1. **Different event orderings**: Events are emitted in transaction execution order
2. **Different transaction indices**: Transactions are indexed by their position in the ledger
3. **Potential state divergence**: If any state update depends on prior transaction effects (gas pools, sequence numbers, aggregators), different orders can produce different final states
4. **Consensus failure**: Validators would be unable to agree on the state root, causing a chain halt or fork

This meets the Critical severity criteria of "Consensus/Safety violations" and "Non-recoverable network partition" per the Aptos bug bounty program.

## Likelihood Explanation

**Current Likelihood: Low** - Sharded execution does not appear to be enabled in production consensus currently. The implementation is found primarily in benchmark code.

**Future Likelihood: High IF deployed without fixes** - If sharded execution is deployed as currently implemented:
- Different validators may have different local configurations (especially after rolling upgrades)
- Configuration drift over time is common in distributed systems
- No on-chain enforcement prevents configuration mismatches
- The vulnerability would be automatically triggered on every block execution

## Recommendation

**Before deploying sharded execution to production consensus:**

1. **Make partitioner configuration consensus-critical**: Move `load_imbalance_tolerance` and all other partitioner parameters into the on-chain configuration that all validators must agree upon:

```rust
// In types/src/block_executor/config.rs
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct BlockExecutorConfigFromOnchain {
    pub block_gas_limit_type: BlockGasLimitType,
    enable_per_block_gas_limit: bool,
    per_block_gas_limit: Option<u64>,
    gas_price_to_burn: Option<u64>,
    // ADD THESE:
    pub partitioner_config: Option<PartitionerConfigOnchain>,
}

#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct PartitionerConfigOnchain {
    pub load_imbalance_tolerance: OrderedFloat<f32>, // Use OrderedFloat for deterministic serialization
    pub max_partitioning_rounds: usize,
    pub cross_shard_dep_avoid_threshold: OrderedFloat<f32>,
    // ... other partitioner parameters
}
```

2. **Validate configuration consistency**: Add validation in consensus to ensure all nodes use identical partitioner configurations from on-chain config.

3. **Add determinism tests**: Create tests that verify identical blocks produce identical state roots across different partitioner configurations (if any variance is allowed).

4. **Alternative approach**: Keep sharded execution as a pure optimization that does NOT affect transaction ordering. Ensure the flattened order always matches the original block order proposed by consensus.

## Proof of Concept

```rust
// Conceptual PoC demonstrating the issue
// This would need to be integrated into the test framework

#[test]
fn test_partitioner_config_affects_determinism() {
    let transactions = create_test_block(100); // 100 transactions
    let num_shards = 10;
    
    // Validator A configuration
    let partitioner_a = ConnectedComponentPartitioner {
        load_imbalance_tolerance: 2.0,
    };
    let partitioned_a = PartitionerV2::new(
        8, 4, 0.9, 64, false,
        Box::new(partitioner_a)
    ).partition(transactions.clone(), num_shards);
    
    // Validator B configuration (different tolerance)
    let partitioner_b = ConnectedComponentPartitioner {
        load_imbalance_tolerance: 3.0,
    };
    let partitioned_b = PartitionerV2::new(
        8, 4, 0.9, 64, false,
        Box::new(partitioner_b)
    ).partition(transactions.clone(), num_shards);
    
    // Flatten to execution order
    let order_a = PartitionedTransactions::flatten(partitioned_a);
    let order_b = PartitionedTransactions::flatten(partitioned_b);
    
    // ASSERTION FAILS: Different configurations produce different orderings
    assert_eq!(
        order_a.iter().map(|t| t.test_only_hash()).collect::<Vec<_>>(),
        order_b.iter().map(|t| t.test_only_hash()).collect::<Vec<_>>(),
        "Different partitioner configs must produce same transaction order for consensus!"
    );
}
```

## Notes

The current implementation appears to be experimental/benchmark-only. However, the question specifically asks about "a future release" where this configuration might change. The vulnerability is latent and would become exploitable if sharded execution is deployed to production consensus without proper safeguards. The code infrastructure for sharded execution exists in production paths, indicating this feature is being developed for eventual deployment.

### Citations

**File:** execution/block-partitioner/src/pre_partition/connected_component/config.rs (L17-23)
```rust
impl Default for ConnectedComponentPartitionerConfig {
    fn default() -> Self {
        ConnectedComponentPartitionerConfig {
            load_imbalance_tolerance: 2.0,
        }
    }
}
```

**File:** execution/block-partitioner/src/pre_partition/connected_component/mod.rs (L88-91)
```rust
        // Calculate txn group size limit.
        let group_size_limit = ((state.num_txns() as f32) * self.load_imbalance_tolerance
            / (state.num_executor_shards as f32))
            .ceil() as usize;
```

**File:** execution/block-partitioner/src/pre_partition/connected_component/mod.rs (L96-114)
```rust
        let group_metadata: Vec<(usize, usize)> = txns_by_set
            .iter()
            .enumerate()
            .flat_map(|(set_idx, txns)| {
                let num_chunks = txns.len().div_ceil(group_size_limit);
                let mut ret = vec![(set_idx, group_size_limit); num_chunks];
                let last_chunk_size = txns.len() - group_size_limit * (num_chunks - 1);
                ret[num_chunks - 1] = (set_idx, last_chunk_size);
                ret
            })
            .collect();

        // Assign groups to shards using longest-processing-time first scheduling.
        let tasks: Vec<u64> = group_metadata
            .iter()
            .map(|(_, size)| (*size) as u64)
            .collect();
        let (_longest_pole, shards_by_group) =
            longest_processing_time_first(&tasks, state.num_executor_shards);
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L68-89)
```rust
        let out = match transactions {
            ExecutableTransactions::Unsharded(txns) => {
                Self::by_transaction_execution_unsharded::<V>(
                    executor,
                    txns,
                    auxiliary_infos,
                    parent_state,
                    state_view,
                    onchain_config,
                    transaction_slice_metadata,
                )?
            },
            // TODO: Execution with auxiliary info is yet to be supported properly here for sharded transactions
            ExecutableTransactions::Sharded(txns) => Self::by_transaction_execution_sharded::<V>(
                txns,
                auxiliary_infos,
                parent_state,
                state_view,
                onchain_config,
                transaction_slice_metadata.append_state_checkpoint_to_block(),
            )?,
        };
```

**File:** types/src/block_executor/partitioner.rs (L378-394)
```rust
    pub fn flatten(block: Vec<SubBlocksForShard<T>>) -> Vec<T> {
        let num_shards = block.len();
        let mut flattened_txns = Vec::new();
        let num_rounds = block[0].num_sub_blocks();
        let mut ordered_blocks = vec![SubBlock::empty(); num_shards * num_rounds];
        for (shard_id, sub_blocks) in block.into_iter().enumerate() {
            for (round, sub_block) in sub_blocks.into_sub_blocks().into_iter().enumerate() {
                ordered_blocks[round * num_shards + shard_id] = sub_block;
            }
        }

        for sub_block in ordered_blocks.into_iter() {
            flattened_txns.extend(sub_block.into_txns());
        }

        flattened_txns
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L98-115)
```rust
        let num_rounds = sharded_output[0].len();
        let mut aggregated_results = vec![];
        let mut ordered_results = vec![vec![]; num_executor_shards * num_rounds];
        // Append the output from individual shards in the round order
        for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
            for (round, result) in results_from_shard.into_iter().enumerate() {
                ordered_results[round * num_executor_shards + shard_id] = result;
            }
        }

        for result in ordered_results.into_iter() {
            aggregated_results.extend(result);
        }

        // Lastly append the global output
        aggregated_results.extend(global_output);

        Ok(aggregated_results)
```

**File:** types/src/block_executor/config.rs (L82-90)
```rust
/// Configuration from on-chain configuration, that is
/// required to be the same across all nodes.
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct BlockExecutorConfigFromOnchain {
    pub block_gas_limit_type: BlockGasLimitType,
    enable_per_block_gas_limit: bool,
    per_block_gas_limit: Option<u64>,
    gas_price_to_burn: Option<u64>,
}
```
