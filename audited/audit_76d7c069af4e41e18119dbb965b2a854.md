# Audit Report

## Title
Unbounded Task Spawning DoS in RandManager Randomness Generation

## Summary
The `RandManager::spawn_aggregate_shares_task()` function spawns unbounded tokio tasks for each incoming block without rate limiting. During high throughput or catch-up scenarios, this leads to resource exhaustion that can cause validator node slowdowns.

## Finding Description

The vulnerability exists in the randomness generation pipeline where consensus-ordered blocks flow to the RandManager before execution.

**Vulnerable Code Flow:**

1. Consensus orders blocks and sends them via `ExecutionProxyClient::finalize_order()` [1](#0-0) 

2. Blocks flow through **unbounded channels** to RandManager [2](#0-1)  and [3](#0-2) 

3. For each block in the batch, `process_incoming_blocks()` calls `process_incoming_metadata()` [4](#0-3) 

4. Each call spawns a task via `spawn_aggregate_shares_task()` [5](#0-4) 

5. The task uses unbounded `tokio::spawn()` without executor limits [6](#0-5) 

6. Each task sleeps 300ms then multicasts to network [7](#0-6) 

7. Tasks abort only when blocks are dequeued from BlockQueue [8](#0-7)  and [9](#0-8) 

**Critical Design Flaw:**

The verification task correctly uses `BoundedExecutor` [10](#0-9)  but the aggregate shares task does not. This inconsistency allows unbounded task accumulation when blocks arrive faster than randomness can be generated.

**Why BufferManager Backpressure Doesn't Prevent This:**

BufferManager's backpressure mechanism [11](#0-10)  and [12](#0-11)  applies AFTER RandManager in the pipeline. The RandManager receives blocks through unbounded channels [13](#0-12)  before backpressure can take effect.

**Trigger Scenarios:**

1. **Catch-up/Recovery**: Validator processes many historical blocks rapidly after downtime
2. **High Throughput**: Sustained high block ordering rate during network congestion
3. **Batch Processing**: `path_from_ordered_root()` returns multiple blocks in single batch

## Impact Explanation

**Severity: High** (up to $50,000 per Aptos Bug Bounty)

This vulnerability causes **validator node slowdowns through resource exhaustion**, matching the High Severity criterion defined in the Aptos bug bounty program. Specifically:

- **Tokio Runtime Saturation**: Unbounded task spawning exhausts tokio runtime threads
- **Memory Exhaustion**: Each task holds Arc clones, metadata, and state objects
- **Performance Degradation**: Affects validator consensus participation
- **Potential Crashes**: Extreme accumulation may trigger OOM conditions

This violates the **Resource Limits** security invariant requiring all operations to respect computational limits. While it doesn't directly compromise consensus safety or funds, it degrades network availability and validator performance critical for network operation.

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability can be triggered through legitimate operations:

1. **Normal Catch-Up**: Common scenario when validators rejoin after downtime
2. **No Special Access**: Triggered through normal consensus block ordering
3. **No Byzantine Behavior Required**: Occurs during legitimate high-throughput periods
4. **Inherent Design Flaw**: Unbounded spawning is fundamental to current implementation

The 300ms sleep per task means even moderate block rates (3-4 blocks/sec) can cause task accumulation when randomness generation lags behind block ordering.

## Recommendation

Use `BoundedExecutor` for aggregate shares tasks, consistent with the verification task pattern:

1. Pass the existing `bounded_executor` to `spawn_aggregate_shares_task()`
2. Replace `tokio::spawn()` with `bounded_executor.spawn()` 
3. Consider adding queue size limits to BlockQueue with monitoring
4. Implement explicit backpressure before RandManager receives blocks

## Proof of Concept

A complete PoC would require:
1. Mock consensus that orders blocks rapidly (10+ blocks/sec)
2. Simulated slow randomness share collection (>1 sec per block)
3. Monitoring of spawned task count and memory usage
4. Demonstration of tokio runtime saturation

The vulnerability is evident from code inspection showing the design inconsistency between bounded verification tasks and unbounded aggregation tasks.

## Notes

The core issue is the design inconsistency: verification tasks use `BoundedExecutor` while aggregate shares tasks use unbounded `tokio::spawn()`. This violates the principle of uniform resource management and creates a clear attack vector during high-throughput scenarios. The BufferManager's backpressure mechanism cannot prevent this since it operates downstream of the RandManager in the pipeline architecture.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L233-234)
```rust
        let (ordered_block_tx, ordered_block_rx) = unbounded::<OrderedBlocks>();
        let (rand_ready_block_tx, rand_ready_block_rx) = unbounded::<OrderedBlocks>();
```

**File:** consensus/src/pipeline/execution_client.rs (L438-448)
```rust
            (Some(rand_config), None) => {
                let (ordered_block_tx, rand_ready_block_rx, reset_tx_to_rand_manager) = self
                    .make_rand_manager(
                        &epoch_state,
                        fast_rand_config,
                        rand_msg_rx,
                        highest_committed_round,
                        &network_sender,
                        rand_config,
                        consensus_sk,
                    );
```

**File:** consensus/src/pipeline/execution_client.rs (L590-624)
```rust
    async fn finalize_order(
        &self,
        blocks: Vec<Arc<PipelinedBlock>>,
        ordered_proof: WrappedLedgerInfo,
    ) -> ExecutorResult<()> {
        assert!(!blocks.is_empty());
        let mut execute_tx = match self.handle.read().execute_tx.clone() {
            Some(tx) => tx,
            None => {
                debug!("Failed to send to buffer manager, maybe epoch ends");
                return Ok(());
            },
        };

        for block in &blocks {
            block.set_insertion_time();
            if let Some(tx) = block.pipeline_tx().lock().as_mut() {
                tx.order_proof_tx
                    .take()
                    .map(|tx| tx.send(ordered_proof.clone()));
            }
        }

        if execute_tx
            .send(OrderedBlocks {
                ordered_blocks: blocks,
                ordered_proof: ordered_proof.ledger_info().clone(),
            })
            .await
            .is_err()
        {
            debug!("Failed to send to buffer manager, maybe epoch ends");
        }
        Ok(())
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L48-49)
```rust
pub type Sender<T> = UnboundedSender<T>;
pub type Receiver<T> = UnboundedReceiver<T>;
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L132-143)
```rust
    fn process_incoming_blocks(&mut self, blocks: OrderedBlocks) {
        let rounds: Vec<u64> = blocks.ordered_blocks.iter().map(|b| b.round()).collect();
        info!(rounds = rounds, "Processing incoming blocks.");
        let broadcast_handles: Vec<_> = blocks
            .ordered_blocks
            .iter()
            .map(|block| FullRandMetadata::from(block.block()))
            .map(|metadata| self.process_incoming_metadata(metadata))
            .collect();
        let queue_item = QueueItem::new(blocks, Some(broadcast_handles));
        self.block_queue.push_back(queue_item);
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L168-168)
```rust
        self.spawn_aggregate_shares_task(metadata.metadata)
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L234-259)
```rust
            bounded_executor
                .spawn(async move {
                    match bcs::from_bytes::<RandMessage<S, D>>(rand_gen_msg.req.data()) {
                        Ok(msg) => {
                            if msg
                                .verify(
                                    &epoch_state_clone,
                                    &config_clone,
                                    &fast_config_clone,
                                    rand_gen_msg.sender,
                                )
                                .is_ok()
                            {
                                let _ = tx.unbounded_send(RpcRequest {
                                    req: msg,
                                    protocol: rand_gen_msg.protocol,
                                    response_sender: rand_gen_msg.response_sender,
                                });
                            }
                        },
                        Err(e) => {
                            warn!("Invalid rand gen message: {}", e);
                        },
                    }
                })
                .await;
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L274-290)
```rust
            tokio::time::sleep(Duration::from_millis(300)).await;
            let maybe_existing_shares = rand_store.lock().get_all_shares_authors(round);
            if let Some(existing_shares) = maybe_existing_shares {
                let epoch = epoch_state.epoch;
                let request = RequestShare::new(metadata.clone());
                let targets = epoch_state
                    .verifier
                    .get_ordered_account_addresses_iter()
                    .filter(|author| !existing_shares.contains(author))
                    .collect::<Vec<_>>();
                info!(
                    epoch = epoch,
                    round = round,
                    "[RandManager] Start broadcasting share request for {}",
                    targets.len(),
                );
                rb.multicast(request, aggregate_state, targets)
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L301-301)
```rust
        tokio::spawn(Abortable::new(task, abort_registration));
```

**File:** consensus/src/rand/rand_gen/block_queue.rs (L21-21)
```rust
    broadcast_handle: Option<Vec<DropGuard>>,
```

**File:** consensus/src/rand/rand_gen/block_queue.rs (L118-136)
```rust
    pub fn dequeue_rand_ready_prefix(&mut self) -> Vec<OrderedBlocks> {
        let mut rand_ready_prefix = vec![];
        while let Some((_starting_round, item)) = self.queue.first_key_value() {
            if item.num_undecided() == 0 {
                let (_, item) = self.queue.pop_first().unwrap();
                for block in item.blocks() {
                    observe_block(block.timestamp_usecs(), BlockStage::RAND_READY);
                }
                let QueueItem { ordered_blocks, .. } = item;
                debug_assert!(ordered_blocks
                    .ordered_blocks
                    .iter()
                    .all(|block| block.has_randomness()));
                rand_ready_prefix.push(ordered_blocks);
            } else {
                break;
            }
        }
        rand_ready_prefix
```

**File:** consensus/src/pipeline/buffer_manager.rs (L906-910)
```rust
    fn need_back_pressure(&self) -> bool {
        const MAX_BACKLOG: Round = 20;

        self.back_pressure_enabled && self.highest_committed_round + MAX_BACKLOG < self.latest_round
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L938-938)
```rust
                Some(blocks) = self.block_rx.next(), if !self.need_back_pressure() => {
```
