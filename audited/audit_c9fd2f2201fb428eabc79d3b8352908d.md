# Audit Report

## Title
Consensus Pipeline Deadlock Due to Unhandled Execution Errors and Missing Retry Mechanism

## Summary
When the execution pipeline returns an `ExecutorError`, the error is logged but not programmatically handled, causing the affected block to remain stuck in the "Ordered" state indefinitely. The designed retry mechanism in `advance_execution_root()` returns a signal indicating retry is needed, but this return value is completely ignored in all call sites. This leads to a consensus liveness failure where no blocks can progress past the failed block until external intervention (state sync or epoch change) occurs.

## Finding Description

The vulnerability exists in the consensus execution pipeline's error handling path. When execution fails, the error context is preserved in logs but the programmatic handling silently drops the error, causing a deadlock.

**The vulnerable flow:**

1. **ExecutionWaitPhase receives error**: The `process()` method correctly wraps the error in `ExecutionResponse.inner` [1](#0-0) 

2. **BufferManager silently ignores error**: When `process_execution_response()` receives an `ExecutorError`, it logs the error but returns early without advancing the block state [2](#0-1) 

3. **Retry mechanism is broken**: The `advance_execution_root()` method detects when a block is stuck and returns `Some(block_id)` to signal retry is needed [3](#0-2) 

4. **Return value ignored**: All three call sites to `advance_execution_root()` completely ignore the return value [4](#0-3) 

**What causes ExecutorError:**
- `BlockNotFound`: Parent block missing from block tree [5](#0-4) 
- `CouldNotGetData`: Batch request timeout [6](#0-5) 
- `InternalError`: Various database, state view, or serialization errors [7](#0-6) 

**Why this breaks consensus:**
The block that failed execution remains in "Ordered" state and becomes the `execution_root`. Since the block never advances to "Executed" state, all subsequent blocks in the pipeline are blocked. The only recovery mechanisms are external resets (state sync or epoch boundaries) [8](#0-7) 

The error logging only records metrics and warnings, but the consensus layer has no programmatic awareness of the failure [9](#0-8) 

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:

1. **Validator node slowdowns/halt**: When execution fails, the consensus pipeline deadlocks. No new blocks can be committed, effectively halting consensus progress on the affected validator node.

2. **Significant protocol violations**: This violates the **Consensus Liveness** invariant - the protocol should make progress even when individual block executions fail. The silent failure prevents proper error propagation to the consensus layer.

3. **Network availability**: If multiple validators encounter this issue simultaneously (e.g., during state sync issues or network partitions causing batch timeouts), the entire network's ability to commit blocks is compromised until manual intervention.

While this does not cause a safety violation (no incorrect state transitions), the liveness failure is severe enough to require state sync intervention to recover, making this a High severity issue.

## Likelihood Explanation

**High Likelihood:**

1. **Normal operation triggers**: ExecutorError conditions can occur during legitimate operations:
   - State sync operations may cause `BlockNotFound` errors
   - Network delays can cause `CouldNotGetData` timeouts
   - Database errors can trigger `InternalError`

2. **No special permissions required**: Any condition that causes execution to fail will trigger this issue - no malicious validator or attacker action is required.

3. **Observable in production**: The error types defined (`BlockNotFound`, `CouldNotGetData`, `InternalError`) suggest these are anticipated error conditions that should be handled gracefully, not cause pipeline deadlocks.

4. **Design flaw**: The retry mechanism was clearly intended (as evidenced by the `advance_execution_root()` return value), but the implementation is incomplete - the return value is never used anywhere in the codebase [10](#0-9) 

## Recommendation

**Implement proper retry mechanism:**

1. **Use the retry signal**: When `advance_execution_root()` returns `Some(block_id)`, schedule a retry with exponential backoff:

```rust
// In buffer_manager.rs, modify the main event loop
Some(response) = self.execution_wait_phase_rx.next() => {
    monitor!("buffer_manager_process_execution_wait_response", {
        self.process_execution_response(response).await;
        if let Some(block_id) = self.advance_execution_root() {
            // Block is stuck, schedule retry
            let sender = self.execution_schedule_phase_tx.clone();
            let item = self.buffer.get(&self.execution_root);
            let request = self.create_new_request(ExecutionRequest {
                ordered_blocks: item.get_blocks().clone(),
            });
            Self::spawn_retry_request(sender, request, Duration::from_millis(500));
        }
        if self.signing_root.is_none() {
            self.advance_signing_root().await;
        }
    });
}
```

2. **Add retry limits**: Implement maximum retry attempts to prevent infinite loops on persistent failures.

3. **Escalate persistent failures**: After N retries, trigger a buffer reset or state sync to recover.

4. **Improve error propagation**: Consider adding error callbacks to notify the consensus layer of execution failures so it can make informed decisions.

## Proof of Concept

**Rust reproduction steps:**

1. Create a test that injects an `ExecutorError` into the execution pipeline:

```rust
#[test]
fn test_execution_error_deadlock() {
    // Setup buffer manager with test harness
    let (mut buffer_manager, block_tx, ...) = prepare_buffer_manager();
    
    // Send ordered blocks
    block_tx.send(OrderedBlocks { ... }).await;
    
    // Inject ExecutorError by replacing the execution future
    // with one that returns Err(ExecutorError::BlockNotFound(...))
    
    // Observe that:
    // 1. Error is logged
    // 2. Block remains in Ordered state
    // 3. execution_root doesn't advance
    // 4. Subsequent blocks cannot be processed
    // 5. No retry is attempted
    
    // Verify consensus is deadlocked until manual reset
}
```

2. Verify the return value from `advance_execution_root()` at line 957 is ignored by adding a debug print and confirming it returns `Some(block_id)` but no action is taken.

3. Confirm that only external reset (via `ResetRequest` with `ResetSignal::TargetRound` or `ResetSignal::Stop`) can clear the stuck block [11](#0-10) 

## Notes

This vulnerability represents a classic "error handling gap" where the infrastructure for recovery exists (the retry signal from `advance_execution_root()`) but was never wired up to actual retry logic. The error context is preserved in logs but lost from the programmatic control flow, causing silent failures from the consensus layer's perspective. This is a liveness vulnerability, not a safety vulnerability - it does not cause incorrect state transitions, but it does halt consensus progress requiring manual intervention.

### Citations

**File:** consensus/src/pipeline/execution_wait_phase.rs (L49-56)
```rust
    async fn process(&self, req: ExecutionWaitRequest) -> ExecutionResponse {
        let ExecutionWaitRequest { block_id, fut } = req;

        ExecutionResponse {
            block_id,
            inner: fut.await,
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L429-452)
```rust
    fn advance_execution_root(&mut self) -> Option<HashValue> {
        let cursor = self.execution_root;
        self.execution_root = self
            .buffer
            .find_elem_from(cursor.or_else(|| *self.buffer.head_cursor()), |item| {
                item.is_ordered()
            });
        if self.execution_root.is_some() && cursor == self.execution_root {
            // Schedule retry.
            self.execution_root
        } else {
            sample!(
                SampleRate::Frequency(2),
                info!(
                    "Advance execution root from {:?} to {:?}",
                    cursor, self.execution_root
                )
            );
            // Otherwise do nothing, because the execution wait phase is driven by the response of
            // the execution schedule phase, which is in turn fed as soon as the ordered blocks
            // come in.
            None
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L546-576)
```rust
    async fn reset(&mut self) {
        while let Some((_, block)) = self.pending_commit_blocks.pop_first() {
            // Those blocks don't have any dependencies, should be able to finish commit_ledger.
            // Abort them can cause error on epoch boundary.
            block.wait_for_commit_ledger().await;
        }
        while let Some(item) = self.buffer.pop_front() {
            for b in item.get_blocks() {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
        self.buffer = Buffer::new();
        self.execution_root = None;
        self.signing_root = None;
        self.previous_commit_time = Instant::now();
        self.commit_proof_rb_handle.take();
        // purge the incoming blocks queue
        while let Ok(Some(blocks)) = self.block_rx.try_next() {
            for b in blocks.ordered_blocks {
                if let Some(futs) = b.abort_pipeline() {
                    futs.wait_until_finishes().await;
                }
            }
        }
        // Wait for ongoing tasks to finish before sending back ack.
        while self.ongoing_tasks.load(Ordering::SeqCst) > 0 {
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L579-596)
```rust
    async fn process_reset_request(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        info!("Receive reset");

        match signal {
            ResetSignal::Stop => self.stop = true,
            ResetSignal::TargetRound(round) => {
                self.highest_committed_round = round;
                self.latest_round = round;

                let _ = self.drain_pending_commit_proof_till(round);
            },
        }

        self.reset().await;
        let _ = tx.send(ResetAck::default());
        info!("Reset finishes");
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L617-627)
```rust
        let executed_blocks = match inner {
            Ok(result) => result,
            Err(e) => {
                log_executor_error_occurred(
                    e,
                    &counters::BUFFER_MANAGER_RECEIVED_EXECUTOR_ERROR_COUNT,
                    block_id,
                );
                return;
            },
        };
```

**File:** consensus/src/pipeline/buffer_manager.rs (L943-943)
```rust
                        self.advance_execution_root();
```

**File:** consensus/src/pipeline/buffer_manager.rs (L954-960)
```rust
                Some(response) = self.execution_wait_phase_rx.next() => {
                    monitor!("buffer_manager_process_execution_wait_response", {
                    self.process_execution_response(response).await;
                    self.advance_execution_root();
                    if self.signing_root.is_none() {
                        self.advance_signing_root().await;
                    }});
```

**File:** execution/executor/src/block_executor/mod.rs (L206-209)
```rust
        let parent_block = block_vec
            .pop()
            .expect("Must exist.")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
```

**File:** execution/executor-types/src/error.rs (L32-33)
```rust
    #[error("Internal error: {:?}", error)]
    InternalError { error: String },
```

**File:** execution/executor-types/src/error.rs (L41-42)
```rust
    #[error("request timeout")]
    CouldNotGetData,
```

**File:** consensus/src/counters.rs (L1184-1210)
```rust
pub fn log_executor_error_occurred(
    e: ExecutorError,
    counter: &Lazy<IntCounterVec>,
    block_id: HashValue,
) {
    match e {
        ExecutorError::CouldNotGetData => {
            counter.with_label_values(&["CouldNotGetData"]).inc();
            warn!(
                block_id = block_id,
                "Execution error - CouldNotGetData {}", block_id
            );
        },
        ExecutorError::BlockNotFound(block_id) => {
            counter.with_label_values(&["BlockNotFound"]).inc();
            warn!(
                block_id = block_id,
                "Execution error BlockNotFound {}", block_id
            );
        },
        e => {
            counter.with_label_values(&["UnexpectedError"]).inc();
            warn!(
                block_id = block_id,
                "Execution error {:?} for {}", e, block_id
            );
        },
```
