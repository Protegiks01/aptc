# Audit Report

## Title
Stale Peer Monitoring Data Persists After Failed Refresh Operations, Enabling Malicious Peer Prioritization

## Summary
When peer monitoring refresh operations fail (network errors, timeouts, invalid responses), the cached monitoring data (network information, node information, and latency metrics) persists indefinitely without being cleared or marked as stale. This stale data continues to propagate to the consensus observer and mempool components, causing validators to incorrectly prioritize malicious or dead peers for critical operations.

## Finding Description

The peer monitoring service client maintains cached monitoring data for each peer in three state types: `NetworkInfoState`, `NodeInfoState`, and `LatencyInfoState`. When monitoring requests fail, the error handlers only record the failure count but critically fail to invalidate the cached data. [1](#0-0) [2](#0-1) [3](#0-2) 

In all three state types, the `handle_monitoring_service_response_error` function only calls `handle_request_failure()` which increments the failure counter, but the cached response data (`recorded_network_info_response`, `recorded_node_info_response`, `recorded_latency_ping_durations_secs`) remains unchanged.

The metadata updater loop continuously extracts this stale data and propagates it to the system-wide `PeersAndMetadata` structure: [4](#0-3) 

This stale data is then used by critical components for peer selection:

**1. Consensus Observer** - Uses stale distance and latency data for subscription peer selection: [5](#0-4) 

**2. Mempool** - Uses stale health, distance, and latency data for transaction forwarding prioritization: [6](#0-5) [7](#0-6) [8](#0-7) 

**Attack Scenario:**

1. Malicious peer connects and initially provides valid monitoring responses (low latency, close distance to validators, recent ledger timestamp)
2. Based on this good data, the validator selects the malicious peer as high-priority for consensus observation and mempool forwarding
3. Malicious peer stops responding to monitoring requests while maintaining the connection (all requests timeout or return errors)
4. The old "good" monitoring data persists in the peer state indefinitely
5. The metadata updater continues extracting and propagating this stale data
6. Consensus observer and mempool continue prioritizing the malicious peer based on stale data
7. Malicious peer can now withhold transactions, send invalid data, waste validator resources, or cause localized DoS

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program criteria:

- **State inconsistencies requiring intervention**: The peer monitoring system maintains stale state that doesn't reflect reality, requiring manual intervention or peer disconnection to resolve
- **Network inefficiency and resource wastage**: Validators continue sending transactions to unresponsive peers and subscribing to malicious consensus observers
- **Localized availability impact**: Individual validators can experience degraded performance, though not total network failure

While this doesn't directly cause fund loss or consensus safety violations, it enables significant operational disruption and can be weaponized by malicious actors to degrade validator performance and network health.

## Likelihood Explanation

**Likelihood: High**

This vulnerability is highly likely to be exploited because:

1. **No special permissions required**: Any network peer can trigger this by connecting and then stopping responses to monitoring requests
2. **Natural occurrence**: Network instability or temporary peer failures can accidentally trigger this condition, demonstrating it's not just a theoretical attack
3. **Long persistence window**: Once stale data is cached, it persists until the peer disconnects or successfully responds again
4. **No freshness validation**: The system has no TTL, timestamp checking, or staleness detection mechanism
5. **Silent failure**: The issue only produces warnings in logs but doesn't affect peer selection decisions

The request tracker does track consecutive failures, but this counter is never used to invalidate cached data or exclude peers from selection: [9](#0-8) 

## Recommendation

Implement a multi-layered approach to prevent stale data propagation:

**1. Clear cached data on failures**: Modify error handlers to clear or mark cached monitoring data as invalid after consecutive failures:

```rust
fn handle_monitoring_service_response_error(
    &mut self,
    peer_network_id: &PeerNetworkId,
    error: Error,
) {
    // Handle the failure
    self.handle_request_failure();
    
    // Clear cached data after threshold of consecutive failures
    let num_consecutive_failures = self.request_tracker.read().get_num_consecutive_failures();
    if num_consecutive_failures >= MAX_CONSECUTIVE_FAILURES_BEFORE_INVALIDATE {
        self.recorded_network_info_response = None;
    }
    
    // Log the error
    warn!(LogSchema::new(LogEntry::NetworkInfoRequest)
        .event(LogEvent::ResponseError)
        .message("Error encountered when requesting network information from the peer!")
        .peer(peer_network_id)
        .error(&error));
}
```

**2. Add timestamp-based freshness validation**: Store timestamps with cached data and validate freshness when extracting metadata:

```rust
pub struct NetworkInfoState {
    recorded_network_info_response: Option<NetworkInformationResponse>,
    last_successful_response_time: Option<Instant>,
    // ... other fields
}

pub fn get_latest_network_info_response(&self) -> Option<NetworkInformationResponse> {
    if let Some(last_response_time) = self.last_successful_response_time {
        let age = self.time_service.now().duration_since(last_response_time);
        if age > Duration::from_secs(MAX_RESPONSE_FRESHNESS_SECS) {
            return None; // Data too old
        }
    }
    self.recorded_network_info_response.clone()
}
```

**3. Implement peer exclusion based on failure threshold**: Use the consecutive failure counter to temporarily exclude unreliable peers from high-priority selection in consensus observer and mempool.

## Proof of Concept

The following Rust integration test demonstrates the vulnerability:

```rust
#[tokio::test]
async fn test_stale_data_persists_after_monitoring_failures() {
    // Setup: Create peer monitoring client and state
    let node_config = NodeConfig::default();
    let time_service = TimeService::mock();
    let mut peer_state = PeerState::new(node_config.clone(), time_service.clone());
    
    // Step 1: Peer provides good initial monitoring response
    let peer_network_id = PeerNetworkId::random();
    let good_network_info = NetworkInformationResponse {
        connected_peers: vec![],
        distance_from_validators: 0, // Close to validators
    };
    
    let peer_state_value = peer_state
        .get_peer_state_value(&PeerStateKey::NetworkInfo)
        .unwrap();
    peer_state_value.write().handle_monitoring_service_response(
        &peer_network_id,
        create_peer_metadata(),
        PeerMonitoringServiceRequest::GetNetworkInformation,
        PeerMonitoringServiceResponse::NetworkInformation(good_network_info.clone()),
        0.1, // Low latency
    );
    
    // Verify good data is cached
    let network_info_state = peer_state.get_network_info_state().unwrap();
    assert_eq!(
        network_info_state.get_latest_network_info_response(),
        Some(good_network_info.clone())
    );
    
    // Step 2: Simulate multiple monitoring request failures
    for _ in 0..10 {
        peer_state_value.write().handle_monitoring_service_response_error(
            &peer_network_id,
            Error::UnexpectedError("Network timeout".to_string()),
        );
    }
    
    // Step 3: Verify stale data still persists despite failures
    let network_info_state = peer_state.get_network_info_state().unwrap();
    let cached_data = network_info_state.get_latest_network_info_response();
    
    // VULNERABILITY: Stale good data still present after 10 consecutive failures
    assert_eq!(cached_data, Some(good_network_info));
    assert_eq!(cached_data.unwrap().distance_from_validators, 0);
    
    // Step 4: Extract metadata - stale data propagates
    let peer_monitoring_metadata = peer_state.extract_peer_monitoring_metadata().unwrap();
    let propagated_network_info = peer_monitoring_metadata.latest_network_info_response;
    
    // VULNERABILITY: Stale data propagates to metadata used for peer selection
    assert!(propagated_network_info.is_some());
    assert_eq!(propagated_network_info.unwrap().distance_from_validators, 0);
    
    // This stale data would cause consensus observer and mempool to incorrectly
    // prioritize this unresponsive peer for critical operations
}
```

## Notes

This vulnerability represents a fundamental design flaw in the peer monitoring service's error handling strategy. The system tracks failure counts but fails to act on them by invalidating stale cached data. The lack of timestamp-based freshness validation or TTL mechanisms means data can remain stale indefinitely as long as the peer connection is maintained.

The issue is particularly concerning because:
- It affects two critical components (consensus observer and mempool)
- It can occur naturally through network instability, not just malicious attacks
- There's no automatic recovery mechanism short of peer disconnection
- The TODO comment in latency monitoring indicates awareness of the problem but incomplete implementation [10](#0-9) 

The vulnerability can be mitigated by implementing the recommended freshness validation and data invalidation mechanisms, ensuring that peer selection decisions are based on recent, reliable monitoring data rather than potentially stale cached responses.

### Citations

**File:** peer-monitoring-service/client/src/peer_states/network_info.rs (L160-174)
```rust
    fn handle_monitoring_service_response_error(
        &mut self,
        peer_network_id: &PeerNetworkId,
        error: Error,
    ) {
        // Handle the failure
        self.handle_request_failure();

        // Log the error
        warn!(LogSchema::new(LogEntry::NetworkInfoRequest)
            .event(LogEvent::ResponseError)
            .message("Error encountered when requesting network information from the peer!")
            .peer(peer_network_id)
            .error(&error));
    }
```

**File:** peer-monitoring-service/client/src/peer_states/node_info.rs (L108-122)
```rust
    fn handle_monitoring_service_response_error(
        &mut self,
        peer_network_id: &PeerNetworkId,
        error: Error,
    ) {
        // Handle the failure
        self.handle_request_failure();

        // Log the error
        warn!(LogSchema::new(LogEntry::NodeInfoRequest)
            .event(LogEvent::ResponseError)
            .message("Error encountered when requesting node information from the peer!")
            .peer(peer_network_id)
            .error(&error));
    }
```

**File:** peer-monitoring-service/client/src/peer_states/latency_info.rs (L64-71)
```rust
        // TODO: If the number of ping failures is too high, disconnect from the node
        let num_consecutive_failures = self.request_tracker.read().get_num_consecutive_failures();
        if num_consecutive_failures >= self.latency_monitoring_config.max_latency_ping_failures {
            warn!(LogSchema::new(LogEntry::LatencyPing)
                .event(LogEvent::TooManyPingFailures)
                .peer(peer_network_id)
                .message("Too many ping failures occurred for the peer!"));
        }
```

**File:** peer-monitoring-service/client/src/peer_states/latency_info.rs (L197-211)
```rust
    fn handle_monitoring_service_response_error(
        &mut self,
        peer_network_id: &PeerNetworkId,
        error: Error,
    ) {
        // Handle the failure
        self.handle_request_failure(peer_network_id);

        // Log the error
        warn!(LogSchema::new(LogEntry::LatencyPing)
            .event(LogEvent::ResponseError)
            .message("Error encountered when pinging peer!")
            .peer(peer_network_id)
            .error(&error));
    }
```

**File:** peer-monitoring-service/client/src/lib.rs (L234-250)
```rust
                let peer_monitoring_metadata =
                    match peer_monitor_state.peer_states.read().get(&peer_network_id) {
                        Some(peer_state) => {
                            peer_state
                                .extract_peer_monitoring_metadata()
                                .unwrap_or_else(|error| {
                                    // Log the error and return the default
                                    warn!(LogSchema::new(LogEntry::MetadataUpdateLoop)
                                        .event(LogEvent::UnexpectedErrorEncountered)
                                        .peer(&peer_network_id)
                                        .error(&error));
                                    PeerMonitoringMetadata::default()
                                })
                        },
                        None => PeerMonitoringMetadata::default(), // Use the default
                    };

```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L283-312)
```rust
pub fn sort_peers_by_subscription_optimality(
    peers_and_metadata: &HashMap<PeerNetworkId, PeerMetadata>,
) -> Vec<PeerNetworkId> {
    // Group peers and latencies by validator distance, i.e., distance -> [(peer, latency)]
    let mut unsupported_peers = Vec::new();
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for (peer_network_id, peer_metadata) in peers_and_metadata {
        // Verify that the peer supports consensus observer
        if !supports_consensus_observer(peer_metadata) {
            unsupported_peers.push(*peer_network_id);
            continue; // Skip the peer
        }

        // Get the distance and latency for the peer
        let distance = get_distance_for_peer(peer_network_id, peer_metadata);
        let latency = get_latency_for_peer(peer_network_id, peer_metadata);

        // If the distance is not found, use the maximum distance
        let distance =
            distance.unwrap_or(aptos_peer_monitoring_service_types::MAX_DISTANCE_FROM_VALIDATORS);

        // If the latency is not found, use a large latency
        let latency = latency.unwrap_or(MAX_PING_LATENCY_SECS);

        // Add the peer and latency to the distance group
        peers_and_latencies_by_distance
            .entry(distance)
            .or_insert_with(Vec::new)
            .push((*peer_network_id, OrderedFloat(latency)));
    }
```

**File:** mempool/src/shared_mempool/priority.rs (L74-120)
```rust
    fn compare_intelligent(
        &self,
        peer_a: &(PeerNetworkId, Option<&PeerMonitoringMetadata>),
        peer_b: &(PeerNetworkId, Option<&PeerMonitoringMetadata>),
    ) -> Ordering {
        // Deconstruct the peer tuples
        let (peer_network_id_a, monitoring_metadata_a) = peer_a;
        let (peer_network_id_b, monitoring_metadata_b) = peer_b;

        // First, compare the peers by health (e.g., sync lag)
        let unhealthy_ordering = compare_peer_health(
            &self.mempool_config,
            &self.time_service,
            monitoring_metadata_a,
            monitoring_metadata_b,
        );
        if !unhealthy_ordering.is_eq() {
            return unhealthy_ordering; // Only return if it's not equal
        }

        // Next, compare by network ID (i.e., Validator > VFN > Public)
        let network_ordering = compare_network_id(
            &peer_network_id_a.network_id(),
            &peer_network_id_b.network_id(),
        );
        if !network_ordering.is_eq() {
            return network_ordering; // Only return if it's not equal
        }

        // Otherwise, compare by peer distance from the validators.
        // This avoids badly configured/connected peers (e.g., broken VN-VFN connections).
        let distance_ordering =
            compare_validator_distance(monitoring_metadata_a, monitoring_metadata_b);
        if !distance_ordering.is_eq() {
            return distance_ordering; // Only return if it's not equal
        }

        // Otherwise, compare by peer ping latency (the lower the better)
        let latency_ordering = compare_ping_latency(monitoring_metadata_a, monitoring_metadata_b);
        if !latency_ordering.is_eq() {
            return latency_ordering; // Only return if it's not equal
        }

        // Otherwise, simply hash the peer IDs and compare the hashes.
        // In practice, this should be relatively rare.
        self.compare_hash(peer_network_id_a, peer_network_id_b)
    }
```

**File:** mempool/src/shared_mempool/priority.rs (L562-589)
```rust
fn check_peer_metadata_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
) -> bool {
    monitoring_metadata
        .and_then(|metadata| {
            metadata
                .latest_node_info_response
                .as_ref()
                .map(|node_information_response| {
                    // Get the peer's ledger timestamp and the current timestamp
                    let peer_ledger_timestamp_usecs =
                        node_information_response.ledger_timestamp_usecs;
                    let current_timestamp_usecs = get_timestamp_now_usecs(time_service);

                    // Calculate the max sync lag before the peer is considered unhealthy (in microseconds)
                    let max_sync_lag_secs =
                        mempool_config.max_sync_lag_before_unhealthy_secs as u64;
                    let max_sync_lag_usecs = max_sync_lag_secs * MICROS_PER_SECOND;

                    // Determine if the peer is healthy
                    current_timestamp_usecs.saturating_sub(peer_ledger_timestamp_usecs)
                        < max_sync_lag_usecs
                })
        })
        .unwrap_or(false) // If metadata is missing, consider the peer unhealthy
}
```

**File:** mempool/src/shared_mempool/priority.rs (L615-639)
```rust
fn compare_validator_distance(
    monitoring_metadata_a: &Option<&PeerMonitoringMetadata>,
    monitoring_metadata_b: &Option<&PeerMonitoringMetadata>,
) -> Ordering {
    // Get the validator distance from the monitoring metadata
    let validator_distance_a = get_distance_from_validators(monitoring_metadata_a);
    let validator_distance_b = get_distance_from_validators(monitoring_metadata_b);

    // Compare the distances
    match (validator_distance_a, validator_distance_b) {
        (Some(validator_distance_a), Some(validator_distance_b)) => {
            // Prioritize the peer with the lowest validator distance
            validator_distance_a.cmp(&validator_distance_b).reverse()
        },
        (Some(_), None) => {
            Ordering::Greater // Prioritize the peer with a validator distance
        },
        (None, Some(_)) => {
            Ordering::Less // Prioritize the peer with a validator distance
        },
        (None, None) => {
            Ordering::Equal // Neither peer has a validator distance
        },
    }
}
```

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L101-104)
```rust
    /// Records a failure for the request
    pub fn record_response_failure(&mut self) {
        self.num_consecutive_request_failures += 1;
    }
```
