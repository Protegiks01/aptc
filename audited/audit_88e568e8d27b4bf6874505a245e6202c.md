# Audit Report

## Title
Race Condition in Remote State View Causes Permanent Executor Shard Liveness Failure

## Summary
A critical race condition exists in the remote executor service's state view management that can cause the executor shard to permanently lose liveness. When `handle_message()` processes stale key-value responses after `init_for_block()` has reset the state view, a panic occurs that prevents state values from being set, causing transaction execution threads to block indefinitely.

## Finding Description

The vulnerability exists in the interaction between three components in the remote state view implementation:

**The Race Condition:**

When a new block arrives, `init_for_block()` completely replaces the `RemoteStateView` by acquiring a write lock and creating a new instance, clearing all existing state keys: [1](#0-0) 

Meanwhile, the `RemoteStateValueReceiver` continuously receives key-value responses from the coordinator and spawns tasks on a rayon thread pool to process them: [2](#0-1) 

Each spawned task calls `handle_message()` which acquires a read lock and processes the response: [3](#0-2) 

The critical vulnerability occurs in `set_state_value()` which contains an unsafe `.unwrap()` that assumes the state key exists in the DashMap: [4](#0-3) 

**The Exploitation Flow:**

1. Block N is executing with state keys K1, K2, K3 requested via `init_for_block()`
2. Response messages M1, M2, M3 arrive from the coordinator and are queued in the rayon thread pool
3. Message M1 is processed successfully
4. Before M2 and M3 are processed, block N+1 arrives
5. `init_for_block()` is called for block N+1, which waits for any active `handle_message()` to release its read lock
6. Once acquired, the write lock allows `init_for_block()` to execute line 119: `*self.state_view.write().unwrap() = RemoteStateView::new()` - this completely replaces the DashMap, removing all keys from block N
7. Message M2 from block N finally gets scheduled by the rayon thread pool
8. `handle_message()` acquires a read lock and calls `set_state_value(&K2, value)`
9. `set_state_value()` executes `.get(state_key).unwrap()` but `.get(K2)` returns `None` because K2 was deleted when the state view was reset
10. The `.unwrap()` panics

**The Critical Impact:**

When the rayon thread pool task panics, it is caught internally by rayon and fails silently. The critical consequence is that `RemoteStateValue::set_value()` is never called for state key K2.

The `RemoteStateValue` synchronization mechanism uses a condition variable to block threads waiting for remote values: [5](#0-4) [6](#0-5) 

Any transaction execution thread that calls `get_value()` for the missing state key will block forever at the condition variable wait (line 33), since `cvar.notify_all()` is never invoked due to the panic.

This is invoked during transaction execution when the executor accesses state through the `RemoteStateViewClient`: [7](#0-6) 

**Liveness Invariant Broken:** The executor shard becomes permanently stuck and cannot process any more blocks, as transactions waiting for the missing state values will never complete.

## Impact Explanation

This vulnerability meets **Critical Severity** criteria per the Aptos bug bounty program under "Total loss of liveness/network availability":

- **Permanent executor shard failure**: The affected executor shard becomes unable to execute transactions and process blocks. Transaction execution threads block indefinitely waiting for state values that will never arrive.

- **Non-recoverable without manual intervention**: Once the race condition triggers, the executor shard is permanently stuck. All subsequent transactions that attempt to read the missing state values will hang forever on the condition variable.

- **Affects consensus participation**: In the sharded execution architecture shown in the production entry point, if one or more executor shards fail, the coordinator cannot complete block execution, preventing consensus from progressing. [8](#0-7) 

- **Cascading failure potential**: If multiple shards experience this race condition simultaneously under high load, the entire sharded execution system fails.

The impact is severe because:
1. The executor service runs as a separate process for sharded execution performance
2. Manual process restart is required to recover
3. The race can recur immediately after restart under the same load conditions
4. No automatic recovery, retry, or error handling mechanism exists in the code

## Likelihood Explanation

**HIGH Likelihood** - This race condition can occur naturally during normal blockchain operation without any attacker involvement:

**Triggering Conditions (All Normal):**
- High block production rate (blocks arriving every 1-2 seconds is standard)
- Network latency in key-value response delivery (normal in distributed systems)
- Multiple concurrent messages queued in the rayon thread pool (normal under load)
- Natural timing between block transitions

**How It Happens in Production:**

The execution flow shows that `init_for_block()` is called for every new block that arrives: [9](#0-8) 

Under normal load conditions:
1. Blocks arrive every 1-2 seconds on Aptos mainnet
2. State key-value responses from the coordinator can be delayed by hundreds of milliseconds due to network latency
3. The rayon thread pool may have multiple pending tasks waiting for CPU scheduling
4. When block N+1 arrives before all responses from block N are processed, the race condition naturally occurs

**No Attacker Required:**

This is a timing vulnerability inherent to the concurrent architecture. The likelihood increases with:
- Higher transaction throughput (more state keys per block)
- Network congestion or jitter (delays response processing)
- Greater number of state keys per block (more messages in flight)
- More shards in the execution environment (more concurrent activity)

The vulnerability is deterministic once the race condition timing aligns - the panic will occur with certainty when a stale message is processed after state reset.

## Recommendation

**Immediate Fix:** Add defensive programming to prevent the panic:

```rust
pub fn set_state_value(&self, state_key: &StateKey, state_value: Option<StateValue>) {
    if let Some(remote_value) = self.state_values.get(state_key) {
        remote_value.set_value(state_value);
    }
    // Silently ignore if key doesn't exist (stale message from previous block)
}
```

**Better Fix:** Implement proper task draining before state reset:

```rust
pub fn init_for_block(&self, state_keys: Vec<StateKey>) {
    // Wait for all pending handle_message tasks to complete
    self.thread_pool.install(|| {
        // This ensures all queued tasks are drained
    });
    
    *self.state_view.write().unwrap() = RemoteStateView::new();
    REMOTE_EXECUTOR_REMOTE_KV_COUNT
        .with_label_values(&[&self.shard_id.to_string(), "prefetch_kv"])
        .inc_by(state_keys.len() as u64);
    self.pre_fetch_state_values(state_keys, false);
}
```

**Best Fix:** Add block version tracking to reject stale messages:

```rust
struct RemoteStateView {
    state_values: DashMap<StateKey, RemoteStateValue>,
    block_version: AtomicU64,  // Add version tracking
}

// In init_for_block, increment version
// In handle_message, include and validate block version in messages
```

## Proof of Concept

While a complete PoC would require setting up the full remote executor infrastructure with network controllers and multiple shards, the race condition can be demonstrated conceptually:

```rust
// Thread 1: Processing message from block N
let state_view_lock = state_view.read().unwrap();
// ... context switch here ...

// Thread 2: New block N+1 arrives
*state_view.write().unwrap() = RemoteStateView::new(); // Clears all keys

// Thread 1: Resumes execution
state_view_lock.set_state_value(&stale_key, value); 
// PANIC: .unwrap() on None because stale_key was deleted
```

The vulnerability is evident from the code structure where:
1. No synchronization exists between the rayon task queue and `init_for_block()`
2. The `.unwrap()` assumes keys always exist
3. RwLock semantics allow the race condition to occur
4. No version tracking or staleness detection prevents processing old messages

### Citations

**File:** execution/executor-service/src/remote_state_view.rs (L44-49)
```rust
    pub fn set_state_value(&self, state_key: &StateKey, state_value: Option<StateValue>) {
        self.state_values
            .get(state_key)
            .unwrap()
            .set_value(state_value);
    }
```

**File:** execution/executor-service/src/remote_state_view.rs (L118-124)
```rust
    pub fn init_for_block(&self, state_keys: Vec<StateKey>) {
        *self.state_view.write().unwrap() = RemoteStateView::new();
        REMOTE_EXECUTOR_REMOTE_KV_COUNT
            .with_label_values(&[&self.shard_id.to_string(), "prefetch_kv"])
            .inc_by(state_keys.len() as u64);
        self.pre_fetch_state_values(state_keys, false);
    }
```

**File:** execution/executor-service/src/remote_state_view.rs (L186-204)
```rust
    fn get_state_value(&self, state_key: &StateKey) -> StateViewResult<Option<StateValue>> {
        let state_view_reader = self.state_view.read().unwrap();
        if state_view_reader.has_state_key(state_key) {
            // If the key is already in the cache then we return it.
            let _timer = REMOTE_EXECUTOR_TIMER
                .with_label_values(&[&self.shard_id.to_string(), "prefetch_wait"])
                .start_timer();
            return state_view_reader.get_state_value(state_key);
        }
        // If the value is not already in the cache then we pre-fetch it and wait for it to arrive.
        let _timer = REMOTE_EXECUTOR_TIMER
            .with_label_values(&[&self.shard_id.to_string(), "non_prefetch_wait"])
            .start_timer();
        REMOTE_EXECUTOR_REMOTE_KV_COUNT
            .with_label_values(&[&self.shard_id.to_string(), "non_prefetch_kv"])
            .inc();
        self.pre_fetch_state_values(vec![state_key.clone()], true);
        state_view_reader.get_state_value(state_key)
    }
```

**File:** execution/executor-service/src/remote_state_view.rs (L233-241)
```rust
    fn start(&self) {
        while let Ok(message) = self.kv_rx.recv() {
            let state_view = self.state_view.clone();
            let shard_id = self.shard_id;
            self.thread_pool.spawn(move || {
                Self::handle_message(shard_id, message, state_view);
            });
        }
    }
```

**File:** execution/executor-service/src/remote_state_view.rs (L243-272)
```rust
    fn handle_message(
        shard_id: ShardId,
        message: Message,
        state_view: Arc<RwLock<RemoteStateView>>,
    ) {
        let _timer = REMOTE_EXECUTOR_TIMER
            .with_label_values(&[&shard_id.to_string(), "kv_responses"])
            .start_timer();
        let bcs_deser_timer = REMOTE_EXECUTOR_TIMER
            .with_label_values(&[&shard_id.to_string(), "kv_resp_deser"])
            .start_timer();
        let response: RemoteKVResponse = bcs::from_bytes(&message.data).unwrap();
        drop(bcs_deser_timer);

        REMOTE_EXECUTOR_REMOTE_KV_COUNT
            .with_label_values(&[&shard_id.to_string(), "kv_responses"])
            .inc();
        let state_view_lock = state_view.read().unwrap();
        trace!(
            "Received state values for shard {} with size {}",
            shard_id,
            response.inner.len()
        );
        response
            .inner
            .into_iter()
            .for_each(|(state_key, state_value)| {
                state_view_lock.set_state_value(&state_key, state_value);
            });
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/remote_state_value.rs (L22-27)
```rust
    pub fn set_value(&self, value: Option<StateValue>) {
        let (lock, cvar) = &*self.value_condition;
        let mut status = lock.lock().unwrap();
        *status = RemoteValueStatus::Ready(value);
        cvar.notify_all();
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/remote_state_value.rs (L29-39)
```rust
    pub fn get_value(&self) -> Option<StateValue> {
        let (lock, cvar) = &*self.value_condition;
        let mut status = lock.lock().unwrap();
        while let RemoteValueStatus::Waiting = *status {
            status = cvar.wait(status).unwrap();
        }
        match &*status {
            RemoteValueStatus::Ready(value) => value.clone(),
            RemoteValueStatus::Waiting => unreachable!(),
        }
    }
```

**File:** execution/executor-service/src/main.rs (L27-48)
```rust
fn main() {
    let args = Args::parse();
    aptos_logger::Logger::new().init();

    let (tx, rx) = crossbeam_channel::unbounded();
    ctrlc::set_handler(move || {
        tx.send(()).unwrap();
    })
    .expect("Error setting Ctrl-C handler");

    let _exe_service = ProcessExecutorService::new(
        args.shard_id,
        args.num_shards,
        args.num_executor_threads,
        args.coordinator_address,
        args.remote_executor_addresses,
    );

    rx.recv()
        .expect("Could not receive Ctrl-C msg from channel.");
    info!("Process executor service shutdown successfully.");
}
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L93-99)
```rust
                    RemoteExecutionRequest::ExecuteBlock(command) => {
                        let init_prefetch_timer = REMOTE_EXECUTOR_TIMER
                            .with_label_values(&[&self.shard_id.to_string(), "init_prefetch"])
                            .start_timer();
                        let state_keys = Self::extract_state_keys(&command);
                        self.state_view_client.init_for_block(state_keys);
                        drop(init_prefetch_timer);
```
