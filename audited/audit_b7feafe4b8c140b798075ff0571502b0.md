# Audit Report

## Title
Missing Identity Element Validation in PVSS Transcript Allows Malicious Dealer to Break Randomness Generation

## Summary
The `sig_mpk_g2` field in `EncryptionKey` is derived from PVSS transcripts without validation that it is not the identity element. A malicious dealer can create a valid PVSS transcript with the dealt public key set to the identity element in G2, causing complete failure of randomness generation and consensus liveness for an entire epoch.

## Finding Description

The vulnerability exists in the PVSS transcript verification flow where the dealt public key (`V0` in G2) is never validated to ensure it is not the identity element.

**Attack Flow:**

1. A malicious validator acting as DKG dealer creates a PVSS transcript with `V0 = identity` in G2 [1](#0-0) 

2. The transcript verification performs cryptographic checks (Proof of Knowledge, range proofs, Low Degree Test, pairing checks) but none explicitly reject the identity element as a valid point [2](#0-1) 

3. The malicious transcript passes on-chain verification and is accepted [3](#0-2) 

4. During epoch setup, `FPTXWeighted::setup` extracts the identity element as `sig_mpk_g2` in the EncryptionKey [4](#0-3) 

5. The EncryptionKey with identity `sig_mpk_g2` is used across all validators for secret sharing [5](#0-4) 

6. Decryption key verification fails because `pairing(digest + hash(identity), identity) = 1_GT`, which only holds if the signature is also the identity element - an invalid decryption key [6](#0-5) [7](#0-6) 

**Root Cause:**

The `EncryptionKey::new` constructor accepts any `G2Affine` point without validation [8](#0-7) 

The arkworks deserialization validates curve membership but explicitly allows the identity element, as demonstrated in tests [9](#0-8) 

## Impact Explanation

**Severity: Critical**

This vulnerability causes **total loss of liveness/network availability** for randomness-dependent consensus operations:

- All validators receive the same broken `EncryptionKey` with identity `sig_mpk_g2`
- Secret share verification fails for all legitimate decryption keys
- Randomness generation becomes non-functional for the entire epoch
- Leader selection and other randomness-dependent consensus mechanisms fail
- The network cannot produce randomness until the next DKG completes (if it ever does)
- Recovery requires manual intervention or a hard fork to override the malicious transcript

This qualifies as **Critical Severity** under Aptos bug bounty criteria: "Total loss of liveness/network availability" and "Non-recoverable network partition (requires hardfork)".

## Likelihood Explanation

**Likelihood: Medium-High**

The attack requires:
- Single malicious validator with dealer privileges (within Byzantine fault tolerance assumptions)
- Ability to create and submit a DKG transcript during epoch transition
- No collusion or majority stake required

**Factors increasing likelihood:**
- No explicit validation prevents this attack
- The identity element is a mathematically valid curve point that passes all existing checks
- A single Byzantine validator (assumed in BFT model) can execute this attack

**Factors decreasing likelihood:**
- Requires validator-level access (but Byzantine validators are assumed in BFT)
- Attack is detectable if transcript content is monitored
- May be caught during aggregation if other dealers' transcripts differ

## Recommendation

Add explicit validation to reject identity elements and low-order points in public keys:

**Option 1: Validate in EncryptionKey constructor**
```rust
impl EncryptionKey {
    pub fn new(sig_mpk_g2: G2Affine, tau_g2: G2Affine) -> Result<Self> {
        // Reject identity element
        if sig_mpk_g2.is_zero() {
            return Err(anyhow::anyhow!("sig_mpk_g2 cannot be identity element"));
        }
        if tau_g2.is_zero() {
            return Err(anyhow::anyhow!("tau_g2 cannot be identity element"));
        }
        Ok(Self { sig_mpk_g2, tau_g2 })
    }
}
```

**Option 2: Validate in PVSS transcript verification** [10](#0-9) 

Add validation after line 214:
```rust
// Validate dealt public key is not identity
if self.subtrs.V0.is_zero() {
    bail!("Dealt public key cannot be identity element");
}
```

**Recommended approach:** Implement both validations for defense in depth.

## Proof of Concept

```rust
#[cfg(test)]
mod test_identity_attack {
    use super::*;
    use ark_ec::AffineRepr;
    
    #[test]
    fn test_identity_encryption_key_breaks_verification() {
        // Create EncryptionKey with identity sig_mpk_g2
        let identity_mpk = G2Affine::zero();
        let tau_g2 = G2Affine::generator();
        
        // This should fail but currently succeeds
        let ek = EncryptionKey::new(identity_mpk, tau_g2);
        
        // Create a valid digest
        let mut rng = thread_rng();
        let digest = Digest::new_for_testing(&mut rng);
        
        // Create any decryption key
        let dk = BIBEDecryptionKey {
            signature_g1: G1Affine::generator(),
        };
        
        // Verification will fail because:
        // pairing(digest + hash(identity), identity) = 1_GT
        // pairing(generator, generator) != 1_GT
        let result = ek.verify_decryption_key(&digest, &dk);
        
        // This demonstrates that verification fails for all valid keys
        assert!(result.is_err());
    }
}
```

**Notes**

- This vulnerability breaks the "Cryptographic Correctness" invariant requiring secure cryptographic operations
- The attack is within Byzantine fault tolerance assumptions (single malicious validator)
- The lack of identity element validation violates standard cryptographic best practices
- Recovery requires epoch rollover or manual intervention, constituting a critical liveness failure

### Citations

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs (L78-91)
```rust
pub struct Subtranscript<E: Pairing> {
    // The dealt public key
    #[serde(serialize_with = "ark_se", deserialize_with = "ark_de")]
    pub V0: E::G2,
    // The dealt public key shares
    #[serde(serialize_with = "ark_se", deserialize_with = "ark_de")]
    pub Vs: Vec<Vec<E::G2>>,
    /// First chunked ElGamal component: C[i][j] = s_{i,j} * G + r_j * ek_i. Here s_i = \sum_j s_{i,j} * B^j // TODO: change notation because B is not a group element?
    #[serde(serialize_with = "ark_se", deserialize_with = "ark_de")]
    pub Cs: Vec<Vec<Vec<E::G1>>>, // TODO: maybe make this and the other fields affine? The verifier will have to do it anyway... and we are trying to speed that up
    /// Second chunked ElGamal component: R[j] = r_j * H
    #[serde(serialize_with = "ark_se", deserialize_with = "ark_de")]
    pub Rs: Vec<Vec<E::G1>>,
}
```

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs (L125-286)
```rust
    fn verify<A: Serialize + Clone>(
        &self,
        sc: &Self::SecretSharingConfig,
        pp: &Self::PublicParameters,
        spks: &[Self::SigningPubKey],
        eks: &[Self::EncryptPubKey],
        sid: &A,
    ) -> anyhow::Result<()> {
        if eks.len() != sc.get_total_num_players() {
            bail!(
                "Expected {} encryption keys, but got {}",
                sc.get_total_num_players(),
                eks.len()
            );
        }
        if self.subtrs.Cs.len() != sc.get_total_num_players() {
            bail!(
                "Expected {} arrays of chunked ciphertexts, but got {}",
                sc.get_total_num_players(),
                self.subtrs.Cs.len()
            );
        }
        if self.subtrs.Vs.len() != sc.get_total_num_players() {
            bail!(
                "Expected {} arrays of commitment elements, but got {}",
                sc.get_total_num_players(),
                self.subtrs.Vs.len()
            );
        }

        // Initialize the **identical** PVSS SoK context
        let sok_cntxt = (
            &spks[self.dealer.id],
            sid.clone(),
            self.dealer.id,
            DST.to_vec(),
        ); // As above, this is a bit hacky... though we have access to `self` now

        {
            // Verify the PoK
            let eks_inner: Vec<_> = eks.iter().map(|ek| ek.ek).collect();
            let lagr_g1: &[E::G1Affine] = match &pp.pk_range_proof.ck_S.msm_basis {
                SrsBasis::Lagrange { lagr: lagr_g1 } => lagr_g1,
                SrsBasis::PowersOfTau { .. } => {
                    bail!("Expected a Lagrange basis, received powers of tau basis instead")
                },
            };
            let hom = hkzg_chunked_elgamal::WeightedHomomorphism::<E>::new(
                lagr_g1,
                pp.pk_range_proof.ck_S.xi_1,
                &pp.pp_elgamal,
                &eks_inner,
            );
            if let Err(err) = hom.verify(
                &TupleCodomainShape(
                    self.sharing_proof.range_proof_commitment.clone(),
                    chunked_elgamal::WeightedCodomainShape {
                        chunks: self.subtrs.Cs.clone(),
                        randomness: self.subtrs.Rs.clone(),
                    },
                ),
                &self.sharing_proof.SoK,
                &sok_cntxt,
            ) {
                bail!("PoK verification failed: {:?}", err);
            }

            // Verify the range proof
            if let Err(err) = self.sharing_proof.range_proof.verify(
                &pp.pk_range_proof.vk,
                sc.get_total_weight() * num_chunks_per_scalar::<E::ScalarField>(pp.ell) as usize,
                pp.ell as usize,
                &self.sharing_proof.range_proof_commitment,
            ) {
                bail!("Range proof batch verification failed: {:?}", err);
            }
        }

        let mut rng = rand::thread_rng(); // TODO: make `rng` a parameter of fn verify()?

        // Do the SCRAPE LDT
        let ldt = LowDegreeTest::random(
            &mut rng,
            sc.get_threshold_weight(),
            sc.get_total_weight() + 1,
            true,
            &sc.get_threshold_config().domain,
        ); // includes_zero is true here means it includes a commitment to f(0), which is in V[n]
        let mut Vs_flat: Vec<_> = self.subtrs.Vs.iter().flatten().cloned().collect();
        Vs_flat.push(self.subtrs.V0);
        // could add an assert_eq here with sc.get_total_weight()
        ldt.low_degree_test_group(&Vs_flat)?;

        // let eks_inner: Vec<_> = eks.iter().map(|ek| ek.ek).collect();
        // let hom = hkzg_chunked_elgamal::WeightedHomomorphism::new(
        //     &pp.pk_range_proof.ck_S.lagr_g1,
        //     pp.pk_range_proof.ck_S.xi_1,
        //     &pp.pp_elgamal,
        //     &eks_inner,
        // );
        // let (sigma_bases, sigma_scalars, beta_powers) = hom.verify_msm_terms(
        //         &TupleCodomainShape(
        //             self.sharing_proof.range_proof_commitment.clone(),
        //             chunked_elgamal::WeightedCodomainShape {
        //                 chunks: self.subtrs.Cs.clone(),
        //                 randomness: self.subtrs.Rs.clone(),
        //             },
        //         ),
        //         &self.sharing_proof.SoK,
        //         &sok_cntxt,
        //     );
        // let ldt_msm_terms = ldt.ldt_msm_input(&Vs_flat)?;
        // use aptos_crypto::arkworks::msm::verify_msm_terms_with_start;
        // verify_msm_terms_with_start(ldt_msm_terms, sigma_bases, sigma_scalars, beta_powers);

        // Now compute the final MSM // TODO: merge this multi_exp with the PoK verification, as in YOLO YOSO? // TODO2: and use the iterate stuff you developed? it's being forgotten here
        let mut base_vec = Vec::new();
        let mut exp_vec = Vec::new();

        let beta = sample_field_element(&mut rng);
        let powers_of_beta = utils::powers(beta, sc.get_total_weight() + 1);

        let Cs_flat: Vec<_> = self.subtrs.Cs.iter().flatten().cloned().collect();
        assert_eq!(
            Cs_flat.len(),
            sc.get_total_weight(),
            "Number of ciphertexts does not equal number of weights"
        ); // TODO what if zero weight?
           // could add an assert_eq here with sc.get_total_weight()

        for i in 0..Cs_flat.len() {
            for j in 0..Cs_flat[i].len() {
                let base = Cs_flat[i][j];
                let exp = pp.powers_of_radix[j] * powers_of_beta[i];
                base_vec.push(base);
                exp_vec.push(exp);
            }
        }

        let weighted_Cs = E::G1::msm(&E::G1::normalize_batch(&base_vec), &exp_vec)
            .expect("Failed to compute MSM of Cs in chunky");

        let weighted_Vs = E::G2::msm(
            &E::G2::normalize_batch(&Vs_flat[..sc.get_total_weight()]), // Don't use the last entry of `Vs_flat`
            &powers_of_beta[..sc.get_total_weight()],
        )
        .expect("Failed to compute MSM of Vs in chunky");

        let res = E::multi_pairing(
            [
                weighted_Cs.into_affine(),
                *pp.get_encryption_public_params().message_base(),
            ],
            [pp.get_commitment_base(), (-weighted_Vs).into_affine()],
        ); // Making things affine here rather than converting the two bases to group elements, since that's probably what they would be converted to anyway: https://github.com/arkworks-rs/algebra/blob/c1f4f5665504154a9de2345f464b0b3da72c28ec/ec/src/models/bls12/g1.rs#L14

        if PairingOutput::<E>::ZERO != res {
            return Err(anyhow::anyhow!("Expected zero during multi-pairing check"));
        }

        Ok(())
    }
```

**File:** crates/aptos-dkg/src/pvss/chunky/weighted_transcript.rs (L313-315)
```rust
    fn get_dealt_public_key(&self) -> Self::DealtPubKey {
        Self::DealtPubKey::new(self.V0.into_affine())
    }
```

**File:** types/src/dkg/real_dkg/mod.rs (L332-400)
```rust
    fn verify_transcript(
        params: &Self::PublicParams,
        trx: &Self::Transcript,
    ) -> anyhow::Result<()> {
        // Verify dealer indices are valid.
        let dealers = trx
            .main
            .get_dealers()
            .iter()
            .map(|player| player.id)
            .collect::<Vec<usize>>();
        let num_validators = params.session_metadata.dealer_validator_set.len();
        ensure!(
            dealers.iter().all(|id| *id < num_validators),
            "real_dkg::verify_transcript failed with invalid dealer index."
        );

        let all_eks = params.pvss_config.eks.clone();

        let addresses = params.verifier.get_ordered_account_addresses();
        let dealers_addresses = dealers
            .iter()
            .filter_map(|&pos| addresses.get(pos))
            .cloned()
            .collect::<Vec<_>>();

        let spks = dealers_addresses
            .iter()
            .filter_map(|author| params.verifier.get_public_key(author))
            .collect::<Vec<_>>();

        let aux = dealers_addresses
            .iter()
            .map(|address| (params.pvss_config.epoch, address))
            .collect::<Vec<_>>();

        trx.main.verify(
            &params.pvss_config.wconfig,
            &params.pvss_config.pp,
            &spks,
            &all_eks,
            &aux,
        )?;

        // Verify fast path is present if and only if fast_wconfig is present.
        ensure!(
            trx.fast.is_some() == params.pvss_config.fast_wconfig.is_some(),
            "real_dkg::verify_transcript failed with mismatched fast path flag in trx and params."
        );

        if let Some(fast_trx) = trx.fast.as_ref() {
            let fast_dealers = fast_trx
                .get_dealers()
                .iter()
                .map(|player| player.id)
                .collect::<Vec<usize>>();
            ensure!(
                dealers == fast_dealers,
                "real_dkg::verify_transcript failed with inconsistent dealer index."
            );
        }

        if let (Some(fast_trx), Some(fast_wconfig)) =
            (trx.fast.as_ref(), params.pvss_config.fast_wconfig.as_ref())
        {
            fast_trx.verify(fast_wconfig, &params.pvss_config.pp, &spks, &all_eks, &aux)?;
        }

        Ok(())
```

**File:** crates/aptos-batch-encryption/src/schemes/fptx_weighted.rs (L241-243)
```rust
        let mpk_g2: G2Affine = subtranscript.get_dealt_public_key().as_g2();

        let ek = EncryptionKey::new(mpk_g2, digest_key.tau_g2);
```

**File:** crates/aptos-batch-encryption/src/shared/encryption_key.rs (L14-34)
```rust
#[derive(Clone, Debug, Serialize, Deserialize, PartialEq, Eq)]
pub struct EncryptionKey {
    #[serde(serialize_with = "ark_se", deserialize_with = "ark_de")]
    pub(crate) sig_mpk_g2: G2Affine,
    #[serde(serialize_with = "ark_se", deserialize_with = "ark_de")]
    pub(crate) tau_g2: G2Affine,
}

impl EncryptionKey {
    pub fn new(sig_mpk_g2: G2Affine, tau_g2: G2Affine) -> Self {
        Self { sig_mpk_g2, tau_g2 }
    }

    pub fn verify_decryption_key(
        &self,
        digest: &Digest,
        decryption_key: &BIBEDecryptionKey,
    ) -> Result<()> {
        BIBEMasterPublicKey(self.sig_mpk_g2).verify_decryption_key(digest, decryption_key)
    }
}
```

**File:** crates/aptos-batch-encryption/src/shared/key_derivation.rs (L118-133)
```rust
fn verify_bls(
    verification_key_g2: G2Affine,
    digest: &Digest,
    offset: G2Affine,
    signature: G1Affine,
) -> Result<()> {
    let hashed_offset: G1Affine = symmetric::hash_g2_element(offset)?;

    if PairingSetting::pairing(digest.as_g1() + hashed_offset, verification_key_g2)
        == PairingSetting::pairing(signature, G2Affine::generator())
    {
        Ok(())
    } else {
        Err(anyhow::anyhow!("bls verification error"))
    }
}
```

**File:** crates/aptos-batch-encryption/src/shared/key_derivation.rs (L153-164)
```rust
impl BIBEMasterPublicKey {
    pub fn verify_decryption_key(
        &self,
        digest: &Digest,
        decryption_key: &BIBEDecryptionKey,
    ) -> Result<()> {
        verify_bls(self.0, digest, self.0, decryption_key.signature_g1)
            .map_err(|_| BatchEncryptionError::DecryptionKeyVerifyError)?;

        Ok(())
    }
}
```

**File:** crates/aptos-crypto/src/arkworks/serialization.rs (L68-107)
```rust
    fn test_g1_serialization_multiple_points() {
        #[derive(Serialize, Deserialize, PartialEq, Debug)]
        struct A(#[serde(serialize_with = "ark_se", deserialize_with = "ark_de")] G1Affine);

        let mut points = vec![G1Affine::zero()]; // Include zero
        let mut g = G1Projective::generator();

        for _ in 0..MAX_DOUBLINGS {
            points.push(g.into());
            g += g; // double for next
        }

        for p in points {
            let serialized = bcs::to_bytes(&A(p)).expect("Serialization failed");
            let deserialized: A = bcs::from_bytes(&serialized).expect("Deserialization failed");

            assert_eq!(deserialized.0, p, "G1 point round-trip failed for {:?}", p);
        }
    }

    #[test]
    fn test_g2_serialization_multiple_points() {
        #[derive(Serialize, Deserialize, PartialEq, Debug)]
        struct A(#[serde(serialize_with = "ark_se", deserialize_with = "ark_de")] G2Affine);

        let mut points = vec![G2Affine::zero()]; // Include zero
        let mut g = G2Projective::generator();

        for _ in 0..MAX_DOUBLINGS {
            points.push(g.into());
            g += g; // double for next
        }

        for p in points {
            let serialized = bcs::to_bytes(&A(p)).expect("Serialization failed");
            let deserialized: A = bcs::from_bytes(&serialized).expect("Deserialization failed");

            assert_eq!(deserialized.0, p, "G2 point round-trip failed for {:?}", p);
        }
    }
```
