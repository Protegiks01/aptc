# Audit Report

## Title
Storage Start Divergence Causing Network Split During Epoch Transitions

## Summary
During epoch transitions, `storage.start()` can return `FullRecoveryData` for some validators and `PartialRecoveryData` for others based on their local ConsensusDB state. This causes validators to enter different operating modes (RoundManager vs RecoveryManager), leading to a network split where validators in recovery mode cannot participate in consensus. If validators representing >1/3 voting power enter recovery mode, consensus halts completely. [1](#0-0) 

## Finding Description

The vulnerability occurs in the epoch initialization flow where validators start consensus for a new epoch. The critical path is:

**Step 1: Epoch Transition Initiation**
When validators receive an epoch change proof, they sync their AptosDB to the new epoch's ledger info but do NOT clear or sync ConsensusDB: [2](#0-1) 

**Step 2: Storage Start Divergence**
Each validator then calls `storage.start()` which attempts to reconstruct consensus state from ConsensusDB: [3](#0-2) 

The critical branching occurs based on whether `RecoveryData::new()` succeeds or fails: [4](#0-3) 

**Step 3: RecoveryData Construction Failures**
`RecoveryData::new()` calls `find_root()` which can fail in multiple scenarios: [5](#0-4) 

The `find_root_with_window()` function fails when:
- Commit block from ledger info is not found in ConsensusDB blocks (line 137)
- No quorum certificate exists for the commit block (line 142)
- Parent blocks are missing during window traversal (line 178) [6](#0-5) 

**Step 4: Network Split Manifestation**
Validators that successfully construct RecoveryData start RoundManager and participate in consensus. Validators that fail start RecoveryManager, which does NOT participate in consensus and instead tries to sync from peers: [7](#0-6) 

Critically, RecoveryManager exits the entire process upon successful recovery, requiring manual restart: [8](#0-7) 

**Root Cause:**
ConsensusDB state is not synchronized between validators during epoch transitions. Different validators can have different blocks/QCs in ConsensusDB due to:
- Network partitions during block propagation
- Timing differences in crash/restart cycles
- Asynchronous block persistence
- Different pruning states

## Impact Explanation

**Critical Severity** - Meets "Total loss of liveness/network availability" and "Non-recoverable network partition" criteria:

1. **Consensus Halt**: If validators representing ≥34% voting power enter RecoveryManager mode, the remaining validators cannot reach the 67% quorum threshold, causing complete consensus halt.

2. **Reduced Byzantine Fault Tolerance**: Even if <34% enter recovery mode, the network operates with reduced validator participation, lowering safety margins against Byzantine behavior.

3. **Manual Intervention Required**: Validators in RecoveryManager mode exit the process after recovery and require manual restart, creating extended periods of reduced participation.

4. **Non-Deterministic Failure**: The vulnerability is triggered by natural network conditions, not malicious actors, making it unpredictable and potentially catastrophic.

5. **Violation of Safety Invariant**: Breaks the fundamental consensus invariant that the system must maintain liveness and safety under <1/3 Byzantine failures.

## Likelihood Explanation

**High Likelihood** - This vulnerability will naturally occur during epoch transitions under common network conditions:

1. **Regular Trigger**: Epoch transitions happen periodically (every few hours/days), providing regular opportunities for the issue to manifest.

2. **Common Preconditions**: Network partitions, timing variations, and asynchronous block propagation are normal in distributed systems.

3. **No Attacker Required**: The vulnerability is triggered by normal protocol operation under realistic network conditions, not malicious behavior.

4. **Cumulative Risk**: Each epoch transition carries risk. With multiple transitions over time, the probability of divergence approaches certainty.

**Realistic Scenario:**
- Network partition occurs during epoch E causing validators V1,V2 to advance while V3,V4 lag
- Epoch-ending block is produced at round R by V1,V2
- V3,V4 eventually receive epoch change proof and sync AptosDB via state sync
- V1,V2 have blocks up to round R in ConsensusDB → FullRecoveryData → RoundManager
- V3,V4 only have blocks up to round R-10 in ConsensusDB → find_root fails → PartialRecoveryData → RecoveryManager
- Network split occurs

## Recommendation

Implement ConsensusDB synchronization or clearing during epoch transitions to ensure all validators start with consistent state:

**Option 1: Clear ConsensusDB on Epoch Transition**
```rust
async fn initiate_new_epoch(&mut self, proof: EpochChangeProof) -> anyhow::Result<()> {
    let ledger_info = proof.verify(self.epoch_state())?;
    
    self.shutdown_current_processor().await;
    *self.pending_blocks.lock() = PendingBlocks::new();
    
    // ADDED: Clear ConsensusDB to ensure clean state
    self.storage.consensus_db().clear_all_data()
        .context("Failed to clear ConsensusDB")?;
    
    self.execution_client.sync_to_target(ledger_info.clone()).await?;
    monitor!("reconfig", self.await_reconfig_notification().await);
    Ok(())
}
```

**Option 2: Fallback to Recovery Mode for All Validators on Divergence**
If `storage.start()` returns `PartialRecoveryData`, broadcast this state to other validators and coordinate a network-wide recovery. This ensures all validators either start normally or all enter recovery mode together.

**Option 3: Enhanced Validation Before Starting**
Before calling `storage.start()`, verify that ConsensusDB contains blocks matching the latest committed ledger info. If not, proactively enter recovery mode or clear ConsensusDB.

## Proof of Concept

```rust
// Reproduction scenario demonstrating the vulnerability
// This can be added as a test in consensus/src/epoch_manager.rs

#[tokio::test]
async fn test_storage_start_divergence_network_split() {
    // Setup: 4 validators with equal voting power
    let validators = create_validators(4);
    
    // Simulate epoch E running to round 100
    // All validators have blocks 90-100 in ConsensusDB
    
    // Create network partition
    // V1, V2 continue to round 110
    // V3, V4 stay at round 100 (partitioned)
    
    // Epoch change occurs at round 110
    let epoch_change_block = create_epoch_ending_block(110);
    
    // V1, V2 have blocks 90-110 in ConsensusDB
    let v1_consensus_db = mock_consensus_db_with_blocks(90..=110);
    let v2_consensus_db = mock_consensus_db_with_blocks(90..=110);
    
    // V3, V4 only have blocks 90-100 in ConsensusDB
    let v3_consensus_db = mock_consensus_db_with_blocks(90..=100);
    let v4_consensus_db = mock_consensus_db_with_blocks(90..=100);
    
    // All validators sync AptosDB to round 110 via state sync
    let aptos_db = mock_aptos_db_at_round(110);
    
    // Start new epoch for each validator
    let v1_storage = StorageWriteProxy::new_with_dbs(v1_consensus_db, aptos_db.clone());
    let v2_storage = StorageWriteProxy::new_with_dbs(v2_consensus_db, aptos_db.clone());
    let v3_storage = StorageWriteProxy::new_with_dbs(v3_consensus_db, aptos_db.clone());
    let v4_storage = StorageWriteProxy::new_with_dbs(v4_consensus_db, aptos_db.clone());
    
    // Call storage.start() for each validator
    let v1_result = v1_storage.start(true, Some(10));
    let v2_result = v2_storage.start(true, Some(10));
    let v3_result = v3_storage.start(true, Some(10));
    let v4_result = v4_storage.start(true, Some(10));
    
    // ASSERTION: V1 and V2 get FullRecoveryData
    assert!(matches!(v1_result, LivenessStorageData::FullRecoveryData(_)));
    assert!(matches!(v2_result, LivenessStorageData::FullRecoveryData(_)));
    
    // ASSERTION: V3 and V4 get PartialRecoveryData (DIVERGENCE!)
    assert!(matches!(v3_result, LivenessStorageData::PartialRecoveryData(_)));
    assert!(matches!(v4_result, LivenessStorageData::PartialRecoveryData(_)));
    
    // VULNERABILITY CONFIRMED:
    // - V1, V2 (50% voting power) will start RoundManager
    // - V3, V4 (50% voting power) will start RecoveryManager
    // - 50% < 67% quorum threshold
    // - CONSENSUS HALTS
}
```

## Notes

This vulnerability represents a fundamental flaw in the epoch transition protocol where local storage state divergence leads to operational mode divergence. The issue is exacerbated by:

1. **No validation** that ConsensusDB state matches AptosDB state before starting consensus
2. **No synchronization** of ConsensusDB between validators during epoch transitions
3. **Silent fallback** to recovery mode without coordinating with other validators
4. **Process exit** requirement after recovery, extending the window of reduced participation

The vulnerability affects all Aptos networks and can manifest during any epoch transition under realistic network conditions. It violates the fundamental liveness guarantee of BFT consensus protocols and requires immediate remediation.

### Citations

**File:** consensus/src/epoch_manager.rs (L544-569)
```rust
    async fn initiate_new_epoch(&mut self, proof: EpochChangeProof) -> anyhow::Result<()> {
        let ledger_info = proof
            .verify(self.epoch_state())
            .context("[EpochManager] Invalid EpochChangeProof")?;
        info!(
            LogSchema::new(LogEvent::NewEpoch).epoch(ledger_info.ledger_info().next_block_epoch()),
            "Received verified epoch change",
        );

        // shutdown existing processor first to avoid race condition with state sync.
        self.shutdown_current_processor().await;
        *self.pending_blocks.lock() = PendingBlocks::new();
        // make sure storage is on this ledger_info too, it should be no-op if it's already committed
        // panic if this doesn't succeed since the current processors are already shutdown.
        self.execution_client
            .sync_to_target(ledger_info.clone())
            .await
            .context(format!(
                "[EpochManager] State sync to new epoch {}",
                ledger_info
            ))
            .expect("Failed to sync to new epoch");

        monitor!("reconfig", self.await_reconfig_notification().await);
        Ok(())
    }
```

**File:** consensus/src/epoch_manager.rs (L1367-1418)
```rust
    async fn start_new_epoch_with_jolteon(
        &mut self,
        consensus_key: Arc<PrivateKey>,
        epoch_state: Arc<EpochState>,
        consensus_config: OnChainConsensusConfig,
        execution_config: OnChainExecutionConfig,
        onchain_randomness_config: OnChainRandomnessConfig,
        jwk_consensus_config: OnChainJWKConsensusConfig,
        network_sender: NetworkSender,
        payload_client: Arc<dyn PayloadClient>,
        payload_manager: Arc<dyn TPayloadManager>,
        rand_config: Option<RandConfig>,
        fast_rand_config: Option<RandConfig>,
        rand_msg_rx: aptos_channel::Receiver<AccountAddress, IncomingRandGenRequest>,
        secret_share_msg_rx: aptos_channel::Receiver<AccountAddress, IncomingSecretShareRequest>,
    ) {
        match self.storage.start(
            consensus_config.order_vote_enabled(),
            consensus_config.window_size(),
        ) {
            LivenessStorageData::FullRecoveryData(initial_data) => {
                self.recovery_mode = false;
                self.start_round_manager(
                    consensus_key,
                    initial_data,
                    epoch_state,
                    consensus_config,
                    execution_config,
                    onchain_randomness_config,
                    jwk_consensus_config,
                    Arc::new(network_sender),
                    payload_client,
                    payload_manager,
                    rand_config,
                    fast_rand_config,
                    rand_msg_rx,
                    secret_share_msg_rx,
                )
                .await
            },
            LivenessStorageData::PartialRecoveryData(ledger_data) => {
                self.recovery_mode = true;
                self.start_recovery_manager(
                    ledger_data,
                    consensus_config,
                    epoch_state,
                    Arc::new(network_sender),
                )
                .await
            },
        }
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L102-201)
```rust
    pub fn find_root_with_window(
        &self,
        blocks: &mut Vec<Block>,
        quorum_certs: &mut Vec<QuorumCert>,
        order_vote_enabled: bool,
        window_size: u64,
    ) -> Result<RootInfo> {
        // We start from the block that storage's latest ledger info, if storage has end-epoch
        // LedgerInfo, we generate the virtual genesis block
        let (latest_commit_id, latest_ledger_info_sig) =
            if self.storage_ledger.ledger_info().ends_epoch() {
                let genesis =
                    Block::make_genesis_block_from_ledger_info(self.storage_ledger.ledger_info());
                let genesis_qc = QuorumCert::certificate_for_genesis_from_ledger_info(
                    self.storage_ledger.ledger_info(),
                    genesis.id(),
                );
                let genesis_ledger_info = genesis_qc.ledger_info().clone();
                let genesis_id = genesis.id();
                blocks.push(genesis);
                quorum_certs.push(genesis_qc);
                (genesis_id, genesis_ledger_info)
            } else {
                (
                    self.storage_ledger.ledger_info().consensus_block_id(),
                    self.storage_ledger.clone(),
                )
            };

        // sort by (epoch, round) to guarantee the topological order of parent <- child
        blocks.sort_by_key(|b| (b.epoch(), b.round()));

        let latest_commit_idx = blocks
            .iter()
            .position(|block| block.id() == latest_commit_id)
            .ok_or_else(|| format_err!("unable to find root: {}", latest_commit_id))?;
        let commit_block = blocks[latest_commit_idx].clone();
        let commit_block_quorum_cert = quorum_certs
            .iter()
            .find(|qc| qc.certified_block().id() == commit_block.id())
            .ok_or_else(|| format_err!("No QC found for root: {}", commit_block.id()))?
            .clone();

        let (root_ordered_cert, root_commit_cert) = if order_vote_enabled {
            // We are setting ordered_root same as commit_root. As every committed block is also ordered, this is fine.
            // As the block store inserts all the fetched blocks and quorum certs and execute the blocks, the block store
            // updates highest_ordered_cert accordingly.
            let root_ordered_cert =
                WrappedLedgerInfo::new(VoteData::dummy(), latest_ledger_info_sig.clone());
            (root_ordered_cert.clone(), root_ordered_cert)
        } else {
            let root_ordered_cert = quorum_certs
                .iter()
                .find(|qc| qc.commit_info().id() == commit_block.id())
                .ok_or_else(|| format_err!("No LI found for root: {}", latest_commit_id))?
                .clone()
                .into_wrapped_ledger_info();
            let root_commit_cert = root_ordered_cert
                .create_merged_with_executed_state(latest_ledger_info_sig)
                .expect("Inconsistent commit proof and evaluation decision, cannot commit block");
            (root_ordered_cert, root_commit_cert)
        };

        let window_start_round = calculate_window_start_round(commit_block.round(), window_size);
        let mut id_to_blocks = HashMap::new();
        blocks.iter().for_each(|block| {
            id_to_blocks.insert(block.id(), block);
        });

        let mut current_block = &commit_block;
        while !current_block.is_genesis_block()
            && current_block.quorum_cert().certified_block().round() >= window_start_round
        {
            if let Some(parent_block) = id_to_blocks.get(&current_block.parent_id()) {
                current_block = *parent_block;
            } else {
                bail!("Parent block not found for block {}", current_block.id());
            }
        }
        let window_start_id = current_block.id();

        let window_start_idx = blocks
            .iter()
            .position(|block| block.id() == window_start_id)
            .ok_or_else(|| format_err!("unable to find window root: {}", window_start_id))?;
        let window_start_block = blocks.remove(window_start_idx);

        info!(
            "Commit block is {}, window block is {}",
            commit_block, window_start_block
        );

        Ok(RootInfo {
            commit_root_block: Box::new(commit_block),
            window_root_block: Some(Box::new(window_start_block)),
            quorum_cert: commit_block_quorum_cert,
            ordered_cert: root_ordered_cert,
            commit_cert: root_commit_cert,
        })
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L348-419)
```rust
    pub fn new(
        last_vote: Option<Vote>,
        ledger_recovery_data: LedgerRecoveryData,
        mut blocks: Vec<Block>,
        root_metadata: RootMetadata,
        mut quorum_certs: Vec<QuorumCert>,
        highest_2chain_timeout_cert: Option<TwoChainTimeoutCertificate>,
        order_vote_enabled: bool,
        window_size: Option<u64>,
    ) -> Result<Self> {
        let root = ledger_recovery_data
            .find_root(
                &mut blocks,
                &mut quorum_certs,
                order_vote_enabled,
                window_size,
            )
            .with_context(|| {
                // for better readability
                blocks.sort_by_key(|block| block.round());
                quorum_certs.sort_by_key(|qc| qc.certified_block().round());
                format!(
                    "\nRoot: {}\nBlocks in db: {}\nQuorum Certs in db: {}\n",
                    ledger_recovery_data.storage_ledger.ledger_info(),
                    blocks
                        .iter()
                        .map(|b| format!("\n{}", b))
                        .collect::<Vec<String>>()
                        .concat(),
                    quorum_certs
                        .iter()
                        .map(|qc| format!("\n{}", qc))
                        .collect::<Vec<String>>()
                        .concat(),
                )
            })?;

        // If execution pool is enabled, use the window_root, else use the commit_root
        let (root_id, epoch) = match &root.window_root_block {
            None => {
                let commit_root_id = root.commit_root_block.id();
                let epoch = root.commit_root_block.epoch();
                (commit_root_id, epoch)
            },
            Some(window_root_block) => {
                let window_start_id = window_root_block.id();
                let epoch = window_root_block.epoch();
                (window_start_id, epoch)
            },
        };
        let blocks_to_prune = Some(Self::find_blocks_to_prune(
            root_id,
            &mut blocks,
            &mut quorum_certs,
        ));

        Ok(RecoveryData {
            last_vote: match last_vote {
                Some(v) if v.epoch() == epoch => Some(v),
                _ => None,
            },
            root,
            root_metadata,
            blocks,
            quorum_certs,
            blocks_to_prune,
            highest_2chain_timeout_certificate: match highest_2chain_timeout_cert {
                Some(tc) if tc.epoch() == epoch => Some(tc),
                _ => None,
            },
        })
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L519-596)
```rust
    fn start(&self, order_vote_enabled: bool, window_size: Option<u64>) -> LivenessStorageData {
        info!("Start consensus recovery.");
        let raw_data = self
            .db
            .get_data()
            .expect("unable to recover consensus data");

        let last_vote = raw_data
            .0
            .map(|bytes| bcs::from_bytes(&bytes[..]).expect("unable to deserialize last vote"));

        let highest_2chain_timeout_cert = raw_data.1.map(|b| {
            bcs::from_bytes(&b).expect("unable to deserialize highest 2-chain timeout cert")
        });
        let blocks = raw_data.2;
        let quorum_certs: Vec<_> = raw_data.3;
        let blocks_repr: Vec<String> = blocks.iter().map(|b| format!("\n\t{}", b)).collect();
        info!(
            "The following blocks were restored from ConsensusDB : {}",
            blocks_repr.concat()
        );
        let qc_repr: Vec<String> = quorum_certs
            .iter()
            .map(|qc| format!("\n\t{}", qc))
            .collect();
        info!(
            "The following quorum certs were restored from ConsensusDB: {}",
            qc_repr.concat()
        );
        // find the block corresponding to storage latest ledger info
        let latest_ledger_info = self
            .aptos_db
            .get_latest_ledger_info()
            .expect("Failed to get latest ledger info.");
        let accumulator_summary = self
            .aptos_db
            .get_accumulator_summary(latest_ledger_info.ledger_info().version())
            .expect("Failed to get accumulator summary.");
        let ledger_recovery_data = LedgerRecoveryData::new(latest_ledger_info);

        match RecoveryData::new(
            last_vote,
            ledger_recovery_data.clone(),
            blocks,
            accumulator_summary.into(),
            quorum_certs,
            highest_2chain_timeout_cert,
            order_vote_enabled,
            window_size,
        ) {
            Ok(mut initial_data) => {
                (self as &dyn PersistentLivenessStorage)
                    .prune_tree(initial_data.take_blocks_to_prune())
                    .expect("unable to prune dangling blocks during restart");
                if initial_data.last_vote.is_none() {
                    self.db
                        .delete_last_vote_msg()
                        .expect("unable to cleanup last vote");
                }
                if initial_data.highest_2chain_timeout_certificate.is_none() {
                    self.db
                        .delete_highest_2chain_timeout_certificate()
                        .expect("unable to cleanup highest 2-chain timeout cert");
                }
                info!(
                    "Starting up the consensus state machine with recovery data - [last_vote {}], [highest timeout certificate: {}]",
                    initial_data.last_vote.as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                    initial_data.highest_2chain_timeout_certificate().as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                );

                LivenessStorageData::FullRecoveryData(initial_data)
            },
            Err(e) => {
                error!(error = ?e, "Failed to construct recovery data");
                LivenessStorageData::PartialRecoveryData(ledger_recovery_data)
            },
        }
    }
```

**File:** consensus/src/recovery_manager.rs (L120-174)
```rust
    pub async fn start(
        mut self,
        mut event_rx: aptos_channel::Receiver<
            (Author, Discriminant<VerifiedEvent>),
            (Author, VerifiedEvent),
        >,
        close_rx: oneshot::Receiver<oneshot::Sender<()>>,
    ) {
        info!(epoch = self.epoch_state.epoch, "RecoveryManager started");
        let mut close_rx = close_rx.into_stream();
        loop {
            futures::select! {
                (peer_id, event) = event_rx.select_next_some() => {
                    let result = match event {
                        VerifiedEvent::ProposalMsg(proposal_msg) => {
                            monitor!(
                                "process_recovery",
                                self.process_proposal_msg(*proposal_msg).await
                            )
                        }
                        VerifiedEvent::VoteMsg(vote_msg) => {
                            monitor!("process_recovery", self.process_vote_msg(*vote_msg).await)
                        }
                        VerifiedEvent::UnverifiedSyncInfo(sync_info) => {
                            monitor!(
                                "process_recovery",
                                self.sync_up(&sync_info, peer_id).await
                            )
                        }
                        unexpected_event => Err(anyhow!("Unexpected event: {:?}", unexpected_event)),
                    }
                    .with_context(|| format!("from peer {}", peer_id));

                    match result {
                        Ok(_) => {
                            info!("Recovery finishes for epoch {}, RecoveryManager stopped. Please restart the node", self.epoch_state.epoch);
                            process::exit(0);
                        },
                        Err(e) => {
                            counters::ERROR_COUNT.inc();
                            warn!(error = ?e, kind = error_kind(&e));
                        }
                    }
                }
                close_req = close_rx.select_next_some() => {
                    if let Ok(ack_sender) = close_req {
                        ack_sender.send(()).expect("[RecoveryManager] Fail to ack shutdown");
                    }
                    break;
                }
            }
        }
        info!(epoch = self.epoch_state.epoch, "RecoveryManager stopped");
    }
}
```
