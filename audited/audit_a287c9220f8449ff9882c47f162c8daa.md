# Audit Report

## Title
Cross-Shard Message Replay Attack Enables Consensus Safety Violations Through Missing Consensus Round Binding

## Summary
Cross-shard messages in the sharded block executor are not cryptographically bound to specific consensus blocks, enabling replay attacks across different consensus rounds. An attacker with network access can capture messages from one consensus block and replay them during execution of a different consensus block, causing validators to compute different state roots and violating consensus safety.

## Finding Description

The sharded block executor uses cross-shard messages to communicate transaction write sets between execution shards. These messages are transmitted over persistent network channels but lack any cryptographic binding to the consensus block they belong to.

**Vulnerability Components:**

1. **Missing Block Context in Messages**: The `CrossShardMsg` structure contains only transaction state updates with no consensus block identifier. [1](#0-0) 

2. **No Cryptographic Verification**: The `receive_cross_shard_msg()` function deserializes messages without any signature verification or authentication. [2](#0-1) 

3. **Persistent Network Infrastructure**: The `RemoteCrossShardClient` is created once and reused across all consensus blocks, with the same network channels persisting. [3](#0-2) 

4. **No Block Identifiers in Execution Commands**: The `ExecutorShardCommand::ExecuteSubBlocks` contains no consensus round, block hash, or epoch information. [4](#0-3) 

5. **Unauthenticated Network Transport**: The GRPC service accepts messages without cryptographic authentication. [5](#0-4) 

**Attack Scenario:**

1. Attacker monitors network traffic between executor shards during consensus block N execution
2. Attacker captures `RemoteTxnWriteMsg` messages containing state updates for execution round 1
3. During consensus block M execution (different block, same execution round 1), attacker replays captured messages on the `cross_shard_1` channel
4. Victim shard's `CrossShardCommitReceiver` accepts and applies the replayed state updates [6](#0-5) 

5. This causes incorrect cross-shard dependencies to be resolved with wrong values, leading to different execution results across validators
6. Different validators compute different state roots for block M, violating consensus safety

**Root Cause:** The system conflates execution partitioning rounds (0-7) with consensus block identity. Messages are routed by execution round but contain no cryptographic proof of which consensus block they belong to.

## Impact Explanation

**Severity: CRITICAL** (per Aptos Bug Bounty - Consensus/Safety Violations, up to $1,000,000)

This vulnerability breaks two fundamental invariants:

1. **Deterministic Execution Violation**: Validators executing the same consensus block can produce different state roots due to replayed cross-shard messages containing incorrect state values from previous blocks.

2. **Consensus Safety Violation**: AptosBFT's safety guarantee requires that once a block is committed at round R with state root S, no validator can commit a different state root S' for round R. This attack enables exactly that scenario:
   - Honest validators compute state root S₁ for block M
   - Validators receiving replayed messages compute state root S₂ ≠ S₁ for block M
   - This causes consensus divergence and potential chain splits

The impact includes:
- **Chain Splits**: Different validators commit different histories, requiring hard fork recovery
- **State Inconsistency**: Validators cannot reach agreement on the canonical chain state
- **Loss of Liveness**: Network may halt if quorum cannot be reached on conflicting state roots
- **Fund Safety Compromise**: Inconsistent state could enable double-spending or fund theft

This affects **all validators** running distributed sharded execution and requires **hard fork intervention** to resolve.

## Likelihood Explanation

**Likelihood: HIGH**

The attack is highly feasible because:

1. **No Special Access Required**: Attacker only needs network packet capture/injection capability on the inter-shard communication channels (standard network adversary model)

2. **Deterministic Trigger**: The attack succeeds whenever:
   - Two consensus blocks use the same execution partitioning rounds
   - Transactions have similar cross-shard dependencies
   - Both conditions are common in normal operation

3. **No Detection Mechanism**: The receiving shard has no way to detect replayed messages since they are validly-formatted `CrossShardMsg` structures

4. **Persistent Infrastructure**: The global static executors ensure channels persist across blocks [7](#0-6) 

5. **Production Deployment**: The remote executor infrastructure is actively used in distributed execution mode [8](#0-7) 

The only barrier is network access to inter-shard channels, which is expected in a distributed validator deployment.

## Recommendation

**Immediate Fix: Add Cryptographic Block Binding to Cross-Shard Messages**

1. **Extend Message Structure** to include consensus block context:
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct CrossShardMsgEnvelope {
    pub block_id: HashValue,        // Consensus block hash
    pub epoch: u64,                  // Consensus epoch
    pub consensus_round: u64,        // Consensus round
    pub execution_round: RoundId,    // Execution partitioning round
    pub msg: CrossShardMsg,          // Actual message payload
    pub signature: Ed25519Signature, // Sender's signature over above fields
}
```

2. **Modify `send_cross_shard_msg()`** to include block context and sign:
```rust
fn send_cross_shard_msg(&self, shard_id: ShardId, round: RoundId, msg: CrossShardMsg, 
                        block_context: BlockContext, signing_key: &Ed25519PrivateKey) {
    let envelope = CrossShardMsgEnvelope {
        block_id: block_context.block_id,
        epoch: block_context.epoch,
        consensus_round: block_context.round,
        execution_round: round,
        msg,
        signature: signing_key.sign(&bcs::to_bytes(&(block_context, round, &msg)).unwrap()),
    };
    // ... send envelope
}
```

3. **Modify `receive_cross_shard_msg()`** to verify block context:
```rust
fn receive_cross_shard_msg(&self, current_round: RoundId, expected_block_context: &BlockContext, 
                           sender_public_key: &Ed25519PublicKey) -> CrossShardMsg {
    let envelope = self.receive_envelope(current_round);
    
    // Verify block context matches
    assert_eq!(envelope.block_id, expected_block_context.block_id);
    assert_eq!(envelope.epoch, expected_block_context.epoch);
    assert_eq!(envelope.consensus_round, expected_block_context.round);
    assert_eq!(envelope.execution_round, current_round);
    
    // Verify signature
    assert!(sender_public_key.verify(&bcs::to_bytes(&(expected_block_context, current_round, &envelope.msg)).unwrap(), 
                                     &envelope.signature).is_ok());
    
    envelope.msg
}
```

4. **Pass Block Context Through Execution Stack**: Modify `ExecutorShardCommand::ExecuteSubBlocks` to include block context: [4](#0-3) 

**Additional Hardening:**
- Implement per-block channel isolation (create new channels for each consensus block)
- Add message sequence numbers and replay windows
- Enable TLS/mutual authentication for inter-shard GRPC connections
- Add monitoring for unexpected message timing patterns

## Proof of Concept

```rust
// Conceptual PoC - demonstrates the attack flow
// This would require a full distributed test environment to execute

use aptos_types::block_executor::partitioner::RoundId;

#[test]
fn test_cross_shard_replay_attack() {
    // Setup: Two validator nodes with distributed sharded execution
    let node_a = setup_validator_with_sharded_execution();
    let node_b = setup_validator_with_sharded_execution();
    
    // Block N execution
    let block_n = create_test_block(/* consensus round 100 */);
    let captured_messages = Vec::new();
    
    // Attacker intercepts messages during block N execution (round 1)
    node_a.execute_block_with_capture(block_n, &mut captured_messages);
    
    // Block M execution (different block, consensus round 101)
    let block_m = create_test_block(/* consensus round 101 */);
    
    // Node A executes block M honestly
    let state_root_a = node_a.execute_block(block_m.clone());
    
    // Attacker replays captured messages to Node B during block M execution
    for msg in captured_messages {
        if msg.execution_round == 1 {
            // Inject message on cross_shard_1 channel during block M execution
            node_b.inject_cross_shard_message(1, msg);
        }
    }
    let state_root_b = node_b.execute_block(block_m);
    
    // Consensus safety violated: different state roots for same block
    assert_ne!(state_root_a, state_root_b, 
               "VULNERABILITY: Replayed messages caused state divergence");
}
```

**Exploitation Requirements:**
1. Network packet capture on inter-shard GRPC channels
2. Packet injection capability (standard MitM tools)
3. Timing: inject during target block's execution round
4. Two blocks with similar cross-shard dependency patterns

**Real-World Scenario:** An attacker positioned as a network intermediary between distributed executor shards can capture and replay messages to cause consensus divergence during high transaction load periods when cross-shard dependencies are common.

## Notes

The vulnerability exists because the system assumes network isolation between consensus blocks but reuses the same communication infrastructure. The execution partitioning rounds (0-7) were designed for intra-block parallelism, not inter-block message authentication. The missing consensus context in cross-shard messages creates an authentication gap that violates the fundamental assumption that all validators execute blocks deterministically.

This issue specifically affects distributed sharded execution deployments using `RemoteExecutorClient` and `RemoteCrossShardClient`. Local sharded execution using in-process channels is not vulnerable to network-based replay, but the architectural flaw remains.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/messages.rs (L7-18)
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
pub enum CrossShardMsg {
    RemoteTxnWriteMsg(RemoteTxnWrite),
    StopMsg,
}

#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct RemoteTxnWrite {
    state_key: StateKey,
    // The write op is None if the transaction is aborted.
    write_op: Option<WriteOp>,
}
```

**File:** execution/executor-service/src/remote_cross_shard_client.rs (L61-66)
```rust
    fn receive_cross_shard_msg(&self, current_round: RoundId) -> CrossShardMsg {
        let rx = self.message_rxs[current_round].lock().unwrap();
        let message = rx.recv().unwrap();
        let msg: CrossShardMsg = bcs::from_bytes(&message.to_bytes()).unwrap();
        msg
    }
```

**File:** execution/executor-service/src/remote_executor_service.rs (L37-40)
```rust
        let cross_shard_client = Arc::new(RemoteCrossShardClient::new(
            &mut controller,
            remote_shard_addresses,
        ));
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L42-50)
```rust
pub enum ExecutorShardCommand<S> {
    ExecuteSubBlocks(
        Arc<S>,
        SubBlocksForShard<AnalyzedTransaction>,
        usize,
        BlockExecutorConfigFromOnchain,
    ),
    Stop,
}
```

**File:** secure/net/src/grpc_network_service/mod.rs (L93-115)
```rust
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/cross_shard_client.rs (L31-44)
```rust
        loop {
            let msg = cross_shard_client.receive_cross_shard_msg(round);
            match msg {
                RemoteTxnWriteMsg(txn_commit_msg) => {
                    let (state_key, write_op) = txn_commit_msg.take();
                    cross_shard_state_view
                        .set_value(&state_key, write_op.and_then(|w| w.as_state_value()));
                },
                CrossShardMsg::StopMsg => {
                    trace!("Cross shard commit receiver stopped for round {}", round);
                    break;
                },
            }
        }
```

**File:** execution/executor-service/src/local_executor_helper.rs (L14-21)
```rust
pub static SHARDED_BLOCK_EXECUTOR: Lazy<
    Arc<Mutex<ShardedBlockExecutor<CachedStateView, LocalExecutorClient<CachedStateView>>>>,
> = Lazy::new(|| {
    info!("LOCAL_SHARDED_BLOCK_EXECUTOR created");
    Arc::new(Mutex::new(
        LocalExecutorClient::create_local_sharded_block_executor(AptosVM::get_num_shards(), None),
    ))
});
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L261-267)
```rust
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
```
