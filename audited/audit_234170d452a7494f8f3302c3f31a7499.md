# Audit Report

## Title
Mempool Coordinator Panic on Empty or Terminated Network Event Streams

## Summary
The mempool coordinator's `bootstrap()` function passes `network_service_events` to `start_shared_mempool()` without validation. If the network event stream collection is empty or all streams are closed/terminated before the coordinator's main event loop begins, the coordinator will immediately panic when `select_next_some()` is called on a terminated stream, causing mempool service unavailability.

## Finding Description

The vulnerability exists in the mempool initialization flow across two files:

In `bootstrap()`, the `network_service_events` parameter is passed directly to `start_shared_mempool()` without any validation: [1](#0-0) 

The `start_shared_mempool()` function then passes this to the `coordinator()` without validation: [2](#0-1) 

In the `coordinator()` function, the network events are transformed and combined: [3](#0-2) 

The critical issue occurs in the main event loop where `select_next_some()` is called: [4](#0-3) 

The `select_next_some()` method from the futures library will **panic with "SelectNextSome polled after terminated"** if called on a stream that has already returned `None`. This behavior is documented in test code: [5](#0-4) 

**Attack Scenarios:**

1. **Empty Network Configuration**: If `network_service_events.into_network_and_events()` returns an empty HashMap (when node has no configured networks), the resulting Vec will be empty. `select_all([])` creates a stream that immediately completes, causing `events.select_next_some()` to panic on first iteration.

2. **Pre-Closed Network Streams**: If all `NetworkEvents` streams in the HashMap are already closed/terminated before coordinator starts (due to network layer failure or race condition), `select_all()` will produce a terminated stream, triggering the same panic.

3. **Streams Closing During Initialization**: Between when the coordinator waits for the initial reconfig event and when it enters the main loop, if all network streams close, the panic occurs when the loop first executes.

The network configuration flow shows how empty networks can occur: [6](#0-5) 

And the network handles are collected in a loop that could potentially be empty: [7](#0-6) 

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **API/Service Crash**: The mempool coordinator crashes immediately upon entering its main event loop, making the mempool service completely unavailable. This falls under "API crashes" in the High severity category.

2. **Node Availability Impact**: A crashed mempool prevents the node from accepting new transactions, processing transaction broadcasts, and participating in transaction propagation across the network.

3. **No Graceful Degradation**: Unlike other error paths that log errors and continue, this panic terminates the coordinator task abruptly with no recovery mechanism.

4. **Production Plausibility**: While misconfiguration is one path, network layer failures during startup or initialization race conditions could trigger this in production environments.

## Likelihood Explanation

The likelihood is **Medium to Low** but the issue is concerning:

**Factors Increasing Likelihood:**
- No defensive validation exists anywhere in the call chain
- Network initialization timing issues could cause race conditions  
- Configuration errors during node deployment could result in empty network configs
- Network layer bugs could cause premature stream termination

**Factors Decreasing Likelihood:**
- Production nodes typically have at least one configured network
- Network layer is generally robust and unlikely to provide closed streams
- Requires specific timing or configuration conditions

However, the **complete lack of defensive programming** makes this a ticking time bombâ€”any future changes to network initialization that alter timing could trigger this panic.

## Recommendation

Add validation in `bootstrap()` or `start_shared_mempool()` to ensure network events are present and valid:

```rust
pub fn bootstrap(
    config: &NodeConfig,
    db: Arc<dyn DbReader>,
    network_client: NetworkClient<MempoolSyncMsg>,
    network_service_events: NetworkServiceEvents<MempoolSyncMsg>,
    client_events: MempoolEventsReceiver,
    quorum_store_requests: Receiver<QuorumStoreRequest>,
    mempool_listener: MempoolNotificationListener,
    mempool_reconfig_events: ReconfigNotificationListener<DbBackedOnChainConfig>,
    peers_and_metadata: Arc<PeersAndMetadata>,
) -> Runtime {
    // Validate network events are present
    let network_and_events = network_service_events.into_network_and_events();
    if network_and_events.is_empty() {
        panic!("Mempool requires at least one configured network, but received empty NetworkServiceEvents");
    }
    let validated_events = NetworkServiceEvents::new(network_and_events);
    
    let runtime = aptos_runtimes::spawn_named_runtime("shared-mem".into(), None);
    let mempool = Arc::new(Mutex::new(CoreMempool::new(config)));
    let vm_validator = Arc::new(RwLock::new(PooledVMValidator::new(
        Arc::clone(&db),
        num_cpus::get(),
    )));
    start_shared_mempool(
        runtime.handle(),
        config,
        mempool,
        network_client,
        validated_events, // Use validated events
        client_events,
        quorum_store_requests,
        mempool_listener,
        mempool_reconfig_events,
        db,
        vm_validator,
        vec![],
        peers_and_metadata,
    );
    runtime
}
```

Alternatively, modify the coordinator to handle terminated streams gracefully by using `select_next_some()` only on streams guaranteed to be non-empty, or use a different pattern that doesn't panic on `None`.

## Proof of Concept

```rust
#[cfg(test)]
mod mempool_panic_test {
    use super::*;
    use std::collections::HashMap;
    
    #[tokio::test]
    #[should_panic(expected = "SelectNextSome polled after terminated")]
    async fn test_empty_network_events_causes_panic() {
        // Create empty network events (simulating misconfigured node)
        let empty_network_events = HashMap::new();
        let network_service_events = NetworkServiceEvents::new(empty_network_events);
        
        // Transform events as coordinator does
        let network_events: Vec<_> = network_service_events
            .into_network_and_events()
            .into_iter()
            .map(|(network_id, events)| events.map(move |event| (network_id, event)))
            .collect();
        
        // This creates a stream that immediately completes
        let mut events = futures::stream::select_all(network_events).fuse();
        
        // Attempting to use select_next_some in a select! will panic
        use futures::StreamExt;
        loop {
            futures::select! {
                // This will panic because events stream is already terminated
                event = events.select_next_some() => {
                    // Never reached
                    println!("Got event: {:?}", event);
                },
                complete => break,
            }
        }
    }
}
```

This test demonstrates that when `NetworkServiceEvents` contains an empty HashMap, the resulting stream from `select_all()` immediately terminates, and calling `select_next_some()` in the futures::select! macro causes a panic with the message "SelectNextSome polled after terminated".

## Notes

While this vulnerability requires specific conditions to trigger (empty network configuration or all network streams pre-closed), the **complete absence of validation** represents a critical defensive programming failure. The mempool is a core component responsible for transaction propagation, and its crash prevents nodes from processing transactions. The fix is straightforward and should be implemented to prevent future issues as network initialization logic evolves.

### Citations

**File:** mempool/src/shared_mempool/runtime.rs (L66-76)
```rust
    executor.spawn(coordinator(
        smp,
        executor.clone(),
        network_service_events,
        client_events,
        quorum_store_requests,
        mempool_listener,
        mempool_reconfig_events,
        config.mempool.shared_mempool_peer_update_interval_ms,
        peers_and_metadata,
    ));
```

**File:** mempool/src/shared_mempool/runtime.rs (L91-123)
```rust
pub fn bootstrap(
    config: &NodeConfig,
    db: Arc<dyn DbReader>,
    network_client: NetworkClient<MempoolSyncMsg>,
    network_service_events: NetworkServiceEvents<MempoolSyncMsg>,
    client_events: MempoolEventsReceiver,
    quorum_store_requests: Receiver<QuorumStoreRequest>,
    mempool_listener: MempoolNotificationListener,
    mempool_reconfig_events: ReconfigNotificationListener<DbBackedOnChainConfig>,
    peers_and_metadata: Arc<PeersAndMetadata>,
) -> Runtime {
    let runtime = aptos_runtimes::spawn_named_runtime("shared-mem".into(), None);
    let mempool = Arc::new(Mutex::new(CoreMempool::new(config)));
    let vm_validator = Arc::new(RwLock::new(PooledVMValidator::new(
        Arc::clone(&db),
        num_cpus::get(),
    )));
    start_shared_mempool(
        runtime.handle(),
        config,
        mempool,
        network_client,
        network_service_events,
        client_events,
        quorum_store_requests,
        mempool_listener,
        mempool_reconfig_events,
        db,
        vm_validator,
        vec![],
        peers_and_metadata,
    );
    runtime
```

**File:** mempool/src/shared_mempool/coordinator.rs (L76-82)
```rust
    // Transform events to also include the network id
    let network_events: Vec<_> = network_service_events
        .into_network_and_events()
        .into_iter()
        .map(|(network_id, events)| events.map(move |event| (network_id, event)))
        .collect();
    let mut events = select_all(network_events).fuse();
```

**File:** mempool/src/shared_mempool/coordinator.rs (L106-129)
```rust
    loop {
        let _timer = counters::MAIN_LOOP.start_timer();
        ::futures::select! {
            msg = client_events.select_next_some() => {
                handle_client_request(&mut smp, &bounded_executor, msg).await;
            },
            msg = quorum_store_requests.select_next_some() => {
                tasks::process_quorum_store_request(&smp, msg);
            },
            reconfig_notification = mempool_reconfig_events.select_next_some() => {
                handle_mempool_reconfig_event(&mut smp, &bounded_executor, reconfig_notification.on_chain_configs).await;
            },
            (peer, backoff) = scheduled_broadcasts.select_next_some() => {
                tasks::execute_broadcast(peer, backoff, &mut smp, &mut scheduled_broadcasts, executor.clone()).await;
            },
            (network_id, event) = events.select_next_some() => {
                handle_network_event(&bounded_executor, &mut smp, network_id, event).await;
            },
            _ = update_peers_interval.tick().fuse() => {
                handle_update_peers(peers_and_metadata.clone(), &mut smp, &mut scheduled_broadcasts, executor.clone()).await;
            },
            complete => break,
        }
    }
```

**File:** state-sync/data-streaming-service/src/tests/streaming_service.rs (L1552-1585)
```rust
#[tokio::test(flavor = "multi_thread")]
#[should_panic(expected = "SelectNextSome polled after terminated")]
async fn test_terminate_stream() {
    // Create a new streaming client and service
    let streaming_client = create_streaming_client_and_service();

    // Request a state value stream
    let mut stream_listener = streaming_client
        .get_all_state_values(MAX_ADVERTISED_STATES - 1, None)
        .await
        .unwrap();

    // Fetch the first state value notification and then terminate the stream
    let data_notification = get_data_notification(&mut stream_listener).await.unwrap();
    match data_notification.data_payload {
        DataPayload::StateValuesWithProof(_) => {},
        data_payload => unexpected_payload_type!(data_payload),
    }

    // Terminate the stream
    let result = streaming_client
        .terminate_stream_with_feedback(
            stream_listener.data_stream_id,
            Some(NotificationAndFeedback::new(
                data_notification.notification_id,
                NotificationFeedback::InvalidPayloadData,
            )),
        )
        .await;
    assert_ok!(result);

    // Verify the streaming service has removed the stream (polling should panic)
    loop {
        let data_notification = get_data_notification(&mut stream_listener).await.unwrap();
```

**File:** aptos-node/src/network.rs (L218-227)
```rust
fn extract_network_configs(node_config: &NodeConfig) -> Vec<NetworkConfig> {
    let mut network_configs: Vec<NetworkConfig> = node_config.full_node_networks.to_vec();
    if let Some(network_config) = node_config.validator_network.as_ref() {
        // Ensure that mutual authentication is enabled by default!
        if !network_config.mutual_authentication {
            panic!("Validator networks must always have mutual_authentication enabled!");
        }
        network_configs.push(network_config.clone());
    }
    network_configs
```

**File:** aptos-node/src/network.rs (L360-368)
```rust
        // Register mempool (both client and server) with the network
        let mempool_network_handle = register_client_and_service_with_network(
            &mut network_builder,
            network_id,
            &network_config,
            mempool_network_configuration(node_config),
            true,
        );
        mempool_network_handles.push(mempool_network_handle);
```
