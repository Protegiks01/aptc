# Audit Report

## Title
TOCTOU Race Condition in CachedStateView Enables Non-Deterministic State Reads During Parallel Execution

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) race condition exists in `CachedStateView::get_state_slot()` where concurrent threads can read different values for the same state key, violating the deterministic execution invariant critical for consensus safety. This occurs when the hot state is asynchronously updated by the Committer thread while parallel transaction execution is reading from it.

## Finding Description

The vulnerability exists in the interaction between three components:

**1. CachedStateView** reads state from multiple layers with a memorization pattern: [1](#0-0) 

The `get_state_slot()` method checks memorized cache first, then calls `get_unmemorized()`: [2](#0-1) 

**2. HotStateBase** uses a DashMap that is shared across all state views: [3](#0-2) 

The hot state view is shared via `Arc::clone()`: [4](#0-3) 

**3. Committer thread** asynchronously inserts/removes entries from the shared HotStateBase: [5](#0-4) 

The Committer spawns in a separate thread: [6](#0-5) 

**Race Condition Scenario:**

During parallel transaction execution via BlockSTM, multiple threads share the same `CachedStateView` as their base view: [7](#0-6) 

BlockSTM uses 32 IO threads for parallel reads: [8](#0-7) 

When two threads concurrently call `get_state_slot(key_K)`:

1. **Thread A** checks memorized cache → MISS
2. **Thread B** checks memorized cache → MISS
3. **Thread A** calls `get_unmemorized()` → checks hot state → returns None → falls through to cold DB read
4. **Committer thread** inserts `key_K` into hot state DashMap with updated value
5. **Thread B** calls `get_unmemorized()` → checks hot state → returns Some(value) from newly inserted entry
6. **Thread A** calls `try_insert()` with cold DB value, succeeds
7. **Thread B** calls `try_insert()` with hot state value, fails (Entry::Occupied)

The `try_insert()` implementation only inserts if vacant: [9](#0-8) 

8. **Thread A** returns cold DB value (line 296 returns the local `slot` variable)
9. **Thread B** returns hot state value (line 296 returns its local `slot` variable) - **DIFFERENT VALUES**

**The critical bug**: Each thread returns the value it fetched (`slot` variable), NOT the value actually stored in the memorized cache. This breaks the invariant that all reads of the same key from the same state view return identical values.

The TODO comment suggests awareness of duplicate reads but not the race condition consequences: [10](#0-9) 

## Impact Explanation

**Severity: CRITICAL** (up to $1,000,000 per Aptos Bug Bounty)

This vulnerability breaks **Invariant #1: Deterministic Execution** - "All validators must produce identical state roots for identical blocks."

**Consensus Safety Violation:**
- Different validator nodes executing the same block can have their worker threads read different values for the same state key
- Transactions executing in parallel can observe inconsistent base state  
- This leads to different execution results and different state roots
- Validators will fail to reach consensus on the block's state commitment
- Can cause chain splits or consensus liveness failures

**Concrete Impact:**
1. **Non-deterministic execution:** Two transactions in the same block reading the same key can get different values, producing different write sets
2. **Consensus divergence:** Different validators will compute different state roots for identical blocks
3. **Transaction re-execution cascades:** The BlockSTM validation will detect read inconsistencies, triggering re-executions, but the race can recur
4. **Potential chain halt:** If enough validators observe different states, the network cannot reach consensus on block commitments

This directly meets the Critical severity criteria of "Consensus/Safety violations" as it can cause validators to produce divergent state roots for the same block, potentially requiring a hard fork to resolve.

## Likelihood Explanation

**Likelihood: HIGH**

This race condition occurs naturally during normal blockchain operation:

1. **No attacker action required:** The vulnerability is triggered by the system's own asynchronous hot state management, not by malicious inputs
2. **Frequent trigger conditions:**
   - Happens whenever parallel transaction execution coincides with hot state updates
   - Hot state is updated continuously as new blocks are committed
   - Parallel execution occurs on every block with multiple transactions
3. **Wide timing window:** The race window exists throughout the entire duration of parallel block execution (potentially milliseconds to seconds)
4. **High concurrency:** BlockSTM uses 32 IO threads for reading, maximizing concurrent access to the base view

## Recommendation

The fix is to return the value from the memorized cache after insertion, not the locally fetched value:

```rust
pub fn get_state_slot(&self, state_key: &StateKey) -> StateViewResult<StateSlot> {
    // First check if requested key is already memorized.
    if let Some(slot) = self.memorized.get_cloned(state_key) {
        COUNTER.inc_with(&["sv_memorized"]);
        return Ok(slot);
    }

    let slot = self.get_unmemorized(state_key)?;
    self.memorized.try_insert(state_key, &slot);
    
    // FIX: Return the value that ended up in the cache, not the locally fetched value
    // This ensures all threads return the same value for the same key
    Ok(self.memorized.get_cloned(state_key).unwrap_or(slot))
}
```

Alternative fix: Use a compare-and-swap pattern or ensure the hot state is immutable during parallel execution of a block.

## Proof of Concept

While a full PoC would require a multi-threaded Rust test with precise timing control, the race condition can be demonstrated through code analysis:

1. Two threads simultaneously call `CachedStateView::get_state_slot()` for the same key
2. Both miss the memorized cache (line 288)
3. Both call `get_unmemorized()` (line 294)
4. Thread A reads from cold DB, Thread B reads from hot state (after Committer update)
5. Thread A's `try_insert()` succeeds, Thread B's fails
6. Both return their local `slot` values (line 296), which differ

The vulnerability is evident from the code structure where each thread returns its locally fetched value rather than re-reading from the cache after the race-prone insertion.

### Citations

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L39-45)
```rust
static IO_POOL: Lazy<rayon::ThreadPool> = Lazy::new(|| {
    rayon::ThreadPoolBuilder::new()
        .num_threads(32)
        .thread_name(|index| format!("kv_reader_{}", index))
        .build()
        .unwrap()
});
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L75-84)
```rust
    pub fn try_insert(&self, state_key: &StateKey, slot: &StateSlot) {
        let shard_id = state_key.get_shard_id();

        match self.shard(shard_id).entry(state_key.clone()) {
            Entry::Occupied(_) => {},
            Entry::Vacant(entry) => {
                entry.insert(slot.clone());
            },
        };
    }
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L233-253)
```rust
    fn get_unmemorized(&self, state_key: &StateKey) -> Result<StateSlot> {
        COUNTER.inc_with(&["sv_unmemorized"]);

        let ret = if let Some(slot) = self.speculative.get_state_slot(state_key) {
            COUNTER.inc_with(&["sv_hit_speculative"]);
            slot
        } else if let Some(slot) = self.hot.get_state_slot(state_key) {
            COUNTER.inc_with(&["sv_hit_hot"]);
            slot
        } else if let Some(base_version) = self.base_version() {
            COUNTER.inc_with(&["sv_cold"]);
            StateSlot::from_db_get(
                self.cold
                    .get_state_value_with_version_by_version(state_key, base_version)?,
            )
        } else {
            StateSlot::ColdVacant
        };

        Ok(ret)
    }
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L283-297)
```rust
    fn get_state_slot(&self, state_key: &StateKey) -> StateViewResult<StateSlot> {
        let _timer = TIMER.timer_with(&["get_state_value"]);
        COUNTER.inc_with(&["sv_total_get"]);

        // First check if requested key is already memorized.
        if let Some(slot) = self.memorized.get_cloned(state_key) {
            COUNTER.inc_with(&["sv_memorized"]);
            return Ok(slot);
        }

        // TODO(aldenhu): reduce duplicated gets
        let slot = self.get_unmemorized(state_key)?;
        self.memorized.try_insert(state_key, &slot);
        Ok(slot)
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L73-98)
```rust
pub struct HotStateBase<K = StateKey, V = StateSlot>
where
    K: Eq + std::hash::Hash,
{
    shards: [Shard<K, V>; NUM_STATE_SHARDS],
}

impl<K, V> HotStateBase<K, V>
where
    K: Clone + Eq + std::hash::Hash,
    V: Clone,
{
    fn new_empty(max_items_per_shard: usize) -> Self {
        Self {
            shards: arr![Shard::new(max_items_per_shard); 16],
        }
    }

    fn get_from_shard(&self, shard_id: usize, key: &K) -> Option<Ref<'_, K, V>> {
        self.shards[shard_id].get(key)
    }

    fn len(&self) -> usize {
        self.shards.iter().map(|s| s.len()).sum()
    }
}
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L131-136)
```rust
    pub fn get_committed(&self) -> (Arc<dyn HotStateView>, State) {
        let state = self.committed.lock().clone();
        let base = self.base.clone();

        (base, state)
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L173-178)
```rust
    fn spawn(base: Arc<HotStateBase>, committed: Arc<Mutex<State>>) -> SyncSender<State> {
        let (tx, rx) = std::sync::mpsc::sync_channel(MAX_HOT_STATE_COMMIT_BACKLOG);
        std::thread::spawn(move || Self::new(base, committed, rx).run());

        tx
    }
```

**File:** storage/aptosdb/src/state_store/hot_state.rs (L235-275)
```rust
    fn commit(&mut self, to_commit: &State) {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["hot_state_commit"]);

        let mut n_insert = 0;
        let mut n_update = 0;
        let mut n_evict = 0;

        let delta = to_commit.make_delta(&self.committed.lock());
        for shard_id in 0..NUM_STATE_SHARDS {
            for (key, slot) in delta.shards[shard_id].iter() {
                if slot.is_hot() {
                    let key_size = key.size();
                    self.total_key_bytes += key_size;
                    self.total_value_bytes += slot.size();
                    if let Some(old_slot) = self.base.shards[shard_id].insert(key, slot) {
                        self.total_key_bytes -= key_size;
                        self.total_value_bytes -= old_slot.size();
                        n_update += 1;
                    } else {
                        n_insert += 1;
                    }
                } else if let Some((key, old_slot)) = self.base.shards[shard_id].remove(&key) {
                    self.total_key_bytes -= key.size();
                    self.total_value_bytes -= old_slot.size();
                    n_evict += 1;
                }
            }
            self.heads[shard_id] = to_commit.latest_hot_key(shard_id);
            self.tails[shard_id] = to_commit.oldest_hot_key(shard_id);
            assert_eq!(
                self.base.shards[shard_id].len(),
                to_commit.num_hot_items(shard_id)
            );

            debug_assert!(self.validate_lru(shard_id).is_ok());
        }

        COUNTER.inc_with_by(&["hot_state_insert"], n_insert);
        COUNTER.inc_with_by(&["hot_state_update"], n_update);
        COUNTER.inc_with_by(&["hot_state_evict"], n_evict);
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L82-99)
```rust
struct SharedSyncParams<'a, T, E, S>
where
    T: BlockExecutableTransaction,
    E: ExecutorTask<Txn = T>,
    S: TStateView<Key = T::Key> + Sync,
{
    // TODO: should not need to pass base view.
    base_view: &'a S,
    versioned_cache: &'a MVHashMap<T::Key, T::Tag, T::Value, DelayedFieldID>,
    global_module_cache:
        &'a GlobalModuleCache<ModuleId, CompiledModule, Module, AptosModuleExtension>,
    last_input_output: &'a TxnLastInputOutput<T, E::Output>,
    start_shared_counter: u32,
    delayed_field_id_counter: &'a AtomicU32,
    block_limit_processor: &'a ExplicitSyncWrapper<BlockGasLimitProcessor<T>>,
    final_results: &'a ExplicitSyncWrapper<Vec<E::Output>>,
    maybe_block_epilogue_txn_idx: &'a ExplicitSyncWrapper<Option<TxnIndex>>,
}
```
