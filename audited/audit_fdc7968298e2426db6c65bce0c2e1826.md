# Audit Report

## Title
Priority Inversion Attack on Storage Service Thread Pool Causes Validator State Sync Delays

## Summary
The storage service processes all request types (low-priority like `GetNumberOfStatesAtVersion` and critical like `GetEpochEndingLedgerInfos`) with equal priority using a shared blocking thread pool of 64 threads. Attackers can flood the service with low-priority requests to exhaust this pool, delaying critical epoch synchronization requests needed by validators catching up to join consensus, resulting in liveness degradation.

## Finding Description
The vulnerability exists in the storage service's request handling architecture, which processes all request types without prioritization: [1](#0-0) 

Each incoming storage request is immediately spawned as a blocking task. The runtime handling these requests has a hard limit of 64 blocking threads: [2](#0-1) 

The storage service uses its own dedicated runtime: [3](#0-2) 

Network requests are queued in a FIFO manner per `(PeerId, ProtocolId)` pair with no request-type-based prioritization: [4](#0-3) [5](#0-4) 

All request types are defined as equal variants without priority metadata: [6](#0-5) 

The request moderator only validates whether requests can be serviced, not their priority or rate: [7](#0-6) 

**Attack Path:**
1. Attacker controls multiple peers or sends high-rate requests from few peers (each peer can queue 4000 messages)
2. Sends flood of low-priority requests (e.g., `GetNumberOfStatesAtVersion` with valid version numbers)
3. Each request passes validation and spawns a blocking task
4. When >64 concurrent requests are being processed, additional requests queue internally in tokio
5. Critical `GetEpochEndingLedgerInfos` requests from validators attempting to catch up are queued behind spam
6. Validators cannot complete epoch synchronization timely, delaying their participation in consensus

## Impact Explanation
This is a **High severity** vulnerability per Aptos bug bounty criteria:
- **Validator node slowdowns**: Directly delays validators catching up to current epoch
- **Significant protocol violations**: Breaks the resource limits invariant that operations must respect computational limits
- **Liveness degradation**: If multiple validators are affected during epoch transitions, consensus progress can be significantly delayed

While this doesn't cause total network failure (existing validators continue), it prevents new/recovering validators from joining consensus in a timely manner, degrading network resilience and decentralization.

## Likelihood Explanation
**Likelihood: Medium-High**

**Attacker Requirements:**
- Any network peer can send storage service requests
- No special privileges needed
- Can use multiple peers to amplify attack (each peer queues independently)

**Feasibility:**
- Requests pass validation if version numbers are valid
- No per-request-type rate limiting exists  
- Sustained rate of ~6400 requests/sec needed to exhaust 64-thread pool (assuming 10ms/request)
- Achievable from distributed attack or high-bandwidth peers

**Mitigations Present (Insufficient):**
- Request moderator only validates request serviceability, not priority
- Per-peer invalid request counting doesn't apply to valid spam
- Network channel limits (4000/peer) don't prevent attack across multiple peers

## Recommendation
Implement request prioritization in the storage service:

1. **Add priority levels to request types** - Classify requests by criticality:
   - **Critical**: `GetEpochEndingLedgerInfos` (epoch sync)
   - **High**: Transaction/output data for active sync
   - **Low**: Metadata queries like `GetNumberOfStatesAtVersion`

2. **Use priority queues** - Replace FIFO network channel with priority-based scheduling

3. **Implement per-request-type rate limiting** - Limit low-priority requests per peer independently

4. **Reserve thread capacity for critical requests** - Dedicate subset of blocking threads for high-priority requests

Example fix (conceptual):
```rust
// In requests.rs, add priority method
impl DataRequest {
    pub fn priority(&self) -> RequestPriority {
        match self {
            Self::GetEpochEndingLedgerInfos(_) => RequestPriority::Critical,
            Self::GetNumberOfStatesAtVersion(_) => RequestPriority::Low,
            // ... classify others
        }
    }
}

// In lib.rs, use priority queue and separate thread pools
// Or use BoundedExecutor with priority-based semaphores
```

## Proof of Concept

```rust
// Conceptual PoC - demonstrates attack pattern
// In practice, would require network setup

use aptos_storage_service_types::requests::{DataRequest, StorageServiceRequest};
use std::time::Duration;

#[tokio::test]
async fn test_priority_inversion_attack() {
    // Setup: Start storage service server
    let storage_service = setup_storage_service().await;
    
    // Attacker: Flood with low-priority requests
    let attack_tasks: Vec<_> = (0..100).map(|_| {
        tokio::spawn(async {
            loop {
                let request = StorageServiceRequest::new(
                    DataRequest::GetNumberOfStatesAtVersion(1000000),
                    false
                );
                // Send request to storage service
                send_request(request).await;
                tokio::time::sleep(Duration::from_millis(10)).await;
            }
        })
    }).collect();
    
    // Victim: Validator trying to catch up
    let start = Instant::now();
    let critical_request = StorageServiceRequest::new(
        DataRequest::GetEpochEndingLedgerInfos(EpochEndingLedgerInfoRequest {
            start_epoch: 1,
            expected_end_epoch: 10,
        }),
        false
    );
    
    // This should be fast, but will be delayed due to thread pool exhaustion
    let response = send_request_with_timeout(critical_request, Duration::from_secs(5)).await;
    let elapsed = start.elapsed();
    
    // Assert: Critical request took longer than expected due to priority inversion
    assert!(elapsed > Duration::from_secs(2), "Priority inversion detected");
    
    // Cleanup
    for task in attack_tasks {
        task.abort();
    }
}
```

**Notes**
- The vulnerability is confirmed by examining the complete request handling path from network receipt through blocking task spawning
- No existing prioritization mechanism exists between different storage service request types  
- The 64-thread limit is shared across all request types, creating a clear resource contention bottleneck
- State sync clients (including validators catching up) use this same storage service network protocol for epoch synchronization
- While consensus itself uses local storage directly, validators joining/rejoining consensus depend on state sync completing successfully

### Citations

**File:** state-sync/storage-service/server/src/lib.rs (L389-419)
```rust
        while let Some(network_request) = self.network_requests.next().await {
            // All handler methods are currently CPU-bound and synchronous
            // I/O-bound, so we want to spawn on the blocking thread pool to
            // avoid starving other async tasks on the same runtime.
            let storage = self.storage.clone();
            let config = self.storage_service_config;
            let cached_storage_server_summary = self.cached_storage_server_summary.clone();
            let optimistic_fetches = self.optimistic_fetches.clone();
            let subscriptions = self.subscriptions.clone();
            let lru_response_cache = self.lru_response_cache.clone();
            let request_moderator = self.request_moderator.clone();
            let time_service = self.time_service.clone();
            self.runtime.spawn_blocking(move || {
                Handler::new(
                    cached_storage_server_summary,
                    optimistic_fetches,
                    lru_response_cache,
                    request_moderator,
                    storage,
                    subscriptions,
                    time_service,
                )
                .process_request_and_respond(
                    config,
                    network_request.peer_network_id,
                    network_request.protocol_id,
                    network_request.storage_service_request,
                    network_request.response_sender,
                );
            });
        }
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-50)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;

    // Verify the given name has an appropriate length
    if thread_name.len() > MAX_THREAD_NAME_LENGTH {
        panic!(
            "The given runtime thread name is too long! Max length: {}, given name: {}",
            MAX_THREAD_NAME_LENGTH, thread_name
        );
    }

    // Create the runtime builder
    let atomic_id = AtomicUsize::new(0);
    let thread_name_clone = thread_name.clone();
    let mut builder = Builder::new_multi_thread();
    builder
        .thread_name_fn(move || {
            let id = atomic_id.fetch_add(1, Ordering::SeqCst);
            format!("{}-{}", thread_name_clone, id)
        })
        .on_thread_start(on_thread_start)
        .disable_lifo_slot()
        // Limit concurrent blocking tasks from spawn_blocking(), in case, for example, too many
        // Rest API calls overwhelm the node.
        .max_blocking_threads(MAX_BLOCKING_THREADS)
```

**File:** aptos-node/src/state_sync.rs (L274-291)
```rust
    let storage_service_runtime = aptos_runtimes::spawn_named_runtime("stor-server".into(), None);

    // Spawn the state sync storage service servers on the runtime
    let storage_reader = StorageReader::new(
        config.storage_service,
        Arc::clone(&db_rw.reader),
        TimeService::real(),
    );
    let service = StorageServiceServer::new(
        config,
        storage_service_runtime.handle().clone(),
        storage_reader,
        TimeService::real(),
        peers_and_metadata,
        StorageServiceNetworkEvents::new(network_service_events),
        storage_service_listener,
    );
    storage_service_runtime.spawn(service.start());
```

**File:** aptos-node/src/network.rs (L147-167)
```rust
pub fn storage_service_network_configuration(node_config: &NodeConfig) -> NetworkApplicationConfig {
    let direct_send_protocols = vec![]; // The storage service does not use direct send
    let rpc_protocols = vec![ProtocolId::StorageServiceRpc];
    let max_network_channel_size = node_config
        .state_sync
        .storage_service
        .max_network_channel_size as usize;

    let network_client_config =
        NetworkClientConfig::new(direct_send_protocols.clone(), rpc_protocols.clone());
    let network_service_config = NetworkServiceConfig::new(
        direct_send_protocols,
        rpc_protocols,
        aptos_channel::Config::new(max_network_channel_size)
            .queue_style(QueueStyle::FIFO)
            .counters(
                &aptos_storage_service_server::metrics::PENDING_STORAGE_SERVER_NETWORK_EVENTS,
            ),
    );
    NetworkApplicationConfig::new(network_client_config, network_service_config)
}
```

**File:** network/framework/src/protocols/network/mod.rs (L202-210)
```rust
        peer_mgr_notifs_rx: aptos_channel::Receiver<(PeerId, ProtocolId), ReceivedMessage>,
        max_parallel_deserialization_tasks: Option<usize>,
        allow_out_of_order_delivery: bool,
    ) -> Self;
}

impl<TMessage: Message + Send + Sync + 'static> NewNetworkEvents for NetworkEvents<TMessage> {
    fn new(
        peer_mgr_notifs_rx: aptos_channel::Receiver<(PeerId, ProtocolId), ReceivedMessage>,
```

**File:** state-sync/storage-service/types/src/requests.rs (L33-56)
```rust
/// A single data request.
#[derive(Clone, Debug, Deserialize, Eq, Hash, PartialEq, Serialize)]
pub enum DataRequest {
    GetEpochEndingLedgerInfos(EpochEndingLedgerInfoRequest), // Fetches a list of epoch ending ledger infos
    GetNewTransactionOutputsWithProof(NewTransactionOutputsWithProofRequest), // Optimistically fetches new transaction outputs
    GetNewTransactionsWithProof(NewTransactionsWithProofRequest), // Optimistically fetches new transactions
    GetNumberOfStatesAtVersion(Version), // Fetches the number of states at the specified version
    GetServerProtocolVersion,            // Fetches the protocol version run by the server
    GetStateValuesWithProof(StateValuesWithProofRequest), // Fetches a list of states with a proof
    GetStorageServerSummary,             // Fetches a summary of the storage server state
    GetTransactionOutputsWithProof(TransactionOutputsWithProofRequest), // Fetches a list of transaction outputs with a proof
    GetTransactionsWithProof(TransactionsWithProofRequest), // Fetches a list of transactions with a proof
    GetNewTransactionsOrOutputsWithProof(NewTransactionsOrOutputsWithProofRequest), // Optimistically fetches new transactions or outputs
    GetTransactionsOrOutputsWithProof(TransactionsOrOutputsWithProofRequest), // Fetches a list of transactions or outputs with a proof
    SubscribeTransactionOutputsWithProof(SubscribeTransactionOutputsWithProofRequest), // Subscribes to transaction outputs with a proof
    SubscribeTransactionsOrOutputsWithProof(SubscribeTransactionsOrOutputsWithProofRequest), // Subscribes to transactions or outputs with a proof
    SubscribeTransactionsWithProof(SubscribeTransactionsWithProofRequest), // Subscribes to transactions with a proof

    // All the requests listed below are for transaction data v2 (i.e., transactions with auxiliary information).
    // TODO: eventually we should deprecate all the old request types.
    GetTransactionDataWithProof(GetTransactionDataWithProofRequest), // Fetches transaction data with a proof
    GetNewTransactionDataWithProof(GetNewTransactionDataWithProofRequest), // Optimistically fetches new transaction data with a proof
    SubscribeTransactionDataWithProof(SubscribeTransactionDataWithProofRequest), // Subscribes to transaction data with a proof
}
```

**File:** state-sync/storage-service/server/src/moderator.rs (L134-196)
```rust
    pub fn validate_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> Result<(), Error> {
        // Validate the request and time the operation
        let validate_request = || {
            // If the peer is being ignored, return an error
            if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
                if peer_state.is_ignored() {
                    return Err(Error::TooManyInvalidRequests(format!(
                        "Peer is temporarily ignored. Unable to handle request: {:?}",
                        request
                    )));
                }
            }

            // Get the latest storage server summary
            let storage_server_summary = self.cached_storage_server_summary.load();

            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }

            Ok(()) // The request is valid
        };
        utils::execute_and_time_duration(
            &metrics::STORAGE_REQUEST_VALIDATION_LATENCY,
            Some((peer_network_id, request)),
            None,
            validate_request,
            None,
        )
    }
```
