# Audit Report

## Title
Stale Global Data Summary Cache Causes Latency Monitor to Miss Advertised Versions and Under-Report Network State

## Summary
The `AptosDataClient`'s global data summary cache can remain stale for up to 100ms (one full polling interval), causing the latency monitor to miss newly advertised versions from peers and under-report the actual network state. This occurs because individual peer storage summaries are updated immediately upon receiving poll responses, but the global summary cache is only refreshed at the start of each polling loop.

## Finding Description
The vulnerability exists in the interaction between three components:

1. **Cache Update Timing** [1](#0-0) 
   The global summary cache is updated only at the beginning of each polling loop iteration.

2. **Peer Summary Updates** [2](#0-1) 
   When peer poll responses arrive (which happens asynchronously after the cache update), individual peer storage summaries are updated immediately.

3. **Latency Monitor Reads Stale Cache** [3](#0-2) 
   The latency monitor retrieves the highest advertised version by calling `get_global_data_summary()`.

4. **Cache Implementation** [4](#0-3) 
   The implementation simply returns the cached value without checking peer state freshness.

**Timeline of the Issue:**

- **T=0ms** (Poll Round N): Cache is updated with data from previous round's peer responses. New poll requests are sent to peers.
- **T=20-80ms**: Peer responses arrive with new versions (e.g., version 1100). `update_peer_storage_summary()` updates individual peer states [5](#0-4) , but the global cache remains stale (still showing version 1000).
- **T=50ms**: Latency monitor checks network state via `get_global_data_summary()`, receives stale cache showing version 1000, and **misses version 1100** for latency tracking.
- **T=100ms** (Poll Round N+1): Cache is finally updated with version 1100, but versions advertised between T=0ms and T=100ms were never tracked by the latency monitor.

The polling interval defaults to 100ms [6](#0-5) , and the latency monitor also runs at 100ms intervals [7](#0-6) , but they are started independently [8](#0-7) , creating desynchronization.

## Impact Explanation
This issue qualifies as **Medium severity** based on the following:

1. **State Inconsistency**: The latency monitor maintains state about when versions are "first seen" [9](#0-8) . Missing versions creates incorrect historical tracking.

2. **Under-Reporting Network State**: The monitor fails to accurately report what versions are actually available in the network, potentially affecting operational decisions based on these metrics.

3. **Monitoring Accuracy**: The latency metrics (seen-to-sync, propose-to-seen) are used for performance monitoring and debugging. Incorrect metrics can mask real synchronization issues.

While this doesn't directly threaten consensus safety or fund security, it creates **state inconsistencies in monitoring data** that could require operational intervention to debug sync issues, fitting the Medium severity criteria: "State inconsistencies requiring intervention."

## Likelihood Explanation
**Likelihood: High**

This issue occurs continuously during normal operation:
- Both the poller and latency monitor run at 100ms intervals by default
- Peer responses arrive asynchronously throughout each polling round
- The latency monitor can read the cache multiple times while it's stale
- In a fast-moving blockchain with frequent version updates, dozens to hundreds of versions could be missed per staleness window

The issue is deterministic and happens in every polling cycle where peer responses arrive after the cache update but before the latency monitor check.

## Recommendation

**Solution**: Update the global summary cache immediately when peer storage summaries are updated, rather than only at polling loop boundaries.

Modify the `update_peer_storage_summary` method to trigger a cache update:

```rust
// In client.rs
pub fn update_peer_storage_summary(&self, peer: PeerNetworkId, summary: StorageServerSummary) {
    self.peer_states.update_summary(peer, summary);
    
    // Immediately update the global cache to reflect the new peer data
    let _ = self.update_global_summary_cache();
}
```

Alternatively, use a more reactive approach where the cache is invalidated and lazily recomputed on next access, ensuring the latency monitor always sees fresh data.

## Proof of Concept

```rust
#[tokio::test]
async fn test_cache_staleness_causes_missed_versions() {
    use std::sync::Arc;
    use aptos_config::config::AptosDataClientConfig;
    use aptos_time_service::TimeService;
    
    // Setup data client and latency monitor
    let time_service = TimeService::mock();
    let config = Arc::new(AptosDataClientConfig::default());
    let (data_client, poller) = create_test_data_client(config, time_service.clone());
    let latency_monitor = create_latency_monitor(data_client.clone());
    
    // 1. Start of polling round - cache is updated
    data_client.update_global_summary_cache().unwrap();
    let initial_summary = data_client.get_global_data_summary();
    assert_eq!(initial_summary.advertised_data.highest_synced_ledger_info()
        .unwrap().ledger_info().version(), 1000);
    
    // 2. Simulate peer response with new version arriving mid-round
    let peer = create_test_peer();
    let new_summary = create_storage_summary_with_version(1100);
    data_client.update_peer_storage_summary(peer, new_summary);
    
    // 3. Latency monitor checks cache (before next polling round)
    let stale_summary = data_client.get_global_data_summary();
    
    // BUG: Cache still shows version 1000, missing version 1100!
    assert_eq!(stale_summary.advertised_data.highest_synced_ledger_info()
        .unwrap().ledger_info().version(), 1000); // STALE!
    
    // 4. Next polling round - cache finally updates
    time_service.advance_ms(100);
    data_client.update_global_summary_cache().unwrap();
    let fresh_summary = data_client.get_global_data_summary();
    assert_eq!(fresh_summary.advertised_data.highest_synced_ledger_info()
        .unwrap().ledger_info().version(), 1100); // NOW fresh
    
    // Result: Version 1100 was never tracked by latency monitor
    // during the staleness window, causing incorrect metrics
}
```

## Notes

The issue is exacerbated by the fact that the poller and latency monitor are started at slightly different times and run on independent tickers, making their execution unpredictable relative to each other. The 100ms staleness window may seem short, but in a high-throughput blockchain environment, this can result in significant under-reporting of network state and missed latency tracking for many versions.

### Citations

**File:** state-sync/aptos-data-client/src/poller.rs (L267-274)
```rust
    // Create and start the latency monitor
    start_latency_monitor(
        poller.data_client_config.clone(),
        poller.data_client.clone(),
        poller.storage.clone(),
        poller.time_service.clone(),
        poller.runtime.clone(),
    );
```

**File:** state-sync/aptos-data-client/src/poller.rs (L292-303)
```rust
        // Update the global storage summary
        if let Err(error) = poller.data_client.update_global_summary_cache() {
            sample!(
                SampleRate::Duration(Duration::from_secs(POLLER_LOG_FREQ_SECS)),
                warn!(
                    (LogSchema::new(LogEntry::DataSummaryPoller)
                        .event(LogEvent::AggregateSummary)
                        .message("Unable to update global summary cache!")
                        .error(&error))
                );
            );
        }
```

**File:** state-sync/aptos-data-client/src/poller.rs (L436-439)
```rust
        // Update the summary for the peer
        data_summary_poller
            .data_client
            .update_peer_storage_summary(peer, storage_summary);
```

**File:** state-sync/aptos-data-client/src/latency_monitor.rs (L126-141)
```rust
            // Get the highest advertised version from the global data summary
            let advertised_data = &self.data_client.get_global_data_summary().advertised_data;
            let highest_advertised_version = match advertised_data.highest_synced_ledger_info() {
                Some(ledger_info) => ledger_info.ledger_info().version(),
                None => {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(LATENCY_MONITOR_LOG_FREQ_SECS)),
                        warn!(
                            (LogSchema::new(LogEntry::LatencyMonitor)
                                .event(LogEvent::AggregateSummary)
                                .message("Unable to get the highest advertised version!"))
                        );
                    );
                    continue; // Continue to the next round
                },
            };
```

**File:** state-sync/aptos-data-client/src/latency_monitor.rs (L235-285)
```rust
    /// into the map and garbage collecting any old versions.
    fn update_advertised_version_timestamps(
        &mut self,
        highest_synced_version: u64,
        highest_advertised_version: u64,
    ) {
        // Check if we're still catching up to the latest version
        if !self.caught_up_to_latest {
            if highest_synced_version + MAX_VERSION_LAG_TO_TOLERATE >= highest_advertised_version {
                info!(
                    (LogSchema::new(LogEntry::LatencyMonitor)
                        .event(LogEvent::CaughtUpToLatest)
                        .message(
                            "We've caught up to the latest version! Starting the latency monitor."
                        ))
                );
                self.caught_up_to_latest = true; // We've caught up
            } else {
                sample!(
                    SampleRate::Duration(Duration::from_secs(LATENCY_MONITOR_LOG_FREQ_SECS)),
                    info!(
                        (LogSchema::new(LogEntry::LatencyMonitor)
                            .event(LogEvent::WaitingForCatchup)
                            .message("Waiting for the node to catch up to the latest version before starting the latency monitor."))
                    );
                );

                return; // We're still catching up, so we shouldn't update the advertised version timestamps
            }
        }

        // Get the current time (instant and timestamp)
        let time_now_instant = self.time_service.now();
        let timestamp_now_usecs = self.get_timestamp_now_usecs();

        // Create the advertised version metadata
        let seen_after_sync = highest_synced_version >= highest_advertised_version;
        let advertised_version_metadata =
            AdvertisedVersionMetadata::new(time_now_instant, timestamp_now_usecs, seen_after_sync);

        // Insert the newly seen version into the advertised version timestamps
        self.advertised_versions
            .insert(highest_advertised_version, advertised_version_metadata);

        // If the map is too large, garbage collect the old versions
        while self.advertised_versions.len() > MAX_NUM_TRACKED_VERSION_ENTRIES {
            // Remove the lowest version from the map by popping the first
            // item. This is possible because BTreeMaps are sorted by key.
            self.advertised_versions.pop_first();
        }
    }
```

**File:** state-sync/aptos-data-client/src/client.rs (L931-933)
```rust
    fn get_global_data_summary(&self) -> GlobalDataSummary {
        self.global_summary_cache.load().clone().deref().clone()
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L325-330)
```rust
    pub fn update_summary(&self, peer: PeerNetworkId, storage_summary: StorageServerSummary) {
        self.peer_to_state
            .entry(peer)
            .or_insert(PeerState::new(self.data_client_config.clone()))
            .update_storage_summary(storage_summary);
    }
```

**File:** config/src/config/state_sync_config.rs (L355-355)
```rust
            poll_loop_interval_ms: 100,
```

**File:** config/src/config/state_sync_config.rs (L468-468)
```rust
            latency_monitor_loop_interval_ms: 100,
```
