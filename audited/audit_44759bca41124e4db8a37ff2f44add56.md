# Audit Report

## Title
Reliable Broadcast Panic on Empty Receiver List in Randomness Consensus

## Summary
The `to_bytes_by_protocol()` function returns an empty HashMap when the peers vector is empty, which can cause a node panic when `multicast()` is called with an empty receivers list. While JWK consensus is protected by guards, the randomness consensus subsystems (rand_gen and secret_sharing) have exploitable code paths where filtered validator lists can become empty, causing validator node crashes.

## Finding Description
The security question asks about JWK consensus's `to_bytes_by_protocol()` behavior with empty peers. Investigation reveals: [1](#0-0) 

When peers is empty, `to_bytes_by_protocol()` returns `Ok(HashMap::new())` - an empty HashMap, not an error. This flows through: [2](#0-1) 

The empty HashMap itself doesn't cause immediate issues. However, when `multicast()` in the reliable broadcast library is called with an empty receivers list, it triggers a panic: [3](#0-2) 

When receivers is empty, no futures are added to `rpc_futures` (lines 164-166). The `tokio::select!` at line 168 has both streams empty, causing the `else` branch at line 203 to execute: `unreachable!("Should aggregate with all responses")`, which panics.

**JWK Consensus Protection:** [4](#0-3) 

JWK consensus has a guard: the manager only starts if `my_index.is_some()`, which requires at least one validator. Additionally, JWK consensus only calls `broadcast()` (not `multicast()` directly): [5](#0-4) 

**Vulnerable Paths in Randomness Consensus:**

Two locations call `multicast()` with filtered validator lists that can become empty: [6](#0-5) [7](#0-6) 

Both filter validators to exclude those in `existing_shares`. If all validators have already submitted shares before the broadcast task executes, `targets` becomes empty, and `multicast()` is called with an empty list, causing a panic.

## Impact Explanation
**Severity: Medium (up to $10,000)**

This violates the **Resource Limits** and **Consensus Liveness** invariants:
- Causes validator node crashes (loss of availability)
- Affects randomness consensus subsystem required for validator selection and fairness
- Does not cause consensus safety violations or funds loss
- Requires node restart to recover
- Multiple validators could crash simultaneously if conditions align

Impact is limited to availability, not safety or funds, placing this in Medium severity category per Aptos bug bounty criteria.

## Likelihood Explanation
**Likelihood: Medium**

In normal operation with low latency:
1. Randomness share request task spawns with 300ms delay
2. Fast validators submit shares during this delay  
3. All validators complete share submission before broadcast executes
4. Filtered list becomes empty
5. Node panics

This becomes more likely with:
- Small validator sets (fewer shares needed)
- Fast network conditions (shares propagate quickly)
- High validator responsiveness
- Delayed task execution (the 300ms sleep increases window)

No malicious attacker action required - this is a race condition in normal operation.

## Recommendation
Add empty receivers check before calling `multicast()` in reliable broadcast:

```rust
// In crates/reliable-broadcast/src/lib.rs, multicast function
pub fn multicast<S: BroadcastStatus<Req, Res> + 'static>(
    &self,
    message: S::Message,
    aggregating: S,
    receivers: Vec<Author>,
) -> impl Future<Output = anyhow::Result<S::Aggregated>> + 'static + use<S, Req, TBackoff, Res>
where
    <<S as BroadcastStatus<Req, Res>>::Response as TryFrom<Res>>::Error: Debug,
{
    // Add guard for empty receivers
    if receivers.is_empty() {
        return async move {
            // Return early - no aggregation needed
            Err(anyhow::anyhow!("Cannot multicast to empty receiver list"))
        }.boxed();
    }
    
    // Existing implementation...
}
```

Alternatively, fix callers in randomness consensus to check before calling multicast:

```rust
// In rand_manager.rs and secret_share_manager.rs
let targets = epoch_state
    .verifier
    .get_ordered_account_addresses_iter()
    .filter(|author| !existing_shares.contains(author))
    .collect::<Vec<_>>();

if !targets.is_empty() {
    rb.multicast(request, aggregate_state, targets)
        .await
        .expect("Broadcast cannot fail");
} else {
    info!("All validators already have shares, skipping broadcast");
}
```

## Proof of Concept
To demonstrate this vulnerability:

1. Set up local testnet with 4 validators
2. Configure randomness consensus with fast share propagation
3. Trigger randomness generation round
4. Observe share submission timing
5. If all 4 validators submit shares within the 300ms window before broadcast task executes, validator nodes crash with panic at `unreachable!` macro

Reproduction requires timing alignment but is achievable in testing environment with controlled network delays.

## Notes
- JWK consensus specifically is protected by the `my_index.is_some()` guard and does not directly call `multicast()`
- The underlying vulnerability exists in the shared `reliable-broadcast` library affecting randomness consensus
- Fix should be applied at the library level to protect all consumers
- This answers the security question: `to_bytes_by_protocol()` returns empty HashMap (not error), and yes, it can cause downstream panics in systems that call `multicast()` with filtered empty lists

### Citations

**File:** network/framework/src/application/interface.rs (L288-304)
```rust
    fn to_bytes_by_protocol(
        &self,
        peers: Vec<PeerNetworkId>,
        message: Message,
    ) -> anyhow::Result<HashMap<PeerNetworkId, Bytes>> {
        let peers_per_protocol = self.group_peers_by_protocol(peers);
        // Convert to bytes per protocol
        let mut bytes_per_peer = HashMap::new();
        for (protocol_id, peers) in peers_per_protocol {
            let bytes: Bytes = protocol_id.to_bytes(&message)?.into();
            for peer in peers {
                bytes_per_peer.insert(peer, bytes.clone());
            }
        }

        Ok(bytes_per_peer)
    }
```

**File:** crates/aptos-jwk-consensus/src/network_interface.rs (L64-79)
```rust
    pub fn to_bytes_by_protocol(
        &self,
        peers: Vec<PeerId>,
        message: JWKConsensusMsg,
    ) -> anyhow::Result<HashMap<PeerId, Bytes>> {
        let peer_network_ids: Vec<PeerNetworkId> = peers
            .into_iter()
            .map(|peer| self.get_peer_network_id_for_peer(peer))
            .collect();
        Ok(self
            .network_client
            .to_bytes_by_protocol(peer_network_ids, message)?
            .into_iter()
            .map(|(peer_network_id, bytes)| (peer_network_id.peer_id(), bytes))
            .collect())
    }
```

**File:** crates/reliable-broadcast/src/lib.rs (L167-206)
```rust
            loop {
                tokio::select! {
                    Some((receiver, result)) = rpc_futures.next() => {
                        let aggregating = aggregating.clone();
                        let future = executor.spawn(async move {
                            (
                                    receiver,
                                    result
                                        .and_then(|msg| {
                                            msg.try_into().map_err(|e| anyhow::anyhow!("{:?}", e))
                                        })
                                        .and_then(|ack| aggregating.add(receiver, ack)),
                            )
                        }).await;
                        aggregate_futures.push(future);
                    },
                    Some(result) = aggregate_futures.next() => {
                        let (receiver, result) = result.expect("spawned task must succeed");
                        match result {
                            Ok(may_be_aggragated) => {
                                if let Some(aggregated) = may_be_aggragated {
                                    return Ok(aggregated);
                                }
                            },
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
                        }
                    },
                    else => unreachable!("Should aggregate with all responses")
                }
            }
        }
```

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L161-197)
```rust
        let my_index = epoch_state
            .verifier
            .address_to_validator_index()
            .get(&self.my_addr)
            .copied();

        info!(
            epoch = epoch_state.epoch,
            "EpochManager starting new epoch."
        );

        let features = payload.get::<Features>().unwrap_or_default();
        let jwk_consensus_config = payload.get::<OnChainJWKConsensusConfig>();
        let onchain_observed_jwks = payload.get::<ObservedJWKs>().ok();
        let onchain_consensus_config = payload.get::<OnChainConsensusConfig>().unwrap_or_default();

        let (jwk_manager_should_run, oidc_providers) = match jwk_consensus_config {
            Ok(config) => {
                let should_run =
                    config.jwk_consensus_enabled() && onchain_consensus_config.is_vtxn_enabled();
                let providers = config
                    .oidc_providers_cloned()
                    .into_iter()
                    .map(jwks::OIDCProvider::from)
                    .collect();
                (should_run, Some(SupportedOIDCProviders { providers }))
            },
            Err(_) => {
                //TODO: remove this case once the framework change of this commit is published.
                let should_run = features.is_enabled(FeatureFlag::JWK_CONSENSUS)
                    && onchain_consensus_config.is_vtxn_enabled();
                let providers = payload.get::<SupportedOIDCProviders>().ok();
                (should_run, providers)
            },
        };

        if jwk_manager_should_run && my_index.is_some() {
```

**File:** crates/aptos-jwk-consensus/src/update_certifier.rs (L67-69)
```rust
        let task = async move {
            let qc_update = rb.broadcast(req, agg_state).await.expect("cannot fail");
            ConsensusMode::log_certify_done(epoch, &qc_update);
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L279-293)
```rust
                let targets = epoch_state
                    .verifier
                    .get_ordered_account_addresses_iter()
                    .filter(|author| !existing_shares.contains(author))
                    .collect::<Vec<_>>();
                info!(
                    epoch = epoch,
                    round = round,
                    "[RandManager] Start broadcasting share request for {}",
                    targets.len(),
                );
                rb.multicast(request, aggregate_state, targets)
                    .await
                    .expect("Broadcast cannot fail");
                info!(
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L253-266)
```rust
                let targets = epoch_state
                    .verifier
                    .get_ordered_account_addresses_iter()
                    .filter(|author| !existing_shares.contains(author))
                    .collect::<Vec<_>>();
                info!(
                    epoch = epoch,
                    round = metadata.round,
                    "[SecretShareManager] Start broadcasting share request for {}",
                    targets.len(),
                );
                rb.multicast(request, aggregate_state, targets)
                    .await
                    .expect("Broadcast cannot fail");
```
