# Audit Report

## Title
Resource Leak in Consensus Observer Subscription Protocol - Publisher Retains Zombie Subscribers

## Summary
When an observer receives a `SubscribeAck` response from a publisher but fails to register the subscription before a crash or task cancellation, the publisher permanently retains the peer in its active subscribers list without any cleanup notification. This creates "zombie subscribers" that consume network bandwidth and CPU resources indefinitely as the publisher continues sending consensus messages to non-functional subscriptions.

## Finding Description

The consensus observer subscription protocol has a critical gap in its resource cleanup mechanism between the observer and publisher sides:

**Publisher-Side Behavior:**
When a Subscribe RPC request is received, the publisher immediately adds the peer to its `active_subscribers` HashSet before sending the SubscribeAck response: [1](#0-0) 

**Observer-Side Behavior:**
When the observer receives a SubscribeAck, it creates a subscription object locally: [2](#0-1) 

The subscription object is then added to `active_observer_subscriptions` in an asynchronous task: [3](#0-2) 

**The Vulnerability:**
If the observer node crashes, panics, or the subscription creation task is cancelled AFTER receiving SubscribeAck (line 144) but BEFORE the subscription is registered (line 237-239), the following occurs:

1. **Publisher state**: Peer remains in `active_subscribers` indefinitely
2. **Observer state**: No subscription exists in `active_observer_subscriptions`
3. **Cleanup mechanism**: None - no Unsubscribe notification is sent to the publisher

The `ConsensusObserverSubscription` struct has no Drop implementation that would automatically send cleanup notifications: [4](#0-3) 

The publisher's garbage collection only removes peers that are **disconnected**, not inactive subscribers with live connections: [5](#0-4) 

**Exploitation Path:**
1. Malicious actor establishes connections to multiple publishers
2. Sends Subscribe RPC requests
3. Receives SubscribeAck (publisher adds attacker to active_subscribers)
4. Immediately drops the subscription objects without registering them
5. Keeps TCP connections alive
6. Publisher continuously sends messages to these zombie subscribers via `publish_message()` [6](#0-5) 

**No Publisher-Side Limits:**
Unlike the observer side which enforces `max_concurrent_subscriptions` (default: 2), the publisher has NO limit on the number of subscribers: [7](#0-6) 

This allows unbounded accumulation of zombie subscribers from multiple malicious nodes.

## Impact Explanation

**Severity: Medium to High**

This vulnerability causes **progressive resource exhaustion** on consensus publisher nodes (validators and VFNs):

1. **Network Bandwidth Waste**: Every consensus message (blocks, commits, payloads) is serialized and sent to all zombie subscribers, consuming bandwidth proportional to message size Ã— number of zombies

2. **CPU Overhead**: Message serialization tasks run for each zombie subscriber, consuming CPU cycles

3. **Channel Backpressure**: The outbound message channel (max size: 1000 by default) can experience backpressure if zombie subscribers accumulate

4. **Performance Degradation**: As zombie count increases, publisher nodes experience measurable slowdowns in consensus message propagation

Under the Aptos bug bounty criteria:
- **High Severity** justification: "Validator node slowdowns" - if enough zombie subscribers accumulate, this directly impacts validator/publisher performance
- **Medium Severity** justification: Resource exhaustion requiring operational intervention to clear zombie subscribers

The impact scales with:
- Number of malicious nodes creating zombies
- Consensus message frequency and size  
- Duration of attack (accumulation over time)

## Likelihood Explanation

**Likelihood: Medium**

This vulnerability has multiple realistic occurrence scenarios:

**Accidental Triggers:**
- Observer node crashes after receiving SubscribeAck but before subscription registration
- Kubernetes pod terminations during subscription setup
- Task cancellations during graceful shutdown
- Network interruptions causing partial state updates

**Malicious Exploitation:**
- Requires only network connectivity to publisher nodes (no validator privileges needed)
- Attacker can control timing precisely: receive SubscribeAck, then drop subscription
- Can be automated and scaled across multiple attack nodes
- Keeps connections alive to bypass garbage collection

**Barriers:**
- Attacker needs persistent network connections to targets
- Some OS-level connection limits apply
- Publisher restarts clear zombie state
- Garbage collection eventually cleans up if connections drop

The combination of accidental occurrence possibility and deliberate exploitation feasibility makes this a realistic threat.

## Recommendation

Implement automatic cleanup via a Drop implementation or explicit unsubscribe-on-drop mechanism:

**Solution 1: Explicit Cleanup Channel**
Add a cleanup notifier to the subscription struct:

```rust
pub struct ConsensusObserverSubscription {
    // ... existing fields ...
    cleanup_sender: Option<mpsc::Sender<PeerNetworkId>>,
}

impl Drop for ConsensusObserverSubscription {
    fn drop(&mut self) {
        if let Some(sender) = &mut self.cleanup_sender {
            let peer = self.peer_network_id;
            let _ = sender.try_send(peer); // Best-effort notification
        }
    }
}
```

The subscription manager should spawn a cleanup task that sends Unsubscribe RPCs for dropped subscriptions.

**Solution 2: Publisher-Side Timeout**
Add activity tracking and timeout for inactive subscribers:

```rust
struct SubscriberState {
    peer_network_id: PeerNetworkId,
    last_activity: Instant,
}

// In garbage_collect_subscriptions(), also check:
if last_activity.elapsed() > SUBSCRIBER_TIMEOUT {
    // Remove inactive subscriber even if connected
}
```

**Solution 3: Two-Phase Subscription**
Require observers to send a periodic "SubscriptionHeartbeat" RPC. Publishers remove subscribers that miss heartbeats.

**Immediate Mitigation:**
Add publisher-side limit on maximum subscribers per network to bound resource consumption:

```rust
const MAX_SUBSCRIBERS_PER_NETWORK: usize = 100;

fn add_active_subscriber(&self, peer_network_id: PeerNetworkId) -> Result<(), Error> {
    let mut subscribers = self.active_subscribers.write();
    let network_count = subscribers.iter()
        .filter(|p| p.network_id() == peer_network_id.network_id())
        .count();
    
    if network_count >= MAX_SUBSCRIBERS_PER_NETWORK {
        return Err(Error::TooManySubscribers);
    }
    
    subscribers.insert(peer_network_id);
    Ok(())
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_zombie_subscriber_resource_leak() {
    // Setup: Create publisher and observer
    let network_id = NetworkId::Public;
    let peers_and_metadata = PeersAndMetadata::new(&[network_id]);
    let network_client = NetworkClient::new(vec![], vec![], hashmap![], peers_and_metadata.clone());
    let consensus_observer_client = Arc::new(ConsensusObserverClient::new(network_client));
    
    let (consensus_publisher, _) = ConsensusPublisher::new(
        ConsensusObserverConfig::default(),
        consensus_observer_client.clone(),
    );

    // Add peer connection
    let zombie_peer = PeerNetworkId::new(network_id, PeerId::random());
    let connection_metadata = ConnectionMetadata::mock(zombie_peer.peer_id());
    peers_and_metadata.insert_connection_metadata(zombie_peer, connection_metadata).unwrap();

    // Simulate: Observer sends Subscribe, Publisher adds to active_subscribers
    let subscribe_request = ConsensusObserverRequest::Subscribe;
    let (response_tx, _response_rx) = oneshot::channel();
    let network_message = ConsensusPublisherNetworkMessage::new(
        zombie_peer,
        subscribe_request,
        ResponseSender::new(response_tx),
    );
    
    // Publisher processes subscription (adds to active_subscribers)
    consensus_publisher.process_network_message(network_message);
    
    // Verify: Peer is in active_subscribers
    assert!(consensus_publisher.get_active_subscribers().contains(&zombie_peer));
    
    // Simulate: Observer receives SubscribeAck but crashes before registration
    // (subscription object created but never added to active_observer_subscriptions)
    // The subscription object goes out of scope here without cleanup
    
    // Attack: Publish messages - they will be sent to zombie subscriber
    let test_message = ConsensusObserverMessage::new_ordered_block_message(
        vec![],
        LedgerInfoWithSignatures::new(
            LedgerInfo::new(BlockInfo::empty(), HashValue::zero()),
            AggregateSignature::empty(),
        ),
    );
    
    // This sends message to zombie subscriber, wasting resources
    consensus_publisher.publish_message(test_message.clone());
    
    // Verify: Garbage collection doesn't remove zombie (connection still alive)
    consensus_publisher.garbage_collect_subscriptions();
    assert!(consensus_publisher.get_active_subscribers().contains(&zombie_peer));
    
    // Proof: Zombie subscriber persists indefinitely, consuming resources on each publish
    for _ in 0..100 {
        consensus_publisher.publish_message(test_message.clone());
    }
    
    // Zombie still there - no cleanup mechanism exists
    assert!(consensus_publisher.get_active_subscribers().contains(&zombie_peer));
}
```

**Notes:**
- The vulnerability exists in production code affecting validators and VFNs
- No cleanup notification is sent when subscriptions are dropped observer-side
- Publisher has no timeout or activity tracking for subscribers
- Exploit requires only network access, no privileged roles
- Impact scales with number of zombie subscribers and message frequency

### Citations

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L99-137)
```rust
    fn garbage_collect_subscriptions(&self) {
        // Get the set of active subscribers
        let active_subscribers = self.get_active_subscribers();

        // Get the connected peers and metadata
        let peers_and_metadata = self.consensus_observer_client.get_peers_and_metadata();
        let connected_peers_and_metadata =
            match peers_and_metadata.get_connected_peers_and_metadata() {
                Ok(connected_peers_and_metadata) => connected_peers_and_metadata,
                Err(error) => {
                    // We failed to get the connected peers and metadata
                    warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::UnexpectedError)
                        .message(&format!(
                            "Failed to get connected peers and metadata! Error: {:?}",
                            error
                        )));
                    return;
                },
            };

        // Identify the active subscribers that are no longer connected
        let connected_peers: HashSet<PeerNetworkId> =
            connected_peers_and_metadata.keys().cloned().collect();
        let disconnected_subscribers: HashSet<PeerNetworkId> = active_subscribers
            .difference(&connected_peers)
            .cloned()
            .collect();

        // Remove any subscriptions from peers that are no longer connected
        for peer_network_id in &disconnected_subscribers {
            self.remove_active_subscriber(peer_network_id);
            info!(LogSchema::new(LogEntry::ConsensusPublisher)
                .event(LogEvent::Subscription)
                .message(&format!(
                    "Removed peer subscription due to disconnection! Peer: {:?}",
                    peer_network_id
                )));
        }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L181-192)
```rust
            ConsensusObserverRequest::Subscribe => {
                // Add the peer to the set of active subscribers
                self.add_active_subscriber(peer_network_id);
                info!(LogSchema::new(LogEntry::ConsensusPublisher)
                    .event(LogEvent::Subscription)
                    .message(&format!(
                        "New peer subscribed to consensus updates! Peer: {:?}",
                        peer_network_id
                    )));

                // Send a simple subscription ACK
                response_sender.send(ConsensusObserverResponse::SubscribeAck);
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L212-232)
```rust
    pub fn publish_message(&self, message: ConsensusObserverDirectSend) {
        // Get the active subscribers
        let active_subscribers = self.get_active_subscribers();

        // Send the message to all active subscribers
        for peer_network_id in &active_subscribers {
            // Send the message to the outbound receiver for publishing
            let mut outbound_message_sender = self.outbound_message_sender.clone();
            if let Err(error) =
                outbound_message_sender.try_send((*peer_network_id, message.clone()))
            {
                // The message send failed
                warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::SendDirectSendMessage)
                        .message(&format!(
                            "Failed to send outbound message to the receiver for peer {:?}! Error: {:?}",
                            peer_network_id, error
                    )));
            }
        }
    }
```

**File:** consensus/src/consensus_observer/observer/subscription_utils.rs (L144-162)
```rust
            Ok(ConsensusObserverResponse::SubscribeAck) => {
                // Log the successful subscription
                info!(
                    LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                        "Successfully subscribed to peer: {}!",
                        potential_peer
                    ))
                );

                // Create the new subscription
                let subscription = ConsensusObserverSubscription::new(
                    consensus_observer_config,
                    db_reader.clone(),
                    potential_peer,
                    time_service.clone(),
                );

                // Return the successful subscription
                return (Some(subscription), peers_with_failed_attempts);
```

**File:** consensus/src/consensus_observer/observer/subscription_manager.rs (L236-239)
```rust
            for subscription in new_subscriptions {
                active_observer_subscriptions
                    .lock()
                    .insert(subscription.get_peer_network_id(), subscription);
```

**File:** consensus/src/consensus_observer/observer/subscription.rs (L39-59)
```rust
impl ConsensusObserverSubscription {
    pub fn new(
        consensus_observer_config: ConsensusObserverConfig,
        db_reader: Arc<dyn DbReader>,
        peer_network_id: PeerNetworkId,
        time_service: TimeService,
    ) -> Self {
        // Get the current time
        let time_now = time_service.now();

        // Create a new subscription
        Self {
            consensus_observer_config,
            db_reader,
            peer_network_id,
            last_message_receive_time: time_now,
            last_optimality_check_time_and_peers: (time_now, HashSet::new()),
            highest_synced_version_and_time: (0, time_now),
            time_service,
        }
    }
```

**File:** config/src/config/consensus_observer_config.rs (L63-84)
```rust
impl Default for ConsensusObserverConfig {
    fn default() -> Self {
        Self {
            observer_enabled: false,
            publisher_enabled: false,
            max_network_channel_size: 1000,
            max_parallel_serialization_tasks: num_cpus::get(), // Default to the number of CPUs
            network_request_timeout_ms: 5_000,                 // 5 seconds
            garbage_collection_interval_ms: 60_000,            // 60 seconds
            max_num_pending_blocks: 150, // 150 blocks (sufficient for existing production networks)
            progress_check_interval_ms: 5_000, // 5 seconds
            max_concurrent_subscriptions: 2, // 2 streams should be sufficient
            max_subscription_sync_timeout_ms: 15_000, // 15 seconds
            max_subscription_timeout_ms: 15_000, // 15 seconds
            subscription_peer_change_interval_ms: 180_000, // 3 minutes
            subscription_refresh_interval_ms: 600_000, // 10 minutes
            observer_fallback_duration_ms: 600_000, // 10 minutes
            observer_fallback_startup_period_ms: 60_000, // 60 seconds
            observer_fallback_progress_threshold_ms: 10_000, // 10 seconds
            observer_fallback_sync_lag_threshold_ms: 15_000, // 15 seconds
        }
    }
```
