# Audit Report

## Title
Race Condition in BlockPayloadStore Allows Insertion of Stale Payloads After Cleanup

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) race condition exists between `insert_block_payload()` and `remove_blocks_for_epoch_round()` in the `BlockPayloadStore`. This allows stale block payloads from old epochs/rounds to be inserted after they have been cleaned up, leading to state inconsistency in the consensus observer.

## Finding Description

The vulnerability stems from a double-lock pattern in `insert_block_payload()` that creates a race window: [1](#0-0) 

The function acquires the `block_payloads` lock twice:
1. First at line 86 to check the size limit (lock is immediately released)
2. Second at lines 106-108 to insert the payload (lock is acquired again)

Between these two lock acquisitions, `remove_blocks_for_epoch_round()` can execute and remove blocks: [2](#0-1) 

This function uses `split_off()` to remove all blocks with `(epoch, round) <= (target_epoch, target_round)`.

**Race Scenario:**

**Thread 1** (Message Handler): Receives block payload for epoch=5, round=10
- Calls `insert_block_payload()` at line 86, checks size → passes → **releases lock**
- Prepares payload status at lines 98-103 (no lock held)

**Thread 2** (Commit Callback): Commits blocks up to epoch=5, round=15
- Calls `remove_blocks_for_epoch_round(5, 15)` 
- Acquires lock at line 117, removes all blocks ≤ (5, 15) including (5, 10) if it exists
- Releases lock

**Thread 1** (continues):
- Acquires lock at line 106-108
- **Inserts stale payload (5, 10) that should have been cleaned up**

The `block_payloads` Arc<Mutex<...>> is shared across multiple components through `get_block_payloads()`: [3](#0-2) 

This reference is passed to `ConsensusObserverPayloadManager` which can access it from the execution pipeline thread: [4](#0-3) [5](#0-4) 

The payload manager locks `block_payloads` directly during execution: [6](#0-5) 

This creates multiple concurrent access points where the race can occur.

## Impact Explanation

This vulnerability represents a **High Severity** issue per Aptos bug bounty criteria because it causes:

1. **State Inconsistency**: Stale payloads accumulate after they should be cleaned up, violating the State Consistency invariant (Critical Invariant #4). The consensus observer's view of available payloads diverges from what should exist based on committed blocks.

2. **Memory Accumulation**: Old payloads are never garbage collected properly, leading to unbounded memory growth in long-running nodes.

3. **Execution Pipeline Confusion**: The execution pipeline may fetch incorrect or outdated payloads when `get_transactions()` is called, potentially affecting block processing correctness.

4. **Protocol Violation**: The cleanup mechanism (`remove_blocks_for_epoch_round()`) is intended to maintain invariants about which blocks are available. Bypassing this breaks protocol assumptions about payload availability.

While this doesn't directly cause consensus safety violations or fund loss, it causes "significant protocol violations" and "state inconsistencies requiring intervention" which qualify as High severity issues.

## Likelihood Explanation

**Likelihood: High**

This race condition is highly likely to occur in production because:

1. **No External Synchronization**: While `ObserverBlockData` provides some protection, the `block_payloads` mutex is shared across threads through Arc cloning, allowing concurrent access from:
   - Message handling thread (inserting payloads)
   - Commit callback thread (removing payloads)
   - Execution pipeline thread (reading payloads)

2. **Common Message Patterns**: In distributed systems, messages frequently arrive out of order. A block payload for round 10 could arrive after a commit decision for round 15 is processed, triggering this exact race.

3. **No Ordering Guarantees**: The network layer provides no guarantees about message ordering between different message types (block payloads vs. commit decisions), making this race inevitable under normal operation.

4. **Reproducible**: The race window between lines 86 and 106-108 is significant (involves object creation and pattern matching), making it practically exploitable.

## Recommendation

The fix is to hold the lock continuously throughout the entire `insert_block_payload()` operation:

```rust
pub fn insert_block_payload(
    &mut self,
    block_payload: BlockPayload,
    verified_payload_signatures: bool,
) {
    // Acquire lock once and hold it for the entire operation
    let mut block_payloads = self.block_payloads.lock();
    
    // Verify that the number of payloads doesn't exceed the maximum
    let max_num_pending_blocks = self.consensus_observer_config.max_num_pending_blocks as usize;
    if block_payloads.len() >= max_num_pending_blocks {
        warn!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "Exceeded the maximum number of payloads: {:?}. Dropping block: {:?}!",
                max_num_pending_blocks,
                block_payload.block(),
            ))
        );
        return; // Drop the block if we've exceeded the maximum
    }

    // Create the new payload status
    let epoch_and_round = (block_payload.epoch(), block_payload.round());
    let payload_status = if verified_payload_signatures {
        BlockPayloadStatus::AvailableAndVerified(block_payload)
    } else {
        BlockPayloadStatus::AvailableAndUnverified(block_payload)
    };

    // Insert the new payload status (lock still held)
    block_payloads.insert(epoch_and_round, payload_status);
    // Lock is released here when block_payloads goes out of scope
}
```

Alternatively, add an epoch/round check after acquiring the second lock to reject stale insertions:

```rust
// After line 106, before insert:
let mut block_payloads = self.block_payloads.lock();

// Check if this payload is from a round that should have been cleaned up
// by comparing against the minimum round still in the map
if let Some(((min_epoch, min_round), _)) = block_payloads.first_key_value() {
    if (epoch_and_round.0, epoch_and_round.1) < (*min_epoch, *min_round) {
        // This is a stale payload, drop it
        return;
    }
}

block_payloads.insert(epoch_and_round, payload_status);
```

## Proof of Concept

```rust
#[cfg(test)]
mod race_condition_test {
    use super::*;
    use std::sync::Arc;
    use std::thread;
    use std::time::Duration;

    #[test]
    fn test_insert_remove_race_condition() {
        // Create a block payload store
        let consensus_observer_config = ConsensusObserverConfig {
            max_num_pending_blocks: 1000,
            ..ConsensusObserverConfig::default()
        };
        let block_payload_store = Arc::new(Mutex::new(
            BlockPayloadStore::new(consensus_observer_config)
        ));

        // Create payloads for rounds 10-20
        let epoch = 5u64;
        for round in 10..20 {
            let block_payload = create_block_payload(epoch, round);
            block_payload_store.lock().insert_block_payload(block_payload, true);
        }

        // Verify all payloads exist
        let initial_count = block_payload_store.lock()
            .get_block_payloads()
            .lock()
            .len();
        assert_eq!(initial_count, 10);

        // Thread 1: Try to insert a payload for round 10 (old)
        let store_clone_1 = Arc::clone(&block_payload_store);
        let handle1 = thread::spawn(move || {
            let block_payload = create_block_payload(epoch, 10);
            // Simulate the race by adding a delay between check and insert
            let mut store = store_clone_1.lock();
            let len = store.block_payloads.lock().len();
            thread::sleep(Duration::from_millis(10)); // Race window
            if len < 1000 {
                store.block_payloads.lock().insert(
                    (epoch, 10),
                    BlockPayloadStatus::AvailableAndVerified(block_payload)
                );
            }
        });

        // Thread 2: Remove blocks up to round 15
        let store_clone_2 = Arc::clone(&block_payload_store);
        let handle2 = thread::spawn(move || {
            thread::sleep(Duration::from_millis(5)); // Let thread 1 start
            store_clone_2.lock().remove_blocks_for_epoch_round(epoch, 15);
        });

        handle1.join().unwrap();
        handle2.join().unwrap();

        // Check if stale payload (round 10) was inserted after cleanup
        let final_payloads = block_payload_store.lock()
            .get_block_payloads()
            .lock()
            .clone();
        
        // BUG: Round 10 should not exist as it was cleaned up by thread 2
        // but thread 1's delayed insert re-added it
        if final_payloads.contains_key(&(epoch, 10)) {
            panic!("Race condition detected: Stale payload (5, 10) exists after cleanup!");
        }
    }
}
```

## Notes

This vulnerability is particularly concerning because:

1. **Subtle Bug**: The double-lock pattern appears safe at first glance but creates a critical race window.

2. **Real-World Occurrence**: In production networks with high message throughput and variable network delays, this race will occur frequently.

3. **Cascading Effects**: Stale payloads can accumulate indefinitely, eventually causing the `max_num_pending_blocks` limit to be reached, preventing legitimate new payloads from being inserted.

4. **Consensus Observer Specific**: This affects the consensus observer component, which is critical for light clients and full nodes that don't participate in consensus but need to observe and verify blockchain state.

The fix should be applied immediately by holding the lock continuously throughout the insertion operation to eliminate the race window entirely.

### Citations

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L74-76)
```rust
    pub fn get_block_payloads(&self) -> Arc<Mutex<BTreeMap<(u64, Round), BlockPayloadStatus>>> {
        self.block_payloads.clone()
    }
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L79-109)
```rust
    pub fn insert_block_payload(
        &mut self,
        block_payload: BlockPayload,
        verified_payload_signatures: bool,
    ) {
        // Verify that the number of payloads doesn't exceed the maximum
        let max_num_pending_blocks = self.consensus_observer_config.max_num_pending_blocks as usize;
        if self.block_payloads.lock().len() >= max_num_pending_blocks {
            warn!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Exceeded the maximum number of payloads: {:?}. Dropping block: {:?}!",
                    max_num_pending_blocks,
                    block_payload.block(),
                ))
            );
            return; // Drop the block if we've exceeded the maximum
        }

        // Create the new payload status
        let epoch_and_round = (block_payload.epoch(), block_payload.round());
        let payload_status = if verified_payload_signatures {
            BlockPayloadStatus::AvailableAndVerified(block_payload)
        } else {
            BlockPayloadStatus::AvailableAndUnverified(block_payload)
        };

        // Insert the new payload status
        self.block_payloads
            .lock()
            .insert(epoch_and_round, payload_status);
    }
```

**File:** consensus/src/consensus_observer/observer/payload_store.rs (L112-119)
```rust
    pub fn remove_blocks_for_epoch_round(&self, epoch: u64, round: Round) {
        // Determine the round to split off
        let split_off_round = round.saturating_add(1);

        // Remove the blocks from the payload store
        let mut block_payloads = self.block_payloads.lock();
        *block_payloads = block_payloads.split_off(&(epoch, split_off_round));
    }
```

**File:** consensus/src/consensus_observer/observer/epoch_state.rs (L84-118)
```rust
    pub async fn wait_for_epoch_start(
        &mut self,
        block_payloads: Arc<
            Mutex<BTreeMap<(u64, aptos_consensus_types::common::Round), BlockPayloadStatus>>,
        >,
    ) -> (
        Arc<dyn TPayloadManager>,
        OnChainConsensusConfig,
        OnChainExecutionConfig,
        OnChainRandomnessConfig,
    ) {
        // Extract the epoch state and on-chain configs
        let (epoch_state, consensus_config, execution_config, randomness_config) =
            extract_on_chain_configs(&self.node_config, &mut self.reconfig_events).await;

        // Update the local epoch state and quorum store config
        self.epoch_state = Some(epoch_state.clone());
        self.execution_pool_window_size = consensus_config.window_size();
        self.quorum_store_enabled = consensus_config.quorum_store_enabled();
        info!(
            LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                "New epoch started: {:?}. Execution pool window: {:?}. Quorum store enabled: {:?}",
                epoch_state.epoch, self.execution_pool_window_size, self.quorum_store_enabled,
            ))
        );

        // Create the payload manager
        let payload_manager: Arc<dyn TPayloadManager> = if self.quorum_store_enabled {
            Arc::new(ConsensusObserverPayloadManager::new(
                block_payloads,
                self.consensus_publisher.clone(),
            ))
        } else {
            Arc::new(DirectMempoolPayloadManager {})
        };
```

**File:** consensus/src/payload_manager/co_payload_manager.rs (L29-58)
```rust
async fn get_transactions_for_observer(
    block: &Block,
    block_payloads: &Arc<Mutex<BTreeMap<(u64, Round), BlockPayloadStatus>>>,
    consensus_publisher: &Option<Arc<ConsensusPublisher>>,
) -> ExecutorResult<(Vec<SignedTransaction>, Option<u64>, Option<u64>)> {
    // The data should already be available (as consensus observer will only ever
    // forward a block to the executor once the data has been received and verified).
    let block_payload = match block_payloads.lock().entry((block.epoch(), block.round())) {
        Entry::Occupied(mut value) => match value.get_mut() {
            BlockPayloadStatus::AvailableAndVerified(block_payload) => block_payload.clone(),
            BlockPayloadStatus::AvailableAndUnverified(_) => {
                // This shouldn't happen (the payload should already be verified)
                let error = format!(
                    "Payload data for block epoch {}, round {} is unverified!",
                    block.epoch(),
                    block.round()
                );
                return Err(InternalError { error });
            },
        },
        Entry::Vacant(_) => {
            // This shouldn't happen (the payload should already be present)
            let error = format!(
                "Missing payload data for block epoch {}, round {}!",
                block.epoch(),
                block.round()
            );
            return Err(InternalError { error });
        },
    };
```

**File:** consensus/src/payload_manager/co_payload_manager.rs (L78-92)
```rust
pub struct ConsensusObserverPayloadManager {
    txns_pool: Arc<Mutex<BTreeMap<(u64, Round), BlockPayloadStatus>>>,
    consensus_publisher: Option<Arc<ConsensusPublisher>>,
}

impl ConsensusObserverPayloadManager {
    pub fn new(
        txns_pool: Arc<Mutex<BTreeMap<(u64, Round), BlockPayloadStatus>>>,
        consensus_publisher: Option<Arc<ConsensusPublisher>>,
    ) -> Self {
        Self {
            txns_pool,
            consensus_publisher,
        }
    }
```
