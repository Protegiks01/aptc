# Audit Report

## Title
Unbounded Exponential Backoff in JWK Consensus Causes Resource Exhaustion and Prolonged InProgress States

## Summary
The JWK consensus mechanism lacks a maximum delay cap on its exponential backoff retry policy, causing retry intervals to grow unboundedly (potentially reaching hours or days). While Byzantine validators cannot directly prevent quorum formation when 2f+1 honest validators agree, the missing `max_delay()` configuration creates a resource exhaustion vulnerability where consensus states remain stuck in `InProgress` for extended periods when quorum cannot be reached due to view divergence.

## Finding Description

The JWK consensus initialization creates a `ReliableBroadcast` instance with an uncapped exponential backoff policy: [1](#0-0) 

This differs from other consensus components that properly configure `max_delay()`: [2](#0-1) 

When `update_certifier.start_produce()` initiates consensus, it spawns an async task that calls `rb.broadcast()`: [3](#0-2) 

The reliable broadcast retry mechanism treats view mismatch errors by retrying indefinitely with exponential backoff: [4](#0-3) 

The observation aggregation strictly enforces view consistency, rejecting any mismatched responses: [5](#0-4) 

**Attack Scenario:**

1. During an OIDC provider key rotation, validators query the JWK endpoint at different times
2. Validator V1 observes JWK set A (pre-rotation), starts consensus, and broadcasts requests
3. Validators V2, V3 observe JWK set B (post-rotation), start their own consensus
4. When V2, V3 respond to V1's request, they send observations for set B (their current state)
5. V1's aggregation rejects these responses due to view mismatch (set B â‰  set A)
6. The reliable broadcast retries with exponential backoff: 5ms, 10ms, 20ms, 40ms, 80ms, 160ms...
7. After 30 iterations, the delay reaches ~5.4M seconds (62 days) between retries
8. The consensus remains in `InProgress` state indefinitely [6](#0-5) 

Byzantine validators can exacerbate this by deliberately responding with stale or fabricated observations, increasing the likelihood of view divergence and forcing longer retry cycles.

## Impact Explanation

This qualifies as **High Severity** per Aptos bug bounty criteria:

- **Validator node slowdowns**: The unbounded backoff causes async tasks to sleep for extremely long periods (hours to days), consuming task executor resources
- **Significant protocol violations**: The JWK consensus mechanism fails to make timely progress, preventing validators from updating on-chain JWK sets used for OIDC authentication
- **Resource exhaustion**: Each stuck consensus task holds memory for the `InProgress` state and maintains futures that never resolve

While Byzantine validators cannot prevent quorum formation when 2f+1 honest validators agree on the same observation, the unbounded retry mechanism creates a liveness degradation that affects network health.

## Likelihood Explanation

**High likelihood** of occurrence:

1. **Natural occurrence**: OIDC providers routinely rotate JWKs, creating time windows where different validators observe different states
2. **Low attacker complexity**: Byzantine validators can trivially send mismatched observations to trigger the exponential backoff
3. **No defensive mechanisms**: Unlike other consensus components, JWK consensus lacks retry limits, timeout mechanisms, or bounded backoff
4. **Permanent impact**: Once stuck in exponential backoff, the consensus state remains degraded until epoch transition or on-chain state changes

The issue manifests in normal operation during key rotations, not requiring deliberate Byzantine manipulation.

## Recommendation

Add `max_delay()` configuration to the exponential backoff policy, consistent with other consensus components:

```rust
let rb = ReliableBroadcast::new(
    self.my_addr,
    epoch_state.verifier.get_ordered_account_addresses(),
    Arc::new(network_sender),
    ExponentialBackoff::from_millis(5)
        .factor(1000)
        .max_delay(Duration::from_millis(10000)),  // Cap at 10 seconds
    aptos_time_service::TimeService::real(),
    Duration::from_millis(1000),
    BoundedExecutor::new(8, tokio::runtime::Handle::current()),
);
```

Additional improvements:
1. Implement a maximum retry count to abort consensus after reasonable attempts
2. Add timeout detection to transition stuck `InProgress` states back to `NotStarted`
3. Consider implementing view reconciliation when validators observe divergent JWK states

## Proof of Concept

```rust
// Reproduction in crates/aptos-jwk-consensus/src/epoch_manager.rs
// Demonstrates unbounded backoff growth

#[tokio::test]
async fn test_unbounded_backoff_growth() {
    use tokio_retry::strategy::ExponentialBackoff;
    use std::time::Duration;
    
    // Current implementation (unbounded)
    let mut unbounded_backoff = ExponentialBackoff::from_millis(5);
    
    println!("Unbounded backoff delays:");
    for i in 0..35 {
        if let Some(delay) = unbounded_backoff.next() {
            println!("Retry {}: {} seconds", i, delay.as_secs_f64());
        }
    }
    // After 30 retries: ~5.4M seconds (62 days)
    // After 35 retries: ~171M seconds (5.4 years)
    
    // Recommended implementation (bounded)
    let mut bounded_backoff = ExponentialBackoff::from_millis(5)
        .factor(1000)
        .max_delay(Duration::from_millis(10000));
    
    println!("\nBounded backoff delays:");
    for i in 0..35 {
        if let Some(delay) = bounded_backoff.next() {
            println!("Retry {}: {} seconds", i, delay.as_secs_f64());
        }
    }
    // All retries capped at 10 seconds
}
```

**Notes**

The vulnerability stems from an incomplete configuration rather than a logic error. While Byzantine validators alone cannot prevent quorum formation (2f+1 honest validators can still reach consensus), the missing `max_delay()` cap creates a practical denial-of-service condition where JWK consensus tasks become stuck in exponentially growing sleep cycles. This particularly affects network liveness during OIDC provider key rotations, a routine operational event.

The fix is straightforward and follows the established pattern used throughout other consensus components in the Aptos codebase.

### Citations

**File:** crates/aptos-jwk-consensus/src/epoch_manager.rs (L204-212)
```rust
            let rb = ReliableBroadcast::new(
                self.my_addr,
                epoch_state.verifier.get_ordered_account_addresses(),
                Arc::new(network_sender),
                ExponentialBackoff::from_millis(5),
                aptos_time_service::TimeService::real(),
                Duration::from_millis(1000),
                BoundedExecutor::new(8, tokio::runtime::Handle::current()),
            );
```

**File:** consensus/src/dag/bootstrap.rs (L570-572)
```rust
        let rb_backoff_policy = ExponentialBackoff::from_millis(rb_config.backoff_policy_base_ms)
            .factor(rb_config.backoff_policy_factor)
            .max_delay(Duration::from_millis(rb_config.backoff_policy_max_delay_ms));
```

**File:** crates/aptos-jwk-consensus/src/update_certifier.rs (L67-69)
```rust
        let task = async move {
            let qc_update = rb.broadcast(req, agg_state).await.expect("cannot fail");
            ConsensusMode::log_certify_done(epoch, &qc_update);
```

**File:** crates/reliable-broadcast/src/lib.rs (L191-200)
```rust
                            Err(e) => {
                                log_rpc_failure(e, receiver);

                                let backoff_strategy = backoff_policies
                                    .get_mut(&receiver)
                                    .expect("should be present");
                                let duration = backoff_strategy.next().expect("should produce value");
                                rpc_futures
                                    .push(send_message(receiver, Some(duration)));
                            },
```

**File:** crates/aptos-jwk-consensus/src/observation_aggregation/mod.rs (L81-84)
```rust
        ensure!(
            self.local_view == peer_view,
            "adding peer observation failed with mismatched view"
        );
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L216-228)
```rust
        self.states_by_key.insert(
            (update.issuer.clone(), update.kid.clone()),
            ConsensusState::InProgress {
                my_proposal: ObservedKeyLevelUpdate {
                    author: self.my_addr,
                    observed: update,
                    signature,
                },
                abort_handle_wrapper: QuorumCertProcessGuard {
                    handle: abort_handle,
                },
            },
        );
```
