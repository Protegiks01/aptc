# Audit Report

## Title
Non-Atomic Batch Writes in Event Pruner Cause API Failures and False Corruption Errors

## Summary
The EventStorePruner performs two separate database write operations to different RocksDB instances without transaction isolation, creating a race condition window where concurrent API queries can observe partial pruning state, resulting in false "DB corruption" errors and service disruption.

## Finding Description

The EventStorePruner's `prune()` method executes two non-atomic batch writes when the internal indexer database is enabled: [1](#0-0) 

These operations write to two separate RocksDB instances:
1. **Lines 76-78**: Write to the indexer database, deleting event index entries from `EventByKeySchema` and `EventByVersionSchema`
2. **Line 80**: Write to the event database, deleting actual event data from `EventSchema`

Between these two writes, there exists a critical race condition window where the system is in an inconsistent state:
- Event indices have been deleted from the indexer DB
- Event data still exists in the event DB

When users query events via the public API during this window, the system performs index lookups that fail to find deleted index entries. The `lookup_events_by_key` method detects sequence number discontinuities and returns an error: [2](#0-1) 

This breaks the **State Consistency** invariant, which requires that "state transitions must be atomic and verifiable." The pruning operation spans two separate database writes that are observable as partial state by concurrent readers.

**Attack Path:**
1. EventStorePruner begins pruning events at versions 100-200 containing sequence numbers 50-54
2. Line 76-78 executes: Index entries for seq 50-54 are deleted from EventByKeySchema
3. **RACE WINDOW**: Before line 80 executes
4. User calls `/accounts/:address/events/:creation_number?start=48&limit=10`
5. API invokes `get_events_by_event_key`: [3](#0-2) 

6. `lookup_events_by_key` seeks index for seq 48, 49 (found), then seq 50 (missing - deleted)
7. Iterator jumps to seq 55 (next available after pruned range)
8. Sequence discontinuity detected: expected 50, found 55
9. Error returned: "DB corruption: Sequence number not continuous"

This vulnerability is exposed through public API endpoints: [4](#0-3) 

## Impact Explanation

**Severity: High** - This qualifies as "API crashes" under the High severity category (up to $50,000).

**Impact:**
- **Service Disruption**: Public event query APIs return 500 Internal Server Error during pruning operations
- **False Corruption Alerts**: Error messages incorrectly indicate "DB corruption" when the database is functioning normally
- **Application Failures**: Client applications dependent on event queries will crash or enter error states
- **Operational Confusion**: Node operators may perform unnecessary recovery procedures based on false corruption errors
- **Availability Impact**: During active pruning (which occurs regularly based on configured prune windows), event querying becomes unreliable

The vulnerability affects all nodes with pruning enabled and the internal indexer active, which is the standard configuration for full nodes serving API requests.

## Likelihood Explanation

**Likelihood: High**

This vulnerability occurs naturally during normal operations:

1. **Frequent Trigger**: Pruning runs automatically based on the configured prune window (typically daily or continuous)
2. **Wide Race Window**: The window between the two database writes depends on RocksDB write latency, which can be milliseconds to seconds under load
3. **No Special Access Required**: Any external user calling the public API can trigger the error
4. **High Traffic Scenarios**: Production APIs serving many concurrent requests have high probability of queries landing in the race window
5. **No Mitigation**: There are no locks or synchronization mechanisms protecting readers from observing partial state: [5](#0-4) 

The parallel execution of sub-pruners means the EventStorePruner can run concurrently with read operations without any coordination.

## Recommendation

Implement atomic pruning by either:

**Option 1: Single Database Write (Preferred)**
When the indexer database is separate, collect both index deletions and event deletions into a single batch for each database, ensuring each database's state remains internally consistent:

```rust
pub fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
    let mut event_db_batch = SchemaBatch::new();
    let mut indexer_batch = if self.indexer_db().is_some_and(|db| db.event_enabled()) {
        Some(SchemaBatch::new())
    } else {
        None
    };
    
    let indices_batch = indexer_batch.as_mut().unwrap_or(&mut event_db_batch);
    
    // Collect both operations before writing
    let num_events_per_version = self.ledger_db.event_db().prune_event_indices(
        current_progress,
        target_version,
        Some(indices_batch),
    )?;
    self.ledger_db.event_db().prune_events(
        num_events_per_version,
        current_progress,
        target_version,
        &mut event_db_batch,
    )?;
    event_db_batch.put::<DbMetadataSchema>(
        &DbMetadataKey::EventPrunerProgress,
        &DbMetadataValue::Version(target_version),
    )?;
    
    // Write both databases atomically (each internally consistent)
    if let Some(mut batch) = indexer_batch {
        batch.put::<InternalIndexerMetadataSchema>(
            &IndexerMetadataKey::EventPrunerProgress,
            &IndexerMetadataValue::Version(target_version),
        )?;
        self.expect_indexer_db().get_inner_db_ref().write_schemas(batch)?;
    }
    // Event DB already contains all necessary operations
    self.ledger_db.event_db().write_schemas(event_db_batch)
}
```

**Option 2: Read Lock Protection**
Implement a read-write lock where pruning acquires write lock and readers acquire read lock. This adds coordination overhead but ensures readers never observe partial state.

## Proof of Concept

```rust
#[cfg(test)]
mod test_race_condition {
    use super::*;
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    #[test]
    fn test_concurrent_read_during_pruning() {
        // Setup: Create AptosDB with events and enable pruning
        let tmpdir = TempPath::new();
        let db = AptosDB::new_for_test(&tmpdir);
        
        // Insert events with sequence numbers 0-100
        let event_key = EventKey::random();
        for seq in 0..100 {
            let events = vec![ContractEvent::new_v1(
                event_key,
                seq,
                TypeTag::Bool,
                bcs::to_bytes(&true).unwrap(),
            )];
            db.save_transactions(..., events, ...)?;
        }
        
        // Start pruning in background thread
        let db_clone = db.clone();
        let barrier = Arc::new(Barrier::new(2));
        let barrier_clone = barrier.clone();
        
        let pruner_thread = thread::spawn(move || {
            let pruner = EventStorePruner::new(...);
            barrier_clone.wait(); // Synchronize start
            pruner.prune(0, 50) // Prune first 50 events
        });
        
        // Query events concurrently
        let query_thread = thread::spawn(move || {
            barrier.wait(); // Synchronize start
            thread::sleep(Duration::from_millis(5)); // Hit race window
            
            // This should succeed but will fail with "DB corruption"
            db.get_events_by_event_key(&event_key, 45, Order::Ascending, 10, 100)
        });
        
        let pruner_result = pruner_thread.join().unwrap();
        let query_result = query_thread.join().unwrap();
        
        // Assertion: Query should succeed but will fail during race condition
        assert!(query_result.is_err());
        assert!(query_result.unwrap_err().to_string().contains("not continuous"));
    }
}
```

## Notes

This vulnerability only manifests when:
1. The internal indexer database is enabled and separated from the main ledger database
2. Event indexing is active (`indexer_db.event_enabled() == true`)
3. Pruning is configured and running

The issue does not affect configurations where all data resides in a single database (the fallback path at lines 52-54), as both indices and events are written in a single atomic batch operation. [6](#0-5) 

The vulnerability represents a violation of ACID properties specifically regarding Isolation, where concurrent transactions can observe intermediate states of ongoing operations.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L47-54)
```rust
        let indices_batch = if let Some(indexer_db) = self.indexer_db() {
            if indexer_db.event_enabled() {
                indexer_batch = Some(SchemaBatch::new());
            }
            indexer_batch.as_mut()
        } else {
            Some(&mut batch)
        };
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L76-80)
```rust
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
```

**File:** storage/aptosdb/src/event_store/mod.rs (L130-136)
```rust
            if seq != cur_seq {
                let msg = if cur_seq == start_seq_num {
                    "First requested event is probably pruned."
                } else {
                    "DB corruption: Sequence number not continuous."
                };
                db_other_bail!("{} expected: {}, actual: {}", msg, cur_seq, seq);
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L1132-1137)
```rust
        let mut event_indices = self.event_store.lookup_events_by_key(
            event_key,
            first_seq,
            real_limit,
            ledger_version,
        )?;
```

**File:** api/src/events.rs (L47-88)
```rust
    async fn get_events_by_creation_number(
        &self,
        accept_type: AcceptType,
        /// Hex-encoded 32 byte Aptos account, with or without a `0x` prefix, for
        /// which events are queried. This refers to the account that events were
        /// emitted to, not the account hosting the move module that emits that
        /// event type.
        address: Path<Address>,
        /// Creation number corresponding to the event stream originating
        /// from the given account.
        creation_number: Path<U64>,
        /// Starting sequence number of events.
        ///
        /// If unspecified, by default will retrieve the most recent events
        start: Query<Option<U64>>,
        /// Max number of events to retrieve.
        ///
        /// If unspecified, defaults to default page size
        limit: Query<Option<u16>>,
    ) -> BasicResultWith404<Vec<VersionedEvent>> {
        fail_point_poem("endpoint_get_events_by_event_key")?;
        self.context
            .check_api_output_enabled("Get events by event key", &accept_type)?;
        let page = Page::new(
            start.0.map(|v| v.0),
            limit.0,
            self.context.max_events_page_size(),
        );

        // Ensure that account exists
        let api = self.clone();
        api_spawn_blocking(move || {
            let account = Account::new(api.context.clone(), address.0, None, None, None)?;
            api.list(
                account.latest_ledger_info,
                accept_type,
                page,
                EventKey::new(creation_number.0 .0, address.0.into()),
            )
        })
        .await
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```
