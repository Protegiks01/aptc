# Audit Report

## Title
Decompression Bomb Vulnerability in Telemetry Service Log Ingestion Endpoint Allowing Memory Exhaustion

## Summary
The `handle_log_ingest()` function in the Aptos telemetry service contains a decompression bomb vulnerability where the `content_length_limit` checks only the compressed payload size (≤1MB), but gzip decompression occurs afterward without any bounds checking. An authenticated attacker can send a highly compressed payload that expands to gigabytes when decompressed, causing memory exhaustion and service denial.

## Finding Description

The vulnerability exists in the request handling pipeline of the telemetry service log ingestion endpoint. The code flow is:

1. **Content-Length Check (Compressed Size Only)**: The warp filter applies `content_length_limit(MAX_CONTENT_LENGTH)` where `MAX_CONTENT_LENGTH = 1024 * 1024` (1MB). [1](#0-0) 

2. **Body Aggregation**: The compressed body is aggregated [2](#0-1) 

3. **Unsafe Decompression**: If the `Content-Encoding` header is "gzip", the code creates a `GzDecoder` and directly pipes it to `serde_json::from_reader()` without checking the decompressed size [3](#0-2) 

The core issue is that the compressed payload can be ≤1MB but expand to an arbitrary size during decompression. The `serde_json::from_reader()` call attempts to deserialize the entire decompressed stream into `Vec<String>`, allocating memory for the full expanded data.

**Attack Authentication**: While the endpoint requires authentication, the authentication mechanism accepts "Unknown" node types. Any peer with a valid keypair can authenticate as `PeerRole::Unknown` and then access the log ingestion endpoint. [4](#0-3) [5](#0-4) 

**Contrast with Safe Pattern**: The codebase includes a safe decompression pattern in `aptos-compression/src/lib.rs` that checks the decompressed size BEFORE allocating memory and rejects payloads exceeding a maximum size. [6](#0-5) [7](#0-6) 

The log ingestion code does not follow this pattern.

## Impact Explanation

**Actual Impact**: This vulnerability affects the **telemetry service**, which is a separate auxiliary service that collects monitoring data from Aptos nodes. The telemetry service does not run on validator nodes themselves but operates as an independent component. [8](#0-7) 

An attacker can cause memory exhaustion and crash the telemetry service, disrupting monitoring and observability infrastructure. However, this does NOT directly affect:
- Validator node operation or consensus
- Transaction processing or execution
- State management or storage
- On-chain governance or staking

**Severity Assessment**: While the vulnerability is real and exploitable, its impact is limited to an auxiliary monitoring service rather than core blockchain infrastructure. Per the Aptos bug bounty criteria:
- This qualifies as "API crashes" (the telemetry service API crashes)
- However, it is NOT "Validator node slowdowns" as the telemetry service runs separately from validator nodes

The appropriate severity classification depends on whether the telemetry service is considered critical infrastructure. Given it's an auxiliary monitoring component, this would likely fall between **Medium-High** severity rather than meeting the strict "High" criteria focused on validator node impacts.

## Likelihood Explanation

**High Likelihood**: The attack is straightforward to execute:
1. Any actor can generate a valid keypair
2. Authenticate to the telemetry service as an "Unknown" node (no special privileges required)
3. Craft a gzip bomb (standard attack technique - compress repetitive data like `['A' * 1000000] * 10000` into <1MB)
4. Send with `Content-Encoding: gzip` header
5. Service attempts to decompress and deserialize gigabytes of data, exhausting memory

The attack requires minimal resources and no privileged access beyond basic authentication.

## Recommendation

Implement bounded decompression following the existing safe pattern in `aptos-compression`. The fix should:

1. **Check decompressed size before allocation**: Parse the compressed stream metadata or incrementally decompress while tracking size
2. **Enforce maximum decompressed size limit**: Set `MAX_DECOMPRESSED_SIZE` (e.g., 10MB) and reject payloads exceeding it
3. **Use streaming decompression with limits**: Implement a `LimitedReader` wrapper that tracks bytes read and aborts if exceeded

**Recommended Fix Pattern**:
```rust
// Add constant for max decompressed size
const MAX_DECOMPRESSED_SIZE: usize = 10 * 1024 * 1024; // 10MB

// In handle_log_ingest, replace unsafe decompression:
let log_messages: Vec<String> = if let Some(encoding) = encoding {
    if encoding.eq_ignore_ascii_case("gzip") {
        let mut decoder = GzDecoder::new(body.reader());
        let mut decompressed = Vec::new();
        
        // Use take() to limit read size
        let mut limited_reader = decoder.take(MAX_DECOMPRESSED_SIZE as u64);
        limited_reader.read_to_end(&mut decompressed).map_err(|e| {
            debug!("decompression failed or exceeded size limit: {}", e);
            ServiceError::bad_request(LogIngestError::PayloadTooLarge.into())
        })?;
        
        // Check if limit was reached (indicates potential attack)
        if decompressed.len() >= MAX_DECOMPRESSED_SIZE {
            return Err(reject::custom(ServiceError::bad_request(
                LogIngestError::PayloadTooLarge.into()
            )));
        }
        
        serde_json::from_slice(&decompressed).map_err(|e| {
            debug!("unable to deserialize body: {}", e);
            ServiceError::bad_request(LogIngestError::UnexpectedPayloadBody.into())
        })?
    } else {
        return Err(reject::custom(ServiceError::bad_request(
            LogIngestError::UnexpectedContentEncoding.into(),
        )));
    }
} else {
    serde_json::from_reader(body.reader()).map_err(|e| {
        error!("unable to deserialize body: {}", e);
        ServiceError::bad_request(LogIngestError::UnexpectedPayloadBody.into())
    })?
};
```

**Alternative**: Use the existing `aptos-compression` crate's safe decompression functions if adapting them to work with gzip format.

## Proof of Concept

```rust
// PoC: Generate a gzip bomb that compresses to <1MB but expands to 100MB
use flate2::write::GzEncoder;
use flate2::Compression;
use std::io::Write;

#[test]
fn test_decompression_bomb() {
    // Create repetitive data that compresses well
    let mut large_vec = Vec::new();
    for _ in 0..10000 {
        large_vec.push("A".repeat(10000)); // 100MB total when expanded
    }
    
    let json_data = serde_json::to_vec(&large_vec).unwrap();
    println!("Original size: {} bytes", json_data.len());
    
    // Compress the data
    let mut encoder = GzEncoder::new(Vec::new(), Compression::best());
    encoder.write_all(&json_data).unwrap();
    let compressed = encoder.finish().unwrap();
    
    println!("Compressed size: {} bytes", compressed.len());
    println!("Compression ratio: {}x", json_data.len() / compressed.len());
    
    // If compressed size is < 1MB but expands to 100MB, attack succeeds
    assert!(compressed.len() < 1024 * 1024, "Compressed payload must be <1MB");
    assert!(json_data.len() > 10 * 1024 * 1024, "Decompressed must be >10MB");
    
    // Attacker would send this compressed payload with:
    // - Content-Encoding: gzip header
    // - Valid authentication JWT
    // Result: Service attempts to deserialize 100MB into memory, causing exhaustion
}
```

## Notes

This vulnerability demonstrates a classic decompression bomb attack pattern where input validation occurs on compressed data but decompression happens without bounds checking. While the vulnerability is real and exploitable, its practical impact is limited to the auxiliary telemetry monitoring service rather than core blockchain infrastructure. Organizations should consider the criticality of their telemetry service when prioritizing remediation.

The same vulnerability pattern may exist in other endpoints that accept compressed data (e.g., `prometheus_push_metrics.rs`, `custom_contract_ingest.rs`) and should be audited similarly.

### Citations

**File:** crates/aptos-telemetry-service/src/constants.rs (L5-5)
```rust
pub const MAX_CONTENT_LENGTH: u64 = 1024 * 1024;
```

**File:** crates/aptos-telemetry-service/src/log_ingest.rs (L27-33)
```rust
        .and(with_auth(context, vec![
            NodeType::Validator,
            NodeType::ValidatorFullNode,
            NodeType::PublicFullNode,
            NodeType::UnknownFullNode,
            NodeType::UnknownValidator,
        ]))
```

**File:** crates/aptos-telemetry-service/src/log_ingest.rs (L35-36)
```rust
        .and(warp::body::content_length_limit(MAX_CONTENT_LENGTH))
        .and(warp::body::aggregate())
```

**File:** crates/aptos-telemetry-service/src/log_ingest.rs (L64-70)
```rust
    let log_messages: Vec<String> = if let Some(encoding) = encoding {
        if encoding.eq_ignore_ascii_case("gzip") {
            let decoder = GzDecoder::new(body.reader());
            serde_json::from_reader(decoder).map_err(|e| {
                debug!("unable to decode and deserialize body: {}", e);
                ServiceError::bad_request(LogIngestError::UnexpectedPayloadBody.into())
            })?
```

**File:** crates/aptos-telemetry-service/src/auth.rs (L88-102)
```rust
                None => {
                    // if not, verify that their peerid is constructed correctly from their public key
                    let derived_remote_peer_id =
                        aptos_types::account_address::from_identity_public_key(remote_public_key);
                    if derived_remote_peer_id != body.peer_id {
                        return Err(reject::custom(ServiceError::forbidden(
                            ServiceErrorCode::AuthError(
                                AuthError::PublicKeyMismatch,
                                body.chain_id,
                            ),
                        )));
                    } else {
                        Ok((*epoch, PeerRole::Unknown))
                    }
                },
```

**File:** crates/aptos-compression/src/lib.rs (L91-121)
```rust
/// Decompresses the compressed data stream
pub fn decompress(
    compressed_data: &CompressedData,
    client: CompressionClient,
    max_size: usize,
) -> Result<Vec<u8>, Error> {
    // Start the decompression timer
    let start_time = Instant::now();

    // Check size of the data and initialize raw_data
    let decompressed_size = match get_decompressed_size(compressed_data, max_size) {
        Ok(size) => size,
        Err(error) => {
            let error_string = format!("Failed to get decompressed size: {}", error);
            return create_decompression_error(&client, error_string);
        },
    };
    let mut raw_data = vec![0u8; decompressed_size];

    // Decompress the data
    if let Err(error) = lz4::block::decompress_to_buffer(compressed_data, None, &mut raw_data) {
        let error_string = format!("Failed to decompress the data: {}", error);
        return create_decompression_error(&client, error_string);
    };

    // Stop the timer and update the metrics
    metrics::observe_decompression_operation_time(&client, start_time);
    metrics::update_decompression_metrics(&client, compressed_data, &raw_data);

    Ok(raw_data)
}
```

**File:** crates/aptos-compression/src/lib.rs (L150-184)
```rust
fn get_decompressed_size(
    compressed_data: &CompressedData,
    max_size: usize,
) -> Result<usize, Error> {
    // Ensure that the compressed data is at least 4 bytes long
    if compressed_data.len() < 4 {
        return Err(DecompressionError(format!(
            "Compressed data must be at least 4 bytes long! Got: {}",
            compressed_data.len()
        )));
    }

    // Parse the size prefix
    let size = (compressed_data[0] as i32)
        | ((compressed_data[1] as i32) << 8)
        | ((compressed_data[2] as i32) << 16)
        | ((compressed_data[3] as i32) << 24);
    if size < 0 {
        return Err(DecompressionError(format!(
            "Parsed size prefix in buffer must not be negative! Got: {}",
            size
        )));
    }

    // Ensure that the size is not greater than the max size limit
    let size = size as usize;
    if size > max_size {
        return Err(DecompressionError(format!(
            "Parsed size prefix in buffer is too big: {} > {}",
            size, max_size
        )));
    }

    Ok(size)
}
```

**File:** crates/aptos-telemetry-service/e2e-test/setup.sh (L537-537)
```shellscript
    RUST_LOG=debug nohup cargo run -- -f "$SCRIPT_DIR/telemetry-config.yaml" > "$TEST_DIR/telemetry.log" 2>&1 &
```
