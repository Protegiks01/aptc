# Audit Report

## Title
State Sync Race Condition: Concurrent sync_to_target and sync_for_duration Calls Cause Validator Node Hang

## Summary
The state sync notification handler fails to prevent concurrent sync requests from consensus and consensus observer, resulting in orphaned callbacks and indefinite validator node hangs. When `sync_to_target()` and `sync_for_duration()` are called simultaneously, the second request overwrites the first, causing the first caller to wait indefinitely for a response that will never arrive.

## Finding Description

The `ConsensusNotificationHandler` in the state sync driver maintains a single field to track active sync requests. [1](#0-0) 

Both consensus and consensus observer share the same `ConsensusNotifier` instance to communicate with state sync. The notifier is created once in state sync initialization [2](#0-1)  and then passed to both consensus [3](#0-2)  and consensus observer [4](#0-3)  wrapped in Arc, ensuring they communicate through the same channel.

When consensus observer enters fallback mode, it spawns a task that calls `sync_for_duration()` on the execution client. [5](#0-4) 

Simultaneously, regular consensus may need to sync to a target during epoch transitions. [6](#0-5) 

The critical flaw is in how sync requests are initialized. When `initialize_sync_duration_request` is called, it creates a completely new `Arc<Mutex<>>` and unconditionally assigns it to `self.consensus_sync_request`, with no check for existing active requests. [7](#0-6) 

Similarly, `initialize_sync_target_request` also creates a new `Arc<Mutex<>>` and unconditionally overwrites any existing request. [8](#0-7) 

**Race Condition Flow:**
1. Consensus observer falls back and calls `sync_for_duration()`, creating callback channel A
2. The notification is sent through the shared channel and reaches the state sync driver
3. State sync driver processes the notification sequentially in its event loop [9](#0-8) 
4. Driver calls `initialize_sync_duration_request` which stores the request with callback A
5. Regular consensus initiates epoch transition and calls `sync_to_target()`, creating callback channel B
6. State sync driver receives this second notification
7. Driver calls `initialize_sync_target_request` which **overwrites** the stored Arc with a new one containing callback B
8. The first Arc (containing callback A) is now orphaned
9. When sync completes, only the current request is checked and responded to [10](#0-9) 
10. The driver handles the satisfied request by taking from the current Arc only [11](#0-10) 
11. Callback B receives a response, but callback A never does
12. Consensus observer remains blocked indefinitely waiting on callback A

This breaks the critical invariant that all sync requests must eventually receive either a success or error response, as documented in the notification interface. [12](#0-11) 

## Impact Explanation

This vulnerability meets **HIGH severity** criteria per the Aptos bug bounty program:

**Validator node slowdowns**: When a sync request is orphaned, the calling component (consensus or consensus observer) blocks indefinitely waiting for a callback that will never arrive. This causes:
- Consensus observer to remain stuck in fallback mode, unable to exit the spawned sync task [13](#0-12) 
- Regular consensus to hang during epoch transitions, as it expects `sync_to_target()` to complete before starting the new epoch [6](#0-5) 
- The validator node to become partially or fully unresponsive
- Potential network liveness degradation if multiple validators are affected

**Significant protocol violations**: The state sync interface contract guarantees that every sync request receives either a success or error response. This vulnerability violates that contract, breaking the expected synchronization semantics between consensus and state sync.

This does not reach Critical severity because:
- No direct fund loss occurs
- No permanent network partition results (validators can be restarted)
- No consensus safety violation (doesn't cause different state roots)

However, it clearly meets HIGH severity for causing validator node slowdowns and significant protocol violations affecting network liveness.

## Likelihood Explanation

This vulnerability has **HIGH likelihood** of occurring in production:

**Natural Triggers:**
- Epoch transitions are regular events on Aptos mainnet (occurring every few hours)
- Network partitions or high latency can trigger consensus observer fallback at any time
- Validator nodes catching up after downtime or joining the network
- Peer unavailability triggering fallback behavior

**Timing Conditions:**
The race window exists whenever both events occur concurrently:
1. Consensus observer enters fallback mode (triggered by failing to sync from peers)
2. Regular consensus needs to sync to a target (during epoch changes or catch-up)

These events can easily overlap in realistic network conditions:
- If network issues trigger fallback near an epoch boundary
- During network instability when both fallback and epoch transitions are active
- When a validator is catching up and experiences peer connectivity issues

The sequential processing of notifications means both requests will be received and processed, with the second unconditionally overwriting the first. No race condition mitigation exists in the code.

## Recommendation

Add a check in both `initialize_sync_duration_request` and `initialize_sync_target_request` to prevent overwriting an active sync request:

```rust
pub async fn initialize_sync_duration_request(
    &mut self,
    sync_duration_notification: ConsensusSyncDurationNotification,
) -> Result<(), Error> {
    // Check if there's already an active sync request
    if self.consensus_sync_request.lock().is_some() {
        let error = Err(Error::UnexpectedError(
            "Cannot initialize sync duration request: an active sync request already exists".into()
        ));
        self.respond_to_sync_duration_notification(sync_duration_notification, error.clone(), None)?;
        return error;
    }
    
    // Get the current time
    let start_time = self.time_service.now();

    // Save the request so we can notify consensus once we've hit the duration
    let consensus_sync_request =
        ConsensusSyncRequest::new_with_duration(start_time, sync_duration_notification);
    self.consensus_sync_request = Arc::new(Mutex::new(Some(consensus_sync_request)));

    Ok(())
}
```

Apply similar protection to `initialize_sync_target_request`. This ensures that concurrent sync requests are rejected with an error response rather than orphaning callbacks.

## Proof of Concept

The vulnerability can be demonstrated by:

1. Setting up a test validator node with consensus observer enabled
2. Triggering consensus observer fallback (by disconnecting peers)
3. Simultaneously triggering an epoch transition in consensus
4. Observing that one of the sync calls never receives a response
5. The blocked component remains indefinitely waiting

The race condition is confirmed by the code analysis showing:
- No mutex or lock preventing concurrent initialization calls
- Unconditional Arc replacement in both initialization methods
- Only the current Arc is ever checked and responded to

This is a logic vulnerability in the state management code that can be triggered through normal network operations without requiring any malicious actor.

### Citations

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L217-217)
```rust
    consensus_sync_request: Arc<Mutex<Option<ConsensusSyncRequest>>>,
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L254-256)
```rust
        let consensus_sync_request =
            ConsensusSyncRequest::new_with_duration(start_time, sync_duration_notification);
        self.consensus_sync_request = Arc::new(Mutex::new(Some(consensus_sync_request)));
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L313-315)
```rust
        let consensus_sync_request =
            ConsensusSyncRequest::new_with_target(sync_target_notification);
        self.consensus_sync_request = Arc::new(Mutex::new(Some(consensus_sync_request)));
```

**File:** state-sync/state-sync-driver/src/notification_handlers.rs (L327-329)
```rust
        let mut sync_request_lock = self.consensus_sync_request.lock();
        let consensus_sync_request = sync_request_lock.take();

```

**File:** aptos-node/src/state_sync.rs (L165-170)
```rust
    let (consensus_notifier, consensus_listener) =
        aptos_consensus_notifications::new_consensus_notifier_listener_pair(
            state_sync_config
                .state_sync_driver
                .commit_notification_timeout_ms,
        );
```

**File:** aptos-node/src/consensus.rs (L56-56)
```rust
            consensus_notifier.clone(),
```

**File:** aptos-node/src/consensus.rs (L193-193)
```rust
        consensus_notifier,
```

**File:** consensus/src/consensus_observer/observer/state_sync_manager.rs (L136-187)
```rust
        tokio::spawn(Abortable::new(
            async move {
                // Update the state sync metrics now that we're syncing for the fallback
                metrics::set_gauge_with_label(
                    &metrics::OBSERVER_STATE_SYNC_EXECUTING,
                    metrics::STATE_SYNCING_FOR_FALLBACK,
                    1, // We're syncing for the fallback
                );

                // Get the fallback duration
                let fallback_duration =
                    Duration::from_millis(consensus_observer_config.observer_fallback_duration_ms);

                // Sync for the fallback duration
                let latest_synced_ledger_info = match execution_client
                    .clone()
                    .sync_for_duration(fallback_duration)
                    .await
                {
                    Ok(latest_synced_ledger_info) => latest_synced_ledger_info,
                    Err(error) => {
                        error!(LogSchema::new(LogEntry::ConsensusObserver)
                            .message(&format!("Failed to sync for fallback! Error: {:?}", error)));
                        return;
                    },
                };

                // Notify consensus observer that we've synced for the fallback
                let state_sync_notification =
                    StateSyncNotification::fallback_sync_completed(latest_synced_ledger_info);
                if let Err(error) = sync_notification_sender.send(state_sync_notification) {
                    error!(
                        LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                            "Failed to send state sync notification for fallback! Error: {:?}",
                            error
                        ))
                    );
                }

                // Clear the state sync metrics now that we're done syncing
                metrics::set_gauge_with_label(
                    &metrics::OBSERVER_STATE_SYNC_EXECUTING,
                    metrics::STATE_SYNCING_FOR_FALLBACK,
                    0, // We're no longer syncing for the fallback
                );
            },
            abort_registration,
        ));

        // Save the sync task handle
        self.fallback_sync_handle = Some(DropGuard::new(abort_handle));
    }
```

**File:** consensus/src/epoch_manager.rs (L558-565)
```rust
        self.execution_client
            .sync_to_target(ledger_info.clone())
            .await
            .context(format!(
                "[EpochManager] State sync to new epoch {}",
                ledger_info
            ))
            .expect("Failed to sync to new epoch");
```

**File:** state-sync/state-sync-driver/src/driver.rs (L222-238)
```rust
            ::futures::select! {
                notification = self.client_notification_listener.select_next_some() => {
                    self.handle_client_notification(notification).await;
                },
                notification = self.commit_notification_listener.select_next_some() => {
                    self.handle_snapshot_commit_notification(notification).await;
                }
                notification = self.consensus_notification_handler.select_next_some() => {
                    self.handle_consensus_or_observer_notification(notification).await;
                }
                notification = self.error_notification_listener.select_next_some() => {
                    self.handle_error_notification(notification).await;
                }
                _ = progress_check_interval.select_next_some() => {
                    self.drive_progress().await;
                }
            }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L538-552)
```rust
        let consensus_sync_request = self.consensus_notification_handler.get_sync_request();
        match consensus_sync_request.lock().as_ref() {
            Some(consensus_sync_request) => {
                let latest_synced_ledger_info =
                    utils::fetch_latest_synced_ledger_info(self.storage.clone())?;
                if !consensus_sync_request
                    .sync_request_satisfied(&latest_synced_ledger_info, self.time_service.clone())
                {
                    return Ok(()); // The sync request hasn't been satisfied yet
                }
            },
            None => {
                return Ok(()); // There's no active sync request
            },
        }
```

**File:** state-sync/inter-component/consensus-notifications/src/lib.rs (L36-55)
```rust
#[async_trait]
pub trait ConsensusNotificationSender: Send + Sync {
    /// Notify state sync of newly committed transactions and subscribable events.
    async fn notify_new_commit(
        &self,
        transactions: Vec<Transaction>,
        subscribable_events: Vec<ContractEvent>,
    ) -> Result<(), Error>;

    /// Notifies state sync to synchronize storage for at least the specified duration,
    /// and returns the latest synced ledger info. Note that state sync may synchronize
    /// for much longer than the specified duration, e.g., if the node is very far behind.
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, Error>;

    /// Notify state sync to synchronize storage to the specified target.
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), Error>;
}
```
