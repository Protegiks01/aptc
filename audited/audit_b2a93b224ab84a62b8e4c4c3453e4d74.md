# Audit Report

## Title
Network Amplification Attack via Unconstrained EpochRetrievalRequest Range

## Summary
The `EpochRetrievalRequest` handler in the consensus layer lacks range size validation, allowing authenticated peers to trigger network amplification attacks with an ~86,000x amplification factor (16-byte request → ~1.38 MB response). This enables malicious validators to exhaust bandwidth, CPU, and memory resources of other validators, degrading consensus performance.

## Finding Description

The `EpochRetrievalRequest` structure contains only two u64 fields (`start_epoch` and `end_epoch`), totaling 16 bytes: [1](#0-0) 

When processed, the handler retrieves epoch-ending ledger infos from the database without validating the range size: [2](#0-1) 

The only validation performed is checking that `end_epoch <= self.epoch()`: [3](#0-2) 

The database implementation caps responses at `MAX_NUM_EPOCH_ENDING_LEDGER_INFO = 100` ledger infos: [4](#0-3) [5](#0-4) 

Each `LedgerInfoWithSignatures` contains a `LedgerInfo` and an `AggregateSignature`. When epoch changes occur, the optional `next_epoch_state` field includes a full `ValidatorVerifier` with all validator public keys and voting powers: [6](#0-5) [7](#0-6) 

A malicious validator can exploit this by:
1. Sending continuous `EpochRetrievalRequest` messages with `start_epoch = current_epoch - 99` and `end_epoch = current_epoch`
2. Each request triggers retrieval of 100 epoch-ending ledger infos from storage
3. With ~100 validators per epoch, each `LedgerInfoWithSignatures` with epoch change data is ~13.8 KB (validator keys, signatures, metadata)
4. Total response size: 100 × 13.8 KB ≈ 1.38 MB per request
5. Amplification factor: 1.38 MB / 16 bytes ≈ 86,000x

Unlike block retrieval which validates the requested range size, epoch retrieval has no such protection: [8](#0-7) 

There is no caching, rate limiting, or per-peer throttling for epoch retrieval requests, allowing sustained attacks.

## Impact Explanation

This vulnerability meets **High Severity** criteria per the Aptos Bug Bounty program: "Validator node slowdowns."

A single malicious validator can continuously send small requests that force victim validators to:
- Execute expensive database queries for 100 epochs repeatedly
- Serialize ~1.38 MB of response data per request
- Transmit large responses consuming network bandwidth
- Process potentially hundreds of concurrent requests (no effective rate limiting on direct send messages)

With multiple colluding validators or sustained attacks:
- Network bandwidth can be exhausted (saturating 10 Gbps requires ~900 requests/second, easily achievable)
- CPU resources are consumed by repeated database queries and serialization
- Memory is consumed by buffering large responses
- Consensus performance degrades due to resource contention
- In extreme cases, validator liveness may be affected

This breaks the **Resource Limits** invariant (#9): "All operations must respect gas, storage, and computational limits."

## Likelihood Explanation

**Likelihood: High**

This vulnerability is highly likely to be exploited because:

1. **Low attacker requirements**: Any validator on the network can execute this attack (Byzantine validators are part of the threat model)
2. **Trivial execution**: Requires only sending small request messages repeatedly
3. **No detection**: No logging or alerting for excessive epoch retrieval requests
4. **Significant impact**: Measurable degradation of validator performance
5. **Economic incentive**: Validators could attack competitors to gain unfair advantage in block production

Validators are assumed to have adversarial actors controlling up to 1/3 of the stake in the AptosBFT threat model. A compromised or malicious validator can trivially exploit this vulnerability.

## Recommendation

Implement range size validation for epoch retrieval requests, similar to block retrieval:

```rust
fn process_epoch_retrieval(
    &mut self,
    request: EpochRetrievalRequest,
    peer_id: AccountAddress,
) -> anyhow::Result<()> {
    debug!(
        LogSchema::new(LogEvent::ReceiveEpochRetrieval)
            .remote_peer(peer_id)
            .epoch(self.epoch()),
        "[EpochManager] receive {}", request,
    );
    
    // Add range size validation
    const MAX_EPOCHS_PER_REQUEST: u64 = 10; // Reasonable limit
    let epoch_range = request.end_epoch.saturating_sub(request.start_epoch);
    if epoch_range > MAX_EPOCHS_PER_REQUEST {
        warn!(
            "[EpochManager] Ignoring epoch retrieval with excessive range: {} epochs from {}",
            epoch_range, peer_id
        );
        counters::EPOCH_MANAGER_ISSUES_DETAILS
            .with_label_values(&["excessive_epoch_retrieval_range"])
            .inc();
        return Ok(());
    }
    
    let proof = self
        .storage
        .aptos_db()
        .get_epoch_ending_ledger_infos(request.start_epoch, request.end_epoch)
        .map_err(DbError::from)
        .context("[EpochManager] Failed to get epoch proof")?;
    let msg = ConsensusMsg::EpochChangeProof(Box::new(proof));
    if let Err(err) = self.network_sender.send_to(peer_id, msg) {
        warn!(
            "[EpochManager] Failed to send epoch proof to {}, with error: {:?}",
            peer_id, err,
        );
    }
    Ok(())
}
```

Additionally:
- Implement per-peer rate limiting for epoch retrieval requests
- Add monitoring/alerting for excessive epoch retrieval patterns
- Consider caching recent epoch change proofs to reduce database load

## Proof of Concept

```rust
// Simulated attack demonstrating amplification
// This would be run by a malicious validator node

use aptos_consensus_types::epoch_retrieval::EpochRetrievalRequest;
use aptos_types::account_address::AccountAddress;

async fn amplification_attack(
    network_sender: &ConsensusNetworkClient,
    target_validator: AccountAddress,
    current_epoch: u64,
) {
    // Continuously send requests with maximum range
    loop {
        let request = EpochRetrievalRequest {
            start_epoch: current_epoch.saturating_sub(99),
            end_epoch: current_epoch,
        };
        
        let msg = ConsensusMsg::EpochRetrievalRequest(Box::new(request));
        
        // Fire and forget - no rate limiting
        let _ = network_sender.send_to(target_validator, msg);
        
        // Request size: 16 bytes
        // Expected response: ~1.38 MB
        // Amplification: ~86,000x
        
        // Can send hundreds of requests per second
        // Total bandwidth attack: 100+ MB/s from 16 KB/s input
    }
}

// Metrics to observe on victim validator:
// - Increased CPU usage from database queries
// - Increased memory usage from buffering responses
// - Network bandwidth saturation
// - Degraded consensus performance (higher block times)
// - Potential liveness issues if resources exhausted
```

**Notes**

The vulnerability exists because epoch retrieval lacks the range validation present in block retrieval. While the response size is capped at 100 epoch-ending ledger infos (via `MAX_NUM_EPOCH_ENDING_LEDGER_INFO`), this still allows significant amplification. The attack requires validator network access, but Byzantine validators are explicitly part of the AptosBFT threat model. This is distinct from network-level DoS attacks (which are out of scope) because it exploits application-level logic to achieve amplification through legitimate protocol messages.

### Citations

**File:** consensus/consensus-types/src/epoch_retrieval.rs (L8-12)
```rust
#[derive(Clone, Debug, Deserialize, Serialize)]
pub struct EpochRetrievalRequest {
    pub start_epoch: u64,
    pub end_epoch: u64,
}
```

**File:** consensus/src/epoch_manager.rs (L451-476)
```rust
    fn process_epoch_retrieval(
        &mut self,
        request: EpochRetrievalRequest,
        peer_id: AccountAddress,
    ) -> anyhow::Result<()> {
        debug!(
            LogSchema::new(LogEvent::ReceiveEpochRetrieval)
                .remote_peer(peer_id)
                .epoch(self.epoch()),
            "[EpochManager] receive {}", request,
        );
        let proof = self
            .storage
            .aptos_db()
            .get_epoch_ending_ledger_infos(request.start_epoch, request.end_epoch)
            .map_err(DbError::from)
            .context("[EpochManager] Failed to get epoch proof")?;
        let msg = ConsensusMsg::EpochChangeProof(Box::new(proof));
        if let Err(err) = self.network_sender.send_to(peer_id, msg) {
            warn!(
                "[EpochManager] Failed to send epoch proof to {}, with error: {:?}",
                peer_id, err,
            );
        }
        Ok(())
    }
```

**File:** consensus/src/epoch_manager.rs (L587-594)
```rust
                    BlockRetrievalRequest::V1(v1) => {
                        if v1.num_blocks() > max_blocks_allowed {
                            warn!(
                                "Ignore block retrieval with too many blocks: {}",
                                v1.num_blocks()
                            );
                            continue;
                        }
```

**File:** consensus/src/epoch_manager.rs (L1677-1686)
```rust
            ConsensusMsg::EpochRetrievalRequest(request) => {
                ensure!(
                    request.end_epoch <= self.epoch(),
                    "[EpochManager] Received EpochRetrievalRequest beyond what we have locally"
                );
                monitor!(
                    "process_epoch_retrieval",
                    self.process_epoch_retrieval(*request, peer_id)
                )?;
            },
```

**File:** storage/aptosdb/src/common.rs (L7-9)
```rust
// TODO: Either implement an iteration API to allow a very old client to loop through a long history
// or guarantee that there is always a recent enough waypoint and client knows to boot from there.
pub(crate) const MAX_NUM_EPOCH_ENDING_LEDGER_INFO: usize = 100;
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L991-1005)
```rust
    /// Returns ledger infos reflecting epoch bumps starting with the given epoch. If there are no
    /// more than `MAX_NUM_EPOCH_ENDING_LEDGER_INFO` results, this function returns all of them,
    /// otherwise the first `MAX_NUM_EPOCH_ENDING_LEDGER_INFO` results are returned and a flag
    /// (when true) will be used to indicate the fact that there is more.
    fn get_epoch_ending_ledger_infos(
        &self,
        start_epoch: u64,
        end_epoch: u64,
    ) -> Result<(Vec<LedgerInfoWithSignatures>, bool)> {
        self.get_epoch_ending_ledger_infos_impl(
            start_epoch,
            end_epoch,
            MAX_NUM_EPOCH_ENDING_LEDGER_INFO,
        )
    }
```

**File:** types/src/block_info.rs (L27-44)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct BlockInfo {
    /// The epoch to which the block belongs.
    epoch: u64,
    /// The consensus protocol is executed in rounds, which monotonically increase per epoch.
    round: Round,
    /// The identifier (hash) of the block.
    id: HashValue,
    /// The accumulator root hash after executing this block.
    executed_state_id: HashValue,
    /// The version of the latest transaction after executing this block.
    version: Version,
    /// The timestamp this block was proposed by a proposer.
    timestamp_usecs: u64,
    /// An optional field containing the next epoch info
    next_epoch_state: Option<EpochState>,
}
```

**File:** types/src/epoch_state.rs (L17-22)
```rust
#[derive(Clone, Deserialize, Eq, PartialEq, Serialize)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct EpochState {
    pub epoch: u64,
    pub verifier: Arc<ValidatorVerifier>,
}
```
