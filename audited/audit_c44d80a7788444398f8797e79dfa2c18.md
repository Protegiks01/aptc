# Audit Report

## Title
Priority Inversion in Data Stream Queue Causes Indefinite Request Starvation

## Summary
The `request_missing_data()` function in the data streaming service pushes missing data requests to the front of the request queue, creating a priority inversion vulnerability. When combined with head-of-line blocking in the queue processing logic, this allows malicious peers to indefinitely starve older requests by repeatedly sending valid but truncated responses, preventing state synchronization progress.

## Finding Description

The data streaming service maintains a FIFO queue of pending client requests in `sent_data_requests`. [1](#0-0) 

Normal data requests are added to the back of the queue using `push_back`. [2](#0-1) 

However, when a response doesn't fully satisfy the original request (truncated data), a missing data request is created and pushed to the **front** of the queue using `push_front`. [3](#0-2) 

The queue processing logic in `pop_pending_response_queue()` only processes requests from the front and returns `None` if the front request hasn't received a response yet. [4](#0-3) 

This creates a head-of-line blocking scenario where requests behind the front cannot be processed even if their responses are ready. When missing data is requested, processing breaks early with `head_of_line_blocked = true`. [5](#0-4) 

**Critical Flaw:** The `request_failure_count` is reset to 0 every time a notification is successfully sent, even for partial/truncated data. [6](#0-5) 

This means the stream never hits the `max_request_retry` limit (default: 5) when receiving truncated responses, as each partial response resets the counter.

**Attack Scenario:**
1. Node sends concurrent requests: R1 (versions 1-1000), R2 (versions 1001-2000), R3 (versions 2001-3000)
2. Queue state: [R1, R2, R3]
3. Malicious peer responds to R1 with versions 1-500 (valid but truncated)
4. Missing data request MR1 created for versions 501-1000, pushed to front
5. Queue state: [MR1, R2, R3]
6. Processing breaks with `head_of_line_blocked = true`
7. Even if R2 and R3 responses arrive, they cannot be processed because MR1 is at the front
8. Malicious peer delays or never responds to MR1, or responds with another truncated response
9. Process repeats: MR1', MR1'', etc. keep getting pushed to front
10. R2 and R3 are permanently starved
11. `request_failure_count` is reset on each partial response, never reaching retry limit

Truncated responses pass the sanity check (which only validates response type) [7](#0-6)  and do not trigger `notify_bad_response`, so the malicious peer is not penalized.

## Impact Explanation

**Severity: High (Validator Node Slowdowns) / Medium (State Inconsistencies)**

This vulnerability causes:
- **State sync starvation**: Nodes cannot make progress syncing to the latest state
- **Validator node degradation**: Active validators may slow down if state sync is affected
- **New node onboarding failure**: New validators cannot join the network if they cannot sync
- **Network health degradation**: Multiple nodes experiencing this simultaneously affects overall network performance

While this does not directly cause consensus violations or loss of funds, it significantly impacts network availability and node operations, meeting the High severity criteria of "Validator node slowdowns" per the Aptos bug bounty program.

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability is highly likely to occur because:

1. **Low attacker requirements**: Any network peer can send truncated responses without detection
2. **No peer penalties**: Truncated but valid responses don't trigger bad response notifications
3. **Persistent impact**: Once triggered, requests remain starved indefinitely
4. **Natural occurrence possible**: Even honest peers with poor network conditions could inadvertently trigger this by sending partial data
5. **Amplification effect**: A single slow/malicious response can block an entire queue of ready responses

The attack requires no special privileges, just the ability to respond to data requests on the peer-to-peer network.

## Recommendation

Implement multiple fixes to address this vulnerability:

**1. Track per-request retry counts instead of global failure count:**
```rust
// Add to PendingClientResponse struct
pub struct PendingClientResponse {
    pub client_request: DataClientRequest,
    pub client_response: Option<Result<Response<ResponsePayload>, aptos_data_client::error::Error>>,
    pub retry_count: u64,  // NEW: Track retries per request
    pub missing_data_count: u64,  // NEW: Track missing data iterations
}
```

**2. Limit missing data iterations per request:**
```rust
fn request_missing_data(
    &mut self,
    data_client_request: &DataClientRequest,
    response_payload: &ResponsePayload,
    retry_count: u64,
    missing_data_count: u64,
) -> Result<bool, Error> {
    // NEW: Enforce maximum missing data requests per original request
    const MAX_MISSING_DATA_REQUESTS: u64 = 3;
    
    if missing_data_count >= MAX_MISSING_DATA_REQUESTS {
        warn!("Exceeded maximum missing data requests for request: {:?}", data_client_request);
        return Err(Error::TooManyMissingDataRequests);
    }
    
    if let Some(missing_data_request) = create_missing_data_request(data_client_request, response_payload)? {
        // ... existing code ...
        // Carry forward the retry tracking
        let mut pending_response = self.send_client_request(false, missing_data_request.clone());
        pending_response.lock().retry_count = retry_count;
        pending_response.lock().missing_data_count = missing_data_count + 1;
        
        self.get_sent_data_requests()?.push_front(pending_response);
        return Ok(true);
    }
    Ok(false)
}
```

**3. Implement non-blocking queue processing:**
```rust
fn pop_pending_response_queue(&mut self) -> Result<Option<PendingClientResponse>, Error> {
    let sent_data_requests = self.get_sent_data_requests()?;
    
    // NEW: Scan for ANY ready response, not just the front
    for (index, data_request) in sent_data_requests.iter().enumerate() {
        if data_request.lock().client_response.is_some() {
            // Found a ready response, remove and return it
            return Ok(sent_data_requests.remove(index));
        }
    }
    
    Ok(None)
}
```

**4. Penalize peers sending excessive truncated responses:**
```rust
// In request_missing_data, track repeated truncations from same peer
if missing_data_count > 1 {
    // Consider this suspicious behavior and notify
    self.notify_bad_response(response_context, ResponseError::InvalidData);
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod priority_inversion_test {
    use super::*;
    use crate::tests::utils::MockAptosDataClient;
    
    #[tokio::test]
    async fn test_missing_data_starves_old_requests() {
        // Create mock data client that returns truncated responses
        let config = AptosDataClientConfig::default();
        let streaming_config = DataStreamingServiceConfig::default();
        let mock_client = MockAptosDataClient::new(config.clone(), true, false, true, true);
        
        // Create stream
        let (mut stream, _listener) = DataStream::new(
            config,
            streaming_config,
            1,
            &StreamRequest::get_all_transactions(...),
            stream_notifier,
            mock_client,
            Arc::new(U64IdGenerator::new()),
            &advertised_data,
            TimeService::mock(),
        ).unwrap();
        
        // Initialize with three concurrent requests
        stream.initialize_data_requests(global_summary).unwrap();
        
        // Simulate scenario:
        // 1. R1, R2, R3 sent
        // 2. R1 completes with truncated data
        // 3. MR1 created and pushed to front
        // 4. R2, R3 responses arrive but cannot be processed
        
        // Verify queue state
        let (sent_requests, _) = stream.get_sent_requests_and_notifications();
        assert_eq!(sent_requests.as_ref().unwrap().len(), 3);
        
        // Process first response (truncated)
        stream.process_data_responses(global_summary).await.unwrap();
        
        // Verify missing data request at front
        let queue = sent_requests.as_ref().unwrap();
        assert!(is_missing_data_request(&queue.front().unwrap()));
        
        // Verify old requests still in queue but blocked
        assert_eq!(queue.len(), 3); // MR1 + R2 + R3
        
        // Simulate R2, R3 completing while MR1 still pending
        // These requests are ready but cannot be processed
        mark_request_ready(&queue[1]);
        mark_request_ready(&queue[2]);
        
        // Attempt to process - should fail to process R2, R3
        let processed = stream.pop_pending_response_queue().unwrap();
        assert!(processed.is_none()); // MR1 not ready, so nothing processed
        
        // R2 and R3 are starved despite being ready
        assert!(queue[1].lock().client_response.is_some());
        assert!(queue[2].lock().client_response.is_some());
    }
}
```

## Notes

This vulnerability represents a classic priority inversion and head-of-line blocking problem in a concurrent request queue. The combination of:
1. Front-insertion for missing data requests
2. Strict FIFO processing that blocks on the first unavailable item
3. Failure count reset on partial successes
4. Lack of per-request retry tracking

Creates a perfect storm where malicious or slow peers can indefinitely starve legitimate data synchronization requests. This is particularly concerning for state sync operations where timely data delivery is critical for network participation.

The fix requires both architectural changes (non-blocking queue processing) and policy changes (limits on missing data iterations, peer penalties for excessive truncations).

### Citations

**File:** state-sync/data-streaming-service/src/data_stream.rs (L89-89)
```rust
    sent_data_requests: Option<VecDeque<PendingClientResponse>>,
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L303-304)
```rust
                self.get_sent_data_requests()?
                    .push_back(pending_client_response);
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L472-516)
```rust
                        let mut head_of_line_blocked = false;
                        match self.request_missing_data(client_request, &client_response.payload) {
                            Ok(missing_data_requested) => {
                                if missing_data_requested {
                                    head_of_line_blocked = true; // We're now head of line blocked on the missing data
                                }
                            },
                            Err(error) => {
                                warn!(LogSchema::new(LogEntry::ReceivedDataResponse)
                                    .stream_id(self.data_stream_id)
                                    .event(LogEvent::Error)
                                    .error(&error)
                                    .message("Failed to determine if missing data was requested!"));
                            },
                        }

                        // If the request was a subscription request and the subscription
                        // stream is lagging behind the data advertisements, the stream
                        // engine should be notified (e.g., so that it can catch up).
                        if client_request.is_subscription_request() {
                            if let Err(error) = self.check_subscription_stream_lag(
                                &global_data_summary,
                                &client_response.payload,
                            ) {
                                self.notify_new_data_request_error(client_request, error)?;
                                head_of_line_blocked = true; // We're now head of line blocked on the failed stream
                            }
                        }

                        // The response is valid, send the data notification to the client
                        self.send_data_notification_to_client(client_request, client_response)
                            .await?;

                        // If the request is for specific data, increase the prefetching limit.
                        // Note: we don't increase the limit for new data requests because
                        // those don't invoke the prefetcher (as we're already up-to-date).
                        if !client_request.is_new_data_request() {
                            self.dynamic_prefetching_state
                                .increase_max_concurrent_requests();
                        }

                        // If we're head of line blocked, we should return early
                        if head_of_line_blocked {
                            break;
                        }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L668-671)
```rust
            // Push the pending response to the front of the queue
            self.get_sent_data_requests()?
                .push_front(pending_client_response);

```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L680-693)
```rust
    fn pop_pending_response_queue(&mut self) -> Result<Option<PendingClientResponse>, Error> {
        let sent_data_requests = self.get_sent_data_requests()?;
        let pending_client_response = if let Some(data_request) = sent_data_requests.front() {
            if data_request.lock().client_response.is_some() {
                // We've received a response! Pop the requests off the queue.
                sent_data_requests.pop_front()
            } else {
                None
            }
        } else {
            None
        };
        Ok(pending_client_response)
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L806-808)
```rust
            // Reset the failure count. We've sent a notification and can move on.
            self.request_failure_count = 0;
        }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L1292-1379)
```rust
fn sanity_check_client_response_type(
    data_client_request: &DataClientRequest,
    data_client_response: &Response<ResponsePayload>,
) -> bool {
    match data_client_request {
        DataClientRequest::EpochEndingLedgerInfos(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::EpochEndingLedgerInfos(_)
            )
        },
        DataClientRequest::NewTransactionOutputsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionOutputsWithProof(_)
            )
        },
        DataClientRequest::NewTransactionsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionsWithProof(_)
            )
        },
        DataClientRequest::NewTransactionsOrOutputsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionsWithProof(_)
            ) || matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionOutputsWithProof(_)
            )
        },
        DataClientRequest::NumberOfStates(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NumberOfStates(_)
            )
        },
        DataClientRequest::StateValuesWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::StateValuesWithProof(_)
            )
        },
        DataClientRequest::SubscribeTransactionsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionsWithProof(_)
            )
        },
        DataClientRequest::SubscribeTransactionOutputsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionOutputsWithProof(_)
            )
        },
        DataClientRequest::SubscribeTransactionsOrOutputsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionsWithProof(_)
            ) || matches!(
                data_client_response.payload,
                ResponsePayload::NewTransactionOutputsWithProof(_)
            )
        },
        DataClientRequest::TransactionsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::TransactionsWithProof(_)
            )
        },
        DataClientRequest::TransactionOutputsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::TransactionOutputsWithProof(_)
            )
        },
        DataClientRequest::TransactionsOrOutputsWithProof(_) => {
            matches!(
                data_client_response.payload,
                ResponsePayload::TransactionsWithProof(_)
            ) || matches!(
                data_client_response.payload,
                ResponsePayload::TransactionOutputsWithProof(_)
            )
        },
    }
}
```
