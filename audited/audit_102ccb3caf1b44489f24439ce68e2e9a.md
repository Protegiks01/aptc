# Audit Report

## Title
Validator Process Termination on Storage Read Failure During Cache Priming

## Summary
A panic in parallel state reads during cache priming causes the entire validator node process to exit, rather than gracefully handling the error. This occurs due to an incorrect `.expect("Must succeed.")` assumption that storage reads never fail.

## Finding Description

The `prime_cache` function in `CachedStateView` uses a static rayon thread pool (`IO_POOL`) to parallelize state reads. Within these parallel reads, the code assumes all storage operations will succeed: [1](#0-0) 

The critical issue is at line 217 where `.expect("Must succeed.")` is used. However, storage reads can legitimately fail due to:

1. **Pruned state versions**: The database checks if the requested version has been pruned [2](#0-1) 

2. **Database I/O errors**: RocksDB operations can fail during iterator creation or seeking [3](#0-2) 

When a storage read fails, the error propagates through `get_state_slot`: [4](#0-3) 

The `.expect()` call then panics. Critically, Aptos sets up a global panic handler that **exits the entire process** when a panic occurs: [5](#0-4) 

This panic handler is installed at validator node startup: [6](#0-5) 

**Important Clarification**: The security question asks about "thread pool poisoning," but this is **not** what occurs. Rayon thread pools are designed to handle panics gracefully without becoming poisoned. The actual issue is **worse**: the panic triggers the global panic handler which terminates the entire validator process with exit code 12.

## Impact Explanation

This meets **HIGH severity** criteria per the Aptos bug bounty program:
- "Validator node slowdowns" / "API crashes"
- Causes complete validator node termination
- Validator becomes unable to process any transactions
- Requires node restart to recover

If multiple validators encounter storage errors simultaneously (e.g., during heavy load, aggressive pruning, or infrastructure issues), this could impact network consensus performance.

The vulnerability violates the principle of graceful degradation - a recoverable storage error should not cause complete process termination.

## Likelihood Explanation

**Moderate to High likelihood** of natural occurrence:

1. **Pruning race conditions**: During state synchronization or catch-up, concurrent pruning operations could cause version access conflicts
2. **Storage infrastructure issues**: Disk I/O errors, corruption, or resource exhaustion
3. **Configuration mismatches**: Aggressive pruning settings combined with slow state sync

While not easily exploitable by external attackers without infrastructure access, this is a reliability vulnerability that can manifest under production conditions.

## Recommendation

Replace the `.expect()` with proper error handling that propagates the error instead of panicking:

```rust
fn prime_cache_for_keys<'a, T: IntoIterator<Item = &'a StateKey> + Send>(
    &self,
    keys: T,
) -> Result<()> {
    let results: Vec<_> = rayon::scope(|s| {
        let handles: Vec<_> = keys.into_iter().map(|key| {
            s.spawn(move |_| self.get_state_value(key))
        }).collect();
        handles.into_iter().map(|h| h.join()).collect()
    });
    
    // Check if any reads failed
    for result in results {
        if let Err(e) = result {
            return Err(anyhow::anyhow!("State cache priming failed: {}", e));
        }
    }
    Ok(())
}
```

This allows the caller (`prime_cache` in the executor) to handle the error gracefully, potentially retrying or logging the issue without crashing the validator.

## Proof of Concept

A Rust unit test demonstrating the vulnerability:

```rust
#[test]
#[should_panic(expected = "Must succeed")]
fn test_cache_prime_panic_on_db_error() {
    use std::sync::Arc;
    use aptos_storage_interface::DbReader;
    
    // Mock DbReader that returns an error
    struct FailingDbReader;
    impl DbReader for FailingDbReader {
        fn get_state_value_with_version_by_version(
            &self,
            _state_key: &StateKey,
            _version: Version,
        ) -> Result<Option<(Version, StateValue)>> {
            Err(anyhow::anyhow!("Simulated DB error"))
        }
    }
    
    let state = State::new_empty();
    let view = CachedStateView::new_impl(
        StateViewId::Miscellaneous,
        Arc::new(FailingDbReader),
        Arc::new(EmptyHotState),
        state.clone(),
        state,
    );
    
    let key = StateKey::raw(b"test_key");
    let updates = create_test_state_updates(&[key]);
    
    // This will panic and crash the process due to .expect()
    view.prime_cache(&updates, PrimingPolicy::All).unwrap();
}
```

**Notes**

The security question's premise about "thread pool poisoning" is technically incorrect. Rayon's thread pool architecture is resilient to panics within spawned tasks - panics are caught and propagated without poisoning the pool itself. However, the actual vulnerability is **more severe**: the global panic handler causes complete process termination, bringing down the entire validator node rather than just affecting the thread pool. This demonstrates a critical gap in error handling where recoverable storage errors trigger unrecoverable process crashes.

### Citations

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L214-222)
```rust
        rayon::scope(|s| {
            keys.into_iter().for_each(|key| {
                s.spawn(move |_| {
                    self.get_state_value(key).expect("Must succeed.");
                })
            });
        });
        Ok(())
    }
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L233-253)
```rust
    fn get_unmemorized(&self, state_key: &StateKey) -> Result<StateSlot> {
        COUNTER.inc_with(&["sv_unmemorized"]);

        let ret = if let Some(slot) = self.speculative.get_state_slot(state_key) {
            COUNTER.inc_with(&["sv_hit_speculative"]);
            slot
        } else if let Some(slot) = self.hot.get_state_slot(state_key) {
            COUNTER.inc_with(&["sv_hit_hot"]);
            slot
        } else if let Some(base_version) = self.base_version() {
            COUNTER.inc_with(&["sv_cold"]);
            StateSlot::from_db_get(
                self.cold
                    .get_state_value_with_version_by_version(state_key, base_version)?,
            )
        } else {
            StateSlot::ColdVacant
        };

        Ok(ret)
    }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L305-315)
```rust
    pub(super) fn error_if_state_kv_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.state_store.state_kv_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L374-402)
```rust
    pub(crate) fn get_state_value_with_version_by_version(
        &self,
        state_key: &StateKey,
        version: Version,
    ) -> Result<Option<(Version, StateValue)>> {
        let mut read_opts = ReadOptions::default();

        // We want `None` if the state_key changes in iteration.
        read_opts.set_prefix_same_as_start(true);
        if !self.enabled_sharding() {
            let mut iter = self
                .db_shard(state_key.get_shard_id())
                .iter_with_opts::<StateValueSchema>(read_opts)?;
            iter.seek(&(state_key.clone(), version))?;
            Ok(iter
                .next()
                .transpose()?
                .and_then(|((_, version), value_opt)| value_opt.map(|value| (version, value))))
        } else {
            let mut iter = self
                .db_shard(state_key.get_shard_id())
                .iter_with_opts::<StateValueByKeyHashSchema>(read_opts)?;
            iter.seek(&(state_key.hash(), version))?;
            Ok(iter
                .next()
                .transpose()?
                .and_then(|((_, version), value_opt)| value_opt.map(|value| (version, value))))
        }
    }
```

**File:** crates/crash-handler/src/lib.rs (L26-58)
```rust
pub fn setup_panic_handler() {
    panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}

// Formats and logs panic information
fn handle_panic(panic_info: &PanicHookInfo<'_>) {
    // The Display formatter for a PanicHookInfo contains the message, payload and location.
    let details = format!("{}", panic_info);
    let backtrace = format!("{:#?}", Backtrace::new());

    let info = CrashInfo { details, backtrace };
    let crash_info = toml::to_string_pretty(&info).unwrap();
    error!("{}", crash_info);
    // TODO / HACK ALARM: Write crash info synchronously via eprintln! to ensure it is written before the process exits which error! doesn't guarantee.
    // This is a workaround until https://github.com/aptos-labs/aptos-core/issues/2038 is resolved.
    eprintln!("{}", crash_info);

    // Wait till the logs have been flushed
    aptos_logger::flush();

    // Do not kill the process if the panics happened at move-bytecode-verifier.
    // This is safe because the `state::get_state()` uses a thread_local for storing states. Thus the state can only be mutated to VERIFIER by the thread that's running the bytecode verifier.
    //
    // TODO: once `can_unwind` is stable, we should assert it. See https://github.com/rust-lang/rust/issues/92988.
    if state::get_state() == VMState::VERIFIER || state::get_state() == VMState::DESERIALIZER {
        return;
    }

    // Kill the process
    process::exit(12);
}
```

**File:** aptos-node/src/lib.rs (L233-234)
```rust
    // Setup panic handler
    aptos_crash_handler::setup_panic_handler();
```
