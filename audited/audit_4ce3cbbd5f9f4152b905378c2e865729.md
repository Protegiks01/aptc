# Audit Report

## Title
Memory Duplication in StateKey Registry Entry Structure Enables Resource Exhaustion Attack via Large Table Item Keys

## Summary
The `Entry` struct in the StateKey registry stores both deserialized and encoded forms of `StateKeyInner`, causing memory duplication. For table items with large keys (up to 1MB per transaction limit), each cached Entry consumes approximately twice the key size in memory. Combined with the hot state's item-count-based capacity (4M items across 16 shards) rather than memory-size-based limiting, an attacker can create numerous large table item keys to exhaust node memory and cause crashes.

## Finding Description
The `Entry` struct is defined to cache StateKey data for deduplication: [1](#0-0) 

When a StateKey is created for a table item, the system:
1. Stores the deserialized `StateKeyInner::TableItem` containing the full key as `Vec<u8>`
2. Encodes this to `Bytes` format, duplicating the key data
3. Caches the Entry in the registry

The encoding process explicitly duplicates the key bytes: [2](#0-1) 

The `StateKeyInner::TableItem` variant stores the key directly: [3](#0-2) 

Transaction limits allow table item keys up to 1MB per write operation: [4](#0-3) 

The hot state cache has a capacity based on item count (250,000 per shard, 4M total across 16 shards), not memory size: [5](#0-4) 

The `ShardedStateCache` has no size-based eviction, only insert-on-absence: [6](#0-5) 

**Attack Path:**
1. Attacker submits transactions creating table items with large keys (e.g., 100KB-1MB each)
2. Each StateKey Entry stores key data twice (deserialized + encoded), consuming ~2x memory
3. Keys are promoted to hot state and cached
4. Over multiple transactions, attacker fills hot state capacity with large entries
5. Memory consumption: 4M entries Ã— 200KB per entry (for 100KB keys) = 800GB total
6. Node runs out of memory, crashes, or becomes unresponsive

## Impact Explanation
This qualifies as **High Severity** under Aptos bug bounty criteria: "Validator node slowdowns" and potential node crashes affecting network availability. 

While not directly causing consensus failure or fund loss, memory exhaustion can:
- Crash validator nodes, reducing network decentralization
- Cause state sync failures and block processing delays
- Require node operators to manually intervene and restart with increased memory
- Create denial-of-service conditions affecting liveness (though not meeting "total loss of liveness" for Critical severity)

The impact is amplified because the hot state capacity is measured in item count rather than memory size, creating a mismatch where a small number of very large keys can consume vastly more memory than expected.

## Likelihood Explanation
**Medium-to-High Likelihood** with the following considerations:

**Barriers:**
- Attacker must pay gas costs for large write operations (89 gas per byte)
- Creating 4M entries with 100KB keys requires sustained transaction submission
- Gas costs scale with data size, making large-scale attacks expensive

**Enabling Factors:**
- No explicit memory-based limiting on hot state or ShardedStateCache
- Memory duplication multiplies actual consumption vs. transaction payload
- Once keys are in hot state, they persist until evicted by item count, not memory pressure
- Attack can be conducted gradually over time across multiple accounts

The attack is feasible for well-funded adversaries or during periods of network stress when memory exhaustion has multiplicative effects.

## Recommendation
Implement memory-aware capacity limits and eliminate unnecessary duplication:

1. **Remove memory duplication**: Store only the encoded `Bytes` in Entry and deserialize on-demand, or use copy-on-write semantics
2. **Add memory-based capacity limits**: Track actual memory consumption in hot state and evict based on memory pressure, not just item count
3. **Implement tighter key size validation**: Add explicit limits on table item key sizes at the Move/native function level (e.g., max 10KB for keys)
4. **Add memory usage monitoring**: Track per-Entry memory consumption and emit warnings when approaching limits

Example fix for Entry struct:
```rust
pub struct Entry {
    pub encoded: Bytes,  // Single source of truth
    pub hash_value: HashValue,
    // Deserialize on-demand: fn inner(&self) -> StateKeyInner
}
```

## Proof of Concept
Due to the nature of this vulnerability requiring multiple transactions and substantial gas expenditure, a full PoC would require:

1. Move script creating table items with large keys:
```move
// Pseudo-code - actual implementation requires table operations
public entry fun create_large_table_items(sender: &signer) {
    let handle = table::new<vector<u8>, u64>();
    let i = 0;
    while (i < 100) {
        let large_key = vector::empty<u8>();
        let j = 0;
        while (j < 100000) {  // 100KB key
            vector::push_back(&mut large_key, (i % 256 as u8));
            j = j + 1;
        };
        table::add(&mut handle, large_key, i);
        i = i + 1;
    };
}
```

2. Memory profiling showing Entry sizes and hot state consumption
3. Demonstration of node memory exhaustion after sustained attack

The vulnerability is confirmed by code analysis showing the memory duplication pattern and unbounded memory growth potential in the caching layers.

## Notes
The core issue is the combination of: (1) memory duplication in Entry structure, (2) item-count-based rather than memory-based capacity limiting, and (3) allowance of large table item keys. While individual components have partial mitigations (gas costs, item count limits), the interaction creates an exploitable resource exhaustion vector.

### Citations

**File:** types/src/state_store/state_key/registry.rs (L28-33)
```rust
#[derive(Debug)]
pub struct Entry {
    pub deserialized: StateKeyInner,
    pub encoded: Bytes,
    pub hash_value: HashValue,
}
```

**File:** types/src/state_store/state_key/inner.rs (L51-55)
```rust
    TableItem {
        handle: TableHandle,
        #[serde(with = "serde_bytes")]
        key: Vec<u8>,
    },
```

**File:** types/src/state_store/state_key/inner.rs (L71-75)
```rust
            StateKeyInner::TableItem { handle, key } => {
                writer.write_all(&[StateKeyTag::TableItem as u8])?;
                bcs::serialize_into(&mut writer, &handle)?;
                writer.write_all(key)?;
            },
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L154-157)
```rust
            max_bytes_per_write_op: NumBytes,
            { 5.. => "max_bytes_per_write_op" },
            1 << 20, // a single state item is 1MB max
        ],
```

**File:** config/src/config/storage_config.rs (L256-264)
```rust
impl Default for HotStateConfig {
    fn default() -> Self {
        Self {
            max_items_per_shard: 250_000,
            refresh_interval_versions: 100_000,
            delete_on_restart: true,
            compute_root_hash: true,
        }
    }
```

**File:** storage/storage-interface/src/state_store/state_view/cached_state_view.rs (L47-85)
```rust
#[derive(Debug)]
pub struct ShardedStateCache {
    next_version: Version,
    pub shards: [StateCacheShard; NUM_STATE_SHARDS],
}

impl ShardedStateCache {
    pub fn new_empty(version: Option<Version>) -> Self {
        Self {
            next_version: version.map_or(0, |v| v + 1),
            shards: Default::default(),
        }
    }

    fn shard(&self, shard_id: usize) -> &StateCacheShard {
        &self.shards[shard_id]
    }

    pub fn get_cloned(&self, state_key: &StateKey) -> Option<StateSlot> {
        self.shard(state_key.get_shard_id())
            .get(state_key)
            .map(|r| r.clone())
    }

    pub fn next_version(&self) -> Version {
        self.next_version
    }

    pub fn try_insert(&self, state_key: &StateKey, slot: &StateSlot) {
        let shard_id = state_key.get_shard_id();

        match self.shard(shard_id).entry(state_key.clone()) {
            Entry::Occupied(_) => {},
            Entry::Vacant(entry) => {
                entry.insert(slot.clone());
            },
        };
    }
}
```
