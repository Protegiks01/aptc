# Audit Report

## Title
Validator Node Crash During State Sync Due to Missing SecretShareStore Reset

## Summary
The `SecretShareManager` is not reset during state synchronization, causing validator nodes to crash when reprocessing blocks after `sync_to_target()`. The shared `SecretShareStore` retains stale entries from before the reset, and attempting to add self-shares for previously processed rounds triggers a panic, resulting in validator unavailability.

## Finding Description

The vulnerability exists in how the secret sharing pipeline handles state synchronization. Multiple `SecretShareAggregateState` instances share the same `Arc<Mutex<SecretShareStore>>` throughout an epoch, and this store is keyed by round only, not by full metadata. [1](#0-0) [2](#0-1) 

The `SecretShareStore` uses a `HashMap<Round, SecretShareItem>` indexed by round only: [3](#0-2) 

When a block is processed, its self-share is added via `add_self_share()`, which transitions the round's entry from `PendingMetadata` to `PendingDecision` state with specific metadata: [4](#0-3) 

The critical bug occurs when `add_share_with_metadata()` is called on an item already in `PendingDecision` state - it unconditionally fails: [5](#0-4) 

This failure is treated as fatal and causes a panic: [6](#0-5) 

**The vulnerability is triggered during state synchronization:**

When `sync_to_target()` is called, it resets the rand manager and buffer manager, but **NOT** the secret share manager: [7](#0-6) [8](#0-7) 

The reset only retrieves and uses `reset_tx_to_rand_manager` and `reset_tx_to_buffer_manager`, completely omitting `reset_tx_to_secret_share_manager`.

Even when the `SecretShareManager::process_reset()` is called (during `end_epoch()`), it only updates `highest_known_round` and clears the block queue, but **does NOT clear the `secret_share_map`**: [9](#0-8) 

**Attack Scenario:**
1. Validator processes rounds 100-110 normally, populating `secret_share_map[100..110]` with entries in `PendingDecision` state
2. State sync is triggered to round 105 due to network partition or validator restart
3. `sync_to_target()` resets buffer and rand managers but NOT secret share manager
4. `secret_share_map` still contains rounds 100-110 in their previous states
5. Blocks for rounds 106-110 are reprocessed after sync
6. For each block, `process_incoming_block()` calls `add_self_share()`
7. Since round 106 entry already exists in `PendingDecision` state, `add_share_with_metadata()` fails with "Cannot add self share in PendingDecision state"
8. Line 147 panics: `.expect("Add self dec share should succeed")`
9. Validator node crashes

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty criteria:
- **Validator node crashes**: The panic causes immediate termination of the validator process
- **Loss of liveness**: Affected validators cannot participate in consensus until manually restarted
- **Network degradation**: If multiple validators sync simultaneously (common after network partitions), the network could lose significant consensus power

While this doesn't directly cause fund loss or consensus safety violations, it represents a **significant protocol violation** that can cause **validator node unavailability** during state synchronization - a critical operation for network recovery.

## Likelihood Explanation

**Likelihood: HIGH**

This issue has high probability of occurrence because:

1. **State sync is a routine operation**: Validators frequently sync when:
   - Joining the network
   - Recovering from downtime
   - Falling behind during network issues
   - Performing maintenance restarts

2. **No Byzantine behavior required**: This is a legitimate code path that triggers during normal operations

3. **Deterministic trigger**: Any state sync to a round that was previously processed will trigger the panic

4. **Multiple validators affected**: During network partitions or upgrades, many validators may sync simultaneously, causing coordinated crashes

The only requirement is that a validator syncs to a round lower than rounds it previously processed (common during network issues or restarts with stale state).

## Recommendation

**Fix 1: Reset SecretShareStore during state sync**

Modify `ExecutionProxyClient::reset()` to include the secret share manager:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager, reset_tx_to_secret_share_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
            handle.reset_tx_to_secret_share_manager.clone(), // ADD THIS
        )
    };

    // ... existing rand_manager reset ...

    // ADD THIS BLOCK:
    if let Some(mut reset_tx) = reset_tx_to_secret_share_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::SecretShareResetDropped)?;
        ack_rx.await.map_err(|_| Error::SecretShareResetDropped)?;
    }

    // ... existing buffer_manager reset ...
}
```

**Fix 2: Clear secret_share_map in SecretShareManager::process_reset()**

```rust
fn process_reset(&mut self, request: ResetRequest) {
    let ResetRequest { tx, signal } = request;
    let target_round = match signal {
        ResetSignal::Stop => 0,
        ResetSignal::TargetRound(round) => round,
    };
    self.block_queue = BlockQueue::new();
    
    // MODIFY THIS BLOCK:
    let mut store = self.secret_share_store.lock();
    store.update_highest_known_round(target_round);
    // Clear stale entries beyond target round to prevent conflicts
    store.secret_share_map.retain(|&round, _| round <= target_round);
    drop(store); // Release lock
    
    self.stop = matches!(signal, ResetSignal::Stop);
    let _ = tx.send(ResetAck::default());
}
```

**Fix 3: Make add_self_share() idempotent**

Allow re-adding self-shares for rounds in `Decided` or matching `PendingDecision` states:

```rust
fn add_share_with_metadata(
    &mut self,
    share: SecretShare,
    share_weights: &HashMap<Author, u64>,
) -> anyhow::Result<()> {
    // ... existing code ...
    let new_item = match item {
        SecretShareItem::PendingMetadata(mut share_aggregator) => {
            // ... existing code ...
        },
        SecretShareItem::PendingDecision { metadata, .. } => {
            // CHANGE: Allow if metadata matches
            if metadata == share.metadata {
                return Ok(()); // Already processed with same metadata
            }
            bail!("Cannot add self share with different metadata in PendingDecision state");
        },
        SecretShareItem::Decided { .. } => return Ok(()),
    };
    // ... rest of code ...
}
```

## Proof of Concept

```rust
// File: consensus/src/rand/secret_sharing/secret_share_manager_test.rs

#[tokio::test]
async fn test_state_sync_crash_vulnerability() {
    // Setup validator with secret sharing enabled
    let (mut manager, _, mut incoming_blocks, mut reset_rx, _) = 
        create_test_secret_share_manager();
    
    // Step 1: Process blocks for rounds 100-105
    let blocks_batch1 = create_ordered_blocks(100, 105);
    manager.process_incoming_blocks(blocks_batch1).await;
    
    // Verify rounds 100-105 are in PendingDecision state
    {
        let store = manager.secret_share_store.lock();
        for round in 100..=105 {
            let item = store.secret_share_map.get(&round).unwrap();
            assert!(matches!(item, SecretShareItem::PendingDecision { .. }));
        }
    }
    
    // Step 2: Trigger state sync reset to round 102
    let (reset_tx, reset_rx) = oneshot::channel();
    let reset_request = ResetRequest {
        tx: reset_tx,
        signal: ResetSignal::TargetRound(102),
    };
    manager.process_reset(reset_request);
    
    // Verify: secret_share_map still contains rounds 103-105!
    {
        let store = manager.secret_share_store.lock();
        assert!(store.secret_share_map.contains_key(&103)); // BUG: Still exists
        assert!(store.secret_share_map.contains_key(&104));
        assert!(store.secret_share_map.contains_key(&105));
    }
    
    // Step 3: Reprocess blocks after sync (rounds 103-105)
    let blocks_batch2 = create_ordered_blocks(103, 105);
    
    // THIS WILL PANIC: add_self_share() expects success but fails
    // because rounds 103-105 are already in PendingDecision state
    let result = std::panic::catch_unwind(|| {
        tokio::runtime::Runtime::new().unwrap().block_on(async {
            manager.process_incoming_blocks(blocks_batch2).await;
        });
    });
    
    assert!(result.is_err(), "Expected panic due to state sync bug");
    println!("âœ“ Vulnerability confirmed: Node crashes during state sync");
}
```

---

**Notes**

This vulnerability demonstrates a critical oversight in the state synchronization logic where the secret sharing subsystem is not properly reset. The root cause is the incomplete reset in `sync_to_target()` which omits the `SecretShareManager`, combined with the expectation that `add_self_share()` will always succeed. The shared store design is not inherently flawed, but the lack of proper cleanup during resets creates a denial-of-service vector that affects validator availability during routine state synchronization operations.

### Citations

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L142-148)
```rust
        {
            let mut secret_share_store = self.secret_share_store.lock();
            secret_share_store.update_highest_known_round(block.round());
            secret_share_store
                .add_self_share(self_secret_share.clone())
                .expect("Add self dec share should succeed");
        }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L172-184)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.secret_share_store
            .lock()
            .update_highest_known_round(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L237-243)
```rust
    fn spawn_share_requester_task(&self, metadata: SecretShareMetadata) -> DropGuard {
        let rb = self.reliable_broadcast.clone();
        let aggregate_state = Arc::new(SecretShareAggregateState::new(
            self.secret_share_store.clone(),
            metadata.clone(),
            self.config.clone(),
        ));
```

**File:** consensus/src/rand/secret_sharing/reliable_broadcast_state.rs (L26-36)
```rust
    pub fn new(
        secret_share_store: Arc<Mutex<SecretShareStore>>,
        secret_share_metadata: SecretShareMetadata,
        secret_share_config: SecretShareConfig,
    ) -> Self {
        Self {
            secret_share_store,
            secret_share_metadata,
            secret_share_config,
        }
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L156-182)
```rust
    fn add_share_with_metadata(
        &mut self,
        share: SecretShare,
        share_weights: &HashMap<Author, u64>,
    ) -> anyhow::Result<()> {
        let item = std::mem::replace(self, Self::new(Author::ONE));
        let share_weight = *share_weights
            .get(share.author())
            .expect("Author must exist in weights");
        let new_item = match item {
            SecretShareItem::PendingMetadata(mut share_aggregator) => {
                let metadata = share.metadata.clone();
                share_aggregator.retain(share.metadata(), share_weights);
                share_aggregator.add_share(share, share_weight);
                SecretShareItem::PendingDecision {
                    metadata,
                    share_aggregator,
                }
            },
            SecretShareItem::PendingDecision { .. } => {
                bail!("Cannot add self share in PendingDecision state");
            },
            SecretShareItem::Decided { .. } => return Ok(()),
        };
        let _ = std::mem::replace(self, new_item);
        Ok(())
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L207-214)
```rust
pub struct SecretShareStore {
    epoch: u64,
    self_author: Author,
    secret_share_config: SecretShareConfig,
    secret_share_map: HashMap<Round, SecretShareItem>,
    highest_known_round: u64,
    decision_tx: Sender<SecretSharedKey>,
}
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L237-257)
```rust
    pub fn add_self_share(&mut self, share: SecretShare) -> anyhow::Result<()> {
        assert!(
            self.self_author == share.author,
            "Only self shares can be added with metadata"
        );
        let peer_weights = self.secret_share_config.get_peer_weights();
        let metadata = share.metadata();
        ensure!(metadata.epoch == self.epoch, "Share from different epoch");
        ensure!(
            metadata.round <= self.highest_known_round + FUTURE_ROUNDS_TO_ACCEPT,
            "Share from future round"
        );

        let item = self
            .secret_share_map
            .entry(metadata.round)
            .or_insert_with(|| SecretShareItem::new(self.self_author));
        item.add_share_with_metadata(share, peer_weights)?;
        item.try_aggregate(&self.secret_share_config, self.decision_tx.clone());
        Ok(())
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L661-672)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```
