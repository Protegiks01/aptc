# Audit Report

## Title
Malicious Peer Can Trigger Persistent Backoff Mode to Censor Transaction Propagation via Timing Attacks

## Summary
A malicious peer node can repeatedly trigger backoff mode in the mempool transaction broadcasting mechanism by sending ACK responses with `backoff: true`, causing transaction propagation delays from 10ms to 30 seconds (3000x slowdown). This constitutes a timing-based transaction censorship attack where an attacker controlling peer nodes can significantly delay transaction propagation across the network.

## Finding Description

The mempool transaction broadcasting system uses two timing intervals controlled by configuration: `shared_mempool_tick_interval_ms` (default 10ms) for normal broadcasts and `shared_mempool_backoff_interval_ms` (default 30 seconds) for backoff mode. [1](#0-0) 

When a peer receives a broadcast, it can respond with an ACK containing a `backoff: true` flag to signal backpressure. [2](#0-1) 

The vulnerability exists in how backoff mode is activated and enforced:

**Backoff Activation:** When an ACK with `backoff: true` is received, backoff mode is unconditionally activated for that peer. [3](#0-2) 

**Backoff Enforcement:** Once in backoff mode, normal broadcasts (scheduled at 10ms intervals) are rejected, forcing the system to wait for backoff broadcasts scheduled at 30-second intervals. [4](#0-3) 

**Deactivation Timing:** Backoff mode is only deactivated after a broadcast successfully completes. [5](#0-4) 

**Attack Sequence:**
1. Attacker's peer node receives a normal broadcast from victim node
2. Attacker sends ACK with `backoff: true` 
3. Victim's broadcast interval switches from 10ms to 30 seconds for that peer
4. After 30 seconds, a backoff broadcast executes and backoff mode deactivates
5. Normal 10ms broadcast resumes for one cycle
6. Attacker sends another `backoff: true` ACK
7. Cycle repeats indefinitely

The broadcast scheduling mechanism uses these intervals to determine when to propagate transactions. [6](#0-5) 

An attacker controlling multiple peer connections can create widespread delays in transaction propagation, effectively censoring transactions through timing manipulation rather than direct rejection.

## Impact Explanation

This qualifies as **Medium Severity** per the Aptos bug bounty criteria ("State inconsistencies requiring intervention" and network performance degradation):

1. **Transaction Propagation Delay:** Broadcasts are delayed from 10ms to 30 seconds (3000x slowdown), significantly impacting transaction confirmation times
2. **Partial Network Censorship:** While not complete censorship, the extreme delay in propagation can prevent time-sensitive transactions from being included in blocks promptly
3. **Network Liveness Impact:** If attackers control key upstream peers (e.g., VFNs for PFNs, or multiple validator connections), they can create bottlenecks in transaction flow
4. **Compounding Effect:** Multiple attacker-controlled peers can multiply the impact across the network topology
5. **User Experience Degradation:** Users submitting transactions may experience significant delays before their transactions reach consensus

The attack does not directly cause loss of funds or consensus safety violations, but it does significantly degrade network availability and transaction throughput.

## Likelihood Explanation

**Likelihood: High**

1. **Low Attack Complexity:** The attacker only needs to send ACK messages with `backoff: true` - no cryptographic attacks or complex exploits required
2. **No Special Privileges Needed:** Any connected peer can send ACK responses; no validator or special network role required
3. **Difficult to Distinguish:** Legitimate mempool full conditions vs. malicious backoff triggering are indistinguishable at the protocol level
4. **Persistent Effect:** Once triggered, the attack can be maintained indefinitely by repeatedly sending backoff ACKs
5. **Scalable Impact:** Attacker can run multiple malicious peer nodes to affect multiple victim nodes simultaneously

The attack is straightforward to execute and requires minimal resources beyond operating peer nodes.

## Recommendation

Implement rate limiting and validation for backoff mode activation:

1. **Add Backoff Rate Limiting:** Limit how frequently backoff mode can be triggered per peer within a time window
2. **Add Backoff Verification:** Verify that the peer's mempool is genuinely full before honoring backoff requests (though this requires additional protocol changes)
3. **Add Backoff Timeout:** Automatically disable backoff mode after a maximum duration even without successful broadcast
4. **Add Peer Reputation:** Track peers that frequently trigger backoff and deprioritize or disconnect them

**Suggested Code Fix for `network.rs`:**

```rust
// Add to BroadcastInfo struct:
pub struct BroadcastInfo {
    pub sent_messages: BTreeMap<MempoolMessageId, SystemTime>,
    pub retry_messages: BTreeSet<MempoolMessageId>,
    pub backoff_mode: bool,
    // NEW FIELDS:
    pub backoff_count: u64,
    pub last_backoff_time: Option<SystemTime>,
    pub backoff_mode_started: Option<SystemTime>,
}

// Modify process_broadcast_ack to add rate limiting:
pub fn process_broadcast_ack(...) {
    // ... existing code ...
    
    if backoff {
        // Check if too many backoffs in short time window (e.g., > 5 in 1 minute)
        let now = SystemTime::now();
        if let Some(last_backoff) = sync_state.broadcast_info.last_backoff_time {
            if now.duration_since(last_backoff).unwrap().as_secs() < 60 {
                sync_state.broadcast_info.backoff_count += 1;
                if sync_state.broadcast_info.backoff_count > 5 {
                    warn!("Peer {} triggering excessive backoff, ignoring", peer);
                    return;
                }
            } else {
                sync_state.broadcast_info.backoff_count = 1;
            }
        }
        sync_state.broadcast_info.last_backoff_time = Some(now);
        sync_state.broadcast_info.backoff_mode = true;
        sync_state.broadcast_info.backoff_mode_started = Some(now);
    }
}

// Add timeout check in determine_broadcast_batch:
fn determine_broadcast_batch(...) {
    // ... existing code ...
    
    // Auto-disable backoff mode after maximum duration (e.g., 5 minutes)
    if state.broadcast_info.backoff_mode {
        if let Some(started) = state.broadcast_info.backoff_mode_started {
            if SystemTime::now().duration_since(started).unwrap().as_secs() > 300 {
                state.broadcast_info.backoff_mode = false;
                warn!("Backoff mode timeout for peer {}, disabling", peer);
            }
        }
    }
    
    if state.broadcast_info.backoff_mode && !scheduled_backoff {
        return Err(BroadcastError::PeerNotScheduled(peer));
    }
    // ... rest of existing code ...
}
```

## Proof of Concept

```rust
// Integration test demonstrating the attack
#[tokio::test]
async fn test_malicious_backoff_timing_attack() {
    // Setup: Create a mempool node and malicious peer
    let (mut victim_mempool, mut attacker_peer) = setup_mempool_and_peer().await;
    
    // Initial state: Normal broadcast interval is 10ms
    let start = Instant::now();
    
    // Step 1: Victim broadcasts to attacker
    let broadcast_1 = victim_mempool.execute_broadcast(&attacker_peer).await;
    assert!(broadcast_1.is_ok());
    
    // Step 2: Attacker sends malicious ACK with backoff=true
    let ack = MempoolSyncMsg::BroadcastTransactionsResponse {
        message_id: extract_message_id(broadcast_1),
        retry: false,
        backoff: true,  // Malicious backoff trigger
    };
    victim_mempool.process_ack(&attacker_peer, ack);
    
    // Step 3: Verify backoff mode is activated
    assert!(victim_mempool.is_backoff_mode(&attacker_peer));
    
    // Step 4: Attempt immediate broadcast - should be rejected
    tokio::time::sleep(Duration::from_millis(10)).await;
    let broadcast_2 = victim_mempool.execute_broadcast(&attacker_peer).await;
    assert!(broadcast_2.is_err());  // Rejected due to backoff mode
    
    // Step 5: Wait for backoff interval (30 seconds)
    tokio::time::sleep(Duration::from_secs(30)).await;
    let broadcast_3 = victim_mempool.execute_broadcast_with_backoff(&attacker_peer).await;
    assert!(broadcast_3.is_ok());
    
    // Step 6: Verify timing - should be ~30 seconds, not 10ms
    let elapsed = start.elapsed();
    assert!(elapsed.as_secs() >= 30);
    assert!(elapsed.as_secs() < 31);  // Approximately 30 seconds
    
    // Step 7: Attacker can repeat by sending another backoff ACK
    let ack_2 = MempoolSyncMsg::BroadcastTransactionsResponse {
        message_id: extract_message_id(broadcast_3),
        retry: false,
        backoff: true,  // Repeat attack
    };
    victim_mempool.process_ack(&attacker_peer, ack_2);
    
    // Attack can continue indefinitely, causing 3000x slowdown in propagation
    assert!(victim_mempool.is_backoff_mode(&attacker_peer));
}
```

## Notes

The vulnerability is specifically in the mempool's broadcast timing mechanism, where the `Interval`-based scheduling (via `ScheduledBroadcast` futures that wrap tokio sleep timers) can be manipulated by malicious peers sending backoff ACKs. [7](#0-6) 

The `Interval` type itself from the time service is not directly vulnerable - it's a standard stream-based timer implementation. [8](#0-7)  The vulnerability lies in how the mempool uses timing intervals and allows external actors (peers) to control which interval (10ms vs 30s) is used for broadcasts without proper validation or rate limiting.

### Citations

**File:** config/src/config/mempool_config.rs (L111-112)
```rust
            shared_mempool_tick_interval_ms: 10,
            shared_mempool_backoff_interval_ms: 30_000,
```

**File:** mempool/src/shared_mempool/network.rs (L56-63)
```rust
    BroadcastTransactionsResponse {
        message_id: MempoolMessageId,
        /// Retry signal from recipient if there are txns in corresponding broadcast
        /// that were rejected from mempool but may succeed on resend.
        retry: bool,
        /// A backpressure signal from the recipient when it is overwhelmed (e.g., mempool is full).
        backoff: bool,
    },
```

**File:** mempool/src/shared_mempool/network.rs (L352-354)
```rust
        if backoff {
            sync_state.broadcast_info.backoff_mode = true;
        }
```

**File:** mempool/src/shared_mempool/network.rs (L392-394)
```rust
        if state.broadcast_info.backoff_mode && !scheduled_backoff {
            return Err(BroadcastError::PeerNotScheduled(peer));
        }
```

**File:** mempool/src/shared_mempool/network.rs (L627-627)
```rust
        state.broadcast_info.backoff_mode = false;
```

**File:** mempool/src/shared_mempool/tasks.rs (L110-121)
```rust
    let interval_ms = if schedule_backoff {
        smp.config.shared_mempool_backoff_interval_ms
    } else {
        smp.config.shared_mempool_tick_interval_ms
    };

    scheduled_broadcasts.push(ScheduledBroadcast::new(
        Instant::now() + Duration::from_millis(interval_ms),
        peer,
        schedule_backoff,
        executor,
    ))
```

**File:** mempool/src/shared_mempool/types.rs (L124-173)
```rust
pub(crate) struct ScheduledBroadcast {
    /// Time of scheduled broadcast
    deadline: Instant,
    peer: PeerNetworkId,
    backoff: bool,
    waker: Arc<Mutex<Option<Waker>>>,
}

impl ScheduledBroadcast {
    pub fn new(deadline: Instant, peer: PeerNetworkId, backoff: bool, executor: Handle) -> Self {
        let waker: Arc<Mutex<Option<Waker>>> = Arc::new(Mutex::new(None));
        let waker_clone = waker.clone();

        if deadline > Instant::now() {
            let tokio_instant = tokio::time::Instant::from_std(deadline);
            executor.spawn(async move {
                tokio::time::sleep_until(tokio_instant).await;
                let mut waker = waker_clone.lock();
                if let Some(waker) = waker.take() {
                    waker.wake()
                }
            });
        }

        Self {
            deadline,
            peer,
            backoff,
            waker,
        }
    }
}

impl Future for ScheduledBroadcast {
    type Output = (PeerNetworkId, bool);

    // (peer, whether this broadcast was scheduled as a backoff broadcast)

    fn poll(self: Pin<&mut Self>, context: &mut Context) -> Poll<Self::Output> {
        if Instant::now() < self.deadline {
            let waker_clone = context.waker().clone();
            let mut waker = self.waker.lock();
            *waker = Some(waker_clone);

            Poll::Pending
        } else {
            Poll::Ready((self.peer, self.backoff))
        }
    }
}
```

**File:** crates/aptos-time-service/src/interval.rs (L17-62)
```rust
/// Stream returned by [`TimeServiceTrait::interval`](crate::TimeServiceTrait::interval).
///
/// Mostly taken from [`tokio::time::Interval`] but uses our `Sleep` future.
#[pin_project]
#[must_use = "streams do nothing unless you `.await` or poll them"]
#[derive(Debug)]
pub struct Interval {
    #[pin]
    delay: Sleep,
    period: Duration,
}

impl Interval {
    pub fn new(delay: Sleep, period: Duration) -> Self {
        assert!(period > ZERO_DURATION, "`period` must be non-zero.");

        Self { delay, period }
    }
}

impl Stream for Interval {
    type Item = ();

    fn poll_next(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Option<Self::Item>> {
        let mut this = self.project();

        // Wait for the delay to be done
        ready!(this.delay.as_mut().poll(cx));

        // Reset the delay before next round
        this.delay.reset(*this.period);

        Poll::Ready(Some(()))
    }
}

impl FusedStream for Interval {
    /// We implement [`FusedStream`] here to make it more convenient for API
    /// consumers when using an [`Interval`] inside a `futures::select!`.
    ///
    /// Note: an [`Interval`] stream never ends, so this function always returns
    /// `false`.
    fn is_terminated(&self) -> bool {
        false
    }
}
```
