# Audit Report

## Title
Faucet Checker State Inconsistency Due to Missing Rollback on Partial Checker Failures

## Summary
The Aptos Faucet's checker system executes multiple checkers sequentially, with some checkers modifying persistent state during their `check()` phase. When an earlier checker passes and modifies state, but a later checker fails, the request is rejected without calling `complete()` on any checker. This leaves state-modifying checkers (specifically `MemoryRatelimitChecker` and `RedisRatelimitChecker`) in an inconsistent state where rate limit counters are permanently incremented despite the request being rejected.

## Finding Description

The faucet checker architecture implements a two-phase protocol:
1. **Check Phase**: Checkers validate requests and may modify state (e.g., increment rate limit counters)
2. **Complete Phase**: Checkers finalize or rollback state based on the outcome

The vulnerability occurs because checkers are executed sequentially in cost order, and the `complete()` method is only invoked if the entire request processing succeeds past the checker validation phase. [1](#0-0) 

State-modifying checkers increment their counters during `check()`: [2](#0-1) [3](#0-2) 

However, when checkers fail during the validation phase in `preprocess_request()`, the function returns early with an error: [4](#0-3) 

This early return prevents execution from reaching the `complete()` phase: [5](#0-4) [6](#0-5) 

**Attack Scenario:**

Given checker costs: MemoryRatelimitChecker (20), RedisRatelimitChecker (100)

1. Attacker sends a request that passes `MemoryRatelimitChecker.check()`
2. `MemoryRatelimitChecker` increments its in-memory counter
3. `RedisRatelimitChecker.check()` fails (either returns rejection reasons or throws connection error)
4. `preprocess_request()` returns `Err` at line 273-278
5. `fund_inner()` returns early at line 296 due to the `?` operator
6. `complete()` is never called on any checker
7. The memory counter remains permanently incremented

This can be repeated to exhaust legitimate users' rate limit quotas without actually consuming any faucet resources.

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per the Aptos bug bounty program criteria:
- **State inconsistencies requiring intervention**: Rate limit counters become desynchronized from actual funded requests
- **Limited manipulation**: Attackers can cause unfair rate limiting of legitimate users by exhausting their quotas

While this doesn't affect blockchain consensus or core protocol operations, it creates:
1. Denial of service for legitimate faucet users whose quotas are artificially depleted
2. Operational inconsistency requiring manual intervention to reset counters
3. Potential for attackers to consume rate limit budgets across multiple IPs/identifiers without receiving funds

The faucet is a critical infrastructure component for testnet/devnet operations, making its availability important for ecosystem development.

## Likelihood Explanation

**Likelihood: High**

This vulnerability will trigger automatically in common operational scenarios:

1. **Configuration-based trigger**: When both `MemoryRatelimitChecker` and `RedisRatelimitChecker` are enabled (common configuration), and Redis experiences connection issues, every request passing the memory check will leave it in an inconsistent state.

2. **Rate limit exhaustion**: When a user exhausts their Redis quota but still has memory quota, each subsequent request increments the memory counter without decrementing it.

3. **Minimal attacker requirements**: No special privileges needed - any user can send requests. The attacker only needs to trigger conditions where early checkers pass but later checkers fail.

## Recommendation

Implement a rollback mechanism for checkers when `preprocess_request()` fails. This requires tracking which checkers have executed successfully and calling a rollback method on them if a later checker fails.

**Option 1: Add rollback method to CheckerTrait**

```rust
#[async_trait]
#[enum_dispatch]
pub trait CheckerTrait: Sync + Send + 'static {
    async fn check(
        &self,
        data: CheckerData,
        dry_run: bool,
    ) -> Result<Vec<RejectionReason>, AptosTapError>;

    // New rollback method
    async fn rollback(&self, _data: CheckerData) -> Result<(), AptosTapError> {
        Ok(())
    }

    async fn complete(&self, _data: CompleteData) -> Result<(), AptosTapError> {
        Ok(())
    }

    fn cost(&self) -> u8;
}
```

Then modify `preprocess_request()`:

```rust
async fn preprocess_request(
    &self,
    fund_request: &FundRequest,
    source_ip: RealIp,
    header_map: &HeaderMap,
    dry_run: bool,
) -> poem::Result<(CheckerData, bool, Option<SemaphorePermit<'_>>), AptosTapError> {
    // ... existing setup code ...

    let mut checked_indices = Vec::new();
    let mut rejection_reasons = Vec::new();
    
    for (i, checker) in self.checkers.iter().enumerate() {
        match checker.check(checker_data.clone(), dry_run).await {
            Ok(reasons) => {
                rejection_reasons.extend(reasons);
                checked_indices.push(i);
                if !rejection_reasons.is_empty() && self.return_rejections_early {
                    break;
                }
            }
            Err(e) => {
                // Rollback all successful checkers
                for idx in checked_indices.iter().rev() {
                    let _ = self.checkers[*idx].rollback(checker_data.clone()).await;
                }
                return Err(AptosTapError::new_with_error_code(e, AptosTapErrorCode::CheckerError));
            }
        }
    }

    if !rejection_reasons.is_empty() {
        // Rollback all successful checkers
        for idx in checked_indices.iter().rev() {
            let _ = self.checkers[*idx].rollback(checker_data.clone()).await;
        }
        return Err(AptosTapError::new(
            format!("Request rejected by {} checkers", rejection_reasons.len()),
            AptosTapErrorCode::Rejected,
        )
        .rejection_reasons(rejection_reasons));
    }

    Ok((checker_data, false, permit))
}
```

**Option 2: Call complete() with a failure indicator**

Alternatively, modify `complete()` to accept a success/failure indicator and call it even when `preprocess_request()` fails, allowing checkers to clean up their state.

## Proof of Concept

```rust
// Rust test case demonstrating the vulnerability
#[tokio::test]
async fn test_checker_state_inconsistency() {
    use std::sync::Arc;
    use std::net::IpAddr;
    
    // Setup MemoryRatelimitChecker with max 5 requests
    let memory_checker = MemoryRatelimitChecker::new(
        MemoryRatelimitCheckerConfig {
            max_requests_per_day: 5,
            max_entries_in_map: NonZeroUsize::new(1000).unwrap(),
        }
    );
    
    // Create test checker data
    let test_ip: IpAddr = "1.2.3.4".parse().unwrap();
    let checker_data = CheckerData {
        time_request_received_secs: get_current_time_secs(),
        receiver: AccountAddress::random(),
        source_ip: test_ip,
        headers: Arc::new(HeaderMap::new()),
    };
    
    // Simulate 3 successful checks that pass and increment counter
    for _ in 0..3 {
        let result = memory_checker.check(checker_data.clone(), false).await;
        assert!(result.is_ok());
        assert!(result.unwrap().is_empty());
    }
    
    // Verify internal state shows 4 requests (3 increments + initial 1)
    let state = memory_checker.ip_to_requests_today.lock().await;
    assert_eq!(*state.peek(&test_ip).unwrap(), 4);
    drop(state);
    
    // Now simulate the bug: another check passes, but we never call complete()
    // This would happen if a later checker with higher cost fails
    let result = memory_checker.check(checker_data.clone(), false).await;
    assert!(result.is_ok());
    
    // Counter is now at 5 (should be at limit)
    let state = memory_checker.ip_to_requests_today.lock().await;
    assert_eq!(*state.peek(&test_ip).unwrap(), 5);
    drop(state);
    
    // Even though we never called complete() to finalize, the counter is permanently incremented
    // User is now at their limit without having received 5 successful fundings
    
    // Next request will be rejected
    let result = memory_checker.check(checker_data.clone(), false).await;
    assert!(result.is_ok());
    assert!(!result.unwrap().is_empty()); // Rejection reason returned
    
    // The user is unfairly rate-limited due to the state inconsistency
}
```

This PoC demonstrates how the memory rate limit counter can be exhausted without corresponding successful funding operations when `complete()` is never called due to later checker failures.

## Notes

This vulnerability is specific to the faucet service and does not impact core blockchain consensus or state. However, it represents a violation of the state consistency invariant within the faucet system's checker framework. The issue affects operational availability of the faucet service, which is critical infrastructure for network onboarding and testing.

The vulnerability exists because the checker system implements a two-phase commit protocol without proper rollback semantics. The comment at line 329-331 in fund.rs acknowledges that checker completion failures "could lead to an unintended data state," but the system fails to handle the case where checkers don't reach the completion phase at all. [7](#0-6)

### Citations

**File:** crates/aptos-faucet/core/src/server/run.rs (L141-143)
```rust
        // Sort Checkers by cost, where lower numbers is lower cost, and lower
        // cost Checkers are at the start of the vec.
        checkers.sort_by_key(|a| a.cost());
```

**File:** crates/aptos-faucet/core/src/checkers/memory_ratelimit.rs (L86-88)
```rust
        } else if !dry_run {
            *requests_today += 1;
        }
```

**File:** crates/aptos-faucet/core/src/checkers/redis_ratelimit.rs (L263-301)
```rust
        if !dry_run {
            let incremented_limit_value = match limit_value {
                Some(_) => conn.incr(&key, 1).await.map_err(|e| {
                    AptosTapError::new_with_error_code(
                        format!("Failed to increment redis key {}: {}", key, e),
                        AptosTapErrorCode::StorageError,
                    )
                })?,
                // If the limit value doesn't exist, create it and set the
                // expiration time.
                None => {
                    let (incremented_limit_value,): (i64,) = redis::pipe()
                        .atomic()
                        .incr(&key, 1)
                        // Expire at the end of the day roughly.
                        .expire(&key, seconds_until_next_day as usize)
                        // Only set the expiration if one isn't already set.
                        // Only works with Redis 7 sadly.
                        // .arg("NX")
                        .ignore()
                        .query_async(&mut *conn)
                        .await
                        .map_err(|e| {
                            AptosTapError::new_with_error_code(
                                format!("Failed to increment value for redis key {}: {}", key, e),
                                AptosTapErrorCode::StorageError,
                            )
                        })?;
                    incremented_limit_value
                },
            };

            // Check limit again, to ensure there wasn't a get / set race.
            if let Some(rejection_reason) =
                self.check_limit_value(Some(incremented_limit_value), seconds_until_next_day)
            {
                return Ok(vec![rejection_reason]);
            }
        }
```

**File:** crates/aptos-faucet/core/src/endpoints/fund.rs (L261-278)
```rust
        // Ensure request passes checkers.
        let mut rejection_reasons = Vec::new();
        for checker in &self.checkers {
            rejection_reasons.extend(checker.check(checker_data.clone(), dry_run).await.map_err(
                |e| AptosTapError::new_with_error_code(e, AptosTapErrorCode::CheckerError),
            )?);
            if !rejection_reasons.is_empty() && self.return_rejections_early {
                break;
            }
        }

        if !rejection_reasons.is_empty() {
            return Err(AptosTapError::new(
                format!("Request rejected by {} checkers", rejection_reasons.len()),
                AptosTapErrorCode::Rejected,
            )
            .rejection_reasons(rejection_reasons));
        }
```

**File:** crates/aptos-faucet/core/src/endpoints/fund.rs (L294-296)
```rust
        let (checker_data, bypass, _semaphore_permit) = self
            .preprocess_request(&fund_request, source_ip, header_map, dry_run)
            .await?;
```

**File:** crates/aptos-faucet/core/src/endpoints/fund.rs (L329-331)
```rust
        // Give all Checkers the chance to run the completion step. We should
        // monitor for failures in these steps because they could lead to an
        // unintended data state.
```

**File:** crates/aptos-faucet/core/src/endpoints/fund.rs (L342-346)
```rust
            for checker in &self.checkers {
                checker.complete(complete_data.clone()).await.map_err(|e| {
                    AptosTapError::new_with_error_code(e, AptosTapErrorCode::CheckerError)
                })?;
            }
```
