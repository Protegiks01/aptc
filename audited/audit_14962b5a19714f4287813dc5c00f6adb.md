# Audit Report

## Title
Asymmetric Peer Scoring Causes Permanent Exclusion of Honest Peers During Transient Network Issues

## Summary
The peer scoring mechanism in the Aptos data client uses an asymmetric update policy where errors apply a multiplicative penalty (×0.95) while successes apply an additive reward (+1.0). This asymmetry causes honest peers experiencing transient network issues (40-60% packet loss) to inexorably drift toward MIN_SCORE and become permanently excluded from state sync operations, even after network conditions improve.

## Finding Description

The vulnerability exists in the peer scoring system implemented in the state sync data client. The core issue lies in the `update_score_error()` function's use of multiplicative scoring for `NotUseful` errors (which includes `InvalidData` errors): [1](#0-0) [2](#0-1) 

When a peer returns invalid data or experiences network issues, the `ResponseError::InvalidData` error is classified as `ErrorType::NotUseful`: [3](#0-2) 

**Mathematical Analysis of Score Decay:**

Starting from `STARTING_SCORE` (50.0):
- Each `NotUseful` error: score × 0.95 (loses ~2.5 points at score 50)
- Each success: score + 1.0 (gains 1.0 point)
- **Breakeven requires ~71% success rate** at typical scores
- After ~14 consecutive errors: score drops from 50.0 to 25.0 (IGNORE_PEER_THRESHOLD)
- After ~76 consecutive errors: score approaches 1.0 (near MIN_SCORE)

**Permanent Exclusion Mechanism:**

Once a peer's score drops below `IGNORE_PEER_THRESHOLD` (25.0), the peer becomes ignored: [4](#0-3) 

Ignored peers are excluded from the global data summary and cannot serve data requests: [5](#0-4) [6](#0-5) 

The configuration enables peer ignoring by default: [7](#0-6) 

**Attack Scenario:**

1. Honest peers experience transient network issues (infrastructure problems, partial network partition, packet loss)
2. Their responses trigger `InvalidData` errors in various legitimate scenarios:
   - Network timeouts/RPC errors → NotUseful classification (client.rs:865)
   - Data validation failures due to packet corruption (bootstrapper.rs:928, 949, 972)
   - Index mismatches due to network race conditions (bootstrapper.rs:924-935) [8](#0-7) [9](#0-8) 

3. With 40-60% packet loss (realistic during network issues), peers cannot maintain a 71% success rate
4. Scores decay exponentially: 50 → 47.5 → 45.1 → 42.8 → ... → 25.0
5. Once below 25.0, peers are excluded from serving data requests
6. Recovery is slow (requires 25 successful polls to recover from 25.0 to 50.0) and can be repeatedly interrupted by intermittent issues
7. Multiple peers affected simultaneously → reduced peer pool → state sync disruption

**Recovery Limitations:**

While peers can theoretically recover through storage summary polls (which are always allowed), recovery is severely limited:
- Recovery rate: +1.0 per successful poll
- Poll interval: 100ms (default), but not all peers polled each round
- Intermittent issues reset recovery progress
- During recovery, peers remain excluded from serving data requests [10](#0-9) [11](#0-10) 

## Impact Explanation

This vulnerability qualifies as **Medium Severity** ($10,000 tier) under the Aptos bug bounty program because it causes:

1. **State Sync Disruption**: When multiple honest peers experience network issues simultaneously (common during infrastructure problems, DDoS, or partial network partitions), the available peer pool for state sync is artificially reduced
2. **Prolonged Sync Degradation**: New nodes joining the network face longer sync times due to reduced peer availability
3. **Network Resilience Degradation**: The system's ability to tolerate transient network issues is compromised, as temporary problems cause permanent peer exclusion

This does not reach High/Critical severity because:
- No direct consensus violations or fund loss
- Network can still function with remaining peers
- Does not require a hardfork to resolve
- Primarily affects state sync performance, not consensus safety

However, it clearly exceeds Low severity as it causes measurable state sync degradation requiring manual intervention (peer reconnection or score reset).

## Likelihood Explanation

**Likelihood: High**

This vulnerability will naturally occur in production environments:

1. **Network issues are common**: Infrastructure problems, DDoS attacks, packet loss, and partial network partitions occur regularly in distributed systems
2. **No attacker required**: This is a bug in the protocol logic, not requiring malicious action
3. **Affects honest peers**: The vulnerability targets well-intentioned nodes experiencing legitimate network problems
4. **Cascading effect**: During widespread network issues (e.g., datacenter problems), multiple peers are affected simultaneously, amplifying the impact
5. **Already observable in tests**: The codebase includes a test specifically demonstrating peer recovery from being "banned" due to score degradation, indicating this is a known concern [12](#0-11) 

## Recommendation

Implement a **symmetric or less aggressive scoring mechanism** that doesn't unfairly penalize honest peers experiencing transient issues:

### Option 1: Additive Scoring (Recommended)
Replace multiplicative penalties with additive ones:

```rust
// In peer_states.rs, update constants:
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
const NOT_USEFUL_PENALTY: f64 = 2.0;  // Additive penalty
const MALICIOUS_PENALTY: f64 = 5.0;   // Larger penalty for malicious behavior

// Update update_score_error():
fn update_score_error(&mut self, error: ErrorType) {
    let penalty = match error {
        ErrorType::NotUseful => NOT_USEFUL_PENALTY,
        ErrorType::Malicious => MALICIOUS_PENALTY,
    };
    self.score = f64::max(self.score - penalty, MIN_SCORE);
}
```

This creates symmetric scoring where success and failure have comparable impact, preventing exponential decay.

### Option 2: Decay Rate Limiting
Limit how frequently scores can be decreased:

```rust
// Add to PeerState:
last_error_time: Option<Instant>,
error_decay_window_ms: u64,  // From config, e.g., 5000ms

fn update_score_error(&mut self, error: ErrorType) {
    // Only apply penalty if outside decay window
    let now = Instant::now();
    let should_apply_penalty = match self.last_error_time {
        None => true,
        Some(last) => now.duration_since(last).as_millis() as u64 > self.error_decay_window_ms,
    };
    
    if should_apply_penalty {
        let multiplier = match error {
            ErrorType::NotUseful => NOT_USEFUL_MULTIPLIER,
            ErrorType::Malicious => MALICIOUS_MULTIPLIER,
        };
        self.score = f64::max(self.score * multiplier, MIN_SCORE);
        self.last_error_time = Some(now);
    }
}
```

### Option 3: Temporary Ignoring with Auto-Recovery
Instead of permanent score-based ignoring, use time-based temporary ignoring:

```rust
// Add to PeerState:
temporary_ignore_until: Option<Instant>,

fn is_temporarily_ignored(&self, now: Instant) -> bool {
    match self.temporary_ignore_until {
        Some(until) => now < until,
        None => false,
    }
}

// After N consecutive errors, temporarily ignore for T seconds
// Then automatically clear ignore status
```

## Proof of Concept

The following test demonstrates the vulnerability by simulating a peer experiencing 50% packet loss:

```rust
#[tokio::test]
async fn honest_peer_with_intermittent_network_issues() {
    use crate::{
        client::AptosDataClient,
        interface::ResponseError,
        tests::{mock::MockNetwork, utils},
    };
    use aptos_config::config::AptosDataClientConfig;

    // Create data client with peer ignoring enabled (default)
    let data_client_config = AptosDataClientConfig {
        ignore_low_score_peers: true,
        ..Default::default()
    };
    
    let base_config = utils::create_validator_base_config();
    let (mut mock_network, _, client, _) =
        MockNetwork::new(Some(base_config), Some(data_client_config), None);

    // Add a peer experiencing network issues
    let (peer, network_id) = utils::add_peer_to_network(
        crate::priority::PeerPriority::HighPriority, 
        &mut mock_network
    );
    
    // Update peer with valid storage summary
    client.update_peer_storage_summary(peer, utils::create_storage_summary(200));
    client.update_global_summary_cache().unwrap();

    // Spawn handler that simulates 50% success rate (realistic for network issues)
    tokio::spawn(async move {
        let mut success_counter = 0;
        while let Some(network_request) = mock_network.next_request(network_id).await {
            success_counter += 1;
            if success_counter % 2 == 0 {
                // 50% success - send valid response
                utils::send_transaction_response(network_request);
            } else {
                // 50% failure - send error (simulating network issues)
                utils::send_error_response(network_request);
            }
        }
    });

    // Send 50 requests (50% will succeed, 50% will fail)
    let mut peer_became_ignored = false;
    for i in 0..50 {
        let result = client
            .get_transactions_with_proof(200, 0, 200, false, 10_000)
            .await;
        
        // Check if peer became ignored
        if let Err(crate::error::Error::DataIsUnavailable(_)) = result {
            peer_became_ignored = true;
            println!("Peer ignored after {} requests with 50% success rate", i);
            break;
        }
    }

    // With 50% success rate (well above the minimum needed), 
    // the peer should NOT be ignored, but due to asymmetric scoring it will be
    assert!(peer_became_ignored, 
        "Honest peer with 50% success rate was incorrectly ignored due to asymmetric scoring");
    
    // Verify peer is excluded from global data summary
    client.update_global_summary_cache().unwrap();
    let global_summary = client.get_global_data_summary();
    let expected_range = aptos_storage_service_types::responses::CompleteDataRange::new(0, 200).unwrap();
    assert!(!global_summary.advertised_data.transactions.contains(&expected_range),
        "Ignored peer should be excluded from global data summary");
}
```

This test demonstrates that a peer with 50% success rate (realistic during network issues and well above the theoretical minimum) will still be incorrectly marked as ignored due to the asymmetric scoring mechanism, confirming the vulnerability.

## Notes

The vulnerability specifically affects honest peers during **transient network issues**, which distinguishes it from the intended behavior of penalizing persistently malicious or broken peers. The asymmetric scoring (multiplicative penalty vs additive reward) creates a mathematical bias that cannot differentiate between:

1. A malicious peer consistently sending bad data (should be penalized)
2. An honest peer experiencing 40-60% packet loss due to infrastructure issues (should not be permanently excluded)

The existing test `bad_peer_is_eventually_added_back` shows that recovery is possible but relies on successful storage summary polls over time, which is insufficient for peers experiencing intermittent issues that repeatedly interrupt recovery progress.

### Citations

**File:** state-sync/aptos-data-client/src/peer_states.rs (L33-43)
```rust
const MAX_SCORE: f64 = 100.0;
const MIN_SCORE: f64 = 0.0;
const STARTING_SCORE: f64 = 50.0;
/// Add this score on a successful response.
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
/// Not necessarily a malicious response, but not super useful.
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
/// Likely to be a malicious response.
const MALICIOUS_MULTIPLIER: f64 = 0.8;
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L54-63)
```rust
impl From<ResponseError> for ErrorType {
    fn from(error: ResponseError) -> Self {
        match error {
            ResponseError::InvalidData | ResponseError::InvalidPayloadDataType => {
                ErrorType::NotUseful
            },
            ResponseError::ProofVerificationError => ErrorType::Malicious,
        }
    }
}
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L143-149)
```rust
    pub fn get_storage_summary_if_not_ignored(&self) -> Option<&StorageServerSummary> {
        if self.is_ignored() {
            None
        } else {
            self.storage_summary.as_ref()
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L152-160)
```rust
    fn is_ignored(&self) -> bool {
        // Only ignore peers if the config allows it
        if !self.data_client_config.ignore_low_score_peers {
            return false;
        }

        // Otherwise, ignore peers with a low score
        self.score <= IGNORE_PEER_THRESHOLD
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L163-165)
```rust
    fn update_score_success(&mut self) {
        self.score = f64::min(self.score + SUCCESSFUL_RESPONSE_DELTA, MAX_SCORE);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L168-174)
```rust
    fn update_score_error(&mut self, error: ErrorType) {
        let multiplier = match error {
            ErrorType::NotUseful => NOT_USEFUL_MULTIPLIER,
            ErrorType::Malicious => MALICIOUS_MULTIPLIER,
        };
        self.score = f64::max(self.score * multiplier, MIN_SCORE);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L216-227)
```rust
        if let Some(peer_state) = self.peer_to_state.get(peer) {
            return match peer_state.get_storage_summary_if_not_ignored() {
                Some(storage_summary) => {
                    storage_summary.can_service(&self.data_client_config, time_service, request)
                },
                None => false, // The peer is temporarily ignored
            };
        }

        // Otherwise, the request cannot be serviced
        false
    }
```

**File:** config/src/config/state_sync_config.rs (L343-357)
```rust
    pub poll_loop_interval_ms: u64,
}

impl Default for AptosDataPollerConfig {
    fn default() -> Self {
        Self {
            additional_polls_per_peer_bucket: 1,
            min_polls_per_second: 5,
            max_num_in_flight_priority_polls: 30,
            max_num_in_flight_regular_polls: 30,
            max_polls_per_second: 20,
            peer_bucket_size: 10,
            poll_loop_interval_ms: 100,
        }
    }
```

**File:** config/src/config/state_sync_config.rs (L466-466)
```rust
            ignore_low_score_peers: true,
```

**File:** state-sync/aptos-data-client/src/client.rs (L830-866)
```rust
            Err(error) => {
                // Convert network error and storage service error types into
                // data client errors. Also categorize the error type for scoring
                // purposes.
                let client_error = match error {
                    aptos_storage_service_client::Error::RpcError(rpc_error) => match rpc_error {
                        RpcError::NotConnected(_) => {
                            Error::DataIsUnavailable(rpc_error.to_string())
                        },
                        RpcError::TimedOut => {
                            Error::TimeoutWaitingForResponse(rpc_error.to_string())
                        },
                        _ => Error::UnexpectedErrorEncountered(rpc_error.to_string()),
                    },
                    aptos_storage_service_client::Error::StorageServiceError(err) => {
                        Error::UnexpectedErrorEncountered(err.to_string())
                    },
                    _ => Error::UnexpectedErrorEncountered(error.to_string()),
                };

                warn!(
                    (LogSchema::new(LogEntry::StorageServiceResponse)
                        .event(LogEvent::ResponseError)
                        .request_type(&request.get_label())
                        .request_id(id)
                        .peer(&peer)
                        .error(&client_error))
                );

                increment_request_counter(
                    &metrics::ERROR_RESPONSES,
                    client_error.get_label(),
                    peer,
                );

                self.notify_bad_response(id, peer, &request, ErrorType::NotUseful);
                Err(client_error)
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L920-958)
```rust
        notification_id: NotificationId,
        state_value_chunk_with_proof: &StateValueChunkWithProof,
    ) -> Result<(), Error> {
        // Verify the payload start index is valid
        let expected_start_index = self.state_value_syncer.next_state_index_to_process;
        if expected_start_index != state_value_chunk_with_proof.first_index {
            self.reset_active_stream(Some(NotificationAndFeedback::new(
                notification_id,
                NotificationFeedback::InvalidPayloadData,
            )))
            .await?;
            return Err(Error::VerificationError(format!(
                "The start index of the state values was invalid! Expected: {:?}, received: {:?}",
                expected_start_index, state_value_chunk_with_proof.first_index
            )));
        }

        // Verify the end index and number of state values is valid
        let expected_num_state_values = state_value_chunk_with_proof
            .last_index
            .checked_sub(state_value_chunk_with_proof.first_index)
            .and_then(|version| version.checked_add(1)) // expected_num_state_values = last_index - first_index + 1
            .ok_or_else(|| {
                Error::IntegerOverflow("The expected number of state values has overflown!".into())
            })?;
        let num_state_values = state_value_chunk_with_proof.raw_values.len() as u64;
        if expected_num_state_values != num_state_values {
            self.reset_active_stream(Some(NotificationAndFeedback::new(
                notification_id,
                NotificationFeedback::InvalidPayloadData,
            )))
            .await?;
            return Err(Error::VerificationError(format!(
                "The expected number of state values was invalid! Expected: {:?}, received: {:?}",
                expected_num_state_values, num_state_values,
            )));
        }

        Ok(())
```

**File:** state-sync/aptos-data-client/src/tests/peers.rs (L274-376)
```rust
async fn bad_peer_is_eventually_added_back() {
    // Ensure the properties hold for all peer priorities
    for peer_priority in PeerPriority::get_all_ordered_priorities() {
        // Create a base config for a validator
        let base_config = utils::create_validator_base_config();

        // Create a data client config with peer ignoring enabled
        let data_client_config = AptosDataClientConfig {
            enable_transaction_data_v2: false,
            ignore_low_score_peers: true,
            ..Default::default()
        };

        // Create the mock network, mock time, client and poller
        let (mut mock_network, mut mock_time, client, poller) =
            MockNetwork::new(Some(base_config), Some(data_client_config), None);

        // Add a connected peer
        let (_, network_id) = utils::add_peer_to_network(peer_priority, &mut mock_network);

        // Start the poller
        tokio::spawn(poller::start_poller(poller));

        // Spawn a handler for the peer
        let highest_synced_version = 200;
        tokio::spawn(async move {
            while let Some(network_request) = mock_network.next_request(network_id).await {
                // Determine the data response based on the request
                let data_response = match network_request.storage_service_request.data_request {
                    DataRequest::GetTransactionsWithProof(_) => {
                        DataResponse::TransactionsWithProof(TransactionListWithProof::new_empty())
                    },
                    DataRequest::GetStorageServerSummary => DataResponse::StorageServerSummary(
                        utils::create_storage_summary(highest_synced_version),
                    ),
                    _ => panic!(
                        "Unexpected storage request: {:?}",
                        network_request.storage_service_request
                    ),
                };

                // Send the response
                let storage_response = StorageServiceResponse::new(
                    data_response,
                    network_request.storage_service_request.use_compression,
                )
                .unwrap();
                network_request.response_sender.send(Ok(storage_response));
            }
        });

        // Wait until the request range is serviceable by the peer
        let transaction_range = CompleteDataRange::new(0, highest_synced_version).unwrap();
        utils::wait_for_transaction_advertisement(
            &client,
            &mut mock_time,
            &data_client_config,
            transaction_range,
        )
        .await;

        // Keep decreasing this peer's score by considering their responses invalid.
        // Eventually the score drops below the threshold and it is ignored.
        for _ in 0..20 {
            // Send a request to fetch transactions from the peer
            let request_timeout = data_client_config.response_timeout_ms;
            let result = client
                .get_transactions_with_proof(200, 0, 200, false, request_timeout)
                .await;

            // Notify the client that the response was bad
            if let Ok(response) = result {
                response
                    .context
                    .response_callback
                    .notify_bad_response(crate::interface::ResponseError::ProofVerificationError);
            }
        }

        // Verify that the peer is eventually ignored and this data range becomes unserviceable
        client.update_global_summary_cache().unwrap();
        let global_summary = client.get_global_data_summary();
        assert!(!global_summary
            .advertised_data
            .transactions
            .contains(&transaction_range));

        // Keep elapsing time so the peer is eventually added back (it
        // will still respond to the storage summary requests).
        for _ in 0..10 {
            utils::advance_polling_timer(&mut mock_time, &data_client_config).await;
        }

        // Verify the peer is no longer ignored and this request range is serviceable
        utils::wait_for_transaction_advertisement(
            &client,
            &mut mock_time,
            &data_client_config,
            transaction_range,
        )
        .await;
    }
}
```
