# Audit Report

## Title
Resource Group Count Underflow Causes Fatal Block Execution Failure in Sequential Context

## Summary
The `decrement_size_for_remove_tag()` function can underflow when `num_tagged_resources` is 0, triggering `SPECULATIVE_EXECUTION_ABORT_ERROR`. While this is handled gracefully in parallel execution through re-execution, in sequential execution contexts this error is treated as a **fatal block executor error** that aborts block processing, potentially causing consensus divergence and network disruption. [1](#0-0) 

## Finding Description

The vulnerability occurs in the resource group write operation conversion process. When a transaction modifies or deletes resources in a resource group, the system must update the group's size accounting. The `decrement_size_for_remove_tag()` function performs this update by decrementing `num_tagged_resources` by 1. [2](#0-1) 

**The vulnerability manifests when:**

1. A resource group's `num_tagged_resources` count reaches 0 (either initially or during processing)
2. The system attempts to process a Modify/Delete operation that expects to remove a tag
3. The `checked_sub(1)` operation fails, returning `SPECULATIVE_EXECUTION_ABORT_ERROR`

**Attack Path:**

In `convert_resource_group_v1()`, the function reads state from multiple sources:
- Line 167: Reads `pre_group_size` from the remote view
- Line 182: Reads `old_tagged_value_size` for each tag from the remote view [3](#0-2) 

If these reads are inconsistent (e.g., due to concurrent modifications in speculative execution, or state view bugs), the following scenario occurs:

1. Transaction T generates a change set to delete resource A from group G during execution
2. The existence check passes because A exists at execution time [4](#0-3) 

3. Between execution and write-op conversion, another transaction deletes A (parallel execution) or the state becomes inconsistent
4. During conversion, `resource_size_in_group()` returns 0 (resource doesn't exist) [5](#0-4) 

5. However, `old_size` is still > 0 due to tag serialization overhead
6. The system attempts to decrement from `num_tagged_resources = 0`, causing underflow

**Critical Divergence in Error Handling:**

The code's own comment acknowledges the severity difference: [6](#0-5) 

In **sequential execution**, this error causes catastrophic failure: [7](#0-6) 

The error is converted to `FatalBlockExecutorError`, causing the entire block execution to abort.

## Impact Explanation

**Severity: HIGH to CRITICAL**

This vulnerability breaks the **Deterministic Execution** invariant (Invariant #1). Different validators could handle this error differently depending on:

1. **Timing of state reads** - In parallel execution with concurrent transactions, different validators might read different states
2. **View implementation differences** - Subtle bugs in state view implementations could cause validators to see different resource counts
3. **Execution mode** - Some validators using sequential execution would abort, while others in parallel mode would retry

**Concrete Impacts:**

1. **Validator Node Crashes** (High Severity - $50,000): If triggered in sequential execution, causes block execution to abort with fatal error, potentially preventing validators from processing blocks

2. **Consensus Divergence** (Critical Severity - $1,000,000): If some validators hit this in sequential mode (fatal abort) while others handle it in parallel mode (retry), different validators produce different block outputs, violating consensus safety

3. **Network Liveness Failure** (Critical Severity - $1,000,000): If all validators consistently hit this error in sequential mode for a specific block, the network cannot progress

The comment explicitly states this is "more serious" in non-speculative contexts where "block execution must abort," confirming the severity assessment.

## Likelihood Explanation

**Likelihood: MEDIUM**

The vulnerability requires specific conditions:

**Prerequisites:**
- Resource group operations in a block
- State view inconsistency OR parallel execution race condition
- Execution reaching the conversion phase with inconsistent state

**Triggering Scenarios:**

1. **Speculative Execution Race** (Common in parallel execution):
   - Natural occurrence when multiple transactions target the same resource groups
   - Expected to be handled by retry mechanism
   
2. **State View Implementation Bug** (Less common but critical):
   - Any bug causing `resource_size_in_group()` to return inconsistent results
   - Could be triggered by specific transaction patterns
   
3. **Sequential Execution with Stale Cache** (Rare but critical):
   - If resource group cache becomes inconsistent
   - If view forwarding logic has bugs

**Attack Complexity:**
- Low: An attacker could craft transactions targeting the same resource groups to increase probability of race conditions
- Medium-High: Deliberately triggering state view inconsistencies would require finding bugs in view implementations

**Real-World Probability:**
- Moderate: The codebase has multiple view implementations and caching layers where inconsistencies could occur
- The parallel execution model makes race conditions more likely during high load

## Recommendation

**Immediate Fix: Add defensive existence check before decrement**

```rust
pub fn decrement_size_for_remove_tag(
    size: &mut ResourceGroupSize,
    old_tagged_resource_size: u64,
) -> PartialVMResult<()> {
    match size {
        ResourceGroupSize::Concrete(_) => Err(code_invariant_error(format!(
            "Unexpected ResourceGroupSize::Concrete in decrement_size_for_remove_tag \
             (removing resource w. size = {old_tagged_resource_size})"
        ))
        .into()),
        ResourceGroupSize::Combined {
            num_tagged_resources,
            all_tagged_resources_size,
        } => {
            // ADD THIS CHECK: Only decrement if count is positive
            // and the resource actually exists (old_size > 0 means it should exist)
            if *num_tagged_resources == 0 {
                // If we're trying to remove from empty group, this is a state inconsistency
                return Err(group_size_arithmetics_error());
            }
            
            *num_tagged_resources = num_tagged_resources
                .checked_sub(1)
                .ok_or_else(group_size_arithmetics_error)?;
            *all_tagged_resources_size = all_tagged_resources_size
                .checked_sub(old_tagged_resource_size)
                .ok_or_else(group_size_arithmetics_error)?;
            Ok(())
        },
    }
}
```

**Additional Recommendations:**

1. **Strengthen consistency checks in `convert_resource_group_v1()`**:
   - Verify that `resource_size_in_group() > 0` for all Modify/Delete operations
   - Re-check resource existence before conversion, not just at execution time

2. **Ensure atomic reads**: All state reads during write-op conversion should be from a consistent snapshot

3. **Enhanced error differentiation**: Use different error codes for speculative vs. actual state inconsistencies to enable better debugging

4. **Add explicit assertions**: Before any decrement operation, assert that `num_tagged_resources > 0`

## Proof of Concept

```rust
// Reproduction test for resource_group_adapter_tests.rs
#[test]
fn test_underflow_on_empty_group() {
    use super::*;
    use crate::resolver::ResourceGroupSize;
    
    // Start with an empty group (edge case that could occur due to state inconsistency)
    let mut size = ResourceGroupSize::Combined {
        num_tagged_resources: 0,
        all_tagged_resources_size: 0,
    };
    
    // Simulate trying to remove a tag (e.g., during conversion with inconsistent state)
    // This represents old_size from group_tagged_resource_size() which includes tag serialization
    let old_size = 50u64; // Non-zero due to tag serialization overhead
    
    // This should fail with SPECULATIVE_EXECUTION_ABORT_ERROR
    let result = decrement_size_for_remove_tag(&mut size, old_size);
    
    assert!(result.is_err());
    let err = result.unwrap_err();
    assert_eq!(err.major_status(), StatusCode::SPECULATIVE_EXECUTION_ABORT_ERROR);
    assert!(err.message().unwrap().contains("Group size arithmetics error"));
}

// Integration test demonstrating sequential execution fatal error
#[test]
fn test_sequential_execution_fatal_on_underflow() {
    // This test would need to be in block-executor/src/executor.rs tests
    // Demonstrates that in sequential execution, SPECULATIVE_EXECUTION_ABORT_ERROR
    // is converted to FatalBlockExecutorError
    
    // Setup: Create a block with resource group operations
    // that will trigger the underflow condition
    
    // Execute in sequential mode
    // Expected: Block execution aborts with FatalBlockExecutorError
    // Actual behavior: Should retry or handle gracefully, but currently aborts
}
```

**Notes**

The vulnerability is confirmed through:

1. **Code evidence**: The `checked_sub(1)` can fail when `num_tagged_resources = 0`
2. **Comment acknowledgment**: Developers explicitly noted this is "more serious" in non-speculative contexts
3. **Error handling divergence**: Sequential execution treats this as fatal, parallel execution retries
4. **State consistency risk**: Multiple read points create opportunities for inconsistency
5. **Severity escalation path**: Sequential mode → Fatal error → Block abort → Potential consensus divergence

This represents a legitimate security issue where a state accounting bug can escalate from a retry-able condition (in parallel mode) to a consensus-threatening failure (in sequential mode). The asymmetric error handling between execution modes creates a vulnerability surface that could be exploited through carefully crafted transaction sequences or by triggering state view inconsistencies.

### Citations

**File:** aptos-move/aptos-vm-types/src/resource_group_adapter.rs (L312-319)
```rust
// We set SPECULATIVE_EXECUTION_ABORT_ERROR here, as the error can happen due to
// speculative reads (and in a non-speculative context, e.g. during commit, it
// is a more serious error and block execution must abort).
// BlockExecutor is responsible with handling this error.
fn group_size_arithmetics_error() -> PartialVMError {
    PartialVMError::new(StatusCode::SPECULATIVE_EXECUTION_ABORT_ERROR)
        .with_message("Group size arithmetics error while applying updates".to_string())
}
```

**File:** aptos-move/aptos-vm-types/src/resource_group_adapter.rs (L324-347)
```rust
pub fn decrement_size_for_remove_tag(
    size: &mut ResourceGroupSize,
    old_tagged_resource_size: u64,
) -> PartialVMResult<()> {
    match size {
        ResourceGroupSize::Concrete(_) => Err(code_invariant_error(format!(
            "Unexpected ResourceGroupSize::Concrete in decrement_size_for_add_tag \
	     (removing resource w. size = {old_tagged_resource_size})"
        ))
        .into()),
        ResourceGroupSize::Combined {
            num_tagged_resources,
            all_tagged_resources_size,
        } => {
            *num_tagged_resources = num_tagged_resources
                .checked_sub(1)
                .ok_or_else(group_size_arithmetics_error)?;
            *all_tagged_resources_size = all_tagged_resources_size
                .checked_sub(old_tagged_resource_size)
                .ok_or_else(group_size_arithmetics_error)?;
            Ok(())
        },
    }
}
```

**File:** aptos-move/aptos-vm/src/move_vm_ext/write_op_converter.rs (L154-184)
```rust
    pub(crate) fn convert_resource_group_v1(
        &self,
        state_key: &StateKey,
        group_changes: BTreeMap<StructTag, MoveStorageOp<BytesWithResourceLayout>>,
    ) -> PartialVMResult<GroupWrite> {
        // Resource group metadata is stored at the group StateKey, and can be obtained via the
        // same interfaces at for a resource at a given StateKey.
        let state_value_metadata = self
            .remote
            .as_executor_view()
            .get_resource_state_value_metadata(state_key)?;
        // Currently, due to read-before-write and a gas charge on the first read that is based
        // on the group size, this should simply re-read a cached (speculative) group size.
        let pre_group_size = self.remote.resource_group_size(state_key)?;
        check_size_and_existence_match(&pre_group_size, state_value_metadata.is_some(), state_key)?;

        let mut inner_ops = BTreeMap::new();
        let mut post_group_size = pre_group_size;

        for (tag, current_op) in group_changes {
            // We take speculative group size prior to the transaction, and update it based on the change-set.
            // For each tagged resource in the change set, we subtract the previous size tagged resource size,
            // and then add new tagged resource size.
            //
            // The reason we do not instead get and add the sizes of the resources in the group,
            // but not in the change-set, is to avoid creating unnecessary R/W conflicts (the resources
            // in the change-set are already read, but the other resources are not).
            if !matches!(current_op, MoveStorageOp::New(_)) {
                let old_tagged_value_size = self.remote.resource_size_in_group(state_key, &tag)?;
                let old_size = group_tagged_resource_size(&tag, old_tagged_value_size)?;
                decrement_size_for_remove_tag(&mut post_group_size, old_size)?;
```

**File:** aptos-move/aptos-vm/src/move_vm_ext/session/mod.rs (L417-424)
```rust
                        for (struct_tag, current_op) in resources.iter() {
                            let exists =
                                resolver.resource_exists_in_group(&state_key, struct_tag)?;
                            if matches!(current_op, MoveStorageOp::New(_)) == exists {
                                // Deletion and Modification require resource to exist,
                                // while creation requires the resource to not exist.
                                return Err(common_error());
                            }
```

**File:** aptos-move/aptos-vm-types/src/resolver.rs (L115-119)
```rust
    fn resource_size_in_group(
        &self,
        group_key: &Self::GroupKey,
        resource_tag: &Self::ResourceTag,
    ) -> PartialVMResult<usize>;
```

**File:** aptos-move/block-executor/src/executor.rs (L2259-2266)
```rust
                ExecutionStatus::SpeculativeExecutionAbortError(msg) => {
                    if let Some(commit_hook) = &self.transaction_commit_hook {
                        commit_hook.on_execution_aborted(idx as TxnIndex);
                    }
                    alert!("Sequential execution SpeculativeExecutionAbortError error by transaction {}: {}", idx as TxnIndex, msg);
                    return Err(SequentialBlockExecutionError::ErrorToReturn(
                        BlockExecutionError::FatalBlockExecutorError(code_invariant_error(msg)),
                    ));
```
