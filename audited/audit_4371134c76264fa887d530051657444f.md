# Audit Report

## Title
Indexer Data Corruption via Missing Table Type Validation in Token Pending Claims Processing

## Summary
The `CurrentTokenPendingClaim::from_write_table_item()` function in the token indexer lacks validation to ensure that table items being processed belong to legitimate `PendingClaims` resources. This allows attackers to inject false pending claim records into the indexer database by creating custom Move modules with tables matching the expected key-value types.

## Finding Description
The token indexer processes table write operations to index token-related data. When processing pending token claims, the code validates that table items have the correct key type (`TokenOfferId`) and value type (`Token`), but critically fails to validate that the table itself belongs to a `PendingClaims` resource of type `0x3::token_transfers::PendingClaims`. [1](#0-0) 

The function retrieves table metadata at line 66 which includes a `table_type` field, but never validates it. In contrast, the analogous function for token ownerships explicitly validates the table type: [2](#0-1) 

An attacker can exploit this by:
1. Deploying a custom Move module containing a resource with `Table<TokenOfferId, Token>` 
2. Writing entries to this custom table with fabricated token offer data
3. The indexer processes these writes and creates `CurrentTokenPendingClaim` records
4. False pending claims appear in the indexer database, visible to dApps and users querying the indexer API

The table metadata system tracks table types correctly: [3](#0-2) 

However, this information is not used to filter out non-PendingClaims tables in the claim processing logic.

## Impact Explanation
This vulnerability falls into the **Medium severity** category as an indexer data integrity issue. While it does not directly cause fund loss or consensus violations, it creates "state inconsistencies" in the indexer database that could:

- Mislead users and dApps about pending token offers
- Enable social engineering attacks by displaying fake offers
- Consume database resources with spurious records
- Undermine trust in the indexer's data accuracy

The actual blockchain state remains secure - the legitimate `token_transfers.move` contract will reject attempts to claim from fake tables. However, the indexer serves as a critical data layer for the ecosystem, and its corruption represents a security concern.

## Likelihood Explanation
Exploitation is **highly likely** because:
- Any user can deploy Move modules to the blockchain
- Creating a custom resource with the matching table structure is trivial
- No special permissions or validator access required
- The attack leaves the indexer database persistently corrupted until manual intervention
- Multiple attackers could spam the indexer with fake claims

The technical complexity is low, requiring only basic Move programming knowledge and standard transaction submission.

## Recommendation
Add table type validation matching the pattern used in `token_ownerships.rs`:

```rust
pub fn from_write_table_item(
    table_item: &APIWriteTableItem,
    txn_version: i64,
    txn_timestamp: chrono::NaiveDateTime,
    table_handle_to_owner: &TableHandleToOwner,
) -> anyhow::Result<Option<Self>> {
    let table_item_data = table_item.data.as_ref().unwrap();

    let maybe_offer = match TokenWriteSet::from_table_item_type(
        table_item_data.key_type.as_str(),
        &table_item_data.key,
        txn_version,
    )? {
        Some(TokenWriteSet::TokenOfferId(inner)) => Some(inner),
        _ => None,
    };
    if let Some(offer) = maybe_offer {
        let table_handle = standardize_address(&table_item.handle.to_string());
        let maybe_table_metadata = table_handle_to_owner.get(&table_handle);
        
        // ADD THIS VALIDATION
        if let Some(tm) = maybe_table_metadata {
            if tm.table_type != "0x3::token_transfers::PendingClaims" {
                return Ok(None);
            }
        }
        // Continue with existing logic...
```

Apply the same fix to `from_delete_table_item()` for consistency.

## Proof of Concept

**Malicious Move Module:**
```move
module attacker::fake_claims {
    use std::signer;
    use aptos_std::table::{Self, Table};
    use aptos_token::token::{Token, TokenId};
    
    struct FakeResource has key {
        fake_table: Table<TokenOfferId, Token>,
    }
    
    struct TokenOfferId has copy, drop, store {
        to_addr: address,
        token_id: TokenId,
    }
    
    public entry fun create_fake_claim(
        attacker: &signer,
        victim: address,
        token_id: TokenId,
        token: Token,
    ) acquires FakeResource {
        if (!exists<FakeResource>(signer::address_of(attacker))) {
            move_to(attacker, FakeResource {
                fake_table: table::new(),
            });
        };
        
        let resource = borrow_global_mut<FakeResource>(signer::address_of(attacker));
        let fake_offer_id = TokenOfferId { to_addr: victim, token_id };
        table::add(&mut resource.fake_table, fake_offer_id, token);
        // This table write will be processed by the indexer as a legitimate claim
    }
}
```

**Indexer Processing:**
When the attacker calls `create_fake_claim`, the table write operation is included in the transaction's write set. The indexer processes this write set, sees a table item with key type `TokenOfferId` and value type `Token`, and creates a `CurrentTokenPendingClaim` record without validating that the table belongs to `0x3::token_transfers::PendingClaims`.

**Verification:**
Query the indexer database after the attack:
```sql
SELECT * FROM current_token_pending_claims 
WHERE from_address = '<attacker_address>';
```
This will show fake pending claims that don't exist in legitimate PendingClaims resources.

## Notes
This is fundamentally an **indexer data integrity issue**, not a consensus or Move VM vulnerability. The actual blockchain state remains secure, and users cannot claim tokens from fake tables through the legitimate `claim()` function in `token_transfers.move`. However, the indexer is a critical infrastructure component that dApps and wallets rely on for querying blockchain state, making its data accuracy a security concern for the broader ecosystem.

### Citations

**File:** crates/indexer/src/models/token_models/token_claims.rs (L38-114)
```rust
    pub fn from_write_table_item(
        table_item: &APIWriteTableItem,
        txn_version: i64,
        txn_timestamp: chrono::NaiveDateTime,
        table_handle_to_owner: &TableHandleToOwner,
    ) -> anyhow::Result<Option<Self>> {
        let table_item_data = table_item.data.as_ref().unwrap();

        let maybe_offer = match TokenWriteSet::from_table_item_type(
            table_item_data.key_type.as_str(),
            &table_item_data.key,
            txn_version,
        )? {
            Some(TokenWriteSet::TokenOfferId(inner)) => Some(inner),
            _ => None,
        };
        if let Some(offer) = maybe_offer {
            let maybe_token = match TokenWriteSet::from_table_item_type(
                table_item_data.value_type.as_str(),
                &table_item_data.value,
                txn_version,
            )? {
                Some(TokenWriteSet::Token(inner)) => Some(inner),
                _ => None,
            };
            if let Some(token) = maybe_token {
                let table_handle = standardize_address(&table_item.handle.to_string());

                let maybe_table_metadata = table_handle_to_owner.get(&table_handle);

                if let Some(table_metadata) = maybe_table_metadata {
                    let token_id = offer.token_id;
                    let token_data_id_struct = token_id.token_data_id;
                    let collection_data_id_hash =
                        token_data_id_struct.get_collection_data_id_hash();
                    let token_data_id_hash = token_data_id_struct.to_hash();
                    // Basically adding 0x prefix to the previous 2 lines. This is to be consistent with Token V2
                    let collection_id = token_data_id_struct.get_collection_id();
                    let token_data_id = token_data_id_struct.to_id();
                    let collection_name = token_data_id_struct.get_collection_trunc();
                    let name = token_data_id_struct.get_name_trunc();

                    return Ok(Some(Self {
                        token_data_id_hash,
                        property_version: token_id.property_version,
                        from_address: standardize_address(&table_metadata.owner_address),
                        to_address: standardize_address(&offer.to_addr),
                        collection_data_id_hash,
                        creator_address: standardize_address(&token_data_id_struct.creator),
                        collection_name,
                        name,
                        amount: token.amount,
                        table_handle,
                        last_transaction_version: txn_version,
                        last_transaction_timestamp: txn_timestamp,
                        token_data_id,
                        collection_id,
                    }));
                } else {
                    aptos_logger::warn!(
                        transaction_version = txn_version,
                        table_handle = table_handle,
                        "Missing table handle metadata for TokenClaim. {:?}",
                        table_handle_to_owner
                    );
                }
            } else {
                aptos_logger::warn!(
                    transaction_version = txn_version,
                    value_type = table_item_data.value_type,
                    value = table_item_data.value,
                    "Expecting token as value for key = token_offer_id",
                );
            }
        }
        Ok(None)
    }
```

**File:** crates/indexer/src/models/token_models/token_ownerships.rs (L90-93)
```rust
        if let Some(tm) = maybe_table_metadata {
            if tm.table_type != "0x3::token::TokenStore" {
                return Ok(None);
            }
```

**File:** crates/indexer/src/models/token_models/tokens.rs (L396-412)
```rust
        let value = TableMetadataForToken {
            owner_address: standardize_address(&resource.address),
            table_type: write_resource.data.typ.to_string(),
        };
        let table_handle: TableHandle = match TokenResource::from_resource(
            &type_str,
            resource.data.as_ref().unwrap(),
            txn_version,
        )? {
            TokenResource::CollectionResource(collection_resource) => {
                collection_resource.collection_data.get_handle()
            },
            TokenResource::TokenStoreResource(inner) => inner.tokens.get_handle(),
            TokenResource::PendingClaimsResource(inner) => inner.pending_claims.get_handle(),
        };
        Ok(Some(HashMap::from([(table_handle, value)])))
    }
```
