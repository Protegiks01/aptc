# Audit Report

## Title
Race Condition in Batch Metadata Updates Enables Data Corruption Across Concurrent Backfill Processes

## Summary
The `do_upload()` function in the file store backfiller contains a race condition where multiple concurrent backfill processes writing to the same version range will overwrite each other's batch metadata files, causing metadata to reference incorrect transaction files and rendering previously written data unreachable.

## Finding Description

The vulnerability exists in how batch metadata files are stored during backfill operations. The system uses a suffix (backfill_id) to distinguish transaction files from different backfill processes, but the metadata file path does not incorporate this suffix. [1](#0-0) 

The `batch_metadata.suffix` is set to `self.backfill_id`, which is unique per backfill instance. [2](#0-1) 

However, the metadata file path is determined solely by version, without incorporating the suffix: [3](#0-2) 

In contrast, transaction files correctly include the suffix in their paths: [4](#0-3) 

When multiple backfill processes run concurrently on overlapping version ranges:
1. Process A (backfill_id=12345) writes transaction files as `{version}_12345` and metadata with `suffix: Some(12345)`
2. Process B (backfill_id=67890) writes transaction files as `{version}_67890` and overwrites the same metadata.json with `suffix: Some(67890)`
3. Readers retrieve the metadata pointing to suffix 67890 but Process A's files become unreachable
4. If Process B hasn't completed, readers fail to find expected files

The file store implementations provide no conflict detection or atomic update mechanisms: [5](#0-4) [6](#0-5) 

## Impact Explanation

This qualifies as **High Severity** under the Aptos bug bounty criteria for "API crashes" and "Significant protocol violations":

- **Data Corruption**: Metadata inconsistency causes the indexer to reference non-existent or incorrect transaction files
- **Service Disruption**: The indexer gRPC API fails when attempting to serve data based on corrupted metadata [7](#0-6) 
- **Data Loss**: Transaction files become permanently unreachable once their metadata is overwritten
- **Resource Waste**: Duplicate storage of the same transaction data across multiple suffixes

While this does not directly affect blockchain consensus or validator operations, it impacts critical indexer infrastructure that dApps and services depend on for querying blockchain data.

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability can manifest in several realistic scenarios:

1. **Operational Error**: Administrators running multiple backfill instances with different progress files but overlapping ranges (e.g., to speed up backfilling or due to misconfiguration)
2. **Automation Issues**: Automated deployment systems starting concurrent backfill jobs without coordination
3. **Recovery Scenarios**: Multiple recovery attempts after failures, each generating a new backfill_id

The system explicitly supports concurrent processing within a single instance via `backfill_processing_task_count` [8](#0-7) , but lacks protection against multiple separate instances.

The timestamp-based backfill_id generation ensures each instance has a unique identifier, but this doesn't prevent metadata file conflicts.

## Recommendation

**Option 1: Include Suffix in Metadata Path** (Recommended)

Modify `get_path_for_batch_metadata()` to accept an optional suffix parameter and incorporate it into the path, similar to how transaction files are handled. This ensures each backfill process writes to a distinct metadata file.

**Option 2: Implement File Locking**

Add distributed locking (similar to the FileLock implementation in the Move package system) to prevent concurrent writes to the same metadata file. However, this adds complexity and potential performance bottlenecks.

**Option 3: Single Metadata File with Array of Suffixes**

Store an array of valid suffixes in the metadata file and implement atomic read-modify-write operations. This allows multiple backfills to coexist but requires careful synchronization.

**Recommended Fix (Option 1):**

Modify the metadata path to include the suffix when writing during backfill operations, and update the reader to handle both suffixed and non-suffixed metadata paths for backward compatibility.

## Proof of Concept

```rust
// Scenario: Two concurrent backfill processes
// 
// Process A configuration:
// - backfill_id: 1700000000 (timestamp when started)
// - starting_version: 0
// - ending_version: 100000
// - progress_file_path: "/tmp/backfill_a.json"
// - file_store: GCS bucket "indexer-data"
//
// Process B configuration:
// - backfill_id: 1700000100 (started 100 seconds later)
// - starting_version: 0  
// - ending_version: 100000
// - progress_file_path: "/tmp/backfill_b.json"
// - file_store: Same GCS bucket "indexer-data"
//
// Timeline:
// T1: Process A writes: gs://indexer-data/0/0_1700000000
// T2: Process A writes: gs://indexer-data/0/metadata.json 
//     Content: {"files":[...], "suffix":1700000000}
// T3: Process B writes: gs://indexer-data/0/0_1700000100
// T4: Process B writes: gs://indexer-data/0/metadata.json
//     Content: {"files":[...], "suffix":1700000100} ‚Üê OVERWRITES A's metadata
//
// Result:
// - File gs://indexer-data/0/0_1700000000 exists but is unreachable
// - Metadata points to suffix 1700000100
// - Readers trying to access version 0 will look for 0_1700000100
// - If Process B hasn't written that file yet, reads fail
//
// Reproduction steps:
// 1. Start backfiller instance A with config above
// 2. After it writes some data, start instance B with overlapping range
// 3. Observe metadata.json being overwritten
// 4. Attempt to read data - will fail or return wrong results
```

## Notes

This vulnerability is specific to the **indexer infrastructure** and does not affect core blockchain consensus, execution, or validator operations. However, it represents a significant operational risk for the Aptos ecosystem's data availability infrastructure that many dApps and services rely upon.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L80-83)
```rust
                    backfill_id: SystemTime::now()
                        .duration_since(UNIX_EPOCH)
                        .unwrap()
                        .as_secs(),
```

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/processor.rs (L249-249)
```rust
            batch_metadata.suffix = Some(self.backfill_id);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L56-64)
```rust
    pub fn get_path_for_version(&self, version: u64, suffix: Option<u64>) -> PathBuf {
        let mut buf = self.get_folder_name(version);
        if let Some(suffix) = suffix {
            buf.push(format!("{version}_{suffix}"));
        } else {
            buf.push(format!("{version}"));
        }
        buf
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L67-73)
```rust
    pub fn get_path_for_batch_metadata(&self, version: u64) -> PathBuf {
        let folder = self.get_folder_name(version);
        let mut batch_metadata_path = PathBuf::new();
        batch_metadata_path.push(folder);
        batch_metadata_path.push(METADATA_FILE_NAME);
        batch_metadata_path
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L122-124)
```rust
            let transactions = self
                .get_transaction_file_at_version(current_version, batch_metadata.suffix, retries)
                .await;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/gcs.rs (L120-137)
```rust
    async fn save_raw_file(&self, file_path: PathBuf, data: Vec<u8>) -> Result<()> {
        let path = self.get_path(file_path);
        trace!(
            "Uploading object to {}/{}.",
            self.bucket_name,
            path.as_str()
        );
        Object::create(
            self.bucket_name.as_str(),
            data,
            path.as_str(),
            JSON_FILE_TYPE,
        )
        .await
        .map_err(anyhow::Error::msg)?;

        Ok(())
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/local.rs (L61-69)
```rust
    async fn save_raw_file(&self, file_path: PathBuf, data: Vec<u8>) -> Result<()> {
        let file_path = self.path.join(file_path);
        if let Some(parent) = file_path.parent() {
            tokio::fs::create_dir_all(parent).await?;
        }
        tokio::fs::write(file_path, data)
            .await
            .map_err(anyhow::Error::msg)
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-v2-file-store-backfiller/src/lib.rs (L22-23)
```rust
    #[serde(default = "default_backfill_processing_task_count")]
    pub backfill_processing_task_count: usize,
```
