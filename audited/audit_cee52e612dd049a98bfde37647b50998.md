# Audit Report

## Title
Duplicate Peer Selection Vulnerability Due to Same PeerId Across Multiple Networks

## Summary
The peer prioritization and selection logic allows the same physical peer (identified by `PeerId`) to be selected multiple times when it appears on different networks (e.g., `NetworkId::Validator` and `NetworkId::Vfn`), causing duplicate requests, bandwidth waste, and divergent peer state tracking.

## Finding Description

The `get_peer_priority()` function determines peer priority based on the `NetworkId` component of a `PeerNetworkId`, not on the underlying `PeerId`. [1](#0-0) 

Aptos validators are configured to use the same `peer_id_name` ("owner_account") for both their validator network and VFN network connections, meaning the same `PeerId` exists on multiple `NetworkId`s. [2](#0-1) 

When a validator queries available peers, the same physical peer appears as two distinct `PeerNetworkId` entries with different priorities:
- `PeerNetworkId(NetworkId::Validator, peer_X)` → `HighPriority`
- `PeerNetworkId(NetworkId::Vfn, peer_X)` → `MediumPriority`

The peer selection functions `choose_peers_for_optimistic_fetch()` and `choose_peers_for_specific_data_request()` iterate through priority tiers and select peers without deduplicating by `PeerId`. [3](#0-2) 

Since `PeerNetworkId` is a composite of `(NetworkId, PeerId)`, the `HashSet<PeerNetworkId>` treats these as different entries even though they represent the same physical node. [4](#0-3) 

This causes:
1. **Duplicate requests**: The same data request is sent to the same peer twice via different network connections
2. **Bandwidth waste**: Unnecessary duplicate network traffic
3. **Divergent peer states**: Two separate `PeerState` entries are tracked for the same physical peer, which can diverge in latency, responsiveness, and other metrics
4. **Monitoring confusion**: Metrics and debugging become confusing when the same peer appears multiple times with different states

## Impact Explanation

**Severity: MEDIUM** (up to $10,000 per Aptos Bug Bounty)

This vulnerability causes **state inconsistencies requiring intervention**:
- The data client maintains separate peer states for what is physically the same node, violating the assumption that each peer has a single, consistent state
- Duplicate requests can cause validator slowdowns when the same peer is unnecessarily queried multiple times for identical data
- Debugging and operational monitoring become significantly more difficult when the same peer appears with different priorities and states

This does not rise to HIGH severity because:
- It does not directly cause API crashes or consensus violations
- It does not cause permanent liveness failures

This exceeds LOW severity because:
- It causes observable operational degradation (bandwidth waste, duplicate processing)
- It creates state inconsistencies that require manual intervention to understand and debug
- It affects core state synchronization functionality used by all validators

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability occurs under normal operational conditions:
1. The default validator configuration uses the same `peer_id_name` for both validator and VFN networks
2. Validators regularly connect to each other on both the validator network and VFN network
3. Multi-fetch is enabled by default for improved performance
4. No attacker action is required - this happens automatically during normal peer selection

The issue manifests whenever:
- A validator has a peer connected on multiple networks with the same `PeerId`
- A data request requires selecting multiple peers (multi-fetch enabled)
- The number of peers needed exceeds what's available in a single priority tier

## Recommendation

Implement deduplication by `PeerId` when selecting peers across multiple priority tiers:

```rust
// In choose_peers_for_optimistic_fetch and choose_peers_for_specific_data_request
fn choose_peers_with_deduplication(
    &self,
    serviceable_peers_by_priorities: Vec<HashSet<PeerNetworkId>>,
    num_peers_for_request: usize,
) -> crate::error::Result<HashSet<PeerNetworkId>, Error> {
    let mut selected_peers = HashSet::new();
    let mut selected_peer_ids = HashSet::new(); // Track PeerIds to prevent duplicates
    
    for serviceable_peers in serviceable_peers_by_priorities {
        // Filter out peers whose PeerId has already been selected
        let unique_peers: HashSet<PeerNetworkId> = serviceable_peers
            .into_iter()
            .filter(|peer| !selected_peer_ids.contains(&peer.peer_id()))
            .collect();
        
        let num_peers_remaining = num_peers_for_request.saturating_sub(selected_peers.len());
        let peers = self.choose_random_peers_by_distance_and_latency(
            unique_peers,
            num_peers_remaining,
        );
        
        // Track the PeerIds of selected peers
        for peer in &peers {
            selected_peer_ids.insert(peer.peer_id());
        }
        
        selected_peers.extend(peers);
        
        if selected_peers.len() >= num_peers_for_request {
            return Ok(selected_peers);
        }
    }
    
    if !selected_peers.is_empty() {
        Ok(selected_peers)
    } else {
        Err(Error::DataIsUnavailable("No unique peers available".to_string()))
    }
}
```

Additionally, consider whether peer states should be tracked per `PeerId` rather than per `PeerNetworkId` to avoid divergent state tracking for the same physical node.

## Proof of Concept

```rust
#[cfg(test)]
mod test_duplicate_peer_selection {
    use super::*;
    use aptos_config::{
        config::BaseConfig,
        network_id::{NetworkId, PeerNetworkId},
    };
    use aptos_types::PeerId;
    use std::collections::HashSet;
    
    #[test]
    fn test_same_peer_selected_multiple_times() {
        // Simulate a validator with a peer connected on both networks
        let same_peer_id = PeerId::random();
        
        // Create PeerNetworkIds for the same peer on different networks
        let peer_on_validator = PeerNetworkId::new(NetworkId::Validator, same_peer_id);
        let peer_on_vfn = PeerNetworkId::new(NetworkId::Vfn, same_peer_id);
        
        // Both are different PeerNetworkIds
        assert_ne!(peer_on_validator, peer_on_vfn);
        
        // But they have the same underlying PeerId
        assert_eq!(peer_on_validator.peer_id(), peer_on_vfn.peer_id());
        
        // Simulate the scenario where both appear in different priority buckets
        let mut high_priority_peers = HashSet::new();
        high_priority_peers.insert(peer_on_validator);
        
        let mut medium_priority_peers = HashSet::new();
        medium_priority_peers.insert(peer_on_vfn);
        
        // Simulate selecting from both buckets
        let mut selected_peers = HashSet::new();
        selected_peers.extend(high_priority_peers);
        selected_peers.extend(medium_priority_peers);
        
        // The same physical peer is now selected twice!
        assert_eq!(selected_peers.len(), 2);
        
        // Extract the underlying PeerIds
        let selected_peer_ids: HashSet<_> = selected_peers
            .iter()
            .map(|p| p.peer_id())
            .collect();
        
        // Only one unique PeerId, but two PeerNetworkIds were selected
        assert_eq!(selected_peer_ids.len(), 1);
        assert_eq!(selected_peers.len(), 2);
        
        println!("VULNERABILITY CONFIRMED: Same peer selected {} times", selected_peers.len());
    }
}
```

This test demonstrates that the same `PeerId` can appear as multiple `PeerNetworkId` entries in the selected peers set, confirming the duplicate selection vulnerability.

### Citations

**File:** state-sync/aptos-data-client/src/priority.rs (L53-72)
```rust
pub fn get_peer_priority(
    base_config: Arc<BaseConfig>,
    peers_and_metadata: Arc<PeersAndMetadata>,
    peer: &PeerNetworkId,
) -> PeerPriority {
    // Handle the case that this node is a validator
    let peer_network_id = peer.network_id();
    if base_config.role.is_validator() {
        // Validators should highly prioritize other validators
        if peer_network_id.is_validator_network() {
            return PeerPriority::HighPriority;
        }

        // VFNs should be prioritized over PFNs. Note: having PFNs
        // connected to a validator is a rare (but possible) scenario.
        return if peer_network_id.is_vfn_network() {
            PeerPriority::MediumPriority
        } else {
            PeerPriority::LowPriority
        };
```

**File:** config/src/config/test_data/validator.yaml (L24-46)
```yaml
full_node_networks:
    - listen_address: "/ip4/0.0.0.0/tcp/6181"
      max_outbound_connections: 0
      identity:
          type: "from_storage"
          key_name: "fullnode_network"
          peer_id_name: "owner_account"
          backend:
              type: "vault"
              server: "https://127.0.0.1:8200"
              ca_certificate: "/full/path/to/certificate"
              token:
                  from_disk: "/full/path/to/token"
      network_id:
          private: "vfn"

validator_network:
    discovery_method: "onchain"
    listen_address: "/ip4/0.0.0.0/tcp/6180"
    identity:
        type: "from_storage"
        key_name: "validator_network"
        peer_id_name: "owner_account"
```

**File:** state-sync/aptos-data-client/src/client.rs (L349-383)
```rust
    fn choose_peers_for_optimistic_fetch(
        &self,
        request: &StorageServiceRequest,
        serviceable_peers_by_priorities: Vec<HashSet<PeerNetworkId>>,
        num_peers_for_request: usize,
    ) -> crate::error::Result<HashSet<PeerNetworkId>, Error> {
        // Select peers by priority (starting with the highest priority first)
        let mut selected_peers = HashSet::new();
        for serviceable_peers in serviceable_peers_by_priorities {
            // Select peers by distance and latency
            let num_peers_remaining = num_peers_for_request.saturating_sub(selected_peers.len());
            let peers = self.choose_random_peers_by_distance_and_latency(
                serviceable_peers,
                num_peers_remaining,
            );

            // Add the peers to the entire set
            selected_peers.extend(peers);

            // If we have selected enough peers, return early
            if selected_peers.len() >= num_peers_for_request {
                return Ok(selected_peers);
            }
        }

        // If selected peers is empty, return an error
        if !selected_peers.is_empty() {
            Ok(selected_peers)
        } else {
            Err(Error::DataIsUnavailable(format!(
                "Unable to select peers for optimistic fetch request: {:?}",
                request
            )))
        }
    }
```

**File:** state-sync/aptos-data-client/src/client.rs (L394-419)
```rust
        // Select peers by priority (starting with the highest priority first)
        let mut selected_peers = HashSet::new();
        for serviceable_peers in serviceable_peers_by_priorities {
            // Select peers by distance and latency
            let num_peers_remaining = num_peers_for_request.saturating_sub(selected_peers.len());
            let peers = self.choose_random_peers_by_latency(serviceable_peers, num_peers_remaining);

            // Add the peers to the entire set
            selected_peers.extend(peers);

            // If we have selected enough peers, return early
            if selected_peers.len() >= num_peers_for_request {
                return Ok(selected_peers);
            }
        }

        // If selected peers is empty, return an error
        if !selected_peers.is_empty() {
            Ok(selected_peers)
        } else {
            Err(Error::DataIsUnavailable(format!(
                "Unable to select peers for specific data request: {:?}",
                request
            )))
        }
    }
```
