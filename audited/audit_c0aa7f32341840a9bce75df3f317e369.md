# Audit Report

## Title
Division by Zero Panic in Mempool Configuration - Node Crash via num_sender_buckets=0

## Summary
The `MempoolConfig::sanitize()` function fails to validate that `num_sender_buckets > 0`, allowing a node operator to configure their node with `num_sender_buckets: 0`. This causes immediate division by zero panics when any transaction is processed, resulting in complete node unavailability. [1](#0-0) 

## Finding Description
The mempool configuration allows `num_sender_buckets` (a `u8` field) to be deserialized from YAML configuration files without validation. The sanitize function contains only a TODO comment indicating missing validation. [2](#0-1) 

When `num_sender_buckets` is set to 0, the `sender_bucket()` function performs a modulo operation that triggers division by zero: [3](#0-2) 

This function is called in critical transaction processing paths:
- Transaction insertion and metrics tracking
- Transaction removal during commit/rejection
- Garbage collection of expired transactions
- Timeline index operations for broadcast [4](#0-3) [5](#0-4) [6](#0-5) 

Additionally, `TransactionStore::new()` initializes an empty `timeline_index` HashMap when `num_sender_buckets` is 0, causing subsequent `.unwrap()` calls to panic when accessing non-existent bucket entries. [7](#0-6) 

The configuration sanitizer is called during node startup and explicitly validates MempoolConfig: [8](#0-7) 

## Impact Explanation
This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria: "Validator node slowdowns / API crashes / Significant protocol violations."

The impact is severe:
- **Complete Node DoS**: Any node configured with `num_sender_buckets: 0` will panic and crash immediately upon receiving its first transaction
- **Unrecoverable State**: The node cannot restart and resume operation - it will continue crashing on any transaction processing attempt
- **All Node Types Affected**: Validators, Validator Fullnodes, and Public Fullnodes are all vulnerable
- **Network Disruption**: If multiple validators accidentally use this configuration, it could impact consensus liveness

While this requires the node operator to misconfigure their own node, it represents a critical availability failure that should be prevented through configuration validation.

## Likelihood Explanation
**Likelihood: Medium-Low**

The vulnerability requires:
1. A node operator to explicitly set `num_sender_buckets: 0` in their configuration YAML
2. The default value is 4, and the optimizer sets it to 1 for validators/VFNs [9](#0-8) [10](#0-9) 

However, misconfigurations do occur, and the presence of the TODO comment suggests this validation gap is known but unaddressed. A node operator attempting to "disable" sender bucketing or experimenting with minimal configurations could accidentally trigger this.

## Recommendation
Add validation in the `MempoolConfig::sanitize()` function to enforce `num_sender_buckets > 0`:

```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let mempool_config = &node_config.mempool;
        
        // Validate num_sender_buckets is non-zero to prevent division by zero
        if mempool_config.num_sender_buckets == 0 {
            return Err(Error::ConfigSanitizerFailed(
                Self::get_sanitizer_name(),
                "num_sender_buckets must be greater than 0 to prevent division by zero in sender bucketing".into(),
            ));
        }
        
        Ok(())
    }
}
```

## Proof of Concept

Create a test configuration file `malicious_config.yaml`:
```yaml
mempool:
  num_sender_buckets: 0
```

Rust reproduction test:
```rust
#[test]
#[should_panic(expected = "attempt to calculate the remainder with a divisor of zero")]
fn test_zero_sender_buckets_division_by_zero() {
    use aptos_config::config::MempoolConfig;
    use aptos_types::account_address::AccountAddress;
    use crate::core_mempool::transaction_store::{sender_bucket, TransactionStore};

    // Create config with num_sender_buckets = 0
    let mut config = MempoolConfig::default();
    config.num_sender_buckets = 0;

    // This will create empty timeline_index
    let _store = TransactionStore::new(&config);

    // This will panic with division by zero
    let address = AccountAddress::random();
    let _bucket = sender_bucket(&address, 0);
}
```

The node will crash immediately when processing any transaction with this configuration, resulting in complete unavailability.

---

**Notes:**
- This vulnerability exists because configuration validation is incomplete, as indicated by the TODO comment in the sanitize function
- The default values and optimizer prevent this in standard configurations, but explicit misconfiguration is possible
- The fix is straightforward: add a single validation check to reject zero-valued `num_sender_buckets`

### Citations

**File:** config/src/config/mempool_config.rs (L97-97)
```rust
    pub num_sender_buckets: u8,
```

**File:** config/src/config/mempool_config.rs (L137-137)
```rust
            num_sender_buckets: 4,
```

**File:** config/src/config/mempool_config.rs (L176-184)
```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        Ok(()) // TODO: add reasonable verifications
    }
}
```

**File:** config/src/config/mempool_config.rs (L210-213)
```rust
            if local_mempool_config_yaml["num_sender_buckets"].is_null() {
                mempool_config.num_sender_buckets = 1;
                modified_config = true;
            }
```

**File:** mempool/src/core_mempool/transaction_store.rs (L42-47)
```rust
pub fn sender_bucket(
    address: &AccountAddress,
    num_sender_buckets: MempoolSenderBucket,
) -> MempoolSenderBucket {
    address.as_ref()[address.as_ref().len() - 1] as MempoolSenderBucket % num_sender_buckets
}
```

**File:** mempool/src/core_mempool/transaction_store.rs (L104-111)
```rust
    pub(crate) fn new(config: &MempoolConfig) -> Self {
        let mut timeline_index = HashMap::new();
        for sender_bucket in 0..config.num_sender_buckets {
            timeline_index.insert(
                sender_bucket,
                MultiBucketTimelineIndex::new(config.broadcast_buckets.clone()).unwrap(),
            );
        }
```

**File:** mempool/src/core_mempool/transaction_store.rs (L554-566)
```rust
                let sender_bucket = sender_bucket(address, self.num_sender_buckets);
                let ready_for_quorum_store = !self.priority_index.contains(txn);

                self.priority_index.insert(txn);

                // If timeline_state is `NonQualified`, then the transaction is never added to the timeline_index,
                // and never broadcasted to the shared mempool.
                let ready_for_mempool_broadcast = txn.timeline_state == TimelineState::NotReady;
                if ready_for_mempool_broadcast {
                    self.timeline_index
                        .get_mut(&sender_bucket)
                        .unwrap()
                        .insert(txn);
```

**File:** mempool/src/core_mempool/transaction_store.rs (L744-752)
```rust
        let sender_bucket = sender_bucket(&txn.get_sender(), self.num_sender_buckets);
        self.timeline_index
            .get_mut(&sender_bucket)
            .unwrap_or_else(|| {
                panic!(
                    "Unable to get the timeline index for the sender bucket {}",
                    sender_bucket
                )
            })
```

**File:** mempool/src/core_mempool/mempool.rs (L352-359)
```rust
            counters::SENDER_BUCKET_FREQUENCIES
                .with_label_values(&[sender_bucket(
                    &sender,
                    self.transactions.num_sender_buckets(),
                )
                .to_string()
                .as_str()])
                .inc();
```

**File:** config/src/config/config_sanitizer.rs (L62-62)
```rust
        MempoolConfig::sanitize(node_config, node_type, chain_id)?;
```
