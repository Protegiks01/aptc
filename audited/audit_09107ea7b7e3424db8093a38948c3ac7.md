# Audit Report

## Title
Unvalidated Peer Monitoring Service Configuration Enables Resource Exhaustion via Request Spam

## Summary
The `PeerState::new()` function accepts a `NodeConfig` parameter without validating peer monitoring service interval values. Zero or overflow-inducing configuration values cause the peer monitoring client to spam unlimited requests to all connected peers, leading to CPU exhaustion, network flooding, and node performance degradation.

## Finding Description

The vulnerability exists in the peer monitoring service client initialization flow where configuration values are used without validation: [1](#0-0) 

The `node_config` parameter is passed directly to `PeerStateValue::new()` without any bounds checking: [2](#0-1) 

Configuration values are extracted and used to create request trackers: [3](#0-2) 

The critical flaw occurs in `RequestTracker::new()` where the interval is multiplied by 1000 without overflow checking: [4](#0-3) 

**Attack Scenario 1 - Zero Interval:**
If `latency_ping_interval_ms = 0`, then `request_interval_usec = 0`. The `new_request_required()` function will always return true after the first request: [5](#0-4) 

With a zero-duration interval, the condition `now > last_request_time + Duration::from_micros(0)` is always true for any elapsed time, causing continuous request spam.

**Attack Scenario 2 - Overflow:**
If `request_interval_ms > u64::MAX / 1000`, the multiplication wraps to a very small value, causing similar excessive request frequency.

**Propagation Path:**
1. Corrupted/malicious config loaded at node startup
2. `PeerState::new()` called for each connected peer with unvalidated config
3. Zero intervals create `RequestTracker` with zero-duration intervals  
4. Peer monitoring loop runs every second, checking all peers and state keys
5. `new_request_required()` always returns true, spawning unlimited requests
6. For N peers and 3 state types (LatencyInfo, NetworkInfo, NodeInfo), this generates 3N requests/second continuously [6](#0-5) 

**Missing Validation:**
The `ConfigSanitizer` implementation does not validate `PeerMonitoringServiceConfig` values: [7](#0-6) 

No sanitizer exists for peer monitoring configuration intervals, timeout values, or other numeric constraints.

## Impact Explanation

**Severity: Medium to High**

This vulnerability causes validator node slowdowns and resource exhaustion:

- **CPU Exhaustion**: Continuous spawning of tokio tasks for each request
- **Network Flooding**: Unlimited monitoring requests to all connected peers (300+ requests/second for 100 peers)
- **Peer Disconnections**: Remote peers may rate-limit or disconnect the spamming node
- **Consensus Degradation**: CPU and network saturation can prevent timely consensus participation
- **State Sync Failures**: Resource exhaustion affects ability to sync state

This fits **High Severity** criteria: "Validator node slowdowns" and "Significant protocol violations", or **Medium Severity**: "State inconsistencies requiring intervention" (node falls behind and requires restart/config fix).

The impact is contained to the misconfigured node but can cascade to affect consensus if multiple validators are affected by the same config generation bug.

## Likelihood Explanation

**Likelihood: Medium**

While this requires a corrupted configuration file, several realistic scenarios enable exploitation:

1. **Accidental Corruption**: Disk errors, file system corruption, or incomplete writes during config updates
2. **Config Generation Bugs**: Automated config generation tools with bugs that produce zero/invalid values  
3. **Operator Error**: Manual config editing mistakes (typos, missing digits)
4. **Software Bugs**: Config merging or templating logic that produces invalid values
5. **Default Value Issues**: If config deserialization defaults to 0 for missing fields

The vulnerability is **always exploitable** once the corrupted config is loaded - no further attacker action needed. Node operators may not immediately notice the issue until performance degrades significantly.

## Recommendation

Implement comprehensive config validation in both the `ConfigSanitizer` and at point-of-use:

**1. Add ConfigSanitizer implementation:**

```rust
impl ConfigSanitizer for PeerMonitoringServiceConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let config = &node_config.peer_monitoring_service;
        let sanitizer_name = Self::get_sanitizer_name();
        
        // Validate latency monitoring config
        validate_interval("latency_ping_interval_ms", 
            config.latency_monitoring.latency_ping_interval_ms, 1000, 3600000, &sanitizer_name)?;
        validate_interval("latency_ping_timeout_ms",
            config.latency_monitoring.latency_ping_timeout_ms, 100, 60000, &sanitizer_name)?;
            
        // Validate network monitoring config  
        validate_interval("network_info_request_interval_ms",
            config.network_monitoring.network_info_request_interval_ms, 1000, 3600000, &sanitizer_name)?;
        validate_interval("network_info_request_timeout_ms",
            config.network_monitoring.network_info_request_timeout_ms, 100, 60000, &sanitizer_name)?;
            
        // Validate node monitoring config
        validate_interval("node_info_request_interval_ms",
            config.node_monitoring.node_info_request_interval_ms, 1000, 3600000, &sanitizer_name)?;
        validate_interval("node_info_request_timeout_ms",
            config.node_monitoring.node_info_request_timeout_ms, 100, 60000, &sanitizer_name)?;
            
        // Validate max_num_latency_pings_to_retain is non-zero
        if config.latency_monitoring.max_num_latency_pings_to_retain == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "max_num_latency_pings_to_retain must be greater than 0".into(),
            ));
        }
        
        Ok(())
    }
}

fn validate_interval(name: &str, value: u64, min: u64, max: u64, sanitizer_name: &str) -> Result<(), Error> {
    if value < min || value > max {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name.to_string(),
            format!("{} must be between {} and {} ms, got: {}", name, min, max, value),
        ));
    }
    // Check for overflow when converting to microseconds
    if value > u64::MAX / 1000 {
        return Err(Error::ConfigSanitizerFailed(
            sanitizer_name.to_string(),
            format!("{} value too large, would overflow when converted to microseconds: {}", name, value),
        ));
    }
    Ok(())
}
```

**2. Add call to sanitizer in NodeConfig::sanitize():**

Add to line 66 in config_sanitizer.rs:
```rust
PeerMonitoringServiceConfig::sanitize(node_config, node_type, chain_id)?;
```

**3. Add defensive checks in RequestTracker::new():**

```rust
pub fn new(request_interval_ms: u64, time_service: TimeService) -> Self {
    // Defensive validation even if config sanitizer runs
    assert!(request_interval_ms > 0, "request_interval_ms must be greater than 0");
    assert!(request_interval_ms <= u64::MAX / 1000, "request_interval_ms too large for conversion");
    
    let request_interval_usec = request_interval_ms.checked_mul(1000)
        .expect("request_interval_ms overflow when converting to microseconds");
    RequestTracker::new_with_microseconds(request_interval_usec, time_service)
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_config::config::{NodeConfig, LatencyMonitoringConfig};
    use aptos_time_service::TimeService;
    
    #[test]
    #[should_panic(expected = "request_interval_ms must be greater than 0")]
    fn test_zero_interval_causes_panic() {
        // Create a config with zero latency ping interval
        let mut node_config = NodeConfig::default();
        node_config.peer_monitoring_service.latency_monitoring.latency_ping_interval_ms = 0;
        
        // This should panic or fail validation
        let time_service = TimeService::mock();
        let _peer_state = PeerState::new(node_config, time_service);
    }
    
    #[test]
    fn test_zero_interval_causes_infinite_requests() {
        // Create request tracker with zero interval
        let time_service = TimeService::mock();
        let mut tracker = RequestTracker::new_with_microseconds(0, time_service.clone());
        
        // First request
        tracker.request_started();
        tracker.request_completed();
        tracker.record_response_success();
        
        // With zero interval, new requests are ALWAYS required
        assert!(tracker.new_request_required()); // Should be true
        
        // Even immediately after
        assert!(tracker.new_request_required()); // Still true - BUG!
    }
    
    #[test]
    #[should_panic(expected = "overflow")]
    fn test_overflow_interval() {
        let time_service = TimeService::mock();
        // This should cause overflow: u64::MAX / 100 * 1000 > u64::MAX
        let _tracker = RequestTracker::new(u64::MAX / 100, time_service);
    }
}
```

## Notes

This vulnerability violates the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The lack of configuration validation allows corrupted or invalid configurations to cause unbounded resource consumption through request spam, degrading node performance and potentially affecting consensus participation. Defense-in-depth principles require validating all external inputs, including configuration files, even when loaded by trusted operators.

### Citations

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L43-55)
```rust
    pub fn new(node_config: NodeConfig, time_service: TimeService) -> Self {
        // Create a state entry for each peer state key
        let state_entries = Arc::new(RwLock::new(HashMap::new()));
        for peer_state_key in PeerStateKey::get_all_keys() {
            let peer_state_value =
                PeerStateValue::new(node_config.clone(), time_service.clone(), &peer_state_key);
            state_entries
                .write()
                .insert(peer_state_key, Arc::new(RwLock::new(peer_state_value)));
        }

        Self { state_entries }
    }
```

**File:** peer-monitoring-service/client/src/peer_states/key_value.rs (L110-127)
```rust
    pub fn new(
        node_config: NodeConfig,
        time_service: TimeService,
        peer_state_key: &PeerStateKey,
    ) -> Self {
        match peer_state_key {
            PeerStateKey::LatencyInfo => {
                let latency_monitoring_config =
                    node_config.peer_monitoring_service.latency_monitoring;
                LatencyInfoState::new(latency_monitoring_config, time_service).into()
            },
            PeerStateKey::NetworkInfo => NetworkInfoState::new(node_config, time_service).into(),
            PeerStateKey::NodeInfo => {
                let node_monitoring_config = node_config.peer_monitoring_service.node_monitoring;
                NodeInfoState::new(node_monitoring_config, time_service).into()
            },
        }
    }
```

**File:** peer-monitoring-service/client/src/peer_states/latency_info.rs (L35-50)
```rust
    pub fn new(
        latency_monitoring_config: LatencyMonitoringConfig,
        time_service: TimeService,
    ) -> Self {
        let request_tracker = RequestTracker::new(
            latency_monitoring_config.latency_ping_interval_ms,
            time_service,
        );

        Self {
            latency_monitoring_config,
            latency_ping_counter: 0,
            recorded_latency_ping_durations_secs: BTreeMap::new(),
            request_tracker: Arc::new(RwLock::new(request_tracker)),
        }
    }
```

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L22-26)
```rust
    /// Creates a new request tracker with the given request interval in ms
    pub fn new(request_interval_ms: u64, time_service: TimeService) -> Self {
        let request_interval_usec = request_interval_ms * 1000;
        RequestTracker::new_with_microseconds(request_interval_usec, time_service)
    }
```

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L74-90)
```rust
    /// Returns true iff a new request should be sent (based
    /// on the latest response time).
    pub fn new_request_required(&self) -> bool {
        // There's already an in-flight request. A new one should not be sent.
        if self.in_flight_request() {
            return false;
        }

        // Otherwise, check the last request time for freshness
        match self.last_request_time {
            Some(last_request_time) => {
                self.time_service.now()
                    > last_request_time.add(Duration::from_micros(self.request_interval_usec))
            },
            None => true, // A request should be sent immediately
        }
    }
```

**File:** peer-monitoring-service/client/src/peer_states/mod.rs (L41-68)
```rust
    for peer_state_key in PeerStateKey::get_all_keys() {
        let mut num_in_flight_requests = 0;

        // Go through all connected peers and see if we should refresh the state
        for (peer_network_id, peer_metadata) in &connected_peers_and_metadata {
            // Get the peer state
            let peer_state = get_peer_state(&peer_monitor_state, peer_network_id)?;

            // If there's an-flight request, update the metrics counter
            let request_tracker = peer_state.get_request_tracker(&peer_state_key)?;
            if request_tracker.read().in_flight_request() {
                num_in_flight_requests += 1;
            }

            // Update the state if it needs to be refreshed
            let should_refresh_peer_state_key = request_tracker.read().new_request_required();
            if should_refresh_peer_state_key {
                peer_state.refresh_peer_state_key(
                    monitoring_service_config,
                    &peer_state_key,
                    peer_monitoring_client.clone(),
                    *peer_network_id,
                    peer_metadata.clone(),
                    peer_monitor_state.request_id_generator.clone(),
                    time_service.clone(),
                    runtime.clone(),
                )?;
            }
```

**File:** config/src/config/config_sanitizer.rs (L39-70)
```rust
impl ConfigSanitizer for NodeConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        // If config sanitization is disabled, don't do anything!
        if node_config.node_startup.skip_config_sanitizer {
            return Ok(());
        }

        // Sanitize all of the sub-configs
        AdminServiceConfig::sanitize(node_config, node_type, chain_id)?;
        ApiConfig::sanitize(node_config, node_type, chain_id)?;
        BaseConfig::sanitize(node_config, node_type, chain_id)?;
        ConsensusConfig::sanitize(node_config, node_type, chain_id)?;
        DagConsensusConfig::sanitize(node_config, node_type, chain_id)?;
        ExecutionConfig::sanitize(node_config, node_type, chain_id)?;
        sanitize_failpoints_config(node_config, node_type, chain_id)?;
        sanitize_fullnode_network_configs(node_config, node_type, chain_id)?;
        IndexerGrpcConfig::sanitize(node_config, node_type, chain_id)?;
        InspectionServiceConfig::sanitize(node_config, node_type, chain_id)?;
        LoggerConfig::sanitize(node_config, node_type, chain_id)?;
        MempoolConfig::sanitize(node_config, node_type, chain_id)?;
        NetbenchConfig::sanitize(node_config, node_type, chain_id)?;
        StateSyncConfig::sanitize(node_config, node_type, chain_id)?;
        StorageConfig::sanitize(node_config, node_type, chain_id)?;
        InternalIndexerDBConfig::sanitize(node_config, node_type, chain_id)?;
        sanitize_validator_network_config(node_config, node_type, chain_id)?;

        Ok(()) // All configs passed validation
    }
```
