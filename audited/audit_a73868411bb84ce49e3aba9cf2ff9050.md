# Audit Report

## Title
Indexer Event Integrity Vulnerability: Missing Cryptographic Verification of Events Against Committed Event Root Hash

## Summary
The Aptos indexer processes events transmitted via gRPC without verifying their cryptographic integrity against the `event_root_hash` committed in consensus. This allows malicious fullnode operators or MITM attackers on unencrypted connections to send tampered event data (including `BurnEvent`s) that indexers will accept and store, despite validators having cryptographically committed to different event data via the blockchain's `event_root_hash` in `TransactionInfo`.

## Finding Description

Events in Aptos are cryptographically committed through a Merkle accumulator whose root hash is stored in `TransactionInfo.event_root_hash`. This hash is part of blockchain consensus and signed by validators. [1](#0-0) 

The codebase provides a verification function `verify_events_against_root_hash` that validates events against the committed hash: [2](#0-1) 

When fullnodes stream transactions to indexers via gRPC, they transmit both the events AND the `event_root_hash`: [3](#0-2) 

However, indexers process received events without performing any cryptographic verification: [4](#0-3) [5](#0-4) 

The gRPC connection is not secured by default, with TLS being optional: [6](#0-5) 

## Impact Explanation

This falls under **Medium Severity** per bug bounty criteria as it creates "state inconsistencies requiring intervention" in the indexer layer. Specifically:

- **Off-chain data corruption**: Indexers would store tampered `BurnEvent` data with incorrect token addresses or index values
- **Application impact**: DApps relying on indexer data for token supply tracking, NFT burn verification, or transaction history would receive incorrect information
- **Trust boundary violation**: While blockchain consensus remains secure, the indexer layer becomes an untrusted data source

However, this does NOT constitute:
- Consensus violation (validators remain unaffected)
- On-chain fund loss (blockchain state is cryptographically secure)
- Validator node compromise (only indexer infrastructure affected)

## Likelihood Explanation

**Likelihood: Medium**

Attack requires one of:
1. **Malicious fullnode operator**: Indexer connects to attacker-controlled fullnode
2. **MITM attack**: Attacker intercepts unencrypted gRPC stream between legitimate fullnode and indexer

The attack is straightforward:
- Attacker sends legitimate `event_root_hash` values from blockchain
- Attacker sends tampered event data (e.g., modifying `BurnEvent.token` address)
- Indexer accepts both without verification
- Tampered events get stored in indexer database

## Recommendation

Add cryptographic event verification in the indexer processing pipeline:

```rust
// In crates/indexer/src/models/events.rs or transaction processor
use aptos_types::proof::accumulator::InMemoryEventAccumulator;
use aptos_crypto::CryptoHash;

pub fn verify_and_process_events(
    events: &[APIEvent],
    transaction_info: &TransactionInfo,
    transaction_version: i64,
    block_height: i64,
) -> Result<Vec<EventModel>, ProcessingError> {
    // Verify events against committed root hash
    let event_hashes: Vec<_> = events.iter()
        .map(|e| CryptoHash::hash(e))
        .collect();
    let computed_root = InMemoryEventAccumulator::from_leaves(&event_hashes).root_hash();
    
    if computed_root != transaction_info.event_root_hash {
        return Err(ProcessingError::EventIntegrityViolation {
            version: transaction_version,
            expected_hash: transaction_info.event_root_hash,
            computed_hash: computed_root,
        });
    }
    
    // Only process verified events
    Ok(events.iter().enumerate().map(|(index, event)| {
        EventModel::from_event(event, transaction_version, block_height, index as i64)
    }).collect())
}
```

Additionally:
1. **Enforce TLS**: Make TLS mandatory for gRPC connections
2. **Add authentication**: Implement mutual TLS or token-based authentication
3. **Monitor verification failures**: Alert on hash mismatches

## Proof of Concept

```rust
// PoC demonstrating indexer accepts tampered events
// File: crates/indexer/src/test_event_tampering.rs

#[cfg(test)]
mod tests {
    use super::*;
    use aptos_api_types::{Event as APIEvent, TransactionInfo};
    use aptos_crypto::{hash::CryptoHash, HashValue};
    
    #[test]
    fn test_indexer_accepts_tampered_burn_events() {
        // Create legitimate BurnEvent
        let legitimate_event = create_burn_event("0xabc", 123);
        
        // Create corresponding TransactionInfo with correct event_root_hash
        let event_hash = CryptoHash::hash(&legitimate_event);
        let event_root_hash = InMemoryEventAccumulator::from_leaves(&[event_hash]).root_hash();
        let txn_info = create_transaction_info(event_root_hash);
        
        // Attacker tampers with event (changes token address)
        let tampered_event = create_burn_event("0xmalicious", 123);
        
        // Current implementation: Indexer processes tampered event WITHOUT verification
        let indexed_events = EventModel::from_events(
            &[tampered_event],  // Tampered data
            1,                   // version
            100,                 // block_height
        );
        
        // Proof: Tampered event is accepted and would be stored
        assert_eq!(indexed_events.len(), 1);
        assert_eq!(indexed_events[0].account_address, "0xmalicious"); // Tamper succeeded!
        
        // What SHOULD happen: Verification against txn_info.event_root_hash should fail
        // But current code doesn't perform this check
    }
}
```

## Notes

**Answer to original question**: Validators CAN detect if BurnEvent data has been tampered with - the `event_root_hash` in `TransactionInfo` provides this capability. The vulnerability is that **indexers** don't utilize this detection mechanism, creating an integrity gap in the off-chain data layer.

This represents a defense-in-depth failure rather than a core protocol vulnerability. Applications should not blindly trust indexer data for security-critical operations and should verify against on-chain state when necessary.

### Citations

**File:** types/src/transaction/mod.rs (L2023-2051)
```rust
#[derive(Clone, CryptoHasher, BCSCryptoHash, Debug, Eq, PartialEq, Serialize, Deserialize)]
#[cfg_attr(any(test, feature = "fuzzing"), derive(Arbitrary))]
pub struct TransactionInfoV0 {
    /// The amount of gas used.
    gas_used: u64,

    /// The vm status. If it is not `Executed`, this will provide the general error class. Execution
    /// failures and Move abort's receive more detailed information. But other errors are generally
    /// categorized with no status code or other information
    status: ExecutionStatus,

    /// The hash of this transaction.
    transaction_hash: HashValue,

    /// The root hash of Merkle Accumulator storing all events emitted during this transaction.
    event_root_hash: HashValue,

    /// The hash value summarizing all changes caused to the world state by this transaction.
    /// i.e. hash of the output write set.
    state_change_hash: HashValue,

    /// The root hash of the Sparse Merkle Tree describing the world state at the end of this
    /// transaction. Depending on the protocol configuration, this can be generated periodical
    /// only, like per block.
    state_checkpoint_hash: Option<HashValue>,

    /// The hash value summarizing PersistedAuxiliaryInfo.
    auxiliary_info_hash: Option<HashValue>,
}
```

**File:** types/src/transaction/mod.rs (L2629-2643)
```rust
fn verify_events_against_root_hash(
    events: &[ContractEvent],
    transaction_info: &TransactionInfo,
) -> Result<()> {
    let event_hashes: Vec<_> = events.iter().map(CryptoHash::hash).collect();
    let event_root_hash = InMemoryEventAccumulator::from_leaves(&event_hashes).root_hash();
    ensure!(
        event_root_hash == transaction_info.event_root_hash(),
        "The event root hash calculated doesn't match that carried on the \
                         transaction info! Calculated hash {:?}, transaction info hash {:?}",
        event_root_hash,
        transaction_info.event_root_hash()
    );
    Ok(())
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/convert.rs (L570-586)
```rust
pub fn convert_transaction_info(
    transaction_info: &TransactionInfo,
) -> transaction::TransactionInfo {
    transaction::TransactionInfo {
        hash: transaction_info.hash.0.to_vec(),
        state_checkpoint_hash: transaction_info
            .state_checkpoint_hash
            .map(|hash| hash.0.to_vec()),
        state_change_hash: transaction_info.state_change_hash.0.to_vec(),
        event_root_hash: transaction_info.event_root_hash.0.to_vec(),
        gas_used: transaction_info.gas_used.0,
        success: transaction_info.success,
        vm_status: transaction_info.vm_status.to_string(),
        accumulator_root_hash: transaction_info.accumulator_root_hash.0.to_vec(),
        changes: convert_write_set_changes(&transaction_info.changes),
    }
}
```

**File:** crates/indexer/src/models/events.rs (L61-79)
```rust
    pub fn from_events(
        events: &[APIEvent],
        transaction_version: i64,
        transaction_block_height: i64,
    ) -> Vec<Self> {
        events
            .iter()
            .enumerate()
            .map(|(index, event)| {
                Self::from_event(
                    event,
                    transaction_version,
                    transaction_block_height,
                    index as i64,
                )
            })
            .collect::<Vec<EventModel>>()
    }
}
```

**File:** crates/indexer/src/models/transactions.rs (L141-145)
```rust
                    EventModel::from_events(
                        &user_txn.events,
                        user_txn.info.version.0 as i64,
                        block_height,
                    ),
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/runtime.rs (L101-130)
```rust
        let tonic_server = Server::builder()
            .http2_keepalive_interval(Some(std::time::Duration::from_secs(60)))
            .http2_keepalive_timeout(Some(std::time::Duration::from_secs(5)))
            .add_service(reflection_service_clone);

        let router = match use_data_service_interface {
            false => {
                let svc = FullnodeDataServer::new(server)
                    .send_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Gzip);
                tonic_server.add_service(svc)
            },
            true => {
                let svc = RawDataServer::new(localnet_data_server)
                    .send_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Gzip);
                tonic_server.add_service(svc)
            },
        };

        let listener = TcpListener::bind(address).await.unwrap();
        if let Some(port_tx) = port_tx {
            port_tx.send(listener.local_addr().unwrap().port()).unwrap();
        }
        let incoming = TcpIncoming::from_listener(listener, false, None).unwrap();

        // Make port into a config
        router.serve_with_incoming(incoming).await.unwrap();
```
