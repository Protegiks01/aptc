# Audit Report

## Title
Lack of Tamper Detection for External PostgreSQL Indexer Database Enables Undetectable Data Corruption

## Summary
The external PostgreSQL indexer lacks cryptographic validation mechanisms to detect database tampering. If an attacker gains database access and modifies `MoveResource` entries, the system cannot detect or reject the corrupted data when serving queries via the Hasura GraphQL API.

## Finding Description
The Aptos indexer system (`crates/indexer/`) processes blockchain transactions and stores resource data in a PostgreSQL database. The `MoveResource::from_write_resource()` function converts blockchain `WriteResource` objects into database models: [1](#0-0) 

While the system stores a `state_key_hash` field (added via migration), [2](#0-1)  **no validation code exists** that verifies this hash when reading data. The indexer processes transactions and inserts data into PostgreSQL: [3](#0-2) 

The Hasura GraphQL API exposes this data directly without any integrity checks: [4](#0-3) 

**Attack Scenario:**
1. Attacker gains database access via SQL injection, credential theft, or insider threat
2. Attacker modifies `move_resources` table entries (e.g., changing `address`, `data`, `type` fields)
3. Modified data is served to users via GraphQL API without detection
4. Applications relying on indexer data receive incorrect information

The indexer README confirms it's a one-way write pipeline with no read validation: [5](#0-4) 

## Impact Explanation
**This does NOT meet bug bounty Critical/High severity criteria** because:

1. **No Consensus Impact**: The actual blockchain state in AptosDB remains secure with Merkle proof verification. Validators do not use the PostgreSQL indexer for consensus operations.

2. **No Fund Loss**: Blockchain funds are protected by the core protocol. Indexer tampering cannot steal or freeze funds.

3. **Auxiliary Service Only**: The PostgreSQL indexer is a separate query service. The Node API uses the internal RocksDB indexer or AptosDB directly, not the PostgreSQL indexer: [6](#0-5) 

4. **No Protocol Violation**: AptosBFT consensus, Move VM execution, and state commitment remain unaffected.

While this is a **data integrity issue** for the indexer service that could mislead applications, it does not break any of the 10 critical invariants specified (Deterministic Execution, Consensus Safety, State Consistency, etc.), which all apply to the blockchain protocol itself, not auxiliary services.

## Likelihood Explanation
Database access requires significant compromise (SQL injection, stolen credentials, or insider threat), but the lack of validation means tampering would be undetectable once access is obtained.

## Recommendation
Implement tamper detection by validating `state_key_hash` against blockchain state when serving queries, or implement periodic validation against AptosDB with Merkle proof verification similar to the internal indexer validation: [7](#0-6) 

## Proof of Concept
This requires SQL access rather than Move/Rust code:
```sql
-- Attacker with database access modifies resource data
UPDATE move_resources 
SET data = '{"malicious": "data"}'::jsonb,
    address = '0xmalicious_address'
WHERE transaction_version = 12345 AND write_set_change_index = 0;
-- Query via GraphQL returns modified data without detection
```

## Notes
**After strict validation against the checklist:** This issue does **NOT** meet the bug bounty severity criteria because it only affects an auxiliary query service (PostgreSQL indexer), not the core blockchain protocol. The blockchain's consensus, state integrity, and fund security remain protected by Merkle proofs and cryptographic verification in AptosDB. The validation code in `validation.rs` validates the internal RocksDB indexer, not the external PostgreSQL indexer used by Hasura.

Given the extremely high bar for validity and the emphasis that "False positives harm credibility more than missed findings," this is a legitimate data integrity concern for the indexer service but does not constitute a Critical/High severity blockchain protocol vulnerability per the bug bounty program criteria.

### Citations

**File:** crates/indexer/src/models/move_resources.rs (L36-56)
```rust
    pub fn from_write_resource(
        write_resource: &WriteResource,
        write_set_change_index: i64,
        transaction_version: i64,
        transaction_block_height: i64,
    ) -> Self {
        let parsed_data = Self::convert_move_struct_tag(&write_resource.data.typ);
        Self {
            transaction_version,
            transaction_block_height,
            write_set_change_index,
            type_: write_resource.data.typ.to_string(),
            name: parsed_data.name.clone(),
            address: standardize_address(&write_resource.address.to_string()),
            module: parsed_data.module.clone(),
            generic_type_params: parsed_data.generic_type_params,
            data: Some(serde_json::to_value(&write_resource.data.data).unwrap()),
            is_deleted: false,
            state_key_hash: standardize_address(write_resource.state_key_hash.as_str()),
        }
    }
```

**File:** crates/indexer/migrations/2023-04-28-053048_object_token_v2/up.sql (L36-37)
```sql
ALTER TABLE move_resources
ADD COLUMN IF NOT EXISTS state_key_hash VARCHAR(66) NOT NULL DEFAULT '';
```

**File:** crates/indexer/src/processors/default_processor.rs (L337-357)
```rust
fn insert_move_resources(
    conn: &mut PgConnection,
    items_to_insert: &[MoveResource],
) -> Result<(), diesel::result::Error> {
    use schema::move_resources::dsl::*;
    let chunks = get_chunks(items_to_insert.len(), MoveResource::field_count());
    for (start_ind, end_ind) in chunks {
        execute_with_better_error(
            conn,
            diesel::insert_into(schema::move_resources::table)
                .values(&items_to_insert[start_ind..end_ind])
                .on_conflict((transaction_version, write_set_change_index))
                .do_update()
                .set((
                    inserted_at.eq(excluded(inserted_at)),
                    state_key_hash.eq(excluded(state_key_hash)),
                )),
            None,
        )?;
    }
    Ok(())
```

**File:** crates/aptos-localnet/src/hasura_metadata.json (L545-568)
```json
            "table": {
              "name": "move_resources",
              "schema": "legacy_migration_v1"
            },
            "configuration": {
              "column_config": {},
              "custom_column_names": {},
              "custom_name": "move_resources",
              "custom_root_fields": {}
            },
            "select_permissions": [
              {
                "role": "anonymous",
                "permission": {
                  "columns": [
                    "address",
                    "transaction_version"
                  ],
                  "filter": {},
                  "limit": 100,
                  "allow_aggregations": true
                }
              }
            ]
```

**File:** crates/indexer/README.md (L1-7)
```markdown
# Aptos Indexer

> Tails the blockchain's transactions and pushes them into a postgres DB

A fullnode can run an indexer with the proper configs. If enabled, the indexer will tail
transactions in the fullnode with business logic from  each registered `TransactionProcessor`. On
startup, by default, will restart from the first gap (e.g. version 5 if versions succeeded are 0, 1, 2, 3, 4, 6). 
```

**File:** api/src/context.rs (L477-496)
```rust
        let account_iter = if !db_sharding_enabled(&self.node_config) {
            Box::new(
                self.db
                    .get_prefixed_state_value_iterator(
                        &StateKeyPrefix::from(address),
                        prev_state_key,
                        version,
                    )?
                    .map(|item| item.map_err(|err| anyhow!(err.to_string()))),
            )
        } else {
            self.indexer_reader
                .as_ref()
                .ok_or_else(|| format_err!("Indexer reader doesn't exist"))?
                .get_prefixed_state_value_iterator(
                    &StateKeyPrefix::from(address),
                    prev_state_key,
                    version,
                )?
        };
```

**File:** storage/aptosdb/src/db_debugger/validation.rs (L114-146)
```rust
pub fn verify_state_kvs(
    db_root_path: &Path,
    internal_db: &DB,
    target_ledger_version: u64,
) -> Result<()> {
    println!("Validating db statekeys");
    let storage_dir = StorageDirPaths::from_path(db_root_path);
    let state_kv_db =
        StateKvDb::open_sharded(&storage_dir, RocksdbConfig::default(), None, None, false)?;

    //read all statekeys from internal db and store them in mem
    let mut all_internal_keys = HashSet::new();
    let mut iter = internal_db.iter::<StateKeysSchema>()?;
    iter.seek_to_first();
    for (key_ind, state_key_res) in iter.enumerate() {
        let state_key = state_key_res?.0;
        let state_key_hash = state_key.hash();
        all_internal_keys.insert(state_key_hash);
        if key_ind % 10_000_000 == 0 {
            println!("Processed {} keys", key_ind);
        }
    }
    println!(
        "Number of state keys in internal db: {}",
        all_internal_keys.len()
    );
    for shard_id in 0..16 {
        let shard = state_kv_db.db_shard(shard_id);
        println!("Validating state_kv for shard {}", shard_id);
        verify_state_kv(shard, &all_internal_keys, target_ledger_version)?;
    }
    Ok(())
}
```
