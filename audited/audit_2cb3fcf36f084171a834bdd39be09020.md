# Audit Report

## Title
Mempool Peer Health Check Vulnerability: Future Timestamps Incorrectly Marked as Healthy

## Summary
The `check_peer_metadata_health` function in the mempool peer prioritization logic incorrectly handles future timestamps due to the use of `saturating_sub`. A malicious peer can send a `NodeInformationResponse` with `ledger_timestamp_usecs` set to `u64::MAX` or any future timestamp, causing it to be permanently marked as "healthy" and prioritized for transaction broadcasting, enabling transaction censorship and mempool DoS attacks.

## Finding Description
The vulnerability exists in the mempool peer health checking mechanism that determines which peers should receive transaction broadcasts. [1](#0-0) 

The critical flaw is on line 584, where the code calculates the sync lag using `saturating_sub`: [2](#0-1) 

When a malicious peer reports a `ledger_timestamp_usecs` value that is **greater than** the current timestamp (i.e., a future timestamp), the subtraction saturates to `0`, causing the check `0 < max_sync_lag_usecs` to always return `true`, marking the peer as healthy.

**Attack Flow:**

1. Attacker operates a malicious peer node that connects to honest Aptos nodes
2. When responding to `GetNodeInformation` requests from the peer monitoring client, the attacker crafts a response: [3](#0-2) 

3. The attacker sets `ledger_timestamp_usecs = u64::MAX` (or any future timestamp)
4. The client receives this response without validation: [4](#0-3) 

5. The metadata is extracted and used for peer prioritization: [5](#0-4) 

6. During intelligent peer prioritization, health is checked **first** before other criteria: [6](#0-5) 

7. The malicious peer is prioritized over genuinely healthy peers and receives preferential transaction broadcasts, allowing it to:
   - Drop transactions (DoS)
   - Selectively forward transactions (censorship)
   - Monitor transaction patterns (privacy leak for MEV)

**Root Cause:** The lack of edge case testing revealed by the security question masks this production vulnerability. The test only covers normal sync lag scenarios: [7](#0-6) 

The test helper only creates timestamps in the **past**: [8](#0-7) 

No tests verify future timestamps or `u64::MAX` values, allowing this bug to persist.

## Impact Explanation
This vulnerability meets **Medium Severity** criteria per the Aptos bug bounty program:

- **Transaction Censorship**: Malicious peers can selectively drop or delay transactions, affecting mempool integrity
- **Mempool DoS**: Multiple malicious peers can monopolize transaction broadcasts, degrading network performance
- **Privacy Leaks**: Attackers gain preferential visibility into pending transactions for MEV exploitation
- **State Inconsistencies**: Mempool forwarding patterns become unreliable, requiring operator intervention

While this doesn't directly compromise consensus safety or cause fund loss, it significantly degrades mempool functionality and can enable secondary attacks. The impact is limited to the mempool layer and doesn't affect committed blocks, preventing escalation to Critical severity.

## Likelihood Explanation
**Likelihood: Medium-High**

- **Attacker Requirements**: Only requires running a peer node and establishing connections with honest nodes (no special privileges needed)
- **Complexity**: Trivial to exploit - simply set one field to `u64::MAX` in the response
- **Detection Difficulty**: Hard to detect without specific monitoring, as the malicious peer appears "healthy" to all health checks
- **Scope**: Affects all nodes using intelligent peer prioritization (default configuration)

The attack is practically feasible and requires minimal resources, making it a realistic threat.

## Recommendation
Implement proper validation to reject timestamps that are in the future or unreasonably far from the current time:

```rust
fn check_peer_metadata_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
) -> bool {
    monitoring_metadata
        .and_then(|metadata| {
            metadata
                .latest_node_info_response
                .as_ref()
                .map(|node_information_response| {
                    let peer_ledger_timestamp_usecs =
                        node_information_response.ledger_timestamp_usecs;
                    let current_timestamp_usecs = get_timestamp_now_usecs(time_service);
                    
                    // Reject future timestamps or unreasonable values
                    if peer_ledger_timestamp_usecs > current_timestamp_usecs {
                        return false; // Future timestamps are invalid
                    }

                    let max_sync_lag_secs =
                        mempool_config.max_sync_lag_before_unhealthy_secs as u64;
                    let max_sync_lag_usecs = max_sync_lag_secs * MICROS_PER_SECOND;

                    // Safe subtraction now that we know peer timestamp <= current
                    current_timestamp_usecs - peer_ledger_timestamp_usecs < max_sync_lag_usecs
                })
        })
        .unwrap_or(false)
}
```

Additionally, add validation tests: [9](#0-8) 

Add edge case tests for `u64::MAX`, future timestamps, and `Duration::MAX` values to prevent regression.

## Proof of Concept

```rust
#[test]
fn test_future_timestamp_marked_unhealthy() {
    use std::time::Duration;
    
    // Create mempool config with 10 second sync lag tolerance
    let mempool_config = MempoolConfig {
        max_sync_lag_before_unhealthy_secs: 10,
        ..MempoolConfig::default()
    };
    
    let time_service = TimeService::mock();
    
    // Create metadata with future timestamp (u64::MAX)
    let malicious_node_info = NodeInformationResponse {
        ledger_timestamp_usecs: u64::MAX, // Future timestamp
        ..Default::default()
    };
    let malicious_metadata = PeerMonitoringMetadata {
        latest_node_info_response: Some(malicious_node_info),
        ..Default::default()
    };
    
    // BUG: This returns true (healthy) but should return false (unhealthy)
    let is_healthy = check_peer_metadata_health(
        &mempool_config,
        &time_service,
        &Some(&malicious_metadata)
    );
    
    // This assertion FAILS in current code, demonstrating the vulnerability
    assert!(!is_healthy, "Peer with future timestamp should be marked unhealthy");
    
    // Also test near-future timestamp
    let current_time_usecs = get_timestamp_now_usecs(&time_service);
    let future_timestamp = current_time_usecs + 1_000_000; // 1 second in future
    
    let near_future_node_info = NodeInformationResponse {
        ledger_timestamp_usecs: future_timestamp,
        ..Default::default()
    };
    let near_future_metadata = PeerMonitoringMetadata {
        latest_node_info_response: Some(near_future_node_info),
        ..Default::default()
    };
    
    let is_healthy = check_peer_metadata_health(
        &mempool_config,
        &time_service,
        &Some(&near_future_metadata)
    );
    
    // This also FAILS, proving any future timestamp bypasses health checks
    assert!(!is_healthy, "Peer with near-future timestamp should be marked unhealthy");
}
```

## Notes
This vulnerability directly relates to the security question about edge case testing. The test in `test_verify_node_info_state` only covers normal values and doesn't test `Duration::MAX`, `u64::MAX`, or future timestamps. This gap in test coverage allowed a production vulnerability to exist in the mempool peer prioritization logic. While response type mismatches are properly handled, extreme numeric values are not validated, enabling this attack vector.

### Citations

**File:** mempool/src/shared_mempool/priority.rs (L83-92)
```rust
        // First, compare the peers by health (e.g., sync lag)
        let unhealthy_ordering = compare_peer_health(
            &self.mempool_config,
            &self.time_service,
            monitoring_metadata_a,
            monitoring_metadata_b,
        );
        if !unhealthy_ordering.is_eq() {
            return unhealthy_ordering; // Only return if it's not equal
        }
```

**File:** mempool/src/shared_mempool/priority.rs (L562-589)
```rust
fn check_peer_metadata_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
) -> bool {
    monitoring_metadata
        .and_then(|metadata| {
            metadata
                .latest_node_info_response
                .as_ref()
                .map(|node_information_response| {
                    // Get the peer's ledger timestamp and the current timestamp
                    let peer_ledger_timestamp_usecs =
                        node_information_response.ledger_timestamp_usecs;
                    let current_timestamp_usecs = get_timestamp_now_usecs(time_service);

                    // Calculate the max sync lag before the peer is considered unhealthy (in microseconds)
                    let max_sync_lag_secs =
                        mempool_config.max_sync_lag_before_unhealthy_secs as u64;
                    let max_sync_lag_usecs = max_sync_lag_secs * MICROS_PER_SECOND;

                    // Determine if the peer is healthy
                    current_timestamp_usecs.saturating_sub(peer_ledger_timestamp_usecs)
                        < max_sync_lag_usecs
                })
        })
        .unwrap_or(false) // If metadata is missing, consider the peer unhealthy
}
```

**File:** mempool/src/shared_mempool/priority.rs (L687-729)
```rust
    #[test]
    fn test_check_peer_metadata_health() {
        // Create a mempool config with a max sync lag of 10 seconds
        let mempool_config = MempoolConfig {
            max_sync_lag_before_unhealthy_secs: 10,
            ..MempoolConfig::default()
        };

        // Create a mock time service for testing
        let time_service = TimeService::mock();

        // Create monitoring metadata with no sync lag (healthy)
        let monitoring_metadata = create_metadata_with_sync_lag(&time_service, 0);

        // Verify the peer is healthy
        let is_peer_healthy =
            check_peer_metadata_health(&mempool_config, &time_service, &Some(&monitoring_metadata));
        assert!(is_peer_healthy);

        // Elapse some time, but not enough to make the peer unhealthy
        time_service.clone().into_mock().advance_secs(5);

        // Verify the peer is still healthy
        let is_peer_healthy =
            check_peer_metadata_health(&mempool_config, &time_service, &Some(&monitoring_metadata));
        assert!(is_peer_healthy);

        // Elapse some more time, but not enough to make the peer unhealthy
        time_service.clone().into_mock().advance_secs(4);

        // Verify the peer is still healthy
        let is_peer_healthy =
            check_peer_metadata_health(&mempool_config, &time_service, &Some(&monitoring_metadata));
        assert!(is_peer_healthy);

        // Elapse some more time to make the peer unhealthy
        time_service.clone().into_mock().advance_secs(1);

        // Verify the peer is now unhealthy
        let is_peer_healthy =
            check_peer_metadata_health(&mempool_config, &time_service, &Some(&monitoring_metadata));
        assert!(!is_peer_healthy);
    }
```

**File:** mempool/src/shared_mempool/priority.rs (L1529-1532)
```rust
    fn get_timestamp_in_past_usecs(time_service: &TimeService, secs_in_past: usize) -> u64 {
        let now_usecs = get_timestamp_now_usecs(time_service);
        now_usecs - ((secs_in_past as u64) * MICROS_PER_SECOND)
    }
```

**File:** peer-monitoring-service/types/src/response.rs (L93-102)
```rust
/// A response for the node information request
#[derive(Clone, Debug, Default, Deserialize, Eq, PartialEq, Serialize)]
pub struct NodeInformationResponse {
    pub build_information: BTreeMap<String, String>, // The build information of the node
    pub highest_synced_epoch: u64,                   // The highest synced epoch of the node
    pub highest_synced_version: u64,                 // The highest synced version of the node
    pub ledger_timestamp_usecs: u64, // The latest timestamp of the blockchain (in microseconds)
    pub lowest_available_version: u64, // The lowest stored version of the node (in storage)
    pub uptime: Duration,            // The amount of time the peer has been running
}
```

**File:** peer-monitoring-service/client/src/peer_states/node_info.rs (L79-106)
```rust
    fn handle_monitoring_service_response(
        &mut self,
        peer_network_id: &PeerNetworkId,
        _peer_metadata: PeerMetadata,
        _monitoring_service_request: PeerMonitoringServiceRequest,
        monitoring_service_response: PeerMonitoringServiceResponse,
        _response_time_secs: f64,
    ) {
        // Verify the response type is valid
        let node_info_response = match monitoring_service_response {
            PeerMonitoringServiceResponse::NodeInformation(node_information_response) => {
                node_information_response
            },
            _ => {
                warn!(LogSchema::new(LogEntry::NodeInfoRequest)
                    .event(LogEvent::ResponseError)
                    .peer(peer_network_id)
                    .message(
                        "An unexpected response was received instead of a node info response!"
                    ));
                self.handle_request_failure();
                return;
            },
        };

        // Store the new latency ping result
        self.record_node_info_response(node_info_response);
    }
```

**File:** peer-monitoring-service/client/src/peer_states/node_info.rs (L168-207)
```rust
    #[test]
    fn test_verify_node_info_state() {
        // Create the node info state
        let node_monitoring_config = NodeMonitoringConfig::default();
        let time_service = TimeService::mock();
        let mut node_info_state = NodeInfoState::new(node_monitoring_config, time_service);

        // Verify the initial node info state
        verify_empty_node_response(&node_info_state);

        // Handle several valid node info responses and verify the state
        for i in 0..10 {
            // Generate the test data
            let build_information = aptos_build_info::get_build_information();
            let highest_synced_epoch = i;
            let highest_synced_version = (i + 1) * 100;
            let ledger_timestamp_usecs = (i + 1) * 200;
            let lowest_available_version = highest_synced_version - 10;
            let uptime = Duration::from_millis(i * 999);

            // Create the service response
            let node_information_response = NodeInformationResponse {
                build_information,
                highest_synced_epoch,
                highest_synced_version,
                ledger_timestamp_usecs,
                lowest_available_version,
                uptime,
            };

            // Handle the node info response
            handle_monitoring_service_response(
                &mut node_info_state,
                node_information_response.clone(),
            );

            // Verify the latest node info state
            verify_node_info_state(&node_info_state, node_information_response);
        }
    }
```

**File:** peer-monitoring-service/client/src/peer_states/peer_state.rs (L208-211)
```rust
        // Get and store the latest node info response
        let node_info_state = self.get_node_info_state()?;
        let node_info_response = node_info_state.get_latest_node_info_response();
        peer_monitoring_metadata.latest_node_info_response = node_info_response;
```
