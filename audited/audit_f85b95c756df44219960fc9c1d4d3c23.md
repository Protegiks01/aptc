# Audit Report

## Title
Stale Augmented Data Broadcast State Not Reset During Chain Reorganization

## Summary
When a chain reorganization occurs within an epoch (via `fast_forward_sync`), the `RandManager::process_reset()` method clears the block queue and rand store but fails to abort ongoing augmented data broadcast tasks. These tasks continue running with stale state, consuming validator resources and violating protocol correctness invariants.

## Finding Description

During epoch initialization, `RandManager` spawns asynchronous reliable broadcast tasks to distribute and certify augmented data. [1](#0-0) 

These broadcast tasks maintain mutable state through `AugDataCertBuilder` (accumulating partial signatures) and `CertifiedAugDataAckState` (tracking validator acknowledgments). [2](#0-1) [3](#0-2) 

When a chain reorganization occurs within the same epoch (confirmed possible by epoch validation check), [4](#0-3)  the system calls `reset()` via the execution client. [5](#0-4) 

However, `RandManager::process_reset()` only resets the block queue and rand store, **without aborting the broadcast tasks**. [6](#0-5) 

This is a critical oversight because `BufferManager` correctly handles this scenario by storing its broadcast `DropGuard` as a struct field and explicitly dropping it during reset. [7](#0-6) [8](#0-7) 

In contrast, `RandManager` stores the broadcast `DropGuard` only as a local variable in the `start()` function, making it inaccessible to `process_reset()`. [9](#0-8) 

The broadcast tasks spawned continue their operation, including RPC retries with exponential backoff to all validators, accumulating stale partial signatures, and consuming network/CPU resources despite the chain state having moved to a different fork.

## Impact Explanation

This vulnerability meets **High Severity** criteria per Aptos bug bounty rules: **Validator node slowdowns**.

**Resource Consumption:**
- Stale broadcast tasks continue sending RPC requests to all validators with exponential backoff retries
- Each retry cycle consumes network bandwidth, CPU for signature verification, and memory for state tracking
- Multiple reorgs within an epoch accumulate these zombie broadcast tasks

**Protocol Correctness Violation:**
- Stale `AugDataCertBuilder` continues accumulating signatures with partial state from before the reorg
- The broadcast completion logic operates on outdated validator response tracking
- Creates inconsistency where reset is supposed to clean all round-specific state

**Validator Performance Impact:**
- Continuous RPC traffic to peer validators for obsolete broadcast rounds
- Cryptographic operations (signature aggregation) on stale data
- Memory pressure from retained broadcast state structures

## Likelihood Explanation

**Likelihood: High**

Chain reorganizations within the same epoch occur naturally in distributed consensus systems when:
1. A validator falls behind and receives `SyncInfo` from peers indicating a higher quorum cert on a different fork
2. Network partitions temporarily resolve, exposing different chain views
3. Fast-forward sync triggers during recovery mode

The vulnerability triggers **automatically** without requiring:
- Malicious validator actions
- Privileged access
- Complex attack orchestration

Every invocation of `RecoveryManager::sync_up()` followed by `BlockStore::fast_forward_sync()` within an epoch will trigger this bug. [10](#0-9) 

## Recommendation

Store the augmented data broadcast `DropGuard` as a field in the `RandManager` struct (similar to `BufferManager`), and explicitly abort it during `process_reset()`.

**Recommended fix:**

```rust
// In RandManager struct definition (line 51)
pub struct RandManager<S: TShare, D: TAugmentedData> {
    author: Author,
    epoch_state: Arc<EpochState>,
    stop: bool,
    config: RandConfig,
    reliable_broadcast: Arc<ReliableBroadcast<RandMessage<S, D>, ExponentialBackoff>>,
    network_sender: Arc<NetworkSender>,
    decision_rx: Receiver<Randomness>,
    outgoing_blocks: Sender<OrderedBlocks>,
    rand_store: Arc<Mutex<RandStore<S>>>,
    aug_data_store: AugDataStore<D>,
    block_queue: BlockQueue,
    fast_config: Option<RandConfig>,
    
    // NEW: Store broadcast handle for cleanup during reset
    aug_data_broadcast_handle: Option<DropGuard>,
}

// In process_reset() method (line 184)
fn process_reset(&mut self, request: ResetRequest) {
    let ResetRequest { tx, signal } = request;
    let target_round = match signal {
        ResetSignal::Stop => 0,
        ResetSignal::TargetRound(round) => round,
    };
    self.block_queue = BlockQueue::new();
    self.rand_store.lock().reset(target_round);
    
    // NEW: Abort ongoing augmented data broadcast
    self.aug_data_broadcast_handle.take();
    
    self.stop = matches!(signal, ResetSignal::Stop);
    let _ = tx.send(ResetAck::default());
}

// In start() method (line 376), store the guard
self.aug_data_broadcast_handle = Some(self.broadcast_aug_data().await);
```

## Proof of Concept

**Reproduction Steps:**

1. **Setup:** Deploy Aptos network with validators in epoch N

2. **Trigger Initial Broadcast:** Validators start `RandManager`, spawning augmented data broadcast tasks

3. **Partial Progress:** Allow broadcast to collect 40% of signatures (below quorum threshold)

4. **Trigger Reorg:** 
   - Have validator V1 fall behind on fork A (round 10)
   - Network progresses fork B to round 20
   - V1 receives `SyncInfo` with higher QC from fork B
   - V1 calls `RecoveryManager::sync_up()` triggering `fast_forward_sync()`

5. **Observe Bug:**
   - V1's `process_reset()` clears block_queue and rand_store
   - BUT augmented data broadcast task continues running
   - Monitor network traffic: V1 continues sending `AugData` RPC requests to peers
   - Monitor memory: `AugDataCertBuilder` retains partial signature state

6. **Verify Impact:**
   - Repeat reorg multiple times within epoch
   - Observe accumulated RPC traffic and memory growth
   - Measure validator CPU usage on signature operations for stale broadcasts

**Expected Behavior:** `process_reset()` should abort broadcast tasks, immediately stopping RPC retries and releasing state.

**Actual Behavior:** Broadcast tasks continue indefinitely until epoch end, consuming resources and maintaining stale state.

### Citations

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L51-70)
```rust
pub struct RandManager<S: TShare, D: TAugmentedData> {
    author: Author,
    epoch_state: Arc<EpochState>,
    stop: bool,
    config: RandConfig,
    reliable_broadcast: Arc<ReliableBroadcast<RandMessage<S, D>, ExponentialBackoff>>,
    network_sender: Arc<NetworkSender>,

    // local channel received from rand_store
    decision_rx: Receiver<Randomness>,
    // downstream channels
    outgoing_blocks: Sender<OrderedBlocks>,
    // local state
    rand_store: Arc<Mutex<RandStore<S>>>,
    aug_data_store: AugDataStore<D>,
    block_queue: BlockQueue,

    // for randomness fast path
    fast_config: Option<RandConfig>,
}
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L184-194)
```rust
    fn process_reset(&mut self, request: ResetRequest) {
        let ResetRequest { tx, signal } = request;
        let target_round = match signal {
            ResetSignal::Stop => 0,
            ResetSignal::TargetRound(round) => round,
        };
        self.block_queue = BlockQueue::new();
        self.rand_store.lock().reset(target_round);
        self.stop = matches!(signal, ResetSignal::Stop);
        let _ = tx.send(ResetAck::default());
    }
```

**File:** consensus/src/rand/rand_gen/rand_manager.rs (L376-376)
```rust
        let _guard = self.broadcast_aug_data().await;
```

**File:** consensus/src/rand/rand_gen/reliable_broadcast_state.rs (L25-29)
```rust
pub struct AugDataCertBuilder<D> {
    epoch_state: Arc<EpochState>,
    aug_data: AugData<D>,
    partial_signatures: Mutex<PartialSignatures>,
}
```

**File:** consensus/src/rand/rand_gen/reliable_broadcast_state.rs (L69-71)
```rust
pub struct CertifiedAugDataAckState {
    validators: Mutex<HashSet<Author>>,
}
```

**File:** consensus/src/recovery_manager.rs (L84-118)
```rust
    pub async fn sync_up(&mut self, sync_info: &SyncInfo, peer: Author) -> Result<RecoveryData> {
        sync_info.verify(&self.epoch_state.verifier)?;
        ensure!(
            sync_info.highest_round() > self.last_committed_round,
            "[RecoveryManager] Received sync info has lower round number than committed block"
        );
        ensure!(
            sync_info.epoch() == self.epoch_state.epoch,
            "[RecoveryManager] Received sync info is in different epoch than committed block"
        );
        let mut retriever = BlockRetriever::new(
            self.network.clone(),
            peer,
            self.epoch_state
                .verifier
                .get_ordered_account_addresses_iter()
                .collect(),
            self.max_blocks_to_request,
            self.pending_blocks.clone(),
        );
        let recovery_data = BlockStore::fast_forward_sync(
            sync_info.highest_quorum_cert(),
            sync_info.highest_commit_cert(),
            &mut retriever,
            self.storage.clone(),
            self.execution_client.clone(),
            self.payload_manager.clone(),
            self.order_vote_enabled,
            self.window_size,
            None,
        )
        .await?;

        Ok(recovery_data)
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L124-124)
```rust
    commit_proof_rb_handle: Option<DropGuard>,
```

**File:** consensus/src/pipeline/buffer_manager.rs (L563-563)
```rust
        self.commit_proof_rb_handle.take();
```
