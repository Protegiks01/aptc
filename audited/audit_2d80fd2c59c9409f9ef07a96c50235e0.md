# Audit Report

## Title
Missing Timeout Hierarchy Validation Causes State Sync Livelock and Node Synchronization Failure

## Summary
The `StateSyncConfig` and its sub-configurations lack validation to ensure timeout values follow the required hierarchy (client < server, initial < max). This allows node operators to misconfigure timeout values, causing the state synchronization system to enter a livelock where data streams continuously fail and reset without making forward progress, rendering the node unable to synchronize with the blockchain.

## Finding Description

The state sync configuration system defines multiple timeout hierarchies across three configuration structs without enforcing proper ordering constraints:

**Client-side timeouts** (in `AptosDataClientConfig`): [1](#0-0) 

**Server-side timeouts** (in `StorageServiceConfig`): [2](#0-1) 

**The critical flaw**: There is NO `ConfigSanitizer` implementation for `AptosDataClientConfig` or `StorageServiceConfig` to validate these timeout relationships. The existing `StateSyncConfig` sanitizer only validates auto-bootstrapping settings: [3](#0-2) 

**Exploitation Path:**

1. **Broken Exponential Backoff**: If `response_timeout_ms > max_response_timeout_ms` (e.g., 60000 > 10000), the exponential backoff logic breaks: [4](#0-3) 

The `min()` function will always return `max_response_timeout_ms`, defeating exponential backoff entirely. All retries use the same (too short) timeout, causing rapid retry exhaustion.

2. **Zero or Near-Zero Timeouts**: If `response_timeout_ms = 0`, all requests immediately timeout. After `max_request_retry` failures, the stream terminates: [5](#0-4) 

3. **Livelock via Stream Reset**: After `max_num_stream_timeouts` consecutive timeouts, a critical timeout occurs: [6](#0-5) 

The stream is reset, and the driver's infinite loop recreates a new stream with the same broken timeout configuration: [7](#0-6) 

4. **Infinite Livelock**: The cycle repeats indefinitely: create stream → timeout repeatedly → terminate → recreate stream. The node never successfully synchronizes data.

**Example Misconfiguration:**
```yaml
state_sync:
  aptos_data_client:
    response_timeout_ms: 100000  # Initial timeout larger than max!
    max_response_timeout_ms: 1000
```

Or worse:
```yaml
state_sync:
  aptos_data_client:
    response_timeout_ms: 0  # Immediate timeouts
    max_response_timeout_ms: 0
```

## Impact Explanation

**Severity: Medium** (up to $10,000 per Aptos bug bounty)

This vulnerability causes **state synchronization liveness failure** requiring manual intervention:

- **Node Cannot Sync**: The affected node enters a livelock, continuously creating and destroying data streams without successfully fetching blockchain data
- **Network Degradation**: The node wastes network bandwidth and peer resources by sending requests it will immediately timeout
- **Validator Impact**: If a validator node is misconfigured, it falls behind and cannot participate in consensus (though other validators are unaffected)
- **Requires Manual Fix**: The node operator must identify the misconfiguration, correct the timeout values, and restart the node

This fits the **Medium severity** category: "State inconsistencies requiring intervention" - the node cannot maintain consistent state with the network and requires operator intervention to resolve.

The issue does NOT constitute:
- **Critical**: No consensus safety violation (doesn't affect other nodes), no fund loss
- **High**: No API crash (the loop runs indefinitely but doesn't crash)

## Likelihood Explanation

**Likelihood: Medium**

While this requires manual misconfiguration by a node operator, several factors increase the likelihood:

1. **No Validation Feedback**: The configuration loads successfully with no warnings or errors about timeout hierarchy violations
2. **Complex Timeout Relationships**: Operators must understand the relationship between 6+ different timeout fields across 3 configuration structs
3. **Default Values Can Be Overridden**: Any YAML config file can override defaults without validation
4. **Copy-Paste Errors**: Operators copying configurations between environments might inadvertently introduce invalid values
5. **Testing Mistakes**: Developers testing with aggressive timeouts (e.g., 0ms for fast failures) might accidentally deploy these to production

The vulnerability is **not** exploitable by external attackers (requires node operator access), but it's a realistic operational risk.

## Recommendation

Implement `ConfigSanitizer` for `AptosDataClientConfig` and `StorageServiceConfig` to validate timeout hierarchies:

```rust
impl ConfigSanitizer for AptosDataClientConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let aptos_data_client_config = &node_config.state_sync.aptos_data_client;

        // Validate initial < max for exponential backoff
        if aptos_data_client_config.response_timeout_ms 
            > aptos_data_client_config.max_response_timeout_ms {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                format!(
                    "response_timeout_ms ({}) must be <= max_response_timeout_ms ({})",
                    aptos_data_client_config.response_timeout_ms,
                    aptos_data_client_config.max_response_timeout_ms
                ),
            ));
        }

        // Validate all timeouts are positive
        if aptos_data_client_config.response_timeout_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "response_timeout_ms must be greater than 0".to_string(),
            ));
        }

        if aptos_data_client_config.max_response_timeout_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "max_response_timeout_ms must be greater than 0".to_string(),
            ));
        }

        if aptos_data_client_config.optimistic_fetch_timeout_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "optimistic_fetch_timeout_ms must be greater than 0".to_string(),
            ));
        }

        if aptos_data_client_config.subscription_response_timeout_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "subscription_response_timeout_ms must be greater than 0".to_string(),
            ));
        }

        Ok(())
    }
}

impl ConfigSanitizer for StorageServiceConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let storage_service_config = &node_config.state_sync.storage_service;

        // Validate server timeouts are positive
        if storage_service_config.max_storage_read_wait_time_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "max_storage_read_wait_time_ms must be greater than 0".to_string(),
            ));
        }

        if storage_service_config.max_optimistic_fetch_period_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "max_optimistic_fetch_period_ms must be greater than 0".to_string(),
            ));
        }

        if storage_service_config.max_subscription_period_ms == 0 {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "max_subscription_period_ms must be greater than 0".to_string(),
            ));
        }

        Ok(())
    }
}
```

Then update `StateSyncConfig::sanitize()` to call these validators:

```rust
impl ConfigSanitizer for StateSyncConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        StateSyncDriverConfig::sanitize(node_config, node_type, chain_id)?;
        AptosDataClientConfig::sanitize(node_config, node_type, chain_id)?;
        StorageServiceConfig::sanitize(node_config, node_type, chain_id)?;
        Ok(())
    }
}
```

## Proof of Concept

Create a configuration file `broken_timeout_config.yaml`:

```yaml
base:
  role: "full_node"
  
state_sync:
  aptos_data_client:
    response_timeout_ms: 0
    max_response_timeout_ms: 0
  
  storage_service:
    max_storage_read_wait_time_ms: 0
```

Run the node with this configuration:

```bash
cargo build --release
./target/release/aptos-node -f broken_timeout_config.yaml
```

**Expected Behavior (without fix):**
- Node starts successfully (no validation error)
- State sync enters infinite livelock:
  - Data requests timeout immediately (0ms timeout)
  - Stream terminates after 5 retries
  - New stream created
  - Cycle repeats indefinitely
- Node logs show continuous stream creation/termination
- Node never successfully syncs blockchain data

**Expected Behavior (with fix):**
- Node fails to start with clear error message:
```
Error: ConfigSanitizerFailed("AptosDataClientConfigSanitizer", "response_timeout_ms must be greater than 0")
```

This forces the operator to fix the configuration before the node can start, preventing the livelock scenario.

**Notes:**
- The vulnerability is a **missing input validation** issue rather than a logic bug in the timeout handling code itself
- The timeout handling code works correctly when given valid values, but has no defense against invalid configurations
- This is a realistic operational risk that could affect mainnet/testnet nodes if operators make configuration errors
- The fix is straightforward: add validation during configuration loading to fail fast with clear error messages

### Citations

**File:** config/src/config/state_sync_config.rs (L179-180)
```rust
    /// Maximum time (ms) to wait for storage before truncating a response
    pub max_storage_read_wait_time_ms: u64,
```

**File:** config/src/config/state_sync_config.rs (L452-453)
```rust
    /// First timeout (in ms) when waiting for a response
    pub response_timeout_ms: u64,
```

**File:** config/src/config/state_sync_config.rs (L487-520)
```rust
impl ConfigSanitizer for StateSyncConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        // Sanitize the state sync driver config
        StateSyncDriverConfig::sanitize(node_config, node_type, chain_id)
    }
}

impl ConfigSanitizer for StateSyncDriverConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let state_sync_driver_config = &node_config.state_sync.state_sync_driver;

        // Verify that auto-bootstrapping is not enabled for
        // nodes that are fast syncing.
        let fast_sync_enabled = state_sync_driver_config.bootstrapping_mode.is_fast_sync();
        if state_sync_driver_config.enable_auto_bootstrapping && fast_sync_enabled {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                "Auto-bootstrapping should not be enabled for nodes that are fast syncing!"
                    .to_string(),
            ));
        }

        Ok(())
    }
}
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L351-359)
```rust
            let response_timeout_ms = self.data_client_config.response_timeout_ms;
            let max_response_timeout_ms = self.data_client_config.max_response_timeout_ms;

            // Exponentially increase the timeout based on the number of
            // previous failures (but bounded by the max timeout).
            let request_timeout_ms = min(
                max_response_timeout_ms,
                response_timeout_ms * (u32::pow(2, self.request_failure_count as u32) as u64),
            );
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L446-454)
```rust
        if self.stream_engine.is_stream_complete()
            || self.request_failure_count >= self.streaming_service_config.max_request_retry
            || self.send_failure
        {
            if !self.send_failure && self.stream_end_notification_id.is_none() {
                self.send_end_of_stream_notification().await?;
            }
            return Ok(()); // There's nothing left to do
        }
```

**File:** state-sync/state-sync-driver/src/utils.rs (L222-236)
```rust
        // Increase the number of consecutive timeouts for the data stream
        active_data_stream.num_consecutive_timeouts += 1;

        // Check if we've timed out too many times
        if active_data_stream.num_consecutive_timeouts >= max_num_stream_timeouts {
            Err(Error::CriticalDataStreamTimeout(format!(
                "{:?}",
                max_num_stream_timeouts
            )))
        } else {
            Err(Error::DataStreamNotificationTimeout(format!(
                "{:?}",
                timeout_ms
            )))
        }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L221-239)
```rust
        loop {
            ::futures::select! {
                notification = self.client_notification_listener.select_next_some() => {
                    self.handle_client_notification(notification).await;
                },
                notification = self.commit_notification_listener.select_next_some() => {
                    self.handle_snapshot_commit_notification(notification).await;
                }
                notification = self.consensus_notification_handler.select_next_some() => {
                    self.handle_consensus_or_observer_notification(notification).await;
                }
                notification = self.error_notification_listener.select_next_some() => {
                    self.handle_error_notification(notification).await;
                }
                _ = progress_check_interval.select_next_some() => {
                    self.drive_progress().await;
                }
            }
        }
```
