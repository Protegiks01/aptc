# Audit Report

## Title
Configuration Heterogeneity Risk: Insufficient Enforcement of Deterministic Execution Settings Across Validators

## Summary
The `ExecutionConfig::default()` provides local per-node configuration settings for `discard_failed_blocks` and `blockstm_v2_enabled` that are not enforced to be uniform across validators. While the defaults themselves are secure (`false` for both), the lack of sanitizer enforcement allows validators to run with different configurations, potentially violating the "Deterministic Execution" invariant when execution errors occur. [1](#0-0) 

## Finding Description

The execution configuration contains two critical settings that affect block execution behavior:

1. **`discard_failed_blocks`** (defaults to `false`): Controls whether block execution errors cause the validator to discard all transactions or propagate the error. [2](#0-1) 

2. **`blockstm_v2_enabled`** (defaults to `false`): Controls which parallel execution algorithm is used (BlockSTM v1 vs v2). [3](#0-2) 

These settings are part of `BlockExecutorLocalConfig`, not `BlockExecutorConfigFromOnchain`. The on-chain config is explicitly documented as "required to be the same across all nodes," but local configs have no such enforcement: [4](#0-3) 

When `discard_failed_blocks=true`, the block executor silently discards all transactions upon execution failure instead of propagating errors: [5](#0-4) 

The configuration is set globally during node initialization from the local config file: [6](#0-5) 

The `ConfigSanitizer` for `ExecutionConfig` only validates `paranoid_type_verification` and `paranoid_hot_potato_verification` for mainnet nodes, but does NOT enforce uniformity of `discard_failed_blocks` or `blockstm_v2_enabled`: [7](#0-6) 

**Attack Scenario:**

While not directly exploitable by an external attacker, configuration heterogeneity creates consensus divergence risk:

1. Different validator operators independently configure their nodes with different `discard_failed_blocks` values
2. A block is proposed that triggers an execution error (e.g., BlockSTM invariant violation, VM bug)
3. Validators with `discard_failed_blocks=false` fail execution and cannot vote on the block
4. Validators with `discard_failed_blocks=true` accept the block with all transactions discarded and vote
5. This causes either liveness failure (insufficient votes) or potential safety violation if the network splits on which blocks are valid

## Impact Explanation

**Severity: Medium**

This issue represents a **state inconsistency requiring intervention** rather than direct exploitation. The impact includes:

1. **Liveness Risk**: If validators are split on configuration, blocks encountering execution errors may fail to achieve 2f+1 consensus votes, halting the chain
2. **Operational Complexity**: Network operators must coordinate configuration changes across all validators without formal enforcement
3. **Upgrade Fragility**: Rolling out new execution features (like BlockSTM v2) requires careful coordination to avoid consensus splits

However, this does NOT reach Critical severity because:
- It requires operational misconfiguration, not malicious action
- The defaults are actually secure (fail-closed)
- No direct path for external attackers to exploit
- Recovery is possible through operator coordination

## Likelihood Explanation

**Likelihood: Low-Medium**

The likelihood depends on operational practices:

**Low Likelihood in Production:**
- Mainnet validators typically use recommended configurations
- Major node operators coordinate configuration changes
- The defaults are secure and consistently applied

**Medium Likelihood During:**
- Network upgrades when new features are rolled out
- Validator onboarding when new operators join
- Testing/debugging scenarios where operators modify configs
- Cross-network deployments (mainnet vs testnet) with different configs

The smoke test demonstrates this feature is intentionally designed for error recovery scenarios: [8](#0-7) 

## Recommendation

**Add sanitizer enforcement for execution-critical local configurations on mainnet:**

```rust
impl ConfigSanitizer for ExecutionConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let execution_config = &node_config.execution;

        if let Some(chain_id) = chain_id {
            if chain_id.is_mainnet() {
                // Existing checks
                if !execution_config.paranoid_hot_potato_verification {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "paranoid_hot_potato_verification must be enabled for mainnet nodes!".into(),
                    ));
                }
                if !execution_config.paranoid_type_verification {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "paranoid_type_verification must be enabled for mainnet nodes!".into(),
                    ));
                }
                
                // NEW: Enforce deterministic execution settings
                if execution_config.discard_failed_blocks {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "discard_failed_blocks must be disabled (false) for mainnet validators to ensure deterministic consensus behavior!".into(),
                    ));
                }
                
                // Optionally enforce BlockSTM version consistency
                // This could be toggled via feature flag during coordinated upgrades
            }
        }

        Ok(())
    }
}
```

**Alternative/Additional Measures:**
1. Document required configuration values in validator setup guides
2. Add runtime checks that warn if local configs differ from network majority
3. Consider moving critical execution settings to on-chain configuration for automatic synchronization
4. Add metrics/monitoring to detect configuration drift across validators

## Proof of Concept

This is an operational configuration issue rather than a code exploit. A demonstration would require:

```rust
// Hypothetical test showing divergence (not runnable exploit)
#[test]
fn test_config_heterogeneity_consensus_risk() {
    // Create validator A with discard_failed_blocks = false
    let mut config_a = ExecutionConfig::default();
    assert_eq!(config_a.discard_failed_blocks, false); // Secure default
    
    // Validator B operator modifies config to discard_failed_blocks = true
    let mut config_b = ExecutionConfig::default();
    config_b.discard_failed_blocks = true; // Misconfiguration
    
    // When a block with execution error is proposed:
    // - Validator A: Execution fails, cannot vote
    // - Validator B: Execution succeeds with discarded txns, can vote
    // - Result: Potential consensus divergence or liveness failure
    
    // Sanitizer SHOULD reject this for mainnet but currently doesn't
}
```

---

**Notes:**

The defaults themselves (`discard_failed_blocks=false`, `blockstm_v2_enabled=false`) ARE secure-by-default. The vulnerability is the **lack of enforcement** that all validators maintain these secure defaults in production. The design intentionally separates local and on-chain configs for operational flexibility, but critical consensus-affecting settings should be validated for uniformity on mainnet.

### Citations

**File:** config/src/config/execution_config.rs (L45-46)
```rust
    /// Enabled discarding blocks that fail execution due to BlockSTM/VM issue.
    pub discard_failed_blocks: bool,
```

**File:** config/src/config/execution_config.rs (L53-54)
```rust
    /// Whether to use BlockSTMv2 for parallel execution.
    pub blockstm_v2_enabled: bool,
```

**File:** config/src/config/execution_config.rs (L78-96)
```rust
impl Default for ExecutionConfig {
    fn default() -> ExecutionConfig {
        ExecutionConfig {
            genesis: None,
            genesis_file_location: PathBuf::new(),
            // use min of (num of cores/2, DEFAULT_CONCURRENCY_LEVEL) as default concurrency level
            concurrency_level: 0,
            num_proof_reading_threads: 32,
            paranoid_type_verification: true,
            paranoid_hot_potato_verification: true,
            discard_failed_blocks: false,
            processed_transactions_detailed_counters: false,
            genesis_waypoint: None,
            blockstm_v2_enabled: false,
            layout_caches_enabled: true,
            // TODO: consider setting to be true by default.
            async_runtime_checks: false,
        }
    }
```

**File:** config/src/config/execution_config.rs (L157-187)
```rust
impl ConfigSanitizer for ExecutionConfig {
    fn sanitize(
        node_config: &NodeConfig,
        _node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();
        let execution_config = &node_config.execution;

        // If this is a mainnet node, ensure that additional verifiers are enabled
        if let Some(chain_id) = chain_id {
            if chain_id.is_mainnet() {
                if !execution_config.paranoid_hot_potato_verification {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "paranoid_hot_potato_verification must be enabled for mainnet nodes!"
                            .into(),
                    ));
                }
                if !execution_config.paranoid_type_verification {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "paranoid_type_verification must be enabled for mainnet nodes!".into(),
                    ));
                }
            }
        }

        Ok(())
    }
}
```

**File:** types/src/block_executor/config.rs (L51-64)
```rust
/// Local, per-node configuration.
#[derive(Clone, Debug)]
pub struct BlockExecutorLocalConfig {
    // If enabled, uses BlockSTMv2 algorithm / scheduler for parallel execution.
    pub blockstm_v2: bool,
    pub concurrency_level: usize,
    // If specified, parallel execution fallbacks to sequential, if issue occurs.
    // Otherwise, if there is an error in either of the execution, we will panic.
    pub allow_fallback: bool,
    // If true, we will discard the failed blocks and continue with the next block.
    // (allow_fallback needs to be set)
    pub discard_failed_blocks: bool,
    pub module_cache_config: BlockExecutorModuleCacheLocalConfig,
}
```

**File:** aptos-move/block-executor/src/executor.rs (L2648-2663)
```rust
        if self.config.local.discard_failed_blocks {
            // We cannot execute block, discard everything (including block metadata and validator transactions)
            // (TODO: maybe we should add fallback here to first try BlockMetadataTransaction alone)
            let error_code = match sequential_error {
                BlockExecutionError::FatalBlockExecutorError(_) => {
                    StatusCode::DELAYED_FIELD_OR_BLOCKSTM_CODE_INVARIANT_ERROR
                },
                BlockExecutionError::FatalVMError(_) => {
                    StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR
                },
            };
            let ret = (0..signature_verified_block.num_txns())
                .map(|_| E::Output::discard_output(error_code))
                .collect();
            return Ok(BlockOutput::new(ret, None));
        }
```

**File:** aptos-node/src/utils.rs (L52-67)
```rust
/// Sets the Aptos VM configuration based on the node configurations
pub fn set_aptos_vm_configurations(node_config: &NodeConfig) {
    set_layout_caches(node_config.execution.layout_caches_enabled);
    set_paranoid_type_checks(node_config.execution.paranoid_type_verification);
    set_async_runtime_checks(node_config.execution.async_runtime_checks);
    let effective_concurrency_level = if node_config.execution.concurrency_level == 0 {
        ((num_cpus::get() / 2) as u16).clamp(1, DEFAULT_EXECUTION_CONCURRENCY_LEVEL)
    } else {
        node_config.execution.concurrency_level
    };
    AptosVM::set_concurrency_level_once(effective_concurrency_level as usize);
    AptosVM::set_discard_failed_blocks(node_config.execution.discard_failed_blocks);
    AptosVM::set_num_proof_reading_threads_once(
        node_config.execution.num_proof_reading_threads as usize,
    );
    AptosVM::set_blockstm_v2_enabled_once(node_config.execution.blockstm_v2_enabled);
```

**File:** testsuite/smoke-test/src/execution.rs (L19-44)
```rust
#[tokio::test]
async fn fallback_test() {
    let swarm = SwarmBuilder::new_local(1)
        .with_init_config(Arc::new(|_, config, _| {
            config.api.failpoints_enabled = true;
            config.execution.discard_failed_blocks = true;
        }))
        .with_aptos()
        .build()
        .await;

    swarm
        .wait_for_all_nodes_to_catchup_to_epoch(2, Duration::from_secs(60))
        .await
        .expect("Epoch 2 taking too long to come!");

    let client = swarm.validators().next().unwrap().rest_client();

    client
        .set_failpoint(
            "aptos_vm::vm_wrapper::execute_transaction".to_string(),
            "100%return".to_string(),
        )
        .await
        .unwrap();

```
