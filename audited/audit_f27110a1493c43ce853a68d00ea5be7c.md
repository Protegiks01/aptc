# Audit Report

## Title
Peer Resurrection Attack: Malicious Peers Can Bypass Invalid Request Tracking by Disconnecting and Reconnecting

## Summary
The `RequestModerator::refresh_unhealthy_peer_states()` function completely removes the unhealthy state tracking for disconnected peers. A malicious peer can exploit this by sending invalid requests until blocked, disconnecting, and reconnecting with a clean slate, effectively bypassing the rate limiting and exponential backoff mechanisms designed to protect validator nodes from abusive peers.

## Finding Description

The storage service implements a request moderation system to track peers sending invalid requests. When a peer on the public network sends too many invalid requests (default: 500), they are marked as "ignored" and blocked from sending further requests for an exponentially increasing duration.

However, the garbage collection logic in `refresh_unhealthy_peer_states()` removes the entire tracking state for disconnected peers: [1](#0-0) 

The `retain()` call removes any peer that is not in the `connected_peers_and_metadata` map. This means when a peer disconnects, their complete `UnhealthyPeerState` is deleted, including:
- `invalid_request_count`: The accumulated count of invalid requests
- `ignore_start_time`: When the blocking started  
- `min_time_to_ignore_secs`: The exponential backoff duration

This function is called periodically every 1 second (default): [2](#0-1) [3](#0-2) 

**Attack Execution Path:**
1. Malicious peer connects and sends 500 invalid requests (e.g., requesting data beyond synced version)
2. Peer's `invalid_request_count` reaches `max_invalid_requests_per_peer` threshold
3. Peer is marked as ignored via `increment_invalid_request_count()`: [4](#0-3) 

4. Peer's requests are now rejected with `TooManyInvalidRequests` error: [5](#0-4) 

5. **Malicious peer disconnects**
6. Within 1 second, `refresh_unhealthy_peer_states()` runs and removes the peer's entire tracking state
7. **Peer reconnects with a clean slate**
8. Peer can immediately send another 500 invalid requests before being blocked again
9. Process repeats indefinitely

The existing test suite confirms this behavior: [6](#0-5) 

This test shows a peer reconnecting and being tracked as unhealthy again from scratch, confirming state is not persisted across disconnections.

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: Each invalid request must be validated against the storage summary, consuming CPU cycles. A coordinated attack with multiple peer connections can cause significant slowdowns: [7](#0-6) 

2. **Significant Protocol Violation**: The peer moderation system is a critical security control designed to protect nodes from abusive peers. Complete bypass of this system constitutes a significant protocol violation.

3. **Resource Exhaustion**: An attacker controlling multiple IP addresses/connections can amplify the attack, causing sustained resource exhaustion on validator and fullnode infrastructure.

4. **Breaks Resource Limits Invariant**: The attack bypasses the intended rate limiting mechanism, violating the invariant that "all operations must respect gas, storage, and computational limits."

**Quantified Impact:**
- Each invalid request triggers validation logic and storage summary checks
- Default threshold is 500 requests per peer before blocking
- With disconnect/reconnect every ~1 second, attacker can sustain 500 invalid requests/second indefinitely
- Multiple connections multiply this impact linearly
- Affects all public-facing validator nodes and fullnodes

## Likelihood Explanation

**Likelihood: HIGH**

The attack is trivially easy to execute:

1. **No Special Privileges Required**: Any peer on the public network can connect to storage service endpoints
2. **Simple Execution**: Requires only basic network programming to disconnect/reconnect
3. **Low Detection**: Each individual burst of 500 requests appears legitimate until threshold is hit
4. **No Cost to Attacker**: Disconnecting and reconnecting has no cost
5. **Default Configuration Vulnerable**: The issue exists in default configuration settings

**Attacker Requirements:**
- Network access to Aptos nodes (public network)
- Basic ability to send storage service requests
- Simple disconnect/reconnect logic
- No stake, validator access, or insider knowledge needed

**Feasibility:**
The attack is fully automated. A simple script can:
1. Connect to a node
2. Send 500 invalid `GetTransactionsWithProof` requests
3. Disconnect
4. Wait 1-2 seconds
5. Reconnect and repeat

## Recommendation

**Fix Option 1: Persist Peer Reputation State (Recommended)**

Maintain unhealthy peer state even for disconnected peers, using a time-based expiration instead of connection-based garbage collection:

```rust
pub fn refresh_unhealthy_peer_states(&self) -> Result<(), Error> {
    let connected_peers_and_metadata = self
        .peers_and_metadata
        .get_connected_peers_and_metadata()
        .map_err(|error| {
            Error::UnexpectedErrorEncountered(format!(
                "Unable to get connected peers and metadata: {}",
                error
            ))
        })?;

    let mut num_ignored_peers = 0;
    self.unhealthy_peer_states.retain(|peer_network_id, unhealthy_peer_state| {
        // Refresh ignored peer state if connected
        if connected_peers_and_metadata.contains_key(peer_network_id) {
            unhealthy_peer_state.refresh_peer_state(peer_network_id);
        }
        
        // Count ignored peers (only if connected)
        if connected_peers_and_metadata.contains_key(peer_network_id) 
            && unhealthy_peer_state.is_ignored() {
            num_ignored_peers += 1;
        }

        // Only remove peer state after a significant time period (e.g., 24 hours)
        // even if disconnected, to maintain reputation across reconnections
        if let Some(ignore_start_time) = unhealthy_peer_state.ignore_start_time {
            let elapsed = self.time_service.now().duration_since(ignore_start_time);
            // Keep state for at least 24 hours
            elapsed < Duration::from_secs(86400)
        } else {
            // Keep peer state even if not currently ignored
            true
        }
    });

    metrics::set_gauge(
        &metrics::IGNORED_PEER_COUNT,
        NetworkId::Public.as_str(),
        num_ignored_peers,
    );

    Ok(())
}
```

**Fix Option 2: Track by Network Identity**

Track peers by their network identity (IP address) instead of just `PeerNetworkId`, preventing reconnection bypass.

**Fix Option 3: Add Configuration Option**

Add a configuration parameter to control whether to persist peer state across disconnections, defaulting to persistent state.

## Proof of Concept

The following Rust test demonstrates the vulnerability:

```rust
#[tokio::test]
async fn test_peer_resurrection_attack() {
    use crate::tests::{mock::MockClient, utils};
    use aptos_config::config::{NetworkId, PeerNetworkId, StorageServiceConfig};
    use aptos_types::PeerId;
    use std::time::Duration;

    // Create test configuration
    let max_invalid_requests_per_peer = 500;
    let storage_service_config = StorageServiceConfig {
        max_invalid_requests_per_peer,
        request_moderator_refresh_interval_ms: 1000,
        ..Default::default()
    };

    // Create storage client and server
    let (mut mock_client, mut service, _, time_service, peers_and_metadata) =
        MockClient::new(None, Some(storage_service_config));
    
    let highest_synced_version = 100;
    let highest_synced_epoch = 10;
    utils::update_storage_server_summary(&mut service, highest_synced_version, highest_synced_epoch);

    // Get request moderator
    let request_moderator = service.get_request_moderator();
    let unhealthy_peer_states = request_moderator.get_unhealthy_peer_states();
    
    // Spawn server
    tokio::spawn(service.start());

    // ROUND 1: Send invalid requests until blocked
    let attacker_peer = PeerNetworkId::new(NetworkId::Public, PeerId::random());
    peers_and_metadata.insert_connection_metadata(
        attacker_peer,
        create_test_connection_metadata(attacker_peer.peer_id(), 0),
    ).unwrap();

    for _ in 0..max_invalid_requests_per_peer {
        send_invalid_transaction_request(
            highest_synced_version,
            &mut mock_client,
            attacker_peer,
        ).await.unwrap_err();
    }

    // Verify peer is now blocked
    let response = send_invalid_transaction_request(
        highest_synced_version,
        &mut mock_client,
        attacker_peer,
    ).await;
    assert_matches!(response.unwrap_err(), StorageServiceError::TooManyInvalidRequests(_));
    assert!(unhealthy_peer_states.get(&attacker_peer).unwrap().is_ignored());

    // ATTACK: Disconnect the peer
    peers_and_metadata.remove_peer_metadata(attacker_peer, ConnectionId::from(0)).unwrap();

    // Wait for garbage collection (advance time by refresh interval)
    time_service.advance_ms_async(1000).await;
    tokio::time::sleep(Duration::from_millis(100)).await;

    // VULNERABILITY: Peer state has been removed
    assert!(!unhealthy_peer_states.contains_key(&attacker_peer));

    // ROUND 2: Reconnect and repeat attack
    peers_and_metadata.insert_connection_metadata(
        attacker_peer,
        create_test_connection_metadata(attacker_peer.peer_id(), 1),
    ).unwrap();

    // Can send another 500 invalid requests before being blocked again
    for _ in 0..max_invalid_requests_per_peer {
        let response = send_invalid_transaction_request(
            highest_synced_version,
            &mut mock_client,
            attacker_peer,
        ).await;
        // Should get InvalidRequest error, not TooManyInvalidRequests
        assert_matches!(response.unwrap_err(), StorageServiceError::InvalidRequest(_));
    }

    // Peer gets blocked again only after sending another 500 requests
    let response = send_invalid_transaction_request(
        highest_synced_version,
        &mut mock_client,
        attacker_peer,
    ).await;
    assert_matches!(response.unwrap_err(), StorageServiceError::TooManyInvalidRequests(_));

    // IMPACT: Attacker can repeat this cycle indefinitely
    println!("âœ“ Vulnerability confirmed: Peer can bypass rate limiting by disconnecting/reconnecting");
}
```

This test demonstrates that a malicious peer can:
1. Send 500 invalid requests and get blocked
2. Disconnect and have their state removed
3. Reconnect and send another 500 invalid requests
4. Repeat indefinitely, bypassing the intended rate limiting

## Notes

**Additional Context:**

1. The vulnerability affects only public network peers, as validator and VFN peers are not subject to ignoring logic per the condition on line 56 of `moderator.rs`.

2. The exponential backoff mechanism (`min_time_to_ignore_secs` doubling) is completely bypassed since this state is lost on disconnect.

3. A coordinated attack with multiple peer connections can multiply the impact linearly.

4. The existing test suite actually demonstrates this behavior but doesn't treat it as a security issue.

5. This is not a DoS attack (which is out of scope), but rather a bypass of a security control that enables resource exhaustion through protocol-level abuse.

### Citations

**File:** state-sync/storage-service/server/src/moderator.rs (L50-69)
```rust
    pub fn increment_invalid_request_count(&mut self, peer_network_id: &PeerNetworkId) {
        // Increment the invalid request count
        self.invalid_request_count += 1;

        // If the peer is a PFN and has sent too many invalid requests, start ignoring it
        if self.ignore_start_time.is_none()
            && peer_network_id.network_id().is_public_network()
            && self.invalid_request_count >= self.max_invalid_requests
        {
            // TODO: at some point we'll want to terminate the connection entirely

            // Start ignoring the peer
            self.ignore_start_time = Some(self.time_service.now());

            // Log the fact that we're now ignoring the peer
            warn!(LogSchema::new(LogEntry::RequestModeratorIgnoredPeer)
                .peer_network_id(peer_network_id)
                .message("Ignoring peer due to too many invalid requests!"));
        }
    }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L141-149)
```rust
            // If the peer is being ignored, return an error
            if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
                if peer_state.is_ignored() {
                    return Err(Error::TooManyInvalidRequests(format!(
                        "Peer is temporarily ignored. Unable to handle request: {:?}",
                        request
                    )));
                }
            }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L154-186)
```rust
            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }

```

**File:** state-sync/storage-service/server/src/moderator.rs (L213-228)
```rust
        self.unhealthy_peer_states
            .retain(|peer_network_id, unhealthy_peer_state| {
                if connected_peers_and_metadata.contains_key(peer_network_id) {
                    // Refresh the ignored peer state
                    unhealthy_peer_state.refresh_peer_state(peer_network_id);

                    // If the peer is ignored, increment the ignored peer count
                    if unhealthy_peer_state.is_ignored() {
                        num_ignored_peers += 1;
                    }

                    true // The peer is still connected, so we should keep it
                } else {
                    false // The peer is no longer connected, so we should remove it
                }
            });
```

**File:** state-sync/storage-service/server/src/lib.rs (L363-380)
```rust
        self.runtime.spawn(async move {
            // Create a ticker for the refresh interval
            let duration = Duration::from_millis(config.request_moderator_refresh_interval_ms);
            let ticker = time_service.interval(duration);
            futures::pin_mut!(ticker);

            // Periodically refresh the peer states
            loop {
                ticker.next().await;

                // Refresh the unhealthy peer states
                if let Err(error) = request_moderator.refresh_unhealthy_peer_states() {
                    error!(LogSchema::new(LogEntry::RequestModeratorRefresh)
                        .error(&error)
                        .message("Failed to refresh the request moderator!"));
                }
            }
        });
```

**File:** config/src/config/state_sync_config.rs (L201-214)
```rust
            max_invalid_requests_per_peer: 500,
            max_lru_cache_size: 500, // At ~0.6MiB per chunk, this should take no more than 0.5GiB
            max_network_channel_size: 4000,
            max_network_chunk_bytes: SERVER_MAX_MESSAGE_SIZE as u64,
            max_network_chunk_bytes_v2: SERVER_MAX_MESSAGE_SIZE_V2 as u64,
            max_num_active_subscriptions: 30,
            max_optimistic_fetch_period_ms: 5000, // 5 seconds
            max_state_chunk_size: MAX_STATE_CHUNK_SIZE,
            max_storage_read_wait_time_ms: 10_000, // 10 seconds
            max_subscription_period_ms: 30_000,    // 30 seconds
            max_transaction_chunk_size: MAX_TRANSACTION_CHUNK_SIZE,
            max_transaction_output_chunk_size: MAX_TRANSACTION_OUTPUT_CHUNK_SIZE,
            min_time_to_ignore_peers_secs: 300, // 5 minutes
            request_moderator_refresh_interval_ms: 1000, // 1 second
```

**File:** state-sync/storage-service/server/src/tests/request_moderator.rs (L303-318)
```rust
    // Reconnect the first peer
    peers_and_metadata
        .update_connection_state(peer_network_ids[0], ConnectionState::Connected)
        .unwrap();

    // Send an invalid request from the first peer
    send_invalid_transaction_request(
        highest_synced_version,
        &mut mock_client,
        peer_network_ids[0],
    )
    .await
    .unwrap_err();

    // Verify the peer is now tracked as unhealthy
    assert!(unhealthy_peer_states.contains_key(&peer_network_ids[0]));
```
