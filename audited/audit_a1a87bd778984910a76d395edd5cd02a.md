# Audit Report

## Title
Blocking Lock Acquisition in PooledVMValidator::restart() Enables Denial of Service During Reconfiguration

## Summary
The `PooledVMValidator::restart()` function uses blocking lock acquisition that can be indefinitely delayed when concurrent transaction validations hold locks on validators. An attacker can exploit this by flooding the node with transactions during reconfiguration events, delaying critical state updates and causing validators to operate with stale state.

## Finding Description

The vulnerability exists in a two-level locking hierarchy between transaction validation and validator restart operations. [1](#0-0) 

The `restart()` function iterates through all validators in the pool and acquires locks using blocking `.lock().unwrap()` calls. This will block indefinitely if any validator is currently locked by an ongoing validation. [2](#0-1) 

The `validate_transaction()` function acquires a lock on a randomly selected validator and holds it during the entire validation process, which includes cryptographically expensive operations. [3](#0-2) 

Transaction validations happen in parallel using `par_iter()`, meaning multiple validators can be locked simultaneously. Each validation also holds a read lock on the outer `RwLock<PooledVMValidator>`. [4](#0-3) 

During reconfiguration, `restart()` is called to update validator state views. This requires a write lock on the outer `RwLock`, which blocks until all read locks (from ongoing validations) are released, then must acquire mutexes on all individual validators. [5](#0-4) 

The locking primitives have no timeout mechanism - they block indefinitely.

**Attack Path:**

1. Attacker floods the validator node with transactions, particularly those with expensive signature verification (keyless signatures using Groth16 ZKP verification) [6](#0-5) 

2. These transactions keep all validators in the pool busy with locks held during cryptographic operations

3. A reconfiguration event occurs (epoch change or governance update)

4. The `restart()` function is called but blocks at two levels:
   - First, waiting for the write lock while read locks are held by validations
   - Second, waiting for individual validator mutexes that are locked by ongoing validations

5. The blocking delay prevents timely state view updates, causing validators to continue operating with stale state

6. This can lead to incorrect transaction validation decisions based on outdated state

## Impact Explanation

This vulnerability meets **Medium severity** criteria: "State inconsistencies requiring intervention."

The impact includes:
- **State Inconsistency**: Validators continue validating transactions against stale state views after commits occur
- **Incorrect Validation Decisions**: May accept invalid transactions or reject valid ones based on outdated state
- **Delayed Reconfiguration**: Critical configuration updates (validator set changes, consensus parameters) are delayed
- **Amplified by Expensive Operations**: Keyless signatures with Groth16 ZKP verification can take significant time, extending the blocking period

While this doesn't directly cause consensus safety violations (transactions are re-validated during execution), it can cause operational issues and temporary inconsistencies in mempool behavior across validators.

## Likelihood Explanation

**Likelihood: Medium to High**

The attack is feasible because:
- **Low Barrier**: Any user can submit transactions without special privileges
- **Amplification**: Using multiple accounts bypasses per-account rate limits [7](#0-6) 
- **Pool Size**: Pool size equals CPU count (typically 32-64), making it practical to saturate all validators
- **Concurrent Validation**: Parallel validation via `par_iter()` means many validators are simultaneously locked
- **Predictable Timing**: Epoch changes and governance reconfigurations are observable events
- **Expensive Operations**: Attacker can craft transactions with keyless signatures to maximize validation time

The attack requires timing coordination with reconfiguration events, but with sustained transaction flooding, the probability of overlap increases significantly.

## Recommendation

Implement non-blocking lock acquisition with timeout handling:

```rust
fn restart(&mut self) -> Result<()> {
    const LOCK_TIMEOUT: Duration = Duration::from_millis(100);
    let start = Instant::now();
    
    for (idx, vm_validator) in self.vm_validators.iter().enumerate() {
        let elapsed = start.elapsed();
        if elapsed > LOCK_TIMEOUT {
            return Err(anyhow::anyhow!(
                "Restart timeout: locked validators after {:?}, completed {}/{}",
                elapsed, idx, self.vm_validators.len()
            ));
        }
        
        match vm_validator.try_lock() {
            Ok(mut validator) => {
                validator.restart()?;
            },
            Err(_) => {
                // Retry with brief backoff
                std::thread::sleep(Duration::from_micros(100));
                let mut validator = vm_validator.try_lock()
                    .map_err(|_| anyhow::anyhow!(
                        "Failed to acquire lock on validator {} after retry", idx
                    ))?;
                validator.restart()?;
            }
        }
    }
    Ok(())
}
```

Alternative approach: Use `RwLock` for individual validators and upgrade to write lock during restart, or implement a generation-based approach where restart creates new validator instances rather than blocking on existing ones.

Additionally, consider the TODO comment's suggestion to eliminate the pooling architecture entirely since the VM is now thread-safe: [8](#0-7) 

## Proof of Concept

```rust
#[cfg(test)]
mod dos_test {
    use super::*;
    use std::sync::{Arc, Barrier};
    use std::thread;
    use std::time::Duration;
    
    #[test]
    fn test_restart_blocked_by_validation() {
        // Create a validator pool with 4 validators
        let db = Arc::new(MockDbReader::new());
        let mut validator_pool = PooledVMValidator::new(db.clone(), 4);
        let validator_pool_arc = Arc::new(validator_pool.clone());
        
        // Barrier to synchronize threads
        let barrier = Arc::new(Barrier::new(5)); // 4 validators + 1 restart thread
        
        // Spawn 4 validation threads that hold locks
        let mut handles = vec![];
        for i in 0..4 {
            let pool = validator_pool_arc.clone();
            let barrier = barrier.clone();
            let handle = thread::spawn(move || {
                // Acquire a validator and hold the lock
                let vm = pool.get_next_vm();
                let _lock = vm.lock().unwrap();
                
                // Signal ready and wait for all threads
                barrier.wait();
                
                // Hold lock for 5 seconds simulating slow validation
                thread::sleep(Duration::from_secs(5));
            });
            handles.push(handle);
        }
        
        // Spawn restart thread
        let restart_handle = thread::spawn(move || {
            barrier.wait(); // Wait for all validators to be locked
            
            let start = std::time::Instant::now();
            // This will block until all validations complete
            validator_pool.restart().expect("Restart failed");
            let duration = start.elapsed();
            
            // Assert that restart was blocked for ~5 seconds
            assert!(duration.as_secs() >= 4, 
                "Restart completed too quickly: {:?}", duration);
            
            println!("Restart blocked for {:?}", duration);
        });
        
        // Wait for all threads
        for handle in handles {
            handle.join().unwrap();
        }
        restart_handle.join().unwrap();
    }
}
```

## Notes

This vulnerability is exacerbated by:
1. The use of cryptographically expensive operations in validation (Groth16 ZKP verification for keyless signatures)
2. Parallel validation without concurrency limits beyond CPU count
3. No timeout or retry mechanisms in lock acquisition
4. The legacy pooling architecture that was designed when the VM wasn't thread-safe (per TODO comment)

The issue affects validator reliability during reconfiguration events, which are critical for maintaining network consistency and up-to-date consensus parameters.

### Citations

**File:** vm-validator/src/vm_validator.rs (L119-121)
```rust
// A pool of VMValidators that can be used to validate transactions concurrently. This is done because
// the VM is not thread safe today. This is a temporary solution until the VM is made thread safe.
// TODO(loader_v2): Re-implement because VM is thread-safe now.
```

**File:** vm-validator/src/vm_validator.rs (L146-170)
```rust
    fn validate_transaction(&self, txn: SignedTransaction) -> Result<VMValidatorResult> {
        let vm_validator = self.get_next_vm();

        fail_point!("vm_validator::validate_transaction", |_| {
            Err(anyhow::anyhow!(
                "Injected error in vm_validator::validate_transaction"
            ))
        });

        let result = std::panic::catch_unwind(move || {
            let vm_validator_locked = vm_validator.lock().unwrap();

            use aptos_vm::VMValidator;
            let vm = AptosVM::new(&vm_validator_locked.state.environment);
            vm.validate_transaction(
                txn,
                &vm_validator_locked.state.state_view,
                &vm_validator_locked.state,
            )
        });
        if let Err(err) = &result {
            error!("VMValidator panicked: {:?}", err);
        }
        result.map_err(|_| anyhow::anyhow!("panic validating transaction"))
    }
```

**File:** vm-validator/src/vm_validator.rs (L172-177)
```rust
    fn restart(&mut self) -> Result<()> {
        for vm_validator in &self.vm_validators {
            vm_validator.lock().unwrap().restart()?;
        }
        Ok(())
    }
```

**File:** mempool/src/shared_mempool/tasks.rs (L490-503)
```rust
    let validation_results = VALIDATION_POOL.install(|| {
        transactions
            .par_iter()
            .map(|t| {
                let result = smp.validator.read().validate_transaction(t.0.clone());
                // Pre-compute the hash and length if the transaction is valid, before locking mempool
                if result.is_ok() {
                    t.0.committed_hash();
                    t.0.txn_bytes_len();
                }
                result
            })
            .collect::<Vec<_>>()
    });
```

**File:** mempool/src/shared_mempool/tasks.rs (L775-778)
```rust
    if let Err(e) = validator.write().restart() {
        counters::VM_RECONFIG_UPDATE_FAIL_COUNT.inc();
        error!(LogSchema::event_log(LogEntry::ReconfigUpdate, LogEvent::VMUpdateFail).error(&e));
    }
```

**File:** crates/aptos-infallible/src/rwlock.rs (L18-30)
```rust
    /// lock the rwlock in read mode
    pub fn read(&self) -> RwLockReadGuard<'_, T> {
        self.0
            .read()
            .expect("Cannot currently handle a poisoned lock")
    }

    /// lock the rwlock in write mode
    pub fn write(&self) -> RwLockWriteGuard<'_, T> {
        self.0
            .write()
            .expect("Cannot currently handle a poisoned lock")
    }
```

**File:** aptos-move/aptos-vm/src/keyless_validation.rs (L347-362)
```rust
                        let result = zksig.verify_groth16_proof(public_inputs_hash, pvk.unwrap());

                        result.map_err(|_| {
                            // println!("[aptos-vm][groth16] ZKP verification failed");
                            // println!("[aptos-vm][groth16] PIH: {}", public_inputs_hash);
                            // match zksig.proof {
                            //     ZKP::Groth16(proof) => {
                            //         println!("[aptos-vm][groth16] ZKP: {}", proof.hash());
                            //     },
                            // }
                            // println!(
                            //     "[aptos-vm][groth16] PVK: {}",
                            //     Groth16VerificationKey::from(pvk).hash()
                            // );
                            invalid_signature!("Proof verification failed")
                        })?;
```

**File:** mempool/src/shared_mempool/runtime.rs (L104-107)
```rust
    let vm_validator = Arc::new(RwLock::new(PooledVMValidator::new(
        Arc::clone(&db),
        num_cpus::get(),
    )));
```
