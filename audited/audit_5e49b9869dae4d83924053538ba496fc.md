# Audit Report

## Title
Missing Output Validation in Sharded Execution Allows Potential Consensus Divergence

## Summary
The sharded execution path in `by_transaction_execution_sharded()` does not validate that transaction outputs returned from parallel shard execution match the expected structure and ordering of input transactions. This missing validation could allow consensus divergence if the VM or execution layer produces non-deterministic results or has bugs that cause output count/order mismatches.

## Finding Description

In the sharded execution workflow, transactions are partitioned across multiple shards, executed in parallel, then aggregated back into a single ordered output list. The critical vulnerability is that **no validation occurs** between the execution output aggregation and the pairing of outputs with input transactions.

**Location 1**: Output aggregation without validation [1](#0-0) 

The aggregation logic assumes `sharded_output[0].len()` represents the round count for all shards, but never validates that all other shards returned the same number of rounds. It also never validates that each round's output count matches the expected sub-block transaction count.

**Location 2**: Direct pairing without structure validation [2](#0-1) 

The code flattens `partitioned_txns` and pairs them directly with aggregated `transaction_outputs` without validating:
- Total output count matches total transaction count (only checked later via assertion)
- Output order per round matches input transaction order per round
- All shards produced outputs for all expected rounds

**Location 3**: Late assertion instead of early validation [3](#0-2) 

The only validation is a runtime assertion that panics if counts don't match. This happens AFTER outputs are already paired with transactions, meaning ordering mismatches with correct counts go undetected.

**Broken Invariant**: Deterministic Execution - All validators must produce identical state roots for identical blocks. Without validation that outputs match input structure, non-deterministic execution or VM bugs could cause different validators to pair outputs with wrong transactions, computing different state roots.

**Attack Propagation**: While not directly exploitable by external attackers, this gap becomes critical if:
1. The VM has non-deterministic behavior (e.g., race conditions in parallel execution)
2. Different validator nodes run slightly different versions with execution bugs
3. Edge cases in transaction processing produce variable output counts

Different validators would silently compute different state roots, causing consensus failure.

## Impact Explanation

**Severity: Critical** - This qualifies as a "Consensus/Safety violation" under Aptos bug bounty rules.

The missing validation creates a **defense-in-depth failure** that allows non-deterministic execution bugs to cause consensus divergence:

1. **Consensus Safety Violation**: If different validators aggregate outputs differently due to VM bugs or edge cases, they compute different state roots, violating BFT consensus safety.

2. **Silent State Divergence**: Without early validation, output mismatches aren't detected until after pairing with transactions. Ordering errors with correct counts pass through undetected.

3. **Validator Crashes**: If output counts mismatch, the assertion panic crashes validator nodes instead of gracefully handling the error.

4. **No Recovery Path**: Once divergence occurs, validators cannot automatically detect or recover from the mismatch without manual intervention or rollback.

The impact is critical because consensus safety is the fundamental guarantee of any blockchain system. Any vulnerability that can cause state divergence across validators threatens the integrity of the entire network.

## Likelihood Explanation

**Likelihood: Medium**

While this requires a pre-existing VM bug or non-deterministic behavior to exploit, the likelihood is not negligible:

1. **Complexity of Parallel Execution**: Sharded execution introduces parallelism and coordination complexity where race conditions or edge cases could cause non-determinism.

2. **No Testing of Error Cases**: Test files validate happy path (all shards succeed with correct counts) but don't test partial failures or output count mismatches. [4](#0-3) 

3. **Assumption-Heavy Design**: The code assumes deterministic VM behavior without verifying it, making it fragile to future bugs.

4. **Production Deployment Risk**: Sharded execution is a performance optimization likely to be enabled in production, increasing exposure.

## Recommendation

Add explicit validation in `ShardedBlockExecutor::execute_block` to verify output structure matches input structure:

```rust
pub fn execute_block(
    &self,
    state_view: Arc<S>,
    transactions: PartitionedTransactions,
    concurrency_level_per_shard: usize,
    onchain_config: BlockExecutorConfigFromOnchain,
) -> Result<Vec<TransactionOutput>, VMStatus> {
    let _timer = SHARDED_BLOCK_EXECUTION_SECONDS.start_timer();
    let num_executor_shards = self.executor_client.num_shards();
    assert_eq!(num_executor_shards, transactions.num_shards());
    
    // Calculate expected structure
    let expected_num_rounds = transactions.sharded_txns()[0].num_sub_blocks();
    let expected_txn_count_per_round: Vec<Vec<usize>> = transactions
        .sharded_txns()
        .iter()
        .map(|shard| shard.sub_block_iter().map(|sb| sb.num_txns()).collect())
        .collect();
    
    let (sharded_output, global_output) = self
        .executor_client
        .execute_block(state_view, transactions, concurrency_level_per_shard, onchain_config)?
        .into_inner();
    
    // VALIDATION: Verify all shards returned same number of rounds
    for (shard_id, shard_results) in sharded_output.iter().enumerate() {
        ensure!(
            shard_results.len() == expected_num_rounds,
            "Shard {} returned {} rounds, expected {}",
            shard_id, shard_results.len(), expected_num_rounds
        );
        
        // VALIDATION: Verify each round has correct output count
        for (round, outputs) in shard_results.iter().enumerate() {
            let expected_count = expected_txn_count_per_round[shard_id][round];
            ensure!(
                outputs.len() == expected_count,
                "Shard {} round {} returned {} outputs, expected {}",
                shard_id, round, outputs.len(), expected_count
            );
        }
    }
    
    // Continue with existing aggregation logic...
    let num_rounds = expected_num_rounds;
    let mut aggregated_results = vec![];
    // ... rest of function
}
```

Additionally, add validation in `by_transaction_execution_sharded`:

```rust
pub fn by_transaction_execution_sharded<V: VMBlockExecutor>(
    transactions: PartitionedTransactions,
    // ... other params
) -> Result<ExecutionOutput> {
    let expected_txn_count = transactions.num_txns();
    let state_view_arc = Arc::new(state_view);
    let transaction_outputs = Self::execute_block_sharded::<V>(
        transactions.clone(),
        state_view_arc.clone(),
        onchain_config,
    )?;
    
    // VALIDATION: Verify output count matches input count BEFORE pairing
    ensure!(
        transaction_outputs.len() == expected_txn_count,
        "Sharded execution returned {} outputs, expected {} transactions",
        transaction_outputs.len(), expected_txn_count
    );
    
    // Continue with existing parsing...
}
```

## Proof of Concept

The following demonstrates how missing validation allows mismatched outputs to pass through:

```rust
#[test]
fn test_sharded_execution_output_count_mismatch() {
    // Setup: Create partitioned transactions with known structure
    let mut transactions = Vec::new();
    for _ in 0..100 {
        transactions.push(create_test_transaction());
    }
    
    let num_shards = 4;
    let partitioner = PartitionerV2Config::default().build();
    let partitioned_txns = partitioner.partition(transactions, num_shards);
    
    // Expected: 100 transactions should produce 100 outputs
    let expected_count = partitioned_txns.num_txns();
    
    // Simulate bug: Mock executor that returns fewer outputs
    let mock_executor = MockExecutorWithBug::new(|shard_id| {
        // Shard 0 drops last output to simulate bug
        if shard_id == 0 { 
            BugBehavior::DropLastOutput 
        } else { 
            BugBehavior::Normal 
        }
    });
    
    // Execute with buggy executor
    let result = ShardedBlockExecutor::new(mock_executor)
        .execute_block(
            Arc::new(state_view),
            partitioned_txns,
            2,
            BlockExecutorConfigFromOnchain::default()
        );
    
    // Current behavior: Returns Ok with 99 outputs
    // Expected behavior: Should return Err indicating output count mismatch
    assert!(result.is_ok()); // VULNERABILITY: No validation catches mismatch
    assert_eq!(result.unwrap().len(), 99); // Wrong count passes through!
    
    // Later, TransactionsWithOutput::new will panic instead of gracefully handling error
}
```

The PoC demonstrates that output count mismatches are not caught during aggregation, only later via assertion panic, allowing potential state divergence if different validators exhibit different buggy behaviors.

## Notes

This vulnerability is a **defense-in-depth failure** rather than a directly exploitable attack. However, it's critical because:

1. It violates the principle of validating assumptions at system boundaries
2. It makes consensus safety dependent on perfect VM implementation (no bugs, no non-determinism)
3. It provides no early detection mechanism for execution anomalies
4. It could mask critical bugs in the VM or execution layer until after state divergence occurs

The recommended fix adds essential validation checkpoints that catch execution anomalies before they can cause consensus divergence, aligning with defense-in-depth security principles for consensus-critical systems.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L98-115)
```rust
        let num_rounds = sharded_output[0].len();
        let mut aggregated_results = vec![];
        let mut ordered_results = vec![vec![]; num_executor_shards * num_rounds];
        // Append the output from individual shards in the round order
        for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
            for (round, result) in results_from_shard.into_iter().enumerate() {
                ordered_results[round * num_executor_shards + shard_id] = result;
            }
        }

        for result in ordered_results.into_iter() {
            aggregated_results.extend(result);
        }

        // Lastly append the global output
        aggregated_results.extend(global_output);

        Ok(aggregated_results)
```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L197-222)
```rust
        let transaction_outputs = Self::execute_block_sharded::<V>(
            transactions.clone(),
            state_view_arc.clone(),
            onchain_config,
        )?;

        // TODO(Manu): Handle state checkpoint here.

        // TODO(skedia) add logic to emit counters per shard instead of doing it globally.

        // Unwrapping here is safe because the execution has finished and it is guaranteed that
        // the state view is not used anymore.
        let state_view = Arc::try_unwrap(state_view_arc).unwrap();
        Parser::parse(
            state_view.next_version(),
            PartitionedTransactions::flatten(transactions)
                .into_iter()
                .map(|t| t.into_txn().into_inner())
                .collect(),
            transaction_outputs,
            auxiliary_infos,
            parent_state,
            state_view,
            false, // prime_state_cache
            append_state_checkpoint_to_block.is_some(),
        )
```

**File:** execution/executor-types/src/transactions_with_output.rs (L28-29)
```rust
        assert_eq!(transactions.len(), transaction_outputs.len());
        assert_eq!(transactions.len(), persisted_auxiliary_infos.len());
```

**File:** execution/executor-service/src/test_utils.rs (L82-119)
```rust
pub fn compare_txn_outputs(
    unsharded_txn_output: Vec<TransactionOutput>,
    sharded_txn_output: Vec<TransactionOutput>,
) {
    assert_eq!(unsharded_txn_output.len(), sharded_txn_output.len());
    for i in 0..unsharded_txn_output.len() {
        assert_eq!(
            unsharded_txn_output[i].status(),
            sharded_txn_output[i].status()
        );
        assert_eq!(
            unsharded_txn_output[i].gas_used(),
            sharded_txn_output[i].gas_used()
        );
        //assert_eq!(unsharded_txn_output[i].write_set(), sharded_txn_output[i].write_set());
        assert_eq!(
            unsharded_txn_output[i].events(),
            sharded_txn_output[i].events()
        );
        // Global supply tracking for coin is not supported in sharded execution yet, so we filter
        // out the table item from the write set, which has the global supply. This is a hack until
        // we support global supply tracking in sharded execution.
        let unsharded_write_set_without_table_item = unsharded_txn_output[i]
            .write_set()
            .write_op_iter()
            .filter(|(k, _)| matches!(k.inner(), &StateKeyInner::AccessPath(_)))
            .collect::<Vec<_>>();
        let sharded_write_set_without_table_item = sharded_txn_output[i]
            .write_set()
            .write_op_iter()
            .filter(|(k, _)| matches!(k.inner(), &StateKeyInner::AccessPath(_)))
            .collect::<Vec<_>>();
        assert_eq!(
            unsharded_write_set_without_table_item,
            sharded_write_set_without_table_item
        );
    }
}
```
