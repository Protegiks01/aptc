# Audit Report

## Title
Race Condition in finish_abort() Breaks Atomicity of start_next_incarnation Transition Causing Consensus Non-Determinism

## Summary
A race condition exists in `ExecutionStatuses::finish_abort()` when `start_next_incarnation=true`. The claimed "atomic" transition from Executed(i) to Executing(i+1) is not truly atomic because a concurrent `start_abort()` on the new incarnation can succeed between the incarnation update and status transition, causing the newly started incarnation to be immediately aborted. This violates the deterministic execution invariant and can cause consensus disagreement between validators. [1](#0-0) [2](#0-1) 

## Finding Description
The vulnerability occurs in the `finish_abort()` function when called with `start_next_incarnation=true`, which is used during transaction commit when delayed field validation requires re-execution. [3](#0-2) 

**The Race Window:**

While the status lock is held throughout `finish_abort()`, the `start_abort()` function does NOT acquire this lock - it only performs atomic operations on `next_incarnation_to_abort`: [4](#0-3) 

The race occurs as follows:

1. **Thread A (committing transaction)** calls `direct_abort(txn_idx, i, true)` at line 1018 in executor.rs during commit
2. Thread A's `start_abort(txn_idx, i)` sets `next_incarnation_to_abort = i+1`
3. Thread A enters `finish_abort(txn_idx, i, true)` and acquires the status lock
4. Thread A calls `to_pending_scheduling()` which updates `incarnation = i+1` at line 878
5. **RACE WINDOW**: Incarnation is now i+1, but `to_executing()` hasn't been called yet
6. **Thread B (concurrent worker)** detects a dependency on the transaction and calls `start_abort(txn_idx, i+1)`
7. Thread B's `fetch_max(i+2)` succeeds because `prev_value = i+1` (set by Thread A)
8. Thread B sets `next_incarnation_to_abort = i+2`, marking incarnation i+1 for abort
9. Thread A completes by calling `to_executing()`, transitioning status to Executing(i+1)
10. Thread A releases the lock and proceeds to execute incarnation i+1 [5](#0-4) [6](#0-5) 

**The consequence**: When Thread A executes the transaction via `execute_txn_after_commit()`, the execution checks `is_halted_or_aborted(txn_idx, i+1)` which returns true because `next_incarnation_to_abort (i+2) > i+1`. The execution is aborted early as a speculative failure. [7](#0-6) [8](#0-7) 

This violates the fundamental assumption that the re-execution during commit is the final execution before commit. The transaction that was supposed to be atomically re-executed and committed is instead aborted, causing the commit to fail. [9](#0-8) 

## Impact Explanation
**Severity: High (potentially Critical)**

This vulnerability breaks **Critical Invariant #1: Deterministic Execution**. The race condition is timing-dependent and non-deterministic - different validators executing the same block may experience different outcomes based on CPU speed, thread scheduling, and system load:

- **Validator A** (race doesn't occur): Re-execution succeeds → block commits successfully
- **Validator B** (race occurs): Re-execution is aborted → commit fails with error → entire block execution fails

This leads to consensus disagreement where some validators accept a block proposal while others reject it, violating the safety guarantee that all validators must produce identical state roots for identical blocks. In severe cases, this could cause:

1. **Consensus Liveness Failure**: If sufficient validators experience the race, the blockchain could halt
2. **Chain Fork Risk**: Different validator subsets could commit different blocks
3. **State Divergence**: Validators could end up with inconsistent world state

The vulnerability meets **High Severity** criteria (significant protocol violations) and potentially **Critical Severity** if it causes non-recoverable consensus failures.

## Likelihood Explanation
**Likelihood: Medium-High**

The race window is small (microseconds between two function calls), but the likelihood increases significantly under:

1. **High Transaction Load**: More concurrent transactions increase abort frequency
2. **Complex Dependencies**: Transactions with many read/write dependencies trigger more `start_abort()` calls
3. **Delayed Field Operations**: Any transaction using delayed fields that requires re-execution during commit enters the vulnerable code path

An attacker can increase the probability by:
- Submitting transactions that deliberately trigger delayed field validation failures
- Creating dense dependency graphs that maximize concurrent `start_abort()` calls
- Submitting high transaction volumes during network stress

The vulnerability is exploitable by any unprivileged transaction sender without requiring validator access or collusion.

## Recommendation

**Fix: Ensure true atomicity by checking incarnation consistency after status lock acquisition**

Modify `start_abort()` to verify the incarnation hasn't changed while it was attempting the abort. Add an additional validation in `finish_abort()` to detect if a concurrent abort has occurred:

```rust
pub(crate) fn finish_abort(
    &self,
    txn_idx: TxnIndex,
    aborted_incarnation: Incarnation,
    start_next_incarnation: bool,
) -> Result<(), PanicError> {
    let status = &self.statuses[txn_idx as usize];
    let new_incarnation = aborted_incarnation + 1;
    
    // Existing check
    if status.next_incarnation_to_abort.load(Ordering::Relaxed) != new_incarnation {
        return Err(code_invariant_error(...));
    }

    {
        let status_guard = &mut *status.status_with_incarnation.lock();
        
        // Existing validation...
        
        match status_guard.status {
            SchedulingStatus::Executed => {
                self.to_pending_scheduling(...);
                
                if start_next_incarnation {
                    // CRITICAL FIX: Re-check that no concurrent abort occurred
                    // during the transition to PendingScheduling
                    if status.next_incarnation_to_abort.load(Ordering::SeqCst) != new_incarnation {
                        // A concurrent start_abort() succeeded on the new incarnation
                        // Revert to PendingScheduling and fail
                        return Err(code_invariant_error(format!(
                            "Concurrent abort detected during atomic transition for txn {} incarnation {}",
                            txn_idx, new_incarnation
                        )));
                    }
                    
                    let started_incarnation = self.to_executing(txn_idx, status_guard)?;
                    
                    // Existing check...
                }
            },
            // ... rest of match arms
        }
    }
    
    Ok(())
}
```

Alternatively, redesign the abort mechanism to make `start_abort()` acquire the status lock to ensure true atomicity, though this would have performance implications.

## Proof of Concept

The following Rust test demonstrates the race condition:

```rust
#[test]
fn test_race_condition_finish_abort_with_start_next_incarnation() {
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    let statuses = Arc::new(ExecutionStatuses::new_for_test(
        ExecutionQueueManager::new_for_test(1),
        vec![ExecutionStatus::new_for_test(
            StatusWithIncarnation::new_for_test(SchedulingStatus::Executed, 1),
            0,
        )],
    ));
    
    let txn_idx = 0;
    let incarnation = 1;
    
    // Simulate successful start_abort that precedes finish_abort
    statuses.get_status(txn_idx)
        .next_incarnation_to_abort
        .store(incarnation + 1, Ordering::Relaxed);
    
    let barrier = Arc::new(Barrier::new(2));
    let statuses_clone = statuses.clone();
    let barrier_clone = barrier.clone();
    
    // Thread A: Committer calling finish_abort with start_next_incarnation=true
    let thread_a = thread::spawn(move || {
        // Insert artificial delay after to_pending_scheduling
        // to increase race window (in real code, this is naturally small)
        
        barrier_clone.wait(); // Synchronize race timing
        
        // This should succeed and transition to Executing(incarnation + 1)
        statuses_clone.finish_abort(txn_idx, incarnation, true)
    });
    
    let statuses_clone2 = statuses.clone();
    let barrier_clone2 = barrier.clone();
    
    // Thread B: Concurrent worker attempting abort on new incarnation
    let thread_b = thread::spawn(move || {
        barrier_clone2.wait(); // Synchronize race timing
        
        // This should NOT succeed but currently does due to race
        // It marks incarnation+1 for abort before Thread A completes transition
        statuses_clone2.start_abort(txn_idx, incarnation + 1)
    });
    
    let result_a = thread_a.join().unwrap();
    let result_b = thread_b.join().unwrap();
    
    // The race: Thread B's start_abort succeeds
    assert!(result_b.is_ok());
    assert_eq!(result_b.unwrap(), true, "Thread B should succeed in marking for abort");
    
    // Thread A also succeeds, but incarnation is now marked for abort
    assert!(result_a.is_ok());
    
    // Verify race occurred: incarnation is Executing but marked for abort
    let status = statuses.get_status(txn_idx);
    assert_eq!(
        status.status_with_incarnation.lock().status,
        SchedulingStatus::Executing(BTreeSet::new())
    );
    assert_eq!(
        status.next_incarnation_to_abort.load(Ordering::Relaxed),
        incarnation + 2,
        "Race occurred: new incarnation already marked for abort"
    );
    
    // This violates the atomicity guarantee - the newly started incarnation
    // is immediately aborted, causing the commit to fail
}
```

**Notes:**
- The race window exists between lines 695-702 in `finish_abort()` where `to_pending_scheduling()` updates the incarnation but `to_executing()` hasn't completed yet
- The vulnerability is triggered during commit when `abort_pre_final_reexecution()` is called for transactions requiring re-execution
- Different validators experiencing different race outcomes leads to consensus non-determinism, violating the deterministic execution invariant
- The atomicity claim in the documentation (lines 85-86) is violated because `start_abort()` doesn't respect the status lock

### Citations

**File:** aptos-move/block-executor/src/scheduler_status.rs (L49-52)
```rust
          next incarnation, unless start_next_incarnation is true. In this case, the status
          goes directly to 'Executing' without going through 'PendingScheduling'.
        • If the status was 'Executing', it transitions to 'Aborted'. In this case,
          start_next_incarnation must be false.
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L83-86)
```rust
*  [ExecutionStatuses::finish_abort] can be called with start_next_incarnation = true,
   in which case the status must be Executed and it is updated to Executing directly, i.e.
   can be viewed as [ExecutionStatuses::finish_abort] immediately (atomically) followed by
   [ExecutionStatuses::start_executing].
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L531-553)
```rust
    pub(crate) fn start_abort(
        &self,
        txn_idx: TxnIndex,
        incarnation: Incarnation,
    ) -> Result<bool, PanicError> {
        let prev_value = self.statuses[txn_idx as usize]
            .next_incarnation_to_abort
            .fetch_max(incarnation + 1, Ordering::Relaxed);
        match incarnation.cmp(&prev_value) {
            cmp::Ordering::Less => Ok(false),
            cmp::Ordering::Equal => {
                // Increment the counter and clear speculative logs (from the aborted execution).
                counters::SPECULATIVE_ABORT_COUNT.inc();
                clear_speculative_txn_logs(txn_idx as usize);

                Ok(true)
            },
            cmp::Ordering::Greater => Err(code_invariant_error(format!(
                "Try abort incarnation {} > self.next_incarnation_to_abort = {}",
                incarnation, prev_value,
            ))),
        }
    }
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L695-710)
```rust
                    self.to_pending_scheduling(
                        txn_idx,
                        status_guard,
                        new_incarnation,
                        !start_next_incarnation,
                    );
                    if start_next_incarnation {
                        let started_incarnation = self.to_executing(txn_idx, status_guard)?;
                        if Some(aborted_incarnation + 1) != started_incarnation {
                            return Err(code_invariant_error(format!(
                                "Finish abort started incarnation {:?} != expected {}",
                                txn_idx,
                                aborted_incarnation + 1
                            )));
                        }
                    }
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L865-891)
```rust
    // Updates inner status to PendingScheduling with the new incarnation.
    // Also updates the dependency status shortcut to ShouldDefer.
    // The caller must hold the lock on InnerStatus (enforced by the &mut parameter).
    fn to_pending_scheduling(
        &self,
        txn_idx: TxnIndex,
        status_guard: &mut StatusWithIncarnation,
        new_incarnation: Incarnation,
        add_to_schedule: bool,
    ) {
        let status = &self.statuses[txn_idx as usize];
        // Update inner status.
        status_guard.status = SchedulingStatus::PendingScheduling;
        status_guard.incarnation = new_incarnation;

        // Under the lock, update the shortcuts.
        status
            .dependency_shortcut
            .store(DependencyStatus::ShouldDefer as u8, Ordering::Relaxed);

        if add_to_schedule && !status.is_stalled() {
            // Need to schedule the transaction for re-execution. If stalled, then
            // scheduling is deferred to the remove_stall.
            self.execution_queue_manager
                .add_to_schedule(new_incarnation == 1, txn_idx);
        }
    }
```

**File:** aptos-move/block-executor/src/scheduler_status.rs (L893-911)
```rust
    fn to_executing(
        &self,
        txn_idx: TxnIndex,
        status_guard: &mut StatusWithIncarnation,
    ) -> Result<Option<Incarnation>, PanicError> {
        let status = &self.statuses[txn_idx as usize];
        let ret = status_guard.start_executing();
        if ret.is_some() {
            // When status is PendingScheduling the dependency status should be
            // WaitForExecution (default or set by abort under lock).
            status.swap_dependency_status_any(
                &[DependencyStatus::ShouldDefer],
                DependencyStatus::WaitForExecution,
                "start_executing",
            )?;
        }
        Ok(ret)
    }
}
```

**File:** aptos-move/block-executor/src/scheduler_v2.rs (L964-977)
```rust
    pub(crate) fn is_halted_or_aborted(&self, txn_idx: TxnIndex, incarnation: Incarnation) -> bool {
        if self.is_halted() {
            return true;
        }

        if incarnation == 0 {
            // Never interrupt the 0-th incarnation due to an early abort to get the first output
            // estimation (even if it is based on invalidated reads).
            return false;
        }

        self.txn_statuses
            .already_started_abort(txn_idx, incarnation)
    }
```

**File:** aptos-move/block-executor/src/executor.rs (L965-977)
```rust
        if !Self::validate_and_commit_delayed_fields(
            txn_idx,
            versioned_cache,
            last_input_output,
            scheduler.is_v2(),
        )? {
            return Err(code_invariant_error(format!(
                "Delayed field validation after re-execution failed for txn {}",
                txn_idx
            )));
        }

        Ok(())
```

**File:** aptos-move/block-executor/src/executor.rs (L1009-1040)
```rust
        if !Self::validate_and_commit_delayed_fields(
            txn_idx,
            versioned_cache,
            last_input_output,
            scheduler.is_v2(),
        )? {
            // Transaction needs to be re-executed, one final time.
            side_effect_at_commit = true;

            scheduler.abort_pre_final_reexecution::<T, E>(
                txn_idx,
                incarnation,
                last_input_output,
                versioned_cache,
            )?;

            Self::execute_txn_after_commit(
                block.get_txn(txn_idx),
                &block.get_auxiliary_info(txn_idx),
                txn_idx,
                incarnation + 1,
                scheduler,
                versioned_cache,
                last_input_output,
                shared_sync_params.start_shared_counter,
                shared_sync_params.delayed_field_id_counter,
                executor,
                shared_sync_params.base_view,
                global_module_cache,
                runtime_environment,
                &self.config.onchain.block_gas_limit_type,
            )?;
```
