# Audit Report

## Title
Version-Based Vote Splitting in Per-Key JWK Consensus Causes Liveness Failure

## Summary
Byzantine validators can exploit the shared version counter in per-key JWK consensus mode to split honest validator votes across multiple version numbers for the same key update, preventing any single version from reaching quorum. This occurs because completing one key's update advances the issuer's global version counter, invalidating concurrent consensus processes for other keys of the same issuer.

## Finding Description

The JWK consensus system in per-key mode allows validators to reach consensus independently for each key of an OIDC provider. However, all keys of an issuer share a single version counter stored in `ProviderJWKs.version`. When one key's update commits to the chain, it advances this shared version, creating a race condition that splits votes for concurrent key updates.

**The vulnerability manifests in the following code flow:**

1. **Version Coupling in Data Structure**: The `ProviderJWKs` struct includes a shared version field for all keys of an issuer. [1](#0-0) 

2. **Version in Equality Check**: Vote aggregation requires exact match including the version field. [2](#0-1) 

3. **Base Version Assignment**: When validators observe a key change, they create a `KeyLevelUpdate` using the current on-chain version. [3](#0-2) 

4. **Version Conversion**: The `KeyLevelUpdate` is converted to `ProviderJWKs` with `version = base_version + 1`. [4](#0-3) 

5. **Execution-Time Version Check**: Transactions are rejected if the version doesn't match on-chain state. [5](#0-4) 

**Attack Scenario:**

1. OIDC provider updates keys K1, K2, K3 simultaneously; on-chain version is 10
2. All validators observe changes and start consensus with `base_version=10` (target `version=11`)
3. Byzantine validators selectively participate heavily in K1 consensus
4. K1 reaches quorum first and commits, advancing version to 11
5. `ObservedJWKsUpdated` event is emitted but reaches validators at different times
6. Validators who receive the event first call `reset_with_on_chain_state` and restart K2/K3 consensus with `base_version=11` (target `version=12`) [6](#0-5) 
7. Validators who haven't received the event continue voting for K2/K3 with `version=11`
8. Votes are now split between `version=11` and `version=12` for keys K2 and K3
9. Neither version reaches quorum because the `ProviderJWKs` equality check fails when versions differ
10. Byzantine validators repeat this pattern, preventing K2 and K3 from ever reaching quorum

The vulnerability exists because the comparison logic only checks `to_upsert` values, not `base_version`, when determining if consensus should restart: [7](#0-6) 

This means validators can have concurrent consensus processes for the same key with different target versions, splitting the vote pool.

## Impact Explanation

**Severity: High** - This constitutes a significant protocol violation under the Aptos bug bounty program.

**Impact:**
- **Liveness Failure**: Legitimate JWK updates can be indefinitely prevented from reaching consensus, violating the liveness guarantee
- **Keyless Account Impact**: Users relying on keyless accounts with those specific OIDC providers and key IDs will be unable to authenticate
- **Consensus Resource Exhaustion**: Validators waste computational resources on consensus processes that repeatedly fail due to version mismatches
- **Validator Transaction Pool Pollution**: Failed transactions accumulate, degrading system performance

The attack is particularly severe when multiple keys are being updated simultaneously (common for OIDC provider key rotations), as Byzantine validators can cascade failures across all pending updates.

While this doesn't violate consensus safety (no double-spending or chain splits), it does break the availability guarantee for a critical system component. This falls under "Significant protocol violations" (High severity) in the bug bounty criteria.

## Likelihood Explanation

**Likelihood: High**

- **Attacker Requirements**: Requires < 1/3 Byzantine validators, which is the standard security assumption for BFT systems
- **Execution Complexity**: Low - Byzantine validators only need to selectively participate in consensus by timing their votes
- **Attack Window**: The vulnerability is persistent - it exists whenever multiple keys are being updated for the same issuer
- **Real-World Trigger**: OIDC providers regularly perform key rotations affecting multiple keys simultaneously, providing natural exploitation opportunities
- **No Special Privileges Needed**: Byzantine validators don't need any additional capabilities beyond normal consensus participation

The attack is highly realistic because:
1. The per-key consensus mode creates many concurrent consensus processes naturally
2. Network delays already cause validators to receive events at different times
3. Byzantine validators need only amplify this natural timing variance to create sustained vote splits

## Recommendation

**Solution: Implement version-independent vote aggregation or synchronized version updates**

**Option 1: Version-Independent Matching (Recommended)**
Modify the vote aggregation to compare only the key ID and JWK content, not the version:

```rust
// In observation_aggregation/mod.rs
fn add(&self, sender: Author, response: Self::Response) -> anyhow::Result<Option<Self::Aggregated>> {
    // ... existing code ...
    
    // For per-key mode, extract and compare only the key-level content
    let local_key_update = KeyLevelUpdate::try_from_issuer_level_repr(&self.local_view)?;
    let peer_key_update = KeyLevelUpdate::try_from_issuer_level_repr(&peer_view)?;
    
    ensure!(
        local_key_update.issuer == peer_key_update.issuer &&
        local_key_update.kid == peer_key_update.kid &&
        local_key_update.to_upsert == peer_key_update.to_upsert,
        "adding peer observation failed with mismatched key content"
    );
    
    // Use the maximum version to ensure forward progress
    let target_version = std::cmp::max(self.local_view.version, peer_view.version);
    
    // ... continue with aggregation using target_version ...
}
```

**Option 2: Synchronized Version Snapshots**
Take a snapshot of the on-chain version at the start of each observation round and prevent new consensus from starting until all in-flight processes complete:

```rust
// In jwk_manager_per_key.rs
pub struct KeyLevelConsensusManager {
    // ... existing fields ...
    current_version_snapshot: Option<u64>,
    keys_in_flight: HashSet<(Issuer, KID)>,
}

fn process_new_observation(&mut self, issuer: Issuer, jwks: Vec<JWK>) -> Result<()> {
    // Only start new consensus if no keys are in flight
    if !self.keys_in_flight.is_empty() {
        return Ok(()); // Wait for current round to complete
    }
    
    // Lock version for this round
    self.current_version_snapshot = Some(effectively_onchain.version);
    
    // ... proceed with consensus using locked version ...
}
```

**Option 3: Version Tolerance in Execution**
Modify the transaction execution to accept a range of versions rather than exact match:

```rust
// In validator_txns/jwk.rs
// Accept version if it's within a tolerance window
if on_chain.version + 1 != observed.version && 
   on_chain.version != observed.version {
    return Err(Expected(IncorrectVersion));
}
```

**Recommended Approach**: Option 1 is preferred as it maintains the benefits of per-key parallelism while preventing vote splitting. It requires version-independent comparison during vote aggregation but still validates the correct version at execution time.

## Proof of Concept

```rust
// Test case demonstrating vote splitting (pseudo-code for integration test)
#[tokio::test]
async fn test_vote_splitting_attack() {
    // Setup: 4 validators, 1 Byzantine (25% < 33% threshold)
    let mut validators = create_test_validators(4);
    let byzantine_idx = 0;
    
    // Initial state: issuer "Google" at version 10, keys K1 and K2
    setup_onchain_jwks("Google", 10, vec!["K1", "K2"]);
    
    // OIDC provider updates both keys
    let new_k1_jwk = create_test_jwk("K1", "new_value_1");
    let new_k2_jwk = create_test_jwk("K2", "new_value_2");
    
    // All validators observe both changes
    for v in &mut validators {
        v.observe_jwk_change("Google", vec![new_k1_jwk.clone(), new_k2_jwk.clone()]);
    }
    
    // Byzantine validator participates heavily in K1 consensus
    // but delays participation in K2
    validators[byzantine_idx].set_selective_participation(
        vec![("Google", "K1")], // participate
        vec![("Google", "K2")]  // delay
    );
    
    // Allow K1 consensus to complete first
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // Verify K1 committed, version advanced to 11
    assert_eq!(get_onchain_version("Google"), 11);
    
    // Now Byzantine validator starts participating in K2
    validators[byzantine_idx].start_participation("Google", "K2");
    
    // Check that votes are split across versions
    let k2_votes = collect_k2_votes(&validators);
    let version_11_votes = k2_votes.iter().filter(|v| v.target_version == 11).count();
    let version_12_votes = k2_votes.iter().filter(|v| v.target_version == 12).count();
    
    // Neither version has 67% (3 out of 4)
    assert!(version_11_votes < 3);
    assert!(version_12_votes < 3);
    
    // Wait for K2 consensus timeout
    tokio::time::sleep(Duration::from_secs(10)).await;
    
    // Verify K2 failed to reach quorum
    assert_eq!(get_onchain_version("Google"), 11); // Still 11, K2 didn't commit
    
    // Verify K2 consensus is stuck in retry loop
    assert!(validators[1].is_consensus_in_progress("Google", "K2"));
    assert!(validators[2].is_consensus_in_progress("Google", "K2"));
}
```

## Notes

This vulnerability is specific to the **per-key consensus mode** enabled by the `JWK_CONSENSUS_PER_KEY_MODE` feature flag. The traditional per-issuer mode doesn't exhibit this issue because each issuer has only one consensus process at a time, preventing version-based vote splitting.

The root cause is the architectural decision to share a single version counter across all keys of an issuer while allowing parallel consensus per key. This creates a fundamental tension between concurrency and version consistency that Byzantine validators can exploit.

The vulnerability becomes more severe as the number of keys per issuer increases, since more concurrent consensus processes create more opportunities for version conflicts and vote splitting.

### Citations

**File:** types/src/jwks/mod.rs (L122-128)
```rust
#[derive(Clone, Default, Eq, PartialEq, Serialize, Deserialize, CryptoHasher, BCSCryptoHash)]
pub struct ProviderJWKs {
    #[serde(with = "serde_bytes")]
    pub issuer: Issuer,
    pub version: u64,
    pub jwks: Vec<JWKMoveStruct>,
}
```

**File:** types/src/jwks/mod.rs (L342-358)
```rust
    pub fn try_as_issuer_level_repr(&self) -> anyhow::Result<ProviderJWKs> {
        let jwk_repr = self.to_upsert.clone().unwrap_or_else(|| {
            JWK::Unsupported(UnsupportedJWK {
                id: self.kid.clone(),
                payload: DELETE_COMMAND_INDICATOR.as_bytes().to_vec(),
            })
        });
        let version = self
            .base_version
            .checked_add(1)
            .context("KeyLevelUpdate::as_issuer_level_repr failed on version")?;
        Ok(ProviderJWKs {
            issuer: self.issuer.clone(),
            version,
            jwks: vec![JWKMoveStruct::from(jwk_repr)],
        })
    }
```

**File:** crates/aptos-jwk-consensus/src/observation_aggregation/mod.rs (L82-84)
```rust
            self.local_view == peer_view,
            "adding peer observation failed with mismatched view"
        );
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L138-146)
```rust
                        let update = KeyLevelUpdate {
                            issuer: issuer.clone(),
                            base_version: effectively_onchain.version,
                            kid: kid.clone(),
                            to_upsert: Some(y.clone()),
                        };
                        self.maybe_start_consensus(update)
                            .context("process_new_observation failed at upsert consensus init")?;
                    }
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L185-190)
```rust
            Some(ConsensusState::InProgress { my_proposal, .. })
            | Some(ConsensusState::Finished { my_proposal, .. }) => {
                my_proposal.observed.to_upsert == update.to_upsert
            },
            _ => false,
        };
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager_per_key.rs (L244-254)
```rust
        self.states_by_key.retain(|(issuer, _), _| {
            new_onchain_jwks
                .get(issuer)
                .map(|jwks| jwks.version)
                .unwrap_or_default()
                == self
                    .onchain_jwks
                    .get(issuer)
                    .map(|jwks| jwks.version)
                    .unwrap_or_default()
        });
```

**File:** aptos-move/aptos-vm/src/validator_txns/jwk.rs (L127-130)
```rust
        // Check version.
        if on_chain.version + 1 != observed.version {
            return Err(Expected(IncorrectVersion));
        }
```
