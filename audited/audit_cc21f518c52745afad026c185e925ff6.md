# Audit Report

## Title
Missing HTTP Timeout in Telemetry Log Sender Causes Indefinite Event Loop Blocking

## Summary
The `TelemetryLogSender::flush_batch()` function calls `try_send_logs()` which performs HTTP requests without any timeout configuration, unlike other telemetry operations (e.g., Prometheus metrics). When network latency is high or the telemetry service becomes unresponsive, these requests can hang indefinitely, blocking the entire telemetry log event loop and preventing future log flushes. This creates an operational blind spot where monitoring data is lost during critical incidents.

## Finding Description
The telemetry system processes logs through an async event loop that batches and flushes logs periodically. The critical flow is:

1. `TelemetryLogSender::flush_batch()` calls `self.sender.try_send_logs(drained).await` [1](#0-0) 

2. `try_send_logs()` calls `self.post_logs(json.as_bytes()).await` [2](#0-1) 

3. `post_logs()` builds an HTTP POST request WITHOUT a timeout [3](#0-2) 

4. The underlying `reqwest::Client` is created with `reqwest::Client::new()`, which has NO default timeout [4](#0-3) 

In contrast, Prometheus metrics explicitly set an 8-second timeout [5](#0-4) 

The event loop in `TelemetryLogSender::start()` uses `futures::select!` to handle both incoming logs and periodic flushes [6](#0-5) 

When a flush operation blocks on the HTTP request, the entire event loop freezes, causing:
- New logs accumulate in the bounded channel (128 items) [7](#0-6) 
- Once full, logs are silently dropped [8](#0-7) 
- Periodic flushes cannot occur until the hung request completes or TCP times out (typically minutes)

**Attack Scenario:**
An adversary exploiting network conditions or performing targeted network degradation against the telemetry service endpoint can cause validator nodes to lose critical monitoring data during attacks. While this requires network-level capabilities, natural network degradation, DDoS against telemetry infrastructure, or misconfigured telemetry endpoints can trigger the same condition.

## Impact Explanation
This issue creates operational security risks:

1. **Monitoring Blindness During Incidents**: When telemetry logs stop flowing, operators lose visibility into node behavior precisely when debugging is most critical (during attacks or anomalies)

2. **Delayed Incident Response**: Missing telemetry data delays identification of consensus issues, network attacks, or performance degradation

3. **Cascade Effect**: If multiple nodes experience telemetry service issues simultaneously, network-wide monitoring becomes impaired

However, this does NOT affect:
- Consensus operations (telemetry runs in separate tokio task)
- Transaction processing or validation
- State management or storage
- Any critical blockchain invariants

Per Aptos bug bounty criteria, this falls into **Low Severity** ("non-critical implementation bug") rather than Medium, as it doesn't cause state inconsistencies or limited funds loss. The telemetry subsystem is isolated from core blockchain functionality.

## Likelihood Explanation
**Likelihood: Medium-High**

This can occur through:
- Natural network congestion or latency spikes
- Telemetry service maintenance or outages
- Misconfigured telemetry endpoints
- Network path issues between validators and telemetry service

No attacker privileges are required for natural occurrences. Deliberate exploitation would require network-level attack capabilities (which are out of scope per bug bounty rules).

## Recommendation
Add explicit timeout to log POST requests, consistent with the Prometheus metrics timeout pattern:

```rust
async fn post_logs(&self, json: &[u8]) -> Result<Response, anyhow::Error> {
    debug!("Sending logs");

    let mut gzip_encoder = GzEncoder::new(Vec::new(), Compression::default());
    gzip_encoder.write_all(json)?;
    let compressed_bytes = gzip_encoder.finish()?;

    // Send the request with timeout (consistent with prometheus metrics)
    let response = self
        .send_authenticated_request(
            self.client
                .post(self.build_path("ingest/logs")?)
                .header(CONTENT_ENCODING, "gzip")
                .body(compressed_bytes)
                .timeout(Duration::from_secs(PROMETHEUS_PUSH_METRICS_TIMEOUT_SECS)), // ADD THIS
        )
        .await?;

    error_for_status_with_body(response).await
}
```

Alternatively, set a default timeout when creating the reqwest client.

## Proof of Concept

```rust
#[tokio::test]
async fn test_log_sender_blocks_on_slow_network() {
    use std::time::Duration;
    use tokio::time::sleep;
    use httpmock::MockServer;
    
    // Create a mock server that delays responses indefinitely
    let server = MockServer::start();
    let mock = server.mock(|when, then| {
        when.method("POST")
            .path("/api/v1/ingest/logs");
        // Simulate hung connection by never responding
        then.delay(Duration::from_secs(3600));
    });

    let node_config = NodeConfig::default();
    let telemetry_sender = TelemetrySender::new(
        Url::parse(&server.base_url()).unwrap(),
        ChainId::default(),
        &node_config,
    );
    
    // Create channel and log sender
    let (tx, rx) = mpsc::channel(128);
    let log_sender = TelemetryLogSender::new(telemetry_sender);
    
    // Spawn the log sender task
    let handle = tokio::spawn(log_sender.start(rx));
    
    // Send a log that will trigger a flush
    tx.send(TelemetryLog::Log("test".to_string())).await.unwrap();
    
    // Wait briefly
    sleep(Duration::from_millis(100)).await;
    
    // Send many more logs - they should accumulate since flush is blocked
    for i in 0..200 {
        // After 128 logs, channel will be full and sends will fail
        let result = tx.try_send(TelemetryLog::Log(format!("log_{}", i)));
        if i >= 128 {
            assert!(result.is_err()); // Channel should be full
        }
    }
    
    handle.abort(); // Clean up
}
```

## Notes

While this is a genuine implementation bug with operational impact, it does **not meet the Aptos bug bounty's Medium severity criteria** because:

1. It doesn't affect consensus, execution, storage, or any critical blockchain functionality
2. Telemetry runs in an isolated tokio task and cannot block core validator operations
3. The impact is limited to loss of monitoring data, not state corruption or funds

This is more appropriately classified as **Low Severity** under "non-critical implementation bugs" or as a reliability/operational issue rather than a security vulnerability. The fix is straightforward (add timeout) and should be implemented for operational hygiene, but it doesn't represent an exploitable security risk that meets Medium+ severity thresholds.

### Citations

**File:** crates/aptos-telemetry/src/telemetry_log_sender.rs (L69-74)
```rust
    pub async fn flush_batch(&mut self) {
        if !self.batch.is_empty() {
            let drained = self.drain_batch();
            self.sender.try_send_logs(drained).await;
        }
    }
```

**File:** crates/aptos-telemetry/src/telemetry_log_sender.rs (L80-88)
```rust
        loop {
            ::futures::select! {
                log = rx.select_next_some() => {
                    self.handle_next_log(log).await;
                },
                _ = interval.select_next_some() => {
                    self.flush_batch().await;
                },
            }
```

**File:** crates/aptos-telemetry/src/sender.rs (L66-66)
```rust
        let reqwest_client = reqwest::Client::new();
```

**File:** crates/aptos-telemetry/src/sender.rs (L144-144)
```rust
                    .timeout(Duration::from_secs(PROMETHEUS_PUSH_METRICS_TIMEOUT_SECS)),
```

**File:** crates/aptos-telemetry/src/sender.rs (L176-193)
```rust
    pub async fn try_send_logs(&self, batch: Vec<String>) {
        if let Ok(json) = serde_json::to_string(&batch) {
            let len = json.len();

            match self.post_logs(json.as_bytes()).await {
                Ok(_) => {
                    increment_log_ingest_successes_by(batch.len() as u64);
                    debug!("Sent log of length: {}", len);
                },
                Err(error) => {
                    increment_log_ingest_failures_by(batch.len() as u64);
                    debug!("Failed send log of length: {} with error: {}", len, error);
                },
            }
        } else {
            debug!("Failed json serde of batch: {:?}", batch);
        }
    }
```

**File:** crates/aptos-telemetry/src/sender.rs (L195-214)
```rust
    async fn post_logs(&self, json: &[u8]) -> Result<Response, anyhow::Error> {
        debug!("Sending logs");

        let mut gzip_encoder = GzEncoder::new(Vec::new(), Compression::default());
        gzip_encoder.write_all(json)?;
        let compressed_bytes = gzip_encoder.finish()?;

        // Send the request and wait for a response
        let response = self
            .send_authenticated_request(
                self.client
                    .post(self.build_path("ingest/logs")?)
                    .header(CONTENT_ENCODING, "gzip")
                    .body(compressed_bytes),
            )
            .await?;

        // Process the result
        error_for_status_with_body(response).await
    }
```

**File:** aptos-node/src/logger.rs (L13-13)
```rust
const TELEMETRY_LOG_INGEST_BUFFER_SIZE: usize = 128;
```

**File:** crates/aptos-logger/src/telemetry_log_writer.rs (L29-43)
```rust
    pub fn write(&mut self, log: String) -> std::io::Result<usize> {
        let len = log.len();
        match self.tx.try_send(TelemetryLog::Log(log)) {
            Ok(_) => Ok(len),
            Err(err) => {
                if err.is_full() {
                    APTOS_LOG_INGEST_WRITER_FULL.inc_by(len as u64);
                    Err(Error::new(ErrorKind::WouldBlock, "Channel full"))
                } else {
                    APTOS_LOG_INGEST_WRITER_DISCONNECTED.inc_by(len as u64);
                    Err(Error::new(ErrorKind::ConnectionRefused, "Disconnected"))
                }
            },
        }
    }
```
