# Audit Report

## Title
Consensus Observer Block Loss Due to Off-By-One Error in Pending Block Retention Logic

## Summary
The `remove_ready_block()` function in the consensus observer incorrectly drops pipelined ordered blocks when payloads arrive out-of-order, causing observers to lose valid blocks and fall behind consensus, requiring frequent fallbacks to state sync.

## Finding Description

The consensus observer mechanism allows non-validator nodes to stay synchronized with consensus by receiving `OrderedBlock` messages and their corresponding block payloads from consensus publishers. When an `OrderedBlock` (which can contain multiple pipelined blocks spanning multiple rounds) arrives without all its payloads, it's stored as a pending block. When payloads arrive later, `remove_ready_block()` checks if any pending blocks are now complete and ready to process. [1](#0-0) 

The vulnerability occurs in the logic that determines whether to retain or drop incomplete pending blocks. The function uses a strict greater-than comparison that incorrectly drops blocks whose last round equals the received payload round: [2](#0-1) 

**Attack Scenario:**

1. Observer receives `OrderedBlock` containing pipelined blocks at rounds [100, 101, 102]
2. Block is stored as pending (indexed by first block's round: 100)
3. Payload for round 101 arrives (before payloads for 100 and 102)
4. `remove_ready_block(epoch, 101)` is called
5. Function calculates `split_round = 102`
6. OrderedBlock at round 100 stays in `blocks_without_payloads` (since 100 < 102)
7. Block is popped and checked: only payload 101 exists, not 100 or 102
8. `last_pending_block_round = 102`
9. Check `102 > 101` passes, block is retained ✓

**But if the block spans [100, 101]:**

1. Same setup, block spans rounds [100, 101]
2. Payload for round 101 arrives
3. `remove_ready_block(epoch, 101)` is called
4. `split_round = 102`, block at 100 is popped
5. `last_pending_block_round = 101`
6. Check `101 > 101` **FAILS** - block is silently dropped ✗
7. When payload for round 100 arrives later, there's no block to match it

The existing test suite actually demonstrates this behavior as expected: [3](#0-2) 

This test shows that when payloads arrive for all rounds of a pipelined block **except the first**, and we process the last round's payload, the pending block is dropped (store becomes empty at line 934) even though it's incomplete.

**Caller Context:**

When block payloads are received and verified, the observer attempts to process ready pending blocks: [4](#0-3) [5](#0-4) 

The `None` return is handled silently - no error, no retry, the block is permanently lost.

**Invariant Violation:**

This breaks the liveness guarantee that consensus observers should stay synchronized with validators. When observers lose blocks due to payload reordering, they stop making progress, triggering the fallback mechanism: [6](#0-5) 

## Impact Explanation

**Severity: High** (per Aptos Bug Bounty criteria: "Validator node slowdowns" and "Significant protocol violations")

This vulnerability affects all nodes running as consensus observers (typically fullnodes and validator fullnodes):

1. **Liveness Impact**: Observers repeatedly lose blocks and stop making progress, requiring fallback to state sync after the progress threshold expires
2. **Performance Degradation**: State sync is significantly less efficient than consensus observation, increasing latency and resource usage
3. **Network Resource Waste**: Both bandwidth (sending payloads that can't be used) and computational resources (repeated state sync operations)
4. **Availability Concerns**: Observers may be temporarily unavailable during fallback periods

While this doesn't affect consensus safety (observers don't participate in voting), it severely impacts the availability and reliability of observer nodes, which are critical infrastructure for dApps, indexers, and other ecosystem services.

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability can be triggered in two ways:

1. **Natural Network Conditions** (High Probability):
   - Network packet reordering is common in distributed systems
   - UDP-based protocols or different routing paths can cause out-of-order delivery
   - Pipelined blocks spanning multiple rounds are a normal consensus operation
   - No special conditions required - happens during regular operation

2. **Malicious Trigger** (Trivial):
   - Any consensus publisher can deliberately send block payloads in reverse round order
   - Requires no special privileges or Byzantine behavior
   - Can be sustained to keep observers in a degraded state

The vulnerability is actively being tested in the codebase as expected behavior, indicating it's likely already occurring in production deployments.

## Recommendation

Change line 224 from a strict greater-than to greater-than-or-equal comparison:

```rust
// Before (vulnerable):
if last_pending_block_round > received_payload_round {
    blocks_at_higher_rounds.insert(epoch_and_round, pending_block);
}

// After (fixed):
if last_pending_block_round >= received_payload_round {
    blocks_at_higher_rounds.insert(epoch_and_round, pending_block);
}
```

This ensures that blocks whose last round equals the received payload round are retained rather than dropped, allowing them to be completed when earlier payloads arrive.

Additionally, update the test at line 934 to expect the block to remain (not be dropped):

```rust
// This test should be updated to reflect the fix
// Line 934 should verify the block still exists, not that it's been removed
```

## Proof of Concept

```rust
#[test]
fn test_vulnerability_block_dropped_on_equal_round() {
    use aptos_consensus_types::{block::Block, block_data::{BlockData, BlockType}, pipelined_block::PipelinedBlock, quorum_cert::QuorumCert};
    use aptos_crypto::HashValue;
    use aptos_types::{block_info::BlockInfo, aggregate_signature::AggregateSignature, ledger_info::{LedgerInfo, LedgerInfoWithSignatures}};
    
    // Setup: Create a pending block store
    let consensus_observer_config = ConsensusObserverConfig {
        max_num_pending_blocks: 10,
        ..ConsensusObserverConfig::default()
    };
    let mut pending_block_store = PendingBlockStore::new(consensus_observer_config);
    let mut block_payload_store = BlockPayloadStore::new(consensus_observer_config);
    
    let epoch = 1;
    
    // Create a pipelined block spanning rounds [100, 101]
    let block_100 = create_test_block(epoch, 100);
    let block_101 = create_test_block(epoch, 101);
    
    let ordered_block = OrderedBlock::new(
        vec![
            Arc::new(PipelinedBlock::new_ordered(block_100.clone(), OrderedBlockWindow::empty())),
            Arc::new(PipelinedBlock::new_ordered(block_101.clone(), OrderedBlockWindow::empty())),
        ],
        create_test_ledger_info(epoch, 101),
    );
    
    // Insert the pending block
    let observed_block = ObservedOrderedBlock::new_for_testing(ordered_block.clone());
    let pending_block = PendingBlockWithMetadata::new_with_arc(
        PeerNetworkId::random(),
        Instant::now(),
        observed_block,
    );
    pending_block_store.insert_pending_block(pending_block);
    
    // Insert payload ONLY for round 101 (not round 100)
    let payload_101 = BlockPayload::new(block_101.block_info(), BlockTransactionPayload::empty());
    block_payload_store.insert_block_payload(payload_101, true);
    
    // Attempt to remove ready block for round 101
    let ready_block = pending_block_store.remove_ready_block(
        epoch,
        101,
        &mut block_payload_store,
    );
    
    // BUG: The block is dropped even though we're still waiting for payload 100
    assert!(ready_block.is_none(), "Block should not be ready yet");
    
    // VULNERABILITY: Check if the block is still in the store
    // With the bug, it's been dropped. With the fix, it should still be there.
    let block_still_pending = pending_block_store.existing_pending_block(&ordered_block);
    
    // This assertion FAILS with the current code (block was dropped)
    // It should PASS with the fix (block retained)
    assert!(
        block_still_pending,
        "VULNERABILITY: Block was incorrectly dropped! It should be retained until payload for round 100 arrives."
    );
}

fn create_test_block(epoch: u64, round: u64) -> Block {
    let block_info = BlockInfo::new(epoch, round, HashValue::random(), HashValue::random(), round, 0, None);
    let block_data = BlockData::new_for_testing(epoch, round, round, QuorumCert::dummy(), BlockType::Genesis);
    Block::new_for_testing(block_info.id(), block_data, None)
}

fn create_test_ledger_info(epoch: u64, round: u64) -> LedgerInfoWithSignatures {
    LedgerInfoWithSignatures::new(
        LedgerInfo::new(BlockInfo::random_with_epoch(epoch, round), HashValue::random()),
        AggregateSignature::empty(),
    )
}
```

This test demonstrates that with the current implementation, a pipelined block spanning rounds [100, 101] is incorrectly dropped when the payload for round 101 arrives before the payload for round 100, making it impossible to process the block even when the missing payload eventually arrives.

### Citations

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L200-256)
```rust
    pub fn remove_ready_block(
        &mut self,
        received_payload_epoch: u64,
        received_payload_round: Round,
        block_payload_store: &mut BlockPayloadStore,
    ) -> Option<Arc<PendingBlockWithMetadata>> {
        // Calculate the round at which to split the blocks
        let split_round = received_payload_round.saturating_add(1);

        // Split the blocks at the epoch and round
        let mut blocks_at_higher_rounds = self
            .blocks_without_payloads
            .split_off(&(received_payload_epoch, split_round));

        // Check if the last block is ready (this should be the only ready block).
        // Any earlier blocks are considered out-of-date and will be dropped.
        let mut ready_block = None;
        if let Some((epoch_and_round, pending_block)) = self.blocks_without_payloads.pop_last() {
            // If all payloads exist for the block, then the block is ready
            if block_payload_store.all_payloads_exist(pending_block.ordered_block().blocks()) {
                ready_block = Some(pending_block);
            } else {
                // Otherwise, check if we're still waiting for higher payloads for the block
                let last_pending_block_round = pending_block.ordered_block().last_block().round();
                if last_pending_block_round > received_payload_round {
                    blocks_at_higher_rounds.insert(epoch_and_round, pending_block);
                }
            }
        }

        // Check if any out-of-date blocks are going to be dropped
        if !self.blocks_without_payloads.is_empty() {
            info!(
                LogSchema::new(LogEntry::ConsensusObserver).message(&format!(
                    "Dropped {:?} out-of-date pending blocks before epoch and round: {:?}",
                    self.blocks_without_payloads.len(),
                    (received_payload_epoch, received_payload_round)
                ))
            );
        }

        // TODO: optimize this flow!

        // Clear all blocks from the pending block stores
        self.clear_missing_blocks();

        // Update the pending block stores to only include the blocks at higher rounds
        self.blocks_without_payloads = blocks_at_higher_rounds;
        for pending_block in self.blocks_without_payloads.values() {
            let first_block = pending_block.ordered_block().first_block();
            self.blocks_without_payloads_by_hash
                .insert(first_block.id(), pending_block.clone());
        }

        // Return the ready block (if one exists)
        ready_block
    }
```

**File:** consensus/src/consensus_observer/observer/pending_blocks.rs (L911-938)
```rust
        // Incrementally insert and process payloads for the last block (except one)
        let last_block = pending_blocks.last().unwrap().clone();
        for block in last_block.blocks().clone() {
            // Insert the block only if this is not the first block
            let payload_round = block.round();
            if payload_round != last_block.first_block().round() {
                let block_payload =
                    BlockPayload::new(block.block_info(), BlockTransactionPayload::empty());
                block_payload_store.insert_block_payload(block_payload, true);
            }

            // Attempt to remove the block (which might not be ready)
            let ready_block = pending_block_store.lock().remove_ready_block(
                current_epoch,
                payload_round,
                &mut block_payload_store,
            );

            // The block should not be ready
            assert!(ready_block.is_none());

            // Verify that the block still remains or has been removed on the last insert
            if payload_round == last_block.last_block().round() {
                verify_pending_blocks(pending_block_store.clone(), 0, &vec![]);
            } else {
                verify_pending_blocks(pending_block_store.clone(), 1, &vec![last_block.clone()]);
            }
        }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L340-353)
```rust
    /// Orders any ready pending blocks for the given epoch and round
    async fn order_ready_pending_block(&mut self, block_epoch: u64, block_round: Round) {
        // Remove any ready pending block
        let pending_block_with_metadata = self
            .observer_block_data
            .lock()
            .remove_ready_pending_block(block_epoch, block_round);

        // Process the ready ordered block (if it exists)
        if let Some(pending_block_with_metadata) = pending_block_with_metadata {
            self.process_ordered_block(pending_block_with_metadata)
                .await;
        }
    }
```

**File:** consensus/src/consensus_observer/observer/consensus_observer.rs (L432-438)
```rust
        // Check if there are blocks that were missing payloads but are
        // now ready because of the new payload. Note: this should only
        // be done if the payload has been verified correctly.
        if verified_payload {
            self.order_ready_pending_block(block_epoch, block_round)
                .await;
        }
```

**File:** consensus/src/consensus_observer/observer/fallback_manager.rs (L94-116)
```rust
        // Verify that the synced version is increasing appropriately
        let (highest_synced_version, highest_version_timestamp) =
            self.highest_synced_version_and_time;
        if latest_ledger_info_version <= highest_synced_version {
            // The synced version hasn't increased. Check if we should enter fallback mode.
            let duration_since_highest_seen = time_now.duration_since(highest_version_timestamp);
            let fallback_threshold = Duration::from_millis(
                self.consensus_observer_config
                    .observer_fallback_progress_threshold_ms,
            );
            if duration_since_highest_seen > fallback_threshold {
                Err(Error::ObserverProgressStopped(format!(
                    "Consensus observer is not making progress! Highest synced version: {}, elapsed: {:?}",
                    highest_synced_version, duration_since_highest_seen
                )))
            } else {
                Ok(()) // We haven't passed the fallback threshold yet
            }
        } else {
            // The synced version has increased. Update the highest synced version and time.
            self.highest_synced_version_and_time = (latest_ledger_info_version, time_now);
            Ok(())
        }
```
