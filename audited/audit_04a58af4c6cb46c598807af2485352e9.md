# Audit Report

## Title
Validator Node Crash via Consensus Observer Configuration Bypass Leading to Concurrent Database Commits

## Summary
The `ConsensusObserverConfig::optimize()` function fails to validate that `observer_enabled` is disabled for validator nodes. When a validator is deployed with a partial configuration containing `observer_enabled: true`, the optimization logic enables `publisher_enabled` without disabling `observer_enabled`, resulting in both the consensus runtime and consensus observer runtime executing simultaneously. This causes concurrent database commits that trigger a panic with "Concurrent committing detected," crashing the validator node and causing network liveness failures.

## Finding Description

The vulnerability exists in the configuration optimization logic for consensus observer settings. [1](#0-0) 

When serde deserializes a partial configuration with `#[serde(default)]`, missing fields are filled with default values. [2](#0-1) 

The optimization function checks whether fields were manually set by examining the original YAML: [3](#0-2) 

For validator nodes, the function only checks if `publisher_enabled` was manually set, and enables it if not: [4](#0-3) 

**Critical flaw:** The logic does NOT check or override `observer_enabled` for validators. If an operator provides a config file with only `observer_enabled: true`, the final state becomes:
- `observer_enabled = true` (from user config)
- `publisher_enabled = true` (from optimizer)

This triggers creation of both consensus observer and consensus runtimes. The consensus observer creates its own execution client: [5](#0-4) 

The normal consensus runtime also creates an execution client: [6](#0-5) 

Both `BlockExecutor` instances point to the same `AptosDB` database but have **separate** execution locks. When both attempt to commit blocks simultaneously, they encounter AptosDB's concurrency detection mechanism: [7](#0-6) 

The `try_lock().expect()` pattern is designed to detect concurrent commits and **panic immediately** rather than allowing state corruption: [8](#0-7) 

## Impact Explanation

This is a **HIGH severity** vulnerability per the Aptos bug bounty criteria:

1. **Validator Node Crash**: The panic from "Concurrent committing detected" causes immediate validator node termination
2. **Network Liveness Failure**: If multiple validators are misconfigured this way, the network loses consensus participants, potentially falling below the 2/3 threshold needed for progress
3. **Deterministic Execution Violation**: Before crashing, the two execution paths may process different blocks, violating the invariant that all validators produce identical state roots
4. **Availability Impact**: Crashed validators cannot participate in consensus, reducing network throughput and increasing latency

The vulnerability meets the "Validator node slowdowns" and "Significant protocol violations" criteria for High severity ($50,000 tier), as it causes complete validator failure rather than just slowdown.

## Likelihood Explanation

**Likelihood: Medium to High**

- **Easy to Trigger**: Requires only a partial configuration file with `observer_enabled: true`
- **Common Pattern**: Operators frequently use partial configs and rely on defaults, especially when following example configurations
- **No Malicious Intent Required**: This can happen accidentally during deployment or configuration updates
- **Persistent**: Once deployed, the misconfiguration persists until manually corrected
- **Detection Difficulty**: The issue may not manifest immediately if consensus and observer don't attempt concurrent commits initially, making it hard to catch in testing

The vulnerability is more likely to occur during:
- Initial validator deployments using incomplete configs
- Configuration migrations when enabling consensus observer features
- Copy-paste errors from fullnode configs to validator configs

## Recommendation

Add explicit validation in the `optimize()` function to ensure validators cannot have `observer_enabled=true`:

```rust
match node_type {
    NodeType::Validator => {
        // Validators should NEVER run as observers - force disable it
        if observer_manually_set && consensus_observer_config.observer_enabled {
            return Err(Error::ConfigError(
                "Validators cannot have observer_enabled=true. Only publisher_enabled is allowed.".into()
            ));
        }
        // Always ensure observer is disabled for validators
        if consensus_observer_config.observer_enabled {
            consensus_observer_config.observer_enabled = false;
            modified_config = true;
        }
        
        if ENABLE_ON_VALIDATORS && !publisher_manually_set {
            consensus_observer_config.publisher_enabled = true;
            modified_config = true;
        }
    },
    // ... rest of match arms
}
```

Additionally, add a runtime check when creating consensus observer: [9](#0-8) 

Add validation:
```rust
if !node_config.consensus_observer.observer_enabled {
    return;
}

// Add this check
if node_config.validator_network.is_some() {
    panic!("FATAL: Validators cannot run consensus observer! This indicates a configuration error.");
}
```

## Proof of Concept

Create a file `validator_misconfigured.yaml`:
```yaml
base:
  role: "validator"
  
consensus_observer:
  observer_enabled: true
  # publisher_enabled is omitted, will be set to true by optimizer

validator_network:
  network_id: "validator"
  mutual_authentication: true
```

Start the validator node:
```bash
cargo run -p aptos-node -- -f validator_misconfigured.yaml
```

**Expected behavior:** Node starts both consensus and consensus observer runtimes, and crashes with:
```
thread 'tokio-runtime-worker' panicked at 'Concurrent committing detected.', storage/aptosdb/src/db/aptosdb_writer.rs:92:18
```

The crash occurs when both execution paths attempt to commit blocks to the database simultaneously, triggering the `try_lock().expect()` panic in the commit mechanism.

---

**Notes:**

This vulnerability demonstrates how serde's `#[serde(default)]` attribute combined with asymmetric validation logic can create dangerous security gaps. The root cause is the optimization function's failure to enforce the security invariant that validators must not run as consensus observers. The fix requires both configuration-time validation and runtime assertions to prevent this misconfiguration from ever reaching production.

### Citations

**File:** config/src/config/consensus_observer_config.rs (L19-21)
```rust
#[derive(Clone, Copy, Debug, Deserialize, PartialEq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct ConsensusObserverConfig {
```

**File:** config/src/config/consensus_observer_config.rs (L63-85)
```rust
impl Default for ConsensusObserverConfig {
    fn default() -> Self {
        Self {
            observer_enabled: false,
            publisher_enabled: false,
            max_network_channel_size: 1000,
            max_parallel_serialization_tasks: num_cpus::get(), // Default to the number of CPUs
            network_request_timeout_ms: 5_000,                 // 5 seconds
            garbage_collection_interval_ms: 60_000,            // 60 seconds
            max_num_pending_blocks: 150, // 150 blocks (sufficient for existing production networks)
            progress_check_interval_ms: 5_000, // 5 seconds
            max_concurrent_subscriptions: 2, // 2 streams should be sufficient
            max_subscription_sync_timeout_ms: 15_000, // 15 seconds
            max_subscription_timeout_ms: 15_000, // 15 seconds
            subscription_peer_change_interval_ms: 180_000, // 3 minutes
            subscription_refresh_interval_ms: 600_000, // 10 minutes
            observer_fallback_duration_ms: 600_000, // 10 minutes
            observer_fallback_startup_period_ms: 60_000, // 60 seconds
            observer_fallback_progress_threshold_ms: 10_000, // 10 seconds
            observer_fallback_sync_lag_threshold_ms: 15_000, // 15 seconds
        }
    }
}
```

**File:** config/src/config/consensus_observer_config.rs (L106-107)
```rust
        let observer_manually_set = !local_observer_config_yaml["observer_enabled"].is_null();
        let publisher_manually_set = !local_observer_config_yaml["publisher_enabled"].is_null();
```

**File:** config/src/config/consensus_observer_config.rs (L112-118)
```rust
            NodeType::Validator => {
                if ENABLE_ON_VALIDATORS && !publisher_manually_set {
                    // Only enable the publisher for validators
                    consensus_observer_config.publisher_enabled = true;
                    modified_config = true;
                }
            },
```

**File:** consensus/src/consensus_provider.rs (L65-97)
```rust
    let execution_proxy = ExecutionProxy::new(
        Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(aptos_db)),
        txn_notifier,
        state_sync_notifier,
        node_config.transaction_filters.execution_filter.clone(),
        node_config.consensus.enable_pre_commit,
        None,
    );

    let time_service = Arc::new(ClockTimeService::new(runtime.handle().clone()));

    let (timeout_sender, timeout_receiver) =
        aptos_channels::new(1_024, &counters::PENDING_ROUND_TIMEOUTS);
    let (self_sender, self_receiver) =
        aptos_channels::new_unbounded(&counters::PENDING_SELF_MESSAGES);
    let consensus_network_client = ConsensusNetworkClient::new(network_client);
    let bounded_executor = BoundedExecutor::new(
        node_config.consensus.num_bounded_executor_tasks as usize,
        runtime.handle().clone(),
    );
    let rand_storage = Arc::new(RandDb::new(node_config.storage.dir()));

    let execution_client = Arc::new(ExecutionProxyClient::new(
        node_config.consensus.clone(),
        Arc::new(execution_proxy),
        node_config.validator_network.as_ref().unwrap().peer_id(),
        self_sender.clone(),
        consensus_network_client.clone(),
        bounded_executor.clone(),
        rand_storage.clone(),
        node_config.consensus_observer,
        consensus_publisher.clone(),
    ));
```

**File:** consensus/src/consensus_provider.rs (L152-182)
```rust
    let execution_client = if node_config.consensus_observer.observer_enabled {
        // Create the execution proxy
        let txn_notifier = Arc::new(MempoolNotifier::new(
            consensus_to_mempool_sender.clone(),
            node_config.consensus.mempool_executed_txn_timeout_ms,
        ));
        let execution_proxy = ExecutionProxy::new(
            Arc::new(BlockExecutor::<AptosVMBlockExecutor>::new(aptos_db.clone())),
            txn_notifier,
            state_sync_notifier,
            node_config.transaction_filters.execution_filter.clone(),
            node_config.consensus.enable_pre_commit,
            None,
        );

        // Create the execution proxy client
        let bounded_executor =
            BoundedExecutor::new(32, consensus_observer_runtime.handle().clone());
        let rand_storage = Arc::new(RandDb::new(node_config.storage.dir()));
        let execution_proxy_client = Arc::new(ExecutionProxyClient::new(
            node_config.consensus.clone(),
            Arc::new(execution_proxy),
            AccountAddress::ONE,
            self_sender.clone(),
            consensus_network_client,
            bounded_executor,
            rand_storage.clone(),
            node_config.consensus_observer,
            consensus_publisher.clone(),
        ));
        execution_proxy_client as Arc<dyn TExecutionClient>
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L89-92)
```rust
            let _lock = self
                .commit_lock
                .try_lock()
                .expect("Concurrent committing detected.");
```

**File:** storage/aptosdb/src/db/mod.rs (L34-37)
```rust
    /// This is just to detect concurrent calls to `pre_commit_ledger()`
    pre_commit_lock: std::sync::Mutex<()>,
    /// This is just to detect concurrent calls to `commit_ledger()`
    commit_lock: std::sync::Mutex<()>,
```

**File:** aptos-node/src/consensus.rs (L220-223)
```rust
    // If the observer is not enabled, return early
    if !node_config.consensus_observer.observer_enabled {
        return;
    }
```
