# Audit Report

## Title
State Restore Progress Persistence Causes Storage Usage Divergence Across Nodes

## Summary
The state restoration mechanism in `StateValueRestore::add_chunk()` blindly trusts stale usage values from previous failed restore attempts without verifying data consistency. When KV data is rolled back or cleaned up but progress metadata persists, nodes calculate different storage usage values, violating the deterministic execution invariant and causing consensus divergence.

## Finding Description

The vulnerability exists in how state snapshot restoration tracks progress across failed attempts. At line 107 of `state_restore/mod.rs`, the code retrieves the previous usage value from persisted progress: [1](#0-0) 

The progress is loaded from the database without any validation: [2](#0-1) 

The skipping logic assumes chunks can be safely skipped based on the saved `key_hash`: [3](#0-2) 

**Critical Issue**: Progress is persisted atomically with each chunk write: [4](#0-3) 

However, there is **no cleanup mechanism** for this progress metadata when:
1. A restore fails and KV data is rolled back
2. A restore is aborted and restarted
3. Database is restored from backup

The `kv_finish()` method only writes the final usage, it does NOT delete progress entries: [5](#0-4) 

**Attack Scenario**:

1. **Initial state**: Nodes A and B start state snapshot restoration for version V
2. **Divergent progress**: 
   - Node A processes chunks 1-3, saves progress with `usage_A = 300` and `key_hash_A = hash(key3)`
   - Node B processes chunks 1-5, saves progress with `usage_B = 500` and `key_hash_B = hash(key5)`
3. **Failure occurs**: Network issue, crash, or manual intervention
4. **Partial rollback**: KV data is rolled back (e.g., via database restore or pruning), but progress metadata persists because there's no cleanup mechanism
5. **Retry restoration**: Both nodes receive chunks 1-6 again
6. **Node A execution**:
   - Loads progress: `usage = 300`, `key_hash = hash(key3)`
   - Chunks 1-3 are skipped (all keys have `hash(k) <= hash(key3)`)
   - Processes chunks 4-6: `final_usage = 300 + size(chunks 4-6)`
7. **Node B execution**:
   - Loads progress: `usage = 500`, `key_hash = hash(key5)`
   - Chunks 1-5 are skipped (all keys have `hash(k) <= hash(key5)`)
   - Processes chunk 6 only: `final_usage = 500 + size(chunk 6)`
8. **Result**: Node A and Node B calculate different final usage values for the same version

The `get_progress()` consistency check between main DB and indexer DB is insufficient: [6](#0-5) 

This check allows the indexer DB to be ahead of main DB (case at line 1340), which can lead to inconsistent state when writes to the indexer succeed but main DB commits fail.

## Impact Explanation

This vulnerability meets **Medium severity** criteria per the Aptos bug bounty program:

- **State inconsistencies requiring intervention**: Different nodes calculate different storage usage values for the same state snapshot version, breaking deterministic execution
- **Consensus risk**: Storage usage affects state root calculations. If usage diverges, nodes will compute different state roots, potentially causing consensus failures or requiring manual intervention to reconcile
- **Not immediately critical**: Does not directly cause fund loss or total network failure, but creates state divergence that undermines blockchain integrity

The vulnerability affects the **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks." When storage usage calculations diverge, state roots will differ even for identical state data.

## Likelihood Explanation

**Likelihood: Medium-High**

This vulnerability can be triggered in several realistic scenarios:

1. **Failed state sync operations**: Common during network partitions or node crashes
2. **Database maintenance**: Operators performing backups/restores may inadvertently leave stale progress
3. **Version upgrades**: Software updates that restart nodes mid-restoration
4. **Storage pruning**: Automated pruning may clear KV data but not progress metadata

The lack of any cleanup mechanism for restore progress makes this issue inevitable in production environments where restore operations commonly fail or are interrupted.

## Recommendation

**Immediate fixes required**:

1. **Add progress cleanup on restore start**: Before beginning a restore, delete any existing progress for that version to ensure clean state

2. **Validate progress against actual data**: On loading progress, verify that the referenced `key_hash` actually exists in the database with the claimed usage

3. **Make writes atomic**: Ensure indexer DB and main DB are written atomically, or add proper recovery logic for partial write failures

4. **Add progress cleanup on completion**: Delete progress metadata after successful restoration

**Code fix example** (add to `StateValueRestore::new`):

```rust
pub fn new<D: 'static + StateValueWriter<K, V>>(db: Arc<D>, version: Version) -> Self {
    // Clean up any stale progress from previous failed attempts
    db.clear_progress(version).expect("Failed to clear stale progress");
    Self { version, db }
}
```

Add to `StateValueWriter` trait:

```rust
fn clear_progress(&self, version: Version) -> Result<()>;
```

And implement cleanup in `finish()`:

```rust
pub fn finish(self) -> Result<()> {
    let progress = self.db.get_progress(self.version)?;
    self.db.kv_finish(
        self.version,
        progress.map_or(StateStorageUsage::zero(), |p| p.usage),
    )?;
    // Clean up progress metadata after successful completion
    self.db.clear_progress(self.version)?;
    Ok(())
}
```

## Proof of Concept

```rust
// Reproduction test (add to storage/aptosdb/src/state_restore/restore_test.rs)
#[test]
fn test_stale_progress_causes_usage_divergence() {
    use aptos_crypto::{hash::CryptoHash, HashValue};
    use aptos_types::state_store::state_storage_usage::StateStorageUsage;
    
    // Simulate two nodes with different stale progress
    let (mut store_a, mut store_b) = setup_two_nodes();
    let version = 100;
    
    // Node A: Simulate stale progress from processing 3 chunks (300 bytes)
    let chunk_1_2_3 = create_chunks(1, 3, 100); // 3 chunks, 100 bytes each
    for chunk in chunk_1_2_3.clone() {
        store_a.add_chunk(chunk).unwrap();
    }
    let progress_a = store_a.db.get_progress(version).unwrap().unwrap();
    assert_eq!(progress_a.usage.bytes(), 300);
    
    // Node B: Simulate stale progress from processing 5 chunks (500 bytes)  
    let chunk_1_to_5 = create_chunks(1, 5, 100);
    for chunk in chunk_1_to_5.clone() {
        store_b.add_chunk(chunk).unwrap();
    }
    let progress_b = store_b.db.get_progress(version).unwrap().unwrap();
    assert_eq!(progress_b.usage.bytes(), 500);
    
    // Simulate database rollback (clear KV data but NOT progress)
    store_a.rollback_kv_only(); // Progress persists!
    store_b.rollback_kv_only(); // Progress persists!
    
    // Both nodes receive all 6 chunks again
    let all_chunks = create_chunks(1, 6, 100); // 600 bytes total
    
    // Node A: Continues from stale progress (300 bytes)
    for chunk in all_chunks.clone() {
        store_a.add_chunk(chunk).unwrap();
    }
    store_a.finish().unwrap();
    let final_usage_a = get_usage(&store_a, version);
    
    // Node B: Continues from stale progress (500 bytes) 
    for chunk in all_chunks {
        store_b.add_chunk(chunk).unwrap();
    }
    store_b.finish().unwrap();
    let final_usage_b = get_usage(&store_b, version);
    
    // VULNERABILITY: Nodes calculate different usage for same data
    assert_ne!(final_usage_a.bytes(), final_usage_b.bytes());
    println!("Node A usage: {} bytes", final_usage_a.bytes());
    println!("Node B usage: {} bytes", final_usage_b.bytes());
    println!("Expected: 600 bytes");
    
    // This breaks deterministic execution - nodes diverge!
}
```

**Notes**

The vulnerability is exacerbated by the non-atomic write pattern where indexer DB is written before main DB commit, creating additional opportunities for inconsistent state. The absence of any progress validation or cleanup mechanism in the entire codebase (verified via search for cleanup/delete methods) confirms this is a systemic issue, not an oversight in a single code path.

### Citations

**File:** storage/aptosdb/src/state_restore/mod.rs (L90-90)
```rust
        let progress_opt = self.db.get_progress(self.version)?;
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L93-99)
```rust
        if let Some(progress) = progress_opt {
            let idx = chunk
                .iter()
                .position(|(k, _v)| CryptoHash::hash(k) > progress.key_hash)
                .unwrap_or(chunk.len());
            chunk = chunk.split_off(idx);
        }
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L107-107)
```rust
        let mut usage = progress_opt.map_or(StateStorageUsage::zero(), |p| p.usage);
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1254-1257)
```rust
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateSnapshotKvRestoreProgress(version),
            &DbMetadataValue::StateSnapshotProgress(progress),
        )?;
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1281-1282)
```rust
    fn kv_finish(&self, version: Version, usage: StateStorageUsage) -> Result<()> {
        self.ledger_db.metadata_db().put_usage(version, usage)?;
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1338-1348)
```rust
            match (main_db_progress, progress_opt) {
                (None, None) => (),
                (None, Some(_)) => (),
                (Some(main_progress), Some(indexer_progress)) => {
                    if main_progress.key_hash > indexer_progress.key_hash {
                        bail!(
                            "Inconsistent restore progress between main db and internal indexer db. main db: {:?}, internal indexer db: {:?}",
                            main_progress,
                            indexer_progress,
                        );
                    }
```
