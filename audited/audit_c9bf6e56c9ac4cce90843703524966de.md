# Audit Report

## Title
Zero-Capacity BoundedExecutor Configuration Halts Consensus Through Commit Message Verification Deadlock

## Summary
The `num_bounded_executor_tasks` configuration parameter can be set to zero without validation, causing the `BoundedExecutor` to be created with zero capacity. This leads to a deadlock in the BufferManager's commit message verification task, blocking all commit vote processing and halting consensus indefinitely.

## Finding Description

The Aptos consensus pipeline uses a `BoundedExecutor` to spawn asynchronous verification tasks for incoming commit messages. This executor's capacity is configured via the `num_bounded_executor_tasks` parameter in the node configuration.

**Configuration Vulnerability:**

The `num_bounded_executor_tasks` field is defined as a `u64` with a default value of 16, but **no validation exists to prevent it from being set to zero**. [1](#0-0) [2](#0-1) 

The configuration sanitization process checks various consensus parameters but **completely omits validation of `num_bounded_executor_tasks`**: [3](#0-2) 

**Executor Creation with Zero Capacity:**

When the consensus system starts, a `BoundedExecutor` is created using this unvalidated configuration value: [4](#0-3) 

The `BoundedExecutor::new()` function creates a semaphore with the specified capacity without any bounds checking: [5](#0-4) 

**Deadlock in Commit Verification:**

This zero-capacity executor is passed through the consensus pipeline to the `BufferManager`. When the BufferManager starts, it spawns a dedicated verification task that processes incoming commit messages: [6](#0-5) 

The critical issue occurs at the `.await` on line 932. When `bounded_executor.spawn()` is called with zero capacity, the `acquire_permit().await` operation blocks indefinitely: [7](#0-6) [8](#0-7) 

With zero permits available, `acquire_owned().await` will **never return**, causing the entire verification task to hang while waiting for the first commit message.

**Consensus Halting Sequence:**

1. Node starts with `num_bounded_executor_tasks: 0` in configuration
2. BufferManager's verification task begins processing commit messages
3. First incoming commit message triggers `bounded_executor.spawn().await`
4. The spawn operation blocks forever waiting for a semaphore permit that will never be available
5. Verification task is deadlocked and cannot process any commit messages
6. Commit votes accumulate in the `commit_msg_rx` channel without being verified
7. The main BufferManager loop waits for verified messages on `verified_commit_msg_rx` but receives none
8. Without verified commit votes, blocks cannot aggregate quorum certificates
9. Blocks remain in the pipeline indefinitely without reaching the Aggregated state
10. Consensus progress halts completely

This breaks the **Consensus Safety** invariant requiring AptosBFT to maintain liveness, and violates the system requirement for continuous block production.

## Impact Explanation

**Severity: HIGH**

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria:
- **Validator node slowdowns**: The affected validator completely halts consensus processing
- **Significant protocol violations**: Complete loss of consensus liveness on the affected node

While this is a single-node issue (requires misconfiguration of the affected validator), it has severe consequences:

1. **Consensus Liveness Failure**: The affected validator cannot process commit votes, preventing it from participating in consensus
2. **Network Degradation**: If multiple validators are misconfigured, the network may lose quorum
3. **Validator Slashing Risk**: The non-responsive validator may be penalized for failing to vote on blocks
4. **State Divergence**: The halted node falls behind the network and requires state synchronization to recover

The impact approaches **Critical** if multiple validators are affected simultaneously, potentially causing:
- Network partition if more than 1/3 of validators are misconfigured
- Total loss of consensus liveness requiring manual intervention
- Potential hardfork requirement if the issue persists across validator set

## Likelihood Explanation

**Likelihood: MEDIUM**

The likelihood assessment considers several factors:

**Exploitation Vectors:**
1. **Misconfiguration**: A validator operator accidentally sets `num_bounded_executor_tasks: 0` in the configuration file
2. **Configuration Management Error**: Automated deployment scripts or configuration management tools (Ansible, Terraform, Kubernetes) propagate a zero value
3. **Configuration File Corruption**: File system issues or manual editing errors introduce a zero value
4. **Malicious Insider**: A compromised or malicious operator intentionally sets the value to zero

**Likelihood Factors:**

*Increasing Likelihood:*
- No validation prevents this configuration
- Zero is a valid `u64` value and won't cause parse errors
- Configuration files are often edited manually or by scripts
- The default value (16) might be changed without understanding implications
- Error is silent until runtime when commit messages arrive

*Decreasing Likelihood:*
- Most operators use default configurations without modification
- Zero is an unusual value that might trigger suspicion during review
- The issue is immediately apparent when consensus starts (node stops participating)
- Recovery is possible through configuration correction and restart

**Real-World Scenario:**
A DevOps team deploys validator nodes using infrastructure-as-code. A template variable `${EXECUTOR_TASKS}` is unset, defaulting to an empty string that parses as 0. Multiple validators start with zero-capacity executors and immediately halt upon receiving commit messages, taking the network offline.

## Recommendation

Implement validation for `num_bounded_executor_tasks` in the configuration sanitization process to prevent zero or unreasonably small values:

```rust
impl ConfigSanitizer for ConsensusConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        // ... existing checks ...

        // Validate bounded executor tasks
        const MIN_BOUNDED_EXECUTOR_TASKS: u64 = 1;
        const MAX_BOUNDED_EXECUTOR_TASKS: u64 = 1000; // reasonable upper bound
        
        if node_config.consensus.num_bounded_executor_tasks < MIN_BOUNDED_EXECUTOR_TASKS {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                format!(
                    "num_bounded_executor_tasks must be at least {}, got {}",
                    MIN_BOUNDED_EXECUTOR_TASKS,
                    node_config.consensus.num_bounded_executor_tasks
                ),
            ));
        }
        
        if node_config.consensus.num_bounded_executor_tasks > MAX_BOUNDED_EXECUTOR_TASKS {
            return Err(Error::ConfigSanitizerFailed(
                sanitizer_name,
                format!(
                    "num_bounded_executor_tasks must be at most {}, got {}",
                    MAX_BOUNDED_EXECUTOR_TASKS,
                    node_config.consensus.num_bounded_executor_tasks
                ),
            ));
        }

        Ok(())
    }
}
```

**Additional Recommendations:**

1. **Add Runtime Check**: In `BoundedExecutor::new()`, add a panic or error if capacity is zero:
   ```rust
   pub fn new(capacity: usize, executor: Handle) -> Self {
       assert!(capacity > 0, "BoundedExecutor capacity must be greater than 0");
       let semaphore = Arc::new(Semaphore::new(capacity));
       Self { semaphore, executor }
   }
   ```

2. **Configuration Documentation**: Clearly document the valid range for `num_bounded_executor_tasks` and the consequences of incorrect values

3. **Monitoring**: Add metrics to track executor queue depth and alert on persistent backlog

4. **Graceful Degradation**: Consider using `try_spawn()` with fallback logic instead of blocking `.spawn().await`

## Proof of Concept

**Step 1: Create Malicious Configuration**

Create a node configuration file with zero bounded executor tasks:

```yaml
consensus:
  num_bounded_executor_tasks: 0
  # ... other configuration ...
```

**Step 2: Start Validator Node**

```bash
cargo run --bin aptos-node -- --config-path ./malicious-config.yaml
```

**Step 3: Trigger Deadlock**

Once the node joins consensus and receives the first commit message from another validator, the verification task will block:

```rust
// This will happen automatically when the first commit message arrives
// The bounded_executor.spawn().await call will hang indefinitely
```

**Step 4: Observe Consensus Halt**

Monitor the node logs and metrics:
- No new blocks are processed by this validator
- Commit message queue fills up
- No commit votes are aggregated
- The node becomes unresponsive to consensus operations

**Expected Behavior:**
- Node configuration should be rejected during startup with a validation error
- If somehow accepted, runtime should panic with clear error message

**Actual Behavior:**
- Configuration is accepted without validation
- Node starts normally
- Consensus deadlocks upon first commit message
- Node requires restart with corrected configuration to recover

**Verification Command:**
```bash
# Check if node is processing commit messages
grep "Receive commit vote" validator.log | tail -n 10

# No new log entries will appear after the first message with zero-capacity executor
```

## Notes

This vulnerability demonstrates a critical gap in configuration validation that can lead to consensus failure. While the default value is safe (16 tasks), the lack of bounds checking allows operators to inadvertently or maliciously disable consensus participation through a simple configuration change.

The issue is particularly concerning because:
1. The error is silent during startup - the node appears healthy until commit messages arrive
2. Multiple validators could be affected by the same misconfiguration template
3. Recovery requires manual intervention (configuration correction and restart)
4. The root cause is non-obvious without deep knowledge of the executor implementation

The fix is straightforward: add proper validation at the configuration layer to prevent zero or unreasonable values from being used.

### Citations

**File:** config/src/config/consensus_config.rs (L97-97)
```rust
    pub num_bounded_executor_tasks: u64,
```

**File:** config/src/config/consensus_config.rs (L379-379)
```rust
            num_bounded_executor_tasks: 16,
```

**File:** config/src/config/consensus_config.rs (L503-533)
```rust
impl ConfigSanitizer for ConsensusConfig {
    fn sanitize(
        node_config: &NodeConfig,
        node_type: NodeType,
        chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        let sanitizer_name = Self::get_sanitizer_name();

        // Verify that the safety rules and quorum store configs are valid
        SafetyRulesConfig::sanitize(node_config, node_type, chain_id)?;
        QuorumStoreConfig::sanitize(node_config, node_type, chain_id)?;

        // Verify that the consensus-only feature is not enabled in mainnet
        if let Some(chain_id) = chain_id {
            if chain_id.is_mainnet() && is_consensus_only_perf_test_enabled() {
                return Err(Error::ConfigSanitizerFailed(
                    sanitizer_name,
                    "consensus-only-perf-test should not be enabled in mainnet!".to_string(),
                ));
            }
        }

        // Sender block limits must be <= receiver block limits
        Self::sanitize_send_recv_block_limits(&sanitizer_name, &node_config.consensus)?;

        // Quorum store batches must be <= consensus blocks
        Self::sanitize_batch_block_limits(&sanitizer_name, &node_config.consensus)?;

        Ok(())
    }
}
```

**File:** consensus/src/consensus_provider.rs (L81-84)
```rust
    let bounded_executor = BoundedExecutor::new(
        node_config.consensus.num_bounded_executor_tasks as usize,
        runtime.handle().clone(),
    );
```

**File:** crates/bounded-executor/src/executor.rs (L25-30)
```rust
    pub fn new(capacity: usize, executor: Handle) -> Self {
        let semaphore = Arc::new(Semaphore::new(capacity));
        Self {
            semaphore,
            executor,
        }
```

**File:** crates/bounded-executor/src/executor.rs (L33-35)
```rust
    async fn acquire_permit(&self) -> OwnedSemaphorePermit {
        self.semaphore.clone().acquire_owned().await.unwrap()
    }
```

**File:** crates/bounded-executor/src/executor.rs (L45-52)
```rust
    pub async fn spawn<F>(&self, future: F) -> JoinHandle<F::Output>
    where
        F: Future + Send + 'static,
        F::Output: Send + 'static,
    {
        let permit = self.acquire_permit().await;
        self.executor.spawn(future_with_permit(future, permit))
    }
```

**File:** consensus/src/pipeline/buffer_manager.rs (L919-934)
```rust
        spawn_named!("buffer manager verification", async move {
            while let Some((sender, commit_msg)) = commit_msg_rx.next().await {
                let tx = verified_commit_msg_tx.clone();
                let epoch_state_clone = epoch_state.clone();
                bounded_executor
                    .spawn(async move {
                        match commit_msg.req.verify(sender, &epoch_state_clone.verifier) {
                            Ok(_) => {
                                let _ = tx.unbounded_send(commit_msg);
                            },
                            Err(e) => warn!("Invalid commit message: {}", e),
                        }
                    })
                    .await;
            }
        });
```
