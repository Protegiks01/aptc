# Audit Report

## Title
Peer Health Check Bypass via Unvalidated Timestamp in PeerMonitoringMetadata

## Summary
The `PeerMonitoringMetadata` struct accepts `NodeInformationResponse` data from remote peers without validating the semantic correctness of timestamp fields. A malicious peer monitoring service can send `ledger_timestamp_usecs = u64::MAX`, causing the health check logic to incorrectly mark severely out-of-sync peers as healthy due to integer saturation in the subtraction operation. This allows manipulation of mempool peer prioritization and state sync peer selection.

## Finding Description

The `PeerMonitoringMetadata` struct is defined without validation methods to check metadata consistency. [1](#0-0) 

When a peer monitoring client receives a `NodeInformationResponse`, it stores the data without any validation of the timestamp fields: [2](#0-1) 

The stored metadata is later used in mempool peer prioritization to determine peer health. The health check calculates sync lag using saturating subtraction: [3](#0-2) 

**Exploitation Path:**

1. Attacker runs a malicious peer monitoring service server
2. Target node connects and requests node information via `GetNodeInformation` RPC
3. Attacker responds with `NodeInformationResponse` containing:
   - `ledger_timestamp_usecs = u64::MAX` (18,446,744,073,709,551,615 microseconds)
   - Other fields with plausible values
4. The response is accepted and stored without validation
5. When mempool evaluates peer health:
   - Current time ≈ 1.7×10¹⁵ microseconds (year 2024)
   - Calculation: `current_timestamp_usecs.saturating_sub(u64::MAX)`
   - Result: 0 (saturates because current < u64::MAX)
   - Health check: `0 < max_sync_lag_usecs` (typically 10 seconds = 10,000,000 μs)
   - Result: **TRUE** - peer marked as HEALTHY

6. The malicious peer is prioritized in mempool transaction forwarding: [4](#0-3) 

7. The malicious peer is also prioritized in state sync peer selection: [5](#0-4) 

## Impact Explanation

This vulnerability allows attackers to manipulate peer selection mechanisms in both mempool and state synchronization components, qualifying as **Medium Severity** per Aptos bug bounty criteria:

**State Inconsistencies Requiring Intervention:**
- Nodes may preferentially forward transactions to stale or malicious peers
- State sync operations may select compromised peers for data synchronization
- Network resources wasted on peers that provide invalid or outdated data
- Potential for targeted attacks where specific nodes are tricked into preferring attacker-controlled peers

**Does NOT qualify as High/Critical because:**
- Does not directly cause consensus violations or fund loss
- Does not cause permanent network partition
- Workaround exists: nodes can disconnect and reconnect to different peers
- The attack requires the target to actively connect to the attacker's node

## Likelihood Explanation

**High Likelihood:**

1. **Low Attack Complexity:** Attacker only needs to run a peer monitoring service that responds with crafted values
2. **No Special Privileges Required:** Any node on the network can act as a peer monitoring service
3. **No Authentication Required:** The peer monitoring service protocol accepts responses from any connected peer
4. **Wide Attack Surface:** Every node that enables peer monitoring (standard configuration) is vulnerable
5. **Persistent Effect:** Once the malicious metadata is stored, it persists until the peer disconnects or new valid data is received

## Recommendation

Add validation for `NodeInformationResponse` fields before storing them. The validation should check:

1. **Timestamp Sanity Check:** `ledger_timestamp_usecs` must be within reasonable bounds relative to current time
2. **Version Consistency:** `highest_synced_version >= lowest_available_version`
3. **Epoch Validation:** `highest_synced_epoch` must be reasonable

**Recommended Fix in `peer-monitoring-service/client/src/peer_states/node_info.rs`:**

```rust
fn handle_monitoring_service_response(
    &mut self,
    peer_network_id: &PeerNetworkId,
    _peer_metadata: PeerMetadata,
    _monitoring_service_request: PeerMonitoringServiceRequest,
    monitoring_service_response: PeerMonitoringServiceResponse,
    _response_time_secs: f64,
) {
    // Verify the response type is valid
    let node_info_response = match monitoring_service_response {
        PeerMonitoringServiceResponse::NodeInformation(node_information_response) => {
            node_information_response
        },
        _ => {
            warn!(...);
            self.handle_request_failure();
            return;
        },
    };

    // ADD VALIDATION HERE
    let current_timestamp_usecs = TimeService::now_unix_time().as_micros() as u64;
    let max_timestamp_drift_secs = 300; // 5 minutes
    let max_drift_usecs = max_timestamp_drift_secs * 1_000_000;
    
    // Validate timestamp is not in the far future or far past
    if node_info_response.ledger_timestamp_usecs > current_timestamp_usecs + max_drift_usecs {
        warn!(LogSchema::new(LogEntry::NodeInfoRequest)
            .event(LogEvent::InvalidResponse)
            .peer(peer_network_id)
            .message(&format!(
                "Peer returned invalid ledger timestamp (too far in future): {}",
                node_info_response.ledger_timestamp_usecs
            )));
        self.handle_request_failure();
        return;
    }
    
    // Validate version consistency
    if node_info_response.highest_synced_version < node_info_response.lowest_available_version {
        warn!(LogSchema::new(LogEntry::NodeInfoRequest)
            .event(LogEvent::InvalidResponse)
            .peer(peer_network_id)
            .message("Peer returned inconsistent version numbers"));
        self.handle_request_failure();
        return;
    }

    // Store the validated response
    self.record_node_info_response(node_info_response);
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_config::config::{MempoolConfig, NodeType};
    use aptos_peer_monitoring_service_types::{
        response::{NodeInformationResponse, NetworkInformationResponse},
        PeerMonitoringMetadata,
    };
    use aptos_time_service::TimeService;
    use std::time::Duration;

    #[test]
    fn test_malicious_timestamp_bypass_health_check() {
        // Setup
        let mempool_config = MempoolConfig {
            max_sync_lag_before_unhealthy_secs: 10, // 10 second tolerance
            ..MempoolConfig::default()
        };
        let time_service = TimeService::mock();
        
        // Advance time to simulate current timestamp
        time_service.clone().into_mock().advance_secs(1_700_000_000); // ~Year 2024
        
        // Create malicious metadata with u64::MAX timestamp
        let malicious_node_info = NodeInformationResponse {
            build_information: Default::default(),
            highest_synced_epoch: 1000,
            highest_synced_version: 1000000,
            ledger_timestamp_usecs: u64::MAX, // MALICIOUS VALUE
            lowest_available_version: 0,
            uptime: Duration::from_secs(1000),
        };
        
        let malicious_metadata = PeerMonitoringMetadata {
            average_ping_latency_secs: Some(0.05),
            latest_ping_latency_secs: Some(0.05),
            latest_network_info_response: Some(NetworkInformationResponse {
                connected_peers: Default::default(),
                distance_from_validators: 1,
            }),
            latest_node_info_response: Some(malicious_node_info),
            internal_client_state: None,
        };
        
        // Test: The malicious peer should be marked as UNHEALTHY but will be marked HEALTHY
        let is_healthy = check_peer_metadata_health(
            &mempool_config,
            &time_service,
            &Some(&malicious_metadata),
        );
        
        // BUG: This returns TRUE when it should return FALSE
        assert!(is_healthy, "VULNERABILITY: Malicious peer with u64::MAX timestamp is marked as healthy!");
        
        println!("✗ VULNERABILITY CONFIRMED: Peer with impossible timestamp passes health check");
    }
}
```

**Expected Behavior:** The health check should return `false` for a timestamp of u64::MAX.

**Actual Behavior:** The health check returns `true` due to saturation arithmetic, allowing the malicious peer to be prioritized.

### Citations

**File:** peer-monitoring-service/types/src/lib.rs (L44-51)
```rust
#[derive(Clone, Default, Deserialize, PartialEq, Serialize)]
pub struct PeerMonitoringMetadata {
    pub average_ping_latency_secs: Option<f64>, // The average latency ping for the peer
    pub latest_ping_latency_secs: Option<f64>,  // The latest latency ping for the peer
    pub latest_network_info_response: Option<NetworkInformationResponse>, // The latest network info response
    pub latest_node_info_response: Option<NodeInformationResponse>, // The latest node info response
    pub internal_client_state: Option<String>, // A detailed client state string for debugging and logging
}
```

**File:** peer-monitoring-service/client/src/peer_states/node_info.rs (L79-106)
```rust
    fn handle_monitoring_service_response(
        &mut self,
        peer_network_id: &PeerNetworkId,
        _peer_metadata: PeerMetadata,
        _monitoring_service_request: PeerMonitoringServiceRequest,
        monitoring_service_response: PeerMonitoringServiceResponse,
        _response_time_secs: f64,
    ) {
        // Verify the response type is valid
        let node_info_response = match monitoring_service_response {
            PeerMonitoringServiceResponse::NodeInformation(node_information_response) => {
                node_information_response
            },
            _ => {
                warn!(LogSchema::new(LogEntry::NodeInfoRequest)
                    .event(LogEvent::ResponseError)
                    .peer(peer_network_id)
                    .message(
                        "An unexpected response was received instead of a node info response!"
                    ));
                self.handle_request_failure();
                return;
            },
        };

        // Store the new latency ping result
        self.record_node_info_response(node_info_response);
    }
```

**File:** mempool/src/shared_mempool/priority.rs (L74-120)
```rust
    fn compare_intelligent(
        &self,
        peer_a: &(PeerNetworkId, Option<&PeerMonitoringMetadata>),
        peer_b: &(PeerNetworkId, Option<&PeerMonitoringMetadata>),
    ) -> Ordering {
        // Deconstruct the peer tuples
        let (peer_network_id_a, monitoring_metadata_a) = peer_a;
        let (peer_network_id_b, monitoring_metadata_b) = peer_b;

        // First, compare the peers by health (e.g., sync lag)
        let unhealthy_ordering = compare_peer_health(
            &self.mempool_config,
            &self.time_service,
            monitoring_metadata_a,
            monitoring_metadata_b,
        );
        if !unhealthy_ordering.is_eq() {
            return unhealthy_ordering; // Only return if it's not equal
        }

        // Next, compare by network ID (i.e., Validator > VFN > Public)
        let network_ordering = compare_network_id(
            &peer_network_id_a.network_id(),
            &peer_network_id_b.network_id(),
        );
        if !network_ordering.is_eq() {
            return network_ordering; // Only return if it's not equal
        }

        // Otherwise, compare by peer distance from the validators.
        // This avoids badly configured/connected peers (e.g., broken VN-VFN connections).
        let distance_ordering =
            compare_validator_distance(monitoring_metadata_a, monitoring_metadata_b);
        if !distance_ordering.is_eq() {
            return distance_ordering; // Only return if it's not equal
        }

        // Otherwise, compare by peer ping latency (the lower the better)
        let latency_ordering = compare_ping_latency(monitoring_metadata_a, monitoring_metadata_b);
        if !latency_ordering.is_eq() {
            return latency_ordering; // Only return if it's not equal
        }

        // Otherwise, simply hash the peer IDs and compare the hashes.
        // In practice, this should be relatively rare.
        self.compare_hash(peer_network_id_a, peer_network_id_b)
    }
```

**File:** mempool/src/shared_mempool/priority.rs (L562-589)
```rust
fn check_peer_metadata_health(
    mempool_config: &MempoolConfig,
    time_service: &TimeService,
    monitoring_metadata: &Option<&PeerMonitoringMetadata>,
) -> bool {
    monitoring_metadata
        .and_then(|metadata| {
            metadata
                .latest_node_info_response
                .as_ref()
                .map(|node_information_response| {
                    // Get the peer's ledger timestamp and the current timestamp
                    let peer_ledger_timestamp_usecs =
                        node_information_response.ledger_timestamp_usecs;
                    let current_timestamp_usecs = get_timestamp_now_usecs(time_service);

                    // Calculate the max sync lag before the peer is considered unhealthy (in microseconds)
                    let max_sync_lag_secs =
                        mempool_config.max_sync_lag_before_unhealthy_secs as u64;
                    let max_sync_lag_usecs = max_sync_lag_secs * MICROS_PER_SECOND;

                    // Determine if the peer is healthy
                    current_timestamp_usecs.saturating_sub(peer_ledger_timestamp_usecs)
                        < max_sync_lag_usecs
                })
        })
        .unwrap_or(false) // If metadata is missing, consider the peer unhealthy
}
```

**File:** state-sync/aptos-data-client/src/utils.rs (L26-64)
```rust
pub fn choose_random_peers_by_distance_and_latency(
    peers: HashSet<PeerNetworkId>,
    peers_and_metadata: Arc<PeersAndMetadata>,
    num_peers_to_choose: usize,
) -> HashSet<PeerNetworkId> {
    // Group peers and latency weights by validator distance, i.e., distance -> [(peer, latency weight)]
    let mut peers_and_latencies_by_distance = BTreeMap::new();
    for peer in peers {
        if let Some((distance, latency)) =
            get_distance_and_latency_for_peer(&peers_and_metadata, peer)
        {
            let latency_weight = convert_latency_to_weight(latency);
            peers_and_latencies_by_distance
                .entry(distance)
                .or_insert_with(Vec::new)
                .push((peer, latency_weight));
        }
    }

    // Select the peers by distance and latency weights. Note: BTreeMaps are
    // sorted by key, so the entries will be sorted by distance in ascending order.
    let mut selected_peers = HashSet::new();
    for (_, peers_and_latencies) in peers_and_latencies_by_distance {
        // Select the peers by latency weights
        let num_peers_remaining = num_peers_to_choose.saturating_sub(selected_peers.len()) as u64;
        let peers = choose_random_peers_by_weight(num_peers_remaining, peers_and_latencies);

        // Add the peers to the entire set
        selected_peers.extend(peers);

        // If we have selected enough peers, return early
        if selected_peers.len() >= num_peers_to_choose {
            return selected_peers;
        }
    }

    // Return the selected peers
    selected_peers
}
```
