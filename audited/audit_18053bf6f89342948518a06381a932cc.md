# Audit Report

## Title
Safety Rules Metrics Divergence and Resource Amplification on Storage Failure

## Summary
The `set_safety_data()` function in `PersistentSafetyStorage` updates Prometheus metrics before attempting to persist safety data to storage. When storage writes fail due to quota exhaustion or disk full conditions, the metrics reflect state that was never persisted. Repeated failures during consensus operations create unbounded log spam and metric observations without any rate limiting or circuit breaker, amplifying the resource exhaustion beyond the initial storage failure.

## Finding Description

The vulnerability exists in the `set_safety_data()` function which updates critical consensus safety metrics (EPOCH, LAST_VOTED_ROUND, HIGHEST_TIMEOUT_ROUND, PREFERRED_ROUND) before attempting to write to the underlying storage. [1](#0-0) 

When `internal_store.set()` fails at line 160 due to storage quota exhaustion or disk full conditions, the function returns an error but the metrics have already been updated with values that were never persisted. The cached safety data is also cleared (line 166), forcing subsequent reads to attempt storage access.

This creates a failure amplification cycle during normal consensus operations:

1. **Voting on blocks**: When a validator attempts to vote, `guarded_construct_and_sign_vote_two_chain()` calls `set_safety_data()` to persist the updated last_voted_round and last_vote. [2](#0-1) 

2. **Signing timeouts**: When signing a timeout, `guarded_sign_timeout_with_qc()` calls `set_safety_data()` to persist updated timeout rounds. [3](#0-2) 

3. **Order votes**: When creating order votes, `guarded_construct_and_sign_order_vote()` calls `set_safety_data()` to persist state. [4](#0-3) 

Each failure generates additional resource consumption through the `run_and_log` wrapper: [5](#0-4) 

The error logging at line 497 creates unbounded log entries, and the counter at line 498 increments indefinitely. Additionally, histogram timers are created both in `set_safety_data()` (line 151) and `run_and_log()` (line 488), accumulating observations in memory.

Critically, the retry mechanism in `MetricsSafetyRules` only retries for specific errors (NotInitialized, IncorrectEpoch, WaypointOutOfDate) and does NOT retry for `SecureStorageUnexpectedError`: [6](#0-5) 

This means storage failures propagate as errors without retry, but the validator continues attempting to participate in consensus, creating the same failure pattern every round.

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos bug bounty program for multiple reasons:

1. **Validator node slowdowns**: The validator cannot participate in consensus when storage is degraded, and the repeated failures consume additional CPU cycles processing errors and updating metrics.

2. **State inconsistencies requiring intervention**: The metrics (EPOCH, LAST_VOTED_ROUND, HIGHEST_TIMEOUT_ROUND, PREFERRED_ROUND) show values that were never persisted to storage. This creates operational blind spots where monitoring dashboards show incorrect validator state, preventing operators from accurately diagnosing the root cause.

3. **Resource exhaustion amplification**: When storage quota is exhausted, each consensus round generates:
   - 4 gauge metric updates with non-persisted values
   - 2 histogram timer observations (memory accumulation)
   - Warning log entries (potentially filling separate log partitions or remote logging services)
   - Error counter increments

If logs are stored on a separate partition or sent to remote logging infrastructure, this amplifies the storage exhaustion attack to additional resources. The unbounded nature of these failures (no circuit breaker, no rate limiting, no exponential backoff) means the resource consumption continues as long as consensus rounds proceed.

4. **Breaks Resource Limits Invariant**: This violates Critical Invariant #9: "All operations must respect gas, storage, and computational limits." The implementation allows unbounded resource consumption through logs and metrics when storage limits are hit.

## Likelihood Explanation

**Likelihood: Medium-High** once storage degradation occurs.

**Preconditions:**
- Validator storage must be near or at quota/disk capacity
- This can occur through:
  - State bloat attacks on the blockchain
  - Operational misconfiguration (insufficient disk allocation)
  - Gradual accumulation during normal operations
  - Malicious spam targeting storage growth

**Triggering:**
Once storage is degraded, every consensus operation (voting, timeouts, order votes) triggers the failure path. In a healthy network, validators participate in consensus every few hundred milliseconds, meaning the failure pattern repeats continuously without intervention.

**Exploitation:**
While this requires initial storage exhaustion, it significantly amplifies the attack impact:
- Makes recovery harder (operators see wrong metrics)
- Consumes additional resources (logs, memory)
- Prevents automated recovery mechanisms
- Could cascade to exhaust secondary resources (log storage, remote logging services)

## Recommendation

**Primary Fix: Update metrics AFTER successful storage write**

Modify `set_safety_data()` to only update metrics after confirming the storage write succeeded:

```rust
pub fn set_safety_data(&mut self, data: SafetyData) -> Result<(), Error> {
    let _timer = counters::start_timer("set", SAFETY_DATA);
    
    // Attempt storage write FIRST
    match self.internal_store.set(SAFETY_DATA, data.clone()) {
        Ok(_) => {
            // Only update metrics and cache AFTER successful write
            counters::set_state(counters::EPOCH, data.epoch as i64);
            counters::set_state(counters::LAST_VOTED_ROUND, data.last_voted_round as i64);
            counters::set_state(
                counters::HIGHEST_TIMEOUT_ROUND,
                data.highest_timeout_round as i64,
            );
            counters::set_state(counters::PREFERRED_ROUND, data.preferred_round as i64);
            self.cached_safety_data = Some(data);
            Ok(())
        },
        Err(error) => {
            self.cached_safety_data = None;
            Err(Error::SecureStorageUnexpectedError(error.to_string()))
        },
    }
}
```

**Secondary Fix: Add circuit breaker for storage failures**

Implement exponential backoff or circuit breaker pattern when storage errors are detected, similar to the pattern used in the storage service request moderator: [7](#0-6) 

**Tertiary Fix: Add alerting for metric-storage divergence**

Implement health checks that compare cached metrics against actual persisted values, alerting operators when divergence is detected.

## Proof of Concept

```rust
#[cfg(test)]
mod test_storage_exhaustion {
    use super::*;
    use aptos_consensus_types::safety_data::SafetyData;
    use aptos_crypto::bls12381;
    use aptos_secure_storage::{InMemoryStorage, Storage};
    use aptos_types::validator_signer::ValidatorSigner;
    use std::sync::{Arc, Mutex};

    // Mock storage that fails after N successful writes
    struct FailingStorage {
        inner: InMemoryStorage,
        writes_until_failure: Arc<Mutex<usize>>,
    }

    impl FailingStorage {
        fn new(writes_until_failure: usize) -> Self {
            Self {
                inner: InMemoryStorage::new(),
                writes_until_failure: Arc::new(Mutex::new(writes_until_failure)),
            }
        }
    }

    impl KVStorage for FailingStorage {
        fn available(&self) -> Result<(), aptos_secure_storage::Error> {
            self.inner.available()
        }

        fn get<V: serde::de::DeserializeOwned>(
            &self,
            key: &str,
        ) -> Result<aptos_secure_storage::GetResponse<V>, aptos_secure_storage::Error> {
            self.inner.get(key)
        }

        fn set<V: serde::Serialize>(
            &mut self,
            key: &str,
            value: V,
        ) -> Result<(), aptos_secure_storage::Error> {
            let mut counter = self.writes_until_failure.lock().unwrap();
            if *counter == 0 {
                return Err(aptos_secure_storage::Error::IoError(
                    std::io::Error::new(std::io::ErrorKind::Other, "Disk quota exceeded"),
                ));
            }
            *counter -= 1;
            self.inner.set(key, value)
        }
    }

    #[test]
    fn test_metrics_divergence_on_storage_failure() {
        // Initialize storage that will fail after initial setup
        let failing_storage = Storage::from(FailingStorage::new(10));
        let consensus_key = ValidatorSigner::from_int(0).private_key().clone();
        
        let mut storage = PersistentSafetyStorage::initialize(
            failing_storage,
            Author::random(),
            consensus_key,
            Waypoint::default(),
            true,
        );

        // Get initial safety data
        let initial_data = storage.safety_data().unwrap();
        assert_eq!(initial_data.last_voted_round, 0);

        // Attempt to update safety data - this will fail due to storage exhaustion
        let new_data = SafetyData::new(1, 100, 50, 30, None, 10);
        let result = storage.set_safety_data(new_data.clone());
        
        // Verify the call failed
        assert!(result.is_err());

        // VULNERABILITY: Metrics were updated despite storage failure
        // In a real scenario, monitoring would show last_voted_round=100
        // but actual persisted value is still 0
        
        // Demonstrate repeated failures creating resource exhaustion
        for round in 101..200 {
            let data = SafetyData::new(1, round, 50, 30, None, 10);
            let _ = storage.set_safety_data(data);
            // Each failure creates:
            // - 4 metric updates (EPOCH, LAST_VOTED_ROUND, HIGHEST_TIMEOUT_ROUND, PREFERRED_ROUND)
            // - 1 histogram observation
            // - 1 error log
            // - No rate limiting or circuit breaker
        }

        // Actual persisted state hasn't changed
        let persisted_data = storage.safety_data().unwrap();
        assert_eq!(persisted_data.last_voted_round, 0);
        
        // But metrics show last_voted_round=199 (from last failed attempt)
        // This is the metrics divergence vulnerability
    }
}
```

## Notes

This vulnerability represents a defensive programming failure where the ordering of operations (metrics update before storage write) creates observable security impact. While it requires storage exhaustion as a precondition, it significantly amplifies the attack by:

1. Creating operational blind spots through incorrect metrics
2. Consuming additional resources without bounds
3. Preventing accurate diagnosis and recovery

The fix is straightforward: update metrics only after successful storage writes, and implement circuit breaker patterns similar to those already used elsewhere in the Aptos codebase.

### Citations

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L150-170)
```rust
    pub fn set_safety_data(&mut self, data: SafetyData) -> Result<(), Error> {
        let _timer = counters::start_timer("set", SAFETY_DATA);
        counters::set_state(counters::EPOCH, data.epoch as i64);
        counters::set_state(counters::LAST_VOTED_ROUND, data.last_voted_round as i64);
        counters::set_state(
            counters::HIGHEST_TIMEOUT_ROUND,
            data.highest_timeout_round as i64,
        );
        counters::set_state(counters::PREFERRED_ROUND, data.preferred_round as i64);

        match self.internal_store.set(SAFETY_DATA, data.clone()) {
            Ok(_) => {
                self.cached_safety_data = Some(data);
                Ok(())
            },
            Err(error) => {
                self.cached_safety_data = None;
                Err(Error::SecureStorageUnexpectedError(error.to_string()))
            },
        }
    }
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L46-47)
```rust
        self.update_highest_timeout_round(timeout, &mut safety_data);
        self.persistent_storage.set_safety_data(safety_data)?;
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L91-92)
```rust
        safety_data.last_vote = Some(vote.clone());
        self.persistent_storage.set_safety_data(safety_data)?;
```

**File:** consensus/safety-rules/src/safety_rules_2chain.rs (L117-117)
```rust
        self.persistent_storage.set_safety_data(safety_data)?;
```

**File:** consensus/safety-rules/src/safety_rules.rs (L483-500)
```rust
fn run_and_log<F, L, R>(callback: F, log_cb: L, log_entry: LogEntry) -> Result<R, Error>
where
    F: FnOnce() -> Result<R, Error>,
    L: for<'a> Fn(SafetyLogSchema<'a>) -> SafetyLogSchema<'a>,
{
    let _timer = counters::start_timer("internal", log_entry.as_str());
    trace!(log_cb(SafetyLogSchema::new(log_entry, LogEvent::Request)));
    counters::increment_query(log_entry.as_str(), "request");
    callback()
        .inspect(|_v| {
            trace!(log_cb(SafetyLogSchema::new(log_entry, LogEvent::Success)));
            counters::increment_query(log_entry.as_str(), "success");
        })
        .inspect_err(|err| {
            warn!(log_cb(SafetyLogSchema::new(log_entry, LogEvent::Error)).error(err));
            counters::increment_query(log_entry.as_str(), "error");
        })
}
```

**File:** consensus/src/metrics_safety_rules.rs (L71-85)
```rust
    fn retry<T, F: FnMut(&mut Box<dyn TSafetyRules + Send + Sync>) -> Result<T, Error>>(
        &mut self,
        mut f: F,
    ) -> Result<T, Error> {
        let result = f(&mut self.inner);
        match result {
            Err(Error::NotInitialized(_))
            | Err(Error::IncorrectEpoch(_, _))
            | Err(Error::WaypointOutOfDate(_, _, _, _)) => {
                self.perform_initialize()?;
                f(&mut self.inner)
            },
            _ => result,
        }
    }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::{error::Error, logging::LogEntry, metrics, utils, LogSchema};
use aptos_config::{
    config::{AptosDataClientConfig, StorageServiceConfig},
    network_id::{NetworkId, PeerNetworkId},
};
use aptos_logger::warn;
use aptos_network::application::storage::PeersAndMetadata;
use aptos_storage_service_types::{
    requests::StorageServiceRequest, responses::StorageServerSummary,
};
use aptos_time_service::{TimeService, TimeServiceTrait};
use arc_swap::ArcSwap;
use dashmap::DashMap;
use std::{
    sync::Arc,
    time::{Duration, Instant},
};

/// A simple struct that tracks the state of an unhealthy peer
#[derive(Clone, Debug)]
pub struct UnhealthyPeerState {
    ignore_start_time: Option<Instant>, // The time when we first started ignoring the peer
    invalid_request_count: u64,         // The total number of invalid requests from the peer
    max_invalid_requests: u64, // The max number of invalid requests before ignoring the peer
    min_time_to_ignore_secs: u64, // The min time (secs) to ignore the peer (doubles each round)
    time_service: TimeService, // The time service
}

impl UnhealthyPeerState {
    pub fn new(
        max_invalid_requests: u64,
        min_time_to_ignore_secs: u64,
        time_service: TimeService,
    ) -> Self {
        Self {
            ignore_start_time: None,
            invalid_request_count: 0,
            max_invalid_requests,
            min_time_to_ignore_secs,
            time_service,
        }
    }

    /// Increments the invalid request count for the peer and marks
    /// the peer to be ignored if it has sent too many invalid requests.
    /// Note: we only ignore peers on the public network.
    pub fn increment_invalid_request_count(&mut self, peer_network_id: &PeerNetworkId) {
```
