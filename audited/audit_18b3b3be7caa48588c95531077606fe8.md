# Audit Report

## Title
Consensus DB Recovery Panic Causes Total Network Liveness Failure on Malformed Persisted Blocks

## Summary
The `BlockStore::build()` function unconditionally panics on any block or QC insertion failure during recovery, with no fallback mechanism. Malformed blocks that pass BCS deserialization but fail stricter insertion validations (such as QC consistency checks) can be persisted to ConsensusDB during normal operation. On restart, these blocks trigger inevitable panics across all validators, causing total network liveness failure requiring manual intervention. [1](#0-0) [2](#0-1) [3](#0-2) 

## Finding Description

The vulnerability exists in a critical gap between block persistence and validation during the consensus recovery process.

**Normal Operation Flow:**
1. During normal consensus operation, blocks are persisted to storage BEFORE complete tree validation. [4](#0-3) 

2. If tree insertion subsequently fails (line 515), the error is propagated but the block remains in ConsensusDB. [5](#0-4) 

**Recovery Process:**
3. On restart, `RecoveryData::find_blocks_to_prune` validates basic parent relationships but does NOT validate:
   - QC-to-block consistency (whether the QC's certified_block matches the local block's info)
   - Ordered block window ancestor availability
   - Pipeline builder parent requirements [6](#0-5) 

4. The `build()` function attempts to insert all recovered blocks with strict validations that were not checked by `find_blocks_to_prune`, including QC consistency validation: [7](#0-6) 

5. Specifically, `insert_single_quorum_cert` validates that the QC's block_info matches the local block's block_info: [8](#0-7) 

6. The `match_ordered_only` check compares epoch, round, id, and timestamp - if ANY differ, insertion fails: [9](#0-8) 

**Attack Scenarios:**

**Scenario 1: QC Inconsistency via Consensus Bug**
- A consensus bug allows a QC to be propagated and persisted that references a block_info inconsistent with the actual block stored locally
- All validators persist this inconsistent QC during normal operation
- On restart, all validators panic when `insert_single_quorum_cert` detects the mismatch
- Network-wide liveness failure

**Scenario 2: Storage Corruption Leading to Parent Not Found**
- During `insert_block_inner`, if `pipeline_builder` exists, it requires the parent block: [10](#0-9) 

- If parent is missing (due to corruption or race condition), panic occurs

**Scenario 3: Ordered Block Window Ancestor Missing**
- `get_ordered_block_window` walks up the parent chain to build a window and fails if any ancestor is missing: [11](#0-10) 

## Impact Explanation

**Critical Severity** - This vulnerability meets the highest severity criteria:

1. **Total Loss of Liveness/Network Availability**: If malformed blocks are persisted across all validators (via consensus propagation or widespread corruption), every validator will panic on restart with no automatic recovery mechanism. The network becomes completely unavailable.

2. **Non-recoverable Network Partition (requires manual intervention)**: The panic is deterministic and repeatable on every restart attempt. Recovery requires manual pruning of the ConsensusDB on all affected validators - effectively requiring coordinated out-of-band intervention similar to a hardfork.

3. **Consensus Invariant Violation**: This breaks the fundamental liveness guarantee that the network should be able to recover from crashes through the recovery data mechanism.

The vulnerability is amplified because:
- The blocks were accepted during normal operation (passed initial validation)
- The error is only detected during recovery, not at persistence time
- No graceful degradation or fallback exists
- All validators crash simultaneously if the malformed blocks propagated widely

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability can be triggered by:

1. **Consensus Implementation Bugs** (Medium Likelihood): Bugs in consensus logic that allow inconsistent QCs or blocks to propagate are realistic given the complexity of distributed consensus protocols. Historical examples from other blockchains show such bugs occur.

2. **Storage Corruption** (Low-Medium Likelihood): RocksDB corruption, disk failures, or power loss during writes could create inconsistent block/QC pairs. While individual node corruption is common, network-wide impact requires correlated failures or propagation through consensus.

3. **Race Conditions** (Low Likelihood): Timing windows between persistence and validation could theoretically create edge cases, though the async/await model provides some protection.

The vulnerability is realistic because:
- The persist-before-validate pattern is inherent to the current design
- The validation gap between `find_blocks_to_prune` and `build()` is confirmed
- No defensive coding or graceful recovery exists in `build()`
- The `.unwrap_or_else` with panic is explicit and unconditional

## Recommendation

**Immediate Fix**: Replace unconditional panics with graceful error handling and block pruning:

```rust
for block in blocks {
    if block.round() <= root_block_round {
        if let Err(e) = block_store.insert_committed_block(block.clone()).await {
            error!(
                "[BlockStore] Failed to insert committed block during build: {:?}. Pruning block {}.",
                e, block.id()
            );
            // Prune the problematic block from storage
            let _ = storage.prune_tree(vec![block.id()]);
            continue;
        }
    } else {
        if let Err(e) = block_store.insert_block(block.clone()).await {
            error!(
                "[BlockStore] Failed to insert block during build: {:?}. Pruning block {}.",
                e, block.id()
            );
            let _ = storage.prune_tree(vec![block.id()]);
            continue;
        }
    }
}

for qc in quorum_certs {
    if let Err(e) = block_store.insert_single_quorum_cert(qc.clone()) {
        error!(
            "[BlockStore] Failed to insert QC during build: {:?}. Pruning QC for block {}.",
            e, qc.certified_block().id()
        );
        let _ = storage.prune_tree(vec![qc.certified_block().id()]);
        continue;
    }
}
```

**Additional Mitigations**:

1. **Enhanced Pre-Validation**: Add QC consistency validation to `find_blocks_to_prune`:
   - Validate QC block_info matches local block before retention
   - Ensure ordered block window ancestors exist before retention

2. **Pre-Persistence Validation**: Move all critical validations (parent existence, QC consistency) BEFORE `storage.save_tree()` call in `insert_block_inner`.

3. **Recovery Mode Flag**: Add a "recovery mode" that allows validators to start without full block tree reconstruction, syncing from peers instead.

4. **Monitoring**: Add metrics to detect and alert on insertion failures during normal operation, catching problematic blocks before they cause widespread issues.

## Proof of Concept

**Reproduction Steps:**

1. **Setup**: Run a local Aptos validator network with ConsensusDB enabled.

2. **Inject Malformed QC**: Using database manipulation tools, modify a persisted QC in ConsensusDB to have a different round value than the block it certifies (while keeping the same block ID):
   ```rust
   // Pseudo-code for database manipulation
   let db = ConsensusDB::new(db_path);
   let (blocks, qcs) = db.get_all_blocks_and_qcs();
   
   // Find a block and its QC
   let target_block = blocks[0];
   let mut target_qc = qcs.iter().find(|qc| qc.certified_block().id() == target_block.id());
   
   // Corrupt the QC's block_info (change round while keeping ID)
   let corrupted_block_info = BlockInfo::new(
       target_qc.certified_block().epoch(),
       target_qc.certified_block().round() + 1, // Wrong round!
       target_qc.certified_block().id(),
       target_qc.certified_block().executed_state_id(),
       target_qc.certified_block().version(),
       target_qc.certified_block().timestamp_usecs(),
       None
   );
   
   // Create corrupted QC and persist
   let corrupted_qc = QuorumCert::new(/*...with corrupted_block_info...*/);
   db.save_blocks_and_quorum_certificates(vec![], vec![corrupted_qc]);
   ```

3. **Trigger Recovery**: Restart the validator.

4. **Observe Panic**: The validator will panic during `build()` at line 302-304 with message: `"[BlockStore] failed to insert quorum during build"` because the `match_ordered_only` check at line 527-536 fails.

5. **Confirm Non-Recovery**: Every subsequent restart attempt will hit the same panic, confirming permanent liveness failure.

**Expected Output:**
```
thread 'main' panicked at '[BlockStore] failed to insert quorum during build: 
QC for block <block_id> has different Round(11) than local Round(10)'
```

The validator cannot start, and manual ConsensusDB pruning is required to recover.

### Citations

**File:** consensus/src/block_storage/block_store.rs (L287-292)
```rust
                    .unwrap_or_else(|e| {
                        panic!(
                            "[BlockStore] failed to insert committed block during build {:?}",
                            e
                        )
                    });
```

**File:** consensus/src/block_storage/block_store.rs (L294-296)
```rust
                block_store.insert_block(block).await.unwrap_or_else(|e| {
                    panic!("[BlockStore] failed to insert block during build {:?}", e)
                });
```

**File:** consensus/src/block_storage/block_store.rs (L300-304)
```rust
            block_store
                .insert_single_quorum_cert(qc)
                .unwrap_or_else(|e| {
                    panic!("[BlockStore] failed to insert quorum during build{:?}", e)
                });
```

**File:** consensus/src/block_storage/block_store.rs (L464-468)
```rust
        if let Some(pipeline_builder) = &self.pipeline_builder {
            let parent_block = self
                .get_block(pipelined_block.parent_id())
                .ok_or_else(|| anyhow::anyhow!("Parent block not found"))?;

```

**File:** consensus/src/block_storage/block_store.rs (L512-514)
```rust
        self.storage
            .save_tree(vec![pipelined_block.block().clone()], vec![])
            .context("Insert block failed when saving block")?;
```

**File:** consensus/src/block_storage/block_store.rs (L515-516)
```rust
        self.inner.write().insert_block(pipelined_block)
    }
```

**File:** consensus/src/block_storage/block_store.rs (L519-556)
```rust
    pub fn insert_single_quorum_cert(&self, qc: QuorumCert) -> anyhow::Result<()> {
        // If the parent block is not the root block (i.e not None), ensure the executed state
        // of a block is consistent with its QuorumCert, otherwise persist the QuorumCert's
        // state and on restart, a new execution will agree with it.  A new execution will match
        // the QuorumCert's state on the next restart will work if there is a memory
        // corruption, for example.
        match self.get_block(qc.certified_block().id()) {
            Some(pipelined_block) => {
                ensure!(
                    // decoupled execution allows dummy block infos
                    pipelined_block
                        .block_info()
                        .match_ordered_only(qc.certified_block()),
                    "QC for block {} has different {:?} than local {:?}",
                    qc.certified_block().id(),
                    qc.certified_block(),
                    pipelined_block.block_info()
                );
                observe_block(
                    pipelined_block.block().timestamp_usecs(),
                    BlockStage::QC_ADDED,
                );
                if pipelined_block.block().is_opt_block() {
                    observe_block(
                        pipelined_block.block().timestamp_usecs(),
                        BlockStage::QC_ADDED_OPT_BLOCK,
                    );
                }
                pipelined_block.set_qc(Arc::new(qc.clone()));
            },
            None => bail!("Insert {} without having the block in store first", qc),
        };

        self.storage
            .save_tree(vec![], vec![qc.clone()])
            .context("Insert block failed when saving quorum")?;
        self.inner.write().insert_quorum_cert(qc)
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L448-476)
```rust
    fn find_blocks_to_prune(
        root_id: HashValue,
        blocks: &mut Vec<Block>,
        quorum_certs: &mut Vec<QuorumCert>,
    ) -> Vec<HashValue> {
        // prune all the blocks that don't have root as ancestor
        let mut tree = HashSet::new();
        let mut to_remove = HashSet::new();
        tree.insert(root_id);
        // assume blocks are sorted by round already
        blocks.retain(|block| {
            if tree.contains(&block.parent_id()) {
                tree.insert(block.id());
                true
            } else {
                to_remove.insert(block.id());
                false
            }
        });
        quorum_certs.retain(|qc| {
            if tree.contains(&qc.certified_block().id()) {
                true
            } else {
                to_remove.insert(qc.certified_block().id());
                false
            }
        });
        to_remove.into_iter().collect()
    }
```

**File:** types/src/block_info.rs (L196-204)
```rust
    pub fn match_ordered_only(&self, executed_block_info: &BlockInfo) -> bool {
        self.epoch == executed_block_info.epoch
            && self.round == executed_block_info.round
            && self.id == executed_block_info.id
            && (self.timestamp_usecs == executed_block_info.timestamp_usecs
            // executed block info has changed its timestamp because it's a reconfiguration suffix
                || (self.timestamp_usecs > executed_block_info.timestamp_usecs
                    && executed_block_info.has_reconfiguration()))
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L290-298)
```rust
        while !current_block.is_genesis_block()
            && current_block.quorum_cert().certified_block().round() >= window_start_round
        {
            if let Some(current_pipelined_block) = self.get_block(&current_block.parent_id()) {
                current_block = current_pipelined_block.block().clone();
                window.push(current_pipelined_block);
            } else {
                bail!("Parent block not found for block {}", current_block.id());
            }
```
