# Audit Report

## Title
Race Condition Between Bootstrap Completion and Continuous Syncer Initialization Causes Validator Node Panic

## Summary
A race condition exists between the bootstrap completion process and the continuous syncer initialization that can cause validator nodes to panic. When bootstrap completes, it sets the `bootstrapped` flag to `true` before cleaning up the chunk executor. During this window, the continuous syncer can start and reset the chunk executor, which is then finished by the bootstrapper, leaving the continuous syncer with a `None` chunk executor that causes a panic when accessed.

## Finding Description
The vulnerability lies in the asynchronous bootstrap completion flow in the state-sync driver. The critical sequence is: [1](#0-0) 

The `bootstrapped` flag is set to `true` before the cleanup is complete. Inside `notify_listeners_if_bootstrapped()`: [2](#0-1) 

The critical issue occurs at the `await` on line 406. This yields control back to the async runtime, allowing other tasks to execute. During this yield, the driver's event loop can process a progress check interval: [3](#0-2) 

The driver checks if bootstrap is complete: [4](#0-3) 

Since `is_bootstrapped()` returns `true`, the continuous syncer starts: [5](#0-4) 

The continuous syncer resets the chunk executor, creating new internal state. However, when control returns to the bootstrapper, it calls: [6](#0-5) 

This sets the chunk executor's `inner` to `None`: [7](#0-6) 

When the continuous syncer later tries to use the chunk executor (enqueuing chunks, updating ledger, etc.), it calls `with_inner()`: [8](#0-7) 

The `expect("not reset")` on line 94 panics because `inner` is `None`, crashing the validator node.

## Impact Explanation
This vulnerability causes validator node crashes, which falls under **HIGH severity** according to the Aptos bug bounty program ("Validator node slowdowns" and "API crashes"). A crashed validator node:

1. **Affects network liveness**: Reduces the number of active validators, potentially impacting consensus if enough validators crash
2. **Requires manual intervention**: Validator operators must restart nodes
3. **Can occur repeatedly**: If timing conditions persist, nodes may crash on every restart during bootstrap
4. **Disrupts validator operations**: Affects validator rewards and network participation

While this doesn't directly violate consensus safety (the crash prevents incorrect execution), it violates the availability invariant and can cause significant operational disruptions.

## Likelihood Explanation
**Likelihood: MEDIUM**

The race condition requires specific timing:
1. The node must be completing bootstrap
2. The progress check interval must fire during the narrow window between setting `bootstrapped = true` and calling `finish_chunk_executor()`
3. This window occurs during the `reset_active_stream().await` call (approximately a few milliseconds to seconds depending on stream cleanup)

The likelihood increases under these conditions:
- High-frequency progress check intervals (configured via `progress_check_interval_ms`)
- Slow stream cleanup operations (network delays, large pending data)
- Busy validator nodes with high async task contention
- Repeated bootstrap attempts (after restarts or state sync failures)

The race can occur naturally during normal operations without any attacker intervention. It's more likely on production validators under load.

## Recommendation
Add proper synchronization to ensure bootstrap completion is atomic with respect to the bootstrap check. The `bootstrapped` flag should only be set after all cleanup is complete, or the continuous syncer should check a more fine-grained state before using the chunk executor.

**Fix Option 1: Defer setting bootstrapped flag**
```rust
// In bootstrapper.rs, bootstrapping_complete()
pub async fn bootstrapping_complete(&mut self) -> Result<(), Error> {
    info!(LogSchema::new(LogEntry::Bootstrapper)
        .message("The node has successfully bootstrapped!"));
    
    // First notify listeners and cleanup
    if let Some(notifier_channel) = self.bootstrap_notifier_channel.take() {
        if let Err(error) = notifier_channel.send(Ok(())) {
            return Err(Error::CallbackSendFailed(format!(
                "Bootstrap notification error: {:?}",
                error
            )));
        }
    }
    self.reset_active_stream(None).await?;
    self.storage_synchronizer.finish_chunk_executor();
    
    // Only set bootstrapped after cleanup is complete
    self.bootstrapped = true;
    Ok(())
}
```

**Fix Option 2: Use atomic state machine**
Replace the boolean flag with an atomic enum tracking `Bootstrapping`, `FinalizingBootstrap`, and `Bootstrapped` states, preventing the continuous syncer from starting during finalization.

**Fix Option 3: Add synchronization barrier**
Ensure the continuous syncer waits for a finalization signal before starting, synchronized through a proper async barrier or lock.

## Proof of Concept
A reliable PoC is challenging due to timing sensitivity, but the race can be triggered with:

```rust
// Stress test that increases likelihood of race condition
#[tokio::test(flavor = "multi_thread", worker_threads = 4)]
async fn test_bootstrap_completion_race() {
    // Setup: Create a state sync driver during bootstrap
    // Configure very short progress_check_interval_ms (e.g., 10ms)
    
    // Action: Trigger bootstrap completion
    // Simultaneously: Hammer the progress check interval
    
    // Expected: Node should not panic with "not reset"
    // Actual: May panic due to race condition
    
    // This test would need full node setup with:
    // - Mock consensus sending notifications
    // - Progress check intervals configured for <100ms
    // - Slow stream cleanup (mock delays)
    // - Multiple async tasks competing for executor time
}
```

The race can also be observed in production logs showing:
1. "The node has successfully bootstrapped!" message
2. Continuous syncer starting ("Resetting chunk executor")  
3. Panic with message containing "not reset"
4. Stack trace showing `ChunkExecutor::with_inner` as the panic location

---

**Notes:**

This vulnerability violates the state consistency invariant - the chunk executor state management is not properly synchronized. The `BootstrapNotComplete` check (`is_bootstrapped()`) does not properly synchronize with the actual completion of bootstrap cleanup, allowing the continuous syncer to proceed with stale executor state.

The impact is compounded by the fact that this can cause validators to repeatedly crash during bootstrap if timing conditions persist, requiring code-level fixes rather than just configuration changes.

### Citations

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L373-378)
```rust
    pub async fn bootstrapping_complete(&mut self) -> Result<(), Error> {
        info!(LogSchema::new(LogEntry::Bootstrapper)
            .message("The node has successfully bootstrapped!"));
        self.bootstrapped = true;
        self.notify_listeners_if_bootstrapped().await
    }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L396-411)
```rust
    async fn notify_listeners_if_bootstrapped(&mut self) -> Result<(), Error> {
        if self.is_bootstrapped() {
            if let Some(notifier_channel) = self.bootstrap_notifier_channel.take() {
                if let Err(error) = notifier_channel.send(Ok(())) {
                    return Err(Error::CallbackSendFailed(format!(
                        "Bootstrap notification error: {:?}",
                        error
                    )));
                }
            }
            self.reset_active_stream(None).await?;
            self.storage_synchronizer.finish_chunk_executor(); // The bootstrapper is now complete
        }

        Ok(())
    }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L235-237)
```rust
                _ = progress_check_interval.select_next_some() => {
                    self.drive_progress().await;
                }
```

**File:** state-sync/state-sync-driver/src/driver.rs (L693-710)
```rust
        if self.bootstrapper.is_bootstrapped() {
            // Fetch any consensus sync requests
            let consensus_sync_request = self.consensus_notification_handler.get_sync_request();

            // Attempt to continuously sync
            if let Err(error) = self
                .continuous_syncer
                .drive_progress(consensus_sync_request)
                .await
            {
                sample!(
                    SampleRate::Duration(Duration::from_secs(DRIVER_ERROR_LOG_FREQ_SECS)),
                    warn!(LogSchema::new(LogEntry::Driver)
                        .error(&error)
                        .message("Error found when driving progress of the continuous syncer!"));
                );
                metrics::increment_counter(&metrics::CONTINUOUS_SYNCER_ERRORS, error.get_label());
            }
```

**File:** state-sync/state-sync-driver/src/continuous_syncer.rs (L104-105)
```rust
        // Reset the chunk executor to flush any invalid state currently held in-memory
        self.storage_synchronizer.reset_chunk_executor()?;
```

**File:** execution/executor/src/chunk_executor/mod.rs (L89-106)
```rust
    fn with_inner<F, T>(&self, f: F) -> Result<T>
    where
        F: FnOnce(&ChunkExecutorInner<V>) -> Result<T>,
    {
        let locked = self.inner.read();
        let inner = locked.as_ref().expect("not reset");

        let has_pending_pre_commit = inner.has_pending_pre_commit.load(Ordering::Acquire);
        f(inner).map_err(|error| {
            if has_pending_pre_commit {
                panic!(
                    "Hit error with pending pre-committed ledger, panicking. {:?}",
                    error,
                );
            }
            error
        })
    }
```

**File:** execution/executor/src/chunk_executor/mod.rs (L221-225)
```rust
    fn finish(&self) {
        let _guard = CONCURRENCY_GAUGE.concurrency_with(&["chunk", "finish"]);

        *self.inner.write() = None;
    }
```
