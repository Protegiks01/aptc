# Audit Report

## Title
Stale Epoch Cache After Database Truncation in Crash Recovery

## Summary
The `get_db_state()` function returns epoch numbers from an in-memory cache that is not updated after database truncation during crash recovery, causing it to return epoch numbers higher than what exists in the database. This violates the assumption that epoch numbers observed from a given node should be monotonically increasing over time.

## Finding Description

The vulnerability exists in the initialization and crash recovery flow: [1](#0-0) 

The `get_db_state()` function reads the epoch from the cached `latest_ledger_info`: [2](#0-1) 

This cache is populated during initialization by reading from disk: [3](#0-2) 

However, after the cache is populated, `sync_commit_progress()` is called during `StateStore::new()`: [4](#0-3) 

This function can truncate the ledger database back to an earlier version: [5](#0-4) 

The truncation deletes epoch-ending ledger info entries: [6](#0-5) 

**Critical flaw**: After this truncation removes epoch-ending ledger infos from the database, the in-memory cache in `LedgerMetadataDb` is never updated. Therefore, `get_db_state()` continues returning the stale higher epoch number.

**Scenario:**
1. Node commits epoch 5 ledger info to disk
2. Before `OverallCommitProgress` marker is fully updated, node crashes
3. On restart, `LedgerMetadataDb::new()` reads epoch 5 from disk into cache
4. `sync_commit_progress()` detects incomplete commit and truncates back to epoch 4
5. Cache still contains epoch 5
6. `get_db_state()` returns epoch 5 while database actually contains epoch 4

## Impact Explanation

This violates the **State Consistency** invariant. The backup coordinator polls `get_db_state()` to make scheduling decisions: [7](#0-6) 

If epoch numbers go backwards between observations, it creates confusion in the backup system's state tracking. While the actual backup data is read directly from the database (and would be correct), the backup coordinator uses the stale epoch information for scheduling decisions, potentially causing it to attempt backing up epochs that don't exist or skip epochs that should be backed up.

This is a **Medium severity** issue per the bug bounty criteria as it causes state inconsistencies that could require manual intervention to resolve backup coordination issues.

## Likelihood Explanation

This can occur naturally during crash recovery if a node crashes after writing a ledger info but before updating the overall commit progress marker. The timing window is narrow but not negligible, especially during epoch transitions when validators are under heavy load. It requires no attacker actionâ€”just ordinary crash recovery scenarios that happen in production distributed systems.

## Recommendation

Update the in-memory cache after database truncation. Add a call to refresh the latest ledger info from disk after `sync_commit_progress()` completes:

```rust
// In StateStore::new(), after sync_commit_progress:
if !hack_for_tests && !empty_buffered_state_for_restore {
    Self::sync_commit_progress(
        Arc::clone(&ledger_db),
        Arc::clone(&state_kv_db),
        Arc::clone(&state_merkle_db),
        /*crash_if_difference_is_too_large=*/ true,
    );
    
    // Refresh the cache to reflect any truncation that occurred
    ledger_db.metadata_db().refresh_latest_ledger_info_from_db();
}
```

Add a new method in `LedgerMetadataDb`:

```rust
pub(crate) fn refresh_latest_ledger_info_from_db(&self) {
    if let Ok(li) = get_latest_ledger_info_in_db_impl(&self.db) {
        if let Some(ledger_info) = li {
            self.latest_ledger_info.store(Arc::new(Some(ledger_info)));
        }
    }
}
```

## Proof of Concept

Reproduction steps:
1. Run a validator node and wait for an epoch ending transaction to commit
2. Monitor the `LedgerInfoSchema` entries in the database
3. Simulate a crash by sending SIGKILL after epoch N+1 ledger info is written but before the node can update `OverallCommitProgress`
4. Restart the node
5. Query `get_db_state()` via the backup service API - it will return epoch N+1
6. Query the actual database for the latest ledger info - it will show epoch N
7. Observe the discrepancy

This demonstrates that the cached value is stale and not synchronized with the actual database state after truncation.

---

**Notes:**
While this is a real implementation bug that violates state consistency assumptions, it does not meet the criteria for HIGH severity because it is not exploitable by an attacker and requires a crash at a specific timing window. The impact is limited to potential confusion in backup scheduling rather than actual data corruption, as backup data is read directly from the database.

### Citations

**File:** storage/aptosdb/src/backup/backup_handler.rs (L175-184)
```rust
    pub fn get_db_state(&self) -> Result<Option<DbState>> {
        Ok(self
            .ledger_db
            .metadata_db()
            .get_latest_ledger_info_option()
            .map(|li| DbState {
                epoch: li.ledger_info().epoch(),
                committed_version: li.ledger_info().version(),
            }))
    }
```

**File:** storage/aptosdb/src/ledger_db/ledger_metadata_db.rs (L43-51)
```rust
    pub(super) fn new(db: Arc<DB>) -> Self {
        let latest_ledger_info = get_latest_ledger_info_in_db_impl(&db).expect("DB read failed.");
        let latest_ledger_info = ArcSwap::from(Arc::new(latest_ledger_info));

        Self {
            db,
            latest_ledger_info,
        }
    }
```

**File:** storage/aptosdb/src/ledger_db/ledger_metadata_db.rs (L93-98)
```rust
    /// Returns the latest ledger info, or None if it doesn't exist.
    pub(crate) fn get_latest_ledger_info_option(&self) -> Option<LedgerInfoWithSignatures> {
        let ledger_info_ptr = self.latest_ledger_info.load();
        let ledger_info: &Option<_> = ledger_info_ptr.deref();
        ledger_info.clone()
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L353-359)
```rust
        if !hack_for_tests && !empty_buffered_state_for_restore {
            Self::sync_commit_progress(
                Arc::clone(&ledger_db),
                Arc::clone(&state_kv_db),
                Arc::clone(&state_merkle_db),
                /*crash_if_difference_is_too_large=*/ true,
            );
```

**File:** storage/aptosdb/src/state_store/mod.rs (L448-449)
```rust
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");
```

**File:** storage/aptosdb/src/utils/truncation_helper.rs (L394-427)
```rust
fn delete_per_epoch_data(
    ledger_db: &DB,
    start_version: Version,
    batch: &mut SchemaBatch,
) -> Result<()> {
    let mut iter = ledger_db.iter::<LedgerInfoSchema>()?;
    iter.seek_to_last();
    if let Some((epoch, ledger_info)) = iter.next().transpose()? {
        let version = ledger_info.commit_info().version();
        if version >= start_version {
            info!(
                version = version,
                epoch = epoch,
                "Truncate latest epoch data."
            );
            batch.delete::<LedgerInfoSchema>(&epoch)?;
        }
    }

    let mut iter = ledger_db.iter::<EpochByVersionSchema>()?;
    iter.seek(&start_version)?;

    for item in iter {
        let (version, epoch) = item?;
        info!(
            version = version,
            epoch = epoch,
            "Truncate epoch ending data."
        );
        batch.delete::<EpochByVersionSchema>(&version)?;
        batch.delete::<LedgerInfoSchema>(&epoch)?;
    }

    Ok(())
```

**File:** storage/backup/backup-cli/src/coordinators/backup.rs (L179-197)
```rust
    async fn try_refresh_db_state(&self, db_state_broadcast: &watch::Sender<Option<DbState>>) {
        match self.client.get_db_state().await {
            Ok(s) => {
                HEARTBEAT_TS.set(unix_timestamp_sec());
                if s.is_none() {
                    warn!("DB not bootstrapped.");
                } else {
                    db_state_broadcast
                        .send(s)
                        .map_err(|e| anyhow!("Receivers should not be cancelled: {}", e))
                        .unwrap()
                }
            },
            Err(e) => warn!(
                "Failed pulling DbState from local node: {}. Will keep trying.",
                e
            ),
        };
    }
```
