# Audit Report

## Title
Health Checker State Corruption via Async Cancellation and Dropped Notifications Leading to Resource Exhaustion

## Summary
The health checker's `disconnect_peer()` function contains an async cancellation vulnerability that, when combined with the notification channel's message dropping behavior, can leave `health_check_data` in a permanently inconsistent state. This causes infinite retry loops that exhaust validator node resources.

## Finding Description

The vulnerability exists in the `disconnect_peer()` function's handling of async cancellation combined with the notification broadcast system's message dropping policy. [1](#0-0) 

When `disconnect_peer()` is called with a timeout (50ms in the health checker's main loop), the following sequence occurs:

1. **Line 71**: Connection state is synchronously updated to `ConnectionState::Disconnecting` in `PeersAndMetadata`
2. **Lines 72-75**: Async `disconnect_from_peer()` is awaited - **CANCELLATION POINT**
3. **Lines 77-79**: Peer is removed from `health_check_data` **only if result.is_ok()**

The health checker calls this with a 50ms timeout: [2](#0-1) 

When the timeout fires, the disconnect request has typically already been sent to the `PeerManager` (the `push()` operation is synchronous), which will complete the disconnect in the background and broadcast a `LostPeer` notification. However, the notification broadcast system drops messages when channels are full: [3](#0-2) 

**Critical Issue**: At lines 378-384, if a subscriber's channel is full (capacity of 1000 per line 35), the `LostPeer` notification is silently dropped with only a warning logged. [4](#0-3) 

**Attack Scenario:**
1. Attacker causes rapid peer connection/disconnection events (network instability, connection flooding)
2. Health checker's notification channel fills to capacity (1000 messages)
3. Health checker detects ping failures and calls `disconnect_peer()` with 50ms timeout
4. Timeout occurs before await completes, `health_check_data` not cleaned (line 78 not executed)
5. `PeerManager` completes disconnect in background, broadcasts `LostPeer` notification
6. **Notification is dropped** because health checker's channel is full
7. Peer remains permanently in `health_check_data` with state `Disconnecting`
8. Health checker's `connected_peers()` method continues returning this peer: [5](#0-4) 

9. Each round, health checker pings the stuck peer (fails), increments failures, attempts disconnect (times out), repeats indefinitely: [6](#0-5) 

The failure count increments unboundedly with no cap: [7](#0-6) 

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria ("Validator node slowdowns"):

1. **Resource Exhaustion**: Each stuck peer causes continuous wasted operations every `ping_interval` (typically seconds):
   - Failed ping RPC attempts
   - 50ms timeout on disconnect attempts
   - Lock contention on `health_check_data`

2. **Amplification**: With multiple stuck peers, resource consumption multiplies, significantly degrading validator performance

3. **Persistent State**: The inconsistency is permanent until node restart - no self-healing mechanism exists for peers stuck in `Disconnecting` state

4. **Validator Impact**: Degraded validator performance affects consensus participation, block proposal timing, and network responsiveness

While this doesn't directly compromise consensus safety or steal funds, it creates a denial-of-service condition on validator nodes through resource exhaustion, meeting the "Validator node slowdowns" criterion.

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability requires two conditions to align:
1. Network instability or connection flooding to fill notification channels (realistic during network attacks or degradation)
2. Health check failures triggering disconnect timeouts (common during network issues)

Both conditions naturally occur together during network attacks or infrastructure problems, making exploitation realistic. An attacker can deliberately trigger these conditions by:
- Rapidly connecting/disconnecting multiple peer connections
- Causing network delays that trigger health check failures
- Sustaining the attack to keep notification channels saturated

The 1000-message channel capacity seems large, but with multiple network applications subscribing and rapid connection events during attacks, saturation is achievable.

## Recommendation

**Fix 1: Ensure cleanup on cancellation**
Use a guard pattern to ensure `health_check_data` cleanup happens even on cancellation:

```rust
pub async fn disconnect_peer(
    &mut self,
    peer_network_id: PeerNetworkId,
    disconnect_reason: DisconnectReason,
) -> Result<(), Error> {
    let peer_id = peer_network_id.peer_id();
    
    // Set up cleanup guard
    let cleanup_guard = CleanupGuard {
        health_check_data: &self.health_check_data,
        peer_id,
    };
    
    let _ = self.update_connection_state(peer_network_id, ConnectionState::Disconnecting);
    let result = self
        .network_client
        .disconnect_from_peer(peer_network_id, disconnect_reason)
        .await;
    
    // Cleanup happens in Drop impl even on cancellation
    Ok(result?)
}

struct CleanupGuard<'a> {
    health_check_data: &'a RwLock<HashMap<PeerId, HealthCheckData>>,
    peer_id: PeerId,
}

impl Drop for CleanupGuard<'_> {
    fn drop(&mut self) {
        self.health_check_data.write().remove(&self.peer_id);
    }
}
```

**Fix 2: Make notification delivery reliable**
Change notification channel from `try_send` to async `send` with backpressure, or increase priority for critical `LostPeer` notifications.

**Fix 3: Add periodic cleanup**
Implement periodic garbage collection to remove peers in `Disconnecting` state that haven't completed within a timeout threshold.

## Proof of Concept

```rust
// Rust integration test demonstrating the vulnerability
#[tokio::test]
async fn test_health_checker_cancellation_leak() {
    use network::protocols::health_checker::*;
    use tokio::time::{timeout, Duration};
    
    // Setup health checker with mock network
    let (mut health_checker, mock_network) = setup_test_health_checker().await;
    let peer_id = PeerId::random();
    
    // Add peer to health_check_data
    health_checker.network_interface.create_peer_and_health_data(peer_id, 0);
    
    // Simulate notification channel saturation
    // Fill the channel with 1000+ connection events
    for _ in 0..1100 {
        mock_network.send_connection_event(ConnectionNotification::NewPeer(
            generate_mock_metadata(),
            NetworkId::Validator,
        )).await;
    }
    
    // Trigger disconnect with timeout
    let peer_network_id = PeerNetworkId::new(NetworkId::Validator, peer_id);
    let result = timeout(
        Duration::from_millis(50),
        health_checker.network_interface.disconnect_peer(
            peer_network_id,
            DisconnectReason::NetworkHealthCheckFailure,
        ),
    ).await;
    
    // Verify timeout occurred
    assert!(result.is_err());
    
    // Simulate background disconnect completion and dropped notification
    mock_network.complete_disconnect(peer_id).await;
    // LostPeer notification is dropped due to full channel
    
    // Verify peer is stuck in health_check_data
    let connected_peers = health_checker.network_interface.connected_peers();
    assert!(connected_peers.contains(&peer_id), "Peer leaked in health_check_data");
    
    // Verify connection state shows Disconnecting
    let metadata = health_checker.network_interface
        .get_peers_and_metadata()
        .get_metadata_for_peer(peer_network_id)
        .unwrap();
    assert_eq!(metadata.get_connection_state(), ConnectionState::Disconnecting);
    
    // Verify infinite retry loop begins
    for _ in 0..5 {
        tokio::time::sleep(Duration::from_secs(1)).await;
        // Each round: ping fails, disconnect times out, repeat
        assert!(connected_peers.contains(&peer_id), "Peer still stuck");
    }
}
```

## Notes

This vulnerability demonstrates a subtle interaction between three components:
1. Async cancellation semantics in Rust
2. Best-effort notification delivery with message dropping
3. Inconsistent state management across `health_check_data` and `PeersAndMetadata`

The issue is exacerbated by the lack of defensive programming - no bounded retry limits, no cleanup guards, no periodic garbage collection of stuck peers. While other network applications use `get_connected_peers_and_metadata()` which correctly checks connection state, the health checker's internal `connected_peers()` method relies solely on `health_check_data`, creating the inconsistency.

### Citations

**File:** network/framework/src/protocols/health_checker/interface.rs (L59-61)
```rust
    pub fn connected_peers(&self) -> Vec<PeerId> {
        self.health_check_data.read().keys().cloned().collect()
    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L65-81)
```rust
    pub async fn disconnect_peer(
        &mut self,
        peer_network_id: PeerNetworkId,
        disconnect_reason: DisconnectReason,
    ) -> Result<(), Error> {
        // Possibly already disconnected, but try anyways
        let _ = self.update_connection_state(peer_network_id, ConnectionState::Disconnecting);
        let result = self
            .network_client
            .disconnect_from_peer(peer_network_id, disconnect_reason)
            .await;
        let peer_id = peer_network_id.peer_id();
        if result.is_ok() {
            self.health_check_data.write().remove(&peer_id);
        }
        result
    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L110-116)
```rust
    pub fn increment_peer_round_failure(&mut self, peer_id: PeerId, round: u64) {
        if let Some(health_check_data) = self.health_check_data.write().get_mut(&peer_id) {
            if health_check_data.round <= round {
                health_check_data.failures += 1;
            }
        }
    }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L353-364)
```rust
                self.network_interface
                    .increment_peer_round_failure(peer_id, round);

                // If the ping failures are now more than
                // `self.ping_failures_tolerated`, we disconnect from the node.
                // The HealthChecker only performs the disconnect. It relies on
                // ConnectivityManager or the remote peer to re-establish the connection.
                let failures = self
                    .network_interface
                    .get_peer_failures(peer_id)
                    .unwrap_or(0);
                if failures > self.ping_failures_tolerated {
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L373-391)
```rust
                    if let Err(err) = timeout(
                        Duration::from_millis(50),
                        self.network_interface.disconnect_peer(
                            peer_network_id,
                            DisconnectReason::NetworkHealthCheckFailure,
                        ),
                    )
                    .await
                    {
                        warn!(
                            NetworkSchema::new(&self.network_context)
                                .remote_peer(&peer_id),
                            error = ?err,
                            "{} Failed to disconnect from peer: {} with error: {:?}",
                            self.network_context,
                            peer_id.short_str(),
                            err
                        );
                    }
```

**File:** network/framework/src/application/storage.rs (L31-35)
```rust
// notification_backlog is how many ConnectionNotification items can be queued waiting for an app to receive them.
// Beyond this, new messages will be dropped if the app is not handling them fast enough.
// We make this big enough to fit an initial burst of _all_ the connected peers getting notified.
// Having 100 connected peers is common, 500 not unexpected
const NOTIFICATION_BACKLOG: usize = 1000;
```

**File:** network/framework/src/application/storage.rs (L371-395)
```rust
    fn broadcast(&self, event: ConnectionNotification) {
        let mut listeners = self.subscribers.lock();
        let mut to_del = vec![];
        for i in 0..listeners.len() {
            let dest = listeners.get_mut(i).unwrap();
            if let Err(err) = dest.try_send(event.clone()) {
                match err {
                    TrySendError::Full(_) => {
                        // Tried to send to an app, but the app isn't handling its messages fast enough.
                        // Drop message. Maybe increment a metrics counter?
                        sample!(
                            SampleRate::Duration(Duration::from_secs(1)),
                            warn!("PeersAndMetadata.broadcast() failed, some app is slow"),
                        );
                    },
                    TrySendError::Closed(_) => {
                        to_del.push(i);
                    },
                }
            }
        }
        for evict in to_del.into_iter() {
            listeners.swap_remove(evict);
        }
    }
```
