# Audit Report

## Title
Non-Deterministic Indexer State Due to Database Query Fallback with Silent Failure Handling

## Summary
The indexer's object deletion processing contains a critical determinism flaw where database query failures result in silent data omission, causing different indexers to reach divergent states when processing identical blockchain data. This violates the fundamental requirement that all indexers must deterministically produce the same indexed state.

## Finding Description

The `Object::from_delete_resource` function in the indexer processes deletion events for objects. When an object is deleted that was created in a previous transaction batch, the function attempts to retrieve the object's owner information to preserve it in the historical record. [1](#0-0) 

The critical flaw occurs in the error handling logic: [2](#0-1) 

When the database lookup via `get_object_owner` fails, the function logs an error but returns `Ok(None)`, which causes the deletion record to be silently skipped. The `get_object_owner` function implements retry logic with timing dependencies: [3](#0-2) 

The retry constants are defined as: [4](#0-3) 

**How the vulnerability manifests:**

1. Indexer A processes a batch containing an object deletion
2. The deleted object was created in a previous batch
3. Indexer A's database query succeeds → deletion record is inserted
4. Indexer B processes the same batch but experiences database unavailability or timing issues
5. After 5 retries (2.5 seconds total), the query fails
6. Indexer B's deletion record is silently skipped → `Ok(None)` returned
7. **Result**: Indexer A has the deletion record, Indexer B does not
8. The indexers have divergent states despite processing identical blockchain data

The calling code in the transaction processor shows how this affects the final database state: [5](#0-4) 

When `from_delete_resource` returns `Ok(None)`, the object is not added to `all_objects` or `all_current_objects`, meaning it's never inserted into the database for that indexer.

**Similar patterns exist elsewhere:**

The same vulnerability pattern appears in token collection processing: [6](#0-5) 

This demonstrates that the non-deterministic database fallback pattern is systemic across the indexer codebase.

## Impact Explanation

This qualifies as **HIGH severity** under "Significant protocol violations" because:

1. **Data Integrity Violation**: Multiple independent indexers are expected to produce identical database states when processing the same blockchain data. This is fundamental to the indexer's role as a trusted data source.

2. **API Inconsistency**: Applications querying different indexers will receive inconsistent responses about object ownership history and deletion status, breaking the determinism guarantee.

3. **Operational Impact**: Indexer operators cannot trust that their databases are correct even when processing the same chain, requiring manual intervention and backfills as suggested by the error message.

4. **Cascading Effects**: Downstream systems (block explorers, wallets, analytics tools) relying on indexer data may make incorrect decisions based on incomplete or inconsistent data.

While this does not affect consensus or on-chain state directly, it violates the core invariant that indexers must deterministically process blockchain data, which is critical infrastructure for the Aptos ecosystem.

## Likelihood Explanation

**LIKELIHOOD: HIGH**

This issue will occur regularly in production because:

1. **Database Unavailability**: Network partitions, database restarts, connection pool exhaustion, and maintenance windows are common operational events
2. **Concurrent Processing**: The indexer processes batches in parallel, creating race conditions where one task's writes haven't been committed when another task queries
3. **Timing Dependencies**: The retry logic with sleeps means outcomes depend on timing, making results non-deterministic by design
4. **Missing Data**: The error message "You probably should backfill db" indicates this is a known operational issue that occurs when indexers are catching up or recovering
5. **No Recovery Mechanism**: Once the `Ok(None)` path is taken, the data is permanently missing from that indexer's database with no automatic recovery

The vulnerability requires no attacker action—it's triggered by normal operational conditions.

## Recommendation

**Solution**: Make database query failures fatal during indexer processing instead of silently skipping data. The indexer should halt and require operator intervention rather than producing incorrect state.

**Immediate Fix:**

Replace the silent failure handling with a hard error that prevents the indexer from continuing with inconsistent state:

```rust
pub fn from_delete_resource(
    delete_resource: &DeleteResource,
    txn_version: i64,
    write_set_change_index: i64,
    object_mapping: &HashMap<CurrentObjectPK, CurrentObject>,
    conn: &mut PgPoolConnection,
) -> anyhow::Result<Option<(Self, CurrentObject)>> {
    if delete_resource.resource.to_string() == "0x1::object::ObjectGroup" {
        let resource = MoveResource::from_delete_resource(
            delete_resource,
            0,
            txn_version,
            0,
        );
        let previous_object = if let Some(object) = object_mapping.get(&resource.address) {
            object.clone()
        } else {
            // CRITICAL: Database lookup failure must be fatal to prevent state divergence
            Self::get_object_owner(conn, &resource.address)
                .with_context(|| format!(
                    "FATAL: Cannot process object deletion at version {} without owner info for {}. \
                     Indexer state would diverge. Database must be backfilled before continuing.",
                    txn_version, resource.address
                ))?
        };
        // ... rest of function
    } else {
        Ok(None)
    }
}
```

**Long-term Solution:**

1. Implement a pre-processing validation step that verifies all required data is available before processing a batch
2. Build the complete dependency graph for each batch and ensure all referenced objects exist in either the in-memory cache or database
3. Add state checkpointing and verification between indexers to detect divergence
4. Implement automatic recovery mechanisms that detect and repair missing data

**Configuration Change:**

Update the retry logic to use exponential backoff and fail more gracefully, but still propagate errors:

```rust
const QUERY_RETRIES: u32 = 10;
const QUERY_RETRY_DELAY_MS: u64 = 100; // Start at 100ms
const QUERY_RETRY_BACKOFF: u64 = 2; // Double each retry
```

## Proof of Concept

```rust
#[cfg(test)]
mod test_indexer_determinism {
    use super::*;
    use diesel::r2d2::{ConnectionManager, Pool};
    use diesel::PgConnection;
    
    #[test]
    fn test_non_deterministic_state_from_db_failure() {
        // Setup: Create two independent indexer instances with separate databases
        let pool1 = create_test_db_pool("indexer_1");
        let pool2 = create_test_db_pool("indexer_2");
        
        // Step 1: Both indexers process transaction that creates an object
        let create_txn = create_test_object_transaction(100, "0xABCD");
        process_transaction(&pool1, create_txn.clone());
        process_transaction(&pool2, create_txn.clone());
        
        // Verify: Both indexers have the object
        assert!(object_exists(&pool1, "0xABCD"));
        assert!(object_exists(&pool2, "0xABCD"));
        
        // Step 2: Simulate database unavailability for indexer_2
        // This could be network partition, connection pool exhaustion, etc.
        simulate_db_unavailability(&pool2);
        
        // Step 3: Both indexers process transaction that deletes the object
        let delete_txn = create_test_object_deletion(200, "0xABCD");
        let result1 = process_transaction(&pool1, delete_txn.clone());
        let result2 = process_transaction(&pool2, delete_txn.clone());
        
        // Step 4: Restore database availability for indexer_2
        restore_db_availability(&pool2);
        
        // VULNERABILITY: Indexers now have divergent states
        let obj1 = get_object_history(&pool1, "0xABCD");
        let obj2 = get_object_history(&pool2, "0xABCD");
        
        // Indexer 1 has complete history: created at v100, deleted at v200
        assert_eq!(obj1.len(), 2);
        assert_eq!(obj1[1].is_deleted, true);
        assert_eq!(obj1[1].transaction_version, 200);
        
        // Indexer 2 has incomplete history: only creation record
        // Deletion was silently skipped due to database query failure
        assert_eq!(obj2.len(), 1); // DIVERGENCE!
        assert_eq!(obj2[0].is_deleted, false);
        
        // Current state also differs
        let current1 = get_current_object(&pool1, "0xABCD");
        let current2 = get_current_object(&pool2, "0xABCD");
        
        assert_eq!(current1.is_deleted, true);
        assert_eq!(current2.is_deleted, false); // WRONG STATE
        
        // APIs querying these indexers return contradictory information
        println!("Indexer 1 API: Object 0xABCD is deleted");
        println!("Indexer 2 API: Object 0xABCD still exists");
        println!("DETERMINISM VIOLATED: Same blockchain data, different indexer states");
    }
    
    fn simulate_db_unavailability(pool: &Pool<ConnectionManager<PgConnection>>) {
        // Close all connections to simulate database unavailability
        // In real scenario: network partition, database restart, maintenance
        drop_all_connections(pool);
    }
}
```

## Notes

This vulnerability is explicitly acknowledged in the codebase through error messages like "You probably should backfill db," indicating it's treated as an operational issue rather than a security vulnerability. However, the security implication is severe: **deterministic data processing is a core requirement for distributed systems**, and silent failures violate this fundamental guarantee.

The issue affects not just object deletions but also token collection processing and potentially other indexer operations that use the same database fallback pattern with silent failure handling. A comprehensive audit of all indexer models using `QUERY_RETRIES` pattern is recommended.

### Citations

**File:** crates/indexer/src/models/v2_objects.rs (L111-164)
```rust
    pub fn from_delete_resource(
        delete_resource: &DeleteResource,
        txn_version: i64,
        write_set_change_index: i64,
        object_mapping: &HashMap<CurrentObjectPK, CurrentObject>,
        conn: &mut PgPoolConnection,
    ) -> anyhow::Result<Option<(Self, CurrentObject)>> {
        if delete_resource.resource.to_string() == "0x1::object::ObjectGroup" {
            let resource = MoveResource::from_delete_resource(
                delete_resource,
                0, // Placeholder, this isn't used anyway
                txn_version,
                0, // Placeholder, this isn't used anyway
            );
            let previous_object = if let Some(object) = object_mapping.get(&resource.address) {
                object.clone()
            } else {
                match Self::get_object_owner(conn, &resource.address) {
                    Ok(owner) => owner,
                    Err(_) => {
                        aptos_logger::error!(
                            transaction_version = txn_version,
                            lookup_key = &resource.address,
                            "Missing object owner for object. You probably should backfill db.",
                        );
                        return Ok(None);
                    },
                }
            };
            Ok(Some((
                Self {
                    transaction_version: txn_version,
                    write_set_change_index,
                    object_address: resource.address.clone(),
                    owner_address: previous_object.owner_address.clone(),
                    state_key_hash: resource.state_key_hash.clone(),
                    guid_creation_num: previous_object.last_guid_creation_num.clone(),
                    allow_ungated_transfer: previous_object.allow_ungated_transfer,
                    is_deleted: true,
                },
                CurrentObject {
                    object_address: resource.address,
                    owner_address: previous_object.owner_address.clone(),
                    state_key_hash: resource.state_key_hash,
                    last_guid_creation_num: previous_object.last_guid_creation_num.clone(),
                    allow_ungated_transfer: previous_object.allow_ungated_transfer,
                    last_transaction_version: txn_version,
                    is_deleted: true,
                },
            )))
        } else {
            Ok(None)
        }
    }
```

**File:** crates/indexer/src/models/v2_objects.rs (L167-192)
```rust
    fn get_object_owner(
        conn: &mut PgPoolConnection,
        object_address: &str,
    ) -> anyhow::Result<CurrentObject> {
        let mut retried = 0;
        while retried < QUERY_RETRIES {
            retried += 1;
            match CurrentObjectQuery::get_by_address(object_address, conn) {
                Ok(res) => {
                    return Ok(CurrentObject {
                        object_address: res.object_address,
                        owner_address: res.owner_address,
                        state_key_hash: res.state_key_hash,
                        allow_ungated_transfer: res.allow_ungated_transfer,
                        last_guid_creation_num: res.last_guid_creation_num,
                        last_transaction_version: res.last_transaction_version,
                        is_deleted: res.is_deleted,
                    })
                },
                Err(_) => {
                    std::thread::sleep(std::time::Duration::from_millis(QUERY_RETRY_DELAY_MS));
                },
            }
        }
        Err(anyhow::anyhow!("Failed to get object owner"))
    }
```

**File:** crates/indexer/src/models/token_models/collection_datas.rs (L23-24)
```rust
pub const QUERY_RETRIES: u32 = 5;
pub const QUERY_RETRY_DELAY_MS: u64 = 500;
```

**File:** crates/indexer/src/processors/default_processor.rs (L557-573)
```rust
                    WriteSetChange::DeleteResource(inner) => {
                        // Passing all_current_objects into the function so that we can get the owner of the deleted
                        // resource if it was handled in the same batch
                        if let Some((object, current_object)) = Object::from_delete_resource(
                            inner,
                            txn_version,
                            index,
                            &all_current_objects,
                            &mut conn,
                        )
                        .unwrap()
                        {
                            all_objects.push(object.clone());
                            all_current_objects
                                .insert(object.object_address.clone(), current_object.clone());
                        }
                    },
```

**File:** crates/indexer/src/models/token_models/v2_collections.rs (L211-227)
```rust
                    match Self::get_collection_creator_for_v1(conn, &table_handle).context(format!(
                        "Failed to get collection creator for table handle {}, txn version {}",
                        table_handle, txn_version
                    )) {
                        Ok(ca) => ca,
                        Err(_) => {
                            // Try our best by getting from the older collection data
                            match CollectionData::get_collection_creator(conn, &table_handle) {
                                Ok(creator) => creator,
                                Err(_) => {
                                    aptos_logger::error!(
                                        transaction_version = txn_version,
                                        lookup_key = &table_handle,
                                        "Failed to get collection v2 creator for table handle. You probably should backfill db."
                                    );
                                    return Ok(None);
                                },
```
