# Audit Report

## Title
Network Address Spoofing via Missing x25519 Public Key Validation Enables Malicious Validator Operators to Redirect Peer Connections

## Summary
A malicious validator operator can register network addresses containing attacker-controlled x25519 public keys without cryptographic proof of possession, causing honest validators to establish authenticated connections to attacker-controlled servers instead of the legitimate validator. This enables network-level denial-of-service attacks and consensus message interception, though consensus safety is preserved through BLS signature verification.

## Finding Description

The vulnerability exists in the `update_network_and_fullnode_addresses` function which accepts arbitrary network addresses without validating that the embedded x25519 public keys belong to the validator. [1](#0-0) 

Unlike consensus key rotation which requires cryptographic proof-of-possession to prevent rogue-key attacks: [2](#0-1) 

The network address update performs only operator authentication but no validation of the x25519 public keys embedded in the addresses.

**Attack Flow:**

1. Malicious validator operator generates attacker-controlled x25519 keypair (K_attack_pub, K_attack_priv)
2. Operator constructs network addresses pointing to attacker servers: `/ip4/<attacker-ip>/tcp/6180/noise-ik/<K_attack_pub>/handshake/0`
3. Operator calls `update_network_and_fullnode_addresses` with these addresses (passes operator authentication)
4. Addresses are stored on-chain without validation
5. During peer discovery, honest validators extract these addresses: [3](#0-2) 

6. Honest validators parse the malicious network addresses and extract the attacker's public key: [4](#0-3) 

7. When dialing the peer, validators use the attacker's public key for the Noise IK handshake: [5](#0-4) 

8. The attacker's server successfully completes the handshake (possesses K_attack_priv), and honest validators believe they're connected to the legitimate validator
9. The attacker can now intercept, delay, or drop consensus messages

**Why Consensus Safety is Preserved:**

All consensus votes are BLS-signed and verified, preventing message forgery: [6](#0-5) 

**Why the Attack Still Succeeds:**

The peer authentication during Noise handshake only verifies that the remote peer possesses the private key for the advertised public key, not that this public key legitimately belongs to the validator: [7](#0-6) 

The `peer.keys` set is populated from the network addresses themselves, creating a circular trust: [8](#0-7) 

## Impact Explanation

**High Severity** per Aptos Bug Bounty criteria:

- **Validator node slowdowns**: Malicious operators can cause legitimate validators to connect to wrong servers, enabling selective message dropping or delays that slow down consensus
- **Significant protocol violations**: The peer discovery and connection establishment protocol is subverted, violating the assumption that ValidatorInfo contains legitimate validator endpoints

**Impact Scope:**
- Each compromised validator operator can redirect all peer connections intended for their validator
- Affects network liveness and consensus participation but NOT consensus safety (BLS signatures prevent message forgery)
- Can cause partial network partitions if multiple validators are compromised
- Does not enable theft of funds, chain splits, or double-spending

## Likelihood Explanation

**Moderate to High Likelihood:**

- Requires a malicious validator operator (insider threat)
- Trivial to execute once operator access is obtained (single transaction call)
- No technical complexity - just register addresses with different public keys
- Detection is limited: the `NETWORK_KEY_MISMATCH` metric only alerts the compromised validator about their own key mismatch, not other validators: [9](#0-8) 

- Other validators have no mechanism to detect that a peer's registered network key differs from their expected legitimate key

## Recommendation

Implement proof-of-possession validation for x25519 network keys similar to BLS consensus keys:

1. Require validators to sign a challenge with their x25519 private key when updating network addresses
2. Verify the signature matches the public key embedded in the network addresses before accepting the update
3. Consider storing the canonical x25519 public key separately in ValidatorConfig and validating that network addresses contain only this authorized key

**Proposed Fix for `stake.move`:**

```move
public entry fun update_network_and_fullnode_addresses(
    operator: &signer,
    pool_address: address,
    new_network_addresses: vector<u8>,
    new_fullnode_addresses: vector<u8>,
    network_key_proof: vector<u8>,  // Signature proving possession of network private key
) acquires StakePool, ValidatorConfig {
    // ... existing checks ...
    
    // Validate that network addresses contain only the authorized network public key
    // and that the operator possesses the corresponding private key
    assert!(
        verify_network_key_proof(pool_address, new_network_addresses, network_key_proof),
        error::invalid_argument(EINVALID_NETWORK_KEY_PROOF)
    );
    
    // ... rest of function ...
}
```

## Proof of Concept

```move
#[test(operator = @0x123, framework = @aptos_framework)]
public entry fun test_malicious_network_address_registration(
    operator: signer,
    framework: signer,
) {
    // Setup: Initialize a validator with legitimate network key
    let pool_address = @0x456;
    let legitimate_network_key = x"deadbeef..."; // Legitimate x25519 pubkey
    
    // Attack: Operator generates their own attacker-controlled keypair
    let attacker_network_key = x"cafebabe..."; // Attacker's x25519 pubkey
    
    // Construct malicious network address with attacker's public key
    // Format: /ip4/1.2.3.4/tcp/6180/noise-ik/<attacker_key>/handshake/0
    let malicious_addrs = construct_network_address(
        vector[1, 2, 3, 4], // Attacker IP
        6180,
        attacker_network_key,
        0
    );
    
    // This call succeeds - no validation of the public key!
    stake::update_network_and_fullnode_addresses(
        &operator,
        pool_address,
        bcs::to_bytes(&vector[malicious_addrs]),
        bcs::to_bytes(&vector[]),
    );
    
    // Verify: The malicious address is now registered on-chain
    let config = stake::get_validator_config(pool_address);
    let registered_addrs = bcs::from_bytes<vector<NetworkAddress>>(
        config.network_addresses
    );
    
    // Honest validators will now extract the attacker's public key
    // and connect to the attacker's server
    assert!(
        extract_noise_pubkey(registered_addrs[0]) == attacker_network_key,
        1
    );
}
```

## Notes

- This vulnerability requires validator operator access (insider threat scenario)
- The existing `NETWORK_KEY_MISMATCH` metric only detects when a validator's own registered key differs from their local key, not when other validators have registered malicious keys
- Genesis-time validation checks for duplicate network keys across validators but runtime updates have no such validation: [10](#0-9) 

- While consensus safety is preserved through BLS signature verification, network-level attacks can still cause significant liveness degradation and validator slowdowns qualifying as High severity per the bug bounty program

### Citations

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L910-952)
```text
    public entry fun rotate_consensus_key(
        operator: &signer,
        pool_address: address,
        new_consensus_pubkey: vector<u8>,
        proof_of_possession: vector<u8>,
    ) acquires StakePool, ValidatorConfig {
        check_stake_permission(operator);
        assert_reconfig_not_in_progress();
        assert_stake_pool_exists(pool_address);

        let stake_pool = borrow_global_mut<StakePool>(pool_address);
        assert!(signer::address_of(operator) == stake_pool.operator_address, error::unauthenticated(ENOT_OPERATOR));

        assert!(exists<ValidatorConfig>(pool_address), error::not_found(EVALIDATOR_CONFIG));
        let validator_info = borrow_global_mut<ValidatorConfig>(pool_address);
        let old_consensus_pubkey = validator_info.consensus_pubkey;
        // Checks the public key has a valid proof-of-possession to prevent rogue-key attacks.
        let pubkey_from_pop = &bls12381::public_key_from_bytes_with_pop(
            new_consensus_pubkey,
            &proof_of_possession_from_bytes(proof_of_possession)
        );
        assert!(option::is_some(pubkey_from_pop), error::invalid_argument(EINVALID_PUBLIC_KEY));
        validator_info.consensus_pubkey = new_consensus_pubkey;

        if (std::features::module_event_migration_enabled()) {
            event::emit(
                RotateConsensusKey {
                    pool_address,
                    old_consensus_pubkey,
                    new_consensus_pubkey,
                },
            );
        } else {
            event::emit_event(
                &mut stake_pool.rotate_consensus_key_events,
                RotateConsensusKeyEvent {
                    pool_address,
                    old_consensus_pubkey,
                    new_consensus_pubkey,
                },
            );
        };
    }
```

**File:** aptos-move/framework/aptos-framework/sources/stake.move (L954-995)
```text
    /// Update the network and full node addresses of the validator. This only takes effect in the next epoch.
    public entry fun update_network_and_fullnode_addresses(
        operator: &signer,
        pool_address: address,
        new_network_addresses: vector<u8>,
        new_fullnode_addresses: vector<u8>,
    ) acquires StakePool, ValidatorConfig {
        check_stake_permission(operator);
        assert_reconfig_not_in_progress();
        assert_stake_pool_exists(pool_address);
        let stake_pool = borrow_global_mut<StakePool>(pool_address);
        assert!(signer::address_of(operator) == stake_pool.operator_address, error::unauthenticated(ENOT_OPERATOR));
        assert!(exists<ValidatorConfig>(pool_address), error::not_found(EVALIDATOR_CONFIG));
        let validator_info = borrow_global_mut<ValidatorConfig>(pool_address);
        let old_network_addresses = validator_info.network_addresses;
        validator_info.network_addresses = new_network_addresses;
        let old_fullnode_addresses = validator_info.fullnode_addresses;
        validator_info.fullnode_addresses = new_fullnode_addresses;

        if (std::features::module_event_migration_enabled()) {
            event::emit(
                UpdateNetworkAndFullnodeAddresses {
                    pool_address,
                    old_network_addresses,
                    new_network_addresses,
                    old_fullnode_addresses,
                    new_fullnode_addresses,
                },
            );
        } else {
            event::emit_event(
                &mut stake_pool.update_network_and_fullnode_addresses_events,
                UpdateNetworkAndFullnodeAddressesEvent {
                    pool_address,
                    old_network_addresses,
                    new_network_addresses,
                    old_fullnode_addresses,
                    new_fullnode_addresses,
                },
            );
        };
    }
```

**File:** network/discovery/src/validator_set.rs (L44-66)
```rust
    fn find_key_mismatches(&self, onchain_keys: Option<&HashSet<x25519::PublicKey>>) {
        let mismatch = onchain_keys.map_or(0, |pubkeys| {
            if !pubkeys.contains(&self.expected_pubkey) {
                error!(
                    NetworkSchema::new(&self.network_context),
                    "Onchain pubkey {:?} differs from local pubkey {}",
                    pubkeys,
                    self.expected_pubkey
                );
                1
            } else {
                0
            }
        });

        NETWORK_KEY_MISMATCH
            .with_label_values(&[
                self.network_context.role().as_str(),
                self.network_context.network_id().as_str(),
                self.network_context.peer_id().short_str().as_str(),
            ])
            .set(mismatch);
    }
```

**File:** network/discovery/src/validator_set.rs (L108-150)
```rust
pub(crate) fn extract_validator_set_updates(
    network_context: NetworkContext,
    node_set: ValidatorSet,
) -> PeerSet {
    let is_validator = network_context.network_id().is_validator_network();

    // Decode addresses while ignoring bad addresses
    node_set
        .into_iter()
        .map(|info| {
            let peer_id = *info.account_address();
            let config = info.into_config();

            let addrs = if is_validator {
                config
                    .validator_network_addresses()
                    .map_err(anyhow::Error::from)
            } else {
                config
                    .fullnode_network_addresses()
                    .map_err(anyhow::Error::from)
            }
            .map_err(|err| {
                inc_by_with_context(&DISCOVERY_COUNTS, &network_context, "read_failure", 1);

                warn!(
                    NetworkSchema::new(&network_context),
                    "OnChainDiscovery: Failed to parse any network address: peer: {}, err: {}",
                    peer_id,
                    err
                )
            })
            .unwrap_or_default();

            let peer_role = if is_validator {
                PeerRole::Validator
            } else {
                PeerRole::ValidatorFullNode
            };
            (peer_id, Peer::from_addrs(peer_role, addrs))
        })
        .collect()
}
```

**File:** network/framework/src/transport/mod.rs (L471-513)
```rust
    fn parse_dial_addr(
        addr: &NetworkAddress,
    ) -> io::Result<(NetworkAddress, x25519::PublicKey, u8)> {
        use aptos_types::network_address::Protocol::*;

        let protos = addr.as_slice();

        // parse out the base transport protocol(s), which we will just ignore
        // and leave for the base_transport to actually parse and dial.
        // TODO(philiphayes): protos[..X] is kinda hacky. `Transport` trait
        // should handle this.
        let (base_transport_protos, base_transport_suffix) = parse_ip_tcp(protos)
            .map(|x| (&protos[..2], x.1))
            .or_else(|| parse_dns_tcp(protos).map(|x| (&protos[..2], x.1)))
            .or_else(|| parse_memory(protos).map(|x| (&protos[..1], x.1)))
            .ok_or_else(|| {
                io::Error::new(
                    io::ErrorKind::InvalidInput,
                    format!(
                        "Unexpected dialing network address: '{}', expected: \
                         memory, ip+tcp, or dns+tcp",
                        addr
                    ),
                )
            })?;

        // parse out the aptosnet protocols (noise ik and handshake)
        match base_transport_suffix {
            [NoiseIK(pubkey), Handshake(version)] => {
                let base_addr = NetworkAddress::try_from(base_transport_protos.to_vec())
                    .expect("base_transport_protos is always non-empty");
                Ok((base_addr, *pubkey, *version))
            },
            _ => Err(io::Error::new(
                io::ErrorKind::InvalidInput,
                format!(
                    "Unexpected dialing network address: '{}', expected: \
                     '/../noise-ik/<pubkey>/handshake/<version>'",
                    addr
                ),
            )),
        }
    }
```

**File:** network/framework/src/transport/mod.rs (L537-569)
```rust
    pub fn dial(
        &self,
        peer_id: PeerId,
        addr: NetworkAddress,
    ) -> io::Result<
        impl Future<Output = io::Result<Connection<NoiseStream<TTransport::Output>>>>
            + Send
            + 'static
            + use<TTransport>,
    > {
        // parse aptosnet protocols
        // TODO(philiphayes): `Transport` trait should include parsing in `dial`?
        let (base_addr, pubkey, handshake_version) = Self::parse_dial_addr(&addr)?;

        // Check that the parsed handshake version from the dial addr is supported.
        if self.ctxt.handshake_version != handshake_version {
            return Err(io::Error::new(
                io::ErrorKind::InvalidData,
                format!(
                    "Attempting to dial remote with unsupported handshake version: {}, expected: {}",
                    handshake_version, self.ctxt.handshake_version,
                ),
            ));
        }

        // try to connect socket
        let fut_socket = self.base_transport.dial(peer_id, base_addr)?;

        // outbound dial upgrade task
        let upgrade_fut = upgrade_outbound(self.ctxt.clone(), fut_socket, addr, peer_id, pubkey);
        let upgrade_fut = timeout_io(self.time_service.clone(), TRANSPORT_TIMEOUT, upgrade_fut);
        Ok(upgrade_fut)
    }
```

**File:** consensus/consensus-types/src/vote.rs (L149-175)
```rust
    /// Verifies that the consensus data hash of LedgerInfo corresponds to the vote info,
    /// and then verifies the signature.
    pub fn verify(&self, validator: &ValidatorVerifier) -> anyhow::Result<()> {
        // TODO(ibalajiarun): Ensure timeout is None if RoundTimeoutMsg is enabled.

        ensure!(
            self.ledger_info.consensus_data_hash() == self.vote_data.hash(),
            "Vote's hash mismatch with LedgerInfo"
        );
        validator
            .optimistic_verify(self.author(), &self.ledger_info, &self.signature)
            .context("Failed to verify Vote")?;
        if let Some((timeout, signature)) = &self.two_chain_timeout {
            ensure!(
                (timeout.epoch(), timeout.round())
                    == (self.epoch(), self.vote_data.proposed().round()),
                "2-chain timeout has different (epoch, round) than Vote"
            );
            timeout.verify(validator)?;
            validator
                .verify(self.author(), &timeout.signing_format(), signature)
                .context("Failed to verify 2-chain timeout signature")?;
        }
        // Let us verify the vote data as well
        self.vote_data().verify()?;
        Ok(())
    }
```

**File:** network/framework/src/noise/handshake.rs (L488-500)
```rust
    fn authenticate_inbound(
        remote_peer_short: ShortHexStr,
        peer: &Peer,
        remote_public_key: &x25519::PublicKey,
    ) -> Result<PeerRole, NoiseHandshakeError> {
        if !peer.keys.contains(remote_public_key) {
            return Err(NoiseHandshakeError::UnauthenticatedClientPubkey(
                remote_peer_short,
                hex::encode(remote_public_key.as_slice()),
            ));
        }
        Ok(peer.role)
    }
```

**File:** config/src/config/network_config.rs (L498-504)
```rust
    pub fn from_addrs(role: PeerRole, addresses: Vec<NetworkAddress>) -> Peer {
        let keys: HashSet<x25519::PublicKey> = addresses
            .iter()
            .filter_map(NetworkAddress::find_noise_proto)
            .collect();
        Peer::new(addresses, keys, role)
    }
```

**File:** crates/aptos/src/genesis/mod.rs (L775-825)
```rust

            match (
                validator.full_node_host.as_ref(),
                validator.full_node_network_public_key.as_ref(),
            ) {
                (None, None) => {
                    info!("Validator {} does not have a full node setup", name);
                },
                (Some(_), None) | (None, Some(_)) => {
                    errors.push(CliError::UnexpectedError(format!(
                        "Validator {} has a full node host or public key but not both",
                        name
                    )));
                },
                (Some(full_node_host), Some(full_node_network_public_key)) => {
                    // Ensure that the validator and the full node aren't the same
                    let validator_host = validator.validator_host.as_ref().unwrap();
                    let validator_network_public_key =
                        validator.validator_network_public_key.as_ref().unwrap();
                    if validator_host == full_node_host {
                        errors.push(CliError::UnexpectedError(format!(
                            "Validator {} has a validator and a full node host that are the same {:?}",
                            name,
                            validator_host
                        )));
                    }
                    if !unique_hosts.insert(validator.full_node_host.as_ref().unwrap().clone()) {
                        errors.push(CliError::UnexpectedError(format!(
                            "Validator {} has a repeated full node host {:?}",
                            name,
                            validator.full_node_host.as_ref().unwrap()
                        )));
                    }

                    if validator_network_public_key == full_node_network_public_key {
                        errors.push(CliError::UnexpectedError(format!(
                            "Validator {} has a validator and a full node network public key that are the same {}",
                            name,
                            validator_network_public_key
                        )));
                    }
                    if !unique_network_keys.insert(validator.full_node_network_public_key.unwrap())
                    {
                        errors.push(CliError::UnexpectedError(format!(
                            "Validator {} has a repeated full node network key {}",
                            name,
                            validator.full_node_network_public_key.unwrap()
                        )));
                    }
                },
            }
```
