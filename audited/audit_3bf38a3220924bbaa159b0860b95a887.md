# Audit Report

## Title
Epoch Transition Denial of Service: Stale Batch Loading Causes Validator Node Panic During Restart

## Summary
The quorum store batch management system loads batches from all epochs without epoch filtering when `is_new_epoch=false`, only checking expiration timestamps. When a validator node restarts within the same epoch after an epoch transition, stale batches from previous epochs that haven't expired are loaded into memory and consume storage quotas. If quotas are exhausted, the node panics during initialization and cannot recover, creating a denial of service condition.

## Finding Description

The vulnerability exists in the batch store initialization logic. When `BatchStore::new()` is called with `is_new_epoch=false`, the system calls `populate_cache_and_gc_expired_batches_v1()` and `populate_cache_and_gc_expired_batches_v2()` to restore the in-memory cache from persistent storage. [1](#0-0) 

The critical flaw is that `get_all_batches()` and `get_all_batches_v2()` return ALL batches from the database without any epoch filtering: [2](#0-1) [3](#0-2) 

The `populate_cache_and_gc_expired_batches_v1()` function only filters by expiration time, NOT by epoch: [4](#0-3) 

At lines 264-279, the code checks `if expiration < gc_timestamp` but does NOT check the batch epoch. Batches from previous epochs are loaded into the cache if they haven't expired yet. Line 278 contains `.expect("Storage limit exceeded upon BatchReader construction")` which panics if quota is exhausted. The same issue exists in `populate_cache_and_gc_expired_batches_v2()`: [5](#0-4) 

The `is_new_epoch` flag is determined by checking if the latest committed ledger info marks an epoch end: [6](#0-5) 

**Attack Scenario:**

1. **Epoch N - Batch Creation**: Validators create many batches with expiration times (default 60 seconds) in epoch N. These are stored in the quorum store database.

2. **Epoch Transition**: Epoch N ends and epoch N+1 begins. The epoch-ending block is committed.

3. **Progress in Epoch N+1**: Several blocks are committed in the new epoch. The latest ledger info no longer has `ends_epoch=true`.

4. **Validator Restart**: A validator node restarts (crash, maintenance, or upgrade).

5. **Vulnerability Triggered**: 
   - `is_new_epoch = latest_ledger_info.ends_epoch()` returns `false`
   - `populate_cache_and_gc_expired_batches_v1()` and `v2()` are called instead of epoch-aware GC functions
   - `get_all_batches()` retrieves batches from epoch N that haven't expired
   - These old batches are inserted into the cache, consuming quota
   - When quota is exhausted, `insert_to_cache()` returns an error
   - The `.expect()` panics with "Storage limit exceeded upon BatchReader construction"
   - **The validator node crashes and cannot start up**

The quota system enforces strict limits through `QuotaManager::update_quota()`: [7](#0-6) 

If `db_balance` or `batch_balance` is insufficient, the function returns an error which causes the panic.

In contrast, when `is_new_epoch=true`, the correct epoch-aware garbage collection runs: [8](#0-7) [9](#0-8) 

These functions check `if epoch < current_epoch` (lines 199 and 232) and delete old batches, preventing the resource exhaustion issue.

## Impact Explanation

This vulnerability qualifies as **HIGH severity** under the Aptos bug bounty program criteria:

- **Validator node crashes**: Affected validators panic during startup and cannot recover without manual intervention (database deletion or waiting for batch expiration, potentially minutes to hours depending on batch expiration settings)
- **Network availability impact**: If multiple validators are affected simultaneously (common after epoch transitions when nodes restart for upgrades), this could degrade network liveness
- **API crashes**: The panic occurs during initialization, effectively making the validator node unavailable

This maps to the HIGH severity category "API Crashes" - the validator node panics during initialization and cannot serve its consensus or API functions.

While this doesn't directly cause consensus safety violations or fund loss (which would be CRITICAL), the availability impact is severe enough to warrant HIGH severity classification, particularly given that:
1. It can affect multiple validators simultaneously during common operational patterns
2. Recovery requires manual intervention or extended downtime
3. It's easily triggerable after normal epoch transitions with validator restarts

## Likelihood Explanation

**Likelihood: HIGH**

This vulnerability is highly likely to occur in production:

1. **Natural Trigger Conditions**: Epoch transitions happen regularly in Aptos (when validator sets change). After an epoch transition, validators commonly restart for upgrades or maintenance. If a restart occurs after a few blocks have been committed in the new epoch, `is_new_epoch=false` is guaranteed.

2. **No Malicious Action Required**: The vulnerability can trigger from normal operations. The default batch expiration is 60 seconds: [10](#0-9) 

During high traffic, many batches could be created. If an epoch transition occurs and a validator restarts within 60 seconds, old batches are still unexpired and will be loaded.

3. **Common Operational Pattern**: The scenario (epoch transition → blocks committed → validator restart) is a standard operational pattern, not an edge case.

4. **Feasible Quota Exhaustion**: The default quotas are finite and can be exhausted with legitimate high-traffic operations:
   - `db_quota`: 300,000,000 bytes (~300 MB)
   - `batch_quota`: 300,000 batches

## Recommendation

Add epoch filtering to `populate_cache_and_gc_expired_batches_v1()` and `populate_cache_and_gc_expired_batches_v2()` to match the behavior of the epoch-aware GC functions. The functions should only load batches from the current epoch:

```rust
fn populate_cache_and_gc_expired_batches_v1(
    db: Arc<dyn QuorumStoreStorage>,
    current_epoch: u64,
    last_certified_time: u64,
    expiration_buffer_usecs: u64,
    batch_store: &BatchStore,
) {
    let db_content = db
        .get_all_batches()
        .expect("failed to read v1 data from db");
    
    let mut expired_keys = Vec::new();
    let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
    
    for (digest, value) in db_content {
        let epoch = value.epoch();
        let expiration = value.expiration();
        
        // Add epoch check here
        if epoch < current_epoch {
            expired_keys.push(digest);
        } else if expiration < gc_timestamp {
            expired_keys.push(digest);
        } else {
            batch_store
                .insert_to_cache(&value.into())
                .expect("Storage limit exceeded upon BatchReader construction");
        }
    }
    
    tokio::task::spawn_blocking(move || {
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    });
}
```

Apply the same fix to `populate_cache_and_gc_expired_batches_v2()`.

## Proof of Concept

The existing test `test_batch_store_bootstrap_gc_expiry` demonstrates the scenario but doesn't trigger the vulnerability due to generous quotas: [11](#0-10) 

To reproduce the vulnerability, modify this test to:
1. Use restrictive quotas (e.g., `db_quota: 100`, `batch_quota: 2`)
2. Create multiple batches in epoch 9 with long expiration
3. Create BatchStore for epoch 10 with `is_new_epoch=false`
4. Observe the panic: "Storage limit exceeded upon BatchReader construction"

The vulnerability occurs because the second `BatchStore::new()` call with the same epoch and `is_new_epoch=false` attempts to load all batches (including those from epoch 9) without epoch filtering, exhausting the quota and causing a panic.

## Notes

This is a logic bug in the batch loading mechanism that creates an asymmetry: when `is_new_epoch=true`, old batches are filtered by epoch, but when `is_new_epoch=false`, they are only filtered by expiration. This asymmetry can cause validator nodes to panic during restart, affecting network availability. The vulnerability doesn't require malicious intent - it can occur during legitimate high-traffic operations combined with normal epoch transitions and validator restarts.

### Citations

**File:** consensus/src/quorum_store/batch_store.rs (L64-84)
```rust
    pub(crate) fn update_quota(&mut self, num_bytes: usize) -> anyhow::Result<StorageMode> {
        if self.batch_balance == 0 {
            counters::EXCEEDED_BATCH_QUOTA_COUNT.inc();
            bail!("Batch quota exceeded ");
        }

        if self.db_balance >= num_bytes {
            self.batch_balance -= 1;
            self.db_balance -= num_bytes;

            if self.memory_balance >= num_bytes {
                self.memory_balance -= num_bytes;
                Ok(StorageMode::MemoryAndPersisted)
            } else {
                Ok(StorageMode::PersistedOnly)
            }
        } else {
            counters::EXCEEDED_STORAGE_QUOTA_COUNT.inc();
            bail!("Storage quota exceeded ");
        }
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L156-176)
```rust
        if is_new_epoch {
            tokio::task::spawn_blocking(move || {
                Self::gc_previous_epoch_batches_from_db_v1(db_clone.clone(), epoch);
                Self::gc_previous_epoch_batches_from_db_v2(db_clone, epoch);
            });
        } else {
            Self::populate_cache_and_gc_expired_batches_v1(
                db_clone.clone(),
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
            Self::populate_cache_and_gc_expired_batches_v2(
                db_clone,
                epoch,
                last_certified_time,
                expiration_buffer_usecs,
                &batch_store,
            );
        }
```

**File:** consensus/src/quorum_store/batch_store.rs (L181-210)
```rust
    fn gc_previous_epoch_batches_from_db_v1(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db.get_all_batches().expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L212-243)
```rust
    fn gc_previous_epoch_batches_from_db_v2(db: Arc<dyn QuorumStoreStorage>, current_epoch: u64) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read data from db");
        info!(
            epoch = current_epoch,
            "QS: Read batches from storage. Len: {}",
            db_content.len(),
        );

        let mut expired_keys = Vec::new();
        for (digest, value) in db_content {
            let epoch = value.epoch();

            trace!(
                "QS: Batchreader recovery content epoch {:?}, digest {}",
                epoch,
                digest
            );

            if epoch < current_epoch {
                expired_keys.push(digest);
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        db.delete_batches(expired_keys)
            .expect("Deletion of expired keys should not fail");
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L245-290)
```rust
    fn populate_cache_and_gc_expired_batches_v1(
        db: Arc<dyn QuorumStoreStorage>,
        current_epoch: u64,
        last_certified_time: u64,
        expiration_buffer_usecs: u64,
        batch_store: &BatchStore,
    ) {
        let db_content = db
            .get_all_batches()
            .expect("failed to read v1 data from db");
        info!(
            epoch = current_epoch,
            "QS: Read v1 batches from storage. Len: {}, Last Cerified Time: {}",
            db_content.len(),
            last_certified_time
        );

        let mut expired_keys = Vec::new();
        let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
        for (digest, value) in db_content {
            let expiration = value.expiration();

            trace!(
                "QS: Batchreader recovery content exp {:?}, digest {}",
                expiration,
                digest
            );

            if expiration < gc_timestamp {
                expired_keys.push(digest);
            } else {
                batch_store
                    .insert_to_cache(&value.into())
                    .expect("Storage limit exceeded upon BatchReader construction");
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        tokio::task::spawn_blocking(move || {
            db.delete_batches(expired_keys)
                .expect("Deletion of expired keys should not fail");
        });
    }
```

**File:** consensus/src/quorum_store/batch_store.rs (L292-336)
```rust
    fn populate_cache_and_gc_expired_batches_v2(
        db: Arc<dyn QuorumStoreStorage>,
        current_epoch: u64,
        last_certified_time: u64,
        expiration_buffer_usecs: u64,
        batch_store: &BatchStore,
    ) {
        let db_content = db
            .get_all_batches_v2()
            .expect("failed to read v1 data from db");
        info!(
            epoch = current_epoch,
            "QS: Read v1 batches from storage. Len: {}, Last Cerified Time: {}",
            db_content.len(),
            last_certified_time
        );

        let mut expired_keys = Vec::new();
        let gc_timestamp = last_certified_time.saturating_sub(expiration_buffer_usecs);
        for (digest, value) in db_content {
            let expiration = value.expiration();
            trace!(
                "QS: Batchreader recovery content exp {:?}, digest {}",
                expiration,
                digest
            );

            if expiration < gc_timestamp {
                expired_keys.push(digest);
            } else {
                batch_store
                    .insert_to_cache(&value)
                    .expect("Storage limit exceeded upon BatchReader construction");
            }
        }

        info!(
            "QS: Batch store bootstrap expired keys len {}",
            expired_keys.len()
        );
        tokio::task::spawn_blocking(move || {
            db.delete_batches_v2(expired_keys)
                .expect("Deletion of expired keys should not fail");
        });
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L103-108)
```rust
    fn get_all_batches(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfo>>> {
        let mut iter = self.db.iter::<BatchSchema>()?;
        iter.seek_to_first();
        iter.map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<HashValue, PersistedValue<BatchInfo>>>>()
    }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L133-138)
```rust
    fn get_all_batches_v2(&self) -> Result<HashMap<HashValue, PersistedValue<BatchInfoExt>>> {
        let mut iter = self.db.iter::<BatchV2Schema>()?;
        iter.seek_to_first();
        iter.map(|res| res.map_err(Into::into))
            .collect::<Result<HashMap<HashValue, PersistedValue<BatchInfoExt>>>>()
    }
```

**File:** consensus/src/quorum_store/quorum_store_builder.rs (L239-258)
```rust
        let latest_ledger_info_with_sigs = self
            .aptos_db
            .get_latest_ledger_info()
            .expect("could not get latest ledger info");
        let last_committed_timestamp = latest_ledger_info_with_sigs.commit_info().timestamp_usecs();
        let is_new_epoch = latest_ledger_info_with_sigs.ledger_info().ends_epoch();

        let batch_requester = BatchRequester::new(
            self.epoch,
            self.author,
            self.config.batch_request_num_peers,
            self.config.batch_request_retry_limit,
            self.config.batch_request_retry_interval_ms,
            self.config.batch_request_rpc_timeout_ms,
            self.network_sender.clone(),
            self.verifier.clone(),
        );
        let batch_store = Arc::new(BatchStore::new(
            self.epoch,
            is_new_epoch,
```

**File:** config/src/config/quorum_store_config.rs (L131-135)
```rust
            batch_expiry_gap_when_init_usecs: Duration::from_secs(60).as_micros() as u64,
            remote_batch_expiry_gap_when_init_usecs: Duration::from_millis(500).as_micros() as u64,
            memory_quota: 120_000_000,
            db_quota: 300_000_000,
            batch_quota: 300_000,
```

**File:** consensus/src/quorum_store/tests/batch_store_test.rs (L289-338)
```rust
fn test_batch_store_bootstrap_gc_expiry() {
    let tmp_dir = TempPath::new();
    let digest_1 = HashValue::random();
    let memory_quota = 10;
    let tmp_dir_path = tmp_dir.path().to_path_buf();
    let tmp_dir_path_clone = tmp_dir_path.clone();

    let runtime = tokio::runtime::Builder::new_multi_thread().build().unwrap();
    runtime.block_on(async move {
        let db = Arc::new(QuorumStoreDB::new(&tmp_dir_path_clone));
        let (signers, _validator_verifier) = random_validator_verifier(4, None, false);

        let store = Arc::new(BatchStore::new(
            10, // epoch
            false,
            10, // last committed round
            db,
            memory_quota, // memory_quota
            2001,         // db quota
            2001,         // batch quota
            signers[0].clone(),
            0,
        ));

        let request_1 = request_for_test(&digest_1, 50, 20, Some(vec![]));
        // Should be stored in memory and DB.
        assert!(!store.persist(vec![request_1]).is_empty());
    });
    drop(runtime);

    let runtime = tokio::runtime::Builder::new_multi_thread().build().unwrap();
    runtime.block_on(async move {
        let db = Arc::new(QuorumStoreDB::new(&tmp_dir));
        let (signers, _validator_verifier) = random_validator_verifier(4, None, false);

        let store = Arc::new(BatchStore::new(
            10, // epoch
            false,
            45, // last committed round
            db,
            memory_quota, // memory_quota
            2001,         // db quota
            2001,         // batch quota
            signers[0].clone(),
            10,
        ));

        store.get_batch_from_local(&digest_1).unwrap();
    });
}
```
