# Audit Report

## Title
BatchInfo Size Field Validation Bypass Leading to Consensus Disagreement and Resource Exhaustion

## Summary
The `BatchInfo` struct's `num_txns` and `num_bytes` fields are not validated against actual payload content for inline batches, allowing malicious validators to specify arbitrary values that cause inconsistent size computations across different code paths, leading to consensus disagreement and potential resource exhaustion.

## Finding Description

The vulnerability exists in how inline batches are validated and processed across the consensus layer. The critical flaw is a validation gap in `Payload::verify_inline_batches()` which only verifies the payload digest but not the `num_txns` and `num_bytes` metadata fields. [1](#0-0) 

This function only checks that the computed digest matches the stored digest, but never validates that the actual transaction count and byte size match the claimed `num_txns` and `num_bytes` fields in `BatchInfo`. [2](#0-1) 

The `BatchInfo` struct stores these fields without any runtime invariant enforcement. When inline batches are included in proposals, there are three critical inconsistencies:

**Inconsistency 1: Batch Selection vs Validation**

During batch selection for proposals, `BatchProofQueue` uses `batch.size()` which calls `PayloadTxnsSize::new()`: [3](#0-2) 

This normalization occurs here: [4](#0-3) 

However, when validators receive the proposal and validate payload size: [5](#0-4) 

The `payload.size()` method for inline batches uses the RAW, unnormalized `batch_info.num_bytes()` value: [6](#0-5) 

**Inconsistency 2: Count vs Bytes**

The `payload.len()` method counts actual transactions: [7](#0-6) 

But `payload.size()` uses the claimed byte size from metadata, not actual transaction bytes.

**Attack Scenario:**

1. Malicious validator creates a batch with 10 actual transactions totaling 1000 bytes
2. Sets `num_txns=100`, `num_bytes=10` in `BatchInfo` (violating the invariant that count â‰¤ bytes)
3. Computes correct digest from actual transactions and broadcasts as inline batch in proposal
4. During proposer's batch selection: `batch.size()` normalizes to `(100, 100)`, proposer thinks batch is 100 bytes
5. Proposer may reject other valid batches thinking block is nearly full
6. Validators receive proposal and call `payload.size()` using raw `num_bytes=10`, accepting the proposal
7. Execution layer processes 1000 actual bytes, causing resource exhaustion

Alternatively, underreporting (setting `num_bytes=1000` when actual is 100000 bytes) allows bypassing block size limits, causing validators to accept oversized blocks.

## Impact Explanation

This vulnerability has **High Severity** impact according to Aptos bug bounty criteria:

1. **Validator Node Slowdowns**: Malicious validators can force honest validators to execute blocks with far more transactions than the configured limits, causing processing delays and degraded network performance.

2. **Significant Protocol Violations**: The vulnerability breaks the "Resource Limits" critical invariant (#9) that "all operations must respect gas, storage, and computational limits." It also violates "Deterministic Execution" (#1) as validators compute different payload sizes during selection vs validation.

3. **Consensus Liveness Impact**: A sophisticated attacker could craft batches that pass validation on some validators but cause resource exhaustion on others, potentially stalling consensus or causing inconsistent voting behavior.

## Likelihood Explanation

The likelihood is **HIGH** because:

1. **No Special Privileges Required**: Any validator can create and broadcast malicious `BatchInfo` structures with inconsistent size fields
2. **Simple to Exploit**: The attack only requires setting incorrect metadata fields; no complex cryptographic attacks needed
3. **Difficult to Detect**: The digest verification passes because it only validates transaction hashes, not metadata
4. **Existing Code Paths**: The vulnerable normalization and validation paths are actively used in production consensus code

## Recommendation

Implement strict validation of `num_txns` and `num_bytes` fields against actual payload content for inline batches:

```rust
pub fn verify_inline_batches<'a, T: TBatchInfo + 'a>(
    inline_batches: impl Iterator<Item = (&'a T, &'a Vec<SignedTransaction>)>,
) -> anyhow::Result<()> {
    for (batch, payload) in inline_batches {
        // Existing digest check
        let computed_digest = BatchPayload::new(batch.author(), payload.clone()).hash();
        ensure!(
            computed_digest == *batch.digest(),
            "Hash of the received inline batch doesn't match the digest value"
        );
        
        // NEW: Validate num_txns matches actual count
        ensure!(
            batch.num_txns() == payload.len() as u64,
            "BatchInfo num_txns {} doesn't match actual transaction count {}",
            batch.num_txns(),
            payload.len()
        );
        
        // NEW: Validate num_bytes matches actual size
        let actual_bytes = BatchPayload::new(batch.author(), payload.clone()).num_bytes();
        ensure!(
            batch.num_bytes() == actual_bytes as u64,
            "BatchInfo num_bytes {} doesn't match actual payload size {}",
            batch.num_bytes(),
            actual_bytes
        );
    }
    Ok(())
}
```

Additionally, enforce the `PayloadTxnsSize` invariants at `BatchInfo` construction time rather than normalizing silently:

```rust
impl BatchInfo {
    pub fn new(
        author: PeerId,
        batch_id: BatchId,
        epoch: u64,
        expiration: u64,
        digest: HashValue,
        num_txns: u64,
        num_bytes: u64,
        gas_bucket_start: u64,
    ) -> anyhow::Result<Self> {
        // Enforce invariant: count <= bytes
        ensure!(
            num_txns <= num_bytes,
            "Invalid BatchInfo: num_txns {} exceeds num_bytes {}",
            num_txns,
            num_bytes
        );
        Ok(Self {
            author,
            batch_id,
            epoch,
            expiration,
            digest,
            num_txns,
            num_bytes,
            gas_bucket_start,
        })
    }
}
```

## Proof of Concept

```rust
#[test]
fn test_inline_batch_size_mismatch_attack() {
    use aptos_consensus_types::common::Payload;
    use aptos_consensus_types::proof_of_store::BatchInfo;
    use aptos_types::transaction::SignedTransaction;
    
    // Create actual transactions (10 txns, ~1000 bytes)
    let actual_txns: Vec<SignedTransaction> = create_test_transactions(10);
    let actual_payload = BatchPayload::new(PeerId::random(), actual_txns.clone());
    let actual_bytes = actual_payload.num_bytes();
    
    // Malicious BatchInfo with WRONG size metadata
    let malicious_batch_info = BatchInfo::new(
        PeerId::random(),
        BatchId::new(0),
        1, // epoch
        u64::MAX, // expiration
        actual_payload.hash(), // Correct digest
        100, // WRONG: claim 100 txns when there are only 10
        10,  // WRONG: claim 10 bytes when there are ~1000
        0,
    );
    
    // Create inline batch payload
    let inline_batches = vec![(malicious_batch_info.clone(), actual_txns)];
    let payload = Payload::QuorumStoreInlineHybrid(
        inline_batches,
        ProofWithData::empty(),
        None,
    );
    
    // Verification PASSES (only checks digest)
    let verifier = ValidatorVerifier::new(vec![]);
    let proof_cache = ProofCache::new(1000);
    assert!(payload.verify(&verifier, &proof_cache, true).is_ok());
    
    // But size computation is WRONG
    let claimed_size = payload.size(); // Returns 10 (from malicious num_bytes)
    let actual_size = actual_bytes;    // Actually ~1000 bytes
    
    // Consensus disagreement: proposer normalized to 100, validators see 10, 
    // but execution must process 1000 actual bytes
    assert_ne!(claimed_size, actual_size);
    println!("Claimed size: {}, Actual size: {}", claimed_size, actual_size);
    
    // This allows bypassing max_receiving_block_bytes limits
}
```

## Notes

This vulnerability demonstrates a critical gap between cryptographic verification (digest-based) and resource accounting (size-based). The fix requires validating all resource-related metadata fields against actual payload content, not just cryptographic commitments. The issue is particularly severe because it affects both block proposal generation and validation, creating multiple attack vectors for resource exhaustion and consensus manipulation.

### Citations

**File:** consensus/consensus-types/src/common.rs (L292-299)
```rust
            Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _)
            | Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _) => {
                proof_with_data.num_txns()
                    + inline_batches
                        .iter()
                        .map(|(_, txns)| txns.len())
                        .sum::<usize>()
            },
```

**File:** consensus/consensus-types/src/common.rs (L505-512)
```rust
            Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _)
            | Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _) => {
                proof_with_data.num_bytes()
                    + inline_batches
                        .iter()
                        .map(|(batch_info, _)| batch_info.num_bytes() as usize)
                        .sum::<usize>()
            },
```

**File:** consensus/consensus-types/src/common.rs (L541-556)
```rust
    pub fn verify_inline_batches<'a, T: TBatchInfo + 'a>(
        inline_batches: impl Iterator<Item = (&'a T, &'a Vec<SignedTransaction>)>,
    ) -> anyhow::Result<()> {
        for (batch, payload) in inline_batches {
            // TODO: Can cloning be avoided here?
            let computed_digest = BatchPayload::new(batch.author(), payload.clone()).hash();
            ensure!(
                computed_digest == *batch.digest(),
                "Hash of the received inline batch doesn't match the digest value for batch {:?}: {} != {}",
                batch,
                computed_digest,
                batch.digest()
            );
        }
        Ok(())
    }
```

**File:** consensus/consensus-types/src/proof_of_store.rs (L49-58)
```rust
pub struct BatchInfo {
    author: PeerId,
    batch_id: BatchId,
    epoch: u64,
    expiration: u64,
    digest: HashValue,
    num_txns: u64,
    num_bytes: u64,
    gas_bucket_start: u64,
}
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L651-658)
```rust
                        if cur_all_txns + batch.size() > max_txns
                            || unique_txns > max_txns_after_filtering
                        {
                            // Exceeded the limit for requested bytes or number of transactions.
                            full = true;
                            return false;
                        }
                        cur_all_txns += batch.size();
```

**File:** consensus/consensus-types/src/utils.rs (L20-44)
```rust
    pub fn new(count: u64, bytes: u64) -> Self {
        match Self::try_new(count, bytes) {
            Ok(txns_size) => txns_size,
            Err(err) => {
                warn!(
                    "Invalid input for PayloadTxnsSize. Normalizing. Count: {}, Bytes: {}, Err: {}",
                    count, bytes, err
                );
                Self::new_normalized(count, bytes)
            },
        }
    }

    fn new_normalized(count: u64, bytes: u64) -> Self {
        let mut count = count;
        let mut bytes = bytes;
        if count > bytes {
            bytes = count;
        }
        if count == 0 || bytes == 0 {
            count = 0;
            bytes = 0;
        }
        Self { count, bytes }
    }
```

**File:** consensus/src/round_manager.rs (L1178-1193)
```rust
        let payload_len = proposal.payload().map_or(0, |payload| payload.len());
        let payload_size = proposal.payload().map_or(0, |payload| payload.size());
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```
