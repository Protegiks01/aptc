# Audit Report

## Title
Lack of Error-Network Event Correlation Enables Undetected Coordinated Byzantine Attacks

## Summary
The peer monitoring service client does not subscribe to network connection/disconnection events and cannot correlate error patterns across multiple peers with network-level events. This architectural gap prevents detection of coordinated Byzantine attacks where multiple malicious peers synchronize their misbehavior and connection patterns.

## Finding Description

The peer monitoring service client tracks peer errors in isolation without correlating them with network connection events or cross-peer patterns. The system has two separate mechanisms that never interact:

**Error Tracking (Per-Peer)**: Each peer has a `RequestTracker` that counts consecutive failures [1](#0-0) , incremented on each error [2](#0-1) 

**Network Events**: The `PeersAndMetadata` system broadcasts `ConnectionNotification::LostPeer` and `NewPeer` events when peers disconnect/connect [3](#0-2) , and provides a subscription mechanism [4](#0-3) 

**Missing Correlation**: The peer monitoring service client never subscribes to connection notifications [5](#0-4) . It only polls for connected peers periodically and performs garbage collection on disconnected peers [6](#0-5) 

Error handling occurs independently per peer without timing correlation [7](#0-6) , and errors are only logged with basic metadata [8](#0-7) 

**Attack Scenario**: Byzantine validators can:
1. Coordinate to simultaneously fail requests across multiple peers
2. Immediately disconnect after sending errors to avoid sustained monitoring
3. Reconnect with clean state after error counters reset through garbage collection
4. Repeat this pattern to interfere with consensus without triggering per-peer failure thresholds

## Impact Explanation

**Medium Severity** - This represents a state monitoring weakness requiring operational intervention:

- Does not directly break consensus safety (AptosBFT still requires 2f+1 honest votes)
- Does not cause immediate fund loss
- However, enables Byzantine validators to mask coordinated misbehavior, making it significantly harder to detect and respond to ongoing attacks
- Operators cannot distinguish between normal network issues and sophisticated coordinated attacks
- The lack of cross-peer correlation means attack patterns that would be obvious when viewed holistically remain invisible

Per the Aptos bug bounty, this fits "state inconsistencies requiring intervention" as operators must manually investigate suspicious patterns without automated tooling.

## Likelihood Explanation

**High Likelihood** of exploitation by sophisticated Byzantine actors:

- No technical barriers - Byzantine validators simply coordinate timing
- Error state resets when peers disconnect/garbage collect, incentivizing this pattern
- Current metrics only track per-peer counters, missing aggregate patterns
- Detection requires manual log analysis across multiple nodes

## Recommendation

Implement event correlation by subscribing to connection notifications and tracking temporal patterns:

```rust
// In peer-monitoring-service/client/src/lib.rs
pub async fn start_peer_monitor_with_correlation(
    node_config: NodeConfig,
    peer_monitoring_client: PeerMonitoringServiceClient<NetworkClient<PeerMonitoringServiceMessage>>,
    peer_monitor_state: PeerMonitorState,
    time_service: TimeService,
    runtime: Option<Handle>,
) {
    let peers_and_metadata = peer_monitoring_client.get_peers_and_metadata();
    
    // NEW: Subscribe to connection events
    let mut connection_notifications = peers_and_metadata.subscribe();
    
    // NEW: Spawn correlation task
    let correlation_state = Arc::new(RwLock::new(CorrelationState::new()));
    spawn_correlation_task(
        correlation_state.clone(),
        connection_notifications,
        peer_monitor_state.clone(),
    );
    
    // ... existing monitoring loop
}

// NEW: Track cross-peer patterns
struct CorrelationState {
    recent_errors: VecDeque<(PeerNetworkId, Instant, ErrorType)>,
    recent_disconnects: VecDeque<(PeerNetworkId, Instant)>,
}

impl CorrelationState {
    fn detect_coordinated_attack(&self) -> Option<AttackPattern> {
        // Check for multiple peers failing within short window
        // Check for errors followed by immediate disconnections
        // Check for synchronized reconnection patterns
    }
}
```

Additionally:
- Add metrics for cross-peer error correlation
- Implement alerts for suspicious patterns (>3 peers erroring within 10s window)
- Preserve error history across disconnection/reconnection cycles
- Log coordinated patterns as `SecurityEvent::CoordinatedByzantineBehavior`

## Proof of Concept

While a full PoC requires multiple coordinated validator nodes, the gap can be demonstrated:

```rust
#[tokio::test]
async fn test_missing_correlation_allows_coordinated_attack() {
    // Setup multiple mock peers
    let peer_monitor_state = PeerMonitorState::new();
    let time_service = TimeService::mock();
    
    // Simulate 5 Byzantine peers
    for i in 0..5 {
        let peer_id = PeerNetworkId::random();
        peer_monitor_state.peer_states.write().insert(
            peer_id,
            PeerState::new(NodeConfig::default(), time_service.clone()),
        );
    }
    
    // Byzantine attack: All 5 peers fail requests simultaneously
    let attack_time = time_service.now();
    for peer_id in peer_monitor_state.peer_states.read().keys() {
        // Simulate error for each peer
        let peer_state = peer_monitor_state.peer_states.read().get(peer_id).unwrap().clone();
        let state_value = peer_state.get_peer_state_value(&PeerStateKey::LatencyInfo).unwrap();
        state_value.write().handle_monitoring_service_response_error(
            peer_id,
            Error::NetworkError("Coordinated attack".to_string()),
        );
    }
    
    // All peers disconnect immediately after errors
    // In real system, this would be ConnectionNotification::LostPeer events
    // But peer monitoring service doesn't subscribe to these events!
    
    // ISSUE: No detection mechanism exists to flag this obvious coordinated pattern
    // Each peer is tracked independently, pattern goes unnoticed
    
    // After garbage collection, peers can reconnect with clean state
    // and repeat the attack without triggering cumulative thresholds
}
```

## Notes

This is an architectural security gap rather than a traditional code vulnerability. The peer monitoring service was designed for performance monitoring, not Byzantine detection. However, given its role in tracking peer health and its access to both error streams and network events, it represents the ideal location for implementing coordinated attack detection. The missing correlation capability significantly weakens the network's ability to identify and respond to sophisticated Byzantine behavior, particularly below the consensus fault tolerance threshold where such behavior can still degrade performance and reliability.

### Citations

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L10-19)
```rust
/// A simple container that tracks request and response states
#[derive(Clone, Debug)]
pub struct RequestTracker {
    in_flight_request: bool, // If there is a request currently in-flight
    last_request_time: Option<Instant>, // The most recent request time
    last_response_time: Option<Instant>, // The most recent response time
    num_consecutive_request_failures: u64, // The number of consecutive request failures
    request_interval_usec: u64, // The interval (usec) between requests
    time_service: TimeService, // The time service to use for duration calculation
}
```

**File:** peer-monitoring-service/client/src/peer_states/request_tracker.rs (L101-104)
```rust
    /// Records a failure for the request
    pub fn record_response_failure(&mut self) {
        self.num_consecutive_request_failures += 1;
    }
```

**File:** network/framework/src/application/storage.rs (L371-395)
```rust
    fn broadcast(&self, event: ConnectionNotification) {
        let mut listeners = self.subscribers.lock();
        let mut to_del = vec![];
        for i in 0..listeners.len() {
            let dest = listeners.get_mut(i).unwrap();
            if let Err(err) = dest.try_send(event.clone()) {
                match err {
                    TrySendError::Full(_) => {
                        // Tried to send to an app, but the app isn't handling its messages fast enough.
                        // Drop message. Maybe increment a metrics counter?
                        sample!(
                            SampleRate::Duration(Duration::from_secs(1)),
                            warn!("PeersAndMetadata.broadcast() failed, some app is slow"),
                        );
                    },
                    TrySendError::Closed(_) => {
                        to_del.push(i);
                    },
                }
            }
        }
        for evict in to_del.into_iter() {
            listeners.swap_remove(evict);
        }
    }
```

**File:** network/framework/src/application/storage.rs (L397-419)
```rust
    /// subscribe() returns a channel for receiving NewPeer/LostPeer events.
    /// subscribe() immediately sends all* current connections as NewPeer events.
    /// (* capped at NOTIFICATION_BACKLOG, currently 1000, use get_connected_peers() to be sure)
    pub fn subscribe(&self) -> tokio::sync::mpsc::Receiver<ConnectionNotification> {
        let (sender, receiver) = tokio::sync::mpsc::channel(NOTIFICATION_BACKLOG);
        let peers_and_metadata = self.peers_and_metadata.read();
        'outer: for (network_id, network_peers_and_metadata) in peers_and_metadata.iter() {
            for (_addr, peer_metadata) in network_peers_and_metadata.iter() {
                let event = ConnectionNotification::NewPeer(
                    peer_metadata.connection_metadata.clone(),
                    *network_id,
                );
                if let Err(err) = sender.try_send(event) {
                    warn!("could not send initial NewPeer on subscribe(): {:?}", err);
                    break 'outer;
                }
            }
        }
        // I expect the peers_and_metadata read lock to still be in effect until after listeners.push() below
        let mut listeners = self.subscribers.lock();
        listeners.push(sender);
        receiver
    }
```

**File:** peer-monitoring-service/client/src/lib.rs (L59-87)
```rust
pub async fn start_peer_monitor(
    node_config: NodeConfig,
    network_client: NetworkClient<PeerMonitoringServiceMessage>,
    runtime: Option<Handle>,
) {
    // Create a new monitoring client and peer monitor state
    let peer_monitoring_client = PeerMonitoringServiceClient::new(network_client);
    let peer_monitor_state = PeerMonitorState::new();

    // Spawn the peer metadata updater
    let time_service = TimeService::real();
    spawn_peer_metadata_updater(
        node_config.peer_monitoring_service,
        peer_monitor_state.clone(),
        peer_monitoring_client.get_peers_and_metadata(),
        time_service.clone(),
        runtime.clone(),
    );

    // Start the peer monitor
    start_peer_monitor_with_state(
        node_config,
        peer_monitoring_client,
        peer_monitor_state,
        time_service,
        runtime,
    )
    .await
}
```

**File:** peer-monitoring-service/client/src/lib.rs (L131-156)
```rust
        // Garbage collect the peer states (to remove disconnected peers)
        garbage_collect_peer_states(&peer_monitor_state, &connected_peers_and_metadata);

        // Ensure all peers have a state (and create one for newly connected peers)
        create_states_for_new_peers(
            &node_config,
            &peer_monitor_state,
            &time_service,
            &connected_peers_and_metadata,
        );

        // Refresh the peer states
        if let Err(error) = peer_states::refresh_peer_states(
            &monitoring_service_config,
            peer_monitor_state.clone(),
            peer_monitoring_client.clone(),
            connected_peers_and_metadata,
            time_service.clone(),
            runtime.clone(),
        ) {
            warn!(LogSchema::new(LogEntry::PeerMonitorLoop)
                .event(LogEvent::UnexpectedErrorEncountered)
                .error(&error)
                .message("Failed to refresh peer states!"));
        }
    }
```

**File:** peer-monitoring-service/client/src/peer_states/latency_info.rs (L196-211)
```rust

    fn handle_monitoring_service_response_error(
        &mut self,
        peer_network_id: &PeerNetworkId,
        error: Error,
    ) {
        // Handle the failure
        self.handle_request_failure(peer_network_id);

        // Log the error
        warn!(LogSchema::new(LogEntry::LatencyPing)
            .event(LogEvent::ResponseError)
            .message("Error encountered when pinging peer!")
            .peer(peer_network_id)
            .error(&error));
    }
```

**File:** peer-monitoring-service/client/src/network.rs (L116-132)
```rust
        Err(error) => {
            warn!(
                (LogSchema::new(LogEntry::SendRequest)
                    .event(LogEvent::ResponseError)
                    .request_type(request.get_label())
                    .request_id(request_id)
                    .peer(peer_network_id)
                    .error(&error))
            );
            metrics::increment_request_counter(
                &metrics::ERROR_RESPONSES,
                error.get_label(),
                peer_network_id,
            );
            Err(error)
        },
    }
```
