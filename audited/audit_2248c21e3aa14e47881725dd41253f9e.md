# Audit Report

## Title
Configuration Validation Bypass Causes PFN Panic via Invalid Load Balancing Parameters

## Summary
The `max_number_of_upstream_peers` field in `LoadBalancingThresholdConfig` can be set to 0 without validation, and when combined with `latency_slack_between_top_upstream_peers` = 0, causes a panic in the peer selection logic, resulting in a Public Fullnode (PFN) crash.

## Finding Description

The `MempoolConfig::sanitize()` function contains no validation for `LoadBalancingThresholdConfig` fields, despite having a TODO comment indicating the need for "reasonable verifications": [1](#0-0) 

Both `max_number_of_upstream_peers` (u8) and `latency_slack_between_top_upstream_peers` (u64) can be set to 0 in the configuration: [2](#0-1) 

In `priority.rs`, the load balancing logic attempts to select upstream peers based on these thresholds. When `max_number_of_upstream_peers` is set to 0, the code applies a safety wrapper `max(1, ...)` to ensure at least 1 peer: [3](#0-2) 

However, the peer selection loop uses strict latency comparison (`<` operator, not `<=`) with the configured `latency_slack_between_top_upstream_peers`: [4](#0-3) 

**The Vulnerability Chain:**

1. Operator configures: `max_number_of_upstream_peers: 0` and `latency_slack_between_top_upstream_peers: 0`
2. System calculates: `num_top_peers = max(1, min(num_sender_buckets, 0)) = 1`
3. For the first peer with lowest latency: `base_ping_latency = X`
4. Loop checks: `ping_latency < base_ping_latency + 0` → `X < X` → **FALSE**
5. First peer is NOT added; all subsequent peers have higher latency and also fail
6. Result: `top_peers` remains **empty** despite `num_top_peers = 1`
7. Assertion fails and PFN **panics**: [5](#0-4) 

The panic occurs during peer priority updates when the PFN attempts to assign sender buckets with an empty `top_peers` list: [6](#0-5) 

## Impact Explanation

This vulnerability causes **PFN crashes (panic)**, meeting the **High Severity** criteria per the Aptos bug bounty program: "API crashes". 

While this requires configuration control, it represents a critical lack of input validation that allows invalid states to crash production nodes. PFN crashes affect:
- Service availability for users relying on that PFN
- Transaction submission and query capabilities
- Network resilience (reduced number of operational PFNs)

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits" - the system should gracefully handle invalid configurations rather than panicking.

## Likelihood Explanation

**Medium likelihood** due to:
- Operators have full control over their node configurations
- No validation prevents invalid values
- Could occur through:
  - Honest misconfiguration (setting 0 thinking it disables load balancing)
  - Copy-paste errors from example configs
  - Automated config generation bugs
- The TODO comment in sanitize() indicates developers recognized validation was needed but incomplete

While this requires operator action, the complete absence of validation makes this a realistic threat, especially for operators unfamiliar with the load balancing implementation details.

## Recommendation

Add comprehensive validation in `MempoolConfig::sanitize()`:

```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        // Validate load balancing thresholds
        for (idx, threshold) in self.load_balancing_thresholds.iter().enumerate() {
            if threshold.max_number_of_upstream_peers == 0 {
                return Err(Error::ConfigSanitizerFailed(format!(
                    "load_balancing_thresholds[{}].max_number_of_upstream_peers must be at least 1",
                    idx
                )));
            }
            if threshold.latency_slack_between_top_upstream_peers == 0 {
                return Err(Error::ConfigSanitizerFailed(format!(
                    "load_balancing_thresholds[{}].latency_slack_between_top_upstream_peers must be positive",
                    idx
                )));
            }
        }
        
        // Validate thresholds are in ascending order
        for i in 1..self.load_balancing_thresholds.len() {
            if self.load_balancing_thresholds[i].avg_mempool_traffic_threshold_in_tps
                <= self.load_balancing_thresholds[i - 1].avg_mempool_traffic_threshold_in_tps
            {
                return Err(Error::ConfigSanitizerFailed(
                    "load_balancing_thresholds must be in ascending order by traffic threshold".to_string()
                ));
            }
        }
        
        Ok(())
    }
}
```

Additionally, change the comparison to `<=` to be more defensive:
```rust
|| ping_latency.unwrap() <= base_ping_latency.unwrap() + (threshold_config.latency_slack_between_top_upstream_peers as f64) / 1000.0
```

## Proof of Concept

**Rust Test Case:**

```rust
#[test]
#[should_panic(expected = "assertion failed")]
fn test_zero_max_peers_with_zero_slack_causes_panic() {
    use aptos_config::config::{MempoolConfig, LoadBalancingThresholdConfig};
    use aptos_time_service::TimeService;
    
    // Create malicious config with both values set to 0
    let mut mempool_config = MempoolConfig::default();
    mempool_config.load_balancing_thresholds = vec![
        LoadBalancingThresholdConfig {
            avg_mempool_traffic_threshold_in_tps: 500,
            latency_slack_between_top_upstream_peers: 0, // ZERO slack
            max_number_of_upstream_peers: 0, // ZERO max peers
        },
    ];
    
    let mut prioritized_peers_state = PrioritizedPeersState::new(
        mempool_config,
        NodeType::PublicFullnode,
        TimeService::mock(),
    );
    
    // Create peers with latency metadata
    let peer1 = create_public_peer();
    let metadata1 = create_metadata_with_latency(Some(0.1));
    let peer2 = create_public_peer_2();
    let metadata2 = create_metadata_with_latency(Some(0.2));
    
    let peers_and_metadata = vec![
        (peer1, Some(&metadata1)),
        (peer2, Some(&metadata2)),
    ];
    
    // This will panic due to assertion failure on line 397
    prioritized_peers_state.update_prioritized_peers(
        peers_and_metadata,
        1000, // High traffic to trigger load balancing
        1000,
    );
}
```

**Configuration Example (mempool.yaml):**
```yaml
mempool:
  load_balancing_thresholds:
    - avg_mempool_traffic_threshold_in_tps: 500
      latency_slack_between_top_upstream_peers: 0  # Causes panic
      max_number_of_upstream_peers: 0              # Combined with this
```

**Notes**

This vulnerability specifically affects Public Fullnodes (PFNs) that use load balancing for mempool traffic distribution. Validator Fullnodes (VFNs) have special handling that bypasses the vulnerable code path by selecting peers from the VFN network directly. The missing validation is a code quality issue that violates defensive programming principles and the documented TODO for adding "reasonable verifications".

### Citations

**File:** config/src/config/mempool_config.rs (L15-27)
```rust
pub struct LoadBalancingThresholdConfig {
    /// PFN load balances the traffic to multiple upstream FNs. The PFN calculates the average mempool traffic in TPS received since
    /// the last peer udpate. If the average received mempool traffic is greater than this threshold, then the below limits are used
    /// to decide the number of upstream peers to forward the mempool traffic.
    pub avg_mempool_traffic_threshold_in_tps: u64,
    /// Suppose the smallest ping latency amongst the connected upstream peers is `x`. If the average received mempool traffic is
    /// greater than `avg_mempool_traffic_threshold_in_tps`, then the PFN will forward mempool traffic to only those upstream peers
    /// with ping latency less than `x + latency_slack_between_top_upstream_peers`.
    pub latency_slack_between_top_upstream_peers: u64,
    /// If the average received mempool traffic is greater than avg_mempool_traffic_threshold_in_tps, then PFNs will forward to at most
    /// `max_number_of_upstream_peers` upstream FNs.
    pub max_number_of_upstream_peers: u8,
}
```

**File:** config/src/config/mempool_config.rs (L176-184)
```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        Ok(()) // TODO: add reasonable verifications
    }
}
```

**File:** mempool/src/shared_mempool/priority.rs (L319-329)
```rust
        let num_top_peers = max(
            1,
            min(
                self.mempool_config.num_sender_buckets,
                if self.mempool_config.enable_max_load_balancing_at_any_load {
                    u8::MAX
                } else {
                    threshold_config.max_number_of_upstream_peers
                },
            ),
        );
```

**File:** mempool/src/shared_mempool/priority.rs (L370-388)
```rust
            for peer in self.prioritized_peers.read().iter() {
                if top_peers.len() >= num_top_peers as usize {
                    break;
                }

                let ping_latency = peer_monitoring_data
                    .get(peer)
                    .and_then(|metadata| get_peer_ping_latency(metadata));

                if base_ping_latency.is_none()
                    || ping_latency.is_none()
                    || ping_latency.unwrap()
                        < base_ping_latency.unwrap()
                            + (threshold_config.latency_slack_between_top_upstream_peers as f64)
                                / 1000.0
                {
                    top_peers.push(*peer);
                }
            }
```

**File:** mempool/src/shared_mempool/priority.rs (L395-397)
```rust
        assert!(top_peers.len() <= num_top_peers as usize);
        // Top peers shouldn't be empty if prioritized_peers is not zero
        assert!(self.prioritized_peers.read().is_empty() || !top_peers.is_empty());
```

**File:** mempool/src/shared_mempool/priority.rs (L404-409)
```rust
                self.peer_to_sender_buckets
                    .entry(*top_peers.get(peer_index).unwrap())
                    .or_default()
                    .insert(bucket_index, BroadcastPeerPriority::Primary);
                peer_index = (peer_index + 1) % top_peers.len();
            }
```
