# Audit Report

## Title
Asymmetric Error Handling in Network Writer Task Allows Silent Message Loss and Connection Liveness Issues

## Summary
The peer writer task handles write errors (including `poll_flush()` failures) asymmetrically compared to read errors. When `poll_flush()` fails during message transmission, the error is logged but the connection remains open and the writer continues processing messages. This contrasts with read errors which immediately terminate the connection. This asymmetry allows connections to appear healthy while silently dropping messages, causing consensus message loss and validator node slowdowns.

## Finding Description

The network layer's peer writer task has an asymmetric error handling design that violates the fail-fast principle:

**Write Error Path (peer/mod.rs):** [1](#0-0) 

When `writer.send(&message)` fails (which internally calls `poll_flush()`), the error is only logged with a warning. The connection remains open and the writer task continues processing subsequent messages.

**Read Error Path (peer/mod.rs):** [2](#0-1) 

In contrast, when read operations encounter IoErrors, the connection is immediately terminated with `DisconnectReason::InputOutputError`.

**The `poll_flush()` Implementation Chain:**

1. TCP Socket level: [3](#0-2) 

2. Multiplex Sink level: [4](#0-3) 

**Attack Scenario:**

1. An attacker causes network conditions (congestion, TCP flow control manipulation) that make TCP flush operations fail or timeout
2. The writer task attempts to send consensus messages (votes, proposals) with a 30-second timeout: [5](#0-4) 

3. Messages timeout after 30 seconds and are effectively dropped
4. The application layer's RPC requests timeout: [6](#0-5) 

5. However, the connection remains "healthy" from the peer manager's perspective
6. More consensus messages are queued and also fail to send
7. This continues until the health checker accumulates enough failures to disconnect: [7](#0-6) 

During this window (multiple ping intervals Ã— `ping_failures_tolerated`), consensus messages are lost, rounds stall, and the validator experiences slowdowns.

## Impact Explanation

This qualifies as **High Severity** per the Aptos Bug Bounty criteria:

**Validator Node Slowdowns:** When write operations fail but connections remain open, consensus messages (votes, proposals, sync requests) are queued but timeout after 30 seconds. This causes:
- Consensus rounds to stall waiting for messages that never arrive
- RPC timeouts forcing retries and delays
- Degraded validator performance
- Potential loss of rewards for affected validators

The health checker may take multiple intervals to detect the issue (e.g., with default `ping_failures_tolerated=3` and typical ping intervals, this could be 30-90 seconds of degraded operation).

The asymmetry between read and write error handling means:
- Read failures are detected immediately (connection closed)
- Write failures persist until health check detection (delayed)
- Applications see timeouts but cannot identify the root cause quickly

## Likelihood Explanation

**Likelihood: Medium-High**

This can occur in several realistic scenarios:

1. **Network Congestion:** During periods of high network load, TCP buffers fill up causing flush operations to block or timeout
2. **Bandwidth Throttling:** ISP or network equipment throttling can cause persistent flush delays
3. **TCP Flow Control:** Slow receiver can trigger flow control mechanisms that prevent flush operations from completing
4. **Targeted Attack:** An adversarial peer could intentionally slow down receives to trigger this condition
5. **Infrastructure Issues:** Cloud provider network problems, router misconfigurations

The issue is more likely during:
- Network partition recovery when many nodes reconnect simultaneously
- Large block propagation when bandwidth is saturated
- State synchronization when large amounts of data are transmitted

## Recommendation

**Immediate Fix:** Terminate connections on persistent write failures, matching the behavior of read error handling.

**Recommended Changes:**

1. Track consecutive write failures in the writer task
2. Terminate the connection after a threshold of failures
3. Ensure symmetry with read error handling

**Code Fix (conceptual):**

```rust
// In start_writer_task, track consecutive failures
let mut consecutive_failures = 0;
const MAX_WRITE_FAILURES: u32 = 3;

loop {
    futures::select! {
        message = stream.select_next_some() => {
            match timeout(transport::TRANSPORT_TIMEOUT, writer.send(&message)).await {
                Ok(Ok(())) => {
                    consecutive_failures = 0; // Reset on success
                },
                Ok(Err(err)) | Err(_) => {
                    consecutive_failures += 1;
                    warn!(
                        log_context,
                        error = %err,
                        consecutive_failures = consecutive_failures,
                        "{} Error in sending message to peer: {}, failures: {}",
                        network_context,
                        remote_peer_id.short_str(),
                        consecutive_failures
                    );
                    
                    // Terminate connection after threshold
                    if consecutive_failures >= MAX_WRITE_FAILURES {
                        info!(
                            log_context,
                            "{} Terminating connection due to persistent write failures",
                            network_context
                        );
                        break; // Exit writer task, triggering cleanup
                    }
                }
            }
        }
        _ = close_rx => {
            break;
        }
    }
}
```

2. Ensure the peer shutdown is triggered when the writer task exits abnormally
3. Add metrics for write failure tracking

## Proof of Concept

**Reproduction Steps:**

1. Set up a test with two validators connected via the network layer
2. Inject a fault that causes `poll_flush()` to return errors or timeout:
   - Use a test transport that simulates slow flush operations
   - Or use network throttling tools (tc, wondershaper) to limit bandwidth severely
3. Send consensus messages (e.g., votes) from validator A to validator B
4. Observe that:
   - Messages timeout after 30 seconds (TRANSPORT_TIMEOUT)
   - Warnings are logged about send failures
   - Connection remains in peer manager's active connections
   - Subsequent messages continue to be queued and fail
   - Health checker takes multiple intervals to detect and disconnect
5. Measure the time between first write failure and connection termination
6. Compare with read failure behavior (immediate termination)

**Expected Behavior:**
- Write failures should terminate connection similar to read failures
- Fail-fast principle should apply to both directions
- Applications should receive immediate feedback about connection health

**Actual Behavior:**
- Write failures are logged but connection persists
- Multiple messages fail before health checker detects issues
- Asymmetric handling creates operational blind spot

## Notes

The vulnerability stems from an architectural decision to treat write errors as transient/recoverable while read errors are fatal. While this may have been intended to tolerate temporary network issues, it creates an exploitable window where:

1. Message loss is silent from the peer manager's perspective
2. Applications receive timeouts without connection closure
3. Health checking introduces significant detection delay
4. Consensus operations are disrupted during the detection window

The 30-second `TRANSPORT_TIMEOUT` combined with health checker intervals creates a window of 30-90+ seconds where a validator can experience degraded performance before the issue is resolved through connection termination and reconnection.

This is particularly concerning for consensus operations where message timeliness is critical for liveness, even though AptosBFT's safety properties remain intact (the protocol tolerates message loss up to 1/3 Byzantine validators).

### Citations

**File:** network/framework/src/peer/mod.rs (L358-369)
```rust
                futures::select! {
                    message = stream.select_next_some() => {
                        if let Err(err) = timeout(transport::TRANSPORT_TIMEOUT,writer.send(&message)).await {
                            warn!(
                                log_context,
                                error = %err,
                                "{} Error in sending message to peer: {}",
                                network_context,
                                remote_peer_id.short_str(),
                            );
                        }
                    }
```

**File:** network/framework/src/peer/mod.rs (L588-591)
```rust
                ReadError::IoError(_) => {
                    // IoErrors are mostly unrecoverable so just close the connection.
                    self.shutdown(DisconnectReason::InputOutputError);
                    return Err(err.into());
```

**File:** network/netcore/src/transport/tcp.rs (L394-396)
```rust
    fn poll_flush(mut self: Pin<&mut Self>, context: &mut Context) -> Poll<io::Result<()>> {
        Pin::new(&mut self.inner).poll_flush(context)
    }
```

**File:** network/framework/src/protocols/wire/messaging/v1/mod.rs (L298-303)
```rust
    fn poll_flush(self: Pin<&mut Self>, cx: &mut Context<'_>) -> Poll<Result<(), Self::Error>> {
        self.project()
            .framed_write
            .poll_flush(cx)
            .map_err(WriteError::IoError)
    }
```

**File:** network/framework/src/transport/mod.rs (L41-41)
```rust
pub const TRANSPORT_TIMEOUT: Duration = Duration::from_secs(30);
```

**File:** network/framework/src/protocols/rpc/mod.rs (L515-525)
```rust
        let wait_for_response = self
            .time_service
            .timeout(timeout, response_rx)
            .map(|result| {
                // Flatten errors.
                match result {
                    Ok(Ok(response)) => Ok(Bytes::from(response.raw_response)),
                    Ok(Err(oneshot::Canceled)) => Err(RpcError::UnexpectedResponseChannelCancel),
                    Err(timeout::Elapsed) => Err(RpcError::TimedOut),
                }
            });
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L364-377)
```rust
                if failures > self.ping_failures_tolerated {
                    info!(
                        NetworkSchema::new(&self.network_context).remote_peer(&peer_id),
                        "{} Disconnecting from peer: {}",
                        self.network_context,
                        peer_id.short_str()
                    );
                    let peer_network_id =
                        PeerNetworkId::new(self.network_context.network_id(), peer_id);
                    if let Err(err) = timeout(
                        Duration::from_millis(50),
                        self.network_interface.disconnect_peer(
                            peer_network_id,
                            DisconnectReason::NetworkHealthCheckFailure,
```
