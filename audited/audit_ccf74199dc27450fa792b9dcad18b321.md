# Audit Report

## Title
Memory Exhaustion via Unbounded Record Size During Backup Restore

## Summary
The backup restore system processes record bytes from backup storage without validating record sizes before memory allocation, allowing malformed backup files to trigger unbounded memory allocation up to 4GB per record, causing denial of service during restore operations.

## Finding Description

The vulnerability exists in the backup restore data flow across multiple files:

**During backup creation**, transaction records are read from the backup service client and stored in chunks: [1](#0-0) 

These record bytes are written to backup storage with a 4-byte size prefix but without any integrity hash or size validation metadata.

**During restore**, records are read back using `read_record_bytes()`: [2](#0-1) 

The critical vulnerability is at line 54-60 where:
1. A 4-byte size prefix is read as u32 (up to 4GB)
2. Memory is allocated with `BytesMut::with_capacity(record_size)` **without any size validation**
3. No upper bound check against reasonable limits

**Then during deserialization**, the potentially oversized record bytes are deserialized without BCS size limits: [3](#0-2) 

The deserialization uses plain `bcs::from_bytes(&record_bytes)?` without size limits, happening **before** any cryptographic verification.

**Attack Path:**
1. Attacker gains write access to backup storage (cloud storage misconfiguration, compromised backup credentials, insider threat, or storage corruption)
2. Attacker modifies a chunk file's record size prefix from legitimate size (e.g., 4KB = `0x00001000`) to malicious size (e.g., 1GB = `0x3B9ACA00` or 4GB = `0xFFFFFFFF`)
3. Victim initiates restore operation
4. `read_record_bytes()` allocates up to 4GB memory per malformed record
5. Either: System runs out of memory (OOM kill), or allocation succeeds but subsequent operations fail
6. Restore process crashes before reaching cryptographic verification

**Broken Invariant:**
- **Resource Limits (Invariant #9)**: "All operations must respect gas, storage, and computational limits"
- Memory allocation occurs without bounds checking, violating resource limit guarantees

## Impact Explanation

This qualifies as **Medium Severity** per Aptos bug bounty criteria:
- **"State inconsistencies requiring intervention"**: Restore operations fail, preventing disaster recovery
- **Resource exhaustion DoS**: Memory exhaustion crashes restore processes

While backup chunks are limited to ~128MB during creation, there is no enforcement during restore: [4](#0-3) 

This limit is only used during backup creation for chunking decisions, not during restore validation.

**Affected Operations:**
- All backup restore operations (transaction, state snapshot, epoch ending)
- Database recovery from backups
- Disaster recovery procedures
- Backup verification operations

## Likelihood Explanation

**Likelihood: Medium**

**Prerequisites:**
- Write access to backup storage (cloud storage, file system, etc.)
- Target must initiate restore operation

**Realistic Scenarios:**
1. **Cloud storage misconfiguration**: Publicly writable S3 buckets, incorrect IAM policies
2. **Compromised backup credentials**: Stolen access keys, leaked credentials
3. **Insider threat**: Malicious operator with backup storage access
4. **Non-malicious corruption**: Storage bit flips, network transmission errors, filesystem bugs

Backup storage is often less secured than live validator infrastructure and may be:
- Stored in third-party cloud services
- Accessible by operations teams without validator keys
- Replicated across multiple locations with varying security postures
- Subject to different access controls than consensus-critical systems

## Recommendation

Implement size validation before memory allocation in `read_record_bytes()`:

```rust
async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
    const MAX_RECORD_SIZE: usize = 256 * 1024 * 1024; // 256MB limit
    
    let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
    // read record size
    let mut size_buf = BytesMut::with_capacity(4);
    self.read_full_buf_or_none(&mut size_buf).await?;
    if size_buf.is_empty() {
        return Ok(None);
    }

    let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
    
    // Validate record size before allocation
    if record_size > MAX_RECORD_SIZE {
        bail!(
            "Record size {} exceeds maximum allowed size of {} bytes",
            record_size,
            MAX_RECORD_SIZE
        );
    }
    
    if record_size == 0 {
        return Ok(Some(Bytes::new()));
    }

    // read record
    let mut record_buf = BytesMut::with_capacity(record_size);
    self.read_full_buf_or_none(&mut record_buf).await?;
    if record_buf.is_empty() {
        bail!("Hit EOF when reading record.")
    }

    Ok(Some(record_buf.freeze()))
}
```

Additionally, use `bcs::from_bytes_with_limit()` for deserialization with appropriate size limits.

## Proof of Concept

```rust
#[cfg(test)]
mod test_malformed_backup {
    use super::*;
    use std::io::Cursor;
    
    #[tokio::test]
    async fn test_oversized_record_allocation() {
        // Create malformed backup with 1GB size prefix
        let malicious_size: u32 = 1_000_000_000; // 1GB
        let mut malformed_backup = malicious_size.to_be_bytes().to_vec();
        // Append some garbage data (less than declared size to trigger EOF)
        malformed_backup.extend_from_slice(&[0u8; 1000]);
        
        let mut cursor = Cursor::new(malformed_backup);
        
        // This will attempt to allocate 1GB of memory
        let result = cursor.read_record_bytes().await;
        
        // Current implementation will either:
        // 1. Allocate 1GB (memory exhaustion)
        // 2. Fail with EOF error after allocation
        // Both are problematic for DoS
        
        // With fix, should fail immediately with size validation error
        assert!(result.is_err());
    }
}
```

## Notes

This vulnerability demonstrates a defense-in-depth failure where the system assumes backup storage integrity without validation. While cryptographic verification exists (via transaction accumulator proofs), it occurs **after** unbounded memory allocation and deserialization, allowing resource exhaustion attacks before reaching verification.

The issue affects all backup types (transactions, state snapshots, epoch endings) that use the `read_record_bytes()` utility function.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/transaction/backup.rs (L87-105)
```rust
        while let Some(record_bytes) = transactions_file.read_record_bytes().await? {
            if should_cut_chunk(&chunk_bytes, &record_bytes, self.max_chunk_size) {
                let chunk = self
                    .write_chunk(
                        &backup_handle,
                        &chunk_bytes,
                        chunk_first_ver,
                        current_ver - 1,
                    )
                    .await?;
                chunks.push(chunk);
                chunk_bytes = vec![];
                chunk_first_ver = current_ver;
            }

            chunk_bytes.extend((record_bytes.len() as u32).to_be_bytes());
            chunk_bytes.extend(&record_bytes);
            current_ver += 1;
        }
```

**File:** storage/backup/backup-cli/src/utils/read_record_bytes.rs (L44-67)
```rust
    async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
        let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
        // read record size
        let mut size_buf = BytesMut::with_capacity(4);
        self.read_full_buf_or_none(&mut size_buf).await?;
        if size_buf.is_empty() {
            return Ok(None);
        }

        // empty record
        let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
        if record_size == 0 {
            return Ok(Some(Bytes::new()));
        }

        // read record
        let mut record_buf = BytesMut::with_capacity(record_size);
        self.read_full_buf_or_none(&mut record_buf).await?;
        if record_buf.is_empty() {
            bail!("Hit EOF when reading record.")
        }

        Ok(Some(record_buf.freeze()))
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L112-131)
```rust
        while let Some(record_bytes) = file.read_record_bytes().await? {
            let (txn, aux_info, txn_info, events, write_set): (
                _,
                PersistedAuxiliaryInfo,
                _,
                _,
                WriteSet,
            ) = match manifest.format {
                TransactionChunkFormat::V0 => {
                    let (txn, txn_info, events, write_set) = bcs::from_bytes(&record_bytes)?;
                    (
                        txn,
                        PersistedAuxiliaryInfo::None,
                        txn_info,
                        events,
                        write_set,
                    )
                },
                TransactionChunkFormat::V1 => bcs::from_bytes(&record_bytes)?,
            };
```

**File:** storage/backup/backup-cli/src/utils/mod.rs (L51-57)
```rust
    // Defaults to 128MB, so concurrent chunk downloads won't take up too much memory.
    #[clap(
        long = "max-chunk-size",
        default_value_t = 134217728,
        help = "Maximum chunk file size in bytes."
    )]
    pub max_chunk_size: usize,
```
