# Audit Report

## Title
Parallel Sub-Pruner Execution Causes Atomic State Inconsistency in Database Sharding Mode

## Summary
When storage sharding is enabled, parallel sub-pruners in `LedgerPruner` execute independently across separate RocksDB instances. A failure in one sub-pruner after others have successfully committed creates a partially pruned state where transaction data exists without its associated events and write sets, violating the State Consistency invariant and leaving the database in a corrupted state.

## Finding Description

The vulnerability exists in the parallel pruning architecture when `enable_storage_sharding` is configured. Each sub-pruner (EventStorePruner, TransactionPruner, WriteSetPruner, etc.) writes to a completely separate RocksDB instance with no cross-database atomicity guarantees. [1](#0-0) 

The `try_for_each()` method has short-circuit semantics: when any sub-pruner returns an error, it immediately returns that error, but other parallel tasks may have already completed and committed their writes to their respective databases. [2](#0-1) 

When sharding is enabled, each sub-pruner operates on a separate RocksDB instance: [3](#0-2) 

**Attack Scenario:**

1. **Initial State:** All pruners at version 0, target version set to 1000, storage sharding enabled

2. **Execution Sequence:**
   - LedgerMetadataPruner successfully prunes to version 1000
   - Parallel sub-pruners execute via `par_iter().try_for_each()`
   - EventStorePruner successfully deletes events [0, 1000) from `event_db` and commits
   - WriteSetPruner successfully deletes write sets [0, 1000) from `write_set_db` and commits  
   - TransactionPruner encounters I/O error and fails
   - `try_for_each()` returns error, parent progress NOT updated

3. **Resulting Inconsistent State:**
   - Transaction data for versions [0, 1000): **EXISTS**
   - Events for versions [0, 1000): **DELETED**
   - Write sets for versions [0, 1000): **DELETED**
   - LedgerPrunerProgress: 0 (not updated due to error)

Each sub-pruner maintains independent progress in its database: [4](#0-3) [5](#0-4) [6](#0-5) 

The worker retry mechanism does not prevent the inconsistency: [7](#0-6) 

**Broken Invariant:** This violates **Critical Invariant #4: State Consistency** - "State transitions must be atomic and verifiable via Merkle proofs." Transaction components should be atomically present or absent, but this bug creates orphaned transactions without their events and write sets.

## Impact Explanation

**Severity: HIGH** - This qualifies as "Significant protocol violations" and "State inconsistencies requiring intervention" per the Aptos bug bounty program.

**Impact Details:**

1. **Data Integrity Violation**: Database queries return incomplete transaction data - transactions exist but their events and write sets are missing

2. **State Consistency Breach**: Violates the fundamental guarantee that all transaction components are atomically stored

3. **Consensus Risk**: If nodes query the inconsistent state during execution or state proof generation, different nodes could compute different state roots, potentially causing consensus divergence

4. **Recovery Complexity**: The inconsistent state persists until node restart or successful retry, potentially hours or days in production

5. **Validator Impact**: All validators running with storage sharding enabled are vulnerable to this issue during normal operations

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

This vulnerability triggers under realistic failure conditions:

- **Trigger Conditions**: I/O errors, disk space exhaustion, filesystem corruption, resource limits, database corruption
- **Configuration**: Affects all deployments with `enable_storage_sharding: true` (common in production)
- **Frequency**: Pruning runs continuously in background worker, increasing exposure window
- **Detection Difficulty**: Inconsistent state is silent - no alarms or explicit errors after partial failure

The parallel execution pattern is used across multiple pruner types: [8](#0-7) [9](#0-8) 

## Recommendation

**Fix: Implement Two-Phase Commit for Parallel Sub-Pruners**

Modify the pruning architecture to separate the prepare phase from the commit phase:

```rust
// In LedgerPruner::prune()
let batches: Vec<_> = THREAD_MANAGER.get_background_pool().install(|| {
    self.sub_pruners.par_iter().map(|sub_pruner| {
        // Phase 1: Prepare batch (no DB writes)
        let batch = sub_pruner.prepare_batch(progress, current_batch_target_version)?;
        Ok((sub_pruner, batch))
    }).collect()
})?;

// Phase 2: Commit all batches sequentially (fast, minimal failure window)
for (sub_pruner, batch) in batches {
    sub_pruner.commit_batch(batch)?;
}
```

**Alternative Fix: Transaction Log for Rollback**

Maintain a pruning transaction log that records which sub-pruners completed. On failure, roll back successful sub-pruners to previous state.

**Immediate Mitigation:**

1. Add health checks to detect inconsistent pruner progress states
2. Implement automatic recovery that checks metadata consistency on startup
3. Add alerting when sub-pruner progress diverges from parent progress
4. Consider disabling storage sharding for critical deployments until fixed

## Proof of Concept

```rust
#[cfg(test)]
mod test_pruner_inconsistency {
    use super::*;
    
    #[test]
    fn test_parallel_pruner_partial_failure() {
        // Setup: Create LedgerDb with sharding enabled
        let temp_dir = TempPath::new();
        let mut config = RocksdbConfigs::default();
        config.enable_storage_sharding = true;
        let ledger_db = Arc::new(LedgerDb::new(
            &temp_dir,
            config,
            None,
            None,
            false
        ).unwrap());
        
        // Commit transactions with events and write sets
        commit_test_transactions(&ledger_db, 0, 1000);
        
        // Create pruner
        let pruner = LedgerPruner::new(ledger_db.clone(), None).unwrap();
        pruner.set_target_version(1000);
        
        // Inject failure in TransactionPruner by corrupting its database
        corrupt_transaction_db(&ledger_db);
        
        // Attempt to prune - should fail
        let result = pruner.prune(100);
        assert!(result.is_err());
        
        // Verify inconsistent state:
        // - Events are deleted
        let events_result = ledger_db.event_db().get_events_by_version(500);
        assert!(events_result.is_err()); // Events deleted
        
        // - Write sets are deleted  
        let ws_result = ledger_db.write_set_db().get_write_set(500);
        assert!(ws_result.is_err()); // Write sets deleted
        
        // - But transactions still exist
        let txn_result = ledger_db.transaction_db().get_transaction(500);
        assert!(txn_result.is_ok()); // Transaction still exists!
        
        // INCONSISTENCY DETECTED: Transaction exists without events/write sets
    }
}
```

**Notes**

The vulnerability is particularly severe because:

1. It affects the core storage layer used by all validator nodes
2. The inconsistency is silent and may go undetected for extended periods
3. Storage sharding is a performance optimization commonly enabled in production
4. The same pattern exists in `StateKvPruner` and `StateMerklePruner`, multiplying the attack surface
5. No current safeguards detect or prevent this partial pruning state

The fix requires architectural changes to ensure atomicity across multiple RocksDB instances, either through two-phase commit or by reverting to single-database pruning with careful transaction batching.

### Citations

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L174-182)
```rust
        let ledger_db_folder = db_root_path.as_ref().join(LEDGER_DB_FOLDER_NAME);

        let mut event_db = None;
        let mut persisted_auxiliary_info_db = None;
        let mut transaction_accumulator_db = None;
        let mut transaction_auxiliary_data_db = None;
        let mut transaction_db = None;
        let mut transaction_info_db = None;
        let mut write_set_db = None;
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L184-250)
```rust
            s.spawn(|_| {
                let event_db_raw = Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(EVENT_DB_NAME),
                        EVENT_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                );
                event_db = Some(EventDb::new(
                    event_db_raw.clone(),
                    EventStore::new(event_db_raw),
                ));
            });
            s.spawn(|_| {
                persisted_auxiliary_info_db = Some(PersistedAuxiliaryInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME),
                        PERSISTED_AUXILIARY_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_accumulator_db = Some(TransactionAccumulatorDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME),
                        TRANSACTION_ACCUMULATOR_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_auxiliary_data_db = Some(TransactionAuxiliaryDataDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME),
                        TRANSACTION_AUXILIARY_DATA_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )))
            });
            s.spawn(|_| {
                transaction_db = Some(TransactionDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_DB_NAME),
                        TRANSACTION_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L43-81)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let mut indexer_batch = None;

        let indices_batch = if let Some(indexer_db) = self.indexer_db() {
            if indexer_db.event_enabled() {
                indexer_batch = Some(SchemaBatch::new());
            }
            indexer_batch.as_mut()
        } else {
            Some(&mut batch)
        };
        let num_events_per_version = self.ledger_db.event_db().prune_event_indices(
            current_progress,
            target_version,
            indices_batch,
        )?;
        self.ledger_db.event_db().prune_events(
            num_events_per_version,
            current_progress,
            target_version,
            &mut batch,
        )?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;

        if let Some(mut indexer_batch) = indexer_batch {
            indexer_batch.put::<InternalIndexerMetadataSchema>(
                &IndexerMetadataKey::EventPrunerProgress,
                &IndexerMetadataValue::Version(target_version),
            )?;
            self.expect_indexer_db()
                .get_inner_db_ref()
                .write_schemas(indexer_batch)?;
        }
        self.ledger_db.event_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L37-74)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        let candidate_transactions =
            self.get_pruning_candidate_transactions(current_progress, target_version)?;
        self.ledger_db
            .transaction_db()
            .prune_transaction_by_hash_indices(
                candidate_transactions.iter().map(|(_, txn)| txn.hash()),
                &mut batch,
            )?;
        self.ledger_db.transaction_db().prune_transactions(
            current_progress,
            target_version,
            &mut batch,
        )?;
        self.transaction_store
            .prune_transaction_summaries_by_account(&candidate_transactions, &mut batch)?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        if let Some(indexer_db) = self.internal_indexer_db.as_ref() {
            if indexer_db.transaction_enabled() {
                let mut index_batch = SchemaBatch::new();
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut index_batch)?;
                index_batch.put::<InternalIndexerMetadataSchema>(
                    &IndexerMetadataKey::TransactionPrunerProgress,
                    &IndexerMetadataValue::Version(target_version),
                )?;
                indexer_db.get_inner_db_ref().write_schemas(index_batch)?;
            } else {
                self.transaction_store
                    .prune_transaction_by_account(&candidate_transactions, &mut batch)?;
            }
        }
        self.ledger_db.transaction_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/write_set_pruner.rs (L25-33)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        WriteSetDb::prune(current_progress, target_version, &mut batch)?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::WriteSetPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_db.write_set_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-68)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L67-78)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/mod.rs (L174-189)
```rust
        THREAD_MANAGER
            .get_background_pool()
            .install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(current_progress, target_version, batch_size)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state merkle shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })
            .map_err(Into::into)
    }
```
