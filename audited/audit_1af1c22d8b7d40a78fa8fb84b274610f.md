# Audit Report

## Title
Unauthenticated Remote Coordinator in Executor Service Enables State Corruption Through Arbitrary Transaction Execution

## Summary
The executor-service accepts a `coordinator_address` parameter without any validation or authentication, allowing a malicious coordinator to send arbitrary execution commands that are blindly trusted and executed by executor shards. This breaks critical deterministic execution invariants and enables state corruption across sharded execution environments.

## Finding Description
The executor-service is designed for distributed sharded block execution, where multiple executor processes communicate with a central coordinator. The vulnerability exists in the complete absence of authentication or validation mechanisms: [1](#0-0) 

The `coordinator_address` is accepted as a command-line argument and passed directly to the service without any validation: [2](#0-1) 

The RemoteCoordinatorClient establishes network channels to this coordinator address with no authentication: [3](#0-2) 

When execution commands arrive, they are deserialized from the coordinator and executed without any verification: [4](#0-3) 

The underlying NetworkController provides no authentication mechanisms: [5](#0-4) 

**Attack Path:**
1. Attacker deploys malicious coordinator or intercepts network traffic to executor-service
2. Malicious coordinator sends `RemoteExecutionRequest::ExecuteBlock` commands with:
   - Arbitrary `AnalyzedTransaction` payloads (even if never in consensus)
   - Different transactions to different shards (causing state divergence)
   - Manipulated `BlockExecutorConfigFromOnchain` parameters
3. Executor shards blindly deserialize and execute these commands
4. Different shards produce different state roots, violating **Invariant #1: Deterministic Execution**
5. State corruption propagates when results are merged

## Impact Explanation
This vulnerability achieves **Critical Severity** under Aptos bug bounty criteria:

1. **State Consistency Violation**: Different executor shards can be fed different transactions, causing them to compute different state roots for the same block. This violates the core invariant that "all validators must produce identical state roots for identical blocks."

2. **Unauthorized Transaction Execution**: A malicious coordinator can inject arbitrary transactions that were never part of consensus, potentially enabling:
   - Execution of unsigned transactions
   - Bypassing transaction validation (signature, sequence number, gas checks)
   - Direct manipulation of blockchain state

3. **Consensus Safety Breakdown**: When sharded execution is integrated into the consensus pipeline (as shown in do_get_execution_output.rs), state divergence across shards would cause validators to disagree on state roots, potentially leading to chain splits or requiring manual intervention. [6](#0-5) 

## Likelihood Explanation
**Likelihood: Medium to High** (depending on deployment)

The executor-service is currently used primarily in benchmarking scenarios, but the code path is integrated into production execution workflows. If deployed in any semi-production or testing environment with multiple machines:

1. **Network interception**: An attacker positioned on the network between coordinator and executors can redirect traffic
2. **Misconfiguration**: If executor-service processes are started with incorrect coordinator addresses (no validation warns users)
3. **Insider threat**: Any operator who can start executor-service processes can point them to a malicious coordinator

The vulnerability is **exploitable by design** - there are zero authentication checks, making exploitation trivial once network access is achieved.

## Recommendation
Implement mutual authentication between coordinator and executor shards using cryptographic credentials:

**Recommended Fix:**

1. **Add TLS with mutual authentication** to NetworkController:
   - Coordinator should have a certificate/key pair
   - Executor shards should verify coordinator's certificate against trusted CA
   - Coordinato should verify executor identities

2. **Add coordinator address validation** at startup:
   - Verify coordinator address against trusted configuration
   - Implement allowlist of valid coordinator addresses
   - Add warning if coordinator address is not localhost in production

3. **Add message authentication**:
   - Sign all ExecuteBlockCommand messages with coordinator's private key
   - Executors verify signatures before executing
   - Include nonce/timestamp to prevent replay attacks

4. **Add transaction integrity checks**:
   - Include Merkle root or hash of transaction batch in command
   - Executors verify transaction authenticity before execution
   - Cross-check that all shards received identical commands

**Code Fix Skeleton:**
```rust
// In Args struct, add trusted coordinator public key
pub struct Args {
    // ... existing fields ...
    #[clap(long)]
    pub coordinator_public_key: Option<String>,
}

// In main.rs, validate coordinator
fn validate_coordinator(addr: &SocketAddr, pubkey: &Option<String>) -> Result<()> {
    if pubkey.is_none() {
        return Err(anyhow!("Coordinator public key required for authentication"));
    }
    // Implement cryptographic validation
    Ok(())
}

// In RemoteCoordinatorClient, verify message signatures
fn receive_execute_command(&self) -> ExecutorShardCommand<RemoteStateViewClient> {
    match self.command_rx.recv() {
        Ok(message) => {
            // ADD: Verify message signature before deserializing
            if !self.verify_coordinator_signature(&message) {
                return ExecutorShardCommand::Stop;
            }
            // ... existing deserialization code ...
        }
    }
}
```

## Proof of Concept
**Demonstration of vulnerability:**

```rust
// malicious_coordinator.rs
// This demonstrates how a malicious coordinator can send arbitrary commands

use aptos_executor_service::{ExecuteBlockCommand, RemoteExecutionRequest};
use aptos_secure_net::network_controller::{Message, NetworkController};
use std::net::{IpAddr, Ipv4Addr, SocketAddr};

fn main() {
    // Malicious coordinator listens on expected address
    let malicious_coord_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 52200);
    let mut controller = NetworkController::new(
        "malicious-coordinator".to_string(),
        malicious_coord_addr,
        5000,
    );
    
    // Create channel to send commands to executor shard 0
    let shard_0_tx = controller.create_outbound_channel(
        SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), 52201),
        "execute_command_0".to_string(),
    );
    
    controller.start();
    
    // Create malicious ExecuteBlockCommand with arbitrary transactions
    let malicious_command = RemoteExecutionRequest::ExecuteBlock(ExecuteBlockCommand {
        sub_blocks: create_malicious_sub_blocks(), // Attacker-controlled transactions
        concurrency_level: 1,
        onchain_config: manipulated_config(), // Attacker-controlled config
    });
    
    // Send to executor - NO AUTHENTICATION REQUIRED
    let serialized = bcs::to_bytes(&malicious_command).unwrap();
    shard_0_tx.send(Message::new(serialized)).unwrap();
    
    println!("Malicious command sent - executor will blindly execute!");
}
```

**To reproduce:**
1. Start executor-service with: `./executor-service --coordinator-address 127.0.0.1:52200 --shard-id 0 --num-shards 1 --remote-executor-addresses 127.0.0.1:52201`
2. Run malicious coordinator that sends arbitrary ExecuteBlockCommand
3. Observe executor executes commands without verification
4. Different coordinators can send different commands to different shards, causing state divergence

**Notes**
- This vulnerability is in production code paths (integrated via do_get_execution_output.rs) but primarily used for benchmarking/testing scenarios
- The severity depends heavily on deployment context - if used in any multi-machine setup, the risk is **Critical**
- The complete absence of authentication violates defense-in-depth principles even for internal tools
- The TOCTOU aspect is that the coordinator address is set once at startup and never re-validated, but the deeper issue is the complete lack of authentication throughout the service lifetime

### Citations

**File:** execution/executor-service/src/main.rs (L10-25)
```rust
struct Args {
    #[clap(long, default_value_t = 8)]
    pub num_executor_threads: usize,

    #[clap(long)]
    pub shard_id: usize,

    #[clap(long)]
    pub num_shards: usize,

    #[clap(long, num_args = 1..)]
    pub remote_executor_addresses: Vec<SocketAddr>,

    #[clap(long)]
    pub coordinator_address: SocketAddr,
}
```

**File:** execution/executor-service/src/main.rs (L37-43)
```rust
    let _exe_service = ProcessExecutorService::new(
        args.shard_id,
        args.num_shards,
        args.num_executor_threads,
        args.coordinator_address,
        args.remote_executor_addresses,
    );
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L27-46)
```rust
    pub fn new(
        shard_id: ShardId,
        controller: &mut NetworkController,
        coordinator_address: SocketAddr,
    ) -> Self {
        let execute_command_type = format!("execute_command_{}", shard_id);
        let execute_result_type = format!("execute_result_{}", shard_id);
        let command_rx = controller.create_inbound_channel(execute_command_type);
        let result_tx =
            controller.create_outbound_channel(coordinator_address, execute_result_type);

        let state_view_client =
            RemoteStateViewClient::new(shard_id, controller, coordinator_address);

        Self {
            state_view_client: Arc::new(state_view_client),
            command_rx,
            result_tx,
            shard_id,
        }
```

**File:** execution/executor-service/src/remote_cordinator_client.rs (L80-113)
```rust
    fn receive_execute_command(&self) -> ExecutorShardCommand<RemoteStateViewClient> {
        match self.command_rx.recv() {
            Ok(message) => {
                let _rx_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx"])
                    .start_timer();
                let bcs_deser_timer = REMOTE_EXECUTOR_TIMER
                    .with_label_values(&[&self.shard_id.to_string(), "cmd_rx_bcs_deser"])
                    .start_timer();
                let request: RemoteExecutionRequest = bcs::from_bytes(&message.data).unwrap();
                drop(bcs_deser_timer);

                match request {
                    RemoteExecutionRequest::ExecuteBlock(command) => {
                        let init_prefetch_timer = REMOTE_EXECUTOR_TIMER
                            .with_label_values(&[&self.shard_id.to_string(), "init_prefetch"])
                            .start_timer();
                        let state_keys = Self::extract_state_keys(&command);
                        self.state_view_client.init_for_block(state_keys);
                        drop(init_prefetch_timer);

                        let (sub_blocks, concurrency, onchain_config) = command.into();
                        ExecutorShardCommand::ExecuteSubBlocks(
                            self.state_view_client.clone(),
                            sub_blocks,
                            concurrency,
                            onchain_config,
                        )
                    },
                }
            },
            Err(_) => ExecutorShardCommand::Stop,
        }
    }
```

**File:** secure/net/src/network_controller/mod.rs (L72-93)
```rust
/// NetworkController is the main entry point for sending and receiving messages over the network.
/// 1. If a node acts as both client and server, albeit in different contexts, GRPC needs separate
///    runtimes for client context and server context. Otherwise we a hang in GRPC. This seems to be
///    an internal bug in GRPC.
/// 2. We want to use tokio runtimes because it is best for async IO and tonic GRPC
///    implementation is async. However, we want the rest of the system (remote executor service)
///    to use rayon thread pools because it is best for CPU bound tasks.
/// 3. NetworkController, InboundHandler and OutboundHandler work as a bridge between the sync and
///    async worlds.
/// 4. We need to shutdown all the async tasks spawned by the NetworkController runtimes, otherwise
///    the program will hang, or have resource leaks.
#[allow(dead_code)]
pub struct NetworkController {
    inbound_handler: Arc<Mutex<InboundHandler>>,
    outbound_handler: OutboundHandler,
    inbound_rpc_runtime: Runtime,
    outbound_rpc_runtime: Runtime,
    inbound_server_shutdown_tx: Option<oneshot::Sender<()>>,
    outbound_task_shutdown_tx: Option<Sender<Message>>,
    listen_addr: SocketAddr,
}

```

**File:** execution/executor/src/workflow/do_get_execution_output.rs (L256-276)
```rust
    fn execute_block_sharded<V: VMBlockExecutor>(
        partitioned_txns: PartitionedTransactions,
        state_view: Arc<CachedStateView>,
        onchain_config: BlockExecutorConfigFromOnchain,
    ) -> Result<Vec<TransactionOutput>> {
        if !get_remote_addresses().is_empty() {
            Ok(V::execute_block_sharded(
                &REMOTE_SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        } else {
            Ok(V::execute_block_sharded(
                &SHARDED_BLOCK_EXECUTOR.lock(),
                partitioned_txns,
                state_view,
                onchain_config,
            )?)
        }
    }
```
