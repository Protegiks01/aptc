# Audit Report

## Title
Chain Fork via Non-Deterministic Proposer Election Due to Database State Divergence

## Summary
The `LeaderReputation` proposer election implementation relies on local database state (historical block metadata and accumulator root hash) to compute proposer selection. When validators have different database states—even temporarily—they will select different proposers for the same round, causing validators to reject each other's blocks and fork the chain. Additionally, on-chain consensus config deserialization failures cause validators to fall back to different default configurations, potentially using entirely different proposer election algorithms.

## Finding Description

The vulnerability exists in two critical code paths:

**Path 1: Database-Dependent Proposer Election**

The `LeaderReputation` implementation queries local database state to select proposers: [1](#0-0) 

The seed for random proposer selection includes the `root_hash` obtained from the local database. If validators have divergent database states, they compute different seeds and select different proposers: [2](#0-1) 

When `get_accumulator_root_hash` fails, it returns `HashValue::zero()` instead of propagating the error. This creates non-determinism: validators with successful queries use the actual root hash while validators with failures use zero.

The code explicitly warns about this issue: [3](#0-2) 

**Path 2: Configuration Fallback Divergence**

During epoch initialization, if the on-chain consensus config fails to deserialize, validators fall back to a hardcoded default: [4](#0-3) 

The default proposer election type is: [5](#0-4) 

If the on-chain config specifies a different type (e.g., `RotatingProposer`) but some validators fail to deserialize it while others succeed, they will use different proposer election algorithms.

**Consensus Safety Violation**

Once validators disagree on the expected proposer, they reject each other's proposals: [6](#0-5) [7](#0-6) 

This causes validators to diverge in their blockchain view, resulting in a chain fork.

## Impact Explanation

This vulnerability qualifies as **Critical Severity** under the Aptos Bug Bounty program:

- **Consensus/Safety Violations**: Breaks AptosBFT safety guarantees by allowing validators to commit different blocks for the same round
- **Non-recoverable Network Partition (requires hardfork)**: Once validators fork due to proposer disagreement, they cannot reconcile without manual intervention
- **Total Loss of Liveness**: If enough validators disagree on proposers, no quorum can be reached for any block

The impact affects the entire network and violates the fundamental invariant: "All validators must produce identical state roots for identical blocks." When validators select different proposers, they produce and commit different blocks, permanently forking the chain.

## Likelihood Explanation

**Path 1 (Database Divergence):** Medium-High likelihood during:
- State synchronization after network partitions
- Validators joining with incomplete historical data  
- Database corruption or pruning inconsistencies
- High network latency causing temporary desynchronization

**Path 2 (Config Fallback):** Low-Medium likelihood during:
- On-chain governance updates to consensus configuration
- Code version mismatches during validator upgrades
- Memory corruption or storage failures affecting config deserialization
- BCS deserialization bugs with malformed on-chain data

The vulnerability is particularly dangerous because:
1. The warning message confirms developers are aware this can happen
2. No explicit safeguards prevent validators with different DB states from participating in the same consensus round
3. Errors return default values (`HashValue::zero()`) rather than halting consensus

## Recommendation

**Immediate Mitigations:**

1. **Fail-Fast on Database Errors**: Propagate errors instead of returning default values:

```rust
// In leader_reputation.rs, replace unwrap_or_else with expect/bail
let root_hash = self
    .aptos_db
    .get_accumulator_root_hash(max_version)
    .expect("CRITICAL: Failed to fetch accumulator root hash - database inconsistency detected");
```

2. **Strict Config Loading**: Remove the fallback to default configuration:

```rust
// In epoch_manager.rs
let consensus_config = onchain_consensus_config
    .expect("CRITICAL: Failed to load on-chain consensus config - cannot proceed safely");
```

3. **Proposer Election Verification**: Add a cross-validator proposer verification mechanism where validators include their expected proposer in their votes, detecting mismatches before committing blocks.

**Long-Term Solutions:**

1. **Deterministic Proposer Election**: Move proposer election logic to depend only on committed blockchain state, not local database queries:
   - Use epoch boundary committed state as the basis for all proposer computations
   - Include proposer seed in the LedgerInfo so all validators use the same value
   
2. **Sync Enforcement**: Add explicit checks preventing validators from participating in consensus if their database is not sufficiently synchronized:
   - Verify accumulator root hash matches peers before participating in rounds
   - Require minimum history depth before enabling LeaderReputation

3. **Config Version Pinning**: Include consensus config version hash in epoch state to detect configuration mismatches.

## Proof of Concept

```rust
// Simulation of fork scenario:
// 
// Setup:
// - Validator A: Synced, has blocks 0-1000, root_hash = 0xABCD
// - Validator B: Lagging, has blocks 0-900, root_hash = 0x1234
// - Both use LeaderReputation with use_root_hash=true
//
// Round 1001:
// Step 1: Both query block metadata for round 1001
// - Validator A: gets events 900-1000, root_hash=0xABCD at version 1000
// - Validator B: gets events 800-900, fails to get root_hash, uses HashValue::zero()
//
// Step 2: Compute proposer seed
// - Validator A: seed = concat(0xABCD, epoch.to_bytes(), round.to_bytes())
// - Validator B: seed = concat(0x0000, epoch.to_bytes(), round.to_bytes())
//
// Step 3: Random selection with choose_index()
// - Validator A: choose_index(weights, seed_A) -> selects Proposer X (index 5)
// - Validator B: choose_index(weights, seed_B) -> selects Proposer Y (index 12)
//
// Step 4: Proposer X creates Block_X
// - Validator A: is_valid_proposer(X, 1001) -> TRUE, accepts Block_X
// - Validator B: is_valid_proposer(X, 1001) -> FALSE (expects Y), REJECTS Block_X
//
// Step 5: Proposer Y creates Block_Y  
// - Validator A: is_valid_proposer(Y, 1001) -> FALSE (expects X), REJECTS Block_Y
// - Validator B: is_valid_proposer(Y, 1001) -> TRUE, accepts Block_Y
//
// Result: CHAIN FORK - Validator A on chain with Block_X, Validator B on chain with Block_Y
//
// To reproduce in test environment:
// 1. Set up two validator nodes with LeaderReputation enabled
// 2. Pause sync on one validator to create database lag
// 3. Advance rounds and observe proposer disagreement in logs
// 4. Verify validators reject each other's blocks with "InvalidConsensusProposal" errors
```

## Notes

The explicit warning message "Elected proposers are unlikely to match!!" in the codebase confirms that developers recognize this issue can occur. However, no mitigation prevents consensus participation when this condition is detected. The use of local, potentially divergent database state for consensus-critical proposer selection fundamentally violates the determinism requirement of Byzantine Fault Tolerant consensus protocols. While rare under normal operation, the catastrophic impact (permanent chain fork requiring hard fork recovery) combined with realistic triggering conditions (temporary desync, database lag) qualifies this as a Critical severity vulnerability.

### Citations

**File:** consensus/src/liveness/leader_reputation.rs (L116-122)
```rust
            if !has_larger {
                // error, and not a fatal, in an unlikely scenario that we have many failed consecutive rounds,
                // and nobody has any newer successful blocks.
                warn!(
                    "Local history is too old, asking for {} epoch and {} round, and latest from db is {} epoch and {} round! Elected proposers are unlikely to match!!",
                    target_epoch, target_round, events.first().map_or(0, |e| e.event.epoch()), events.first().map_or(0, |e| e.event.round()))
            }
```

**File:** consensus/src/liveness/leader_reputation.rs (L153-163)
```rust
            let root_hash = self
                .aptos_db
                .get_accumulator_root_hash(max_version)
                .unwrap_or_else(|_| {
                    error!(
                        "We couldn't fetch accumulator hash for the {} version, for {} epoch, {} round",
                        max_version, target_epoch, target_round,
                    );
                    HashValue::zero()
                });
            (result, root_hash)
```

**File:** consensus/src/liveness/leader_reputation.rs (L695-734)
```rust
impl ProposerElection for LeaderReputation {
    fn get_valid_proposer_and_voting_power_participation_ratio(
        &self,
        round: Round,
    ) -> (Author, VotingPowerRatio) {
        let target_round = round.saturating_sub(self.exclude_round);
        let (sliding_window, root_hash) = self.backend.get_block_metadata(self.epoch, target_round);
        let voting_power_participation_ratio =
            self.compute_chain_health_and_add_metrics(&sliding_window, round);
        let mut weights =
            self.heuristic
                .get_weights(self.epoch, &self.epoch_to_proposers, &sliding_window);
        let proposers = &self.epoch_to_proposers[&self.epoch];
        assert_eq!(weights.len(), proposers.len());

        // Multiply weights by voting power:
        let stake_weights: Vec<u128> = weights
            .iter_mut()
            .enumerate()
            .map(|(i, w)| *w as u128 * self.voting_powers[i] as u128)
            .collect();

        let state = if self.use_root_hash {
            [
                root_hash.to_vec(),
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        } else {
            [
                self.epoch.to_le_bytes().to_vec(),
                round.to_le_bytes().to_vec(),
            ]
            .concat()
        };

        let chosen_index = choose_index(stake_weights, state);
        (proposers[chosen_index], voting_power_participation_ratio)
    }
```

**File:** consensus/src/epoch_manager.rs (L1178-1201)
```rust
        let onchain_consensus_config: anyhow::Result<OnChainConsensusConfig> = payload.get();
        let onchain_execution_config: anyhow::Result<OnChainExecutionConfig> = payload.get();
        let onchain_randomness_config_seq_num: anyhow::Result<RandomnessConfigSeqNum> =
            payload.get();
        let randomness_config_move_struct: anyhow::Result<RandomnessConfigMoveStruct> =
            payload.get();
        let onchain_jwk_consensus_config: anyhow::Result<OnChainJWKConsensusConfig> = payload.get();
        let dkg_state = payload.get::<DKGState>();

        if let Err(error) = &onchain_consensus_config {
            warn!("Failed to read on-chain consensus config {}", error);
        }

        if let Err(error) = &onchain_execution_config {
            warn!("Failed to read on-chain execution config {}", error);
        }

        if let Err(error) = &randomness_config_move_struct {
            warn!("Failed to read on-chain randomness config {}", error);
        }

        self.epoch_state = Some(epoch_state.clone());

        let consensus_config = onchain_consensus_config.unwrap_or_default();
```

**File:** types/src/on_chain_config/consensus_config.rs (L481-506)
```rust
impl Default for ConsensusConfigV1 {
    fn default() -> Self {
        Self {
            decoupled_execution: true,
            back_pressure_limit: 10,
            exclude_round: 40,
            max_failed_authors_to_store: 10,
            proposer_election_type: ProposerElectionType::LeaderReputation(
                LeaderReputationType::ProposerAndVoterV2(ProposerAndVoterConfig {
                    active_weight: 1000,
                    inactive_weight: 10,
                    failed_weight: 1,
                    failure_threshold_percent: 10, // = 10%
                    // In each round we get stastics for the single proposer
                    // and large number of validators. So the window for
                    // the proposers needs to be significantly larger
                    // to have enough useful statistics.
                    proposer_window_num_validators_multiplier: 10,
                    voter_window_num_validators_multiplier: 1,
                    weight_by_voting_power: true,
                    use_history_from_previous_epoch_max_count: 5,
                }),
            ),
        }
    }
}
```

**File:** consensus/src/liveness/unequivocal_proposer_election.rs (L46-60)
```rust
    pub fn is_valid_proposal(&self, block: &Block) -> bool {
        block.author().is_some_and(|author| {
            let valid_author = self.is_valid_proposer(author, block.round());
            if !valid_author {
                warn!(
                    SecurityEvent::InvalidConsensusProposal,
                    "Proposal is not from valid author {}, expected {} for round {} and id {}",
                    author,
                    self.get_valid_proposer(block.round()),
                    block.round(),
                    block.id()
                );

                return false;
            }
```

**File:** consensus/src/round_manager.rs (L1195-1200)
```rust
        ensure!(
            self.proposer_election.is_valid_proposal(&proposal),
            "[RoundManager] Proposer {} for block {} is not a valid proposer for this round or created duplicate proposal",
            author,
            proposal,
        );
```
