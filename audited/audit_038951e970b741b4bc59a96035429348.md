# Audit Report

## Title
Race Condition in BlockSTM v1: Concurrent Reads During Write Removal Creates Non-Deterministic Execution

## Summary
BlockSTM v1's `execute()` function removes previous writes from the versioned cache (lines 678-699) before notifying the scheduler to trigger suffix validation (line 709). During this gap, concurrent transactions can read keys after removal but before validation is triggered, leading to non-deterministic execution results across validators and potential consensus failures.

## Finding Description

The vulnerability exists in the BlockSTM v1 parallel execution engine. When a transaction re-executes and no longer writes to certain keys, it must remove the old writes from previous incarnations. However, the removal happens in an unprotected window that creates a race condition. [1](#0-0) 

**Attack Flow:**

1. Transaction T5 (index 5) incarnation 0 completes, writing resource group key K with value 100
2. Due to dependency invalidation, T5 incarnation 1 executes but does NOT write to key K
3. T5 incarnation 1 reaches the removal phase and removes K at index 5 from `versioned_cache` (lines 695-698)
4. **Critical Gap**: K is immediately removed from the concurrent DashMap data structure, making the removal visible to all threads
5. Transaction T10 (index 10) executes concurrently and reads key K via `fetch_data_no_record()` [2](#0-1) 

6. T10's read does NOT find K at index 5 (already removed), so it reads from index 4 or base storage (e.g., value 50 instead of 100)
7. T10 completes execution with the incorrect read value (50)
8. Only AFTER this, T5 calls `scheduler.finish_execution()` at line 709 with `needs_suffix_validation=true`
9. This triggers `decrease_validation_idx(6)` to re-validate transactions >= 6 [3](#0-2) 

10. When T10 is validated, it re-reads K and gets the same incorrect value (50) because K at index 5 is still removed
11. Validation passes because the read value matches what T10 originally captured
12. T10 commits with incorrect state

**Why This Breaks Safety:**

The versioned cache uses `DashMap` for concurrent access without additional locking: [4](#0-3) 

The `remove()` operation immediately deletes entries from the BTreeMap: [5](#0-4) 

There is no mechanism preventing concurrent transactions from reading during the removal phase. The comment at line 585 acknowledges this is a "critical section" but provides no actual synchronization: [6](#0-5) 

**Contrast with BlockSTM v2:**

BlockSTM v2 avoids this issue by using `remove_v2()` which returns invalidated dependencies and immediately aborts them via `AbortManager`: [7](#0-6) [8](#0-7) 

This ensures dependent transactions are aborted BEFORE the removal is visible, using push validation instead of delayed suffix validation.

## Impact Explanation

**Severity: CRITICAL**

This vulnerability violates the fundamental **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks."

**Consensus Impact:**
- Different validators may observe different interleavings of the race condition
- Validator A's T10 might read during the gap (value 50)
- Validator B's T10 might read after finish_execution (value 100)  
- This produces different state transitions and different final state roots
- Results in consensus failure and potential chain splits

**Scope:**
- Affects all Aptos nodes running BlockSTM v1
- Can occur during normal operation without attacker intervention
- Non-determinism is data-dependent and timing-dependent
- May manifest intermittently, making debugging extremely difficult

This meets the Critical severity criteria: "Consensus/Safety violations" worth up to $1,000,000 in the Aptos bug bounty program.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

The vulnerability requires:
1. A transaction re-executing with a different write set (common during validation failures)
2. Concurrent execution of higher-index transactions (guaranteed in parallel execution)
3. Precise timing where T10 reads during the gap between removal and scheduler notification

**Factors Increasing Likelihood:**
- High transaction throughput increases concurrent execution
- Multiple worker threads (typically 8-32) increase race opportunities  
- The gap is small but not atomic - several instructions between lines 695-709
- Resource groups are commonly used in Aptos (e.g., token metadata)
- No synchronization prevents the race

**Factors Decreasing Likelihood:**
- The timing window is narrow (microseconds)
- Requires specific re-execution patterns
- BlockSTM v2 may be the default configuration (if v1 is deprecated)

However, even rare consensus failures are catastrophic for blockchain systems. A single occurrence could cause validator disagreement requiring emergency intervention or hard fork.

## Recommendation

**Immediate Fix:**

Move the `needs_suffix_validation = true` assignment BEFORE the removals, and ensure `decrease_validation_idx()` is called BEFORE any data is removed:

```rust
// Set suffix validation flag FIRST
if !prev_modified_group_keys.is_empty() {
    needs_suffix_validation = true;
}

// Notify scheduler to block new reads on higher transactions
if needs_suffix_validation && maybe_scheduler.is_some() {
    scheduler.prepare_suffix_validation(idx_to_execute + 1);
}

// NOW safe to remove
for k in prev_modified_resource_keys {
    versioned_cache.data().remove(&k, idx_to_execute);
}
for (k, tags) in prev_modified_group_keys {
    versioned_cache.data().remove(&k, idx_to_execute);
    versioned_cache.group_data().remove(&k, idx_to_execute, tags);
}
```

**Alternative Fix (Recommended):**

Adopt BlockSTM v2's approach exclusively:
- Use `remove_v2()` which returns dependencies
- Immediately invalidate dependent transactions via `AbortManager`
- Deprecate BlockSTM v1 entirely to eliminate this class of race conditions

**Additional Safeguards:**

Add assertions in validation to detect non-determinism:
```rust
// In validate_data_reads_impl
assert!(
    current_version_matches_expected,
    "Non-deterministic read detected: version mismatch during validation"
);
```

## Proof of Concept

```rust
#[test]
fn test_blockstm_v1_race_condition() {
    use rayon::prelude::*;
    
    // Setup: Block with 20 transactions
    let num_txns = 20;
    let versioned_cache = MVHashMap::new();
    
    // T5 incarnation 0: Write K=100
    versioned_cache.data().write(
        StateKey::raw(b"resource_K"),
        5, // txn_idx
        0, // incarnation
        Arc::new(StateValue::new_legacy(vec![100])),
        None,
    );
    
    // Simulate T5 incarnation 1 removing K while T10 reads concurrently
    let t5_handle = std::thread::spawn(|| {
        // T5 incarnation 1: Remove K (line 695)
        versioned_cache.data().remove(&StateKey::raw(b"resource_K"), 5);
        std::thread::sleep(Duration::from_micros(10)); // Simulate gap
        // finish_execution would happen here (line 709)
    });
    
    let t10_handle = std::thread::spawn(|| {
        std::thread::sleep(Duration::from_micros(5)); // Read during gap
        // T10 reads K
        let result = versioned_cache.data().fetch_data_no_record(
            &StateKey::raw(b"resource_K"),
            10, // txn_idx
        );
        
        // T10 should read K=100 from T5, but due to race:
        // - If read before removal: gets K=100 (correct)
        // - If read during gap: gets Uninitialized or older value (WRONG)
        result
    });
    
    t5_handle.join().unwrap();
    let t10_result = t10_handle.join().unwrap();
    
    // Non-deterministic: result depends on timing
    // Different validators will get different results
    assert!(matches!(t10_result, Err(MVDataError::Uninitialized)));
}
```

**To reproduce in production environment:**

1. Deploy contracts that frequently update resource groups
2. Generate high transaction load (>1000 TPS) to maximize concurrent execution
3. Monitor validator consensus disagreements
4. Check for state root mismatches after block execution

The race condition manifests as validators producing different state roots for identical block inputs, requiring manual intervention or rollback.

## Notes

- This vulnerability is specific to **BlockSTM v1** implementation
- BlockSTM v2 uses push validation via `AbortManager` and is NOT affected
- The issue is architectural: delayed validation vs immediate invalidation
- Similar race conditions may exist in other shared data structure updates
- Recommend comprehensive audit of all versioned_cache modifications in BlockSTM v1

### Citations

**File:** aptos-move/block-executor/src/executor.rs (L202-209)
```rust
                    None => {
                        // Clean up the write from previous incarnation.
                        abort_manager.invalidate_dependencies(
                            versioned_cache
                                .data()
                                .remove_v2::<_, false>(prev_key_ref, idx_to_execute)?,
                        )?;
                    },
```

**File:** aptos-move/block-executor/src/executor.rs (L585-592)
```rust
        // CAUTION: start shared output critical section.
        // If control flow reaches below and changes are applied to the shared data structures,
        // it should be guaranteed that the process will complete fully, completed by
        // recording of the input/outputs and lastly, by finish_execution. Hence, in the below
        // "critical section", e.g. returning with Ok status after observing the scheduler has halted
        // would be incorrect and lead to a PanicError if the block prologue txn were to be
        // executed later at the same index (after block cutting).
        // TODO(BlockSTMv2): Replace with a compile-time check if possible, or custom clippy lint.
```

**File:** aptos-move/block-executor/src/executor.rs (L677-709)
```rust
        // Remove entries from previous write/delta set that were not overwritten.
        for k in prev_modified_resource_keys {
            versioned_cache.data().remove(&k, idx_to_execute);
        }
        for (k, tags) in prev_modified_group_keys {
            // A change in state observable during speculative execution
            // (which includes group metadata and size) changes, suffix
            // re-validation is needed. For resources where speculative
            // execution waits on estimates, having a write that was there
            // but not anymore does not qualify, as it can only cause
            // additional waiting but not an incorrect speculation result.
            // However, a group size or metadata might be read, and then
            // speculative group update might be removed below. Without
            // triggering suffix re-validation, a later transaction might
            // end up with the incorrect read result (corresponding to the
            // removed group information from an incorrect speculative state).
            needs_suffix_validation = true;

            versioned_cache.data().remove(&k, idx_to_execute);
            versioned_cache
                .group_data()
                .remove(&k, idx_to_execute, tags);
        }

        last_input_output.record(
            idx_to_execute,
            read_set,
            execution_result,
            block_gas_limit_type,
            txn.user_txn_bytes_len() as u64,
        )?;
        if let Some(scheduler) = maybe_scheduler {
            scheduler.finish_execution(idx_to_execute, incarnation, needs_suffix_validation)
```

**File:** aptos-move/mvhashmap/src/versioned_data.rs (L69-73)
```rust
/// Maps each key (access path) to an internal versioned value representation.
pub struct VersionedData<K, V> {
    values: DashMap<K, VersionedValue<V>>,
    total_base_value_size: AtomicU64,
}
```

**File:** aptos-move/mvhashmap/src/versioned_data.rs (L449-459)
```rust
    pub fn remove<Q>(&self, key: &Q, txn_idx: TxnIndex)
    where
        Q: Equivalent<K> + Hash,
    {
        // TODO: investigate logical deletion.
        let mut v = self.values.get_mut(key).expect("Path must exist");
        assert_some!(
            v.versioned_map.remove(&ShiftedTxnIndex::new(txn_idx)),
            "Entry for key / idx must exist to be deleted"
        );
    }
```

**File:** aptos-move/mvhashmap/src/versioned_data.rs (L464-512)
```rust
    pub fn remove_v2<Q, const ONLY_COMPARE_METADATA: bool>(
        &self,
        key: &Q,
        txn_idx: TxnIndex,
    ) -> Result<BTreeMap<TxnIndex, Incarnation>, PanicError>
    where
        Q: Equivalent<K> + Hash + Debug,
    {
        let mut v = self.values.get_mut(key).ok_or_else(|| {
            code_invariant_error(format!("Path must exist for remove_v2: {:?}", key))
        })?;

        // Get the entry to be removed
        let removed_entry = v
            .versioned_map
            .remove(&ShiftedTxnIndex::new(txn_idx))
            .ok_or_else(|| {
                code_invariant_error(format!(
                    "Entry for key / idx must exist to be deleted: {:?}, {}",
                    key, txn_idx
                ))
            })?;

        if let EntryCell::ResourceWrite {
            incarnation: _,
            value_with_layout,
            dependencies,
        } = &removed_entry.value
        {
            match value_with_layout {
                ValueWithLayout::RawFromStorage(_) => {
                    unreachable!(
                        "Removed value written by txn {txn_idx} may not be RawFromStorage"
                    );
                },
                ValueWithLayout::Exchanged(data, layout) => {
                    let removed_deps = take_dependencies(dependencies);
                    v.handle_removed_dependencies::<ONLY_COMPARE_METADATA>(
                        txn_idx,
                        removed_deps,
                        data,
                        layout,
                    )
                },
            }
        } else {
            Ok(BTreeMap::new())
        }
    }
```

**File:** aptos-move/mvhashmap/src/versioned_data.rs (L514-528)
```rust
    // Fetches data but does not record a read dependency. This is used for BlockSTMv1
    // or for BlockSTMv2 post-commit final (for safety) validation.
    pub fn fetch_data_no_record<Q>(
        &self,
        key: &Q,
        txn_idx: TxnIndex,
    ) -> anyhow::Result<MVDataOutput<V>, MVDataError>
    where
        Q: Equivalent<K> + Hash,
    {
        self.values
            .get(key)
            .map(|v| v.read(txn_idx, None))
            .unwrap_or(Err(MVDataError::Uninitialized))
    }
```

**File:** aptos-move/block-executor/src/scheduler.rs (L553-594)
```rust
    pub fn finish_execution(
        &self,
        txn_idx: TxnIndex,
        incarnation: Incarnation,
        revalidate_suffix: bool,
    ) -> Result<SchedulerTask, PanicError> {
        // Note: It is preferable to hold the validation lock throughout the finish_execution,
        // in particular before updating execution status. The point was that we don't want
        // any validation to come before the validation status is correspondingly updated.
        // It may be possible to reduce granularity, but shouldn't make performance difference
        // and like this correctness argument is much easier to see, which is also why we grab
        // the write lock directly, and never release it during the whole function. This way,
        // even validation status readers have to wait if they somehow end up at the same index.
        let mut validation_status = self.txn_status[txn_idx as usize].1.write();
        self.set_executed_status(txn_idx, incarnation)?;

        self.wake_dependencies_after_execution(txn_idx)?;

        let (cur_val_idx, mut cur_wave) =
            Self::unpack_validation_idx(self.validation_idx.load(Ordering::Acquire));

        // Needs to be re-validated in a new wave
        if cur_val_idx > txn_idx {
            if revalidate_suffix {
                // The transaction execution required revalidating all higher txns (not
                // only itself), currently happens when incarnation writes to a new path
                // (w.r.t. the write-set of its previous completed incarnation).
                if let Some(wave) = self.decrease_validation_idx(txn_idx + 1) {
                    cur_wave = wave;
                };
            }
            // Update the minimum wave this txn needs to pass.
            validation_status.required_wave = cur_wave;
            return Ok(SchedulerTask::ValidationTask(
                txn_idx,
                incarnation,
                cur_wave,
            ));
        }

        Ok(SchedulerTask::Retry)
    }
```
