# Audit Report

## Title
State Snapshot Restore Missing Final Root Hash Verification Allows Incomplete State Restoration

## Summary
The state snapshot restoration process in `JellyfishMerkleRestore` verifies each chunk's `SparseMerkleRangeProof` but lacks final root hash verification in `finish_impl()`, allowing an attacker with backup storage access to deliver incomplete state that passes validation.

## Finding Description

The vulnerability exists in the state snapshot restoration flow where chunk-by-chunk verification is performed, but no final verification ensures completeness.

**Manifest Structure and Trust Model:**
The `StateSnapshotBackup` manifest is loaded as unauthenticated JSON, with only the `root_hash` authenticated via `LedgerInfoWithSignatures` in the proof file. [1](#0-0) 

During restoration, the manifest's `root_hash` is verified against the authenticated proof: [2](#0-1) 

**Chunk Verification Process:**
Each chunk's `SparseMerkleRangeProof` is verified in `add_chunk_impl()`, which calls `verify()` to check that accumulated state plus proof's right_siblings reconstruct the expected root hash: [3](#0-2) 

The `verify()` method computes left siblings from accumulated state and combines with the proof's right siblings to reconstruct and verify against `expected_root_hash`: [4](#0-3) 

**Critical Flaw:**
After all chunks are processed, `finish_impl()` freezes partial nodes and writes to storage WITHOUT verifying the final tree's root hash matches `expected_root_hash`: [5](#0-4) 

The only root hash verification occurs when creating a NEW `JellyfishMerkleRestore` instance to check if a previous restore completed correctly: [6](#0-5) 

**Attack Execution:**
1. Attacker with backup storage access modifies the manifest's `chunks` array to remove entries
2. Remaining chunks have valid `SparseMerkleRangeProof`s where `right_siblings` cryptographically commit to keys in removed chunks
3. During restoration, each chunk verifies successfully: `accumulated_state + proof.right_siblings = expected_root_hash`
4. The proof's `right_siblings` represent "keys to be added later," but those chunks never arrive
5. `finish_impl()` completes without checking that all promised keys were delivered
6. An incomplete tree is written to storage with a root hash ≠ `expected_root_hash`

## Impact Explanation

**Impact Classification: MEDIUM Severity**

This vulnerability qualifies as a "Limited Protocol Violation" with "State inconsistencies requiring manual intervention" under the MEDIUM severity category (up to $10,000).

The vulnerability causes:
- **State Inconsistency**: Restored state tree has incorrect root hash, breaking Merkle proof guarantees
- **Operational Disruption**: Node with incomplete state cannot participate in consensus correctly
- **Manual Intervention Required**: Node must be taken offline and re-restored from clean backup
- **Detection Through Normal Operation**: Validators with correct state continue functioning; compromised node detected via state sync failures

This does NOT qualify as HIGH severity because:
- It does NOT cause validator node slowdowns or API crashes (HIGH criteria)
- It does NOT directly violate consensus (validators with correct state remain unaffected)
- It requires compromised backup storage, not affecting trusted components
- Impact is limited to operational disruption of individual nodes

## Likelihood Explanation

**Likelihood: Medium-High**

Attack prerequisites:
- Attacker control over backup storage (compromised S3 bucket, malicious backup provider, MITM on backup downloads)
- Victim node performing state snapshot restoration
- No validator stake or network position required

The attack is straightforward:
1. Download legitimate backup files
2. Modify JSON manifest to remove chunk entries
3. Serve modified manifest to victim
4. Restoration succeeds with incomplete state

Backup storage compromise scenarios include cloud storage breaches, supply chain attacks on backup infrastructure, or malicious third-party backup services. The manifest design (unauthenticated JSON with authenticated proof) indicates backup storage is intentionally untrusted, making this a valid threat model.

## Recommendation

Add final root hash verification in `finish_impl()` before writing to storage:

```rust
pub fn finish_impl(mut self) -> Result<()> {
    self.wait_for_async_commit()?;
    
    // ... existing special case handling ...
    
    self.freeze(0);
    
    // NEW: Verify final root hash before writing
    let root_node_key = NodeKey::new_empty_path(self.version);
    let root_node = self.frozen_nodes.get(&root_node_key)
        .ok_or_else(|| AptosDbError::Other("Root node not found after restoration".into()))?;
    let actual_root_hash = root_node.hash();
    ensure!(
        actual_root_hash == self.expected_root_hash,
        "Final root hash mismatch. Expected: {}, Actual: {}",
        self.expected_root_hash,
        actual_root_hash
    );
    
    self.store.write_node_batch(&self.frozen_nodes)?;
    Ok(())
}
```

## Proof of Concept

A complete test demonstrating the vulnerability would:
1. Create a legitimate state tree with multiple chunks
2. Generate `SparseMerkleRangeProof`s for each chunk
3. Remove last chunks from restoration process
4. Show that `finish_impl()` succeeds despite incomplete state
5. Verify final root hash ≠ expected root hash

Test verification shows the final root hash is checked externally but NOT during restoration: [7](#0-6) 

The restore flow calls `finish_impl()` without intermediate verification: [8](#0-7) 

## Notes

- The vulnerability is valid but severity should be MEDIUM (not HIGH as originally claimed) per Aptos bug bounty criteria
- Backup storage is designed to be untrusted (JSON manifest with authenticated proof separation)
- Impact is limited to operational disruption of individual nodes, not network-wide consensus violations
- Detection occurs through state sync failures when the compromised node attempts consensus participation
- The fix is straightforward: add final root hash verification in `finish_impl()`

### Citations

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/manifest.rs (L29-51)
```rust
/// State snapshot backup manifest, representing a complete state view at specified version.
#[derive(Deserialize, Serialize)]
pub struct StateSnapshotBackup {
    /// Version at which this state snapshot is taken.
    pub version: Version,
    /// Epoch in which this state snapshot is taken.
    pub epoch: u64,
    /// Hash of the state tree root.
    pub root_hash: HashValue,
    /// All account blobs in chunks.
    pub chunks: Vec<StateSnapshotChunk>,
    /// BCS serialized
    /// `Tuple(TransactionInfoWithProof, LedgerInfoWithSignatures)`.
    ///   - The `TransactionInfoWithProof` is at `Version` above, and carries the same `root_hash`
    /// above; It proves that at specified version the root hash is as specified in a chain
    /// represented by the LedgerInfo below.
    ///   - The signatures on the `LedgerInfoWithSignatures` has a version greater than or equal to
    /// the version of this backup but is within the same epoch, so the signatures on it can be
    /// verified by the validator set in the same epoch, which can be provided by an
    /// `EpochStateBackup` recovered prior to this to the DB; Requiring it to be in the same epoch
    /// limits the requirement on such `EpochStateBackup` to no older than the same epoch.
    pub proof: FileHandle,
}
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L123-136)
```rust
        let manifest: StateSnapshotBackup =
            self.storage.load_json_file(&self.manifest_handle).await?;
        let (txn_info_with_proof, li): (TransactionInfoWithProof, LedgerInfoWithSignatures) =
            self.storage.load_bcs_file(&manifest.proof).await?;
        txn_info_with_proof.verify(li.ledger_info(), manifest.version)?;
        let state_root_hash = txn_info_with_proof
            .transaction_info()
            .ensure_state_checkpoint_hash()?;
        ensure!(
            state_root_hash == manifest.root_hash,
            "Root hash mismatch with that in proof. root hash: {}, expected: {}",
            manifest.root_hash,
            state_root_hash,
        );
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L196-206)
```rust
        let (finished, partial_nodes, previous_leaf) = if let Some(root_node) =
            tree_reader.get_node_option(&NodeKey::new_empty_path(version), "restore")?
        {
            info!("Previous restore is complete, checking root hash.");
            ensure!(
                root_node.hash() == expected_root_hash,
                "Previous completed restore has root hash {}, expecting {}",
                root_node.hash(),
                expected_root_hash,
            );
            (true, vec![], None)
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L386-392)
```rust
            self.add_one(key, value_hash);
            self.num_keys_received += 1;
        }

        // Verify what we have added so far is all correct.
        self.verify(proof)?;

```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L624-697)
```rust
    /// Verifies that all states that have been added so far (from the leftmost one to
    /// `self.previous_leaf`) are correct, i.e., we are able to construct `self.expected_root_hash`
    /// by combining all existing states and `proof`.
    #[allow(clippy::collapsible_if)]
    fn verify(&self, proof: SparseMerkleRangeProof) -> Result<()> {
        let previous_leaf = self
            .previous_leaf
            .as_ref()
            .expect("The previous leaf must exist.");

        let previous_key = previous_leaf.account_key();
        // If we have all siblings on the path from root to `previous_key`, we should be able to
        // compute the root hash. The siblings on the right are already in the proof. Now we
        // compute the siblings on the left side, which represent all the states that have ever
        // been added.
        let mut left_siblings = vec![];

        // The following process might add some extra placeholder siblings on the left, but it is
        // nontrivial to determine when the loop should stop. So instead we just add these
        // siblings for now and get rid of them in the next step.
        let mut num_visited_right_siblings = 0;
        for (i, bit) in previous_key.iter_bits().enumerate() {
            if bit {
                // This node is a right child and there should be a sibling on the left.
                let sibling = if i >= self.partial_nodes.len() * 4 {
                    *SPARSE_MERKLE_PLACEHOLDER_HASH
                } else {
                    Self::compute_left_sibling(
                        &self.partial_nodes[i / 4],
                        previous_key.get_nibble(i / 4),
                        (3 - i % 4) as u8,
                    )
                };
                left_siblings.push(sibling);
            } else {
                // This node is a left child and there should be a sibling on the right.
                num_visited_right_siblings += 1;
            }
        }
        ensure!(
            num_visited_right_siblings >= proof.right_siblings().len(),
            "Too many right siblings in the proof.",
        );

        // Now we remove any extra placeholder siblings at the bottom. We keep removing the last
        // sibling if 1) it's a placeholder 2) it's a sibling on the left.
        for bit in previous_key.iter_bits().rev() {
            if bit {
                if *left_siblings.last().expect("This sibling must exist.")
                    == *SPARSE_MERKLE_PLACEHOLDER_HASH
                {
                    left_siblings.pop();
                } else {
                    break;
                }
            } else if num_visited_right_siblings > proof.right_siblings().len() {
                num_visited_right_siblings -= 1;
            } else {
                break;
            }
        }

        // Left siblings must use the same ordering as the right siblings in the proof
        left_siblings.reverse();

        // Verify the proof now that we have all the siblings
        proof
            .verify(
                self.expected_root_hash,
                SparseMerkleLeafNode::new(*previous_key, previous_leaf.value_hash()),
                left_siblings,
            )
            .map_err(Into::into)
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L750-789)
```rust
    pub fn finish_impl(mut self) -> Result<()> {
        self.wait_for_async_commit()?;
        // Deal with the special case when the entire tree has a single leaf or null node.
        if self.partial_nodes.len() == 1 {
            let mut num_children = 0;
            let mut leaf = None;
            for i in 0..16 {
                if let Some(ref child_info) = self.partial_nodes[0].children[i] {
                    num_children += 1;
                    if let ChildInfo::Leaf(node) = child_info {
                        leaf = Some(node.clone());
                    }
                }
            }

            match num_children {
                0 => {
                    let node_key = NodeKey::new_empty_path(self.version);
                    assert!(self.frozen_nodes.is_empty());
                    self.frozen_nodes.insert(node_key, Node::Null);
                    self.store.write_node_batch(&self.frozen_nodes)?;
                    return Ok(());
                },
                1 => {
                    if let Some(node) = leaf {
                        let node_key = NodeKey::new_empty_path(self.version);
                        assert!(self.frozen_nodes.is_empty());
                        self.frozen_nodes.insert(node_key, node.into());
                        self.store.write_node_batch(&self.frozen_nodes)?;
                        return Ok(());
                    }
                },
                _ => (),
            }
        }

        self.freeze(0);
        self.store.write_node_batch(&self.frozen_nodes)?;
        Ok(())
    }
```

**File:** storage/aptosdb/src/state_restore/restore_test.rs (L251-252)
```rust
    let actual_root_hash = tree.get_root_hash(version).unwrap();
    assert_eq!(actual_root_hash, expected_root_hash);
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L260-272)
```rust
    fn finish(self) -> Result<()> {
        match self.restore_mode {
            StateSnapshotRestoreMode::KvOnly => self.kv_restore.lock().take().unwrap().finish()?,
            StateSnapshotRestoreMode::TreeOnly => {
                self.tree_restore.lock().take().unwrap().finish_impl()?
            },
            StateSnapshotRestoreMode::Default => {
                // for tree only mode, we also need to write the usage to DB
                self.kv_restore.lock().take().unwrap().finish()?;
                self.tree_restore.lock().take().unwrap().finish_impl()?
            },
        }
        Ok(())
```
