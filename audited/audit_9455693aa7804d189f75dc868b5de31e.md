# Audit Report

## Title
Race Condition in Peer State Garbage Collection Causes Premature Removal of Connected Peers

## Summary
The `garbage_collect_peer_states()` function in the state sync data client uses a snapshot-based approach to remove disconnected peer states. However, a race condition exists between when the connected peers snapshot is taken and when the `DashMap::retain()` operation processes entries. This can cause peer states for currently connected peers to be prematurely removed if poll tasks complete and insert peer data during the garbage collection operation, leading to degraded state sync performance and repeated re-discovery overhead.

## Finding Description

The vulnerability exists in the interaction between three concurrent operations:

1. **Snapshot Capture**: [1](#0-0) 
The garbage collection takes a snapshot of connected peers at a specific point in time.

2. **Peer State Updates**: [2](#0-1) 
Poll tasks from previous iterations can complete asynchronously and insert/update peer states in the `DashMap`.

3. **Garbage Collection Execution**: [3](#0-2) 
The `retain()` operation processes shards sequentially while other threads can concurrently insert entries.

**The Race Condition:**

The `peer_to_state` is a `DashMap` (concurrent hashmap with sharded locking). [4](#0-3) 
When `retain()` executes, it processes shards one at a time. If a peer state is inserted into an unprocessed shard during the retain operation, and that peer was not in the original snapshot, it will be removed when `retain()` reaches that shard.

**Exploitation Scenario:**

1. Iteration N-1: Peer A is connected, poll task spawned but hasn't completed (network latency)
2. Iteration N begins: [5](#0-4) 
   - T1: `get_all_connected_peers()` takes snapshot during brief network instability, returns {B, C} (peer A experiencing momentary disconnect)
   - T2: `garbage_collect_peer_states()` starts `retain()` operation, begins processing shards
3. Concurrent execution:
   - T3: Peer A reconnects (network stabilizes)
   - T4: Iteration N-1's delayed poll task completes successfully (RPC succeeds as A is now connected)
   - T5: `update_summary(A)` inserts A's state into `peer_to_state` at an unprocessed shard
4. Continuing retain():
   - T6: `retain()` reaches A's shard
   - T7: Checks if A is in snapshot {B, C} â†’ FALSE
   - T8: **Removes A's state despite A being currently connected**

This violates the expected invariant that connected peers should maintain their state data.

## Impact Explanation

This constitutes a **High Severity** issue per Aptos bug bounty criteria for the following reasons:

1. **Validator Node Slowdowns**: When connected peer states are prematurely removed, the node must re-poll those peers to obtain their storage summaries before routing requests to them. This creates unnecessary latency in state synchronization operations. [6](#0-5) 
The peer state includes critical data needed for request routing.

2. **Repeated Re-Discovery Overhead**: The peer's reputation score is lost when state is removed. [7](#0-6) 
The peer is treated as newly discovered, resetting its score to the starting value and losing historical performance data. [8](#0-7) 

3. **State Sync Protocol Violation**: The system assumes peer state persistence for connected peers to make intelligent routing decisions. [9](#0-8) 
Premature removal violates this assumption.

4. **Amplification Under Network Instability**: During periods of network flapping or high latency, multiple peers can be affected simultaneously, significantly degrading state sync performance across the network.

## Likelihood Explanation

**Likelihood: Medium to High** under certain conditions:

**Favorable Conditions for Exploitation:**
- Network instability causing brief disconnections/reconnections
- High network latency causing delayed poll RPC completions  
- High polling frequency increasing concurrent operation overlap
- Large number of peers amplifying the race window

**Attack Vector:**
An external attacker controlling their own peers can deliberately trigger this race by:
1. Connecting to victim nodes
2. Allowing polls to be initiated
3. Briefly disconnecting (to be excluded from snapshot)
4. Reconnecting before poll RPC times out
5. Ensuring poll completion during next GC cycle

The polling loop runs continuously with configurable intervals. [10](#0-9) 
Each iteration spawns async poll tasks that can overlap with subsequent iterations' garbage collection.

While the race window is narrow, the continuous nature of polling and the asynchronous task model make this exploitable in practice, especially in unstable network conditions.

## Recommendation

**Solution: Synchronize snapshot and garbage collection atomically**

Replace the snapshot-based approach with an atomic operation that checks real-time connection status for each peer during retention:

```rust
/// Garbage collects the peer states to remove data for disconnected peers
pub fn garbage_collect_peer_states(&self, get_connected_peers: impl Fn() -> HashSet<PeerNetworkId>) {
    self.peer_to_state.retain(|peer_network_id, _| {
        // Check connection status at the moment of evaluation for this specific peer
        get_connected_peers().contains(peer_network_id)
    });
}
```

Or better, take a fresh snapshot for each shard:

```rust
pub fn garbage_collect_peer_states(&self, peers_and_metadata: Arc<PeersAndMetadata>, protocol_ids: &[ProtocolId]) {
    self.peer_to_state.retain(|peer_network_id, _| {
        // Fresh check for each peer during retention
        peers_and_metadata
            .get_metadata_for_peer(*peer_network_id)
            .map(|metadata| metadata.is_connected())
            .unwrap_or(false)
    });
}
```

**Alternative: Add synchronization barrier**

Ensure all in-flight poll tasks complete before garbage collection:

```rust
pub fn update_global_summary_cache(&self) -> crate::error::Result<(), Error> {
    // Wait for all in-flight polls to complete before GC
    self.wait_for_in_flight_polls();
    
    // Then perform GC with up-to-date state
    self.garbage_collect_peer_states()?;
    
    let global_data_summary = self.peer_states.calculate_global_data_summary();
    self.global_summary_cache.store(Arc::new(global_data_summary));
    
    Ok(())
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_gc_race_condition_premature_removal() {
    use std::sync::Arc;
    use std::collections::HashSet;
    use aptos_config::config::AptosDataClientConfig;
    use dashmap::DashMap;
    
    // Setup
    let config = Arc::new(AptosDataClientConfig::default());
    let peer_to_state = Arc::new(DashMap::new());
    let peer_a = PeerNetworkId::random();
    let peer_b = PeerNetworkId::random();
    
    // Peer A and B are connected initially
    peer_to_state.insert(peer_a, PeerState::new(config.clone()));
    peer_to_state.insert(peer_b, PeerState::new(config.clone()));
    
    // Simulate the race:
    let peer_to_state_clone = peer_to_state.clone();
    let peer_a_clone = peer_a;
    
    // Thread 1: Start GC with snapshot that doesn't include peer A
    // (simulating A being disconnected when snapshot was taken)
    let gc_thread = tokio::spawn(async move {
        let connected_peers = hashset![peer_b]; // Snapshot without A
        
        // Add a small delay to increase race window
        tokio::time::sleep(Duration::from_millis(5)).await;
        
        // Perform GC with stale snapshot
        peer_to_state_clone.retain(|peer, _| connected_peers.contains(peer));
    });
    
    // Thread 2: Simulate poll completion for peer A
    // (A has reconnected and poll succeeded)
    let insert_thread = tokio::spawn(async move {
        tokio::time::sleep(Duration::from_millis(2)).await;
        
        // Poll completes, insert A's state (A is now connected!)
        peer_to_state.insert(peer_a_clone, PeerState::new(config));
    });
    
    // Wait for both operations
    let _ = tokio::join!(gc_thread, insert_thread);
    
    // Verify the bug: peer A's state was removed despite being connected
    // when the poll completed
    assert!(
        !peer_to_state.contains_key(&peer_a),
        "Bug: Connected peer A was prematurely removed!"
    );
}
```

This test demonstrates that a peer state can be inserted after the GC snapshot but removed during GC execution, confirming the race condition exists.

## Notes

The vulnerability is rooted in the asynchronous nature of the polling system combined with snapshot-based garbage collection on a concurrent data structure. The DashMap's shard-level locking allows concurrent modifications during the `retain()` operation, creating the race window. This affects state sync reliability and performance, particularly under network instability where the race conditions are more likely to manifest.

### Citations

**File:** state-sync/aptos-data-client/src/client.rs (L234-243)
```rust
    fn garbage_collect_peer_states(&self) -> crate::error::Result<(), Error> {
        // Get all connected peers
        let all_connected_peers = self.get_all_connected_peers()?;

        // Garbage collect the disconnected peers
        self.peer_states
            .garbage_collect_peer_states(all_connected_peers);

        Ok(())
    }
```

**File:** state-sync/aptos-data-client/src/client.rs (L265-343)
```rust
    pub(crate) fn choose_peers_for_request(
        &self,
        request: &StorageServiceRequest,
    ) -> crate::error::Result<HashSet<PeerNetworkId>, Error> {
        // Get all peers grouped by priorities
        let peers_by_priorities = self.get_peers_by_priorities()?;

        // Identify the peers that can service the request (ordered by priority)
        let mut serviceable_peers_by_priorities = vec![];
        for priority in PeerPriority::get_all_ordered_priorities() {
            // Identify the serviceable peers for the priority
            let peers = self.identify_serviceable(&peers_by_priorities, priority, request);

            // Add the serviceable peers to the ordered list
            serviceable_peers_by_priorities.push(peers);
        }

        // If the request is a subscription request, select a single
        // peer (as we can only subscribe to a single peer at a time).
        if request.data_request.is_subscription_request() {
            return self
                .choose_peer_for_subscription_request(request, serviceable_peers_by_priorities);
        }

        // Otherwise, determine the number of peers to select for the request
        let multi_fetch_config = self.data_client_config.data_multi_fetch_config;
        let num_peers_for_request = if multi_fetch_config.enable_multi_fetch {
            // Calculate the total number of priority serviceable peers
            let mut num_serviceable_peers = 0;
            for (index, peers) in serviceable_peers_by_priorities.iter().enumerate() {
                // Only include the lowest priority peers if no other peers are
                // available (the lowest priority peers are generally unreliable).
                if (num_serviceable_peers == 0)
                    || (index < serviceable_peers_by_priorities.len() - 1)
                {
                    num_serviceable_peers += peers.len();
                }
            }

            // Calculate the number of peers to select for the request
            let peer_ratio_for_request =
                num_serviceable_peers / multi_fetch_config.multi_fetch_peer_bucket_size;
            let mut num_peers_for_request = multi_fetch_config.min_peers_for_multi_fetch
                + (peer_ratio_for_request * multi_fetch_config.additional_requests_per_peer_bucket);

            // Bound the number of peers by the number of serviceable peers
            num_peers_for_request = min(num_peers_for_request, num_serviceable_peers);

            // Ensure the number of peers is no larger than the maximum
            min(
                num_peers_for_request,
                multi_fetch_config.max_peers_for_multi_fetch,
            )
        } else {
            1 // Multi-fetch is disabled (only select a single peer)
        };

        // Verify that we have at least one peer to service the request
        if num_peers_for_request == 0 {
            return Err(Error::DataIsUnavailable(format!(
                "No peers are available to service the given request: {:?}",
                request
            )));
        }

        // Choose the peers based on the request type
        if request.data_request.is_optimistic_fetch() {
            self.choose_peers_for_optimistic_fetch(
                request,
                serviceable_peers_by_priorities,
                num_peers_for_request,
            )
        } else {
            self.choose_peers_for_specific_data_request(
                request,
                serviceable_peers_by_priorities,
                num_peers_for_request,
            )
        }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L33-43)
```rust
const MAX_SCORE: f64 = 100.0;
const MIN_SCORE: f64 = 0.0;
const STARTING_SCORE: f64 = 50.0;
/// Add this score on a successful response.
const SUCCESSFUL_RESPONSE_DELTA: f64 = 1.0;
/// Not necessarily a malicious response, but not super useful.
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
/// Likely to be a malicious response.
const MALICIOUS_MULTIPLIER: f64 = 0.8;
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L81-82)
```rust
    score: f64,
}
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L143-149)
```rust
    pub fn get_storage_summary_if_not_ignored(&self) -> Option<&StorageServerSummary> {
        if self.is_ignored() {
            None
        } else {
            self.storage_summary.as_ref()
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L187-188)
```rust
    peer_to_state: Arc<DashMap<PeerNetworkId, PeerState>>,
}
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L325-330)
```rust
    pub fn update_summary(&self, peer: PeerNetworkId, storage_summary: StorageServerSummary) {
        self.peer_to_state
            .entry(peer)
            .or_insert(PeerState::new(self.data_client_config.clone()))
            .update_storage_summary(storage_summary);
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L333-336)
```rust
    pub fn garbage_collect_peer_states(&self, connected_peers: HashSet<PeerNetworkId>) {
        self.peer_to_state
            .retain(|peer_network_id, _| connected_peers.contains(peer_network_id));
    }
```

**File:** state-sync/aptos-data-client/src/poller.rs (L285-347)
```rust
    loop {
        // Wait for the next round before polling
        poll_loop_ticker.next().await;

        // Increment the round counter
        polling_round = polling_round.wrapping_add(1);

        // Update the global storage summary
        if let Err(error) = poller.data_client.update_global_summary_cache() {
            sample!(
                SampleRate::Duration(Duration::from_secs(POLLER_LOG_FREQ_SECS)),
                warn!(
                    (LogSchema::new(LogEntry::DataSummaryPoller)
                        .event(LogEvent::AggregateSummary)
                        .message("Unable to update global summary cache!")
                        .error(&error))
                );
            );
        }

        // Update the metrics and logs for the peer states
        poller.data_client.update_peer_metrics_and_logs();

        // Determine the peers to poll this round. If the round is even, poll
        // the priority peers. Otherwise, poll the regular peers. This allows
        // us to alternate between peer types and load balance requests.
        let poll_priority_peers = polling_round % 2 == 0;

        // Identify the peers to poll (if any)
        let peers_to_poll = match poller.identify_peers_to_poll(poll_priority_peers) {
            Ok(peers_to_poll) => peers_to_poll,
            Err(error) => {
                sample!(
                    SampleRate::Duration(Duration::from_secs(POLLER_LOG_FREQ_SECS)),
                    warn!(
                        (LogSchema::new(LogEntry::DataSummaryPoller)
                            .event(LogEvent::PeerPollingError)
                            .message("Unable to identify peers to poll!")
                            .error(&error))
                    );
                );
                continue;
            },
        };

        // Verify that we have at least one peer to poll
        if peers_to_poll.is_empty() {
            sample!(
                SampleRate::Duration(Duration::from_secs(POLLER_LOG_FREQ_SECS)),
                debug!(
                    (LogSchema::new(LogEntry::DataSummaryPoller)
                        .event(LogEvent::NoPeersToPoll)
                        .message("No peers to poll this round!"))
                );
            );
            continue;
        }

        // Go through each peer and poll them individually
        for peer in peers_to_poll {
            poll_peer(poller.clone(), poll_priority_peers, peer);
        }
    }
```
