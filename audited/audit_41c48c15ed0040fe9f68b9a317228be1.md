# Audit Report

## Title
QuorumStoreInlineHybrid Batch Duplication Enables Resource Exhaustion via Unvalidated Payload Construction

## Summary
A Byzantine validator can construct a `QuorumStoreInlineHybrid` payload containing duplicate `BatchInfo` entries in both `proofs` and `inline_batches` collections. The validation logic checks each collection independently but fails to verify uniqueness across both, enabling resource waste through duplicate transaction transmission and state corruption in quorum store tracking.

## Finding Description

The `Payload::QuorumStoreInlineHybrid` variant contains two separate collections: `inline_batches` (batches with embedded transactions) and `proof_with_data.proofs` (batch references via ProofOfStore). During payload verification, the system validates these collections independently without checking for duplicates between them. [1](#0-0) 

The verification only ensures proof signatures are valid and inline batch digests match their transactions, but never checks if the same `BatchInfo` appears in both collections.

The legitimate block construction path in `proof_manager.rs` explicitly prevents this by excluding already-selected batches when pulling inline batches: [2](#0-1) 

However, a Byzantine validator can bypass this by manually constructing a malformed payload. During transaction retrieval, both collections are processed sequentially without deduplication: [3](#0-2) 

This causes duplicate transactions to be transmitted across the network and verified multiple times. While transaction deduplication occurs before execution, the resource waste during retrieval and transmission is significant. [4](#0-3) 

Most critically, during commit notification, both collections are processed separately, causing duplicate batches to be notified twice: [5](#0-4) 

This double notification corrupts quorum store state tracking. In `batch_proof_queue.rs`, the `mark_committed` function decrements proof counters without checking for duplicates: [6](#0-5) 

The `dec_remaining_proofs()` call on line 859 would execute twice for a duplicate batch, corrupting the `remaining_proofs` counter used for back pressure calculations and batch selection logic.

## Impact Explanation

This vulnerability qualifies as **Medium-High Severity** under the Aptos bug bounty criteria:

**Validator Node Slowdowns (High Severity):**
- 2x network bandwidth consumption for duplicate transaction transmission
- 2x computational cost for batch verification and processing
- Increased memory pressure during transaction handling
- Can degrade validator performance across the network

**Protocol Violations (Medium-High Severity):**
- Breaks the protocol invariant that batch identifiers should be unique within a payload
- Corrupts quorum store state tracking (proof queue counters)
- Invalid back pressure calculations affecting future batch selection
- Unreliable metrics and monitoring data

**State Corruption:**
The double-decrementing of proof queue counters can lead to incorrect remaining proof counts, affecting:
- Back pressure calculations for batch generation
- Batch selection logic for future blocks
- System resource management decisions

Unlike pure network-level DoS attacks (out of scope), this exploits a protocol-level validation gap that causes state corruption in consensus components.

## Likelihood Explanation

**Likelihood: Medium-High**

**Attacker Requirements:**
- Must be a validator (within Byzantine threat model - system tolerates < 1/3 Byzantine validators)
- No special privileges required beyond normal validator block proposal
- Single validator can exploit without coordination

**Attack Complexity:**
- Low: Construct a `Payload` struct with duplicate `BatchInfo` across collections
- No timing requirements or race conditions
- Deterministic and repeatable attack
- Can be triggered whenever the attacker proposes a block

**Detection:**
- Moderate difficulty: No monitoring for duplicate batches in payloads
- Transaction deduplication masks execution impact
- Operators may only observe increased bandwidth/CPU usage
- State corruption in proof queue may go unnoticed until affecting back pressure

## Recommendation

Add validation in `Payload::verify()` to check for duplicate `BatchInfo` across `proofs` and `inline_batches`:

```rust
(true, Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _))
| (true, Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _)) => {
    Self::verify_with_cache(&proof_with_data.proofs, verifier, proof_cache)?;
    Self::verify_inline_batches(
        inline_batches.iter().map(|(info, txns)| (info, txns)),
    )?;
    
    // NEW: Check for duplicate BatchInfo across collections
    let mut seen_batches = HashSet::new();
    for proof in &proof_with_data.proofs {
        ensure!(
            seen_batches.insert(proof.info()),
            "Duplicate batch found in proofs and inline_batches"
        );
    }
    for (batch_info, _) in inline_batches {
        ensure!(
            seen_batches.insert(batch_info),
            "Duplicate batch found in proofs and inline_batches"
        );
    }
    
    Ok(())
},
```

## Proof of Concept

A Byzantine validator can create a malicious payload as follows:

1. Generate a valid `BatchInfo` B with 1000 transactions
2. Create a `ProofOfStore` for batch B with valid signatures
3. Construct a `QuorumStoreInlineHybrid` payload containing:
   - `proof_with_data.proofs`: [ProofOfStore(B)]
   - `inline_batches`: [(B, transactions)]
4. Propose a block with this payload

The payload will pass validation since:
- Proof signatures are valid (verified independently)
- Inline batch digest matches transactions (verified independently)
- No check exists for duplicates across collections

Result:
- Batch B's transactions transmitted twice (2000 total)
- Batch B verified twice
- Batch B notified to quorum store twice during commit
- Proof queue counters decremented twice, corrupting state

## Notes

This vulnerability demonstrates a validation gap where legitimate block construction prevents duplicates through exclusion logic in `proof_manager.rs`, but the validation layer (`Payload::verify()`) does not enforce this invariant. A Byzantine validator can exploit this gap to cause resource waste and state corruption that affects validator performance and quorum store integrity.

### Citations

**File:** consensus/consensus-types/src/common.rs (L590-597)
```rust
            (true, Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _))
            | (true, Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _)) => {
                Self::verify_with_cache(&proof_with_data.proofs, verifier, proof_cache)?;
                Self::verify_inline_batches(
                    inline_batches.iter().map(|(info, txns)| (info, txns)),
                )?;
                Ok(())
            },
```

**File:** consensus/src/quorum_store/proof_manager.rs (L167-174)
```rust
                let (inline_batches, inline_payload_size, _) =
                    self.batch_proof_queue.pull_batches_with_transactions(
                        &excluded_batches
                            .iter()
                            .cloned()
                            .chain(proof_block.iter().map(|proof| proof.info().clone()))
                            .chain(opt_batches.clone())
                            .collect(),
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L134-150)
```rust
        let all_transactions = {
            let mut all_txns = process_qs_payload(
                proof_with_data,
                self.batch_reader.clone(),
                block,
                &self.ordered_authors,
            )
            .await?;
            all_txns.append(
                &mut inline_batches
                    .iter()
                    // TODO: Can clone be avoided here?
                    .flat_map(|(_batch_info, txns)| txns.clone())
                    .collect(),
            );
            all_txns
        };
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L189-201)
```rust
                Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _)
                | Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _) => {
                    inline_batches
                        .iter()
                        .map(|(batch_info, _)| batch_info.clone().into())
                        .chain(
                            proof_with_data
                                .proofs
                                .iter()
                                .map(|proof| proof.info().clone().into()),
                        )
                        .collect::<Vec<_>>()
                },
```

**File:** consensus/src/block_preparer.rs (L99-99)
```rust
            let deduped_txns = txn_deduper.dedup(filtered_txns);
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L846-863)
```rust
    pub(crate) fn mark_committed(&mut self, batches: Vec<BatchInfoExt>) {
        let start = Instant::now();
        for batch in batches.into_iter() {
            let batch_key = BatchKey::from_info(&batch);
            if let Some(item) = self.items.get(&batch_key) {
                if let Some(ref proof) = item.proof {
                    let insertion_time = item
                        .proof_insertion_time
                        .expect("Insertion time is updated with proof");
                    counters::pos_to_commit(
                        proof.gas_bucket_start(),
                        insertion_time.elapsed().as_secs_f64(),
                    );
                    self.dec_remaining_proofs(&batch.author(), batch.num_txns());
                    counters::GARBAGE_COLLECTED_IN_PROOF_QUEUE_COUNTER
                        .with_label_values(&["committed_proof"])
                        .inc();
                }
```
