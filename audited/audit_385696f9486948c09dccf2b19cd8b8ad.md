# Audit Report

## Title
Consensus Observer Subscription Desynchronization Due to Aggressive Garbage Collection Timing

## Summary
The `garbage_collection_interval_ms` configuration parameter lacks minimum value validation, allowing it to be set to arbitrarily small values. When combined with network reconnection events, this creates a timing vulnerability where consensus observers can be removed from the active subscribers list while their connection remains active, causing them to silently miss consensus messages for up to 15 seconds before detection.

## Finding Description

The consensus publisher's garbage collection mechanism removes subscribers based solely on network connection state, without coordination with the observer's subscription management. This creates a state desynchronization vulnerability when the following conditions occur: [1](#0-0) 

The garbage collection logic identifies disconnected subscribers by comparing active subscribers against connected peers from the network metadata. However, there is no minimum bound validation on `garbage_collection_interval_ms`: [2](#0-1) [3](#0-2) 

The vulnerability manifests through this attack sequence:

1. **Initial State**: Consensus observer O is subscribed to publisher P and receiving messages normally
2. **Network Event**: Observer O experiences a brief network disconnection (packet loss, router issue, connection reset)
3. **Connection State Update**: Network layer marks O's connection state as `Disconnecting` or removes the peer from connected peers metadata
4. **Aggressive Garbage Collection**: If `garbage_collection_interval_ms` is set to a small value (e.g., 100ms), GC runs frequently and removes O from `active_subscribers`
5. **Reconnection**: Network connection is restored within milliseconds, O returns to `connected_peers` with `ConnectionState::Connected`
6. **Desynchronization Window**: Observer O's subscription manager doesn't run health checks until the next `progress_check_interval_ms` cycle (default 5 seconds): [4](#0-3) 

7. **Silent Message Loss**: Publisher sends consensus messages (OrderedBlock, CommitDecision) but O is NOT in `active_subscribers`, so messages are not delivered: [5](#0-4) 

8. **Delayed Detection**: Observer doesn't detect the issue until either `progress_check_interval_ms` (5 seconds) or `max_subscription_timeout_ms` (15 seconds) expires: [6](#0-5) 

The critical issue is that subscriptions are identified by `PeerNetworkId` only, with no session token or reconnection handling: [7](#0-6) 

During active consensus rounds (1-3 seconds per round), missing messages for 5-15 seconds means missing multiple critical consensus rounds. The observer believes it's subscribed (connection is healthy) but the publisher has silently removed the subscription.

## Impact Explanation

**Medium Severity** - State inconsistencies requiring intervention (per Aptos bug bounty criteria):

1. **State Desynchronization**: Creates inconsistent state between observer (believes subscribed) and publisher (subscription removed), violating the assumption that subscription state is synchronized
2. **Consensus Observer Degradation**: Affected observers miss OrderedBlock and CommitDecision messages during active consensus rounds, forcing fallback to state sync
3. **Network-Wide Impact**: If multiple observers experience this simultaneously due to network instability or deliberate manipulation, it could degrade the consensus observer network's effectiveness
4. **Silent Failure**: The issue is not immediately detected - observers continue operating believing they're receiving updates when they're not
5. **Configuration Attack Surface**: Lack of validation on `garbage_collection_interval_ms` allows misconfiguration (intentional or accidental) to exacerbate the issue

This does not reach High/Critical severity because:
- It doesn't directly break consensus safety or validator operations
- Observers eventually recover through fallback mechanisms
- No funds are lost or state permanently corrupted
- It requires specific network conditions to trigger

## Likelihood Explanation

**Moderate to High Likelihood**:

1. **No Input Validation**: The `garbage_collection_interval_ms` parameter has no minimum value validation, allowing it to be set to 0 or very small values (10ms, 50ms, 100ms)
2. **Common Network Conditions**: Brief network disconnections during connection resets, router updates, or packet loss are common in distributed systems
3. **Large Timing Gap**: The default 5-second gap between publisher garbage collection and observer health checks creates a wide window for exploitation
4. **Misconfiguration Risk**: Operators may set aggressive garbage collection intervals thinking it improves cleanup efficiency, unaware of the timing vulnerability
5. **Adversarial Scenarios**: An attacker controlling network infrastructure could deliberately cause brief disconnections at strategic times during consensus rounds

The vulnerability becomes more likely when:
- `garbage_collection_interval_ms` < 1000ms (more frequent cleanup)
- Network experiences instability or is under adversarial control
- Multiple observers are deployed (increases probability some are affected)

## Recommendation

Implement multiple defensive measures:

**1. Add Minimum Value Validation**

Add configuration sanitizer for `ConsensusObserverConfig` similar to `ConsensusConfig`:

```rust
impl ConfigSanitizer for ConsensusObserverConfig {
    fn sanitize(
        &mut self,
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        // Enforce minimum garbage collection interval of 30 seconds
        // to prevent timing attacks and excessive cleanup overhead
        const MIN_GARBAGE_COLLECTION_INTERVAL_MS: u64 = 30_000;
        if self.garbage_collection_interval_ms < MIN_GARBAGE_COLLECTION_INTERVAL_MS {
            return Err(Error::ConfigSanitizerFailed(
                "ConsensusObserverConfig".to_string(),
                format!(
                    "garbage_collection_interval_ms must be at least {}ms, got {}ms",
                    MIN_GARBAGE_COLLECTION_INTERVAL_MS,
                    self.garbage_collection_interval_ms
                ),
            ));
        }
        Ok(())
    }
}
```

**2. Add Graceful Reconnection Handling**

Modify garbage collection to track recent disconnections and provide a grace period:

```rust
// Add field to ConsensusPublisher
recently_disconnected: Arc<RwLock<HashMap<PeerNetworkId, Instant>>>,

// In garbage_collect_subscriptions, add grace period check:
const RECONNECTION_GRACE_PERIOD_MS: u64 = 10_000; // 10 seconds

for peer_network_id in &disconnected_subscribers {
    // Check if peer recently disconnected
    let should_remove = {
        let mut recent_disconnects = self.recently_disconnected.write();
        match recent_disconnects.get(peer_network_id) {
            Some(disconnect_time) => {
                // Only remove if disconnected for longer than grace period
                disconnect_time.elapsed() > Duration::from_millis(RECONNECTION_GRACE_PERIOD_MS)
            },
            None => {
                // First time seeing as disconnected, record time
                recent_disconnects.insert(*peer_network_id, Instant::now());
                false
            }
        }
    };
    
    if should_remove {
        self.remove_active_subscriber(peer_network_id);
    }
}
```

**3. Reduce Observer Health Check Interval**

Consider reducing `progress_check_interval_ms` default from 5000ms to 1000-2000ms to detect subscription loss faster.

## Proof of Concept

```rust
#[tokio::test]
async fn test_garbage_collection_timing_vulnerability() {
    use consensus::consensus_observer::publisher::consensus_publisher::ConsensusPublisher;
    use aptos_config::config::ConsensusObserverConfig;
    use aptos_network::application::storage::PeersAndMetadata;
    use aptos_network::application::metadata::ConnectionState;
    use aptos_config::network_id::{NetworkId, PeerNetworkId};
    use aptos_types::PeerId;
    use std::time::Duration;
    use tokio::time::sleep;
    
    // Create network setup
    let network_id = NetworkId::Validator;
    let peers_and_metadata = PeersAndMetadata::new(&[network_id]);
    let network_client = NetworkClient::new(vec![], vec![], hashmap![], peers_and_metadata.clone());
    let consensus_observer_client = Arc::new(ConsensusObserverClient::new(network_client));
    
    // Create publisher with aggressive GC interval (100ms instead of 60s default)
    let mut config = ConsensusObserverConfig::default();
    config.garbage_collection_interval_ms = 100; // VULNERABLE: No validation!
    
    let (consensus_publisher, _) = ConsensusPublisher::new(
        config,
        consensus_observer_client,
    );
    
    // Add observer as connected peer and subscriber
    let observer_peer = PeerNetworkId::new(network_id, PeerId::random());
    let connection_metadata = ConnectionMetadata::mock(observer_peer.peer_id());
    peers_and_metadata.insert_connection_metadata(observer_peer, connection_metadata.clone()).unwrap();
    
    // Subscribe the observer
    let subscribe_msg = ConsensusPublisherNetworkMessage::new(
        observer_peer,
        ConsensusObserverRequest::Subscribe,
        ResponseSender::new_for_test(),
    );
    consensus_publisher.process_network_message(subscribe_msg);
    
    // Verify observer is subscribed
    assert_eq!(consensus_publisher.get_active_subscribers().len(), 1);
    
    // Simulate brief network disconnection (e.g., 50ms)
    peers_and_metadata.update_connection_state(observer_peer, ConnectionState::Disconnecting).unwrap();
    
    // Wait for aggressive GC to run (100ms interval)
    sleep(Duration::from_millis(150)).await;
    consensus_publisher.garbage_collect_subscriptions();
    
    // Observer subscription removed by GC
    assert_eq!(consensus_publisher.get_active_subscribers().len(), 0);
    
    // Network reconnects quickly
    peers_and_metadata.update_connection_state(observer_peer, ConnectionState::Connected).unwrap();
    
    // VULNERABILITY: Observer is now connected but NOT subscribed
    // Messages published here would NOT reach the observer
    let message = ConsensusObserverMessage::new_ordered_block_message(
        vec![],
        LedgerInfoWithSignatures::new(/* ... */),
    );
    consensus_publisher.publish_message(message);
    
    // Observer would miss this message for up to 5-15 seconds
    // until health check or timeout detects the issue
    assert_eq!(consensus_publisher.get_active_subscribers().len(), 0);
    // Observer is connected but receives no messages - state desynchronization!
}
```

## Notes

This vulnerability demonstrates a classic distributed systems timing issue where lack of coordination between components (publisher's aggressive garbage collection vs. observer's periodic health checks) combined with insufficient input validation creates a state desynchronization window. The issue is exacerbated by the absence of minimum bounds checking on the `garbage_collection_interval_ms` parameter, allowing operators to unknowingly or maliciously configure the system into a vulnerable state.

### Citations

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L93-96)
```rust
    /// Adds the given subscriber to the set of active subscribers
    fn add_active_subscriber(&self, peer_network_id: PeerNetworkId) {
        self.active_subscribers.write().insert(peer_network_id);
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L98-155)
```rust
    /// Garbage collect inactive subscriptions by removing peers that are no longer connected
    fn garbage_collect_subscriptions(&self) {
        // Get the set of active subscribers
        let active_subscribers = self.get_active_subscribers();

        // Get the connected peers and metadata
        let peers_and_metadata = self.consensus_observer_client.get_peers_and_metadata();
        let connected_peers_and_metadata =
            match peers_and_metadata.get_connected_peers_and_metadata() {
                Ok(connected_peers_and_metadata) => connected_peers_and_metadata,
                Err(error) => {
                    // We failed to get the connected peers and metadata
                    warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::UnexpectedError)
                        .message(&format!(
                            "Failed to get connected peers and metadata! Error: {:?}",
                            error
                        )));
                    return;
                },
            };

        // Identify the active subscribers that are no longer connected
        let connected_peers: HashSet<PeerNetworkId> =
            connected_peers_and_metadata.keys().cloned().collect();
        let disconnected_subscribers: HashSet<PeerNetworkId> = active_subscribers
            .difference(&connected_peers)
            .cloned()
            .collect();

        // Remove any subscriptions from peers that are no longer connected
        for peer_network_id in &disconnected_subscribers {
            self.remove_active_subscriber(peer_network_id);
            info!(LogSchema::new(LogEntry::ConsensusPublisher)
                .event(LogEvent::Subscription)
                .message(&format!(
                    "Removed peer subscription due to disconnection! Peer: {:?}",
                    peer_network_id
                )));
        }

        // Update the number of active subscribers for each network
        let active_subscribers = self.get_active_subscribers();
        for network_id in peers_and_metadata.get_registered_networks() {
            // Calculate the number of active subscribers for the network
            let num_active_subscribers = active_subscribers
                .iter()
                .filter(|peer_network_id| peer_network_id.network_id() == network_id)
                .count() as i64;

            // Update the active subscriber metric
            metrics::set_gauge(
                &metrics::PUBLISHER_NUM_ACTIVE_SUBSCRIBERS,
                &network_id,
                num_active_subscribers,
            );
        }
    }
```

**File:** consensus/src/consensus_observer/publisher/consensus_publisher.rs (L212-232)
```rust
    pub fn publish_message(&self, message: ConsensusObserverDirectSend) {
        // Get the active subscribers
        let active_subscribers = self.get_active_subscribers();

        // Send the message to all active subscribers
        for peer_network_id in &active_subscribers {
            // Send the message to the outbound receiver for publishing
            let mut outbound_message_sender = self.outbound_message_sender.clone();
            if let Err(error) =
                outbound_message_sender.try_send((*peer_network_id, message.clone()))
            {
                // The message send failed
                warn!(LogSchema::new(LogEntry::ConsensusPublisher)
                        .event(LogEvent::SendDirectSendMessage)
                        .message(&format!(
                            "Failed to send outbound message to the receiver for peer {:?}! Error: {:?}",
                            peer_network_id, error
                    )));
            }
        }
    }
```

**File:** config/src/config/consensus_observer_config.rs (L34-35)
```rust
    /// Interval (in milliseconds) to garbage collect peer state
    pub garbage_collection_interval_ms: u64,
```

**File:** config/src/config/consensus_observer_config.rs (L38-39)
```rust
    /// Interval (in milliseconds) to check progress of the consensus observer
    pub progress_check_interval_ms: u64,
```

**File:** config/src/config/consensus_observer_config.rs (L46-47)
```rust
    /// Maximum message timeout (in milliseconds) for active subscriptions
    pub max_subscription_timeout_ms: u64,
```

**File:** config/src/config/consensus_observer_config.rs (L71-71)
```rust
            garbage_collection_interval_ms: 60_000,            // 60 seconds
```
