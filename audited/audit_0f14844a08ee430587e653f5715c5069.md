# Audit Report

## Title
Non-Atomic File Writes in Indexer File Store Lead to Corruption and Service Disruption

## Summary
The indexer file store implementation uses non-atomic file write operations that can result in partial writes during system crashes, leading to corrupted files that cause persistent crash loops requiring manual intervention or full reindexing.

## Finding Description

The `FileEntry::from_transactions()` function prepares transaction data for storage by serializing and compressing it in memory. [1](#0-0) 

However, the actual file writing operations in both file store implementations use `tokio::fs::write()` without atomic write guarantees:

**V1 Local File Store:** [2](#0-1) 

**V2 Local File Store:** [3](#0-2) 

The `tokio::fs::write()` function writes data directly to the target file path without using the atomic write-to-temp-then-rename pattern. If a crash occurs during the write operation (due to OOM, hardware failure, forced shutdown, or resource exhaustion attacks), the file will be left in a partially written state.

**Corruption Impact Chain:**

1. During file write, a crash leaves a partially written transaction data file or batch metadata file
2. On recovery, the `FileStoreUploader::recover()` attempts to read batch metadata [4](#0-3) 
3. The `FileStoreReader::get_batch_metadata()` deserializes the corrupted file using `.expect()` which panics on failure [5](#0-4) 
4. Similarly, reading corrupted transaction files causes panics during LZ4 decompression or protobuf deserialization [6](#0-5) 
5. The indexer enters a crash loop, repeatedly panicking when attempting to read the corrupted file
6. Manual intervention is required to delete corrupted files or restore from backup

## Impact Explanation

This vulnerability qualifies as **Medium Severity** per Aptos bug bounty criteria:

- **State inconsistencies requiring intervention**: The file store becomes corrupted and cannot self-recover. Operators must manually identify and delete corrupted files, restore from backup, or run the backfiller tool to regenerate data.

- **API service disruption**: The indexer gRPC service enters a crash loop and cannot serve transaction data to clients until manually repaired.

- **Potential full reindexing requirement**: In severe corruption scenarios, operators may need to clear the entire file store and reindex from genesis or a checkpoint.

While this does not affect blockchain consensus or validator operations (the indexer is off-chain infrastructure), it disrupts critical data availability services that applications and users depend on.

## Likelihood Explanation

**Medium-High Likelihood:**

- Crashes during operation are realistic: OOM conditions, hardware failures, power loss, forced shutdowns during maintenance, or kernel panics
- The vulnerability window occurs during every file write operation, which happens frequently (every 50MB of transaction data in V2)
- An attacker could potentially trigger crashes through resource exhaustion attacks (e.g., memory pressure, disk space exhaustion) timed with file writes
- Production indexers process millions of transactions continuously, increasing cumulative exposure over time
- No automatic recovery mechanism existsâ€”the system will not self-heal

## Recommendation

Implement atomic file writes using the write-to-temp-then-rename pattern, following the existing pattern used elsewhere in the Aptos codebase.

**Reference Implementation:**
The codebase already demonstrates proper atomic writes in the secure storage module: [7](#0-6) 

**Recommended Fix for LocalFileStore:**

```rust
async fn save_raw_file(&self, file_path: PathBuf, data: Vec<u8>) -> Result<()> {
    let file_path = self.path.join(file_path);
    if let Some(parent) = file_path.parent() {
        tokio::fs::create_dir_all(parent).await?;
    }
    
    // Write to temporary file first
    let temp_path = file_path.with_extension("tmp");
    tokio::fs::write(&temp_path, data).await?;
    
    // Optionally fsync for durability
    let file = tokio::fs::File::open(&temp_path).await?;
    file.sync_all().await?;
    
    // Atomically rename to final destination
    tokio::fs::rename(&temp_path, &file_path)
        .await
        .map_err(anyhow::Error::msg)
}
```

Apply the same pattern to both V1 and V2 file store implementations, and to metadata file writes.

## Proof of Concept

```rust
#[tokio::test]
async fn test_partial_write_corruption() {
    use tempfile::TempDir;
    use std::path::PathBuf;
    
    let temp_dir = TempDir::new().unwrap();
    let file_path = temp_dir.path().join("test.bin");
    
    // Simulate partial write by writing incomplete data
    let full_data = vec![1u8; 1000];
    let partial_data = vec![1u8; 500]; // Only half written
    tokio::fs::write(&file_path, partial_data).await.unwrap();
    
    // Attempt to read as FileEntry
    let bytes = tokio::fs::read(&file_path).await.unwrap();
    
    // This will panic during decompression with "Lz4 decompression failed"
    let result = std::panic::catch_unwind(|| {
        FileEntry::new(bytes, StorageFormat::Lz4CompressedProto)
            .into_transactions_in_storage()
    });
    
    assert!(result.is_err(), "Should panic on corrupted file");
}
```

To reproduce in a live environment:
1. Start the indexer file store
2. Send SIGKILL during an active file write operation
3. Restart the indexer
4. Observe crash loop with "Batch metadata JSON is invalid" or "Lz4 decompression failed" panic

**Notes:**

This vulnerability represents a **data durability and availability issue** in critical indexing infrastructure. While the indexer operates off-chain and doesn't directly affect consensus, it provides essential data services. The lack of atomic writes violates basic file system safety principles and creates operational fragility. The codebase already demonstrates knowledge of proper atomic write patterns in other components, making this omission particularly concerning.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L191-237)
```rust
    pub fn from_transactions(
        transactions: Vec<Transaction>,
        storage_format: StorageFormat,
    ) -> Self {
        let mut bytes = Vec::new();
        let starting_version = transactions
            .first()
            .expect("Cannot build empty file")
            .version;
        match storage_format {
            StorageFormat::Lz4CompressedProto => {
                let t = TransactionsInStorage {
                    starting_version: Some(transactions.first().unwrap().version),
                    transactions,
                };
                t.encode(&mut bytes).expect("proto serialization failed.");
                let mut compressed = EncoderBuilder::new()
                    .level(0)
                    .build(Vec::new())
                    .expect("Lz4 compression failed.");
                compressed
                    .write_all(&bytes)
                    .expect("Lz4 compression failed.");
                FileEntry::Lz4CompressionProto(compressed.finish().0)
            },
            StorageFormat::Base64UncompressedProto => {
                panic!("Base64UncompressedProto is not supported.")
            },
            StorageFormat::JsonBase64UncompressedProto => {
                let transactions_in_base64 = transactions
                    .into_iter()
                    .map(|transaction| {
                        let mut bytes = Vec::new();
                        transaction
                            .encode(&mut bytes)
                            .expect("proto serialization failed.");
                        base64::encode(bytes)
                    })
                    .collect::<Vec<String>>();
                let file = TransactionsLegacyFile {
                    starting_version,
                    transactions_in_base64,
                };
                let json = serde_json::to_vec(&file).expect("json serialization failed.");
                FileEntry::JsonBase64UncompressedProto(json)
            },
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L262-292)
```rust
    pub fn into_transactions_in_storage(self) -> TransactionsInStorage {
        match self {
            FileEntry::Lz4CompressionProto(bytes) => {
                let mut decompressor = Decoder::new(&bytes[..]).expect("Lz4 decompression failed.");
                let mut decompressed = Vec::new();
                decompressor
                    .read_to_end(&mut decompressed)
                    .expect("Lz4 decompression failed.");
                TransactionsInStorage::decode(decompressed.as_slice())
                    .expect("proto deserialization failed.")
            },
            FileEntry::JsonBase64UncompressedProto(bytes) => {
                let file: TransactionsLegacyFile =
                    serde_json::from_slice(bytes.as_slice()).expect("json deserialization failed.");
                let transactions = file
                    .transactions_in_base64
                    .into_iter()
                    .map(|base64| {
                        let bytes: Vec<u8> =
                            base64::decode(base64).expect("base64 decoding failed.");
                        Transaction::decode(bytes.as_slice())
                            .expect("proto deserialization failed.")
                    })
                    .collect::<Vec<Transaction>>();
                TransactionsInStorage {
                    starting_version: Some(file.starting_version),
                    transactions,
                }
            },
        }
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator/local.rs (L185-185)
```rust
                match tokio::fs::write(txns_path, file_entry.into_inner()).await {
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/local.rs (L66-66)
```rust
        tokio::fs::write(file_path, data)
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L87-117)
```rust
    async fn recover(&self) -> Result<(u64, BatchMetadata)> {
        let _timer = TIMER.with_label_values(&["recover"]).start_timer();

        let mut version = self
            .reader
            .get_latest_version()
            .await
            .expect("Latest version must exist.");
        info!("Starting recovering process, current version in storage: {version}.");
        let mut num_folders_checked = 0;
        let mut buffered_batch_metadata_to_recover = BatchMetadata::default();
        while let Some(batch_metadata) = self.reader.get_batch_metadata(version).await {
            let batch_last_version = batch_metadata.files.last().unwrap().last_version;
            version = batch_last_version;
            if version % NUM_TXNS_PER_FOLDER != 0 {
                buffered_batch_metadata_to_recover = batch_metadata;
                break;
            }
            num_folders_checked += 1;
            if num_folders_checked >= MAX_NUM_FOLDERS_TO_CHECK_FOR_RECOVERY {
                panic!(
                    "File store metadata is way behind batch metadata, data might be corrupted."
                );
            }
        }

        self.update_file_store_metadata(version).await?;

        info!("Finished recovering process, recovered at version: {version}.");

        Ok((version, buffered_batch_metadata_to_recover))
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_reader.rs (L170-176)
```rust
    pub async fn get_batch_metadata(&self, version: u64) -> Option<BatchMetadata> {
        self.reader
            .get_raw_file(self.get_path_for_batch_metadata(version))
            .await
            .expect("Failed to get batch metadata.")
            .map(|data| serde_json::from_slice(&data).expect("Batch metadata JSON is invalid."))
    }
```

**File:** secure/storage/src/on_disk.rs (L64-70)
```rust
    fn write(&self, data: &HashMap<String, Value>) -> Result<(), Error> {
        let contents = serde_json::to_vec(data)?;
        let mut file = File::create(self.temp_path.path())?;
        file.write_all(&contents)?;
        fs::rename(&self.temp_path, &self.file_path)?;
        Ok(())
    }
```
