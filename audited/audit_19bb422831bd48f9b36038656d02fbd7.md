# Audit Report

## Title
Missing Quorum Certificate Signature Verification During Consensus Recovery

## Summary
The consensus recovery process in `persistent_liveness_storage.rs` and `block_store.rs` loads quorum certificates from the database without verifying their BLS signatures. While this creates a defense-in-depth weakness, forged QCs cannot propagate to honest validators due to signature verification at all network boundaries.

## Finding Description

The vulnerability exists in the consensus recovery flow:

**Step 1: No verification during save**
The `save_tree()` function accepts and persists QCs without signature verification: [1](#0-0) 

**Step 2: No verification during recovery**
The `start()` function loads QCs from the database and creates `RecoveryData` without verifying signatures: [2](#0-1) 

**Step 3: Unverified insertion into block tree**
During `BlockStore::build()`, recovered QCs are inserted via `insert_single_quorum_cert()` which only checks structural consistency, not signatures: [3](#0-2) [4](#0-3) 

**Critical Context: Network verification prevents propagation**

However, forged QCs cannot propagate to honest validators because:

1. **SyncInfo messages verify QCs**: When SyncInfo is received, signatures are verified before processing: [5](#0-4) [6](#0-5) 

2. **OrderVoteMsg verifies embedded QCs**: Order votes verify QC signatures on first receipt: [7](#0-6) 

3. **Proposals verify QCs**: Block proposals have their QCs verified via `Block::validate_signature()` which calls `QuorumCert::verify()`: [8](#0-7) 

## Impact Explanation

**Actual Impact: Medium Severity - State Inconsistency (Local Only)**

This issue does NOT meet **Critical Severity** criteria because:
- ❌ No loss of funds
- ❌ No network-wide consensus safety violation  
- ❌ No network partition or liveness failure
- ❌ Forged QCs are rejected at all network boundaries

The impact is limited to **local state corruption** of a single validator whose database contains forged QCs. This could occur through:
- Database compromise (malware, filesystem corruption)
- Software bugs causing invalid QCs to be saved
- Intentional manipulation by a Byzantine validator operator

The corrupted validator may make incorrect local decisions, but cannot spread the corruption to honest validators due to signature verification at network boundaries.

## Likelihood Explanation

**Low Likelihood** - Requires one of:
1. Database compromise with ability to inject forged QCs
2. Byzantine validator intentionally corrupting their own state
3. Software bug causing invalid QCs to persist

In all cases, the impact is self-contained to the compromised/Byzantine validator.

## Recommendation

Implement signature verification during recovery as defense-in-depth:

```rust
// In BlockStore::build(), after line 299:
for qc in quorum_certs {
    // Verify QC signatures before inserting during recovery
    qc.verify(&epoch_state.verifier)
        .context("QC signature verification failed during recovery")?;
    
    block_store
        .insert_single_quorum_cert(qc)
        .unwrap_or_else(|e| {
            panic!("[BlockStore] failed to insert quorum during build{:?}", e)
        });
}
```

Alternatively, add verification in `RecoveryData::new()`: [9](#0-8) 

## Proof of Concept

This would require a Rust test that:
1. Manually constructs a QC with invalid signatures
2. Directly saves it to ConsensusDB via `save_tree()`
3. Triggers recovery via `start()`
4. Verifies the forged QC is loaded without error
5. Demonstrates it cannot be sent to other validators (they reject it)

However, per the validation criteria, this issue does **not meet the Critical severity threshold** required for a full exploit PoC, as it only causes local state inconsistency without network-wide impact.

---

## Notes

**Final Assessment**: While technically valid as a defense-in-depth weakness, this finding does **not constitute a Critical consensus vulnerability** because:

1. Forged QCs cannot propagate beyond the compromised validator
2. Network-wide consensus safety is maintained under <1/3 Byzantine assumption
3. Honest validators are protected by signature verification at all network entry points
4. Impact is limited to self-inflicted corruption of an already-compromised validator

This represents a **Medium severity** state inconsistency issue, not a Critical consensus attack vector.

### Citations

**File:** consensus/src/persistent_liveness_storage.rs (L348-419)
```rust
    pub fn new(
        last_vote: Option<Vote>,
        ledger_recovery_data: LedgerRecoveryData,
        mut blocks: Vec<Block>,
        root_metadata: RootMetadata,
        mut quorum_certs: Vec<QuorumCert>,
        highest_2chain_timeout_cert: Option<TwoChainTimeoutCertificate>,
        order_vote_enabled: bool,
        window_size: Option<u64>,
    ) -> Result<Self> {
        let root = ledger_recovery_data
            .find_root(
                &mut blocks,
                &mut quorum_certs,
                order_vote_enabled,
                window_size,
            )
            .with_context(|| {
                // for better readability
                blocks.sort_by_key(|block| block.round());
                quorum_certs.sort_by_key(|qc| qc.certified_block().round());
                format!(
                    "\nRoot: {}\nBlocks in db: {}\nQuorum Certs in db: {}\n",
                    ledger_recovery_data.storage_ledger.ledger_info(),
                    blocks
                        .iter()
                        .map(|b| format!("\n{}", b))
                        .collect::<Vec<String>>()
                        .concat(),
                    quorum_certs
                        .iter()
                        .map(|qc| format!("\n{}", qc))
                        .collect::<Vec<String>>()
                        .concat(),
                )
            })?;

        // If execution pool is enabled, use the window_root, else use the commit_root
        let (root_id, epoch) = match &root.window_root_block {
            None => {
                let commit_root_id = root.commit_root_block.id();
                let epoch = root.commit_root_block.epoch();
                (commit_root_id, epoch)
            },
            Some(window_root_block) => {
                let window_start_id = window_root_block.id();
                let epoch = window_root_block.epoch();
                (window_start_id, epoch)
            },
        };
        let blocks_to_prune = Some(Self::find_blocks_to_prune(
            root_id,
            &mut blocks,
            &mut quorum_certs,
        ));

        Ok(RecoveryData {
            last_vote: match last_vote {
                Some(v) if v.epoch() == epoch => Some(v),
                _ => None,
            },
            root,
            root_metadata,
            blocks,
            quorum_certs,
            blocks_to_prune,
            highest_2chain_timeout_certificate: match highest_2chain_timeout_cert {
                Some(tc) if tc.epoch() == epoch => Some(tc),
                _ => None,
            },
        })
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L493-497)
```rust
    fn save_tree(&self, blocks: Vec<Block>, quorum_certs: Vec<QuorumCert>) -> Result<()> {
        Ok(self
            .db
            .save_blocks_and_quorum_certificates(blocks, quorum_certs)?)
    }
```

**File:** consensus/src/persistent_liveness_storage.rs (L519-596)
```rust
    fn start(&self, order_vote_enabled: bool, window_size: Option<u64>) -> LivenessStorageData {
        info!("Start consensus recovery.");
        let raw_data = self
            .db
            .get_data()
            .expect("unable to recover consensus data");

        let last_vote = raw_data
            .0
            .map(|bytes| bcs::from_bytes(&bytes[..]).expect("unable to deserialize last vote"));

        let highest_2chain_timeout_cert = raw_data.1.map(|b| {
            bcs::from_bytes(&b).expect("unable to deserialize highest 2-chain timeout cert")
        });
        let blocks = raw_data.2;
        let quorum_certs: Vec<_> = raw_data.3;
        let blocks_repr: Vec<String> = blocks.iter().map(|b| format!("\n\t{}", b)).collect();
        info!(
            "The following blocks were restored from ConsensusDB : {}",
            blocks_repr.concat()
        );
        let qc_repr: Vec<String> = quorum_certs
            .iter()
            .map(|qc| format!("\n\t{}", qc))
            .collect();
        info!(
            "The following quorum certs were restored from ConsensusDB: {}",
            qc_repr.concat()
        );
        // find the block corresponding to storage latest ledger info
        let latest_ledger_info = self
            .aptos_db
            .get_latest_ledger_info()
            .expect("Failed to get latest ledger info.");
        let accumulator_summary = self
            .aptos_db
            .get_accumulator_summary(latest_ledger_info.ledger_info().version())
            .expect("Failed to get accumulator summary.");
        let ledger_recovery_data = LedgerRecoveryData::new(latest_ledger_info);

        match RecoveryData::new(
            last_vote,
            ledger_recovery_data.clone(),
            blocks,
            accumulator_summary.into(),
            quorum_certs,
            highest_2chain_timeout_cert,
            order_vote_enabled,
            window_size,
        ) {
            Ok(mut initial_data) => {
                (self as &dyn PersistentLivenessStorage)
                    .prune_tree(initial_data.take_blocks_to_prune())
                    .expect("unable to prune dangling blocks during restart");
                if initial_data.last_vote.is_none() {
                    self.db
                        .delete_last_vote_msg()
                        .expect("unable to cleanup last vote");
                }
                if initial_data.highest_2chain_timeout_certificate.is_none() {
                    self.db
                        .delete_highest_2chain_timeout_certificate()
                        .expect("unable to cleanup highest 2-chain timeout cert");
                }
                info!(
                    "Starting up the consensus state machine with recovery data - [last_vote {}], [highest timeout certificate: {}]",
                    initial_data.last_vote.as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                    initial_data.highest_2chain_timeout_certificate().as_ref().map_or_else(|| "None".to_string(), |v| v.to_string()),
                );

                LivenessStorageData::FullRecoveryData(initial_data)
            },
            Err(e) => {
                error!(error = ?e, "Failed to construct recovery data");
                LivenessStorageData::PartialRecoveryData(ledger_recovery_data)
            },
        }
    }
```

**File:** consensus/src/block_storage/block_store.rs (L299-305)
```rust
        for qc in quorum_certs {
            block_store
                .insert_single_quorum_cert(qc)
                .unwrap_or_else(|e| {
                    panic!("[BlockStore] failed to insert quorum during build{:?}", e)
                });
        }
```

**File:** consensus/src/block_storage/block_store.rs (L519-556)
```rust
    pub fn insert_single_quorum_cert(&self, qc: QuorumCert) -> anyhow::Result<()> {
        // If the parent block is not the root block (i.e not None), ensure the executed state
        // of a block is consistent with its QuorumCert, otherwise persist the QuorumCert's
        // state and on restart, a new execution will agree with it.  A new execution will match
        // the QuorumCert's state on the next restart will work if there is a memory
        // corruption, for example.
        match self.get_block(qc.certified_block().id()) {
            Some(pipelined_block) => {
                ensure!(
                    // decoupled execution allows dummy block infos
                    pipelined_block
                        .block_info()
                        .match_ordered_only(qc.certified_block()),
                    "QC for block {} has different {:?} than local {:?}",
                    qc.certified_block().id(),
                    qc.certified_block(),
                    pipelined_block.block_info()
                );
                observe_block(
                    pipelined_block.block().timestamp_usecs(),
                    BlockStage::QC_ADDED,
                );
                if pipelined_block.block().is_opt_block() {
                    observe_block(
                        pipelined_block.block().timestamp_usecs(),
                        BlockStage::QC_ADDED_OPT_BLOCK,
                    );
                }
                pipelined_block.set_qc(Arc::new(qc.clone()));
            },
            None => bail!("Insert {} without having the block in store first", qc),
        };

        self.storage
            .save_tree(vec![], vec![qc.clone()])
            .context("Insert block failed when saving quorum")?;
        self.inner.write().insert_quorum_cert(qc)
    }
```

**File:** consensus/src/round_manager.rs (L878-896)
```rust
    async fn sync_up(&mut self, sync_info: &SyncInfo, author: Author) -> anyhow::Result<()> {
        let local_sync_info = self.block_store.sync_info();
        if sync_info.has_newer_certificates(&local_sync_info) {
            info!(
                self.new_log(LogEvent::ReceiveNewCertificate)
                    .remote_peer(author),
                "Local state {},\n remote state {}", local_sync_info, sync_info
            );
            // Some information in SyncInfo is ahead of what we have locally.
            // First verify the SyncInfo (didn't verify it in the yet).
            sync_info.verify(&self.epoch_state.verifier).map_err(|e| {
                error!(
                    SecurityEvent::InvalidSyncInfoMsg,
                    sync_info = sync_info,
                    remote_peer = author,
                    error = ?e,
                );
                VerifyError::from(e)
            })?;
```

**File:** consensus/src/round_manager.rs (L1577-1590)
```rust
                let vote_reception_result = if !self.pending_order_votes.exists(&li_digest) {
                    let start = Instant::now();
                    order_vote_msg
                        .quorum_cert()
                        .verify(&self.epoch_state.verifier)
                        .context("[OrderVoteMsg QuorumCert verification failed")?;
                    counters::VERIFY_MSG
                        .with_label_values(&["order_vote_qc"])
                        .observe(start.elapsed().as_secs_f64());
                    self.pending_order_votes.insert_order_vote(
                        order_vote_msg.order_vote(),
                        &self.epoch_state.verifier,
                        Some(order_vote_msg.quorum_cert().clone()),
                    )
```

**File:** consensus/consensus-types/src/sync_info.rs (L138-212)
```rust
    pub fn verify(&self, validator: &ValidatorVerifier) -> anyhow::Result<()> {
        let epoch = self.highest_quorum_cert.certified_block().epoch();
        ensure!(
            epoch == self.highest_ordered_cert().commit_info().epoch(),
            "Multi epoch in SyncInfo - HOC and HQC"
        );
        ensure!(
            epoch == self.highest_commit_cert().commit_info().epoch(),
            "Multi epoch in SyncInfo - HOC and HCC"
        );
        if let Some(tc) = &self.highest_2chain_timeout_cert {
            ensure!(epoch == tc.epoch(), "Multi epoch in SyncInfo - TC and HQC");
        }

        ensure!(
            self.highest_quorum_cert.certified_block().round()
                >= self.highest_ordered_cert().commit_info().round(),
            "HQC has lower round than HOC"
        );

        ensure!(
            self.highest_ordered_round() >= self.highest_commit_round(),
            format!(
                "HOC {} has lower round than HLI {}",
                self.highest_ordered_cert(),
                self.highest_commit_cert()
            )
        );

        ensure!(
            *self.highest_ordered_cert().commit_info() != BlockInfo::empty(),
            "HOC has no committed block"
        );

        ensure!(
            *self.highest_commit_cert().commit_info() != BlockInfo::empty(),
            "HLI has empty commit info"
        );

        // we don't have execution in unit tests, so this check would fail
        #[cfg(not(any(test, feature = "fuzzing")))]
        {
            ensure!(
                !self.highest_commit_cert().commit_info().is_ordered_only(),
                "HLI {} has ordered only commit info",
                self.highest_commit_cert().commit_info()
            );
        }

        self.highest_quorum_cert
            .verify(validator)
            .and_then(|_| {
                self.highest_ordered_cert
                    .as_ref()
                    .map_or(Ok(()), |cert| cert.verify(validator))
                    .context("Fail to verify ordered certificate")
            })
            .and_then(|_| {
                // we do not verify genesis ledger info
                if self.highest_commit_cert.commit_info().round() > 0 {
                    self.highest_commit_cert
                        .verify(validator)
                        .context("Fail to verify commit certificate")?
                }
                Ok(())
            })
            .and_then(|_| {
                if let Some(tc) = &self.highest_2chain_timeout_cert {
                    tc.verify(validator)?;
                }
                Ok(())
            })
            .context("Fail to verify SyncInfo")?;
        Ok(())
    }
```

**File:** consensus/consensus-types/src/quorum_cert.rs (L119-148)
```rust
    pub fn verify(&self, validator: &ValidatorVerifier) -> anyhow::Result<()> {
        let vote_hash = self.vote_data.hash();
        ensure!(
            self.ledger_info().ledger_info().consensus_data_hash() == vote_hash,
            "Quorum Cert's hash mismatch LedgerInfo"
        );
        // Genesis's QC is implicitly agreed upon, it doesn't have real signatures.
        // If someone sends us a QC on a fake genesis, it'll fail to insert into BlockStore
        // because of the round constraint.
        if self.certified_block().round() == 0 {
            ensure!(
                self.parent_block() == self.certified_block(),
                "Genesis QC has inconsistent parent block with certified block"
            );
            ensure!(
                self.certified_block() == self.ledger_info().ledger_info().commit_info(),
                "Genesis QC has inconsistent commit block with certified block"
            );
            ensure!(
                self.ledger_info().get_num_voters() == 0,
                "Genesis QC should not carry signatures"
            );
            return Ok(());
        }
        self.ledger_info()
            .verify_signatures(validator)
            .context("Fail to verify QuorumCert")?;
        self.vote_data.verify()?;
        Ok(())
    }
```
