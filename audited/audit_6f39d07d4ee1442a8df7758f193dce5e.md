# Audit Report

## Title
Node Configuration Divergence Risk: async_runtime_checks_enabled Breaks Deterministic Execution Invariant

## Summary
The `async_runtime_checks_enabled` parameter is sourced from node-level configuration rather than on-chain consensus state, allowing validators to execute transactions through fundamentally different code paths (inline type checks vs. deferred trace replay checks). This creates a potential for consensus divergence if the two checking mechanisms produce different results.

## Finding Description

The vulnerability exists in the execution environment initialization where `async_runtime_checks_enabled` is set from node configuration without being included in the environment hash or enforced across validators. [1](#0-0) 

This value comes from a global static variable set at node startup, not from on-chain state: [2](#0-1) 

The configuration is loaded from the execution config file with no default enforcement for mainnet: [3](#0-2) [4](#0-3) 

When validators have different settings, they execute blocks through different paths:

**Path 1 (async_runtime_checks_enabled = false):**
- Uses inline type checks during execution [5](#0-4) 

**Path 2 (async_runtime_checks_enabled = true for blocks > 3 txns):**
- Skips inline checks during execution (uses `NoRuntimeTypeCheck`)
- Defers checks to post-commit trace replay [6](#0-5) 

The critical issue is that the environment hash is finalized BEFORE `async_runtime_checks_enabled` is set, meaning environments with different values are considered equal: [7](#0-6) 

The config sanitizer for mainnet does NOT enforce consistency of this setting: [8](#0-7) 

## Impact Explanation

This breaks the **Deterministic Execution** invariant: "All validators must produce identical state roots for identical blocks." 

While the system has a fallback mechanism to sequential execution with inline checks, the vulnerability manifests if:
1. There exists a discrepancy in error detection between inline checking and trace replay checking
2. A transaction triggers this discrepancy
3. Validators have different `async_runtime_checks` configurations

In such cases:
- Validators with async checks disabled would reject the transaction during inline checking
- Validators with async checks enabled would accept the transaction (no inline check) and potentially miss the error if trace replay has a bug

This would result in **different state roots**, causing a **consensus failure** requiring a hard fork to resolve.

**Severity: Critical** - This constitutes a Consensus/Safety violation per the bug bounty criteria.

## Likelihood Explanation

**Likelihood: Medium-High**

The configuration defaults to `false` but includes a TODO comment suggesting it may be enabled by default in the future. Validators can freely configure this setting with no enforcement mechanism. The likelihood increases because:

1. The setting is performance-related, incentivizing validators to experiment with different values
2. No documentation warns that this affects consensus
3. No validation ensures consistency across the validator set
4. The fallback mechanism assumes perfect equivalence between checking approaches, which may not hold with complex VM edge cases

## Recommendation

**Immediate Fix:**
1. Add `async_runtime_checks_enabled` to the environment hash computation to detect configuration mismatches:

```rust
// In Environment::new(), before line 300:
sha3_256.update(&[if get_async_runtime_checks() { 1u8 } else { 0u8 }]);
```

2. Add validation to the config sanitizer to enforce consistent settings for mainnet:

```rust
// In execution_config.rs, add to sanitize():
if chain_id.is_mainnet() && execution_config.async_runtime_checks {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "async_runtime_checks must be disabled for mainnet for consensus safety".into(),
    ));
}
```

**Long-term Solution:**
Make runtime checking strategy an on-chain parameter controlled by governance, not node configuration.

## Proof of Concept

**Scenario Setup:**
1. Deploy two validator nodes with identical configurations except `execution.async_runtime_checks`: Node A sets it to `false`, Node B sets it to `true`
2. Submit a block with > 3 transactions including one that triggers a type check error that inline checks detect but trace replay misses (requires finding a specific VM edge case)

**Expected Result:**
- Node A rejects the transaction during inline execution
- Node B accepts the transaction (no inline check), trace replay passes
- Nodes produce different state roots
- Consensus fails

**Configuration to reproduce:**

```yaml
# Node A config
execution:
  async_runtime_checks: false
  
# Node B config  
execution:
  async_runtime_checks: true
```

The PoC requires identifying a specific transaction that exposes a discrepancy between the two checking mechanisms, which would require extensive fuzzing of the Move VM with both execution paths.

## Notes

The vulnerability is architectural: consensus-critical behavior should never depend on node-level configuration that isn't validated for consistency. Even if current implementations of inline and async checks are equivalent, future VM changes could introduce subtle differences, making this a ticking time bomb for consensus failures.

### Citations

**File:** aptos-move/aptos-vm-environment/src/environment.rs (L300-316)
```rust
        let hash = sha3_256.finalize().into();

        #[allow(deprecated)]
        Self {
            chain_id,
            features,
            timed_features,
            keyless_pvk,
            keyless_configuration,
            gas_feature_version,
            gas_params,
            storage_gas_params,
            runtime_environment,
            inject_create_signer_for_gov_sim,
            hash,
            verifier_bytes,
            async_runtime_checks_enabled: get_async_runtime_checks(),
```

**File:** aptos-move/aptos-vm-environment/src/prod_configs.rs (L60-63)
```rust
/// Returns the async check flag if already set, and false otherwise.
pub fn get_async_runtime_checks() -> bool {
    ASYNC_RUNTIME_CHECKS.get().cloned().unwrap_or(false)
}
```

**File:** config/src/config/execution_config.rs (L59-59)
```rust
    pub async_runtime_checks: bool,
```

**File:** config/src/config/execution_config.rs (L94-94)
```rust
            async_runtime_checks: false,
```

**File:** config/src/config/execution_config.rs (L166-183)
```rust
        // If this is a mainnet node, ensure that additional verifiers are enabled
        if let Some(chain_id) = chain_id {
            if chain_id.is_mainnet() {
                if !execution_config.paranoid_hot_potato_verification {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "paranoid_hot_potato_verification must be enabled for mainnet nodes!"
                            .into(),
                    ));
                }
                if !execution_config.paranoid_type_verification {
                    return Err(Error::ConfigSanitizerFailed(
                        sanitizer_name,
                        "paranoid_type_verification must be enabled for mainnet nodes!".into(),
                    ));
                }
            }
        }
```

**File:** third_party/move/move-vm/runtime/src/interpreter.rs (L244-247)
```rust
        let paranoid_type_checks =
            !trace_recorder.is_enabled() && interpreter.vm_config.paranoid_type_checks;
        let optimize_trusted_code =
            !trace_recorder.is_enabled() && interpreter.vm_config.optimize_trusted_code;
```

**File:** aptos-move/block-executor/src/executor.rs (L1234-1241)
```rust
        if environment.async_runtime_checks_enabled() && !trace.is_empty() {
            // Note that the trace may be empty (if block was small and executor decides not to
            // collect the trace and replay, or if the VM decides it is not profitable to do this
            // check for this particular transaction), so we check it in advance.
            let result = {
                counters::update_txn_trace_counters(&trace);
                let _timer = TRACE_REPLAY_SECONDS.start_timer();
                TypeChecker::new(&latest_view).replay(&trace)
```
