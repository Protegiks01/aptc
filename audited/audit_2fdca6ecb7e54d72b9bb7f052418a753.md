# Audit Report

## Title
Mixed-Epoch SyncInfo Generation During BlockStore Rebuild Causes Consensus Liveness Failure

## Summary
The `sync_info()` function can return a `SyncInfo` containing certificates from different epochs when `rebuild()` is called during epoch transitions. This occurs because the old epoch's timeout certificate is rolled over without epoch validation, violating the critical invariant that all certificates in a `SyncInfo` must be from the same epoch. This causes other nodes to reject proposals, votes, and timeouts from the affected node, breaking consensus liveness.

## Finding Description
The vulnerability exists in the `BlockStore::rebuild()` method which is called during state synchronization. When a node syncs to a new epoch, it rolls over the previous epoch's highest timeout certificate into the new epoch's `BlockTree` without any epoch consistency validation.

**Root Cause Location 1 - Timeout Certificate Rollover:** [1](#0-0) 

The code retrieves the old timeout certificate and passes it to the new `BlockTree` constructor without checking if it matches the new epoch.

**Root Cause Location 2 - No Epoch Validation in BlockTree Constructor:** [2](#0-1) 

The `BlockTree::new()` validates that the window root and ordered cert are from the same epoch (line 113), but does NOT validate the `highest_2chain_timeout_cert` epoch at line 145.

**Root Cause Location 3 - Unvalidated SyncInfo Construction:** [3](#0-2) 

The `sync_info()` method constructs a `SyncInfo` by combining certificates from the `BlockTree` without calling `verify()`, allowing mixed-epoch certificates to be packaged together.

**Attack Propagation:**

1. Node A operates in epoch N with a timeout certificate TC_N at round R
2. Epoch N ends with a quorum certificate that has `ends_epoch() == true`
3. Node A receives blocks from epoch N+1 via sync protocol and calls `add_certs()` which triggers `rebuild()`
4. The `rebuild()` creates a new `BlockTree` with:
   - Blocks, QCs, ordered cert, commit cert from epoch N+1
   - Timeout cert TC_N from epoch N (rolled over)
5. When Node A needs to propose, vote, or timeout, it calls `sync_info()` which returns a mixed-epoch `SyncInfo`
6. Node A broadcasts this in messages (ProposalMsg, VoteMsg, RoundTimeoutMsg, or standalone SyncInfo)

**Message Broadcast Points:** [4](#0-3) [5](#0-4) [6](#0-5) 

**Verification Failure at Recipients:** [7](#0-6) 

When other nodes receive the mixed-epoch `SyncInfo`, they verify it and the check fails at: [8](#0-7) 

This causes the message to be rejected with "Multi epoch in SyncInfo - TC and HQC" error, preventing the proposal/vote/timeout from being processed.

## Impact Explanation
This is a **High to Critical Severity** vulnerability that breaks consensus liveness:

1. **Consensus Liveness Violation**: If the affected node is a valid proposer, its proposals are rejected by all other validators, blocking consensus progress for that round. This violates the fundamental liveness guarantee of AptosBFT.

2. **Network-Wide Sync Failure**: Multiple nodes experiencing this during epoch transitions can create cascading sync failures, as nodes cannot synchronize state from affected peers.

3. **Validator Reputation Damage**: The affected validator's proposals and votes are systematically rejected, potentially triggering reputation penalties in the leader election system.

4. **No Recovery Without Manual Intervention**: The rolled-over timeout certificate persists until the node restarts or a new timeout certificate with a higher round from the current epoch is created, meaning the issue can persist across multiple rounds.

This meets **High Severity** criteria per the Aptos bug bounty program as it causes "Significant protocol violations" and can lead to validator node operational issues. In scenarios with many affected nodes, it approaches **Critical Severity** as it could cause "Total loss of liveness/network availability."

## Likelihood Explanation
**Likelihood: Medium to High**

This vulnerability occurs automatically during normal operations when:

1. **Epoch Transitions**: The network undergoes a planned epoch change (happens periodically)
2. **State Sync During Transition**: A node is catching up via state sync when the epoch changes
3. **Pre-existing Timeout Certificate**: The node had created or received a timeout certificate in the old epoch

**Triggering Conditions:**
- Validators restarting during epoch transitions
- Nodes experiencing network partitions during epoch changes
- Fast state sync operations that span epoch boundaries

**No Attacker Required**: This is a protocol-level bug that manifests during normal operations. No malicious actor needs to exploit it - it happens naturally when the timing of state sync aligns with epoch transitions.

**Frequency**: Given that epoch transitions happen regularly (typically daily or based on governance), and validators commonly restart or sync during operations, this issue likely affects validators multiple times per year in production networks.

## Recommendation
**Fix 1: Clear Timeout Certificate During Epoch Transitions**

In `block_store.rs`, modify the `rebuild()` function to NOT roll over the timeout certificate from the old tree:

```rust
pub async fn rebuild(
    &self,
    root: RootInfo,
    root_metadata: RootMetadata,
    blocks: Vec<Block>,
    quorum_certs: Vec<QuorumCert>,
) {
    // ... existing code ...
    
    // DO NOT rollover the previous highest TC if we're rebuilding across epochs
    let prev_2chain_htc = if let Some(tc) = self.highest_2chain_timeout_cert() {
        // Only keep the TC if it's from the same epoch as the new root
        if tc.epoch() == root.commit_cert.commit_info().epoch() {
            Some(tc.as_ref().clone())
        } else {
            None  // Discard TC from old epoch
        }
    } else {
        None
    };
    
    // ... rest of function ...
}
```

**Fix 2: Add Epoch Validation in BlockTree Constructor**

In `block_tree.rs`, add validation in the `new()` constructor:

```rust
pub(super) fn new(
    commit_root_id: HashValue,
    window_root: PipelinedBlock,
    root_quorum_cert: QuorumCert,
    root_ordered_cert: WrappedLedgerInfo,
    root_commit_cert: WrappedLedgerInfo,
    max_pruned_blocks_in_mem: usize,
    highest_2chain_timeout_cert: Option<Arc<TwoChainTimeoutCertificate>>,
) -> Self {
    assert_eq!(window_root.epoch(), root_ordered_cert.commit_info().epoch());
    assert!(window_root.round() <= root_ordered_cert.commit_info().round());
    
    // NEW: Validate timeout certificate epoch matches
    if let Some(tc) = &highest_2chain_timeout_cert {
        assert_eq!(
            tc.epoch(),
            root_ordered_cert.commit_info().epoch(),
            "Timeout certificate epoch {} doesn't match root epoch {}",
            tc.epoch(),
            root_ordered_cert.commit_info().epoch()
        );
    }
    
    // ... rest of function ...
}
```

**Fix 3: Add Defensive Validation in sync_info()**

In `block_store.rs`, add epoch consistency validation before returning `SyncInfo`:

```rust
fn sync_info(&self) -> SyncInfo {
    let sync_info = SyncInfo::new_decoupled(
        self.highest_quorum_cert().as_ref().clone(),
        self.highest_ordered_cert().as_ref().clone(),
        self.highest_commit_cert().as_ref().clone(),
        self.highest_2chain_timeout_cert()
            .map(|tc| tc.as_ref().clone()),
    );
    
    // Defensive check: ensure epoch consistency
    // This should never fail if fixes 1&2 are applied, but provides defense in depth
    #[cfg(debug_assertions)]
    {
        if let Err(e) = sync_info.verify(&self.inner.read().ordered_root().epoch_state().verifier) {
            error!("CRITICAL: Generated invalid SyncInfo: {}", e);
            panic!("Generated mixed-epoch SyncInfo");
        }
    }
    
    sync_info
}
```

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_consensus_types::{
        timeout_2chain::TwoChainTimeoutCertificate,
        block::Block,
    };
    
    #[tokio::test]
    async fn test_mixed_epoch_syncinfo_during_rebuild() {
        // Setup: Create a BlockStore in epoch N with a timeout certificate
        let epoch_n = 5;
        let epoch_n_plus_1 = 6;
        
        // Create initial BlockStore with timeout cert from epoch N
        let tc_epoch_n = create_timeout_cert(epoch_n, 100);
        let storage = Arc::new(MockStorage::new());
        let initial_data = create_recovery_data(epoch_n);
        let block_store = BlockStore::new(
            storage.clone(),
            initial_data,
            // ... other params ...
        );
        
        // Insert the timeout certificate from epoch N
        block_store.insert_2chain_timeout_certificate(Arc::new(tc_epoch_n)).unwrap();
        
        // Simulate epoch transition: rebuild with blocks from epoch N+1
        let root_n_plus_1 = create_root_info(epoch_n_plus_1);
        let blocks_n_plus_1 = vec![create_block(epoch_n_plus_1, 1)];
        let qcs_n_plus_1 = vec![create_qc(epoch_n_plus_1, 1)];
        
        // This is the vulnerable operation - it rolls over the old TC
        block_store.rebuild(
            root_n_plus_1,
            RootMetadata::default(),
            blocks_n_plus_1,
            qcs_n_plus_1,
        ).await;
        
        // Get sync_info - this will contain mixed epochs!
        let sync_info = block_store.sync_info();
        
        // Verify the bug: highest_quorum_cert is from epoch N+1
        assert_eq!(sync_info.highest_quorum_cert().certified_block().epoch(), epoch_n_plus_1);
        
        // But highest_2chain_timeout_cert is from epoch N
        assert_eq!(sync_info.highest_2chain_timeout_cert().unwrap().epoch(), epoch_n);
        
        // Try to verify - this should FAIL
        let verifier = create_verifier(epoch_n_plus_1);
        let result = sync_info.verify(&verifier);
        
        // Assertion: verification fails with epoch mismatch
        assert!(result.is_err());
        assert!(result.unwrap_err().to_string().contains("Multi epoch in SyncInfo"));
        
        println!("âœ“ VULNERABILITY CONFIRMED: SyncInfo contains certificates from epochs {} and {}", 
                 epoch_n_plus_1, epoch_n);
    }
}
```

## Notes
This vulnerability demonstrates a critical gap in epoch transition handling where the timeout certificate's epoch is not validated when rebuilding the block tree. The issue is particularly concerning because:

1. It violates the explicit epoch consistency checks in `SyncInfo::verify()` 
2. It affects normal operations, not just adversarial scenarios
3. The rolled-over timeout certificate can persist and cause repeated failures
4. It breaks the sync protocol's ability to propagate state during critical epoch transitions

The fix requires ensuring epoch consistency is maintained throughout the rebuild process, with defensive validation at multiple layers to prevent similar issues in the future.

### Citations

**File:** consensus/src/block_storage/block_store.rs (L370-379)
```rust
        // Rollover the previous highest TC from the old tree to the new one.
        let prev_2chain_htc = self
            .highest_2chain_timeout_cert()
            .map(|tc| tc.as_ref().clone());
        let _ = Self::build(
            root,
            root_metadata,
            blocks,
            quorum_certs,
            prev_2chain_htc,
```

**File:** consensus/src/block_storage/block_store.rs (L680-688)
```rust
    fn sync_info(&self) -> SyncInfo {
        SyncInfo::new_decoupled(
            self.highest_quorum_cert().as_ref().clone(),
            self.highest_ordered_cert().as_ref().clone(),
            self.highest_commit_cert().as_ref().clone(),
            self.highest_2chain_timeout_cert()
                .map(|tc| tc.as_ref().clone()),
        )
    }
```

**File:** consensus/src/block_storage/block_tree.rs (L104-148)
```rust
    pub(super) fn new(
        commit_root_id: HashValue,
        window_root: PipelinedBlock,
        root_quorum_cert: QuorumCert,
        root_ordered_cert: WrappedLedgerInfo,
        root_commit_cert: WrappedLedgerInfo,
        max_pruned_blocks_in_mem: usize,
        highest_2chain_timeout_cert: Option<Arc<TwoChainTimeoutCertificate>>,
    ) -> Self {
        assert_eq!(window_root.epoch(), root_ordered_cert.commit_info().epoch());
        assert!(window_root.round() <= root_ordered_cert.commit_info().round());
        let window_root_id = window_root.id();

        // Build the tree from the window root block which is <= the commit root block.
        let mut id_to_block = HashMap::new();
        let mut round_to_ids = BTreeMap::new();
        round_to_ids.insert(window_root.round(), window_root_id);
        id_to_block.insert(window_root_id, LinkableBlock::new(window_root));
        counters::NUM_BLOCKS_IN_TREE.set(1);

        let root_quorum_cert = Arc::new(root_quorum_cert);
        let mut id_to_quorum_cert = HashMap::new();
        id_to_quorum_cert.insert(
            root_quorum_cert.certified_block().id(),
            Arc::clone(&root_quorum_cert),
        );

        let pruned_block_ids = VecDeque::with_capacity(max_pruned_blocks_in_mem);

        BlockTree {
            id_to_block,
            ordered_root_id: commit_root_id,
            commit_root_id, // initially we set commit_root_id = root_id
            window_root_id,
            highest_certified_block_id: commit_root_id,
            highest_quorum_cert: Arc::clone(&root_quorum_cert),
            highest_ordered_cert: Arc::new(root_ordered_cert),
            highest_commit_cert: Arc::new(root_commit_cert),
            id_to_quorum_cert,
            pruned_block_ids,
            max_pruned_blocks_in_mem,
            highest_2chain_timeout_cert,
            round_to_ids,
        }
    }
```

**File:** consensus/src/round_manager.rs (L491-491)
```rust
            let sync_info = self.block_store.sync_info();
```

**File:** consensus/src/round_manager.rs (L878-896)
```rust
    async fn sync_up(&mut self, sync_info: &SyncInfo, author: Author) -> anyhow::Result<()> {
        let local_sync_info = self.block_store.sync_info();
        if sync_info.has_newer_certificates(&local_sync_info) {
            info!(
                self.new_log(LogEvent::ReceiveNewCertificate)
                    .remote_peer(author),
                "Local state {},\n remote state {}", local_sync_info, sync_info
            );
            // Some information in SyncInfo is ahead of what we have locally.
            // First verify the SyncInfo (didn't verify it in the yet).
            sync_info.verify(&self.epoch_state.verifier).map_err(|e| {
                error!(
                    SecurityEvent::InvalidSyncInfoMsg,
                    sync_info = sync_info,
                    remote_peer = author,
                    error = ?e,
                );
                VerifyError::from(e)
            })?;
```

**File:** consensus/src/round_manager.rs (L1000-1000)
```rust
                .broadcast_sync_info(self.block_store.sync_info())
```

**File:** consensus/src/round_manager.rs (L1034-1034)
```rust
            let round_timeout_msg = RoundTimeoutMsg::new(timeout, self.block_store.sync_info());
```

**File:** consensus/consensus-types/src/sync_info.rs (L148-150)
```rust
        if let Some(tc) = &self.highest_2chain_timeout_cert {
            ensure!(epoch == tc.epoch(), "Multi epoch in SyncInfo - TC and HQC");
        }
```
