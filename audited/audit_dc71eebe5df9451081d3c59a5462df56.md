# Audit Report

## Title
Disabled Epoch Snapshot Pruner Causes Fast Sync Failure for New Nodes, Harming Network Decentralization

## Summary
When `epoch_snapshot_pruner_config` is disabled while the network relies on epoch snapshots for fast sync, new nodes joining the network will fail to synchronize beyond the `state_merkle_pruner` window (~1 million versions by default), effectively preventing network growth and harming decentralization.

## Finding Description

Aptos maintains two separate state pruners with different purposes:

1. **`state_merkle_pruner`**: Prunes intra-epoch state updates with a default window of 1,000,000 versions
2. **`epoch_snapshot_pruner`**: Preserves complete state trees at epoch boundaries for fast sync with a default window of 80,000,000 versions [1](#0-0) 

When new nodes join the network using fast sync mode (`BootstrappingMode::DownloadLatestStates`), they request state values at epoch-ending versions to bootstrap quickly without replaying all transactions: [2](#0-1) 

The vulnerability occurs when `epoch_snapshot_pruner_config.enable` is set to `false`. In this configuration: [3](#0-2) 

The pruner's `prune_window` becomes 0, causing its `min_readable_version` to equal the latest version. When a new node requests state at an epoch-ending version older than the `state_merkle_pruner` window, the storage service performs a two-tier availability check: [4](#0-3) 

With the epoch snapshot pruner disabled:
- Line 283-284: Check fails because the requested version < state_merkle min_readable (only ~1M versions available)
- Line 292-294: Fallback check fails because epoch_snapshot min_readable = latest_version (prune_window=0)
- Line 295-301: Error is returned, sync fails

**Attack Scenario:**
1. Network operators disable `epoch_snapshot_pruner` (perhaps to save disk space)
2. Network continues running normally for existing nodes
3. After 1M+ versions pass, state beyond the `state_merkle_pruner` window is pruned
4. New node attempts fast sync, requests state at epoch ending
5. Storage service returns error: "epoch snapshots are available at >= {latest_version}"
6. New node cannot complete synchronization and cannot join network [5](#0-4) 

## Impact Explanation

This vulnerability causes **availability degradation** affecting network health:

- **New Node Onboarding Failure**: Nodes cannot fast sync to join the network, as state at epoch boundaries becomes unavailable beyond 1M versions
- **Network Decentralization Impact**: Only nodes that synced before the pruning window can participate, creating a permissioned-like network
- **Degraded Data Availability**: Violates the documented guarantee that epoch snapshots are "important for the health of the chain"

The configuration sanitizer already warns about this risk: [6](#0-5) 

This qualifies as **Medium severity** per the Aptos bug bounty criteria:
- State inconsistencies requiring intervention (new nodes cannot sync)
- Not Critical because existing nodes continue functioning
- Not High because no validator slowdown or API crashes
- Matches Medium: "State inconsistencies requiring intervention"

## Likelihood Explanation

**Likelihood: Medium**

Factors increasing likelihood:
- Operators may disable pruners to save disk space without understanding consequences
- Default configuration has `enable: true`, but operators can override
- No runtime enforcement prevents disabling the pruner
- Impact is delayed (only affects new nodes after pruning occurs)

Factors decreasing likelihood:
- Default configuration is safe
- ConfigSanitizer provides warnings
- Documentation explains importance

The vulnerability is realistic because:
1. Configuration changes are common operational tasks
2. Disk space pressure could motivate disabling pruners
3. Impact is not immediately visible (existing nodes work fine)
4. Warning is just a log message, not a hard failure

## Recommendation

**Immediate Fix:**
Make `epoch_snapshot_pruner` mandatory for nodes serving state sync data. Add validation in `ConfigSanitizer` to prevent disabling:

```rust
// In storage_config.rs ConfigSanitizer implementation
if !config.storage_pruner_config.epoch_snapshot_pruner_config.enable {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "epoch_snapshot_pruner_config cannot be disabled as it is required for \
         fast sync. New nodes will be unable to join the network if epoch \
         snapshots are pruned.".to_string(),
    ));
}
```

**Alternative Fix:**
If allowing disable is necessary, enforce a minimum prune_window:

```rust
let epoch_snapshot_prune_window = config
    .storage_pruner_config
    .epoch_snapshot_pruner_config
    .prune_window;
    
if config.storage_pruner_config.epoch_snapshot_pruner_config.enable 
    && epoch_snapshot_prune_window < 50_000_000 {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "epoch_snapshot_prune_window must be >= 50M to ensure new nodes can fast sync".to_string(),
    ));
}
```

**Long-term Solution:**
- Document the dependency between fast sync and epoch snapshots more prominently
- Add metrics to track availability of epoch snapshots
- Implement node capability advertising (nodes without epoch snapshots don't advertise state sync capability)

## Proof of Concept

```rust
// Rust integration test demonstrating the vulnerability
#[tokio::test]
async fn test_fast_sync_fails_with_disabled_epoch_snapshot_pruner() {
    // Setup: Create network with epoch_snapshot_pruner disabled
    let mut storage_config = StorageConfig::default();
    storage_config.storage_pruner_config.epoch_snapshot_pruner_config = 
        EpochSnapshotPrunerConfig {
            enable: false,  // Vulnerability: pruner disabled
            prune_window: 0,
            batch_size: 0,
        };
    
    // Simulate existing node with data
    let node1 = setup_validator_with_config(storage_config.clone()).await;
    
    // Generate > 1M versions of transactions across multiple epochs
    for _ in 0..10 {
        commit_epoch_with_transactions(&node1, 150_000).await;
    }
    
    // Trigger pruning
    wait_for_pruning_to_complete(&node1).await;
    
    // Attempt: New node tries to fast sync
    let mut fast_sync_config = NodeConfig::default();
    fast_sync_config.state_sync.state_sync_driver.bootstrapping_mode = 
        BootstrappingMode::DownloadLatestStates;
    
    let result = setup_fullnode_with_config(fast_sync_config, node1.peer_id()).await;
    
    // Expected: Fast sync fails because epoch snapshots are unavailable
    assert!(result.is_err());
    assert!(result.unwrap_err().to_string().contains("epoch snapshots are available at"));
}
```

## Notes

The vulnerability stems from the architectural dependency between two independent subsystems:
- **State Sync** expects epoch-ending snapshots to be available for fast bootstrapping
- **Storage Pruning** can independently remove these snapshots if misconfigured

The default configuration is safe, but the system allows unsafe configurations without hard enforcement. The warning in `ConfigSanitizer` is insufficient protection because operators may not notice or understand its implications.

This is a **configuration vulnerability** rather than a code bug, but it has concrete security impact on network availability and decentralization. The fix should enforce the dependency relationship between these subsystems at the configuration validation level.

### Citations

**File:** storage/README.md (L111-119)
```markdown
    # This configures the inter-epoch state tree pruner. If a state tree node is
    # overwritten by a later transaction that's in a later epoch, it's gonna be
    # pruned later by this pruner according to these configs. The prune window
    # looks large (the unit is number of transactions) but at each position in
    # the tree only the last node in its epoch among all updates to the same
    # position is kept (or pruned) by this pruner. Effectively these configs
    # guarantees complete state trees (or "epoch snapshots") at the end of
    # each recent epochs are available for peers to access, which is important
    # for the health of the chain.
```

**File:** config/src/config/storage_config.rs (L318-322)
```rust
    epoch_snapshot_pruner_config: EpochSnapshotPrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
    },
```

**File:** config/src/config/storage_config.rs (L419-420)
```rust
            // This is based on ~5K TPS * 2h/epoch * 2 epochs. -- epoch ending snapshots are used
            // by state sync in fast sync mode.
```

**File:** config/src/config/storage_config.rs (L714-716)
```rust
        if epoch_snapshot_prune_window < 50_000_000 {
            warn!("Epoch snapshot prune_window is too small, harming network data availability.");
        }
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L273-303)
```rust
    pub(super) fn error_if_state_merkle_pruned(
        &self,
        data_type: &str,
        version: Version,
    ) -> Result<()> {
        let min_readable_version = self
            .state_store
            .state_db
            .state_merkle_pruner
            .get_min_readable_version();
        if version >= min_readable_version {
            return Ok(());
        }

        let min_readable_epoch_snapshot_version = self
            .state_store
            .state_db
            .epoch_snapshot_pruner
            .get_min_readable_version();
        if version >= min_readable_epoch_snapshot_version {
            self.ledger_db.metadata_db().ensure_epoch_ending(version)
        } else {
            bail!(
                "{} at version {} is pruned. snapshots are available at >= {}, epoch snapshots are available at >= {}",
                data_type,
                version,
                min_readable_version,
                min_readable_epoch_snapshot_version,
            )
        }
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L880-890)
```rust
    fn get_state_value_chunk_with_proof(
        &self,
        version: Version,
        first_index: usize,
        chunk_size: usize,
    ) -> Result<StateValueChunkWithProof> {
        gauged_api("get_state_value_chunk_with_proof", || {
            self.error_if_state_merkle_pruned("State merkle", version)?;
            self.state_store
                .get_value_chunk_with_proof(version, first_index, chunk_size)
        })
```
