# Audit Report

## Title
Timestamp Unit Inconsistency in BigQueryRow Causes Time-Series Data Corruption in Telemetry Analytics

## Summary
The `event_timestamp` field in the `BigQueryRow` struct is populated with inconsistent units across different ingestion endpoints. The standard custom event endpoint converts timestamps from microseconds to seconds, while the custom contract event endpoint stores timestamps directly in microseconds. This unit inconsistency causes severe time-series ordering corruption in BigQuery analytics, making events from custom contract endpoints appear to be ~53,000 years in the future.

## Finding Description

The telemetry service defines a `BigQueryRow` struct that stores event data including an `event_timestamp` field: [1](#0-0) 

This field is populated through two different code paths with **inconsistent units**:

**Path 1: Standard Custom Event Endpoint** - Converts to seconds: [2](#0-1) 

Here, the timestamp is parsed from microseconds, converted to a `Duration`, and then stored as **seconds** using `.as_secs()`.

**Path 2: Custom Contract Event Endpoint** - Keeps microseconds: [3](#0-2) 

Here, the timestamp is parsed directly as a `u64` and stored without conversion, keeping it in **microseconds**.

Both paths write to the same BigQuery table and are queried together in analytics dashboards: [4](#0-3) 

The query uses `$__timeFilter(event_timestamp)` which expects consistent timestamp units. When events from both paths are queried:
- Events from custom_event: timestamp ~1,700,000,000 (seconds, year 2023)
- Events from custom_contract: timestamp ~1,700,000,000,000,000 (microseconds interpreted as seconds, year ~55,000)

This causes complete breakdown of time-series ordering and filtering in analytics.

## Impact Explanation

This issue fits the **Low Severity** category ($1,000) under "Non-critical implementation bugs":

1. **Data Integrity**: Telemetry analytics data is corrupted, making it impossible to correlate events from different sources
2. **Monitoring Compromise**: Time-based queries, dashboards, and alerting systems that rely on `event_timestamp` will produce incorrect results
3. **Indirect Security Impact**: If monitoring systems fail to detect anomalies due to corrupted time-series data, actual security issues could go unnoticed
4. **No Direct Protocol Impact**: This does not affect consensus, state management, transaction execution, or any on-chain operations

The issue does not meet Critical, High, or Medium severity criteria as it does not involve fund loss, consensus violations, or direct protocol failures.

## Likelihood Explanation

**Likelihood: High** - This bug is actively occurring in production:

1. Both endpoints are functional and accept telemetry data
2. The custom contract ingestion feature is documented and in use
3. Every event ingested through the custom contract endpoint has a corrupted timestamp
4. The inconsistency is deterministic and happens on every request
5. Analytics queries are already affected whenever they include data from both sources

The bug requires no attacker actionâ€”it's an implementation error that occurs during normal operation.

## Recommendation

Standardize the timestamp unit to **seconds** across all ingestion paths. Modify the custom contract ingestion handler to convert microseconds to seconds:

```rust
// In custom_contract_ingest.rs, line 371-380
let event_timestamp: u64 = body.timestamp_micros.parse()
    .map_err(|_| {
        record_custom_contract_error(
            &contract_name,
            CustomContractEndpoint::EventsIngest,
            CustomContractErrorType::InvalidPayload,
        );
        reject::custom(ServiceError::bad_request(
            CustomEventIngestError::InvalidTimestamp(body.timestamp_micros.clone()).into(),
        ))
    })?;

// Convert microseconds to seconds for consistency
let event_timestamp_seconds = event_timestamp / 1_000_000;

// Then use event_timestamp_seconds in BigQueryRow (line 435)
let row = BigQueryRow {
    event_identity: event_identity.clone(),
    event_name: event.name,
    event_timestamp: event_timestamp_seconds,
    event_params,
};
```

Additionally, add documentation to the `BigQueryRow` struct clarifying the expected unit:

```rust
pub(crate) struct BigQueryRow {
    #[serde(flatten)]
    pub event_identity: EventIdentity,
    pub event_name: String,
    /// Unix timestamp in seconds since epoch (not microseconds)
    pub event_timestamp: u64,
    pub event_params: Vec<serde_json::Value>,
}
```

## Proof of Concept

**Reproduction Steps:**

1. Send telemetry event via custom_event endpoint with timestamp_micros = "1700000000000000" (Nov 2023 in microseconds)
2. Send telemetry event via custom_contract endpoint with same timestamp_micros = "1700000000000000"
3. Query BigQuery: `SELECT event_timestamp, event_name FROM custom_events ORDER BY event_timestamp`

**Expected Result:** Both events should have similar timestamps around 1,700,000,000 seconds

**Actual Result:**
- Event from custom_event: `event_timestamp = 1700000000` (correctly converted to seconds)
- Event from custom_contract: `event_timestamp = 1700000000000000` (incorrectly kept in microseconds)

The second event appears to be from the year 55,895 CE when interpreted as seconds, breaking all time-series analytics.

**Verification Query:**
```sql
SELECT 
  role_type,
  MIN(event_timestamp) as min_ts,
  MAX(event_timestamp) as max_ts,
  MAX(event_timestamp) - MIN(event_timestamp) as range_seconds
FROM custom_events 
GROUP BY role_type
```

Events from Custom role_type will show timestamps ~1,000,000x larger than other role types, confirming the unit mismatch.

---

**Notes:**

While the security question asked about "timezone confusion," the actual issue discovered is a **timestamp unit inconsistency** (seconds vs. microseconds), not a timezone problem. Unix timestamps are inherently UTC-based and do not carry timezone information. However, the end result is the same: incorrect time-series ordering in analytics that could compromise monitoring and alerting systems, with indirect security implications.

### Citations

**File:** crates/aptos-telemetry-service/src/types/telemetry.rs (L24-31)
```rust
#[derive(Debug, Serialize, Clone)]
pub(crate) struct BigQueryRow {
    #[serde(flatten)]
    pub event_identity: EventIdentity,
    pub event_name: String,
    pub event_timestamp: u64,
    pub event_params: Vec<serde_json::Value>,
}
```

**File:** crates/aptos-telemetry-service/src/custom_event.rs (L98-110)
```rust
    let duration =
        Duration::from_micros(body.timestamp_micros.as_str().parse::<u64>().map_err(|_| {
            ServiceError::bad_request(
                CustomEventIngestError::InvalidTimestamp(body.timestamp_micros).into(),
            )
        })?);

    let row = BigQueryRow {
        event_identity: EventIdentity::from(claims),
        event_name: telemetry_event.name.clone(),
        event_timestamp: duration.as_secs(),
        event_params,
    };
```

**File:** crates/aptos-telemetry-service/src/custom_contract_ingest.rs (L370-438)
```rust
    // Parse timestamp
    let event_timestamp: u64 = body.timestamp_micros.parse().map_err(|_| {
        record_custom_contract_error(
            &contract_name,
            CustomContractEndpoint::EventsIngest,
            CustomContractErrorType::InvalidPayload,
        );
        reject::custom(ServiceError::bad_request(
            CustomEventIngestError::InvalidTimestamp(body.timestamp_micros.clone()).into(),
        ))
    })?;

    // Get the custom contract instance
    let instance = context.get_custom_contract(&contract_name).ok_or_else(|| {
        error!("custom contract '{}' not configured", contract_name);
        record_custom_contract_error(
            &contract_name,
            CustomContractEndpoint::EventsIngest,
            CustomContractErrorType::ContractNotConfigured,
        );
        reject::custom(ServiceError::internal(
            CustomEventIngestError::EmptyPayload.into(),
        ))
    })?;

    // Get the BigQuery client for this custom contract
    if let Some(bq_client) = &instance.bigquery_client {
        use crate::types::telemetry::BigQueryRow;

        // Get the node type from the contract instance config
        let node_type = NodeType::Custom(instance.config.node_type_name.clone());

        // Create event identity for custom contract client using chain_id from JWT claims
        let event_identity = EventIdentity {
            peer_id,
            chain_id,
            role_type: node_type,
            epoch: 0,
            uuid: Uuid::new_v4(),
        };

        // Convert events to BigQuery rows and build insert request
        let mut insert_request = TableDataInsertAllRequest::new();

        for event in body.events {
            // Add contract_name to event params
            let mut event_params: Vec<serde_json::Value> = event
                .params
                .into_iter()
                .map(|(key, value)| {
                    serde_json::json!({
                        "key": key,
                        "value": {"string_value": value}
                    })
                })
                .collect();
            // Append contract_name as an additional parameter
            event_params.push(serde_json::json!({
                "key": "contract_name",
                "value": {"string_value": contract_name.clone()}
            }));

            let row = BigQueryRow {
                event_identity: event_identity.clone(),
                event_name: event.name,
                event_timestamp,
                event_params,
            };

```

**File:** dashboards/overview.json (L760-776)
```json
      "targets": [
        {
          "datasource": { "type": "grafana-bigquery-datasource", "uid": "${BigQuery}" },
          "editorMode": "code",
          "format": 1,
          "location": "US",
          "project": "analytics-test-345723",
          "rawQuery": true,
          "rawSql": "SELECT count(DISTINCT peer_id) as unique_connected_pfns \nFROM `analytics-test-345723.aptos_node_telemetry.custom_events` \nWHERE $__timeFilter(event_timestamp) and (role_type in (\"Unknown\", \"PublicFullNode\",\"UnknownFullNode\"))\nand if(chain_id='1','mainnet',if(chain_id='2','testnet',null)) = \"$chain_name\";",
          "refId": "A",
          "sql": {
            "columns": [{ "parameters": [], "type": "function" }],
            "groupBy": [{ "property": { "type": "string" }, "type": "groupBy" }],
            "limit": 50
          }
        }
      ],
```
