# Audit Report

## Title
Chunk Commit Queue Stuck State Leading to Complete Node Liveness Loss

## Summary
A critical flaw in the chunk commit queue management causes permanent liveness loss when database operations fail during commit. The queue becomes irrecoverably stuck, blocking all future transaction commits until node restart.

## Finding Description

The `commit_chunk_impl()` function in the chunk executor follows a three-step process that violates atomicity guarantees: [1](#0-0) 

**Step 1:** `next_chunk_to_commit()` removes the chunk from the queue by calling `.take()`, which consumes the value and replaces it with `None`: [2](#0-1) 

**Step 2:** `save_transactions()` writes the chunk to disk. This operation can fail with errors from:
- `pre_commit_ledger()` validation failures
- `commit_ledger()` metadata write failures  
- Database I/O errors
- Or panic via `.unwrap()` calls in parallel database writes [3](#0-2) [4](#0-3) 

**Step 3:** `dequeue_committed()` should remove the processed entry, but only executes if Step 2 succeeds: [5](#0-4) 

**The Vulnerability:**

If `save_transactions()` fails (or anything between Steps 1-3 fails):
1. The error propagates via the `?` operator
2. `dequeue_committed()` is never called
3. The queue front remains as `None` (chunk consumed but not dequeued)
4. Subsequent calls to `next_chunk_to_commit()` fail with "Next chunk to commit has already been processed"
5. All future commits are permanently blocked

The developers acknowledge this is unrecoverable: [6](#0-5) 

**Alternative Scenario (Answering the Specific Question):**

If `save_transactions()` succeeds (chunk written to disk) but `dequeue_committed()` subsequently fails its `ensure()` checks due to queue corruption or race conditions, the result is identical - the queue becomes stuck with a partially processed entry at the front, blocking all future commits.

## Impact Explanation

This is a **HIGH severity** vulnerability per Aptos bug bounty criteria:

1. **Complete Validator Liveness Loss**: The affected node cannot commit any transactions, effectively removing it from consensus participation
2. **State Inconsistency**: If `save_transactions()` partially succeeds before failure, the database may contain uncommitted pre-committed data
3. **No Automatic Recovery**: The node remains stuck until manual restart
4. **Cascading Failures**: If triggered during state sync, the node falls behind and never catches up
5. **Network Impact**: Multiple validators encountering this simultaneously degrades network liveness

This qualifies as "Validator node slowdowns" and "Significant protocol violations" (HIGH severity) or potentially "Total loss of liveness" (CRITICAL severity) if it affects multiple validators concurrently.

## Likelihood Explanation

**Medium to High Likelihood:**

1. **Database I/O Failures**: Disk full, filesystem errors, corruption, hardware failures
2. **Resource Exhaustion**: Memory pressure causing write failures
3. **Concurrent Modification**: Race conditions between reset/finish and commit operations
4. **Network Disruptions**: During state sync, network issues can cause partial writes
5. **Testing Evidence**: The fail_point injection at line 274 suggests this is a known failure mode

The comment explicitly states "there's no practical strategy to recover from this error," confirming this is a recognized but unresolved issue.

## Recommendation

Implement atomic commit with rollback capability:

```rust
fn commit_chunk_impl(&self) -> Result<ExecutedChunk> {
    let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__total"]);
    
    // Create a savepoint/checkpoint of the queue state
    let chunk = {
        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__next_chunk_to_commit"]);
        self.commit_queue.lock().next_chunk_to_commit()?
    };

    let output = chunk.output.expect_complete_result();
    let num_txns = output.num_transactions_to_commit();
    
    // Try to save transactions
    let save_result = if chunk.ledger_info_opt.is_some() || num_txns != 0 {
        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__save_txns"]);
        self.db.writer.save_transactions(
            output.as_chunk_to_commit(),
            chunk.ledger_info_opt.as_ref(),
            false,
        )
    } else {
        Ok(())
    };

    // Handle failure with rollback
    match save_result {
        Ok(()) => {
            let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__dequeue_and_return"]);
            self.commit_queue.lock().dequeue_committed()?;
            Ok(chunk)
        }
        Err(e) => {
            // Rollback: restore the chunk to the queue
            self.commit_queue.lock().restore_chunk_after_failure(chunk)?;
            Err(e)
        }
    }
}
```

Add to `ChunkCommitQueue`:

```rust
pub(crate) fn restore_chunk_after_failure(&mut self, chunk: ExecutedChunk) -> Result<()> {
    ensure!(!self.to_commit.is_empty(), "Cannot restore to empty queue");
    ensure!(
        self.to_commit.front().unwrap().is_none(),
        "Cannot restore, front is not None"
    );
    *self.to_commit.front_mut().unwrap() = Some(chunk);
    Ok(())
}
```

## Proof of Concept

```rust
#[test]
fn test_commit_queue_stuck_on_save_failure() {
    // Setup: Create chunk executor with mock DB that fails on save
    let mut mock_db = MockDbWriter::new();
    mock_db.expect_save_transactions()
        .times(1)
        .returning(|_, _, _| Err(anyhow!("Simulated disk I/O error")));
    
    let executor = ChunkExecutor::new(DbReaderWriter::new(
        Arc::new(mock_db.clone()),
        Arc::new(mock_db),
    ));
    
    // Enqueue a chunk for commit
    executor.enqueue_chunk_by_execution(/* valid chunk */).unwrap();
    executor.update_ledger().unwrap();
    
    // First commit attempt fails
    let result = executor.commit_chunk();
    assert!(result.is_err());
    
    // Queue is now stuck - second commit attempt also fails
    let result2 = executor.commit_chunk();
    assert!(result2.is_err());
    assert!(result2.unwrap_err().to_string().contains("already been processed"));
    
    // All future commits blocked - liveness lost
    assert!(executor.commit_chunk().is_err());
}
```

**Notes:**

This vulnerability breaks the **State Consistency** invariant (atomic state transitions) and causes **liveness loss** as defined in the Aptos bug bounty. Recovery requires node restart, during which the queue is reinitialized: [7](#0-6) 

The issue is particularly severe because the error handling wrapper will panic if `has_pending_pre_commit` is true: [8](#0-7) 

This amplifies the liveness impact, causing immediate node crash rather than graceful degradation.

### Citations

**File:** execution/executor/src/chunk_executor/mod.rs (L96-105)
```rust
        let has_pending_pre_commit = inner.has_pending_pre_commit.load(Ordering::Acquire);
        f(inner).map_err(|error| {
            if has_pending_pre_commit {
                panic!(
                    "Hit error with pending pre-committed ledger, panicking. {:?}",
                    error,
                );
            }
            error
        })
```

**File:** execution/executor/src/chunk_executor/mod.rs (L261-288)
```rust
    fn commit_chunk_impl(&self) -> Result<ExecutedChunk> {
        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__total"]);
        let chunk = {
            let _timer =
                CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__next_chunk_to_commit"]);
            self.commit_queue.lock().next_chunk_to_commit()?
        };

        let output = chunk.output.expect_complete_result();
        let num_txns = output.num_transactions_to_commit();
        if chunk.ledger_info_opt.is_some() || num_txns != 0 {
            let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__save_txns"]);
            // TODO(aldenhu): remove since there's no practical strategy to recover from this error.
            fail_point!("executor::commit_chunk", |_| {
                Err(anyhow::anyhow!("Injected error in commit_chunk"))
            });
            self.db.writer.save_transactions(
                output.as_chunk_to_commit(),
                chunk.ledger_info_opt.as_ref(),
                false, // sync_commit
            )?;
        }

        let _timer = CHUNK_OTHER_TIMERS.timer_with(&["commit_chunk_impl__dequeue_and_return"]);
        self.commit_queue.lock().dequeue_committed()?;

        Ok(chunk)
    }
```

**File:** execution/executor/src/chunk_executor/chunk_commit_queue.rs (L49-63)
```rust
    pub(crate) fn new_from_db(db: &Arc<dyn DbReader>) -> Result<Self> {
        let LedgerSummary {
            state,
            state_summary,
            transaction_accumulator,
        } = db.get_pre_committed_ledger_summary()?;

        Ok(Self {
            latest_state: state,
            latest_state_summary: state_summary,
            latest_txn_accumulator: transaction_accumulator,
            to_commit: VecDeque::new(),
            to_update_ledger: VecDeque::new(),
        })
    }
```

**File:** execution/executor/src/chunk_executor/chunk_commit_queue.rs (L133-142)
```rust
    pub(crate) fn next_chunk_to_commit(&mut self) -> Result<ExecutedChunk> {
        let chunk_opt = self
            .to_commit
            .front_mut()
            .ok_or_else(|| anyhow!("No chunk to commit."))?;
        let chunk = chunk_opt
            .take()
            .ok_or_else(|| anyhow!("Next chunk to commit has already been processed."))?;
        Ok(chunk)
    }
```

**File:** execution/executor/src/chunk_executor/chunk_commit_queue.rs (L144-152)
```rust
    pub(crate) fn dequeue_committed(&mut self) -> Result<()> {
        ensure!(!self.to_commit.is_empty(), "to_commit is empty.");
        ensure!(
            self.to_commit.front().unwrap().is_none(),
            "Head of to_commit has not been processed."
        );
        self.to_commit.pop_front();
        Ok(())
    }
```

**File:** storage/storage-interface/src/lib.rs (L608-628)
```rust
    fn save_transactions(
        &self,
        chunk: ChunkToCommit,
        ledger_info_with_sigs: Option<&LedgerInfoWithSignatures>,
        sync_commit: bool,
    ) -> Result<()> {
        // For reconfig suffix.
        if ledger_info_with_sigs.is_none() && chunk.is_empty() {
            return Ok(());
        }

        if !chunk.is_empty() {
            self.pre_commit_ledger(chunk.clone(), sync_commit)?;
        }
        let version_to_commit = if let Some(ledger_info_with_sigs) = ledger_info_with_sigs {
            ledger_info_with_sigs.ledger_info().version()
        } else {
            chunk.expect_last_version()
        };
        self.commit_ledger(version_to_commit, ledger_info_with_sigs, Some(chunk))
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L276-318)
```rust
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
```
