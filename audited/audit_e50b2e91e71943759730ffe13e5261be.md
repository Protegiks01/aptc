# Audit Report

## Title
Checkpoint Creation Race Condition Leading to Inconsistent Database Snapshot

## Summary
The `AptosDB::create_checkpoint()` function creates database checkpoints by sequentially checkpointing multiple separate RocksDB instances (LedgerDb, StateKvDb, StateMerkleDb) without coordinating with the validator's commit locks. This allows a race condition where checkpoint creation can capture an inconsistent state across databases if commits occur concurrently, potentially resulting in a checkpoint that doesn't represent any valid committed blockchain state.

## Finding Description

The checkpoint creation process in [1](#0-0)  opens new database handles and creates checkpoints sequentially for different database components.

Meanwhile, the validator's commit process in [2](#0-1)  spawns seven parallel tasks that write to different databases simultaneously. The TODO comment at [3](#0-2)  explicitly acknowledges this can cause inconsistencies.

The vulnerability occurs because:

1. **No Lock Coordination**: The checkpoint function is static and doesn't acquire the `pre_commit_lock` or `commit_lock` defined in [4](#0-3) 

2. **Sequential vs Parallel Operations**: Checkpoints are created sequentially while commits write in parallel across databases

3. **Race Condition Window**: 
   - Time T1: Checkpoint captures LedgerDb (version N)
   - Time T2: Validator commits version N+1 to all databases in parallel
   - Time T3: Checkpoint captures StateKvDb (version N+1) 
   - Time T4: Checkpoint captures StateMerkleDb (version N+1)

4. **Invalid State**: The resulting checkpoint has LedgerDb at version N but StateKvDb/StateMerkleDb at version N+1 - a state that never existed during normal operation and violates the **State Consistency** invariant.

## Impact Explanation

**Severity Assessment: Medium**

While the recovery mechanism in [5](#0-4)  can detect and attempt to fix inconsistencies through truncation, this vulnerability still poses risks:

1. **Recovery Failures**: If the checkpoint captures `OverallCommitProgress` at version N+1 (written in [6](#0-5) ) but StateKvDb has data only up to version N, recovery may fail or require manual intervention.

2. **State Sync Issues**: Validators restoring from inconsistent checkpoints may serve incorrect state to light clients or other validators during state synchronization.

3. **Merkle Tree Corruption**: The inconsistency between databases could cause Merkle proof verification failures, breaking the ability to validate state transitions.

This qualifies as **Medium severity** per the bug bounty criteria: "State inconsistencies requiring intervention."

## Likelihood Explanation

**Likelihood: Low to Very Low**

The likelihood is mitigated by several factors:

1. **RocksDB Lock Protection**: The checkpoint code opens databases with `readonly=false` in [7](#0-6) , which should fail if another process (the validator) already has them open due to RocksDB's LOCK file mechanism.

2. **Test-Only Documentation**: [8](#0-7)  explicitly marks checkpoint creation as "test-only."

3. **No Production API**: There's no REST API or RPC endpoint that triggers checkpoint creation on running validators.

However, the vulnerability could manifest if:
- RocksDB's lock mechanism has bugs or race conditions
- Checkpoint is programmatically called within the validator process
- File system permissions allow bypassing lock protection

## Recommendation

**Fix: Add atomic checkpoint coordination**

1. Make `create_checkpoint` an instance method rather than static
2. Acquire both `pre_commit_lock` and `commit_lock` before checkpoint creation
3. Create all database checkpoints atomically within a critical section

```rust
// In storage/aptosdb/src/db/mod.rs
pub fn create_checkpoint_atomic(
    &self,
    cp_path: impl AsRef<Path>,
) -> Result<()> {
    // Acquire locks to prevent concurrent commits
    let _pre_lock = self.pre_commit_lock.lock().unwrap();
    let _commit_lock = self.commit_lock.lock().unwrap();
    
    // Now create checkpoints atomically
    Self::create_checkpoint(
        self.db_root_path(),
        cp_path,
        self.sharding_enabled,
    )
}
```

Alternatively, document that checkpoints must only be created when the validator is completely stopped, and add runtime checks to verify no commit operations are in progress.

## Proof of Concept

The race condition can be demonstrated programmatically (though difficult to trigger due to lock protection):

```rust
// Conceptual PoC - would need to be run within validator process
// to bypass RocksDB lock file protection

use std::sync::Arc;
use std::thread;

// Thread 1: Validator committing
let validator_db = Arc::clone(&db);
thread::spawn(move || {
    loop {
        // Simulate continuous commits
        validator_db.pre_commit_ledger(chunk, false).unwrap();
        validator_db.commit_ledger(version, None, None).unwrap();
    }
});

// Thread 2: Checkpoint creation
thread::spawn(|| {
    // This would capture inconsistent state if it runs
    // between the parallel database writes
    AptosDB::create_checkpoint(
        source_path,
        checkpoint_path,
        true,
    ).unwrap();
});

// Verification: Check checkpoint consistency
// by comparing commit progress across databases
```

The inconsistency would be detected by inspecting the checkpoint's `OverallCommitProgress`, `LedgerCommitProgress`, and `StateKvCommitProgress` values across different database components.

## Notes

This vulnerability represents a **design flaw in checkpoint atomicity** rather than a directly exploitable attack vector. The RocksDB lock file mechanism provides practical protection in most scenarios, but the lack of explicit coordination between checkpoint creation and commit operations violates defensive programming principles and could lead to subtle corruption issues in edge cases or under race conditions in RocksDB itself.

The existing `sync_commit_progress` recovery mechanism acknowledges this issue exists, as evidenced by the comments in [9](#0-8)  and [10](#0-9)  which explicitly state that commit progress isn't guaranteed to be atomic across databases.

### Citations

**File:** storage/aptosdb/src/db/mod.rs (L35-37)
```rust
    pre_commit_lock: std::sync::Mutex<()>,
    /// This is just to detect concurrent calls to `commit_ledger()`
    commit_lock: std::sync::Mutex<()>,
```

**File:** storage/aptosdb/src/db/mod.rs (L172-205)
```rust
    pub fn create_checkpoint(
        db_path: impl AsRef<Path>,
        cp_path: impl AsRef<Path>,
        sharding: bool,
    ) -> Result<()> {
        let start = Instant::now();

        info!(sharding = sharding, "Creating checkpoint for AptosDB.");

        LedgerDb::create_checkpoint(db_path.as_ref(), cp_path.as_ref(), sharding)?;
        if sharding {
            StateKvDb::create_checkpoint(db_path.as_ref(), cp_path.as_ref())?;
            StateMerkleDb::create_checkpoint(
                db_path.as_ref(),
                cp_path.as_ref(),
                sharding,
                /* is_hot = */ true,
            )?;
        }
        StateMerkleDb::create_checkpoint(
            db_path.as_ref(),
            cp_path.as_ref(),
            sharding,
            /* is_hot = */ false,
        )?;

        info!(
            db_path = db_path.as_ref(),
            cp_path = cp_path.as_ref(),
            time_ms = %start.elapsed().as_millis(),
            "Made AptosDB checkpoint."
        );
        Ok(())
    }
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L103-107)
```rust
            ledger_batch.put::<DbMetadataSchema>(
                &DbMetadataKey::OverallCommitProgress,
                &DbMetadataValue::Version(version),
            )?;
            self.ledger_db.metadata_db().write_schemas(ledger_batch)?;
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L263-322)
```rust
    fn calculate_and_commit_ledger_and_state_kv(
        &self,
        chunk: &ChunkToCommit,
        skip_index_and_usage: bool,
    ) -> Result<HashValue> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions__work"]);

        let mut new_root_hash = HashValue::zero();
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });

        Ok(new_root_hash)
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L327-327)
```rust
            /*readonly=*/ false,
```

**File:** aptos-node/src/storage.rs (L135-135)
```rust
/// so that the existing data won't change. For now this is a test-only feature.
```
