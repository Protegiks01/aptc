# Audit Report

## Title
Bounded Channel Backpressure Causes Complete Indexer-GRPC-Manager Stall on Master Nodes

## Summary
The `FileStoreOperatorV2` uses a bounded channel with capacity 5 to communicate between the transaction processing task and the upload task. When the upload consumer is slow (e.g., due to network issues or slow cloud storage), the channel fills up and causes a blocking send operation. This prevents the `file_store_version` from being updated in the cache, which blocks cache garbage collection, ultimately causing the entire indexer-grpc-manager master node to stall and stop processing new transactions.

## Finding Description

The indexer-grpc-manager architecture on master nodes has a critical backpressure vulnerability:

**The Bounded Channel**: A bounded channel with capacity 5 is created for communication between transaction processing and file upload tasks. [1](#0-0) 

**Blocking Send Operation**: When the channel is full, the send operation blocks indefinitely until space becomes available. [2](#0-1) 

**File Store Version Update Dependency**: For master nodes, the `file_store_version` in the cache is updated exclusively by the FileStoreUploader calling `get_transactions_from_cache` with `update_file_store_version=true`. [3](#0-2) 

Master nodes do NOT watch the file store for version updates, unlike non-master nodes. [4](#0-3) 

**Cache Garbage Collection Limitation**: The cache GC can only free transactions up to the `file_store_version`. When `file_store_version` stops updating, GC cannot free newer transactions. [5](#0-4) 

**DataManager Stall**: When the cache exceeds `max_cache_size` (default 5GB) and GC cannot free enough space, the DataManager's main loop blocks indefinitely waiting for successful GC. [6](#0-5) 

**Deadlock Scenario**:
1. Upload task is slow (network latency, slow GCS, high upload volume)
2. Bounded channel (capacity 5) fills up with pending upload batches
3. FileStoreUploader's producer task blocks on `tx.send().await` waiting for channel space
4. While blocked, FileStoreUploader cannot call `get_transactions_from_cache` to update `file_store_version`
5. Cache continues growing as DataManager ingests new transactions from fullnodes
6. Cache GC can only free up to stale `file_store_version`, insufficient to bring cache below `max_cache_size`
7. DataManager blocks waiting for GC to succeed
8. **Complete stall**: DataManager cannot ingest new transactions, FileStoreUploader cannot upload, indexer API becomes unresponsive

## Impact Explanation

**Severity: High** - This qualifies under "API crashes" and "Significant protocol violations" categories.

The complete stall of the indexer-grpc-manager master node results in:
- **API Unavailability**: All gRPC clients querying the indexer receive no responses or timeouts
- **Data Pipeline Failure**: Transaction indexing stops, creating a growing gap in indexed data
- **Cascading Failures**: Dependent services (block explorers, analytics platforms, wallets) lose access to transaction data
- **Manual Intervention Required**: The deadlock requires service restart and potential cache clearing

While this does not directly affect consensus or validator operations, the indexer-grpc-manager is a critical infrastructure component for ecosystem services that depend on transaction data queries.

## Likelihood Explanation

**Likelihood: High** - This scenario is highly likely to occur in production:

- **Common Trigger Conditions**: 
  - Network congestion or latency spikes to cloud storage (GCS/S3)
  - Temporary storage service degradation
  - High transaction volume periods
  - Bandwidth limitations
  
- **Small Channel Capacity**: With only 5 buffer slots, sustained slow uploads can quickly fill the channel

- **No Safety Mechanism**: There is no timeout, circuit breaker, or alternative path when backpressure occurs

- **Production Evidence**: Cloud storage performance variations are routine operational challenges

## Recommendation

**Immediate Fix**: Replace the bounded channel with an unbounded channel or implement proper backpressure handling.

**Option 1 - Unbounded Channel**:
```rust
let (tx, mut rx) = channel::<(_, BatchMetadata, _)>(usize::MAX);
```

**Option 2 - Non-blocking Send with Error Handling**:
```rust
match tx.try_send((transactions, self.buffer_batch_metadata.clone(), end_batch)) {
    Ok(_) => {},
    Err(tokio::sync::mpsc::error::TrySendError::Full(_)) => {
        warn!("Upload channel full, applying backpressure");
        // Implement retry logic or drop policy
    },
    Err(e) => return Err(anyhow::Error::msg(e)),
}
```

**Option 3 - Update file_store_version Independently**:
For master nodes, periodically update `file_store_version` by reading from the file store directly (similar to non-master nodes), decoupling it from the FileStoreUploader's progress: [7](#0-6) 

**Recommended Approach**: Combine Option 1 (increase channel capacity significantly or make it unbounded) with Option 3 (periodic independent version updates) for defense in depth.

## Proof of Concept

To reproduce this vulnerability in a test environment:

1. **Setup**: Deploy indexer-grpc-manager as master with default configuration
2. **Slow Upload Simulation**: Inject artificial latency into the upload task by adding sleep in `do_upload`:
   - Modify line 141 in `file_store_uploader.rs` to add `tokio::time::sleep(Duration::from_secs(10)).await;`
3. **Trigger High Volume**: Connect to a fullnode with high transaction throughput
4. **Observe**:
   - Channel fills up within ~50 seconds (5 batches Ã— 10s upload delay)
   - Producer task blocks on send
   - Cache continues growing
   - After cache reaches 5GB, DataManager logs "Filestore is lagging behind, cache is full"
   - DataManager stops processing new transactions
   - API queries timeout or receive stale data

**Monitoring Metrics**:
- `IS_FILE_STORE_LAGGING` metric set to 1 [8](#0-7) 
- `CACHE_SIZE` reaches `MAX_CACHE_SIZE` (5GB)
- `FILE_STORE_VERSION_IN_CACHE` stops increasing

---

**Notes**: This vulnerability specifically affects the indexer-grpc-manager service, which is part of the Aptos ecosystem infrastructure for serving historical transaction data. While it does not directly compromise consensus or validator operations, it represents a significant availability issue for API consumers and dependent ecosystem services.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L137-137)
```rust
            let (tx, mut rx) = channel::<(_, BatchMetadata, _)>(5);
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/file_store_uploader.rs (L157-162)
```rust
                        data_manager
                            .get_transactions_from_cache(
                                next_version,
                                MAX_SIZE_PER_FILE,
                                /*update_file_store_version=*/ true,
                            )
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/file_store_operator_v2/file_store_operator.rs (L80-82)
```rust
        tx.send((transactions, self.buffer_batch_metadata.clone(), end_batch))
            .await
            .map_err(anyhow::Error::msg)?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L63-80)
```rust
    fn maybe_gc(&mut self) -> bool {
        if self.cache_size <= self.max_cache_size {
            return true;
        }

        while self.start_version < self.file_store_version.load(Ordering::SeqCst)
            && self.cache_size > self.target_cache_size
        {
            let transaction = self.transactions.pop_front().unwrap();
            self.cache_size -= transaction.encoded_len();
            self.start_version += 1;
        }

        CACHE_SIZE.set(self.cache_size as i64);
        CACHE_START_VERSION.set(self.start_version as i64);

        self.cache_size <= self.max_cache_size
    }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L179-179)
```rust
        let watch_file_store_version = !is_master;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L235-256)
```rust
                loop {
                    trace!("Maybe running GC.");
                    if self.cache.write().await.maybe_gc() {
                        IS_FILE_STORE_LAGGING.set(0);
                        trace!("GC is done, file store is not lagging.");
                        break;
                    }
                    IS_FILE_STORE_LAGGING.set(1);
                    // If file store is lagging, we are not inserting more data.
                    let cache = self.cache.read().await;
                    warn!("Filestore is lagging behind, cache is full [{}, {}), known_latest_version ({}).",
                          cache.start_version,
                          cache.start_version + cache.transactions.len() as u64,
                          self.metadata_manager.get_known_latest_version());
                    tokio::time::sleep(Duration::from_millis(100)).await;
                    if watch_file_store_version {
                        self.update_file_store_version_in_cache(
                            &cache, /*version_can_go_backward=*/ false,
                        )
                        .await;
                    }
                }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-manager/src/data_manager.rs (L403-419)
```rust
    async fn update_file_store_version_in_cache(
        &self,
        cache: &RwLockReadGuard<'_, Cache>,
        version_can_go_backward: bool,
    ) {
        let file_store_version = self.file_store_reader.get_latest_version().await;
        if let Some(file_store_version) = file_store_version {
            let file_store_version_before_update = cache
                .file_store_version
                .fetch_max(file_store_version, Ordering::SeqCst);
            FILE_STORE_VERSION_IN_CACHE.set(file_store_version as i64);
            info!("Updated file_store_version in cache to {file_store_version}.");
            if !version_can_go_backward && file_store_version_before_update > file_store_version {
                panic!("File store version is going backward, data might be corrupted. {file_store_version_before_update} v.s. {file_store_version}");
            };
        }
    }
```
