# Audit Report

## Title
Database Write-Mode Access in Read-Only Debugging Tool Enables Unintended Data Modification

## Summary
The `watch opened` debugging tool opens AptosDB with write access (`readonly=false`) while using default pruner configurations that automatically delete historical blockchain data. This API misuse violates the principle of least privilege and can cause unintended state modifications during debugging operations.

## Finding Description

The debugging tool at `storage/aptosdb/src/db_debugger/watch/opened.rs` is designed to open and monitor an AptosDB instance, but it incorrectly opens the database in write mode with active data pruning: [1](#0-0) 

The tool uses `StorageConfig::default()` which enables all pruning operations by default: [2](#0-1) 

When the database opens in write mode, background pruner workers are spawned and initialized: [3](#0-2) 

These pruner workers run continuously in background threads, deleting old blockchain data: [4](#0-3) 

In contrast, other read-only debugging tools correctly use `readonly=true`: [5](#0-4) 

Additionally, RocksDB enforces exclusive locking when opened in write mode, preventing concurrent access from production nodes: [6](#0-5) 

## Impact Explanation

This issue qualifies as **Medium severity** under Aptos bug bounty criteria for "State inconsistencies requiring intervention":

1. **Unintended Data Loss**: Pruner threads delete historical transaction data, state snapshots, and epoch information that operators expect to preserve during debugging
2. **Database Lock Conflicts**: Exclusive write-mode locking prevents the production node from accessing its own database
3. **State Consistency Risk**: Partial pruning during interrupted debugging sessions could leave the database in an inconsistent state requiring manual recovery

While the tool requires local access, the security impact stems from the API's failure to enforce appropriate access modes for non-production contexts, violating the State Consistency invariant.

## Likelihood Explanation

**Likelihood: Medium** - This vulnerability manifests when:
- Operators use the debugging tool on stopped validator nodes for troubleshooting
- The tool's name ("watch opened") suggests passive observation, not active modification
- No warnings indicate that historical data will be permanently deleted
- The default configuration enables destructive operations without explicit opt-in

The issue is realistic because debugging database state is a common operational task, and the tool's interface provides no indication of its destructive behavior.

## Recommendation

Modify the tool to enforce read-only access and disable pruners:

```rust
pub fn run(self) -> Result<()> {
    let mut config = StorageConfig::default();
    config.set_data_dir(self.db_dir);
    config.rocksdb_configs.enable_storage_sharding =
        self.sharding_config.enable_storage_sharding;
    config.hot_state_config.delete_on_restart = false;
    // FIXED: Disable all pruners for debugging
    config.storage_pruner_config = NO_OP_STORAGE_PRUNER_CONFIG;

    let _db = AptosDB::open(
        config.get_dir_paths(),
        true, /* readonly - FIXED: Use read-only mode */
        config.storage_pruner_config,
        config.rocksdb_configs,
        false, /* enable_indexer - FIXED: Disable indexer in debug mode */
        config.buffered_state_target_items,
        config.max_num_nodes_per_lru_cache_shard,
        None,
        config.hot_state_config,
    )
    .expect("Failed to open AptosDB");
    
    println!("AptosDB opened in READ-ONLY mode. Kill to exit.");
    
    loop {
        std::thread::sleep(std::time::Duration::from_secs(1));
    }
}
```

Additionally, add validation in `AptosDB::open_internal` to prevent write-mode access for debugging contexts: [7](#0-6) 

## Proof of Concept

```bash
# 1. Stop a validator node
systemctl stop aptos-node

# 2. Run the watch debugging tool
db-tool debug watch opened --db-dir /opt/aptos/data

# Expected: Tool opens in read-only mode, no data modification
# Actual: Background pruners start deleting historical data

# 3. Check pruner activity in logs (if node was previously running)
# Data deletion occurs silently in background threads

# 4. Attempt to restart node while tool is running
systemctl start aptos-node
# Result: Node fails to start due to database lock held by debugging tool
```

**Notes:**

While this issue requires operator-level access to exploit, it represents a legitimate API design flaw that violates security best practices. The debugging tool should follow the principle of least privilege by defaulting to read-only access, preventing unintended data modification and lock conflicts that could impact node availability.

### Citations

**File:** storage/aptosdb/src/db_debugger/watch/opened.rs (L28-39)
```rust
        let _db = AptosDB::open(
            config.get_dir_paths(),
            false, /* readonly */
            config.storage_pruner_config,
            config.rocksdb_configs,
            config.enable_indexer,
            config.buffered_state_target_items,
            config.max_num_nodes_per_lru_cache_shard,
            None,
            config.hot_state_config,
        )
        .expect("Failed to open AptosDB");
```

**File:** config/src/config/storage_config.rs (L387-431)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
}

impl Default for StateMerklePrunerConfig {
    fn default() -> Self {
        StateMerklePrunerConfig {
            enable: true,
            // This allows a block / chunk being executed to have access to a non-latest state tree.
            // It needs to be greater than the number of versions the state committing thread is
            // able to commit during the execution of the block / chunk. If the bad case indeed
            // happens due to this being too small, a node restart should recover it.
            // Still, defaulting to 1M to be super safe.
            prune_window: 1_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
}

impl Default for EpochSnapshotPrunerConfig {
    fn default() -> Self {
        Self {
            enable: true,
            // This is based on ~5K TPS * 2h/epoch * 2 epochs. -- epoch ending snapshots are used
            // by state sync in fast sync mode.
            // The setting is in versions, not epochs, because this makes it behave more like other
            // pruners: a slower network will have longer history in db with the same pruner
            // settings, but the disk space take will be similar.
            // settings.
            prune_window: 80_000_000,
            // A 10k transaction block (touching 60k state values, in the case of the account
            // creation benchmark) on a 4B items DB (or 1.33B accounts) yields 300k JMT nodes
            batch_size: 1_000,
        }
    }
}
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L124-127)
```rust
        ensure!(
            pruner_config.eq(&NO_OP_STORAGE_PRUNER_CONFIG) || !readonly,
            "Do not set prune_window when opening readonly.",
        );
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L162-182)
```rust
        if !readonly {
            if let Some(version) = myself.get_synced_version()? {
                myself
                    .ledger_pruner
                    .maybe_set_pruner_target_db_version(version);
                myself
                    .state_store
                    .state_kv_pruner
                    .maybe_set_pruner_target_db_version(version);
            }
            if let Some(version) = myself.get_latest_state_checkpoint_version()? {
                myself
                    .state_store
                    .state_merkle_pruner
                    .maybe_set_pruner_target_db_version(version);
                myself
                    .state_store
                    .epoch_snapshot_pruner
                    .maybe_set_pruner_target_db_version(version);
            }
        }
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L52-69)
```rust
    // Loop that does the real pruning job.
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/db_debugger/examine/print_db_versions.rs (L47-55)
```rust
        let (ledger_db, _hot_state_merkle_db, state_merkle_db, state_kv_db) = AptosDB::open_dbs(
            &StorageDirPaths::from_path(&self.db_dir),
            rocksdb_config,
            env,
            block_cache,
            /*readonly=*/ true,
            /*max_num_nodes_per_lru_cache_shard=*/ 0,
            /*reset_hot_state=*/ false,
        )?;
```

**File:** storage/schemadb/src/lib.rs (L89-99)
```rust
    /// Open db in readonly mode
    /// Note that this still assumes there's only one process that opens the same DB.
    /// See `open_as_secondary`
    pub fn open_cf_readonly(
        opts: &Options,
        path: impl AsRef<Path>,
        name: &str,
        cfds: Vec<ColumnFamilyDescriptor>,
    ) -> DbResult<DB> {
        Self::open_cf_impl(opts, path, name, cfds, OpenMode::ReadOnly)
    }
```
