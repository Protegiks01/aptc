# Audit Report

## Title
Race Condition Between Parallel Pruning and Sequential Backup Iterator Creation Causes Incomplete Transaction History Snapshots

## Summary
The backup system creates multiple RocksDB iterators sequentially from different database components (transaction_db, transaction_info_db, event_db, etc.) while the pruner deletes data from these same databases in parallel. This creates a race condition where different iterators capture inconsistent point-in-time snapshots, causing backup operations to fail with "NotFound" errors and producing incomplete transaction histories that cannot be successfully restored.

## Finding Description

The vulnerability exists in the interaction between two subsystems:

**Backup Iterator Creation** (Sequential):
The `BackupHandler::get_transaction_iter()` method creates five separate RocksDB iterators sequentially, not atomically: [1](#0-0) 

Each iterator is created at a slightly different point in time, and each captures an implicit RocksDB snapshot at the moment of its creation.

**Parallel Pruning Execution**:
The `LedgerPruner` executes sub-pruners (including `TransactionInfoPruner`) in parallel using Rayon's `par_iter()`: [2](#0-1) 

The `TransactionInfoPruner` deletes transaction info entries for pruned versions: [3](#0-2) [4](#0-3) 

**The Race Condition**:

When storage sharding is enabled, each database component (transaction_db, transaction_info_db, event_db, etc.) is a separate RocksDB instance: [5](#0-4) 

Without sharding, they share the same instance but still create independent snapshots: [6](#0-5) 

**Attack Scenario** (Operational Failure, Not Malicious):

1. Backup operation requests versions 1000-2000
2. `txn_iter` is created from transaction_db → captures snapshot S1 at time T1
3. Pruner commits deletions for versions 0-1500 to transaction_info_db at time T2 (between T1 and T3)
4. `txn_info_iter` is created from transaction_info_db → captures snapshot S2 at time T3 (post-deletion)
5. When backup reads version 1000:
   - `txn_iter.next()` returns the transaction (present in snapshot S1)
   - `txn_info_iter.next()` returns `None` (deleted before snapshot S2)
6. Backup fails with error: "TransactionInfo not found when Transaction exists, version 1000" [7](#0-6) 

This breaks the **State Consistency** invariant: backup operations must capture atomic, consistent snapshots of all related data to ensure successful restoration.

## Impact Explanation

This is a **Medium Severity** issue per Aptos bug bounty criteria because it causes "State inconsistencies requiring intervention":

1. **Backup Integrity Failure**: Backups capture incomplete transaction history with missing TransactionInfo, Events, or WriteSet data for some versions
2. **Failed Disaster Recovery**: Restoring from such incomplete backups will fail, preventing node recovery
3. **Operational Disruption**: Backup operations fail intermittently with "NotFound" errors, requiring manual intervention and retry
4. **No Direct Fund Loss**: No funds are stolen or frozen, but backup infrastructure reliability is compromised
5. **Requires Intervention**: Operators must coordinate backup and pruning schedules to avoid the race window

The impact is limited to operational reliability rather than consensus safety or direct fund loss, placing it firmly in the Medium severity category.

## Likelihood Explanation

**Likelihood: Medium-High**

The vulnerability is moderately likely to occur in production:

1. **Continuous Pruner Operation**: The pruner worker runs continuously in a background thread: [8](#0-7) 

2. **Narrow But Real Timing Window**: While the 5 iterators are created in quick succession (microseconds apart), the pruner can commit deletions between any two iterator creations

3. **Production Configuration**: Storage sharding is commonly enabled in production for performance: [9](#0-8) 

4. **No Synchronization Mechanism**: There is no lock, atomic snapshot, or coordination between backup and pruning operations to prevent this race

5. **Backup Frequency**: Continuous backup operations (configured with `transaction_batch_size` and `state_snapshot_interval_epochs`) increase the probability of collision: [10](#0-9) 

## Recommendation

**Solution: Create Atomic Cross-Database Snapshot for Backup Operations**

Implement a mechanism to ensure all backup iterators see a consistent point-in-time view:

**Option 1: Explicit RocksDB Snapshot Coordination** (Preferred for sharded configuration)
```rust
// In BackupHandler, create a snapshot manager that coordinates across databases
pub fn get_transaction_iter_with_snapshot(
    &self,
    start_version: Version,
    num_transactions: usize,
) -> Result<impl Iterator<Item = Result<...>> + '_> {
    // Create explicit snapshots from all databases at the same logical point
    let snapshot_manager = SnapshotManager::new();
    
    // Pause pruner or mark versions as in-use
    self.ledger_db.mark_versions_in_backup(start_version, start_version + num_transactions as u64)?;
    
    let txn_iter = self.ledger_db.transaction_db()
        .get_transaction_iter_with_snapshot(start_version, num_transactions, &snapshot_manager)?;
    let txn_info_iter = self.ledger_db.transaction_info_db()
        .get_transaction_info_iter_with_snapshot(start_version, num_transactions, &snapshot_manager)?;
    // ... other iterators ...
    
    // Release marker when iterator is dropped
    Ok(iterators_with_cleanup)
}
```

**Option 2: Backup Version Range Protection** (Simpler)
```rust
// In PrunerManager, check for active backups before pruning
fn maybe_set_pruner_target_db_version(&self, latest_version: Version) {
    // Only prune versions that are outside active backup ranges
    let safe_target = self.get_safe_pruning_target(latest_version);
    if safe_target > self.get_min_readable_version() + self.pruning_batch_size {
        self.set_pruner_target_db_version(safe_target);
    }
}
```

**Option 3: Physical Checkpoint for Backups** (Most Robust)
Use RocksDB checkpoints which are atomic across all column families/databases: [11](#0-10) 

## Proof of Concept

```rust
#[test]
fn test_backup_pruning_race_condition() {
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    // Setup: Create AptosDB with sharding enabled
    let tmpdir = tempfile::tempdir().unwrap();
    let mut config = RocksdbConfigs::default();
    config.enable_storage_sharding = true;
    let db = AptosDB::new_for_test_with_config(&tmpdir, config);
    
    // Commit 2000 transactions
    for i in 0..2000 {
        db.save_transactions(...); // populate with test data
    }
    
    // Configure pruner with small prune window
    let pruner_config = LedgerPrunerConfig {
        enable: true,
        prune_window: 500,
        batch_size: 100,
        ..Default::default()
    };
    
    // Create barrier to synchronize threads
    let barrier = Arc::new(Barrier::new(2));
    let barrier_clone = Arc::clone(&barrier);
    let db_clone = Arc::clone(&db);
    
    // Thread 1: Start backup for versions 1000-1500
    let backup_handle = thread::spawn(move || {
        barrier_clone.wait(); // Synchronize start
        thread::sleep(Duration::from_micros(10)); // Let pruner start
        
        let backup_handler = db_clone.get_backup_handler();
        let result = backup_handler.get_transaction_iter(1000, 500);
        
        // Try to consume iterator - should fail if race occurs
        let mut count = 0;
        for item in result.unwrap() {
            match item {
                Ok(_) => count += 1,
                Err(e) => {
                    println!("Backup failed: {}", e);
                    assert!(e.to_string().contains("TransactionInfo not found"));
                    return Err(e);
                }
            }
        }
        Ok(count)
    });
    
    // Thread 2: Trigger pruning of versions 0-1500
    let pruner_handle = thread::spawn(move || {
        barrier.wait(); // Synchronize start
        
        db.ledger_pruner_manager.wake_and_wait_pruner(2000).unwrap();
        // This prunes up to version 1500 (2000 - prune_window)
    });
    
    pruner_handle.join().unwrap();
    let backup_result = backup_handle.join().unwrap();
    
    // Assertion: Backup should either succeed completely or fail with NotFound
    // In current implementation, it can fail due to race condition
    match backup_result {
        Ok(count) => assert_eq!(count, 500, "Should backup all 500 versions"),
        Err(e) => panic!("Race condition detected: {}", e),
    }
}
```

**Notes:**
- This race condition occurs due to the lack of atomicity in creating multiple iterators from different RocksDB instances (with sharding) or different snapshots (without sharding)
- The pruner runs continuously and can delete data between sequential iterator creations
- No explicit synchronization exists between backup and pruning operations
- The timing window is narrow but real, making this a probabilistic failure
- Impact is limited to backup integrity rather than consensus or fund safety, justifying Medium severity classification

### Citations

**File:** storage/aptosdb/src/backup/backup_handler.rs (L56-75)
```rust
        let txn_iter = self
            .ledger_db
            .transaction_db()
            .get_transaction_iter(start_version, num_transactions)?;
        let mut txn_info_iter = self
            .ledger_db
            .transaction_info_db()
            .get_transaction_info_iter(start_version, num_transactions)?;
        let mut event_vec_iter = self
            .ledger_db
            .event_db()
            .get_events_by_version_iter(start_version, num_transactions)?;
        let mut write_set_iter = self
            .ledger_db
            .write_set_db()
            .get_write_set_iter(start_version, num_transactions)?;
        let mut persisted_aux_info_iter = self
            .ledger_db
            .persisted_auxiliary_info_db()
            .get_persisted_auxiliary_info_iter(start_version, num_transactions)?;
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L81-86)
```rust
            let txn_info = txn_info_iter.next().ok_or_else(|| {
                AptosDbError::NotFound(format!(
                    "TransactionInfo not found when Transaction exists, version {}",
                    version
                ))
            })??;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L78-84)
```rust
            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_info_pruner.rs (L25-33)
```rust
    fn prune(&self, current_progress: Version, target_version: Version) -> Result<()> {
        let mut batch = SchemaBatch::new();
        TransactionInfoDb::prune(current_progress, target_version, &mut batch)?;
        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::TransactionInfoPrunerProgress,
            &DbMetadataValue::Version(target_version),
        )?;
        self.ledger_db.transaction_info_db().write_schemas(batch)
    }
```

**File:** storage/aptosdb/src/ledger_db/transaction_info_db.rs (L95-100)
```rust
    pub(crate) fn prune(begin: Version, end: Version, batch: &mut SchemaBatch) -> Result<()> {
        for version in begin..end {
            batch.delete::<TransactionInfoSchema>(&version)?;
        }
        Ok(())
    }
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L150-171)
```rust
        if !sharding {
            info!("Individual ledger dbs are not enabled!");
            return Ok(Self {
                ledger_metadata_db: LedgerMetadataDb::new(Arc::clone(&ledger_metadata_db)),
                event_db: EventDb::new(
                    Arc::clone(&ledger_metadata_db),
                    EventStore::new(Arc::clone(&ledger_metadata_db)),
                ),
                persisted_auxiliary_info_db: PersistedAuxiliaryInfoDb::new(Arc::clone(
                    &ledger_metadata_db,
                )),
                transaction_accumulator_db: TransactionAccumulatorDb::new(Arc::clone(
                    &ledger_metadata_db,
                )),
                transaction_auxiliary_data_db: TransactionAuxiliaryDataDb::new(Arc::clone(
                    &ledger_metadata_db,
                )),
                transaction_db: TransactionDb::new(Arc::clone(&ledger_metadata_db)),
                transaction_info_db: TransactionInfoDb::new(Arc::clone(&ledger_metadata_db)),
                write_set_db: WriteSetDb::new(Arc::clone(&ledger_metadata_db)),
                enable_storage_sharding: false,
            });
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L174-200)
```rust
        let ledger_db_folder = db_root_path.as_ref().join(LEDGER_DB_FOLDER_NAME);

        let mut event_db = None;
        let mut persisted_auxiliary_info_db = None;
        let mut transaction_accumulator_db = None;
        let mut transaction_auxiliary_data_db = None;
        let mut transaction_db = None;
        let mut transaction_info_db = None;
        let mut write_set_db = None;
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            s.spawn(|_| {
                let event_db_raw = Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(EVENT_DB_NAME),
                        EVENT_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                );
                event_db = Some(EventDb::new(
                    event_db_raw.clone(),
                    EventStore::new(event_db_raw),
                ));
            });
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L53-69)
```rust
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** config/src/config/storage_config.rs (L203-203)
```rust
    pub enable_storage_sharding: bool,
```

**File:** storage/backup/backup-cli/src/coordinators/backup.rs (L60-70)
```rust
    #[clap(
        long,
        default_value_t = 1000000,
        help = "The frequency (in transaction versions) to take an incremental transaction backup. \
        Making a transaction backup every 10 Million versions will result in the latest transaction \
        to appear in the backup potentially 10 Million versions later. If the net work is running \
        at 1 thousand transactions per second, that is roughly 3 hours. On the other hand, if \
        backups are too frequent and hence small, it slows down loading the backup metadata by too \
        many small files. "
    )]
    pub transaction_batch_size: usize,
```

**File:** storage/schemadb/src/lib.rs (L355-362)
```rust
    /// Creates new physical DB checkpoint in directory specified by `path`.
    pub fn create_checkpoint<P: AsRef<Path>>(&self, path: P) -> DbResult<()> {
        rocksdb::checkpoint::Checkpoint::new(&self.inner)
            .into_db_res()?
            .create_checkpoint(path)
            .into_db_res()?;
        Ok(())
    }
```
