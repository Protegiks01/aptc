# Audit Report

## Title
Indexer gRPC Data Service Lacks Graceful Shutdown Mechanism Leading to Resource Leaks

## Summary
The indexer-grpc-data-service does not implement any signal handling or graceful shutdown mechanism. When the service receives SIGINT (Ctrl+C) or SIGTERM, or when the main future is cancelled, spawned background tasks continue running indefinitely, and resources including Redis connections, file handles, and gRPC server tasks are not properly cleaned up.

## Finding Description

The indexer-grpc-data-service main function simply awaits `args.run()` without any signal handling: [1](#0-0) 

The `ServerArgs::run()` method calls `run_server_with_config()` which spawns two independent tokio tasks without any cancellation mechanism: [2](#0-1) 

These spawned tasks are created with `tokio::spawn()` and monitored via `tokio::select!`, but there is **no signal handler** registered (confirmed via grep search showing zero occurrences of `tokio::signal::ctrl_c`, `SIGINT`, or `SIGTERM` in the entire `ecosystem/indexer-grpc/` directory).

The service configuration establishes critical resources:

1. **Redis Connection**: Created in the config's `run()` method: [3](#0-2) 

2. **gRPC Server Tasks**: Multiple server tasks are spawned for TLS and non-TLS endpoints: [4](#0-3) 

3. **Per-Request Tasks**: Each client request spawns an additional task with its own Redis connection: [5](#0-4) [6](#0-5) 

**When cancellation occurs:**
1. If SIGINT/SIGTERM is sent without a handler, the process terminates immediately
2. All spawned tokio tasks are abruptly killed by the OS
3. Redis connections remain open on the server side (not properly closed via QUIT command)
4. Active file operations may be interrupted mid-operation
5. gRPC streams to clients are abruptly disconnected without proper error responses
6. If the future is cancelled programmatically (in a managed environment), spawned tasks continue running indefinitely since tokio::spawn tasks are NOT automatically cancelled when their parent future is cancelled

**Contrast with Proper Implementation:**
Other parts of the Aptos codebase correctly implement graceful shutdown using `CancellationToken`: [7](#0-6) 

## Impact Explanation

**Medium Severity** - This qualifies as "API crashes" (High) or "State inconsistencies requiring intervention" (Medium) per the bug bounty program.

**Specific Impacts:**

1. **Redis Connection Pool Exhaustion**: Repeated restarts leak connections on the Redis server. Each leaked connection consumes server resources. If multiple indexer services share the same Redis instance, this can cause cascading failures affecting:
   - Other indexer processors that use Redis for coordination
   - Cache workers that depend on Redis availability
   - Transaction importer services

2. **Resource Exhaustion**: Leaked file descriptors and memory from background tasks can accumulate over time in managed environments where the future is cancelled but the process isn't killed.

3. **Data Inconsistency**: Active gRPC streams are abruptly terminated without sending proper error responses, potentially leaving clients in inconsistent states.

4. **Operational Impact**: In production environments with automated restart mechanisms (k8s, systemd), this creates a resource leak that compounds over time.

**Important Note**: This vulnerability does NOT affect blockchain consensus, validator operations, or blockchain state. The Aptos blockchain continues to operate normally. This is purely an indexer infrastructure reliability issue.

## Likelihood Explanation

**High Likelihood** - This issue manifests in common operational scenarios:

1. **Normal Operations**: Operators routinely restart services for:
   - Configuration updates
   - Version upgrades
   - Resource management
   - Debugging

2. **Automated Systems**: Container orchestration systems (Kubernetes) frequently send SIGTERM during:
   - Rolling updates
   - Node drains
   - Pod evictions
   - Autoscaling events

3. **Emergency Situations**: When issues occur, operators may repeatedly restart services to recover, compounding the leak.

The issue requires no special attacker access or exploit - it happens during normal service lifecycle management.

## Recommendation

Implement proper graceful shutdown handling using the pattern already established elsewhere in the codebase:

**1. Add signal handling in main():**
```rust
use tokio_util::sync::CancellationToken;

#[tokio::main]
async fn main() -> Result<()> {
    let args = ServerArgs::parse();
    let shutdown = CancellationToken::new();
    
    tokio::select! {
        result = args.run::<IndexerGrpcDataServiceConfig>() => result,
        _ = tokio::signal::ctrl_c() => {
            println!("Received shutdown signal, cleaning up...");
            shutdown.cancel();
            Ok(())
        }
    }
}
```

**2. Modify run_server_with_config() to accept CancellationToken:**
```rust
pub async fn run_server_with_config<C>(
    config: GenericConfig<C>, 
    shutdown: CancellationToken
) -> Result<()>
where
    C: RunnableConfig,
{
    let task_handler = tokio::spawn(async move { /* ... */ });
    let main_task_handler = tokio::spawn(async move { /* ... */ });
    
    tokio::select! {
        _ = shutdown.cancelled() => {
            // Graceful shutdown initiated
            task_handler.abort();
            main_task_handler.abort();
            Ok(())
        },
        res = task_handler => { /* existing error handling */ },
        res = main_task_handler => { /* existing error handling */ },
    }
}
```

**3. Pass CancellationToken to spawned tasks** to allow cooperative cancellation of data_fetcher_task and server tasks.

**4. Ensure Redis connections are properly closed** by awaiting task completion before exiting.

## Proof of Concept

```rust
// File: test_shutdown.rs
// Compile: cargo test --package aptos-indexer-grpc-data-service

use tokio::time::Duration;
use std::process::{Command, Stdio};

#[tokio::test]
async fn test_ungraceful_shutdown_leaks_resources() {
    // Start the indexer data service
    let mut child = Command::new("target/debug/aptos-indexer-grpc-data-service")
        .arg("--config-path")
        .arg("test_config.yaml")
        .stdout(Stdio::null())
        .stderr(Stdio::null())
        .spawn()
        .expect("Failed to start service");
    
    // Wait for service to initialize
    tokio::time::sleep(Duration::from_secs(2)).await;
    
    // Check initial Redis connections
    let initial_conns = get_redis_connection_count();
    
    // Send SIGTERM (simulating graceful shutdown request)
    nix::sys::signal::kill(
        nix::unistd::Pid::from_raw(child.id() as i32),
        nix::sys::signal::Signal::SIGTERM
    ).expect("Failed to send SIGTERM");
    
    // Wait for process to terminate
    tokio::time::sleep(Duration::from_secs(1)).await;
    
    // Check if connections leaked
    let final_conns = get_redis_connection_count();
    
    // VULNERABILITY: Connections are leaked because no cleanup occurred
    assert!(
        final_conns > initial_conns,
        "Expected resource leak: initial={}, final={}",
        initial_conns, final_conns
    );
    
    // Cleanup
    let _ = child.kill();
}

fn get_redis_connection_count() -> usize {
    // Query Redis for CLIENT LIST to count connections
    // Implementation depends on redis-cli or redis crate
    // This demonstrates the leak detection methodology
    unimplemented!("Connect to Redis and count active connections")
}
```

**Demonstration Steps:**
1. Configure and start the indexer-grpc-data-service with Redis backend
2. Send SIGTERM to the process
3. Query Redis server with `CLIENT LIST` command
4. Observe lingering connections from the terminated service
5. Repeat steps 1-4 multiple times
6. Observe connection count growing with each restart, confirming the leak

## Notes

This vulnerability is classified as **Medium severity** because:
- It does NOT affect blockchain consensus, validator operations, or on-chain state
- It IS an infrastructure reliability issue that can cause service degradation
- The indexer infrastructure is important for ecosystem usability but not critical for blockchain operation

The fix should be prioritized for production deployments where service restarts are common and Redis connection pools have finite capacity.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/main.rs (L13-17)
```rust
#[tokio::main]
async fn main() -> Result<()> {
    let args = ServerArgs::parse();
    args.run::<IndexerGrpcDataServiceConfig>().await
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L46-77)
```rust
pub async fn run_server_with_config<C>(config: GenericConfig<C>) -> Result<()>
where
    C: RunnableConfig,
{
    let health_port = config.health_check_port;
    // Start liveness and readiness probes.
    let config_clone = config.clone();
    let task_handler = tokio::spawn(async move {
        register_probes_and_metrics_handler(config_clone, health_port).await;
        anyhow::Ok(())
    });
    let main_task_handler =
        tokio::spawn(async move { config.run().await.expect("task should exit with Ok.") });
    tokio::select! {
        res = task_handler => {
            if let Err(e) = res {
                error!("Probes and metrics handler panicked or was shutdown: {:?}", e);
                process::exit(1);
            } else {
                panic!("Probes and metrics handler exited unexpectedly");
            }
        },
        res = main_task_handler => {
            if let Err(e) = res {
                error!("Main task panicked or was shutdown: {:?}", e);
                process::exit(1);
            } else {
                panic!("Main task exited unexpectedly");
            }
        },
    }
}
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/config.rs (L166-169)
```rust
        let redis_conn = redis::Client::open(self.redis_read_replica_address.0.clone())?
            .get_tokio_connection_manager()
            .await?;
        println!(">>>> Redis connection established");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/config.rs (L197-238)
```rust
        let mut tasks = vec![];
        if let Some(config) = &self.data_service_grpc_non_tls_config {
            let listen_address = config.data_service_grpc_listen_address;
            tracing::info!(
                grpc_address = listen_address.to_string().as_str(),
                "[data service] starting gRPC server with non-TLS."
            );
            tasks.push(tokio::spawn(async move {
                Server::builder()
                    .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
                    .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
                    .add_service(svc_clone)
                    .add_service(reflection_service_clone)
                    .serve(listen_address)
                    .await
                    .map_err(|e| anyhow::anyhow!(e))
            }));
        }
        if let Some(config) = &self.data_service_grpc_tls_config {
            let listen_address = config.data_service_grpc_listen_address;
            let cert = tokio::fs::read(config.cert_path.clone()).await?;
            let key = tokio::fs::read(config.key_path.clone()).await?;
            let identity = tonic::transport::Identity::from_pem(cert, key);
            tracing::info!(
                grpc_address = listen_address.to_string().as_str(),
                "[Data Service] Starting gRPC server with TLS."
            );
            tasks.push(tokio::spawn(async move {
                Server::builder()
                    .http2_keepalive_interval(Some(HTTP2_PING_INTERVAL_DURATION))
                    .http2_keepalive_timeout(Some(HTTP2_PING_TIMEOUT_DURATION))
                    .tls_config(tonic::transport::ServerTlsConfig::new().identity(identity))?
                    .add_service(svc)
                    .add_service(reflection_service)
                    .serve(listen_address)
                    .await
                    .map_err(|e| anyhow::anyhow!(e))
            }));
        }

        futures::future::try_join_all(tasks).await?;
        Ok(())
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L192-208)
```rust
        tokio::spawn({
            let request_metadata = request_metadata.clone();
            async move {
                data_fetcher_task(
                    redis_client,
                    file_store_operator,
                    cache_storage_format,
                    request_metadata,
                    transactions_count,
                    tx,
                    txns_to_strip_filter,
                    current_version,
                    in_memory_cache,
                )
                .await;
            }
        });
```

**File:** ecosystem/indexer-grpc/indexer-grpc-data-service/src/service.rs (L390-411)
```rust
    let conn = match redis_client.get_tokio_connection_manager().await {
        Ok(conn) => conn,
        Err(e) => {
            ERROR_COUNT
                .with_label_values(&["redis_connection_failed"])
                .inc();
            // Connection will be dropped anyway, so we ignore the error here.
            let _result = tx
                .send_timeout(
                    Err(Status::unavailable(
                        "[Data Service] Cannot connect to Redis; please retry.",
                    )),
                    RESPONSE_CHANNEL_SEND_TIMEOUT,
                )
                .await;
            error!(
                error = e.to_string(),
                "[Data Service] Failed to get redis connection."
            );
            return;
        },
    };
```

**File:** aptos-move/aptos-workspace-server/src/lib.rs (L1-100)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

//! This library runs and manages a set of services that makes up a local Aptos network.
//! - node
//!     - node API
//!     - indexer grpc
//! - faucet
//! - indexer
//!     - postgres db
//!     - processors
//!     - indexer API
//!
//! The services are bound to unique OS-assigned ports to allow for multiple local networks
//! to operate simultaneously, enabling testing and development in isolated environments.
//!
//! ## Key Features:
//! - Shared Futures
//!     - The code makes extensive use of shared futures across multiple services,
//!       ensuring orderly startup while maximizing parallel execution.
//! - Graceful Shutdown
//!     - When a `Ctrl-C` signal is received or if any of the services fail to start
//!       or exit unexpectedly, the system attempts to gracefully shut down all services,
//!       cleaning up resources like Docker containers, volumes and networks.

mod common;
mod services;

use anyhow::{anyhow, Context, Result};
use aptos_localnet::docker::get_docker;
use clap::Parser;
use common::make_shared;
use futures::TryFutureExt;
use services::{
    docker_common::create_docker_network_permanent, indexer_api::start_indexer_api,
    processors::start_all_processors,
};
use std::time::Duration;
use tokio::io::{AsyncBufReadExt, BufReader};
use tokio_util::sync::CancellationToken;
use uuid::Uuid;

#[derive(Parser)]
enum ControlCommand {
    Stop,
}

/// Starts an async task that reads and processes commands from stdin.
///
/// Currently there is only one command:
/// - stop: shuts down the services gracefully.
async fn start_command_processor(shutdown: CancellationToken) {
    let reader = BufReader::new(tokio::io::stdin());
    let mut lines = reader.lines();

    loop {
        tokio::select! {
            line = lines.next_line() => {
                match line {
                    Ok(Some(input)) => {
                        let res = ControlCommand::try_parse_from(format!("dummy {}", input).split_whitespace());
                        // Note: clap expects a program name so we add a dummy one.
                        match res {
                            Ok(cmd) => match cmd {
                                ControlCommand::Stop => {
                                    no_panic_println!("\nStop command received. Shutting down services. This may take a while.\n");
                                    shutdown.cancel();
                                    break;
                                }
                            }
                            Err(_) => {
                                no_panic_eprintln!("Invalid command: \"{}\"", input);
                            }
                        }
                    }
                    Ok(None) => {
                        break;
                    }
                    Err(err) => {
                        no_panic_eprintln!("Error reading from stdin: {}", err);
                        break;
                    }
                }
            }
            _ = shutdown.cancelled() => {
                break;
            }
        }
    }
}

async fn run_all_services(timeout: u64) -> Result<()> {
    let test_dir = tempfile::tempdir()?;
    let test_dir = test_dir.path();
    no_panic_println!("Created test directory: {}", test_dir.display());

    let instance_id = Uuid::new_v4();

    // Phase 0: Start the background services

```
