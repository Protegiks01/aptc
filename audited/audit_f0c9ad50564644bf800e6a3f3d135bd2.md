# Audit Report

## Title
Single Malicious Peer Can Manipulate Global Data Summary Without Quorum Requirements

## Summary
The `calculate_global_data_summary()` function in the Aptos data client does not enforce any minimum quorum of agreeing peers before trusting advertised data. A single malicious peer can advertise falsely high blockchain versions, causing the victim node to waste resources attempting to sync to non-existent data, resulting in node slowdowns and service degradation.

## Finding Description

The vulnerability exists in the state synchronization data client's global data summary calculation. [1](#0-0) 

The function aggregates advertised data from **all** non-ignored peers without any consensus or quorum mechanism. It simply collects storage summaries and pushes all advertised data ranges into vectors without verification. [2](#0-1) 

Most critically, when determining the highest synced version, the system takes the **maximum** version across all peers' advertisements with no majority requirement. [3](#0-2) 

**Attack Propagation Path:**

1. Malicious peer connects to victim node and sends storage summary with falsely high `synced_ledger_info` (e.g., version 1,000,000,000 when real blockchain is at version 1,000,000)

2. `calculate_global_data_summary()` includes this malicious advertisement in the aggregated data without validation

3. The global summary is cached and used by critical components:
   - Data streaming service checks "availability" based on this summary [4](#0-3) 
   - State sync driver uses it to determine if peers are active [5](#0-4) 
   - Latency monitor tracks lag using the falsely advertised version [6](#0-5) 

4. Node attempts to create data streams and request data for the falsely advertised versions

5. Requests either timeout (waiting for non-existent data) or return invalid data (caught by proof verification) [7](#0-6) 

6. Node wastes CPU, network bandwidth, and time retrying failed requests while being unable to sync from honest peers efficiently

The availability check uses `AdvertisedData::contains_range()` which only verifies that at least one peer advertises the data range, not that a majority agrees. [8](#0-7) 

## Impact Explanation

**HIGH Severity** per Aptos bug bounty criteria:

This vulnerability directly causes **"Validator node slowdowns"** (explicitly listed as High Severity in the bug bounty program):

- Nodes waste computational resources requesting and retrying non-existent data
- Network bandwidth is consumed with failed requests
- State synchronization is delayed or blocked entirely
- Nodes with only one or few connected peers are completely vulnerable
- During network partitions or bootstrap, a single malicious peer can dominate the global summary

**Why NOT Critical:**
- Does NOT break consensus safety (actual transaction data is cryptographically verified with proofs before commitment)
- Does NOT allow fund theft or minting
- Does NOT cause permanent network damage requiring a hardfork
- Malicious peer will eventually be penalized by the peer scoring system, though this takes time

The impact is significant because state sync is critical for:
- New validators bootstrapping
- Fullnodes staying synchronized
- Nodes recovering from downtime

## Likelihood Explanation

**HIGH Likelihood:**

**Attacker Requirements:**
- Only needs to connect as a network peer (no special privileges)
- Can send arbitrary storage service summaries
- No cryptographic bypass needed

**Attack Complexity:** Very Low
- Attacker simply advertises false `StorageServerSummary` with inflated versions
- No coordination with other attackers required
- Single malicious peer is sufficient

**Scenarios Where Vulnerability is Maximized:**
1. Node with only 1 connected peer (common during bootstrap)
2. Network partition where victim has few peers
3. New fullnode joining the network
4. Node behind restrictive firewall with limited peer connections

The configuration has no minimum peer requirement option, and no quorum threshold is enforced anywhere in the data summary calculation logic. [9](#0-8) 

## Recommendation

Implement a quorum-based consensus mechanism for the global data summary:

```rust
pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
    let storage_summaries: Vec<StorageServerSummary> = self
        .peer_to_state
        .iter()
        .filter_map(|peer_state| {
            peer_state
                .value()
                .get_storage_summary_if_not_ignored()
                .cloned()
        })
        .collect();

    // NEW: Enforce minimum peer quorum
    const MIN_PEERS_FOR_QUORUM: usize = 3;
    if storage_summaries.len() < MIN_PEERS_FOR_QUORUM {
        return GlobalDataSummary::empty();
    }

    // Calculate the global data summary using quorum-based aggregation
    let mut advertised_data = AdvertisedData::empty();
    
    // NEW: For synced_ledger_infos, use median instead of all peers
    let mut synced_versions: Vec<(u64, LedgerInfoWithSignatures)> = storage_summaries
        .iter()
        .filter_map(|s| {
            s.data_summary.synced_ledger_info.as_ref().map(|li| {
                (li.ledger_info().version(), li.clone())
            })
        })
        .collect();
    
    synced_versions.sort_by_key(|(v, _)| *v);
    let median_idx = synced_versions.len() / 2;
    if let Some((_, median_ledger_info)) = synced_versions.get(median_idx) {
        advertised_data.synced_ledger_infos.push(median_ledger_info.clone());
    }

    // NEW: For data ranges, only include ranges advertised by majority
    let mut epoch_range_counts: HashMap<CompleteDataRange<Epoch>, usize> = HashMap::new();
    for summary in &storage_summaries {
        if let Some(epoch_range) = summary.data_summary.epoch_ending_ledger_infos {
            *epoch_range_counts.entry(epoch_range).or_insert(0) += 1;
        }
    }
    
    let majority_threshold = (storage_summaries.len() / 2) + 1;
    for (range, count) in epoch_range_counts {
        if count >= majority_threshold {
            advertised_data.epoch_ending_ledger_infos.push(range);
        }
    }

    // Apply similar logic for states, transactions, transaction_outputs...
    
    // Continue with chunk size calculation (median already provides some protection)
    // ...
}
```

Additionally, add configuration option for minimum peer quorum:
```rust
pub struct AptosDataClientConfig {
    // ... existing fields ...
    
    /// Minimum number of peers required before trusting global data summary
    pub min_peers_for_global_summary: usize,
}
```

## Proof of Concept

```rust
#[tokio::test]
async fn test_single_malicious_peer_manipulates_global_summary() {
    use aptos_config::config::AptosDataClientConfig;
    use aptos_storage_service_types::responses::{
        CompleteDataRange, DataSummary, ProtocolMetadata, StorageServerSummary,
    };
    use aptos_types::ledger_info::LedgerInfoWithSignatures;
    use std::sync::Arc;

    // Create data client with default config
    let config = Arc::new(AptosDataClientConfig::default());
    let peer_states = PeerStates::new(config);

    // Simulate a single malicious peer
    let malicious_peer = PeerNetworkId::random();
    
    // Create storage summary with falsely high version (1 billion)
    let fake_ledger_info = create_fake_ledger_info_with_version(1_000_000_000);
    let malicious_summary = StorageServerSummary {
        protocol_metadata: ProtocolMetadata::default(),
        data_summary: DataSummary {
            synced_ledger_info: Some(fake_ledger_info),
            epoch_ending_ledger_infos: Some(CompleteDataRange::new(0, 10000).unwrap()),
            transactions: Some(CompleteDataRange::new(0, 1_000_000_000).unwrap()),
            transaction_outputs: Some(CompleteDataRange::new(0, 1_000_000_000).unwrap()),
            states: Some(CompleteDataRange::new(0, 1_000_000_000).unwrap()),
        },
    };

    // Update peer with malicious summary
    peer_states.update_summary(malicious_peer, malicious_summary);

    // Calculate global summary - VULNERABILITY: accepts single peer's data
    let global_summary = peer_states.calculate_global_data_summary();

    // Verify the malicious data is included
    let highest_version = global_summary
        .advertised_data
        .highest_synced_ledger_info()
        .unwrap()
        .ledger_info()
        .version();

    // EXPLOIT CONFIRMED: Single malicious peer sets the highest version
    assert_eq!(highest_version, 1_000_000_000);
    
    // This will cause the node to:
    // 1. Try to sync to version 1 billion
    // 2. Send requests to the malicious peer
    // 3. Waste resources on timeouts or invalid proof errors
    // 4. Be unable to sync properly with honest peers
}
```

## Notes

While the actual transaction data is verified with cryptographic proofs before commitment (preventing consensus violations), the **availability advertisement** itself is trusted without quorum. This breaks the assumption that the global data summary represents a consensus view of the network's available data. 

The median-based chunk size calculation provides partial protection, but the core issue remains: advertised data ranges are aggregated from all peers without majority verification. [10](#0-9) 

The peer scoring mechanism (`ignore_low_score_peers`) provides eventual mitigation but doesn't prevent the initial attack and allows repeated exploitation by rotating malicious peers. [11](#0-10)

### Citations

**File:** state-sync/aptos-data-client/src/peer_states.rs (L152-160)
```rust
    fn is_ignored(&self) -> bool {
        // Only ignore peers if the config allows it
        if !self.data_client_config.ignore_low_score_peers {
            return false;
        }

        // Otherwise, ignore peers with a low score
        self.score <= IGNORE_PEER_THRESHOLD
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L338-408)
```rust
    /// Calculates a global data summary using all known storage summaries
    pub fn calculate_global_data_summary(&self) -> GlobalDataSummary {
        // Gather all storage summaries, but exclude peers that are ignored
        let storage_summaries: Vec<StorageServerSummary> = self
            .peer_to_state
            .iter()
            .filter_map(|peer_state| {
                peer_state
                    .value()
                    .get_storage_summary_if_not_ignored()
                    .cloned()
            })
            .collect();

        // If we have no peers, return an empty global summary
        if storage_summaries.is_empty() {
            return GlobalDataSummary::empty();
        }

        // Calculate the global data summary using the advertised peer data
        let mut advertised_data = AdvertisedData::empty();
        let mut max_epoch_chunk_sizes = vec![];
        let mut max_state_chunk_sizes = vec![];
        let mut max_transaction_chunk_sizes = vec![];
        let mut max_transaction_output_chunk_sizes = vec![];
        for summary in storage_summaries {
            // Collect aggregate data advertisements
            if let Some(epoch_ending_ledger_infos) = summary.data_summary.epoch_ending_ledger_infos
            {
                advertised_data
                    .epoch_ending_ledger_infos
                    .push(epoch_ending_ledger_infos);
            }
            if let Some(states) = summary.data_summary.states {
                advertised_data.states.push(states);
            }
            if let Some(synced_ledger_info) = summary.data_summary.synced_ledger_info.as_ref() {
                advertised_data
                    .synced_ledger_infos
                    .push(synced_ledger_info.clone());
            }
            if let Some(transactions) = summary.data_summary.transactions {
                advertised_data.transactions.push(transactions);
            }
            if let Some(transaction_outputs) = summary.data_summary.transaction_outputs {
                advertised_data
                    .transaction_outputs
                    .push(transaction_outputs);
            }

            // Collect preferred max chunk sizes
            max_epoch_chunk_sizes.push(summary.protocol_metadata.max_epoch_chunk_size);
            max_state_chunk_sizes.push(summary.protocol_metadata.max_state_chunk_size);
            max_transaction_chunk_sizes.push(summary.protocol_metadata.max_transaction_chunk_size);
            max_transaction_output_chunk_sizes
                .push(summary.protocol_metadata.max_transaction_output_chunk_size);
        }

        // Calculate optimal chunk sizes based on the advertised data
        let optimal_chunk_sizes = calculate_optimal_chunk_sizes(
            &self.data_client_config,
            max_epoch_chunk_sizes,
            max_state_chunk_sizes,
            max_transaction_chunk_sizes,
            max_transaction_output_chunk_sizes,
        );
        GlobalDataSummary {
            advertised_data,
            optimal_chunk_sizes,
        }
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L416-443)
```rust
/// To calculate the optimal chunk size, we take the median for each
/// chunk size parameter. This works well when we have an honest
/// majority that mostly agrees on the same chunk sizes.
pub(crate) fn calculate_optimal_chunk_sizes(
    config: &AptosDataClientConfig,
    max_epoch_chunk_sizes: Vec<u64>,
    max_state_chunk_sizes: Vec<u64>,
    max_transaction_chunk_sizes: Vec<u64>,
    max_transaction_output_chunk_size: Vec<u64>,
) -> OptimalChunkSizes {
    let epoch_chunk_size = median_or_max(max_epoch_chunk_sizes, config.max_epoch_chunk_size);
    let state_chunk_size = median_or_max(max_state_chunk_sizes, config.max_state_chunk_size);
    let transaction_chunk_size = median_or_max(
        max_transaction_chunk_sizes,
        config.max_transaction_chunk_size,
    );
    let transaction_output_chunk_size = median_or_max(
        max_transaction_output_chunk_size,
        config.max_transaction_output_chunk_size,
    );

    OptimalChunkSizes {
        epoch_chunk_size,
        state_chunk_size,
        transaction_chunk_size,
        transaction_output_chunk_size,
    }
}
```

**File:** state-sync/aptos-data-client/src/global_summary.rs (L153-173)
```rust
    pub fn contains_range(
        lowest: u64,
        highest: u64,
        advertised_ranges: &[CompleteDataRange<u64>],
    ) -> bool {
        for item in lowest..=highest {
            let mut item_exists = false;

            for advertised_range in advertised_ranges {
                if advertised_range.contains(item) {
                    item_exists = true;
                    break;
                }
            }

            if !item_exists {
                return false;
            }
        }
        true
    }
```

**File:** state-sync/aptos-data-client/src/global_summary.rs (L184-198)
```rust
    pub fn highest_synced_ledger_info(&self) -> Option<LedgerInfoWithSignatures> {
        let highest_synced_position = self
            .synced_ledger_infos
            .iter()
            .map(|ledger_info_with_sigs| ledger_info_with_sigs.ledger_info().version())
            .position_max();

        if let Some(highest_synced_position) = highest_synced_position {
            self.synced_ledger_infos
                .get(highest_synced_position)
                .cloned()
        } else {
            None
        }
    }
```

**File:** state-sync/data-streaming-service/src/streaming_service.rs (L272-287)
```rust
        let stream_id = self.stream_id_generator.next();
        let advertised_data = self.get_global_data_summary().advertised_data.clone();
        let (data_stream, stream_listener) = DataStream::new(
            self.data_client_config,
            self.streaming_service_config,
            stream_id,
            &request_message.stream_request,
            stream_update_notifier,
            self.aptos_data_client.clone(),
            self.notification_id_generator.clone(),
            &advertised_data,
            self.time_service.clone(),
        )?;

        // Verify the data stream can be fulfilled using the currently advertised data
        data_stream.ensure_data_is_available(&advertised_data)?;
```

**File:** state-sync/state-sync-driver/src/driver.rs (L671-678)
```rust
        // Fetch the global data summary and verify we have active peers
        let global_data_summary = self.aptos_data_client.get_global_data_summary();
        if global_data_summary.is_empty() {
            trace!(LogSchema::new(LogEntry::Driver).message(
                "The global data summary is empty! It's likely that we have no active peers."
            ));
            return self.check_auto_bootstrapping().await;
        }
```

**File:** state-sync/aptos-data-client/src/latency_monitor.rs (L127-141)
```rust
            let advertised_data = &self.data_client.get_global_data_summary().advertised_data;
            let highest_advertised_version = match advertised_data.highest_synced_ledger_info() {
                Some(ledger_info) => ledger_info.ledger_info().version(),
                None => {
                    sample!(
                        SampleRate::Duration(Duration::from_secs(LATENCY_MONITOR_LOG_FREQ_SECS)),
                        warn!(
                            (LogSchema::new(LogEntry::LatencyMonitor)
                                .event(LogEvent::AggregateSummary)
                                .message("Unable to get the highest advertised version!"))
                        );
                    );
                    continue; // Continue to the next round
                },
            };
```

**File:** state-sync/aptos-data-client/src/interface.rs (L183-187)
```rust
pub enum ResponseError {
    InvalidData,
    InvalidPayloadDataType,
    ProofVerificationError,
}
```

**File:** config/src/config/state_sync_config.rs (L411-458)
```rust
#[derive(Clone, Copy, Debug, Deserialize, PartialEq, Eq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct AptosDataClientConfig {
    /// Whether transaction data v2 is enabled
    pub enable_transaction_data_v2: bool,
    /// The aptos data poller config for the data client
    pub data_poller_config: AptosDataPollerConfig,
    /// The aptos data multi-fetch config for the data client
    pub data_multi_fetch_config: AptosDataMultiFetchConfig,
    /// Whether or not to ignore peers with low peer scores
    pub ignore_low_score_peers: bool,
    /// The aptos latency filtering config for the data client
    pub latency_filtering_config: AptosLatencyFilteringConfig,
    /// The interval (milliseconds) at which to refresh the latency monitor
    pub latency_monitor_loop_interval_ms: u64,
    /// Maximum number of epoch ending ledger infos per chunk
    pub max_epoch_chunk_size: u64,
    /// Maximum number of output reductions (division by 2) before transactions are returned,
    /// e.g., if 1000 outputs are requested in a single data chunk, and this is set to 1, then
    /// we'll accept anywhere between 1000 and 500 outputs. Any less, and the server should
    /// return transactions instead of outputs.
    // TODO: migrate away from this, and use cleaner chunk packing configs and logic.
    pub max_num_output_reductions: u64,
    /// Maximum lag (in seconds) we'll tolerate when sending optimistic fetch requests
    pub max_optimistic_fetch_lag_secs: u64,
    /// Maximum number of bytes to send in a single response
    pub max_response_bytes: u64,
    /// Maximum timeout (in ms) when waiting for a response (after exponential increases)
    pub max_response_timeout_ms: u64,
    /// Maximum number of state keys and values per chunk
    pub max_state_chunk_size: u64,
    /// Maximum lag (in seconds) we'll tolerate when sending subscription requests
    pub max_subscription_lag_secs: u64,
    /// Maximum number of transactions per chunk
    pub max_transaction_chunk_size: u64,
    /// Maximum number of transaction outputs per chunk
    pub max_transaction_output_chunk_size: u64,
    /// Timeout (in ms) when waiting for an optimistic fetch response
    pub optimistic_fetch_timeout_ms: u64,
    /// The duration (in seconds) after which to panic if no progress has been made
    pub progress_check_max_stall_time_secs: u64,
    /// First timeout (in ms) when waiting for a response
    pub response_timeout_ms: u64,
    /// Timeout (in ms) when waiting for a subscription response
    pub subscription_response_timeout_ms: u64,
    /// Whether or not to request compression for incoming data
    pub use_compression: bool,
}
```
