# Audit Report

## Title
Storage Service Use_Compression Flag Lacks Validation Enabling CPU Exhaustion DoS Attack

## Summary
The `use_compression` flag in `StorageServiceRequest` accepts client-specified compression requests without validation or rate limiting. Attackers can force storage servers to perform expensive LZ4 compression operations on all responses, causing CPU exhaustion and validator node slowdowns.

## Finding Description

The storage service allows any peer to request data with compression enabled by setting `use_compression=true` in the `StorageServiceRequest` struct. The server unconditionally honors this flag without any validation beyond basic data availability checks. [1](#0-0) 

When processing requests, the handler directly passes the client-controlled `use_compression` flag to response creation: [2](#0-1) 

The `StorageServiceResponse::new()` function performs CPU-intensive operations when compression is requested: [3](#0-2) 

This involves:
1. BCS serialization of the entire response data
2. LZ4 compression using the `aptos-compression` crate
3. Both operations are timed in metrics, confirming they consume measurable CPU time

The `RequestModerator` validates only whether the data can be served, NOT whether compression should be permitted: [4](#0-3) 

The moderator checks if the request can be satisfied by the storage summary, but performs no validation on the `use_compression` flag itself.

Additionally, the LRU cache treats compressed and uncompressed responses as separate entries because `StorageServiceRequest` includes `use_compression` in its hash/equality comparison. This means:
- An attacker can bypass the cache by requesting the same data with different compression flags
- Each unique combination requires a new compression operation

## Impact Explanation

This vulnerability allows an attacker to cause **validator node slowdowns**, which is explicitly categorized as **High Severity** (up to $50,000) in the Aptos bug bounty program.

The attack scenario:
1. Attacker connects to a validator's or VFN's storage service as a public network peer
2. Sends valid storage service requests with `use_compression=true`
3. Each request forces the server to:
   - Serialize potentially large response data (up to 40 MiB for v2 requests)
   - Perform LZ4 compression (CPU-intensive even in FAST mode)
4. With the default `max_network_channel_size` of 4000, an attacker can maintain thousands of concurrent compression requests
5. By varying request parameters or alternating the compression flag, the attacker bypasses cache protection

The compression operations consume CPU resources that should be available for consensus operations, transaction execution, and other critical validator functions. Sustained compression abuse can degrade validator performance, potentially affecting block proposal timing and consensus participation.

## Likelihood Explanation

**Likelihood: High**

The attack is trivial to execute:
- No special privileges required (any peer can connect)
- No validator collusion needed
- Simple to automate (just set `use_compression=true` on all requests)
- The default configuration provides no protection against this abuse

The only limiting factors are:
- `max_invalid_requests_per_peer` (500) only blocks INVALID requests, not valid requests with compression
- `max_network_channel_size` (4000) limits concurrent requests but is quite high
- LRU cache provides some protection but can be bypassed

An attacker needs only to:
1. Connect to the storage service
2. Send valid requests for data that exists
3. Set `use_compression=true` on all requests
4. Potentially vary request parameters to avoid cache hits

## Recommendation

Implement compression request validation and rate limiting:

```rust
// In StorageServiceConfig, add:
pub max_compression_requests_per_peer_per_minute: u64,
pub compression_request_ratio_limit: f64, // e.g., 0.5 for 50% of requests

// In RequestModerator, add compression tracking:
pub struct CompressionRequestTracker {
    compression_requests: DashMap<PeerNetworkId, VecDeque<Instant>>,
    total_requests: DashMap<PeerNetworkId, u64>,
}

// Validate compression requests in RequestModerator::validate_request:
pub fn validate_request(
    &self,
    peer_network_id: &PeerNetworkId,
    request: &StorageServiceRequest,
) -> Result<(), Error> {
    // Existing validation...
    
    // Validate compression requests
    if request.use_compression {
        self.validate_compression_request(peer_network_id)?;
    }
    
    // Continue with existing validation...
}

fn validate_compression_request(
    &self,
    peer_network_id: &PeerNetworkId,
) -> Result<(), Error> {
    // Check per-minute rate limit
    let mut compression_requests = self.compression_tracker
        .compression_requests
        .entry(*peer_network_id)
        .or_insert_with(VecDeque::new);
    
    let now = self.time_service.now();
    let one_minute_ago = now - Duration::from_secs(60);
    
    // Remove requests older than 1 minute
    while let Some(&timestamp) = compression_requests.front() {
        if timestamp < one_minute_ago {
            compression_requests.pop_front();
        } else {
            break;
        }
    }
    
    // Check if limit exceeded
    if compression_requests.len() >= self.config.max_compression_requests_per_peer_per_minute {
        return Err(Error::TooManyInvalidRequests(
            "Compression request rate limit exceeded".to_string()
        ));
    }
    
    compression_requests.push_back(now);
    Ok(())
}
```

Additionally, consider:
- Setting a lower default for compression requests (e.g., 100 per minute per peer)
- Monitoring compression metrics to detect abuse
- Potentially restricting compression to specific request types that benefit most from it

## Proof of Concept

```rust
// This PoC demonstrates the attack by sending multiple compression requests
// Add to state-sync/storage-service/server/src/tests/mod.rs

#[tokio::test]
async fn test_compression_resource_exhaustion() {
    use crate::tests::{mock::MockClient, utils};
    use aptos_storage_service_types::requests::StorageServiceRequest;
    use std::time::Instant;
    
    // Setup storage service
    let (mut mock_client, mut service, _, _, _) = MockClient::new(None, None);
    let end_version = 10000;
    utils::update_storage_server_summary(&mut service, end_version, 10);
    tokio::spawn(service.start());
    
    // Measure time for requests WITHOUT compression
    let start = Instant::now();
    for i in 0..100 {
        let _ = utils::get_transactions_with_proof(
            &mut mock_client,
            i * 100,
            (i + 1) * 100 - 1,
            end_version,
            false,
            false, // no compression
            false,
            1024 * 1024,
        ).await;
    }
    let no_compression_time = start.elapsed();
    
    // Measure time for requests WITH compression (forced by attacker)
    let start = Instant::now();
    for i in 0..100 {
        let _ = utils::get_transactions_with_proof(
            &mut mock_client,
            i * 100,
            (i + 1) * 100 - 1,
            end_version,
            false,
            true, // force compression
            false,
            1024 * 1024,
        ).await;
    }
    let compression_time = start.elapsed();
    
    // Compression should take significantly longer due to CPU overhead
    println!("Without compression: {:?}", no_compression_time);
    println!("With compression: {:?}", compression_time);
    assert!(compression_time > no_compression_time * 2, 
        "Compression should add significant overhead");
}
```

This demonstrates that an attacker forcing compression on all requests causes measurable CPU overhead, degrading validator performance.

### Citations

**File:** state-sync/storage-service/types/src/requests.rs (L10-13)
```rust
pub struct StorageServiceRequest {
    pub data_request: DataRequest, // The data to fetch from the storage service
    pub use_compression: bool,     // Whether or not the client wishes data to be compressed
}
```

**File:** state-sync/storage-service/server/src/handler.rs (L217-225)
```rust
            DataRequest::GetServerProtocolVersion => {
                let data_response = self.get_server_protocol_version();
                StorageServiceResponse::new(data_response, request.use_compression)
                    .map_err(|error| error.into())
            },
            DataRequest::GetStorageServerSummary => {
                let data_response = self.get_storage_server_summary();
                StorageServiceResponse::new(data_response, request.use_compression)
                    .map_err(|error| error.into())
```

**File:** state-sync/storage-service/types/src/responses.rs (L74-93)
```rust
    pub fn new(data_response: DataResponse, perform_compression: bool) -> Result<Self, Error> {
        if perform_compression {
            // Serialize and compress the raw data
            let raw_data = bcs::to_bytes(&data_response)
                .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;
            let compressed_data = aptos_compression::compress(
                raw_data,
                CompressionClient::StateSync,
                MAX_APPLICATION_MESSAGE_SIZE,
            )?;

            // Create the compressed response
            let label = data_response.get_label().to_string() + COMPRESSION_SUFFIX_LABEL;
            Ok(StorageServiceResponse::CompressedResponse(
                label,
                compressed_data,
            ))
        } else {
            Ok(StorageServiceResponse::RawResponse(data_response))
        }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L134-188)
```rust
    pub fn validate_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> Result<(), Error> {
        // Validate the request and time the operation
        let validate_request = || {
            // If the peer is being ignored, return an error
            if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
                if peer_state.is_ignored() {
                    return Err(Error::TooManyInvalidRequests(format!(
                        "Peer is temporarily ignored. Unable to handle request: {:?}",
                        request
                    )));
                }
            }

            // Get the latest storage server summary
            let storage_server_summary = self.cached_storage_server_summary.load();

            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }

            Ok(()) // The request is valid
        };
```
