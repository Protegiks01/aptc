# Audit Report

## Title
Async Cancellation Safety Vulnerability in Backup System Leaves Orphaned Files and Storage Inconsistency

## Summary
The `Command::run()` async function in the backup system lacks proper cancellation handling, causing orphaned child processes to continue writing partial backup files to storage when the task is cancelled. These partial files are never cleaned up automatically, leading to storage exhaustion and backup system degradation over time.

## Finding Description

The backup system's async implementation has a critical cancellation safety flaw. When `Command::run()` is cancelled mid-execution (e.g., via Ctrl+C, SIGTERM, or process termination), partial backup files are left in storage without cleanup. [1](#0-0) 

The root cause exists in the concurrent chunk writing process. When using the `CommandAdapter` storage backend (cloud storage like S3/GCS), each chunk write spawns a child process. The `ChildStdinAsDataSink` wrapper implements `AsyncWrite` but lacks a `Drop` implementation. [2](#0-1) 

When the async task is cancelled:
1. In-flight `write_chunk` futures are dropped during concurrent execution
2. `ChildStdinAsDataSink` instances are dropped without calling `shutdown()`
3. Spawned child processes (e.g., `gsutil cp`, `aws s3 cp`) become orphaned and continue running
4. Partial chunk files are successfully written to cloud storage
5. Manifest file is never created (written after all chunks complete)
6. Metadata entry is never saved (written after manifest) [3](#0-2) 

The backup discovery system relies exclusively on metadata files, not directory scanning: [4](#0-3) 

Directories without metadata files are invisible to the system and never cleaned up. The cleanup command is not implemented: [5](#0-4) 

## Impact Explanation

This vulnerability meets **High Severity** criteria per Aptos bug bounty:

1. **Validator node slowdowns**: As orphaned backup files accumulate, storage quotas are exhausted. Once storage is full, backup operations fail permanently, degrading disaster recovery capability. In cloud environments with billing limits, quota exhaustion can halt backup operations entirely.

2. **State inconsistencies requiring intervention**: Backup storage is left in an inconsistent state with partial files that are invisible to the backup system but consume storage resources. Manual intervention is required to identify and remove these orphaned directories.

3. **Operational security degradation**: Loss of backup capability compromises validator resilience. If a disaster occurs (hardware failure, database corruption) when backups are broken, validators cannot be restored, potentially causing network liveness issues if multiple validators are affected.

## Likelihood Explanation

**Very High Likelihood**:

1. **Common user behavior**: Operators frequently cancel long-running backup operations using Ctrl+C when they realize backups are misconfigured or taking too long
2. **Large state snapshots**: State snapshot backups can take hours for mainnet-scale databases, increasing the window for cancellation
3. **Process termination scenarios**: Backups can be cancelled through various mechanisms:
   - Manual Ctrl+C (SIGINT)
   - Process timeout kills (SIGTERM)
   - OOM killer (SIGKILL)
   - Container/pod termination in Kubernetes
   - System shutdown during backup
4. **No protective mechanisms**: The code has no signal handlers, graceful shutdown logic, or cleanup guards
5. **Accumulation over time**: Each cancelled backup leaves orphaned files that accumulate indefinitely

## Recommendation

Implement proper async cancellation safety through multiple layers:

**1. Add Drop implementation for cleanup:**
```rust
impl Drop for ChildStdinAsDataSink<'_> {
    fn drop(&mut self) {
        if let Some(child) = self.child.take() {
            // Kill the child process if dropped without shutdown
            tokio::spawn(async move {
                if let Some(mut child_process) = child.child.into_inner() {
                    let _ = child_process.kill().await;
                }
            });
        }
    }
}
```

**2. Implement backup cleanup on failure:**
Add a cleanup handler in `run_impl()` that removes the backup directory if the manifest is not successfully written:

```rust
async fn run_impl(mut self) -> Result<FileHandle> {
    self.version = Some(self.get_version_for_epoch_ending(self.epoch).await?);
    let backup_handle = self.storage
        .create_backup_with_random_suffix(&self.backup_name())
        .await?;
    
    // Track whether we completed successfully
    let mut completed = false;
    
    let result = async {
        // ... existing backup logic ...
        let manifest_handle = self.write_manifest(&backup_handle, chunks).await?;
        completed = true;
        Ok(manifest_handle)
    }.await;
    
    // Cleanup on failure
    if !completed {
        let _ = self.storage.delete_backup(&backup_handle).await;
    }
    
    result
}
```

**3. Add signal handling for graceful shutdown:**
Implement a cancellation token pattern in `Command::run()`:

```rust
pub async fn run(self) -> Result<()> {
    let cancel_token = CancellationToken::new();
    let cancel_token_clone = cancel_token.clone();
    
    tokio::spawn(async move {
        tokio::signal::ctrl_c().await.ok();
        cancel_token_clone.cancel();
    });
    
    tokio::select! {
        result = self.run_impl() => result,
        _ = cancel_token.cancelled() => {
            info!("Backup cancelled, cleaning up...");
            Err(anyhow!("Backup cancelled by user"))
        }
    }
}
```

**4. Implement orphaned file cleanup:**
Complete the cleanup command implementation to scan for and remove backup directories without metadata files.

## Proof of Concept

**Reproduction Steps:**

1. Configure backup to cloud storage (GCS/S3)
2. Start a large state snapshot backup:
   ```bash
   cargo run --bin db-tool -- backup oneoff state-snapshot \
     --state-snapshot-epoch 100 \
     --command-adapter-config /path/to/gcs-config.yaml
   ```

3. Wait for concurrent chunk writes to begin (check logs for "Chunk written")

4. Send SIGINT (Ctrl+C) to cancel the backup

5. Verify orphaned files in cloud storage:
   ```bash
   gsutil ls -r gs://your-bucket/backups/
   # Will show directories like "state_epoch_100_ver_12345.XXXX/" with chunk files
   ```

6. Verify no metadata file exists:
   ```bash
   gsutil ls gs://your-bucket/backups/metadata/
   # No corresponding state_snapshot metadata entry
   ```

7. Verify files are invisible to backup system:
   ```bash
   cargo run --bin db-tool -- backup query backup-storage-state \
     --command-adapter-config /path/to/gcs-config.yaml
   # Orphaned backup will not appear in output
   ```

8. Repeat cancellation multiple times to accumulate orphaned files and observe storage consumption growth

**Expected Behavior:**
- Backup directory should be removed on cancellation
- Child processes should be terminated
- No orphaned files left in storage

**Actual Behavior:**
- Partial chunk files remain in cloud storage
- Background upload processes complete successfully
- Storage consumption grows with each cancelled backup
- Manual cleanup required to remove orphaned directories

## Notes

This vulnerability affects all three backup types (EpochEnding, StateSnapshot, Transaction) and both storage backends (LocalFs, CommandAdapter), though the impact is most severe with CommandAdapter due to orphaned child processes. The issue is particularly problematic in production environments where backups run continuously and storage quotas are enforced.

### Citations

**File:** storage/db-tool/src/backup.rs (L169-255)
```rust
    pub async fn run(self) -> Result<()> {
        match self {
            Command::Oneoff(opt) => {
                let client = Arc::new(BackupServiceClient::new_with_opt(opt.client));
                let global_opt = opt.global;

                match opt.backup_type {
                    BackupType::EpochEnding { opt, storage } => {
                        EpochEndingBackupController::new(
                            opt,
                            global_opt,
                            client,
                            storage.init_storage().await?,
                        )
                        .run()
                        .await?;
                    },
                    BackupType::StateSnapshot { opt, storage } => {
                        StateSnapshotBackupController::new(
                            opt,
                            global_opt,
                            client,
                            storage.init_storage().await?,
                        )
                        .run()
                        .await?;
                    },
                    BackupType::Transaction { opt, storage } => {
                        TransactionBackupController::new(
                            opt,
                            global_opt,
                            client,
                            storage.init_storage().await?,
                        )
                        .run()
                        .await?;
                    },
                }
            },
            Command::Continuously(opt) => {
                BackupCoordinator::new(
                    opt.coordinator,
                    opt.global,
                    Arc::new(BackupServiceClient::new_with_opt(opt.client)),
                    opt.storage.init_storage().await?,
                )
                .run()
                .await?;
            },
            Command::Query(typ) => match typ {
                OneShotQueryType::NodeState(opt) => {
                    let client = BackupServiceClient::new_with_opt(opt.client);
                    if let Some(db_state) = client.get_db_state().await? {
                        println!("{}", db_state)
                    } else {
                        println!("DB not bootstrapped.")
                    }
                },
                OneShotQueryType::BackupStorageState(opt) => {
                    let view = cache::sync_and_load(
                        &opt.metadata_cache,
                        opt.storage.init_storage().await?,
                        opt.concurrent_downloads.get(),
                    )
                    .await?;
                    println!("{}", view.get_storage_state()?)
                },
            },
            Command::Verify(opt) => {
                VerifyCoordinator::new(
                    opt.storage.init_storage().await?,
                    opt.metadata_cache_opt,
                    opt.trusted_waypoints_opt,
                    opt.concurrent_downloads.get(),
                    opt.start_version.unwrap_or(0),
                    opt.end_version.unwrap_or(Version::MAX),
                    opt.state_snapshot_before_version.unwrap_or(Version::MAX),
                    opt.skip_epoch_endings,
                    opt.validate_modules,
                    opt.output_transaction_analysis,
                )?
                .run()
                .await?
            },
        }
        Ok(())
    }
```

**File:** storage/backup/backup-cli/src/storage/command_adapter/command.rs (L167-223)
```rust
pub(super) struct ChildStdinAsDataSink<'a> {
    child: Option<SpawnedCommand>,
    join_fut: Option<BoxFuture<'a, Result<()>>>,
}

impl ChildStdinAsDataSink<'_> {
    fn new(child: SpawnedCommand) -> Self {
        Self {
            child: Some(child),
            join_fut: None,
        }
    }
}

impl AsyncWrite for ChildStdinAsDataSink<'_> {
    fn poll_write(
        mut self: Pin<&mut Self>,
        cx: &mut Context<'_>,
        buf: &[u8],
    ) -> Poll<Result<usize, tokio::io::Error>> {
        if self.join_fut.is_some() {
            Poll::Ready(Err(tokio::io::ErrorKind::BrokenPipe.into()))
        } else {
            Pin::new(self.child.as_mut().unwrap().stdin()).poll_write(cx, buf)
        }
    }

    fn poll_flush(
        mut self: Pin<&mut Self>,
        cx: &mut Context<'_>,
    ) -> Poll<Result<(), tokio::io::Error>> {
        if self.join_fut.is_some() {
            Poll::Ready(Err(tokio::io::ErrorKind::BrokenPipe.into()))
        } else {
            Pin::new(self.child.as_mut().unwrap().stdin()).poll_flush(cx)
        }
    }

    fn poll_shutdown(
        mut self: Pin<&mut Self>,
        cx: &mut Context<'_>,
    ) -> Poll<Result<(), tokio::io::Error>> {
        if self.join_fut.is_none() {
            let res = Pin::new(self.child.as_mut().unwrap().stdin()).poll_shutdown(cx);
            if let Poll::Ready(Ok(_)) = res {
                // pipe shutdown successful
                self.join_fut = Some(self.child.take().unwrap().join().boxed())
            } else {
                return res;
            }
        }

        Pin::new(self.join_fut.as_mut().unwrap())
            .poll(cx)
            .map_err(tokio::io::Error::other)
    }
}
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/backup.rs (L235-269)
```rust
    async fn run_impl(mut self) -> Result<FileHandle> {
        self.version = Some(self.get_version_for_epoch_ending(self.epoch).await?);
        let backup_handle = self
            .storage
            .create_backup_with_random_suffix(&self.backup_name())
            .await?;

        let record_stream = Box::pin(self.record_stream(self.concurrent_data_requests).await?);
        let chunker = Chunker::new(record_stream, self.max_chunk_size).await?;

        let start = Instant::now();
        let chunk_stream = futures::stream::try_unfold(chunker, |mut chunker| async {
            Ok(chunker.next_chunk().await?.map(|chunk| (chunk, chunker)))
        });

        let chunk_manifest_fut_stream =
            chunk_stream.map_ok(|chunk| self.write_chunk(&backup_handle, chunk));

        let chunks: Vec<_> = chunk_manifest_fut_stream
            .try_buffered_x(8, 4) // 4 concurrently, at most 8 results in buffer.
            .map_ok(|chunk_manifest| {
                let last_idx = chunk_manifest.last_idx;
                info!(
                    last_idx = last_idx,
                    values_per_second =
                        ((last_idx + 1) as f64 / start.elapsed().as_secs_f64()) as u64,
                    "Chunk written."
                );
                chunk_manifest
            })
            .try_collect()
            .await?;

        self.write_manifest(&backup_handle, chunks).await
    }
```

**File:** storage/backup/backup-cli/src/metadata/cache.rs (L1-50)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE

use crate::{
    metadata::{view::MetadataView, Metadata},
    metrics::metadata::{NUM_META_DOWNLOAD, NUM_META_FILES, NUM_META_MISS},
    storage::{BackupStorage, FileHandle},
    utils::{error_notes::ErrorNotes, stream::StreamX},
};
use anyhow::{anyhow, Context, Result};
use aptos_logger::prelude::*;
use aptos_temppath::TempPath;
use async_trait::async_trait;
use clap::Parser;
use std::{
    collections::{HashMap, HashSet},
    path::{Path, PathBuf},
    sync::Arc,
    time::Instant,
};
use tokio::{
    fs::{create_dir_all, read_dir, remove_file, OpenOptions},
    io::{AsyncRead, AsyncReadExt},
};
use tokio_stream::{wrappers::ReadDirStream, StreamExt};

#[derive(Clone, Parser)]
pub struct MetadataCacheOpt {
    #[clap(
        long = "metadata-cache-dir",
        value_parser,
        help = "Metadata cache dir. If specified and shared across runs, \
        metadata files in cache won't be downloaded again from backup source, speeding up tool \
        boot up significantly. Cache content can be messed up if used across the devnet, \
        the testnet and the mainnet, hence it [Defaults to temporary dir]."
    )]
    dir: Option<PathBuf>,
}

impl MetadataCacheOpt {
    // in case we save things other than the cached files.
    const SUB_DIR: &'static str = "cache";

    pub fn new(dir: Option<impl AsRef<Path>>) -> Self {
        Self {
            dir: dir.map(|dir| dir.as_ref().to_path_buf()),
        }
    }

    pub(crate) fn cache_dir(&self) -> PathBuf {
```

**File:** storage/db-tool/src/backup_maintenance.rs (L1-100)
```rust
// Copyright (c) Aptos Foundation
// Licensed pursuant to the Innovation-Enabling Source Code License, available at https://github.com/aptos-labs/aptos-core/blob/main/LICENSE
use anyhow::Result;
use aptos_backup_cli::{
    coordinators::backup::BackupCompactor,
    metadata::cache::MetadataCacheOpt,
    storage::DBToolStorageOpt,
    utils::{storage_ext::BackupStorageExt, ConcurrentDownloadsOpt},
};
use clap::{Parser, Subcommand};

/// Support compacting and cleaning obsolete metadata files
#[derive(Subcommand)]
pub enum Command {
    #[clap(about = "Compact metdata files")]
    Compact(CompactionOpt),
    #[clap(about = "Cleanup the backup metadata files")]
    Cleanup(CleanupOpt),
    #[clap(about = "Display the backup meatdata in human-readable JSON format.")]
    ReadMetadata(ReadMetadataOpt),
}

#[derive(Parser)]
pub struct CompactionOpt {
    /// Specify how many epoch files to be merged in one compacted epoch ending metadata file
    #[clap(long, default_value_t = 1)]
    pub epoch_ending_file_compact_factor: usize,
    /// Specify how many state snapshot files to be merged in one compacted state snapshot metadata file
    #[clap(long, default_value_t = 1)]
    pub state_snapshot_file_compact_factor: usize,
    /// Specify how many transaction files to be merged in one transaction metadata file
    #[clap(long, default_value_t = 1)]
    pub transaction_file_compact_factor: usize,
    #[clap(flatten)]
    pub metadata_cache_opt: MetadataCacheOpt,
    #[clap(flatten)]
    pub storage: DBToolStorageOpt,
    #[clap(flatten)]
    pub concurrent_downloads: ConcurrentDownloadsOpt,
    /// Specify how many seconds to keep compacted metadata file before moving them to backup folder
    #[clap(
        long,
        default_value_t = 86400,
        help = "Remove metadata files replaced by compaction after specified seconds. They were not replaced right away after compaction in case they are being read then."
    )]
    pub remove_compacted_file_after: u64,
}

#[derive(Parser)]
pub struct CleanupOpt {
    #[clap(flatten)]
    pub storage: DBToolStorageOpt,
}

#[derive(Parser)]
pub struct ReadMetadataOpt {
    #[clap(flatten)]
    pub storage: DBToolStorageOpt,
    pub path: String,
}

impl Command {
    pub async fn run(self) -> Result<()> {
        match self {
            Command::Compact(opt) => {
                let compactor = BackupCompactor::new(
                    opt.epoch_ending_file_compact_factor,
                    opt.state_snapshot_file_compact_factor,
                    opt.transaction_file_compact_factor,
                    opt.metadata_cache_opt,
                    opt.storage.init_storage().await?,
                    opt.concurrent_downloads.get(),
                    opt.remove_compacted_file_after,
                );
                compactor.run().await?
            },
            Command::Cleanup(_) => {
                // TODO: add cleanup logic for removing obsolete metadata files
            },
            Command::ReadMetadata(opt) => {
                println!("Reading metadata file at: {}...", opt.path);
                let storage = opt.storage.init_storage().await?;
                let json_value = storage
                    .load_json_file::<serde_json::Value>(&opt.path)
                    .await?;
                println!("{}", serde_json::to_string_pretty(&json_value).unwrap());
            },
        }
        Ok(())
    }
}


```
