# Audit Report

## Title
Premature TxnGuard Drop Causes Validator Transactions to Be Silently Lost from Pool

## Summary
The `TxnGuard` RAII pattern in the validator transaction pool can cause quorum-certified validator transactions to be silently removed from the pool before being committed to a block. This occurs when state transitions in JWK consensus and DKG managers drop the guard prematurely, violating the invariant that validator transactions achieving quorum should eventually be committed.

## Finding Description

The `TxnGuard` struct implements a Drop trait that automatically removes transactions from the validator transaction pool when the guard goes out of scope. [1](#0-0) 

This guard is returned when calling `VTxnPoolState::put()` and must be held to keep the transaction in the pool. [2](#0-1) 

The vulnerability manifests in multiple scenarios:

**Scenario 1: JWK Consensus State Reset (Most Critical)**

In the JWK consensus manager, when a validator achieves a quorum-certified JWK update, it stores the transaction in the pool with the guard held in `ConsensusState::Finished`. [3](#0-2) 

However, when `reset_with_on_chain_state()` is called (triggered by `ObservedJWKsUpdated` events when another validator's transaction commits first), the manager can replace or remove the state: [4](#0-3) 

And: [5](#0-4) 

When the old `PerProviderState` containing `ConsensusState::Finished { vtxn_guard, ... }` is dropped, the `TxnGuard` is dropped, triggering transaction removal. [6](#0-5) 

**Scenario 2: DKG Manager State Transition**

Similar issue exists in the DKG manager where `std::mem::take()` is used to transition states. If `process_aggregated_transcript` is called when already in `Finished` state, the taken state (containing the guard) is dropped: [7](#0-6) 

**Attack Path:**
1. Validator A observes new JWKs for issuer X
2. Validator A achieves quorum certificate and calls `process_quorum_certified_update`
3. Transaction is placed in pool with `TxnGuard` stored in `ConsensusState::Finished`
4. Transaction is waiting to be pulled into a block by consensus
5. **BEFORE** Validator A's transaction is committed, Validator B's transaction for the same issuer commits
6. `ObservedJWKsUpdated` event is emitted and received by all validators
7. Validator A's `reset_with_on_chain_state()` is called
8. Old state is replaced/removed via `insert()` or `retain()`
9. `TxnGuard` is dropped, removing Validator A's transaction from pool
10. Validator A's quorum-certified transaction is permanently lost

The consensus proposal generator filters out pending transactions but cannot recover dropped transactions. [8](#0-7) 

## Impact Explanation

**Severity: High**

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria because it constitutes a "Significant protocol violation" where:

1. **Protocol Correctness**: Validator transactions that achieved quorum certification are silently lost, violating the fundamental guarantee that quorum-certified updates should eventually be committed
2. **Consensus Liveness**: Critical validator transactions (DKG results, JWK updates) may fail to commit, potentially blocking epoch transitions or breaking cryptographic protocol requirements
3. **State Inconsistency**: Different validators may have different views of which validator transactions are pending, causing potential consensus disagreements
4. **No Recovery Mechanism**: Once dropped, the transaction is permanently lost with no automatic retry logic

While this doesn't directly cause consensus safety violations (chain splits), it breaks the liveness and correctness guarantees of the validator transaction subsystem.

## Likelihood Explanation

**Likelihood: High**

This vulnerability is **highly likely** to occur in production:

1. **Natural Race Condition**: In a distributed system with multiple validators, it's common for multiple validators to observe the same off-chain state changes (JWKs, DKG results) and independently achieve quorum
2. **Fast Block Times**: Aptos has sub-second block times, making the window for race conditions very small but frequent
3. **Multiple Issuers**: JWK consensus runs per-issuer, so having multiple issuers increases the probability of conflicting updates
4. **No Coordination**: Validators don't coordinate on validator transaction submission, making races inevitable
5. **Event-Driven Reset**: The `ObservedJWKsUpdated` event triggers state resets immediately upon any on-chain JWK change, providing frequent opportunities for the bug

The bug doesn't require any malicious behavior—it occurs naturally due to timing in distributed consensus.

## Recommendation

**Solution: Separate Transaction Lifetime from State Lifetime**

The root cause is tying transaction lifetime to state machine lifecycle. Instead:

1. **Don't drop guards on state reset**: Maintain a separate collection of active validator transactions independent of consensus state
2. **Explicit transaction removal**: Only remove transactions when they're committed or explicitly invalidated
3. **Add transaction tracking**: Track pending validator transactions separately and only clean them up on epoch boundaries

**Code Fix Approach:**

```rust
// In jwk_manager/mod.rs
pub struct IssuerLevelConsensusManager {
    // ... existing fields ...
    
    // NEW: Track active validator transactions separately
    active_vtxns: HashMap<Issuer, TxnGuard>,
}

impl IssuerLevelConsensusManager {
    pub fn process_quorum_certified_update(&mut self, update: QuorumCertifiedUpdate) -> Result<()> {
        let issuer = update.update.issuer.clone();
        let state = self.states_by_issuer.entry(issuer.clone()).or_default();
        
        match &state.consensus_state {
            ConsensusState::InProgress { my_proposal, .. } => {
                let txn = ValidatorTransaction::ObservedJWKUpdate(update.clone());
                let vtxn_guard = self.vtxn_pool.put(...);
                
                // Store guard separately, not in state
                self.active_vtxns.insert(issuer.clone(), vtxn_guard);
                
                state.consensus_state = ConsensusState::Finished {
                    // Don't store guard in state
                    my_proposal: my_proposal.clone(),
                    quorum_certified: update.clone(),
                };
                Ok(())
            },
            _ => Err(anyhow!("...")),
        }
    }
    
    pub fn reset_with_on_chain_state(&mut self, on_chain_state: AllProvidersJWKs) -> Result<()> {
        // ... existing logic ...
        
        // Clean up active transactions for issuers that are no longer in consensus
        self.active_vtxns.retain(|issuer, _| {
            onchain_issuer_set.contains(issuer) && 
            self.states_by_issuer.get(issuer)
                .map_or(false, |state| matches!(state.consensus_state, ConsensusState::Finished { .. }))
        });
        
        Ok(())
    }
    
    async fn tear_down(&mut self, ack_tx: Option<oneshot::Sender<()>>) -> Result<()> {
        // Drop all active transactions on epoch boundary
        self.active_vtxns.clear();
        // ... rest of teardown ...
    }
}
```

Apply similar pattern to DKG manager.

## Proof of Concept

```rust
#[cfg(test)]
mod test {
    use super::*;
    use aptos_types::jwks::*;
    use aptos_crypto::bls12381::PrivateKey;
    
    #[tokio::test]
    async fn test_txn_guard_premature_drop_vulnerability() {
        // Setup: Create JWK manager with validator transaction pool
        let vtxn_pool = VTxnPoolState::default();
        let consensus_key = Arc::new(PrivateKey::generate_for_testing());
        let epoch_state = Arc::new(EpochState::empty());
        let update_certifier = Arc::new(MockUpdateCertifier::new());
        
        let mut manager = IssuerLevelConsensusManager::new(
            consensus_key.clone(),
            AccountAddress::random(),
            epoch_state.clone(),
            update_certifier,
            vtxn_pool.clone(),
        );
        
        // Issuer X with initial on-chain state at version 0
        let issuer_x = b"https://issuer-x.com".to_vec();
        let initial_state = AllProvidersJWKs {
            entries: vec![ProviderJWKs {
                issuer: issuer_x.clone(),
                version: 0,
                jwks: vec![],
            }],
        };
        
        manager.reset_with_on_chain_state(initial_state.clone()).unwrap();
        
        // Validator A observes new JWKs and achieves quorum
        let new_jwks = vec![test_jwk()];
        manager.process_new_observation(issuer_x.clone(), new_jwks.clone()).unwrap();
        
        // Simulate quorum achievement - create certified update
        let certified_update = QuorumCertifiedUpdate {
            update: ProviderJWKs {
                issuer: issuer_x.clone(),
                version: 1,
                jwks: new_jwks,
            },
            multi_sig: test_multi_sig(),
        };
        
        manager.process_quorum_certified_update(certified_update.clone()).unwrap();
        
        // Verify transaction is in pool
        let txns_before = vtxn_pool.pull(
            Instant::now() + Duration::from_secs(1),
            10,
            1_000_000,
            TransactionFilter::empty(),
        );
        assert_eq!(txns_before.len(), 1, "Transaction should be in pool");
        
        // VULNERABILITY: Validator B's transaction commits first
        // This triggers ObservedJWKsUpdated event with version 1
        let updated_state = AllProvidersJWKs {
            entries: vec![ProviderJWKs {
                issuer: issuer_x.clone(),
                version: 1,  // Changed version
                jwks: vec![test_different_jwk()],  // Different JWKs from B
            }],
        };
        
        // This call drops the old state, including the TxnGuard
        manager.reset_with_on_chain_state(updated_state).unwrap();
        
        // BUG: Transaction is now removed from pool!
        let txns_after = vtxn_pool.pull(
            Instant::now() + Duration::from_secs(1),
            10,
            1_000_000,
            TransactionFilter::empty(),
        );
        assert_eq!(txns_after.len(), 0, "BUG: Transaction was removed from pool!");
        
        // The quorum-certified transaction from Validator A is permanently lost
        // It will never be included in any block
    }
}
```

## Notes

This vulnerability is particularly insidious because:

1. **Silent Failure**: No error is logged when transactions are dropped—they simply disappear from the pool
2. **Design Pattern Issue**: The RAII pattern (using Drop for cleanup) is appropriate for epoch boundaries but inappropriate for mid-epoch state transitions
3. **Multiple Attack Surfaces**: Both JWK consensus and DKG manager are affected, indicating a systematic design flaw
4. **Production Impact**: This likely already causes occasional transaction loss in mainnet, though it may be masked by retry mechanisms at higher layers

The fix requires careful refactoring to separate transaction lifetime management from state machine lifecycle, ensuring validator transactions persist until explicitly committed or invalidated.

### Citations

**File:** crates/validator-transaction-pool/src/lib.rs (L126-134)
```rust
/// Returned for `txn` when you call `PoolState::put(txn, ...)`.
/// If this is dropped, `txn` will be deleted from the pool (if it has not been).
///
/// This allows the pool to be emptied on epoch boundaries.
#[derive(Clone)]
pub struct TxnGuard {
    pool: Arc<Mutex<PoolStateInner>>,
    seq_num: u64,
}
```

**File:** crates/validator-transaction-pool/src/lib.rs (L202-206)
```rust
impl Drop for TxnGuard {
    fn drop(&mut self) {
        self.pool.lock().try_delete(self.seq_num);
    }
}
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L252-253)
```rust
        self.states_by_issuer
            .retain(|issuer, _| onchain_issuer_set.contains(issuer));
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L270-273)
```rust
                let old_value = self.states_by_issuer.insert(
                    on_chain_provider_jwks.issuer.clone(),
                    PerProviderState::new(on_chain_provider_jwks),
                );
```

**File:** crates/aptos-jwk-consensus/src/jwk_manager/mod.rs (L332-350)
```rust
        match &state.consensus_state {
            ConsensusState::InProgress { my_proposal, .. } => {
                //TODO: counters
                let txn = ValidatorTransaction::ObservedJWKUpdate(update.clone());
                let vtxn_guard =
                    self.vtxn_pool
                        .put(Topic::JWK_CONSENSUS(issuer.clone()), Arc::new(txn), None);
                state.consensus_state = ConsensusState::Finished {
                    vtxn_guard,
                    my_proposal: my_proposal.clone(),
                    quorum_certified: update.clone(),
                };
                info!(
                    epoch = self.epoch_state.epoch,
                    issuer = String::from_utf8(issuer).ok(),
                    version = update.update.version,
                    "certified update accepted."
                );
                Ok(())
```

**File:** crates/aptos-jwk-consensus/src/types.rs (L110-114)
```rust
    Finished {
        vtxn_guard: TxnGuard,
        my_proposal: T,
        quorum_certified: QuorumCertifiedUpdate,
    },
```

**File:** dkg/src/dkg_manager/mod.rs (L384-423)
```rust
        self.state = match std::mem::take(&mut self.state) {
            InnerState::InProgress {
                start_time,
                my_transcript,
                ..
            } => {
                let agg_transcript_ready_time = duration_since_epoch();
                let secs_since_dkg_start =
                    agg_transcript_ready_time.as_secs_f64() - start_time.as_secs_f64();
                DKG_STAGE_SECONDS
                    .with_label_values(&[self.my_addr.to_hex().as_str(), "agg_transcript_ready"])
                    .observe(secs_since_dkg_start);

                let txn = ValidatorTransaction::DKGResult(DKGTranscript {
                    metadata: DKGTranscriptMetadata {
                        epoch: self.epoch_state.epoch,
                        author: self.my_addr,
                    },
                    transcript_bytes: bcs::to_bytes(&agg_trx)
                        .map_err(|e| anyhow!("transcript serialization error: {e}"))?,
                });
                let vtxn_guard = self.vtxn_pool.put(
                    Topic::DKG,
                    Arc::new(txn),
                    Some(self.pull_notification_tx.clone()),
                );
                info!(
                    epoch = self.epoch_state.epoch,
                    my_addr = self.my_addr,
                    "[DKG] aggregated transcript put into vtxn pool."
                );
                InnerState::Finished {
                    vtxn_guard,
                    start_time,
                    my_transcript,
                    proposed: false,
                }
            },
            _ => bail!("[DKG] aggregated transcript only expected during DKG"),
        };
```

**File:** consensus/src/liveness/proposal_generator.rs (L643-650)
```rust
        let pending_validator_txn_hashes: HashSet<HashValue> = pending_blocks
            .iter()
            .filter_map(|block| block.validator_txns())
            .flatten()
            .map(ValidatorTransaction::hash)
            .collect();
        let validator_txn_filter =
            vtxn_pool::TransactionFilter::PendingTxnHashSet(pending_validator_txn_hashes);
```
