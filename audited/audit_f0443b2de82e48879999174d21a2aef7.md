# Audit Report

## Title
Missing Validation of Shard Round Count Consistency in Sharded Block Executor

## Summary
The sharded block executor's aggregation logic assumes all shards return the same number of execution rounds but does not validate this invariant at runtime. If shards return inconsistent round counts due to bugs, crashes, or serialization issues, transactions can be silently skipped, violating the deterministic execution invariant critical for consensus safety.

## Finding Description

In the sharded block executor aggregation flow, the system makes an unchecked assumption about round count consistency across shards. [1](#0-0) 

The code determines `num_rounds` solely from `sharded_output[0].len()` without validating that all other shards returned the same number of rounds. It then pre-allocates `ordered_results` based on this assumption and iterates through each shard's results.

**Vulnerability Mechanism:**

If shard 0 returns 3 rounds but shard 1 returns only 2 rounds:
1. `num_rounds` is set to 3
2. `ordered_results` is sized for 6 slots (2 shards Ã— 3 rounds)
3. Shard 1's inner loop only iterates twice, leaving `ordered_results[5]` (round 2, shard 1) as an empty `vec![]`
4. When `extend()` is called on the empty vec, it contributes nothing, silently skipping all transactions that should have been in shard 1's round 2

The same assumption exists in the aggregator service: [2](#0-1) 

This code would panic with an index out of bounds error if shards have inconsistent round counts, causing a denial of service.

**Root Cause:**

The partitioner guarantees all shards receive the same number of rounds: [3](#0-2) 

All shards iterate through `0..final_num_rounds`, ensuring consistency by design. However, the aggregation code does not verify this guarantee holds at runtime.

**When This Could Occur:**

While the partitioner ensures consistency, violations could arise from:
- Serialization/deserialization bugs in remote executor communication
- Memory corruption or race conditions
- Partial execution with improper error handling
- Future code changes breaking the invariant
- Undiscovered bugs in the partitioner itself

## Impact Explanation

**Severity: HIGH** (per Aptos bug bounty criteria)

This issue constitutes a "significant protocol violation" because it breaks Critical Invariant #1 (Deterministic Execution):
- **Transaction Loss**: Transactions assigned to missing rounds are silently dropped from execution results
- **Determinism Violation**: Different validators could produce different transaction outputs if they experience different failure modes
- **State Inconsistency**: Validators would compute different state roots for the same block
- **Consensus Safety Risk**: Could cause chain splits if validators disagree on which transactions were executed

While not directly exploitable by an external attacker, this represents a critical gap in defensive programming that could manifest under exceptional conditions (crashes, bugs, or corruption), leading to consensus failure.

## Likelihood Explanation

**Likelihood: LOW to MEDIUM**

Under normal operation, this issue will not manifest because:
- The partitioner correctly ensures all shards get identical round counts
- Error propagation causes failures rather than partial returns
- No normal code path violates the assumption

However, likelihood increases under:
- Remote executor deployment (serialization attack surface)
- High system load causing race conditions
- Future code modifications breaking invariants
- Edge cases in error handling

The lack of validation means **any** future bug introducing inconsistency will cause silent transaction loss rather than failing safely.

## Recommendation

Add explicit validation that all shards returned the expected number of rounds:

```rust
pub fn execute_block(
    &self,
    state_view: Arc<S>,
    transactions: PartitionedTransactions,
    concurrency_level_per_shard: usize,
    onchain_config: BlockExecutorConfigFromOnchain,
) -> Result<Vec<TransactionOutput>, VMStatus> {
    // ... existing code ...
    
    let (sharded_output, global_output) = self
        .executor_client
        .execute_block(/* ... */)?.into_inner();
    
    // Validate round count consistency
    let num_executor_shards = self.executor_client.num_shards();
    let num_rounds = sharded_output[0].len();
    for (shard_id, shard_results) in sharded_output.iter().enumerate() {
        if shard_results.len() != num_rounds {
            return Err(VMStatus::error(
                StatusCode::INTERNAL_INVARIANT_VIOLATION,
                Some(format!(
                    "Shard {} returned {} rounds but expected {}",
                    shard_id, shard_results.len(), num_rounds
                ))
            ));
        }
    }
    
    // ... rest of aggregation logic ...
}
```

Similarly, add validation in the aggregator service before accessing shard outputs by round index.

## Proof of Concept

This vulnerability cannot be demonstrated through a Move test or standard Rust test because it requires inducing an internal inconsistency that normal code paths prevent. A proper PoC would require:

1. Mocking the executor client to return inconsistent round counts
2. Observing silent transaction skipping in aggregation
3. Demonstrating state root divergence between validators

However, I cannot provide a realistic PoC showing **attacker exploitation** because there is no external attack vector to trigger this condition. This is fundamentally a **defensive programming gap** rather than an exploitable vulnerability.

## Notes

**Critical Assessment:** Upon rigorous validation against the checklist, this issue **fails the exploitability requirement**. While the missing validation is a legitimate code quality issue that could cause serious problems if the invariant is violated by bugs, **there is no demonstrated attack path for an unprivileged attacker to exploit this**.

The security question asks "Can shards return empty Vec<Vec<TransactionOutput>> for some rounds" - the answer is **not through any attacker-controlled mechanism**. This would require internal bugs in the partitioner, serialization layer, or executor, none of which are attacker-controllable.

This represents a **robustness/defensive programming concern** rather than an exploitable security vulnerability meeting bug bounty criteria.

### Citations

**File:** aptos-move/aptos-vm/src/sharded_block_executor/mod.rs (L98-115)
```rust
        let num_rounds = sharded_output[0].len();
        let mut aggregated_results = vec![];
        let mut ordered_results = vec![vec![]; num_executor_shards * num_rounds];
        // Append the output from individual shards in the round order
        for (shard_id, results_from_shard) in sharded_output.into_iter().enumerate() {
            for (round, result) in results_from_shard.into_iter().enumerate() {
                ordered_results[round * num_executor_shards + shard_id] = result;
            }
        }

        for result in ordered_results.into_iter() {
            aggregated_results.extend(result);
        }

        // Lastly append the global output
        aggregated_results.extend(global_output);

        Ok(aggregated_results)
```

**File:** aptos-move/aptos-vm/src/sharded_block_executor/sharded_aggregator_service.rs (L175-191)
```rust
    let num_rounds = sharded_output[0].len();

    // The first element is 0, which is the delta for shard 0 in round 0. +1 element will contain
    // the delta for the global shard
    let mut aggr_total_supply_delta = vec![DeltaU128::default(); num_shards * num_rounds + 1];

    // No need to parallelize this as the runtime is O(num_shards * num_rounds)
    // TODO: Get this from the individual shards while getting 'sharded_output'
    let mut aggr_ts_idx = 1;
    for round in 0..num_rounds {
        sharded_output.iter().for_each(|shard_output| {
            let mut curr_delta = DeltaU128::default();
            // Though we expect all the txn_outputs to have total_supply, there can be
            // exceptions like 'block meta' (first txn in the block) and 'chkpt info' (last txn
            // in the block) which may not have total supply. Hence we iterate till we find the
            // last txn with total supply.
            for txn in shard_output[round].iter().rev() {
```

**File:** execution/block-partitioner/src/v2/build_edge.rs (L73-86)
```rust
        let sharded_txns = (0..state.num_executor_shards)
            .map(|shard_id| {
                let sub_blocks: Vec<SubBlock<AnalyzedTransaction>> = (0..final_num_rounds)
                    .map(|round_id| {
                        state.sub_block_matrix[round_id][shard_id]
                            .lock()
                            .unwrap()
                            .take()
                            .unwrap()
                    })
                    .collect();
                SubBlocksForShard::new(shard_id, sub_blocks)
            })
            .collect();
```
