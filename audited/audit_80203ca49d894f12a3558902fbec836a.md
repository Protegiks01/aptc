# Audit Report

## Title
Staging Area Pollution Leading to Corrupted Package Publishing in Large Packages Framework

## Summary
The `large_packages` module lacks session isolation in its `StagingArea` resource, allowing residual data from incomplete publish attempts to pollute subsequent package publishing operations. When users call `stage_code_chunk()` multiple times without completing the publish operation, the staging area persists with partial data that gets incorrectly mixed with new package chunks.

## Finding Description
The vulnerability exists in the `stage_code_chunk_internal()` function's handling of the persistent `StagingArea` resource. [1](#0-0) 

When a `StagingArea` already exists, the function reuses it instead of creating a fresh one. The critical issue occurs in how chunks are accumulated: [2](#0-1) 

Metadata chunks are unconditionally appended to existing metadata. [3](#0-2) 

Code chunks at existing indices are appended rather than replaced.

**Attack Scenario:**
1. User stages chunks for Package A (modules M_A0, M_A1) via repeated `stage_code_chunk()` calls
2. User abandons the publish attempt (realizes mistake, changes mind, encounters error)
3. `StagingArea` persists with Package A's data
4. User later stages chunks for Package B (module M_B0) 
5. Chunks get appended: `code[0]` = M_A0_bytecode + M_B0_bytecode (corrupted)
6. Old module M_A1 remains at `code[1]`
7. Metadata becomes Package_A_metadata + Package_B_metadata (corrupted) [4](#0-3) 

The cleanup only occurs after successful publish. If users don't complete the publish or fail to manually call `cleanup_staging_area()`, the staging area remains polluted. [5](#0-4) 

The `assemble_module_code()` function blindly iterates through all stored indices, combining old and new data without validation.

## Impact Explanation
**Severity: Medium**

While the CLI provides warnings [6](#0-5) , these protections are client-side only and can be bypassed:

1. **Most Likely Impact**: Corrupted metadata fails BCS deserialization [7](#0-6) , causing transaction failure with wasted gas and user confusion

2. **Edge Case Impact**: If chunk boundaries align such that concatenated bytecode forms valid (but semantically incorrect) modules, wrong code could be published. This breaks the State Consistency invariant - published package metadata won't match actual bytecode, potentially affecting:
   - Modules managing user funds or assets
   - Governance or staking logic
   - Critical protocol operations

3. **No Consensus Impact**: The `StagingArea` is per-account on-chain state, processed deterministically by all validators

This qualifies as **Medium Severity** per the bug bounty criteria: "State inconsistencies requiring intervention" and potential "Limited funds loss" if incorrect modules are published that manage assets.

## Likelihood Explanation
**Likelihood: Medium-High**

The vulnerability is easily triggered through normal user behavior:
- Users starting package publishes then abandoning them (common during development/testing)
- SDK/tool implementations that don't check staging area state before new publishes
- Users unfamiliar with the cleanup requirement
- No automatic expiry or session management in the on-chain code

The fact that CLI-level protections exist indicates this scenario is expected and common enough to warrant explicit warnings.

## Recommendation
Add session isolation to prevent cross-package contamination. Introduce a session identifier and package hash to the `StagingArea` structure:

```move
struct StagingArea has key {
    session_id: u64,  // Increment on each new publish session
    package_hash: vector<u8>,  // Hash of first metadata chunk
    metadata_serialized: vector<u8>,
    code: SmartTable<u64, vector<u8>>,
    last_module_idx: u64
}
```

Modify `stage_code_chunk_internal()` to validate session continuity:
1. Calculate hash of first metadata chunk in new session
2. If `StagingArea` exists but `package_hash` differs, abort with `ESESSION_MISMATCH`
3. Reset staging area if hash validation fails
4. This ensures chunks from different packages cannot mix

Alternative: Implement automatic cleanup after configurable timeout (e.g., 1 hour) to prevent indefinite staging area pollution.

## Proof of Concept

```move
#[test(user = @0x123)]
fun test_staging_area_pollution(user: &signer) {
    // Setup: Deploy large_packages module
    
    // Step 1: Stage chunks for Package A
    large_packages::stage_code_chunk(
        user,
        b"metadata_A_part1",  // Package A metadata chunk
        vector[0u16],
        vector[b"module_A0_chunk1"]  // First chunk of module A0
    );
    
    large_packages::stage_code_chunk(
        user,
        b"metadata_A_part2",
        vector[0u16, 1u16],
        vector[b"module_A0_chunk2", b"module_A1_full"]  // Complete module A1
    );
    
    // Step 2: Abandon Package A publish (simulate user error/change of mind)
    // StagingArea now contains partial Package A data
    
    // Step 3: Start staging Package B (different package)
    large_packages::stage_code_chunk(
        user,
        b"metadata_B_part1",  // Package B metadata - gets APPENDED
        vector[0u16],
        vector[b"module_B0_chunk1"]  // Gets APPENDED to module A0
    );
    
    // Step 4: Attempt to publish - will have corrupted data
    large_packages::stage_code_chunk_and_publish_to_account(
        user,
        b"metadata_B_part2",
        vector[0u16],
        vector[b"module_B0_chunk2"]
    );
    
    // Result: 
    // - metadata = "metadata_A_part1metadata_A_part2metadata_B_part1metadata_B_part2" (CORRUPTED)
    // - code[0] = "module_A0_chunk1module_A0_chunk2module_B0_chunk1module_B0_chunk2" (CORRUPTED) 
    // - code[1] = "module_A1_full" (OLD PACKAGE DATA)
    // - Likely fails at deserialization, but if it passes, publishes wrong code
}
```

## Notes
- This vulnerability exists at the Move framework level, not in the Rust chunked_publish.rs helper [8](#0-7) , which only creates transaction payloads
- CLI-level protections can be bypassed by using SDKs, direct transaction submission, or ignoring prompts
- The on-chain code has no session validation, package identity verification, or automatic cleanup mechanisms
- This breaks the State Consistency invariant when corrupted but valid bytecode is published

### Citations

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L86-94)
```text
        let staging_area =
            stage_code_chunk_internal(
                owner,
                metadata_chunk,
                code_indices,
                code_chunks
            );
        publish_to_account(owner, staging_area);
        cleanup_staging_area(owner);
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L145-154)
```text
        if (!exists<StagingArea>(owner_address)) {
            move_to(
                owner,
                StagingArea {
                    metadata_serialized: vector[],
                    code: smart_table::new(),
                    last_module_idx: 0
                }
            );
        };
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L158-160)
```text
        if (!vector::is_empty(&metadata_chunk)) {
            vector::append(&mut staging_area.metadata_serialized, metadata_chunk);
        };
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L167-170)
```text
            if (smart_table::contains(&staging_area.code, idx)) {
                vector::append(
                    smart_table::borrow_mut(&mut staging_area.code, idx), inner_code
                );
```

**File:** aptos-move/framework/aptos-experimental/sources/large_packages.move (L213-225)
```text
    inline fun assemble_module_code(staging_area: &mut StagingArea): vector<vector<u8>> {
        let last_module_idx = staging_area.last_module_idx;
        let code = vector[];
        let i = 0;
        while (i <= last_module_idx) {
            vector::push_back(
                &mut code,
                *smart_table::borrow(&staging_area.code, i)
            );
            i = i + 1;
        };
        code
    }
```

**File:** crates/aptos/src/move_tool/mod.rs (L1704-1714)
```rust
    if !is_staging_area_empty(txn_options, large_packages_module_address).await? {
        let message = format!(
            "The resource {}::large_packages::StagingArea under account {} is not empty.\
        \nThis may cause package publishing to fail if the data is unexpected. \
        \nUse the `aptos move clear-staging-area` command to clean up the `StagingArea` resource under the account.",
            large_packages_module_address, account_address,
        )
            .bold();
        println!("{}", message);
        prompt_yes_with_override("Do you want to proceed?", txn_options.prompt_options)?;
    }
```

**File:** aptos-move/framework/aptos-framework/sources/code.move (L256-259)
```text
    public entry fun publish_package_txn(owner: &signer, metadata_serialized: vector<u8>, code: vector<vector<u8>>)
    acquires PackageRegistry {
        publish_package(owner, util::from_bytes<PackageMetadata>(metadata_serialized), code)
    }
```

**File:** aptos-move/framework/src/chunked_publish.rs (L142-162)
```rust
fn large_packages_stage_code_chunk_and_publish_to_account(
    metadata_chunk: Vec<u8>,
    code_indices: Vec<u16>,
    code_chunks: Vec<Vec<u8>>,
    large_packages_module_address: AccountAddress,
) -> TransactionPayload {
    // TODO[Orderless]: Change this to payload v2 format.
    TransactionPayload::EntryFunction(EntryFunction::new(
        ModuleId::new(
            large_packages_module_address,
            ident_str!("large_packages").to_owned(),
        ),
        ident_str!("stage_code_chunk_and_publish_to_account").to_owned(),
        vec![],
        vec![
            bcs::to_bytes(&metadata_chunk).unwrap(),
            bcs::to_bytes(&code_indices).unwrap(),
            bcs::to_bytes(&code_chunks).unwrap(),
        ],
    ))
}
```
