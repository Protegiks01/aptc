# Audit Report

## Title
Memory Exhaustion via Unbounded Allocation in Backup Restore Before Cryptographic Verification

## Summary
The `read_record_bytes()` function allocates memory based on untrusted size values from backup files without validation or limits. Critically, this allocation occurs **before** cryptographic verification in all restore operations (transactions, state snapshots, epoch endings). An attacker hosting a malicious backup repository can craft files with inflated record sizes (e.g., 0x3FFFFFFF bytes â‰ˆ 1GB each) to exhaust node memory during restore, preventing node bootstrapping and causing denial of service.

## Finding Description

The vulnerability exists in the backup restore parsing logic where memory allocation precedes data authenticity verification. [1](#0-0) 

At line 54, `record_size` is read from the file as a u32 and cast to usize without validation. At line 60, `BytesMut::with_capacity(record_size)` directly allocates this amount of memory before any verification occurs.

**Attack Flow Across All Restore Types:**

**1. Transaction Restore:** [2](#0-1) 

Records are read and memory allocated (lines 105-137), then proof is loaded (line 147), and only then is verification performed (line 167).

**2. State Snapshot Restore:** [3](#0-2) 

At line 261, `read_record_bytes()` allocates memory for all state records before the proof is even loaded (line 192 in the caller).

**3. Epoch Ending Restore:** [4](#0-3) 

Records are read at line 167, with trusted waypoint verification only occurring afterward (lines 129-135 in the caller).

**Public Backup Repository Attack Vector:**

Aptos supports bootstrapping from public backup repositories: [5](#0-4) 

The documentation explicitly mentions "default data resource is hosted by AptosLabs" (line 18), confirming the ecosystem relies on public backup sources.

**Exploitation Scenario:**
1. Attacker creates malicious backup files where each record size prefix is 0x3FFFFFFF (1,073,741,823 bytes)
2. With 10 records per chunk, this attempts to allocate ~10GB immediately
3. Multiple chunks compound the allocation
4. Attacker hosts these on a public S3/GCS bucket or HTTP server
5. Operator runs: `aptos-debugger aptos-db restore bootstrap-db --command-adapter-config attacker.yaml`
6. Memory exhaustion occurs during record reading, **before** cryptographic verification can reject the malicious data
7. Node crashes, preventing bootstrapping

**Invariant Violation:**

This breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits." The restore operation performs unbounded memory allocation based on untrusted external input.

## Impact Explanation

**High Severity** per Aptos bug bounty criteria:
- **"API crashes"**: The restore CLI tool crashes due to memory exhaustion
- **"Validator node slowdowns"**: Prevents operators from bootstrapping validator nodes, impacting network participation

This is NOT a "network-level DoS" (which is out of scope). This is an **application-level resource exhaustion vulnerability** in file parsing, similar to classic billion laughs XML attacks. The vulnerability exploits missing input validation in a parser that processes potentially untrusted data.

**Impact Scope:**
- Prevents new nodes from joining the network via public backups
- Enables targeted DoS against specific operators
- Could be weaponized against network expansion during high-growth periods
- No node can successfully restore from the malicious backup source

## Likelihood Explanation

**Medium-High Likelihood:**

**Attacker Requirements:**
- Host malicious backup files (trivial - any S3/GCS bucket or HTTP server)
- Social engineer operators to use attacker's repository (realistic - operators seek "fast" backup sources)
- No validator privileges, credentials, or insider access required

**Realistic Attack Conditions:**
- Aptos ecosystem actively promotes public backup repositories for bootstrapping
- Operators frequently search for faster backup sources
- The docker-compose example shows this is a standard operational pattern
- Attackers could advertise malicious repositories on forums, Discord, documentation comments
- Even sophisticated operators may fall victim if the repository appears legitimate initially

**Execution Complexity:** Low - creating malicious backup files requires only writing size prefixes, no cryptographic operations needed.

## Recommendation

**Immediate Fix: Add Maximum Record Size Validation**

Add a constant maximum record size and validate before allocation:

```rust
// In read_record_bytes.rs
const MAX_RECORD_SIZE: usize = 128 * 1024 * 1024; // 128MB reasonable limit

async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
    let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
    // read record size
    let mut size_buf = BytesMut::with_capacity(4);
    self.read_full_buf_or_none(&mut size_buf).await?;
    if size_buf.is_empty() {
        return Ok(None);
    }

    // empty record
    let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
    if record_size == 0 {
        return Ok(Some(Bytes::new()));
    }
    
    // SECURITY: Validate record size to prevent memory exhaustion
    ensure!(
        record_size <= MAX_RECORD_SIZE,
        "Record size {} exceeds maximum allowed size {}",
        record_size,
        MAX_RECORD_SIZE
    );

    // read record
    let mut record_buf = BytesMut::with_capacity(record_size);
    self.read_full_buf_or_none(&mut record_buf).await?;
    if record_buf.is_empty() {
        bail!("Hit EOF when reading record.")
    }

    Ok(Some(record_buf.freeze()))
}
```

**Additional Hardening:**
1. Implement streaming verification where possible
2. Add cumulative memory tracking per chunk
3. Document trusted backup sources in official documentation
4. Add checksum verification of chunk files before processing records

## Proof of Concept

```rust
// Test demonstrating memory exhaustion vulnerability
// File: storage/backup/backup-cli/tests/memory_exhaustion_test.rs

use bytes::BytesMut;
use std::io::Write;
use tempfile::NamedTempFile;
use aptos_backup_cli::utils::read_record_bytes::ReadRecordBytes;

#[tokio::test]
async fn test_malicious_backup_memory_exhaustion() {
    // Create malicious backup file with inflated size prefix
    let mut file = NamedTempFile::new().unwrap();
    
    // Write 10 records, each claiming to be 1GB (0x3FFFFFFF bytes)
    for _ in 0..10 {
        let malicious_size: u32 = 0x3FFFFFFF; // ~1GB
        file.write_all(&malicious_size.to_be_bytes()).unwrap();
        
        // Write minimal actual data (verification will fail, but after allocation)
        file.write_all(&[0u8; 100]).unwrap();
    }
    
    file.flush().unwrap();
    
    // Attempt to read the malicious file
    let mut reader = tokio::fs::File::open(file.path()).await.unwrap();
    
    // This will attempt to allocate ~10GB before any verification
    // In production, this would exhaust memory and crash the node
    let result = reader.read_record_bytes().await;
    
    // Without the fix, this either:
    // 1. OOMs and crashes
    // 2. Allocates massive memory successfully (if enough RAM exists)
    // With the fix, this should fail with size validation error
    assert!(result.is_err(), "Should reject oversized records");
}

#[tokio::test] 
async fn test_legitimate_backup_still_works() {
    // Verify the fix doesn't break legitimate backups
    let mut file = NamedTempFile::new().unwrap();
    
    let data = b"legitimate backup data";
    let size = (data.len() as u32).to_be_bytes();
    
    file.write_all(&size).unwrap();
    file.write_all(data).unwrap();
    file.flush().unwrap();
    
    let mut reader = tokio::fs::File::open(file.path()).await.unwrap();
    let result = reader.read_record_bytes().await.unwrap().unwrap();
    
    assert_eq!(result.as_ref(), data);
}
```

**Notes**

This vulnerability is particularly dangerous because:

1. **Trusted Waypoints Don't Protect**: Even with `--trust-waypoint` configured, memory exhaustion occurs during the file reading phase, which happens **before** waypoint verification in the execution flow.

2. **All Restore Types Affected**: Transaction, state snapshot, and epoch ending restores all follow the same vulnerable pattern of read-then-verify rather than verify-then-read.

3. **Public Backup Infrastructure**: The Aptos ecosystem's reliance on public backup repositories for node bootstrapping creates a realistic attack surface. Operators routinely restore from semi-trusted sources.

4. **No Authentication Required**: Unlike attacks requiring validator compromise or stake manipulation, this attack only requires hosting malicious files and social engineering operators to use them.

5. **Defense-in-Depth Failure**: The cryptographic verification mechanisms (waypoints, signatures, proofs) are bypassed because resource exhaustion occurs earlier in the pipeline.

The fix must balance security (preventing exhaustion) with functionality (supporting legitimate large state snapshots). A 128MB per-record limit is reasonable given that legitimate records are typically much smaller, while still accommodating potential growth in state size.

### Citations

**File:** storage/backup/backup-cli/src/utils/read_record_bytes.rs (L44-67)
```rust
    async fn read_record_bytes(&mut self) -> Result<Option<Bytes>> {
        let _timer = BACKUP_TIMER.timer_with(&["read_record_bytes"]);
        // read record size
        let mut size_buf = BytesMut::with_capacity(4);
        self.read_full_buf_or_none(&mut size_buf).await?;
        if size_buf.is_empty() {
            return Ok(None);
        }

        // empty record
        let record_size = u32::from_be_bytes(size_buf.as_ref().try_into()?) as usize;
        if record_size == 0 {
            return Ok(Some(Bytes::new()));
        }

        // read record
        let mut record_buf = BytesMut::with_capacity(record_size);
        self.read_full_buf_or_none(&mut record_buf).await?;
        if record_buf.is_empty() {
            bail!("Hit EOF when reading record.")
        }

        Ok(Some(record_buf.freeze()))
    }
```

**File:** storage/backup/backup-cli/src/backup_types/transaction/restore.rs (L100-137)
```rust
    async fn load(
        manifest: TransactionChunk,
        storage: &Arc<dyn BackupStorage>,
        epoch_history: Option<&Arc<EpochHistory>>,
    ) -> Result<Self> {
        let mut file = BufReader::new(storage.open_for_read(&manifest.transactions).await?);
        let mut txns = Vec::new();
        let mut persisted_aux_info = Vec::new();
        let mut txn_infos = Vec::new();
        let mut event_vecs = Vec::new();
        let mut write_sets = Vec::new();

        while let Some(record_bytes) = file.read_record_bytes().await? {
            let (txn, aux_info, txn_info, events, write_set): (
                _,
                PersistedAuxiliaryInfo,
                _,
                _,
                WriteSet,
            ) = match manifest.format {
                TransactionChunkFormat::V0 => {
                    let (txn, txn_info, events, write_set) = bcs::from_bytes(&record_bytes)?;
                    (
                        txn,
                        PersistedAuxiliaryInfo::None,
                        txn_info,
                        events,
                        write_set,
                    )
                },
                TransactionChunkFormat::V1 => bcs::from_bytes(&record_bytes)?,
            };
            txns.push(txn);
            persisted_aux_info.push(aux_info);
            txn_infos.push(txn_info);
            event_vecs.push(events);
            write_sets.push(write_set);
        }
```

**File:** storage/backup/backup-cli/src/backup_types/state_snapshot/restore.rs (L253-266)
```rust
    async fn read_state_value(
        storage: &Arc<dyn BackupStorage>,
        file_handle: FileHandle,
    ) -> Result<Vec<(StateKey, StateValue)>> {
        let mut file = storage.open_for_read(&file_handle).await?;

        let mut chunk = vec![];

        while let Some(record_bytes) = file.read_record_bytes().await? {
            chunk.push(bcs::from_bytes(&record_bytes)?);
        }

        Ok(chunk)
    }
```

**File:** storage/backup/backup-cli/src/backup_types/epoch_ending/restore.rs (L160-172)
```rust
    async fn read_chunk(
        &self,
        file_handle: &FileHandleRef,
    ) -> Result<Vec<LedgerInfoWithSignatures>> {
        let mut file = self.storage.open_for_read(file_handle).await?;
        let mut chunk = vec![];

        while let Some(record_bytes) = file.read_record_bytes().await? {
            chunk.push(bcs::from_bytes(&record_bytes)?);
        }

        Ok(chunk)
    }
```

**File:** docker/compose/data-restore/docker-compose.yaml (L14-32)
```yaml
      # Depends on which cloud backup data you use, replace this with either:
      # `s3.yaml` (AWS S3)
      # `gcs.yaml` (GCP GCS)
      # You can update the yaml file to specify where you want to download data from,
      # default data resource is hosted by AptosLabs.
      - type: bind
        source: ./s3.yaml
        target: /opt/aptos/etc/restore.yaml
        read_only: true
    environment:
      - HOME=/tmp
      - RUST_LOG=debug
    command: >
      sh -c "
      /usr/local/bin/aptos-debugger aptos-db restore bootstrap-db --concurrent-downloads 2 \
        --target-db-dir /opt/aptos/data/db
        --metadata-cache-dir /tmp/aptos-restore-metadata \
        --command-adapter-config /opt/aptos/etc/restore.yaml
      "
```
