# Audit Report

## Title
Memory Exhaustion via BatchInfo Metadata Spoofing in Inline Transaction Validation

## Summary
A Byzantine validator can exploit a validation gap in the consensus layer to cause memory exhaustion on victim validators. The vulnerability arises because payload size validation trusts the `BatchInfo.num_bytes` metadata field, while actual memory consumption during transaction processing uses the real transaction data, allowing significant amplification attacks that bypass configured resource limits.

## Finding Description

The vulnerability exists in the `QuorumStoreInlineHybrid` payload handling where size validation and memory consumption operate on different data sources.

**Size Validation Using Metadata:**
The `Payload::size()` method calculates payload size by summing `batch_info.num_bytes()` from metadata rather than computing actual transaction sizes: [1](#0-0) 

This contrasts with `DirectMempool` payloads which correctly compute actual transaction sizes by iterating through transactions: [2](#0-1) 

**Block Size Validation:**
The block size is validated against `max_receiving_block_bytes` using this metadata-based size calculation: [3](#0-2) 

**Insufficient Verification:**
The `verify_inline_batches()` function only validates that transaction digests match, without verifying that `num_bytes` matches actual transaction size: [4](#0-3) 

**Memory Consumption Through Transaction Cloning:**
During block processing, transactions are cloned multiple times using the actual transaction data:

First clone occurs during transaction filtering: [5](#0-4) 

Second clone occurs during transaction extraction for execution: [6](#0-5) 

**Attack Scenario:**
1. Byzantine validator constructs a `QuorumStoreInlineHybrid` payload with inline batches containing:
   - `BatchInfo.num_bytes = 100,000` (100 KB in metadata)
   - `Vec<SignedTransaction>` with actual transactions totaling 10 MB
   - Correct `BatchInfo.digest` (hash matches the transactions)

2. Victim validator receives the proposal:
   - `payload.size()` returns 100 KB based on metadata
   - Size check passes (100 KB < 6 MB default `max_receiving_block_bytes` limit)
   - `verify_inline_batches()` passes (digest is valid)

3. During processing:
   - `check_denied_inline_transactions()` clones 10 MB of actual transactions
   - `get_transactions_quorum_store_inline_hybrid()` clones another 10 MB
   - Total: 20 MB consumed despite 100 KB passing validation

4. With default max transaction size of 64 KB (or 1 MB for governance), attackers can achieve 100x+ amplification factors.

## Impact Explanation

**Severity: High**

This vulnerability enables resource exhaustion attacks that align with the **"Validator Node Slowdowns (High)"** category in the Aptos bug bounty program. The impact includes:

- **Performance Degradation**: Excessive memory consumption degrades validator performance, affecting consensus participation and block processing speed
- **Potential OOM Crashes**: Severe memory exhaustion can trigger out-of-memory conditions, crashing validator nodes
- **Consensus Liveness Impact**: Affected validators may fail to vote or process blocks, reducing network liveness when multiple validators are targeted

The vulnerability is bounded by the BFT assumption (requires < 1/3 Byzantine validators) and does not directly cause fund loss or consensus safety violations. However, it fundamentally breaks the resource limit invariants that are critical for BFT system stability.

This is NOT a "Network DoS attack" (which is out of scope) but rather a protocol-level validation bug that allows configured resource limits to be bypassed through metadata spoofing.

## Likelihood Explanation

**Likelihood: High**

The attack is highly realistic because:
- Byzantine validators are explicitly part of the AptosBFT threat model (system tolerates up to 1/3 Byzantine validators)
- Creating malicious payloads requires only manipulating the `num_bytes` field in `BatchInfo` during block proposal construction
- No cryptographic bypasses or signature forgeries are needed - the digest verification passes correctly
- The vulnerability exists in production consensus code with no compensating controls
- Attack is silent and difficult to detect until memory exhaustion manifests
- No validation exists to detect metadata/actual size mismatches

Validator rotation and decentralization mean compromised or malicious validators are within the assumed threat model.

## Recommendation

Add validation to verify that `BatchInfo.num_bytes` matches the actual serialized size of transactions in inline batches:

```rust
pub fn verify_inline_batches<'a, T: TBatchInfo + 'a>(
    inline_batches: impl Iterator<Item = (&'a T, &'a Vec<SignedTransaction>)>,
) -> anyhow::Result<()> {
    for (batch, payload) in inline_batches {
        let computed_digest = BatchPayload::new(batch.author(), payload.clone()).hash();
        ensure!(
            computed_digest == *batch.digest(),
            "Hash of the received inline batch doesn't match the digest value"
        );
        
        // ADD THIS VALIDATION:
        let actual_num_bytes = payload
            .par_iter()
            .with_min_len(100)
            .map(|txn| txn.raw_txn_bytes_len())
            .sum::<usize>();
        ensure!(
            actual_num_bytes == batch.num_bytes() as usize,
            "BatchInfo.num_bytes {} doesn't match actual transaction size {} for batch {:?}",
            batch.num_bytes(),
            actual_num_bytes,
            batch.digest()
        );
    }
    Ok(())
}
```

This ensures that metadata accurately represents the actual payload size before allowing the block to proceed through validation.

## Proof of Concept

The vulnerability can be demonstrated by constructing a test that:
1. Creates a `BatchInfo` with artificially low `num_bytes` value
2. Pairs it with a `Vec<SignedTransaction>` containing larger actual transactions
3. Constructs a `QuorumStoreInlineHybrid` payload with correct digest
4. Observes that `payload.size()` returns the low metadata value
5. Observes that transaction cloning operations consume memory based on actual transaction sizes

The lack of validation between metadata and actual data is confirmed by the code citations provided above, demonstrating that no such check exists in the current codebase.

---

**Notes:**
- Configuration: Default `max_receiving_block_bytes` is 6 MB [7](#0-6) 
- Default max transaction size is 64 KB, with governance transactions up to 1 MB [8](#0-7) 
- Block proposals are serialized and transmitted over the network, allowing Byzantine validators to craft arbitrary payloads [9](#0-8)

### Citations

**File:** consensus/consensus-types/src/common.rs (L496-500)
```rust
            Payload::DirectMempool(txns) => txns
                .par_iter()
                .with_min_len(100)
                .map(|txn| txn.raw_txn_bytes_len())
                .sum(),
```

**File:** consensus/consensus-types/src/common.rs (L505-512)
```rust
            Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _)
            | Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _) => {
                proof_with_data.num_bytes()
                    + inline_batches
                        .iter()
                        .map(|(batch_info, _)| batch_info.num_bytes() as usize)
                        .sum::<usize>()
            },
```

**File:** consensus/consensus-types/src/common.rs (L541-556)
```rust
    pub fn verify_inline_batches<'a, T: TBatchInfo + 'a>(
        inline_batches: impl Iterator<Item = (&'a T, &'a Vec<SignedTransaction>)>,
    ) -> anyhow::Result<()> {
        for (batch, payload) in inline_batches {
            // TODO: Can cloning be avoided here?
            let computed_digest = BatchPayload::new(batch.author(), payload.clone()).hash();
            ensure!(
                computed_digest == *batch.digest(),
                "Hash of the received inline batch doesn't match the digest value for batch {:?}: {} != {}",
                batch,
                computed_digest,
                batch.digest()
            );
        }
        Ok(())
    }
```

**File:** consensus/src/round_manager.rs (L1179-1193)
```rust
        let payload_size = proposal.payload().map_or(0, |payload| payload.size());
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L142-149)
```rust
            all_txns.append(
                &mut inline_batches
                    .iter()
                    // TODO: Can clone be avoided here?
                    .flat_map(|(_batch_info, txns)| txns.clone())
                    .collect(),
            );
            all_txns
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L576-581)
```rust
        Payload::QuorumStoreInlineHybrid(inline_batches, ..) => {
            // Flatten the inline batches and return the transactions
            inline_batches
                .iter()
                .flat_map(|(_batch_info, txns)| txns.clone())
                .collect()
```

**File:** config/src/config/consensus_config.rs (L231-231)
```rust
            max_receiving_block_bytes: 6 * 1024 * 1024, // 6MB
```

**File:** aptos-move/aptos-gas-schedule/src/gas_schedule/transaction.rs (L73-80)
```rust
            max_transaction_size_in_bytes: NumBytes,
            "max_transaction_size_in_bytes",
            64 * 1024
        ],
        [
            max_transaction_size_in_bytes_gov: NumBytes,
            { RELEASE_V1_13.. => "max_transaction_size_in_bytes.gov" },
            1024 * 1024
```

**File:** consensus/consensus-types/src/proposal_msg.rs (L13-25)
```rust
#[derive(Clone, Debug, Deserialize, Eq, PartialEq, Serialize)]
pub struct ProposalMsg {
    proposal: Block,
    sync_info: SyncInfo,
}

impl ProposalMsg {
    /// Creates a new proposal.
    pub fn new(proposal: Block, sync_info: SyncInfo) -> Self {
        Self {
            proposal,
            sync_info,
        }
```
