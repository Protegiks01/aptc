# Audit Report

## Title
DKG Deadlock Due to Unchecked `randomness_override_seq_num` Divergence Across Validators

## Summary
The DKG (Distributed Key Generation) initialization process does not validate that critical configuration parameters are consistent across validators. Specifically, divergent `randomness_override_seq_num` values can cause a subset of validators to opt out of DKG participation, preventing the remaining validators from reaching the 2/3+ quorum required for completion. This results in indefinite DKG retries and blocks epoch transitions until manual intervention via `force_end_epoch()`.

## Finding Description

When validators start a new epoch, each independently evaluates whether to participate in DKG based on their local `randomness_override_seq_num` configuration parameter: [1](#0-0) 

The `OnChainRandomnessConfig::from_configs` method performs a comparison: if the local override sequence number exceeds the on-chain value, randomness is force-disabled for that validator: [2](#0-1) [3](#0-2) 

Validators with `randomness_enabled = false` do not spawn a DKGManager and do not participate in the DKG protocol: [4](#0-3) 

Meanwhile, participating validators use ReliableBroadcast to collect transcripts and require 2/3+ voting power to reach quorum: [5](#0-4) 

**The Critical Flaw**: There is no cross-validator validation ensuring all validators have consistent `randomness_override_seq_num` values. The system only logs a warning locally but does not prevent or detect divergence: [6](#0-5) 

**Attack Scenario (Honest Misconfiguration)**:
1. On-chain `RandomnessConfigSeqNum.seq_num = 5`
2. During an emergency recovery, Validator Operators A and B set `randomness_override_seq_num = 6` to force-disable randomness
3. Validator Operators C and D leave it at default (0) or set to a different value
4. At epoch transition, A and B skip DKG participation while C and D participate
5. If A+B control â‰¥34% voting power, C+D cannot reach 2/3 quorum
6. DKG never completes, blocking epoch transition indefinitely [7](#0-6) 

The epoch transition remains blocked because `try_start()` detects an incomplete session for the current epoch and returns early. The only recovery path is manual intervention via `force_end_epoch()`: [8](#0-7) 

## Impact Explanation

This vulnerability qualifies as **High Severity** per Aptos bug bounty criteria:

1. **Validator node slowdowns**: Participating validators continuously retry DKG with exponential backoff, consuming CPU and network resources indefinitely.

2. **Significant protocol violations**: Epoch transitions are blocked, preventing:
   - Validator set updates
   - Governance proposal execution
   - On-chain configuration changes
   - Staking reward distribution

3. **Requires manual intervention**: Recovery requires calling `force_end_epoch()`, which is documented as "WARNING: currently only used by tests," indicating it's not part of normal operational procedures.

The impact extends beyond individual validators to the entire network's ability to progress epochs and execute governance decisions.

## Likelihood Explanation

**Likelihood: Medium to High**

This scenario is realistic because:

1. **Emergency Recovery Procedures**: The `randomness_override_seq_num` parameter is specifically designed for emergency recovery from randomness stalls. During such emergencies, validator operators may act independently without perfect coordination.

2. **Decentralized Operations**: Validator operators are independent entities. Configuration changes are not automatically synchronized across the network.

3. **Lack of Validation**: The system provides no warnings, checks, or coordination mechanisms to detect divergent configurations before they cause deadlock.

4. **Documentation Gaps**: The randomness recovery procedure documentation does not emphasize the critical requirement for coordinated configuration changes across all validators.

5. **Testing Gaps**: The test suite includes `randomness_stall_recovery.rs` but does not test the divergent configuration scenario across multiple validators.

## Recommendation

Implement validation to detect and prevent parameter divergence:

**Option 1: On-Chain Coordination**
Extend the DKG start event to include a hash of critical parameters. Validators verify this hash matches their local configuration before participating:

```rust
// In dkg/src/epoch_manager.rs, add validation:
let expected_override = dkg_session_metadata.randomness_override_seq_num;
if self.randomness_override_seq_num != expected_override {
    error!(
        "Randomness override mismatch: local={}, expected={}. Refusing to participate in DKG.",
        self.randomness_override_seq_num, expected_override
    );
    // Emit metric for monitoring
    // Do not spawn DKGManager
    return Ok(());
}
```

**Option 2: Consensus-Level Check**
Add a pre-DKG validation phase where validators broadcast their intended configuration and abort if >1/3 voting power has divergent values.

**Option 3: Stricter Operational Procedures**
Document that `randomness_override_seq_num` changes MUST be coordinated across all validators before epoch transitions, and add monitoring alerts when values diverge.

**Immediate Mitigation**:
- Add metrics/alerts when `randomness_override_seq_num > onchain_seq_num`
- Document the coordination requirement in operator runbooks
- Enhance `force_end_epoch()` to be a supported recovery mechanism with proper safeguards

## Proof of Concept

**Reproduction Steps**:

1. Configure a 4-validator testnet with equal voting power (25% each)
2. Set on-chain `RandomnessConfigSeqNum.seq_num = 5`
3. Enable randomness features (`RECONFIGURE_WITH_DKG`)
4. Configure validators:
   - Validators 1-2: `randomness_override_seq_num = 6` (force-disabled)
   - Validators 3-4: `randomness_override_seq_num = 0` (enabled)
5. Trigger epoch transition via governance proposal
6. Observe:
   - Validators 1-2 log: "Randomness will be force-disabled by local config!"
   - Validators 3-4 spawn DKGManager and start broadcasting transcripts
   - Validators 3-4 have 50% voting power, cannot reach 66.67% quorum
   - ReliableBroadcast continues retrying indefinitely
   - `reconfiguration_with_dkg::try_start()` returns early on subsequent calls
   - Epoch transition never completes

**Expected Behavior**: System should detect parameter divergence and either:
- Reject DKG start if coordination is impossible
- Fall back to epoch transition without randomness
- Provide clear operator guidance on resolution

**Actual Behavior**: Indefinite deadlock requiring manual `force_end_epoch()` call.

## Notes

The `rb_config` parameter also lacks cross-validator validation, but its impact is less severe. Divergent ReliableBroadcast configurations (RPC timeouts, backoff policies) cause asymmetric retry behavior and potential performance degradation, but do not prevent eventual DKG completion since validators retry indefinitely. The critical issue is `randomness_override_seq_num` causing validators to opt out entirely.

This vulnerability demonstrates a coordination failure in distributed system design: validators make independent local decisions based on configuration without verifying those decisions are consistent network-wide.

### Citations

**File:** dkg/src/epoch_manager.rs (L176-190)
```rust
        info!(
            epoch = epoch_state.epoch,
            local = self.randomness_override_seq_num,
            onchain = onchain_randomness_config_seq_num.seq_num,
            "Checking randomness config override."
        );
        if self.randomness_override_seq_num > onchain_randomness_config_seq_num.seq_num {
            warn!("Randomness will be force-disabled by local config!");
        }

        let onchain_randomness_config = OnChainRandomnessConfig::from_configs(
            self.randomness_override_seq_num,
            onchain_randomness_config_seq_num.seq_num,
            randomness_config_move_struct.ok(),
        );
```

**File:** dkg/src/epoch_manager.rs (L199-259)
```rust
        let randomness_enabled =
            consensus_config.is_vtxn_enabled() && onchain_randomness_config.randomness_enabled();
        if let (true, Some(my_index)) = (randomness_enabled, my_index) {
            let DKGState {
                in_progress: in_progress_session,
                ..
            } = payload.get::<DKGState>().unwrap_or_default();

            let network_sender = self.create_network_sender();
            let rb = ReliableBroadcast::new(
                self.my_addr,
                epoch_state.verifier.get_ordered_account_addresses(),
                Arc::new(network_sender),
                ExponentialBackoff::from_millis(self.rb_config.backoff_policy_base_ms)
                    .factor(self.rb_config.backoff_policy_factor)
                    .max_delay(Duration::from_millis(
                        self.rb_config.backoff_policy_max_delay_ms,
                    )),
                aptos_time_service::TimeService::real(),
                Duration::from_millis(self.rb_config.rpc_timeout_ms),
                BoundedExecutor::new(8, tokio::runtime::Handle::current()),
            );
            let agg_trx_producer = AggTranscriptProducer::new(rb);

            let (dkg_start_event_tx, dkg_start_event_rx) =
                aptos_channel::new(QueueStyle::KLAST, 1, None);
            self.dkg_start_event_tx = Some(dkg_start_event_tx);

            let (dkg_rpc_msg_tx, dkg_rpc_msg_rx) = aptos_channel::new::<
                AccountAddress,
                (AccountAddress, IncomingRpcRequest),
            >(QueueStyle::FIFO, 100, None);
            self.dkg_rpc_msg_tx = Some(dkg_rpc_msg_tx);
            let (dkg_manager_close_tx, dkg_manager_close_rx) = oneshot::channel();
            self.dkg_manager_close_tx = Some(dkg_manager_close_tx);
            let my_pk = epoch_state
                .verifier
                .get_public_key(&self.my_addr)
                .ok_or_else(|| anyhow!("my pk not found in validator set"))?;
            let dealer_sk = self
                .key_storage
                .consensus_sk_by_pk(my_pk.clone())
                .map_err(|e| {
                    anyhow!("dkg new epoch handling failed with consensus sk lookup err: {e}")
                })?;
            let dkg_manager = DKGManager::<DefaultDKG>::new(
                Arc::new(dealer_sk),
                Arc::new(my_pk),
                my_index,
                self.my_addr,
                epoch_state,
                Arc::new(agg_trx_producer),
                self.vtxn_pool.clone(),
            );
            tokio::spawn(dkg_manager.run(
                in_progress_session,
                dkg_start_event_rx,
                dkg_rpc_msg_rx,
                dkg_manager_close_rx,
            ));
        };
```

**File:** types/src/on_chain_config/randomness_config.rs (L138-151)
```rust
    /// Used by DKG and Consensus on a new epoch to determine the actual `OnChainRandomnessConfig` to be used.
    pub fn from_configs(
        local_seqnum: u64,
        onchain_seqnum: u64,
        onchain_raw_config: Option<RandomnessConfigMoveStruct>,
    ) -> Self {
        if local_seqnum > onchain_seqnum {
            Self::default_disabled()
        } else {
            onchain_raw_config
                .and_then(|onchain_raw| OnChainRandomnessConfig::try_from(onchain_raw).ok())
                .unwrap_or_else(OnChainRandomnessConfig::default_if_missing)
        }
    }
```

**File:** types/src/on_chain_config/randomness_config.rs (L193-211)
```rust
    pub fn default_disabled() -> Self {
        OnChainRandomnessConfig::Off
    }

    pub fn default_if_missing() -> Self {
        OnChainRandomnessConfig::Off
    }

    pub fn default_for_genesis() -> Self {
        OnChainRandomnessConfig::V2(ConfigV2::default())
    }

    pub fn randomness_enabled(&self) -> bool {
        match self {
            OnChainRandomnessConfig::Off => false,
            OnChainRandomnessConfig::V1(_) => true,
            OnChainRandomnessConfig::V2(_) => true,
        }
    }
```

**File:** dkg/src/transcript_aggregation/mod.rs (L122-152)
```rust
        let threshold = self.epoch_state.verifier.quorum_voting_power();
        let power_check_result = self
            .epoch_state
            .verifier
            .check_voting_power(trx_aggregator.contributors.iter(), true);
        let new_total_power = match &power_check_result {
            Ok(x) => Some(*x),
            Err(VerifyError::TooLittleVotingPower { voting_power, .. }) => Some(*voting_power),
            _ => None,
        };
        let maybe_aggregated = power_check_result
            .ok()
            .map(|_| trx_aggregator.trx.clone().unwrap());
        info!(
            epoch = self.epoch_state.epoch,
            peer = sender,
            is_self = is_self,
            peer_power = peer_power,
            new_total_power = new_total_power,
            threshold = threshold,
            threshold_exceeded = maybe_aggregated.is_some(),
            "[DKG] added transcript from validator {}, {} out of {} aggregated.",
            self.epoch_state
                .verifier
                .address_to_validator_index()
                .get(&sender)
                .unwrap(),
            new_total_power.unwrap_or(0),
            threshold
        );
        Ok(maybe_aggregated)
```

**File:** aptos-move/framework/aptos-framework/sources/reconfiguration_with_dkg.move (L24-40)
```text
    public(friend) fun try_start() {
        let incomplete_dkg_session = dkg::incomplete_session();
        if (option::is_some(&incomplete_dkg_session)) {
            let session = option::borrow(&incomplete_dkg_session);
            if (dkg::session_dealer_epoch(session) == reconfiguration::current_epoch()) {
                return
            }
        };
        reconfiguration_state::on_reconfig_start();
        let cur_epoch = reconfiguration::current_epoch();
        dkg::start(
            cur_epoch,
            randomness_config::current(),
            stake::cur_validator_consensus_infos(),
            stake::next_validator_consensus_infos(),
        );
    }
```

**File:** aptos-move/framework/aptos-framework/sources/aptos_governance.move (L694-703)
```text
    /// Change epoch immediately.
    /// If `RECONFIGURE_WITH_DKG` is enabled and we are in the middle of a DKG,
    /// stop waiting for DKG and enter the new epoch without randomness.
    ///
    /// WARNING: currently only used by tests. In most cases you should use `reconfigure()` instead.
    /// TODO: migrate these tests to be aware of async reconfiguration.
    public entry fun force_end_epoch(aptos_framework: &signer) {
        system_addresses::assert_aptos_framework(aptos_framework);
        reconfiguration_with_dkg::finish(aptos_framework);
    }
```
