# Audit Report

## Title
State Snapshot Restore Progress Markers Not Cleaned Up on Failure, Leading to Potential Merkle Tree Corruption on Retry with Different Backup Source

## Summary
When `run_impl()` in the replay verify coordinator fails during state snapshot restoration, it leaves `StateSnapshotKvRestoreProgress` metadata in the database without cleanup. If a subsequent run uses a different backup source with a different state root hash at the same version, the system attempts to resume the incomplete restoration, resulting in a corrupted Jellyfish Merkle tree that mixes nodes from two different state roots.

## Finding Description

The replay verify coordinator's `run_impl()` function performs state snapshot and transaction restoration without any error cleanup logic. When a failure occurs during state snapshot restoration, the following partial state remains: [1](#0-0) 

The function catches errors but provides no rollback mechanism. During state snapshot restoration, progress is tracked via `StateSnapshotKvRestoreProgress` metadata: [2](#0-1) 

When restoration completes successfully, the `kv_finish()` method is called but **does not delete the progress marker**: [3](#0-2) [4](#0-3) 

On a subsequent run, the coordinator checks for in-progress restorations: [5](#0-4) 

**The Attack Scenario:**

1. **First run** with backup source A:
   - State snapshot at version 100 with root hash H1 begins restoration
   - Partial Jellyfish Merkle tree nodes for H1 written to database
   - `StateSnapshotKvRestoreProgress(100)` metadata written
   - Restoration fails (network error, corrupted chunk, etc.)
   - **No cleanup occurs** - partial state remains

2. **Second run** with backup source B:
   - `get_in_progress_state_kv_snapshot()` finds version 100 marker
   - `expect_state_snapshot(100)` retrieves snapshot from backup B with root hash H2
   - `JellyfishMerkleRestore::new()` is called with `expected_root_hash = H2` [6](#0-5) 

The JMT restoration attempts to recover partial nodes (from H1) and continue with chunks expecting H2, resulting in a corrupted tree where the lower levels expect H1 but upper levels expect H2.

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs."

## Impact Explanation

This is a **Medium Severity** issue (state inconsistencies requiring intervention) because:

1. **State Corruption**: Results in an invalid Jellyfish Merkle tree where nodes from different expected root hashes are mixed
2. **Verification Failure**: The corrupted tree will fail root hash verification, preventing the database from being usable
3. **Manual Intervention Required**: Operators must manually clear the database and restart restoration from scratch
4. **Limited Scope**: Only affects backup/restore operations via backup-cli, not live consensus or validator operation

While this doesn't directly compromise live validator consensus, it prevents successful database recovery from backups, which is critical for disaster recovery and new node bootstrapping.

## Likelihood Explanation

**Likelihood: Medium-Low**

This requires:
- Operator-level access to run the backup-cli replay-verify tool
- A failed restoration attempt (network issues, corrupted backup data, resource exhaustion)
- A subsequent retry with a different backup source pointing to the same version
- The different backup source having a different root hash at that version (indicating chain divergence, backup corruption, or malicious backup server)

While individual conditions are not rare, their combination is unusual in normal operation. However, in disaster recovery scenarios where operators may try multiple backup sources, this becomes more likely.

## Recommendation

Implement cleanup logic in the error path and ensure progress markers are deleted upon successful completion:

**Option 1: Delete progress markers on finish**
Modify `StateStore::kv_finish()` to delete the `StateSnapshotKvRestoreProgress` metadata after successful completion:

```rust
fn kv_finish(&self, version: Version, usage: StateStorageUsage) -> Result<()> {
    self.ledger_db.metadata_db().put_usage(version, usage)?;
    
    // Delete the progress marker after successful completion
    let mut batch = SchemaBatch::new();
    batch.delete::<DbMetadataSchema>(&DbMetadataKey::StateSnapshotKvRestoreProgress(version))?;
    self.state_kv_db.metadata_db().write_schemas(batch)?;
    
    // ... rest of the indexer DB logic
    Ok(())
}
```

**Option 2: Validate root hash on resume**
Store the expected root hash in the progress metadata and validate it when resuming:

```rust
// In StateSnapshotProgress, add expected_root_hash field
pub struct StateSnapshotProgress {
    pub key_hash: HashValue,
    pub usage: StateStorageUsage,
    pub expected_root_hash: HashValue, // Add this
}

// In JellyfishMerkleRestore::new(), validate on resume:
if let Some((node_key, leaf_node)) = tree_reader.get_rightmost_leaf(version)? {
    // Verify the progress marker's expected root hash matches
    let progress = /* get from metadata */;
    ensure!(
        progress.expected_root_hash == expected_root_hash,
        "Cannot resume: expected root hash changed from {} to {}",
        progress.expected_root_hash,
        expected_root_hash
    );
    // ... continue recovery
}
```

**Option 3: Add explicit cleanup in coordinator**
Add a cleanup method that gets called on errors:

```rust
impl ReplayVerifyCoordinator {
    async fn run_impl(self) -> Result<(), ReplayError> {
        // ... existing logic
        
        // Wrap in a result that cleans up on error
        let result = self.run_impl_inner().await;
        if result.is_err() {
            // Clean up progress markers
            if let RestoreRunMode::Restore { restore_handler } = &*self.restore_handler {
                restore_handler.cleanup_in_progress_markers()?;
            }
        }
        result
    }
}
```

## Proof of Concept

```rust
// Test demonstrating the issue (requires integration test setup)
#[tokio::test]
async fn test_failed_restore_leaves_stale_markers() {
    // Setup: Create two backup sources with different root hashes at version 100
    let db = create_test_db();
    let backup_a = create_backup_with_root_hash("hash_A", 100);
    let backup_b = create_backup_with_root_hash("hash_B", 100);
    
    // First run with backup A - fail partway through
    let coordinator_a = ReplayVerifyCoordinator::new(
        Arc::new(backup_a),
        /* ... other params */
        100, // end_version
    )?;
    
    // Simulate failure during state snapshot restore
    let result = coordinator_a.run().await;
    assert!(result.is_err());
    
    // Verify progress marker exists
    let progress = db.get::<DbMetadataSchema>(
        &DbMetadataKey::StateSnapshotKvRestoreProgress(100)
    )?;
    assert!(progress.is_some()); // Progress marker left behind!
    
    // Second run with backup B - attempts to resume
    let coordinator_b = ReplayVerifyCoordinator::new(
        Arc::new(backup_b), // Different backup source
        /* ... other params */
        100,
    )?;
    
    let result = coordinator_b.run().await;
    // This should fail, but currently attempts to merge incompatible states
    
    // Verify tree is now corrupted
    let root_node = db.get_node(&NodeKey::new_empty_path(100))?;
    // Root hash will be neither hash_A nor hash_B - corrupted state!
}
```

## Notes

This vulnerability specifically affects the backup-cli tool's replay-verify functionality, not the core validator consensus path. However, it represents a significant operational risk for node operators performing database recovery, as it can result in an unusable database state that requires complete re-initialization. The lack of atomic restore operations and cleanup logic violates the state consistency invariant and creates potential for merkle tree corruption when backup sources change between retry attempts.

### Citations

**File:** storage/backup/backup-cli/src/coordinators/replay_verify.rs (L84-98)
```rust
    pub async fn run(self) -> Result<(), ReplayError> {
        info!("ReplayVerify coordinator started.");
        let ret = self.run_impl().await;

        if let Err(e) = &ret {
            error!(
                error = ?e,
                "ReplayVerify coordinator failed."
            );
        } else {
            info!("ReplayVerify coordinator exiting with success.");
        }

        ret
    }
```

**File:** storage/backup/backup-cli/src/coordinators/replay_verify.rs (L121-142)
```rust
        let (state_snapshot, snapshot_version) = if let Some(version) =
            run_mode.get_in_progress_state_kv_snapshot()?
        {
            info!(
                version = version,
                "Found in progress state snapshot restore",
            );
            (
                Some(metadata_view.expect_state_snapshot(version)?),
                Some(version),
            )
        } else if let Some(snapshot) = metadata_view.select_state_snapshot(self.start_version)? {
            let snapshot_version = snapshot.version;
            info!(
                "Found state snapshot backup at epoch {}, will replay from version {}.",
                snapshot.epoch,
                snapshot_version + 1
            );
            (Some(snapshot), Some(snapshot_version))
        } else {
            (None, None)
        };
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1244-1279)
```rust
    fn write_kv_batch(
        &self,
        version: Version,
        node_batch: &StateValueBatch,
        progress: StateSnapshotProgress,
    ) -> Result<()> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_value_writer_write_chunk"]);
        let mut batch = SchemaBatch::new();
        let mut sharded_schema_batch = self.state_kv_db.new_sharded_native_batches();

        batch.put::<DbMetadataSchema>(
            &DbMetadataKey::StateSnapshotKvRestoreProgress(version),
            &DbMetadataValue::StateSnapshotProgress(progress),
        )?;

        if self.internal_indexer_db.is_some()
            && self
                .internal_indexer_db
                .as_ref()
                .unwrap()
                .statekeys_enabled()
        {
            let keys = node_batch.keys().map(|key| key.0.clone()).collect();
            self.internal_indexer_db
                .as_ref()
                .unwrap()
                .write_keys_to_indexer_db(&keys, version, progress)?;
        }
        self.shard_state_value_batch(
            &mut sharded_schema_batch,
            node_batch,
            self.state_kv_db.enabled_sharding(),
        )?;
        self.state_kv_db
            .commit(version, Some(batch), sharded_schema_batch)
    }
```

**File:** storage/aptosdb/src/state_store/mod.rs (L1281-1314)
```rust
    fn kv_finish(&self, version: Version, usage: StateStorageUsage) -> Result<()> {
        self.ledger_db.metadata_db().put_usage(version, usage)?;
        if let Some(internal_indexer_db) = self.internal_indexer_db.as_ref() {
            if version > 0 {
                let mut batch = SchemaBatch::new();
                batch.put::<InternalIndexerMetadataSchema>(
                    &MetadataKey::LatestVersion,
                    &MetadataValue::Version(version - 1),
                )?;
                if internal_indexer_db.statekeys_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::StateVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                if internal_indexer_db.transaction_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::TransactionVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                if internal_indexer_db.event_enabled() {
                    batch.put::<InternalIndexerMetadataSchema>(
                        &MetadataKey::EventVersion,
                        &MetadataValue::Version(version - 1),
                    )?;
                }
                internal_indexer_db
                    .get_inner_db_ref()
                    .write_schemas(batch)?;
            }
        }

        Ok(())
```

**File:** storage/aptosdb/src/state_restore/mod.rs (L129-135)
```rust
    pub fn finish(self) -> Result<()> {
        let progress = self.db.get_progress(self.version)?;
        self.db.kv_finish(
            self.version,
            progress.map_or(StateStorageUsage::zero(), |p| p.usage),
        )
    }
```

**File:** storage/jellyfish-merkle/src/restore/mod.rs (L189-221)
```rust
    pub fn new<D: 'static + TreeReader<K> + TreeWriter<K>>(
        store: Arc<D>,
        version: Version,
        expected_root_hash: HashValue,
        async_commit: bool,
    ) -> Result<Self> {
        let tree_reader = Arc::clone(&store);
        let (finished, partial_nodes, previous_leaf) = if let Some(root_node) =
            tree_reader.get_node_option(&NodeKey::new_empty_path(version), "restore")?
        {
            info!("Previous restore is complete, checking root hash.");
            ensure!(
                root_node.hash() == expected_root_hash,
                "Previous completed restore has root hash {}, expecting {}",
                root_node.hash(),
                expected_root_hash,
            );
            (true, vec![], None)
        } else if let Some((node_key, leaf_node)) = tree_reader.get_rightmost_leaf(version)? {
            // If the system crashed in the middle of the previous restoration attempt, we need
            // to recover the partial nodes to the state right before the crash.
            (
                false,
                Self::recover_partial_nodes(tree_reader.as_ref(), version, node_key)?,
                Some(leaf_node),
            )
        } else {
            (
                false,
                vec![InternalInfo::new_empty(NodeKey::new_empty_path(version))],
                None,
            )
        };
```
