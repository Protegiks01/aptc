# Audit Report

## Title
Indexer Cache Worker Panic-Restart Loop Due to Missing Redis Connection Backoff

## Summary
The indexer-grpc-cache-worker lacks exponential backoff for Redis connection failures, causing immediate panic and process termination when Redis is unavailable. Combined with automatic restart policies in deployment configurations, this creates an infinite fast panic-restart loop that exhausts system resources and prevents the indexer service from recovering.

## Finding Description

The cache worker's connection handling violates the principle of graceful degradation under transient failures. The vulnerability manifests through three interconnected code paths:

**1. Redis Connection Without Backoff** [1](#0-0) 

The worker attempts Redis connection in an infinite loop without any retry logic or exponential backoff. When `get_tokio_connection_manager()` fails, the error immediately propagates via the `?` operator.

**2. Panic on Connection Failure** [2](#0-1) 

The `worker.run()` result is unwrapped with `.expect()`, converting connection failures into panics that terminate the process.

**3. Panic Handler Exits Process** [3](#0-2) 

The panic handler logs crash information and immediately exits with code 12, ensuring no recovery.

**4. Automatic Restart Policy** [4](#0-3) 

The deployment configuration specifies `restart: unless-stopped`, causing Docker to immediately restart the container after crash.

**Attack Scenario:**
When Redis becomes temporarily unavailable (network partition, service restart, resource exhaustion, configuration error), the following loop executes:

1. Worker starts and enters `worker.run()` infinite loop
2. Attempts Redis connection on line 114 without backoff
3. Connection fails immediately
4. Error propagates to `.expect()` on lib.rs:59
5. Process panics and exits with code 12
6. Docker restarts container in <1 second
7. **Loop repeats at maximum speed**

Contrast this with the gRPC connection handling: [5](#0-4) 

The gRPC client implements proper exponential backoff with `backoff::ExponentialBackoff::default()`, retrying transient failures with increasing delays. The Redis connection has no equivalent protection.

## Impact Explanation

**Severity: Medium** (per Aptos bug bounty criteria)

This qualifies as **Medium severity** under "State inconsistencies requiring intervention":

1. **Service Availability Impact**: The indexer gRPC API becomes completely unavailable during the crash loop, preventing users from querying blockchain data
2. **Resource Exhaustion**: 
   - Rapid CPU consumption in tight restart loop (potentially hundreds of restarts per minute)
   - Log spam fills disk space with panic backtraces and connection errors
   - File descriptor exhaustion from repeated connection attempts
   - Network congestion from continuous connection attempts to Redis
3. **Cache Inconsistency**: The cache cannot update, causing the data service to serve stale blockchain state
4. **Operational Cost**: May trigger restart limits in Kubernetes/orchestration systems, requiring manual intervention

While this doesn't directly affect consensus or validator operations, it breaks the indexer service's availability guarantee and creates a denial-of-service condition that requires external intervention to resolve.

## Likelihood Explanation

**Likelihood: High**

This is highly likely to occur in production environments:

- Redis service restarts during maintenance
- Network partitions between cache worker and Redis
- Redis resource exhaustion or OOM kills
- Configuration errors in Redis connection strings
- Cloud infrastructure transient failures

The vulnerability triggers on **any** Redis connection failure, not requiring attacker action. Normal operational conditions can cause this, making it a practical reliability and security concern.

## Recommendation

Implement exponential backoff for Redis connections matching the pattern used for gRPC connections:

```rust
// In worker.rs, replace the direct connection attempt with:
pub async fn run(&mut self) -> Result<()> {
    loop {
        // Re-connect to Redis with exponential backoff
        let conn = backoff::future::retry(
            backoff::ExponentialBackoff {
                max_elapsed_time: Some(std::time::Duration::from_secs(60)),
                ..Default::default()
            },
            || async {
                match self.redis_client.get_tokio_connection_manager().await {
                    Ok(conn) => {
                        tracing::info!("[Indexer Cache] Redis connection established.");
                        Ok(conn)
                    },
                    Err(e) => {
                        tracing::error!(
                            "[Indexer Cache] Failed to connect to Redis: {}. Retrying...",
                            e
                        );
                        Err(backoff::Error::transient(e))
                    },
                }
            },
        )
        .await
        .context("Failed to establish Redis connection after retries")?;

        // Rest of the loop remains the same
        let mut rpc_client = create_grpc_client(self.fullnode_grpc_address.clone()).await;
        // ...
    }
}
```

Additionally, consider removing the `.expect()` in `lib.rs` and allowing graceful error propagation:

```rust
// In lib.rs, replace:
worker.run().await.context("Failed to run cache worker")
```

## Proof of Concept

**Reproduction Steps:**

1. Deploy the indexer-grpc stack using the provided docker-compose configuration
2. Verify the cache worker is running: `docker-compose logs indexer-grpc-cache-worker`
3. Stop Redis to simulate connection failure: `docker-compose stop redis`
4. Observe rapid panic-restart loop: `docker-compose logs -f indexer-grpc-cache-worker`

**Expected Observable Behavior:**
- Continuous panic messages in logs: `"Cache worker failed"`
- Crash info with backtrace showing `get_tokio_connection_manager` failure
- Process exit code 12 repeated in container logs
- Container restart count increasing rapidly: `docker ps` shows restart count
- High CPU usage from restart overhead
- Log file size growing rapidly

**Metrics to Monitor:**
- Container restart count via `docker inspect`
- Log volume growth rate
- CPU usage of Docker daemon
- Time between crash and restart (should be <1 second)

**Recovery:**
- Start Redis: `docker-compose start redis`
- Worker should connect on next restart attempt
- If restart limits exceeded, manual container restart required

This demonstrates the complete attack surface from connection failure through resource exhaustion, confirming the absence of proper backoff mechanisms for Redis connections.

## Notes

The vulnerability is particularly concerning because:
- It affects a critical production service (blockchain data indexing)
- The fix is straightforward (add exponential backoff like the gRPC connection)
- The impact compounds over time (log accumulation, resource exhaustion)
- Similar patterns may exist in other indexer components (`indexer-grpc-file-store`, `indexer-grpc-data-service` also use Redis without visible backoff in their connection initialization)

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/worker.rs (L112-116)
```rust
            let conn = self
                .redis_client
                .get_tokio_connection_manager()
                .await
                .context("Get redis connection failed.")?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-cache-worker/src/lib.rs (L55-59)
```rust
        worker
            .run()
            .await
            .context("Failed to run cache worker")
            .expect("Cache worker failed");
```

**File:** ecosystem/indexer-grpc/indexer-grpc-server-framework/src/lib.rs (L149-167)
```rust
pub fn setup_panic_handler() {
    std::panic::set_hook(Box::new(move |pi: &PanicHookInfo<'_>| {
        handle_panic(pi);
    }));
}

// Formats and logs panic information
fn handle_panic(panic_info: &PanicHookInfo<'_>) {
    // The Display formatter for a PanicHookInfo contains the message, payload and location.
    let details = format!("{}", panic_info);
    let backtrace = format!("{:#?}", Backtrace::new());
    let info = CrashInfo { details, backtrace };
    let crash_info = toml::to_string_pretty(&info).unwrap();
    error!("{}", crash_info);
    // TODO / HACK ALARM: Write crash info synchronously via eprintln! to ensure it is written before the process exits which error! doesn't guarantee.
    // This is a workaround until https://github.com/aptos-labs/aptos-core/issues/2038 is resolved.
    eprintln!("{}", crash_info);
    // Kill the process
    process::exit(12);
```

**File:** docker/compose/indexer-grpc/docker-compose.yaml (L40-58)
```yaml
  indexer-grpc-cache-worker:
    image: "${INDEXER_GRPC_IMAGE_REPO:-aptoslabs/indexer-grpc}:${IMAGE_TAG:-main}"
    networks:
      shared:
        ipv4_address: 172.16.1.13
    restart: unless-stopped
    volumes:
      - type: volume # XXX: needed now before refactor https://github.com/aptos-labs/aptos-core/pull/8139
        source: indexer-grpc-file-store
        target: /opt/aptos/file-store
      - type: bind
        source: ./cache-worker-config.yaml
        target: /opt/aptos/cache-worker-config.yaml
    command:
      - '/usr/local/bin/aptos-indexer-grpc-cache-worker'
      - '--config-path'
      - '/opt/aptos/cache-worker-config.yaml'
    depends_on:
      - redis
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/lib.rs (L36-63)
```rust
pub async fn create_grpc_client(address: Url) -> GrpcClientType {
    backoff::future::retry(backoff::ExponentialBackoff::default(), || async {
        match FullnodeDataClient::connect(address.to_string()).await {
            Ok(client) => {
                tracing::info!(
                    address = address.to_string(),
                    "[Indexer Cache] Connected to indexer gRPC server."
                );
                Ok(client
                    .max_decoding_message_size(usize::MAX)
                    .max_encoding_message_size(usize::MAX)
                    .send_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Gzip)
                    .accept_compressed(CompressionEncoding::Zstd))
            },
            Err(e) => {
                tracing::error!(
                    address = address.to_string(),
                    "[Indexer Cache] Failed to connect to indexer gRPC server: {}",
                    e
                );
                Err(backoff::Error::transient(e))
            },
        }
    })
    .await
    .unwrap()
}
```
