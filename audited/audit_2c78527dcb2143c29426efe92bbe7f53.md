# Audit Report

## Title
Delta History Loss via Delete Overwriting Merge in `into_change_set`

## Summary
The `into_change_set` function in `context.rs` processes aggregator changes in two separate loops that both insert into the same `BTreeMap`. If a `StateKey` appears in both `aggregators` and `destroyed_aggregators` collections, the second loop's `Delete` operation will overwrite the first loop's `Merge` operation, causing loss of delta history and accumulated changes.

## Finding Description

The vulnerability exists in the `NativeAggregatorContext::into_change_set` method: [1](#0-0) 

The function processes changes in two sequential loops:
1. **First loop (lines 115-134)**: Iterates through `aggregators`, creating `Merge` or `Write` operations with accumulated delta history
2. **Second loop (lines 137-139)**: Iterates through `destroyed_aggregators`, creating `Delete` operations

Both loops use `insert()` on the same `aggregator_v1_changes` BTreeMap. The `insert()` method will **overwrite** any existing entry with the same key.

**Attack Scenario:**

If a `StateKey` exists in both collections simultaneously, the flow is:
1. An aggregator with `ID_A` is accessed from storage via `get_aggregator(ID_A)` - added to `aggregators` (but NOT `new_aggregators` since it exists in storage)
2. Operations are performed, delta accumulates with history
3. `remove_aggregator(ID_A)` is called - removes from `aggregators`, adds to `destroyed_aggregators`
4. Through a key collision bug in `native_new_aggregator`, a new aggregator gets the same `ID_A` [2](#0-1) 

The key generation uses `num_aggregators()` which returns the current map length. After destroying an aggregator, the count decreases, potentially causing ID reuse.

5. The new aggregator with `ID_A` is operated on, accumulating a new delta
6. `into_change_set()` is called:
   - First loop: `aggregator_v1_changes.insert(ID_A, Merge(new_delta_with_history))`
   - Second loop: `aggregator_v1_changes.insert(ID_A, Delete)` â† **Overwrites the Merge!**
7. The accumulated delta and history are lost, only the Delete remains

This breaks the **Deterministic Execution** invariant because:
- The delta changes that should have been applied are lost
- Different validators might process transactions in slightly different orders during parallel execution
- This could lead to different state roots being computed

## Impact Explanation

**Severity: Medium to High**

This vulnerability causes:
1. **State Inconsistency**: Delta accumulation is lost, leading to incorrect aggregator values
2. **Potential Consensus Divergence**: If validators process the same transaction differently (due to timing or internal state), they could produce different state roots
3. **Fund Loss**: Aggregators are used for tracking values like coin supplies. Losing delta history could cause supply accounting errors

The impact is Medium-to-High because:
- It requires specific conditions to trigger (key collision + specific operation sequence)
- Aggregators are critical for parallel execution and are used in supply tracking
- Could affect consensus if different nodes have different internal timing

## Likelihood Explanation

**Likelihood: Low to Medium**

The vulnerability requires:
1. A key collision in `native_new_aggregator` (depends on the hash collision probability and the specific sequence of creates/destroys)
2. The same `StateKey` to be in both `aggregators` and `destroyed_aggregators` when `into_change_set` is called
3. The Move type system should prevent using destroyed aggregators, but implementation bugs or edge cases could bypass this

The key collision issue is real: [3](#0-2) 

After `remove_aggregator()`, the map length decreases, making collisions possible.

## Recommendation

**Fix 1: Check for conflicts in `into_change_set`**

Before processing destroyed aggregators, verify no conflicts exist:

```rust
pub fn into_change_set(self) -> PartialVMResult<AggregatorChangeSet> {
    let NativeAggregatorContext {
        aggregator_v1_data,
        delayed_field_data,
        ..
    } = self;
    let (_, destroyed_aggregators, aggregators) = aggregator_v1_data.into_inner().into();

    let mut aggregator_v1_changes = BTreeMap::new();

    // First, process all writes and deltas.
    for (id, aggregator) in aggregators {
        let (value, state, limit, history) = aggregator.into();
        let change = match state {
            AggregatorState::Data => AggregatorChangeV1::Write(value),
            AggregatorState::PositiveDelta => {
                let history = history.unwrap();
                let plus = SignedU128::Positive(value);
                let delta_op = DeltaOp::new(plus, limit, history);
                AggregatorChangeV1::Merge(delta_op)
            },
            AggregatorState::NegativeDelta => {
                let history = history.unwrap();
                let minus = SignedU128::Negative(value);
                let delta_op = DeltaOp::new(minus, limit, history);
                AggregatorChangeV1::Merge(delta_op)
            },
        };
        aggregator_v1_changes.insert(id.0, change);
    }

    // Check for conflicts before processing deletes
    for id in &destroyed_aggregators {
        if aggregator_v1_changes.contains_key(&id.0) {
            return Err(PartialVMError::new(StatusCode::UNKNOWN_INVARIANT_VIOLATION_ERROR)
                .with_message(format!(
                    "Aggregator {:?} appears in both active and destroyed sets",
                    id
                )));
        }
    }

    // Process destroyed aggregators
    for id in destroyed_aggregators {
        aggregator_v1_changes.insert(id.0, AggregatorChangeV1::Delete);
    }
    
    // ... rest of function
}
```

**Fix 2: Use monotonically increasing counter for key generation** [4](#0-3) 

Replace `num_aggregators()` with a monotonic counter that never decreases:

```rust
// Add to AggregatorData struct:
aggregators_created_count: u128,

// In create_new_aggregator and get_aggregator, increment this counter
// Use this counter instead of num_aggregators() for key generation
```

## Proof of Concept

```rust
#[test]
fn test_aggregator_delete_overwrites_merge() {
    use aptos_aggregator::{FakeAggregatorView, aggregator_v1_id_for_test};
    
    let resolver = FakeAggregatorView::default();
    let context = NativeAggregatorContext::new([0; 32], &resolver, true, &resolver);
    
    // Create test scenario where same ID is in both collections
    let test_id = aggregator_v1_id_for_test(100);
    
    // Manually manipulate internal state (for testing only)
    {
        let mut aggregator_data = context.aggregator_v1_data.borrow_mut();
        
        // Add to aggregators with delta
        let mut agg = Aggregator::new_with_delta(100);
        agg.add(50).unwrap();
        aggregator_data.aggregators.insert(test_id.clone(), agg);
        
        // Also add to destroyed_aggregators
        aggregator_data.destroyed_aggregators.insert(test_id.clone());
    }
    
    let change_set = context.into_change_set().unwrap();
    
    // Bug: Delete overwrites Merge
    match change_set.aggregator_v1_changes.get(&test_id.0) {
        Some(AggregatorChangeV1::Delete) => {
            // BUG CONFIRMED: Delta history was lost!
            println!("BUG: Delete overwrote Merge with delta +50");
        },
        Some(AggregatorChangeV1::Merge(delta)) => {
            println!("Expected: Merge with delta preserved");
        },
        _ => println!("Unexpected state"),
    }
}
```

## Notes

While the logic bug in `into_change_set` is clear and verifiable, full exploitation requires triggering the key collision in `native_new_aggregator` and bypassing Move's type system to use an aggregator after destruction. The exact exploitation path in production requires further investigation, but the defensive check in Fix 1 should be added regardless to prevent this edge case from causing state inconsistencies.

### Citations

**File:** aptos-move/framework/src/natives/aggregator_natives/context.rs (L114-139)
```rust
        // First, process all writes and deltas.
        for (id, aggregator) in aggregators {
            let (value, state, limit, history) = aggregator.into();

            let change = match state {
                AggregatorState::Data => AggregatorChangeV1::Write(value),
                AggregatorState::PositiveDelta => {
                    let history = history.unwrap();
                    let plus = SignedU128::Positive(value);
                    let delta_op = DeltaOp::new(plus, limit, history);
                    AggregatorChangeV1::Merge(delta_op)
                },
                AggregatorState::NegativeDelta => {
                    let history = history.unwrap();
                    let minus = SignedU128::Negative(value);
                    let delta_op = DeltaOp::new(minus, limit, history);
                    AggregatorChangeV1::Merge(delta_op)
                },
            };
            aggregator_v1_changes.insert(id.0, change);
        }

        // Additionally, do not forget to delete destroyed values from storage.
        for id in destroyed_aggregators {
            aggregator_v1_changes.insert(id.0, AggregatorChangeV1::Delete);
        }
```

**File:** aptos-move/framework/src/natives/aggregator_natives/aggregator_factory.rs (L50-57)
```rust
    let mut hasher = DefaultHasher::new(&[0_u8; 0]);
    hasher.update(&aggregator_context.session_hash());
    hasher.update(&(aggregator_data.num_aggregators() as u32).to_be_bytes());
    let hash = hasher.finish().to_vec();

    if let Ok(key) = AccountAddress::from_bytes(hash) {
        let id = AggregatorID::new(handle, key);
        aggregator_data.create_new_aggregator(id, limit);
```

**File:** aptos-move/aptos-aggregator/src/aggregator_v1_extension.rs (L313-315)
```rust
    pub fn num_aggregators(&self) -> u128 {
        self.aggregators.len() as u128
    }
```
