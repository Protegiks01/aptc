# Audit Report

## Title
Pruner Infinite Loop Vulnerability Due to Zero Batch Size Causing Unbounded Storage Growth

## Summary
The AptosDB pruner subsystem lacks validation for zero `batch_size` configuration, allowing the pruner worker to enter an infinite loop that consumes CPU cycles without making any progress on database cleanup. This leads to unbounded storage growth and eventual node failure due to disk exhaustion.

## Finding Description

The vulnerability exists in the database pruning subsystem across three pruner implementations: `LedgerPruner`, `StateKvPruner`, and `StateMerklePruner`. When a pruner is configured with `enable: true` and `batch_size: 0`, the pruner worker thread enters an infinite loop without making any progress. [1](#0-0) 

The `PrunerWorkerInner` struct stores `batch_size` without validation and passes it directly to the pruner's `prune()` method in the work loop: [2](#0-1) 

**Attack Path for LedgerPruner:**

In `LedgerPruner::prune()`, when `max_versions` (batch_size) is 0: [3](#0-2) 

The critical bug occurs at line 67-68: `current_batch_target_version = min(progress + max_versions as Version, target_version)`. When `max_versions` is 0, this becomes `min(progress + 0, target_version) = progress` (since `progress < target_version` in the loop condition). At line 86, `progress` is set to this unchanged value, creating an infinite loop.

**Attack Path for StateKvPruner:** [4](#0-3) 

Same vulnerability: line 56-57 calculates `current_batch_target_version = min(progress + 0, target_version) = progress`, and line 80 assigns this unchanged value back to `progress`.

**Attack Path for StateMerklePruner:** [5](#0-4) 

When `max_nodes_to_prune` is 0, the `get_stale_node_indices()` call returns empty indices (line 205 condition `indices.len() < limit` is false when limit=0), but if there's still data to prune, the `done` flag remains false (line 78-83), causing the outer loop at line 64 to continue indefinitely.

**Configuration Attack Vector:**

The configuration system has no validation preventing this: [6](#0-5) 

The `NO_OP_STORAGE_PRUNER_CONFIG` explicitly sets `batch_size: 0` for disabled pruners. However, there's no validation in the config sanitizer to prevent `enable: true` with `batch_size: 0`. [7](#0-6) [8](#0-7) 

While the manager only creates a worker when `enable: true`, it doesn't validate that `batch_size > 0`. [9](#0-8) 

## Impact Explanation

This is a **Medium severity** vulnerability under the Aptos bug bounty program criteria:

1. **State Inconsistencies Requiring Intervention**: The pruner fails to clean up old database entries, causing state inconsistencies where the database grows beyond configured limits.

2. **Resource Exhaustion**: 
   - Pruner worker thread consumes CPU in a tight loop
   - Database size grows unbounded as old versions are never pruned
   - Eventually causes disk exhaustion and node failure

3. **Node Availability**: Once disk space is exhausted, the node becomes unavailable and cannot participate in consensus or serve queries, affecting network liveness.

4. **Breaks Resource Limits Invariant**: Violates "All operations must respect gas, storage, and computational limits" - the system fails to enforce storage cleanup limits.

While not a Critical severity issue (no direct loss of funds or consensus safety violation), it causes significant operational impact requiring manual intervention to recover.

## Likelihood Explanation

**Likelihood: Medium-Low**

**Factors Increasing Likelihood:**
- No validation exists in the configuration system to prevent this
- The `NO_OP_STORAGE_PRUNER_CONFIG` demonstrates that `batch_size: 0` is a valid configuration value in the type system
- Configuration files can be modified by node operators or through automated deployment systems
- An attacker with access to a node's configuration (through compromised deployment systems, insider threat, or misconfiguration) could exploit this

**Factors Decreasing Likelihood:**
- Default configurations use safe non-zero values (5,000 for ledger, 1,000 for state)
- Requires either misconfiguration or malicious configuration change
- Most node operators use default or well-tested configurations

**Attack Scenarios:**
1. **Accidental Misconfiguration**: Operator mistakenly sets `batch_size: 0` while enabling pruner
2. **Malicious Configuration**: Attacker with config access deliberately sets zero batch size
3. **Configuration Template Error**: Deployment automation uses incorrect template

## Recommendation

**Add validation at multiple layers:**

1. **Configuration Validation** - Add to `ConfigSanitizer::sanitize()` in `config/src/config/storage_config.rs`:

```rust
// In ConfigSanitizer::sanitize() around line 707
if config.storage_pruner_config.ledger_pruner_config.enable 
    && config.storage_pruner_config.ledger_pruner_config.batch_size == 0 {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "ledger_pruner_config.batch_size must be greater than 0 when pruning is enabled.".to_string(),
    ));
}

if config.storage_pruner_config.state_merkle_pruner_config.enable 
    && config.storage_pruner_config.state_merkle_pruner_config.batch_size == 0 {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "state_merkle_pruner_config.batch_size must be greater than 0 when pruning is enabled.".to_string(),
    ));
}

if config.storage_pruner_config.epoch_snapshot_pruner_config.enable 
    && config.storage_pruner_config.epoch_snapshot_pruner_config.batch_size == 0 {
    return Err(Error::ConfigSanitizerFailed(
        sanitizer_name,
        "epoch_snapshot_pruner_config.batch_size must be greater than 0 when pruning is enabled.".to_string(),
    ));
}
```

2. **Runtime Validation** - Add defensive check in `PrunerWorker::new()` in `storage/aptosdb/src/pruner/pruner_worker.rs`:

```rust
pub(crate) fn new(pruner: Arc<dyn DBPruner>, batch_size: usize, name: &str) -> Self {
    assert!(batch_size > 0, "Pruner batch_size must be greater than 0");
    // ... rest of implementation
}
```

3. **Pruner-Level Validation** - Add early return in each pruner's `prune()` method:

```rust
fn prune(&self, batch_size: usize) -> Result<Version> {
    if batch_size == 0 {
        return Err(anyhow::anyhow!("batch_size must be greater than 0"));
    }
    // ... rest of implementation
}
```

## Proof of Concept

```rust
// Add this test to storage/aptosdb/src/pruner/ledger_pruner/mod.rs
#[cfg(test)]
mod zero_batch_size_test {
    use super::*;
    use crate::ledger_db::LedgerDb;
    use aptos_temppath::TempPath;
    use std::sync::Arc;
    use std::time::Duration;

    #[test]
    #[should_panic(expected = "timeout")]
    fn test_zero_batch_size_causes_infinite_loop() {
        // Setup
        let tmpdir = TempPath::new();
        let db = LedgerDb::new_for_test(&tmpdir);
        let pruner = Arc::new(LedgerPruner::new(Arc::new(db), None).unwrap());
        
        // Set target version ahead of progress
        pruner.set_target_version(100);
        
        // Spawn pruner with zero batch size in separate thread
        let pruner_clone = Arc::clone(&pruner);
        let handle = std::thread::spawn(move || {
            // This should loop forever with batch_size=0
            let _ = pruner_clone.prune(0);
        });
        
        // Wait briefly - if prune() returns, test fails
        // If it hangs, we've demonstrated the bug
        let timeout = Duration::from_secs(5);
        match handle.join_timeout(timeout) {
            Ok(_) => panic!("prune() should have looped infinitely but returned"),
            Err(_) => panic!("timeout - demonstrates infinite loop with batch_size=0"),
        }
    }
}
```

**To reproduce manually:**
1. Create a node configuration with `storage.storage_pruner_config.ledger_pruner_config.enable: true` and `storage.storage_pruner_config.ledger_pruner_config.batch_size: 0`
2. Start the node and allow transactions to accumulate
3. Observe pruner worker thread consuming CPU in infinite loop
4. Observe database size growing unbounded
5. Eventually disk fills and node crashes

## Notes

This vulnerability affects all three pruner types (Ledger, StateMerkle, StateKv) identically. The root cause is lack of validation that batch_size must be positive when pruning is enabled. While default configurations are safe, the absence of validation creates a potential DoS vector through configuration manipulation.

### Citations

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L30-49)
```rust
pub struct PrunerWorkerInner {
    /// The worker will sleep for this period of time after pruning each batch.
    pruning_time_interval_in_ms: u64,
    /// The pruner.
    pruner: Arc<dyn DBPruner>,
    /// A threshold to control how many items we prune for each batch.
    batch_size: usize,
    /// Indicates whether the pruning loop should be running. Will only be set to true on pruner
    /// destruction.
    quit_worker: AtomicBool,
}

impl PrunerWorkerInner {
    fn new(pruner: Arc<dyn DBPruner>, batch_size: usize) -> Arc<Self> {
        Arc::new(Self {
            pruning_time_interval_in_ms: if cfg!(test) { 100 } else { 1 },
            pruner,
            batch_size,
            quit_worker: AtomicBool::new(false),
        })
```

**File:** storage/aptosdb/src/pruner/pruner_worker.rs (L52-69)
```rust
    // Loop that does the real pruning job.
    fn work(&self) {
        while !self.quit_worker.load(Ordering::SeqCst) {
            let pruner_result = self.pruner.prune(self.batch_size);
            if pruner_result.is_err() {
                sample!(
                    SampleRate::Duration(Duration::from_secs(1)),
                    error!(error = ?pruner_result.err().unwrap(),
                        "Pruner has error.")
                );
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
                continue;
            }
            if !self.pruner.is_pruning_pending() {
                sleep(Duration::from_millis(self.pruning_time_interval_in_ms));
            }
        }
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L62-92)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning ledger data."
            );
            self.ledger_metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.sub_pruners.par_iter().try_for_each(|sub_pruner| {
                    sub_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| anyhow!("{} failed to prune: {err}", sub_pruner.name()))
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning ledger data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/state_kv_pruner/mod.rs (L49-86)
```rust
    fn prune(&self, max_versions: usize) -> Result<Version> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["state_kv_pruner__prune"]);

        let mut progress = self.progress();
        let target_version = self.target_version();

        while progress < target_version {
            let current_batch_target_version =
                min(progress + max_versions as Version, target_version);

            info!(
                progress = progress,
                target_version = current_batch_target_version,
                "Pruning state kv data."
            );
            self.metadata_pruner
                .prune(progress, current_batch_target_version)?;

            THREAD_MANAGER.get_background_pool().install(|| {
                self.shard_pruners.par_iter().try_for_each(|shard_pruner| {
                    shard_pruner
                        .prune(progress, current_batch_target_version)
                        .map_err(|err| {
                            anyhow!(
                                "Failed to prune state kv shard {}: {err}",
                                shard_pruner.shard_id(),
                            )
                        })
                })
            })?;

            progress = current_batch_target_version;
            self.record_progress(progress);
            info!(progress = progress, "Pruning state kv data is done.");
        }

        Ok(target_version)
    }
```

**File:** storage/aptosdb/src/pruner/state_merkle_pruner/state_merkle_shard_pruner.rs (L58-100)
```rust
    pub(in crate::pruner) fn prune(
        &self,
        current_progress: Version,
        target_version: Version,
        max_nodes_to_prune: usize,
    ) -> Result<()> {
        loop {
            let mut batch = SchemaBatch::new();
            let (indices, next_version) = StateMerklePruner::get_stale_node_indices(
                &self.db_shard,
                current_progress,
                target_version,
                max_nodes_to_prune,
            )?;

            indices.into_iter().try_for_each(|index| {
                batch.delete::<JellyfishMerkleNodeSchema>(&index.node_key)?;
                batch.delete::<S>(&index)
            })?;

            let mut done = true;
            if let Some(next_version) = next_version {
                if next_version <= target_version {
                    done = false;
                }
            }

            if done {
                batch.put::<DbMetadataSchema>(
                    &S::progress_metadata_key(Some(self.shard_id)),
                    &DbMetadataValue::Version(target_version),
                )?;
            }

            self.db_shard.write_schemas(batch)?;

            if done {
                break;
            }
        }

        Ok(())
    }
```

**File:** config/src/config/storage_config.rs (L306-323)
```rust
pub const NO_OP_STORAGE_PRUNER_CONFIG: PrunerConfig = PrunerConfig {
    ledger_pruner_config: LedgerPrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
        user_pruning_window_offset: 0,
    },
    state_merkle_pruner_config: StateMerklePrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
    },
    epoch_snapshot_pruner_config: EpochSnapshotPrunerConfig {
        enable: false,
        prune_window: 0,
        batch_size: 0,
    },
};
```

**File:** config/src/config/storage_config.rs (L387-395)
```rust
impl Default for LedgerPrunerConfig {
    fn default() -> Self {
        LedgerPrunerConfig {
            enable: true,
            prune_window: 90_000_000,
            batch_size: 5_000,
            user_pruning_window_offset: 200_000,
        }
    }
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L113-121)
```rust
        let pruner_worker = if ledger_pruner_config.enable {
            Some(Self::init_pruner(
                Arc::clone(&ledger_db),
                ledger_pruner_config,
                internal_indexer_db,
            ))
        } else {
            None
        };
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/ledger_pruner_manager.rs (L141-160)
```rust
    fn init_pruner(
        ledger_db: Arc<LedgerDb>,
        ledger_pruner_config: LedgerPrunerConfig,
        internal_indexer_db: Option<InternalIndexerDB>,
    ) -> PrunerWorker {
        let pruner = Arc::new(
            LedgerPruner::new(ledger_db, internal_indexer_db)
                .expect("Failed to create ledger pruner."),
        );

        PRUNER_WINDOW
            .with_label_values(&["ledger_pruner"])
            .set(ledger_pruner_config.prune_window as i64);

        PRUNER_BATCH_SIZE
            .with_label_values(&["ledger_pruner"])
            .set(ledger_pruner_config.batch_size as i64);

        PrunerWorker::new(pruner, ledger_pruner_config.batch_size, "ledger")
    }
```
