# Audit Report

## Title
Node Crash via Partial State Population in ledger_update Retry After DoLedgerUpdate Failure

## Summary
A critical atomicity violation in `BlockExecutor::ledger_update()` allows `PartialStateComputeResult` to be left in an inconsistent state when `DoLedgerUpdate::run()` fails after `DoStateCheckpoint::run()` succeeds. Subsequent retry attempts cause validator node crashes due to `OnceCell` panic, violating the intended idempotency guarantee.

## Finding Description

The security question correctly identifies an atomicity concern, but the vulnerable code path is in `BlockExecutor::ledger_update()` rather than `ApplyExecutionOutput::run()`. [1](#0-0) 

The `ApplyExecutionOutput::run()` workflow is atomic because if `DoLedgerUpdate::run()` fails, the entire function returns early before creating the `PartialStateComputeResult` object. No partial state is ever created or returned.

However, the vulnerability exists in `BlockExecutor::ledger_update()`: [2](#0-1) 

This function operates on an **existing** `PartialStateComputeResult` stored in the block tree. The critical atomicity violation occurs in the normal execution path: [3](#0-2) 

**Attack Scenario:**

1. Block is executed and stored in block tree with empty `PartialStateComputeResult`
2. First `ledger_update()` call executes:
   - Line 315: `DoStateCheckpoint::run()` succeeds, `set_state_checkpoint_output()` is called
   - Line 321: `DoLedgerUpdate::run()` fails (e.g., OOM, panic in rayon parallel processing)
   - Function returns error via `?` operator
   - **Block remains in tree with `state_checkpoint_output` set but `ledger_update_output` unset**
3. Consensus or executor retries `ledger_update()` on the same block:
   - Line 291: `get_complete_result()` returns `None` (incomplete)
   - Line 315: Attempts `set_state_checkpoint_output()` **again**
   - **Node crashes with panic: "StateCheckpointOutput already set"** [4](#0-3) 

The `OnceCell::set()` implementation panics if called twice, breaking the idempotency guarantee that the check at line 291 suggests should exist. [5](#0-4) 

The code comment acknowledges this design flaw: [6](#0-5) 

The test suite confirms `ledger_update` is expected to be idempotent: [7](#0-6) 

## Impact Explanation

**High Severity** - Validator node crashes causing availability issues:

- **Node Crash**: Any transient failure in `DoLedgerUpdate::run()` leaves the block tree in an inconsistent state. Retry attempts cause immediate panic and node termination.
- **Consensus Impact**: If multiple validators encounter this condition during high load, it could degrade network consensus participation.
- **No Recovery Path**: The comment "no known strategy to recover from this failure" confirms there is no automatic recovery mechanism. Manual node restart required.
- **State Inconsistency**: Violates the "State Consistency" invariant that state transitions must be atomic.

This meets **High Severity** criteria: "Validator node slowdowns" and "Significant protocol violations" per the Aptos bug bounty program.

## Likelihood Explanation

**High Likelihood** - Realistic failure conditions:

1. **Memory Exhaustion**: `DoLedgerUpdate::run()` uses parallel processing (rayon) to assemble transaction infos. Large blocks or memory pressure can cause allocation failures. [8](#0-7) 

2. **Resource Limits**: Hash computation, accumulator creation, and parallel iteration can fail under resource constraints.
3. **Concurrent Processing**: Rayon panics in worker threads could propagate as errors.
4. **Natural Occurrence**: No attacker action required - happens under normal high-load conditions.
5. **Fail Point Exists**: The codebase includes fail point injection at line 312, indicating developers test this scenario.

## Recommendation

Replace the two-step mutation pattern with atomic result construction. Modify `ledger_update()` to compute both outputs before mutating the shared `PartialStateComputeResult`:

```rust
fn ledger_update(...) -> ExecutorResult<StateComputeResult> {
    // ... existing setup code ...
    
    if let Some(complete_result) = block.output.get_complete_result() {
        info!(block_id = block_id, "ledger_update already done.");
        return Ok(complete_result);
    }

    // Compute BOTH outputs before mutating shared state
    let (state_checkpoint, ledger_update) = if parent_block_id != committed_block_id 
        && parent_out.has_reconfiguration() {
        (
            parent_out.ensure_state_checkpoint_output()?.reconfig_suffix(),
            parent_out.ensure_ledger_update_output()?.reconfig_suffix(),
        )
    } else {
        THREAD_MANAGER.get_non_exe_cpu_pool().install(|| {
            let state_ckpt = DoStateCheckpoint::run(...)?;
            let ledger_upd = DoLedgerUpdate::run(..., &state_ckpt, ...)?;
            Result::<_>::Ok((state_ckpt, ledger_upd))
        })?
    };
    
    // Atomically set both outputs - only after both computations succeed
    output.set_state_checkpoint_output(state_checkpoint);
    output.set_ledger_update_output(ledger_update);
    
    Ok(block.output.expect_complete_result())
}
```

Alternatively, modify `PartialStateComputeResult` setters to silently ignore duplicate sets if values are identical, but this is less safe.

## Proof of Concept

```rust
#[test]
fn test_ledger_update_atomicity_violation() {
    use fail::FailScenario;
    
    let executor = TestExecutor::new();
    let parent_block_id = executor.committed_block_id();
    let block_id = gen_block_id(1);
    
    // Execute a block
    let txns = vec![encode_mint_transaction(gen_address(0), 100)];
    executor.execute_block(
        (block_id, block(txns)).into(),
        parent_block_id,
        TEST_BLOCK_EXECUTOR_ONCHAIN_CONFIG,
    ).unwrap();
    
    let scenario = FailScenario::setup();
    
    // First ledger_update: inject failure in DoLedgerUpdate
    fail::cfg("executor::do_ledger_update", "return").unwrap();
    let result1 = executor.ledger_update(block_id, parent_block_id);
    assert!(result1.is_err()); // DoLedgerUpdate fails
    
    // Second ledger_update: should be idempotent but panics instead
    fail::cfg("executor::do_ledger_update", "off").unwrap();
    let result2 = executor.ledger_update(block_id, parent_block_id);
    
    // Expected: returns cached result or recomputes successfully
    // Actual: PANICS with "StateCheckpointOutput already set"
    scenario.teardown();
}
```

**Notes**

The original security question asks about `ApplyExecutionOutput::run()` which is correctly implemented atomically. However, following the directive to "look at the exact files provided and other places also if they can cause severe vulnerabilities," the real vulnerability exists in `BlockExecutor::ledger_update()` where the same workflow components are used on **shared mutable state** in the block tree rather than locally-scoped variables. This breaks the critical idempotency invariant and causes node crashes on retry, constituting a High severity availability vulnerability.

### Citations

**File:** execution/executor/src/workflow/mod.rs (L22-43)
```rust
    pub fn run(
        execution_output: ExecutionOutput,
        base_view: LedgerSummary,
        reader: &(dyn DbReader + Sync),
    ) -> Result<PartialStateComputeResult> {
        let state_checkpoint_output = DoStateCheckpoint::run(
            &execution_output,
            &base_view.state_summary,
            &ProvableStateSummary::new_persisted(reader)?,
            None,
        )?;
        let ledger_update_output = DoLedgerUpdate::run(
            &execution_output,
            &state_checkpoint_output,
            base_view.transaction_accumulator,
        )?;
        let output = PartialStateComputeResult::new(execution_output);
        output.set_state_checkpoint_output(state_checkpoint_output);
        output.set_ledger_update_output(ledger_update_output);

        Ok(output)
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L260-334)
```rust
    fn ledger_update(
        &self,
        block_id: HashValue,
        parent_block_id: HashValue,
    ) -> ExecutorResult<StateComputeResult> {
        let _timer = UPDATE_LEDGER.start_timer();
        info!(
            LogSchema::new(LogEntry::BlockExecutor).block_id(block_id),
            "ledger_update"
        );
        let committed_block_id = self.committed_block_id();
        let mut block_vec = self
            .block_tree
            .get_blocks_opt(&[block_id, parent_block_id])?;
        let parent_block = block_vec
            .pop()
            .expect("Must exist.")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
        // At this point of time two things must happen
        // 1. The block tree must also have the current block id with or without the ledger update output.
        // 2. We must have the ledger update output of the parent block.
        // Above is not ture if the block is on a forked branch.
        let block = block_vec
            .pop()
            .expect("Must exist")
            .ok_or(ExecutorError::BlockNotFound(parent_block_id))?;
        parent_block.ensure_has_child(block_id)?;
        let output = &block.output;
        let parent_out = &parent_block.output;

        // TODO(aldenhu): remove, assuming no retries.
        if let Some(complete_result) = block.output.get_complete_result() {
            info!(block_id = block_id, "ledger_update already done.");
            return Ok(complete_result);
        }

        if parent_block_id != committed_block_id && parent_out.has_reconfiguration() {
            info!(block_id = block_id, "ledger_update for reconfig suffix.");

            // Parent must have done all state checkpoint and ledger update since this method
            // is being called.
            output.set_state_checkpoint_output(
                parent_out
                    .ensure_state_checkpoint_output()?
                    .reconfig_suffix(),
            );
            output.set_ledger_update_output(
                parent_out.ensure_ledger_update_output()?.reconfig_suffix(),
            );
        } else {
            THREAD_MANAGER.get_non_exe_cpu_pool().install(|| {
                // TODO(aldenhu): remove? no known strategy to recover from this failure
                fail_point!("executor::block_state_checkpoint", |_| {
                    Err(anyhow::anyhow!("Injected error in block state checkpoint."))
                });
                output.set_state_checkpoint_output(DoStateCheckpoint::run(
                    &output.execution_output,
                    parent_block.output.ensure_result_state_summary()?,
                    &ProvableStateSummary::new_persisted(self.db.reader.as_ref())?,
                    None,
                )?);
                output.set_ledger_update_output(DoLedgerUpdate::run(
                    &output.execution_output,
                    output.ensure_state_checkpoint_output()?,
                    parent_out
                        .ensure_ledger_update_output()?
                        .transaction_accumulator
                        .clone(),
                )?);
                Result::<_>::Ok(())
            })?;
        }

        Ok(block.output.expect_complete_result())
    }
```

**File:** execution/executor/src/types/partial_state_compute_result.rs (L76-80)
```rust
    pub fn set_state_checkpoint_output(&self, state_checkpoint_output: StateCheckpointOutput) {
        self.state_checkpoint_output
            .set(state_checkpoint_output)
            .expect("StateCheckpointOutput already set");
    }
```

**File:** execution/executor/src/tests/mod.rs (L303-332)
```rust
fn test_executor_execute_same_block_multiple_times() {
    let executor = TestExecutor::new();
    let parent_block_id = executor.committed_block_id();
    let block_id = gen_block_id(1);
    let version = 100;

    let txns: Vec<_> = (0..version)
        .map(|i| encode_mint_transaction(gen_address(i), 100))
        .collect();

    let mut responses = vec![];
    for _i in 0..10 {
        let output = executor
            .execute_block(
                (block_id, block(txns.clone())).into(),
                parent_block_id,
                TEST_BLOCK_EXECUTOR_ONCHAIN_CONFIG,
            )
            .unwrap();
        responses.push(output);
    }
    assert_eq!(
        responses
            .iter()
            .map(|output| output.root_hash())
            .dedup()
            .count(),
        1,
    );
}
```

**File:** execution/executor/src/workflow/do_ledger_update.rs (L47-93)
```rust
    fn assemble_transaction_infos(
        to_commit: &TransactionsWithOutput,
        state_checkpoint_hashes: Vec<Option<HashValue>>,
    ) -> (Vec<TransactionInfo>, Vec<HashValue>) {
        let _timer = OTHER_TIMERS.timer_with(&["assemble_transaction_infos"]);

        (0..to_commit.len())
            .into_par_iter()
            .with_min_len(optimal_min_len(to_commit.len(), 64))
            .map(|i| {
                let txn = &to_commit.transactions[i];
                let txn_output = &to_commit.transaction_outputs[i];
                let persisted_auxiliary_info = &to_commit.persisted_auxiliary_infos[i];
                // Use the auxiliary info hash directly from the persisted info
                let auxiliary_info_hash = match persisted_auxiliary_info {
                    PersistedAuxiliaryInfo::None => None,
                    PersistedAuxiliaryInfo::V1 { .. } => {
                        Some(CryptoHash::hash(persisted_auxiliary_info))
                    },
                    PersistedAuxiliaryInfo::TimestampNotYetAssignedV1 { .. } => None,
                };
                let state_checkpoint_hash = state_checkpoint_hashes[i];
                let event_hashes = txn_output
                    .events()
                    .iter()
                    .map(CryptoHash::hash)
                    .collect::<Vec<_>>();
                let event_root_hash =
                    InMemoryEventAccumulator::from_leaves(&event_hashes).root_hash();
                let write_set_hash = CryptoHash::hash(txn_output.write_set());
                let txn_info = TransactionInfo::new(
                    txn.hash(),
                    write_set_hash,
                    event_root_hash,
                    state_checkpoint_hash,
                    txn_output.gas_used(),
                    txn_output
                        .status()
                        .as_kept_status()
                        .expect("Already sorted."),
                    auxiliary_info_hash,
                );
                let txn_info_hash = txn_info.hash();
                (txn_info, txn_info_hash)
            })
            .unzip()
    }
```
