# Audit Report

## Title
Time-of-Check to Time-of-Use Race Condition in Storage Service Request Moderator Allows Exponential Backoff Bypass

## Summary
A TOCTOU (Time-of-Check to Time-of-Use) race condition exists in the `RequestModerator::validate_request` function where concurrent access to `unhealthy_peer_states` between request validation threads and the periodic refresh task causes inconsistent peer behavior classification. This allows malicious peers to bypass exponential backoff penalties and send significantly more invalid requests than intended, potentially causing resource exhaustion.

## Finding Description

The storage service request moderator tracks peers that send invalid requests using a `DashMap<PeerNetworkId, UnhealthyPeerState>`. When a peer sends too many invalid requests (default: 500), it gets ignored for a duration that doubles with each offense (default initial: 5 minutes). A background refresh task runs every second to unblock peers that have been ignored long enough. [1](#0-0) 

The vulnerability exists in the `validate_request` function, which performs two separate DashMap operations with a gap between them: [2](#0-1) 

**The Race Condition Flow:**

1. **CHECK (lines 142-149)**: Thread reads peer state using `get()` to check if ignored
2. **GAP (lines 151-159)**: Request validation occurs (peer state can change during this)
3. **USE (lines 161-178)**: Thread writes to peer state using `entry().or_insert_with()` to increment counter

During the gap, the concurrent refresh task can modify the peer state: [3](#0-2) 

The refresh calls `refresh_peer_state()` which resets the invalid request counter: [4](#0-3) 

**Exploitation Scenario:**

Initial state: Peer has sent 500 invalid requests (max threshold), has been ignored, and 5 minutes have elapsed.

1. **Thread A** (validating new invalid request): Lines 142-149 check if peer is ignored → finds peer is NOT ignored (was just unblocked moments before)
2. **Thread A** continues to validation logic (lines 155-159) → validation fails
3. **Thread B** (refresh task): Executes `retain()` → calls `refresh_peer_state()` on this peer → resets `invalid_request_count = 0`, `ignore_start_time = None`, doubles backoff
4. **Thread A** continues: Lines 161-178 → increments counter from 0 to 1 (not 501!) → does NOT trigger ignore state (1 < 500)

**Result:** Peer sent 501 total invalid requests but only has a counter of 1 and is not ignored. The exponential backoff mechanism is completely bypassed.

The configuration values make this timing window exploitable: [5](#0-4) 

With a 1-second refresh interval, an attacker can time bursts of invalid requests around the refresh window to repeatedly trigger the race condition.

**Attack Vector:**

A malicious peer can:
1. Send 499 invalid requests rapidly
2. Send 1 more to trigger ignore state  
3. Wait ~5 minutes for the first unblock
4. Time subsequent bursts of invalid requests around the 1-second refresh intervals
5. Exploit the race condition to have counters reset mid-validation
6. Repeat indefinitely, evading progressively longer bans

Multiple coordinated malicious peers can amplify this attack to exhaust storage service resources across the network.

## Impact Explanation

This vulnerability has **Medium** severity per the Aptos bug bounty criteria:

The storage service is critical infrastructure for state synchronization. Nodes falling behind consensus or new nodes joining the network depend on storage services from validator nodes to catch up. 

**Impact:**
- **Resource Exhaustion**: Malicious peers can send unlimited invalid requests, consuming CPU cycles for validation, memory for state tracking, and disk I/O for logging
- **Service Degradation**: If multiple malicious peers exploit this simultaneously, storage services on validator nodes can become overloaded
- **State Sync Disruption**: Legitimate nodes attempting to sync may experience delays if storage services are degraded
- **Log Spam**: Excessive invalid request warnings can obscure genuine security events [6](#0-5) 

While this doesn't directly cause consensus violations or fund loss, it affects network availability and validator performance, categorizing it as Medium severity: "State inconsistencies requiring intervention" and validator slowdowns.

## Likelihood Explanation

**Likelihood: Medium-High**

The exploit requires:
- Knowledge of the refresh interval timing (1 second by default, discoverable through observation)
- Ability to send invalid storage service requests (any network peer can do this)
- Timing precision to hit the ~1 second window repeatedly (achievable with simple scripting)

The attack is **realistic** because:
1. The refresh task runs every 1 second - a large enough window
2. Request validation takes non-trivial time (checks against storage summary, processes errors)
3. Public network peers can freely connect and send requests
4. No authentication or rate limiting prevents the initial burst of invalid requests

An attacker doesn't need validator access or sophisticated capabilities - just network connectivity and basic timing logic.

## Recommendation

**Fix: Atomic Check-and-Modify Operation**

Replace the separate `get()` and `entry()` operations with a single atomic operation. Modify `validate_request` to use `entry()` for both checking and modifying:

```rust
pub fn validate_request(
    &self,
    peer_network_id: &PeerNetworkId,
    request: &StorageServiceRequest,
) -> Result<(), Error> {
    let validate_request = || {
        // Get the latest storage server summary
        let storage_server_summary = self.cached_storage_server_summary.load();

        // Verify the request is serviceable
        if !storage_server_summary.can_service(
            &self.aptos_data_client_config,
            self.time_service.clone(),
            request,
        ) {
            // Use entry() for atomic check-and-modify
            let mut unhealthy_peer_state = self
                .unhealthy_peer_states
                .entry(*peer_network_id)
                .or_insert_with(|| {
                    UnhealthyPeerState::new(
                        self.storage_service_config.max_invalid_requests_per_peer,
                        self.storage_service_config.min_time_to_ignore_peers_secs,
                        self.time_service.clone(),
                    )
                });

            // Check if ignored AFTER acquiring exclusive lock
            if unhealthy_peer_state.is_ignored() {
                return Err(Error::TooManyInvalidRequests(format!(
                    "Peer is temporarily ignored. Unable to handle request: {:?}",
                    request
                )));
            }

            // Increment counter while holding the lock
            unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

            return Err(Error::InvalidRequest(format!(
                "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                request, storage_server_summary
            )));
        }

        Ok(())
    };
    // ... rest of function
}
```

This ensures the check and modification happen atomically within a single DashMap entry lock, eliminating the race condition window.

**Alternative: Read-Modify-Write Lock Pattern**

If the above changes are too invasive, add version tracking to detect stale reads and retry.

## Proof of Concept

```rust
#[cfg(test)]
mod race_condition_test {
    use super::*;
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    #[test]
    fn test_toctou_race_in_validate_request() {
        // Setup
        let time_service = TimeService::mock();
        let max_invalid_requests = 5;
        let min_time_to_ignore = 1;
        
        // Create moderator with test config
        let config = StorageServiceConfig {
            max_invalid_requests_per_peer: max_invalid_requests,
            min_time_to_ignore_peers_secs: min_time_to_ignore,
            request_moderator_refresh_interval_ms: 100,
            ..Default::default()
        };
        
        let moderator = Arc::new(RequestModerator::new(
            AptosDataClientConfig::default(),
            Arc::new(ArcSwap::new(Arc::new(StorageServerSummary::default()))),
            Arc::new(PeersAndMetadata::new(&[NetworkId::Public])),
            config,
            time_service.clone(),
        ));
        
        let peer = PeerNetworkId::new(NetworkId::Public, PeerId::random());
        
        // Step 1: Send max_invalid_requests to trigger ignore
        for _ in 0..max_invalid_requests {
            let invalid_request = create_invalid_request();
            let _ = moderator.validate_request(&peer, &invalid_request);
        }
        
        // Verify peer is ignored
        assert!(is_peer_ignored(&moderator, &peer));
        
        // Step 2: Advance time to unblock
        time_service.into_mock().advance(Duration::from_secs(min_time_to_ignore));
        
        // Step 3: Trigger race condition
        let barrier = Arc::new(Barrier::new(2));
        let moderator_clone = moderator.clone();
        let barrier_clone = barrier.clone();
        
        // Thread 1: validate_request (will hit race window)
        let handle1 = thread::spawn(move || {
            barrier_clone.wait(); // Synchronize start
            let invalid_request = create_invalid_request();
            moderator_clone.validate_request(&peer, &invalid_request)
        });
        
        // Thread 2: refresh (will reset counter mid-validation)
        let moderator_clone2 = moderator.clone();
        let handle2 = thread::spawn(move || {
            barrier.wait(); // Synchronize start
            thread::sleep(Duration::from_micros(100)); // Let validate_request start
            moderator_clone2.refresh_unhealthy_peer_states()
        });
        
        handle1.join().unwrap();
        handle2.join().unwrap();
        
        // Step 4: Verify race condition occurred
        // Counter should be reset (1) instead of incremented (6)
        let peer_state = moderator.get_unhealthy_peer_states().get(&peer).unwrap();
        assert_eq!(peer_state.invalid_request_count, 1); // Should be 6 without race!
        assert!(!peer_state.is_ignored()); // Should be ignored without race!
    }
}
```

**Notes:**

This race condition affects all validator nodes running storage services that accept requests from public network peers. The vulnerability enables resource exhaustion attacks that could degrade state sync performance across the network. The atomic fix ensures thread-safe peer behavior classification without introducing performance overhead from global locks.

### Citations

**File:** state-sync/storage-service/server/src/moderator.rs (L64-68)
```rust
            // Log the fact that we're now ignoring the peer
            warn!(LogSchema::new(LogEntry::RequestModeratorIgnoredPeer)
                .peer_network_id(peer_network_id)
                .message("Ignoring peer due to too many invalid requests!"));
        }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L79-98)
```rust
    pub fn refresh_peer_state(&mut self, peer_network_id: &PeerNetworkId) {
        if let Some(ignore_start_time) = self.ignore_start_time {
            let ignored_duration = self.time_service.now().duration_since(ignore_start_time);
            if ignored_duration >= Duration::from_secs(self.min_time_to_ignore_secs) {
                // Reset the invalid request count
                self.invalid_request_count = 0;

                // Reset the ignore start time
                self.ignore_start_time = None;

                // Double the min time to ignore the peer
                self.min_time_to_ignore_secs *= 2;

                // Log the fact that we're no longer ignoring the peer
                warn!(LogSchema::new(LogEntry::RequestModeratorIgnoredPeer)
                    .peer_network_id(peer_network_id)
                    .message("No longer ignoring peer! Enough time has elapsed."));
            }
        }
    }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L111-111)
```rust
    unhealthy_peer_states: Arc<DashMap<PeerNetworkId, UnhealthyPeerState>>,
```

**File:** state-sync/storage-service/server/src/moderator.rs (L134-196)
```rust
    pub fn validate_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> Result<(), Error> {
        // Validate the request and time the operation
        let validate_request = || {
            // If the peer is being ignored, return an error
            if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
                if peer_state.is_ignored() {
                    return Err(Error::TooManyInvalidRequests(format!(
                        "Peer is temporarily ignored. Unable to handle request: {:?}",
                        request
                    )));
                }
            }

            // Get the latest storage server summary
            let storage_server_summary = self.cached_storage_server_summary.load();

            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }

            Ok(()) // The request is valid
        };
        utils::execute_and_time_duration(
            &metrics::STORAGE_REQUEST_VALIDATION_LATENCY,
            Some((peer_network_id, request)),
            None,
            validate_request,
            None,
        )
    }
```

**File:** state-sync/storage-service/server/src/moderator.rs (L198-238)
```rust
    /// Refresh the unhealthy peer states and garbage collect disconnected peers
    pub fn refresh_unhealthy_peer_states(&self) -> Result<(), Error> {
        // Get the currently connected peers
        let connected_peers_and_metadata = self
            .peers_and_metadata
            .get_connected_peers_and_metadata()
            .map_err(|error| {
                Error::UnexpectedErrorEncountered(format!(
                    "Unable to get connected peers and metadata: {}",
                    error
                ))
            })?;

        // Remove disconnected peers and refresh ignored peer states
        let mut num_ignored_peers = 0;
        self.unhealthy_peer_states
            .retain(|peer_network_id, unhealthy_peer_state| {
                if connected_peers_and_metadata.contains_key(peer_network_id) {
                    // Refresh the ignored peer state
                    unhealthy_peer_state.refresh_peer_state(peer_network_id);

                    // If the peer is ignored, increment the ignored peer count
                    if unhealthy_peer_state.is_ignored() {
                        num_ignored_peers += 1;
                    }

                    true // The peer is still connected, so we should keep it
                } else {
                    false // The peer is no longer connected, so we should remove it
                }
            });

        // Update the number of ignored peers
        metrics::set_gauge(
            &metrics::IGNORED_PEER_COUNT,
            NetworkId::Public.as_str(),
            num_ignored_peers,
        );

        Ok(())
    }
```

**File:** config/src/config/state_sync_config.rs (L201-214)
```rust
            max_invalid_requests_per_peer: 500,
            max_lru_cache_size: 500, // At ~0.6MiB per chunk, this should take no more than 0.5GiB
            max_network_channel_size: 4000,
            max_network_chunk_bytes: SERVER_MAX_MESSAGE_SIZE as u64,
            max_network_chunk_bytes_v2: SERVER_MAX_MESSAGE_SIZE_V2 as u64,
            max_num_active_subscriptions: 30,
            max_optimistic_fetch_period_ms: 5000, // 5 seconds
            max_state_chunk_size: MAX_STATE_CHUNK_SIZE,
            max_storage_read_wait_time_ms: 10_000, // 10 seconds
            max_subscription_period_ms: 30_000,    // 30 seconds
            max_transaction_chunk_size: MAX_TRANSACTION_CHUNK_SIZE,
            max_transaction_output_chunk_size: MAX_TRANSACTION_OUTPUT_CHUNK_SIZE,
            min_time_to_ignore_peers_secs: 300, // 5 minutes
            request_moderator_refresh_interval_ms: 1000, // 1 second
```
