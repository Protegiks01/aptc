# Audit Report

## Title
Race Condition in Persisted Auxiliary Info Retrieval Causes Non-Deterministic Transaction Re-Execution

## Summary
The storage service creates multiple database iterators sequentially without atomic snapshot consistency, combined with parallel commit operations for different transaction components. This race condition can cause `persisted_auxiliary_info_iterator` to return stale or mismatched data, which is then used during transaction re-execution, leading to different `monotonically_increasing_counter()` values and breaking deterministic execution guarantees.

## Finding Description

The vulnerability exists in the interaction between three components:

**1. Non-Atomic Iterator Creation** [1](#0-0) 

The storage service creates four separate iterators sequentially. Each iterator receives an implicit RocksDB snapshot at the moment of creation, not a shared atomic snapshot across all four iterators.

**2. Parallel Commit Pattern** [2](#0-1) 

Transaction data and auxiliary info are committed in separate parallel threads. While the scope ensures all threads complete, each thread commits its batch independently to RocksDB at different times, creating visibility windows where some data is committed but other data is not yet visible.

**3. Auxiliary Info Usage in Execution** [3](#0-2) 

The persisted auxiliary info is converted to `AuxiliaryInfo` and passed to transaction execution. [4](#0-3) 

The transaction index from auxiliary info becomes part of transaction metadata. [5](#0-4) 

The `transaction_index` is directly embedded into the `monotonically_increasing_counter()` at bit position 24-55. This counter value is returned to Move code and can influence execution decisions.

**Attack Scenario:**

1. Node A commits transaction chunk at version V with auxiliary info containing `V1{transaction_index: 5}`
2. At time T1: `commit_transactions` completes, transactions become visible
3. At time T1.5: Storage service receives request for version V, creates `transaction_iterator` (sees committed transactions)
4. At time T2: Before `commit_auxiliary_info` completes, storage service creates `persisted_auxiliary_info_iterator` (doesn't see new auxiliary info, returns `PersistedAuxiliaryInfo::None` or stale values)
5. State sync node receives mismatched data: correct transactions paired with wrong auxiliary info
6. During apply, wrong auxiliary info is stored to local database
7. Later, node enters verification mode or needs transaction replay
8. Re-execution uses wrong auxiliary info â†’ `monotonically_increasing_counter()` returns different value than original execution
9. If Move smart contract uses this counter for state-changing logic (unique ID generation, ordering, randomness), execution diverges
10. Node produces different state root than validators, cannot achieve consensus

## Impact Explanation

This vulnerability breaks **Critical Invariant #1: Deterministic Execution** - "All validators must produce identical state roots for identical blocks."

**Severity: High** (per Aptos Bug Bounty criteria: "Significant protocol violations")

The impact qualifies as High because:
- Breaks fundamental deterministic execution guarantee
- Causes permanent state divergence between nodes  
- Affects any Move code using `transaction_context::monotonically_increasing_counter()`
- No way to recover without manual intervention (wrong data is permanently stored)
- Could affect multiple nodes simultaneously if they sync during the race window

While not Critical severity because:
- Does not directly cause fund loss
- Requires specific conditions (re-execution + counter-dependent Move code)
- Race window is narrow (microseconds during commit)
- Most state sync operations apply outputs rather than re-execute

## Likelihood Explanation

**Likelihood: Medium**

Factors increasing likelihood:
- Race window occurs during every transaction commit (happens continuously)
- Multiple storage service instances serve state sync requests constantly
- Re-execution occurs in several scenarios: VerifyExecutionMode, transaction replay after crashes, backup restore, debugging/validation operations
- `monotonically_increasing_counter()` is an officially supported Move feature that developers are encouraged to use for unique ID generation

Factors decreasing likelihood:
- Race window is microseconds (though with high transaction throughput, many requests fall within windows)
- Normal state sync applies outputs without re-execution (but verification modes do re-execute)
- Requires Move code actually using the counter for state-changing decisions

## Recommendation

**Fix 1: Atomic Snapshot for All Iterators**

Create a single RocksDB snapshot and use it for all four iterators:

```rust
// In storage.rs, get_transactions_with_proof_by_size()
let db_snapshot = self.storage.get_snapshot()?; // Add snapshot method to DbReader

let transaction_iterator = self.storage
    .get_transaction_iterator_with_snapshot(&db_snapshot, start_version, num_transactions_to_fetch)?;
let transaction_info_iterator = self.storage
    .get_transaction_info_iterator_with_snapshot(&db_snapshot, start_version, num_transactions_to_fetch)?;
let transaction_events_iterator = self.storage
    .get_events_iterator_with_snapshot(&db_snapshot, start_version, num_transactions_to_fetch)?;
let persisted_auxiliary_info_iterator = self.storage
    .get_persisted_auxiliary_info_iterator_with_snapshot(&db_snapshot, start_version, num_transactions_to_fetch)?;
```

**Fix 2: Atomic Commit for All Transaction Components**

Modify the commit pattern to write all transaction components in a single atomic batch:

```rust
// In aptosdb_writer.rs, calculate_and_commit_ledger_and_state_kv()
let mut combined_batch = SchemaBatch::new();

// Add all components to single batch instead of separate threads
self.add_transactions_to_batch(&mut combined_batch, chunk)?;
self.add_auxiliary_info_to_batch(&mut combined_batch, chunk)?;
self.add_transaction_infos_to_batch(&mut combined_batch, chunk)?;
// ... other components

// Single atomic commit
self.ledger_db.write_schemas(combined_batch)?;
```

**Fix 3: Verification on Retrieval**

Add consistency verification when zipping iterators to detect and abort on mismatched data:

```rust
// Verify all iterators returned the expected count
ensure!(
    transactions.len() == transaction_infos.len() 
    && transactions.len() == persisted_auxiliary_infos.len()
    && (include_events && transactions.len() == transaction_events.len()),
    "Iterator length mismatch detected - potential race condition"
);
```

## Proof of Concept

```rust
// Reproduction test (pseudo-code demonstrating the race)
#[test]
fn test_auxiliary_info_race_condition() {
    // Setup: Deploy Move module using monotonically_increasing_counter()
    let counter_module = r#"
        module 0x1::test_counter {
            use aptos_framework::transaction_context;
            
            public entry fun generate_id(): u128 {
                transaction_context::monotonically_increasing_counter(timestamp::now_microseconds())
            }
        }
    "#;
    
    // Step 1: Start committing transactions on Node A
    let transactions = create_test_transactions_with_counter_calls(100);
    
    // Step 2: Inject delay between commit_transactions and commit_auxiliary_info
    tokio::spawn(async {
        node_a.commit_chunk_with_injected_delay(transactions).await;
    });
    
    // Step 3: Request data from storage service during commit window
    tokio::time::sleep(Duration::from_micros(50)); // Hit the race window
    let response = storage_service.get_transactions_with_proof(
        start_version, end_version, proof_version
    ).await.unwrap();
    
    // Step 4: Verify auxiliary info mismatch
    let aux_infos = response.get_auxiliary_infos();
    assert!(aux_infos.iter().any(|info| matches!(info, PersistedAuxiliaryInfo::None)));
    
    // Step 5: Apply to Node B and later re-execute
    node_b.apply_chunk(response).await.unwrap();
    
    // Step 6: Trigger re-execution (e.g., verification mode)
    let execution_output_b = node_b.re_execute_chunk(transactions).await.unwrap();
    let execution_output_a = node_a.get_execution_output(transactions).await.unwrap();
    
    // Step 7: Verify state root divergence
    assert_ne!(
        execution_output_a.state_root(),
        execution_output_b.state_root(),
        "State roots diverged due to wrong auxiliary info"
    );
}
```

## Notes

The vulnerability is exacerbated by the fact that `PersistedAuxiliaryInfo::None` is treated as a valid value (for backward compatibility with versions before auxiliary info was introduced), so the mismatch doesn't trigger immediate errors - it silently produces incorrect execution results. This makes the bug particularly insidious as it can propagate through the network before being detected.

### Citations

**File:** state-sync/storage-service/server/src/storage.rs (L374-394)
```rust
        let transaction_iterator = self
            .storage
            .get_transaction_iterator(start_version, num_transactions_to_fetch)?;
        let transaction_info_iterator = self
            .storage
            .get_transaction_info_iterator(start_version, num_transactions_to_fetch)?;
        let transaction_events_iterator = if include_events {
            self.storage
                .get_events_iterator(start_version, num_transactions_to_fetch)?
        } else {
            // If events are not included, create a fake iterator (they will be dropped anyway)
            Box::new(std::iter::repeat_n(
                Ok(vec![]),
                num_transactions_to_fetch as usize,
            ))
        };
        let persisted_auxiliary_info_iterator =
            self.storage.get_persisted_auxiliary_info_iterator(
                start_version,
                num_transactions_to_fetch as usize,
            )?;
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L271-319)
```rust
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });
```

**File:** execution/executor/src/chunk_executor/transaction_chunk.rs (L104-107)
```rust
            persisted_aux_info
                .into_iter()
                .map(|info| AuxiliaryInfo::new(info, None))
                .collect(),
```

**File:** aptos-move/aptos-vm/src/transaction_metadata.rs (L122-122)
```rust
            transaction_index_kind: auxiliary_info.transaction_index_kind(),
```

**File:** aptos-move/framework/src/natives/transaction_context.rs (L187-211)
```rust
        // monotonically_increasing_counter (128 bits) = `<reserved_byte (8 bits)> || timestamp_us (64 bits) || transaction_index (32 bits) || session counter (8 bits) || local_counter (16 bits)`
        // reserved_byte: 0 for block/chunk execution (V1), 1 for validation/simulation (TimestampNotYetAssignedV1)
        let timestamp_us = safely_pop_arg!(args, u64);
        let transaction_index_kind = user_transaction_context.transaction_index_kind();

        let (reserved_byte, transaction_index) = match transaction_index_kind {
            TransactionIndexKind::BlockExecution { transaction_index } => {
                (0u128, transaction_index)
            },
            TransactionIndexKind::ValidationOrSimulation { transaction_index } => {
                (1u128, transaction_index)
            },
            TransactionIndexKind::NotAvailable => {
                return Err(SafeNativeError::Abort {
                    abort_code: error::invalid_state(abort_codes::ETRANSACTION_INDEX_NOT_AVAILABLE),
                });
            },
        };

        let mut monotonically_increasing_counter: u128 = reserved_byte << 120;
        monotonically_increasing_counter |= (timestamp_us as u128) << 56;
        monotonically_increasing_counter |= (transaction_index as u128) << 24;
        monotonically_increasing_counter |= session_counter << 16;
        monotonically_increasing_counter |= local_counter;
        Ok(smallvec![Value::u128(monotonically_increasing_counter)])
```
