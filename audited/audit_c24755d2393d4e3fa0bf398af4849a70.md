# Audit Report

## Title
Indefinite Historical Consensus Key Retention Enables Long-Range Attacks on Syncing Clients

## Summary
The secure storage system retains historical consensus private keys for up to 4 rotations (VaultStorage) or 2 rotations (CryptoKVStorage) without any operational need. If an attacker compromises a quorum of validators' storage infrastructure, they gain access to these historical keys, enabling the creation of forged epoch-ending signatures that could fool clients syncing from older waypoints via long-range attacks.

## Finding Description
The `export_private_key_for_version()` function in the secure storage layer allows retrieval of historical consensus private keys by their public key identifier. [1](#0-0) 

The VaultStorage implementation retains up to 4 key versions as defined by `MAX_NUM_KEY_VERSIONS`. [2](#0-1)  Historical keys are only trimmed when new rotations occur. [3](#0-2) 

The CryptoKVStorage implementation retains 2 versions (current + previous). [4](#0-3) 

**Critical Finding**: Analysis of the codebase reveals that neither `export_private_key_for_version()` nor `sign_using_version()` are used in any production consensus, safety rules, or validator operational code - they only appear in test files and trait implementations. This means **there is no legitimate operational reason to retain historical keys**.

However, during epoch transitions, validators do load the appropriate key for the new epoch via `consensus_sk_by_pk()`. [5](#0-4)  This function checks for explicitly stored versioned keys but in practice only the current key is used. [6](#0-5) 

**The Attack Scenario:**
1. Attacker compromises secure storage of 2/3+ validators
2. Gains access to up to 4 epochs worth of historical consensus keys per validator
3. Can forge epoch-ending `LedgerInfoWithSignatures` for those historical epochs [7](#0-6) 
4. Creates malicious `EpochChangeProof` to fool syncing clients
5. Targets clients with waypoints in the compromised epoch range

The vulnerability violates **Consensus Safety** (invariant #2) for syncing clients and **Cryptographic Correctness** (invariant #10) by unnecessarily expanding the attack surface for key compromise.

## Impact Explanation
**Severity: Medium**

This issue meets the Medium severity criteria per the Aptos bug bounty: "State inconsistencies requiring intervention."

While this cannot break live consensus (current epoch validation uses only current keys), it enables long-range attacks against syncing clients:

1. **Limited Direct Impact**: Cannot affect validators already participating in current-epoch consensus
2. **Syncing Client Vulnerability**: Clients syncing with waypoints 2-4 epochs old could accept forged history
3. **Defense-in-Depth Violation**: Unnecessarily extends the window of key compromise from 1 epoch to 4 epochs
4. **Prerequisite Attack Complexity**: Requires compromising quorum of validators' infrastructure (already catastrophic)

However, the impact is **NOT Critical** because:
- Most clients use recent waypoints (not 4+ epochs old)
- Requires catastrophic mass compromise as prerequisite
- Social consensus on canonical chain provides additional protection
- Detection systems would likely identify mass validator compromise

## Likelihood Explanation
**Likelihood: Low**

The exploitation requires:
1. **Catastrophic prerequisite**: Compromising secure storage of 2/3+ validators simultaneously
2. **Timing window**: Keys must still be retained (within 4 rotations for Vault)
3. **Vulnerable targets**: Syncing clients with waypoints in the historical epoch range
4. **Detection evasion**: Avoiding monitoring systems that would detect mass compromise

This makes exploitation highly unlikely in practice. However, the issue still warrants attention because:
- It unnecessarily expands the attack surface
- No operational benefit justifies the retained keys
- Violates least-privilege principle
- Defense-in-depth failure

## Recommendation
Implement automatic key deletion for consensus keys that are no longer in the active validator set:

1. **Immediate deletion on epoch change**: When a validator's key is rotated or removed from the validator set, delete the old private key from storage
2. **Maximum retention policy**: Never retain more than 1 previous version (for transition safety during epoch boundaries)
3. **Explicit deletion API**: Add `delete_consensus_key_version()` method to CryptoStorage trait
4. **Cleanup on initialization**: During SafetyRules initialization, delete any keys not matching the current epoch's expected key

Pseudocode for the fix in `consensus/safety-rules/src/safety_rules.rs`:

```rust
fn guarded_initialize(&mut self, proof: &EpochChangeProof) -> Result<(), Error> {
    // ... existing verification code ...
    
    // After successful key reconciliation, delete any old keys
    if let Some(expected_key) = expected_key {
        // Delete any previous versions not matching expected key
        self.persistent_storage.delete_old_consensus_keys(expected_key)?;
    }
    
    // ... rest of initialization ...
}
```

And in `secure/storage/vault/src/lib.rs`, modify trim to be more aggressive:

```rust
const MAX_NUM_KEY_VERSIONS: u32 = 1; // Only keep current version
```

## Proof of Concept
Due to the prerequisite requirement of compromising validator infrastructure, a full exploit PoC cannot be demonstrated in a test environment. However, the following test demonstrates that historical keys are indeed accessible:

```rust
#[test]
fn test_historical_key_retention_vulnerability() {
    use aptos_crypto::{ed25519::Ed25519PrivateKey, PrivateKey, Uniform};
    use aptos_secure_storage::{CryptoStorage, VaultStorage};
    use rand::thread_rng;
    
    let mut storage = VaultStorage::new(/* ... vault config ... */);
    
    // Simulate 4 epoch key rotations
    let mut historical_keys = vec![];
    storage.create_key("consensus_key").unwrap();
    
    for i in 0..4 {
        let current_pk = storage.get_public_key("consensus_key").unwrap().public_key;
        historical_keys.push(current_pk.clone());
        storage.rotate_key("consensus_key").unwrap();
    }
    
    // Demonstrate all 4 historical keys are still accessible
    for historical_pk in historical_keys {
        let result = storage.export_private_key_for_version("consensus_key", historical_pk);
        assert!(result.is_ok(), "Historical key should still be accessible (VULNERABILITY)");
    }
    
    // This demonstrates that even after 4 rotations, all historical keys remain accessible
    // In a real attack, compromising storage at this point gives attacker 4 epochs of keys
}
```

**Notes**:
- The vulnerability exists but requires privileged access (validator compromise) to exploit
- Production code does not use `export_private_key_for_version()` or `sign_using_version()`, indicating no operational need for historical key retention
- The recommendation to delete old keys immediately would eliminate this attack vector entirely
- This is primarily a defense-in-depth issue that becomes relevant only after catastrophic validator compromise

### Citations

**File:** secure/storage/src/storage.rs (L57-63)
```rust
    fn export_private_key_for_version(
        &self,
        name: &str,
        version: Ed25519PublicKey,
    ) -> Result<Ed25519PrivateKey, Error> {
        Storage::export_private_key_for_version(self, name, version)
    }
```

**File:** secure/storage/vault/src/lib.rs (L26-28)
```rust
/// The max number of key versions held in vault at any one time.
/// Keys are trimmed in FIFO order.
const MAX_NUM_KEY_VERSIONS: u32 = 4;
```

**File:** secure/storage/vault/src/lib.rs (L356-390)
```rust
    pub fn trim_key_versions(&self, name: &str) -> Result<Ed25519PublicKey, Error> {
        // Read all keys and versions
        let all_pub_keys = self.read_ed25519_key(name)?;

        // Find the maximum and minimum versions
        let max_version = all_pub_keys
            .iter()
            .map(|resp| resp.version)
            .max()
            .ok_or_else(|| Error::NotFound("transit/".into(), name.into()))?;
        let min_version = all_pub_keys
            .iter()
            .map(|resp| resp.version)
            .min()
            .ok_or_else(|| Error::NotFound("transit/".into(), name.into()))?;

        // Trim keys if too many versions exist
        if (max_version - min_version) >= MAX_NUM_KEY_VERSIONS {
            // let min_available_version = max_version - MAX_NUM_KEY_VERSIONS + 1;
            let min_available_version = max_version
                .checked_sub(MAX_NUM_KEY_VERSIONS)
                .and_then(|n| n.checked_add(1))
                .ok_or_else(|| {
                    Error::OverflowError("trim_key_versions::min_available_version".into())
                })?;
            self.set_minimum_encrypt_decrypt_version(name, min_available_version)?;
            self.set_minimum_available_version(name, min_available_version)?;
        };

        let newest_pub_key = all_pub_keys
            .iter()
            .find(|pub_key| pub_key.version == max_version)
            .ok_or_else(|| Error::NotFound("transit/".into(), name.into()))?;
        Ok(newest_pub_key.value.clone())
    }
```

**File:** secure/storage/src/crypto_kv_storage.rs (L30-53)
```rust
    fn export_private_key_for_version(
        &self,
        name: &str,
        version: Ed25519PublicKey,
    ) -> Result<Ed25519PrivateKey, Error> {
        let current_private_key = self.export_private_key(name)?;
        if current_private_key.public_key().eq(&version) {
            return Ok(current_private_key);
        }

        match self.export_private_key(&get_previous_version_name(name)) {
            Ok(previous_private_key) => {
                if previous_private_key.public_key().eq(&version) {
                    Ok(previous_private_key)
                } else {
                    Err(Error::KeyVersionNotFound(name.into(), version.to_string()))
                }
            },
            Err(Error::KeyNotSet(_)) => {
                Err(Error::KeyVersionNotFound(name.into(), version.to_string()))
            },
            Err(e) => Err(e),
        }
    }
```

**File:** consensus/safety-rules/src/persistent_safety_storage.rs (L106-132)
```rust
    pub fn consensus_sk_by_pk(
        &self,
        pk: bls12381::PublicKey,
    ) -> Result<bls12381::PrivateKey, Error> {
        let _timer = counters::start_timer("get", CONSENSUS_KEY);
        let pk_hex = hex::encode(pk.to_bytes());
        let explicit_storage_key = format!("{}_{}", CONSENSUS_KEY, pk_hex);
        let explicit_sk = self
            .internal_store
            .get::<bls12381::PrivateKey>(explicit_storage_key.as_str())
            .map(|v| v.value);
        let default_sk = self.default_consensus_sk();
        let key = match (explicit_sk, default_sk) {
            (Ok(sk_0), _) => sk_0,
            (Err(_), Ok(sk_1)) => sk_1,
            (Err(_), Err(_)) => {
                return Err(Error::ValidatorKeyNotFound("not found!".to_string()));
            },
        };
        if key.public_key() != pk {
            return Err(Error::SecureStorageMissingDataError(format!(
                "Incorrect sk saved for {:?} the expected pk",
                pk
            )));
        }
        Ok(key)
    }
```

**File:** consensus/safety-rules/src/safety_rules.rs (L312-343)
```rust
        let author = self.persistent_storage.author()?;
        let expected_key = epoch_state.verifier.get_public_key(&author);
        let initialize_result = match expected_key {
            None => Err(Error::ValidatorNotInSet(author.to_string())),
            Some(expected_key) => {
                let current_key = self.signer().ok().map(|s| s.public_key());
                if current_key == Some(expected_key.clone()) {
                    info!(
                        SafetyLogSchema::new(LogEntry::KeyReconciliation, LogEvent::Success),
                        "in set",
                    );
                    Ok(())
                } else {
                    // Try to export the consensus key directly from storage.
                    match self.persistent_storage.consensus_sk_by_pk(expected_key) {
                        Ok(consensus_key) => {
                            self.validator_signer =
                                Some(ValidatorSigner::new(author, Arc::new(consensus_key)));
                            Ok(())
                        },
                        Err(Error::SecureStorageMissingDataError(error)) => {
                            Err(Error::ValidatorKeyNotFound(error))
                        },
                        Err(error) => Err(error),
                    }
                }
            },
        };
        initialize_result.inspect_err(|error| {
            info!(SafetyLogSchema::new(LogEntry::KeyReconciliation, LogEvent::Error).error(error),);
            self.validator_signer = None;
        })
```

**File:** types/src/epoch_change.rs (L66-118)
```rust
    pub fn verify(&self, verifier: &dyn Verifier) -> Result<&LedgerInfoWithSignatures> {
        ensure!(
            !self.ledger_info_with_sigs.is_empty(),
            "The EpochChangeProof is empty"
        );
        ensure!(
            !verifier
                .is_ledger_info_stale(self.ledger_info_with_sigs.last().unwrap().ledger_info()),
            "The EpochChangeProof is stale as our verifier is already ahead \
             of the entire EpochChangeProof"
        );
        let mut verifier_ref = verifier;

        for ledger_info_with_sigs in self
            .ledger_info_with_sigs
            .iter()
            // Skip any stale ledger infos in the proof prefix. Note that with
            // the assertion above, we are guaranteed there is at least one
            // non-stale ledger info in the proof.
            //
            // It's useful to skip these stale ledger infos to better allow for
            // concurrent client requests.
            //
            // For example, suppose the following:
            //
            // 1. My current trusted state is at epoch 5.
            // 2. I make two concurrent requests to two validators A and B, who
            //    live at epochs 9 and 11 respectively.
            //
            // If A's response returns first, I will ratchet my trusted state
            // to epoch 9. When B's response returns, I will still be able to
            // ratchet forward to 11 even though B's EpochChangeProof
            // includes a bunch of stale ledger infos (for epochs 5, 6, 7, 8).
            //
            // Of course, if B's response returns first, we will reject A's
            // response as it's completely stale.
            .skip_while(|&ledger_info_with_sigs| {
                verifier.is_ledger_info_stale(ledger_info_with_sigs.ledger_info())
            })
        {
            // Try to verify each (epoch -> epoch + 1) jump in the EpochChangeProof.
            verifier_ref.verify(ledger_info_with_sigs)?;
            // While the original verification could've been via waypoints,
            // all the next epoch changes are verified using the (already
            // trusted) validator sets.
            verifier_ref = ledger_info_with_sigs
                .ledger_info()
                .next_epoch_state()
                .ok_or_else(|| format_err!("LedgerInfo doesn't carry a ValidatorSet"))?;
        }

        Ok(self.ledger_info_with_sigs.last().unwrap())
    }
```
