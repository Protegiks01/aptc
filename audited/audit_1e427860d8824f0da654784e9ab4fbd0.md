# Audit Report

## Title
Memory Leak in Consensus Network Channels Due to Undrained Messages After Sender Drop

## Summary
When the `NetworkTask` sender is dropped (e.g., due to crash or panic) while messages remain queued in consensus channels, these messages persist in memory indefinitely if the receivers are not actively draining them. Each RPC request message contains a `oneshot::Sender` callback that ties up network layer resources. In long-running validators experiencing intermittent `NetworkTask` failures, this leads to gradual memory accumulation.

## Finding Description

The consensus network layer uses `aptos_channel` for communication between `NetworkTask` and `EpochManager`. The vulnerability lies in the channel's memory management when senders are dropped. [1](#0-0) 

When `NetworkTask::new()` creates these channels, it establishes three communication channels with bounded capacity (10 for consensus, 50 for quorum store, 10 for RPC). The channels use `Arc<Mutex<SharedState>>` to share their internal queue between sender and receiver. [2](#0-1) 

When a sender is dropped, it decrements `num_senders` and wakes the receiver, but **messages already in the internal queue remain**. The `SharedState` persists as long as either sender or receiver holds an `Arc` reference. [3](#0-2) 

The `Receiver::drop()` only sets a flag—it **does not clear the internal queue**. Messages remain in `SharedState::internal_queue` until explicitly drained by polling.

**The Critical Issue with RPC Requests:**

RPC requests contain `oneshot::Sender<Result<Bytes, RpcError>>` callbacks for sending responses: [4](#0-3) 

When `NetworkTask` receives an RPC request and pushes it to the channel, the oneshot sender is embedded in the message: [5](#0-4) 

If `NetworkTask` crashes after pushing RPC requests but before they're processed:
1. The sender is dropped
2. RPC messages with their `oneshot::Sender` callbacks remain in the channel queue
3. The network layer holds corresponding `oneshot::Receiver`s waiting for responses
4. Both sides of the oneshot channel remain allocated, tying up network resources
5. If `EpochManager` is slow to drain or blocked, these accumulate in memory

**Exploitation Path:**

In a long-running validator over months:
1. `NetworkTask` crashes intermittently (network issues, panics, OOM)
2. At each crash, 10-50 messages may be queued (per channel capacity limits)
3. Each RPC message contains block data, batches, or proofs (potentially large payloads)
4. If `EpochManager` is temporarily blocked or slow, draining is delayed
5. Messages accumulate across multiple crash-restart cycles
6. Memory gradually leaks, eventually causing node slowdown or OOM

**Invariant Violation:**

This breaks the **Resource Limits** invariant (Invariant #9): "All operations must respect gas, storage, and computational limits." The unbounded accumulation of undrained channel messages violates memory resource limits for long-running validators.

## Impact Explanation

**Severity: Medium** (up to $10,000 per Aptos Bug Bounty)

This qualifies as **Medium severity** because:
- **Validator node slowdowns**: Gradual memory accumulation degrades validator performance
- **State inconsistencies requiring intervention**: Eventually requires node restart to clear accumulated memory
- Does not directly cause consensus violations or fund loss
- Requires multiple preconditions (crashes + slow draining) but is realistic over months of operation

The impact is bounded by channel capacity limits (max ~70 messages per crash) but accumulates over time. In a validator running for 6-12 months with weekly crashes, this could accumulate to hundreds of MB or GB depending on message sizes.

## Likelihood Explanation

**Likelihood: Medium-High**

This is likely to occur because:
1. **Crash scenarios are common**: Network failures, OOM conditions, and panics occur in production validators
2. **No automatic cleanup**: The channel implementation provides no automatic queue cleanup when senders drop
3. **Long-running context**: Validators run for months/years without restarts
4. **Slow draining is realistic**: `EpochManager` may be temporarily blocked on epoch transitions, state sync, or heavy consensus operations

The vulnerability doesn't require attacker action—normal operational conditions trigger it. However, the memory leak is gradual (bounded per incident) rather than catastrophic.

## Recommendation

**Implement automatic queue cleanup when all senders are dropped:**

Modify `aptos_channel::Sender::drop()` to clear the internal queue when `num_senders` reaches 0:

```rust
impl<K: Eq + Hash + Clone, M> Drop for Sender<K, M> {
    fn drop(&mut self) {
        let mut shared_state = self.shared_state.lock();
        
        debug_assert!(shared_state.num_senders > 0);
        shared_state.num_senders -= 1;
        
        if shared_state.num_senders == 0 {
            // Clear the queue to prevent memory leaks
            shared_state.internal_queue.clear();
            
            if let Some(waker) = shared_state.waker.take() {
                waker.wake();
            }
        }
    }
}
```

**Alternative/Additional Fix:**

Add monitoring and periodic cleanup in `NetworkTask::start()`:

```rust
// Periodically check channel health
tokio::spawn(async move {
    let mut interval = tokio::time::interval(Duration::from_secs(300));
    loop {
        interval.tick().await;
        counters::CONSENSUS_CHANNEL_MSGS.collect();
        // Log warnings if channels are near capacity for extended periods
    }
});
```

**Additional Mitigation:**

Implement timeout-based cleanup for RPC requests to prevent indefinite accumulation of oneshot channels.

## Proof of Concept

```rust
#[tokio::test]
async fn test_channel_memory_leak_on_sender_drop() {
    use aptos_channel::{aptos_channel, message_queues::QueueStyle};
    use futures::StreamExt;
    use std::sync::Arc;
    
    // Create a channel with capacity 10
    let (sender, mut receiver) = aptos_channel::new::<u64, Vec<u8>>(
        QueueStyle::FIFO,
        10,
        None,
    );
    
    // Push messages
    for i in 0..10 {
        sender.push(i, vec![0u8; 1024 * 1024]).unwrap(); // 1MB each
    }
    
    // Drop sender without draining receiver
    drop(sender);
    
    // Receiver still exists but not polling
    // Messages remain in memory
    tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
    
    // Verify messages are still queued
    let msg1 = receiver.next().await;
    assert!(msg1.is_some()); // Message still exists
    
    // In production: if receiver is never polled, these 10MB stay in memory
    // Multiply by crashes over months = significant leak
}

#[tokio::test]  
async fn test_rpc_oneshot_accumulation() {
    use futures::channel::oneshot;
    
    let mut senders = Vec::new();
    let mut receivers = Vec::new();
    
    // Simulate 100 RPC requests with oneshot channels
    for _ in 0..100 {
        let (tx, rx) = oneshot::channel::<Vec<u8>>();
        senders.push(tx);
        receivers.push(rx);
    }
    
    // Drop senders without responding (simulating messages stuck in queue)
    drop(senders);
    
    // Receivers remain allocated, tying up memory
    // In production: these accumulate in network layer
    tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;
    
    // Memory is only freed when receivers are also dropped
    assert_eq!(receivers.len(), 100);
}
```

## Notes

The vulnerability is **exacerbated** by the lack of automatic cleanup in the channel implementation. While individual incidents are bounded by channel capacity, the cumulative effect over months of validator operation can be significant. The issue is particularly severe for RPC channels where each message ties up network layer resources through oneshot channels.

The recommended fix (automatic queue clearing on sender drop) is safe because once all senders are dropped, no new messages can arrive, making it safe to discard pending messages. This aligns with the semantic expectation that dropped senders indicate the producer is no longer active.

### Citations

**File:** consensus/src/network.rs (L118-123)
```rust
#[derive(Debug)]
pub struct IncomingBlockRetrievalRequest {
    pub req: BlockRetrievalRequest,
    pub protocol: ProtocolId,
    pub response_sender: oneshot::Sender<Result<Bytes, RpcError>>,
}
```

**File:** consensus/src/network.rs (L757-769)
```rust
        let (consensus_messages_tx, consensus_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            10,
            Some(&counters::CONSENSUS_CHANNEL_MSGS),
        );
        let (quorum_store_messages_tx, quorum_store_messages) = aptos_channel::new(
            QueueStyle::FIFO,
            // TODO: tune this value based on quorum store messages with backpressure
            50,
            Some(&counters::QUORUM_STORE_CHANNEL_MSGS),
        );
        let (rpc_tx, rpc_rx) =
            aptos_channel::new(QueueStyle::FIFO, 10, Some(&counters::RPC_CHANNEL_MSGS));
```

**File:** consensus/src/network.rs (L943-1026)
```rust
                Event::RpcRequest(peer_id, msg, protocol, callback) => {
                    counters::CONSENSUS_RECEIVED_MSGS
                        .with_label_values(&[msg.name()])
                        .inc();
                    let req = match msg {
                        // TODO @bchocho @hariria revisit deprecation later once BlockRetrievalRequest enum is released
                        ConsensusMsg::DeprecatedBlockRetrievalRequest(request) => {
                            debug!(
                                remote_peer = peer_id,
                                event = LogEvent::ReceiveBlockRetrieval,
                                "{}",
                                request
                            );
                            IncomingRpcRequest::DeprecatedBlockRetrieval(
                                DeprecatedIncomingBlockRetrievalRequest {
                                    req: *request,
                                    protocol,
                                    response_sender: callback,
                                },
                            )
                        },
                        ConsensusMsg::BlockRetrievalRequest(request) => {
                            debug!(
                                remote_peer = peer_id,
                                event = LogEvent::ReceiveBlockRetrieval,
                                "{:?}",
                                request
                            );
                            IncomingRpcRequest::BlockRetrieval(IncomingBlockRetrievalRequest {
                                req: *request,
                                protocol,
                                response_sender: callback,
                            })
                        },
                        ConsensusMsg::BatchRequestMsg(request) => {
                            debug!(
                                remote_peer = peer_id,
                                event = LogEvent::ReceiveBatchRetrieval,
                                "{}",
                                request
                            );
                            IncomingRpcRequest::BatchRetrieval(IncomingBatchRetrievalRequest {
                                req: *request,
                                protocol,
                                response_sender: callback,
                            })
                        },
                        ConsensusMsg::DAGMessage(req) => {
                            IncomingRpcRequest::DAGRequest(IncomingDAGRequest {
                                req,
                                sender: peer_id,
                                responder: RpcResponder {
                                    protocol,
                                    response_sender: callback,
                                },
                            })
                        },
                        ConsensusMsg::CommitMessage(req) => {
                            IncomingRpcRequest::CommitRequest(IncomingCommitRequest {
                                req: *req,
                                protocol,
                                response_sender: callback,
                            })
                        },
                        ConsensusMsg::RandGenMessage(req) => {
                            IncomingRpcRequest::RandGenRequest(IncomingRandGenRequest {
                                req,
                                sender: peer_id,
                                protocol,
                                response_sender: callback,
                            })
                        },
                        _ => {
                            warn!(remote_peer = peer_id, "Unexpected msg: {:?}", msg);
                            continue;
                        },
                    };
                    if let Err(e) = self
                        .rpc_tx
                        .push((peer_id, discriminant(&req)), (peer_id, req))
                    {
                        warn!(error = ?e, "aptos channel closed");
                    };
                },
```

**File:** crates/channel/src/aptos_channel.rs (L127-140)
```rust
impl<K: Eq + Hash + Clone, M> Drop for Sender<K, M> {
    fn drop(&mut self) {
        let mut shared_state = self.shared_state.lock();

        debug_assert!(shared_state.num_senders > 0);
        shared_state.num_senders -= 1;

        if shared_state.num_senders == 0 {
            if let Some(waker) = shared_state.waker.take() {
                waker.wake();
            }
        }
    }
}
```

**File:** crates/channel/src/aptos_channel.rs (L157-163)
```rust
impl<K: Eq + Hash + Clone, M> Drop for Receiver<K, M> {
    fn drop(&mut self) {
        let mut shared_state = self.shared_state.lock();
        debug_assert!(!shared_state.receiver_dropped);
        shared_state.receiver_dropped = true;
    }
}
```
