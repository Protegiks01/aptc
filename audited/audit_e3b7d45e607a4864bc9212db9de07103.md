# Audit Report

## Title
Non-Atomic Backup Reads Create Impossible Database States Violating Consensus Consistency

## Summary
The `BackupHandler::get_transaction_iter()` function creates 5 separate RocksDB iterators sequentially to read transaction data from different databases. While each iterator has snapshot isolation, they capture snapshots at different moments in time. Since transaction commits to these databases occur in parallel, a backup can mix data from multiple consensus-committed states, creating an impossible database state that never existed atomically in consensus.

## Finding Description

The vulnerability occurs in the interaction between two critical code paths:

**Write Path (Parallel Commits):** [1](#0-0) 

The `calculate_and_commit_ledger_and_state_kv()` function spawns multiple threads that commit to different databases in parallel: events, write_sets, transactions, auxiliary_info, and transaction_infos. These commits complete at different times with no synchronization between them during the write phase.

**Read Path (Sequential Iterator Creation):** [2](#0-1) 

The `get_transaction_iter()` function creates 5 separate RocksDB iterators sequentially, one for each database. Each iterator captures a snapshot at the moment of its creation.

**The Race Condition:**

1. At time T1, consensus commits transaction N atomically across all databases
2. At time T2, a backup begins and creates the transaction_db iterator (sees up to version N)
3. At time T3, transaction N+1 begins committing in parallel to the 5 databases
4. At time T4, transaction N+1's commit to transaction_info_db completes
5. At time T5, the backup creates the transaction_info_db iterator (sees version N+1)
6. At time T6, transaction N+1's commit to event_db completes
7. At time T7, the backup creates the event_db iterator (sees version N)
8. The backup now contains: transaction N+1's transaction_info but transaction N's events

**Atomicity Guarantee During Recovery:** [3](#0-2) 

The `sync_commit_progress()` function ensures consistency by using `OverallCommitProgress` as the atomic marker. However, this only helps during **recovery after crashes**, not during **concurrent reads during normal operation**.

**No Read-Write Synchronization:** [4](#0-3) 

The `pre_commit_lock` and `commit_lock` only prevent concurrent commits, not reads during commits.

This violates the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." The backup represents a state that was never atomically committed by consensus.

## Impact Explanation

**Critical Severity** - This meets the "Consensus/Safety violations" category for the following reasons:

1. **Consensus Consistency Violation**: The backup can contain transaction data that represents a database state never committed by AptosBFT consensus. Consensus commits are atomic across all databases via the `OverallCommitProgress` marker, but backups can read a mix of pre-commit and post-commit data.

2. **Potential for Chain Splits**: If a node restores from such an inconsistent backup, it could compute different state roots than other nodes, potentially causing consensus disagreements or requiring manual intervention.

3. **Impossible State Representation**: The backup violates the fundamental assumption that all stored states are valid consensus-committed states. A transaction's info, events, and write_set are meant to be atomic - they either all exist at a version or none do.

4. **Merkle Tree Inconsistency**: The transaction accumulator and state merkle trees assume atomicity. A backup with mismatched data could have transaction_info that doesn't match the actual transaction, causing merkle proof verification failures.

## Likelihood Explanation

**High Likelihood** - This issue will occur in production with certainty:

1. **Normal Operation**: Backups run continuously in production environments while the chain is processing transactions. The race window is significant because parallel commits take milliseconds to complete.

2. **No Special Conditions Required**: Any backup operation during active transaction processing will have a probability of hitting this race condition. Given that backups occur regularly (every few minutes/hours) and transactions commit continuously, the overlap is inevitable.

3. **Multiple Database Shards**: With storage sharding enabled (the default production configuration), the 5 databases are separate physical RocksDB instances, making the race window even larger.

4. **Iterator Creation Time**: Creating 5 iterators sequentially takes non-negligible time, especially under load, increasing the likelihood of capturing different snapshots.

## Recommendation

**Solution: Use RocksDB Snapshots for Atomic Multi-Database Reads**

Create explicit RocksDB snapshots for all databases before creating iterators, ensuring all 5 iterators read from the same atomic point in time:

```rust
pub fn get_transaction_iter(
    &self,
    start_version: Version,
    num_transactions: usize,
) -> Result<
    impl Iterator<
            Item = Result<(
                Transaction,
                PersistedAuxiliaryInfo,
                TransactionInfo,
                Vec<ContractEvent>,
                WriteSet,
            )>,
        > + '_,
> {
    // Create a consistent snapshot version by reading OverallCommitProgress
    let snapshot_version = self
        .ledger_db
        .metadata_db()
        .get_synced_version()?
        .ok_or_else(|| AptosDbError::Other("No committed version found".to_string()))?;
    
    // Ensure start_version doesn't exceed snapshot
    ensure!(
        start_version <= snapshot_version,
        "start_version {} exceeds snapshot_version {}",
        start_version,
        snapshot_version
    );
    
    // Create ReadOptions with snapshot for each database
    let mut txn_opts = ReadOptions::default();
    txn_opts.set_snapshot(/* create snapshot */);
    
    // Apply same snapshot to all 5 iterators
    let txn_iter = self
        .ledger_db
        .transaction_db()
        .get_transaction_iter_with_opts(start_version, num_transactions, txn_opts.clone())?;
    // ... repeat for all 5 databases with same snapshot
    
    // Rest of implementation remains the same
}
```

**Alternative: Use Commit Lock for Backup Reads**

Acquire a read lock during backup operations that blocks commits:
- Requires implementing reader-writer lock semantics
- May impact write performance during backups

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use std::sync::{Arc, Barrier};
    use std::thread;
    
    #[test]
    fn test_concurrent_backup_commit_race() {
        // Setup: Create AptosDB with test data
        let db = setup_test_db();
        let barrier = Arc::new(Barrier::new(2));
        
        // Thread 1: Commit transaction
        let db1 = db.clone();
        let barrier1 = barrier.clone();
        let commit_handle = thread::spawn(move || {
            barrier1.wait(); // Synchronize start
            
            // Commit transaction N+1 (will complete in parallel to multiple DBs)
            db1.pre_commit_ledger(create_test_chunk(), false).unwrap();
            thread::sleep(Duration::from_millis(50)); // Simulate commit time
            db1.commit_ledger(version_n_plus_1, None, None).unwrap();
        });
        
        // Thread 2: Start backup
        let db2 = db.clone();
        let barrier2 = barrier.clone();
        let backup_handle = thread::spawn(move || {
            barrier2.wait(); // Synchronize start
            thread::sleep(Duration::from_millis(10)); // Let commit start
            
            // Create iterators sequentially (race condition window)
            let backup_handler = BackupHandler::new(
                db2.state_store.clone(),
                db2.ledger_db.clone()
            );
            
            let iter = backup_handler
                .get_transaction_iter(version_n, 2)
                .unwrap();
            
            // Collect data from backup
            iter.collect::<Vec<_>>()
        });
        
        commit_handle.join().unwrap();
        let backup_data = backup_handle.join().unwrap();
        
        // Verify inconsistency: backup may have transaction N+1's info 
        // but transaction N's events, creating impossible state
        verify_backup_consistency(&backup_data); // This should FAIL
    }
}
```

The test demonstrates that concurrent backup and commit operations can produce inconsistent backup data where different components (transaction, transaction_info, events, write_set, auxiliary_info) come from different consensus-committed versions.

---

**Notes:**

This vulnerability is particularly concerning because:
1. It only manifests during concurrent operations, making it difficult to detect in testing
2. The inconsistency may not be immediately apparent - only discovered when restoring the backup
3. RocksDB's iterator snapshot isolation gives a false sense of security - each iterator is isolated, but together they're not atomic
4. The `sync_commit_progress()` recovery mechanism doesn't help for online reads

### Citations

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L263-322)
```rust
    fn calculate_and_commit_ledger_and_state_kv(
        &self,
        chunk: &ChunkToCommit,
        skip_index_and_usage: bool,
    ) -> Result<HashValue> {
        let _timer = OTHER_TIMERS_SECONDS.timer_with(&["save_transactions__work"]);

        let mut new_root_hash = HashValue::zero();
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            // TODO(grao): Write progress for each of the following databases, and handle the
            // inconsistency at the startup time.
            //
            // TODO(grao): Consider propagating the error instead of panic, if necessary.
            s.spawn(|_| {
                self.commit_events(
                    chunk.first_version,
                    chunk.transaction_outputs,
                    skip_index_and_usage,
                )
                .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .write_set_db()
                    .commit_write_sets(chunk.first_version, chunk.transaction_outputs)
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .transaction_db()
                    .commit_transactions(
                        chunk.first_version,
                        chunk.transactions,
                        skip_index_and_usage,
                    )
                    .unwrap()
            });
            s.spawn(|_| {
                self.ledger_db
                    .persisted_auxiliary_info_db()
                    .commit_auxiliary_info(chunk.first_version, chunk.persisted_auxiliary_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_state_kv_and_ledger_metadata(chunk, skip_index_and_usage)
                    .unwrap()
            });
            s.spawn(|_| {
                self.commit_transaction_infos(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
            s.spawn(|_| {
                new_root_hash = self
                    .commit_transaction_accumulator(chunk.first_version, chunk.transaction_infos)
                    .unwrap()
            });
        });

        Ok(new_root_hash)
    }
```

**File:** storage/aptosdb/src/backup/backup_handler.rs (L56-76)
```rust
        let txn_iter = self
            .ledger_db
            .transaction_db()
            .get_transaction_iter(start_version, num_transactions)?;
        let mut txn_info_iter = self
            .ledger_db
            .transaction_info_db()
            .get_transaction_info_iter(start_version, num_transactions)?;
        let mut event_vec_iter = self
            .ledger_db
            .event_db()
            .get_events_by_version_iter(start_version, num_transactions)?;
        let mut write_set_iter = self
            .ledger_db
            .write_set_db()
            .get_write_set_iter(start_version, num_transactions)?;
        let mut persisted_aux_info_iter = self
            .ledger_db
            .persisted_auxiliary_info_db()
            .get_persisted_auxiliary_info_iter(start_version, num_transactions)?;

```

**File:** storage/aptosdb/src/state_store/mod.rs (L410-502)
```rust
    pub fn sync_commit_progress(
        ledger_db: Arc<LedgerDb>,
        state_kv_db: Arc<StateKvDb>,
        state_merkle_db: Arc<StateMerkleDb>,
        crash_if_difference_is_too_large: bool,
    ) {
        let ledger_metadata_db = ledger_db.metadata_db();
        if let Some(overall_commit_progress) = ledger_metadata_db
            .get_synced_version()
            .expect("DB read failed.")
        {
            info!(
                overall_commit_progress = overall_commit_progress,
                "Start syncing databases..."
            );
            let ledger_commit_progress = ledger_metadata_db
                .get_ledger_commit_progress()
                .expect("Failed to read ledger commit progress.");
            assert_ge!(ledger_commit_progress, overall_commit_progress);

            let state_kv_commit_progress = state_kv_db
                .metadata_db()
                .get::<DbMetadataSchema>(&DbMetadataKey::StateKvCommitProgress)
                .expect("Failed to read state K/V commit progress.")
                .expect("State K/V commit progress cannot be None.")
                .expect_version();
            assert_ge!(state_kv_commit_progress, overall_commit_progress);

            // LedgerCommitProgress was not guaranteed to commit after all ledger changes finish,
            // have to attempt truncating every column family.
            info!(
                ledger_commit_progress = ledger_commit_progress,
                "Attempt ledger truncation...",
            );
            let difference = ledger_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_ledger_db(ledger_db.clone(), overall_commit_progress)
                .expect("Failed to truncate ledger db.");

            // State K/V commit progress isn't (can't be) written atomically with the data,
            // because there are shards, so we have to attempt truncation anyway.
            info!(
                state_kv_commit_progress = state_kv_commit_progress,
                "Start state KV truncation..."
            );
            let difference = state_kv_commit_progress - overall_commit_progress;
            if crash_if_difference_is_too_large {
                assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
            }
            truncate_state_kv_db(
                &state_kv_db,
                state_kv_commit_progress,
                overall_commit_progress,
                std::cmp::max(difference as usize, 1), /* batch_size */
            )
            .expect("Failed to truncate state K/V db.");

            let state_merkle_max_version = get_max_version_in_state_merkle_db(&state_merkle_db)
                .expect("Failed to get state merkle max version.")
                .expect("State merkle max version cannot be None.");
            if state_merkle_max_version > overall_commit_progress {
                let difference = state_merkle_max_version - overall_commit_progress;
                if crash_if_difference_is_too_large {
                    assert_le!(difference, MAX_COMMIT_PROGRESS_DIFFERENCE);
                }
            }
            let state_merkle_target_version = find_tree_root_at_or_before(
                ledger_metadata_db,
                &state_merkle_db,
                overall_commit_progress,
            )
            .expect("DB read failed.")
            .unwrap_or_else(|| {
                panic!(
                    "Could not find a valid root before or at version {}, maybe it was pruned?",
                    overall_commit_progress
                )
            });
            if state_merkle_target_version < state_merkle_max_version {
                info!(
                    state_merkle_max_version = state_merkle_max_version,
                    target_version = state_merkle_target_version,
                    "Start state merkle truncation..."
                );
                truncate_state_merkle_db(&state_merkle_db, state_merkle_target_version)
                    .expect("Failed to truncate state merkle db.");
            }
        } else {
            info!("No overall commit progress was found!");
        }
    }
```

**File:** storage/aptosdb/src/db/mod.rs (L34-37)
```rust
    /// This is just to detect concurrent calls to `pre_commit_ledger()`
    pre_commit_lock: std::sync::Mutex<()>,
    /// This is just to detect concurrent calls to `commit_ledger()`
    commit_lock: std::sync::Mutex<()>,
```
