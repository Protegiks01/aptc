# Audit Report

## Title
Per-Stream Timeout Retry Counter Allows Malicious Peers to Force Repeated Stream Resets While Evading Reputation Penalties

## Summary
A malicious peer can exploit the per-stream `request_failure_count` tracking in the data streaming service to force honest nodes into multiple stream reinitialization cycles before being ignored. The vulnerability allows attackers to prevent state synchronization for approximately 10 minutes by timing out repeatedly while maintaining sufficient reputation score (above 25.0 ignore threshold) across 2-3 stream recreation cycles. [1](#0-0) 

## Finding Description

The state synchronization system tracks request failures using a per-stream counter `request_failure_count`, which gets reset to 0 whenever a new stream is created. When this counter reaches `max_request_retry` (default: 5), the stream terminates and sends an end-of-stream notification. [2](#0-1) 

When a timeout occurs, the peer reputation system applies a penalty by multiplying the peer's score by 0.95 (NOT_USEFUL_MULTIPLIER). Peers are only ignored when their score drops below 25.0. [3](#0-2) [4](#0-3) 

The exploit works as follows:

**Cycle 1:**
- Malicious peer (initial score: 50.0) is selected for data requests
- Peer times out 5 consecutive times with exponential backoff (10s, 20s, 40s, 60s, 60s)
- Each timeout triggers peer penalty: score *= 0.95
- After 5 timeouts: score = 50 * 0.95^5 ≈ 38.7 (above 25.0 threshold)
- Stream reaches `max_request_retry` and terminates [5](#0-4) 

**Stream Reinitialization:**
- Bootstrapper/ContinuousSyncer detects EndOfStream notification
- Calls `reset_active_stream()` which terminates the stream
- Next `drive_progress()` call creates a new stream with `request_failure_count = 0` [6](#0-5) 

**Cycle 2:**
- Same malicious peer (score 38.7) can be selected again
- 5 more timeouts, score drops to ≈ 30.0
- Still above ignore threshold, stream terminates again

**Cycle 3:**
- Peer (score 30.0) selected again
- 5 more timeouts, score drops to ≈ 23.3
- NOW below ignore threshold (25.0), peer is finally ignored [7](#0-6) 

The root cause is that timeouts trigger `notify_bad_response` with `ErrorType::NotUseful`, applying the 0.95 multiplier, but the failure counter resets per-stream rather than accumulating per-peer across stream recreations. [8](#0-7) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty criteria for the following reasons:

1. **Validator Node Slowdowns**: Nodes attempting to synchronize state are forced through multiple stream reinitialization cycles, wasting approximately 570 seconds (~10 minutes) with exponential backoff timeouts before the malicious peer is ignored.

2. **Significant Protocol Violation**: Prevents successful state synchronization during the attack window, breaking the state consistency invariant. New nodes cannot bootstrap properly, and nodes that fall behind cannot catch up efficiently.

3. **Availability Impact**: While not a total loss of liveness (the peer is eventually ignored), the attack significantly degrades the node's ability to maintain synchronized state with the network during the 10-minute attack window.

4. **Resource Exhaustion**: Each timeout cycle wastes computational resources and network bandwidth with exponentially increasing timeout durations (up to 60 seconds per retry).

The impact is multiplied if multiple malicious peers coordinate the attack, as each peer can independently force 2-3 stream resets before being ignored.

## Likelihood Explanation

The attack is **highly likely** to succeed because:

1. **Low Complexity**: Malicious peers simply need to delay or drop responses to cause timeouts - no sophisticated attack mechanism required.

2. **No Special Privileges**: Any network peer can perform this attack without validator access or special permissions.

3. **Timing Control**: Attackers can easily time out "just before completing data transfers" by monitoring request sizes and deliberately disconnecting after receiving requests but before sending responses.

4. **Mathematical Certainty**: The score calculation is deterministic: 50 * 0.95^15 ≈ 23.3, requiring exactly ~15 timeouts (3 cycles) to drop below the ignore threshold.

5. **Common Attack Surface**: State synchronization is a critical path for new nodes joining the network and for nodes recovering from downtime - a high-value target.

The attack becomes less effective in environments with many high-quality peers (malicious peer less likely to be selected), but remains exploitable when:
- Limited peer availability
- Network conditions favor the malicious peer (low latency, high priority)
- Target node is bootstrapping from scratch with no reputation history

## Recommendation

**Primary Fix**: Track timeout failures per-peer across stream lifecycles instead of per-stream.

Add a cross-stream failure tracker to the `PeerStates` structure:

```rust
// In state-sync/aptos-data-client/src/peer_states.rs
pub struct PeerState {
    // Existing fields...
    
    /// Track consecutive timeout failures across all streams
    consecutive_timeout_failures: u64,
    
    /// Timestamp of last timeout to enable decay
    last_timeout_time: Option<Instant>,
}
```

Modify the data stream to check peer-level failure counts:

```rust
// In state-sync/data-streaming-service/src/data_stream.rs
fn should_retry_with_peer(&self, peer: &PeerNetworkId) -> bool {
    let peer_timeout_count = self.aptos_data_client
        .get_peer_timeout_count(peer);
    
    // Use combined stream + peer-level count
    let total_failures = self.request_failure_count + peer_timeout_count;
    total_failures < self.streaming_service_config.max_request_retry
}
```

**Alternative Fix**: Implement faster peer score degradation for consecutive timeouts:

```rust
// Apply exponential penalty for repeated timeouts
const REPEATED_TIMEOUT_MULTIPLIER: f64 = 0.80; // More aggressive than 0.95

fn update_score_on_timeout(&mut self, is_consecutive: bool) {
    let multiplier = if is_consecutive {
        REPEATED_TIMEOUT_MULTIPLIER
    } else {
        NOT_USEFUL_MULTIPLIER
    };
    self.score = f64::max(self.score * multiplier, MIN_SCORE);
}
```

**Configuration Tuning**: Reduce `max_request_retry` from 5 to 3 to limit the attack window per stream cycle.

## Proof of Concept

```rust
#[tokio::test]
async fn test_timeout_retry_loop_exploit() {
    use aptos_config::config::{AptosDataClientConfig, DataStreamingServiceConfig};
    use aptos_data_client::peer_states::{PeerState, PeerStates};
    use std::sync::Arc;

    // Setup
    let mut config = AptosDataClientConfig::default();
    config.ignore_low_score_peers = true;
    let data_client_config = Arc::new(config);
    let peer_states = Arc::new(PeerStates::new(data_client_config.clone()));
    
    // Simulate malicious peer with initial score 50.0
    let malicious_peer = create_test_peer_network_id();
    
    // Cycle 1: 5 timeouts
    for _ in 0..5 {
        peer_states.update_score_error(malicious_peer, ErrorType::NotUseful);
    }
    let score_after_cycle_1 = peer_states.get_peer_score(&malicious_peer).unwrap();
    assert!(score_after_cycle_1 > 25.0, "Peer should not be ignored after cycle 1");
    assert!(score_after_cycle_1 < 40.0, "Score should have decreased significantly");
    
    // Cycle 2: 5 more timeouts (simulating new stream with reset counter)
    for _ in 0..5 {
        peer_states.update_score_error(malicious_peer, ErrorType::NotUseful);
    }
    let score_after_cycle_2 = peer_states.get_peer_score(&malicious_peer).unwrap();
    assert!(score_after_cycle_2 > 25.0, "Peer should still not be ignored after cycle 2");
    
    // Cycle 3: 5 more timeouts
    for _ in 0..5 {
        peer_states.update_score_error(malicious_peer, ErrorType::NotUseful);
    }
    let score_after_cycle_3 = peer_states.get_peer_score(&malicious_peer).unwrap();
    assert!(score_after_cycle_3 <= 25.0, "Peer should now be ignored after cycle 3");
    
    // Verify peer is ignored
    let is_ignored = peer_states.is_peer_ignored(&malicious_peer);
    assert!(is_ignored, "Malicious peer should be ignored after 15 timeouts");
    
    // Demonstrate time wasted:
    // Cycle 1: 10s + 20s + 40s + 60s + 60s = 190s
    // Cycle 2: 190s
    // Cycle 3: 190s
    // Total: 570 seconds (~10 minutes of blocked state synchronization)
}
```

## Notes

The vulnerability is exacerbated by the exponential backoff timeout strategy, which increases the time wasted per timeout (10s → 20s → 40s → 60s → 60s). While this backoff is intended to give slow peers more time to respond, it ironically provides malicious peers with more opportunity to waste the honest node's time.

The stream-level timeout mechanism (`num_consecutive_timeouts` in `DataStreamListener`) does not mitigate this attack because it only triggers when the stream produces no notifications at all. In this attack, the stream actively produces timeout error responses, so the stream-level timeout never activates. [9](#0-8)

### Citations

**File:** state-sync/data-streaming-service/src/data_stream.rs (L110-110)
```rust
    request_failure_count: u64,
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L446-454)
```rust
        if self.stream_engine.is_stream_complete()
            || self.request_failure_count >= self.streaming_service_config.max_request_retry
            || self.send_failure
        {
            if !self.send_failure && self.stream_end_notification_id.is_none() {
                self.send_end_of_stream_notification().await?;
            }
            return Ok(()); // There's nothing left to do
        }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L729-744)
```rust
    fn resend_data_client_request(
        &mut self,
        data_client_request: &DataClientRequest,
    ) -> Result<(), Error> {
        // Increment the number of client failures for this request
        self.request_failure_count += 1;

        // Resend the client request
        let pending_client_response = self.send_client_request(true, data_client_request.clone());

        // Push the pending response to the head of the sent requests queue
        self.get_sent_data_requests()?
            .push_front(pending_client_response);

        Ok(())
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L39-43)
```rust
const NOT_USEFUL_MULTIPLIER: f64 = 0.95;
/// Likely to be a malicious response.
const MALICIOUS_MULTIPLIER: f64 = 0.8;
/// Ignore a peer when their score dips below this threshold.
const IGNORE_PEER_THRESHOLD: f64 = 25.0;
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L152-160)
```rust
    fn is_ignored(&self) -> bool {
        // Only ignore peers if the config allows it
        if !self.data_client_config.ignore_low_score_peers {
            return false;
        }

        // Otherwise, ignore peers with a low score
        self.score <= IGNORE_PEER_THRESHOLD
    }
```

**File:** state-sync/aptos-data-client/src/peer_states.rs (L168-174)
```rust
    fn update_score_error(&mut self, error: ErrorType) {
        let multiplier = match error {
            ErrorType::NotUseful => NOT_USEFUL_MULTIPLIER,
            ErrorType::Malicious => MALICIOUS_MULTIPLIER,
        };
        self.score = f64::max(self.score * multiplier, MIN_SCORE);
    }
```

**File:** state-sync/state-sync-driver/src/bootstrapper.rs (L1538-1556)
```rust
    /// Resets the currently active data stream and speculative state
    pub async fn reset_active_stream(
        &mut self,
        notification_and_feedback: Option<NotificationAndFeedback>,
    ) -> Result<(), Error> {
        if let Some(active_data_stream) = &self.active_data_stream {
            let data_stream_id = active_data_stream.data_stream_id;
            utils::terminate_stream_with_feedback(
                &mut self.streaming_client,
                data_stream_id,
                notification_and_feedback,
            )
            .await?;
        }

        self.active_data_stream = None;
        self.speculative_stream_state = None;
        Ok(())
    }
```

**File:** state-sync/aptos-data-client/src/client.rs (L834-867)
```rust
                let client_error = match error {
                    aptos_storage_service_client::Error::RpcError(rpc_error) => match rpc_error {
                        RpcError::NotConnected(_) => {
                            Error::DataIsUnavailable(rpc_error.to_string())
                        },
                        RpcError::TimedOut => {
                            Error::TimeoutWaitingForResponse(rpc_error.to_string())
                        },
                        _ => Error::UnexpectedErrorEncountered(rpc_error.to_string()),
                    },
                    aptos_storage_service_client::Error::StorageServiceError(err) => {
                        Error::UnexpectedErrorEncountered(err.to_string())
                    },
                    _ => Error::UnexpectedErrorEncountered(error.to_string()),
                };

                warn!(
                    (LogSchema::new(LogEntry::StorageServiceResponse)
                        .event(LogEvent::ResponseError)
                        .request_type(&request.get_label())
                        .request_id(id)
                        .peer(&peer)
                        .error(&client_error))
                );

                increment_request_counter(
                    &metrics::ERROR_RESPONSES,
                    client_error.get_label(),
                    peer,
                );

                self.notify_bad_response(id, peer, &request, ErrorType::NotUseful);
                Err(client_error)
            },
```

**File:** state-sync/state-sync-driver/src/utils.rs (L200-237)
```rust
pub async fn get_data_notification(
    max_stream_wait_time_ms: u64,
    max_num_stream_timeouts: u64,
    active_data_stream: Option<&mut DataStreamListener>,
) -> Result<DataNotification, Error> {
    let active_data_stream = active_data_stream
        .ok_or_else(|| Error::UnexpectedError("The active data stream does not exist!".into()))?;

    let timeout_ms = Duration::from_millis(max_stream_wait_time_ms);
    if let Ok(data_notification) = timeout(timeout_ms, active_data_stream.select_next_some()).await
    {
        // Update the metrics for the data notification receive latency
        metrics::observe_duration(
            &metrics::DATA_NOTIFICATION_LATENCIES,
            metrics::NOTIFICATION_CREATE_TO_RECEIVE,
            data_notification.creation_time,
        );

        // Reset the number of consecutive timeouts for the data stream
        active_data_stream.num_consecutive_timeouts = 0;
        Ok(data_notification)
    } else {
        // Increase the number of consecutive timeouts for the data stream
        active_data_stream.num_consecutive_timeouts += 1;

        // Check if we've timed out too many times
        if active_data_stream.num_consecutive_timeouts >= max_num_stream_timeouts {
            Err(Error::CriticalDataStreamTimeout(format!(
                "{:?}",
                max_num_stream_timeouts
            )))
        } else {
            Err(Error::DataStreamNotificationTimeout(format!(
                "{:?}",
                timeout_ms
            )))
        }
    }
```
