# Audit Report

## Title
Incorrect `more` Flag in Epoch Change Proofs Causes State Sync Denial of Service

## Summary
The `get_epoch_ending_ledger_infos_by_size()` function hardcodes the `more` parameter to `false` when constructing `EpochChangeProof`, even when the response has been truncated due to size, time, or configuration limits. This causes clients to reject valid state sync responses, leading to a denial of service when syncing across many epochs. [1](#0-0) 

## Finding Description

The storage service server provides epoch change proofs to clients for state synchronization. The `EpochChangeProof` structure contains a `more` boolean field that indicates whether additional epoch changes exist beyond those included in the proof. [2](#0-1) 

This flag is critical for client verification logic. When a client receives an epoch change proof that doesn't reach the latest ledger info's epoch, it checks the `more` flag to determine if this is expected: [3](#0-2) 

If `more` is `false` but there's a gap between the proof's coverage and the latest ledger info, verification fails with "Inconsistent epoch change proof and latest ledger info".

The vulnerability occurs because the server-side implementation always sets `more = false`, regardless of whether the response was actually truncated: [4](#0-3) 

The response can be truncated for multiple reasons:
1. **Configuration limits**: `max_epoch_chunk_size` caps the number of epochs fetched
2. **Size constraints**: Response exceeds `max_response_size` 
3. **Time constraints**: Reading exceeds `max_storage_read_wait_time_ms`
4. **Iterator exhaustion**: Database iterator runs out of data [5](#0-4) 

When any truncation occurs, the server should set `more = true`, but it doesn't. The database layer correctly implements this logic: [6](#0-5) 

The database's `get_epoch_ending_ledger_infos()` method properly returns a `more` flag that gets incorporated into the `EpochChangeProof`: [7](#0-6) 

However, the storage service's new "size and time-aware chunking" implementation bypasses this and uses an iterator directly, then hardcodes `more = false`.

## Impact Explanation

This vulnerability causes **Critical Severity** denial of service for state synchronization:

**Scenario**: When a network has experienced many epoch changes (e.g., 100+ epochs), and a client attempts to sync from an old epoch:

1. Client requests epochs 1 through 100
2. Server calculates it can only return epochs 1-50 due to size limits
3. Server returns `EpochChangeProof` with epochs 1-50 and **`more = false`** (incorrect)
4. Client receives latest ledger info at epoch 100
5. Client verification sees: proof covers up to epoch 50, latest is epoch 100, `more = false`
6. Client rejects with "Inconsistent epoch change proof and latest ledger info"
7. Client cannot make progress in state synchronization

This breaks the **State Consistency** invariant: "State transitions must be atomic and verifiable via Merkle proofs." Clients are unable to verify and synchronize state, causing total loss of liveness for affected nodes.

This constitutes "Total loss of liveness/network availability" per the Aptos bug bounty criteria, qualifying as **Critical Severity**.

The existing test suite explicitly demonstrates this exact failure mode: [8](#0-7) 

## Likelihood Explanation

**Very High** - This bug will trigger automatically in production scenarios:

1. **Common condition**: Networks with many epochs (which naturally accumulate over time)
2. **No attacker needed**: This is a logic bug, not an attack vector
3. **Wide impact**: Affects any client syncing across more epochs than the chunk size allows
4. **Existing test shows awareness**: The test suite already knows this scenario must be handled correctly, indicating developers anticipated this situation

The likelihood of occurrence increases as:
- The network ages and accumulates more epochs
- Configuration limits (`max_epoch_chunk_size`) are restrictive
- Network conditions cause slow responses triggering time limits
- Epoch changes contain large validator sets (increasing size)

## Recommendation

The fix is straightforward: Calculate whether the response was truncated and set the `more` flag accordingly:

```rust
// Calculate if the response was truncated
let num_fetched = epoch_ending_ledger_infos.len() as u64;
let more = num_fetched < num_ledger_infos_to_fetch 
    || num_ledger_infos_to_fetch < expected_num_ledger_infos;

// Create the epoch change proof with correct 'more' flag
let epoch_change_proof = EpochChangeProof::new(epoch_ending_ledger_infos, more);
```

The response should be considered truncated (and `more = true`) if:
- The number of fetched ledger infos is less than `num_ledger_infos_to_fetch` (size/time truncation occurred)
- OR `num_ledger_infos_to_fetch` is less than `expected_num_ledger_infos` (config limit truncation occurred)

Replace line 289 in `state-sync/storage-service/server/src/storage.rs` with the corrected logic above.

## Proof of Concept

The vulnerability can be demonstrated using the existing test infrastructure. The test `test_ratchet_succeeds_with_more` already proves this scenario:

**Test Setup**: [9](#0-8) 

**Reproduction Steps**:

1. Create an epoch change proof that doesn't reach the latest ledger info
2. Set `more = false` (mimicking the buggy server behavior)
3. Attempt client verification
4. **Result**: Verification fails with "Should return Err when more is false and there's a gap"
5. Change `more = true` (correct behavior)
6. Attempt client verification again  
7. **Result**: Verification succeeds

To reproduce in a real network scenario:

1. Deploy a network and let it run through multiple epochs (> `max_epoch_chunk_size`)
2. Start a new client attempting to sync from genesis
3. Client will request epoch proofs from epoch 0 to current epoch N
4. Server will truncate response due to `max_epoch_chunk_size` limit
5. Server returns proof with `more = false` (bug)
6. Client verification will fail
7. Client cannot sync and remains stuck

**Notes**

This vulnerability demonstrates a critical mismatch between the database layer's correct implementation of the `more` flag and the storage service's new chunking implementation. The legacy implementation correctly delegates to the database layer which sets the flag properly, but the new "size and time-aware chunking" path reimplements this logic incorrectly.

The fix should ensure consistency between the database layer's behavior and the storage service layer's behavior by properly calculating the `more` flag based on actual truncation conditions rather than hardcoding it to `false`.

### Citations

**File:** state-sync/storage-service/server/src/storage.rs (L217-220)
```rust
        // Calculate the number of ledger infos to fetch
        let expected_num_ledger_infos = inclusive_range_len(start_epoch, expected_end_epoch)?;
        let max_num_ledger_infos = self.config.max_epoch_chunk_size;
        let num_ledger_infos_to_fetch = min(expected_num_ledger_infos, max_num_ledger_infos);
```

**File:** state-sync/storage-service/server/src/storage.rs (L256-286)
```rust
        while !response_progress_tracker.is_response_complete() {
            match epoch_ending_ledger_info_iterator.next() {
                Some(Ok(epoch_ending_ledger_info)) => {
                    // Calculate the number of serialized bytes for the epoch ending ledger info
                    let num_serialized_bytes = get_num_serialized_bytes(&epoch_ending_ledger_info)
                        .map_err(|error| Error::UnexpectedErrorEncountered(error.to_string()))?;

                    // Add the ledger info to the list
                    if response_progress_tracker
                        .data_items_fits_in_response(true, num_serialized_bytes)
                    {
                        epoch_ending_ledger_infos.push(epoch_ending_ledger_info);
                        response_progress_tracker.add_data_item(num_serialized_bytes);
                    } else {
                        break; // Cannot add any more data items
                    }
                },
                Some(Err(error)) => {
                    return Err(Error::StorageErrorEncountered(error.to_string()));
                },
                None => {
                    // Log a warning that the iterator did not contain all the expected data
                    warn!(
                        "The epoch ending ledger info iterator is missing data! \
                        Start epoch: {:?}, expected end epoch: {:?}, num ledger infos to fetch: {:?}",
                        start_epoch, expected_end_epoch, num_ledger_infos_to_fetch
                    );
                    break;
                },
            }
        }
```

**File:** state-sync/storage-service/server/src/storage.rs (L289-289)
```rust
        let epoch_change_proof = EpochChangeProof::new(epoch_ending_ledger_infos, false);
```

**File:** types/src/epoch_change.rs (L38-41)
```rust
pub struct EpochChangeProof {
    pub ledger_info_with_sigs: Vec<LedgerInfoWithSignatures>,
    pub more: bool,
}
```

**File:** types/src/trusted_state.rs (L183-186)
```rust
            } else if latest_li.ledger_info().epoch() > new_epoch && epoch_change_proof.more {
                epoch_change_li
            } else {
                bail!("Inconsistent epoch change proof and latest ledger info");
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L66-76)
```rust
    fn get_epoch_ending_ledger_infos(
        &self,
        start_epoch: u64,
        end_epoch: u64,
    ) -> Result<EpochChangeProof> {
        gauged_api("get_epoch_ending_ledger_infos", || {
            let (ledger_info_with_sigs, more) =
                Self::get_epoch_ending_ledger_infos(self, start_epoch, end_epoch)?;
            Ok(EpochChangeProof::new(ledger_info_with_sigs, more))
        })
    }
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L1044-1048)
```rust
        let (paging_epoch, more) = if end_epoch - start_epoch > limit as u64 {
            (start_epoch + limit as u64, true)
        } else {
            (end_epoch, false)
        };
```

**File:** types/src/unit_tests/trusted_state_test.rs (L363-411)
```rust
    fn test_ratchet_succeeds_with_more(
        (_vsets, mut lis_with_sigs, latest_li, accumulator) in arb_update_proof(
            1,    /* start epoch */
            1,    /* start version */
            3,    /* version delta */
            3..6, /* epoch changes */
            1..3, /* validators per epoch */
        ),
    ) {
        let initial_li_with_sigs = lis_with_sigs.remove(0);
        let initial_li = initial_li_with_sigs.ledger_info();
        let trusted_state = TrustedState::try_from_epoch_change_li(
            initial_li,
            accumulator.get_accumulator_summary(initial_li.version()),
        ).unwrap();

        // remove the last LI from the proof
        lis_with_sigs.pop();

        let expected_latest_epoch_change_li = lis_with_sigs.last().unwrap().clone();
        let expected_latest_version = expected_latest_epoch_change_li
            .ledger_info()
            .version();

        // ratcheting with more = false should fail, since the state proof claims
        // we're done syncing epoch changes but doesn't get us all the way to the
        // latest ledger info
        let mut change_proof = EpochChangeProof::new(lis_with_sigs, false /* more */);
        trusted_state
            .verify_and_ratchet_inner(&latest_li, &change_proof)
            .expect_err("Should return Err when more is false and there's a gap");

        // ratcheting with more = true is fine
        change_proof.more = true;
        let trusted_state_change = trusted_state
            .verify_and_ratchet_inner(&latest_li, &change_proof)
            .expect("Should succeed with more in EpochChangeProof");

        match trusted_state_change {
            TrustedStateChange::Epoch {
                new_state,
                latest_epoch_change_li,
            } => {
                assert_eq!(new_state.version(), expected_latest_version);
                assert_eq!(latest_epoch_change_li, &expected_latest_epoch_change_li);
            }
            _ => panic!("Unexpected ratchet result"),
        };
    }
```
