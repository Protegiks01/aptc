# Audit Report

## Title
Critical Liveness Failure: State Inconsistency Due to Missing Secret Share Manager Reset During State Sync

## Summary
The `reset()` function in the execution client fails to reset the `secret_share_manager` during state synchronization operations, while resetting `rand_manager` and `buffer_manager`. This creates a critical state inconsistency across the consensus pipeline that can lead to permanent consensus halt.

## Finding Description

The vulnerability exists in the consensus execution client's reset mechanism. During state sync operations (`sync_for_duration()` and `sync_to_target()`), the system calls `reset()` to synchronize all managers to a target round. However, the `reset()` function has a critical omission. [1](#0-0) 

The `reset()` function only extracts and sends reset signals to `rand_manager` and `buffer_manager`, but completely omits `reset_tx_to_secret_share_manager`. Compare this to the `end_epoch()` function which correctly resets all three managers: [2](#0-1) 

This creates a critical state inconsistency where:
- `rand_manager` is reset to the target round with cleared queue
- `buffer_manager` is reset to the target round with cleared state  
- `secret_share_manager` is NOT reset and retains stale state from before sync
- The coordinator is never reset and retains stale tracking information

**State Sync Trigger Points:** [3](#0-2) [4](#0-3) 

**Consensus Dependency on Secret Sharing:**

The coordinator waits for BOTH randomness AND secret sharing to be ready before forwarding blocks: [5](#0-4) 

When secret share manager has stale state while other managers have fresh state after reset, blocks cannot properly progress through the pipeline, causing consensus to halt.

**Silent Key Discard Mechanism:**

If aggregated keys arrive for blocks that don't exist in the queue, they are silently discarded: [6](#0-5) 

**Decided State Prevents Re-aggregation:**

Once a round is marked as "Decided", subsequent attempts to add shares return early: [7](#0-6) 

**Asynchronous Aggregation:**

The aggregation process is asynchronous via `spawn_blocking`, creating timing windows: [8](#0-7) 

**Event Loop Race Condition:**

The secret share manager processes reset and aggregated keys concurrently: [9](#0-8) 

## Impact Explanation

**Critical Severity: Total Loss of Liveness/Network Availability**

This vulnerability satisfies the Critical severity criteria for "Total loss of liveness/network availability":

1. **Permanent Consensus Halt**: The state inconsistency prevents blocks from progressing through the consensus pipeline, halting all validators.

2. **Network-Wide Impact**: All honest validators performing state sync will encounter this bug, causing coordinated failure.

3. **No Recovery Mechanism**: The codebase provides no way to detect or recover from this state inconsistency without manual intervention or restart.

4. **Triggered by Normal Operations**: State sync is routine (validator restarts, catching up after downtime, network healing).

## Likelihood Explanation

**High Likelihood**

This vulnerability has high probability of occurrence:

1. **Normal Operation Trigger**: Every state sync operation (`sync_for_duration`, `sync_to_target`) calls the buggy `reset()` function.

2. **No Attacker Required**: Pure logic bug in reset coordination.

3. **Common Scenarios**: Validator restarts, network catch-up, partition healing all trigger state sync.

4. **Timing Window**: Asynchronous aggregation creates race conditions during reset operations.

## Recommendation

Modify the `reset()` function to include `reset_tx_to_secret_share_manager` in the reset operation, matching the pattern used in `end_epoch()`:

```rust
async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
    let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager, reset_tx_to_secret_share_manager) = {
        let handle = self.handle.read();
        (
            handle.reset_tx_to_rand_manager.clone(),
            handle.reset_tx_to_buffer_manager.clone(),
            handle.reset_tx_to_secret_share_manager.clone(), // ADD THIS
        )
    };

    // Reset rand_manager (existing code)
    if let Some(mut reset_tx) = reset_tx_to_rand_manager {
        // ... existing reset logic ...
    }

    // Reset secret_share_manager (ADD THIS BLOCK)
    if let Some(mut reset_tx) = reset_tx_to_secret_share_manager {
        let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
        reset_tx
            .send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::TargetRound(target.commit_info().round()),
            })
            .await
            .map_err(|_| Error::SecretShareResetDropped)?;
        ack_rx.await.map_err(|_| Error::SecretShareResetDropped)?;
    }

    // Reset buffer_manager (existing code)
    if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
        // ... existing reset logic ...
    }

    Ok(())
}
```

## Proof of Concept

This is a logic bug in the reset coordination mechanism. The PoC would require:

1. Setting up a local testnet with secret sharing enabled
2. Triggering blocks to be processed with secret share aggregation in progress
3. Triggering state sync via `sync_to_target()` during aggregation
4. Observing that `secret_share_manager` retains stale state while other managers reset
5. Attempting to process new blocks and observing consensus halt

The bug is evident from code inspection: the `reset()` function objectively omits the secret share manager while `end_epoch()` includes it, demonstrating the inconsistency.

## Notes

The vulnerability is confirmed by direct code comparison between `reset()` (lines 674-709) and `end_epoch()` (lines 711-760) in execution_client.rs. The former omits `reset_tx_to_secret_share_manager` while the latter correctly includes it. This inconsistency in the reset mechanism creates dangerous state divergence during routine state sync operations, violating consensus liveness guarantees.

### Citations

**File:** consensus/src/pipeline/execution_client.rs (L311-365)
```rust
    fn make_coordinator(
        mut rand_manager_input_tx: UnboundedSender<OrderedBlocks>,
        mut rand_ready_block_rx: UnboundedReceiver<OrderedBlocks>,
        mut secret_share_manager_input_tx: UnboundedSender<OrderedBlocks>,
        mut secret_ready_block_rx: UnboundedReceiver<OrderedBlocks>,
    ) -> (
        UnboundedSender<OrderedBlocks>,
        futures_channel::mpsc::UnboundedReceiver<OrderedBlocks>,
    ) {
        let (ordered_block_tx, mut ordered_block_rx) = unbounded::<OrderedBlocks>();
        let (mut ready_block_tx, ready_block_rx) = unbounded::<OrderedBlocks>();

        tokio::spawn(async move {
            let mut inflight_block_tracker: HashMap<
                HashValue,
                (
                    OrderedBlocks,
                    /* rand_ready */ bool,
                    /* secret ready */ bool,
                ),
            > = HashMap::new();
            loop {
                let entry = select! {
                    Some(ordered_blocks) = ordered_block_rx.next() => {
                        let _ = rand_manager_input_tx.send(ordered_blocks.clone()).await;
                        let _ = secret_share_manager_input_tx.send(ordered_blocks.clone()).await;
                        let first_block_id = ordered_blocks.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.insert(first_block_id, (ordered_blocks, false, false));
                        inflight_block_tracker.entry(first_block_id)
                    },
                    Some(rand_ready_block) = rand_ready_block_rx.next() => {
                        let first_block_id = rand_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.1 = true;
                        })
                    },
                    Some(secret_ready_block) = secret_ready_block_rx.next() => {
                        let first_block_id = secret_ready_block.ordered_blocks.first().expect("Cannot be empty").id();
                        inflight_block_tracker.entry(first_block_id).and_modify(|result| {
                            result.2 = true;
                        })
                    },
                };
                let Entry::Occupied(o) = entry else {
                    unreachable!("Entry must exist");
                };
                if o.get().1 && o.get().2 {
                    let (_, (ordered_blocks, _, _)) = o.remove_entry();
                    let _ = ready_block_tx.send(ordered_blocks).await;
                }
            }
        });

        (ordered_block_tx, ready_block_rx)
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L642-659)
```rust
    async fn sync_for_duration(
        &self,
        duration: Duration,
    ) -> Result<LedgerInfoWithSignatures, StateSyncError> {
        fail_point!("consensus::sync_for_duration", |_| {
            Err(anyhow::anyhow!("Injected error in sync_for_duration").into())
        });

        // Sync for the specified duration
        let result = self.execution_proxy.sync_for_duration(duration).await;

        // Reset the rand and buffer managers to the new synced round
        if let Ok(latest_synced_ledger_info) = &result {
            self.reset(latest_synced_ledger_info).await?;
        }

        result
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L661-672)
```rust
    async fn sync_to_target(&self, target: LedgerInfoWithSignatures) -> Result<(), StateSyncError> {
        fail_point!("consensus::sync_to_target", |_| {
            Err(anyhow::anyhow!("Injected error in sync_to_target").into())
        });

        // Reset the rand and buffer managers to the target round
        self.reset(&target).await?;

        // TODO: handle the state sync error (e.g., re-push the ordered
        // blocks to the buffer manager when it's reset but sync fails).
        self.execution_proxy.sync_to_target(target).await
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L674-709)
```rust
    async fn reset(&self, target: &LedgerInfoWithSignatures) -> Result<()> {
        let (reset_tx_to_rand_manager, reset_tx_to_buffer_manager) = {
            let handle = self.handle.read();
            (
                handle.reset_tx_to_rand_manager.clone(),
                handle.reset_tx_to_buffer_manager.clone(),
            )
        };

        if let Some(mut reset_tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx: ack_tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::RandResetDropped)?;
            ack_rx.await.map_err(|_| Error::RandResetDropped)?;
        }

        if let Some(mut reset_tx) = reset_tx_to_buffer_manager {
            // reset execution phase and commit phase
            let (tx, rx) = oneshot::channel::<ResetAck>();
            reset_tx
                .send(ResetRequest {
                    tx,
                    signal: ResetSignal::TargetRound(target.commit_info().round()),
                })
                .await
                .map_err(|_| Error::ResetDropped)?;
            rx.await.map_err(|_| Error::ResetDropped)?;
        }

        Ok(())
    }
```

**File:** consensus/src/pipeline/execution_client.rs (L711-760)
```rust
    async fn end_epoch(&self) {
        let (
            reset_tx_to_rand_manager,
            reset_tx_to_buffer_manager,
            reset_tx_to_secret_share_manager,
        ) = {
            let mut handle = self.handle.write();
            handle.reset()
        };

        if let Some(mut tx) = reset_tx_to_rand_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop rand manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop rand manager");
        }

        if let Some(mut tx) = reset_tx_to_secret_share_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop secret share manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop secret share manager");
        }

        if let Some(mut tx) = reset_tx_to_buffer_manager {
            let (ack_tx, ack_rx) = oneshot::channel();
            tx.send(ResetRequest {
                tx: ack_tx,
                signal: ResetSignal::Stop,
            })
            .await
            .expect("[EpochManager] Fail to drop buffer manager");
            ack_rx
                .await
                .expect("[EpochManager] Fail to drop buffer manager");
        }
        self.execution_proxy.end_epoch();
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L186-190)
```rust
    fn process_aggregated_key(&mut self, secret_share_key: SecretSharedKey) {
        if let Some(item) = self.block_queue.item_mut(secret_share_key.metadata.round) {
            item.set_secret_shared_key(secret_share_key.metadata.round, secret_share_key);
        }
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_manager.rs (L353-378)
```rust
        while !self.stop {
            tokio::select! {
                Some(blocks) = incoming_blocks.next() => {
                    self.process_incoming_blocks(blocks).await;
                }
                Some(reset) = reset_rx.next() => {
                    while matches!(incoming_blocks.try_next(), Ok(Some(_))) {}
                    self.process_reset(reset);
                }
                Some(secret_shared_key) = self.decision_rx.next() => {
                    self.process_aggregated_key(secret_shared_key);
                }
                Some(request) = verified_msg_rx.next() => {
                    self.handle_incoming_msg(request);
                }
                _ = interval.tick().fuse() => {
                    self.observe_queue();
                },
            }
            let maybe_ready_blocks = self.block_queue.dequeue_ready_prefix();
            if !maybe_ready_blocks.is_empty() {
                self.process_ready_blocks(maybe_ready_blocks);
            }
        }
        info!("SecretShareManager stopped");
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L38-72)
```rust
    pub fn try_aggregate(
        self,
        secret_share_config: &SecretShareConfig,
        metadata: SecretShareMetadata,
        decision_tx: Sender<SecretSharedKey>,
    ) -> Either<Self, SecretShare> {
        if self.total_weight < secret_share_config.threshold() {
            return Either::Left(self);
        }
        observe_block(
            metadata.timestamp,
            BlockStage::SECRET_SHARING_ADD_ENOUGH_SHARE,
        );
        let dec_config = secret_share_config.clone();
        let self_share = self
            .get_self_share()
            .expect("Aggregated item should have self share");
        tokio::task::spawn_blocking(move || {
            let maybe_key = SecretShare::aggregate(self.shares.values(), &dec_config);
            match maybe_key {
                Ok(key) => {
                    let dec_key = SecretSharedKey::new(metadata, key);
                    let _ = decision_tx.unbounded_send(dec_key);
                },
                Err(e) => {
                    warn!(
                        epoch = metadata.epoch,
                        round = metadata.round,
                        "Aggregation error: {e}"
                    );
                },
            }
        });
        Either::Right(self_share)
    }
```

**File:** consensus/src/rand/secret_sharing/secret_share_store.rs (L156-182)
```rust
    fn add_share_with_metadata(
        &mut self,
        share: SecretShare,
        share_weights: &HashMap<Author, u64>,
    ) -> anyhow::Result<()> {
        let item = std::mem::replace(self, Self::new(Author::ONE));
        let share_weight = *share_weights
            .get(share.author())
            .expect("Author must exist in weights");
        let new_item = match item {
            SecretShareItem::PendingMetadata(mut share_aggregator) => {
                let metadata = share.metadata.clone();
                share_aggregator.retain(share.metadata(), share_weights);
                share_aggregator.add_share(share, share_weight);
                SecretShareItem::PendingDecision {
                    metadata,
                    share_aggregator,
                }
            },
            SecretShareItem::PendingDecision { .. } => {
                bail!("Cannot add self share in PendingDecision state");
            },
            SecretShareItem::Decided { .. } => return Ok(()),
        };
        let _ = std::mem::replace(self, new_item);
        Ok(())
    }
```
