# Audit Report

## Title
Indexer File Store Processor: Infinite Metadata Upload Retry Loop Can Cause Permanent Transaction Data Loss

## Summary
The file store processor's metadata update logic contains an infinite retry loop that, when combined with cache eviction, can lead to permanent loss of blockchain transaction data. If metadata file uploads fail persistently (e.g., due to GCS service disruptions), the processor becomes stuck while the cache continues evicting old transactions, resulting in unrecoverable data loss.

## Finding Description

The vulnerability exists in the interaction between three components:

**1. Infinite Retry Loop Without Timeout** [1](#0-0) 

When `update_file_store_metadata_with_timeout()` fails, the processor enters an infinite retry loop. It logs errors and increments a metric counter, but never gives up or panics. The process becomes stuck at this version.

**2. Active Cache Eviction** [2](#0-1) [3](#0-2) 

The cache actively evicts transactions older than 300,000 versions. When processing version N â‰¥ 300,000, version (N - 300,000) is deleted from Redis.

**3. Critical Ordering Issue** [4](#0-3) 

The cache's file store version marker is updated BEFORE the metadata file upload is attempted. This creates a window where:
- Transactions are uploaded to GCS/local storage
- Cache marker is updated 
- But metadata file update fails and retries infinitely

**Exploitation Path:**

1. File store processor successfully fetches transaction batch from Redis cache
2. Transactions are uploaded to GCS successfully [5](#0-4) 
3. Redis cache marker (`FILE_STORE_LATEST_VERSION`) is updated successfully
4. Metadata file upload to GCS fails (network partition, permission issues, service disruption)
5. Processor enters infinite retry loop, logging errors but making no progress
6. Meanwhile, cache worker continues processing new blockchain transactions
7. After ~300,000 new transactions (batch size: 1000) [6](#0-5) , original transaction versions are evicted from cache
8. When processor is killed/restarted, it reads old version from metadata file [7](#0-6) 
9. Attempts to fetch evicted transactions from cache
10. `get_transactions()` fails the `ensure!()` check when transaction count doesn't match [8](#0-7) 
11. Process panics, transactions permanently lost (not in cache, not reflected in metadata)

**Time Window:** At 1000 TPS: 300 seconds (5 minutes). At 5000 TPS: 60 seconds (1 minute).

## Impact Explanation

This qualifies as **Medium Severity** under the Aptos Bug Bounty Program criteria:
- **State inconsistencies requiring intervention**: The metadata file and actual stored transaction files become out of sync
- **Limited data loss**: Blockchain transaction data in the indexer system is permanently lost

While this doesn't directly affect consensus or the main blockchain (this is the indexer subsystem), it breaks the data availability guarantee for the indexer, which is critical infrastructure for dApps and ecosystem services.

## Likelihood Explanation

**Moderate to High Likelihood:**
- GCS service disruptions, though rare, do occur
- Network partitions between indexer and cloud storage are realistic
- Permission/configuration issues during deployments can trigger this
- The 1-5 minute window for data loss is concerningly short on busy networks
- No automatic recovery mechanism exists
- Error logging doesn't include actual error details, making diagnosis difficult [9](#0-8) 

## Recommendation

**1. Add Maximum Retry Limit with Circuit Breaker:**
```rust
const MAX_METADATA_UPLOAD_RETRIES: u32 = 50; // ~25 seconds with 500ms sleep
const METADATA_UPLOAD_FAILURE_PANIC_THRESHOLD: u32 = 100; // ~50 seconds

let mut retry_count = 0;
while self
    .file_store_operator
    .update_file_store_metadata_with_timeout(chain_id, batch_start_version)
    .await
    .is_err()
{
    retry_count += 1;
    tracing::error!(
        batch_start_version = batch_start_version,
        retry_count = retry_count,
        "Failed to update file store metadata. Retrying."
    );
    
    if retry_count >= METADATA_UPLOAD_FAILURE_PANIC_THRESHOLD {
        panic!(
            "Failed to update metadata after {} retries. \
             Panicking to prevent data loss from cache eviction.",
            retry_count
        );
    }
    
    std::thread::sleep(std::time::Duration::from_millis(500));
    METADATA_UPLOAD_FAILURE_COUNT.inc();
}
```

**2. Reorder Operations - Update Metadata Before Cache:**
Move the cache update to AFTER successful metadata update to maintain consistency.

**3. Add Detailed Error Logging:**
Log the actual error details, not just a generic message.

**4. Add Monitoring Alerts:**
Configure alerts when `METADATA_UPLOAD_FAILURE_COUNT` exceeds threshold within a time window.

## Proof of Concept

```rust
#[tokio::test]
async fn test_metadata_failure_causes_data_loss() {
    // Setup mock Redis cache and file store
    let mut mock_file_store = MockFileStoreOperator::new();
    
    // Configure metadata updates to always fail
    mock_file_store
        .expect_update_file_store_metadata_with_timeout()
        .returning(|_, _| Err(anyhow::anyhow!("GCS unavailable")));
    
    // Configure transaction uploads to succeed
    mock_file_store
        .expect_upload_transaction_batch()
        .returning(|_, txns| {
            let start = txns.first().unwrap().version;
            let end = txns.last().unwrap().version;
            Ok((start, end))
        });
    
    let mut processor = Processor {
        cache_operator: create_mock_cache(),
        file_store_operator: Box::new(mock_file_store),
        chain_id: 1,
    };
    
    // Run processor in background task
    let handle = tokio::spawn(async move {
        processor.run().await
    });
    
    // Simulate time passing and cache eviction
    tokio::time::sleep(Duration::from_secs(60)).await;
    
    // Kill the stuck processor
    handle.abort();
    
    // Verify: transactions uploaded but metadata not updated
    // On restart, would attempt to re-fetch evicted transactions
    // Result: data loss
}
```

## Notes

This vulnerability demonstrates a critical gap in error handling where operational resilience was sacrificed for simplicity. The infinite retry loop was likely intended to be resilient, but without bounds it creates a worse failure mode than immediate panicking would. The fix must balance between retry resilience and fail-fast behavior to prevent silent data loss.

### Citations

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L116-123)
```rust
        let metadata = self
            .file_store_operator
            .get_file_store_metadata()
            .await
            .unwrap();
        ensure!(metadata.chain_id == chain_id, "Chain ID mismatch.");

        let mut batch_start_version = metadata.version;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L183-186)
```rust
                    let (start, end) = file_store_operator_clone
                        .upload_transaction_batch(chain_id, transactions)
                        .await
                        .unwrap();
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L258-260)
```rust
            self.cache_operator
                .update_file_store_latest_version(batch_start_version)
                .await?;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-file-store/src/processor.rs (L261-273)
```rust
            while self
                .file_store_operator
                .update_file_store_metadata_with_timeout(chain_id, batch_start_version)
                .await
                .is_err()
            {
                tracing::error!(
                    batch_start_version = batch_start_version,
                    "Failed to update file store metadata. Retrying."
                );
                std::thread::sleep(std::time::Duration::from_millis(500));
                METADATA_UPLOAD_FAILURE_COUNT.inc();
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L23-23)
```rust
const CACHE_SIZE_EVICTION_LOWER_BOUND: u64 = 300_000_u64;
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L282-289)
```rust
            if version >= CACHE_SIZE_EVICTION_LOWER_BOUND {
                let key = CacheEntry::build_key(
                    version - CACHE_SIZE_EVICTION_LOWER_BOUND,
                    self.storage_format,
                )
                .to_string();
                redis_pipeline.cmd("DEL").arg(key).ignore();
            }
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/cache_operator.rs (L389-392)
```rust
        ensure!(
            transactions.len() == transaction_count as usize,
            "Failed to get all transactions from cache."
        );
```

**File:** ecosystem/indexer-grpc/indexer-grpc-utils/src/compression_util.rs (L12-12)
```rust
pub const FILE_ENTRY_TRANSACTION_COUNT: u64 = 1000;
```
