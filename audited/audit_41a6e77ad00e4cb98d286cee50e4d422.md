# Audit Report

## Title
Non-Atomic Pruner Progress Updates Cause Node Initialization Failures and State Inconsistencies

## Summary
The `write_pruner_progress()` function in LedgerDb updates pruner progress across 8 sub-databases using sequential, non-atomic write operations. Partial failures during this process leave sub-databases with inconsistent pruning boundaries, causing node initialization failures and potential state inconsistencies.

## Finding Description

The vulnerability exists in the ledger pruner progress persistence mechanism. When `save_min_readable_version()` is called (typically after fast sync completion), it invokes `LedgerDb::write_pruner_progress()`, which updates pruner progress for 8 sub-databases sequentially: [1](#0-0) 

Each sub-database update is a separate `db.put()` operation. For example, in EventDb: [2](#0-1) 

The `?` operator causes early return on any failure, leaving subsequent databases unupdated. This violates the **State Consistency** invariant requiring atomic state transitions.

**Critical Flow:**
1. Fast sync completes at version 1000
2. `save_min_readable_version(1000)` is called from the restore path: [3](#0-2) 

3. Sequential updates begin:
   - event_db → 1000 ✓
   - persisted_auxiliary_info_db → 1000 ✓  
   - transaction_accumulator_db → 1000 ✓
   - **[CRASH/DISK FULL/I/O ERROR]**
   - transaction_db → remains at old value (e.g., 800)
   - ledger_metadata_db → remains at 800 (source of truth)

4. On restart, `LedgerPruner::new()` reads `metadata_progress = 800`: [4](#0-3) 

5. Each sub-pruner initializes and attempts to "catch up": [5](#0-4) 

6. **Critical Bug:** EventStorePruner finds `progress = 1000` but `metadata_progress = 800`, calls `prune(1000, 800)`

7. The prune operation attempts `(800 - 1000) as usize`, causing integer underflow: [6](#0-5) 

The `(end - start) as usize` at line 202 wraps to a massive number (u64::MAX - 199), causing the iterator to attempt reading billions of non-existent versions.

**Inconsistent Validation:**
While `TransactionPruner` has safeguards against invalid ranges: [7](#0-6) 

Other sub-pruners like `EventStorePruner` lack this validation, causing initialization failures.

## Impact Explanation

**High Severity** - This bug causes:

1. **Total Node Initialization Failure**: Nodes experiencing partial pruner progress updates cannot restart successfully. The integer underflow causes either panic (debug builds) or infinite iteration attempting to read non-existent data (release builds), leading to timeout/hang.

2. **State Query Inconsistencies**: If some sub-databases successfully update their progress while others don't, the system advertises `min_readable_version = 800` (from metadata), but events for versions 800-1000 have been pruned while transactions haven't. Queries succeed validation but fail data retrieval. [8](#0-7) 

The validation only checks the overall `min_readable_version`, not individual sub-database states.

3. **Consensus Risk**: Different nodes experiencing partial failures at different points have divergent views of data availability. While this doesn't directly break consensus on state roots, it creates operational inconsistencies that could lead to network instability during coordinated operations (upgrades, synchronized fast syncs).

This meets **High Severity** criteria: "Validator node slowdowns" (complete initialization failure), "API crashes" (node cannot serve requests), and "Significant protocol violations" (state consistency guarantees broken).

## Likelihood Explanation

**High Likelihood** - This can occur in multiple realistic scenarios:

1. **Disk Space Exhaustion**: During fast sync, disk fills up after some sub-databases update
2. **I/O Errors**: Hardware failures, NFS disconnections, or filesystem corruption during write sequence
3. **Process Termination**: Node crashes, OOM kills, or SIGKILL during the update window
4. **Resource Contention**: High disk I/O load causes individual writes to timeout

The vulnerability window is small (milliseconds for 8 sequential writes) but occurs during critical state sync operations when nodes are already under load. Fast sync is a common operation for new validators joining the network or nodes catching up after downtime.

Nodes in sharded mode are MORE vulnerable since each sub-database is a separate physical RocksDB instance, increasing failure surface area: [9](#0-8) 

## Recommendation

**Immediate Fix**: Use atomic batch operations for all pruner progress updates:

```rust
pub(crate) fn write_pruner_progress(&self, version: Version) -> Result<()> {
    info!("Writing pruner progress {version} for all ledger sub pruners.");
    
    // Create a single atomic batch for all updates
    let mut batch = SchemaBatch::new();
    
    // Add all pruner progress updates to the batch
    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::EventPrunerProgress,
        &DbMetadataValue::Version(version),
    )?;
    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::PersistedAuxiliaryInfoPrunerProgress,
        &DbMetadataValue::Version(version),
    )?;
    // ... add all other sub-database progress keys
    batch.put::<DbMetadataSchema>(
        &DbMetadataKey::LedgerPrunerProgress,
        &DbMetadataValue::Version(version),
    )?;
    
    // Write atomically - all succeed or all fail
    self.ledger_metadata_db.write_schemas(batch)
}
```

**Additional Safeguards**:
1. Add validation in all sub-pruner `prune()` methods to check `target_version >= current_progress`
2. Add reconciliation logic to detect and repair inconsistent progress on startup
3. Log warnings when sub-pruner progress deviates from metadata progress

## Proof of Concept

```rust
// Integration test demonstrating the vulnerability
#[test]
fn test_partial_pruner_progress_update_failure() {
    // 1. Initialize AptosDB with version 800
    let db = setup_test_db();
    commit_transactions_up_to_version(&db, 800);
    
    // 2. Simulate fast sync completion to version 1000
    restore_transactions(&db, 801, 1000);
    
    // 3. Manually trigger partial failure scenario:
    //    Update some sub-databases but not others
    db.ledger_db.event_db().write_pruner_progress(1000).unwrap();
    db.ledger_db.persisted_auxiliary_info_db().write_pruner_progress(1000).unwrap();
    // Leave transaction_db, ledger_metadata_db at 800
    
    // 4. Restart: create new LedgerPruner (simulates node restart)
    let result = LedgerPruner::new(
        Arc::clone(&db.ledger_db),
        None
    );
    
    // 5. Verify initialization fails or hangs
    assert!(result.is_err() || /* hangs in event_store_pruner catch-up */);
    
    // 6. If initialization succeeds, verify inconsistent state
    if let Ok(pruner) = result {
        // metadata_progress = 800
        assert_eq!(pruner.progress(), 800);
        
        // But event_db has progress = 1000
        let event_progress = db.ledger_db.event_db_raw()
            .get::<DbMetadataSchema>(&DbMetadataKey::EventPrunerProgress)
            .unwrap().unwrap().expect_version();
        assert_eq!(event_progress, 1000);
        
        // Query for version 900 passes validation but fails on event retrieval
        assert!(db.error_if_ledger_pruned("test", 900).is_ok());
        assert!(db.ledger_db.event_db().get_events_by_version(900).is_err());
    }
}
```

**Notes:**
- The codebase uses `SchemaBatch` for atomic writes elsewhere, but `write_pruner_progress()` doesn't leverage this
- The issue is exacerbated in sharded mode where sub-databases are physically separate
- Current error handling (early return with `?`) guarantees partial updates rather than all-or-nothing semantics

### Citations

**File:** storage/aptosdb/src/ledger_db/mod.rs (L174-293)
```rust
        let ledger_db_folder = db_root_path.as_ref().join(LEDGER_DB_FOLDER_NAME);

        let mut event_db = None;
        let mut persisted_auxiliary_info_db = None;
        let mut transaction_accumulator_db = None;
        let mut transaction_auxiliary_data_db = None;
        let mut transaction_db = None;
        let mut transaction_info_db = None;
        let mut write_set_db = None;
        THREAD_MANAGER.get_non_exe_cpu_pool().scope(|s| {
            s.spawn(|_| {
                let event_db_raw = Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(EVENT_DB_NAME),
                        EVENT_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                );
                event_db = Some(EventDb::new(
                    event_db_raw.clone(),
                    EventStore::new(event_db_raw),
                ));
            });
            s.spawn(|_| {
                persisted_auxiliary_info_db = Some(PersistedAuxiliaryInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(PERSISTED_AUXILIARY_INFO_DB_NAME),
                        PERSISTED_AUXILIARY_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_accumulator_db = Some(TransactionAccumulatorDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_ACCUMULATOR_DB_NAME),
                        TRANSACTION_ACCUMULATOR_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_auxiliary_data_db = Some(TransactionAuxiliaryDataDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_AUXILIARY_DATA_DB_NAME),
                        TRANSACTION_AUXILIARY_DATA_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )))
            });
            s.spawn(|_| {
                transaction_db = Some(TransactionDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_DB_NAME),
                        TRANSACTION_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                transaction_info_db = Some(TransactionInfoDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(TRANSACTION_INFO_DB_NAME),
                        TRANSACTION_INFO_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
            s.spawn(|_| {
                write_set_db = Some(WriteSetDb::new(Arc::new(
                    Self::open_rocksdb(
                        ledger_db_folder.join(WRITE_SET_DB_NAME),
                        WRITE_SET_DB_NAME,
                        &rocksdb_configs.ledger_db_config,
                        env,
                        block_cache,
                        readonly,
                    )
                    .unwrap(),
                )));
            });
        });

        // TODO(grao): Handle data inconsistency.

        Ok(Self {
            ledger_metadata_db: LedgerMetadataDb::new(ledger_metadata_db),
            event_db: event_db.unwrap(),
            persisted_auxiliary_info_db: persisted_auxiliary_info_db.unwrap(),
            transaction_accumulator_db: transaction_accumulator_db.unwrap(),
            transaction_auxiliary_data_db: transaction_auxiliary_data_db.unwrap(),
            transaction_db: transaction_db.unwrap(),
            transaction_info_db: transaction_info_db.unwrap(),
            write_set_db: write_set_db.unwrap(),
            enable_storage_sharding: true,
        })
```

**File:** storage/aptosdb/src/ledger_db/mod.rs (L373-388)
```rust
    pub(crate) fn write_pruner_progress(&self, version: Version) -> Result<()> {
        info!("Fast sync is done, writing pruner progress {version} for all ledger sub pruners.");
        self.event_db.write_pruner_progress(version)?;
        self.persisted_auxiliary_info_db
            .write_pruner_progress(version)?;
        self.transaction_accumulator_db
            .write_pruner_progress(version)?;
        self.transaction_auxiliary_data_db
            .write_pruner_progress(version)?;
        self.transaction_db.write_pruner_progress(version)?;
        self.transaction_info_db.write_pruner_progress(version)?;
        self.write_set_db.write_pruner_progress(version)?;
        self.ledger_metadata_db.write_pruner_progress(version)?;

        Ok(())
    }
```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L47-52)
```rust
    pub(super) fn write_pruner_progress(&self, version: Version) -> Result<()> {
        self.db.put::<DbMetadataSchema>(
            &DbMetadataKey::EventPrunerProgress,
            &DbMetadataValue::Version(version),
        )
    }
```

**File:** storage/aptosdb/src/ledger_db/event_db.rs (L192-202)
```rust
    pub(crate) fn prune_event_indices(
        &self,
        start: Version,
        end: Version,
        mut indices_batch: Option<&mut SchemaBatch>,
    ) -> Result<Vec<usize>> {
        let mut ret = Vec::new();

        let mut current_version = start;

        for events in self.get_events_by_version_iter(start, (end - start) as usize)? {
```

**File:** storage/aptosdb/src/db/aptosdb_writer.rs (L225-234)
```rust
            self.ledger_pruner.save_min_readable_version(version)?;
            self.state_store
                .state_merkle_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .epoch_snapshot_pruner
                .save_min_readable_version(version)?;
            self.state_store
                .state_kv_pruner
                .save_min_readable_version(version)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/mod.rs (L129-134)
```rust
        let metadata_progress = ledger_metadata_pruner.progress()?;

        info!(
            metadata_progress = metadata_progress,
            "Created ledger metadata pruner, start catching up all sub pruners."
        );
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/event_store_pruner.rs (L90-106)
```rust
        let progress = get_or_initialize_subpruner_progress(
            ledger_db.event_db_raw(),
            &DbMetadataKey::EventPrunerProgress,
            metadata_progress,
        )?;

        let myself = EventStorePruner {
            ledger_db,
            internal_indexer_db,
        };

        info!(
            progress = progress,
            metadata_progress = metadata_progress,
            "Catching up EventStorePruner."
        );
        myself.prune(progress, metadata_progress)?;
```

**File:** storage/aptosdb/src/pruner/ledger_pruner/transaction_pruner.rs (L111-111)
```rust
        ensure!(end >= start, "{} must be >= {}", end, start);
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L261-271)
```rust
    pub(super) fn error_if_ledger_pruned(&self, data_type: &str, version: Version) -> Result<()> {
        let min_readable_version = self.ledger_pruner.get_min_readable_version();
        ensure!(
            version >= min_readable_version,
            "{} at version {} is pruned, min available version is {}.",
            data_type,
            version,
            min_readable_version
        );
        Ok(())
    }
```
