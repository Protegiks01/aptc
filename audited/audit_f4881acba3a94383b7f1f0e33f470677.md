# Audit Report

## Title
Consensus Divergence via Unverified WeightedConfigBlstrs in Weighted VUF Evaluation

## Summary
The weighted VUF (Verifiable Unpredictable Function) system used for consensus randomness generation lacks cryptographic verification that all validators use identical `WeightedConfigBlstrs` when deriving evaluations from aggregated proofs. This allows different validators using different configurations to compute different randomness values from the same inputs, causing immediate consensus divergence and chain splits.

## Finding Description

The Aptos consensus randomness system uses a weighted VUF implementation (PinkasWUF) where validators aggregate randomness shares and derive final randomness values. The critical vulnerability lies in the `derive_eval()` function's dependency on `WeightedConfigBlstrs` without any cryptographic verification. [1](#0-0) 

The `derive_eval()` function computes Lagrange coefficients based on the provided `wconfig` parameter, using it to determine virtual player IDs, weights, and the batch evaluation domain: [2](#0-1) 

Different `wconfig` values produce different virtual player IDs (line 295) and different batch evaluation domains (line 311), resulting in different Lagrange coefficients (line 312) and ultimately different evaluations.

**Critical Gap**: The `verify_proof()` function does NOT validate the `wconfig` used for aggregation: [3](#0-2) 

Notice that `verify_proof()` only performs cryptographic validation of proof shares without any `wconfig` parameter. There is no mechanism to verify that all validators are using the same configuration.

**Consensus Flow**: Each validator independently computes randomness: [4](#0-3) 

At line 130, each validator calls `aggregate_shares(&rand_config.wconfig, ...)` and at line 134 calls `derive_eval(&rand_config.wconfig, ...)` using their own `RandConfig`. The `wconfig` comes from on-chain DKG state: [5](#0-4) 

**Attack Scenarios**:

1. **Malicious Validator**: A compromised validator deliberately constructs a different `wconfig` with modified weights or thresholds, causing their randomness computation to diverge.

2. **State Sync Race Condition**: During epoch transitions, validators reading DKG state at different moments could observe different `DKGSessionMetadata` if updates are in progress.

3. **Rounding Algorithm Bug**: Despite fixed-point arithmetic, a platform-dependent or implementation bug in `DKGRounding::new()` could cause non-deterministic results: [6](#0-5) 

**Impact Chain**:
1. Validators compute different randomness values (R1 vs R2)
2. Different randomness gets included in block metadata: [7](#0-6) 

3. Block execution uses different randomness values
4. State roots diverge
5. **Chain split** - validators cannot agree on the canonical chain

## Impact Explanation

**Severity: Critical** (per Aptos bug bounty criteria)

This vulnerability causes **Consensus/Safety violations** - the most critical category. When validators compute different randomness values:

- **Chain Split**: Different validators commit different blocks with different state roots, fragmenting the network
- **Non-recoverable without Hardfork**: Once validators diverge on randomness, they cannot reconcile without manual intervention
- **Total Liveness Loss**: If >1/3 of validators diverge, consensus permanently stalls

The impact satisfies the Critical severity criteria: "Consensus/Safety violations" and "Non-recoverable network partition (requires hardfork)".

## Likelihood Explanation

**Likelihood: Medium**

While the system is designed for determinism through fixed-point arithmetic and on-chain state coordination, several realistic scenarios could trigger this:

1. **Implementation Bugs**: Any non-determinism in the rounding algorithm or wconfig construction
2. **Race Conditions**: Epoch transition timing where validators observe different on-chain states
3. **State Sync Issues**: Brief forks or state synchronization delays causing temporary inconsistencies
4. **Malicious Validators**: Compromised validators (though requiring insider access)

The lack of cryptographic defense-in-depth means ANY trigger condition immediately causes catastrophic failure with no recovery mechanism.

## Recommendation

**Fix 1: Cryptographically Bind wconfig to Proofs**

Include a commitment to the `wconfig` (or `DKGSessionMetadata`) in the aggregated proof structure and verify it during `verify_proof()`:

```rust
// In PinkasWUF::Proof
pub struct Proof {
    shares: Vec<(Player, G2Projective)>,
    wconfig_hash: [u8; 32],  // Add commitment
}

// In verify_proof
fn verify_proof(
    pp: &Self::PublicParameters,
    pk: &Self::PubKey,
    apks: &[Option<Self::AugmentedPubKeyShare>],
    msg: &[u8],
    proof: &Self::Proof,
    expected_wconfig_hash: [u8; 32],  // Add parameter
) -> anyhow::Result<()> {
    ensure!(
        proof.wconfig_hash == expected_wconfig_hash,
        "wconfig mismatch detected"
    );
    // ... existing verification
}
```

**Fix 2: Include DKGSessionMetadata Hash in Consensus**

Add the hash of `DKGSessionMetadata` to the epoch state or block metadata, making it consensus-verified:

```rust
pub struct EpochState {
    // ... existing fields
    pub dkg_metadata_hash: Option<HashValue>,  // Add this
}
```

**Fix 3: Validate wconfig During Epoch Transition**

Add explicit validation that all validators see the same wconfig:

```rust
impl RandConfig {
    pub fn verify_wconfig_consistency(
        &self,
        expected_total_weight: usize,
        expected_threshold: usize,
    ) -> anyhow::Result<()> {
        ensure!(
            self.wconfig.get_total_weight() == expected_total_weight,
            "wconfig weight mismatch"
        );
        ensure!(
            self.wconfig.get_threshold_weight() == expected_threshold,
            "wconfig threshold mismatch"
        );
        Ok(())
    }
}
```

## Proof of Concept

```rust
#[test]
fn test_wconfig_tampering_causes_divergence() {
    use aptos_dkg::weighted_vuf::traits::WeightedVUF;
    use aptos_dkg::weighted_vuf::pinkas::PinkasWUF as WVUF;
    use aptos_dkg::pvss::WeightedConfigBlstrs;
    
    // Setup: Create two different wconfigs
    let wconfig_honest = WeightedConfigBlstrs::new(
        3,
        vec![1, 1, 1, 1],  // 4 validators, threshold 3
    ).unwrap();
    
    let wconfig_malicious = WeightedConfigBlstrs::new(
        3,
        vec![2, 1, 1, 0],  // DIFFERENT weights - attacker has 2x weight
    ).unwrap();
    
    // Simulate validators creating shares (not shown for brevity)
    // All validators use honest wconfig to create shares
    let shares = create_test_shares(&wconfig_honest);
    
    // Honest validator aggregates with correct wconfig
    let proof_honest = WVUF::aggregate_shares(&wconfig_honest, &shares);
    
    // Malicious validator aggregates with wrong wconfig  
    let proof_malicious = WVUF::aggregate_shares(&wconfig_malicious, &shares);
    
    // Both derive evaluations
    let eval_honest = WVUF::derive_eval(
        &wconfig_honest,
        &pp,
        msg,
        &apks,
        &proof_honest,
        &thread_pool,
    ).unwrap();
    
    let eval_malicious = WVUF::derive_eval(
        &wconfig_malicious,
        &pp,
        msg,
        &apks,
        &proof_malicious,
        &thread_pool,
    ).unwrap();
    
    // PROOF: Different evaluations from different wconfigs
    assert_ne!(eval_honest, eval_malicious, 
        "VULNERABILITY: Different wconfigs produce different evaluations!");
    
    // This causes consensus divergence:
    // - Honest validators compute randomness from eval_honest
    // - Malicious validator computes randomness from eval_malicious
    // - Different randomness values → different block execution → chain split
}
```

**Notes**

The vulnerability is inherent in the design: the weighted VUF evaluation depends critically on the `WeightedConfigBlstrs` parameter, yet there is no cryptographic mechanism to ensure all validators use identical configurations. The system relies entirely on social consensus (all validators reading the same on-chain state and computing deterministically) without cryptographic defense-in-depth. While carefully designed determinism (fixed-point arithmetic) reduces likelihood, it cannot eliminate implementation bugs, race conditions, or malicious tampering that would cause immediate, catastrophic consensus failure.

### Citations

**File:** crates/aptos-dkg/src/weighted_vuf/pinkas/mod.rs (L192-208)
```rust
    fn derive_eval(
        wc: &WeightedConfigBlstrs,
        _pp: &Self::PublicParameters,
        _msg: &[u8],
        apks: &[Option<Self::AugmentedPubKeyShare>],
        proof: &Self::Proof,
        thread_pool: &ThreadPool,
    ) -> anyhow::Result<Self::Evaluation> {
        let (rhs, rks, lagr, ranges) =
            Self::collect_lagrange_coeffs_shares_and_rks(wc, apks, proof)?;

        // Compute the RK multiexps in parallel
        let lhs = Self::rk_multiexps(proof, rks, &lagr, &ranges, thread_pool);

        // Interpolate the WVUF evaluation in parallel
        Ok(Self::multi_pairing(lhs, rhs, thread_pool))
    }
```

**File:** crates/aptos-dkg/src/weighted_vuf/pinkas/mod.rs (L210-265)
```rust
    /// Verifies the proof shares (using batch verification)
    fn verify_proof(
        pp: &Self::PublicParameters,
        _pk: &Self::PubKey,
        apks: &[Option<Self::AugmentedPubKeyShare>],
        msg: &[u8],
        proof: &Self::Proof,
    ) -> anyhow::Result<()> {
        if proof.len() >= apks.len() {
            bail!("Number of proof shares ({}) exceeds number of APKs ({}) when verifying aggregated WVUF proof", proof.len(), apks.len());
        }

        // TODO: Fiat-Shamir transform instead of RNG
        let tau = random_scalar(&mut thread_rng());
        let taus = get_powers_of_tau(&tau, proof.len());

        // [share_i^{\tau^i}]_{i \in [0, n)}
        let shares = proof
            .iter()
            .map(|(_, share)| share)
            .zip(taus.iter())
            .map(|(share, tau)| share.mul(tau))
            .collect::<Vec<G2Projective>>();

        let mut pis = Vec::with_capacity(proof.len());
        for (player, _) in proof {
            if player.id >= apks.len() {
                bail!(
                    "Player index {} falls outside APK vector of length {}",
                    player.id,
                    apks.len()
                );
            }

            pis.push(
                apks[player.id]
                    .as_ref()
                    .ok_or_else(|| anyhow!("Missing APK for player {}", player.get_id()))?
                    .0
                    .pi,
            );
        }

        let h = Self::hash_to_curve(msg);
        let sum_of_taus: Scalar = taus.iter().sum();

        if multi_pairing(
            pis.iter().chain([pp.g_neg].iter()),
            shares.iter().chain([h.mul(sum_of_taus)].iter()),
        ) != Gt::identity()
        {
            bail!("Multipairing check in batched aggregate verification failed");
        }

        Ok(())
    }
```

**File:** crates/aptos-dkg/src/weighted_vuf/pinkas/mod.rs (L273-314)
```rust
    pub fn collect_lagrange_coeffs_shares_and_rks<'a>(
        wc: &WeightedConfigBlstrs,
        apks: &'a [Option<(RandomizedPKs, Vec<DealtPubKeyShare>)>],
        proof: &'a Vec<(Player, <Self as WeightedVUF>::ProofShare)>,
    ) -> anyhow::Result<(
        Vec<&'a G2Projective>,
        Vec<&'a Vec<G1Projective>>,
        Vec<Scalar>,
        Vec<Range<usize>>,
    )> {
        // Collect all the evaluation points associated with each player's augmented pubkey sub shares.
        let mut sub_player_ids = Vec::with_capacity(wc.get_total_weight());
        // The G2 shares
        let mut shares = Vec::with_capacity(proof.len());
        // The RKs of each player
        let mut rks = Vec::with_capacity(proof.len());
        // The starting & ending index of each player in the `lagr` coefficients vector
        let mut ranges = Vec::with_capacity(proof.len());

        let mut k = 0;
        for (player, share) in proof {
            for j in 0..wc.get_player_weight(player) {
                sub_player_ids.push(wc.get_virtual_player(player, j).id);
            }

            let apk = apks[player.id]
                .as_ref()
                .ok_or_else(|| anyhow!("Missing APK for player {}", player.get_id()))?;

            rks.push(&apk.0.rks);
            shares.push(share);

            let w = wc.get_player_weight(player);
            ranges.push(k..k + w);
            k += w;
        }

        // Compute the Lagrange coefficients associated with those evaluation points
        let batch_dom = wc.get_batch_evaluation_domain();
        let lagr = lagrange_coefficients(batch_dom, &sub_player_ids[..], &Scalar::ZERO);
        Ok((shares, rks, lagr, ranges))
    }
```

**File:** consensus/src/rand/rand_gen/types.rs (L97-148)
```rust
    fn aggregate<'a>(
        shares: impl Iterator<Item = &'a RandShare<Self>>,
        rand_config: &RandConfig,
        rand_metadata: RandMetadata,
    ) -> anyhow::Result<Randomness>
    where
        Self: Sized,
    {
        let timer = std::time::Instant::now();
        let mut apks_and_proofs = vec![];
        for share in shares {
            let id = rand_config
                .validator
                .address_to_validator_index()
                .get(share.author())
                .copied()
                .ok_or_else(|| {
                    anyhow!(
                        "Share::aggregate failed with invalid share author: {}",
                        share.author
                    )
                })?;
            let apk = rand_config
                .get_certified_apk(share.author())
                .ok_or_else(|| {
                    anyhow!(
                        "Share::aggregate failed with missing apk for share from {}",
                        share.author
                    )
                })?;
            apks_and_proofs.push((Player { id }, apk.clone(), share.share().share));
        }

        let proof = WVUF::aggregate_shares(&rand_config.wconfig, &apks_and_proofs);
        let metadata_serialized = bcs::to_bytes(&rand_metadata).map_err(|e| {
            anyhow!("Share::aggregate failed with metadata serialization error: {e}")
        })?;
        let eval = WVUF::derive_eval(
            &rand_config.wconfig,
            &rand_config.vuf_pp,
            metadata_serialized.as_slice(),
            &rand_config.get_all_certified_apk(),
            &proof,
            THREAD_MANAGER.get_exe_cpu_pool(),
        )
        .map_err(|e| anyhow!("Share::aggregate failed with WVUF derive_eval error: {e}"))?;
        debug!("WVUF derivation time: {} ms", timer.elapsed().as_millis());
        let eval_bytes = bcs::to_bytes(&eval)
            .map_err(|e| anyhow!("Share::aggregate failed with eval serialization error: {e}"))?;
        let rand_bytes = Sha3_256::digest(eval_bytes.as_slice()).to_vec();
        Ok(Randomness::new(rand_metadata, rand_bytes))
    }
```

**File:** consensus/src/epoch_manager.rs (L1128-1135)
```rust
        let rand_config = RandConfig::new(
            self.author,
            new_epoch,
            new_epoch_state.verifier.clone(),
            vuf_pp.clone(),
            keys,
            dkg_pub_params.pvss_config.wconfig.clone(),
        );
```

**File:** types/src/dkg/real_dkg/rounding/mod.rs (L295-361)
```rust
fn compute_profile_fixed_point(
    validator_stakes: &Vec<u64>,
    stake_per_weight: U64F64,
    secrecy_threshold_in_stake_ratio: U64F64,
    maybe_fast_secrecy_threshold_in_stake_ratio: Option<U64F64>,
) -> DKGRoundingProfile {
    // Use fixed-point arithmetic to ensure the same result across machines.
    // See paper for details of the rounding algorithm
    // https://eprint.iacr.org/2024/198
    let one = U64F64::from_num(1);
    let stake_sum: u64 = validator_stakes.iter().sum::<u64>();
    let stake_sum_fixed = U64F64::from_num(stake_sum);
    let mut delta_down_fixed = U64F64::from_num(0);
    let mut delta_up_fixed = U64F64::from_num(0);
    let mut validator_weights: Vec<u64> = vec![];
    for stake in validator_stakes {
        let ideal_weight_fixed = U64F64::from_num(*stake) / stake_per_weight;
        // rounded to the nearest integer
        let rounded_weight_fixed = (ideal_weight_fixed + (one / 2)).floor();
        let rounded_weight = rounded_weight_fixed.to_num::<u64>();
        validator_weights.push(rounded_weight);
        if ideal_weight_fixed > rounded_weight_fixed {
            delta_down_fixed += ideal_weight_fixed - rounded_weight_fixed;
        } else {
            delta_up_fixed += rounded_weight_fixed - ideal_weight_fixed;
        }
    }
    let weight_total: u64 = validator_weights.clone().into_iter().sum();
    let delta_total_fixed = delta_down_fixed + delta_up_fixed;
    let reconstruct_threshold_in_weights_fixed =
        (secrecy_threshold_in_stake_ratio * stake_sum_fixed / stake_per_weight + delta_up_fixed)
            .ceil()
            + one;
    let reconstruct_threshold_in_weights: u64 = min(
        weight_total,
        reconstruct_threshold_in_weights_fixed.to_num::<u64>(),
    );
    let stake_gap_fixed = stake_per_weight * delta_total_fixed / stake_sum_fixed;
    let reconstruct_threshold_in_stake_ratio = secrecy_threshold_in_stake_ratio + stake_gap_fixed;

    let (fast_reconstruct_threshold_in_stake_ratio, fast_reconstruct_threshold_in_weights) =
        if let Some(fast_secrecy_threshold_in_stake_ratio) =
            maybe_fast_secrecy_threshold_in_stake_ratio
        {
            let recon_threshold = fast_secrecy_threshold_in_stake_ratio + stake_gap_fixed;
            let recon_weight = min(
                weight_total,
                ((fast_secrecy_threshold_in_stake_ratio * stake_sum_fixed / stake_per_weight
                    + delta_up_fixed)
                    .ceil()
                    + one)
                    .to_num::<u64>(),
            );
            (Some(recon_threshold), Some(recon_weight))
        } else {
            (None, None)
        };

    DKGRoundingProfile {
        validator_weights,
        secrecy_threshold_in_stake_ratio,
        reconstruct_threshold_in_stake_ratio,
        reconstruct_threshold_in_weights,
        fast_reconstruct_threshold_in_stake_ratio,
        fast_reconstruct_threshold_in_weights,
    }
}
```

**File:** types/src/block_metadata_ext.rs (L23-34)
```rust
#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]
pub struct BlockMetadataWithRandomness {
    pub id: HashValue,
    pub epoch: u64,
    pub round: u64,
    pub proposer: AccountAddress,
    #[serde(with = "serde_bytes")]
    pub previous_block_votes_bitvec: Vec<u8>,
    pub failed_proposer_indices: Vec<u32>,
    pub timestamp_usecs: u64,
    pub randomness: Option<Randomness>,
}
```
