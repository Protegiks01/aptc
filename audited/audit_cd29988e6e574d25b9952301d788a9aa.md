# Audit Report

## Title
Health Checker State Inconsistency Allows Unhealthy Peers to Bypass Disconnection via Round-Unaware Failure Reset

## Summary
The `reset_peer_failures()` function unconditionally clears peer failure counters without validating round numbers, while `increment_peer_round_failure()` respects round semantics. This asymmetry allows malicious or unhealthy peers to reset their failure counters by sending incoming ping requests, bypassing the health check mechanism and avoiding disconnection indefinitely.

## Finding Description

The health checker protocol tracks peer health using round-based failure counting. The system maintains two critical state variables per peer: `round` (current health check round) and `failures` (accumulated failure count). [1](#0-0) 

The vulnerability stems from inconsistent round-awareness across three functions:

**Function 1: `increment_peer_round_failure()`** - This function correctly implements round semantics by only incrementing failures when the provided round is current or future. [2](#0-1) 

**Function 2: `reset_peer_round_state()`** - This function properly validates that the new round is newer before resetting both the round number and failures. [3](#0-2) 

**Function 3: `reset_peer_failures()` (VULNERABLE)** - This function unconditionally resets failures to zero without any round validation. [4](#0-3) 

The vulnerable function is called when handling incoming ping requests from remote peers. [5](#0-4) 

**Attack Scenario:**

1. Node A monitors peer B's health in round N
2. Node A sends multiple outgoing pings to B, which fail
3. Node A increments B's failure counter using `increment_peer_round_failure()` (respects round N)
4. B's failure count approaches `ping_failures_tolerated` threshold
5. **Before disconnection**, peer B sends an incoming ping request to Node A
6. Node A handles the ping via `handle_ping_request()` and calls `reset_peer_failures()`
7. B's failure counter is reset to 0, **even though we're still in round N where those failures occurred**
8. B avoids disconnection and can repeat this pattern indefinitely

The health checker's documented invariant states: "If a certain number of successive liveness probes for a peer fail, the HealthChecker initiates a disconnect from the peer." [6](#0-5) 

This invariant is broken because peer B can strategically send incoming pings to reset its failure counter within the same round, bypassing the "successive failures" requirement.

## Impact Explanation

This vulnerability qualifies as **Medium severity** under Aptos bug bounty criteria for the following reasons:

1. **State Inconsistency**: The health checker maintains inconsistent state where failures accumulated in round N can be erased within the same round N through incoming pings, violating round-based state management semantics.

2. **Network Health Degradation**: Unhealthy or malicious peers can maintain connections indefinitely by periodically sending incoming pings, even while consistently failing to respond to outgoing health checks.

3. **Validator Performance Impact**: If this affects validator nodes, unhealthy validators can remain in the connected peer set, potentially degrading consensus performance and increasing network latency.

4. **Resource Exhaustion**: Nodes waste connection slots and resources on peers that fail health checks but game the system to avoid disconnection.

5. **Security Mechanism Bypass**: The entire purpose of the health checker—ensuring peer liveness and disconnecting unhealthy peers—can be systematically circumvented.

This does not constitute Critical or High severity because:
- No direct fund loss or consensus safety violation occurs
- Network does not partition or lose total availability
- Individual nodes can still manually disconnect problematic peers
- The impact is limited to network health rather than protocol correctness

However, it clearly meets Medium severity criteria: "State inconsistencies requiring intervention" and impacts network operational health.

## Likelihood Explanation

**Likelihood: High**

The attack is highly likely to occur for several reasons:

1. **Low Attacker Requirements**: Any connected peer can exploit this vulnerability. No special privileges, validator status, or cryptographic capabilities are required.

2. **Simple Execution**: The attack requires only sending periodic ping messages, which is normal health checker protocol behavior. No complex message crafting or timing coordination is needed.

3. **Independent Health Checkers**: Each node runs its own independent health checker with its own `ping_interval` and round counter. Peers can send pings at any time without coordination with the target node's health check rounds.

4. **Observable Behavior**: A malicious peer can observe that it's becoming unhealthy (e.g., network issues, resource constraints) and proactively send incoming pings to avoid disconnection.

5. **Undetectable**: From Node A's perspective, receiving incoming pings from peer B appears as normal health checker protocol activity. There's no way to distinguish between legitimate health checks and strategic exploitation.

6. **Natural Occurrence**: Even non-malicious peers experiencing intermittent network issues might inadvertently trigger this behavior, as their health checker independently sends pings while simultaneously failing to respond to incoming pings due to network problems.

## Recommendation

The vulnerability should be fixed by making `reset_peer_failures()` round-aware, similar to `increment_peer_round_failure()` and `reset_peer_round_state()`. 

**Option 1: Add round parameter and validation to `reset_peer_failures()`**

Modify the function signature to accept a round parameter and only reset failures if the round is newer:

```rust
pub fn reset_peer_failures(&mut self, peer_id: PeerId, round: u64) {
    if let Some(health_check_data) = self.health_check_data.write().get_mut(&peer_id) {
        // Only reset failures if moving to a new round or if current round matches
        if round >= health_check_data.round {
            health_check_data.failures = 0;
        }
    }
}
```

Then update the caller in `handle_ping_request()` to pass the current round:

```rust
// Line 303 in mod.rs
self.network_interface.reset_peer_failures(peer_id, self.round);
```

**Option 2: Remove `reset_peer_failures()` call from incoming ping handler**

Remove the call to `reset_peer_failures()` from `handle_ping_request()` entirely. Let failures only be reset through successful outgoing ping responses via `reset_peer_round_state()`. This ensures failures are only cleared when the peer successfully responds to our health checks, not when it initiates its own.

**Option 3: Use `reset_peer_round_state()` instead**

Replace the call to `reset_peer_failures()` with `reset_peer_round_state()`, but this would require the incoming ping to include round information from the sender, which may not be desirable.

**Recommended Approach: Option 1** provides the most balanced solution, maintaining the ability to use incoming pings as a health signal (as mentioned in the "Future Work" comments) while ensuring round consistency.

## Proof of Concept

The following Rust test demonstrates the vulnerability:

```rust
#[tokio::test]
async fn test_health_check_failure_reset_vulnerability() {
    use crate::protocols::health_checker::{
        builder::HealthCheckerBuilder,
        interface::HealthCheckNetworkInterface,
    };
    use aptos_time_service::TimeService;
    use std::time::Duration;

    // Setup health checker for Node A monitoring Peer B
    let time_service = TimeService::mock();
    let (network_client, mut network_events) = /* setup network */;
    let mut health_interface = HealthCheckNetworkInterface::new(network_client, network_events);
    
    let peer_b = PeerId::random();
    let round = 100;
    
    // Peer B connects - Node A creates health data
    health_interface.create_peer_and_health_data(peer_b, round);
    
    // Node A's health checker performs outgoing pings to Peer B in round 100
    // All pings fail - increment failures 5 times
    for _ in 0..5 {
        health_interface.increment_peer_round_failure(peer_b, round);
    }
    
    // Verify failures accumulated
    assert_eq!(health_interface.get_peer_failures(peer_b), Some(5));
    
    // ATTACK: Peer B sends an incoming ping to Node A
    // This triggers handle_ping_request() which calls reset_peer_failures()
    health_interface.reset_peer_failures(peer_b);
    
    // VULNERABILITY: Failures are reset to 0 even though we're still in round 100
    // where those failures occurred
    assert_eq!(health_interface.get_peer_failures(peer_b), Some(0));
    
    // Peer B has successfully bypassed health check disconnection
    // Node A continues pinging in round 100, failures increment again
    health_interface.increment_peer_round_failure(peer_b, round);
    assert_eq!(health_interface.get_peer_failures(peer_b), Some(1));
    
    // Peer B can repeat this pattern indefinitely to avoid disconnection
    // Even though it consistently fails outgoing health checks
}

#[tokio::test]
async fn test_correct_behavior_with_round_advancement() {
    // This test shows the CORRECT behavior via reset_peer_round_state()
    let mut health_interface = /* setup */;
    let peer_b = PeerId::random();
    
    // Round 100: accumulate failures
    health_interface.create_peer_and_health_data(peer_b, 100);
    for _ in 0..5 {
        health_interface.increment_peer_round_failure(peer_b, 100);
    }
    assert_eq!(health_interface.get_peer_failures(peer_b), Some(5));
    
    // Advance to round 101 - this correctly resets failures
    health_interface.reset_peer_round_state(peer_b, 101);
    assert_eq!(health_interface.get_peer_failures(peer_b), Some(0));
    
    // Attempting to reset with stale round does nothing (correct behavior)
    health_interface.increment_peer_round_failure(peer_b, 101);
    assert_eq!(health_interface.get_peer_failures(peer_b), Some(1));
    
    // Try to reset with same round 101 - should not reset (correct)
    health_interface.reset_peer_round_state(peer_b, 101);
    assert_eq!(health_interface.get_peer_failures(peer_b), Some(1));
}
```

This PoC demonstrates:
1. The vulnerability: `reset_peer_failures()` clears failures within the same round
2. The correct behavior: `reset_peer_round_state()` only clears failures when advancing rounds
3. The exploitability: A peer can strategically reset its failure counter by sending incoming pings

### Citations

**File:** network/framework/src/protocols/health_checker/interface.rs (L26-30)
```rust
#[derive(Clone, Copy, Default, Debug, Eq, PartialEq)]
pub struct HealthCheckData {
    pub round: u64,
    pub failures: u64,
}
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L110-116)
```rust
    pub fn increment_peer_round_failure(&mut self, peer_id: PeerId, round: u64) {
        if let Some(health_check_data) = self.health_check_data.write().get_mut(&peer_id) {
            if health_check_data.round <= round {
                health_check_data.failures += 1;
            }
        }
    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L120-124)
```rust
    pub fn reset_peer_failures(&mut self, peer_id: PeerId) {
        if let Some(health_check_data) = self.health_check_data.write().get_mut(&peer_id) {
            health_check_data.failures = 0;
        }
    }
```

**File:** network/framework/src/protocols/health_checker/interface.rs (L128-135)
```rust
    pub fn reset_peer_round_state(&mut self, peer_id: PeerId, round: u64) {
        if let Some(health_check_data) = self.health_check_data.write().get_mut(&peer_id) {
            if round > health_check_data.round {
                health_check_data.round = round;
                health_check_data.failures = 0;
            }
        }
    }
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L10-12)
```rust
//! If a certain number of successive liveness probes for a peer fail, the HealthChecker initiates a
//! disconnect from the peer. It relies on ConnectivityManager or the remote peer to re-establish
//! the connection.
```

**File:** network/framework/src/protocols/health_checker/mod.rs (L302-303)
```rust
        // Record Ingress HC here and reset failures.
        self.network_interface.reset_peer_failures(peer_id);
```
