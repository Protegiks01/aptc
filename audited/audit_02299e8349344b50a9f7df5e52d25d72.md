# Audit Report

## Title
Mempool Peer Prioritization Panic Due to Zero Latency Slack Configuration

## Summary
A configuration-dependent assertion failure in the mempool's peer prioritization system causes fullnode crashes when `latency_slack_between_top_upstream_peers` is set to 0 and all connected peers have ping latency data. This results in a denial of service affecting node availability.

## Finding Description

The vulnerability exists in the `update_sender_bucket_for_peers()` function within the mempool's load balancing logic. When selecting top upstream peers for transaction forwarding, the function uses a latency-based filtering mechanism that fails under specific conditions. [1](#0-0) 

The code populates a `top_peers` vector by filtering peers based on their ping latencies. For non-ValidatorFullNode nodes, or when VFN nodes have no VFN network peers, the fallback logic activates: [2](#0-1) 

The critical issue is in the peer selection condition. A peer is added to `top_peers` if any of these conditions are true:
1. `base_ping_latency.is_none()` (first peer has no latency data)
2. `ping_latency.is_none()` (current peer has no latency data)  
3. `ping_latency < base_ping_latency + slack`

**Vulnerability Scenario:**
When `latency_slack_between_top_upstream_peers` is configured to 0 milliseconds AND all peers have ping latency data:

- For the first peer: `ping_latency = base_ping_latency` (e.g., 0.05s)
- Condition check: `0.05 < 0.05 + 0` evaluates to `0.05 < 0.05` = **FALSE**
- The first peer is NOT added to `top_peers`
- All subsequent peers with higher latencies also fail the check
- Result: `top_peers` remains empty

This violates an assertion that expects `top_peers` to be non-empty when `prioritized_peers` is non-empty: [3](#0-2) 

The node panics at this assertion, causing immediate shutdown. Additionally, if the assertion were somehow bypassed, line 408 would cause a division-by-zero panic when computing `peer_index % top_peers.len()`: [4](#0-3) 

The vulnerability is triggered during periodic peer updates, which occur every 1 second by default: [5](#0-4) 

The configuration parameter has no validation to prevent zero values: [6](#0-5) 

## Impact Explanation

**High Severity** - This vulnerability causes validator fullnodes (VFNs) and public fullnodes (PFNs) to crash when the misconfiguration is present, meeting the Aptos bug bounty criteria for "Validator node slowdowns" and "API crashes" (High Severity up to $50,000).

**Affected Nodes:**
- All fullnodes (VFN and PFN) with `latency_slack_between_top_upstream_peers` set to 0
- Validators are unaffected as they skip peer prioritization logic [7](#0-6) 

**Impact Details:**
1. **Immediate node crash** - Unrecoverable panic requiring manual restart
2. **Continuous crash loop** - Node crashes every ~1 second upon restart if peers still have latency data
3. **Network partition risk** - Multiple misconfigured nodes offline simultaneously
4. **Transaction relay disruption** - Mempool transaction propagation fails for affected nodes

While this requires a configuration error, the lack of validation makes it easy to accidentally trigger, and the failure mode is catastrophic rather than graceful.

## Likelihood Explanation

**Medium-High Likelihood** for the following reasons:

**Triggering Conditions:**
1. Node operator sets `latency_slack_between_top_upstream_peers: 0` in custom configuration
2. All connected upstream peers have ping latency monitoring data available
3. Node is a fullnode (VFN or PFN), not a validator

**Likelihood Factors:**

*In Favor of Occurrence:*
- No configuration validation prevents setting slack to 0
- Operators might set 0 thinking it means "exact match only" or "disable load balancing"
- Default configurations across multiple thresholds could be globally changed
- Ping latencies are typically available after peer connection stabilizes [8](#0-7) 

*Against Occurrence:*
- Default configuration uses non-zero values (50-150ms)
- Requires explicit custom configuration
- Most operators use default configurations

However, the complete lack of validation combined with a non-obvious failure mode significantly increases the risk of accidental misconfiguration in production environments.

## Recommendation

**Immediate Fix - Add Configuration Validation:**

Add validation to the `MempoolConfig::sanitize()` method to reject zero or negative slack values:

```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        // Validate load balancing thresholds
        for threshold in &self.load_balancing_thresholds {
            if threshold.latency_slack_between_top_upstream_peers == 0 {
                return Err(Error::ConfigSanitizerFailed(
                    "latency_slack_between_top_upstream_peers".to_string(),
                    "Must be greater than 0 to ensure at least one peer is selected".to_string(),
                ));
            }
        }
        Ok(())
    }
}
```

**Defense-in-Depth Fix - Adjust Selection Logic:**

Modify the peer selection condition to use `<=` instead of `<`, ensuring the base peer is always included when slack is 0:

```rust
if base_ping_latency.is_none()
    || ping_latency.is_none()
    || ping_latency.unwrap()
        <= base_ping_latency.unwrap()  // Changed from < to <=
            + (threshold_config.latency_slack_between_top_upstream_peers as f64)
                / 1000.0
{
    top_peers.push(*peer);
}
```

**Additional Safeguard:**

Remove the assertion and handle the empty `top_peers` case gracefully:

```rust
if top_peers.is_empty() {
    warn!("No top peers selected despite having prioritized peers. Using first peer as fallback.");
    if let Some(first_peer) = self.prioritized_peers.read().first() {
        top_peers = vec![*first_peer];
    }
}
```

## Proof of Concept

```rust
#[test]
#[should_panic(expected = "assertion failed")]
fn test_zero_latency_slack_causes_panic() {
    use aptos_config::config::{LoadBalancingThresholdConfig, MempoolConfig, NodeType};
    use aptos_time_service::TimeService;
    
    // Create config with zero latency slack
    let mempool_config = MempoolConfig {
        load_balancing_thresholds: vec![LoadBalancingThresholdConfig {
            avg_mempool_traffic_threshold_in_tps: 0,
            latency_slack_between_top_upstream_peers: 0, // VULNERABLE CONFIG
            max_number_of_upstream_peers: 2,
        }],
        ..MempoolConfig::default()
    };
    
    let mut prioritized_peers_state = PrioritizedPeersState::new(
        mempool_config,
        NodeType::PublicFullnode,
        TimeService::mock(),
    );
    
    // Create multiple peers, all with ping latency data
    let peer_metadata_1 = create_metadata_with_distance_and_latency(1, 0.05);
    let peer_1 = (create_public_peer(), Some(&peer_metadata_1));
    
    let peer_metadata_2 = create_metadata_with_distance_and_latency(1, 0.06);
    let peer_2 = (create_public_peer(), Some(&peer_metadata_2));
    
    let peer_metadata_3 = create_metadata_with_distance_and_latency(1, 0.07);
    let peer_3 = (create_public_peer(), Some(&peer_metadata_3));
    
    let all_peers = vec![peer_1, peer_2, peer_3];
    
    // This should panic due to assertion failure in update_sender_bucket_for_peers
    prioritized_peers_state.update_prioritized_peers(all_peers, 1000, 1000);
}
```

**Notes:**
- The vulnerability requires explicit misconfiguration, but lack of validation makes it easy to trigger accidentally
- The panic occurs during normal operation (periodic peer updates), not just at startup
- The fix requires minimal code changes to add validation and defensive programming
- This affects network stability as multiple misconfigured nodes could fail simultaneously

### Citations

**File:** mempool/src/shared_mempool/priority.rs (L272-280)
```rust
    fn update_sender_bucket_for_peers(
        &mut self,
        peer_monitoring_data: &HashMap<PeerNetworkId, Option<&PeerMonitoringMetadata>>,
        num_mempool_txns_received_since_peers_updated: u64,
        num_committed_txns_received_since_peers_updated: u64,
    ) {
        // TODO: If the top peer set didn't change, then don't change the Primary sender bucket assignment.
        // TODO: (Minor) If the load is low, don't do load balancing for Failover buckets.
        assert!(self.prioritized_peers.read().len() == peer_monitoring_data.len());
```

**File:** mempool/src/shared_mempool/priority.rs (L362-389)
```rust
        if top_peers.is_empty() {
            let base_ping_latency = self.prioritized_peers.read().first().and_then(|peer| {
                peer_monitoring_data
                    .get(peer)
                    .and_then(|metadata| get_peer_ping_latency(metadata))
            });

            // Extract top peers with ping latency less than base_ping_latency + 50 ms
            for peer in self.prioritized_peers.read().iter() {
                if top_peers.len() >= num_top_peers as usize {
                    break;
                }

                let ping_latency = peer_monitoring_data
                    .get(peer)
                    .and_then(|metadata| get_peer_ping_latency(metadata));

                if base_ping_latency.is_none()
                    || ping_latency.is_none()
                    || ping_latency.unwrap()
                        < base_ping_latency.unwrap()
                            + (threshold_config.latency_slack_between_top_upstream_peers as f64)
                                / 1000.0
                {
                    top_peers.push(*peer);
                }
            }
        }
```

**File:** mempool/src/shared_mempool/priority.rs (L395-397)
```rust
        assert!(top_peers.len() <= num_top_peers as usize);
        // Top peers shouldn't be empty if prioritized_peers is not zero
        assert!(self.prioritized_peers.read().is_empty() || !top_peers.is_empty());
```

**File:** mempool/src/shared_mempool/priority.rs (L402-409)
```rust
            let mut peer_index = 0;
            for bucket_index in 0..self.mempool_config.num_sender_buckets {
                self.peer_to_sender_buckets
                    .entry(*top_peers.get(peer_index).unwrap())
                    .or_default()
                    .insert(bucket_index, BroadcastPeerPriority::Primary);
                peer_index = (peer_index + 1) % top_peers.len();
            }
```

**File:** mempool/src/shared_mempool/coordinator.rs (L124-125)
```rust
            _ = update_peers_interval.tick().fuse() => {
                handle_update_peers(peers_and_metadata.clone(), &mut smp, &mut scheduled_broadcasts, executor.clone()).await;
```

**File:** config/src/config/mempool_config.rs (L138-169)
```rust
            load_balancing_thresholds: vec![
                LoadBalancingThresholdConfig {
                    avg_mempool_traffic_threshold_in_tps: 500,
                    latency_slack_between_top_upstream_peers: 50,
                    max_number_of_upstream_peers: 2,
                },
                LoadBalancingThresholdConfig {
                    avg_mempool_traffic_threshold_in_tps: 1000,
                    latency_slack_between_top_upstream_peers: 50,
                    max_number_of_upstream_peers: 3,
                },
                LoadBalancingThresholdConfig {
                    avg_mempool_traffic_threshold_in_tps: 1500,
                    latency_slack_between_top_upstream_peers: 75,
                    max_number_of_upstream_peers: 4,
                },
                LoadBalancingThresholdConfig {
                    avg_mempool_traffic_threshold_in_tps: 2500,
                    latency_slack_between_top_upstream_peers: 100,
                    max_number_of_upstream_peers: 5,
                },
                LoadBalancingThresholdConfig {
                    avg_mempool_traffic_threshold_in_tps: 3500,
                    latency_slack_between_top_upstream_peers: 125,
                    max_number_of_upstream_peers: 6,
                },
                LoadBalancingThresholdConfig {
                    avg_mempool_traffic_threshold_in_tps: 4500,
                    latency_slack_between_top_upstream_peers: 150,
                    max_number_of_upstream_peers: 7,
                },
            ],
```

**File:** config/src/config/mempool_config.rs (L176-183)
```rust
impl ConfigSanitizer for MempoolConfig {
    fn sanitize(
        _node_config: &NodeConfig,
        _node_type: NodeType,
        _chain_id: Option<ChainId>,
    ) -> Result<(), Error> {
        Ok(()) // TODO: add reasonable verifications
    }
```

**File:** mempool/src/shared_mempool/network.rs (L237-240)
```rust
        // Only fullnodes should prioritize peers (e.g., VFNs and PFNs)
        if self.node_type.is_validator() {
            return;
        }
```
