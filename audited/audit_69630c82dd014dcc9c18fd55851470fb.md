# Audit Report

## Title
Memory Exhaustion in Node-Checker via Concurrent Check Requests with Non-Shared Cache

## Summary
The Aptos Node Health Checker (NHC) service is vulnerable to memory exhaustion attacks. An attacker can send many concurrent HTTP requests to the `/check` endpoint targeting the same node, causing the service to allocate multiple large `Scrape` objects in memory. This occurs because each request creates a new `MetricsProvider` instance with its own `OutputCache`, preventing cache sharing across concurrent requests.

## Finding Description
The node-checker service exposes a public HTTP endpoint `/check` that allows anyone to request health checks for target nodes. [1](#0-0)  The endpoint has no rate limiting or authentication. [2](#0-1) 

When a check request is processed, the `SyncRunner.run()` method creates a new `MetricsProvider` instance for the target node on each request. [3](#0-2)  Each `MetricsProvider` instance is initialized with its own `OutputCache`. [4](#0-3) 

The `OutputCache` uses instance-level locking via `RwLock`, which only prevents concurrent access within the same cache instance. [5](#0-4)  When multiple concurrent requests check the same target node, each request gets its own `MetricsProvider` with its own cache, bypassing any cache benefits and causing redundant metric scraping.

The `MinimumPeersChecker.check()` method calls `target_metrics_provider.provide().await`, which triggers metrics scraping. [6](#0-5)  The scraping downloads and parses the entire Prometheus metrics output from the target node into a `Scrape` object. [7](#0-6) 

For an Aptos validator or fullnode, the metrics endpoint can return several megabytes of data. With concurrent requests, this causes linear memory growth:
- 100 concurrent requests × 2MB per scrape = 200MB
- Plus HTTP buffers, parsing overhead, and other allocations
- Can easily exceed 500MB-1GB under sustained attack

This violates **Invariant #9: Resource Limits** - the service does not properly limit memory consumption per operation.

## Impact Explanation
This is a **Medium severity** vulnerability as specified in the security question. The impact includes:

1. **Denial of Service**: The node-checker service can be exhausted and crash due to OOM, making it unavailable for legitimate users
2. **Service Degradation**: Even without crashing, high memory pressure causes slowdowns affecting all users
3. **Resource Consumption**: Unnecessary load on target nodes from redundant metrics scraping

However, this does NOT affect:
- Validator consensus operations
- Blockchain state or transaction processing  
- User funds or on-chain assets
- Core blockchain API availability

The node-checker is a peripheral monitoring service, not a critical blockchain component, which limits the severity to Medium per the Aptos bug bounty program's definition: "State inconsistencies requiring intervention" and service availability issues.

## Likelihood Explanation
This vulnerability has **HIGH likelihood** of exploitation because:

1. **No Authentication**: The `/check` endpoint is publicly accessible with no authentication required
2. **Simple Attack**: Requires only standard HTTP GET requests with query parameters
3. **Low Attacker Cost**: No special tools or resources needed beyond basic HTTP client
4. **Immediate Effect**: Memory exhaustion occurs within seconds of sustained requests
5. **Default Configuration**: The cache TTL is only 1000ms (1 second), providing minimal protection [8](#0-7) 

The attack is trivially executable by any malicious actor and requires no specialized knowledge.

## Recommendation
Implement a **global cache** shared across all requests, keyed by `(node_url, metrics_port)`:

```rust
// In a new file: ecosystem/node-checker/src/provider/global_cache.rs
use std::sync::Arc;
use std::collections::HashMap;
use tokio::sync::RwLock;
use once_cell::sync::Lazy;

type CacheKey = (String, u16); // (node_url, metrics_port)

static GLOBAL_METRICS_CACHE: Lazy<Arc<RwLock<HashMap<CacheKey, Arc<OutputCache<Scrape>>>>>> = 
    Lazy::new(|| Arc::new(RwLock::new(HashMap::new())));

pub async fn get_or_create_cache(
    node_url: String,
    metrics_port: u16,
    cache_ttl: Duration,
) -> Arc<OutputCache<Scrape>> {
    let key = (node_url.clone(), metrics_port);
    
    // Try to get existing cache
    {
        let cache_map = GLOBAL_METRICS_CACHE.read().await;
        if let Some(cache) = cache_map.get(&key) {
            return cache.clone();
        }
    }
    
    // Create new cache if not exists
    let mut cache_map = GLOBAL_METRICS_CACHE.write().await;
    cache_map.entry(key)
        .or_insert_with(|| Arc::new(OutputCache::new(cache_ttl)))
        .clone()
}
```

Then modify `MetricsProvider::new()` to use the global cache:

```rust
pub async fn new(
    config: MetricsProviderConfig,
    client: Arc<reqwest::Client>,
    url: Url,
    metrics_port: u16,
) -> Self {
    let output_cache = get_or_create_cache(
        url.to_string(),
        metrics_port,
        Duration::from_millis(config.common.cache_ttl_ms)
    ).await;
    
    Self {
        config,
        client,
        metrics_url: url,
        output_cache,
    }
}
```

Additional mitigations:
1. Add rate limiting per IP address (e.g., 10 requests per minute)
2. Add request timeout and maximum concurrent request limits
3. Implement cache eviction to prevent unbounded growth
4. Add monitoring/alerting for high memory usage

## Proof of Concept

```rust
// PoC: Concurrent request attack
use reqwest::Client;
use tokio::task::JoinSet;

#[tokio::test]
async fn test_memory_exhaustion_attack() {
    let nhc_url = "http://localhost:20121";
    let target_node = "http://target-fullnode.example.com";
    let client = Client::new();
    
    let mut tasks = JoinSet::new();
    
    // Send 100 concurrent requests to check the same target node
    for _ in 0..100 {
        let client = client.clone();
        let url = format!(
            "{}/check?baseline_configuration_id=devnet_fullnode&node_url={}&metrics_port=9101",
            nhc_url, target_node
        );
        
        tasks.spawn(async move {
            let response = client.get(&url).send().await;
            println!("Request completed: {:?}", response.is_ok());
        });
    }
    
    // All requests will create separate MetricsProvider instances
    // Each will independently fetch and parse metrics from the target
    // Memory usage will grow linearly with concurrent requests
    
    while let Some(result) = tasks.join_next().await {
        result.unwrap();
    }
    
    // At peak, memory usage = num_requests × scrape_size
    // For 100 requests × 2MB = ~200MB minimum
    println!("Attack completed. Check node-checker memory usage.");
}
```

To reproduce:
1. Start node-checker service: `cargo run -p aptos-node-checker -- server run --baseline-config-paths <config> --listen-address 0.0.0.0`
2. Run the PoC test with a valid target node URL
3. Monitor memory usage: `watch -n 0.5 'ps aux | grep node-checker'`
4. Observe linear memory growth with concurrent requests

## Notes
- The vulnerability exists because `target_metrics_provider` is not wrapped in `Arc` in the `ProviderCollection`, unlike baseline providers. [9](#0-8) 
- The reqwest HTTP client is created fresh per request with default settings and no connection limits. [10](#0-9) 
- While baseline providers are cached across requests via `Arc`, target providers are recreated each time, preventing any cache benefits for repeated checks of the same target. [11](#0-10)

### Citations

**File:** ecosystem/node-checker/src/server/api.rs (L29-45)
```rust
    #[oai(path = "/check", method = "get")]
    async fn check(
        &self,
        /// The ID of the baseline node configuration to use for the evaluation, e.g. devnet_fullnode
        baseline_configuration_id: Query<String>,
        /// The URL of the node to check, e.g. http://44.238.19.217 or http://fullnode.mysite.com
        node_url: Query<Url>,
        /// If given, we will assume the metrics service is available at the given port.
        metrics_port: Query<Option<u16>>,
        /// If given, we will assume the API is available at the given port.
        api_port: Query<Option<u16>>,
        /// If given, we will assume that clients can communicate with your node via noise at the given port.
        noise_port: Query<Option<u16>>,
        /// A public key for the node, e.g. 0x44fd1324c66371b4788af0b901c9eb8088781acb29e6b8b9c791d5d9838fbe1f.
        /// This is only necessary for certain checkers, e.g. HandshakeChecker.
        public_key: Query<Option<String>>,
    ) -> poem::Result<Json<CheckSummary>> {
```

**File:** ecosystem/node-checker/src/server/run.rs (L51-66)
```rust
    let cors = Cors::new().allow_methods(vec![Method::GET]);

    Server::new(TcpListener::bind((
        args.server_args.listen_address,
        args.server_args.listen_port,
    )))
    .run(
        Route::new()
            .nest(api_endpoint, api_service)
            .nest("/spec", ui)
            .at("/spec.json", spec_json)
            .at("/spec.yaml", spec_yaml)
            .with(cors),
    )
    .await
    .map_err(anyhow::Error::msg)
```

**File:** ecosystem/node-checker/src/runner/sync_runner.rs (L101-101)
```rust
        let mut provider_collection = self.provider_collection.clone();
```

**File:** ecosystem/node-checker/src/runner/sync_runner.rs (L104-111)
```rust
        if let Ok(metrics_client) = target_node_address.get_metrics_client(Duration::from_secs(4)) {
            let metrics_client = Arc::new(metrics_client);
            provider_collection.target_metrics_provider = Some(MetricsProvider::new(
                self.provider_configs.metrics.clone(),
                metrics_client.clone(),
                target_node_address.url.clone(),
                target_node_address.get_metrics_port().unwrap(),
            ));
```

**File:** ecosystem/node-checker/src/provider/metrics.rs (L48-56)
```rust
        let output_cache = Arc::new(OutputCache::new(Duration::from_millis(
            config.common.cache_ttl_ms,
        )));
        Self {
            config,
            client,
            metrics_url: url,
            output_cache,
        }
```

**File:** ecosystem/node-checker/src/provider/metrics.rs (L59-85)
```rust
    pub async fn get_scrape(&self) -> Result<Scrape, ProviderError> {
        let response = self
            .client
            .get(self.metrics_url.clone())
            .send()
            .await
            .with_context(|| format!("Failed to get data from {}", self.metrics_url))
            .map_err(|e| ProviderError::RetryableEndpointError("/metrics", e))?;
        let body = response
            .text()
            .await
            .with_context(|| {
                format!(
                    "Failed to process response body from {} as text",
                    self.metrics_url
                )
            })
            .map_err(|e| ProviderError::ParseError(anyhow!(e)))?;
        Scrape::parse(body.lines().map(|l| Ok(l.to_string())))
            .with_context(|| {
                format!(
                    "Failed to parse response text from {} as a Prometheus scrape",
                    self.metrics_url
                )
            })
            .map_err(|e| ProviderError::ParseError(anyhow!(e)))
    }
```

**File:** ecosystem/node-checker/src/provider/cache.rs (L35-54)
```rust
    pub async fn get(
        &self,
        func: impl Future<Output = Result<T, ProviderError>>,
    ) -> Result<T, ProviderError> {
        // If the cache isn't too old and there is a value, return it.
        if self.last_run.read().await.elapsed() < self.cache_ttl {
            if let Some(last_output) = &*self.last_output.read().await {
                return Ok(last_output.clone());
            }
        }

        // Otherwise fetch the value and update the cache. We take the locks while
        // fetching the new value so we don't waste effort fetching it multiple times.
        let mut last_output = self.last_output.write().await;
        let mut last_run = self.last_run.write().await;
        let new_output = func.await?;
        *last_output = Some(new_output.clone());
        *last_run = Instant::now();
        Ok(new_output)
    }
```

**File:** ecosystem/node-checker/src/checker/minimum_peers.rs (L104-122)
```rust
    async fn check(
        &self,
        providers: &ProviderCollection,
    ) -> Result<Vec<CheckResult>, CheckerError> {
        let target_metrics_provider = get_provider!(
            providers.target_metrics_provider,
            self.config.common.required,
            MetricsProvider
        );
        let scrape = match target_metrics_provider.provide().await {
            Ok(scrape) => scrape,
            Err(e) => {
                return Ok(vec![Self::build_result(
                    "Failed to check node peers".to_string(),
                    0,
                    format!("Failed to scrape metrics from your node: {:#}", e),
                )])
            },
        };
```

**File:** ecosystem/node-checker/src/provider/mod.rs (L60-62)
```rust
    fn default_cache_ttl_ms() -> u64 {
        1000
    }
```

**File:** ecosystem/node-checker/src/provider/provider_collection.rs (L40-40)
```rust
    pub target_metrics_provider: Option<MetricsProvider>,
```

**File:** ecosystem/node-checker/src/configuration/node_address.rs (L94-104)
```rust
    pub fn get_metrics_client(&self, timeout: Duration) -> Result<reqwest::Client> {
        match self.metrics_port {
            Some(_) => Ok(reqwest::ClientBuilder::new()
                .timeout(timeout)
                .cookie_provider(self.cookie_store.clone())
                .build()
                .unwrap()),
            None => Err(anyhow!(
                "Cannot build metrics client without a metrics port"
            )),
        }
```
