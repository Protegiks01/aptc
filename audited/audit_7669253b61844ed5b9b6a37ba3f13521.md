# Audit Report

## Title
TOCTOU Race Condition Between Batch Cache Invalidation and Database Read in Quorum Store

## Summary
A Time-of-Check-Time-of-Use (TOCTOU) race condition exists between `clear_expired_payload()` removing batches from the cache and `delete_batches()` deleting them from persistent storage, while concurrent threads may be reading those batches via `get_batch_from_local()`. This can cause validators to fail retrieving batches needed for consensus, leading to incomplete blocks and potential consensus divergence.

## Finding Description

The vulnerability occurs in the batch lifecycle management within the Quorum Store component. The issue manifests when two concurrent operations race:

**Thread 1 (Batch Expiration):**
1. Calls `update_certified_timestamp()` after block commit [1](#0-0) 
2. Executes `clear_expired_payload()` which removes expired batches from the in-memory cache (`db_cache`) [2](#0-1) 
3. Subsequently calls `delete_batches()` to remove those batches from persistent storage [3](#0-2) 

**Thread 2 (Batch Retrieval):**
1. Calls `get_batch_from_local()` during block building to retrieve batch transactions [4](#0-3) 
2. Finds the batch in cache with `StorageMode::PersistedOnly` [5](#0-4) 
3. Attempts to read from database via `get_batch_from_db()` [6](#0-5) 

**The Race Window:**
Between Thread 2 checking the cache (step 2) and reading from the database (step 3), Thread 1 can remove the entry from cache and delete it from the database. When Thread 2 attempts the database read via `get_batch()`, it returns `None`, causing `get_batch_from_db()` to return `ExecutorError::CouldNotGetData` [7](#0-6) .

The deletion operations use only relaxed writes (`write_schemas_relaxed`) [8](#0-7) , and there is no synchronization mechanism (lock, atomic operation, or database transaction) coordinating the cache check and database read sequence.

This breaks the **State Consistency** invariant because batch availability becomes non-deterministic based on thread scheduling, and potentially violates **Deterministic Execution** if different validators experience different race outcomes when building the same block.

## Impact Explanation

**Severity: HIGH** (up to $50,000 per Aptos Bug Bounty)

This vulnerability falls under "Significant protocol violations" and "Validator node slowdowns":

1. **Consensus Impact**: When `get_batch_from_local()` fails, the consensus layer logs a warning and skips the batch [9](#0-8) . This means transactions that should be included in a block are omitted, potentially causing:
   - Different validators building different block content if they experience the race at different times
   - Reduced throughput as transactions are dropped from proposals
   - Need for re-proposal of affected transactions

2. **Non-Deterministic Behavior**: The race condition introduces non-determinism in validator behavior, which could lead to proposal failures or validators disagreeing on block content.

3. **Data Integrity**: Validators may build incomplete blocks, affecting the integrity of the transaction ordering and inclusion guarantees that users expect.

While this does not directly cause fund loss or complete network failure, it represents a significant protocol violation that can degrade consensus performance and reliability.

## Likelihood Explanation

**Likelihood: MEDIUM to HIGH**

The race condition will occur when these conditions coincide:
1. A batch is stored with `StorageMode::PersistedOnly` (payload not in memory, only metadata cached)
2. The batch is approaching expiration (within the `expiration_buffer_usecs` window)
3. A block commit triggers `update_certified_timestamp()` causing batch expiration
4. Simultaneously, another thread is building a new block that references the expiring batch

These conditions are realistic during normal consensus operation:
- High transaction volumes can exceed memory quotas, forcing `PersistedOnly` storage mode [10](#0-9) 
- Block commits (`notify_commit`) and block proposals happen concurrently in consensus
- The timing window, while small, exists across multiple function calls without synchronization

The frequency depends on validator load and batch expiration patterns, but given the concurrent nature of consensus operations, this race can be expected to occur periodically under normal network operation.

## Recommendation

Implement atomic read-or-delete semantics by holding the cache entry lock during the database read operation. Modify `get_batch_from_local()` to ensure atomicity:

```rust
pub(crate) fn get_batch_from_local(
    &self,
    digest: &HashValue,
) -> ExecutorResult<PersistedValue<BatchInfoExt>> {
    // Use entry API to maintain lock during DB read
    match self.db_cache.entry(*digest) {
        Occupied(entry) => {
            let value = entry.get().clone();
            if value.payload_storage_mode() == StorageMode::PersistedOnly {
                // Hold the entry lock while reading from DB
                // This prevents the entry from being removed during our read
                match self.get_batch_from_db(digest, value.batch_info().is_v2()) {
                    Ok(db_value) => Ok(db_value),
                    Err(e) => {
                        // Entry was in cache but not in DB - possible race
                        // Check if entry is still valid
                        if entry.get().digest() == digest {
                            warn!("Batch metadata in cache but not in DB: {}", digest);
                        }
                        Err(e)
                    }
                }
            } else {
                Ok(value)
            }
        },
        Vacant(_) => Err(ExecutorError::CouldNotGetData),
    }
}
```

Alternatively, implement a reference counting or read-write lock mechanism where:
1. Readers acquire a read lock before checking the cache
2. The expiration process acquires a write lock before removing from cache and DB
3. This ensures no reader can be between cache-check and DB-read when deletion occurs

## Proof of Concept

```rust
#[cfg(test)]
mod race_condition_test {
    use super::*;
    use std::sync::{Arc, Barrier};
    use std::thread;
    use aptos_crypto::HashValue;
    
    #[test]
    fn test_delete_get_race_condition() {
        // Setup: Create a batch store with a batch in PersistedOnly mode
        let db = Arc::new(QuorumStoreDB::new(temp_dir()));
        let batch_store = Arc::new(BatchStore::new(
            1, // epoch
            false, // is_new_epoch
            0, // last_certified_time
            db.clone(),
            1024, // memory_quota (small to force PersistedOnly)
            10240, // db_quota
            100, // batch_quota
            validator_signer,
            60_000_000, // expiration_buffer_usecs
        ));
        
        // Create and save a batch with PersistedOnly storage
        let batch_digest = HashValue::random();
        let batch_info = create_test_batch_info(batch_digest, 1000);
        let persisted_value = PersistedValue::new(batch_info, None); // No payload = PersistedOnly
        
        batch_store.save(&persisted_value).unwrap();
        
        // Setup barrier for thread synchronization
        let barrier = Arc::new(Barrier::new(2));
        let barrier_clone = barrier.clone();
        let batch_store_clone = batch_store.clone();
        let digest_clone = batch_digest;
        
        // Thread 1: Delete the batch
        let delete_thread = thread::spawn(move || {
            barrier_clone.wait(); // Synchronize start
            // Trigger expiration and deletion
            batch_store_clone.update_certified_timestamp(2000); // timestamp > batch expiration
        });
        
        // Thread 2: Try to read the batch
        let read_thread = thread::spawn(move || {
            barrier.wait(); // Synchronize start
            thread::sleep(Duration::from_micros(100)); // Small delay to hit race window
            batch_store.get_batch_from_local(&digest_clone)
        });
        
        delete_thread.join().unwrap();
        let read_result = read_thread.join().unwrap();
        
        // The race condition manifests when:
        // 1. Read thread finds entry in cache (PersistedOnly)
        // 2. Delete thread removes from cache and DB
        // 3. Read thread tries to read from DB and gets None
        // Result: ExecutorError::CouldNotGetData despite entry existing initially
        
        assert!(
            read_result.is_err(),
            "Race condition: Batch was in cache but deleted before DB read completed"
        );
    }
}
```

## Notes

The vulnerability exists in the broader batch lifecycle management where the gap between cache operations and database operations is not protected by atomic transactions or proper locking. The underlying SchemaDB/RocksDB provides thread-safe individual operations [11](#0-10) , but does not provide atomicity across the sequence: cache-check â†’ DB-read, which this code path requires.

A secondary issue identified: `gc_previous_epoch_batches_from_db_v2()` incorrectly calls `delete_batches()` instead of `delete_batches_v2()` [12](#0-11) , which should be corrected as a separate bug.

### Citations

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L169-170)
```rust
        self.batch_reader
            .update_certified_timestamp(block_timestamp);
```

**File:** consensus/src/quorum_store/batch_store.rs (L241-241)
```rust
        db.delete_batches(expired_keys)
```

**File:** consensus/src/quorum_store/batch_store.rs (L383-392)
```rust
            let value_to_be_stored = if self
                .peer_quota
                .entry(author)
                .or_insert(QuotaManager::new(
                    self.db_quota,
                    self.memory_quota,
                    self.batch_quota,
                ))
                .update_quota(value.num_bytes() as usize)?
                == StorageMode::PersistedOnly
```

**File:** consensus/src/quorum_store/batch_store.rs (L451-458)
```rust
            let removed_value = match self.db_cache.entry(h) {
                Occupied(entry) => {
                    // We need to check up-to-date expiration again because receiving the same
                    // digest with a higher expiration would update the persisted value and
                    // effectively extend the expiration.
                    if entry.get().expiration() <= expiration_time {
                        self.persist_subscribers.remove(entry.get().digest());
                        Some(entry.remove())
```

**File:** consensus/src/quorum_store/batch_store.rs (L535-536)
```rust
        let expired_keys = self.clear_expired_payload(certified_time);
        if let Err(e) = self.db.delete_batches(expired_keys) {
```

**File:** consensus/src/quorum_store/batch_store.rs (L561-567)
```rust
            match self.db.get_batch(digest) {
                Ok(Some(value)) => Ok(value.into()),
                Ok(None) | Err(_) => {
                    warn!("Could not get batch from db");
                    Err(ExecutorError::CouldNotGetData)
                },
            }
```

**File:** consensus/src/quorum_store/batch_store.rs (L575-577)
```rust
        if let Some(value) = self.db_cache.get(digest) {
            if value.payload_storage_mode() == StorageMode::PersistedOnly {
                self.get_batch_from_db(digest, value.batch_info().is_v2())
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L540-540)
```rust
            if let Ok(mut persisted_value) = self.batch_store.get_batch_from_local(batch.digest()) {
```

**File:** consensus/src/quorum_store/batch_proof_queue.rs (L544-549)
```rust
            } else {
                warn!(
                    "Couldn't find a batch in local storage while creating inline block: {:?}",
                    batch.digest()
                );
            }
```

**File:** consensus/src/quorum_store/quorum_store_db.rs (L99-99)
```rust
        self.db.write_schemas_relaxed(batch)?;
```

**File:** storage/schemadb/src/lib.rs (L216-232)
```rust
    pub fn get<S: Schema>(&self, schema_key: &S::Key) -> DbResult<Option<S::Value>> {
        let _timer = APTOS_SCHEMADB_GET_LATENCY_SECONDS.timer_with(&[S::COLUMN_FAMILY_NAME]);

        let k = <S::Key as KeyCodec<S>>::encode_key(schema_key)?;
        let cf_handle = self.get_cf_handle(S::COLUMN_FAMILY_NAME)?;

        let result = self.inner.get_cf(cf_handle, k).into_db_res()?;
        APTOS_SCHEMADB_GET_BYTES.observe_with(
            &[S::COLUMN_FAMILY_NAME],
            result.as_ref().map_or(0.0, |v| v.len() as f64),
        );

        result
            .map(|raw_value| <S::Value as ValueCodec<S>>::decode_value(&raw_value))
            .transpose()
            .map_err(Into::into)
    }
```
