# Audit Report

## Title
QuorumStoreInlineHybrid Payload Allows Duplicate Batch Inclusion Leading to Transaction Count Inflation

## Summary
A malicious block proposer can craft a `QuorumStoreInlineHybrid` payload containing the same transaction batch in both `inline_batches` and `proof_with_data` components. This bypasses validation checks, inflates the reported transaction count, but results in fewer transactions being executed after deduplication, enabling throughput manipulation and resource waste.

## Finding Description

The `QuorumStoreInlineHybrid` payload type combines two transaction delivery mechanisms: proof-of-store batches (referenced by cryptographic proofs) and inline batches (transactions embedded directly). The vulnerability exists because the payload verification does not check for duplicate batches between these two components. [1](#0-0) 

The verification only validates proof signatures and inline batch digest correctness, but fails to ensure batch digests are unique across both components.

When a malicious proposer creates a payload where the same `BatchInfo` appears in both `inline_batches` and `proof_with_data`, the payload length calculation sums transactions from both sources without detecting duplicates: [2](#0-1) 

This inflated count passes block size validation checks in `RoundManager`: [3](#0-2) 

However, during transaction extraction, the same transactions appear twice: [4](#0-3) 

The duplicate transactions are only removed later by `TxnHashAndAuthenticatorDeduper` during block preparation: [5](#0-4) 

**Attack Scenario:**
1. Malicious proposer creates `QuorumStoreInlineHybrid` payload with:
   - `proof_with_data`: ProofOfStore for BatchInfo_A (500 transactions)
   - `inline_batches`: Same BatchInfo_A with 500 transactions embedded
2. `payload.len()` returns 1000 transactions
3. Block size validation passes (assuming limit â‰¥ 1000)
4. Transaction extraction yields [tx1...tx500, tx1...tx500] (duplicates)
5. Deduplication reduces to [tx1...tx500] (only 500 unique transactions)
6. Only 500 transactions execute despite payload claiming 1000

This breaks the invariant that payload size checks correspond to actual execution.

## Impact Explanation

**Medium Severity** - This vulnerability enables:

1. **Throughput Manipulation**: Blocks appear to contain more transactions than actually executed, reducing effective network throughput. A proposer claiming 10,000 transactions could deliver only 5,000 unique ones.

2. **Resource Exhaustion**: Network bandwidth, storage, and CPU cycles wasted transmitting, storing, and hashing duplicate data that gets filtered before execution.

3. **Validation Bypass**: Block size limits (`max_receiving_block_txns`) are checked against inflated counts, allowing proposers to pass validation while delivering reduced transaction throughput.

4. **State Inconsistency Risk**: If validators have different batch availability or apply deduplication inconsistently, they might execute different transaction sets, potentially breaking consensus determinism.

While this doesn't directly cause fund loss or complete consensus failure, it constitutes a state inconsistency requiring intervention and enables malicious throughput degradation, qualifying as **Medium Severity** per the bug bounty program.

## Likelihood Explanation

**High Likelihood** - The attack requires:
- Being a valid block proposer (validator with proposer turn)
- Crafting a malicious `QuorumStoreInlineHybrid` payload (straightforward)
- No special timing or race conditions needed

Since approximately 1/N validators (where N is validator count) become proposers each round, and creating the malicious payload is trivial, a single compromised validator could exploit this repeatedly. The honest payload creation code prevents this overlap, but a malicious actor can easily bypass this by constructing the payload manually. [6](#0-5) 

## Recommendation

**Fix 1: Add duplicate batch validation in `Payload::verify()`**

Modify the verification for `QuorumStoreInlineHybrid` to ensure batch digests are unique:

```rust
(true, Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _))
| (true, Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _)) => {
    Self::verify_with_cache(&proof_with_data.proofs, verifier, proof_cache)?;
    Self::verify_inline_batches(
        inline_batches.iter().map(|(info, txns)| (info, txns)),
    )?;
    
    // NEW: Check for duplicate batches
    let mut seen_digests = HashSet::new();
    for proof in &proof_with_data.proofs {
        ensure!(
            seen_digests.insert(proof.digest()),
            "Duplicate batch digest found in proof_with_data: {}",
            proof.digest()
        );
    }
    for (batch_info, _) in inline_batches {
        ensure!(
            seen_digests.insert(batch_info.digest()),
            "Duplicate batch digest found between inline_batches and proof_with_data: {}",
            batch_info.digest()
        );
    }
    
    Ok(())
},
```

**Fix 2: Deduplicate during extraction (defense-in-depth)**

Add batch digest checking in `get_transactions_quorum_store_inline_hybrid` to filter duplicates before appending inline transactions.

## Proof of Concept

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use aptos_consensus_types::{
        common::{Payload, ProofWithData},
        proof_of_store::{BatchInfo, ProofOfStore},
    };
    use aptos_types::transaction::SignedTransaction;
    
    #[test]
    fn test_duplicate_batch_in_hybrid_payload() {
        // Create a batch with 100 transactions
        let batch_info = create_test_batch_info();
        let txns: Vec<SignedTransaction> = create_test_transactions(100);
        
        // Create proof for the batch
        let proof = create_test_proof(batch_info.clone());
        let proof_with_data = ProofWithData::new(vec![proof]);
        
        // Create inline batches with THE SAME batch
        let inline_batches = vec![(batch_info.clone(), txns.clone())];
        
        // Create hybrid payload with duplicate batch
        let payload = Payload::QuorumStoreInlineHybrid(
            inline_batches,
            proof_with_data,
            None,
        );
        
        // Check reported length - should be 200 (100 + 100 duplicates)
        assert_eq!(payload.len(), 200);
        
        // Verify payload (should pass - this is the bug!)
        let verifier = create_test_verifier();
        let proof_cache = ProofCache::new();
        assert!(payload.verify(&verifier, &proof_cache, true).is_ok());
        
        // Extract transactions
        let all_batches = create_batch_map();
        let extracted = extract_txns_from_block(&block, &all_batches).unwrap();
        
        // Extracted list contains duplicates: [tx1...tx100, tx1...tx100]
        assert_eq!(extracted.len(), 200);
        
        // After deduplication (as done in BlockPreparer)
        let deduper = TxnHashAndAuthenticatorDeduper::new();
        let deduped = deduper.dedup(extracted.into_iter().cloned().map(|t| t.into()).collect());
        
        // Only 100 unique transactions remain
        assert_eq!(deduped.len(), 100);
        
        // This demonstrates the vulnerability: payload claims 200 txns,
        // passes validation, but only 100 execute
    }
}
```

This test demonstrates that a payload with duplicate batches passes verification but results in fewer transactions being executed after deduplication, confirming the vulnerability.

## Notes

The honest payload creation code in `proof_manager.rs` already excludes previously selected batches when pulling inline batches, preventing this issue in normal operation. However, the lack of validation allows a malicious proposer to bypass this protection. The fix should enforce this invariant at the verification layer to prevent exploitation.

### Citations

**File:** consensus/consensus-types/src/common.rs (L292-299)
```rust
            Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _)
            | Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _) => {
                proof_with_data.num_txns()
                    + inline_batches
                        .iter()
                        .map(|(_, txns)| txns.len())
                        .sum::<usize>()
            },
```

**File:** consensus/consensus-types/src/common.rs (L590-596)
```rust
            (true, Payload::QuorumStoreInlineHybrid(inline_batches, proof_with_data, _))
            | (true, Payload::QuorumStoreInlineHybridV2(inline_batches, proof_with_data, _)) => {
                Self::verify_with_cache(&proof_with_data.proofs, verifier, proof_cache)?;
                Self::verify_inline_batches(
                    inline_batches.iter().map(|(info, txns)| (info, txns)),
                )?;
                Ok(())
```

**File:** consensus/src/round_manager.rs (L1178-1193)
```rust
        let payload_len = proposal.payload().map_or(0, |payload| payload.len());
        let payload_size = proposal.payload().map_or(0, |payload| payload.size());
        ensure!(
            num_validator_txns + payload_len as u64 <= self.local_config.max_receiving_block_txns,
            "Payload len {} exceeds the limit {}",
            payload_len,
            self.local_config.max_receiving_block_txns,
        );

        ensure!(
            validator_txns_total_bytes + payload_size as u64
                <= self.local_config.max_receiving_block_bytes,
            "Payload size {} exceeds the limit {}",
            payload_size,
            self.local_config.max_receiving_block_bytes,
        );
```

**File:** consensus/src/payload_manager/quorum_store_payload_manager.rs (L134-150)
```rust
        let all_transactions = {
            let mut all_txns = process_qs_payload(
                proof_with_data,
                self.batch_reader.clone(),
                block,
                &self.ordered_authors,
            )
            .await?;
            all_txns.append(
                &mut inline_batches
                    .iter()
                    // TODO: Can clone be avoided here?
                    .flat_map(|(_batch_info, txns)| txns.clone())
                    .collect(),
            );
            all_txns
        };
```

**File:** consensus/src/block_preparer.rs (L99-99)
```rust
            let deduped_txns = txn_deduper.dedup(filtered_txns);
```

**File:** consensus/src/quorum_store/proof_manager.rs (L168-174)
```rust
                    self.batch_proof_queue.pull_batches_with_transactions(
                        &excluded_batches
                            .iter()
                            .cloned()
                            .chain(proof_block.iter().map(|proof| proof.info().clone()))
                            .chain(opt_batches.clone())
                            .collect(),
```
