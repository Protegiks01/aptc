# Audit Report

## Title
Immediate Retry Without Backoff on TooManyRequested Errors Amplifies Storage Layer DoS

## Summary
The data streaming service's retry logic does not implement proper backoff delays when encountering `TooManyRequested` errors from the storage layer. Instead, it immediately retries failed requests up to 5 times with only timeout increases (not delays between retries), creating a retry storm that amplifies DoS impact on storage services under load.

## Finding Description

When the storage layer enforces request limits and returns `AptosDbError::TooManyRequested`, the error propagates through the storage service to the data streaming client. The data stream's error handling logic treats all errors identically without differentiating between error types. [1](#0-0) [2](#0-1) 

The storage layer validates requests against `MAX_REQUEST_LIMIT` (20,000 items) and returns this error when exceeded: [3](#0-2) [4](#0-3) 

The critical vulnerability lies in the data stream's error handling. When any error occurs (including `TooManyRequested`), the `handle_data_client_error` function immediately retries without differentiation: [5](#0-4) 

The retry mechanism in `resend_data_client_request` immediately increments the failure counter and resends the request: [6](#0-5) 

The exponential backoff only applies to the **request timeout**, not to any delay between retries: [7](#0-6) 

The retry limit defaults to 5 attempts: [8](#0-7) 

**Attack Scenario:**
1. Multiple peers request large datasets (>20,000 items) from a storage service
2. Storage service returns `TooManyRequested` errors due to load
3. Each peer immediately retries 5 times without any delay
4. This creates a multiplicative effect (N peers × 5 retries = 5N requests)
5. The retry storm prevents the overloaded storage service from recovering
6. Legitimate requests are blocked during the retry storm

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under the Aptos bug bounty program criteria for the following reasons:

1. **Storage Layer Slowdown**: The immediate retry behavior can cause significant storage service degradation, aligning with "validator node slowdowns" (High severity) but limited to storage subsystem
2. **Amplified DoS Effect**: A single oversized request from N peers generates 5N requests to the storage layer, amplifying resource exhaustion
3. **Operational Impact**: While not causing fund loss or consensus violation, it degrades network availability and state synchronization
4. **Recovery Prevention**: The retry storm prevents natural recovery from transient overload conditions

The impact is constrained to Medium (not High) because:
- It affects storage/state-sync services, not consensus directly
- No fund loss or permanent state corruption occurs
- The issue requires multiple peers to significantly amplify the effect
- Services can eventually recover once retries exhaust

## Likelihood Explanation

**Likelihood: Medium to High**

This vulnerability is likely to manifest in production because:

1. **Legitimate Trigger Conditions**: Nodes routinely request large datasets during state sync, especially when catching up after downtime
2. **Network Pressure Points**: During epoch transitions or high transaction volume, storage services naturally experience increased load
3. **No Attacker Sophistication Required**: Any peer can trigger this by requesting data beyond the limit—no special privileges or attack tooling needed
4. **Automatic Behavior**: The retry logic is automatic and cannot be disabled by operators
5. **Cascading Effect**: When one node experiences TooManyRequested, multiple requesting peers amplify the problem simultaneously

The TODO comment in the code acknowledges this gap: [9](#0-8) 

## Recommendation

Implement error-specific retry policies with exponential backoff delays for `TooManyRequested` errors:

1. **Add delay between retries**: Implement sleep/backoff between retry attempts, not just timeout increases
2. **Differentiate error types**: Handle rate-limiting errors differently from transient network errors
3. **Implement jitter**: Add randomization to prevent synchronized retry storms across peers
4. **Adaptive retry limits**: Reduce retry count for resource exhaustion errors (TooManyRequested) vs. other errors

**Recommended Fix:**

Modify `handle_data_client_error` to differentiate error types and `resend_data_client_request` to include backoff delay:

```rust
// In handle_data_client_error - add error type checking
fn handle_data_client_error(
    &mut self,
    data_client_request: &DataClientRequest,
    data_client_error: &aptos_data_client::error::Error,
) -> Result<(), Error> {
    // Log the error
    warn!(...);
    
    // Determine backoff strategy based on error type
    let backoff_ms = if is_rate_limit_error(data_client_error) {
        // Exponential backoff for rate limiting errors
        let base_backoff_ms = 1000; // 1 second base
        min(
            30000, // max 30 seconds
            base_backoff_ms * (u32::pow(2, self.request_failure_count as u32) as u64)
        )
    } else {
        0 // No delay for other errors
    };
    
    self.resend_data_client_request(data_client_request, backoff_ms)
}

// Add backoff parameter and implement delay
async fn resend_data_client_request(
    &mut self,
    data_client_request: &DataClientRequest,
    backoff_ms: u64,
) -> Result<(), Error> {
    self.request_failure_count += 1;
    
    // Apply backoff delay with jitter
    if backoff_ms > 0 {
        let jitter = rand::random::<u64>() % (backoff_ms / 2);
        tokio::time::sleep(Duration::from_millis(backoff_ms + jitter)).await;
    }
    
    let pending_client_response = self.send_client_request(true, data_client_request.clone());
    self.get_sent_data_requests()?.push_front(pending_client_response);
    Ok(())
}
```

Additionally, reduce max retry count for rate limiting errors from 5 to 2-3 attempts.

## Proof of Concept

```rust
#[cfg(test)]
mod test_too_many_requested_amplification {
    use super::*;
    use std::sync::atomic::{AtomicU64, Ordering};
    use std::sync::Arc;
    
    #[tokio::test]
    async fn test_immediate_retry_amplifies_storage_requests() {
        // Setup: Create a mock storage service that counts requests
        let request_count = Arc::new(AtomicU64::new(0));
        let request_count_clone = request_count.clone();
        
        // Mock storage that returns TooManyRequested after counting
        let mock_storage = MockStorage::new(move |_request| {
            request_count_clone.fetch_add(1, Ordering::SeqCst);
            Err(AptosDbError::TooManyRequested(25000, 20000))
        });
        
        // Create data stream with default config (max_request_retry = 5)
        let mut data_stream = create_test_data_stream(mock_storage);
        
        // Send a single request exceeding the limit
        let large_request = DataClientRequest::GetTransactionsWithProof {
            start_version: 0,
            end_version: 25000, // Exceeds MAX_REQUEST_LIMIT
            proof_version: 25000,
            include_events: false,
        };
        
        // Process the request and retries
        data_stream.send_client_request(false, large_request).await;
        
        // Allow retries to complete
        for _ in 0..10 {
            data_stream.process_data_responses(GlobalDataSummary::empty()).await.ok();
            tokio::time::sleep(Duration::from_millis(10)).await;
        }
        
        // Verify: Without proper backoff, we expect 6 requests (1 initial + 5 retries)
        // happening nearly simultaneously
        let total_requests = request_count.load(Ordering::SeqCst);
        assert_eq!(total_requests, 6, 
            "Expected 6 immediate requests without backoff (1 initial + 5 retries)");
        
        // In production with N peers, this becomes N*6 simultaneous requests
        // amplifying the DoS on the storage layer
    }
    
    #[tokio::test]
    async fn test_retry_storm_with_multiple_peers() {
        let total_storage_requests = Arc::new(AtomicU64::new(0));
        let counter = total_storage_requests.clone();
        
        let mock_storage = MockStorage::new(move |_| {
            counter.fetch_add(1, Ordering::SeqCst);
            Err(AptosDbError::TooManyRequested(25000, 20000))
        });
        
        // Simulate 10 peers each making oversized requests
        let num_peers = 10;
        let mut handles = vec![];
        
        for _ in 0..num_peers {
            let storage_clone = mock_storage.clone();
            let handle = tokio::spawn(async move {
                let mut stream = create_test_data_stream(storage_clone);
                stream.send_client_request(false, large_request()).await;
                // Process retries
                for _ in 0..6 {
                    stream.process_data_responses(GlobalDataSummary::empty()).await.ok();
                }
            });
            handles.push(handle);
        }
        
        // Wait for all peers to complete retries
        for handle in handles {
            handle.await.ok();
        }
        
        // Verify amplification: 10 peers × 6 requests = 60 total requests
        let total = total_storage_requests.load(Ordering::SeqCst);
        assert!(total >= 60, "Expected at least 60 requests from retry storm amplification");
    }
}
```

## Notes

The vulnerability is exacerbated by the generic error handling that doesn't distinguish between transient network errors (which benefit from immediate retry) and resource exhaustion errors (which require backoff). The storage service's `RequestModerator` only handles invalid requests, not oversized valid requests that exceed limits. [10](#0-9) 

This design creates a "thundering herd" problem where legitimate peers all retry simultaneously when a storage service becomes overloaded, preventing natural recovery and amplifying the DoS impact beyond what a single malicious actor could achieve alone.

### Citations

**File:** storage/storage-interface/src/errors.rs (L16-17)
```rust
    #[error("Too many items requested: at least {0} requested, max is {1}")]
    TooManyRequested(u64, u64),
```

**File:** storage/storage-interface/src/lib.rs (L58-58)
```rust
pub const MAX_REQUEST_LIMIT: u64 = 20_000;
```

**File:** storage/aptosdb/src/db/aptosdb_internal.rs (L414-420)
```rust
pub(super) fn error_if_too_many_requested(num_requested: u64, max_allowed: u64) -> Result<()> {
    if num_requested > max_allowed {
        Err(AptosDbError::TooManyRequested(num_requested, max_allowed))
    } else {
        Ok(())
    }
}
```

**File:** storage/aptosdb/src/db/aptosdb_reader.rs (L177-177)
```rust
            error_if_too_many_requested(limit, MAX_REQUEST_LIMIT)?;
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L348-378)
```rust
        } else if !request_retry {
            self.data_client_config.response_timeout_ms
        } else {
            let response_timeout_ms = self.data_client_config.response_timeout_ms;
            let max_response_timeout_ms = self.data_client_config.max_response_timeout_ms;

            // Exponentially increase the timeout based on the number of
            // previous failures (but bounded by the max timeout).
            let request_timeout_ms = min(
                max_response_timeout_ms,
                response_timeout_ms * (u32::pow(2, self.request_failure_count as u32) as u64),
            );

            // Update the retry counter and log the request
            increment_counter_multiple_labels(
                &metrics::RETRIED_DATA_REQUESTS,
                data_client_request.get_label(),
                &request_timeout_ms.to_string(),
            );
            info!(
                (LogSchema::new(LogEntry::RetryDataRequest)
                    .stream_id(self.data_stream_id)
                    .message(&format!(
                        "Retrying data request type: {:?}, with new timeout: {:?} (ms)",
                        data_client_request.get_label(),
                        request_timeout_ms.to_string()
                    )))
            );

            request_timeout_ms
        };
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L711-725)
```rust
    fn handle_data_client_error(
        &mut self,
        data_client_request: &DataClientRequest,
        data_client_error: &aptos_data_client::error::Error,
    ) -> Result<(), Error> {
        // Log the error
        warn!(LogSchema::new(LogEntry::ReceivedDataResponse)
            .stream_id(self.data_stream_id)
            .event(LogEvent::Error)
            .error(&data_client_error.clone().into())
            .message("Encountered a data client error!"));

        // TODO(joshlind): can we identify the best way to react to the error?
        self.resend_data_client_request(data_client_request)
    }
```

**File:** state-sync/data-streaming-service/src/data_stream.rs (L729-744)
```rust
    fn resend_data_client_request(
        &mut self,
        data_client_request: &DataClientRequest,
    ) -> Result<(), Error> {
        // Increment the number of client failures for this request
        self.request_failure_count += 1;

        // Resend the client request
        let pending_client_response = self.send_client_request(true, data_client_request.clone());

        // Push the pending response to the head of the sent requests queue
        self.get_sent_data_requests()?
            .push_front(pending_client_response);

        Ok(())
    }
```

**File:** config/src/config/state_sync_config.rs (L254-277)
```rust
    /// Maximum number of retries for a single client request before a data
    /// stream will terminate.
    pub max_request_retry: u64,

    /// Maximum lag (in seconds) we'll tolerate when sending subscription requests
    pub max_subscription_stream_lag_secs: u64,

    /// The interval (milliseconds) at which to check the progress of each stream.
    pub progress_check_interval_ms: u64,
}

impl Default for DataStreamingServiceConfig {
    fn default() -> Self {
        Self {
            dynamic_prefetching: DynamicPrefetchingConfig::default(),
            enable_subscription_streaming: false,
            global_summary_refresh_interval_ms: 50,
            max_concurrent_requests: MAX_CONCURRENT_REQUESTS,
            max_concurrent_state_requests: MAX_CONCURRENT_STATE_REQUESTS,
            max_data_stream_channel_sizes: 50,
            max_notification_id_mappings: 300,
            max_num_consecutive_subscriptions: 45, // At ~3 blocks per second, this should last ~15 seconds
            max_pending_requests: 50,
            max_request_retry: 5,
```

**File:** state-sync/storage-service/server/src/moderator.rs (L132-196)
```rust
    /// Validates the given request and verifies that the peer is behaving
    /// correctly. If the request fails validation, an error is returned.
    pub fn validate_request(
        &self,
        peer_network_id: &PeerNetworkId,
        request: &StorageServiceRequest,
    ) -> Result<(), Error> {
        // Validate the request and time the operation
        let validate_request = || {
            // If the peer is being ignored, return an error
            if let Some(peer_state) = self.unhealthy_peer_states.get(peer_network_id) {
                if peer_state.is_ignored() {
                    return Err(Error::TooManyInvalidRequests(format!(
                        "Peer is temporarily ignored. Unable to handle request: {:?}",
                        request
                    )));
                }
            }

            // Get the latest storage server summary
            let storage_server_summary = self.cached_storage_server_summary.load();

            // Verify the request is serviceable using the current storage server summary
            if !storage_server_summary.can_service(
                &self.aptos_data_client_config,
                self.time_service.clone(),
                request,
            ) {
                // Increment the invalid request count for the peer
                let mut unhealthy_peer_state = self
                    .unhealthy_peer_states
                    .entry(*peer_network_id)
                    .or_insert_with(|| {
                        // Create a new unhealthy peer state (this is the first invalid request)
                        let max_invalid_requests =
                            self.storage_service_config.max_invalid_requests_per_peer;
                        let min_time_to_ignore_peers_secs =
                            self.storage_service_config.min_time_to_ignore_peers_secs;
                        let time_service = self.time_service.clone();

                        UnhealthyPeerState::new(
                            max_invalid_requests,
                            min_time_to_ignore_peers_secs,
                            time_service,
                        )
                    });
                unhealthy_peer_state.increment_invalid_request_count(peer_network_id);

                // Return the validation error
                return Err(Error::InvalidRequest(format!(
                    "The given request cannot be satisfied. Request: {:?}, storage summary: {:?}",
                    request, storage_server_summary
                )));
            }

            Ok(()) // The request is valid
        };
        utils::execute_and_time_duration(
            &metrics::STORAGE_REQUEST_VALIDATION_LATENCY,
            Some((peer_network_id, request)),
            None,
            validate_request,
            None,
        )
    }
```
