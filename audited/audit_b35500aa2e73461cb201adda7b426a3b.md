# Audit Report

## Title
Connection Exhaustion Vulnerability in gRPC Network Service Due to Missing Concurrent Connection Limits

## Summary
The `GRPCNetworkMessageServiceServerWrapper::start_async()` function in the secure network module lacks any limits on concurrent gRPC connections, allowing an attacker to exhaust server resources (file descriptors, memory, TCP connection table entries) by opening thousands of connections without sending messages, potentially degrading or halting validator execution performance.

## Finding Description

The gRPC server implementation in `start_async()` creates a tonic server with minimal protection mechanisms. [1](#0-0) 

The server configuration only sets an RPC timeout but critically lacks:
- `concurrent_streams_limit()` to limit HTTP2 streams
- Connection-level limits to cap total concurrent connections
- Rate limiting on new connection establishment
- Authentication or authorization mechanisms

This service is used by the `ExecutorService`, which is critical for sharded block execution. [2](#0-1) 

The service binds to network addresses passed as command-line arguments and starts the gRPC server without any connection tracking or limiting mechanisms. [3](#0-2) 

In contrast, the main Aptos network layer implements explicit connection limiting with `inbound_connection_limit` that actively rejects connections when limits are exceeded. [4](#0-3) 

**Attack Path:**
1. Attacker identifies the ExecutorService gRPC port from network reconnaissance or configuration leaks
2. Opens thousands of TCP connections to the service port
3. Completes HTTP2 handshake to establish connections
4. Holds connections open without sending application messages or sends minimal keep-alive traffic
5. Server resources (file descriptors, memory per connection, TCP connection table) become exhausted
6. Legitimate shard-to-shard communication fails or experiences severe delays
7. Block execution slows down or stalls, impacting validator performance and potentially consensus participation

The service also lacks authentication, accepting connections from any network peer that can reach the port. [5](#0-4) 

## Impact Explanation

This vulnerability qualifies as **High Severity** under the Aptos Bug Bounty program criteria because it enables "Validator node slowdowns" - explicitly listed as a High Severity impact category worth up to $50,000.

**Specific Impacts:**
- **Execution Service Degradation**: The ExecutorService handles critical sharded block execution. Resource exhaustion causes execution delays or failures.
- **Consensus Participation Impact**: Slow or stalled execution prevents validators from participating effectively in consensus, potentially causing timeouts and missed block proposals.
- **Cascading Failures**: If multiple shards or coordinators are affected simultaneously, the entire sharded execution system could halt.
- **Resource Exhaustion**: File descriptor exhaustion affects the entire process, potentially impacting other services running in the same process.

The vulnerability breaks the **Resource Limits** invariant: "All operations must respect gas, storage, and computational limits" by allowing unlimited connection consumption without proper bounds checking.

## Likelihood Explanation

**Likelihood: HIGH**

The vulnerability is highly likely to be exploited because:

1. **No Authentication Required**: The gRPC service accepts connections from any network peer without authentication or authorization checks.

2. **Simple Attack Vector**: Opening many TCP connections is trivial using standard tools (e.g., `netcat`, custom scripts, or DDoS tools).

3. **Low Resource Cost for Attacker**: The attacker only needs to establish connections without sending significant data, making the attack cheap to execute.

4. **Network Exposure**: The ExecutorService binds to network addresses that may be reachable by attackers, especially in cloud environments or if firewall rules are misconfigured.

5. **No Built-in Mitigation**: Unlike the main network layer which has explicit connection limits, this service has zero protection at the application level.

6. **Deployment Configuration**: While HAProxy may provide some protection in production deployments, the service itself has no defense, and not all deployments may use HAProxy for internal shard communication.

## Recommendation

Implement comprehensive connection limiting and resource management for the gRPC service:

```rust
async fn start_async(
    self,
    server_addr: SocketAddr,
    rpc_timeout_ms: u64,
    server_shutdown_rx: oneshot::Receiver<()>,
) {
    let reflection_service = tonic_reflection::server::Builder::configure()
        .register_encoded_file_descriptor_set(FILE_DESCRIPTOR_SET)
        .build_v1()
        .unwrap();

    info!("Starting Server async at {:?}", server_addr);
    
    // Add connection limits and HTTP2 settings
    Server::builder()
        .timeout(std::time::Duration::from_millis(rpc_timeout_ms))
        .concurrency_limit_per_connection(100)  // Limit streams per connection
        .tcp_keepalive(Some(std::time::Duration::from_secs(60)))
        .http2_keepalive_interval(Some(std::time::Duration::from_secs(30)))
        .http2_keepalive_timeout(Some(std::time::Duration::from_secs(10)))
        .initial_connection_window_size(Some(1024 * 1024))  // 1MB
        .initial_stream_window_size(Some(512 * 1024))  // 512KB
        .add_service(
            NetworkMessageServiceServer::new(self)
                .max_decoding_message_size(MAX_MESSAGE_SIZE),
        )
        .add_service(reflection_service)
        .serve_with_shutdown(server_addr, async {
            server_shutdown_rx.await.ok();
            info!("Received signal to shutdown server at {:?}", server_addr);
        })
        .await
        .unwrap();
    info!("Server shutdown at {:?}", server_addr);
}
```

**Additional Recommendations:**

1. **Add Connection Tracking**: Implement connection counting and limiting similar to the peer_manager's `inbound_connection_limit` pattern.

2. **Add Authentication**: Implement mutual TLS or Noise protocol authentication to ensure only authorized peers can connect.

3. **Add Rate Limiting**: Use the existing `aptos-rate-limiter` infrastructure to limit connection establishment rate per source IP.

4. **Add Monitoring**: Emit metrics for active connections, rejected connections, and resource usage.

5. **Configuration**: Make connection limits configurable through the service parameters.

## Proof of Concept

```rust
// File: secure/net/tests/connection_exhaustion_test.rs
use aptos_secure_net::grpc_network_service::GRPCNetworkMessageServiceServerWrapper;
use aptos_secure_net::network_controller::{Message, MessageType};
use crossbeam_channel::Sender;
use std::{
    collections::HashMap,
    net::{IpAddr, Ipv4Addr, SocketAddr},
    sync::{Arc, Mutex},
    thread,
    time::Duration,
};
use tokio::{runtime::Runtime, sync::oneshot};

#[test]
fn test_connection_exhaustion_vulnerability() {
    // Setup server
    let server_port = 9999;
    let server_addr = SocketAddr::new(IpAddr::V4(Ipv4Addr::LOCALHOST), server_port);
    
    let server_handlers: Arc<Mutex<HashMap<MessageType, Sender<Message>>>> =
        Arc::new(Mutex::new(HashMap::new()));
    
    let (msg_tx, _msg_rx) = crossbeam_channel::unbounded();
    server_handlers
        .lock()
        .unwrap()
        .insert(MessageType::new("test".to_string()), msg_tx);
    
    let server = GRPCNetworkMessageServiceServerWrapper::new(
        server_handlers,
        server_addr,
    );
    
    let rt = Runtime::new().unwrap();
    let (_server_shutdown_tx, server_shutdown_rx) = oneshot::channel();
    
    server.start(&rt, "test".to_string(), server_addr, 5000, server_shutdown_rx);
    
    // Wait for server to start
    thread::sleep(Duration::from_millis(100));
    
    // Attack: Open many connections without sending data
    let mut connections = Vec::new();
    let target_connections = 1000;
    
    for i in 0..target_connections {
        match std::net::TcpStream::connect_timeout(
            &server_addr,
            Duration::from_millis(100),
        ) {
            Ok(conn) => {
                // Set TCP_NODELAY to keep connection alive
                conn.set_nodelay(true).ok();
                connections.push(conn);
                
                if i % 100 == 0 {
                    println!("Opened {} connections", i + 1);
                }
            }
            Err(e) => {
                println!("Connection {} failed: {:?}", i, e);
                break;
            }
        }
    }
    
    println!("Successfully opened {} connections", connections.len());
    println!("Connections remain open, consuming server resources...");
    
    // Verify that connections are consuming resources
    // In a real attack, the server would slow down or crash
    thread::sleep(Duration::from_secs(5));
    
    // This test demonstrates that there's no limit preventing connection exhaustion
    // In production, this would exhaust file descriptors and memory
    assert!(
        connections.len() >= target_connections / 2,
        "Should be able to open many connections without limit"
    );
}
```

**Notes:**
- This PoC demonstrates that the server accepts arbitrary numbers of connections without limits
- In a real attack scenario with higher connection counts, this would exhaust file descriptors (typically 1024-65536 per process) and memory
- The lack of authentication means any network peer can execute this attack
- Production impact would include execution delays, consensus participation issues, and potential validator penalties

### Citations

**File:** secure/net/src/grpc_network_service/mod.rs (L75-86)
```rust
        Server::builder()
            .timeout(std::time::Duration::from_millis(rpc_timeout_ms))
            .add_service(
                NetworkMessageServiceServer::new(self).max_decoding_message_size(MAX_MESSAGE_SIZE),
            )
            .add_service(reflection_service)
            .serve_with_shutdown(server_addr, async {
                server_shutdown_rx.await.ok();
                info!("Received signal to shutdown server at {:?}", server_addr);
            })
            .await
            .unwrap();
```

**File:** secure/net/src/grpc_network_service/mod.rs (L93-115)
```rust
    async fn simple_msg_exchange(
        &self,
        request: Request<NetworkMessage>,
    ) -> Result<Response<Empty>, Status> {
        let _timer = NETWORK_HANDLER_TIMER
            .with_label_values(&[&self.self_addr.to_string(), "inbound_msgs"])
            .start_timer();
        let remote_addr = request.remote_addr();
        let network_message = request.into_inner();
        let msg = Message::new(network_message.message);
        let message_type = MessageType::new(network_message.message_type);

        if let Some(handler) = self.inbound_handlers.lock().unwrap().get(&message_type) {
            // Send the message to the registered handler
            handler.send(msg).unwrap();
        } else {
            error!(
                "No handler registered for sender: {:?} and msg type {:?}",
                remote_addr, message_type
            );
        }
        Ok(Response::new(Empty {}))
    }
```

**File:** execution/executor-service/src/remote_executor_service.rs (L30-31)
```rust
        let service_name = format!("executor_service-{}", shard_id);
        let mut controller = NetworkController::new(service_name, self_address, 5000);
```

**File:** secure/net/src/network_controller/inbound_handler.rs (L44-62)
```rust
    pub fn start(&self, rt: &Runtime) -> Option<oneshot::Sender<()>> {
        if self.inbound_handlers.lock().unwrap().is_empty() {
            return None;
        }

        let (server_shutdown_tx, server_shutdown_rx) = oneshot::channel();
        // The server is started in a separate task
        GRPCNetworkMessageServiceServerWrapper::new(
            self.inbound_handlers.clone(),
            self.listen_addr,
        )
        .start(
            rt,
            self.service.clone(),
            self.listen_addr,
            self.rpc_timeout_ms,
            server_shutdown_rx,
        );
        Some(server_shutdown_tx)
```

**File:** network/framework/src/peer_manager/mod.rs (L351-388)
```rust
        // Verify that we have not reached the max connection limit for unknown inbound peers
        if conn.metadata.origin == ConnectionOrigin::Inbound {
            // Everything below here is meant for unknown peers only. The role comes from
            // the Noise handshake and if it's not `Unknown` then it is trusted.
            if conn.metadata.role == PeerRole::Unknown {
                // TODO: Keep track of somewhere else to not take this hit in case of DDoS
                // Count unknown inbound connections
                let unknown_inbound_conns = self
                    .active_peers
                    .iter()
                    .filter(|(peer_id, (metadata, _))| {
                        metadata.origin == ConnectionOrigin::Inbound
                            && trusted_peers
                                .get(peer_id)
                                .is_none_or(|peer| peer.role == PeerRole::Unknown)
                    })
                    .count();

                // Reject excessive inbound connections made by unknown peers
                // We control outbound connections with Connectivity manager before we even send them
                // and we must allow connections that already exist to pass through tie breaking.
                if !self
                    .active_peers
                    .contains_key(&conn.metadata.remote_peer_id)
                    && unknown_inbound_conns + 1 > self.inbound_connection_limit
                {
                    info!(
                        NetworkSchema::new(&self.network_context)
                            .connection_metadata_with_address(&conn.metadata),
                        "{} Connection rejected due to connection limit: {}",
                        self.network_context,
                        conn.metadata
                    );
                    counters::connections_rejected(&self.network_context, conn.metadata.origin)
                        .inc();
                    self.disconnect(conn);
                    return;
                }
```
