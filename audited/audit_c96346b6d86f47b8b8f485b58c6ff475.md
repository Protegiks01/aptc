# Audit Report

## Title
Capacity Check Bypass in Batch Encryption Digest Causes Denial of Service via Duplicate Ciphertexts

## Summary
The `digest()` function in the batch encryption system performs a capacity check based on total ciphertext count (including duplicates) rather than unique ciphertext IDs. This allows a malicious validator to cause block decryption failures by including duplicate encrypted transactions that push the total count beyond the setup limit, even when the number of unique ciphertexts is within acceptable bounds. [1](#0-0) 

## Finding Description
The vulnerability exists in how the batch encryption system handles duplicate ciphertexts during digest computation. When `FPTXWeighted::digest()` is called with a slice of ciphertexts, it creates an `IdSet` from the ciphertext IDs without deduplication: [2](#0-1) 

The `IdSet::from_slice()` method accepts all IDs including duplicates: [3](#0-2) 

The capacity is set to `ids.len().next_power_of_two()`, which includes duplicates. When the digest is computed, it checks:

`ids.capacity() > self.tau_powers_g1[round].len() - 1`

This check uses the total count (including duplicates), not the unique count. A malicious validator can exploit this by proposing a block with:
- K unique encrypted transactions where K ≤ N (setup limit)
- M duplicate copies where (K+M).next_power_of_two() > N
- Result: digest() fails even though only K unique ciphertexts need processing

In the consensus pipeline, encrypted transactions reach `decrypt_encrypted_txns` before deduplication occurs: [4](#0-3) 

The deduplication happens later in the `prepare` phase after decryption: [5](#0-4) 

**Attack Path:**
1. Malicious validator proposes block containing duplicate encrypted transactions
2. Other validators receive block and begin decryption pipeline
3. `materialize_block` fetches transactions (no deduplication yet)
4. `decrypt_encrypted_txns` is called with duplicate ciphertexts
5. `FPTXWeighted::digest()` creates IdSet with duplicate IDs
6. Capacity check: `(K+M).next_power_of_two() > N` fails
7. digest() returns error, causing entire block decryption to fail
8. Block is rejected, round wasted, all validators wasted computation

## Impact Explanation
This vulnerability meets **Medium Severity** criteria ($10,000) per the Aptos bug bounty program:
- **State inconsistencies requiring intervention**: Failed block decryption disrupts normal consensus flow
- **Validator node slowdowns**: All validators waste computation attempting to decrypt an intentionally malformed batch
- **Temporary consensus disruption**: Forces a round to be wasted and requires moving to next proposer

The attack does not cause:
- Permanent network partition (validators recover automatically)
- Fund loss or theft
- Consensus safety violations (no chain split)
- Permanent liveness failure

However, it creates a temporary denial of service vector that can be repeatedly exploited by Byzantine validators to slow consensus and waste computational resources across all validators in the network.

## Likelihood Explanation
**Likelihood: Medium**

**Attacker Requirements:**
- Control of a validator node (within Byzantine tolerance)
- Ability to propose blocks with duplicate transactions
- Knowledge of setup capacity limits

**Complexity:**
- Low - straightforward to execute once validator access is obtained
- No complex cryptographic attacks required
- Simply include N duplicate encrypted transactions in proposal

**Realistic Scenario:**
A compromised or malicious validator can systematically propose blocks with duplicate encrypted transactions to degrade network performance. While the system's Byzantine fault tolerance (< 1/3 malicious validators) prevents permanent damage, repeated exploitation would significantly impact consensus efficiency and user experience.

## Recommendation
Deduplicate ciphertext IDs before capacity checking and polynomial construction:

```rust
// In IdSet::from_slice or digest() function
pub fn from_slice_deduplicated(ids: &[Id]) -> Option<Self> {
    let mut seen = std::collections::HashSet::new();
    let unique_ids: Vec<Id> = ids.iter()
        .filter(|id| seen.insert(**id))
        .cloned()
        .collect();
    
    let mut result = Self::with_capacity(unique_ids.len())?;
    for id in unique_ids {
        result.add(&id);
    }
    Some(result)
}
```

Additionally, add early validation in `decrypt_encrypted_txns` to reject blocks with duplicate ciphertexts before expensive digest computation.

## Proof of Concept
```rust
#[test]
fn test_digest_duplicate_ciphertext_dos() {
    use crate::schemes::fptx_weighted::FPTXWeighted;
    use crate::traits::BatchThresholdEncryption;
    
    // Setup supports max 7 unique ciphertexts
    let setup_size = 7;
    let (ek, dk, _, _) = FPTXWeighted::setup_for_testing(
        42, 
        setup_size, 
        1, 
        &WeightedConfigArkworks::new(1, vec![1])
    ).unwrap();
    
    // Create 4 unique ciphertexts
    let mut cts = vec![];
    for i in 0..4 {
        let ct = ek.encrypt(
            &mut thread_rng(), 
            &format!("msg{}", i), 
            &String::from("")
        ).unwrap();
        cts.push(ct);
    }
    
    // Add 4 duplicates: total = 8, but only 4 unique
    // capacity = 8.next_power_of_two() = 8 > 7 → FAILS
    cts.extend(cts[0..4].to_vec());
    
    // This should succeed (4 unique ≤ 7) but fails due to capacity check
    let result = FPTXWeighted::digest(&dk, &cts, 0);
    assert!(result.is_err()); // Demonstrates the bug
    
    // Without duplicates, it works fine
    let result_no_dup = FPTXWeighted::digest(&dk, &cts[0..4], 0);
    assert!(result_no_dup.is_ok());
}
```

## Notes
The vulnerability is exacerbated by the `next_power_of_two()` rounding in capacity calculation, which can amplify the effect of even a small number of duplicates. The mathematical correctness of using the same evaluation proof for duplicate IDs is sound (the polynomial has repeated roots), but the capacity check implementation creates an exploitable denial of service vector.

### Citations

**File:** crates/aptos-batch-encryption/src/shared/digest.rs (L106-136)
```rust
    pub fn digest(
        &self,
        ids: &mut IdSet<UncomputedCoeffs>,
        round: u64,
    ) -> Result<(Digest, EvalProofsPromise)> {
        let round: usize = round as usize;
        if round >= self.tau_powers_g1.len() {
            Err(anyhow!(
                "Tried to compute digest with round greater than setup length."
            ))
        } else if ids.capacity() > self.tau_powers_g1[round].len() - 1 {
            Err(anyhow!(
                "Tried to compute a batch digest with size {}, where setup supports up to size {}",
                ids.capacity(),
                self.tau_powers_g1[round].len() - 1
            ))?
        } else {
            let ids = ids.compute_poly_coeffs();
            let mut coeffs = ids.poly_coeffs();
            coeffs.resize(self.tau_powers_g1[round].len(), Fr::zero());

            let digest = Digest {
                digest_g1: G1Projective::msm(&self.tau_powers_g1[round], &coeffs)
                    .unwrap()
                    .into(),
                round,
            };

            Ok((digest.clone(), EvalProofsPromise::new(digest, ids)))
        }
    }
```

**File:** crates/aptos-batch-encryption/src/schemes/fptx_weighted.rs (L100-111)
```rust
                .signature_share_eval)
            })
            .collect::<Result<Vec<G1Affine>>>()?;

        Ok((
            self.weighted_player,
            evals_raw
                .into_iter()
                .map(|eval| BIBEDecryptionKeyShareValue {
                    signature_share_eval: eval,
                })
                .collect(),
```

**File:** crates/aptos-batch-encryption/src/shared/ids/mod.rs (L63-89)
```rust
    pub fn from_slice(ids: &[Id]) -> Option<Self> {
        let mut result = Self::with_capacity(ids.len())?;
        for id in ids {
            result.add(id);
        }
        Some(result)
    }

    pub fn with_capacity(capacity: usize) -> Option<Self> {
        let capacity = capacity.next_power_of_two();
        Some(Self {
            poly_roots: Vec::new(),
            capacity,
            poly_coeffs: UncomputedCoeffs,
        })
    }

    pub fn capacity(&self) -> usize {
        self.capacity
    }

    pub fn add(&mut self, id: &Id) {
        if self.poly_roots.len() >= self.capacity {
            panic!("Number of ids must be less than capacity");
        }
        self.poly_roots.push(id.root_x);
    }
```

**File:** consensus/src/pipeline/decryption_pipeline_builder.rs (L78-93)
```rust
        let txn_ciphertexts: Vec<Ciphertext> = encrypted_txns
            .iter()
            .map(|txn| {
                // TODO(ibalajiarun): Avoid clone and use reference instead
                txn.payload()
                    .as_encrypted_payload()
                    .expect("must be a encrypted txn")
                    .ciphertext()
                    .clone()
            })
            .collect();

        // TODO(ibalajiarun): Consider using commit block height to reduce trusted setup size
        let encryption_round = block.round();
        let (digest, proofs_promise) =
            FPTXWeighted::digest(&digest_key, &txn_ciphertexts, encryption_round)?;
```

**File:** consensus/src/block_preparer.rs (L89-104)
```rust
        // Transaction filtering, deduplication and shuffling are CPU intensive tasks, so we run them in a blocking task.
        let result = tokio::task::spawn_blocking(move || {
            let filtered_txns = filter_block_transactions(
                txn_filter_config,
                block_id,
                block_author,
                block_epoch,
                block_timestamp_usecs,
                txns,
            );
            let deduped_txns = txn_deduper.dedup(filtered_txns);
            let mut shuffled_txns = {
                let _timer = TXN_SHUFFLE_SECONDS.start_timer();

                txn_shuffler.shuffle(deduped_txns)
            };
```
