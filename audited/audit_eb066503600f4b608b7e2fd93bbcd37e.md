# Audit Report

## Title
Unverified Storage Layer Proofs Allow Consensus Divergence via Compromised DbReader

## Summary
The `ProvableStateSummary::get_proof()` function performs only probabilistic verification (1 in 10,000) of proofs returned by `DbReader.get_state_proof_by_version_ext()`. A compromised storage layer can return malicious Sparse Merkle proofs that are used directly to reconstruct state trees without cryptographic verification, potentially causing validators to compute divergent state roots.

## Finding Description
During block execution, validators update their state Sparse Merkle Trees by fetching proofs from their local storage layer (DbReader) for persisted nodes. The `ProvableStateSummary::get_proof()` method implements only random sampling verification: [1](#0-0) 

The critical issue is at lines 318-322: 9,999 out of 10,000 times, proofs are fetched directly via `get_state_proof_by_version_ext()` without any verification against the known state root. These unverified proofs are then used by the Sparse Merkle Tree updater to reconstruct tree structure: [2](#0-1) 

The proofs are materialized into tree nodes without validation: [3](#0-2) 

During block execution, each validator independently executes blocks and computes state checkpoint hashes: [4](#0-3) 

Critically, block execution passes `None` for `known_state_checkpoints`, meaning there is no external validation of the computed state root: [5](#0-4) 

The computed state root becomes part of TransactionInfo and is used for consensus voting: [6](#0-5) 

Each validator votes based on its locally computed state: [7](#0-6) 

**Attack Path:**
1. Attacker compromises a validator's storage layer (via state sync attack with corrupted snapshot, storage layer bug, or direct manipulation)
2. Compromised DbReader returns malicious proofs with incorrect sibling hashes or leaf values
3. 9,999/10,000 times, these proofs pass through without verification
4. Malicious proofs cause incorrect Sparse Merkle Tree reconstruction
5. Validator computes incorrect state checkpoint hash
6. If multiple validators have identically corrupted storage (coordinated state sync attack), they vote on wrong state roots
7. If >1/3 validators affected, network cannot reach consensus on correct state

## Impact Explanation
This vulnerability breaks **Critical Invariant #1: "Deterministic Execution - All validators must produce identical state roots for identical blocks"**.

**Severity: Critical** - Consensus Safety Violation

While a single validator with corrupted storage would simply fail to participate (liveness issue), a coordinated attack affecting multiple validators through state sync could cause:
- **Consensus divergence**: Different validator sets computing different state roots
- **Network partition risk**: If ≥1/3 validators have corrupted storage, the network cannot commit blocks
- **State inconsistency**: Requires manual intervention or potential hard fork to recover

This meets Aptos Bug Bounty **Critical Severity** criteria for "Consensus/Safety violations" and "Non-recoverable network partition."

## Likelihood Explanation
**Likelihood: Medium**

The attack requires:
- Compromising validator storage (via state sync from malicious peer, storage bugs, or disk corruption)
- Bypassing the 1/10,000 random verification (99.99% success rate per proof fetch)
- Affecting multiple validators for network-wide impact

The probabilistic verification provides minimal protection. The TODO comment at line 300 indicates this is a known limitation: "we cannot verify proof yet."

## Recommendation
Implement mandatory cryptographic verification of all proofs before use:

1. **Always verify proofs against the persisted state root** - Remove the probabilistic sampling and verify every proof:
   - Verify proof against `global_state_summary.root_hash()` for all cold state proofs
   - Verify proof against `hot_state_summary.root_hash()` for all hot state proofs

2. **Add state snapshot verification** - During state sync, cryptographically verify all received state data against trusted ledger info before persisting to storage

3. **Implement proof caching with verification** - Cache verified proofs in memory to amortize verification cost

4. **Add storage integrity checks** - Periodically verify storage consistency against checkpointed roots

## Proof of Concept
```rust
// Reproduction: Malicious DbReader returning incorrect proofs
struct MaliciousDbReader {
    inner: Arc<dyn DbReader>,
    corrupt_key: HashValue,
}

impl DbReader for MaliciousDbReader {
    fn get_state_proof_by_version_ext(
        &self,
        key_hash: &HashValue,
        version: Version,
        root_depth: usize,
        use_hot_state: bool,
    ) -> Result<SparseMerkleProofExt> {
        let mut proof = self.inner.get_state_proof_by_version_ext(
            key_hash, version, root_depth, use_hot_state
        )?;
        
        if *key_hash == self.corrupt_key {
            // Return proof with malicious sibling hash
            // This will cause incorrect tree reconstruction
            proof = create_malicious_proof(key_hash);
        }
        Ok(proof)
    }
    // ... other methods delegate to inner
}

// When multiple validators use this corrupted storage:
// 1. Execute identical block
// 2. Compute different state roots (divergence)
// 3. Vote on different state IDs
// 4. Consensus fails to reach quorum on correct state
```

## Notes
The vulnerability exists because the code explicitly trusts DbReader to return correct proofs 99.99% of the time. While the 1/10,000 verification provides some error detection, it is insufficient to prevent a determined attacker or systematic storage corruption. The impact severity depends on how many validators are affected - a single corrupted validator causes only liveness issues, but coordinated corruption of ≥1/3 validators would cause consensus failure.

### Citations

**File:** storage/storage-interface/src/state_store/state_summary.rs (L293-323)
```rust
    fn get_proof(
        &self,
        key: &HashValue,
        version: Version,
        root_depth: usize,
        use_hot_state: bool,
    ) -> Result<SparseMerkleProofExt> {
        // TODO(HotState): we cannot verify proof yet. In order to verify the proof, we need to
        // fetch and construct the corresponding `HotStateValue` for `key` at `version`, including
        // `hot_since_version`. However, the current in-memory hot state does not support this
        // query, and we might need persist hot state KV to db first.
        if !use_hot_state && rand::random::<usize>() % 10000 == 0 {
            // 1 out of 10000 times, verify the proof.
            let (val_opt, proof) = self
                .db
                // check the full proof
                .get_state_value_with_proof_by_version_ext(
                    key, version, /* root_depth = */ 0, /* use_hot_state = */ false,
                )?;
            proof.verify(
                self.state_summary.global_state_summary.root_hash(),
                *key,
                val_opt.as_ref(),
            )?;
            Ok(proof)
        } else {
            Ok(self
                .db
                .get_state_proof_by_version_ext(key, version, root_depth, use_hot_state)?)
        }
    }
```

**File:** storage/scratchpad/src/sparse_merkle/updater.rs (L150-166)
```rust
    fn from_persisted(
        a_descendant_key: &HashValue,
        depth: usize,
        proof_reader: &impl ProofRead,
    ) -> Result<Self> {
        let proof = proof_reader
            .get_proof(a_descendant_key, depth)
            .ok_or(UpdateError::MissingProof)?;
        if depth > proof.bottom_depth() {
            return Err(UpdateError::ShortProof {
                key: *a_descendant_key,
                num_siblings: proof.bottom_depth(),
                depth,
            });
        }
        Ok(Self::new_on_proof_path(proof, depth))
    }
```

**File:** storage/scratchpad/src/sparse_merkle/updater.rs (L263-278)
```rust
    fn materialize(self, generation: u64) -> InMemSubTreeInfo {
        match self {
            Self::InMem(info) => info,
            Self::Persisted(info) => match info {
                PersistedSubTreeInfo::Leaf { leaf } => {
                    InMemSubTreeInfo::create_leaf_with_proof(&leaf, generation)
                },
                PersistedSubTreeInfo::ProofSibling { hash } => {
                    InMemSubTreeInfo::create_unknown(hash)
                },
                PersistedSubTreeInfo::ProofPathInternal { .. } => {
                    unreachable!()
                },
            },
        }
    }
```

**File:** execution/executor/src/block_executor/mod.rs (L315-320)
```rust
                output.set_state_checkpoint_output(DoStateCheckpoint::run(
                    &output.execution_output,
                    parent_block.output.ensure_result_state_summary()?,
                    &ProvableStateSummary::new_persisted(self.db.reader.as_ref())?,
                    None,
                )?);
```

**File:** execution/executor/src/workflow/do_state_checkpoint.rs (L18-42)
```rust
    pub fn run(
        execution_output: &ExecutionOutput,
        parent_state_summary: &LedgerStateSummary,
        persisted_state_summary: &ProvableStateSummary,
        known_state_checkpoints: Option<Vec<Option<HashValue>>>,
    ) -> Result<StateCheckpointOutput> {
        let _timer = OTHER_TIMERS.timer_with(&["do_state_checkpoint"]);

        let state_summary = parent_state_summary.update(
            persisted_state_summary,
            &execution_output.hot_state_updates,
            execution_output.to_commit.state_update_refs(),
        )?;

        let state_checkpoint_hashes = Self::get_state_checkpoint_hashes(
            execution_output,
            known_state_checkpoints,
            &state_summary,
        )?;

        Ok(StateCheckpointOutput::new(
            state_summary,
            state_checkpoint_hashes,
        ))
    }
```

**File:** types/src/transaction/mod.rs (L2025-2051)
```rust
pub struct TransactionInfoV0 {
    /// The amount of gas used.
    gas_used: u64,

    /// The vm status. If it is not `Executed`, this will provide the general error class. Execution
    /// failures and Move abort's receive more detailed information. But other errors are generally
    /// categorized with no status code or other information
    status: ExecutionStatus,

    /// The hash of this transaction.
    transaction_hash: HashValue,

    /// The root hash of Merkle Accumulator storing all events emitted during this transaction.
    event_root_hash: HashValue,

    /// The hash value summarizing all changes caused to the world state by this transaction.
    /// i.e. hash of the output write set.
    state_change_hash: HashValue,

    /// The root hash of the Sparse Merkle Tree describing the world state at the end of this
    /// transaction. Depending on the protocol configuration, this can be generated periodical
    /// only, like per block.
    state_checkpoint_hash: Option<HashValue>,

    /// The hash value summarizing PersistedAuxiliaryInfo.
    auxiliary_info_hash: Option<HashValue>,
}
```

**File:** consensus/src/round_manager.rs (L1500-1527)
```rust
    async fn vote_block(&mut self, proposed_block: Block) -> anyhow::Result<Vote> {
        let block_arc = self
            .block_store
            .insert_block(proposed_block)
            .await
            .context("[RoundManager] Failed to execute_and_insert the block")?;

        // Short circuit if already voted.
        ensure!(
            self.round_state.vote_sent().is_none(),
            "[RoundManager] Already vote on this round {}",
            self.round_state.current_round()
        );

        ensure!(
            !self.sync_only(),
            "[RoundManager] sync_only flag is set, stop voting"
        );

        let vote_proposal = block_arc.vote_proposal();
        let vote_result = self.safety_rules.lock().construct_and_sign_vote_two_chain(
            &vote_proposal,
            self.block_store.highest_2chain_timeout_cert().as_deref(),
        );
        let vote = vote_result.context(format!(
            "[RoundManager] SafetyRules Rejected {}",
            block_arc.block()
        ))?;
```
