# Audit Report

## Title
Database Handle Confusion in validate_db_data() Causes Validation Against Wrong Database Instance

## Summary
The `validate_db_data()` function in the database debugger module hardcodes the assumption that storage sharding is enabled, causing it to validate against sharded database files instead of the actual production data when run on non-sharded databases. This results in false validation results that could lead operators to accept corrupted data or reject valid data.

## Finding Description

The `validate_db_data()` function has a critical database handle confusion bug where it unconditionally opens sharded databases regardless of the production database's actual configuration. [1](#0-0) 

The function makes two problematic database opening calls:

1. **Line 70**: Calls `verify_state_kvs()` which unconditionally opens the sharded StateKvDb: [2](#0-1) 

2. **Line 72**: Opens AptosDB using `new_for_test_with_sharding()` which forces `enable_storage_sharding: true`: [3](#0-2) 

**The Core Issue**: AptosDB supports both sharded and non-sharded configurations. When `enable_storage_sharding: false`, state KV data is stored in the ledger_db, not in separate sharded files: [4](#0-3) 

However, `StateKvDb::open_sharded()` always opens the sharded databases and sets `enabled_sharding: true`: [5](#0-4) 

**When This Breaks**:

If a production database was created with `enable_storage_sharding: false` (devnet, localnet, test environments, or pre-AIP-97 migration):
- Actual state KV data resides in ledger_db using `StateValueSchema`
- Sharded database files either don't exist, are empty, or contain stale data
- Due to `create_if_missing: true` in RocksDB options, non-existent sharded databases are created as empty
- The validation checks empty/wrong databases while the real data in ledger_db is never validated
- This produces **false negatives**: validation passes when actual data could be corrupted

## Impact Explanation

This vulnerability qualifies as **Medium Severity** under "State inconsistencies requiring intervention" because:

1. **Database Integrity Validation is Security-Critical**: Operators use this tool to verify database integrity before critical operations like backup restoration, post-corruption verification, and migration validation.

2. **False Negative Risk**: If the tool reports a corrupted database as valid, operators may:
   - Restore from corrupted backups
   - Promote corrupted databases to production
   - Continue operating with corrupted data
   - Propagate bad state across the network

3. **Consensus Risk**: If corrupted data passes validation and is used in production, it could cause:
   - State inconsistencies across validator nodes
   - Different nodes having different state roots
   - Potential consensus violations if deterministic execution is compromised

4. **Scope**: While non-sharded databases are primarily used in test environments, the tool's purpose is to be a trusted verification mechanism. Breaking this trust could have cascading effects on operational security.

## Likelihood Explanation

**Likelihood: Medium**

This issue will occur whenever:
1. An operator runs `validate-indexer-db` on any database created with `enable_storage_sharding: false`
2. This includes devnet, localnet, test databases, and any pre-AIP-97 legacy databases

While modern mainnet/testnet databases require sharding to be enabled (enforced by ConfigOptimizer): [6](#0-5) 

The validation tool is designed to work with any database, including those from test environments or older nodes. The tool doesn't check or respect the original database configuration, making this bug always trigger on non-sharded databases.

## Recommendation

The `validate_db_data()` function must detect the actual database configuration and open database handles accordingly. Two approaches:

**Option 1: Detect Sharding Configuration**
```rust
pub fn validate_db_data(
    db_root_path: &Path,
    internal_indexer_db_path: &Path,
    mut target_ledger_version: u64,
) -> Result<()> {
    // First detect if the database uses sharding by checking for shard directories
    let storage_dir = StorageDirPaths::from_path(db_root_path);
    let enable_sharding = storage_dir.state_kv_db_shard_root_path(0).exists();
    
    // Open AptosDB with correct sharding configuration
    let aptos_db = if enable_sharding {
        AptosDB::new_for_test_with_sharding(db_root_path, 1000000)
    } else {
        AptosDB::new_for_test(db_root_path)
    };
    
    // Verify state KVs using appropriate method
    if enable_sharding {
        verify_state_kvs_sharded(db_root_path, &internal_db, target_ledger_version)?;
    } else {
        verify_state_kvs_nonsharded(db_root_path, &internal_db, target_ledger_version)?;
    }
    
    // ... rest of validation
}
```

**Option 2: Accept Sharding Configuration as Parameter**
```rust
pub struct ValidationArgs {
    #[clap(short, long)]
    pub db_root_path: String,
    
    #[clap(short, long)]
    pub internal_indexer_db_path: String,
    
    #[clap(short, long)]
    pub target_version: u64,
    
    #[clap(long, default_value = "true")]
    pub enable_sharding: bool,
}
```

Either approach ensures the validation tool opens the same database configuration as production.

## Proof of Concept

```rust
// Create a non-sharded database
let temp_dir = TempPath::new();
let db = AptosDB::new_for_test(&temp_dir); // Non-sharded

// Write some state KV data
let mut batch = SchemaBatch::new();
batch.put::<StateValueSchema>(&(state_key.clone(), version), &state_value)?;
db.ledger_db.metadata_db().write_schemas(batch)?;

// Run validation tool
let internal_indexer_path = create_test_indexer_db();
validate_db_data(
    &temp_dir,
    &internal_indexer_path,
    version,
)?;

// Result: Validation will pass even though it checked empty sharded DBs
// The actual data in ledger_db was never validated
// If ledger_db data is corrupted, it won't be detected
```

**To reproduce**:
1. Create a database with `enable_storage_sharding: false`
2. Populate it with state KV data via normal operations
3. Run `aptos-debugger aptos-db debug indexer-validation validate-indexer-db --db-root-path <path> --internal-indexer-db-path <path> --target-version <version>`
4. Observe that validation checks non-existent/empty sharded databases instead of actual ledger_db data

## Notes

This is a database handle confusion vulnerability where the validation code makes incorrect assumptions about database configuration. While the default configuration for modern nodes has sharding enabled, the validation tool is designed to work with any database and should respect the actual configuration rather than hardcoding assumptions.

### Citations

**File:** storage/aptosdb/src/db_debugger/validation.rs (L57-112)
```rust
pub fn validate_db_data(
    db_root_path: &Path,
    internal_indexer_db_path: &Path,
    mut target_ledger_version: u64,
) -> Result<()> {
    let num_threads = 30;
    ThreadPoolBuilder::new()
        .num_threads(num_threads)
        .build_global()
        .unwrap();
    let internal_db =
        open_internal_indexer_db(internal_indexer_db_path, &RocksdbConfig::default())?;

    verify_state_kvs(db_root_path, &internal_db, target_ledger_version)?;

    let aptos_db = AptosDB::new_for_test_with_sharding(db_root_path, 1000000);
    let batch_size = 20_000;
    let start_version = aptos_db.get_first_txn_version()?.unwrap();
    target_ledger_version = std::cmp::min(
        aptos_db.get_synced_version()?.unwrap(),
        target_ledger_version,
    );
    assert!(
        start_version < target_ledger_version,
        "{}, {}",
        start_version,
        target_ledger_version
    );
    println!(
        "Validating events and transactions {}, {}",
        start_version, target_ledger_version
    );

    // Calculate ranges and split into chunks
    let ranges: Vec<(u64, u64)> = (start_version..target_ledger_version)
        .step_by(batch_size as usize)
        .map(|start| {
            let end = cmp::min(start + batch_size, target_ledger_version);
            (start, end)
        })
        .collect();

    // Process each chunk in parallel
    ranges.into_par_iter().for_each(|(start, end)| {
        let num_of_txns = end - start;
        println!("Validating transactions from {} to {}", start, end);
        let txns = aptos_db
            .get_transactions(start, num_of_txns, target_ledger_version, true)
            .unwrap();
        verify_batch_txn_events(&txns, &internal_db, start)
            .unwrap_or_else(|_| panic!("{}, {} failed to verify", start, end));
        assert_eq!(txns.get_num_transactions() as u64, num_of_txns);
    });

    Ok(())
}
```

**File:** storage/aptosdb/src/db_debugger/validation.rs (L114-146)
```rust
pub fn verify_state_kvs(
    db_root_path: &Path,
    internal_db: &DB,
    target_ledger_version: u64,
) -> Result<()> {
    println!("Validating db statekeys");
    let storage_dir = StorageDirPaths::from_path(db_root_path);
    let state_kv_db =
        StateKvDb::open_sharded(&storage_dir, RocksdbConfig::default(), None, None, false)?;

    //read all statekeys from internal db and store them in mem
    let mut all_internal_keys = HashSet::new();
    let mut iter = internal_db.iter::<StateKeysSchema>()?;
    iter.seek_to_first();
    for (key_ind, state_key_res) in iter.enumerate() {
        let state_key = state_key_res?.0;
        let state_key_hash = state_key.hash();
        all_internal_keys.insert(state_key_hash);
        if key_ind % 10_000_000 == 0 {
            println!("Processed {} keys", key_ind);
        }
    }
    println!(
        "Number of state keys in internal db: {}",
        all_internal_keys.len()
    );
    for shard_id in 0..16 {
        let shard = state_kv_db.db_shard(shard_id);
        println!("Validating state_kv for shard {}", shard_id);
        verify_state_kv(shard, &all_internal_keys, target_ledger_version)?;
    }
    Ok(())
}
```

**File:** storage/aptosdb/src/db/aptosdb_testonly.rs (L42-63)
```rust
    /// This opens db with sharding enabled.
    pub fn new_for_test_with_sharding<P: AsRef<Path> + Clone>(
        db_root_path: P,
        max_node_cache: usize,
    ) -> Self {
        let db_config = RocksdbConfigs {
            enable_storage_sharding: true,
            ..Default::default()
        };
        Self::open(
            StorageDirPaths::from_path(db_root_path),
            false,
            NO_OP_STORAGE_PRUNER_CONFIG, /* pruner */
            db_config,
            false, /* indexer */
            BUFFERED_STATE_TARGET_ITEMS_FOR_TEST,
            max_node_cache,
            None,
            HotStateConfig::default(),
        )
        .expect("Unable to open AptosDB")
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L54-80)
```rust
    pub(crate) fn new(
        db_paths: &StorageDirPaths,
        rocksdb_configs: RocksdbConfigs,
        env: Option<&Env>,
        block_cache: Option<&Cache>,
        readonly: bool,
        ledger_db: Arc<DB>,
    ) -> Result<Self> {
        let sharding = rocksdb_configs.enable_storage_sharding;
        if !sharding {
            info!("State K/V DB is not enabled!");
            return Ok(Self {
                state_kv_metadata_db: Arc::clone(&ledger_db),
                state_kv_db_shards: arr![Arc::clone(&ledger_db); 16],
                hot_state_kv_db_shards: None,
                enabled_sharding: false,
            });
        }

        Self::open_sharded(
            db_paths,
            rocksdb_configs.state_kv_db_config,
            env,
            block_cache,
            readonly,
        )
    }
```

**File:** storage/aptosdb/src/state_kv_db.rs (L82-171)
```rust
    pub(crate) fn open_sharded(
        db_paths: &StorageDirPaths,
        state_kv_db_config: RocksdbConfig,
        env: Option<&Env>,
        block_cache: Option<&Cache>,
        readonly: bool,
    ) -> Result<Self> {
        let state_kv_metadata_db_path =
            Self::metadata_db_path(db_paths.state_kv_db_metadata_root_path());

        let state_kv_metadata_db = Arc::new(Self::open_db(
            state_kv_metadata_db_path.clone(),
            STATE_KV_METADATA_DB_NAME,
            &state_kv_db_config,
            env,
            block_cache,
            readonly,
            /* is_hot = */ false,
        )?);

        info!(
            state_kv_metadata_db_path = state_kv_metadata_db_path,
            "Opened state kv metadata db!"
        );

        let state_kv_db_shards = (0..NUM_STATE_SHARDS)
            .into_par_iter()
            .map(|shard_id| {
                let shard_root_path = db_paths.state_kv_db_shard_root_path(shard_id);
                let db = Self::open_shard(
                    shard_root_path,
                    shard_id,
                    &state_kv_db_config,
                    env,
                    block_cache,
                    readonly,
                    /* is_hot = */ false,
                )
                .unwrap_or_else(|e| panic!("Failed to open state kv db shard {shard_id}: {e:?}."));
                Arc::new(db)
            })
            .collect::<Vec<_>>()
            .try_into()
            .unwrap();

        let hot_state_kv_db_shards = if readonly {
            // TODO(HotState): do not open it in readonly mode yet, until we have this DB
            // everywhere.
            None
        } else {
            Some(
                (0..NUM_STATE_SHARDS)
                    .into_par_iter()
                    .map(|shard_id| {
                        let shard_root_path = db_paths.hot_state_kv_db_shard_root_path(shard_id);
                        let db = Self::open_shard(
                            shard_root_path,
                            shard_id,
                            &state_kv_db_config,
                            env,
                            block_cache,
                            readonly,
                            /* is_hot = */ true,
                        )
                        .unwrap_or_else(|e| {
                            panic!("Failed to open hot state kv db shard {shard_id}: {e:?}.")
                        });
                        Arc::new(db)
                    })
                    .collect::<Vec<_>>()
                    .try_into()
                    .unwrap(),
            )
        };

        let state_kv_db = Self {
            state_kv_metadata_db,
            state_kv_db_shards,
            hot_state_kv_db_shards,
            enabled_sharding: true,
        };

        if !readonly {
            if let Some(overall_kv_commit_progress) = get_state_kv_commit_progress(&state_kv_db)? {
                truncate_state_kv_db_shards(&state_kv_db, overall_kv_commit_progress)?;
            }
        }

        Ok(state_kv_db)
    }
```

**File:** config/src/config/storage_config.rs (L664-668)
```rust
            if (chain_id.is_testnet() || chain_id.is_mainnet())
                && config_yaml["rocksdb_configs"]["enable_storage_sharding"].as_bool() != Some(true)
            {
                panic!("Storage sharding (AIP-97) is not enabled in node config. Please follow the guide to migration your node, and set storage.rocksdb_configs.enable_storage_sharding to true explicitly in your node config. https://aptoslabs.notion.site/DB-Sharding-Migration-Public-Full-Nodes-1978b846eb7280b29f17ceee7d480730");
            }
```
